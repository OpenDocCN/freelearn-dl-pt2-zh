- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序
- en: We've seen big changes in **Natural Language Processing** (**NLP**) over the
    last 20 years. During this time, we have experienced different paradigms and finally
    entered a new era dominated by the magical transformer architecture. This deep
    learning architecture has come about by inheriting many approaches. Contextual
    word embeddings, multi-head self-attention, positional encoding, parallelizable
    architectures, model compression, transfer learning, and cross-lingual models
    are among those approaches. Starting with the help of various neural-based NLP
    approaches, the transformer architecture gradually evolved into an attention-based
    encoder-decoder architecture and continues to evolve to this day. Now, we are
    seeing new successful variants of this architecture in the literature. Great models
    have emerged that use only the encoder part of it, such as BERT, or only the decoder
    part of it, such as GPT.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的20年间，我们在**自然语言处理**（**NLP**）领域已经见证了巨大的变化。在此期间，我们经历了不同的范式，最终进入了由神奇的变压器架构主宰的新时代。这种深度学习架构是通过继承多种方法而形成的。诸如上下文词嵌入、多头自注意力、位置编码、可并行化的架构、模型压缩、迁移学习和跨语言模型等方法都在其中。从各种基于神经网络的自然语言处理方法开始，变压器架构逐渐演变成为一个基于注意力的编码器-解码器架构，并持续至今。现在，我们在文献中看到了这种架构的新成功变体。有些出色的模型只使用了其编码器部分，比如BERT，或者只使用了其解码器部分，比如GPT。
- en: Throughout the book, we will touch on these NLP approaches and will be able
    to work with transformer models easily thanks to the Transformers library from
    the Hugging Face community. We will provide the solutions step by step to a wide
    variety of NLP problems, ranging from summarization to question-answering. We
    will see that we can achieve state-of-the-art results with the help of transformers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将涉及这些自然语言处理方法，并且能够轻松使用来自Hugging Face社区的Transformers库与变压器模型进行交互。我们将逐步提供各种自然语言处理问题的解决方案，涵盖从摘要到问答等广泛的话题。我们将看到，借助变压器的帮助，我们可以取得最先进的结果。
- en: Who this book is for
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书的受众
- en: This book is for deep learning researchers, hands-on NLP practitioners, and
    machine learning/NLP educators and students who want to start their journey with
    the transformer architecture. Beginner-level machine learning knowledge and a
    good command of Python will help you get the most out of this book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书适合深度学习研究人员、实践型自然语言处理从业者，以及希望以变压器架构开始自己学习之旅的机器学习/自然语言处理教育者和学生。初级机器学习知识和良好的Python掌握能力将帮助您更好地理解本书的内容。
- en: What this book covers
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖的内容
- en: '[*Chapter 1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016), *From Bag-of-Words
    to the Transformers*, provides a brief introduction to the history of NLP, providing
    a comparison between traditional methods, deep learning models such as CNNs, RNNs,
    and LSTMs, and transformer models.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第一章*](B17123_01_Epub_AM.xhtml#_idTextAnchor016)，*从词袋模型到变压器*，简要介绍了自然语言处理的历史，对比了传统方法、深度学习模型（如CNN、RNN和LSTM）与变压器模型。'
- en: '[*Chapter 2*](B17123_02_Epub_AM.xhtml#_idTextAnchor034), *A Hands-On Introduction
    to the Subject*, takes a deeper look at how a transformer model can be used. Tokenizers
    and models such as BERT will be described with hands-on examples.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第二章*](B17123_02_Epub_AM.xhtml#_idTextAnchor034)，*主题的实践导论*，深入探讨了如何使用变压器模型。我们将通过实例描述分词器和BERT等模型。'
- en: '[*Chapter 3*](B17123_03_Epub_AM.xhtml#_idTextAnchor050), *Autoencoding Language
    Models*, is where you will gain knowledge about how to train autoencoding language
    models on any given language from scratch. This training will include pretraining
    and the task-specific training of models.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第三章*](B17123_03_Epub_AM.xhtml#_idTextAnchor050)，*自编码语言模型*，将让您了解如何从头开始在任何给定语言上训练自编码语言模型。此训练将包括模型的预训练和特定任务的训练。'
- en: '[*Chapter 4*](B17123_04_Epub_AM.xhtml#_idTextAnchor067), *Autoregressive and
    Other Language Models*, explores the theoretical details of autoregressive language
    models and teaches you about pretraining them on their own corpus. You will learn
    how to pretrain any language model such as GPT-2 on their own text and use the
    model in various tasks such as language generation.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第四章*](B17123_04_Epub_AM.xhtml#_idTextAnchor067)，*自回归和其他语言模型*，探讨了自回归语言模型的理论细节，并教会您如何在其特定语料库上进行预训练。您将学会如何在自己的文本上预训练GPT-2等任何语言模型，并将其用于诸如语言生成等各种任务。'
- en: '[*Chapter 5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081), *Fine-Tuning Language
    Models for Text Classification*, is where you will learn how to configure a pre-trained
    model for text classification and how to fine-tune it for any text classification
    downstream task, such as sentiment analysis or multi-class classification.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 5 章*](B17123_05_Epub_AM.xhtml#_idTextAnchor081)，*微调语言模型进行文本分类*，是您将学习如何配置预训练模型进行文本分类以及如何为任何文本分类下游任务，例如情感分析或多类分类进行微调的地方。'
- en: '[*Chapter 6*](B17123_06_Epub_AM.xhtml#_idTextAnchor090), *Fine-Tuning Language
    Models for Token Classification*, teaches you how to fine-tune language models
    for token classification tasks such as NER, POS tagging, and question-answering.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 6 章*](B17123_06_Epub_AM.xhtml#_idTextAnchor090)，*微调语言模型进行标记分类*，教您如何微调语言模型以用于诸如
    NER、POS 标注和问答等标记分类任务。'
- en: '[*Chapter 7*](B17123_07_Epub_AM.xhtml#_idTextAnchor099), *Text Representation*,
    is where you will learn about text representation techniques and how to efficiently
    utilize the transformer architecture, especially for unsupervised tasks such as
    clustering, semantic search, and topic modeling.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 7 章*](B17123_07_Epub_AM.xhtml#_idTextAnchor099)，*文本表示*，是您将学习有关文本表示技术以及如何有效利用
    Transformer 架构，特别是对于无监督任务，例如聚类、语义搜索和主题建模的地方。'
- en: '[*Chapter 8*](B17123_08_Epub_AM.xhtml#_idTextAnchor116), *Working with Efficient
    Transformers*, shows you how to make efficient models out of trained models by
    using distillation, pruning, and quantization. Then, you will gain knowledge about
    efficient sparse transformers, such as Linformer and BigBird, and how to work
    with them.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 8 章*](B17123_08_Epub_AM.xhtml#_idTextAnchor116)，*高效 Transformer 的工作*，向您展示了如何通过蒸馏、修剪和量化将经过训练的模型制作成高效模型。然后，您将了解到关于高效稀疏
    Transformer 的知识，例如 Linformer 和 BigBird，以及如何与它们一起工作。'
- en: '[*Chapter 9*](B17123_09_Epub_AM.xhtml#_idTextAnchor129), *Cross-Lingual and
    Multilingual Language Modeling*, is where you will learn about multilingual and
    cross-lingual language model pretraining and the difference between monolingual
    and multilingual pretraining. Causal language modeling and translation language
    modeling are the other topics covered in the chapter.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 9 章*](B17123_09_Epub_AM.xhtml#_idTextAnchor129)，*跨语言和多语言语言建模*，是您将学习有关多语言和跨语言语言模型预训练以及单语和多语预训练之间的区别的地方。该章节还涵盖了因果语言建模和翻译语言建模等其他主题。'
- en: '[*Chapter 10*](B17123_10_Epub_AM.xhtml#_idTextAnchor144), *Serving Transformer
    Models*, will detail how to serve transformer-based NLP solutions in environments
    where CPU/GPU is available. Using **TensorFlow Extended** (**TFX**) for machine
    learning deployment will be described here also.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 10 章*](B17123_10_Epub_AM.xhtml#_idTextAnchor144)，*服务 Transformer 模型*，将详细介绍如何在具有
    CPU/GPU 的环境中提供基于 Transformer 的 NLP 解决方案。还将在此处描述使用 **TensorFlow Extended**（**TFX**）进行机器学习部署。'
- en: '[*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152), *Attention Visualization
    and Experiment Tracking*, will cover two different technical concepts: attention
    visualization and experiment tracking. We will practice them using sophisticated
    tools such as exBERT and BertViz.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 11 章*](B17123_11_Epub_AM.xhtml#_idTextAnchor152)，*注意力可视化和实验跟踪*，将涵盖两个不同的技术概念：注意力可视化和实验跟踪。我们将使用诸如
    exBERT 和 BertViz 等复杂工具来进行实践。'
- en: To get the most out of this book
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要充分利用本书
- en: To follow this book, you need to have a basic knowledge of the Python programming
    language. It is also a required that you know the basics of NLP, deep learning,
    and how deep neural networks work.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要遵循本书，您需要具备 Python 编程语言的基本知识。您还需要了解自然语言处理、深度学习以及深度神经网络的基础知识。
- en: Important note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: All the code in this book has been executed in the Python 3.6 version since
    some of the libraries in the Python 3.9 version are in development stages.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有代码都是在 Python 3.6 版本中执行的，因为 Python 3.9 版本中的一些库仍处于开发阶段。
- en: '![](img/B17123_Preface_Table_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17123_Preface_Table_01.jpg)'
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book''s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting of code.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您使用的是本书的数字版，我们建议您自己输入代码或从本书的 GitHub 代码库中获取代码（链接在下一节中提供）。这样做可以帮助您避免与复制粘贴代码相关的任何潜在错误。**'
- en: Download the example code files
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Mastering-Transformers](https://github.com/PacktPublishing/Mastering-Transformers).
    If there's an update to the code, it will be updated in the GitHub repository.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从GitHub上下载本书的示例代码文件，网址为[https://github.com/PacktPublishing/Mastering-Transformers](https://github.com/PacktPublishing/Mastering-Transformers)。若代码有更新，将在GitHub仓库中更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有其他代码捆绑包，来自我们丰富的图书和视频目录，可在[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)找到。欢迎查阅！
- en: Code in Action
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实践
- en: The Code in Action videos for this book can be viewed at [https://bit.ly/3i4vFzJ](https://bit.ly/3i4vFzJ).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码实践视频可在[https://bit.ly/3i4vFzJ](https://bit.ly/3i4vFzJ)观看。
- en: Download the color images
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图片
- en: 'We also provide a PDF file that has color images of the screenshots and diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781801077651_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781801077651_ColorImages.pdf).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个PDF文件，其中包含本书中使用的截图和图表的彩色图像。您可以在此处下载：[https://static.packt-cdn.com/downloads/9781801077651_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781801077651_ColorImages.pdf)。
- en: Conventions used
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的惯例
- en: There are a number of text conventions used throughout this book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本惯例。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: "Sequences that are shorter than `max_sen_len` (maximum
    sentence length) are padded with a `PAD` value until they are `max_sen_len` in
    length."'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`文本中的代码`：表示文本中的代码单词、数据库表名、文件夹名、文件名、文件扩展名、路径名、废弃的URL、用户输入和Twitter句柄。举个例子："短于`max_sen_len`（最大句子长度）的序列将使用`PAD`值填充，直到其长度达到`max_sen_len`。"'
- en: 'A block of code is set as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块显示如下：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起您对代码块的特别关注时，相关行或条目将以粗体显示：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出均以以下形式呈现：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    "We must now take care of the computational cost of a particular model for a given
    environment (**Random Access Memory** (**RAM**), CPU, and GPU) in terms of memory
    usage and speed."'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要词汇或屏幕上看到的词语。例如，菜单或对话框中的词以**粗体**显示。举个例子："现在我们必须关注特定模型在给定环境下的计算成本（**随机存取存储器（RAM）**，CPU和GPU），包括内存使用和速度。"'
- en: Tips or important notes
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或重要笔记
- en: Appear like this.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如此呈现。
- en: Get in touch
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常欢迎读者的反馈。
- en: '`customercare@packtpub.com` and mention the book title in the subject of your
    message.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 发送邮件至`customercare@packtpub.com`，并在主题中提及书名。
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)
    and fill in the form.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误表**：尽管我们竭尽全力确保内容准确无误，但错误不可避免。如果您在本书中发现错误，我们将不胜感激您向我们报告。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)并填写表格。'
- en: '`copyright@packt.com` with a link to the material.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 发送至`copyright@packt.com`，并附上材料链接。
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您有兴趣成为作者**：如果您在某个专题上有专长并且有兴趣撰写或贡献书籍，请访问[authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share Your Thoughts
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you've read *Mastering Transformers*, we'd love to hear your thoughts!
    Please click here to go straight to the Amazon review page for this book and share
    your feedback.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完《掌握变压器》后，我们很想听听您的想法！请点击此处前往亚马逊翻页，分享您的反馈。
- en: Your review is important to us and the tech community and will help us make
    sure we're delivering excellent quality content.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您的评论对我们和技术社区非常重要，将帮助我们确保我们提供优质的内容。
