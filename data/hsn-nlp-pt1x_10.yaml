- en: '*Chapter 7*: Text Translation Using Sequence-to-Sequence Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 7 章*：使用序列到序列神经网络进行文本翻译'
- en: In the previous two chapters, we used neural networks to classify text and perform
    sentiment analysis. Both tasks involve taking an NLP input and predicting some
    value. In the case of our sentiment analysis, this was a number between 0 and
    1 representing the sentiment of our sentence. In the case of our sentence classification
    model, our output was a multi-class prediction, of which there were several categories
    our sentence belonged to. But what if we wish to make not just a single prediction,
    but predict a whole sentence? In this chapter, we will build a sequence-to-sequence
    model that takes a sentence in one language as input and outputs the translation
    of this sentence in another language.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们使用神经网络来分类文本并执行情感分析。这两项任务都涉及接收 NLP 输入并预测某个值。在情感分析中，这是一个介于 0 和 1 之间的数字，表示我们句子的情感。在句子分类模型中，我们的输出是一个多类别预测，表示句子属于的几个类别之一。但如果我们希望不仅仅是进行单一预测，而是预测整个句子呢？在本章中，我们将构建一个序列到序列模型，将一个语言中的句子作为输入，并输出这个句子在另一种语言中的翻译。
- en: We have already explored several types of neural network architecture used for
    NLP learning, namely recurrent neural networks in [*Chapter 5*](B12365_05_Final_JC_ePub.xhtml#_idTextAnchor092)*,
    Recurrent Neural Networks and Sentiment Analysis*, and convolutional neural networks
    in [*Chapter 6*](B12365_06_Final_JC_ePub.xhtml#_idTextAnchor112)*, Text Classification
    Using CNNs*. In this chapter, we will again be using these familiar RNNs, but
    instead of just building a simple RNN model, we will use RNNs as part of a larger,
    more complex model in order to perform sequence-to-sequence translation. By using
    the underpinnings of RNNs that we learned about in the previous chapters, we can
    show how these concepts can be extended in order to create a variety of models
    that can be fit for purpose.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 5 章*](B12365_05_Final_JC_ePub.xhtml#_idTextAnchor092)*，递归神经网络和情感分析*中，我们已经探讨了用于
    NLP 学习的几种类型的神经网络架构，即递归神经网络，以及[*第 6 章*](B12365_06_Final_JC_ePub.xhtml#_idTextAnchor112)*，使用
    CNN 进行文本分类*中的卷积神经网络。在本章中，我们将再次使用这些熟悉的 RNN，但不再仅构建简单的 RNN 模型，而是将 RNN 作为更大、更复杂模型的一部分，以执行序列到序列的翻译。通过利用我们在前几章学到的
    RNN 基础知识，我们可以展示如何扩展这些概念，以创建适合特定用途的各种模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Theory of sequence-to-sequence models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型的理论
- en: Building a sequence-to-sequence neural network for text translation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为文本翻译构建序列到序列神经网络
- en: Next steps
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后续步骤
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码都可以在[https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x)找到。
- en: Theory of sequence-to-sequence models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到序列模型的理论
- en: Sequence-to-sequence models are very similar to the conventional neural network
    structures we have seen so far. The main difference is that for a model's output,
    we expect another sequence, rather than a binary or multi-class prediction. This
    is particularly useful in tasks such as translation, where we may wish to convert
    a whole sentence into another language.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，序列到序列模型与我们迄今所见的传统神经网络结构非常相似。其主要区别在于，对于模型的输出，我们期望得到另一个序列，而不是一个二进制或多类别预测。这在翻译等任务中特别有用，我们希望将一个完整的句子转换成另一种语言。
- en: 'In the following example, we can see that our English-to-Spanish translation
    maps word to word:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们可以看到我们的英语到西班牙语翻译将单词映射到单词：
- en: '![Figure 7.1 – English to Spanish translation](img/B12365_07_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 英语到西班牙语翻译](img/B12365_07_01.jpg)'
- en: Figure 7.1 – English to Spanish translation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 英语到西班牙语翻译
- en: 'The first word in our input sentence maps nicely to the first word in our output
    sentence. If this were the case for all languages, we could simply pass each word
    in our sentence one by one through our trained model to get an output sentence,
    and there would be no need for any sequence-to-sequence modeling, as shown here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入句子中的第一个单词与输出句子中的第一个单词非常匹配。如果所有语言都是这种情况，我们可以简单地通过我们训练过的模型逐个传递我们句子中的每个单词来获得一个输出句子，那么就不需要进行任何序列到序列建模，如本例所示：
- en: '![Figure 7.2 – English-to-Spanish translation of words'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 英语到西班牙语的单词翻译](img/B12365_07_01.jpg)'
- en: '](img/B12365_07_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_02.jpg)'
- en: Figure 7.2 – English-to-Spanish translation of words
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 英语到西班牙语单词的翻译
- en: 'However, we know from our experience with NLP that language is not as simple
    as this! Single words in one language may map to multiple words in other languages,
    and the order in which these words occur in a grammatically correct sentence may
    not be the same. Therefore, we need a model that can capture the context of a
    whole sentence and output a correct translation, not a model that aims to directly
    translate individual words. This is where sequence-to-sequence modeling becomes
    essential, as seen here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们从自然语言处理的经验中知道，语言并不像这么简单！一种语言中的单词可能映射到另一种语言中的多个单词，并且这些单词在语法正确的句子中出现的顺序可能不同。因此，我们需要一个能够捕获整个句子上下文并输出正确翻译的模型，而不是直接翻译单个单词的模型。这就是序列到序列建模变得至关重要的地方，正如在这里所看到的：
- en: '![Figure 7.3 – Sequence-to-sequence modeling for translation'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – 用于翻译的序列到序列建模'
- en: '](img/B12365_07_03.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_03.jpg)'
- en: Figure 7.3 – Sequence-to-sequence modeling for translation
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 用于翻译的序列到序列建模
- en: 'To train a sequence-to-sequence model that captures the context of the input
    sentence and translates this into an output sentence, we will essentially train
    two smaller models that allow us to do this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个序列到序列模型，捕捉输入句子的上下文并将其转换为输出句子，我们基本上会训练两个较小的模型来实现这一点：
- en: An **encoder** model, which captures the context of our sentence and outputs
    it as a single context vector
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**编码器**模型，它捕获我们句子的上下文并将其输出为单个上下文向量
- en: A **decoder**, which takes the context vector representation of our original
    sentence and translates this into a different language
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**解码器**，它接受我们原始句子的上下文向量表示，并将其翻译为另一种语言
- en: 'So, in reality, our full sequence-to-sequence translation model will actually
    look something like this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，实际上，我们的完整序列到序列翻译模型看起来会像这样：
- en: '![Figure 7.4 – Full sequence-to-sequence model'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.4 – 完整的序列到序列模型'
- en: '](img/B12365_07_04.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_04.jpg)'
- en: Figure 7.4 – Full sequence-to-sequence model
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 完整的序列到序列模型
- en: 'By splitting our models into individual encoder and decoder elements, we are
    effectively modularizing our models. This means that if we wish to train multiple
    models to translate from English into different languages, we do not need to retrain
    the whole model each time. We only need to train multiple different decoders to
    transform our context vector into our output sentences. Then, when making predictions,
    we can simply swap out the decoder that we wish to use for our translation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将我们的模型拆分为单独的编码器和解码器元素，我们有效地模块化了我们的模型。这意味着，如果我们希望训练多个模型从英语翻译成不同的语言，我们不需要每次重新训练整个模型。我们只需训练多个不同的解码器来将我们的上下文向量转换为输出句子。然后，在进行预测时，我们可以简单地替换我们希望用于翻译的解码器：
- en: '![Figure 7.5 – Detailed model layout'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – 详细模型布局'
- en: '](img/B12365_07_05.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_05.jpg)'
- en: Figure 7.5 – Detailed model layout
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 详细模型布局
- en: Next, we will examine the encoder and decoder components of the sequence-to-sequence
    model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查序列到序列模型的编码器和解码器组件。
- en: Encoders
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: The purpose of the encoder element of our sequence-to-sequence model is to be
    able to fully capture the context of our input sentence and represent it as a
    vector. We can do this by using RNNs or, more specifically, LSTMs. As you may
    recall from our previous chapters, RNNs take a sequential input and maintain a
    hidden state throughout this sequence. Each new word in the sequence updates the
    hidden state. Then, at the end of the sequence, we can use the model's final hidden
    state as our input into our next layer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们序列到序列模型的编码器元素的目的是能够完全捕获我们输入句子的上下文，并将其表示为向量。我们可以通过使用循环神经网络或更具体地说是长短期记忆网络来实现这一点。正如您可能从我们之前的章节中记得的那样，循环神经网络接受顺序输入并在整个序列中维护隐藏状态。序列中的每个新单词都会更新隐藏状态。然后，在序列结束时，我们可以使用模型的最终隐藏状态作为我们下一层的输入。
- en: 'In the case of our encoder, the hidden state represents the context vector
    representation of our whole sentence, meaning we can use the hidden state output
    of our RNN to represent the entirety of the input sentence:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的编码器的情况下，隐藏状态代表了我们整个句子的上下文向量表示，这意味着我们可以使用RNN的隐藏状态输出来表示整个输入句子：
- en: '![Figure 7.6 – Examining the encoder'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 检查编码器'
- en: '](img/B12365_07_06.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_06.jpg)'
- en: Figure 7.6 – Examining the encoder
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 检查编码器
- en: We use our final hidden state, *h*n, as our context vector, which we will then
    decode using a trained decoder. It is also worth observing that in the context
    of our sequence-to-sequence models, we append "start" and "end" tokens to the
    beginning and end of our input sentence, respectively. This is because our inputs
    and outputs do not have a finite length and our model needs to be able to learn
    when a sentence should end. Our input sentence will always end with an "end" token,
    which signals to the encoder that the hidden state, at this point, will be used
    as the final context vector representation for this input sentence. Similarly,
    in the decoder step, we will see that our decoder will keep generating words until
    it predicts an "end" token. This allows our decoder to generate actual output
    sentences, as opposed to a sequence of tokens of infinite length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的最终隐藏状态 *h^n* 作为我们的上下文向量，然后使用经过训练的解码器来解码它。同时值得注意的是，在我们的序列到序列模型的背景下，我们在输入句子的开头和结尾分别附加了
    "start" 和 "end" 令牌。这是因为我们的输入和输出并没有固定的长度，我们的模型需要能够学习何时结束一个句子。我们的输入句子总是以 "end" 令牌结束，这向编码器表明此时的隐藏状态将被用作该输入句子的最终上下文向量表示。类似地，在解码器步骤中，我们将看到我们的解码器将继续生成词汇，直到预测到一个
    "end" 令牌。这使得我们的解码器能够生成实际的输出句子，而不是无限长度的令牌序列。
- en: Next, we will look at how the decoder takes this context vector and learns to
    translate it into an output sentence.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看解码器如何利用这个上下文向量学习将其翻译成输出句子。
- en: Decoders
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: 'Our decoder takes the final hidden state from our encoder layer and decodes
    this into a sentence in another language. Our decoder is an RNN, similar to that
    of our encoder, but while our encoder updates its hidden state given its current
    hidden state and the current word in the sentence, our decoder updates its hidden
    state and outputs a token at each iteration, given the current hidden state and
    the previous predicted word in the sentence. This can be seen in the following
    diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器接收来自我们编码器层的最终隐藏状态，并将其解码成另一种语言的句子。我们的解码器是一个RNN，类似于我们的编码器，但是在我们的编码器更新其隐藏状态时考虑当前句子中的当前词汇，我们的解码器在每次迭代中更新其隐藏状态并输出一个令牌，考虑到当前的隐藏状态和先前预测的句子中的词汇。可以在以下图表中看到这一点：
- en: '![Figure 7.7 – Examining the decoder'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.7 – 检查解码器'
- en: '](img/B12365_07_07.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[img/B12365_07_07.jpg)'
- en: Figure 7.7 – Examining the decoder
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 检查解码器
- en: First, our model takes the context vector as the final hidden state from our
    encoder step, *h0*. Our model then aims to predict the next word in the sentence,
    given the current hidden state, and then the previous word in the sentence. We
    know our sentence must begin with a "start" token so, at our first step, our model
    tries to predict the first word in the sentence given the previous hidden state,
    *h0*, and the previous word in the sentence (in this instance, the "start" token).
    Our model makes a prediction ("pienso") and then updates the hidden state to reflect
    the new state of the model, *h1*. Then, at the next step, our model uses the new
    hidden state and the last predicted word to predict the next word in the sentence.
    This continues until the model predicts the "end" token, at which point our model
    stops generating output words.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的模型将上下文向量作为我们编码器步骤的最终隐藏状态 *h0*。然后，我们的模型旨在预测句子中的下一个词汇，给定当前隐藏状态，然后是句子中的前一个词汇。我们知道我们的句子必须以一个
    "start" 令牌开始，因此在第一步，我们的模型尝试根据先前的隐藏状态 *h0* 和句子中的先前词汇（在这种情况下是 "start" 令牌）预测句子中的第一个词汇。我们的模型做出预测（"pienso"），然后更新隐藏状态以反映模型的新状态
    *h1*。然后，在下一步中，我们的模型使用新的隐藏状态和上次预测的词汇来预测句子中的下一个词汇。这一过程持续进行，直到模型预测到 "end" 令牌，此时我们的模型停止生成输出词汇。
- en: The intuition behind this model is in line with what we have learned about language
    representations thus far. Words in any given sentence are dependent on the words
    that come before it. So, to predict any given word in a sentence without considering
    the words that have been predicted before it, this would not make sense as words
    in any given sentence are not independent from one another.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型背后的直觉与我们迄今为止对语言表征的理解是一致的。任何给定句子中的词汇都依赖于它之前的词汇。因此，预测句子中的任何给定词汇而不考虑其之前预测的词汇是没有意义的，因为任何给定句子中的词汇都不是彼此独立的。
- en: 'We learn our model parameters as we have done previously: by making a forward
    pass, calculating the loss of our target sentence against the predicted sentence,
    and backpropagating this loss through the network, updating the parameters as
    we go. However, learning using this process can be very slow because, to begin
    with, our model will have very little predictive power. Since our predictions
    for the words in our target sentence are not independent of one another, if we
    predict the first word in our target sentence incorrectly, subsequent words in
    our output sentence are also unlikely to be correct. To help with this process,
    we can use a technique known as **teacher forcing**.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习模型参数的方法与之前相同：通过进行前向传播，计算目标句子与预测句子的损失，并通过网络反向传播此损失，随着过程更新参数。然而，使用这种过程学习可能非常缓慢，因为起初，我们的模型预测能力很弱。由于我们目标句子中的单词预测不是独立的，如果我们错误地预测了目标句子的第一个单词，那么输出句子中的后续单词也可能不正确。为了帮助这个过程，我们可以使用一种称为**教师强迫**的技术。
- en: Using teacher forcing
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用教师强迫
- en: 'As our model does not make good predictions initially, we will find that any
    initial errors are multiplied exponentially. If our first predicted word in the
    sentence is incorrect, then the rest of the sentence will likely be incorrect
    as well. This is because the predictions our model makes are dependent on the
    previous predictions it makes. This means that any losses our model has can be
    multiplied exponentially. Due to this, we may face the exploding gradient problem,
    making it very difficult for our model to learn anything:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型最初预测不良好，我们会发现任何初始错误都会呈指数增长。如果我们在句子中第一个预测的单词不正确，那么句子的其余部分很可能也是错误的。这是因为我们模型的预测依赖于它先前的预测。这意味着我们模型遇到的任何损失都可能会成倍增加。由于此原因，我们可能面临梯度爆炸问题，使得我们的模型很难学习任何东西：
- en: '![Figure 7.8 – Using teacher forcing'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – 使用教师强迫'
- en: '](img/B12365_07_08.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_08.jpg)'
- en: Figure 7.8 – Using teacher forcing
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 使用教师强迫
- en: 'However, by using **teacher forcing**, we train our model using the correct
    previous target word so that one wrong prediction does not inhibit our model''s
    ability to learn from the correct predictions. This means that if our model makes
    an incorrect prediction at one point in the sentence, it can still make correct
    predictions using subsequent words. While our model will still have incorrectly
    predicted words and will have losses by which we can update our gradients, now,
    we do not suffer from exploding gradients, and our model will learn much more
    quickly:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过使用**教师强迫**，我们训练模型时使用正确的先前目标词，这样一次错误预测不会阻碍模型从正确预测中学习。这意味着如果我们的模型在句子的某一点上做出错误预测，它仍然可以使用后续单词进行正确的预测。虽然我们的模型仍然会有错误的预测单词，并且会有损失可以用来更新我们的梯度，但现在我们不再遭受梯度爆炸，我们的模型会学习得更快：
- en: '![Figure 7.9 – Updating for losses'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9 – 更新损失'
- en: '](img/B12365_07_09.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_09.jpg)'
- en: Figure 7.9 – Updating for losses
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 更新损失
- en: You can consider teacher forcing as a way of helping our model learn independently
    of its previous predictions at each time step. This is so the losses that are
    incurred by a mis-prediction at an early time step are not carried over to later
    time steps.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将教师强迫视为一种帮助我们的模型在每个时间步独立学习的方法。这样，由于在早期时间步骤的误预测造成的损失不会传递到后续时间步骤。
- en: By combining the encoder and decoder steps and applying teacher forcing to help
    our model learn, we can build a sequence-to-sequence model that will allow us
    to translate sequences of one language into another. In the next section, we will
    illustrate how we can build this from scratch using PyTorch.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合编码器和解码器步骤，并应用教师强迫来帮助我们的模型学习，我们可以构建一个序列到序列的模型，允许我们将一种语言的序列翻译成另一种语言。在接下来的部分，我们将演示如何使用
    PyTorch 从头开始构建这个模型。
- en: Building a sequence-to-sequence model for text translation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建文本翻译的序列到序列模型
- en: In order to build our sequence-to-sequence model for translation, we will implement
    the encoder/decoder framework we outlined previously. This will show how the two
    halves of our model can be utilized together in order to capture a representation
    of our data using the encoder and then translate this representation into another
    language using our decoder. In order to do this, we need to obtain our data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的序列到序列翻译模型，我们将实现之前概述的编码器/解码器框架。这将展示我们的模型的两个部分如何结合在一起，以通过编码器捕获数据的表示，然后使用解码器将这个表示翻译成另一种语言。为了做到这一点，我们需要获取我们的数据。
- en: Preparing the data
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: By now, we know enough about machine learning to know that for a task like this,
    we will need a set of training data with corresponding labels. In this case, we
    will need `Torchtext` library that we used in the previous chapter contains a
    dataset that will allow us to get this.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解足够多的关于机器学习的知识，知道对于这样的任务，我们需要一组带有相应标签的训练数据。在这种情况下，我们将需要 `Torchtext` 库，我们在前一章中使用的这个库包含一个数据集，可以帮助我们获得这些数据。
- en: The `Multi30k` dataset in `Torchtext` consists of approximately 30,000 sentences
    with corresponding translations in multiple languages. For this translation task,
    our input sentences will be in English and our output sentences will be in German.
    Our fully trained model will, therefore, allow us to **translate English sentences
    into German**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`Torchtext` 中的 `Multi30k` 数据集包含大约 30,000 个句子及其在多种语言中的对应翻译。对于这个翻译任务，我们的输入句子将是英文，输出句子将是德文。因此，我们完全训练好的模型将能够**将英文句子翻译成德文**。'
- en: 'We will start by extracting our data and preprocessing it. We will once again
    use `spacy`, which contains a built-in dictionary of vocabulary that we can use
    to tokenize our data:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始提取和预处理我们的数据。我们将再次使用 `spacy`，它包含一个内置的词汇表字典，我们可以用它来标记化我们的数据：
- en: 'We start by loading our `spacy` tokenizers into Python. We will need to do
    this once for each language we are using since we will be building two entirely
    separate vocabularies for this task:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将 `spacy` 标记器加载到 Python 中。我们将需要为每种语言执行一次此操作，因为我们将为此任务构建两个完全独立的词汇表：
- en: '[PRE0]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要说明
- en: You may have to install the German vocabulary from the command line by doing
    the following (we installed the English vocabulary in the previous chapter):**python3
    -m spacy download de**
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可能需要通过以下命令行安装德语词汇表（我们在前一章中安装了英语词汇表）：**python3 -m spacy download de**
- en: 'Next, we create a function for each of our languages to tokenize our sentences.
    Note that our tokenizer for our input English sentence reverses the order of the
    tokens:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为每种语言创建一个函数来对我们的句子进行标记化。请注意，我们对输入的英文句子进行标记化时会颠倒 token 的顺序：
- en: '[PRE1]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'While reversing the order of our input sentence is not compulsory, it has been
    shown to improve the model’s ability to learn. If our model consists of two RNNs
    joined together, we can show that the information flow within our model is improved
    when reversing the input sentence. For example, let’s take a basic input sentence
    in English but not reverse it, as follows:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然反转输入句子的顺序并非强制性的，但已被证明可以提高模型的学习能力。如果我们的模型由两个连接在一起的 RNN 组成，我们可以展示在反转输入句子时，模型内部的信息流得到了改善。例如，让我们来看一个基本的英文输入句子，但不进行反转，如下所示：
- en: '![Figure 7.10 – Reversing the input words'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.10 – 反转输入单词'
- en: '](img/B12365_07_10.jpg)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_07_10.jpg)'
- en: Figure 7.10 – Reversing the input words
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.10 – 反转输入单词
- en: 'Here, we can see that in order to predict the first output word, *y0*, correctly,
    our first English word from *x0* must travel through three RNN layers before the
    prediction is made. In terms of learning, this means that our gradients must be
    backpropagated through three RNN layers, while maintaining the flow of information
    through the network. Now, let’s compare this to a situation where we reverse our
    input sentence:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到为了正确预测第一个输出词 *y0*，我们的第一个英文单词从 *x0* 必须通过三个 RNN 层后才能进行预测。从学习的角度来看，这意味着我们的梯度必须通过三个
    RNN 层进行反向传播，同时通过网络保持信息的流动。现在，让我们将其与反转输入句子的情况进行比较：
- en: '![Figure 7.11 – Reversing the input sentence'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.11 – 反转输入句子'
- en: '](img/B12365_07_101.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_07_101.jpg)'
- en: Figure 7.11 – Reversing the input sentence
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.11 – 反转输入句子
- en: We can now see that the distance between the true first word in our input sentence
    and the corresponding word in the output sentence is just one RNN layer. This
    means that the gradients only need to be backpropagated to one layer, meaning
    the flow of information and the ability to learn is much greater for our network
    compared to when the distance between these two words was three layers.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们可以看到，输入句子中真正的第一个单词与输出句子中相应单词之间的距离仅为一个 RNN 层。这意味着梯度只需反向传播到一个层，这样网络的信息流和学习能力与输入输出单词之间距离为三层时相比要大得多。
- en: If we were to calculate the total distances between the input words and their
    output counterparts for the reversed and non-reversed variants, we would see that
    they are the same. However, we have seen previously that the most important word
    in our output sentence is the first one. This is because the words in our output
    sentences are dependent on the words that come before them. If we were to predict
    the first word in the output sentence incorrectly, then chances are the rest of
    the words in our sentences would be predicted incorrectly too. However, by predicting
    the first word correctly, we maximize our chances of predicting the whole sentence
    correctly. Therefore, by minimizing the distance between the first word in our
    output sentence and its input counterpart, we can increase our model’s ability
    to learn this relationship. This increases the chances of this prediction being
    correct, thus maximizing the chances of our entire output sentence being predicted
    correctly.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们计算反向和非反向变体中输入单词与其输出对应单词之间的总距离，我们会发现它们是相同的。然而，我们先前已经看到，输出句子中最重要的单词是第一个单词。这是因为输出句子中的单词依赖于它们之前的单词。如果我们错误地预测输出句子中的第一个单词，那么后面的单词很可能也会被错误地预测。然而，通过正确预测第一个单词，我们最大化了正确预测整个句子的机会。因此，通过最小化输出句子中第一个单词与其输入对应单词之间的距离，我们可以增加模型学习这种关系的能力。这增加了此预测正确的机会，从而最大化了整个输出句子被正确预测的机会。
- en: 'With our tokenizers constructed, we now need to define the fields for our tokenization.
    Notice here how we append start and end tokens to our sequences so that our model
    knows when to begin and end the sequence’s input and output. We also convert all
    our input sentences into lowercase for the sake of simplicity:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了我们构建的分词器，现在我们需要为分词定义字段。请注意，在这里我们如何在我们的序列中添加开始和结束标记，以便我们的模型知道何时开始和结束序列的输入和输出。为了简化起见，我们还将所有的输入句子转换为小写：
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With our fields defined, our tokenization becomes a simple one-liner. The dataset
    containing 30,000 sentences has built-in training, validation, and test sets that
    we can use for our model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了字段后，我们的分词化变成了一个简单的一行代码。包含 30,000 个句子的数据集具有内置的训练、验证和测试集，我们可以用于我们的模型：
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can examine individual sentences using the `examples` property of our dataset
    objects. Here, we can see that the source (`src`) property contains our reversed
    input sentence in English and that our target (`trg`) contains our non-reversed
    output sentence in German:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用数据集对象的`examples`属性来检查单个句子。在这里，我们可以看到源（`src`）属性包含我们英语输入句子的反向，而目标（`trg`）包含我们德语输出句子的非反向：
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives us the following output:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这给了我们以下输出：
- en: '![Figure 7.12 – Training data examples'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.12 – 训练数据示例'
- en: '](img/B12365_07_12.jpg)'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/B12365_07_12.jpg)'
- en: Figure 7.12 – Training data examples
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.12 – 训练数据示例
- en: 'Now, we can examine the size of each of our datasets. Here, we can see that
    our training dataset consists of 29,000 examples and that each of our validation
    and test sets consist of 1,014 and 1,000 examples, respectively. In the past,
    we have used 80%/20% splits for the training and validation data. However, in
    instances like this, where our input and output fields are very sparse and our
    training set is of a limited size, it is often beneficial to train on as much
    data as there is available:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以检查每个数据集的大小。在这里，我们可以看到我们的训练数据集包含 29,000 个示例，而每个验证集和测试集分别包含 1,014 和 1,000
    个示例。在过去，我们通常将训练和验证数据拆分为 80%/20%。然而，在像这样输入输出字段非常稀疏且训练集有限的情况下，通常最好利用所有可用数据进行训练：
- en: '[PRE5]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This returns the following output:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '![](img/B12365_07_13.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B12365_07_13.jpg)'
- en: Figure 7.13 – Data sample lengths
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.13 – 数据样本长度
- en: 'Now, we can build our vocabularies and check their size. Our vocabularies should
    consist of every unique word that was found within our dataset. We can see that
    our German vocabulary is considerably larger than our English vocabulary. Our
    vocabularies are significantly smaller than the true size of each vocabulary for
    each language (every word in the English dictionary). Therefore, since our model
    will only be able to accurately translate words it has seen before, it is unlikely
    that our model will be able to generalize well to all possible sentences in the
    English language. This is why training models like this accurately requires extremely
    large NLP datasets (such as those Google has access to):'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以构建我们的词汇表并检查它们的大小。我们的词汇表应包含数据集中发现的每个唯一单词。我们可以看到我们的德语词汇表比我们的英语词汇表大得多。我们的词汇表比每种语言的真实词汇表大小要小得多（英语词典中的每个单词）。因此，由于我们的模型只能准确地翻译它以前见过的单词，我们的模型不太可能能够很好地泛化到英语语言中的所有可能句子。这就是为什么像这样准确训练模型需要极大的NLP数据集（例如Google可以访问的那些）的原因：
- en: '[PRE6]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives the following output:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将得到以下输出：
- en: '![Figure 7.14 – Vocabulary size of the dataset'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.14 – 数据集的词汇量'
- en: '](img/B12365_07_14.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.14 – 数据集的词汇量](img/B12365_07_14.jpg)'
- en: Figure 7.14 – Vocabulary size of the dataset
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.14 – 数据集的词汇量
- en: 'Finally, we can create our data iterators from our datasets. As we did previously,
    we specify the usage of a CUDA-enabled GPU (if it is available on our system)
    and specify our batch size:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以从我们的数据集创建数据迭代器。与以前一样，我们指定使用支持CUDA的GPU（如果系统上可用），并指定我们的批量大小：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that our data has been preprocessed, we can start building the model itself.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据已经预处理完成，我们可以开始构建模型本身。
- en: Building the encoder
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立编码器
- en: 'Now, we are ready to start building our encoder:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始建立我们的编码器：
- en: 'First, we begin by initializing our model by inheriting from our `nn.Module`
    class, as we’ve done with all our previous models. We initialize with a couple
    of parameters, which we will define later, as well as the number of dimensions
    in the hidden layers within our LSTM layers and the number of LSTM layers:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们通过从我们的`nn.Module`类继承来初始化我们的模型，就像我们之前的所有模型一样。我们初始化一些参数，稍后我们会定义，以及我们的LSTM层中隐藏层的维度数和LSTM层的数量：
- en: '[PRE8]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we define our embedding layer within our encoder, which is the length
    of the number of input dimensions and the depth of the number of embedding dimensions:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在编码器内定义我们的嵌入层，这是输入维度数量和嵌入维度数量的长度：
- en: '[PRE9]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define our actual LSTM layer. This takes our embedded sentences from
    the embedding layer, maintains a hidden state of a defined length, and consists
    of a number of layers (which we will define later as 2). We also implement `dropout`
    to apply regularization to our network:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义实际的LSTM层。这需要我们从嵌入层获取嵌入的句子，保持定义长度的隐藏状态，并包括一些层（稍后我们将定义为2）。我们还实现`dropout`以对网络应用正则化：
- en: '[PRE10]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we define the forward pass within our encoder. We apply the embeddings
    to our input sentences and apply dropout. Then, we pass these embeddings through
    our LSTM layer, which outputs our final hidden state. This will be used by our
    decoder to form our translated sentence:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在编码器内定义前向传播。我们将嵌入应用于我们的输入句子并应用dropout。然后，我们将这些嵌入传递到我们的LSTM层，输出我们的最终隐藏状态。这将由我们的解码器用于形成我们的翻译句子：
- en: '[PRE11]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our encoders will consist of two LSTM layers, which means that our output will
    output two hidden states. This also means that our full LSTM layer, along with
    our encoder, will look something like this, with our model outputting two hidden
    states:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的编码器将包括两个LSTM层，这意味着我们的输出将输出两个隐藏状态。这也意味着我们的完整LSTM层，以及我们的编码器，将看起来像这样，我们的模型输出两个隐藏状态：
- en: '![Figure 7.15 – LSTM model with an encoder](img/B12365_07_15.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – 具有编码器的LSTM模型](img/B12365_07_15.jpg)'
- en: Figure 7.15 – LSTM model with an encoder
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – 具有编码器的LSTM模型
- en: Now that we have built our encoder, let's start building our decoder.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经建立了编码器，让我们开始建立我们的解码器。
- en: Building the decoder
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立解码器
- en: 'Our decoder will take the final hidden states from our encoder''s LSTM layer
    and translate them into an output sentence in another language. We start by initializing
    our decoder in almost exactly the same way as we did for the encoder. The only
    difference here is that we also add a fully connected linear layer. This layer
    will use the final hidden states from our LSTM in order to make predictions regarding
    the correct word in the sentence:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器将从编码器的 LSTM 层中获取最终的隐藏状态，并将其转化为另一种语言的输出句子。我们首先通过几乎完全相同的方式初始化我们的解码器，与编码器的方法略有不同的是，我们还添加了一个全连接线性层。该层将使用
    LSTM 的最终隐藏状态来预测句子中正确的单词：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Our forward pass is incredibly similar to that of our encoder, except with
    the addition of two key steps. We first unsqueeze our input from the previous
    layer so that it''s the correct size for entry into our embedding layer. We also
    add a fully connected layer, which takes the output hidden layer of our RNN layers
    and uses it to make a prediction regarding the next word in the sequence:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的前向传播与编码器非常相似，只是增加了两个关键步骤。首先，我们将前一层的输入展开，以使其适合输入到嵌入层中。然后，我们添加一个全连接层，该层接收我们的
    RNN 层的输出隐藏层，并用其来预测序列中的下一个单词：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Again, similar to our encoder, we use a two-layer LSTM layer within our decoder.
    We take our final hidden state from our encoders and use these to generate the
    first word in our sequence, Y1\. We then update our hidden state and use this
    and Y1 to generate our next word, Y2, repeating this process until our model generates
    an end token. Our decoder looks something like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于编码器，我们在解码器内部使用了一个两层的 LSTM 层。我们取出编码器的最终隐藏状态，并用它们生成序列中的第一个单词 Y1\. 然后，我们更新我们的隐藏状态，并使用它和
    Y1 生成下一个单词 Y2，重复此过程，直到我们的模型生成一个结束标记。我们的解码器看起来像这样：
- en: '![Figure 7.16 – LSTM model with a decoder'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.16 – 带有解码器的 LSTM 模型'
- en: '](img/B12365_07_16.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_16.jpg)'
- en: Figure 7.16 – LSTM model with a decoder
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 带有解码器的 LSTM 模型
- en: 'Here, we can see that defining the encoders and decoders individually is not
    particularly complicated. However, when we combine these steps into one larger
    sequence-to-sequence model, things begin to get interesting:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到单独定义编码器和解码器并不特别复杂。然而，当我们将这些步骤组合成一个更大的序列到序列模型时，事情开始变得有趣：
- en: Constructing the full sequence-to-sequence model
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建完整的序列到序列模型
- en: 'We must now stitch the two halves of our model together to produce the full
    sequence-to-sequence model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须将我们模型的两个部分连接起来，以产生完整的序列到序列模型：
- en: 'We start by creating a new sequence-to-sequence class. This will allow us to
    pass our encoder and decoder to it as arguments:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个新的序列到序列类。这将允许我们将编码器和解码器作为参数传递给它：
- en: '[PRE14]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we create the `forward` method within our `Seq2Seq` class. This is arguably
    the most complicated part of the model. We combine our encoder with our decoder
    and use teacher forcing to help our model learn. We start by creating a tensor
    in which we still store our predictions. We initialize this as a tensor full of
    zeroes, but we still update this with our predictions as we make them. The shape
    of our tensor of zeroes will be the length of our target sentence, the width of
    our batch size, and the depth of our target (German) vocabulary size:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在我们的 `Seq2Seq` 类中创建 `forward` 方法。这可以说是模型中最复杂的部分。我们将编码器与解码器结合起来，并使用教师强迫来帮助我们的模型学习。我们首先创建一个张量，其中存储我们的预测。我们将其初始化为一个全零张量，但随着我们生成预测，我们会更新它。全零张量的形状将是目标句子的长度、批量大小的宽度和目标（德语）词汇表大小的深度：
- en: '[PRE15]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we feed our input sentence into our encoder to get the output hidden
    states:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将输入句子传递到编码器中，以获取输出的隐藏状态：
- en: '[PRE16]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we must loop through our decoder model to generate an output prediction
    for each step in our output sequence. The first element of our output sequence
    will always be the `<start>` token. Our target sequences already contain this
    as the first element, so we just set our initial input equal to this by taking
    the first element of the list:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须循环遍历我们的解码器模型，为输出序列中的每个步骤生成一个输出预测。输出序列的第一个元素始终是 `<start>` 标记。我们的目标序列已将其作为第一个元素，因此我们只需将初始输入设置为这个，通过获取列表的第一个元素：
- en: '[PRE17]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we loop through and make our predictions. We pass our hidden states (from
    the output of our encoder) to our decoder, along with our initial input (which
    is just the `<start>` token). This returns a prediction for all the words in our
    sequence. However, we are only interested in the word within our current step;
    that is, the next word in the sequence. Note how we start our loop from 1 instead
    of 0, so our first prediction is the second word in the sequence (as the first
    word that’s predicted will always be the start token).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们循环并进行预测。我们将我们的隐藏状态（从编码器的输出中获得）传递给我们的解码器，以及我们的初始输入（仅是`<start>`标记）。这将为我们序列中的所有单词返回一个预测。然而，我们只对当前步骤中的单词感兴趣；也就是说，序列中的下一个单词。请注意，我们从1开始循环，而不是从0开始，因此我们的第一个预测是序列中的第二个单词（因为始终预测的第一个单词将始终是起始标记）。
- en: This output consists of a vector of the target vocabulary’s length, with a prediction
    for each word within the vocabulary. We take the `argmax` function to identify
    the actual word that is predicted by the model.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此输出由目标词汇长度的向量组成，每个词汇中都有一个预测。我们使用`argmax`函数来识别模型预测的实际单词。
- en: We then need to select our new input for the next step. We set our teacher forcing
    ratio to 50%, which means that 50% of the time, we will use the prediction we
    just made as our next input into our decoder and that the other 50% of the time,
    we will take the true target. As we discussed previously, this helps our model
    learn much more rapidly than relying on just the model’s predictions.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们需要为下一步选择新的输入。我们将我们的教师强制比例设置为50%，这意味着有50%的时间，我们将使用我们刚刚做出的预测作为我们解码器的下一个输入，而另外50%的时间，我们将采用真实的目标值。正如我们之前讨论的那样，这比仅依赖于模型预测能够更快地让我们的模型学习。
- en: 'We then continue this loop until we have a full prediction for each word in
    the sequence:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们继续这个循环，直到我们对序列中的每个单词都有了完整的预测：
- en: '[PRE18]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we create an instance of our Seq2Seq model that’s ready to be trained.
    We initialize an encoder and a decoder with a selection of hyperparameters, all
    of which can be changed to slightly alter the model:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建一个准备好进行训练的Seq2Seq模型的实例。我们使用一些超参数初始化了一个编码器和一个解码器，所有这些超参数都可以稍微改变模型：
- en: '[PRE19]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We then pass our encoder and decoder to our `Seq2Seq` model in order to create
    the complete model:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将我们的编码器和解码器传递给我们的`Seq2Seq`模型，以创建完整的模型：
- en: '[PRE20]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Try experimenting with different parameters here and see how it affects the
    performance of the model. For instance, having a larger number of dimensions in
    your hidden layers may cause the model to train slower, although the overall final
    performance of the model may be better. Alternatively, the model may overfit.
    Often, it is a matter of experimenting to find the best-performing model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在这里用不同的参数进行实验，并查看它们如何影响模型的性能。例如，在隐藏层中使用更大数量的维度可能会导致模型训练速度较慢，尽管最终模型的性能可能会更好。或者，模型可能会过拟合。通常来说，这是一个通过实验来找到最佳性能模型的问题。
- en: After fully defining our Seq2Seq model, we are now ready to begin training it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全定义了我们的Seq2Seq模型之后，我们现在准备开始训练它。
- en: Training the model
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Our model will begin initialized with weights of 0 across all parts of the
    model. While the model should theoretically be able to learn with no (zero) weights,
    it has been shown that initializing with random weights can help the model learn
    faster. Let''s get started:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型将从整个模型的各个部分开始以0权重进行初始化。虽然理论上模型应该能够学习到没有（零）权重的情况，但已经证明使用随机权重初始化可以帮助模型更快地学习。让我们开始吧：
- en: 'Here, we will initialize our model with the weights of random samples from
    a normal distribution, with the values being between -0.1 and 0.1:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将使用从正态分布中随机抽取的随机样本的权重来初始化我们的模型，其值介于-0.1到0.1之间：
- en: '[PRE21]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, as with all our other models, we define our optimizer and loss functions.
    We’re using cross-entropy loss as we are performing multi-class classification
    (as opposed to binary cross-entropy loss for a binary classification):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，与我们的其他所有模型一样，我们定义我们的优化器和损失函数。我们使用交叉熵损失，因为我们正在执行多类别分类（而不是二元交叉熵损失用于二元分类）：
- en: '[PRE22]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we define the training process within a function called `train()`. First,
    we set our model to train mode and set the epoch loss to `0`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在名为`train()`的函数中定义训练过程。首先，我们将模型设置为训练模式，并将epoch损失设置为`0`：
- en: '[PRE23]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We then loop through each batch within our training iterator and extract the
    sentence to be translated (`src`) and the correct translation of this sentence
    (`trg`). We then zero our gradients (to prevent gradient accumulation) and calculate
    the output of our model by passing our model function our inputs and outputs:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在我们的训练迭代器中循环遍历每个批次，并提取要翻译的句子（`src`）和这个句子的正确翻译（`trg`）。然后我们将梯度归零（以防止梯度累积），通过将我们的输入和输出传递给模型函数来计算模型的输出：
- en: '[PRE24]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we need to calculate the loss of our model’s prediction by comparing
    our predicted output to the true, correct translated sentence. We reshape our
    output data and our target data using the shape and view functions in order to
    create two tensors that can be compared to calculate the loss. We calculate the
    `loss` criterion between our output and `trg` tensors and then backpropagate this
    loss through the network:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过比较我们的预测输出和真实的正确翻译句子来计算模型预测的损失。我们使用形状和视图函数来重塑我们的输出数据和目标数据，以便创建两个可以比较的张量，以计算损失。我们在我们的输出和
    `trg` 张量之间计算 `loss` 损失标准，然后通过网络反向传播这个损失：
- en: '[PRE25]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then implement gradient clipping to prevent exploding gradients within our
    model, step our optimizer in order to perform the necessary parameter updates
    via gradient descent, and finally add the loss of the batch to the epoch loss.
    This whole process is repeated for all the batches within a single training epoch,
    whereby the final averaged loss per batch is returned:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们实施梯度裁剪以防止模型内出现梯度爆炸，通过梯度下降来步进我们的优化器执行必要的参数更新，最后将批次的损失添加到 epoch 损失中。这整个过程针对单个训练
    epoch 中的所有批次重复执行，最终返回每批次的平均损失：
- en: '[PRE26]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After, we create a similar function called `evaluate()`. This function will
    calculate the loss of our validation data across the network in order to evaluate
    how our model performs when translating data it hasn’t seen before. This function
    is almost identical to our `train()` function, with the exception of the fact
    that we switch to evaluation mode:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们创建一个名为 `evaluate()` 的类似函数。这个函数将计算整个网络中验证数据的损失，以评估我们的模型在翻译它之前未见的数据时的表现。这个函数几乎与我们的
    `train()` 函数相同，唯一的区别是我们切换到评估模式：
- en: '[PRE27]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since we don’t perform any updates for our weights, we need to make sure to
    implement `no_grad` mode:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们不对权重进行任何更新，我们需要确保实现 `no_grad` 模式：
- en: '[PRE28]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The only other difference is that we need to make sure we turn off teacher
    forcing when in evaluation mode. We wish to assess our model’s performance on
    unseen data, and enabling teacher forcing would use our correct (target) data
    to help our model make better predictions. We want our model to be able to make
    perfect, unaided predictions:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个不同之处在于，我们需要确保在评估模式下关闭教师强迫。我们希望评估模型在未见数据上的表现，并且启用教师强迫将使用正确的（目标）数据来帮助我们的模型做出更好的预测。我们希望我们的模型能够完美地做出预测：
- en: '[PRE29]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we need to create a training loop, within which our `train()` and
    `evaluate()` functions are called. We begin by defining how many epochs we wish
    to train for and our maximum gradient (for use with gradient clipping). We also
    set our lowest validation loss to infinity. This will be used later to select
    our best-performing model:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要创建一个训练循环，在其中调用我们的 `train()` 和 `evaluate()` 函数。我们首先定义我们希望训练的 epoch 数量以及我们的最大梯度（用于梯度裁剪）。我们还将我们的最低验证损失设置为无穷大。稍后将使用它来选择我们表现最佳的模型：
- en: '[PRE30]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We then loop through each of our epochs and within each epoch, calculate our
    training and validation loss using our `train()` and `evaluate()` functions. We
    also time how long this takes by calling `time.time()` before and after the training
    process:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们循环遍历每一个 epoch，在每一个 epoch 中，使用我们的 `train()` 和 `evaluate()` 函数计算训练和验证损失。我们还通过调用
    `time.time()` 函数在训练过程前后计时：
- en: '[PRE31]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, for each epoch, we determine whether the model we just trained is the
    best-performing model we have seen thus far. If our model performs the best on
    our validation data (if the validation loss is the lowest we have seen so far),
    we save our model:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，对于每个 epoch，我们确定刚刚训练的模型是否是迄今为止表现最佳的模型。如果我们的模型在验证数据上表现最佳（如果验证损失是迄今为止最低的），我们会保存我们的模型：
- en: '[PRE32]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we simply print our output:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们简单地打印我们的输出：
- en: '[PRE33]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If our training is working correctly, we should see the training loss decrease
    over time, like so:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们的训练工作正确，我们应该看到训练损失随时间减少，如下所示：
- en: '![Figure 7.17 – Training the model'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.17 – 训练模型'
- en: '](img/B12365_07_17.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_17.jpg)'
- en: Figure 7.17 – Training the model
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 训练模型
- en: Here, we can see that both our training and validation loss appear to be falling
    over time. We can continue to train our model for a number of epochs, ideally
    until the validation loss reaches its lowest possible value. Now, we can evaluate
    our best-performing model to see how well it performs when making actual translations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的训练和验证损失随时间逐渐下降。我们可以继续训练我们的模型多个epochs，理想情况下直到验证损失达到最低可能值。现在，我们可以评估我们表现最佳的模型，看看它在进行实际翻译时的表现如何。
- en: Evaluating the model
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: In order to evaluate our model, we will take our test set of data and run our
    English sentences through our model to obtain a prediction of the translation
    in German. We will then be able to compare this to the true prediction in order
    to see if our model is making accurate predictions. Let's get started!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们将使用我们的测试数据集，将我们的英语句子通过我们的模型，得到翻译成德语的预测。然后，我们将能够将其与真实预测进行比较，以查看我们的模型是否进行了准确的预测。让我们开始吧！
- en: 'We start by creating a `translate()` function. This is functionally identical
    to the `evaluate()` function we created to calculate the loss over our validation
    set. However, this time, we are not concerned with the loss of our model, but
    rather the predicted output. We pass the model our source and target sentences
    and also make sure we turn teacher forcing off so that our model does not use
    these to make predictions. We then take our model’s predictions and use an `argmax`
    function to determine the index of the word that our model predicted for each
    word in our predicted output sentence:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个`translate()`函数。这个函数与我们创建的`evaluate()`函数在功能上是一样的，用来计算验证集上的损失。但是，这一次我们不关心模型的损失，而是关心预测的输出。我们向模型传递源语句和目标语句，并确保关闭教师强制，这样我们的模型不会用它们来进行预测。然后，我们获取模型的预测结果，并使用`argmax`函数来确定我们预测输出句子中每个词的索引：
- en: '[PRE34]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, we can use this index to obtain the actual predicted word from our German
    vocabulary. Finally, we compare the English input to our model that contains the
    correct German sentence and the predicted German sentence. Note that here, we
    use `[1:-1]` to drop the start and end tokens from our predictions and we reverse
    the order of our English input (since the input sentences were reversed before
    they were fed into the model):'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个索引从我们的德语词汇表中获取实际预测的词。最后，我们将英语输入与包含正确德语句子和预测德语句子的模型进行比较。请注意，在这里，我们使用`[1:-1]`来删除预测中的起始和结束标记，并且我们反转了英语输入的顺序（因为输入句子在进入模型之前已经被反转）：
- en: '[PRE35]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'By doing this, we can compare our predicted output with the correct output
    to assess if our model is able to make accurate predictions. We can see from our
    model’s predictions that our model is able to translate English sentences into
    German, albeit far from perfectly. Some of our model’s predictions are exactly
    the same as the target data, showing that our model translated these sentences
    perfectly:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这样做，我们可以将我们的预测输出与正确输出进行比较，以评估我们的模型是否能够进行准确的预测。从我们模型的预测中可以看出，我们的模型能够将英语句子翻译成德语，尽管远非完美。一些模型的预测与目标数据完全相同，表明我们的模型完美地翻译了这些句子：
- en: '![Figure 7.18 – Translation output part one'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.18 – 翻译输出第一部分'
- en: '](img/B12365_07_18.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_18.jpg)'
- en: Figure 7.18 – Translation output part one
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – 翻译输出第一部分
- en: 'In other instances, our model is off by a single word. In this case, our model
    predicts the word `hüten` instead of `mützen`; however, `hüten` is actually an
    acceptable translation of `mützen`, though the words may not be semantically identical:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，我们的模型只差一个词。在这种情况下，我们的模型预测单词`hüten`而不是`mützen`；然而，`hüten`实际上是`mützen`的可接受翻译，尽管这些词在语义上可能不完全相同：
- en: '![Figure 7.19 – Translation output part two'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.19 – 翻译输出第二部分'
- en: '](img/B12365_07_19.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_19.jpg)'
- en: Figure 7.19 – Translation output part two
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 翻译输出第二部分
- en: 'We can also see examples that seem to have been mistranslated. In the following
    example, the English equivalent of the German sentence that we predicted is “`A
    woman climbs through one`”, which is not equivalent to “`Young woman climbing
    rock face`”. However, the model has still managed to translate key elements of
    the English sentence (woman and climbing):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到一些似乎被错误翻译的例子。在下面的例子中，我们预测的德语句子的英语等效句子是“`A woman climbs through one`”，这与“`Young
    woman climbing rock face`”不相等。然而，模型仍然成功翻译了英语句子的关键元素（woman 和 climbing）：
- en: '![Figure 7.20 – Translation output part three'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.20 – 翻译输出第三部分'
- en: '](img/B12365_07_20.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_07_20.jpg)'
- en: Figure 7.20 – Translation output part three
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 – 翻译输出第三部分
- en: Here, we can see that although our model clearly makes a decent attempt at translating
    English into German, it is far from perfect and makes several mistakes. It certainly
    would not be able to fool a native German speaker! Next, we will discuss a couple
    of ways we could improve our sequence-to-sequence translation model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，虽然我们的模型显然尝试着将英语翻译成德语，但远非完美，并且存在多个错误。它肯定无法欺骗一个德语母语者！接下来，我们将讨论如何改进我们的序列到序列翻译模型的几种方法。
- en: Next steps
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: While we have shown our sequence-to-sequence model to be effective at performing
    language translation, the model we trained from scratch is not a perfect translator
    by any means. This is, in part, due to the relatively small size of our training
    data. We trained our model on a set of 30,000 English/German sentences. While
    this might seem very large, in order to train a perfect model, we would require
    a training set that's several orders of magnitude larger.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们展示了我们的序列到序列模型在执行语言翻译方面是有效的，但我们从头开始训练的模型绝不是完美的翻译器。部分原因在于我们训练数据的相对较小规模。我们在一组
    30,000 个英语/德语句子上训练了我们的模型。虽然这可能看起来非常大，但要训练一个完美的模型，我们需要一个几个数量级更大的训练集。
- en: In theory, we would require several examples of each word in the entire English
    and German languages for our model to truly understand its context and meaning.
    For context, the 30,000 English sentences in our training set consisted of just
    6,000 unique words. The average vocabulary of an English speaker is said to be
    between 20,000 and 30,000 words, which gives us an idea of just how many examples
    sentences we would need to train a model that performs perfectly. This is probably
    why the most accurate translation tools are owned by companies with access to
    vast amounts of language data (such as Google).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，我们需要每个单词在整个英语和德语语言中的多个例子，才能使我们的模型真正理解其上下文和含义。就我们训练集中的情况而言，这包括仅有 6,000 个独特单词的
    30,000 个英语句子。据说，一个英语人士的平均词汇量在 20,000 到 30,000 之间，这让我们对需要训练一个完美执行的模型有了一定的了解。这也许是为什么最准确的翻译工具通常由拥有大量语言数据的公司（如
    Google）拥有。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to build sequence-to-sequence models from scratch.
    We learned how to code up our encoder and decoder components individually and
    how to integrate them into a single model that is able to translate sentences
    from one language into another.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何从头开始构建序列到序列模型。我们学习了如何分别编码和解码组件，并如何将它们整合成一个能够将一种语言的句子翻译成另一种语言的单一模型。
- en: Although our sequence-to-sequence model, which consists of an encoder and a
    decoder, is useful for sequence translation, it is no longer state-of-the-art.
    In the last few years, combining sequence-to-sequence models with attention models
    has been done to achieve state-of-the-art performance.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的序列到序列模型包括编码器和解码器，在序列翻译中很有用，但它已不再是最先进的技术。在过去几年中，结合序列到序列模型和注意力模型已经被用来实现最先进的性能。
- en: In the next chapter, we will discuss how attention networks can be used in the
    context of sequence-to-sequence learning and show how we can use both techniques
    to build a chat bot.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论注意力网络如何在序列到序列学习的背景下使用，并展示我们如何同时使用这两种技术来构建聊天机器人。
