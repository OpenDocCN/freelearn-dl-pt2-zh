- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Using Specialized Libraries
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用专门的库
- en: Nobody needs to do all things by themselves. Neither does PyTorch! We already
    know PyTorch is one of the most powerful frameworks for building deep learning
    models. However, as many other tasks are involved in the model-building process,
    PyTorch relies on specialized libraries and tools to get the job done.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人需要自己做所有的事情。PyTorch也不例外！我们已经知道PyTorch是构建深度学习模型最强大的框架之一。然而，在模型构建过程中涉及许多其他任务时，PyTorch依赖于专门的库和工具来完成工作。
- en: In this chapter, we will learn how to install, use, and configure libraries
    to optimize CPU-based training and multithreading.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何安装、使用和配置库来优化基于CPU的训练和多线程。
- en: 'More important than learning the technical nuances presented in this chapter
    is catching the message it brings: we can improve performance by using and configuring
    third-party libraries specialized in tasks that PyTorch relies on. In this sense,
    we can search for many other options than the ones described in this book.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 比学习本章中呈现的技术细节更重要的是捕捉它所带来的信息：通过使用和配置PyTorch依赖的专门库，我们可以改善性能。在这方面，我们可以寻找比本书中描述的选项更多的选择。
- en: 'Here is what you will learn as part of this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的一部分，您将学到以下内容：
- en: Understanding the concept of multithreading with OpenMP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解使用OpenMP进行多线程处理的概念
- en: Learning how to use and configure OpenMP
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用和配置OpenMP
- en: Understanding IPEX – an API for optimizing the usage of PyTorch on Intel processors
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解IPEX – 一种用于优化在Intel处理器上使用PyTorch的API
- en: Understanding how to install and use IPEX
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何安装和使用IPEX
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the complete code of the examples mentioned in this chapter in
    the book’s GitHub repository at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的GitHub存储库中找到本章提到的所有示例代码，网址为[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main)。
- en: You can access your favorite environment to execute this notebook, such as Google
    Colab or Kaggle.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问您喜欢的环境来执行此笔记本，例如Google Colab或Kaggle。
- en: Multithreading with OpenMP
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenMP进行多线程处理
- en: '**OpenMP** is a library used for parallelizing tasks by harnessing all the
    power of multicore processors by using the multithreading technique. In the context
    of PyTorch, OpenMP is employed to parallelize operations executed in the training
    phase and to accelerate preprocessing tasks related to data augmentation, normalization,
    and so forth.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenMP**是一个库，通过使用多线程技术，利用多核处理器的全部性能来并行化任务。在PyTorch的上下文中，OpenMP被用于并行化在训练阶段执行的操作，以及加速与数据增强、归一化等相关的预处理任务。'
- en: As multithreading is a key concept here, to see how OpenMP works, follow me
    to the next section to understand this technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程是这里的一个关键概念，要了解OpenMP是如何工作的，请跟我进入下一节来理解这项技术。
- en: What is multithreading?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是多线程？
- en: Multithreading is a technique to **parallelize tasks** in a multicore system,
    which, in turn, is a computer system endowed with **multicore processors**. Nowadays,
    any computing system has multicore processors; smartphones, notebooks, and even
    TVs have CPUs with more than one processing core.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程是在多核系统中**并行化任务**的一种技术，这种系统配备有**多核处理器**。如今，任何计算系统都配备有多核处理器；智能手机、笔记本电脑甚至电视都配备了具有多个处理核心的CPU。
- en: 'As an example, let’s look at the notebook that I’m using right now to write
    this book. My notebook possesses one Intel i5-8265U processor, which has eight
    cores, as illustrated in *Figure 4**.1*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们看看我现在用来写这本书的笔记本。我的笔记本配备了一颗Intel i5-8265U处理器，具有八个核心，如*图4**.1*所示：
- en: '![Figure 4.1 – Physical and logical cores](img/B20959_04_1.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 物理核心和逻辑核心](img/B20959_04_1.jpg)'
- en: Figure 4.1 – Physical and logical cores
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 物理核心和逻辑核心
- en: Modern processors have physical and logical cores. A **physical core** is a
    complete and individual processing unit able to perform any computation. A **logical
    core** is a processing entity instantiated from the idle resources of physical
    cores. Therefore, physical cores deliver better performance than logical cores.
    Thus, we should always prefer to use physical units rather than logical ones.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现代处理器具有物理核心和逻辑核心。**物理核心**是完整的独立处理单元，能够执行任何计算。**逻辑核心**是从物理核心的空闲资源实例化出来的处理实体。因此，物理核心比逻辑核心提供更好的性能。因此，我们应该始终优先使用物理单元而不是逻辑单元。
- en: Nevertheless, from the operating system’s point of view, there is no difference
    between physical and logical cores (i.e., the operating system sees the total
    number of cores, regardless of whether they are physical or logical).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，从操作系统的角度来看，物理核心和逻辑核心没有区别（即，操作系统看到的核心总数，无论它们是物理还是逻辑的）。
- en: Important note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The technology responsible for providing logical cores is called **simultaneous
    multithreading**. Each vendor has a commercial name for this technology. Intel
    calls it **hyperthreading**, for example. Details about this topic are out of
    the scope of this book.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 提供逻辑核心的技术称为**同时多线程**。每个厂商对于这项技术都有自己的商业名称。例如，Intel称其为**超线程**。关于这个主题的详细信息超出了本书的范围。
- en: 'We can inspect details about the processor by using the `lscpu` command for
    Linux:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Linux的`lscpu`命令来检查处理器的详细信息：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output shows a bunch of information about the processor, such as the number
    of cores and sockets, frequency, architecture, vendor name, and so on. Let’s examine
    the most relevant fields to our case:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示有关处理器的大量信息，例如核心数、插槽数、频率、架构、厂商名称等等。让我们检查与我们案例最相关的字段：
- en: '**CPU(s)**: The total number of physical and logical cores available on the
    system. “CPU” is used here as a synonym for “core.”'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU(s)**：系统上可用的物理和逻辑核心总数。“CPU”在这里被用作“核心”的同义词。'
- en: '**On-line CPU(s) list**: The identification of cores available on the system.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线CPU(s)列表**：系统上可用核心的识别。'
- en: '`1`.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`。'
- en: '`1`, there are only physical cores on the system. Otherwise, the system has
    both physical and logical cores.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`，系统只有物理核心。否则，系统既有物理核心又有逻辑核心。'
- en: '**Core(s) per socket**: The number of physical cores available on each multicore
    processor.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个插槽核心数**：每个多核处理器上可用的物理核心数量。'
- en: Important note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We can use `lscpu` to get the number of physical and logical cores available
    on the hardware that we are running on. As you will see in the next sections,
    this information is essential to optimize the usage of OpenMP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`lscpu`命令获取运行硬件上可用的物理核心和逻辑核心数量。正如您将在接下来的部分中看到的那样，这些信息对于优化OpenMP的使用至关重要。
- en: Modern servers have hundreds of cores. With such computing power on the table,
    we must find a way to use it properly. Here is where multithreading comes in!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现代服务器拥有数百个核心。面对如此强大的计算能力，我们必须找到一种合理利用的方法。这就是多线程发挥作用的地方！
- en: The **multithreading** technique concerns creating and controlling a set of
    threads to co-operate and accomplish a given task. These threads are spread out
    on processor cores so that the running program can use different cores to treat
    distinct pieces of the computing task. As a result, multiple cores work simultaneously
    on the same task to accelerate its completion.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**多线程**技术涉及创建和控制一组线程来协作并完成给定任务。这些线程分布在处理器核心上，使得运行程序可以使用不同的核心来处理计算任务的不同部分。因此，多个核心同时处理同一任务以加速其完成。'
- en: 'A **thread** is an operating system entity created by processes. A set of threads
    created by a given process share the same memory address space. Consequently,
    threads can communicate with themselves much more easily than processes; they
    just need to read or write the content of some memory address. On the other hand,
    processes must resort to more complicated methods such as message exchanging,
    signals, queues, and so on. This is the reason why we prefer to use threads to
    parallelize a task instead of processes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**线程**是由进程创建的操作系统实体。由给定进程创建的一组线程共享相同的内存地址空间。因此，线程之间的通信比进程容易得多；它们只需读取或写入某个内存地址的内容。另一方面，进程必须依靠更复杂的方法，如消息交换、信号、队列等等。这就是为什么我们更倾向于使用线程来并行化任务而不是进程的原因：'
- en: '![](img/B20959_04_2.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20959_04_2.jpg)'
- en: Figure 4.2 – Threads and processes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 线程和进程
- en: 'However, the benefits of using threads come at a price: we must take care of
    our threads. As threads communicate with themselves through shared memory, they
    can fall under race conditions when multiple threads intend to write on the same
    memory region. In addition, the programmer must keep threads synchronized to prevent
    a thread from waiting indefinitely for some result or action of another thread.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用线程的好处是有代价的：我们必须小心我们的线程。由于线程通过共享内存进行通信，当多个线程试图在同一内存区域上写入时，它们可能会遇到竞争条件。此外，程序员必须保持线程同步，以防止一个线程无限期地等待另一个线程的某个结果或操作。
- en: Important note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'If the concept of threads and processes is new to you, take a break and watch
    the following video on YouTube before moving on to the next section: [https://youtu.be/Dhf-DYO1K78](https://youtu.be/Dhf-DYO1K78).
    If you require more profound material, you can read the article written by Roderick
    Bauer, available at [https://medium.com/@rodbauer/understanding-programs-processes-and-threads-fd9fdede4d88](https://medium.com/@rodbauer/understanding-programs-processes-and-threads-fd9fdede4d88).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果线程和进程的概念对您来说很新，请先观看YouTube上的以下视频，然后再继续下一节：[https://youtu.be/Dhf-DYO1K78](https://youtu.be/Dhf-DYO1K78)。如果您需要更深入的资料，可以阅读Roderick
    Bauer撰写的文章，链接在[https://medium.com/@rodbauer/understanding-programs-processes-and-threads-fd9fdede4d88](https://medium.com/@rodbauer/understanding-programs-processes-and-threads-fd9fdede4d88)。
- en: In short, it is hard work to program threads manually (i.e., on our own). However,
    luckily, OpenMP is here to help. So, let’s learn how to use it, along with PyTorch,
    to accelerate the training phase of our machine learning models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，手动编写线程（即自行编写）是一项艰苦的工作。然而，幸运的是，有OpenMP在这里帮忙。因此，让我们学习如何与PyTorch一起使用它，加速我们的机器学习模型训练阶段。
- en: Using and configuring OpenMP
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用和配置OpenMP
- en: OpenMP is a framework that is able to encapsulate and abstract many drawbacks
    related to programming multiple threads. With this framework, we can parallelize
    our sequential code by employing a set of functions and primitives. When talking
    about multithreading, OpenMP is the de facto standard. This explains why PyTorch
    relies on OpenMP as the default backend to parallelize tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP是一个能够封装和抽象许多与编写多线程程序相关的缺点的框架。通过这个框架，我们可以通过一组函数和原语并行化我们的顺序代码。在谈论多线程时，OpenMP是事实上的标准。这也解释了为什么PyTorch将OpenMP作为默认的后端来并行化任务。
- en: Strictly speaking, we do not need to change anything in PyTorch’s code to use
    OpenMP. Nevertheless, there are some configuration tricks that can increase the
    performance of the training process. Let’s see it in practice!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，我们不需要更改PyTorch的代码就可以使用OpenMP。尽管如此，有一些配置技巧可以提高训练过程的性能。让我们在实践中看看！
- en: Important note
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-cnn_cifar10.ipynb)
    and [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/gomp-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/gomp-cnn_cifar10.ipynb).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分中显示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-cnn_cifar10.ipynb)和[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/gomp-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/gomp-cnn_cifar10.ipynb)处查看。
- en: At first, we will run the same code presented in [*Chapter 2*](B20959_02.xhtml#_idTextAnchor028),
    *Training Models Faster,* to train the CNN model with the CIFAR-10 dataset. The
    environment is configured with GNU OpenMP 4.5 and possesses an Intel processor
    with 32 cores in total, half physical and half logical.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将运行与《第2章》[*Chapter 2*](B20959_02.xhtml#_idTextAnchor028)中相同的代码，*更快地训练模型*，以使用CIFAR-10数据集训练CNN模型。环境配置有GNU
    OpenMP 4.5，并且拥有一个总共32个核心的Intel处理器，一半是物理核心，一半是逻辑核心。
- en: 'To check the OpenMP version and number of threads used in the current environment,
    we can execute the `torch.__config__.parallel_info()` function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查当前环境中使用的OpenMP版本和线程数，我们可以执行`torch.__config__.parallel_info()`函数：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The last line of the output confirms that OpenMP is the parallel backend configured
    in the environment. We can also see it is OpenMP version 4.5, as well as the number
    of threads set and values configured for two environment variables. The `hardware_concurrency()`
    field shows a value of `32`, indicating that the environment is able to run up
    to 32 threads since the system has 32 cores at maximum.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的最后一行确认了OpenMP是配置在环境中的并行后端。我们还可以看到它是OpenMP版本4.5，以及设置的线程数和为两个环境变量配置的值。`hardware_concurrency()`字段显示了一个值为`32`，表明环境能够运行多达32个线程，因为系统最多有32个核心。
- en: In addition, the output provides information on the `get_num_threads()` field,
    which is the number of threads used by OpenMP. The default behavior of OpenMP
    is to use a number of threads equivalent to the number of physical cores. So,
    in this case, the default number of threads is 16.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，输出提供了关于`get_num_threads()`字段的信息，这是OpenMP使用的线程数。OpenMP的默认行为是使用与物理核心数量相等的线程数。因此，在这种情况下，默认线程数为16。
- en: 'The training phase took 178 seconds to run 10 epochs. During the training process,
    we can use the `htop` command to verify how OpenMP binds threads to cores. In
    our experiment, PyTorch/OpenMP has made a configuration that is pictorially described
    in *Figure 4**.3*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练阶段花费178秒来运行10个epochs。在训练过程中，我们可以使用`htop`命令验证OpenMP如何将线程绑定到核心。在我们的实验中，PyTorch/OpenMP进行了一种配置，如*图4**.3*所描述的。
- en: '![Figure 4.3 – Default OpenMP threads allocation](img/B20959_04_3.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 默认的OpenMP线程分配](img/B20959_04_3.jpg)'
- en: Figure 4.3 – Default OpenMP threads allocation
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 默认的OpenMP线程分配
- en: OpenMP has allocated the set of 16 threads to 8 physical cores and 8 logical
    cores. As stated in the previous section, physical cores provide better performance
    than logical ones. Even with physical cores available, OpenMP has used logical
    cores to execute half of PyTorch’s threads.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP将这组16个线程分配给了8个物理核心和8个逻辑核心。如前一节所述，物理核心比逻辑核心提供了更好的性能。即使有物理核心可用，OpenMP也使用了逻辑核心来执行PyTorch线程的一半。
- en: At first sight, the decision to use logical cores, even when having physical
    ones available, may sound silly. However, we should remember that processors are
    used by the entire computing system – that is, they are used for other tasks besides
    our training process. Therefore, the operating system, together with OpenMP, should
    try to be fair enough with all demanding tasks – that is, they should also offer
    the chance to use physical cores.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，即使有物理核心可用，决定使用逻辑核心也可能听起来很愚蠢。然而，我们应该记住，处理器是整个计算系统使用的 – 也就是说，它们用于除我们的训练过程之外的其他任务。因此，操作系统与OpenMP应该尽量对所有的需求任务公平
    – 也就是说，它们也应该提供使用物理核心的机会。
- en: 'Despite the default behavior of OpenMP, we can set up a couple of environment
    variables to change the way OpenMP allocates, controls, and governs threads. The
    following piece of code, appended to the beginning of our CNN/CIFAR-10 code, modifies
    OpenMP operations to improve performance:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管OpenMP的默认行为如此，我们可以设置一对环境变量来改变OpenMP分配、控制和管理线程的方式。下面的代码片段附加到我们的CNN/CIFAR-10代码的开头，修改了OpenMP操作以提高性能：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: These lines set up four environment variables directly from Python code. Before
    explaining what these variables mean, let’s first see the performance improvement
    they have provided.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行直接从Python代码设置了四个环境变量。在解释这些变量的含义之前，让我们先看看它们所提供的性能改进。
- en: 'The training time of the CNN model using the CIFAR-10 dataset was reduced from
    178 seconds to 114 seconds, revealing a performance improvement of 56%! Nothing
    else was changed in the code! In this execution, OpenMP has created the threads
    assignment pictorially described in *Figure 4**.4*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CIFAR-10数据集训练CNN模型的时间从178秒减少到114秒，显示出56%的性能提升！在代码中没有其他更改！在这个执行过程中，OpenMP创建了一个线程分配，如*图4**.4*所描述的。
- en: '![Figure 4.4 – Optimized OpenMP thread allocation](img/B20959_04_4.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 优化的OpenMP线程分配](img/B20959_04_4.jpg)'
- en: Figure 4.4 – Optimized OpenMP thread allocation
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 优化的OpenMP线程分配
- en: As you can see in *Figure 4**.4*, OpenMP has used all 16 physical cores, leaving
    out the logical cores. We can say that binding threads to physical cores is the
    primary reason for such an increase in performance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图4**.4*中所见，OpenMP使用了所有16个物理核心，但未使用逻辑核心。我们可以说，将线程绑定到物理核心是性能增加的主要原因。
- en: 'Let’s break down the set of environment variables configured in this experiment
    to understand how they contribute to improving the performance of our training
    process:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分析在这个实验中配置的环境变量集合，以了解它们如何有助于改进我们训练过程的性能：
- en: '`OMP_NUM_THREADS`: This defines the number of threads used by OpenMP. We have
    set the number of threads to `16`, which is exactly the same value set as the
    default by OpenMP. Although this configuration did not bring any changes to our
    scenario, it is essential to know this option to control the number of threads
    used by OpenMP. This is especially important when running more than one training
    process simultaneously on the same server.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OMP_NUM_THREADS`：这定义了 OpenMP 使用的线程数。我们将线程数设置为 `16`，这与 OpenMP 默认设置的值完全相同。虽然此配置未在我们的场景中带来任何变化，但了解这个选项以控制
    OpenMP 使用的线程数是至关重要的。特别是在同一服务器上同时运行多个训练过程时。'
- en: '`OMP_PROC_BIND`: This determines the thread affinity policy. When set to `TRUE`,
    this configuration tells OpenMP to keep threads running on the same core during
    the entire execution. This configuration prevents threads from being moved from
    cores, thus minimizing performance issues, such as cache missing.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OMP_PROC_BIND`：这确定了线程亲和性策略。当设置为`TRUE`时，这个配置告诉 OpenMP 在整个执行过程中保持线程在同一个核心上运行。这种配置防止线程从核心中移动，从而最小化性能问题，比如缓存未命中。'
- en: '`OMP_SCHEDULE`: This defines the scheduling policy. As we want to statically
    bind threads to cores, we should set this variable to a static policy.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OMP_SCHEDULE`：这定义了调度策略。因为我们希望静态地将线程绑定到核心，所以应将此变量设置为静态策略。'
- en: '`GOMP_CPU_AFFINITY`: This indicates the cores or processors to be used by OpenMP
    to execute threads. In order to only use physical cores, we should indicate processor
    identifications corresponding to the physical cores in the system.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GOMP_CPU_AFFINITY`：这指示 OpenMP 用于执行线程的核心或处理器。为了只使用物理核心，我们应指定与系统中物理核心对应的处理器标识。'
- en: The combination of those variables has greatly accelerated the training process
    of our CNN model. In short, we forced OpenMP to use only physical cores and keep
    threads running on the same core they were initially assigned. As a result, we
    have harnessed all the computing power of the physical cores while minimizing
    the performance pitfalls from the overhead caused by frequent context switching.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量的组合极大加速了我们 CNN 模型的训练过程。简而言之，我们强制 OpenMP 仅使用物理核心，并保持线程在最初分配的同一个核心上运行。因此，我们利用了所有物理核心的计算能力，同时最小化由频繁上下文切换引起的性能问题。
- en: Important note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Essentially, context switching occurs when the operating system decides to interrupt
    the execution of a process to give the opportunity of using a CPU to another process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，当操作系统决定中断一个进程的执行以给另一个进程使用 CPU 的机会时，上下文切换就会发生。
- en: 'OpenMP has a couple more variables to control its behavior besides the ones
    presented in this chapter. To check the current OpenMP configuration, we can set
    the `OMP_DISPLAY_ENV` environment variable to `TRUE` when running our PyTorch
    code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 除了本章介绍的变量外，还有几个变量来控制其行为。为了检查当前的 OpenMP 配置，我们可以在运行 PyTorch 代码时将 `OMP_DISPLAY_ENV`
    环境变量设置为 `TRUE`：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It is interesting to learn how each of those environment variable changes OpenMP
    operation; thus, we can fine-tune for particular scenarios. This output is also
    useful to verify whether changes to the environment variables did indeed take
    place.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 学习每个环境变量如何改变 OpenMP 的操作是非常有趣的；因此，我们可以针对特定场景进行微调。这个输出也有助于验证环境变量的更改是否确实生效。
- en: The experiments described in this section used GNU OpenMP since it is the default
    parallel backend adopted by PyTorch. However, as OpenMP is actually a framework,
    we have other implementations of OpenMP besides the one provided by GNU. One of
    those implementations is Intel OpenMP, which is suitable for Intel processor environments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述的实验使用了 GNU OpenMP，因为它是 PyTorch 采用的默认并行后端。然而，由于 OpenMP 实际上是一个框架，除了 GNU 提供的实现外，我们还有其他的
    OpenMP 实现。其中一个实现是 Intel OpenMP，适用于 Intel 处理器环境。
- en: However, does Intel OpenMP bring relevant improvements? Is it worth using it
    in place of GNU implementation? See for yourself in the next section!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Intel OpenMP 是否带来了显著的改进？是否值得用它来取代 GNU 实现？请在下一节中自行查看！
- en: Using and configuring Intel OpenMP
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用和配置 Intel OpenMP
- en: 'Intel has its own OpenMP implementation, which promises to deliver better performance
    in Intel-based environments. As PyTorch comes with GNU implementation by default,
    we need to take three steps in order to use Intel OpenMP in place of the GNU version:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Intel 有自己的 OpenMP 实现，在 Intel 基础环境中承诺提供更好的性能。由于 PyTorch 默认使用 GNU 实现，我们需要采取三个步骤来使用
    Intel OpenMP 替代 GNU 版本：
- en: Install Intel OpenMP.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Intel OpenMP。
- en: Load the Intel OpenMP libraries.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载Intel OpenMP库。
- en: Set up specific environment variables for Intel OpenMP.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置特定于Intel OpenMP的环境变量。
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/iomp-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/iomp-cnn_cifar10.ipynb).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分展示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/iomp-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/iomp-cnn_cifar10.ipynb)找到。
- en: 'The first step is the easiest one. When considering a Python environment based
    on Anaconda or supporting PIP, we just need to execute one of these commands to
    install Intel OpenMP:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是最简单的一步。考虑到基于Anaconda或支持PIP的Python环境时，我们只需执行以下其中一个命令来安装Intel OpenMP：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After installation, we should prioritize loading Intel OpenMP libraries instead
    of implementing GNU. Otherwise, PyTorch will keep using the libraries of the default
    OpenMP installation, even with Intel OpenMP installed on the system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们应优先加载Intel OpenMP库，而不是使用GNU。否则，即使在系统上安装了Intel OpenMP，PyTorch仍将继续使用默认OpenMP安装的库。
- en: Important note
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If we do not use a PIP or Anaconda-based environment, we can install it on our
    own. This process requires compiling Intel OpenMP to further install it in the
    environment.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用PIP或基于Anaconda的环境，我们可以自行安装它。这个过程需要编译Intel OpenMP，然后在环境中进一步安装它。
- en: 'We enact this configuration by setting the `LD_PRELOAD` environment variable
    before running our code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在运行代码之前设置`LD_PRELOAD`环境变量来执行此配置：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the environment used for these experiments, the Intel OpenMP library is located
    at `/opt/conda/lib/libiomp5.so`. The `LD_PRELOAD` environment variable allows
    for forcing the operating system to load libraries before loading the ones configured
    by default.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些实验所使用的环境中，Intel OpenMP库位于`/opt/conda/lib/libiomp5.so`。`LD_PRELOAD`环境变量允许在默认加载配置之前强制操作系统加载库。
- en: 'At last, we need to set up some environment variables related to Intel OpenMP:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要设置一些与Intel OpenMP相关的环境变量：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`OMP_NUM_THREADS` has the same meaning as the GNU version, whereas `KMP_AFFINITY`
    and `KMP_BLOCKTIME` are exclusive to Intel OpenMP:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`OMP_NUM_THREADS`与GNU版本具有相同的含义，而`KMP_AFFINITY`和`KMP_BLOCKTIME`则是Intel OpenMP的独有功能：'
- en: '`KMP_AFFINITY`: This defines the threads allocation policy. When set to `granularity=fine,compact,1,0`,
    Intel OpenMP binds threads to physical cores, besides trying to keep it that way
    for the entire execution. Thus, in the case of Intel OpenMP, we do not need to
    pass a list of physical cores to force the usage of physical processors, as we
    do in GNU implementation.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMP_AFFINITY`: 这定义了线程的分配策略。当设置为`granularity=fine,compact,1,0`时，Intel OpenMP会将线程绑定到物理核心，尽力在整个执行过程中保持这种方式。因此，在使用Intel
    OpenMP时，我们不需要像GNU实现那样传递一个物理核心列表来强制使用物理处理器。'
- en: '`KMP_BLOCKTIME`: This determines the time that a thread should wait to sleep
    after completing a task. When set to zero, threads go to sleep immediately after
    doing their job, thus minimizing the wastage of processor cycles just to wait
    for another task.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KMP_BLOCKTIME`: 这确定线程在完成任务后应等待休眠的时间。当设置为零时，线程在完成工作后立即进入休眠状态，从而最小化因等待另一个任务而浪费处理器周期。'
- en: 'Similar to the GNU version, Intel OpenMP also outputs the current configuration
    when the `OMP_DISPLAY_ENV` variable is set to `TRUE` (shortened output example):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GNU版本，当`OMP_DISPLAY_ENV`变量设置为`TRUE`时，Intel OpenMP也会输出当前配置（简化的输出示例）：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To compare the performance brought by Intel OpenMP, we take the result provided
    by the GNU implementation as a baseline. The training time of the CNN model using
    the CIFAR-10 dataset was reduced from 114 seconds to 102 seconds, resulting in
    a performance improvement of around 11%. Even though this is not as impressive
    as the first experiment, the performance gain is still interesting. In addition,
    note that we can get better results by using other models, datasets, and computing
    environments.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较Intel OpenMP带来的性能，我们以GNU实现提供的结果作为基准。使用CIFAR-10数据集训练CNN模型的时间从114秒减少到102秒，性能提升约为11%。尽管这不如第一次实验那样令人印象深刻，但性能的提升仍然很有趣。此外，请注意，我们可以通过使用其他模型、数据集和计算环境获得更好的结果。
- en: To summarize, we executed the training process almost 1.7 times faster with
    the configurations shown in this section. No code modification was necessary to
    achieve such improvement; only direct configurations were applied at the environment
    level.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，使用本节中展示的配置，我们的训练过程速度快了近 1.7 倍。为了实现这种改进，并不需要修改代码；只需在环境级别直接进行配置即可。
- en: In the next section, we will learn how to install and use an API provided by
    Intel to accelerate PyTorch’s execution on its processors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习如何安装和使用 Intel 提供的 API，以加速 PyTorch 在其处理器上的执行。
- en: Optimizing Intel CPU with IPEX
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化 Intel CPU 使用 IPEX
- en: '**IPEX** stands for **Intel extension for PyTorch** and is a set of libraries
    and tools provided by Intel to accelerate the training and inference of machine
    learning models.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**IPEX** 代表 **Intel extension for PyTorch**，是由 Intel 提供的一组库和工具，用于加速机器学习模型的训练和推理。'
- en: IPEX is a clear sign by Intel of highlighting the relevance of PyTorch among
    machine learning frameworks. After all, Intel has invested a lot of energy and
    resources in designing and maintaining an API specially created for PyTorch.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: IPEX 是 Intel 强调 PyTorch 在机器学习框架中重要性的明显标志。毕竟，Intel 在设计和维护专门为 PyTorch 创建的 API
    上投入了大量精力和资源。
- en: It is interesting to say that IPEX strongly relies on libraries provided by
    the Intel oneAPI toolset. oneAPI contains libraries and tools specific for machine
    learning applications, such as oneDNN, and other ones to accelerate applications,
    such as oneTBB, in general.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，IPEX 强烈依赖于 Intel oneAPI 工具集提供的库。oneAPI 包含特定于机器学习应用的库和工具，如 oneDNN，以及加速应用程序（如
    oneTBB）的其他工具。
- en: Important note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-densenet121_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-densenet121_cifar10.ipynb)
    and [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/ipex-densenet121_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/ipex-densenet121_cifar10.ipynb).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示的完整代码可在 [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-densenet121_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/baseline-densenet121_cifar10.ipynb)
    和 [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/ipex-densenet121_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter04/ipex-densenet121_cifar10.ipynb)
    处找到。
- en: Let’s learn how to install and use IPEX on our PyTorch code.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何在我们的 PyTorch 代码中安装和使用 IPEX。
- en: Using IPEX
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 IPEX
- en: 'IPEX does not come with PyTorch by default; we need to install it. The easiest
    way to install IPEX is by using PIP along the same lines we followed for OpenMP
    in the last section. So, to install IPEX on a PIP environment, we just need to
    execute the following command:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: IPEX 不会默认与 PyTorch 一起安装；我们需要单独安装它。安装 IPEX 的最简单方法是使用 PIP，与我们在上一节中使用 OpenMP 的方式类似。因此，在
    PIP 环境中安装 IPEX，只需执行以下命令：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After installing IPEX, we can proceed to the default installation of PyTorch.
    Once IPEX is available, we are ready to incorporate it into our PyTorch code.
    The first step concerns importing the IPEX module:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完 IPEX 后，我们可以继续进行 PyTorch 的默认安装。一旦 IPEX 可用，我们就可以将其整合到我们的 PyTorch 代码中。第一步是导入
    IPEX 模块：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using IPEX is very simple. We just need to wrap our model and optimizer with
    the `ipex.optimize` function and let IPEX do the rest. The `ipex.optimize` function
    returns an optimized version of the model and optimizer (SGD, Adam, and so on)
    used to train the model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 IPEX 非常简单。我们只需将我们的模型和优化器用 `ipex.optimize` 函数包装起来，让 IPEX 完成其余工作。`ipex.optimize`
    函数返回一个经过优化的模型和优化器（如 SGD、Adam 等），用于训练模型。
- en: To see the performance improvement provided by IPEX, let’s test it with the
    DenseNet121 model and the CIFAR-10 dataset (we introduced both of these in previous
    chapters).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到 IPEX 提供的性能改进，让我们使用 DenseNet121 模型和 CIFAR-10 数据集进行测试（我们在前几章介绍过它们）。
- en: 'Our baseline execution concerns training DenseNet121 with the CIFAR-10 dataset
    over 10 epochs. For the sake of fairness, we have used Intel OpenMP since we are
    using an Intel-based environment. However, in this case, we do not change the
    `KMP_BLOCKTIME` parameter:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基准执行涉及使用 CIFAR-10 数据集在 10 个 epochs 上训练 DenseNet121。为了公平起见，我们使用了 Intel OpenMP，因为我们使用的是基于
    Intel 的环境。但在这种情况下，我们没有改变 `KMP_BLOCKTIME` 参数。
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The baseline execution took 1,318 seconds to complete 10 epochs, and the resultant
    model obtained an accuracy of approximately 70%.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 基准执行完成 10 个 epochs 花费了 1,318 秒，并且结果模型的准确率约为 70%。
- en: 'As stated before, using IPEX is very simple; we just need to add one single
    line to the baseline code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，使用 IPEX 非常简单；我们只需在基准代码中添加一行代码：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Although `ipex.optimize` accepts other parameters, calling it in this way is
    usually enough to get what we need.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `ipex.optimize` 可以接受其他参数，但通常以这种方式调用已经足够满足我们的需求。
- en: Our IPEX code took 946 seconds to execute the training process of the DenseNet121
    model, representing a performance improvement of nearly 40%. Except for the environment
    variables configured at the beginning of the code and the usage of that single
    line, nothing else was changed in the original code. Thus, IPEX accelerated the
    training process with just one simple modification.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 IPEX 代码花了 946 秒来执行 DenseNet121 模型的训练过程，性能提升了近 40%。除了在代码开头配置的环境变量和使用的那一行之外，原始代码没有进行任何其他更改。因此，IPEX
    只通过一个简单的修改加速了训练过程。
- en: At first sight, IPEX seems similar to the Compile API that we learned about
    in [*Chapter 3*](B20959_03.xhtml#_idTextAnchor044), *Compiling the Model*. Both
    of them require adding a single line of code and using the concept of code wrapping.
    However, the similarities stop there! Unlike the Compile API, IPEX does not compile
    the model; it replaces some default PyTorch operations with its own implementation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，IPEX 看起来类似于我们在[*第三章*](B20959_03.xhtml#_idTextAnchor044)中学习的 Compile API，*编译模型*。两者都需要添加一行代码并使用代码包装的概念。然而，相似之处止步于此！与
    Compile API 不同，IPEX 不会编译模型；它会用自己的实现替换一些默认的 PyTorch 操作。
- en: Follow me to the next section to understand how IPEX works under the hood.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 跟我来到下一节，了解 IPEX 的内部工作原理。
- en: How does IPEX work under the hood?
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPEX 是如何在内部工作的？
- en: 'To understand how IPEX works under the hood, let’s profile the baseline code
    to check which operations the training process has used. The following output
    shows the 10 most consuming operations executed by the training process:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 IPEX 的内部工作原理，让我们分析基准代码，检查训练过程使用了哪些操作。以下输出显示训练过程执行的前 10 个消耗最大的操作：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our baseline code for DenseNet121 and CIFAR-10 executed those operations commonly
    employed on convolutional neural networks, such as `convolution_backward`. No
    surprise here.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 DenseNet121 和 CIFAR-10 的基准代码执行了那些常用于卷积神经网络的操作，例如 `convolution_backward`。这一点毫不意外。
- en: 'Let’s see the profiling output of the IPEX code to verify what changes IPEX
    has made to our baseline code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 看看 IPEX 代码的分析输出，验证 IPEX 对我们基准代码做出了哪些改变：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first thing to notice is the new prefix used on some operations. Besides
    `aten`, which denotes the default PyTorch operations library, we also have the
    `torch_ipex` prefix. The `torch_ipex` prefix indicates the operations provided
    by IPEX. For example, the baseline code used the `convolution_backward` operation
    provided by `aten`, whereas the optimized code used the operation provided by
    IPEX.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是某些操作上的新前缀。除了表示默认 PyTorch 操作库的 `aten` 外，我们还有 `torch_ipex` 前缀。`torch_ipex`
    前缀指示了 IPEX 提供的操作。例如，基准代码使用了 `aten` 提供的 `convolution_backward` 操作，而优化后的代码则使用了 IPEX
    提供的操作。
- en: As you can observe, IPEX did not replace every single operation since it does
    not have an optimized version of all `aten` operations. This behavior is expected
    because some operations are already in their most optimized form. In this case,
    it does not make any sense to try to optimize what is already optimized.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，IPEX 并没有替换每一个操作，因为它没有所有 `aten` 操作的优化版本。这种行为是预期的，因为有些操作已经是最优化的形式。在这种情况下，试图优化已经优化的部分是没有意义的。
- en: '*Figure 4**.5* summarizes the difference between the default PyTorch code and
    the optimized versions by IPEX and the Compile API:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.5* 总结了默认 PyTorch 代码与 IPEX 和 Compile API 优化版本之间的差异：'
- en: "![Figure 4.5 – Differences between the default and optimized code generated\
    \ by IPEX and \uFEFFthe Compile API](img/B20959_04_5.jpg)"
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: "![图 4.5 – IPEX 和 \uFEFFCompile API 生成的默认和优化代码之间的差异](img/B20959_04_5.jpg)"
- en: Figure 4.5 – Differences between the default and optimized code generated by
    IPEX and the Compile API
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – IPEX 和 Compile API 生成的默认和优化代码之间的差异
- en: Unlike the Compile API, IPEX does not create a monolithic piece of compiled
    code. As a result, the optimization process via `ipex.optimize` execution is much
    faster. On the other hand, the compiled code tends to deliver better performance,
    as we discussed in detail in [*Chapter 3*](B20959_03.xhtml#_idTextAnchor044),
    *Compiling* *the Model*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不像编译API，IPEX不会创建一个庞大的编译代码块。因此，通过`ipex.optimize`执行的优化过程要快得多。另一方面，编译后的代码往往会提供更好的性能，正如我们在*第三章*中详细讨论的那样，*编译*
    *模型*。
- en: Important note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: It is interesting to say that we can use IPEX as a compiler backend for the
    Compile API. In doing this, the `torch.compile` function will rely on IPEX to
    compile the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们可以将IPEX用作编译API的编译后端。通过这样做，`torch.compile`函数将依赖于IPEX来编译模型。
- en: As IPEX shows the great gamble made by Intel on PyTorch, it is constantly evolving
    and receiving frequent updates. Therefore, it is important to use the latest version
    of this tool to get the newest improvements.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如IPEX展示了Intel在PyTorch上所做的重大赌注，它在不断发展并接收频繁更新。因此，使用这个工具的最新版本以获得最新的改进非常重要。
- en: The next section provides some questions to help you retain what you have learned
    in this chapter.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节提供了一些问题，帮助您记住本章节学到的内容。
- en: Quiz time!
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验时间！
- en: Let’s review what we have learned in this chapter by answering a few questions.
    At first, try to answer these questions without consulting the material.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答几个问题来回顾本章节学到的内容。首先，尝试回答这些问题，而不查阅资料。
- en: Important note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The answers to all these questions are available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter04-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter04-answers.md).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题的答案都可以在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter04-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter04-answers.md)找到。
- en: Before starting the quiz, remember that it is not a test at all! This section
    aims to complement your learning process by revising and consolidating the content
    covered in this chapter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始测验之前，请记住这根本不是一次测试！本节旨在通过复习和巩固本章节涵盖的内容来补充您的学习过程。
- en: Choose the correct option for the following questions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 选择以下问题的正确选项。
- en: 'A multicore system can have the following two types of computing cores:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多核系统可以具有以下两种类型的计算核心：
- en: Physical and active.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理和活动。
- en: Physical and digital.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理和数字的。
- en: Physical and logical.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理和逻辑的。
- en: Physical and vectorial.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物理和向量的。
- en: A set of threads created by the same process...
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由同一进程创建的一组线程...
- en: May share the same memory address space.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能共享相同的内存地址空间。
- en: Do not share the same memory address space.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不共享相同的内存地址空间。
- en: Is impossible in modern systems.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在现代系统中是不可能的。
- en: Do share the same memory address space.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共享相同的内存地址空间。
- en: Which of the following environment variables can be used to set the number of
    threads used by OpenMP?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个环境变量可用于设置OpenMP使用的线程数？
- en: '`OMP_NUM_PROCS`.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OMP_NUM_PROCS`。'
- en: '`OMP_NUM_THREADS`.'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OMP_NUM_THREADS`。'
- en: '`OMP_NUMBER_OF_THREADS`.'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OMP_NUMBER_OF_THREADS`。'
- en: '`OMP_N_THREADS`.'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`OMP_N_THREADS`。'
- en: In a multicore system, the usage of OpenMP is able to improve the performance
    of the training process because it can...
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多核系统中，使用OpenMP能够提升训练过程的性能，因为它可以...
- en: Allocate the process to the main memory.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将进程分配到主内存。
- en: Bind threads to logical cores.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将线程绑定到逻辑核心。
- en: Bind threads to physical cores.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将线程绑定到物理核心。
- en: Avoid the usage of cache memory.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免使用缓存内存。
- en: Concerning the implementation of OpenMP through Intel and GNU, we can assert
    that...
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于通过Intel和GNU实现OpenMP，我们可以断言...
- en: There is no difference between the performance obtained by both versions.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个版本的性能无差异。
- en: The Intel version can outperform GNU’s implementation when running on Intel
    platforms.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当在Intel平台上运行时，Intel版本可以优于GNU的实现。
- en: The Intel version never outperforms GNU’s implementation when running on Intel
    platforms.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当在Intel平台上运行时，Intel版本从不优于GNU的实现。
- en: The GNU version is always faster than Intel OpenMP, regardless of the hardware
    platform.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无论硬件平台如何，GNU版本始终比Intel OpenMP更快。
- en: IPEX stands for Intel extension for PyTorch and is defined as...
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPEX代表PyTorch的Intel扩展，并被定义为...
- en: A set of low-level hardware instructions.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组低级硬件指令。
- en: A set of code examples.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组代码示例。
- en: A set of libraries and tools.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组库和工具。
- en: A set of documents.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组文档。
- en: What is the strategy adopted by IPEX to accelerate the training process?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPEX 采用什么策略来加速训练过程？
- en: IPEX enables the usage of special hardware instructions.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPEX 能够使用特殊的硬件指令。
- en: IPEX replaces all the training process operations with an optimized version.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPEX 用优化版本替换了训练过程的所有操作。
- en: IPEX fuses all operations of the training process into a monolithic piece of
    code.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPEX 将训练过程的所有操作融合成一个单片代码。
- en: IPEX replaces some of the default PyTorch operations of the training process
    with its own optimized implementations.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPEX 用自己优化的实现替换了训练过程中的一些默认 PyTorch 操作。
- en: What is necessary to change in our original PyTorch code to use IPEX?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们原始的 PyTorch 代码中，为使用 IPEX 需要做哪些改变？
- en: Nothing at all.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一点儿也不需要。
- en: We just need to import the IPEX module.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需导入 IPEX 模块。
- en: We need to import the IPEX module and wrap the model with the `ipex.optimize()`
    method.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要导入 IPEX 模块，并使用 `ipex.optimize()` 方法包装模型。
- en: We just need to use the newest PyTorch version.
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需使用最新的 PyTorch 版本。
- en: Let’s summarize what we’ve covered so far.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止我们讨论过的内容。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You learned that PyTorch relies on third-party libraries to accelerate the training
    process. Besides understanding the concept of multithreading, you have learned
    how to install, configure, and use OpenMP. In addition, you have learned how to
    install and use IPEX, which is a set of libraries developed by Intel to optimize
    the training process of PyTorch code executed on Intel-based platforms.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您了解到 PyTorch 依赖于第三方库来加速训练过程。除了理解多线程的概念外，您还学会了如何安装、配置和使用 OpenMP。此外，您还学会了如何安装和使用
    IPEX，这是由英特尔开发的一组库，用于优化在基于英特尔平台上执行的 PyTorch 代码的训练过程。
- en: OpenMP can accelerate the training process by employing multiple threads to
    parallelize the execution of PyTorch code, whereas IPEX is useful for replacing
    the operations provided by the default PyTorch library by optimizing the operations
    written specifically for Intel hardware.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 可通过使用多线程来并行执行 PyTorch 代码以加速训练过程，而 IPEX 则有助于通过优化专为英特尔硬件编写的操作来替换默认 PyTorch
    库提供的操作。
- en: In the next chapter, you will learn how to create an efficient data pipeline
    to keep the GPU working at peak performance during the entire training process.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何创建一个高效的数据管道，以保持 GPU 在整个训练过程中处于最佳状态。
