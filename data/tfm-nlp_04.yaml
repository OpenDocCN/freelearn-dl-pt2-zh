- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Pretraining a RoBERTa Model from Scratch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始预训练RoBERTa模型
- en: In this chapter, we will build a RoBERTa model from scratch. The model will
    use the bricks of the transformer construction kit we need for BERT models. Also,
    no pretrained tokenizers or models will be used. The RoBERTa model will be built
    following the fifteen-step process described in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从头开始构建一个RoBERTa模型。该模型将使用我们在BERT模型中所需的变压器构造工具的模块。此外，不会使用预训练的分词器或模型。RoBERTa模型将按照本章描述的十五步过程构建。
- en: We will use the knowledge of transformers acquired in the previous chapters
    to build a model that can perform language modeling on masked tokens step by step.
    In *Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    we went through the building blocks of the original Transformer. In *Chapter 3*,
    *Fine-Tuning BERT Models*, we fine-tuned a pretrained BERT model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用在之前章节中获得的变压器知识，逐步构建一个可以对遮蔽标记执行语言建模的模型。在*第2章*，*开始使用变压器模型的体系结构*中，我们研究了原始变压器的构建模块。在*第3章*，*对BERT模型进行微调*中，我们微调了一个预训练的BERT模型。
- en: This chapter will focus on building a pretrained transformer model from scratch
    using a Jupyter notebook based on Hugging Face’s seamless modules. The model is
    named KantaiBERT.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍如何使用基于Hugging Face无缝模块的Jupyter笔记本从头开始构建一个预训练的变压器模型。该模型被命名为KantaiBERT。
- en: KantaiBERT first loads a compilation of Immanuel Kant’s books created for this
    chapter. You will see how the data was obtained. You will also see how to create
    your own datasets for this notebook.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT首先加载了专门为本章创建的伊曼纽尔·康德的书籍的合集。您将看到数据是如何获取的。您还将看到如何为此笔记本创建自己的数据集。
- en: KantaiBERT trains its own tokenizer from scratch. It will build its merge and
    vocabulary files, which will be used during the pretraining process.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT从头训练自己的分词器。它将构建自己的合并和词汇文件，在预训练过程中使用。
- en: KantaiBERT then processes the dataset, initializes a trainer, and trains the
    model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后KantaiBERT处理数据集，初始化一个训练师，并训练模型。
- en: Finally, KantaiBERT uses the trained model to perform an experimental downstream
    language modeling task and fills a mask using Immanuel Kant’s logic.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，KantaiBERT使用训练好的模型执行一个实验性的下游语言建模任务，并使用伊曼纽尔·康德的逻辑填充掩码。
- en: By the end of the chapter, you will know how to build a transformer model from
    scratch. You will have enough knowledge of transformers to face the Industry 4.0
    challenge of using powerful pretrained transformers such as GPT-3 engines that
    require more than development skills to implement them. This chapter prepares
    you for *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解如何从头开始构建一个变压器模型。您将拥有足够的变压器知识，以应对使用强大的预训练变压器（如GPT-3引擎）的第四次工业革命挑战，这需要不仅有开发技能，还需要实施能够实现它们的技能。本章为*第7章*，*GPT-3引擎崛起的超人类变压器*做好了准备。
- en: 'This chapter covers the following topics:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: RoBERTa- and DistilBERT-like models
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似RoBERTa和DistilBERT的模型
- en: How to train a tokenizer from scratch
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从头训练一个分词器
- en: Byte-level byte-pair encoding
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节级字节对编码
- en: Saving the trained tokenizer to files
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练好的分词器保存到文件中
- en: Recreating the tokenizer for the pretraining process
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新创建用于预训练过程的分词器
- en: Initializing a RoBERTa model from scratch
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头初始化RoBERTa模型
- en: Exploring the configuration of the model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索模型的配置
- en: Exploring the 80 million parameters of the model
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索模型的8000万参数
- en: Building the dataset for the trainer
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练师构建数据集
- en: Initializing the trainer
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化训练师
- en: Pretraining the model
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Saving the model
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存模型
- en: Applying the model to the downstream tasks of **Masked Language Modeling** (**MLM**)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型应用于**遮蔽语言建模**（**MLM**）的下游任务
- en: Our first step will be to describe the transformer model that we are going to
    build.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是描述我们将要构建的变压器模型。
- en: Training a tokenizer and pretraining a transformer
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练分词器并预训练变压器
- en: In this chapter, we will train a transformer model named KantaiBERT using the
    building blocks provided by Hugging Face for BERT-like models. We covered the
    theory of the building blocks of the model we will be using in *Chapter 3*, *Fine-Tuning
    BERT Models*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Hugging Face提供的用于类似BERT模型的构建模块来训练一个名为KantaiBERT的变压器模型。我们在*第3章*，*对BERT模型进行微调*中介绍了我们将使用的模型构建模块的理论。
- en: We will describe KantaiBERT, building on the knowledge we acquired in previous
    chapters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在之前章节中获得的知识基础上描述KantaiBERT。
- en: KantaiBERT is a **Robustly Optimized BERT Pretraining Approach** (**RoBERTa**)-like
    model based on the architecture of BERT.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 是一种基于 BERT 架构的**鲁棒优化 BERT 预训练方法**（**RoBERTa**）-风格的模型。
- en: The initial BERT models brought innovative features to the initial transformer
    models, as we saw in *Chapter 3*. RoBERTa increases the performance of transformers
    for downstream tasks by improving the mechanics of the pretraining process.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最初的 BERT 模型为初始的变压器模型带来了创新特性，正如我们在*第 3 章*中看到的那样。RoBERTa 通过改进预训练过程的机制来提高变压器对下游任务的性能。
- en: For example, it does not use `WordPiece` tokenization but goes down to byte-level
    **Byte-Pair Encoding** (**BPE**). This method paved the way for a wide variety
    of BERT and BERT-like models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它不使用`WordPiece`分词，而是下降到字节级别的**字节对编码**（**BPE**）。这种方法为各种 BERT 和类似 BERT 的模型铺平了道路。
- en: In this chapter, KantaiBERT, like BERT, will be trained using **Masked Language
    Modeling** (**MLM**). MLM is a language modeling technique that masks a word in
    a sequence. The transformer model must train to predict the masked word.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，与 BERT 一样，KantaiBERT 将使用**掩码语言建模**（**MLM**）进行培训。MLM 是一种语言建模技术，它会在序列中掩盖一个单词。变压器模型必须训练以预测被掩盖的单词。
- en: KantaiBERT will be trained as a small model with 6 layers, 12 heads, and 84,095,008
    parameters. It might seem that 84 million parameters is a lot. However, the parameters
    are spread over 12 heads, which makes it a relatively small model. A small model
    will make the pretraining experience smooth so that each step can be viewed in
    real time without waiting for hours to see a result.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 将作为一个小型模型进行培训，包括 6 层、12 个头和 84,095,008 个参数。也许 8400 万个参数看起来很多。然而，这些参数分布在
    12 个头上，使其成为一个相对较小的模型。一个小模型将使预训练体验得以顺畅进行，以便每一步都可以实时查看，而不必等待数小时才能看到结果。
- en: KantaiBERT is a DistilBERT-like model because it has the same architecture of
    6 layers and 12 heads. DistilBERT is a distilled version of BERT. DistilBERT,
    as the name suggests, contains fewer parameters than a RoBERTa model. As such,
    it runs much faster, but the results are slightly less accurate than with a RoBERTa
    model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 是一种类似 DistilBERT 的模型，因为它具有相同的 6 层和 12 个头的架构。DistilBERT 是 BERT 的精简版本。如其名所示，DistilBERT
    比 RoBERTa 模型包含更少的参数。因此，它运行速度更快，但与 RoBERTa 模型相比，结果略微不够准确。
- en: We know that large models achieve excellent performance. But what if you want
    to run a model on a smartphone? Miniaturization has been the key to technological
    evolution. Transformers will sometimes have to follow the same path during implementation.
    The Hugging Face approach using a distilled version of BERT is thus a good step
    forward. Distillation using fewer parameters or other such methods in the future
    is a clever way of taking the best of pretraining and making it efficient for
    the needs of many downstream tasks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，大型模型能够取得出色的性能。但是，如果你想在智能手机上运行一个模型会怎样呢？微型化一直是技术发展的关键。在实施过程中，变压器有时必须遵循相同的路径。Hugging
    Face 提出使用 BERT 的精简版本是迈出的一大步。因此，通过使用更少的参数或其他类似方法进行精炼，是将预训练的最佳内容变得有效用于许多下游任务的巧妙方式。
- en: It is important to show all the possible architectures, including running a
    small model on a smartphone. However, the future of transformers will also be
    ready-to-use APIs, as we will see in *Chapter 7*, *The Rise of Suprahuman Transformers
    with GPT-3 Engines*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 显示所有可能的架构很重要，包括在智能手机上运行一个小型模型。然而，变压器的未来也将是准备好的可用 API，正如我们将在*第 7 章*的*超人类变压器与
    GPT-3 引擎的崛起*中所看到的那样。
- en: KantaiBERT will implement a byte-level byte-pair encoding tokenizer like the
    one used by GPT-2\. The special tokens will be the ones used by RoBERTa. BERT
    models most often use a WordPiece tokenizer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 将实现一种类似 GPT-2 所使用的字节级字节对编码分词器。特殊的分词将使用 RoBERTa 所使用的分词。BERT 模型通常使用
    WordPiece 分词器。
- en: There are no token type IDs to indicate which part of a segment a token is a
    part of. The segments will be separated with the separation token `</s>`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 没有标记类型 ID 来指示标记属于哪个片段的哪个部分。片段将用分隔标记`</s>`来分隔。
- en: KantaiBERT will use a custom dataset, train a tokenizer, train the transformer
    model, save it, and run it with an MLM example.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 将使用自定义数据集，训练分词器，训练变压器模型，保存它，并在 MLM 示例中运行它。
- en: Let’s get going and build a transformer from scratch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始从头构建一个变压器。
- en: Building KantaiBERT from scratch
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头构建 KantaiBERT
- en: We will build KantaiBERT in 15 steps from scratch and run it on an MLM example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从头构建 KantaiBERT，并在 MLM 示例中运行它的 15 个步骤。
- en: Open Google Colaboratory (you need a Gmail account). Then upload `KantaiBERT.ipynb`,
    which is on GitHub in this chapter’s directory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 Google Colaboratory（你需要一个 Gmail 帐号）。然后上传`KantaiBERT.ipynb`，它在本章的 GitHub 目录中。
- en: The titles of the 15 steps of this section are similar to the titles of the
    notebook cells, which makes them easy to follow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的 15 个步骤的标题与笔记本单元格的标题类似，这使得它们易于跟踪。
- en: Let’s start by loading the dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载数据集开始。
- en: 'Step 1: Loading the dataset'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：加载数据集
- en: Ready-to-use datasets provide an objective way to train and compare transformers.
    In *Chapter 5*, *Downstream NLP Tasks with Transformers*, we will explore several
    datasets. However, this chapter aims to understand the training process of a transformer
    with notebook cells that can be run in real time without waiting for hours to
    obtain a result.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 预先准备好的数据集提供了一个客观的方法来训练和比较 transformers。在*第五章*，*使用 Transformers 进行下游 NLP 任务*中，我们将探讨几个数据集。然而，本章旨在理解一个
    transformer 的训练过程，使用可以实时运行的笔记本单元格，而不需要等待数小时才能获得结果。
- en: I chose to use the works of Immanuel Kant (1724-1804), the German philosopher
    who was the epitome of the *Age of Enlightenment*. The idea is to introduce human-like
    logic and pretrained reasoning for downstream reasoning tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择使用伊曼纽尔·康德（1724-1804）的作品，他是德国哲学家，是*启蒙时代*的典范。这个想法是为下游推理任务引入类似于人类的逻辑和预训练推理。
- en: Project Gutenberg, [https://www.gutenberg.org](https://www.gutenberg.org), offers
    a wide range of free eBooks that can be downloaded in text format. You can use
    other books if you want to create customized datasets of your own based on books.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg，[https://www.gutenberg.org](https://www.gutenberg.org)，提供了大量免费的可以以文本格式下载的电子书。如果你想基于书籍创建自定义数据集，可以使用其他书籍。
- en: 'I compiled the following three books by Immanuel Kant into a text file named
    `kant.txt`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我将以下三本伊曼纽尔·康德的书编译成名为`kant.txt`的文本文件：
- en: '*The Critique of Pure Reason*'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*纯粹理性批判*'
- en: '*The Critique of Practical Reason*'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实践理性批判*'
- en: '*Fundamental Principles of the Metaphysic of Morals*'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*道德形而上学基本原理*'
- en: '`kant.txt` provides a small training dataset to train the transformer model
    of this chapter. The result obtained remains experimental. For a real-life project,
    I would add the complete works of Immanuel Kant, Rene Descartes, Pascal, and Leibnitz,
    for example.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`kant.txt`为本章的 transformer 模型提供了一个小的训练数据集。所得到的结果仍然是试验性的。对于一个真实的项目，我会添加伊曼纽尔·康德、勒内·笛卡尔、帕斯卡尔和莱布尼茨等人的全部作品，例如。'
- en: 'The text file contains the raw text of the books:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件包含书籍的原始文本：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset is downloaded automatically from GitHub in the first cell of the
    `KantaiBERT.ipynb` notebook.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集将在`KantaiBERT.ipynb`笔记本的第一个单元格中自动从 GitHub 下载。
- en: 'You can also load `kant.txt`, which is in the directory of this chapter on
    GitHub, using Colab’s file manager. In this case, `curl` is used to retrieve it
    from GitHub:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 Colab 的文件管理器加载`kant.txt`，它位于本章在 GitHub 上的目录中。在这种情况下，使用 `curl` 从 GitHub
    检索它：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can see it appear in the Colab file manager pane once you have loaded or
    downloaded it:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载或下载，你可以在 Colab 文件管理器窗格中看到它出现：
- en: '![](img/B17948_04_01.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_04_01.png)'
- en: 'Figure 4.1: Colab file manager'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：Colab 文件管理器
- en: Note that Google Colab deletes the files when you restart the VM.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您重新启动 VM 时，Google Colab 将删除文件。
- en: The dataset is defined and loaded.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被定义并加载。
- en: Do not run the subsequent cells without `kant.txt`. Training data is a prerequisite.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在没有`kant.txt`的情况下运行后续单元格。训练数据是先决条件。
- en: Now, the program will install the Hugging Face transformers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，程序将安装 Hugging Face transformers。
- en: 'Step 2: Installing Hugging Face transformers'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：安装 Hugging Face transformers
- en: 'We will need to install Hugging Face transformers and tokenizers, but we will
    not need TensorFlow in this instance of the Google Colab VM:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装 Hugging Face transformers 和分词器，但在这个 Google Colab VM 实例中我们不需要 TensorFlow：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output displays the versions installed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示已安装的版本：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Transformer versions are evolving at quite a speed. The version you run may
    differ and be displayed differently.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 版本发展得非常迅速。你运行的版本可能会有所不同，并且显示方式也可能不同。
- en: The program will now begin by training a tokenizer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将通过训练一个分词器来开始。
- en: 'Step 3: Training a tokenizer'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3：训练分词器
- en: In this section, the program does not use a pretrained tokenizer. For example,
    a pretrained GPT-2 tokenizer could be used. However, the training process in this
    chapter includes training a tokenizer from scratch.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，程序不使用预训练的分词器。例如，可以使用预训练的 GPT-2 分词器。但是，本章的训练过程包括从头开始训练一个分词器。
- en: 'Hugging Face’s `ByteLevelBPETokenizer()` will be trained using `kant.txt`.
    A BPE tokenizer will break a string or word down into substrings or subwords.
    There are two main advantages to this, among many others:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 的 `ByteLevelBPETokenizer()` 将使用 `kant.txt` 进行训练。 BPE 标记器将字符串或单词分解为子字符串或子词。这有两个主要优点，其中之一是：
- en: The tokenizer can break words into minimal components. Then it will merge these
    small components into statistically interesting ones. For example, “`smaller"
    and smallest`" can become “`small`,” “`er`,” and “`est`.” The tokenizer can go
    further. We could get “`sm`" and “`all`,” for example. In any case, the words
    are broken down into subword tokens and smaller units of subword parts such as
    “`sm`" and “`all`" instead of simply “`small`.”
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记器可以将单词分解为最小组件。然后它将这些小组件合并成统计学上有趣的组件。例如，“`smaller" and smallest`" 可以变成“`small`,”，“`er`,”
    和“`est`。” 标记器可以进一步操作。例如，我们可以得到“`sm`" 和“`all`。” 无论如何，单词都被分解成子词标记和子词部分的更小单位，例如“`sm`"
    和“`all`"，而不仅仅是“`small`。”
- en: The chunks of strings classified as unknown, `unk_token`, using `WordPiece`
    level encoding, will practically disappear.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `WordPiece` 级别编码，将被分类为未知的字符串块 `unk_token`，实际上会消失。
- en: 'In this model, we will be training the tokenizer with the following parameters:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在此模型中，我们将使用以下参数训练标记器：
- en: '`files=paths` is the path to the dataset'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`files=paths` 是数据集的路径'
- en: '`vocab_size=52_000` is the size of our tokenizer’s model length'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size=52_000` 是我们标记器模型长度的大小'
- en: '`min_frequency=2` is the minimum frequency threshold'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_frequency=2` 是最小频率阈值'
- en: '`special_tokens=[]` is a list of special tokens'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`special_tokens=[]` 是特殊标记的列表'
- en: 'In this case, the list of special tokens is:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，特殊标记列表是：
- en: '`<s>`: a start token'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<s>`：一个开始标记'
- en: '`<pad>`: a padding token'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<pad>`：一个填充标记'
- en: '`</s>`: an end token'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`</s>`：一个结束符号'
- en: '`<unk>`: an unknown token'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<unk>`：一个未知标记'
- en: '`<mask>`: the mask token for language modeling'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<mask>`：语言建模的掩码标记'
- en: The tokenizer will be trained to generate merged substring tokens and analyze
    their frequency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器将被训练以生成合并的子字符串标记并分析它们的频率。
- en: 'Let’s take these two words in the middle of a sentence:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在句子中间拿这两个词来说：
- en: '[PRE4]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first step will be to tokenize the string:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将字符串标记化：
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The string is now tokenized into tokens with `Ġ` (whitespace) information.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串现在被标记为带有 `Ġ`（空格）信息的标记。
- en: 'The next step is to replace them with their indices:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是用它们的索引替换它们：
- en: '| ‘Ġthe’ | ‘Ġtoken’ | ‘izer’ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ‘Ġthe’ | ‘Ġtoken’ | ‘izer’ |'
- en: '| 150 | 5430 | 4712 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 150 | 5430 | 4712 |'
- en: 'Table 4.1: Indices for the three tokens'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：三个标记的索引
- en: 'The program runs the tokenizer as expected:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 程序如预期般运行标记器：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The tokenizer outputs the time taken to train:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器输出训练时间：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The tokenizer is trained and ready to be saved.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器已经训练完成并准备保存。
- en: 'Step 4: Saving the files to disk'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 步：将文件保存到磁盘中
- en: 'The tokenizer will generate two files when trained:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成时，标记器将生成两个文件：
- en: '`merges.txt`, which contains the merged tokenized substrings'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merges.txt`，其中包含合并的标记化子字符串'
- en: '`vocab.json`, which contains the indices of the tokenized substrings'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab.json`，其中包含标记化子字符串的索引'
- en: 'The program first creates the `KantaiBERT` directory and then saves the two
    files:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先创建 `KantaiBERT` 目录，然后保存两个文件：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The program output shows that the two files have been saved:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 程序输出显示两个文件已保存：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The two files should appear in the file manager pane:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个文件应该出现在文件管理器窗格中：
- en: '![](img/B17948_04_02.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_04_02.png)'
- en: 'Figure 4.2: Colab file manager'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：Colab 文件管理器
- en: 'The files in this example are small. You can double-click on them to view their
    contents. `merges.txt` contains the tokenized substrings as planned:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例中的文件很小。您可以双击它们以查看其内容。`merges.txt` 包含按计划标记化的子字符串：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`vocab.json` contains the indices:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`vocab.json` 包含以下索引：'
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The trained tokenized dataset files are ready to be processed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的标记化数据集文件已准备好进行处理。
- en: 'Step 5: Loading the trained tokenizer files'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 步：加载训练好的标记器文件
- en: 'We could have loaded pretrained tokenizer files. However, we trained our own
    tokenizer and now are ready to load the files:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以加载预训练的标记器文件。 但是，我们训练了自己的标记器，现在准备加载文件：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The tokenizer can encode a sequence:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器可以对序列进行编码：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`"The Critique of Pure Reason"` will become:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: “`The Critique of Pure Reason`” 将变成：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can also ask to see the number of tokens in this sequence:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以要求看到此序列中的标记数：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will show that there are 6 tokens in the sequence:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将显示序列中有 6 个标记：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The tokenizer now processes the tokens to fit the BERT model variant used in
    this notebook. The post-processor will add a start and end token; for example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器现在处理这些标记以适应本笔记本中使用的 BERT 模型变体。后处理器将添加一个开始和结束标记；例如：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s encode a post-processed sequence:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对一个后处理序列进行编码：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output shows that we now have 8 tokens:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示我们现在有 8 个标记：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we want to see what was added, we can ask the tokenizer to encode the post-processed
    sequence by running the following cell:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想看看添加了什么，我们可以要求分词器对后处理序列进行编码，运行以下单元格：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output shows that the start and end tokens have been added, which brings
    the number of tokens to 8, including start and end tokens:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示已添加了开始和结束标记，这将标记数增加到了 8，包括开始和结束标记：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The data for the training model is now ready to be trained. We will now check
    the system information of the machine we are running the notebook on.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练模型的数据现在已经准备好了。我们现在将检查运行笔记本的机器的系统信息。
- en: 'Step 6: Checking resource constraints: GPU and CUDA'
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 6：检查资源限制：GPU 和 CUDA
- en: KantaiBERT runs at optimal speed with a **Graphics Processing Unit** (**GPU**).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 在 **图形处理单元**（**GPU**）上以最佳速度运行。
- en: 'We will first run a command to see if an NVIDIA GPU card is present:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先运行一个命令来查看 NVIDIA GPU 卡是否存在：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output displays the information and version on the card:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了卡的信息和版本：
- en: '![](img/B17948_04_03.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_04_03.png)'
- en: 'Figure 4.3: Information on the NVIDIA card'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：NVIDIA 卡信息
- en: The output may vary with each Google Colab VM configuration.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能随着每个 Google Colab VM 配置而变化。
- en: 'We will now check to make sure `PyTorch` sees CUDA:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将检查 `PyTorch` 是否看到 CUDA：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result should be `True`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结果应该是 `True`：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Compute Unified Device Architecture** (**CUDA**) was developed by NVIDIA
    to use the parallel computing power of its GPUs.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA**（**计算统一设备体系结构**）是由 NVIDIA 开发的，用于利用其 GPU 的并行计算能力。'
- en: For more on NVIDIA GPUs and CUDA, see *Appendix II*, *Hardware Constraints for
    Transformer Models*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更多关于 NVIDIA GPU 和 CUDA 的信息，请参阅 *附录 II*，*Transformer 模型的硬件限制*。
- en: We are now ready to define the configuration of the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备定义模型的配置。
- en: 'Step 7: Defining the configuration of the model'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 7：定义模型的配置
- en: 'We will be pretraining a RoBERTa-type transformer model using the same number
    of layers and heads as a DistilBERT transformer. The model will have a vocabulary
    size set to 52,000, 12 attention heads, and 6 layers:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与 DistilBERT 变压器相同数量的层和头部来预训练 RoBERTa 类型的变压器模型。模型的词汇量设置为 52,000，具有 12 个注意力头和
    6 个层：
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will explore the configuration in more detail in *Step 9: Initializing a
    model from scratch*.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地探讨配置，见 *步骤 9：从零开始初始化模型*。
- en: Let’s first recreate the tokenizer in our model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先在我们的模型中重新创建分词器。
- en: 'Step 8: Reloading the tokenizer in transformers'
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 8：在 transformers 中重新加载分词器
- en: 'We are now ready to load our trained tokenizer, which is our pretrained tokenizer
    in `RobertaTokenizer.from_pretained()`:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备加载我们训练过的分词器，这是我们预训练的分词器 `RobertaTokenizer.from_pretained()`：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have loaded our trained tokenizer, let’s initialize a RoBERTa model
    from scratch.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们加载了我们的训练过的分词器，让我们从零开始初始化一个 RoBERTa 模型。
- en: 'Step 9: Initializing a model from scratch'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 9：从零开始初始化模型
- en: In this section, we will initialize a model from scratch and examine the size
    of the model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从零开始初始化一个模型，并检查模型的大小。
- en: 'The program first imports a RoBERTa masked model for language modeling:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先导入一个 RoBERTa 掩码模型进行语言建模：
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The model is initialized with the configuration defined in *Step 7*:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用 *步骤 7* 中定义的配置进行初始化：
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If we print the model, we can see that it is a BERT model with 6 layers and
    12 heads:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印模型，我们可以看到它是一个具有 6 层和 12 个头的 BERT 模型：
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The building blocks of the encoder of the original Transformer model are present
    with different dimensions, as shown in this excerpt of the output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 Transformer 模型的编码器的构建模块以不同的尺寸存在，如输出摘录所示：
- en: '[PRE30]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Take some time to go through the details of the output of the configuration
    before continuing. You will get to know the model from the inside.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请花一些时间仔细阅读配置的输出详情。您将从内部了解模型。
- en: The LEGO^®-type building blocks of transformers make it fun to analyze. For
    example, you will note that dropout regularization is present throughout the sublayers.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的 LEGO^® 类型构建模块使得分析变得有趣。例如，您将注意到在子层中存在辍学正则化。
- en: Now, let’s explore the parameters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索参数。
- en: Exploring the parameters
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索参数
- en: The model is small and contains 84,095,008 parameters.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型小巧，含有 84,095,008 个参数。
- en: 'We can check its size:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查其大小：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output shows the approximate number of parameters, which might vary from
    one transformer version to another:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示大约的参数数量，这可能因变压器版本而异：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s now look into the parameters. We first store the parameters in `LP` and
    calculate the length of the list of parameters:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们查看参数。我们首先将参数存储在`LP`中，并计算参数列表的长度：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output shows that there are approximately `108` matrices and vectors, which
    might vary from one transformer model to another:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示大约有`108`个矩阵和向量，这可能因变压器模型而异：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, let’s display the `108` matrices and vectors in the tensors that contain
    them:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们显示张量中的`108`个矩阵和向量：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output displays all the parameters as shown in the following excerpt of
    the output:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示所有参数，如下所示：
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Take a few minutes to peek inside the parameters to understand how transformers
    are built.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 花几分钟时间来观察参数，以了解变压器是如何构建的。
- en: 'The number of parameters is calculated by taking all parameters in the model
    and adding them up; for example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量是通过取模型中的所有参数并相加来计算的；例如：
- en: The vocabulary (52,000) x dimensions (768)
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇表 (52,000) x 维度 (768)
- en: The size of the vectors is `1 x 768`
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的大小为`1 x 768`
- en: The many other dimensions found
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现许多其他维度
- en: You will note that *d*[model] = 768\. There are 12 heads in the model. The dimension
    of *d*[k] for each head will thus be ![](img/B17948_04_001.png).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到* d *[model] = 768。模型中有12个头。因此，每个头的* d *[k]维度将是![](img/B17948_04_001.png)。
- en: This shows, once again, the optimized LEGO^® concept of the building blocks
    of a transformer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 再次展示了变压器优化的LEGO^®概念，其构建快速。
- en: We will now see how the number of parameters of a model is calculated and how
    the figure 84,095,008 is reached.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看一下模型的参数数量是如何计算，以及如何得到图中的 84,095,008。
- en: 'If we hover over **LP** in the notebook, we will see some of the shapes of
    the torch tensors:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在笔记本中悬停在**LP**上，我们将看到torch张量的一些形状：
- en: '![](img/B17948_04_04.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_04_04.png)'
- en: 'Figure 4.4: LP'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：LP
- en: Note that the numbers might vary depending on the version of the transformers
    module you use.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数字可能会根据您使用的transformers模块的版本而变化。
- en: 'We will take this further and count the number of parameters of each tensor.
    First, the program initializes a parameter counter named `np` (number of parameters)
    and goes through the `lp` (`108`) number of elements in the list of parameters:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进一步计算每个张量的参数数量。首先，程序初始化了一个名为`np`（参数数量）的参数计数器，并遍历了参数列表`lp`（`108`）中的元素：
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The parameters are matrices and vectors of different sizes; for example:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是不同大小的矩阵和向量；例如：
- en: 768 x 768
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 768 x 768
- en: 768 x 1
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 768 x 1
- en: '768'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '768'
- en: We can see that some parameters are two-dimensional, and some are one-dimensional.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些参数是二维的，而一些是一维的。
- en: 'An easy way to see if a parameter `p` in the list `LP[p]` has two dimensions
    or not is by doing the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 查看列表中的参数`LP[p]`是否有两个维度的简便方法是：
- en: '[PRE38]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If the parameter has two dimensions, its second dimension will be `L2>0` and
    `PL2=True (2 dimensions=True)`. If the parameter has only one dimension, its second
    dimension will be `L2=1` and `PL2=False (2 dimensions=False)`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果参数有两个维度，那么它的第二个维度将是`L2>0`且`PL2=True (2 dimensions=True)`。如果参数只有一个维度，那么它的第二个维度将是`L2=1`且`PL2=False
    (2 dimensions=False)`。
- en: '`L1` is the size of the first dimension of the parameter. `L3` is the size
    of the parameters defined by:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`L1`是参数的第一个维度大小。`L3`是由下列定义的参数大小：'
- en: '[PRE39]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can now add the parameters up at each step of the loop:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在循环的每一步中添加参数：
- en: '[PRE40]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We will obtain the sum of the parameters, but we also want to see exactly how
    the number of parameters of a transformer model is calculated:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得参数的总和，但我们也想看到变压器模型的参数数量是如何计算的：
- en: '[PRE41]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that if a parameter only has one dimension, `PL2=False`, then we only display
    the first dimension.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果参数只有一个维度，那么`PL2=False`，那么我们只显示第一个维度。
- en: 'The output is the list of how the number of parameters was calculated for all
    the tensors in the model, as shown in the following excerpt:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是所有张量中参数数量的列表，如下所示：
- en: '[PRE42]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The total number of parameters of the RoBERTa model is displayed at the end
    of the list:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa模型的总参数数量显示在列表的末尾：
- en: '[PRE43]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The number of parameters might vary with the version of the libraries used.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量可能会随所使用的库版本而变化。
- en: We now know precisely what the number of parameters represents in a transformer
    model. Take a few minutes to go back and look at the output of the configuration,
    the content of the parameters, and the size of the parameters. At this point,
    you will have a precise mental representation of the building blocks of the model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们精确地知道转换模型中参数的数量代表什么。花几分钟回头看看配置的输出，参数的内容以及参数的大小。在这一点上，你将对模型的构建块有一个精确的心理表征。
- en: The program now builds the dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在构建数据集。
- en: 'Step 10: Building the dataset'
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10步：构建数据集
- en: 'The program will now load the dataset line by line to generate samples for
    batch training with `block_size=128` limiting the length of an example:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将逐行加载数据集以生成用于批量训练的样本，使用`block_size=128`限制一个示例的长度：
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output shows that Hugging Face has invested a considerable amount of resources
    in optimizing the time it takes to process data:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示 Hugging Face 已经投入了大量资源来优化处理数据所需的时间：
- en: '[PRE45]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The wall time, the actual time the processors were active, is optimized.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 墙时间，处理器实际活动的时间，已经被优化。
- en: The program will now define a data collator to create an object for backpropagation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将定义一个数据收集器来创建一个反向传播的对象。
- en: 'Step 11: Defining a data collator'
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11步：定义数据收集器
- en: We need to run a data collator before initializing the trainer. A data collator
    will take samples from the dataset and collate them into batches. The results
    are dictionary-like objects.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化训练器之前，我们需要运行一个数据收集器。数据收集器将从数据集中取样并将其汇集成批次。结果类似于字典对象。
- en: We are preparing a batched sample process for MLM by setting `mlm=True`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设置`mlm=True`为MLM准备了一个批处理样本过程。
- en: We also set the number of masked tokens to train `mlm_probability=0.15`. This
    will determine the percentage of tokens masked during the pretraining process.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将训练的蒙版标记数目设置为`mlm_probability=0.15`。这将决定在预训练过程中屏蔽的标记的百分比。
- en: 'We now initialize `data_collator` with our tokenizer, MLM activated, and the
    proportion of masked tokens set to `0.15`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用我们的分词器初始化`data_collator`，MLM被激活，并且蒙版标记的比例设置为`0.15`：
- en: '[PRE46]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We are now ready to initialize the trainer.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好初始化训练器了。
- en: 'Step 12: Initializing the trainer'
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12步：初始化训练器
- en: The previous steps have prepared the information required to initialize the
    trainer. The dataset has been tokenized and loaded. Our model is built. The data
    collator has been created.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤已经准备好初始化训练器所需的信息。数据集已经被分词化和加载。我们的模型已经构建完成。数据收集器已经被创建。
- en: 'The program can now initialize the trainer. For educational purposes, the program
    trains the model quickly. The number of epochs is limited to one. The GPU comes
    in handy since we can share the batches and multi-process the training tasks:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在可以初始化训练器了。出于教育目的，程序快速训练模型。训练的轮次数被限制为一个。GPU非常方便，因为我们可以共享批次并多进程处理训练任务：
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The model is now ready for training.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经准备好进行训练。
- en: 'Step 13: Pretraining the model'
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第13步：预训练模型
- en: 'Everything is ready. The trainer is launched with one line of code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪。通过一行代码启动训练器：
- en: '[PRE48]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output displays the training process in real time, showing `loss`, `learning
    rate`, `epoch`, and the steps:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了实时的训练过程，显示了`loss`、`learning rate`、`epoch`和步骤：
- en: '[PRE49]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The model has been trained. It’s time to save our work.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已经训练完毕。现在是保存我们工作成果的时候了。
- en: 'Step 14: Saving the final model (+tokenizer + config) to disk'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第14步：将最终模型（+分词器+配置）保存到磁盘上
- en: 'We will now save the model and configuration:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将保存模型和配置：
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Click on **Refresh** in the file manager, and the files should appear:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 点击文件管理器中的**刷新**，文件应该会出现：
- en: '![](img/B17948_04_05.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_04_05.png)'
- en: 'Figure 4.5: Colab file manager'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：Colab文件管理器
- en: '`config.json`, `pytorh_model.bin`, and `training_args.bin` should now appear
    in the file manager.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.json`、`pytorh_model.bin`和`training_args.bin`现在应该出现在文件管理器中。'
- en: '`merges.txt` and `vocab.json` contain the pretrained tokenization of the dataset.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`merges.txt`和`vocab.json`包含数据集的预训练标记化。'
- en: We have built a model from scratch. Let’s import the pipeline to perform a language
    modeling task with our pretrained model and tokenizer.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从零开始构建了一个模型。让我们导入管道来执行一个语言建模任务，使用我们预训练的模型和分词器。
- en: 'Step 15: Language modeling with FillMaskPipeline'
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第15步：使用FillMaskPipeline进行语言建模
- en: 'We will now import a language modeling `fill-mask` task. We will use our trained
    model and trained tokenizer to perform MLM:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将导入一个语言建模的`fill-mask`任务。我们将使用我们训练过的模型和训练过的分词器来执行MLM：
- en: '[PRE51]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now ask our model to think like Immanuel Kant:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以让我们的模型像康德一样思考：
- en: '[PRE52]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will likely change after each run because we are pretraining the
    model from scratch with a limited amount of data. However, the output obtained
    in this run is interesting because it introduces conceptual language modeling:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能会在每个运行中发生变化，因为我们正在从有限的数据量中从头预训练模型。然而，在这次运行中获得的输出很有趣，因为它引入了概念语言建模：
- en: '[PRE53]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The predictions might vary each run and each time Hugging Face updates its models.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 预测可能在每次运行和每次 Hugging Face 更新其模型时都会有所变化。
- en: 'However, the following output comes out often:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，经常会得到以下输出：
- en: '[PRE54]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The goal here was to see how to train a transformer model. We can see that interesting
    human-like predictions can be made.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是看如何训练一个转换器模型。我们可以看到有趣的类人预测是可能的。
- en: These results are experimental and subject to variations during the training
    process. They will change each time we train the model again.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是实验性的，在训练过程中可能会有变化。每次重新训练模型时，它们都会改变。
- en: The model would require much more data from other *Age of Enlightenment* thinkers.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型需要来自其他*启蒙时代*思想家的更多数据。
- en: '*However, the goal of this model is to show that we can create datasets to
    train a transformer for a specific type of complex language modeling task*.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*然而，这个模型的目标是展示我们可以创建数据集来训练一个特定类型的复杂语言建模任务的转换器*。'
- en: Thanks to transformers, we are only at the beginning of a new era of AI!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换器，我们只是处于人工智能新时代的开端！
- en: Next steps
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: You have trained a transformer from scratch. Take some time to imagine what
    you could do in your personal or corporate environment. You could create a dataset
    for a specific task and train it from scratch. Use your areas of interest or company
    projects to experiment with the fascinating world of transformer construction
    kits!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从零开始训练了一个转换器模型。花些时间想象一下你可以在个人或公司环境中做些什么。你可以为特定任务创建一个数据集，并从头开始训练。利用你感兴趣的领域或公司项目来探索转换器构建工具的迷人世界！
- en: 'Once you have made a model you like, you can share it with the Hugging Face
    community. Your model will appear on the Hugging Face models page: [https://huggingface.co/models](https://huggingface.co/models)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了喜欢的模型，就可以与 Hugging Face 社区分享。你的模型将出现在 Hugging Face 模型页面上：[https://huggingface.co/models](https://huggingface.co/models)
- en: 'You can upload your model in a few steps using the instructions described on
    this page: [https://huggingface.co/transformers/model_sharing.html](https://huggingface.co/transformers/model_sharing.html)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照本页描述的说明，用几个步骤上传你的模型：[https://huggingface.co/transformers/model_sharing.html](https://huggingface.co/transformers/model_sharing.html)
- en: You can also download models the Hugging Face community has shared to get new
    ideas for your personal and professional projects.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以下载 Hugging Face 社区共享的模型，以获取个人和专业项目的新思路。
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built `KantaiBERT`, a RoBERTa-like model transformer, from
    scratch using the building blocks provided by Hugging Face.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们使用了 Hugging Face 提供的构建块从零开始构建了 `KantaiBERT`，一个类似 RoBERTa 的模型转换器。
- en: We first started by loading a customized dataset on a specific topic related
    to the works of Immanuel Kant. You can load an existing dataset or create your
    own, depending on your goals. We saw that using a customized dataset provides
    insights into the way a transformer model thinks. However, this experimental approach
    has its limits. It would take a much larger dataset to train a model beyond educational
    purposes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先开始加载一个关于伊曼纽尔·康德作品相关主题的定制数据集。根据你的目标，你可以加载现有数据集或创建自己的数据集。我们发现使用定制数据集可以帮助我们了解转换器模型的思维方式。然而，这种实验方法是有局限性的。要训练一个超越教育目的的模型，需要更大规模的数据集。
- en: The KantaiBERT project was used to train a tokenizer on the `kant.txt` dataset.
    The trained `merges.txt` and `vocab.json` files were saved. A tokenizer was recreated
    with our pretrained files. KantaiBERT built the customized dataset and defined
    a data collator to process the training batches for backpropagation. The trainer
    was initialized, and we explored the parameters of the RoBERTa model in detail.
    The model was trained and saved.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: KantaiBERT 项目被用来在`kant.txt`数据集上训练一个分词器。训练好的`merges.txt`和`vocab.json`文件被保存。用我们预训练的文件重新创建了一个分词器。KantaiBERT建立了定制数据集，并定义了一个数据收集器来处理反向传播的训练批次。训练器被初始化，我们详细探索了
    RoBERTa 模型的参数。模型被训练并保存。
- en: Finally, the saved model was loaded for a downstream language modeling task.
    The goal was to fill the mask using Immanuel Kant’s logic.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，保存的模型被加载，用于一个下游语言建模任务。目标是用伊曼纽尔·康德的逻辑来填补空白。
- en: The door is now wide open for you to experiment on existing or customized datasets
    to see what results you get. You can share your model with the Hugging Face community.
    Transformers are data-driven. You can use this to your advantage to discover new
    ways of using transformers.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在大门已经敞开，你可以尝试现有或定制的数据集来看看你得到了什么结果。你可以与 Hugging Face 社区分享你的模型。变压器是数据驱动的。你可以利用这一点来发现使用变压器的新方法。
- en: You are now ready to learn how to run ready-to-use transformer engines with
    APIs that require no pretraining or fine-tuning. *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*, will take you into the future of AI. And with
    the knowledge of this chapter and the past chapters, you will be ready!
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经准备好了解如何运行无需预训练或微调的 API 的现成变压器引擎。 *第七章*, *超人变压器与 GPT-3 引擎的崛起*，将带领你走进人工智能的未来。通过学习这一章和之前的章节，你将做好准备！
- en: In the next chapter, *Downstream NLP Tasks with Transformers*, we will continue
    our preparation to implement transformers.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*用变压器进行下游 NLP 任务*，我们将继续准备实施变压器。
- en: Questions
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: RoBERTa uses a byte-level byte-pair encoding tokenizer. (True/False)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoBERTa 使用字节级字对编码分词���。(True/False)
- en: A trained Hugging Face tokenizer produces `merges.txt` and `vocab.json`. (True/False)
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过训练的 Hugging Face 分词器会生成 `merges.txt` 和 `vocab.json`。 (True/False)
- en: RoBERTa does not use token-type IDs. (True/False)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoBERTa 不使用 token-type ID。(True/False)
- en: DistilBERT has 6 layers and 12 heads. (True/False)
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DistilBERT 有 6 层和 12 个头。(True/False)
- en: A transformer model with 80 million parameters is enormous. (True/False)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个拥有 8000 万参数的变压器模型是庞大的。(True/False)
- en: We cannot train a tokenizer. (True/False)
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们无法训练分词器。(True/False)
- en: A BERT-like model has 6 decoder layers. (True/False)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT-风格的模型有 6 个解码器层。(True/False)
- en: '**Masked Language Modeling** (**MLM**) predicts a word contained in a mask
    token in a sentence. (True/False)'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**掩盖语言建模** (**MLM**) 预测句子中掩盖的标记中包含的单词。(True/False)'
- en: A BERT-like model has no self-attention sublayers. (True/False)
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT-风格的模型没有自注意力子层。(True/False)
- en: Data collators are helpful for backpropagation. (True/False)
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据收集器对反向传播很有帮助。(True/False)
- en: References
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: '*Yinhan Liu*, *Myle Ott*, *Naman Goyal*, *Jingfei Du*, *Mandar Joshi*, *Danqi
    Chen*, *Omer Levy*, *Mike Lewis*, *Luke Zettlemoyer*, and *Veselin Stoyano*, 2019,
    *RoBERTa: A Robustly Optimized BERT Pretraining Approach*: [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Yinhan Liu*, *Myle Ott*, *Naman Goyal*, *Jingfei Du*, *Mandar Joshi*, *Danqi
    Chen*, *Omer Levy*, *Mike Lewis*, *Luke Zettlemoyer*, 和 *Veselin Stoyano*, 2019,
    *RoBERTa: 一个鲁棒性优化的 BERT 预训练方法*：[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
- en: 'Hugging Face Tokenizer documentation: [https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=tokenizer)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 分词器文档：[https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=tokenizer)
- en: 'The Hugging Face reference notebook: [https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 参考笔记本：[https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb)
- en: 'The Hugging Face reference blog: [https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 参考博客：[https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb)
- en: 'More on BERT: [https://huggingface.co/transformers/model_doc/bert.html](https://huggingface.co/transformers/model_doc/bert.html)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 BERT：[https://huggingface.co/transformers/model_doc/bert.html](https://huggingface.co/transformers/model_doc/bert.html)
- en: 'More DistilBERT: [https://arxiv.org/pdf/1910.01108.pdf](https://arxiv.org/pdf/1910.01108.pdf)'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多 DistilBERT：[https://arxiv.org/pdf/1910.01108.pdf](https://arxiv.org/pdf/1910.01108.pdf)
- en: 'More on RoBERTa: [https://huggingface.co/transformers/model_doc/roberta.html](https://huggingface.co/transformers/model_doc/roberta.html)'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 RoBERTa：[https://huggingface.co/transformers/model_doc/roberta.html](https://huggingface.co/transformers/model_doc/roberta.html)
- en: 'Even more on DistilBERT: [https://huggingface.co/transformers/model_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于 DistilBERT：[https://huggingface.co/transformers/model_doc/distilbert.html](https://huggingface.co/transformers/model_doc/distilbert.html)
- en: Join our book’s Discord space
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 加入该书的 Discord 工作空间，每月与作者进行*问我任何事*会话：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
