- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Predicting Continuous Target Variables with Regression Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归分析预测连续目标变量
- en: 'Throughout the previous chapters, you learned a lot about the main concepts
    behind **supervised learning** and trained many different models for classification
    tasks to predict group memberships or categorical variables. In this chapter,
    we will dive into another subcategory of supervised learning: **regression analysis**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，您学到了关于**监督学习**背后的主要概念，并训练了许多不同的模型来执行分类任务，以预测组成员或分类变量。在本章中，我们将深入探讨另一类监督学习：**回归分析**。
- en: Regression models are used to predict target variables on a continuous scale,
    which makes them attractive for addressing many questions in science. They also
    have applications in industry, such as understanding relationships between variables,
    evaluating trends, or making forecasts. One example is predicting the sales of
    a company in future months.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型用于预测连续尺度上的目标变量，这使它们在解决科学中的许多问题时非常有吸引力。它们在工业中也有应用，例如理解变量之间的关系、评估趋势或进行预测。一个例子是预测公司未来几个月的销售额。
- en: 'In this chapter, we will discuss the main concepts of regression models and
    cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论回归模型的主要概念，并涵盖以下主题：
- en: Exploring and visualizing datasets
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和可视化数据集
- en: Looking at different approaches to implementing linear regression models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看实现线性回归模型的不同方法
- en: Training regression models that are robust to outliers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练对异常值鲁棒的回归模型
- en: Evaluating regression models and diagnosing common problems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估回归模型并诊断常见问题
- en: Fitting regression models to nonlinear data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将回归模型拟合到非线性数据
- en: Introducing linear regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入线性回归
- en: The goal of linear regression is to model the relationship between one or multiple
    features and a continuous target variable. In contrast to classification—a different
    subcategory of supervised learning—regression analysis aims to predict outputs
    on a continuous scale rather than categorical class labels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是建立一个或多个特征与连续目标变量之间的关系模型。与分类不同—监督学习的另一子类—回归分析旨在预测连续尺度上的输出，而不是分类类别标签。
- en: In the following subsections, you will be introduced to the most basic type
    of linear regression, **simple linear regression**, and understand how to relate
    it to the more general, multivariate case (linear regression with multiple features).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，您将了解到最基本类型的线性回归，**简单线性回归**，并理解如何将其与更一般的多变量情况（具有多个特征的线性回归）联系起来。
- en: Simple linear regression
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'The goal of simple (**univariate**) linear regression is to model the relationship
    between a single feature (**explanatory variable**, *x*) and a continuous-valued
    **target** (**response variable**, *y*). The equation of a linear model with one
    explanatory variable is defined as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简单(**单变量**)线性回归的目标是建立单一特征(**解释变量**, *x*)和连续数值**目标**(**响应变量**, *y*)之间的关系模型。具有一个解释变量的线性模型方程定义如下：
- en: '![](img/B17852_09_001.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_001.png)'
- en: Here, the parameter (bias unit), *b*, represents the *y* axis intercept and
    *w*[1] is the weight coefficient of the explanatory variable. Our goal is to learn
    the weights of the linear equation to describe the relationship between the explanatory
    variable and the target variable, which can then be used to predict the responses
    of new explanatory variables that were not part of the training dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数（偏置单元）*b*表示*y*轴截距，*w*[1]是解释变量的权重系数。我们的目标是学习线性方程的权重，以描述解释变量和目标变量之间的关系，然后用于预测不属于训练数据集的新解释变量的响应。
- en: 'Based on the linear equation that we defined previously, linear regression
    can be understood as finding the best-fitting straight line through the training
    examples, as shown in *Figure 9.1*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们之前定义的线性方程，线性回归可以理解为找到穿过训练示例的最佳拟合直线，如*图 9.1*所示：
- en: '![Chart  Description automatically generated](img/B17582_09_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B17582_09_01.png)'
- en: 'Figure 9.1: A simple one-feature linear regression example'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：一个简单的单特征线性回归示例
- en: This best-fitting line is also called the **regression line**, and the vertical
    lines from the regression line to the training examples are the so-called **offsets**
    or **residuals**—the errors of our prediction.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这条最佳拟合线也称为**回归线**，从回归线到训练样本的垂直线称为**偏移量**或**残差**—这是我们预测的误差。
- en: Multiple linear regression
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: 'The previous section introduced simple linear regression, a special case of
    linear regression with one explanatory variable. Of course, we can also generalize
    the linear regression model to multiple explanatory variables; this process is
    called **multiple linear regression**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了简单线性回归，这是线性回归的一种特殊情况，仅涉及一个解释变量。当然，我们也可以推广线性回归模型以涵盖多个解释变量；这个过程称为**多元线性回归**：
- en: '![](img/B17852_09_002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_002.png)'
- en: '*Figure 9.2* shows how the two-dimensional, fitted hyperplane of a multiple
    linear regression model with two features could look:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.2*展示了具有两个特征的多元线性回归模型的二维拟合超平面的样子：'
- en: '![](img/B17582_09_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_09_02.png)'
- en: 'Figure 9.2: A two-feature linear regression model'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：一个两特征线性回归模型
- en: As you can see, visualizations of multiple linear regression hyperplanes in
    a three-dimensional scatterplot are already challenging to interpret when looking
    at static figures. Since we have no good means of visualizing hyperplanes with
    two dimensions in a scatterplot (multiple linear regression models fit to datasets
    with three or more features), the examples and visualizations in this chapter
    will mainly focus on the univariate case, using simple linear regression. However,
    simple and multiple linear regression are based on the same concepts and the same
    evaluation techniques; the code implementations that we will discuss in this chapter
    are also compatible with both types of regression model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过三维散点图中的多元线性回归超平面的可视化已经很难解释。由于我们无法在散点图中良好地可视化具有两个以上特征的数据集（适用于多个特征的多元线性回归模型），本章的示例和可视化主要集中在单变量情况下的简单线性回归。然而，简单和多元线性回归基于相同的概念和评估技术；我们将在本章讨论的代码实现也适用于两种类型的回归模型。
- en: Exploring the Ames Housing dataset
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索艾姆斯房屋数据集
- en: 'Before we implement the first linear regression model, we will discuss a new
    dataset, the Ames Housing dataset, which contains information about individual
    residential property in Ames, Iowa, from 2006 to 2010\. The dataset was collected
    by Dean De Cock in 2011, and additional information is available via the following
    links:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实施第一个线性回归模型之前，我们将讨论一个新数据集，即艾姆斯房屋数据集，其中包含了2006年到2010年间爱荷华州艾姆斯市的个别住宅物业信息。该数据集由迪恩·迪科克于2011年收集，更多信息可通过以下链接获得：
- en: 'A report describing the dataset: [http://jse.amstat.org/v19n3/decock.pdf](http://jse.amstat.org/v19n3/decock.pdf)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述数据集的报告：[http://jse.amstat.org/v19n3/decock.pdf](http://jse.amstat.org/v19n3/decock.pdf)
- en: 'Detailed documentation regarding the dataset’s features: [http://jse.amstat.org/v19n3/decock/DataDocumentation.txt](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关数据集特征的详细文档：[http://jse.amstat.org/v19n3/decock/DataDocumentation.txt](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)
- en: 'The dataset in a tab-separated format: [http://jse.amstat.org/v19n3/decock/AmesHousing.txt](http://jse.amstat.org/v19n3/decock/AmesHousing.txt)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集以制表符分隔的格式存储：[http://jse.amstat.org/v19n3/decock/AmesHousing.txt](http://jse.amstat.org/v19n3/decock/AmesHousing.txt)
- en: As with each new dataset, it is always helpful to explore the data through a
    simple visualization, to get a better feeling of what we are working with, which
    is what we will do in the following subsections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个新数据集，通过简单的可视化来探索数据总是有帮助的，这样我们可以更好地了解我们正在处理的内容，这也是我们将在以下子节中进行的操作。
- en: Loading the Ames Housing dataset into a DataFrame
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将艾姆斯房屋数据集加载到 DataFrame 中
- en: In this section, we will load the Ames Housing dataset using the pandas `read_csv`
    function, which is fast and versatile and a recommended tool for working with
    tabular data stored in a plaintext format.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 pandas 的`read_csv`函数加载艾姆斯房屋数据集，这是一种快速且多功能的工具，推荐用于处理存储在纯文本格式中的表格数据。
- en: The Ames Housing dataset consists of 2,930 examples and 80 features. For simplicity,
    we will only work with a subset of the features, shown in the following list.
    However, if you are curious, follow the link to the full dataset description provided
    at the beginning of this section, and you are encouraged to explore other variables
    in this dataset after reading this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 艾姆斯房屋数据集包含 2,930 个示例和 80 个特征。为简单起见，我们将只使用特征的子集，如下列表所示。但是，如果你感兴趣，可以查看本节开头提供的完整数据集描述链接，并鼓励在阅读本章后探索该数据集中的其他变量。
- en: 'The features we will be working with, including the target variable, are as
    follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的特征，包括目标变量，如下所示：
- en: '`Overall Qual`: Rating for the overall material and finish of the house on
    a scale from 1 (very poor) to 10 (excellent)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Overall Qual`: 房屋整体材料和装饰的评分，范围从1（非常差）到10（优秀）'
- en: '`Overall Cond`: Rating for the overall condition of the house on a scale from
    1 (very poor) to 10 (excellent)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Overall Cond`: 房屋整体条件的评分，范围从1（非常差）到10（优秀）'
- en: '`Gr Liv Area`: Above grade (ground) living area in square feet'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gr Liv Area`: 地面以上的居住面积，以平方英尺为单位'
- en: '`Central Air`: Central air conditioning (N=no, Y=yes)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Central Air`: 中央空调（N=否，Y=是）'
- en: '`Total Bsmt SF`: Total square feet of the basement area'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Total Bsmt SF`: 地下室总面积，以平方英尺为单位'
- en: '`SalePrice`: Sale price in U.S. dollars ($)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SalePrice`: 销售价格（美元）'
- en: 'For the rest of this chapter, we will regard the sale price (`SalePrice`) as
    our target variable—the variable that we want to predict using one or more of
    the five explanatory variables. Before we explore this dataset further, let’s
    load it into a pandas `DataFrame`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将把销售价格 (`SalePrice`) 视为我们的目标变量 —— 我们希望使用五个或更多的解释变量来预测的变量。在进一步探索这个数据集之前，让我们将其加载到一个
    pandas `DataFrame` 中：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To confirm that the dataset was loaded successfully, we can display the first
    five lines of the dataset, as shown in *Figure 9.3*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认数据集已成功加载，我们可以显示数据集的前五行，如 *Figure 9.3* 所示：
- en: '![Table  Description automatically generated](img/B17582_09_03.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B17582_09_03.png)'
- en: 'Figure 9.3: The first five rows of the housing dataset'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 9.3: 房屋数据集的前五行'
- en: 'After loading the dataset, let’s also check the dimensions of the `DataFrame`
    to make sure that it contains the expected number of rows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集后，让我们还检查一下 `DataFrame` 的维度，以确保其包含预期数量的行：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, the `DataFrame` contains 2,930 rows, as expected.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`DataFrame` 包含了预期的 2,930 行。
- en: 'Another aspect we have to take care of is the `''Central Air''` variable, which
    is encoded as type `string`, as we can see in *Figure 9.3*. As we learned in *Chapter
    4*, *Building Good Training Datasets – Data Preprocessing*, we can use the `.map`
    method to convert `DataFrame` columns. The following code will convert the string
    `''``Y''` to the integer 1, and the string `''N''` to the integer 0:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要注意 `'Central Air'` 变量，它被编码为 `string` 类型，正如我们在 *Figure 9.3* 中看到的。正如我们在 *Chapter
    4* 中学到的，在转换 `DataFrame` 列时，我们可以使用 `.map` 方法。以下代码将字符串 `'Y'` 转换为整数 1，字符串 `'N'` 转换为整数
    0：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Lastly, let’s check whether any of the data frame columns contain missing values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们检查数据框中是否有任何缺失值的列：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see, the `Total Bsmt SF` feature variable contains one missing value.
    Since we have a relatively large dataset, the easiest way to deal with this missing
    feature value is to remove the corresponding example from the dataset (for alternative
    methods, please see *Chapter 4*):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`Total Bsmt SF` 特征变量包含一个缺失值。由于我们有一个相对较大的数据集，处理这个缺失的特征值的最简单方法是从数据集中删除相应的示例（有关替代方法，请参见
    *Chapter 4*）：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Visualizing the important characteristics of a dataset
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化数据集的重要特征
- en: '**Exploratory data analysis** (**EDA**) is an important and recommended first
    step prior to the training of a machine learning model. In the rest of this section,
    we will use some simple yet useful techniques from the graphical EDA toolbox that
    may help us to visually detect the presence of outliers, the distribution of the
    data, and the relationships between features.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析** (**EDA**) 是在训练机器学习模型之前的一个重要且推荐的第一步。在本节的其余部分，我们将使用一些简单但有用的技术来自可视化
    EDA 工具箱，这些技术有助于我们在视觉上检测异常值的存在、数据的分布以及特征之间的关系。'
- en: First, we will create a **scatterplot matrix** that allows us to visualize the
    pair-wise correlations between the different features in this dataset in one place.
    To plot the scatterplot matrix, we will use the `scatterplotmatrix` function from
    the mlxtend library ([http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/)),
    which is a Python library that contains various convenience functions for machine
    learning and data science applications in Python.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个 **散点图矩阵**，它允许我们在一个地方可视化数据集中不同特征之间的两两相关性。为了绘制散点图矩阵，我们将使用 mlxtend 库中的
    `scatterplotmatrix` 函数（[http://rasbt.github.io/mlxtend/](http://rasbt.github.io/mlxtend/)），这是一个包含各种方便函数的
    Python 库，用于机器学习和数据科学应用。
- en: You can install the `mlxtend` package via `conda install mlxtend` or `pip install
    mlxtend`. For this chapter, we used mlxtend version 0.19.0.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 `conda install mlxtend` 或 `pip install mlxtend` 安装 `mlxtend` 包。本章中，我们使用的是
    mlxtend 版本 0.19.0。
- en: 'Once the installation is complete, you can import the package and create the
    scatterplot matrix as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以导入包并按如下方式创建散点图矩阵：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can see in *Figure 9.4*, the scatterplot matrix provides us with a useful
    graphical summary of the relationships in a dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图9.4*中所看到的，散点图矩阵为我们提供了数据关系的有用图形总结：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_09_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含图形用户界面描述的图片](img/B17582_09_04.png)'
- en: 'Figure 9.4: A scatterplot matrix of our data'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：我们数据的散点图矩阵
- en: Using this scatterplot matrix, we can now quickly see how the data is distributed
    and whether it contains outliers. For example, we can see (fifth column from the
    left of the bottom row) that there is a somewhat linear relationship between the
    size of the living area above ground (`Gr Liv Area`) and the sale price (`SalePrice`).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个散点图矩阵，我们现在可以快速查看数据的分布情况及其是否包含异常值。例如，我们可以看到（底部行的第五列）地面以上生活区的大小(`Gr Liv Area`)与销售价格(`SalePrice`)之间存在某种线性关系。
- en: Furthermore, we can see in the histogram—the lower-right subplot in the scatterplot
    matrix—that the `SalePrice` variable seems to be skewed by several outliers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在直方图中（散点图矩阵的右下子图），我们可以看到`SalePrice`变量似乎受到几个异常值的影响。
- en: '**The normality assumption of linear regression**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归的正态性假设**'
- en: 'Note that in contrast to common belief, training a linear regression model
    does not require that the explanatory or target variables are normally distributed.
    The normality assumption is only a requirement for certain statistics and hypothesis
    tests that are beyond the scope of this book (for more information on this topic,
    please refer to *Introduction to Linear Regression Analysis* by *Douglas C. Montgomery*,
    *Elizabeth A. Peck*, and *G. Geoffrey Vining*, *Wiley*, pages: 318-319, 2012).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与普遍观念相反，训练线性回归模型并不要求解释变量或目标变量服从正态分布。正态性假设仅适用于某些超出本书范围的统计和假设检验（有关更多信息，请参阅*道格拉斯C.
    蒙哥马利*、*伊丽莎白A. 佩克*和*G. 杰弗里·文宁*的*《线性回归分析导论》*，*Wiley*，2012年，第318-319页）。
- en: Looking at relationships using a correlation matrix
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看相关矩阵以探索关系
- en: In the previous section, we visualized the data distributions of the Ames Housing
    dataset variables in the form of histograms and scatterplots. Next, we will create
    a correlation matrix to quantify and summarize linear relationships between variables.
    A correlation matrix is closely related to the covariance matrix that we covered
    in the section *Unsupervised dimensionality reduction via principal component
    analysis* in *Chapter 5*, *Compressing Data via Dimensionality Reduction*. We
    can interpret the correlation matrix as being a rescaled version of the covariance
    matrix. In fact, the correlation matrix is identical to a covariance matrix computed
    from standardized features.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们通过直方图和散点图的形式可视化了艾姆斯房屋数据集变量的数据分布情况。接下来，我们将创建一个相关矩阵，以量化和总结变量之间的线性关系。相关矩阵与我们在*第5章*
    *通过主成分分析进行无监督降维*中讨论的协方差矩阵密切相关。我们可以将相关矩阵解释为从标准化特征计算的协方差矩阵的重新缩放版本。实际上，相关矩阵与从标准化特征计算的协方差矩阵相同。
- en: 'The correlation matrix is a square matrix that contains the **Pearson product-moment
    correlation coefficient** (often abbreviated as **Pearson’s r**), which measures
    the linear dependence between pairs of features. The correlation coefficients
    are in the range –1 to 1\. Two features have a perfect positive correlation if
    *r* = 1, no correlation if *r* = 0, and a perfect negative correlation if *r*
    = –1\. As mentioned previously, Pearson’s correlation coefficient can simply be
    calculated as the covariance between two features, *x* and *y* (numerator), divided
    by the product of their standard deviations (denominator):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相关矩阵是一个方阵，包含**皮尔逊积矩相关系数**（通常缩写为**皮尔逊r**），用于衡量特征对之间的线性依赖关系。相关系数的取值范围是–1到1。如果*r*
    = 1，则两个特征具有完全正相关性；如果*r* = 0，则没有相关性；如果*r* = –1，则具有完全负相关性。如前所述，皮尔逊相关系数可以简单地计算为两个特征*x*和*y*的协方差（分子）除以它们标准差的乘积（分母）：
- en: '![](img/B17852_09_003.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![我们的数据的散点图矩阵](img/B17852_09_003.png)'
- en: Here, ![](img/B17852_09_004.png) denotes the mean of the corresponding feature,
    ![](img/B17852_09_005.png) is the covariance between the features *x* and *y*,
    and ![](img/B17852_09_006.png) and ![](img/B17852_09_007.png) are the features’
    standard deviations.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17852_09_004.png) 表示相应特征的均值，![](img/B17852_09_005.png) 是特征 *x*
    和 *y* 之间的协方差，![](img/B17852_09_006.png) 和 ![](img/B17852_09_007.png) 是特征的标准差。
- en: '**Covariance versus correlation for standardized features**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准化特征的协方差与相关性**'
- en: 'We can show that the covariance between a pair of standardized features is,
    in fact, equal to their linear correlation coefficient. To show this, let’s first
    standardize the features *x* and *y* to obtain their z-scores, which we will denote
    as *x’* and *y’*, respectively:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明一对标准化特征之间的协方差实际上等于它们的线性相关系数。为了展示这一点，让我们首先对特征 *x* 和 *y* 进行标准化，得到它们的 z 分数，分别记为
    *x’* 和 *y’*：
- en: '![](img/B17852_09_008.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_008.png)'
- en: 'Remember that we compute the (population) covariance between two features as
    follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们计算两个特征之间（总体）协方差的方法如下：
- en: '![](img/B17852_09_009.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_009.png)'
- en: 'Since standardization centers a feature variable at mean zero, we can now calculate
    the covariance between the scaled features as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标准化将特征变量居中于零均值，我们现在可以计算缩放特征之间的协方差如下：
- en: '![](img/B17852_09_010.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_010.png)'
- en: 'Through resubstitution, we then get the following result:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过代入法，我们得到以下结果：
- en: '![](img/B17852_09_011.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_011.png)'
- en: 'Finally, we can simplify this equation as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以简化这个方程如下：
- en: '![](img/B17852_09_012.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_012.png)'
- en: 'In the following code example, we will use NumPy’s `corrcoef` function on the
    five feature columns that we previously visualized in the scatterplot matrix,
    and we will use mlxtend’s `heatmap` function to plot the correlation matrix array
    as a heat map:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们将使用 NumPy 的 `corrcoef` 函数来计算我们先前在散点图矩阵中可视化的五个特征列的相关系数，并使用 mlxtend
    的 `heatmap` 函数将相关矩阵数组绘制为热图：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see in *Figure 9.5*, the correlation matrix provides us with another
    useful summary graphic that can help us to select features based on their respective
    linear correlations:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 *图 9.5* 中所示，相关矩阵为我们提供了另一个有用的摘要图形，可以帮助我们根据它们各自的线性相关性选择特征：
- en: '![Graphical user interface, application  Description automatically generated](img/B17582_09_05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面应用程序说明](img/B17582_09_05.png)'
- en: 'Figure 9.5: A correlation matrix of the selected variables'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：所选变量的相关矩阵
- en: To fit a linear regression model, we are interested in those features that have
    a high correlation with our target variable, `SalePrice`. Looking at the previous
    correlation matrix, we can see that `SalePrice` shows the largest correlation
    with the `Gr Liv Area` variable (`0.71`), which seems to be a good choice for
    an exploratory variable to introduce the concepts of a simple linear regression
    model in the following section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要拟合线性回归模型，我们对那些与目标变量 `SalePrice` 具有高相关性的特征感兴趣。从前面的相关矩阵可以看出，`SalePrice` 与 `Gr
    Liv Area` 变量（`0.71`）显示出最大的相关性，这似乎是引入简单线性回归模型概念的一个不错的选择。
- en: Implementing an ordinary least squares linear regression model
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现普通最小二乘线性回归模型
- en: At the beginning of this chapter, we mentioned that linear regression can be
    understood as obtaining the best-fitting straight line through the examples of
    our training data. However, we have neither defined the term *best-fitting* nor
    have we discussed the different techniques of fitting such a model. In the following
    subsections, we will fill in the missing pieces of this puzzle using the **ordinary
    least squares** (**OLS**) method (sometimes also called **linear least squares**)
    to estimate the parameters of the linear regression line that minimizes the sum
    of the squared vertical distances (residuals or errors) to the training examples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们提到线性回归可以理解为通过我们的训练数据示例获取最佳拟合直线。然而，我们既未定义“最佳拟合”的术语，也未讨论拟合这种模型的不同技术。在接下来的小节中，我们将使用
    **普通最小二乘法**（OLS）方法（有时也称为 **线性最小二乘法**）来估计最小化与训练示例的平方垂直距离（残差或误差）的线性回归线的参数。
- en: Solving regression for regression parameters with gradient descent
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降解决回归参数的回归
- en: Consider our implementation of the **Adaptive Linear Neuron** (**Adaline**)
    from *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*.
    You will remember that the artificial neuron uses a linear activation function.
    Also, we defined a loss function, *L*(**w**), which we minimized to learn the
    weights via optimization algorithms, such as **gradient descent** (**GD**) and
    **stochastic gradient descent** (**SGD**).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们在 *第 2 章* 中对 **自适应线性神经元** (**Adaline**) 的实现，*用于分类的简单机器学习算法训练*。您会记得，这种人工神经元使用线性激活函数。此外，我们定义了一个损失函数
    *L*(**w**)，通过优化算法（如 **梯度下降** (**GD**) 和 **随机梯度下降** (**SGD**)）最小化该函数以学习权重。
- en: 'This loss function in Adaline is the **mean squared error** (**MSE**), which
    is identical to the loss function that we use for OLS:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Adaline 中，这个损失函数是 **均方误差** (**MSE**)，与我们用于 OLS 的损失函数相同：
- en: '![](img/B17852_09_013.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_013.png)'
- en: 'Here, ![](img/B17582_04_008.png) is the predicted value ![](img/B17852_09_015.png)
    (note that the term ![](img/B17852_09_016.png) is just used for convenience to
    derive the update rule of GD). Essentially, OLS regression can be understood as
    Adaline without the threshold function so that we obtain continuous target values
    instead of the class labels `0` and `1`. To demonstrate this, let’s take the GD
    implementation of Adaline from *Chapter 2* and remove the threshold function to
    implement our first linear regression model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_04_008.png) 是预测值 ![](img/B17852_09_015.png)（注意，术语 ![](img/B17852_09_016.png)
    仅用于方便推导 GD 的更新规则）。本质上，OLS 回归可以理解为 Adaline 没有阈值函数，因此我们获得连续的目标值而不是类标签 `0` 和 `1`。为了演示这一点，让我们从
    *第 2 章* 中取出 Adaline 的 GD 实现，并去掉阈值函数来实现我们的第一个线性回归模型：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Weight updates with gradient descent**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用梯度下降更新权重**'
- en: If you need a refresher about how the weights are updated—taking a step in the
    opposite direction of the gradient—please revisit the *Adaptive linear neurons
    and the convergence of learning* section in *Chapter 2*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要关于权重如何更新的复习 —— 即沿梯度相反方向迈出一步，请参阅 *第 2 章* 中的 *自适应线性神经元和学习的收敛* 部分。
- en: 'To see our `LinearRegressionGD` regressor in action, let’s use the `Gr Living
    Area` (size of the living area above ground in square feet) feature from the Ames
    Housing dataset as the explanatory variable and train a model that can predict
    `SalePrice`. Furthermore, we will standardize the variables for better convergence
    of the GD algorithm. The code is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要看看我们的 `LinearRegressionGD` 回归器如何运行，让我们使用 Ames 房屋数据集中的 `Gr Living Area`（地面以上的居住面积，以平方英尺为单位）特征作为解释变量，并训练一个能够预测
    `SalePrice` 的模型。此外，我们将标准化变量以获得更好的 GD 算法收敛性。代码如下：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice the workaround regarding `y_std`, using `np.newaxis` and `flatten`. Most
    data preprocessing classes in scikit-learn expect data to be stored in two-dimensional
    arrays. In the previous code example, the use of `np.newaxis` in `y[:, np.newaxis]`
    added a new dimension to the array. Then, after `StandardScaler` returned the
    scaled variable, we converted it back to the original one-dimensional array representation
    using the `flatten()` method for our convenience.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意关于 `y_std` 的解决方法，使用 `np.newaxis` 和 `flatten`。scikit-learn 中大多数数据预处理类都希望数据存储在二维数组中。在前面的代码示例中，`y[:,
    np.newaxis]` 中使用 `np.newaxis` 添加了一个新的数组维度。然后，在 `StandardScaler` 返回缩放后的变量后，我们使用
    `flatten()` 方法将其转换回原始的一维数组表示，以便我们使用时更方便。
- en: 'We discussed in *Chapter 2* that it is always a good idea to plot the loss
    as a function of the number of epochs (complete iterations) over the training
    dataset when we are using optimization algorithms, such as GD, to check that the
    algorithm converged to a loss minimum (here, a *global* loss minimum):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第 2 章* 中讨论过，当我们使用优化算法（如 GD）时，绘制损失作为训练数据集上的 epoch 数（完整迭代次数）函数，以检查算法是否收敛到损失最小值（这里是
    *全局* 损失最小值）是一个很好的做法：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see in *Figure 9.6*, the GD algorithm converged approximately after
    the tenth epoch:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 *图 9.6* 中所见，GD 算法大约在第十个 epoch 后收敛：
- en: '![Shape  Description automatically generated](img/B17582_09_06.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![形状描述自动生成](img/B17582_09_06.png)'
- en: 'Figure 9.6: The loss function versus the number of epochs'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：损失函数与 epoch 数的关系
- en: 'Next, let’s visualize how well the linear regression line fits the training
    data. To do so, we will define a simple helper function that will plot a scatterplot
    of the training examples and add the regression line:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们可视化线性回归线对训练数据的拟合程度。为此，我们将定义一个简单的辅助函数，用于绘制训练样本的散点图并添加回归线：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we will use this `lin_regplot` function to plot the living area against
    the sale price:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `lin_regplot` 函数绘制居住面积与销售价格：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see in *Figure 9.7*, the linear regression line reflects the general
    trend that house prices tend to increase with the size of the living area:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*图 9.7*中所看到的，线性回归线反映了房屋价格倾向于随着居住面积的增加而增加的一般趋势：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_07.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_07.png)'
- en: 'Figure 9.7: A linear regression plot of sale prices versus living area size'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：销售价格与居住面积大小的线性回归图
- en: Although this observation makes sense, the data also tells us that the living
    area size does not explain house prices very well in many cases. Later in this
    chapter, we will discuss how to quantify the performance of a regression model.
    Interestingly, we can also observe several outliers, for example, the three data
    points corresponding to a standardized living area greater than 6\. We will discuss
    how we can deal with outliers later in this chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种观察是有道理的，但数据还告诉我们，在许多情况下，居住面积大小并不能很好地解释房价。本章后面我们将讨论如何量化回归模型的性能。有趣的是，我们还可以观察到几个异常值，例如，对应于标准化后大于
    6 的生活区的三个数据点。我们将在本章后面讨论如何处理异常值。
- en: 'In certain applications, it may also be important to report the predicted outcome
    variables on their original scale. To scale the predicted price back onto the
    original *price in U.S. dollars* scale, we can simply apply the `inverse_transform`
    method of `StandardScaler`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，报告预测结果变量在其原始比例上也可能很重要。要将预测的价格重新缩放到原始的美元价格尺度上，我们可以简单地应用 `StandardScaler`
    的 `inverse_transform` 方法：
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this code example, we used the previously trained linear regression model
    to predict the price of a house with an aboveground living area of 2,500 square
    feet. According to our model, such a house will be worth $292,507.07.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们使用之前训练过的线性回归模型预测了一个地面以上居住面积为 2,500 平方英尺的房屋的价格。根据我们的模型，这样一栋房子价值 $292,507.07。
- en: 'As a side note, it is also worth mentioning that we technically don’t have
    to update the intercept parameter (for instance, the bias unit, *b*) if we are
    working with standardized variables, since the *y* axis intercept is always 0
    in those cases. We can quickly confirm this by printing the model parameters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一句，值得一提的是，如果我们使用标准化的变量，我们在技术上不必更新截距参数（例如，偏置单元，*b*），因为在这些情况下 *y* 轴截距始终为 0。我们可以通过打印模型参数来快速确认这一点：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Estimating the coefficient of a regression model via scikit-learn
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 scikit-learn 估计回归模型的系数
- en: 'In the previous section, we implemented a working model for regression analysis;
    however, in a real-world application, we may be interested in more efficient implementations.
    For example, many of scikit-learn’s estimators for regression make use of the
    least squares implementation in SciPy (`scipy.linalg.lstsq`), which, in turn,
    uses highly optimized code optimizations based on the **Linear Algebra Package**
    (**LAPACK**). The linear regression implementation in scikit-learn also works
    (better) with unstandardized variables, since it does not use (S)GD-based optimization,
    so we can skip the standardization step:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们实现了一个用于回归分析的工作模型；然而，在实际应用中，我们可能对更高效的实现感兴趣。例如，许多 scikit-learn 中用于回归的估计器使用了
    SciPy 中的最小二乘实现 (`scipy.linalg.lstsq`)，而 SciPy 又使用了基于**线性代数包**（**LAPACK**）的高度优化代码。scikit-learn
    中的线性回归实现也可以（更好地）处理非标准化的变量，因为它不使用（S）GD-based 优化，所以我们可以跳过标准化步骤：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see from executing this code, scikit-learn’s `LinearRegression`
    model, fitted with the unstandardized `Gr Liv Area` and `SalePrice` variables,
    yielded different model coefficients, since the features have not been standardized.
    However, when we compare it to our GD implementation by plotting `SalePrice` against
    `Gr Liv Area`, we can qualitatively see that it fits the data similarly well:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您通过执行此代码所看到的，用未标准化的 `Gr Liv Area` 和 `SalePrice` 变量拟合的 scikit-learn 的 `LinearRegression`
    模型产生了不同的模型系数，因为这些特征没有被标准化。然而，当我们将其与通过绘制 `SalePrice` 对 `Gr Liv Area` 进行的 GD 实现进行比较时，我们可以从质量上看到它与数据的拟合程度相似：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For instance, we can see that the overall result looks identical to our GD
    implementation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到整体结果与我们的 GD 实现看起来是相同的：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_08.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_08.png)'
- en: 'Figure 9.8: A linear regression plot using scikit-learn'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：使用 scikit-learn 的线性回归绘制的线性回归图
- en: '**Analytical solutions of linear regression**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归的解析解**'
- en: 'As an alternative to using machine learning libraries, there is also a closed-form
    solution for solving OLS involving a system of linear equations that can be found
    in most introductory statistics textbooks:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用机器学习库不同的替代方法是，还存在一种用于解决OLS的闭式解，涉及解决线性方程组的系统，这种方法可以在大多数统计学入门教材中找到：
- en: '![](img/B17852_09_017.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_017.png)'
- en: 'We can implement it in Python as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Python中实现如下：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The advantage of this method is that it is guaranteed to find the optimal solution
    analytically. However, if we are working with very large datasets, it can be computationally
    too expensive to invert the matrix in this formula (sometimes also called the
    normal equation), or the matrix containing the training examples may be singular
    (non-invertible), which is why we may prefer iterative methods in certain cases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点在于保证以解析方式找到最优解。然而，如果我们处理的是非常大的数据集，通过在这个公式中求逆矩阵可能会导致计算量过大（有时也称为正规方程），或者包含训练示例的矩阵可能是奇异的（不可逆的），因此在某些情况下我们可能更喜欢使用迭代方法。
- en: If you are interested in more information on how to obtain normal equations,
    take a look at Dr. Stephen Pollock’s chapter *The Classical Linear Regression
    Model*, from his lectures at the University of Leicester, which is available for
    free at [http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf](http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对如何获取正规方程更多信息感兴趣，请查看斯蒂芬·波洛克博士在莱斯特大学的讲座《经典线性回归模型》中的章节，该讲座可以免费获取，网址为[http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf](http://www.le.ac.uk/users/dsgp1/COURSES/MESOMET/ECMETXT/06mesmet.pdf)。
- en: Also, if you want to compare linear regression solutions obtained via GD, SGD,
    the closed-form solution, QR factorization, and singular vector decomposition,
    you can use the `LinearRegression` class implemented in mlxtend ([http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/](http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/)),
    which lets users toggle between these options. Another great library to recommend
    for regression modeling in Python is statsmodels, which implements more advanced
    linear regression models, as illustrated at [https://www.statsmodels.org/stable/examples/index.html#regression](https://www.statsmodels.org/stable/examples/index.html#regression).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您想要比较通过GD、SGD、闭式解、QR分解和奇异值分解获得的线性回归解决方案，可以使用mlxtend中实现的`LinearRegression`类（[http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/](http://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/)），该类允许用户在这些选项之间切换。另一个在Python中推荐的用于回归建模的优秀库是statsmodels，它实现了更高级的线性回归模型，如在[https://www.statsmodels.org/stable/examples/index.html#regression](https://www.statsmodels.org/stable/examples/index.html#regression)中展示的。
- en: Fitting a robust regression model using RANSAC
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RANSAC拟合鲁棒回归模型
- en: Linear regression models can be heavily impacted by the presence of outliers.
    In certain situations, a very small subset of our data can have a big effect on
    the estimated model coefficients. Many statistical tests can be used to detect
    outliers, but these are beyond the scope of the book. However, removing outliers
    always requires our own judgment as data scientists as well as our domain knowledge.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型可能会受到异常值的严重影响。在某些情况下，我们数据的一个非常小的子集可能会对估计的模型系数产生重大影响。许多统计检验可以用来检测异常值，但这些超出了本书的范围。然而，去除异常值始终需要我们作为数据科学家自己的判断以及我们的领域知识。
- en: As an alternative to throwing out outliers, we will look at a robust method
    of regression using the **RANdom SAmple Consensus** (**RANSAC**) algorithm, which
    fits a regression model to a subset of the data, the so-called **inliers**.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作为抛弃异常值的一种替代方案，我们将介绍一种使用**随机采样一致性**（**RANdom SAmple Consensus**，**RANSAC**）算法进行鲁棒回归的方法，该算法将回归模型拟合到数据的一个子集，即所谓的**内点**。
- en: 'We can summarize the iterative RANSAC algorithm as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结迭代的RANSAC算法如下：
- en: Select a random number of examples to be inliers and fit the model.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一定数量的例子作为内点并拟合模型。
- en: Test all other data points against the fitted model and add those points that
    fall within a user-given tolerance to the inliers.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有其他数据点与拟合模型进行测试，并将那些落在用户给定容差范围内的点添加到内点。
- en: Refit the model using all inliers.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有内点重新拟合模型。
- en: Estimate the error of the fitted model versus the inliers.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估算与内点拟合模型之间的误差。
- en: Terminate the algorithm if the performance meets a certain user-defined threshold
    or if a fixed number of iterations was reached; go back to *step 1* otherwise.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果性能达到某个用户定义的阈值或达到固定迭代次数，则终止算法；否则返回到 *步骤 1*。
- en: 'Let’s now use a linear model in combination with the RANSAC algorithm as implemented
    in scikit-learn’s `RANSACRegressor` class:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 scikit-learn 的 `RANSACRegressor` 类中实现的线性模型与 RANSAC 算法结合使用：
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We set the maximum number of iterations of the `RANSACRegressor` to 100, and
    using `min_samples=0.95`, we set the minimum number of the randomly chosen training
    examples to be at least 95 percent of the dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `RANSACRegressor` 的最大迭代次数设置为 100，并且使用 `min_samples=0.95`，将随机选择的训练样本的最小数量设置为数据集的至少
    95%。
- en: By default (via `residual_threshold=None`), scikit-learn uses the **MAD** estimate
    to select the inlier threshold, where MAD stands for the **median absolute deviation**
    of the target values, `y`. However, the choice of an appropriate value for the
    inlier threshold is problem-specific, which is one disadvantage of RANSAC.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下（通过 `residual_threshold=None`），scikit-learn 使用 **MAD** 估计来选择内点阈值，其中 MAD
    表示目标值 `y` 的 **中位数绝对偏差**。然而，选择适当的内点阈值的适用性问题特定，这是 RANSAC 的一个缺点。
- en: 'Many different approaches have been developed in recent years to select a good
    inlier threshold automatically. You can find a detailed discussion in *Automatic
    Estimation of the Inlier Threshold in Robust Multiple Structures Fitting* by *R.
    Toldo* and *A. Fusiello*, *Springer*, 2009 (in *Image Analysis and Processing–ICIAP
    2009*, pages: 123-131).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来已开发了许多不同的方法来自动选择良好的内点阈值。您可以在 *R. Toldo* 和 *A. Fusiello* 的 *Springer*, 2009
    年的 *Image Analysis and Processing–ICIAP 2009* （页面：123-131）中找到详细讨论。
- en: 'Once we have fitted the RANSAC model, let’s obtain the inliers and outliers
    from the fitted RANSAC linear regression model and plot them together with the
    linear fit:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拟合了 RANSAC 模型，让我们从拟合的 RANSAC 线性回归模型中获取内点和外点，并将它们与线性拟合一起绘制：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see in *Figure 9.9*, the linear regression model was fitted on the
    detected set of inliers, which are shown as circles:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 *图 9.9* 中所看到的那样，线性回归模型是在检测到的内点集上拟合的，这些内点显示为圆圈：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_09.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_09.png)'
- en: 'Figure 9.9: Inliers and outliers identified via a RANSAC linear regression
    model'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：通过 RANSAC 线性回归模型识别的内点和外点
- en: 'When we print the slope and intercept of the model by executing the following
    code, the linear regression line will be slightly different from the fit that
    we obtained in the previous section without using RANSAC:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行以下代码打印模型的斜率和截距时，线性回归线将与我们在之前未使用 RANSAC 时得到的拟合略有不同：
- en: '[PRE19]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Remember that we set the `residual_threshold` parameter to `None`, so RANSAC
    was using the MAD to compute the threshold for flagging inliers and outliers.
    The MAD, for this dataset, can be computed as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们将 `residual_threshold` 参数设置为 `None`，因此 RANSAC 使用 MAD 来计算标记内点和外点的阈值。对于此数据集，MAD
    可以计算如下：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So, if we want to identify fewer data points as outliers, we can choose a `residual_threshold`
    value greater than the preceding MAD. For example, *Figure 9.10* shows the inliers
    and outliers of a RANSAC linear regression model with a residual threshold of
    65,000:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们希望将较少的数据点识别为离群值，我们可以选择一个比前面的 MAD 更大的 `residual_threshold` 值。例如，*图 9.10*
    展示了具有 65,000 的残差阈值的 RANSAC 线性回归模型的内点和外点：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_10.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成描述](img/B17582_09_10.png)'
- en: 'Figure 9.10: Inliers and outliers determined by a RANSAC linear regression
    model with a larger residual threshold'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：由具有较大残差阈值的 RANSAC 线性回归模型确定的内点和外点
- en: Using RANSAC, we reduced the potential effect of the outliers in this dataset,
    but we don’t know whether this approach will have a positive effect on the predictive
    performance for unseen data or not. Thus, in the next section, we will look at
    different approaches for evaluating a regression model, which is a crucial part
    of building systems for predictive modeling.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RANSAC，我们减少了数据集中离群值的潜在影响，但我们不知道这种方法是否会对未见数据的预测性能产生积极影响。因此，在接下来的章节中，我们将探讨不同的方法来评估回归模型，这是构建预测建模系统的关键部分。
- en: Evaluating the performance of linear regression models
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估线性回归模型的性能
- en: In the previous section, you learned how to fit a regression model on training
    data. However, you discovered in previous chapters that it is crucial to test
    the model on data that it hasn’t seen during training to obtain a more unbiased
    estimate of its generalization performance.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学会了如何在训练数据上拟合回归模型。然而，你在之前的章节中发现，将模型在训练过程中未见过的数据上进行测试是至关重要的，以获得其泛化性能的更加无偏估计。
- en: 'As you may remember from *Chapter 6*, *Learning Best Practices for Model Evaluation
    and Hyperparameter Tuning*, we want to split our dataset into separate training
    and test datasets, where we will use the former to fit the model and the latter
    to evaluate its performance on unseen data to estimate the generalization performance.
    Instead of proceeding with the simple regression model, we will now use all five
    features in the dataset and train a multiple regression model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的那样，来自第6章《学习模型评估和超参数调整的最佳实践》的内容，我们希望将数据集分成单独的训练和测试数据集，其中我们将使用前者来拟合模型，并使用后者来评估其在未见数据上的性能，以估计泛化性能。现在，我们不再使用简单的回归模型，而是使用数据集中的所有五个特征并训练多元回归模型：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since our model uses multiple explanatory variables, we can’t visualize the
    linear regression line (or hyperplane, to be precise) in a two-dimensional plot,
    but we can plot the residuals (the differences or vertical distances between the
    actual and predicted values) versus the predicted values to diagnose our regression
    model. **Residual plots** are a commonly used graphical tool for diagnosing regression
    models. They can help to detect nonlinearity and outliers and check whether the
    errors are randomly distributed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型使用多个解释变量，我们无法在二维图中可视化线性回归线（或者更精确地说是超平面），但我们可以绘制残差（实际值与预测值之间的差异或垂直距离）与预测值的图，来诊断我们的回归模型。**残差图**是诊断回归模型常用的图形工具，它们有助于检测非线性和异常值，并检查错误是否随机分布。
- en: 'Using the following code, we will now plot a residual plot where we simply
    subtract the true target variables from our predicted responses:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，我们将绘制一个残差图，其中我们简单地从预测响应中减去真实目标变量：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After executing the code, we should see residual plots for the test and training
    datasets with a line passing through the *x* axis origin, as shown in *Figure
    9.11*:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们应该能看到测试和训练数据集的残差图，其中有一条通过*x*轴原点的线，如*图 9.11*所示：
- en: '![Graphical user interface  Description automatically generated with low confidence](img/B17582_09_11.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  低置信度自动生成描述](https://img.example.org/B17582_09_11.png)'
- en: 'Figure 9.11: Residual plots of our data'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：我们数据的残差图
- en: In the case of a perfect prediction, the residuals would be exactly zero, which
    we will probably never encounter in realistic and practical applications. However,
    for a good regression model, we would expect the errors to be randomly distributed
    and the residuals to be randomly scattered around the centerline. If we see patterns
    in a residual plot, it means that our model is unable to capture some explanatory
    information, which has leaked into the residuals, as you can see to a degree in
    our previous residual plot. Furthermore, we can also use residual plots to detect
    outliers, which are represented by the points with a large deviation from the
    centerline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在完美预测的情况下，残差将恰好为零，在现实和实际应用中，我们可能永远不会遇到这种情况。然而，对于一个良好的回归模型，我们期望错误是随机分布的，残差在中心线周围随机分散。如果在残差图中看到模式，这意味着我们的模型无法捕获某些解释信息，这些信息已泄漏到残差中，正如我们之前的残差图中可能看到的那样。此外，我们还可以使用残差图检测异常值，这些异常值由偏离中心线较大的点表示。
- en: 'Another useful quantitative measure of a model’s performance is the **mean
    squared error** (**MSE**) that we discussed earlier as our loss function that
    we minimized to fit the linear regression model. The following is a version of
    the MSE without the ![](img/B17852_09_016.png) scaling factor that is often used
    to simplify the loss derivative in gradient descent:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个衡量模型性能的有用量化指标是我们之前讨论过的**均方误差**（**MSE**），它是我们用来最小化以拟合线性回归模型的损失函数。以下是不带缩放因子的MSE版本，通常用于简化梯度下降中的损失导数：
- en: '![](img/B17852_09_019.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![https://img.example.org/B17852_09_019.png](https://img.example.org/B17852_09_019.png)'
- en: Similar to prediction accuracy in classification contexts, we can use the MSE
    for cross-validation and model selection as discussed in *Chapter 6*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于分类环境中的预测准确性，我们可以使用MSE进行交叉验证和模型选择，如第6章讨论的那样。
- en: Like classification accuracy, MSE also normalizes according to the sample size,
    *n*. This makes it possible to compare across different sample sizes (for example,
    in the context of learning curves) as well.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于分类准确率，MSE也根据样本大小*n*进行归一化。这使得我们可以跨不同的样本大小进行比较（例如，在学习曲线的背景下）。
- en: 'Let’s now compute the MSE of our training and test predictions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算我们的训练和测试预测的MSE：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can see that the MSE on the training dataset is larger than on the test
    set, which is an indicator that our model is slightly overfitting the training
    data in this case. Note that it can be more intuitive to show the error on the
    original unit scale (here, dollar instead of dollar-squared), which is why we
    may choose to compute the square root of the MSE, called *root mean squared error*,
    or the **mean absolute error** (**MAE**), which emphasizes incorrect prediction
    slightly less:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到训练数据集上的MSE大于测试集上的MSE，这表明在这种情况下我们的模型稍微过拟合了训练数据。注意，以原始单位标度（这里是美元而不是美元平方）显示误差可能更直观，因此我们可能选择计算MSE的平方根，称为*均方根误差*，或者**平均绝对误差**（**MAE**），稍微强调错误预测的重要性较小：
- en: '![](img/B17852_09_020.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_020.png)'
- en: 'We can compute the MAE similar to the MSE:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似于MSE计算MAE：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Based on the test set MAE, we can say that the model makes an error of approximately
    $25,000 on average.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 基于测试集的平均绝对误差（MAE），我们可以说该模型的误差大约为$25,000。
- en: When we use the MAE or MSE for comparing models, we need to be aware that these
    are unbounded in contrast to the classification accuracy, for example. In other
    words, the interpretations of the MAE and MSE depend on the dataset and feature
    scaling. For example, if the sale prices were presented as multiples of 1,000
    (with the K suffix), the same model would yield a lower MAE compared to a model
    that worked with unscaled features. To further illustrate this point,
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用MAE或MSE来比较模型时，需要注意它们与例如分类准确率相比没有上限。换句话说，MAE和MSE的解释取决于数据集和特征缩放。例如，如果销售价格以1000的倍数（带有K后缀）表示，同一个模型将产生比处理未缩放特征的模型更低的MAE。为了进一步说明这一点，
- en: '![](img/B17852_09_021.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_021.png)'
- en: 'Thus, it may sometimes be more useful to report the **coefficient of determination**
    (*R*²), which can be understood as a standardized version of the MSE, for better
    interpretability of the model’s performance. Or, in other words, *R*² is the fraction
    of response variance that is captured by the model. The *R*² value is defined
    as:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有时候更有用的是报告**决定系数**（*R*²），它可以理解为MSE的标准化版本，以更好地解释模型的性能。换句话说，*R*²是模型捕捉的响应方差的比例。*R*²的值定义为：
- en: '![](img/B17852_09_022.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_022.png)'
- en: 'Here, SSE is the sum of squared errors, which is similar to the MSE but does
    not include the normalization by sample size *n*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，SSE是平方误差的总和，类似于MSE，但不包括样本大小*n*的归一化：
- en: '![](img/B17852_09_023.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_023.png)'
- en: 'And SST is the total sum of squares:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 而SST则是总平方和：
- en: '![](img/B17852_09_024.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_024.png)'
- en: In other words, SST is simply the variance of the response.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，SST只是响应的方差。
- en: 'Now, let’s briefly show that *R*² is indeed just a rescaled version of the
    MSE:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要展示*R*²实际上只是MSE的重新缩放版本：
- en: '![](img/B17852_09_025.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_025.png)'
- en: For the training dataset, *R*² is bounded between 0 and 1, but it can become
    negative for the test dataset. A negative *R*² means that the regression model
    fits the data worse than a horizontal line representing the sample mean. (In practice,
    this often happens in the case of extreme overfitting, or if we forget to scale
    the test set in the same manner we scaled the training set.) If *R*² = 1, the
    model fits the data perfectly with a corresponding *MSE* = 0.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据集，*R*²被限制在0到1之间，但对于测试数据集可能为负。负的*R*²意味着回归模型比表示样本均值的水平线拟合数据更差。（在实践中，这经常发生在极端过拟合的情况下，或者如果我们忘记以与训练集相同的方式缩放测试集。）如果*R*²
    = 1，则模型完全拟合数据，相应的*MSE* = 0。
- en: 'Evaluated on the training data, the *R*² of our model is 0.77, which isn’t
    great but also not too bad given that we only work with a small set of features.
    However, the *R*² on the test dataset is only slightly smaller, at 0.75, which
    indicates that the model is only overfitting slightly:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上评估，我们模型的*R*²为0.77，这并不理想，但考虑到我们只使用了少量特征，也不算太糟糕。然而，测试数据集上的*R*²略小，为0.75，这表明模型只稍微过拟合了：
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Using regularized methods for regression
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则化方法进行回归
- en: As we discussed in *Chapter 3*, *A Tour of Machine Learning Classifiers Using
    Scikit-Learn*, regularization is one approach to tackling the problem of overfitting
    by adding additional information and thereby shrinking the parameter values of
    the model to induce a penalty against complexity. The most popular approaches
    to regularized linear regression are the so-called **ridge regression**, **least
    absolute shrinkage and selection operator** (**LASSO**), and **elastic net**.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第3章*，*使用Scikit-Learn进行机器学习分类器的一次旅行*中讨论的那样，正则化是通过添加额外信息来解决过拟合问题的一种方法，从而缩小模型参数值以对复杂性施加惩罚。正则化线性回归的最流行方法是所谓的**Ridge回归**，**最小绝对收缩和选择算子**（**LASSO**），以及**弹性网络**。
- en: 'Ridge regression is an L2 penalized model where we simply add the squared sum
    of the weights to the MSE loss function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge回归是一种L2惩罚模型，我们只需将权重的平方和添加到MSE损失函数中：
- en: '![](img/B17852_09_026.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_026.png)'
- en: 'Here, the L2 term is defined as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，L2项被定义如下：
- en: '![](img/B17852_09_027.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_027.png)'
- en: By increasing the value of hyperparameter ![](img/B17852_09_028.png), we increase
    the regularization strength and thereby shrink the weights of our model. Please
    note that, as mentioned in *Chapter 3*, the bias unit *b* is not regularized.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加超参数 ![](img/B17852_09_028.png) 的值，我们增加正则化强度，从而缩小模型的权重。请注意，正如*第3章*中提到的，偏置单元
    *b* 没有经过正则化。
- en: 'An alternative approach that can lead to sparse models is LASSO. Depending
    on the regularization strength, certain weights can become zero, which also makes
    LASSO useful as a supervised feature selection technique:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可以导致稀疏模型的替代方法是LASSO。根据正则化强度，某些权重可以变为零，这也使得LASSO作为一种监督特征选择技术非常有用：
- en: '![](img/B17852_09_029.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_029.png)'
- en: 'Here, the L1 penalty for LASSO is defined as the sum of the absolute magnitudes
    of the model weights, as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，LASSO的L1惩罚被定义为模型权重的绝对值的总和，如下所示：
- en: '![](img/B17852_09_030.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_030.png)'
- en: However, a limitation of LASSO is that it selects at most *n* features if *m*
    > *n*, where *n* is the number of training examples. This may be undesirable in
    certain applications of feature selection. In practice, however, this property
    of LASSO is often an advantage because it avoids saturated models. The saturation
    of a model occurs if the number of training examples is equal to the number of
    features, which is a form of overparameterization. As a consequence, a saturated
    model can always fit the training data perfectly but is merely a form of interpolation
    and thus is not expected to generalize well.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LASSO的一个限制是，如果 *m* > *n*，它最多选择 *n* 个特征，其中 *n* 是训练示例的数量。在某些特征选择的应用中，这可能是不希望的。然而，在实践中，LASSO的这种性质通常是优点，因为它避免了饱和模型。模型的饱和发生在训练示例数等于特征数时，这是一种过度参数化的形式。因此，饱和模型可以完美拟合训练数据，但仅仅是一种插值形式，因此不太可能很好地推广。
- en: 'A compromise between ridge regression and LASSO is elastic net, which has an
    L1 penalty to generate sparsity and an L2 penalty such that it can be used for
    selecting more than *n* features if *m* > *n*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge回归和LASSO之间的折衷方案是弹性网络，它具有L1惩罚以生成稀疏性，并且具有L2惩罚，因此可以用于选择超过 *n* 个特征，如果 *m* >
    *n*：
- en: '![](img/B17852_09_031.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_031.png)'
- en: Those regularized regression models are all available via scikit-learn, and
    their usage is similar to the regular regression model except that we have to
    specify the regularization strength via the parameter ![](img/B17852_09_028.png),
    for example, optimized via k-fold cross-validation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些正则化回归模型都可以通过scikit-learn获得，并且它们的使用与常规回归模型类似，只是我们必须通过参数 ![](img/B17852_09_028.png)
    指定正则化强度，例如通过k折交叉验证进行优化。
- en: 'A ridge regression model can be initialized via:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式初始化Ridge回归模型：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Note that the regularization strength is regulated by the parameter `alpha`,
    which is similar to the parameter ![](img/B17852_09_033.png). Likewise, we can
    initialize a LASSO regressor from the `linear_model` submodule:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，正则化强度由参数 `alpha` 调节，这类似于参数 ![](img/B17852_09_033.png)。同样，我们可以从`linear_model`子模块初始化一个LASSO回归器：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Lastly, the `ElasticNet` implementation allows us to vary the L1 to L2 ratio:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`ElasticNet`实现允许我们改变L1到L2的比例：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: For example, if we set `l1_ratio` to 1.0, the `ElasticNet` regressor would be
    equal to LASSO regression. For more detailed information about the different implementations
    of linear regression, please refer to the documentation at [http://scikit-learn.org/stable/modules/linear_model.html](http://scikit-learn.org/stable/modules/linear_model.html).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果将 `l1_ratio` 设置为 1.0，则 `ElasticNet` 回归器将等同于 LASSO 回归。有关线性回归不同实现的详细信息，请参阅
    [http://scikit-learn.org/stable/modules/linear_model.html](http://scikit-learn.org/stable/modules/linear_model.html)
    的文档。
- en: Turning a linear regression model into a curve – polynomial regression
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将线性回归模型转变为曲线 - 多项式回归
- en: 'In the previous sections, we assumed a linear relationship between explanatory
    and response variables. One way to account for the violation of linearity assumption
    is to use a polynomial regression model by adding polynomial terms:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们假设解释变量和响应变量之间存在线性关系。解决线性假设违反的一种方法是通过添加多项式项使用多项式回归模型：
- en: '![](img/B17852_09_034.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_034.png)'
- en: Here, *d* denotes the degree of the polynomial. Although we can use polynomial
    regression to model a nonlinear relationship, it is still considered a multiple
    linear regression model because of the linear regression coefficients, **w**.
    In the following subsections, we will see how we can add such polynomial terms
    to an existing dataset conveniently and fit a polynomial regression model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d* 表示多项式的次数。虽然我们可以使用多项式回归模型来建模非线性关系，但由于线性回归系数 **w** 的存在，它仍被视为多重线性回归模型。在接下来的小节中，我们将看到如何方便地向现有数据集添加这样的多项式项并拟合多项式回归模型。
- en: Adding polynomial terms using scikit-learn
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 添加多项式项
- en: 'We will now learn how to use the `PolynomialFeatures` transformer class from
    scikit-learn to add a quadratic term (*d* = 2) to a simple regression problem
    with one explanatory variable. Then, we will compare the polynomial to the linear
    fit by following these steps:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习如何使用 scikit-learn 中的 `PolynomialFeatures` 转换器类将二次项（*d* = 2）添加到一个解释变量的简单回归问题中。然后，我们将通过以下步骤比较多项式和线性拟合：
- en: 'Add a second-degree polynomial term:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加二次多项式项：
- en: '[PRE29]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Fit a simple linear regression model for comparison:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了比较，我们首先进行简单线性回归模型的拟合：
- en: '[PRE30]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Fit a multiple regression model on the transformed features for polynomial
    regression:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对转换后的多项式回归特征进行多元回归模型拟合：
- en: '[PRE31]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Plot the results:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE32]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the resulting plot, you can see that the polynomial fit captures the relationship
    between the response and explanatory variables much better than the linear fit:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的图中，您可以看到多项式拟合比线性拟合更好地捕捉了响应和解释变量之间的关系：
- en: '![Chart, line chart, scatter chart  Description automatically generated](img/B17582_09_12.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图，线图，散点图  描述自动生成](img/B17582_09_12.png)'
- en: 'Figure 9.12: A comparison of a linear and quadratic model'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12：线性和二次模型的比较
- en: 'Next, we will compute the MSE and *R*² evaluation metrics:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算 MSE 和 *R*² 评估指标：
- en: '[PRE33]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see after executing the code, the MSE decreased from 570 (linear
    fit) to 61 (quadratic fit); also, the coefficient of determination reflects a
    closer fit of the quadratic model (*R*² = 0.982) as opposed to the linear fit
    (*R*² = 0.832) in this particular toy problem.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行代码后，您可以看到 MSE 从 570（线性拟合）降至 61（二次拟合）；此外，确定系数反映了二次模型的更紧密拟合（*R*² = 0.982），相对于特定的玩具问题中的线性拟合（*R*²
    = 0.832）。
- en: Modeling nonlinear relationships in the Ames Housing dataset
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Ames Housing 数据集中建模非线性关系
- en: In the preceding subsection, you learned how to construct polynomial features
    to fit nonlinear relationships in a toy problem; let’s now take a look at a more
    concrete example and apply those concepts to the data in the Ames Housing dataset.
    By executing the following code, we will model the relationship between sale prices
    and the living area above ground using second-degree (quadratic) and third-degree
    (cubic) polynomials and compare that to a linear fit.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，您学习了如何构建多项式特征来拟合玩具问题中的非线性关系；现在让我们看一个更具体的例子，并将这些概念应用到 Ames Housing 数据集中的数据上。通过执行以下代码，我们将建模销售价格与地面以上居住面积之间的关系，使用二次（二次）和三次（三次）多项式，并将其与线性拟合进行比较。
- en: 'We start by removing the three outliers with a living area greater than 4,000
    square feet, which we can see in previous figures, such as in *Figure 9.8*, so
    that these outliers don’t skew our regression fits:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先移除三个具有大于 4,000 平方英尺的生活面积的离群值，这些离群值可以在之前的图表中看到，比如 *图 9.8*，以确保这些离群值不会影响我们的回归拟合：
- en: '[PRE34]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we fit the regression models:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们拟合回归模型：
- en: '[PRE35]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting plot is shown in *Figure 9.13*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在*图9.13*中：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_13.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![散点图描述](img/B17582_09_13.png)'
- en: 'Figure 9.13: A comparison of different curves fitted to the sale price and
    living area data'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13：不同曲线拟合与销售价格和生活区域数据比较
- en: 'As we can see, using quadratic or cubic features does not really have an effect.
    That’s because the relationship between the two variables appears to be linear.
    So, let’s take a look at another feature, namely, `Overall Qual`. The `Overall
    Qual` variable rates the overall quality of the material and finish of the houses
    and is given on a scale from 1 to 10, where 10 is best:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用二次或三次特征实际上没有影响。这是因为两个变量之间的关系似乎是线性的。因此，让我们看看另一个特征，即`Overall Qual`。`Overall
    Qual`变量评估房屋材料和装饰的整体质量，评分从1到10，其中10为最佳：
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After specifying the `X` and `y` variables, we can reuse the previous code
    and obtain the plot in *Figure 9.14*:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定了`X`和`y`变量之后，我们可以重复使用之前的代码，并获得*图9.14*中的图表：
- en: '![Chart, line chart  Description automatically generated](img/B17582_09_14.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![线性图描述](img/B17582_09_14.png)'
- en: 'Figure 9.14: A linear, quadratic, and cubic fit on the sale price and house
    quality data'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14：销售价格和房屋质量数据的线性、二次和三次拟合比较
- en: As you can see, the quadratic and cubic fits capture the relationship between
    sale prices and the overall quality of the house better than the linear fit. However,
    you should be aware that adding more and more polynomial features increases the
    complexity of a model and therefore increases the chance of overfitting. Thus,
    in practice, it is always recommended to evaluate the performance of the model
    on a separate test dataset to estimate the generalization performance.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，二次和三次拟合比线性拟合更能捕捉房屋销售价格与整体房屋质量之间的关系。但是，您应该注意，添加越来越多的多项式特征会增加模型的复杂性，从而增加过拟合的风险。因此，在实践中，始终建议在单独的测试数据集上评估模型的性能，以估计泛化性能。
- en: Dealing with nonlinear relationships using random forests
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理非线性关系使用随机森林
- en: In this section, we are going to look at **random forest** regression, which
    is conceptually different from the previous regression models in this chapter.
    A random forest, which is an ensemble of multiple **decision trees**, can be understood
    as the sum of piecewise linear functions, in contrast to the global linear and
    polynomial regression models that we discussed previously. In other words, via
    the decision tree algorithm, we subdivide the input space into smaller regions
    that become more manageable.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究与本章前述回归模型在概念上不同的**随机森林**回归。随机森林是多个**决策树**的集成，可以理解为分段线性函数的总和，与我们之前讨论的全局线性和多项式回归模型形成对比。换句话说，通过决策树算法，我们将输入空间分割成更小的区域，使其更易管理。
- en: Decision tree regression
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树回归
- en: 'An advantage of the decision tree algorithm is that it works with arbitrary
    features and does not require any transformation of the features if we are dealing
    with nonlinear data because decision trees analyze one feature at a time, rather
    than taking weighted combinations into account. (Likewise, normalizing or standardizing
    features is not required for decision trees.) As mentioned in *Chapter 3*, *A
    Tour of Machine Learning Classifiers Using Scikit-Learn*, we grow a decision tree
    by iteratively splitting its nodes until the leaves are pure or a stopping criterion
    is satisfied. When we used decision trees for classification, we defined entropy
    as a measure of impurity to determine which feature split maximizes the **information
    gain** (**IG**), which can be defined as follows for a binary split:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的优点是，它可以处理任意特征，并且不需要对特征进行任何转换，如果我们处理的是非线性数据，因为决策树是一次分析一个特征，而不是考虑加权组合。（同样地，对决策树而言，归一化或标准化特征是不必要的。）如*第3章*中提到的，*使用Scikit-Learn进行机器学习分类器的简介*，我们通过迭代地分割节点来生长决策树，直到叶子节点纯净或满足停止条件。当我们用决策树进行分类时，我们定义熵作为不纯度的度量，以确定哪个特征分割最大化**信息增益**（**IG**），对于二元分割，可以定义如下：
- en: '![](img/B17852_09_035.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_035.png)'
- en: 'Here, *x*[i] is the feature to perform the split, *N*[p] is the number of training
    examples in the parent node, *I* is the impurity function, *D*[p] is the subset
    of training examples at the parent node, and *D*[left] and *D*[right] are the
    subsets of training examples at the left and right child nodes after the split.
    Remember that our goal is to find the feature split that maximizes the information
    gain; in other words, we want to find the feature split that reduces the impurities
    in the child nodes most. In *Chapter 3*, we discussed Gini impurity and entropy
    as measures of impurity, which are both useful criteria for classification. To
    use a decision tree for regression, however, we need an impurity metric that is
    suitable for continuous variables, so we define the impurity measure of a node,
    *t*, as the MSE instead:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*[i]是进行分裂的特征，*N*[p]是父节点中的训练样本数，*I*是不纯度函数，*D*[p]是父节点中训练样本的子集，*D*[left]和*D*[right]是分裂后左右子节点中的训练样本子集。记住我们的目标是找到最大化信息增益的特征分裂；换句话说，我们希望找到能够最大程度降低子节点中不纯度的特征分裂。在第三章中，我们讨论了基尼不纯度和熵作为不纯度的度量标准，它们对于分类非常有用。然而，为了将决策树用于回归，我们需要一个适合连续变量的不纯度度量，因此我们将节点*t*的不纯度度量定义为均方误差（MSE）：
- en: '![](img/B17852_09_036.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_036.png)'
- en: 'Here, *N*[t] is the number of training examples at node *t*, *D*[t] is the
    training subset at node *t*, ![](img/B17852_09_037.png) is the true target value,
    and ![](img/B17852_09_038.png) is the predicted target value (sample mean):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*N*[t]是节点*t*上的训练样本数量，*D*[t]是节点*t*上的训练子集，![](img/B17852_09_037.png)是真实的目标值，而![](img/B17852_09_038.png)是预测的目标值（样本均值）：
- en: '![](img/B17852_09_039.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17852_09_039.png)'
- en: In the context of decision tree regression, the MSE is often referred to as
    **within-node variance**, which is why the splitting criterion is also better
    known as **variance reduction**.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树回归的背景下，均方误差（MSE）通常称为**节点内方差**，这也是为什么分裂准则更为人熟知为**方差减少**。
- en: 'To see what the line fit of a decision tree looks like, let’s use the `DecisionTreeRegressor`
    implemented in scikit-learn to model the relationship between the `SalePrice`
    and `Gr Living Area` variables. Note that `SalePrice` and `Gr Living Area` do
    not necessarily represent a nonlinear relationship, but this feature combination
    still demonstrates the general aspects of a regression tree quite nicely:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解决策树回归的拟合线是什么样子，让我们使用scikit-learn中实现的`DecisionTreeRegressor`来建模`SalePrice`与`Gr
    Living Area`变量之间的关系。请注意，`SalePrice`和`Gr Living Area`不一定代表非线性关系，但这种特征组合仍然很好地展示了回归树的一般特性：
- en: '[PRE37]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As you can see in the resulting plot, the decision tree captures the general
    trend in the data. And we can imagine that a regression tree could also capture
    trends in nonlinear data relatively well. However, a limitation of this model
    is that it does not capture the continuity and differentiability of the desired
    prediction. In addition, we need to be careful about choosing an appropriate value
    for the depth of the tree so as to not overfit or underfit the data; here, a depth
    of three seemed to be a good choice.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的图中可以看出，决策树捕捉了数据的一般趋势。我们可以想象，回归树也可以相对较好地捕捉非线性数据的趋势。然而，这种模型的局限性在于它不能捕捉所需预测的连续性和可微性。此外，我们需要注意选择树深度的适当值，以避免数据过拟合或欠拟合；在这里，深度为三似乎是一个不错的选择。
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_09_15.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B17582_09_15.png)'
- en: 'Figure 9.15: A decision tree regression plot'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15：决策树回归图
- en: You are encouraged to experiment with deeper decision trees. Note that the relationship
    between `Gr Living Area` and `SalePrice` is rather linear, so you are also encouraged
    to apply the decision tree to the `Overall Qual` variable instead.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励你尝试更深的决策树。请注意，`Gr Living Area`与`SalePrice`之间的关系相当线性，因此也鼓励你将决策树应用于`Overall
    Qual`变量。
- en: 'In the next section, we will look at a more robust way of fitting regression
    trees: random forests.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨更稳健的回归树拟合方法：随机森林。
- en: Random forest regression
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: As you learned in *Chapter 3*, the random forest algorithm is an ensemble technique
    that combines multiple decision trees. A random forest usually has a better generalization
    performance than an individual decision tree due to randomness, which helps to
    decrease the model’s variance. Other advantages of random forests are that they
    are less sensitive to outliers in the dataset and don’t require much parameter
    tuning. The only parameter in random forests that we typically need to experiment
    with is the number of trees in the ensemble. The basic random forest algorithm
    for regression is almost identical to the random forest algorithm for classification
    that we discussed in *Chapter 3*. The only difference is that we use the MSE criterion
    to grow the individual decision trees, and the predicted target variable is calculated
    as the average prediction across all decision trees.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在 *第三章* 中学到的，随机森林算法是一种集成技术，结合了多个决策树。随机森林通常比单个决策树具有更好的泛化性能，这归功于随机性，它有助于减少模型的方差。随机森林的其他优点是，在数据集中不太敏感于异常值，并且不需要太多的参数调整。在随机森林中，我们通常需要尝试不同的参数是集成中的树的数量。回归的基本随机森林算法几乎与我们在
    *第三章* 中讨论的分类随机森林算法相同。唯一的区别是我们使用 MSE 准则来生长单个决策树，并且预测的目标变量是跨所有决策树的平均预测。
- en: 'Now, let’s use all the features in the Ames Housing dataset to fit a random
    forest regression model on 70 percent of the examples and evaluate its performance
    on the remaining 30 percent, as we have done previously in the *Evaluating the
    performance of linear regression models* section. The code is as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用艾姆斯房屋数据集中的所有特征，在 70% 的示例上拟合一个随机森林回归模型，并在剩余的 30% 上评估其性能，就像我们在 *评估线性回归模型性能*
    部分中所做的那样。代码如下：
- en: '[PRE38]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Unfortunately, you can see that the random forest tends to overfit the training
    data. However, it’s still able to explain the relationship between the target
    and explanatory variables relatively well (![](img/B17852_09_040.png) on the test
    dataset). For comparison, the linear model from the previous section, *Evaluating
    the performance of linear regression models*, which was fit to the same dataset,
    was overfitting less but performed worse on the test set (![](img/B17852_09_041.png)).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，你可以看到随机森林倾向于过度拟合训练数据。然而，它仍能相对良好地解释目标与解释变量之间的关系（在测试数据集上的 ![](img/B17852_09_040.png)）。作为比较，在前一节
    *评估线性回归模型性能* 中拟合到相同数据集的线性模型过度拟合较少，但在测试集上表现较差（![](img/B17852_09_041.png)）。
- en: 'Lastly, let’s also take a look at the residuals of the prediction:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们也来看看预测的残差：
- en: '[PRE39]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As it was already summarized by the *R*² coefficient, you can see that the model
    fits the training data better than the test data, as indicated by the outliers
    in the *y* axis direction. Also, the distribution of the residuals does not seem
    to be completely random around the zero center point, indicating that the model
    is not able to capture all the exploratory information. However, the residual
    plot indicates a large improvement over the residual plot of the linear model
    that we plotted earlier in this chapter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 *R*² 系数已经总结的那样，你可以看到模型对训练数据的拟合比测试数据更好，这由 *y* 轴方向的异常值表示。此外，残差的分布似乎并不完全围绕零中心点随机，这表明模型无法捕捉所有的探索信息。然而，残差图表明相对于本章早期绘制的线性模型的残差图有了很大的改进。
- en: '![Scatter chart  Description automatically generated](img/B17582_09_16.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![散点图 描述自动生成](img/B17582_09_16.png)'
- en: 'Figure 9.16: The residuals of the random forest regression'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：随机森林回归的残差
- en: Ideally, our model error should be random or unpredictable. In other words,
    the error of the predictions should not be related to any of the information contained
    in the explanatory variables; rather, it should reflect the randomness of the
    real-world distributions or patterns. If we find patterns in the prediction errors,
    for example, by inspecting the residual plot, it means that the residual plots
    contain predictive information. A common reason for this could be that explanatory
    information is leaking into those residuals.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们的模型误差应该是随机或不可预测的。换句话说，预测误差不应与解释变量中包含的任何信息相关；相反，它应反映真实世界分布或模式的随机性。如果我们在预测误差中发现模式，例如通过检查残差图，这意味着残差图包含预测信息。这种常见的原因可能是解释信息渗入到这些残差中。
- en: Unfortunately, there is not a universal approach for dealing with non-randomness
    in residual plots, and it requires experimentation. Depending on the data that
    is available to us, we may be able to improve the model by transforming variables,
    tuning the hyperparameters of the learning algorithm, choosing simpler or more
    complex models, removing outliers, or including additional variables.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，在残差图中处理非随机性并没有通用的方法，需要进行试验。根据我们可用的数据，我们可以通过变量转换、调整学习算法的超参数、选择更简单或更复杂的模型、去除异常值或包含额外变量来改进模型。
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: At the beginning of this chapter, you learned about simple linear regression
    analysis to model the relationship between a single explanatory variable and a
    continuous response variable. We then discussed a useful explanatory data analysis
    technique to look at patterns and anomalies in data, which is an important first
    step in predictive modeling tasks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，您学习了简单线性回归分析，用于建模单个解释变量与连续响应变量之间的关系。然后，我们讨论了一种有用的解释性数据分析技术，用于查看数据中的模式和异常值，这是预测建模任务中的重要第一步。
- en: 'We built our first model by implementing linear regression using a gradient-based
    optimization approach. You then saw how to utilize scikit-learn’s linear models
    for regression and also implement a robust regression technique (RANSAC) as an
    approach for dealing with outliers. To assess the predictive performance of regression
    models, we computed the mean sum of squared errors and the related *R*² metric.
    Furthermore, we also discussed a useful graphical approach for diagnosing the
    problems of regression models: the residual plot.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过实现基于梯度的优化方法来构建了我们的第一个模型，使用了线性回归。然后，您看到了如何利用scikit-learn的线性模型进行回归，并实现了一种用于处理异常值的稳健回归技术（RANSAC）。为了评估回归模型的预测性能，我们计算了平均平方误差和相关的*R*²指标。此外，我们还讨论了诊断回归模型问题的有用图形方法：残差图。
- en: After we explored how regularization can be applied to regression models to
    reduce the model complexity and avoid overfitting, we also covered several approaches
    for modeling nonlinear relationships, including polynomial feature transformation
    and random forest regressors.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探讨了如何应用正则化到回归模型以减少模型复杂性和避免过拟合之后，我们还涵盖了几种建模非线性关系的方法，包括多项式特征转换和随机森林回归器。
- en: We discussed supervised learning, classification, and regression analysis in
    detail in the previous chapters. In the next chapter, we are going to learn about
    another interesting subfield of machine learning, unsupervised learning, and also
    how to use cluster analysis to find hidden structures in data in the absence of
    target variables.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们详细讨论了监督学习、分类和回归分析。在下一章中，我们将学习另一个有趣的机器学习子领域，无监督学习，以及如何使用聚类分析在缺乏目标变量的情况下发现数据中的隐藏结构。
- en: Join our book’s Discord space
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作空间，每月进行一次与作者的*问我任何事*会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
