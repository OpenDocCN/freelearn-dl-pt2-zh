- en: Anomaly Detection
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测
- en: diarizationThe predictive/prescriptive AI life cycle of a device starts with
    data collection design. Data is analyzed for factors such as correlation and variance.
    Then the devices start being manufactured. Other than a small number of sample
    devices, there is usually no device failures, that produce machine learning models.
    To compensate for this, most manufacturers use duty cycle thresholds to determine
    whether a device is in a good state or a bad state. These duty cycle standards
    may be that that the device is running too hot or an arbitrary value is put on
    a sensor for an alert. But the data quickly needs more advanced analysis. The
    sheer volume of data can be daunting for an individual. The analyst needs to look
    through millions of records to find the proverbial needle in a haystack. Using
    an analyst-in-the-middle approach using anomaly detection can efficiently help
    to find issues with devices. Anomaly detection is done through statistical, unsupervised,
    or supervised machine learning techniques. In other words, typically an analyst
    starts out looking at a single data point that is being examined for things such
    as spikes and dips. Then, multiple data points are pulled into an unsupervised
    learning model that clusters the data, allowing the data scientist to see when
    a set of values or patterns is not like the other sets of values. Finally, after
    enough device failures happen, the analyst can use the same type of machine learning
    we would use for predictive maintenance. Some machine learning algorithms, such
    as isolated forest, are better suited for anomaly detection, but the principles
    are the same.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 设备的预测/处方AI生命周期始于数据收集设计。数据分析包括相关性和变异等因素。然后开始制造设备。除了少量样品设备外，通常没有导致机器学习模型的设备故障。为了弥补这一点，大多数制造商使用工作周期阈值来确定设备处于良好状态还是坏状态。这些工作周期标准可能是设备运行过热或传感器上设置了警报的任意值。但是数据很快需要更高级的分析。庞大的数据量可能对个人来说是令人生畏的。分析师需要查看数百万条记录，以找到大海捞针的情况。使用分析师中介方法结合异常检测可以有效帮助发现设备问题。异常检测通过统计、非监督或监督的机器学习技术来进行。换句话说，通常分析师首先查看正在检查的单个数据点，以检查是否存在尖峰和波谷等情况。然后，多个数据点被引入无监督学习模型，该模型对数据进行聚类，使数据科学家能够看到某组数值或模式与其他组的数值不同。最后，在发生足够的设备故障之后，分析师可以使用预测性维护所使用的相同类型的机器学习。一些机器学习算法，如孤立森林，更适合用于异常检测，但其原则是相同的。
- en: Anomaly detection can be carried out before enough data is collected for supervised
    learning, or it can be part of a continuous monitoring solution. Anomaly detection
    can, for example, alert you to production issues with different factories. When
    an electrical engineer hands over a physical design to a factory, they perform
    a **bill of material** (**BOM**) optimization. In short, they alter the design
    to be one that can be put together more easily, or one that is more cost effective.
    Most physical devices are produced for a decade. In that time, parts that existed
    when the devices were first made may not be available, which means changes are
    needed to the BOM. Moving to a new manufacturer will also change the design as
    they produce their own BOM optimization. Anomaly detection can help pinpoint new
    issues popping up within your fleet.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测可以在收集足够的数据进行监督学习之前进行，也可以作为连续监控解决方案的一部分。例如，异常检测可以警示您关于不同工厂的生产问题。当电气工程师将物理设计交给工厂时，他们进行物料清单（**BOM**）优化。简而言之，他们修改设计，使其更易组装或更具成本效益。大多数物理设备的生产周期为十年。在此期间，可能不再有设备初次制造时存在的零部件，这意味着需要对BOM进行更改。转向新的制造商也将改变设计，因为他们会进行自己的BOM优化。异常检测可以帮助精确定位在您的设备群中出现的新问题。
- en: There are many ways of looking at anomaly detection. In [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*, in *Analyzing chemical sensors with anomaly detection*
    recipe, we used K-means, a popular anomaly detection algorithm, to determine whether
    the chemical signature of a food item was different from the air. That is just
    one type of anomaly detection. There are many different types of anomaly detection.
    Some are specific to a particular machine and look at something that is abnormal
    over a period of time. Other anomaly detection algorithms look at a device acting
    normally and abnormally using supervised learning. Some devices are affected by
    their local environments or seasonality. Finally, in this chapter, we are going
    to talk about deploying anomaly detection to the edge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以观察异常检测。在[第3章](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml)，*物联网的机器学习*，在*用异常检测分析化学传感器*的配方中，我们使用了K均值，一种流行的异常检测算法，来确定食物的化学特征是否与空气不同。这只是一种异常检测的类型。有许多不同类型的异常检测。有些是针对特定机器的，观察一段时间内是否异常。其他异常检测算法则通过监督学习来观察设备的正常和异常行为。有些设备受其本地环境或季节性影响。最后，在本章中，我们将讨论将异常检测部署到边缘设备上。
- en: 'The following recipes are included in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下配方：
- en: Using Z-Spikes on a Raspberry Pi and Sense HAT
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Raspberry Pi和Sense HAT上使用Z-Spikes
- en: Using autoencoders to detect anomalies in labeled data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器检测标记数据中的异常
- en: Using isolated forest for unlabeled datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对无标签数据使用孤立森林
- en: Detecting time series anomalies with Luminol
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Luminol检测时间序列异常
- en: Detecting seasonality-adjusted anomalies
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测季节调整的异常
- en: Detecting spikes with streaming analytics
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流式分析检测尖峰
- en: Detecting anomalies on the edge
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在边缘检测异常
- en: Using Z-Spikes on a Raspberry Pi and Sense HAT
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Raspberry Pi和Sense HAT上使用Z-Spikes
- en: Spikes or sudden changes to an individual device can warrant an alert. IoT devices
    are often subject to movement and weather. They can be affected by times of day
    or seasons of the year. The fleet of devices could be spread out throughout the
    world. Trying to get clear insights across the entire fleet can be challenging.
    Using a machine learning algorithm that incorporates the entire fleet enables
    us to treat each device separately.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对个别设备的突然变化可能需要警报。物联网设备经常受到移动和天气影响。它们可能受到一天中的时间或一年中的季节的影响。设备群可能分布在全球各地。试图在整个设备群中获得清晰的洞察力可能是具有挑战性的。使用整个设备群的机器学习算法使我们能够单独处理每个设备。
- en: Use cases for Z-Spikes can be a sudden discharge of batteries or a sudden temperature
    increase. People use Z-Spikes to tell whether something has been jostled or is
    suddenly vibrating. Z-Spikes can be used on pumps to see whether there is a blockage.
    Because Z-Spikes do so well across non-homologous environments, they are often
    a great candidate for edge deployments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Z-Spikes的用例可以是电池的突然放电或温度的突然增加。人们使用Z-Spikes来判断是否被震动或突然振动了。Z-Spikes可以用于泵，以查看是否存在堵塞。因为Z-Spikes在非同质环境中表现出色，它们经常成为边缘部署的一个很好的选择。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we are going to deploy Z-Spikes on a Raspberry Pi with a Sense
    HAT. The hardware itself is a fairly common development board and sensor setup
    for people learning about IoT. In fact, students can send their code to the International
    Space Station to be run on their Raspberry Pi and Sense HAT. If you do not have
    the equipment, there is an alternative code in the GitHub repository that simulates
    the device.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将在一个带有Sense HAT的Raspberry Pi上部署Z-Spikes。硬件本身是一个相当常见的开发板和传感器设置，适合学习物联网的人使用。事实上，学生们可以将他们的代码发送到国际空间站上，以在他们的Raspberry
    Pi和Sense HAT上运行。如果你没有这个设备，GitHub仓库中有一个模拟该设备的替代代码。
- en: 'Once you have powered on your Raspberry Pi and attached your Sense HAT, you
    will need to install SciPy. In Python, you can usually install everything you
    need with `pip`, but in this case, you will need to install it through the Linux
    operating system. To do this, run the following commands in a terminal window:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您启动了Raspberry Pi并连接了Sense HAT，您需要安装SciPy。在Python中，通常可以使用`pip`安装所需的一切，但在这种情况下，您需要通过Linux操作系统来安装它。要做到这一点，请在终端窗口中运行以下命令：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You will then need to `pip` install `numpy`, `kafka`, and `sense_hat`. You will
    also need to set up Kafka on a PC. There are instructions in [Chapter 1](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml),
    *Setting up the IoT and AI Environment*, in the *Setting up Kafka* recipe. Do
    not try to set up Kafka on the Raspberry Pi as it requires too much memory. Instead,
    set it up on a PC.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要`pip`安装`numpy`，`kafka`和`sense_hat`。您还需要在PC上设置Kafka。在*设置IoT和AI环境*的*设置Kafka*配方中有指南，请勿尝试在树莓派上设置Kafka，因为其需要太多内存。请在PC上设置。
- en: For the Raspberry Pi, you will need to connect a monitor, keyboard, and mouse.
    There is a Python editor in the developer tools menu. You will also need to know
    the IP address of the Kafka service.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于树莓派，您需要连接显示器、键盘和鼠标。开发者工具菜单中有Python编辑器。您还需要知道Kafka服务的IP地址。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的步骤如下：
- en: 'Import the libraries:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Wait for Sense HAT to register with the OS:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待Sense HAT与操作系统注册：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize the variables:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变量：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a Z-score helper function:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Z-score辅助函数：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `sendAlert` helper function:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`sendAlert`辅助函数：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a `combined_value` helper function:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`combined_value`辅助函数：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Run the `main` function:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`main`函数：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This algorithm is checking whether the last record is more than 4 standard deviations
    (*σ*) from the preceding 1,000 values. *4σ* should have an anomaly 1 in every
    15,787 readings or once every 4 hours. If we were to change that to 4.5 it would
    be once every 40 hours.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法正在检查最后一条记录是否比前1,000个值的4个标准偏差（*σ*）更多。*4σ* 应该在每15,787次读数或每4小时中有一个异常。如果我们将其更改为4.5，则每40小时有一次异常。
- en: We import `scipy` for our Z-score evaluation and `numpy` for data manipulation.
    We then add the script to the Raspberry Pi startup so that the script will start
    automatically whenever there is a power reset. The machine needs to wait for peripherals,
    such as the Sense HAT initialization. The 60-second delay allows the OS to be
    aware of the Sense HAT before trying to initialize it. Then we initialize our
    variables. These variables are the device name, the IP address of the Kafka server,
    and the Sense HAT. Then we enable the Sense HAT's **internal measuring units**
    (**IMUs**). We disable the compass and enable the gyroscope and accelerometer.
    Finally, we create two arrays to put the data in. Next, we create a Z-score helper
    function into which we can input an array of values to return the Z-scores. Next,
    we need a function that we can use to send the alerts. The `sense.gyro_raw` function
    gets the most recent gyroscope and accelerometer reading and puts them into a
    Python object. It then converts it to JSON. We then create a key value that is
    UTF-8 byte encoded. Similarly, we encode the message payload. Next, we create
    a Kafka topic name. Then, we send the key and message to the topic. We then check
    under `__main__` to see whether we are running the current file from a command
    shell. If we are, then we set a counter called `x` to `0`. Then we start an infinite
    loop. Then we start putting in the gyroscope and accelerometer data. We then check
    whether the array has 1,000 elements in it. If so, we remove the last value in
    the array so that we keep the array small. We then increment our counter to accumulate
    2 minutes of data. Finally, we check whether we are over 4 standard deviations
    away from the 1,000 values from our array; if so, we send our alert.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入`scipy`进行Z-score评估和`numpy`进行数据操作。然后我们将脚本添加到树莓派的启动中，这样每次有电源重置时脚本都会自动启动。设备需要等待外围设备初始化，如Sense
    HAT。60秒的延迟允许操作系统在尝试初始化Sense HAT之前感知Sense HAT。然后我们初始化变量。这些变量是设备名称、Kafka服务器的IP地址和Sense
    HAT。然后我们启用Sense HAT的**内部测量单元**（**IMUs**）。我们禁用罗盘并启用陀螺仪和加速度计。最后，我们创建两个数组来存放数据。接下来，我们创建一个Z-score辅助函数，可以输入一个值数组并返回Z-scores。接下来，我们需要一个函数来发送警报。`sense.gyro_raw`函数获取最新的陀螺仪和加速度计读数并将它们放入Python对象中，然后转换为JSON格式。然后我们创建一个UTF-8字节编码的键值。类似地，我们编码消息负载。接下来，我们创建一个Kafka主题名称。然后，我们将键和消息发送到主题。然后，我们在`__main__`下检查是否从命令行运行当前文件。如果是，我们将计数器`x`设置为`0`。然后我们开始一个无限循环。然后我们开始输入陀螺仪和加速度计数据。然后我们检查数组中是否有1,000个元素。如果是这样，我们会移除数组中的最后一个值，以保持数组的小型化。然后我们增加计数器以累积2分钟的数据。最后，我们检查是否超过了来自我们数组的1,000个值的4个标准偏差；如果是，我们发送警报。
- en: While this is a great way of looking at a device, we may want to do anomaly
    detection across our entire fleet. In the next recipes, we are going to create
    a message sender and receiver. If we were to do this in this recipe, we would
    make a Kafka producer message to send data on every iteration of our loop.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是查看设备的一个很好的方式，但我们可能希望在整个设备群中进行异常检测。在接下来的配方中，我们将创建一个消息发送和接收器。如果我们要在这个配方中执行此操作，我们将创建一个Kafka生产者消息，以在循环的每次迭代中发送数据。
- en: Using autoencoders to detect anomalies in labeled data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器来检测标记数据中的异常
- en: If you have labeled data, you can train a model to detect whether the data is
    normal or abnormal. For example, reading the current of an electric motor can
    show when extra drag is put on the motor by such things as failing ball bearings
    or other failing hardware. In IoT, anomalies can be a previously known phenomenon
    or a new event that has not been seen before. As the name suggests, autoencoders
    take in data and encode it to an output. With anomaly detection, we see whether
    a model can determine whether data is non-anomalous. In this recipe, we are going
    to use a Python object detection library called `pyod`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有标记数据，您可以训练一个模型来检测数据是否正常或异常。例如，读取电动机的电流可以显示电动机由于如轴承失效或其他硬件失效而受到额外负载的情况。在物联网中，异常可能是先前已知的现象或以前未见过的新事件。顾名思义，自编码器接收数据并将其编码为输出。通过异常检测，我们可以看到模型能否确定数据是否非异常。在本配方中，我们将使用一个名为`pyod`的Python对象检测库。
- en: Getting ready
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we are going to use data gathered from the motion sensors on
    our Sense HAT. The final recipe in this chapter shows how to generate this dataset.
    We have also put this labeled dataset in the GitHub repository for this book.
    We are going to use a Python outlier detection framework called `pyod` or **Python
    Outlier Detection**. It wraps TensorFlow and performs various machine learning
    algorithms, such as autoencoders and isolated forests.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将使用从Sense HAT运动传感器收集的数据。本章最后的配方展示了如何生成此数据集。我们还将这个带标签的数据集放在了本书的GitHub存储库中。我们将使用一个名为`pyod`或**Python异常检测**的Python异常检测框架。它包装了TensorFlow并执行各种机器学习算法，如自编码器和孤立森林。
- en: How to do it...
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps for this recipe are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的步骤如下：
- en: 'Import the libraries:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load text files into our notebooks using NumPy arrays:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NumPy数组将文本文件加载到我们的笔记本中：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Use the autoencoder algorithm to fix the model to the dataset:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自编码器算法来修复模型到数据集：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Get the prediction scores:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取预测分数：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Save the model:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型：
- en: '[PRE12]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: First, we import `pyod`, our Python object detection library. Then we import
    `numpy` for data manipulation and `pickle` for saving our model. Next, we use
    `numpy` to load our data. Then we train our model and get the prediction scores.
    Finally, we save our model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入`pyod`，我们的Python对象检测库。然后我们导入`numpy`进行数据操作和`pickle`用于保存我们的模型。接下来，我们使用`numpy`加载我们的数据。然后我们训练我们的模型并获得预测分数。最后，我们保存我们的模型。
- en: 'An autoencoder takes data as input and reduces the number of nodes through
    a smaller hidden layer that forces it to reduce the dimensionality. The target
    output for an autoencoder is the input. This allows us to use machine learning
    to train a model on what is non-anomalous. We can then determine how far a value
    falls away from the trained model. These values would be anomalous. The following
    diagram shows conceptually how data is coded into a set of inputs. Then, its dimensionality
    is reduced in the hidden layer and, finally, is outputted into a larger set of
    outputs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器将数据作为输入并通过较小的隐藏层减少节点数量，迫使其降低维度。自编码器的目标输出是输入。这允许我们使用机器学习来训练模型，以识别非异常情况。然后，我们可以确定一个值与训练模型相比的差距有多大。这些值将是异常的。以下图表概念性地展示了数据如何编码为一组输入。然后，在隐藏层中降低其维度，最后输出到一组较大的输出中：
- en: '![](img/dd3fbd34-c76d-40de-8003-e797a77e5cf7.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd3fbd34-c76d-40de-8003-e797a77e5cf7.png)'
- en: There's more...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'After training our model, we need to know at what level to send the alert.
    When training, setting the contamination (see the following code) determines the
    proportion of outliers in the data that are needed to trigger the alerting function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完我们的模型之后，我们需要知道在什么级别发送警报。在训练时，设置污染度（参见以下代码）确定触发警报功能所需的数据中异常值的比例：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We could also change the regularizer, as in the following example. The regularizer is
    used to balance the bias and variance to prevent over and underfitting:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更改正则化器，如下例所示。正则化器用于平衡偏差和方差，以防止过度拟合和欠拟合：
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We could also change the number of neurons, our loss function, or the optimizer.
    This is often referred to as changing or tuning the hyperparameters in data science.
    Tuning the hyperparameters allows us to affect our success metrics, thereby improving
    the model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以更改神经元的数量、损失函数或优化器。这通常被称为在数据科学中改变或调整超参数。调整超参数允许我们影响成功的指标，从而改善模型。
- en: Using isolated forest for unlabeled datasets
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用孤立森林处理无标签数据集
- en: Isolated forest is a popular machine learning algorithm for anomaly detection.
    Isolated forests can assist in complex data models that have overlapping values.
    An isolated forest is an ensemble regression. Rather than using a clustering or
    distance-based algorithm like other machine learning algorithms, it separates
    outlying data points from normal data points. It does this by building a decision
    tree and calculates a score based on node count traversal in its path decision
    tree of where the data lies. In other words, it counts the number of nodes it
    traverses to determine an outcome. The more data that has been trained on a model,
    the more nodes an isolated forest would need to traverse.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 孤立森林是一种流行的异常检测机器学习算法。孤立森林可以帮助处理有重叠值的复杂数据模型。孤立森林是一种集成回归。与其他机器学习算法使用聚类或基于距离的算法不同，它将离群数据点与正常数据点分开。它通过构建决策树并计算基于节点遍历的分数来实现这一点。换句话说，它计算它遍历的节点数来确定结果。模型训练的数据越多，孤立森林需要遍历的节点数就越多。
- en: Similar to the previous recipe, we are going to use `pyod` to easily train a
    model. We are going to use the Sense HAT dataset that is in the GitHub repository.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一篇介绍类似，我们将使用 `pyod` 轻松训练一个模型。我们将使用 GitHub 仓库中的 Sense HAT 数据集。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: If you have completed the previous recipe on autoencoders, then you have everything
    you need. In this recipe, we are using `pyod` for our object detection library.
    The training dataset and the test dataset are in the GitHub repository for this
    book.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经完成了关于自动编码器的前一篇介绍，那么您所需的一切都已准备就绪。在这个示例中，我们使用 `pyod` 进行目标检测库。本书的 GitHub 仓库中包含训练数据集和测试数据集。
- en: How to do it...
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行……
- en: 'The steps for this recipe are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本篇的步骤如下：
- en: 'Import the libraries:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Upload the data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传数据：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Train the model:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Evaluate against the test data:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试数据进行评估：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Save the model:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理……
- en: 'First, we import `pyod`. We then import `numpy` for data processing and `pickle`
    for saving our model. Next, we perform the isolated forest training. Then we evaluate
    our results. We get two different types of results: one is a `1` or `0` to determine
    whether it is normal or anomalous, and the second gives us a score of the test.
    Finally, we save our model.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入 `pyod`。然后导入 `numpy` 进行数据处理，以及 `pickle` 用于保存我们的模型。接下来，进行孤立森林训练。然后我们评估我们的结果。我们得到两种不同类型的结果：一种是用
    `1` 或 `0` 表示是否正常或异常，另一种给出测试的分数。最后，保存我们的模型。
- en: The isolated forest algorithm segments the data using a tree-based approach.
    The more clustered the data is, the more segmented it is. The isolated forest
    algorithm looks for the data that is not part of the dense segmented area by counting
    the amounts of segments it would need to traverse to get there.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 孤立森林算法使用基于树的方法对数据进行分割。数据越密集，分割得越多。孤立森林算法通过计算需要遍历的分割数量来查找不属于密集分割区域的数据。
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'Anomaly detection is one of those analysis techniques where visualization can
    help us determine which hyperparameters and algorithms to use. scikit-learn has
    an example of how to do this on their website ([https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html)).
    A reference to this is in the GitHub repository of this book. The diagram that
    follows is an example of anomaly detection using multiple algorithms and settings
    on a toy dataset. There is no right answer in anomaly detection, but only what
    works best for the problem at hand:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是一种分析技术，可通过可视化帮助我们确定要使用的超参数和算法。scikit-learn 在其网站上有一个示例，展示了如何做到这一点（[https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html)）。这本书的
    GitHub 存储库中有参考资料。接下来的图表是使用多种算法和设置进行异常检测的示例。在异常检测中不存在绝对正确的答案，只有适合手头问题的最佳解决方案：
- en: '![](img/9ff5953f-c353-4f1a-8624-56c659225c79.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ff5953f-c353-4f1a-8624-56c659225c79.png)'
- en: Detecting time series anomalies with Luminol
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Luminol 检测时间序列异常
- en: Luminol is a time series anomaly detection algorithm released by LinkedIn. It
    uses a bitmap to check how many detection strategies, that are robust in datasets,
    tend to drift. It is also very lightweight and can handle large amounts of data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Luminol 是 LinkedIn 发布的时间序列异常检测算法。它使用位图来检查在数据集中具有稳健性的检测策略，往往会漂移。它还非常轻量级，可以处理大量数据。
- en: In this example, we are going to use a publicly accessible IoT dataset from
    the city of Chicago. The city of Chicago has IoT sensors measuring the water quality
    of their lakes. Because the dataset needs some massaging before we get it into
    the right format for anomaly detection, we will use the `prepdata.py` file to
    extract one data point from one lake.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用芝加哥市的公共可访问的物联网数据集。芝加哥市有物联网传感器测量其湖泊的水质。因为数据集在进行异常检测之前需要进行一些整理，我们将使用`prepdata.py`文件从一个湖泊中提取一个数据点。
- en: Getting ready
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To get ready for this recipe, you will need to download the CSV file from the
    GitHub repository for this book. Next, you will need to install `luminol`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备这个食谱，您需要从这本书的 GitHub 存储库下载CSV文件。接下来，您需要安装`luminol`：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How to do it...
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps involved in this recipe are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱涉及的步骤如下：
- en: 'Prep the data with `prepdata.py`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用 `prepdata.py` 准备数据:'
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Import libraries in `Luminol.py`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Luminol.py`中导入库：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Perform anomaly detection:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行异常检测：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Print the anomalies:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印异常：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In the `dataprep` Python library, you will only import `pandas` so that we can
    take the CSV file and turn it into a `pandas` DataFrame. Once we have a `pandas`
    DataFrame we will filter out on `Rainbow Beach` (in our case, we are only looking
    at `Rainbow Beach`). Then we will take out anomalous data such as data where the
    water temperature is below -100 degrees. Then we will convert the `time` string
    into a string that pandas can read. We do this so that when it outputs, it outputs
    to a standard time series format. Then we select only the two columns we need
    to analyze, `Measurement Timestamp` and `Turbidity`. Finally, we save the file
    in CSV format.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在`dataprep` Python库中，您只需导入`pandas`，这样我们就可以将CSV文件转换为`pandas` DataFrame。一旦我们有了`pandas`
    DataFrame，我们将会在`Rainbow Beach`上进行过滤（在我们的情况下，我们只关注`Rainbow Beach`）。然后，我们将剔除水温低于-100度的异常数据。接着，我们将把`time`字符串转换成`pandas`可以读取的格式。我们这样做是为了输出时采用标准的时间序列格式。然后，我们只选择需要分析的两列，`Measurement
    Timestamp`和`Turbidity`。最后，我们将文件以CSV格式保存。
- en: Next, we create a Luminol file. From here, we use `pip` to install `luminol`
    and `time`. We then use the anomaly detector on the CSV file and return all of
    the scores. Finally, we return scores if the value of our score item is greater
    than 0\. In other words, we only return scores if there is an anomaly.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个Luminol文件。从这里开始，我们使用`pip`安装`luminol`和`time`。然后，我们在CSV文件上使用异常检测器并返回所有分数。最后，如果我们的分数项的值大于0，则返回分数。换句话说，只有在存在异常时才返回分数。
- en: There's more...
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In addition to anomaly detection, Luminol can also form correlation analysis.
    This can help the analyst determine whether two time series datasets are correlated
    to each other. So, for example, our dataset from the city of Chicago measured
    various aspects of water purity in their lakes. We could compare lakes against
    each other to see whether there was a common effect in two different lakes at
    the same time.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 除了异常检测外，Luminol还可以进行相关性分析。这有助于分析师确定两个时间序列数据集是否彼此相关。例如，芝加哥市的数据集测量了其湖泊中水质的各个方面。我们可以比较不同湖泊之间是否存在同时发生的共同影响。
- en: Detecting seasonality-adjusted anomalies
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测季节性调整后的异常
- en: Data from a temperature sensor might trend upward throughout the day if the
    device is outdoors. Similarly, the internal temperature of an exterior device
    may be lower in the winter. Not all devices are affected by seasonality but for
    the ones that are, choosing an algorithm that handles seasonality and trends is
    important. According to a research paper (*Automatic Anomaly Detection in the
    Cloud Via Statistical Learning*) from data scientists at Twitter, **Seasonal ESD**
    is a machine learning algorithm that takes seasonality and trends to find anomalies
    regardless of the seasonality.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果设备在户外，温度传感器的数据可能会在一天中趋于上升。同样地，户外设备的内部温度在冬季可能会较低。并非所有设备都受季节性影响，但对于受影响的设备，选择处理季节性和趋势的算法至关重要。根据Twitter的数据科学家在研究论文《云中的自动异常检测通过统计学习》中指出，**季节性ESD**是一种机器学习算法，通过考虑季节性和趋势来发现异常。
- en: For this recipe, we are going to use the city of Chicago lake water purity dataset.
    We are going to pull in the data file we prepared in the *Detecting time series
    anomalies with Luminol* recipe.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本示例，我们将使用芝加哥市湖泊水质数据集。我们将导入我们在*使用Luminol检测时间序列异常*示例中准备的数据文件。
- en: Getting ready
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To get ready, you will need the Seasonal ESD library. This can be installed
    simply with the following `pip` command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备好，您将需要Seasonal ESD库。您可以通过以下`pip`命令简单安装：
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The dataset can be found in the GitHub repository of this book.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的GitHub仓库中可以找到数据集。
- en: How to do it...
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'The steps to execute this recipe are as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这个示例的步骤如下：
- en: 'Import the libraries:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Import and manipulate the data:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入和操作数据：
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Perform anomaly detection:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行异常检测：
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Output the results:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出结果：
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we first imported `numpy` and `pandas` for data manipulation.
    We then imported `sesd`, our anomaly detection package. Next, we got the raw data
    ready for machine learning. We did this by removing the data that clearly had
    an issue, such as sensors that were not working properly. We then filtered the
    data into one column. We then put that column through the seasonal ESD algorithm.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先导入了`numpy`和`pandas`用于数据操作。接着，我们导入了我们的异常检测包`sesd`。然后，我们准备好了机器学习的原始数据。我们通过移除明显存在问题的数据（比如传感器工作不正常的数据）来完成这一步骤。接下来，我们将数据过滤到一个列中。然后，我们将该列数据输入季节性ESD算法中。
- en: 'Similar to the Z-score algorithm in the first recipe, this recipe uses an online
    approach. It uses **Seasonal and Trend decomposition using Loess** (**STL**) decomposition
    as a preprocessing step before doing anomaly detection. A data source may have
    a trend and a season, as shown in the following graph:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与第一个示例中的Z-score算法类似，本示例使用在线方法。在进行异常检测之前，它使用**局部加权回归分解的季节性和趋势分解**（STL）作为预处理步骤。数据源可能具有趋势和季节性，如下图所示：
- en: '![](img/0889f98d-799d-472f-88af-134fa7386937.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0889f98d-799d-472f-88af-134fa7386937.png)'
- en: 'What decomposition allows you to do is look at the trend and the seasonality
    independently (as shown in the following trend graph). This helps to ensure the
    data is not affected by seasonality:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 分解的目的是让你能够独立查看趋势和季节性（如下图所示的趋势图）。这有助于确保数据不受季节性影响：
- en: '![](img/a1453da0-afc3-4f11-9675-c25ffa0376ee.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1453da0-afc3-4f11-9675-c25ffa0376ee.png)'
- en: The Seasonal ESD algorithm is more complicated than the Z-score algorithm. For
    example, Z-score algorithms would show false positives in devices that were stationed
    outdoors.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 季节性ESD算法比Z-score算法更复杂。例如，Z-score算法可能会在户外设备中显示错误的阳性结果。
- en: Detecting spikes with streaming analytics
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流式分析检测尖峰
- en: Stream Analytics is a tool that connects IoT Hub to other resources within Azure
    using a SQL interface. Stream Analytics moves data from IoT Hub to Cosmos DB,
    storage blobs, serverless functions, or a number of other scalable options. Streaming
    analytics has a few functions built-in, and you can create more functions yourself
    using JavaScript; anomaly detection is one of those functions. In this example,
    we are going to use Raspberry Pi to stream gyroscope and acceleration data to
    IoT Hub. Then we'll connect streaming analytics and, using its SQL interface,
    we will output only the anomalous results.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 流分析是一个工具，它使用SQL接口将IoT Hub连接到Azure内部的其他资源。流分析将数据从IoT Hub移动到Cosmos DB、存储块、无服务器函数或多个其他可扩展的选项。流分析内置了一些函数，您还可以使用JavaScript创建更多函数；异常检测就是其中之一。在本例中，我们将使用树莓派将陀螺仪和加速度数据流式传输到IoT
    Hub。然后，我们将连接流分析，并使用其SQL接口仅输出异常结果。
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this experiment, you will need IoT Hub. Next, you'll need to create a streaming
    analytics job. To do this, you will go into the Azure portal and create a new
    streaming analytics job through the **Create new resource** wizard. After you
    create a new streaming analytics job, you will see that there are three main components
    on the **Overview** page. These are inputs, outputs, and queries. Inputs, as the
    name suggests, are the streams you want to input; in our case, we are inputting
    IoT Hub. To connect to IoT Hub you need to click on **Inputs**, then select the
    input type of IoT Hub, and then select the IoT Hub instance you created for this
    recipe. Next, you can create an output. This could be a database such as Cosmos
    DB or a function app so that you can send alerts through any number of messaging
    systems. For the sake of simplicity, we are not going to specify output for this
    recipe. For testing purposes, you can review the output on the Stream Analytics
    query editor.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，您将需要IoT Hub。接下来，您需要创建一个流分析作业。为此，您将进入Azure门户，并通过“创建新资源”向导创建一个新的流分析作业。创建新的流分析作业后，您将看到“概述”页面上有三个主要组件。这些是输入、输出和查询。输入如其名称所示，是您想要输入的流；在我们的情况下，我们正在输入IoT
    Hub。要连接到IoT Hub，您需要点击“输入”，然后选择IoT Hub的输入类型，然后选择您为此配方创建的IoT Hub实例。接下来，您可以创建一个输出。这可以是诸如Cosmos
    DB之类的数据库，或者是函数应用程序，以便通过任何数量的消息系统发送警报。为了简单起见，本配方不会指定输出。为测试目的，您可以在流分析查询编辑器中查看输出。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'The steps for this recipe are as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的步骤如下：
- en: 'Import the libraries:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Declare the variables:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明变量：
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Get a joined device value:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取连接的设备值：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Get and send the data:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取并发送数据：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a SQL query that uses the `AnomalyDetection_SpikeAndDip` algorithm to
    detect anomalies:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建使用`AnomalyDetection_SpikeAndDip`算法检测异常的SQL查询：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: To import the libraries on the Raspberry Pi you will need to log in to the Raspberry
    Pi and use `pip` to install `azure-iot-device` and `SenseHat`. Next, you'll need
    to go onto that machine and create a file called `device.py`. Then you will import
    the `time`, Azure IoT Hub, Sense HAT, and `json` libraries. Next, you'll need
    to go into IoT Hub and create a device through the portal, get your connection
    string, and enter it in the spot where it says Your device key here. You then
    initialize `SenseHat` and set the internal measuring units to `True`, initializing
    our sensors. Then create a helper function that combines our `x`, `y`, and `z`
    data. Next, get the data from sensors and send that to IoT Hub. Finally, wait
    for a second before sending that data again.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要在树莓派上导入库，您需要登录树莓派并使用`pip`安装`azure-iot-device`和`SenseHat`。接下来，您需要进入该设备并创建一个名为`device.py`的文件。然后，您将导入`time`、Azure
    IoT Hub、Sense HAT和`json`库。接下来，您需要进入IoT Hub，并通过门户创建设备，获取您的连接字符串，并将其输入到标有“在此处输入您的设备密钥”的位置。然后，初始化`SenseHat`并将内部测量单位设置为`True`，初始化我们的传感器。然后创建一个帮助函数，将我们的`x`、`y`和`z`数据结合起来。接下来，从传感器获取数据并将其发送到IoT
    Hub。最后，在再次发送数据之前等待一秒钟。
- en: Next, go into the Stream Analytics job that you had set up and click on Edit
    query. From here, create a common table expression. A common table expression
    allows you to make a complex query more simple. Then use the built-in anomaly
    detection spikes and dips algorithm over a 120-second window. The quick editor
    allows you to test live data in the stream and view that the anomaly detectors
    gave the resulting score of anomalous or non-anomalous.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，进入您已设置的流分析作业，并单击“编辑查询”。从这里，创建一个公共表达式。公共表达式允许您简化复杂的查询。然后使用内置的异常检测尖峰和低谷算法，在120秒的窗口内执行。快速编辑器允许您测试实时数据流，并查看异常检测器给出的异常或非异常结果分数。
- en: Detecting anomalies on the edge
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在边缘上检测异常：
- en: In this final recipe, we are going to use `SenseHat` on the Raspberry Pi to
    collect data, train that data on our local computer, then deploy a machine learning
    model on the device. To avoid redundancy after recording your data you will need
    to run either of the recipes on autoencoders or isolated forest from earlier in
    this chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最终的教程中，我们将使用树莓派上的`SenseHat`来收集数据，在我们的本地计算机上训练这些数据，然后在设备上部署机器学习模型。为了避免冗余，在记录数据之后，您需要在本章前面的自编码器或孤立森林中运行任何一个配方。
- en: People use motion sensors in IoT to ensure shipping containers are safely transported aboard
    ships. For example, proving that a shipping container was dropped in a particular
    harbor would help with insurance claims. They are also used for worker safety
    to detect falls or workers acting unsafely. They are also used on devices that
    are prone to vibration when malfunctioning. Some examples of this are washing
    machines, wind turbines, and cement mixers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 人们在物联网中使用运动传感器来确保集装箱安全地运输到船上。例如，证明一个集装箱在特定港口被丢弃会有助于保险理赔。它们还用于保障工人的安全，以便检测跌倒或工人不安全行为。它们还用于设备在发生故障时产生振动的情况。例如，洗衣机、风力发电机和水泥搅拌机等设备。
- en: During the data collection phase, you will need to safely simulate falling or
    working unsafely. You could also put a sensor on a washing machine that is unbalanced.
    The data in the GitHub repository has data from normal work and data that came
    from dancing, which in our case we are calling **anomalous**.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集阶段，您需要安全地模拟跌倒或不安全工作。您还可以在不平衡的洗衣机上放置传感器。GitHub仓库中的数据包含正常工作和来自跳舞的数据，我们称之为**异常**。
- en: Getting ready
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作：
- en: To get ready for this you will need a Raspberry Pi with a Sense HAT. You will
    need a way of getting data from the Raspberry Pi. You can do this by enabling
    `SSH` or using a USB drive. On the Raspberry Pi, you will need to use `pip` to
    install `sense_hat` and `numpy`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做好准备，您将需要一台带有Sense HAT的树莓派。您需要一种从树莓派获取数据的方式。您可以通过启用`SSH`或使用USB驱动器来完成。在树莓派上，您需要使用`pip`安装`sense_hat`和`numpy`。
- en: How to do it...
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps for this recipe are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 该教程的步骤如下：
- en: 'Import the libraries:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Initialize the variables:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化变量：
- en: '[PRE36]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Wait for the user input to start:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待用户输入开始：
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Gather the data:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集数据：
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Output the files to disk for training:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件输出到磁盘进行训练：
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Copy files from the Raspberry Pi to your local computer by using a thumb drive.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用便携式存储设备，从树莓派复制文件到本地计算机。
- en: Train an isolated forest using the isolated forest recipe and output the `pickle`
    file.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用孤立森林配方训练孤立森林，并输出`pickle`文件。
- en: Copy the `iforrest.p` file to the Raspberry Pi and create a file called `AnomalyDetection.py`.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制`iforrest.p`文件到树莓派，并创建一个名为`AnomalyDetection.py`的文件。
- en: 'Import the libraries:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE40]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Load the machine learning file:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载机器学习文件：
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create the output for the LEDs:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为LED创建输出：
- en: '[PRE42]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Predict the anomaly:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测异常：
- en: '[PRE43]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We create two files – one that gathers information (called `Gather.py`) and
    another that detects the anomalies on the device (called `AnomalyDetection.py`).
    In the `Gather.py` file, we import the classes, initialize `SenseHat`, set a variable
    for the number of readings we will be collecting, get both the gyroscopic and
    accelerometer readings, create an array of normal anonymous strings, and set the
    initial gyroscope and sensor ratings. Then we loop through our actions and tell
    the user to press *Enter* when they want to record normal greetings, and then
    tell them to press *Enter* when they want to record anomalous readings. From there,
    we gather data and give feedback to the user to let them know how many more data
    points they will be gathering. At this point, you should be using the device in
    a way that is normal for its use, such as fall detection by holding it close to
    your body. Then, for the next loop of anomalous readings, you drop the device.
    Finally, we create the training and test sets that we will use for the machine
    learning model. We then need to copy the data files into a local computer, and
    then we perform the analysis in the same way as we did the isolated forest earlier
    in this chapter. We would then get a `pickle` file that we will be using in the
    `AnomalyDetection.py` file.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建两个文件 – 一个收集信息（称为`Gather.py`）和另一个在设备上检测异常（称为`AnomalyDetection.py`）。在`Gather.py`文件中，我们导入类，初始化`SenseHat`，设置一个变量来收集读数的数量，获取陀螺仪和加速度计读数，创建一个正常的匿名字符串数组，并设置初始的陀螺仪和传感器评分。然后，我们循环执行操作，并告诉用户在想要记录正常问候时按*Enter*，然后在想要记录异常读数时再次按*Enter*。从那里开始，我们收集数据并向用户反馈，告诉他们他们将收集多少个数据点。此时，您应该以正常的使用方式使用设备，例如通过将其靠近身体来检测跌落。然后，在下一个异常读数循环中，您会放下设备。最后，我们创建用于机器学习模型的训练集和测试集。然后，我们需要将数据文件复制到本地计算机，并像在本章早期使用孤立森林时一样执行分析。然后，我们将得到一个将在`AnomalyDetection.py`文件中使用的`pickle`文件。
- en: From here, we need to create the `AnomalyDetection.py` file that we will be
    using on our Raspberry Pi. Then we load our `pickle` file, which is our machine
    learning model. From here, we are going to create `alert` and not-alert (`clear`)
    variables that we can toggle for the LED display on the sense set. Finally, we
    run the loop, and if it predicts that the device is acting anomalously we display
    an `alert` signal on the sense set; otherwise, we display a `clear` signal.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们需要创建一个名为`AnomalyDetection.py`的文件，该文件将在我们的树莓派上使用。然后我们加载我们的`pickle`文件，这是我们的机器学习模型。从这里开始，我们将创建`alert`和非`alert`（`clear`）变量，这些变量可以在sense
    set的LED显示上切换。最后，我们运行循环，如果预测设备行为异常，我们在sense set上显示一个`alert`信号；否则，我们显示一个`clear`信号。
