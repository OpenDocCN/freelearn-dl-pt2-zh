- en: Deep Learning for Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习用于计算机视觉
- en: In [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml), *Diving Deep into
    Neural Networks,* we built an image classifier using a popular **convolutional
    neural network** (**CNN**) architecture called **ResNet**, but we used this model
    as a black box. In this chapter, we will explore how we can build an architecture
    from scratch to solve image classification problems, which are the most common
    use cases. We will also learn how to use transfer learning, which will help us
    build image classifiers using a very small dataset. Apart from learning how to
    use CNNs, we will also explore what these convolutional networks learn.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml)中，*深入探讨神经网络*，我们使用了一种名为**ResNet**的流行**卷积神经网络**（**CNN**）架构构建了一个图像分类器，但我们将这个模型当作黑盒子使用。在本章中，我们将探索如何从头开始构建架构来解决图像分类问题，这是最常见的用例之一。我们还将学习如何使用迁移学习，这将帮助我们使用非常小的数据集构建图像分类器。除了学习如何使用CNN，我们还将探索这些卷积网络学习到了什么。
- en: 'In this chapter, we will cover the important building blocks of convolutional
    networks. Some of the important topics that we will be covering in this chapter
    are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖卷积网络的重要构建模块。本章将涵盖以下重要主题：
- en: Introduction to neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络介绍
- en: Building a CNN model from scratch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始构建CNN模型
- en: Creating and exploring a VGG16 model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和探索VGG16模型
- en: Calculating pre-convoluted features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算预卷积特征
- en: Understanding what a CNN model learns
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CNN模型学习的内容
- en: Visualizing the weights of the CNN layer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化CNN层的权重
- en: Introduction to neural networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络介绍
- en: 'In the last few years, CNNs have become popular in image recognition, object
    detection, segmentation, and many other areas in the field of computer vision.
    They are also becoming popular in the field of **natural language processing**
    (**NLP**), though they are not commonly used yet. The fundamental difference between
    fully connected layers and convolution layers is the way the weights are connected
    to each other in the intermediate layers. Let''s take a look at the following
    diagram, which shows how fully connected, or linear, layers work:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，CNN在图像识别、目标检测、分割以及计算机视觉领域的许多其他领域中变得流行起来。尽管在**自然语言处理**（**NLP**）领域中尚不常用，但它们也变得流行起来。完全连接层和卷积层之间的根本区别在于中间层中权重连接的方式。让我们看看以下图表，展示了完全连接或线性层的工作原理：
- en: '![](img/25c3a354-5f47-40df-a2ba-813d044132f1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25c3a354-5f47-40df-a2ba-813d044132f1.png)'
- en: 'One of the biggest challenges of using a linear layer or fully connected layers
    for computer vision is that they lose all spatial information, and the complexity
    in terms of the number of weights that are used by fully connected layers is too
    big. For example, when we represent a 224-pixel image as a flat array, we would
    end up with 150, 528 (224 x 224 x 3 channels). When the image is flattened, we
    lose all the spatial information. Let''s look at what a simplified version of
    a CNN looks like:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中使用线性层或完全连接层的最大挑战之一是它们丢失了所有空间信息，并且在使用完全连接层时权重的复杂性太大。例如，当我们将224像素图像表示为平面数组时，我们将得到150,528（224
    x 224 x 3通道）。当图像被展平时，我们失去了所有的空间信息。让我们看看简化版本的CNN是什么样子：
- en: '![](img/b7865455-c3d0-4191-896f-d970ce006af4.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7865455-c3d0-4191-896f-d970ce006af4.png)'
- en: 'All the convolution layer is doing is applying a window of weights called **filters**
    across the image. Before we try to understand convolutions and other building
    blocks in detail, let''s build a simple yet powerful image classifier for the
    MNIST dataset. Once we''ve built this, we will walk through each component of
    the network. We will break down building our image classifier into the following
    steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的卷积层只是在图像上应用称为**过滤器**的权重窗口。在我们试图详细理解卷积和其他构建模块之前，让我们为MNIST数据集构建一个简单而强大的图像分类器。一旦我们建立了这个分类器，我们将逐步分解网络的每个组件。我们将图像分类器的构建分解为以下步骤：
- en: Getting data
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Creating a validation dataset
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建验证数据集
- en: Building our CNN model from scratch
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始构建我们的CNN模型
- en: Training and validating the model
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: MNIST – getting data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST - 获取数据
- en: 'The MNIST dataset contains 60,000 handwritten digits from 0 to 9 for training
    and 10,000 images for a test set. The PyTorch `torchvision` library provides us
    with an MNIST dataset, which downloads the data and provides it in a readily usable
    format. Let''s use the dataset MNIST function to pull the dataset to our local
    machine and then wrap it around `DataLoader`. We will use `torchvision` transformations
    to convert the data into PyTorch tensors and do data normalization. The following
    code takes care of downloading, wrapping data around `DataLoader`, and normalizing
    the data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集包含 60,000 个手写数字（从 0 到 9）用于训练和 10,000 张图像用于测试。PyTorch 的`torchvision`库为我们提供了一个MNIST数据集，它下载数据并以可直接使用的格式提供。让我们使用
    MNIST 函数将数据集下载到本地并将其包装到`DataLoader`中。我们将使用`torchvision`转换将数据转换为PyTorch张量并进行数据标准化。以下代码将处理下载数据，将数据包装到`DataLoader`中，并进行数据标准化：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The previous code provides us with a `DataLoader` variable for the train and
    test datasets. Let''s visualize a few images to get an understanding of what we
    are dealing with. The following code will help us visualize the MNIST images:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码为我们提供了用于训练和测试数据集的`DataLoader`变量。让我们可视化几张图像，以便了解我们正在处理的内容。以下代码将帮助我们可视化MNIST图像：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can pass the `plot_img` method to visualize our dataset. We will pull
    a batch of records from the `DataLoader` variable using the following code and
    plot the images:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以传递`plot_img`方法来可视化我们的数据集。我们将使用以下代码从`DataLoader`变量中获取一批记录，并绘制图像：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The images can be visualized as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图像可以如下进行可视化：
- en: '![](img/61281deb-f80b-457e-8974-83367ab71034.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61281deb-f80b-457e-8974-83367ab71034.png)'
- en: Building a CNN model from scratch
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建CNN模型
- en: 'In this section, we''ll build our own architecture from scratch. Our network
    architecture will contain a combination of different layers, as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将从头开始构建自己的架构。我们的网络架构将包含不同层的组合，如下所示：
- en: Conv2d
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conv2d
- en: MaxPool2d
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MaxPool2d
- en: '**Rectified linear unit** (**ReLU**)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性单元** (**ReLU**)'
- en: View
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视图
- en: Linear layer
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性层
- en: 'Let''s look at a pictorial representation of the architecture we are going
    to implement:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们打算实现的架构的图示表示：
- en: '![](img/70e1ba7b-d4dc-4563-af8a-cbd06265bbc4.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70e1ba7b-d4dc-4563-af8a-cbd06265bbc4.png)'
- en: 'Let''s implement this architecture in PyTorch and then walk through what each
    individual layer does:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在PyTorch中实现这个架构，然后逐步了解每个单独的层的作用：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's understand what each layer does in detail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每一层的作用。
- en: Conv2d
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Conv2d
- en: 'Conv2d takes care of applying a convolutional filter on our MNIST images. Let''s
    try to understand how convolution is applied on a one-dimensional array and then
    learn how a two-dimensional convolution is applied to an image. Take a look at
    the following diagram. Here, we will apply a **Conv1d** of a filter (or kernel)
    that''s size 3 to a tensor of length 7:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Conv2d 负责在我们的 MNIST 图像上应用卷积滤波器。让我们尝试理解如何在一维数组上应用卷积，然后学习如何在图像上应用二维卷积。看看以下图表。在这里，我们将对长度为
    7 的张量应用一个**Conv1d**大小为 3的滤波器（或核）：
- en: '![](img/74b21364-00fa-46d5-af2a-92ad2317df10.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74b21364-00fa-46d5-af2a-92ad2317df10.png)'
- en: 'The bottom boxes represent our input tensor of seven values, while the connected
    boxes represent the output after we apply our convolution filter of size three.
    At the top right corner of the image, the three boxes represent the weights and
    parameters of the **Conv1d** layer. The convolution filter is applied like a window
    and it moves to the following values by skipping one value. The number of values
    to be skipped is called the **stride** and is set to 1 by default. Let''s understand
    how the output values are being calculated by writing down the calculation for
    the first and last outputs:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的框代表我们的输入张量，共有七个值，而连接的框代表我们应用三个大小的卷积滤波器后的输出。在图像的右上角，三个框代表**Conv1d**层的权重和参数。卷积滤波器像窗口一样应用，并且通过跳过一个值来移动到以下值。要跳过的值的数量称为**步幅**，默认设置为
    1。让我们写下第一个和最后一个输出的计算方式来理解输出值是如何被计算出来的：
- en: Output 1 –> (*-0.5209 x 0.2286*) + (*-0.0147 x 2.4488*) + (*-0.321 x -0.9498*)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 1 –> (*-0.5209 x 0.2286*) + (*-0.0147 x 2.4488*) + (*-0.321 x -0.9498*)
- en: Output 5 –> (*-0.5209 x -0.6791*) + (*-0.0147 x -0.6535*) + (*-0.321 x 0.6437*)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 5 –> (*-0.5209 x -0.6791*) + (*-0.0147 x -0.6535*) + (*-0.321 x 0.6437*)
- en: 'So, by now, it should be clear what a convolution does. It applies a filter
    (or kernel), that is, a bunch of weights, on the input by moving it based on the
    value of the stride. In the preceding example, we moved our filter one point at
    a time. If the stride value is 2, then we would move two points at a time. Let''s
    look at a PyTorch implementation of this to understand how it works:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们应该清楚卷积是做什么的了。它通过根据步长的值移动一个滤波器（或卷积核），也就是一组权重，来对输入进行处理。在前面的示例中，我们每次移动我们的滤波器一个点。如果步长值为2，那么我们将一次移动两个点。让我们看一个PyTorch的实现来理解它是如何工作的：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There is another important parameter, called **padding,** that is often used
    with convolutions. As shown in the previous example, if the filter is not applied
    until the end of the data, that is, when there are not enough elements for the
    data to stride, it stops. Padding prevents this by adding zeros to both ends of
    a tensor. Let''s look at a one-dimensional example of how padding works:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个重要的参数叫做**填充（padding）**，通常与卷积一起使用。如前面的例子所示，如果滤波器未应用到数据的末尾，即当数据不足以进行步长时，它就会停止。填充通过向张量的两端添加零来防止这种情况。让我们看一个一维填充如何工作的示例：
- en: '![](img/04aef5fe-05f4-40b2-b860-4b04ad55278d.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04aef5fe-05f4-40b2-b860-4b04ad55278d.png)'
- en: In the preceding diagram, we applied a **Conv1d** layer with padding 2 and stride
    1\. Let's look at how Conv2d works on an image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，我们使用了一个填充为2且步长为1的**Conv1d**层。让我们看看Conv2d在图像上的工作原理。
- en: Before we understand how Conv2d works, I would strongly recommend that you check
    out an amazing blog ([http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/))
    that contains a live demo of how convolution works. After you have spent a few
    minutes playing with the demo, continue reading.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解Conv2d如何工作之前，我强烈建议您查看一个了不起的博客（[http://setosa.io/ev/image-kernels/](http://setosa.io/ev/image-kernels/)），其中包含卷积如何工作的实时演示。在您花几分钟玩弄演示之后，继续阅读。
- en: 'Let''s understand what happened in the demo. In the center box of the image,
    we have two different sets of numbers: one represented in the boxes and the other
    beneath the boxes. The ones represented in the boxes are pixel values, as highlighted
    by the white box on the left-hand photo in the demo. The numbers denoted beneath
    the boxes are the filter (or kernel) values that are being used to sharpen the
    image. The numbers are handpicked to do a particular job. In this case, they are
    sharpening the image. Just like in our previous example, we are doing an element-to-element
    multiplication and summing up all the values to generate the value of the pixel
    in the right-hand image. The generated value is highlighted by the white box on
    the right- hand side of the image.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来理解演示中发生了什么。在图像的中心框中，我们有两组不同的数字：一组在方框中表示，另一组在方框下方。方框中表示的是像素值，正如演示中左侧照片上的白色框所示。方框下方标记的数字是用于锐化图像的滤波器（或卷积核）值。这些数字是特意挑选出来执行特定的任务。在这种情况下，它们是用来锐化图像的。就像我们之前的例子一样，我们进行逐元素乘法并将所有值求和，以生成右侧图像中像素的值。生成的值由图像右侧的白色框突出显示。
- en: 'Though the values in the kernel are handpicked in this example, in CNNs, we
    do not handpick the values; instead, we initialize them randomly and let gradient
    descent and backpropagation tune the values of the kernels. The learned kernels
    will be responsible for identifying different features, such as lines, curves,
    and eyes. Take a look at the following screenshot, where we can see a matrix of
    numbers and see how convolution works:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在此示例中卷积核中的值是手动挑选的，在CNN中，我们不手动挑选这些值；相反，我们随机初始化它们，并让梯度下降和反向传播调整卷积核的值。学习到的卷积核将负责识别不同的特征，如线条、曲线和眼睛。看看以下截图，我们可以看到一个数字矩阵并了解卷积是如何工作的：
- en: '![](img/d54b8108-1fd5-4b42-8c4c-c7d371f0cc5e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d54b8108-1fd5-4b42-8c4c-c7d371f0cc5e.png)'
- en: 'In the preceding screenshot, we assume that the 6 x 6 matrix represents an
    image and we apply the convolution filter of size 3 x 3\. Then, we show how the
    output is generated. To keep it simple, we are just calculating for the highlighted
    portion of the matrix. The output is generated by doing the following calculation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述屏幕截图中，我们假设6 x 6矩阵表示一幅图像，并应用了大小为3 x 3的卷积滤波器。然后，我们展示了如何生成输出。为了保持简单，我们只计算了矩阵的突出部分。输出通过执行以下计算生成：
- en: Output –> *0.86 x 0 + -0.92 x 0 + -0.61 x 1 + -0.32 x -1 + -1.69 x -1 + ........*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 –> *0.86 x 0 + -0.92 x 0 + -0.61 x 1 + -0.32 x -1 + -1.69 x -1 + ........*
- en: The other important parameter that's used in the Conv2d function is `kernel_size`,
    which decides the size of the kernel. Some of the commonly used kernel sizes are
    *1*, *3*, *5*, and *7*. The larger the kernel's size, the larger the area that
    a filter can cover, so it is common to observe filters of *7* or *9* being applied
    to the input data in the early layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Conv2d 函数中使用的另一个重要参数是`kernel_size`，它决定了卷积核的大小。一些常用的卷积核大小包括*1*、*3*、*5*和*7*。卷积核大小越大，滤波器能够覆盖的区域就越大，因此在早期层中常见到应用大小为*7*或*9*的滤波器对输入数据进行处理。
- en: Pooling
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: It is a common practice to add pooling layers after convolution layers since
    they reduce the size of feature maps and the outcomes of convolution layers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层后添加池化层是一种常见的做法，因为它们可以减小特征图的大小，并优化卷积层的输出结果。
- en: 'Pooling offers two different features: one is reducing the size of the data
    to process and the other is forcing the algorithm to not focus on small changes
    in the position of an image. For example, a face detection algorithm should be
    able to detect a face in the picture, irrespective of the position of the face
    in the photo.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 池化提供了两个不同的功能：一个是减小要处理的数据大小，另一个是强制算法不要专注于图像中位置的微小变化。例如，人脸检测算法应该能够在图片中检测到人脸，而不管人脸在照片中的位置如何。
- en: 'Let''s look at how MaxPool2d works. It also uses the same concept of kernel
    size and strides. It differs from convolutions as it does not have any weights
    and just acts on the data generated by each filter from the previous layer. If
    the kernel size is *2 x 2,* then it considers that size in the image and picks
    the maximum of that area. Let''s look at the following diagram, which will make
    it clear how MaxPool2d works:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 MaxPool2d 是如何工作的。它也使用与卷积相同的核大小和步幅的概念。与卷积不同的是，它不具有任何权重，只是作用于前一层每个滤波器生成的数据。如果核大小为*2
    x 2*，则它会在图像中考虑该大小，并选择该区域的最大值。让我们看一下下面的图表，这将清楚地解释 MaxPool2d 的工作原理：
- en: '![](img/95f76a92-ac50-401a-bf5d-5500e09e1509.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95f76a92-ac50-401a-bf5d-5500e09e1509.png)'
- en: 'The box on the left-hand side contains the values of feature maps. After applying
    max pooling, the output is stored on the right-hand side of the box. Let''s look
    at how the output is calculated by writing down the calculation for the values
    in the first row of the output:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的方框包含特征图的值。在应用最大池化后，输出存储在方框的右侧。让我们通过写出第一行输出中数值的计算来查看输出是如何计算的：
- en: '![](img/b7285839-6b38-4e1c-9901-097262274765.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7285839-6b38-4e1c-9901-097262274765.png)'
- en: 'The other commonly used pooling technique is **average pooling**. The maximum
    function is replaced with the average function. The following diagram explains
    how average pooling works:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用的池化技术是**平均池化**。最大函数被替换为平均函数。下图解释了平均池化的工作原理：
- en: '![](img/af03c571-cf8b-48a6-8159-23132edc61d8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af03c571-cf8b-48a6-8159-23132edc61d8.png)'
- en: 'In this example, instead of taking a maximum of four values, we are taking
    the average four values. Let''s write down the calculation to make it easier to
    understand:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们不是取四个值的最大值，而是取这四个值的平均值。让我们写下计算过程，以便更容易理解：
- en: '![](img/f5926b79-2ee8-488a-8cf2-073ae4305415.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5926b79-2ee8-488a-8cf2-073ae4305415.png)'
- en: Nonlinear activation – ReLU
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性激活 - ReLU
- en: 'It is a common and best practice to have a nonlinear layer after max pooling,
    or after convolution is applied. Most network architectures tend to use ReLU or
    different flavors of ReLU. Whatever nonlinear function we choose, it gets applied
    to each element of the feature map. To make it more intuitive, let''s look at
    an example where we apply ReLU on the same feature map that we applied max pooling
    and average pooling to:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用最大池化或平均池化后，通常在卷积层后添加非线性层是一种常见且最佳的做法。大多数网络架构倾向于使用 ReLU 或不同变体的 ReLU。无论我们选择哪种非线性函数，它都会应用于特征图的每个元素。为了使其更直观，让我们看一个示例，在该示例中，我们在应用了最大池化和平均池化的同一特征图上应用
    ReLU：
- en: '![](img/b67dbb6c-be51-48f4-a3f0-aa0d557d1165.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b67dbb6c-be51-48f4-a3f0-aa0d557d1165.png)'
- en: View
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视图
- en: 'It is a common practice to use a fully connected, or linear, layer at the end
    of most networks for image classification problems. Here, we are using a two-dimensional
    convolution that takes a matrix of numbers as input, and outputs another matrix
    of numbers. To apply a linear layer, we need to flatten the matrix, which is a
    tensor of two dimensions, into a vector of one dimension. The following diagram
    shows how view works:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类问题，在大多数网络的最后使用全连接或线性层是一种常见做法。这里，我们使用的是二维卷积，它以一个数字矩阵作为输入，并输出另一个数字矩阵。要应用线性层，我们需要展平矩阵，即将二维张量展平为一维向量。以下图展示了view函数的工作原理：
- en: '![](img/843840b3-e32a-4e73-beab-0d371811788b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/843840b3-e32a-4e73-beab-0d371811788b.png)'
- en: 'Let''s look at the code that''s used in our network, which does exactly the
    same:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下在我们的网络中使用的代码，它确实如此：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As we saw earlier, the view method will flatten an *n*-dimension tensor into
    a one-dimensional tensor. In our network, the first dimension is of each image.
    The input data after batching will have a dimension of *32 x 1 x 28 x 28,* where
    the first number, *32,* will denote that there are *32* images of size *28* height,
    *28* width, and *1* channel since it is a black and white image. When we flatten,
    we don''t want to flatten or mix the data for different images. So, the first
    argument that we pass to the view function will instruct PyTorch to avoid flattening
    the data on the first dimension. The following diagram shows how this works:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，view方法将把一个*n*维张量展平成一个一维张量。在我们的网络中，每个图像的第一维是输入数据。在批处理后，输入数据的维度将为*32
    x 1 x 28 x 28*，其中第一个数字*32*表示有*32*张大小为*28*高、*28*宽、*1*通道的图像，因为这是一张黑白图像。在展平时，我们不希望展平或混合不同图像的数据。因此，我们传递给view函数的第一个参数将指示PyTorch避免在第一维上展平数据。以下图展示了其工作原理：
- en: '![](img/9463a041-acb1-40bb-be9a-67ba4382b0b3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9463a041-acb1-40bb-be9a-67ba4382b0b3.png)'
- en: 'In the preceding diagram, we have data of size *2 x 1 x 2 x 2*; after we apply
    the view function, it converts it into a tensor of size *2 x 1 x 4*. Let''s look
    at another example where we don''t mention the *- 1*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们有大小为*2 x 1 x 2 x 2*的数据；在应用view函数后，它将其转换为大小为*2 x 1 x 4*的张量。让我们看另一个例子，这次我们没有提到*-
    1*：
- en: '![](img/c9de8126-872d-4205-af00-77cb095097d7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9de8126-872d-4205-af00-77cb095097d7.png)'
- en: If we ever forget to mention which dimension to flatten, we may end up with
    unexpected results, so be extra careful at this step.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们忘记了指明要展平的维度，可能会导致意外的结果，因此在这一步要特别小心。
- en: Linear layer
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性层
- en: After we have converted the data from a two-dimensional tensor into a one-dimensional
    tensor, we pass the data through a linear layer, followed by a nonlinear activation
    layer. In our architecture, we have two linear layers, one followed by ReLU and
    the other followed by a `log_softmax` function, which predicts what digit is contained
    in the given image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数据从二维张量转换为一维张量后，我们通过一个线性层，然后是一个非线性激活层来处理数据。在我们的架构中，我们有两个线性层，一个后面跟着ReLU，另一个后面跟着`log_softmax`函数，用于预测给定图像中包含的数字。
- en: Training the model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'To train the model, we need to follow the same process that we followed for
    our previous dogs and cats image classification problem. The following code snippet
    trains our model on the provided dataset:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们需要遵循与之前的狗和猫图像分类问题相同的过程。以下代码片段训练我们的模型，使用提供的数据集：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This method has different logic for training and validation. There are primarily
    two reasons for using different modes:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在训练和验证时有不同的逻辑。主要有两个原因使用不同的模式：
- en: In training mode, dropout removes a percentage of values, which shouldn't happen
    in the validation or testing phase.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模式下，dropout 会移除一定比例的数值，而这种情况在验证或测试阶段不应该发生。
- en: In training mode, we calculate gradients and change the model's parameter value,
    but backpropagation is not required during the testing or validation phases.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模式下，我们计算梯度并改变模型的参数值，但在测试或验证阶段不需要反向传播。
- en: Most of the code in the previous function is self-explanatory. At the end of
    the function, we return the loss and accuracy of the model for that particular
    epoch.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个函数中，大部分代码都是不言自明的。在函数的最后，我们返回该特定epoch模型的损失和准确度。
- en: 'Let''s run the model through the preceding function for 20 iterations and plot
    the loss and accuracy of train and validation to understand how our network performed.
    The following code runs the `fit` method for the train and test dataset for 20
    iterations:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过前面的函数对模型进行 20 次迭代，并绘制训练和验证的损失和准确率，以了解我们的网络表现如何。以下代码运行 `fit` 方法对训练和测试数据集进行
    20 次迭代：
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code plots the training and test loss:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码绘制了训练和测试的损失：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code generates the following graph:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下图表：
- en: '![](img/47b5dd6a-ec5d-4e0e-8e88-fdc77d63df08.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47b5dd6a-ec5d-4e0e-8e88-fdc77d63df08.png)'
- en: 'The following code plots the training and test accuracy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码绘制了训练和测试的准确率：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code generates the following graph:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下图表：
- en: '![](img/69897521-e770-43fb-a494-52a86fd48428.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69897521-e770-43fb-a494-52a86fd48428.png)'
- en: At the end of the 20th epoch, we achieve a test accuracy of 98.9%. We have got
    our simple convolutional model working and almost achieving state of the art results.
    Let's take a look at what happens when we try the same network architecture on
    our dogs versus cats dataset. We will use the data from the previous chapter,
    Chapter 3, *Building Blocks of Neural Networks,* and the architecture from the
    MNIST example with some minor changes. Once we've trained the model, we can evaluate
    it to understand how well our simple architecture performs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 20 个 epoch 结束时，我们实现了 98.9% 的测试准确率。我们已经让我们的简单卷积模型运行，并几乎达到了最新的结果。让我们看看当我们尝试在我们的狗与猫数据集上使用相同的网络架构时会发生什么。我们将使用上一章节第三章*神经网络的基本构建块*中的数据，以及来自
    MNIST 示例的架构，并进行一些小的更改。一旦我们训练了模型，我们可以评估它，以了解我们的简单架构表现如何。
- en: Classifying dogs and cats – CNN from scratch
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始分类狗和猫 - CNN
- en: 'We will use the same architecture but with a few minor changes, as listed here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的架构，但会进行一些小的更改，如下所列：
- en: The input dimensions for the first linear layer will need to change, since the
    dimensions for our cat and dog images are *256, 256*.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一线性层的输入维度需要改变，因为我们的猫和狗图像的尺寸是 *256, 256*。
- en: We will add another linear layer to allow the model to learn more flexibly.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将添加另一个线性层，以使模型能够更灵活地学习。
- en: 'Let''s look at the code that implements the network architecture:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下实现网络架构的代码：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will use the same training function that we used for the MNIST example, so
    I'm not going to include the code here. However, let's look at the plots that
    are generated when the model is trained for 20 iterations.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与 MNIST 示例相同的训练函数，因此我不会在此处包含代码。然而，让我们看一下在对模型进行 20 次迭代训练时生成的图表。
- en: 'The loss of the training and validation datasets is plotted as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和验证集的损失绘制如下：
- en: '![](img/fc0cc48b-993d-4894-b15a-1646ad638b5e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc0cc48b-993d-4894-b15a-1646ad638b5e.png)'
- en: 'The accuracy for the training and validation datasets is plotted as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和验证集的准确率绘制如下：
- en: '![](img/368de14d-b697-4367-9f35-4272c130ae3f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/368de14d-b697-4367-9f35-4272c130ae3f.png)'
- en: From these plots, it is clear that the training loss is decreasing for every
    iteration, but the validation loss gets worse. Accuracy also increases during
    the training, but almost saturates at 75%. This is a clear example showing that
    the model is not generalizing. In the next section, we will look at another technique
    called **transfer learning**, which helps us train more accurate models and provides
    tricks that we can use to make the training faster.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些图表中可以看出，训练损失在每次迭代中都在减少，但验证损失却变得更糟。准确率在训练过程中也在增加，但几乎在 75% 时趋于饱和。这是一个明显的例子，显示模型没有泛化能力。在接下来的部分，我们将看一下另一种称为**迁移学习**的技术，它可以帮助我们训练更精确的模型，并提供可以加快训练的技巧。
- en: Classifying dogs and cats using transfer learning
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习对狗和猫进行分类
- en: Transfer learning is the ability to reuse a trained algorithm on a similar dataset
    without training it from scratch. We humans don't learn to recognize new images
    by analyzing thousands of similar images. We just understand the different features
    that actually differentiate a particular animal, say, a fox, from a dog. We don't
    need to learn what a fox is from understanding what lines, eyes, and other smaller
    features are like. Therefore, we will learn how to use a pretrained model to build
    state of the art image classifiers with very little data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是能够在类似数据集上重复使用已训练的算法，而无需从头开始训练它。我们人类在认识新图像时并不通过分析成千上万张类似的图像来学习。我们只需理解不同的特征，这些特征实际上能够区分一个特定的动物，比如狐狸，与狗的不同之处。我们不需要通过理解线条、眼睛和其他更小的特征来学习什么是狐狸。因此，我们将学习如何利用预训练模型，用极少量的数据建立最先进的图像分类器。
- en: 'The first few layers of a CNN architecture focus on smaller features, such
    as how a line or curve looks. The filters in the later layers of a CNN learn higher-level
    features, such as eyes and fingers, and the last few layers learn to identify
    the exact category. A pretrained model is an algorithm that is trained on a similar
    dataset. Most of the popular algorithms are pretrained on the popular ImageNet
    dataset to identify 1,000 different categories. Such a pretrained model will have
    filter weights tuned to identify various patterns. So, let''s understand how can
    we take advantage of these pretrained weights. We will look into an algorithm
    called **VGG16**, which was one of the earliest algorithms to find success in
    ImageNet competitions. Though there are more modern algorithms, this algorithm
    is still popular as it is simple to understand and use for transfer learning.
    Let''s take a look at the architecture of the VGG16 model and then try to understand
    the architecture and how we can use it to train our image classifier:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构的前几层专注于更小的特征，例如线条或曲线的外观。CNN后面几层的过滤器学习更高级的特征，例如眼睛和手指，最后几层学习识别确切的类别。预训练模型是一种在类似数据集上训练的算法。大多数流行的算法都是在流行的ImageNet数据集上预训练，以识别1000个不同的类别。这样的预训练模型将有调整后的滤波器权重，用于识别各种模式。因此，让我们了解如何利用这些预训练权重。我们将研究一个名为**VGG16**的算法，它是早期在ImageNet竞赛中取得成功的算法之一。尽管有更现代的算法，但这种算法仍然很受欢迎，因为它简单易懂，适合用于迁移学习。让我们先看看VGG16模型的架构，然后试着理解这个架构以及如何用它来训练我们的图像分类器：
- en: '![](img/13583159-3f95-4e41-afa9-201400628a4a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13583159-3f95-4e41-afa9-201400628a4a.png)'
- en: The VGG16 architecture contains five VGG blocks. A block is a set of convolution
    layers, a nonlinear activation function, and a max-pooling function. All the algorithm
    parameters are tuned to achieve state of the art results when it comes to classifying
    1,000 categories. The algorithm takes input data in the form of batches, which
    are normalized by the mean and standard deviation of the ImageNet dataset.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16架构包含五个VGG块。一个块由卷积层、非线性激活函数和最大池化函数组成。所有的算法参数都被调整以达到在分类1000个类别时的最先进结果。该算法接受以批次形式的输入数据，并且数据被ImageNet数据集的均值和标准差进行了归一化。
- en: In transfer learning, we try to capture what the algorithm learns by freezing
    the learned parameters of most of the layers of the architecture. It is often
    good practice to fine-tune only the last layers of the network. In this example,
    we'll train only the last few linear layers and leave the convolutional layers
    intact since the features that are learned by the convolutional features are mostly
    used for all kinds of image problems where the images share similar properties.
    Let's train a VGG16 model using transfer learning to classify dogs and cats. We'll
    walk through the steps to implement this in the next section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们尝试通过冻结大部分层的学习参数来捕捉算法学到的内容。通常，只微调网络的最后几层是一个良好的实践。在这个例子中，我们将仅训练最后几个线性层，保持卷积层不变，因为卷积特征学习的特征大多适用于所有种类的图像问题，这些图像具有相似的属性。让我们使用迁移学习来训练一个VGG16模型，用于狗和猫的分类。接下来的章节中，我们将详细介绍实现的步骤。
- en: Creating and exploring a VGG16 model
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和探索VGG16模型
- en: 'PyTorch provides a set of trained models in its `torchvision` library. Most
    of them accept an argument called `pretrained` when `True`, which downloads the
    weights that have been tuned for the **ImageNet** classification problem. We can
    use the following code to create a VGG16 model:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch在其`torchvision`库中提供了一组经过训练的模型。当参数`pretrained`为`True`时，大多数模型都会接受一个称为`pretrained`的参数，它会下载为解决**ImageNet**分类问题而调整的权重。我们可以使用以下代码创建一个VGG16模型：
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we have our VGG16 model and all the pretrained weights ready to be used.
    When the code is run for the first time, it could take several minutes, depending
    on your internet speed. The size of the weights could be around 500 MB. We can
    take a quick look at the VGG16 model by printing it. Understanding how these networks
    are implemented turns out to be very useful when we use modern architectures.
    Let''s take a look at the model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了我们的VGG16模型和所有预训练的权重准备好使用。当第一次运行代码时，根据您的互联网速度，可能需要几分钟。权重的大小可能约为500 MB。我们可以通过打印来快速查看VGG16模型。当我们使用现代架构时，了解这些网络如何实现实际上非常有用。让我们看看这个模型：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The model summary contains two sequential models: `features` and `classifiers`.
    The `features` sequential model contains the layers that we are going to freeze.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要包含两个顺序模型：`features` 和 `classifiers`。 `features` 顺序模型包含我们将要冻结的层。
- en: Freezing the layers
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冻结层
- en: Let's freeze all the layers of the features model, which contains the convolutional
    block. Freezing the weights in the layers will prevent the weights of these convolutional
    blocks from updating. Since the weights of the model are trained to recognize
    a lot of important features, our algorithm will be able to do the same from the
    very first iteration. The ability to use model's weights, which were initially
    trained for a different use case, is called **transfer learning**.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们冻结特征模型的所有层，其中包含卷积块。冻结这些卷积块的权重将阻止这些层的权重更新。由于模型的权重经过训练以识别许多重要特征，我们的算法将能够从第一次迭代开始做同样的事情。使用模型的权重，这些权重最初是为不同用例而训练的能力，称为**迁移学习**。
- en: 'Now, let''s look at how we can freeze the weights, or parameters, of layers:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何冻结层的权重或参数：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code prevents the optimizer from updating the weights.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码防止优化器更新权重。
- en: Fine-tuning VGG16
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调VGG16
- en: 'The VGG16 model has been trained to classify 1,000 categories, but not trained
    to classify dogs and cats. Therefore, we need to change the output features of
    the last layer from 1,000 to 2\. We can use the following code to do this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16模型已经训练用于分类1000个类别，但尚未训练用于狗和猫的分类。因此，我们需要将最后一层的输出特征从1000更改为2。我们可以使用以下代码来实现这一点：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `vgg.classifier` function gives us access to all the layers inside the
    sequential model, and the sixth element will contain the last layer. When we train
    the VGG16 model, we only need the classifier parameters to be trained. Therefore,
    we only pass `classifier.parameters` to the optimizer, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`vgg.classifier` 函数使我们可以访问顺序模型内的所有层，第六个元素将包含最后一层。当我们训练VGG16模型时，只需要训练分类器参数。因此，我们只传递`classifier.parameters`
    给优化器，如下所示：'
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the VGG16 model
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练VGG16模型
- en: 'So far, we have created the model and optimizer. Since we are using the Dogs
    versus Cats dataset, we can use the same data loaders and the train function to
    train our model. Remember: when we train the model, only the parameters inside
    the classifier change. The following code snippet will train the model for 20
    epochs, thereby reaching a validation accuracy of 98.45%:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了模型和优化器。由于我们使用的是狗与猫数据集，我们可以使用相同的数据加载器和训练函数来训练我们的模型。请记住：当我们训练模型时，只有分类器内部的参数会发生变化。以下代码片段将训练模型20个epoch，从而达到98.45%的验证精度：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let''s visualize the training and validation loss:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化训练和验证损失：
- en: '![](img/64929940-0c34-4bb1-9fad-2e81bb80cf7d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64929940-0c34-4bb1-9fad-2e81bb80cf7d.png)'
- en: 'Let''s visualize the training and validation accuracy:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化训练和验证精度：
- en: '![](img/ba85c972-cae6-4c39-8559-60a335dc3f3c.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba85c972-cae6-4c39-8559-60a335dc3f3c.png)'
- en: 'We can apply a couple of tricks, such as data augmentation, and play with different
    values of the dropout to improve the model''s generalization capabilities. The
    following code snippet changes the dropout value in the classifier module of VGG
    to 0.2 from 0.5 and trains the model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用一些技巧，如数据增强，并尝试不同的dropout值，以提高模型的泛化能力。以下代码片段将VGG分类器模块中的dropout值从0.5更改为0.2，并训练模型：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Training this for a few epochs gave me a slight improvement; you can try playing
    with the different dropout values yourself and see if you can get better results.
    Another important trick we can use to improve model generalization is to add more
    data or do data augmentation. We can perform data augmentation by randomly flipping
    the image horizontally or rotating the image by a small angle. The `torchvision
    transforms` module provides different functionalities for doing data augmentation
    and they do so dynamically, changing for every epoch. We can implement data augmentation
    using the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型训练几个 epoch 后，我注意到略有改善；您可以尝试调整不同的 dropout 值，看看是否可以获得更好的结果。我们可以使用另一个重要的技巧来改善模型的泛化能力，即增加数据或进行数据增强。我们可以通过随机水平翻转图像或将图像旋转一小角度来执行数据增强。`torchvision
    transforms` 模块提供了不同的功能来执行数据增强，并且它们是动态的，每个 epoch 都会变化。我们可以使用以下代码实现数据增强：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Training the model with augmented data improved the model's accuracy by 0.1%
    by running just two epochs; we can run it for a few more epochs to improve further.
    If you have been training these models while reading this book, you will have
    realized that training each epoch could take more than a couple of minutes, depending
    on the GPU you are running. Let's look at a technique where we can train each
    epoch in a few seconds.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用增强数据训练模型使模型的准确性提高了0.1%，只需运行两个 epoch；我们可以再运行几个 epoch 来进一步提高。如果您在阅读本书时训练这些模型，您会意识到每个
    epoch 的训练时间可能超过几分钟，这取决于您正在使用的 GPU。让我们看看一种技术，可以使每个 epoch 的训练时间缩短到几秒钟。
- en: Calculating pre-convoluted features
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算预卷积特征
- en: When we freeze the convolution layers and the training model, the input to the
    fully connected layers, or dense layers, (`vgg.classifier`) is always the same.
    To understand this better, let's treat the convolution block – which in our example
    is the `vgg.features` block – as a function that has learned weights and doesn't
    change during training. So, calculating the convolution features and storing them
    will help us improve the training speed. The time to train the model will decrease
    since we only calculate these features once, instead of calculating for each epoch.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们冻结卷积层和训练模型时，完全连接层或稠密层（`vgg.classifier`）的输入始终保持不变。为了更好地理解这一点，让我们将卷积块——在我们的示例中是
    `vgg.features` 块——视为一个具有学习权重且在训练过程中不会改变的函数。因此，计算卷积特征并存储它们将有助于提高训练速度。训练模型的时间将减少，因为我们只需计算这些特征一次，而不是在每个
    epoch 都计算一次。
- en: 'Let''s visually understand this and implement it:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过可视化理解并实现它：
- en: '![](img/27b0f4e9-1526-45ff-b025-57f6078dd457.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27b0f4e9-1526-45ff-b025-57f6078dd457.png)'
- en: 'The first box depicts how training is done in general, which could be slow
    since we calculate the convolutional features for every epoch, though the values
    don''t change. In the bottom box, we calculate the convolutional features once
    and train only the linear layers. To calculate the pre-convoluted features, we
    need to pass all the training data through the convolution blocks and store them.
    To perform this, we need to select the convolution blocks of the VGG model. Fortunately,
    the PyTorch implementation of VGG16 has two sequential models, so just picking
    the first sequential model''s features is enough. The following code does this
    for us:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个框描述了通常的训练方式，因为我们在每个 epoch 计算卷积特征，尽管值不变，因此可能会很慢。在底部框中，我们只计算一次卷积特征，然后仅训练线性层。为了计算预卷积特征，我们需要通过卷积块传递所有训练数据，并将它们存储起来。为此，我们需要选择
    VGG 模型的卷积块。幸运的是，PyTorch 实现的 VGG16 有两个序列模型，因此只需选择第一个序列模型的特征即可。以下代码为我们执行此操作：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, the `preconvfeat` method takes in the dataset and the `vgg`
    model and returns the convoluted features, along with the labels associated with
    it. The rest of the code is similar to what we used in the preceding examples
    to create data loaders and datasets.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`preconvfeat` 方法接收数据集和 `vgg` 模型，并返回卷积特征以及相关的标签。其余的代码与前面的示例中用来创建数据加载器和数据集的代码类似。
- en: 'Once we have the convolutional features for the train and validation sets,
    we can create a PyTorch dataset and `DataLoader` classes, which will ease up our
    training process. The following code creates the dataset and `DataLoader` for
    our convolutional features:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了训练集和验证集的卷积特征，我们可以创建一个 PyTorch 数据集和 `DataLoader` 类，这将简化我们的训练过程。以下代码创建了用于我们卷积特征的数据集和
    `DataLoader`：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Since we have our new data loaders, which generate batches of convoluted features
    along with labels, we can use the same train function that we have been using
    in the other examples. Now, we will use `vgg.classifier` as the model to create
    the optimizer and fit methods. The following code trains the classifier module
    to identify dogs and cats. On a Titan X GPU, each epoch takes less than five seconds,
    which would otherwise take a few minutes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有了新的数据加载器，它们生成了一批批的卷积特征和标签，我们可以使用在其他示例中使用过的相同训练函数。现在，我们将使用`vgg.classifier`作为模型来创建优化器和拟合方法。以下代码训练分类器模块以识别狗和猫。在Titan
    X GPU上，每个epoch不到五秒，否则可能需要几分钟：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Understanding what a CNN model learns
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CNN模型学习的内容
- en: Deep learning models are often said to not be interpretable. However, there
    are different techniques that we can use to interpret what happens inside these
    models. For images, the features that are learned by convents are interpretable.
    In this section, we will explore two popular techniques so that we can understand
    convents.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型通常被认为是不可解释的。然而，有不同的技术可以帮助我们解释这些模型内部发生的情况。对于图像，卷积层学习到的特征是可解释的。在本节中，我们将探索两种流行的技术，以便理解卷积层。
- en: Visualizing outputs from intermediate layers
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从中间层可视化输出
- en: Visualizing the outputs from intermediate layers will help us understand how
    the input image is being transformed across different layers. Often, the output
    from each layer is called an **activation**. To do this, we should extract the
    output from intermediate layers, which can be done in different ways. PyTorch
    provides a method called `register_forward_hook`, which allows us to pass a function
    that can extract the outputs of a particular layer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中间层的输出将帮助我们理解输入图像在不同层之间的转换方式。通常，每个层的输出称为**激活**。为此，我们应该提取中间层的输出，可以通过不同的方式完成。PyTorch提供了一种称为`register_forward_hook`的方法，允许我们传递一个函数来提取特定层的输出。
- en: 'By default, PyTorch models only store the output of the last layer so that
    they use memory optimally. So, before we inspect what the activations from the
    intermediate layers look like, let''s learn how to extract outputs from the model.
    Take a look at the following code snippet, which extracts outputs. We will walk
    through it to understand what happens:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch模型只存储最后一层的输出，以便更有效地使用内存。因此，在检查中间层的激活输出之前，让我们学习如何从模型中提取输出。看看以下代码片段，它提取了输出。我们将逐步分析以理解发生了什么：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We start by creating a pre-trained VGG model, from which we extract the outputs
    of a particular layer. The `LayerActivations` class instructs PyTorch to store
    the output of a layer in the features variable. Let's walk through each function
    inside the `LayerActivations` class.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个预训练的VGG模型，从中提取特定层的输出。`LayerActivations`类指示PyTorch将该层的输出存储在`features`变量中。让我们逐个了解`LayerActivations`类内的每个函数。
- en: The `_init_` function takes a model and the layer number of the layer that the
    outputs need to be extracted from as arguments. We call the `register_forward_hook`
    method on the layer and pass in a function. PyTorch, when doing a forward pass
    – that is, when the images are passed through the layers – calls the function
    that is passed to the `register_forward_hook` method. This method returns a handle,
    which can be used to deregister the function that is passed to the `register_forward_hook`
    method.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`_init_`函数以模型和需要从中提取输出的层的层编号作为参数。我们在该层上调用`register_forward_hook`方法并传入一个函数。当PyTorch进行前向传播时，即当图像通过各层时，PyTorch调用传递给`register_forward_hook`方法的函数。该方法返回一个句柄，可以用于注销传递给`register_forward_hook`方法的函数。'
- en: The `register_forward_hook` method passes three values to the function that
    we pass to it. The `module` parameter allows us to access the layer itself. The
    second parameter is `input`, which refers to the data that is flowing through
    the layer. The third parameter is `output`, which allows us to access the transformed
    inputs, or activations, of the layer. We store the output of the `features` variable
    in the `LayerActivations` class.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`register_forward_hook`方法将三个值传递给我们传递给它的函数。`module`参数允许我们访问层本身。第二个参数是`input`，它指的是流经该层的数据。第三个参数是`output`，允许我们访问转换后的输入或层的激活。我们将`features`变量的输出存储在`LayerActivations`类中。'
- en: 'The third function takes the hook from the `_init_` function and deregisters
    the function. Now, we can pass the model and the layer numbers of the activations
    we are looking for. Let''s look at the activations that will be created for the
    following image for different layers:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个函数从`_init_`函数中获取钩子并取消注册函数。现在，我们可以传递模型和我们正在寻找的激活层的层数。让我们看看为不同层次的以下图像创建的激活：
- en: '![](img/7b52b298-91e3-43a8-a623-b828e727415f.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b52b298-91e3-43a8-a623-b828e727415f.png)'
- en: 'Let''s visualize some of the activations that are created by the first convolution
    layer and the code that''s used for this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化第一个卷积层生成的一些激活以及用于此目的的代码：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s visualize some of the activations that are created by the fifth convolutional
    layer:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化第五个卷积层生成的一些激活：
- en: '![](img/b30d86fd-298a-4867-bd10-02ba5e7a3209.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b30d86fd-298a-4867-bd10-02ba5e7a3209.png)'
- en: 'Let''s look at the last CNN layer:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看最后的CNN层：
- en: '![](img/d84ef366-2e9b-4c37-a15c-ef24748a92cc.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d84ef366-2e9b-4c37-a15c-ef24748a92cc.png)'
- en: By looking at what the different layers generate, we can see that the early
    layers detect lines and edges, while the final layers tend to learn high-level
    features and are less interpretable.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看不同层生成的内容，我们可以看到早期层次检测线条和边缘，而最终层次则倾向于学习高级特征，不太可解释。
- en: Before we move on to visualizing weights, let's learn how the features maps
    or activations represent themselves after the ReLU layer. So, let's visualize
    the outputs of the second layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续可视化权重之前，让我们学习一下ReLU层之后的特征映射或激活在表现上是如何的。因此，让我们可视化第二层的输出。
- en: If you take a quick look at the fifth image in the second row of the preceding
    image, it looks like the filter is detecting the eyes in the image. When the models
    do not perform, these visualization tricks can help us understand why the model
    may not be working.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您快速查看上述图像的第二行第五幅图像，它看起来像是滤波器在检测图像中的眼睛。当模型表现不佳时，这些可视化技巧可以帮助我们理解模型为何可能无法工作。
- en: Visualizing the weights of the CNN layer
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化CNN层的权重
- en: 'Getting the model weights for a particular layer is straightforward. All the
    model weights can be accessed through the `state_dict` function. The `state_dict`
    function returns a dictionary, with `keys` as its layers and `weights` as its
    values. The following code demonstrates how we can pull weights for a particular
    layer and visualize them:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 获取特定层次的模型权重很简单。所有模型权重都可以通过`state_dict`函数访问。`state_dict`函数返回一个字典，其中`keys`为层，`weights`为其值。以下代码演示了如何提取特定层的权重并可视化它们：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code provides us with the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码给我们提供了以下输出：
- en: '![](img/eeebe3c9-3388-483b-a64a-fd6b9ae61753.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeebe3c9-3388-483b-a64a-fd6b9ae61753.png)'
- en: Each box represents the weights of a filter that are *3* x *3* in size. Each
    filter is trained to identify certain patterns in the images.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框代表一个大小为*3* x *3*的滤波器的权重。每个滤波器都经过训练，用于识别图像中的特定模式。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to build an image classifier using convents,
    as well as how to use a pretrained model. We covered tricks on how to speed up
    the process of training by using pre-convoluted features. We also looked at the
    different techniques we can use to understand what goes on inside a CNN.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用卷积神经网络构建图像分类器，以及如何使用预训练模型。我们探讨了通过使用预卷积特征加速训练过程的技巧。我们还研究了了解CNN内部运行情况的不同技术。
- en: In the next chapter, we will learn how to handle sequential data using recurrent
    neural networks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用循环神经网络处理序列数据。
