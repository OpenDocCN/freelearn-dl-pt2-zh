- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Artificial Neural Network Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络基础知识
- en: An **Artificial Neural Network** (**ANN**) is a supervised learning algorithm
    that is loosely inspired by the way the human brain functions. Similar to the
    way neurons are connected and activated in the human brain, a neural network takes
    input and passes it through a function, resulting in certain subsequent neurons
    getting activated, and consequently, producing the output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）是一种监督学习算法，松散地受人类大脑功能的启发。类似于人类大脑中神经元的连接和激活方式，神经网络接受输入并通过函数传递，导致某些后续神经元被激活，并因此产生输出。'
- en: There are several standard ANN architectures. The universal approximation theorem
    says that we can always find a large enough neural network architecture with the
    right set of weights that can exactly predict any output for any given input.
    This means that for a given dataset/task, we can create an architecture and keep
    adjusting its weights until the ANN predicts what we want it to predict. Adjusting
    the weights until the ANN learns a given task is called training the neural network.
    The ability to train on large datasets and customized architectures is how ANNs
    have gained prominence in solving various relevant tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种标准的ANN架构。通用逼近定理表明，我们总是能找到足够大的神经网络架构和合适的权重集合，可以精确预测任何给定输入的输出。这意味着对于给定的数据集/任务，我们可以创建一个架构，并不断调整其权重，直到ANN预测我们想要的结果。调整权重直到ANN学习到给定任务称为训练神经网络。在解决各种相关任务中，ANN如何通过在大型数据集上训练和定制架构来获得重要性。
- en: 'One of the prominent tasks in computer vision is to recognize the class of
    the object present in an image. ImageNet ([https://www.image-net.org/challenges/LSVRC/index.php](https://www.image-net.org/challenges/LSVRC/index.php))
    was a competition held to identify the class of objects present in an image. The
    reduction in classification error rate over the years is as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉中的一个突出任务是识别图像中存在的对象类别。ImageNet（[https://www.image-net.org/challenges/LSVRC/index.php](https://www.image-net.org/challenges/LSVRC/index.php)）是一个比赛，旨在识别图像中存在的对象类别。多年来分类错误率的减少如下所示：
- en: '![](img/B18457_01_01.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_01.png)'
- en: 'Figure 1.1: Classification error rate in ImageNet competition (source: [https://www.researchgate.net/publication/331789962_Basics_of_Supervised_Deep_Learning](https://www.researchgate.net/publication/331789962_Basics_of_Supervised_Deep_Learning))'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：ImageNet比赛中的分类错误率（来源：[https://www.researchgate.net/publication/331789962_Basics_of_Supervised_Deep_Learning](https://www.researchgate.net/publication/331789962_Basics_of_Supervised_Deep_Learning)）
- en: The year 2012 was when a neural network (AlexNet) won the ImageNet competition.
    As you can see from the preceding chart, there was a considerable reduction in
    errors from the year 2011 to the year 2012 by leveraging neural networks. Since
    then, with more deep and complex neural networks, the classification error kept
    reducing and has surpassed human-level performance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年，神经网络（AlexNet）赢得了ImageNet比赛。正如您从前面的图表中可以看到的那样，从2011年到2012年，错误率显著减少，通过利用神经网络。从那时起，随着更深层和复杂的神经网络，分类错误率继续减少，并超过了人类水平的表现。
- en: 'Not only did neural networks reach a human-level performance in image classification
    (and related tasks like object detection and segmentation) but they have enabled
    a completely new set of use cases. **Generative AI** (**GenAI**) leverages neural
    networks to generate content in multiple ways:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不仅在图像分类（以及对象检测和分割等相关任务）中达到了人类水平的性能，而且还启用了一整套全新的用例。**生成AI**（**GenAI**）利用神经网络以多种方式生成内容：
- en: Generating images from input text
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从输入文本生成图像
- en: Generating novel custom images from input images and text
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从输入图像和文本生成新的定制图像
- en: Leveraging content from multiple input modalities (image, text, and audio) to
    generate new content
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用多个输入模态（图像、文本和音频）生成新内容
- en: Generating video from text/image input
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本/图像输入生成视频
- en: This gives a solid motivation for us to learn and implement neural networks
    for our custom tasks, where applicable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们学习和实现神经网络在我们的定制任务中的应用提供了坚实的动机。
- en: 'In this chapter, we will create a very simple architecture on a simple dataset
    and mainly focus on how the various building blocks (feedforward, backpropagation,
    and learning rate) of an ANN help in adjusting the weights so that the network
    learns to predict the expected outputs from given inputs. We will first learn,
    mathematically, what a neural network is, and then build one from scratch to have
    a solid foundation. Then we will learn about each component responsible for training
    the neural network and code them as well. Overall, we will cover the following
    topics:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在一个简单的数据集上创建一个非常简单的架构，并主要关注人工神经网络的各种构建模块（前馈、反向传播和学习率）如何调整权重，以使网络能够从给定的输入中学习预测期望的输出。我们首先将从数学上学习神经网络是什么，然后从头开始构建一个，以建立坚实的基础。然后，我们将详细了解每个负责训练神经网络的组件，并编写相应的代码。总体而言，我们将涵盖以下主题：
- en: Comparing AI and traditional machine learning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较人工智能和传统机器学习
- en: Learning about the ANN building blocks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关于人工神经网络构建模块
- en: Implementing feedforward propagation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现前向传播
- en: Implementing backpropagation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现反向传播
- en: Putting feedforward propagation and backpropagation together
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将前向传播和反向传播结合起来实现
- en: Understanding the impact of the learning rate
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解学习率的影响
- en: Summarizing the training process of a neural network
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结神经网络的训练过程
- en: All code snippets within this chapter are available in the `Chapter01` folder
    of the Github repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有本章节中的代码片段都可以在 Github 仓库的 `Chapter01` 文件夹中找到，链接为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: We strongly recommend you execute code using the **Open in Colab** button within
    each notebook.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议您通过每个笔记本中的**在 Colab 中打开**按钮执行代码。
- en: Comparing AI and traditional machine learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较人工智能和传统机器学习
- en: 'Traditionally, systems were made intelligent by using sophisticated algorithms
    written by programmers. For example, say you are interested in recognizing whether
    a photo contains a dog or not. In the traditional **Machine Learning** (**ML**)
    setting, an ML practitioner or a subject matter expert first identifies the features
    that need to be extracted from images. Then they extract those features and pass
    them through a well-written algorithm that deciphers the given features to tell
    whether the image is of a dog or not. The following diagram illustrates this idea:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，系统通过程序员编写的复杂算法来实现智能化。例如，假设您有兴趣识别一张照片是否包含狗。在传统的**机器学习**（**ML**）设置中，机器学习从业者或专业主题专家首先确定需要从图像中提取的特征。然后，他们提取这些特征，并通过一个精心编写的算法将给定的特征解析出来，告诉我们图像是否是一只狗。下图说明了这个概念：
- en: '![Diagram  Description automatically generated with low confidence](img/B18457_01_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![使用低置信度自动生成的描述的图表](img/B18457_01_02.png)'
- en: 'Figure 1.2: Traditional Machine Learning workflow for classification'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：传统机器学习用于分类的工作流程
- en: 'Take the following samples:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的样本：
- en: '![](img/B18457_01_03.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_03.png)'
- en: 'Figure 1.3: Sample images to generate rules'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：生成规则的示例图像
- en: 'From the preceding images, a simple rule might be that if an image contains
    three black circles aligned in a triangular shape, it can be classified as a dog.
    However, this rule would fail against this deceptive close-up of a muffin:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述图像中，一个简单的规则可能是，如果一幅图像中有三个黑色圆圈呈三角形排列，那么它可以被分类为狗。然而，这个规则在这张欺骗性的松饼特写图像面前就会失败：
- en: '![](img/B18457_01_04.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_04.png)'
- en: 'Figure 1.4: Image on which simple rules can fail'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4：简单规则可能失败的图像
- en: Of course, this rule also fails when shown an image with anything other than
    a dog’s face close up. Naturally, therefore, the number of manual rules we’d need
    to create for the accurate classification of images can be exponential, especially
    as images become more complex. Therefore, the traditional approach works well
    in a very constrained environment (say, taking a passport photo where all the
    dimensions are constrained within millimeters) and works badly in an unconstrained
    environment, where every image varies a lot.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当显示一个除了狗脸特写之外的图像时，这个规则也会失败。因此，为了准确分类图像，我们需要创建的手动规则数量可能是指数级的，特别是随着图像变得更加复杂。因此，传统方法在非常受限制的环境中效果良好（比如拍护照照片，所有尺寸都在毫米内限制），在无约束环境中效果不佳，因为每幅图像都有很大变化。
- en: We can extend the same line of thought to any domain, such as text or structured
    data. In the past, if someone was interested in programming to solve a real-world
    task, it became necessary for them to understand everything about the input data
    and write as many rules as possible to cover every scenario. This is tedious and
    there is no guarantee that all new scenarios would follow said rules.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将相同的思路扩展到任何领域，比如文本或结构化数据。过去，如果有人对编程以解决现实任务感兴趣，那么了解输入数据的一切并尽可能多地编写规则以涵盖每种情况就变得必要了。这是一件繁琐的事情，并且不能保证所有新情况都会遵循这些规则。
- en: However, by leveraging ANNs, we can do this in a single step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过利用ANN，我们可以在一步中完成这一操作。
- en: Neural networks provide the unique benefit of combining feature extraction (hand-tuning)
    and using those features for classification/regression in a single shot with little
    manual feature engineering. Both these subtasks only require labeled data (for
    example, which pictures are dogs and which are not dogs) and a neural network
    architecture. It does not require a human to come up with rules to classify an
    image, which takes away the majority of the burden traditional techniques impose
    on the programmer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络提供了独特的好处，即在单一步骤中结合特征提取（手动调整）并将这些特征用于分类/回归，只需少量手动特征工程。这两个子任务只需标记数据（例如哪些图片是狗，哪些不是狗）和一个神经网络架构。它不需要人类提出规则来分类图像，这减少了传统技术对程序员的大部分负担。
- en: 'Notice that the main requirement is that we provide a considerable number of
    examples for the task that needs a solution. For example, in the preceding case,
    we need to provide multiple *dog* and *not-dog* pictures to the model so it learns
    the features. A high-level view of how neural networks are leveraged for the task
    of classification is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，主要要求是我们为需要解决方案的任务提供大量示例。例如，在前面的情况下，我们需要为模型提供多个*狗*和*非狗*图片，以便它学习特征。神经网络在分类任务中的高层视图如下所示：
- en: '![Diagram  Description automatically generated](img/B18457_01_05.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18457_01_05.png)'
- en: 'Figure 1.5: Neural network based approach for classification'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：基于神经网络的分类方法
- en: Now that we have gained a very high-level overview of the fundamental reason
    why neural networks perform better than traditional computer vision methods, let’s
    gain a deeper understanding of how neural networks work throughout the various
    sections in this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对神经网络比传统计算机视觉方法表现更好的基本原因有了非常高层次的概述，让我们在本章的各个部分深入了解神经网络如何工作。
- en: Learning about the ANN building blocks
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解ANN构建模块
- en: An ANN is a collection of tensors (weights) and mathematical operations arranged
    in a way that loosely replicates the functioning of a human brain. It can be viewed
    as a mathematical function that takes in one or more tensors as inputs and predicts
    one or more tensors as outputs. The arrangement of operations that connects these
    inputs to outputs is referred to as the architecture of the neural network – which
    we can customize based on the task at hand, that is, based on whether the problem
    contains structured (tabular) or unstructured (image, text, and audio) data (which
    is the list of input and output tensors).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是一组张量（权重）和数学操作，以松散复制人脑功能的方式排列。它可以被视为接受一个或多个张量作为输入并预测一个或多个张量作为输出的数学函数。连接这些输入到输出的操作排列被称为神经网络的架构
    - 我们可以根据手头任务定制它，即基于问题是否包含结构化（表格）或非结构化（图像、文本和音频）数据（这是输入和输出张量列表）。
- en: 'An ANN is made up of the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络由以下组成：
- en: '**Input layers**: These layers take the independent variables as input.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：这些层将独立变量作为输入。'
- en: '**Hidden (intermediate) layers**: These layers connect the input and output
    layers while performing transformations on top of input data. Furthermore, the
    hidden layers contain **nodes** (units/circles in the following diagram) to modify
    their input values into higher-/lower-dimensional values. The functionality to
    achieve a more complex representation is achieved by using various activation
    functions that modify the values of the nodes of intermediate layers.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏（中间）层**：这些层连接输入和输出层，同时在输入数据之上执行变换。此外，隐藏层包含**节点**（在下图中表示为单元/圆圈），用于将它们的输入值修改为更高维/低维的值。通过使用各种激活函数修改中间层节点的值来实现更复杂表示的功能。'
- en: '**Output layer**: This generates the values the input variables are expected
    to result in when passed through the network.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：这生成了输入变量通过网络传递时预期的值。'
- en: 'With this in mind, the typical structure of a neural network is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，神经网络的典型结构如下：
- en: '![Diagram  Description automatically generated](img/B18457_01_06.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18457_01_06.png)'
- en: 'Figure 1.6: Neural network structure'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：神经网络结构
- en: 'The number of **nodes** (circles in the preceding diagram) in the output layer
    depends on the task at hand and whether we are trying to predict a continuous
    variable or a categorical variable. If the output is a continuous variable, the
    output has one node. If the output is categorical with *m* possible classes, there
    will be *m* nodes in the output layer. Let’s zoom into one of the nodes/neurons
    and see what’s happening. A neuron transforms its inputs as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层中的**节点**（在上图中表示为圆圈）的数量取决于手头的任务以及我们是否试图预测连续变量或分类变量。如果输出是连续变量，则输出层有一个节点。如果输出是具有*m*个可能类别的分类变量，则输出层有*m*个节点。让我们放大一个节点/神经元并看看发生了什么。一个神经元的输入转换如下：
- en: '![](img/B18457_01_07.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_07.png)'
- en: 'Figure 1.7: Input transformation at a neuron'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：神经元的输入转换
- en: In the preceding diagram, *x*[1],*x*[2], ..., *x*[n] are the input variables,
    and *w*[0] is the bias term (similar to the way we have a bias in linear/logistic
    regression).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图表中，*x*[1],*x*[2], ..., *x*[n]是输入变量，*w*[0]是偏置项（类似于线性/逻辑回归中的偏差）。
- en: 'Note that *w*[1],*w*[2], ..., *w*[n] are the weights given to each of the input
    variables and *w*[0] is the bias term. The output value *a* is calculated as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*w*[1],*w*[2], ..., *w*[n]是分配给每个输入变量的权重，*w*[0]是偏置项。输出值*a*的计算如下：
- en: '![](img/B18457_01_001.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_001.png)'
- en: As you can see, it is the sum of the products of *weight and input* pairs followed
    by an additional function *f* (the bias term + sum of products). The function
    *f* is the activation function that is used to apply non-linearity on top of this
    sum of products. More details on the activation functions will be provided in
    the next section, on feedforward propagation. Further, more nonlinearity can be
    achieved by having more than one hidden layer, stacking multitudes of neurons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，它是*权重和输入*对的乘积之和，后跟额外的函数*f*（偏置项 + 乘积之和）。函数*f*是激活函数，用于在这些乘积之和之上应用非线性。在接下来的部分中，将详细介绍激活函数。此外，通过具有多个隐藏层，可以实现更多的非线性。
- en: 'At a high level, a neural network is a collection of nodes where each node
    has an adjustable float value called **weight** and the nodes are interconnected
    as a graph to return outputs in a format that is dictated by the architecture
    of the network. The network constitutes three main parts: the input layer, the
    hidden layer(s), and the output layer. Note that you can have a higher *number
    (n) of* hidden layers, with the term *deep* learning referring to the greater
    number of hidden layers. Typically, more hidden layers are needed when the neural
    network has to comprehend something complicated such as image recognition.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，神经网络是一组节点，每个节点具有称为**权重**的可调浮点值，并且节点作为图形互连以按照网络结构返回输出。网络由三个主要部分组成：输入层、隐藏层（们）和输出层。请注意，可以有更多的*隐藏层数
    (n)*，术语*深度*学习指的是更多的隐藏层数。通常，在神经网络需要理解像图像识别这样复杂的事物时，需要更多的隐藏层。
- en: With the architecture of a neural network in mind, let’s learn about feedforward
    propagation, which helps in estimating the amount of error (loss) the network
    architecture has.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到神经网络的架构，让我们了解一下前馈传播，它有助于估计网络架构存在的误差（损失）的量。
- en: Implementing feedforward propagation
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现前馈传播
- en: To build a strong foundational understanding of how feedforward propagation
    works, we’ll go through a toy example of training a neural network where the input
    to the neural network is (1, 1) and the corresponding (expected) output is 0\.
    Here, we are going to find the optimal weights of the neural network based on
    this single input-output pair.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立对前馈传播如何工作的坚实基础理解，我们将通过训练神经网络的玩具示例来进行介绍，其中神经网络的输入为(1, 1)，对应的（期望的）输出为0。在此，我们将根据这对单一输入输出找到神经网络的最优权重。
- en: In real-world projects, there will be thousands of data points on which an ANN
    is trained.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际项目中，将有数千个数据点用于训练人工神经网络。
- en: 'Our neural network architecture for this example contains one hidden layer
    with three nodes in it, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的神经网络架构包含一个包含三个节点的隐藏层，如下所示：
- en: '![](img/B18457_01_08.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_08.png)'
- en: 'Figure 1.8: Sample neural network architecture with 1 hidden layer'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：带有1个隐藏层的示例神经网络架构
- en: Every arrow in the preceding diagram contains exactly one float value (**weight**)
    that is adjustable. There are 9 floats (6 weights corresponding to the connections
    between the input nodes and hidden layer nodes and 3 corresponding to the connections
    between the hidden layer and output layer) that we need to find so that when the
    input is (1,1), the output is as close to (0) as possible. This is what we mean
    by training the neural network. We have not introduced a bias value yet for simplicity
    purposes, but the underlying logic remains the same.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图表中的每个箭头都包含一个浮点值（**权重**），可调整。有9个浮点数（6个对应于输入节点与隐藏层节点之间的连接的权重，3个对应于隐藏层与输出层之间的连接的权重），我们需要找到这些值，使得当输入为（1,1）时，输出尽可能接近（0）。这就是我们所说的训练神经网络。出于简化目的，我们尚未引入偏置值，但基本逻辑保持不变。
- en: 'In the subsequent sections, we will learn the following about the preceding
    network:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将了解前述网络的以下内容：
- en: Calculating hidden layer values
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算隐藏层的值
- en: Performing non-linear activations
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行非线性激活
- en: Estimating the output layer value
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计输出层的值
- en: Calculating the loss value corresponding to the expected value
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算对应于期望值的损失值
- en: Calculating the hidden layer unit values
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算隐藏层单元值
- en: We’ll now assign weights to all the connections. In the first step, we assign
    weights randomly across all the connections. In general, neural networks are initialized
    with random weights before the training starts. Again, for simplicity, while introducing
    the topic, we will **not** include the bias value while learning about feedforward
    propagation and backpropagation. But we will have it while implementing both feedforward
    propagation and backpropagation from scratch in the subsequent section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为所有连接分配权重。在第一步中，我们随机分配所有连接的权重。一般来说，在训练开始之前，神经网络会用随机权重进行初始化。再次强调，为了简化起见，在介绍前向传播和反向传播的过程中，我们**不**包括偏置值。但是在后续章节中实现从头开始的前向传播和反向传播时会有。
- en: Let’s start with initial weights that are randomly initialized between 0 and
    1.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在0到1之间随机初始化的初始权重开始。
- en: '**Important note**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: The final weights after the training process of a neural network don’t need
    to be between a specific set of values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练过程中的最终权重不需要处于特定数值范围内。
- en: A formal representation of weights and values in the network is provided in
    the following diagram (left half) and the randomly initialized weights are provided
    in the network in the right half.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中提供的权重和值的正式表示如下图所示（左半部分），网络中提供的随机初始化权重如右半部分所示。
- en: '![](img/B18457_01_09.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_09.png)'
- en: 'Figure 1.9: (Left) Formal representation of neural network (Right) Random weight
    initialization of the neural network'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：（左）神经网络的正式表示（右）神经网络的随机权重初始化
- en: 'In the next step, we perform the multiplication of the input with weights to
    calculate the values of hidden units in the hidden layer. The hidden layer’s unit
    values before activation are obtained as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们执行输入与权重的乘法，以计算隐藏层中隐藏单元的值。在激活之前，隐藏层单元的值如下获得：
- en: '![](img/B18457_01_002.png)![](img/B18457_01_003.png)![](img/B18457_01_004.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_002.png)![](img/B18457_01_003.png)![](img/B18457_01_004.png)'
- en: 'The hidden layer’s unit values (before activation) that are calculated here
    are also shown in the following diagram:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此处计算的隐藏层单元值（在激活之前）也显示在下图中：
- en: '![](img/B18457_01_10.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_10.png)'
- en: 'Figure 1.10: Hidden layer’s unit values prior to activation'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10：激活前的隐藏层单元值
- en: Now, we will pass the hidden layer values through a non-linear activation function.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过非线性激活函数传递隐藏层的值。
- en: '**Important note**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要提示**'
- en: If we do not apply a non-linear activation function in the hidden layer, the
    neural network becomes a giant linear connection from input to output, no matter
    how many hidden layers exist.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在隐藏层中不应用非线性激活函数，无论存在多少隐藏层，神经网络都会变成从输入到输出的巨大线性连接。
- en: Applying the activation function
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用激活函数
- en: 'Activation functions help in modeling complex relations between the input and
    the output. Some of the frequently used activation functions are calculated as
    follows (where *x* is the input):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有助于建模输入和输出之间的复杂关系。一些经常使用的激活函数如下计算（其中 *x* 是输入）：
- en: '![](img/B18457_01_0021.png)![](img/B18457_01_0031.png)![](img/B18457_01_0041.png)![](img/B18457_01_005.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_0021.png)![](img/B18457_01_0031.png)![](img/B18457_01_0041.png)![](img/B18457_01_005.png)'
- en: 'Visualizations of each of the preceding activations for various input values
    are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于各个先前激活的可视化，用于各种输入值的图形如下：
- en: '![Graphical user interface, diagram, line chart  Description automatically
    generated](img/B18457_01_11.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面、图表、线图说明](img/B18457_01_11.png)'
- en: 'Figure 1.11: Outputs of different activation functions for different input
    values'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：不同输入值的不同激活函数的输出
- en: 'For our example, let’s apply the sigmoid (logistic) activation, *S(x)*, to
    the three hidden layer *sums*. By doing so, we get the following values after
    sigmoid activation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的例子为例，让我们对三个隐藏层的*总和*应用sigmoid（逻辑）激活函数 *S(x)*。通过这样做，我们在sigmoid激活后得到以下数值：
- en: '![](img/B18457_01_006.png)![](img/B18457_01_007.png)![](img/B18457_01_008.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_006.png)![](img/B18457_01_007.png)![](img/B18457_01_008.png)'
- en: Now that we have obtained the hidden layer values after activation, in the next
    section, we will obtain the output layer values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了激活后的隐藏层数值，在接下来的部分中，我们将获得输出层数值。
- en: Calculating the output layer values
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算输出层的数值
- en: 'So far, we have calculated the final hidden layer values after applying the
    sigmoid activation. Using the hidden layer values after activation, and the weight
    values (which are randomly initialized in the first iteration), we will calculate
    the output value for our network:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经计算出了应用sigmoid激活后的最终隐藏层数值。使用激活后的隐藏层数值和权重值（在第一次迭代中随机初始化），我们将计算网络的输出值：
- en: '![](img/B18457_01_12.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_12.png)'
- en: 'Figure 1.12: Applying Sigmoid activation on hidden unit values'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：在隐藏单元值上应用Sigmoid激活
- en: 'We perform the sum of products of the hidden layer values and weight values
    to calculate the output value. Another reminder: we excluded the bias terms that
    need to be added at each unit (node), only to simplify our understanding of the
    working details of feedforward propagation and backpropagation for now and will
    include it while coding up feedforward propagation and backpropagation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行隐藏层值和权重值的乘积之和以计算输出值。另一个提醒：我们排除了需要在每个单元（节点）添加的偏差项，仅简化我们对前向传播和反向传播工作细节的理解，将在编写前向传播和反向传播时包含它：
- en: '![](img/B18457_01_009.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_009.png)'
- en: Because we started with a random set of weights, the value of the output node
    is very different from the target. In this case, the difference is *1.235* (remember,
    the target is 0). Next, let’s calculate the loss value associated with the network
    in its current state.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们从一组随机权重开始，输出节点的值与目标非常不同。在这种情况下，差异为 *1.235*（请记住，目标是0）。接下来，让我们计算与当前网络状态相关的损失值。
- en: Calculating loss values
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算损失值
- en: 'Loss values (alternatively called cost functions) are the values that we optimize
    for in a neural network. To understand how loss values get calculated, let’s look
    at two scenarios:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值（也称为成本函数）是我们在神经网络中优化的值。为了理解如何计算损失值，让我们看看两种情况：
- en: Continuous variable prediction
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续变量预测
- en: Categorical variable prediction
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类变量预测
- en: Calculating loss during continuous variable prediction
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算连续变量预测期间的损失
- en: 'Typically, when the variable is continuous, the loss value is calculated as
    the mean of the square of the difference in actual values and predictions – that
    is, we try to minimize the mean squared error by varying the weight values associated
    with the neural network. The mean squared error value is calculated as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在变量连续时，损失值计算为实际值和预测之间差的平方的平均值 —— 也就是说，我们通过改变与神经网络相关的权重值来最小化均方误差。均方误差值计算如下：
- en: '![](img/B18457_01_010.png)![](img/B18457_01_011.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_010.png)![](img/B18457_01_011.png)'
- en: In the preceding equation, ![](img/B18457_01_012.png) is the actual output.
    ![](img/B18457_01_013.png) is the prediction computed by the neural network ![](img/B18457_01_014.png)
    (whose weights are stored in the form of ![](img/B18457_01_015.png)), where its
    input is ![](img/B18457_01_016.png), and *m* is the number of rows in the dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，![](img/B18457_01_012.png) 是实际输出。![](img/B18457_01_013.png) 是由神经网络计算的预测
    ![](img/B18457_01_014.png)（其权重存储在形式为 ![](img/B18457_01_015.png) 的形式中），其输入为 ![](img/B18457_01_016.png)，*m*
    是数据集中的行数。
- en: The key takeaway should be the fact that for every unique set of weights, the
    neural network would predict a different loss and we need to find the golden set
    of weights for which the loss is zero (or, in realistic scenarios, as close to
    zero as possible).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点是对于每组唯一的权重，神经网络将预测不同的损失，我们需要找到使损失为零（或在现实场景中尽可能接近零）的黄金权重组合。
- en: 'In our example, let’s assume that the outcome that we are predicting is continuous.
    In that case, the loss function value is the mean squared error, which is calculated
    as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，假设我们正在预测的结果是连续的。在这种情况下，损失函数值是均方误差，计算方法如下：
- en: '![](img/B18457_01_017.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_017.png)'
- en: Now that we have calculated the loss value for a continuous variable, we will
    learn about calculating the loss value for a categorical variable.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出连续变量的损失值，接下来学习如何计算分类变量的损失值。
- en: Calculating loss during categorical variable prediction
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在预测分类变量时计算损失
- en: When the variable to predict is discrete (that is, there are only a few categories
    in the variable), we typically use a categorical cross-entropy loss function.
    When the variable to predict has two distinct values within it, the loss function
    is binary cross-entropy.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当要预测的变量是离散的（即变量中仅有少数类别时），通常使用分类交叉熵损失函数。当要预测的变量具有其中两个不同值时，损失函数为二元交叉熵。
- en: 'Binary cross-entropy is calculated as follows, where *y* is the actual value
    of the output, *p* is the predicted value of the output, and *m* is the total
    number of data points:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵的计算方法如下，其中*y*是输出的实际值，*p*是输出的预测值，*m*是数据点的总数：
- en: '![](img/B18457_01_018.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_018.png)'
- en: 'Categorical cross-entropy is calculated as follows, where *y* is the actual
    value of the output, *p* is the predicted value of the output, *m* is the total
    number of data points, and *C* is the total number of classes:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 分类交叉熵的计算方法如下，其中*y*是输出的实际值，*p*是输出的预测值，*m*是数据点的总数，*C*是类别的总数：
- en: '![](img/B18457_01_019.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_019.png)'
- en: 'A simple way of visualizing cross-entropy loss is to look at the prediction
    matrix itself. Say you are predicting five classes – Dog, Cat, Rat, Cow, and Hen
    – in an image recognition problem. The neural network would necessarily have five
    neurons in the last layer with softmax activation (more on softmax in the next
    section). This, it will be forced to predict a probability for every class, for
    every data point. Say there are five images and the prediction probabilities look
    like so (the highlighted cell in each row corresponds to the target class):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的可视化交叉熵损失的方法是查看预测矩阵本身。假设您在图像识别问题中预测五类——狗、猫、老鼠、牛和母鸡。神经网络的最后一层必须有五个神经元，采用softmax激活（关于softmax的更多信息请见下一节）。这样，它将被迫为每个数据点的每个类别预测一个概率。假设有五幅图像，并且预测的概率如下所示（每行中突出显示的单元格对应于目标类）：
- en: '![Table  Description automatically generated](img/B18457_01_13.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B18457_01_13.png)'
- en: 'Figure 1.13: Cross entropy loss calculation'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13：交叉熵损失计算
- en: Note that each row sums to 1\. In the first row, when the target is **Dog**
    and the prediction probability is **0.88**, the corresponding loss is **0.128**
    (which is the negative of the log of **0.88**). Similarly, other losses are computed.
    As you can see, the loss value is less when the probability of the correct class
    is high. As you know, the probabilities range between 0 and 1\. So, the minimum
    possible loss can be 0 (when the probability is 1) and the maximum loss can be
    infinity when the probability is 0.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每一行的总和为1。在第一行中，当目标为**Dog**且预测概率为**0.88**时，相应的损失为**0.128**（这是**0.88**的负对数）。类似地，计算其他损失。正如您所见，当正确类别的概率高时，损失值较低。正如您所知，概率在0到1之间变化。因此，最小可能的损失可以是0（当概率为1时），而最大损失可以是无穷大（当概率为0时）。
- en: The final loss within a dataset is the mean of all individual losses across
    all rows.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，最终的损失是所有行中所有个体损失的平均值。
- en: Now that we have a solid understanding of calculating mean squared error loss
    and cross-entropy loss, let’s get back to our toy example. Assuming our output
    is a continuous variable, we will learn how to minimize the loss value using backpropagation
    in a later section. We will update the weight values ![](img/B18457_01_015.png)
    (which were initialized randomly earlier) to minimize the loss (![](img/B18457_01_021.png)).
    But, before that, let’s first code feedforward propagation in Python using NumPy
    arrays to solidify our understanding of its working details.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对计算均方误差损失和交叉熵损失有了扎实的理解，让我们回到我们的示例中。假设我们的输出是一个连续变量，我们将在后面的章节中学习如何使用反向传播来最小化损失值。我们将更新之前随机初始化的权重值
    ![](img/B18457_01_015.png) 来最小化损失 (![](img/B18457_01_021.png))。但在此之前，让我们先用 Python
    中的 NumPy 数组编写前向传播代码，以加深对其工作原理的理解。
- en: Feedforward propagation in code
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写代码中的前向传播
- en: 'A high-level strategy for coding feedforward propagation is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '编写前向传播代码的高级策略如下:'
- en: Perform a sum product at each neuron.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元执行求和乘积。
- en: Compute activation.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算激活。
- en: Repeat the first two steps at each neuron until the output layer.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个神经元重复前两个步骤直到输出层。
- en: Compute the loss by comparing the prediction with the actual output.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将预测值与实际输出进行比较来计算损失。
- en: The feedforward function takes in input data, current neural network weights,
    and output data as the inputs and returns the loss of the current network state
    as output.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播函数接受输入数据、当前神经网络权重和输出数据作为输入，并返回当前网络状态的损失作为输出。
- en: The feedforward function to calculate the mean squared error loss values across
    all data points is available as `Feed_forward_propagation.ipynb` in the `Chapter01`
    folder of the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '用于计算所有数据点上均方误差损失值的前向传播函数在 GitHub 代码库的 `Chapter01` 文件夹中的 `Feed_forward_propagation.ipynb`
    文件中，链接地址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。 '
- en: 'We strongly encourage you to execute the code notebooks by clicking the **Open
    in Colab** button in each notebook. A sample screenshot is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈建议您点击每个笔记本中的 **在 Colab 中打开** 按钮来执行代码笔记本。示例如下：
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_01_14.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![电脑截图的描述，自动生成](img/B18457_01_14.png)'
- en: 'Figure 1.14: “Open in Colab” button in the notebooks on GitHub'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.14: GitHub 笔记本中的“在 Colab 中打开”按钮'
- en: Once you click on **Open in Colab**, you will be able to execute all the code
    without any hassle and should be able to replicate the results shown in this book.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦点击 **在 Colab 中打开**，您将能够轻松执行所有代码，并应能够复制本书中显示的结果。
- en: 'To make this exercise a little more realistic, we will have bias associated
    with each node. Thus, the weights array will contain not only the weights connecting
    different nodes but also the bias associated with nodes in hidden/output layers.
    With the way to execute code in place, let’s go ahead and code feedforward propagation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使这个练习更加现实，我们将使每个节点都有关联的偏置。因此，权重数组将包含连接不同节点的权重以及隐藏/输出层节点的偏置。现在可以执行代码了，让我们继续编写前向传播代码:'
- en: 'Take the input variable values (`inputs`), `weights` (randomly initialized
    if this is the first iteration), and the actual `outputs` in the provided dataset
    as the parameters of the `feed_forward` function:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将输入变量值 (`inputs`)、权重 (`weights`，如果这是第一次迭代则为随机初始化) 和提供的数据集中的实际输出 (`outputs`)
    作为 `feed_forward` 函数的参数:'
- en: '[PRE0]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Calculate hidden layer values by performing the matrix multiplication (`np.dot`)
    of `inputs` and weight values (`weights[0]`) connecting the input layer to the
    hidden layer and add the bias terms (`weights[1]`) associated with the hidden
    layer’s nodes:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '通过执行输入和连接输入层到隐藏层的权重值 (`weights[0]`) 的矩阵乘法 (`np.dot`) 并添加与隐藏层节点相关的偏置项 (`weights[1]`)
    来计算隐藏层的值:'
- en: '[PRE1]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Apply the sigmoid activation function on top of the hidden layer values obtained
    in the previous step – `pre_hidden`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在前一步骤中获得的隐藏层值 `pre_hidden` 上应用 Sigmoid 激活函数:'
- en: '[PRE2]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Calculate the output layer values by performing the matrix multiplication (`np.dot`)
    of hidden layer activation values (`hidden`) and weights connecting the hidden
    layer to the output layer (`weights[2]`) and summing the output with bias associated
    with the node in the output layer – `weights[3]`:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '通过执行隐藏层激活值 (`hidden`) 和连接隐藏层到输出层的权重 (`weights[2]`) 的矩阵乘法 (`np.dot`)，并将输出与与输出层节点相关的偏置
    `weights[3]` 相加来计算输出层的值:'
- en: '[PRE3]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Calculate the mean squared error value across the dataset and return the mean
    squared error:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据集上的均方误差值，并返回均方误差的均值：
- en: '[PRE4]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We are now able to get the mean squared error value as we forward-pass through
    the network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过网络进行前向传播，我们能够得到均方误差值。
- en: Before we learn about backpropagation, let’s learn about some constituents of
    the feedforward network that we built previously – the activation functions and
    loss value calculation – by implementing them in NumPy so that we have a detailed
    understanding of how they work.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习反向传播之前，让我们学习一些前馈网络的组成部分——激活函数和损失值计算——通过在NumPy中实现它们，以便我们深入了解它们的工作原理。
- en: Activation functions in code
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码中的激活函数
- en: 'While we applied the sigmoid activation on top of the hidden layer values in
    the preceding code, let’s examine other activation functions that are commonly
    used:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在前面的代码中对隐藏层值应用了sigmoid激活，让我们看看其他常用的激活函数：
- en: '**Tanh**: The tanh activation of a value (the hidden layer unit value) is calculated
    as follows:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tanh**：值（隐藏层单元值）的tanh激活计算如下：'
- en: '[PRE5]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**ReLU**: The **Rectified Linear Unit** (**ReLU**) of a value (the hidden layer
    unit value) is calculated as follows:'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU**：值（隐藏层单元值）的**修正线性单元**（**ReLU**）计算如下：'
- en: '[PRE6]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Linear**: The linear activation of a value is the value itself. This is also
    called “identity activation” or “no activation” and is rarely used. This is represented
    as follows:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：值的线性激活是值本身。这也称为“恒等激活”或“无激活”，很少使用。表示如下：'
- en: '[PRE7]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Softmax**: Unlike other activations, softmax is performed on top of an array
    of values. This is generally done to determine the probability of an input belonging
    to one of the *m* number of possible output classes in a given scenario. Let’s
    say we are trying to classify an image of a digit into one of the possible 10
    classes (numbers from 0 to 9).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：与其他激活函数不同，softmax是在一组值之上执行的。这通常用于确定在给定场景中输入属于*m*个可能输出类别中的一个的概率。假设我们试图将一幅数字的图像分类为可能的10个类别（从0到9）中的一个。'
- en: In this case, there are 10 output values, where each output value should represent
    the probability of an input image belonging to one of the 10 classes.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，有10个输出值，其中每个输出值应代表输入图像属于10个类别之一的概率。
- en: 'Softmax activation is used to provide a probability value for each class in
    the output and is calculated as follows:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax激活用于为输出中的每个类别提供概率值，计算如下：
- en: '[PRE8]'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that the two operations on top of input `x` – `np.exp` will make all
    values positive, and the division by `np.sum(np.exp(x))` of all such exponents
    will force all the values to be in between 0 and 1\. This range coincides with
    the probability of an event. And this is what we mean by returning a probability
    vector.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对输入`x`的两个操作——`np.exp`将使所有值变为正数，而通过`np.sum(np.exp(x))`对所有这些指数进行的除法将强制所有值位于0和1之间。这个范围与事件概率重合。这就是我们所说的返回概率向量的含义。
- en: Now that we have learned about various activation functions, next, we will learn
    about the different loss functions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了各种激活函数，接下来我们将学习不同的损失函数。
- en: Loss functions in code
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码中的损失函数
- en: 'Loss values (which are minimized during a neural network training process)
    are minimized by updating weight values. Defining the proper loss function is
    the key to building a working and reliable neural network model. The loss functions
    that are generally used while building a neural network are as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练过程中，通过更新权重值来最小化损失值（在代码中）。定义适当的损失函数是构建工作和可靠的神经网络模型的关键。通常在构建神经网络时使用的损失函数如下：
- en: '**Mean squared error**: The mean squared error is the squared difference between
    the actual and the predicted values of the output. We take a square of the error,
    as the error can be positive or negative (when the predicted value is greater
    than the actual value, and vice versa). Squaring ensures that positive and negative
    errors do not offset each other. We calculate the **mean** of the squared error
    so that the error over two different datasets is comparable when the datasets
    are not of the same size.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**：均方误差是输出的实际值和预测值之间的平方差。我们对误差取平方，因为误差可以是正或负（当预测值大于实际值或反之时）。平方确保正负误差不会互相抵消。我们计算平均值的平方误差，以便在数据集大小不同时比较两个不同数据集上的误差。'
- en: 'The mean squared error between an array of predicted output values (`p`) and
    an array of actual output values (`y`) is calculated as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 预测输出值数组 (`p`) 和实际输出值数组 (`y`) 之间的均方误差计算如下：
- en: '[PRE9]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The mean squared error is typically used when trying to predict a value that
    is continuous in nature.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 当试图预测具有连续性质的值时，通常使用均方误差。
- en: '**Mean absolute error:** The mean absolute error works in a manner that is
    very similar to the mean squared error. The mean absolute error ensures that positive
    and negative errors do not offset each other by taking an average of the absolute
    difference between the actual and predicted values across all data points.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均绝对误差：** 均绝对误差的工作方式与均方误差非常相似。均绝对误差通过计算所有数据点的实际值和预测值之间的绝对差的平均值，确保正负误差不会互相抵消。'
- en: 'The mean absolute error between an array of predicted output values (`p`) and
    an array of actual output values (`y`) is implemented as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 预测输出值数组 (`p`) 和实际输出值数组 (`y`) 之间的均绝对误差实现如下：
- en: '[PRE10]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Similar to the mean squared error, the mean absolute error is generally employed
    on continuous variables.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于均方误差，均绝对误差通常用于连续变量。
- en: '**Binary cross-entropy**: Cross-entropy is a measure of the difference between
    two different distributions: actual and predicted. Binary cross-entropy is applied
    to binary output data, unlike the previous two loss functions that we discussed
    (which are applied during continuous variable prediction).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元交叉熵**：交叉熵是衡量两个不同分布之间差异的指标：实际分布和预测分布。二元交叉熵适用于二进制输出数据，与我们讨论过的前两种损失函数（适用于连续变量预测）不同。'
- en: 'Binary cross-entropy between an array of predicted values (`p`) and an array
    of actual values (`y`) is implemented as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值数组 (`p`) 和实际值数组 (`y`) 之间的二元交叉熵实现如下：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that binary cross-entropy loss has a high value when the predicted value
    is far away from the actual value and a low value when the predicted and actual
    values are close.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当预测值远离实际值时，二元交叉熵损失值很高，而当预测值和实际值接近时，损失值很低。
- en: '**Categorical cross-entropy**: Categorical cross-entropy between an array of
    predicted values (`p`) and an array of actual values (`y`) is implemented as follows:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类交叉熵**：预测值数组 (`p`) 和实际值数组 (`y`) 之间的分类交叉熵实现如下：'
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So far, we have learned about feedforward propagation, and various components
    that constitute it, such as weight initialization, bias associated with nodes,
    and activation and loss functions. In the next section, we will learn about backpropagation,
    a technique to adjust weights so that they will result in a loss that is as small
    as possible.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了前向传播及其组成部分，如权重初始化、与节点相关的偏差以及激活和损失函数。在接下来的部分中，我们将学习反向传播，这是一种调整权重的技术，使得它们导致的损失尽可能小。
- en: Implementing backpropagation
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施反向传播
- en: In feedforward propagation, we connected the input layer to the hidden layer,
    which was then connected to the output layer. In the first iteration, we initialized
    weights randomly and then calculated the loss resulting from those weight values.
    In backpropagation, we take the reverse approach. We start with the loss value
    obtained in feedforward propagation and update the weights of the network in such
    a way that the loss value is minimized as much as possible.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，我们将输入层连接到隐藏层，然后连接到输出层。在第一次迭代中，我们随机初始化权重，然后计算由这些权重值导致的损失。在反向传播中，我们采取相反的方法。我们从前向传播中获得的损失值开始，并以这样的方式更新网络的权重，以使损失值尽可能小。
- en: 'The loss value is reduced as we perform the following steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下步骤减少损失值：
- en: Change each weight within the neural network by a small amount – one at a time.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过逐个地改变神经网络中的每个权重来更新权重。
- en: Measure the change in loss (![](img/B18457_01_022.png)) when the weight value
    is changed (![](img/B18457_01_023.png)).
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当改变权重值时 (![](img/B18457_01_023.png))，测量损失的变化 (![](img/B18457_01_022.png))。
- en: Update the weight by ![](img/B18457_01_024.png) , where *k* is a positive value
    and is a hyperparameter known as the **learning rate**.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重为 ![](img/B18457_01_024.png)，其中 *k* 是一个正值，是称为**学习率**的超参数。
- en: Note that the update made to a particular weight is proportional to the amount
    of loss that is reduced by changing it by a small amount. Intuitively, if changing
    a weight reduces the loss by a large value, then we can update the weight by a
    large amount. However, if the loss reduction is small by changing the weight,
    then we update it only by a small amount.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，对特定权重的更新与通过改变它稍微减少的损失量成比例。直观地说，如果改变一个权重减少了大量损失值，那么我们可以大幅度更新该权重。然而，如果通过改变权重减少的损失较小，那么我们只更新它一小部分。
- en: If the preceding steps are performed *n* number of times on the **entire** dataset
    (where we have done both the feedforward propagation and backpropagation), it
    essentially results in training for *n* **epochs**.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在**整个**数据集上（我们已经进行了前向传播和反向传播）执行了前面的步骤*n*次，这实质上就是训练*n*个**epochs**。
- en: 'As a typical neural network contains thousands/millions of weights, changing
    the value of each weight and checking whether the loss increased or decreased
    is not optimal. The core step in the preceding list is the measurement of change
    of loss when the weight is changed. As you might have studied in calculus, measuring
    this is the same as computing the **gradient** of loss concerning the weight.
    There’s more on leveraging partial derivatives from calculus to calculate the
    gradient of the loss concerning the weight in the next section, on the chain rule
    for backpropagation. In this section though, we will implement gradient descent
    from scratch by updating one weight at a time by a small amount, as detailed at
    the start of this section. However, before implementing backpropagation, let’s
    understand one additional detail of neural networks: the **learning rate**.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于典型神经网络包含成千上万的权重，改变每个权重的值并检查损失是否增加或减少并不是最优的做法。前述列表中的核心步骤是测量在改变权重时损失的变化。正如你在微积分中学习的那样，衡量这一点与计算损失关于权重的**梯度**相同。在下一节中，关于反向传播的链式法则将更多地利用微积分中的偏导数来计算损失关于权重的梯度。但在这一节之前，我们将通过稍微改变一次更新一个权重的方式来从头实现梯度下降。然而，在实施反向传播之前，让我们了解神经网络的一个额外细节：**学习率**。
- en: Intuitively, the learning rate helps in building trust in the algorithm. For
    example, when deciding on the magnitude of the weight update, we would potentially
    not change the weight value by a big amount in one go but update it more slowly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，学习率有助于建立对算法的信任。例如，在决定权重更新的大小时，我们可能不会一次性改变权重值很大的量，而是更慢地更新它。
- en: This results in obtaining stability in our model; we will look at how the learning
    rate helps with stability in the *Understanding the impact of the learning rate*
    section.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致我们的模型获得稳定性；我们将看看学习率如何在*理解学习率影响*部分中帮助稳定性。
- en: This whole process by which we update weights to reduce errors is called **gradient
    descent**. **Stochastic gradient descent** is how errors are minimized in the
    preceding scenario. As mentioned, **gradient** stands for the difference (which
    is the difference in loss values when the weight value is updated by a small amount)
    and **descent** means to reduce. Alternatively, gradient stands for the slope
    (direction of loss drop) and descent means to move toward lower loss. **Stochastic**
    stands for the selection of random samples based on which a decision is taken.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新权重以减少错误的整个过程被称为**梯度下降**。**随机梯度下降**是如何在前述场景中最小化错误的。如前所述，**梯度**表示差异（即当权重值稍微改变时损失值的差异），**下降**意味着减少。另一种解释是，梯度表示斜率（损失下降的方向），下降意味着向更低的损失移动。**随机**代表基于随机样本的选择，根据这些样本作出决策。
- en: Apart from stochastic gradient descent, many other similar optimizers help to
    minimize loss values; the different optimizers will be discussed in the next chapter.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机梯度下降外，还有许多类似的优化器帮助最小化损失值；不同的优化器将在下一章讨论。
- en: In the next two sections, we will learn about coding backpropagation from scratch
    in Python, and will also discuss, in brief, how backpropagation works using the
    chain rule.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，我们将学习如何在Python中从头开始编写反向传播，并简要讨论使用链式法则进行反向传播的工作原理。
- en: Gradient descent in code
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码中的梯度下降
- en: 'Gradient descent is implemented in Python as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降在Python中的实现如下：
- en: The following code is available as `Gradient_descent.ipynb` in the `Chapter01`
    folder of this book’s GitHub repository – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书的GitHub存储库的`Chapter01`文件夹中作为`Gradient_descent.ipynb`提供 - [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Define the feedforward network and calculate the mean squared error loss value,
    as we did in the *Feedforward propagation in code* section:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义前馈网络并计算均方误差损失值，就像我们在*代码中的前向传播*部分所做的那样：
- en: '[PRE13]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Increase each weight and bias value by a very small amount (0.0001) and calculate
    the overall squared error loss value one at a time for each of the weight and
    bias updates.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐个将每个权重和偏置值增加一个非常小的量（0.0001），并计算每个权重和偏置更新的总平方误差损失值。
- en: 'In the following code, we are creating a function named `update_weights`, which
    performs the gradient descent process to update weights. The inputs to the function
    are the input variables to the network – `inputs`, expected `outputs`, `weights`
    (which are randomly initialized at the start of training the model), and the learning
    rate of the model, `lr` (more on the learning rate in a later section):'
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在创建一个名为`update_weights`的函数，该函数执行梯度下降过程以更新权重。函数的输入是网络的输入变量`inputs`、期望输出`outputs`、权重（在训练模型开始时随机初始化）以及模型的学习率`lr`（关于学习率的更多信息将在后面的部分讨论）：
- en: '[PRE14]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Ensure that you deepcopy the list of weights. As the weights will be manipulated
    in later steps, `deepcopy` ensures we can work with multiple copies of weights
    without disturbing the original weight values. We will create three copies of
    the original set of weights that were passed as an input to the function – `original_weights`,
    `temp_weights`, and `updated_weights`:'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保深度复制权重列表。由于权重将在后续步骤中被操作，`deepcopy`确保我们可以使用多个副本的权重而不影响原始权重值。我们将创建三个原始权重集的副本作为函数的输入
    - `original_weights`、`temp_weights`和`updated_weights`：
- en: '[PRE15]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Calculate the loss value (original_loss) with the original set of weights by
    passing inputs, outputs, and original_weights through the feed_forward function:'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将输入、输出和原始权重通过前馈函数传递，计算损失值（original_loss）：
- en: '[PRE16]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will loop through all the layers of the network:'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将遍历网络的所有层：
- en: '[PRE17]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There are a total of four lists of parameters within our neural network – two
    lists for the weight and bias parameters that connect the input to the hidden
    layer and another two lists for the weight and bias parameters that connect the
    hidden layer to the output layer. Now, we loop through all the individual parameters
    and, because each list has a different shape, we leverage `np.ndenumerate` to
    loop through each parameter within a given list:'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的神经网络中有四个参数列表 - 两个用于连接输入到隐藏层的权重和偏置参数列表，另外两个用于连接隐藏层到输出层的权重和偏置参数列表。现在，我们遍历所有的单个参数，并且由于每个列表具有不同的形状，我们利用`np.ndenumerate`来遍历给定列表中的每个参数：
- en: '[PRE18]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we store the original set of weights in `temp_weights`. We select its index
    weight present in the ith layer and increase it by a small value. Finally, we
    compute the new loss with the new set of weights for the neural network:'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将原始权重集存储在`temp_weights`中。我们选择第i层中存在的权重的索引，并增加一个小的值。最后，我们使用神经网络的新权重集计算新的损失：
- en: '[PRE19]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the first line of the preceding code, we reset `temp_weights` to the original
    set of weights as, in each iteration, we update a different parameter to calculate
    the loss when a parameter is updated by a small amount within a given epoch.
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前述代码的第一行中，我们将`temp_weights`重置为原始权重集，因为在每次迭代中，当参数在给定epoch内以微小量更新时，我们会更新不同的参数以计算损失。
- en: 'We calculate the gradient (change in loss value) due to the weight change:'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算由于权重改变而产生的梯度（损失值的变化）：
- en: '[PRE20]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This process of updating a parameter by a very small amount and then calculating
    the gradient is equivalent to the process of differentiation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个通过微小量更新参数然后计算梯度的过程相当于微分的过程。
- en: 'Finally, we update the parameter present in the corresponding ith layer and
    `index` of `updated_`weights. The updated weight value will be reduced in proportion
    to the value of the gradient. Furthermore, instead of completely reducing it by
    a value equal to the gradient value, we bring in a mechanism to build trust slowly
    by using the learning rate – `lr` (more on learning rate in the *Understanding
    the impact of the learning rate* section):'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们更新对应第i层和`index`的参数。更新后的权重值将按照梯度值的比例减少。此外，我们不完全通过减去梯度值来快速减少权重值，而是通过使用学习率`lr`逐步建立信任（有关学习率的更多信息，请参阅*理解学习率影响*部分）：
- en: '[PRE21]'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once the parameter values across all layers and indices within layers are updated,
    we return the updated weight values – `updated_weights`:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦跨所有层和层内索引的参数值更新完成，我们返回更新后的权重值 - `updated_weights`：
- en: '[PRE22]'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE22]'
- en: One of the other parameters in a neural network is the **batch size** considered
    in calculating the loss values.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的另一个参数是用于计算损失值的**批量大小**。
- en: In the preceding scenario, we considered all the data points to calculate the
    loss (mean squared error) value. However, in practice, when we have thousands
    (or in some cases, millions) of data points, the incremental contribution of a
    greater number of data points while calculating the loss value would follow the
    law of diminishing returns, and hence we would be using a batch size that is much
    smaller compared to the total number of data points we have. We will apply gradient
    descent (after feedforward propagation) using one **batch** at a time until we
    exhaust all data points within **one epoch of training**. The typical batch size
    considered in building a model is anywhere between 32 and 1,024\. It’s usually
    a power of 2, and for very, very large models, depending on the scenario, the
    batch size can be less than 32.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的场景中，我们考虑了所有数据点来计算损失（均方误差）值。然而，在实际操作中，当我们有成千上万（或在某些情况下，百万级）的数据点时，计算损失值时更多数据点的增量贡献会遵循边际收益递减法则，因此我们会使用远小于总数据点数的批量大小。我们将应用梯度下降（在前馈传播之后）每次使用一个**批次**，直到我们在**一轮训练的一个时期**内耗尽所有数据点。在构建模型时考虑的典型批次大小介于32和1,024之间。通常是2的幂，并且对于非常大的模型，根据情况，批量大小可以小于32。
- en: Implementing backpropagation using the chain rule
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用链式法则实现反向传播
- en: So far, we have calculated gradients of loss concerning weight by updating the
    weight by a small amount and then calculating the difference between the feedforward
    loss in the original scenario (when the weight was unchanged) and the feedforward
    loss after updating weights. One drawback of updating weight values in this manner
    is that when the network is large (with more weights to update), a large number
    of computations are needed to calculate loss values (and in fact, the computations
    are to be done twice – once where weight values are unchanged, and again, where
    weight values are updated by a small amount). This results in more computations
    and hence requires more resources and time. In this section, we will learn about
    leveraging the chain rule, which does not require us to manually compute loss
    values to come up with the gradient of the loss concerning the weight value.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过更新权重值一小部分来计算与权重相关的损失的梯度，然后计算在原始情景（权重未更改时）和在更新权重后的前向传播损失之间的差异。以这种方式更新权重值的一个缺点是，当网络很大（有更多权重需要更新）时，需要大量计算来计算损失值（实际上，需要两次计算
    - 一次是权重值未更改时，再次是权重值稍微更新后）。这导致更多的计算，因此需要更多的资源和时间。在本节中，我们将学习如何利用链式法则，它不需要我们手动计算损失值即可得出损失对权重值的梯度。
- en: 'In the first iteration (where we initialized weights randomly), the predicted
    value of the output is 1.235\. To get the theoretical formulation, let’s denote
    the weights and hidden layer values and hidden layer activations as *w*, *h*,
    and *a*, respectively, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中（我们随机初始化权重时），输出的预测值为1.235。为了得到理论公式，让我们将权重和隐藏层值及隐藏层激活表示为*w*、*h*和*a*，如下所示：
- en: '![Diagram  Description automatically generated](img/B18457_01_15.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18457_01_15.png)'
- en: 'Figure 1.15: Generalizing the weight initialization process'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15：泛化权重初始化过程
- en: Note that, in the preceding diagrams, we have taken each component value of
    the left diagram and generalized it in the diagram on the right.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图表中，我们已经将左图的每个组件值概括在右图中。
- en: To keep it easy to comprehend, in this section, we will understand how to use
    the chain rule to compute the gradient of loss value with respect to only w[11].
    The same learning can be extended to all the weights and biases of the neural
    network. We encourage you to practice and apply the chain rule calculation to
    the rest of the weights and bias values. Additionally, to keep this simple for
    our learning purposes, we will be working on only one data point, where the input
    is {1,1} and the expected output is {0}.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化理解，在本节中，我们将了解如何使用链规则计算损失值相对于仅 w[11] 的梯度。这种学习方法可以扩展到神经网络的所有权重和偏差。我们鼓励您练习并应用链规则计算到其余的权重和偏差值。此外，为了我们学习的目的简化，我们将仅处理一个数据点，其中输入为
    {1,1}，预期输出为 {0}。
- en: The `chain_rule.ipynb` notebook in the `Chapter01` folder of this book’s GitHub
    repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e) contains the way
    to calculate gradients with respect to changes in weights and biases for all the
    parameters in a network using the chain rule.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的GitHub存储库中`Chapter01`文件夹中的`chain_rule.ipynb`笔记本包含使用链规则计算网络中所有参数的权重和偏差变化的梯度的方法，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Given that we are calculating the gradient of loss value with w[11], let’s
    understand all the intermediate components that are to be included while calculating
    the gradient through the following diagram (the components that do not connect
    the output to w[11] are grayed out in the following diagram):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们正在计算损失值相对于 w[11] 的梯度，请让我们通过以下图表理解所有需要包含的中间组件（在以下图表中，未连接输出到 w[11] 的组件已被标为灰色）：
- en: '![Diagram  Description automatically generated](img/B18457_01_16.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18457_01_16.png)'
- en: 'Figure 1.16: Highlighting the values (h[11], a[11], ŷ) that are needed to calculate
    the gradient of loss w.r.t w[11]'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：突出显示需要计算损失梯度的值（h[11], a[11], ŷ）
- en: From the preceding diagram, we can see that w[11] is contributing to the loss
    value through the path that is highlighted, – ![](img/B18457_01_025.png), ![](img/B18457_01_026.png),
    and ![](img/B18457_01_027.png). Let’s formulate how ![](img/B18457_01_025.png),
    ![](img/B18457_01_026.png), and ![](img/B18457_01_027.png) are obtained individually.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图表中，我们可以看到w[11]通过突出显示的路径对损失值有贡献 – ![](img/B18457_01_025.png), ![](img/B18457_01_026.png),
    和 ![](img/B18457_01_027.png)。让我们详细说明 ![](img/B18457_01_025.png), ![](img/B18457_01_026.png),
    和 ![](img/B18457_01_027.png) 是如何分别获得的。
- en: 'The loss value of the network is represented as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的损失值表示如下：
- en: '![](img/B18457_01_031.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_031.png)'
- en: 'The predicted output value ![](img/B18457_01_027.png) is calculated as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 预测输出值 ![](img/B18457_01_027.png) 计算如下：
- en: '![](img/B18457_01_033.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_033.png)'
- en: 'The hidden layer activation value (sigmoid activation) is calculated as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层激活值（sigmoid 激活）计算如下：
- en: '![](img/B18457_01_034.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_034.png)'
- en: 'The hidden layer value is calculated as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层值计算如下：
- en: '![](img/B18457_01_035.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_035.png)'
- en: 'Now that we have formulated all the equations, let’s calculate the impact of
    the change in the loss value (*C*) with respect to the change in weight ![](img/B18457_01_036.png),
    as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经制定了所有方程，请计算损失值 (*C*) 相对于权重 ![](img/B18457_01_036.png) 的变化的影响。
- en: '![](img/B18457_01_037.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_037.png)'
- en: This is called a **chain rule**. Essentially, we are performing a chain of differentiations
    to fetch the differentiation of our interest.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的**链规则**。本质上，我们正在执行一系列不同的微分以获取我们感兴趣的微分值。
- en: Note that, in the preceding equation, we have built a chain of partial differential
    equations in such a way that we are now able to perform partial differentiation
    on each of the four components individually and, ultimately, calculate the derivative
    of the loss value with respect to the weight value ![](img/B18457_01_036.png).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述方程中，我们已经建立了一系列偏微分方程，使我们能够逐个计算这四个组件的偏导数，并最终计算出损失值相对于权重值 ![](img/B18457_01_036.png)
    的导数。
- en: 'The individual partial derivatives in the preceding equation are computed as
    follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程中的各个偏导数计算如下：
- en: 'The partial derivative of the loss value with respect to the predicted output
    value ![](img/B18457_01_027.png) is as follows:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失值对预测输出值 ![](img/B18457_01_027.png) 的偏导数如下所示：
- en: '![](img/B18457_01_040.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_040.png)'
- en: 'The partial derivative of the predicted output value ![](img/B18457_01_027.png)
    with respect to the hidden layer activation value ![](img/B18457_01_026.png) is
    as follows:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测输出值 ![](img/B18457_01_027.png) 对隐藏层激活值 ![](img/B18457_01_026.png) 的偏导数如下所示：
- en: '![](img/B18457_01_043.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_043.png)'
- en: 'The partial derivative of the hidden layer activation value ![](img/B18457_01_026.png)
    with respect to the hidden layer value prior to activation ![](img/B18457_01_025.png)
    is as follows:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏层激活值 ![](img/B18457_01_026.png) 对隐藏层激活前数值 ![](img/B18457_01_025.png) 的偏导数如下所示：
- en: '![](img/B18457_01_046.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_046.png)'
- en: 'Note that the preceding equation comes from the fact that the derivative of
    the sigmoid function ![](img/B18457_01_047.png) is as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述方程来自于 Sigmoid 函数的导数 ![](img/B18457_01_047.png) 如下所示：
- en: '![](img/B18457_01_048.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_048.png)'
- en: 'The partial derivative of the hidden layer value prior to activation ![](img/B18457_01_025.png)
    with respect to the weight value ![](img/B18457_01_036.png) is as follows:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏层激活前数值对权重值 ![](img/B18457_01_036.png) 的偏导数如下所示：
- en: '![](img/B18457_01_051.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_051.png)'
- en: 'With the calculation of individual partial derivatives in place, the gradient
    of the loss value with respect to ![](img/B18457_01_036.png) is calculated by
    replacing each of the partial differentiation terms with the corresponding value,
    as calculated in the previous steps, as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在各个偏导数计算完毕后，损失值相对于 ![](img/B18457_01_036.png) 的梯度通过用前面步骤中计算的相应值替换每个偏导数项来计算，如下所示：
- en: '![](img/B18457_01_053.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_053.png)'
- en: From the preceding formula, we can see that we are now able to calculate the
    impact on the loss value of a small change in the weight value (the gradient of
    the loss with respect to weight) without brute-forcing our way by recomputing
    the feedforward propagation again.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述公式可以看出，我们现在能够计算在权重值略微变化时对损失值的影响（即损失相对于权重的梯度），而无需通过重新计算前向传播来进行蛮力计算。
- en: 'Next, we will go ahead and update the weight value as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将按以下方式更新权重值：
- en: '![](img/B18457_01_054.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_054.png)'
- en: Working versions of the two methods 1) identifying gradients using the chain
    rule and then updating weights, and 2) updating weight values by learning the
    impact a small change in weight value can have on the loss values, resulting in
    the same values for updated weight values, are provided in the notebook `Chain_rule.ipynb`
    in the `Chapter01` folder of this book’s GitHub repository – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法的工作版本：1）使用链式法则识别梯度，然后更新权重；2）学习权重微小变化对损失值影响的更新权重值，在本书的 GitHub 仓库的 `Chapter01`
    文件夹的笔记本 `Chain_rule.ipynb` 中提供相同的更新权重值，链接为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: In gradient descent, we performed the weight update process sequentially (one
    weight at a time). By leveraging the chain rule, we learned that there is an alternative
    way to calculate the impact of a change in weight by a small amount on the loss
    value, however, with an opportunity to perform computations in parallel.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降中，我们按顺序执行权重更新过程（逐个权重）。通过利用链式法则，我们学到了另一种计算权重值微小变化对损失值影响的替代方法，同时有机会并行计算。
- en: Because we are updating parameters across all layers, the whole process of updating
    parameters can be parallelized. Furthermore, given that in a realistic scenario,
    there can exist millions of parameters across layers, performing the calculation
    for each parameter on a different core of GPU results in the time taken to update
    weights is a much faster exercise than looping through each weight one at a time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在所有层上更新参数，整个参数更新过程可以并行化。此外，考虑到在实际场景中可能存在数百万个层参数，将每个参数在 GPU 的不同核心上进行计算，更新权重的时间比逐个权重循环的速度更快。
- en: Now that we have a solid understanding of backpropagation, both from an intuition
    perspective and also by leveraging the chain rule, let’s learn about how feedforward
    and backpropagation work together to arrive at the optimal weight values.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对反向传播有了坚实的理解，不仅从直觉上，还通过利用链式法则，让我们学习一下如何使前向传播和反向传播共同工作，以获得最佳的权重值。
- en: Putting feedforward propagation and backpropagation together
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将前向传播和反向传播结合起来
- en: In this section, we will build a simple neural network with a hidden layer that
    connects the input to the output on the same toy dataset that we worked on in
    the *Feedforward propagation in code* section and also leverage the `update_weights`
    function that we defined in the previous section to perform backpropagation to
    obtain the optimal weight and bias values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在与*代码中的前向传播*部分中使用的玩具数据集相同的简单神经网络上建立一个隐藏层，并利用我们在前一节中定义的`update_weights`函数执行反向传播以获得最佳的权重和偏差值。
- en: Note that we are not leveraging the chain rule, only to give you a solid understanding
    of the basics of forward and back-propagation. Starting in the next chapter, you
    will not be performing neural network training in this manner.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有利用链式法则，只是为了让您对前向和反向传播的基础有坚实的理解。从下一章开始，您将不会以这种方式执行神经网络训练。
- en: 'We define the model as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下方式定义模型：
- en: The input is connected to a hidden layer that has three units/ nodes.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入连接到具有三个单元/节点的隐藏层。
- en: The hidden layer is connected to the output, which has one unit in the output
    layer.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 隐藏层连接到具有一个输出层中的单元。
- en: The following code is available as `Back_propagation.ipynb` in the `Chapter01`
    folder of this book’s GitHub repository – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书的GitHub存储库`Chapter01`文件夹中作为`Back_propagation.ipynb`提供 – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'We will create the network as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下方式创建网络：
- en: 'Import the relevant packages and define the dataset:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并定义数据集：
- en: '[PRE23]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Initialize the weight and bias values randomly.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重和偏差值。
- en: 'The hidden layer has three units in it and each input node is connected to
    each of the hidden layer units. Hence, there are a total of six weight values
    and three bias values – one bias and two weights (two weights coming from two
    input nodes) corresponding to each of the hidden units. Additionally, the final
    layer has one unit that is connected to the three units of the hidden layer. Hence,
    a total of three weights and one bias dictate the value of the output layer. The
    randomly initialized weights are as follows:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐藏层中有三个单元，并且每个输入节点连接到隐藏层单元中的每一个。因此，总共有六个权重值和三个偏差值 – 每个隐藏单元对应一个偏差和两个权重（两个权重来自两个输入节点）。此外，最终层有一个单元连接到隐藏层的三个单元。因此，总共有三个权重和一个偏差决定输出层的值。随机初始化的权重如下：
- en: '[PRE24]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding code, the first array of parameters corresponds to the 2 x
    3 matrix of weights that connect the input layer to the hidden layer. The second
    array of parameters represents the bias values associated with each node of the
    hidden layer. The third array of parameters corresponds to the 3 x 1 matrix of
    weights joining the hidden layer to the output layer, and the final array of parameters
    represents the bias associated with the output layer.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，第一个参数数组对应于连接输入层到隐藏层的2 x 3权重矩阵。第二个参数数组代表与隐藏层每个节点相关联的偏差值。第三个参数数组对应于连接隐藏层到输出层的3
    x 1权重矩阵，最后一个参数数组表示与输出层相关联的偏差。
- en: 'Run the neural network through 100 epochs of feedforward propagation and backpropagation
    – the functions of which were already learned and defined as `feed_forward` and
    `update_weights` functions in the previous sections:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将神经网络通过100次前向传播和反向传播的时期 – 函数已经学习并在前面的章节中定义为`feed_forward`和`update_weights`函数：
- en: 'Define the `feed_forward` function:'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`feed_forward`函数：
- en: '[PRE25]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define the `update_weights` function (we will learn more about the learning
    rate *lr* in the next section):'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`update_weights`函数（我们将在下一节详细了解学习率*lr*）：
- en: '[PRE26]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Update weights over 100 epochs and fetch the loss value and the updated weight
    values:'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在100个时期内更新权重并获取损失值和更新后的权重值：
- en: '[PRE27]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the loss values:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制损失值图表：
- en: '[PRE28]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding code generates the following plot:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下图表：
- en: '![Chart  Description automatically generated](img/B18457_01_17.png)'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18457_01_17.png)'
- en: 'Figure 1.17: Loss value over increasing epochs'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.17：随着 epochs 增加，损失值的变化情况
- en: 'As you can see, the loss started at around 0.33 and steadily dropped to around
    0.0001\. This is an indication that weights are adjusted according to the input-output
    data, and when an input is given, we can expect it to predict the output that
    we have been comparing it against in the loss function. The output weights are
    as follows:'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你所看到的，损失从约 0.33 开始稳定下降到约 0.0001。这表明权重根据输入输出数据进行调整，当给定输入时，我们可以预期它预测与损失函数中进行比较的输出。输出权重如下：
- en: '[PRE29]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The PyTorch version of the same code with the same weights is demonstrated in
    the file `Auto_gradient_of_tensors.ipynb` in the `Chapter02` folder in the GitHub
    repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e). Revisit this section
    after understanding the core PyTorch concepts in the next chapter. Verify for
    yourself that the input and output are indeed the same regardless of whether the
    network is written in NumPy or PyTorch.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在理解核心 PyTorch 概念之后，可以在 GitHub 仓库的 `Chapter02` 文件夹中的 `Auto_gradient_of_tensors.ipynb`
    文件中演示相同权重的 PyTorch 版本的相同代码。重新访问本节，验证无论网络是用 NumPy 还是 PyTorch 编写，输入和输出都是相同的。
- en: Building a network from scratch using NumPy arrays, while sub-optimal, is done
    in this chapter to give you a solid foundation in the working details of neural
    networks.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 NumPy 数组中从头开始构建网络，虽然不是最佳选择，但在本章中是为了让您对神经网络的工作细节有坚实的基础。
- en: 'Once we have the updated weights, make the predictions for the input by passing
    the input through the network and calculate the output value:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了更新的权重，通过网络传递输入来进行预测，并计算输出值：
- en: '[PRE30]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output of the preceding code is the value of `-0.017`, which is a value
    that is very close to the expected output of 0\. As we train for more epochs,
    the `pred_out` value gets even closer to 0.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出是 `-0.017` 的值，这个值非常接近预期的输出值 0\. 随着 epochs 的增加，`pred_out` 的值甚至更接近 0。
- en: So far, we have learned about feedforward propagation and backpropagation. The
    key piece in the `update_weights` function that we defined here is the learning
    rate, which we will learn about in the next section.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了前向传播和反向传播。在这里定义的 `update_weights` 函数的关键部分是学习率，我们将在下一节中学习它。
- en: Understanding the impact of the learning rate
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解学习率的影响
- en: 'In order to understand how the learning rate impacts the training of a model,
    let’s consider a very simple case in which we try to fit the following equation
    (note that the following equation is different from the toy dataset that we have
    been working on so far):'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解学习率如何影响模型的训练，让我们考虑一个非常简单的情况，我们尝试拟合以下方程（请注意，以下方程不同于我们到目前为止正在处理的玩具数据集）：
- en: '![](img/B18457_01_055.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_01_055.png)'
- en: 'Note that *y* is the output and *x* is the input. With a set of input and expected
    output values, we will try and fit the equation with varying learning rates to
    understand the impact of the learning rate:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*y* 是输出，*x* 是输入。通过一组输入和预期输出值，我们将尝试使用不同的学习率拟合方程，以了解学习率的影响：
- en: The following code is available as `Learning_rate.ipynb` in the `Chapter01`
    folder of this book’s GitHub repository – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可在本书 GitHub 仓库的 `Chapter01` 文件夹中的 `Learning_rate.ipynb` 中找到 – [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Specify the input and output dataset as follows:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定输入和输出数据集如下：
- en: '[PRE31]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Define the `feed_forward` function. Furthermore, in this instance, we will modify
    the network in such a way that we do not have a hidden layer and the architecture
    is as follows:![](img/B18457_01_056.png)
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `feed_forward` 函数。此外，在这个例子中，我们将修改网络，使其没有隐藏层，并且架构如下:![](img/B18457_01_056.png)
- en: 'Note that, in the preceding function, we are estimating the parameters *w*
    and *b*:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在上述函数中，我们正在估计参数 *w* 和 *b*：
- en: '[PRE32]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the `update_weights` function just like we defined it in the *Gradient
    descent in code* section:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `update_weights` 函数，就像我们在 *梯度下降的代码* 部分中定义的那样：
- en: '[PRE33]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Initialize weight and bias values to a random value:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重和偏置值为随机值：
- en: '[PRE34]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that the weight and bias values are randomly initialized to values of 0\.
    Furthermore, the shape of the input weight value is 1 x 1, as the shape of each
    data point in the input is 1 x 1 and the shape of the bias value is 1 x 1 (as
    there is only one node in the output and each output has one value).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，权重和偏置值随机初始化为 0 值。此外，输入权重值的形状为 1 x 1，因为输入中每个数据点的形状为 1 x 1，而偏置值的形状为 1 x 1（因为输出中只有一个节点，每个输出只有一个值）。
- en: 'Let’s leverage the `update_weights` function with a learning rate of 0.01,
    loop through 1,000 iterations, and check how the weight value (`W`) varies over
    increasing epochs:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们利用学习率为 0.01 的`update_weights`函数，循环执行 1,000 次迭代，并检查权重值（`W`）随 epoch 增加的变化情况：
- en: '[PRE35]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that, in the preceding code, we are using a learning rate of 0.01 and repeating
    the `update_weights` function to fetch the modified weight at the end of each
    epoch. Further, in each epoch, we gave the most recent updated weight as an input
    to fetch the updated weight in the next epoch.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述代码中，我们使用学习率为 0.01，并重复`update_weights`函数以在每个 epoch 结束时获取修改后的权重。此外，在每个
    epoch 中，我们将最近更新的权重作为输入，以获取下一个 epoch 中的更新后的权重。
- en: 'Plot the value of the weight parameter at the end of each epoch:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制每个 epoch 结束时的权重参数值：
- en: '[PRE36]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code results in a variation in the weight value over increasing
    epochs as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码导致权重值随 epoch 增加而变化如下：
- en: '![Chart  Description automatically generated](img/B18457_01_18.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表说明](img/B18457_01_18.png)'
- en: 'Figure 1.18: Weight value over increasing epochs when learning rate is 0.01'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18：学习率为 0.01 时随着 epoch 增加的权重值
- en: Note that, in the preceding output, the weight value gradually increased in
    the right direction and then saturated at the optimal value of ~3.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述输出中，权重值逐渐向右方向增加，然后饱和在约为 3 的最佳值。
- en: To understand the impact of the value of the learning rate on arriving at the
    optimal weight values, let’s understand how the weight value varies over increasing
    epochs when the learning rate is 0.1 and when the learning rate is 1.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解学习率对到达最佳权重值的影响，让我们了解在学习率为 0.1 和学习率为 1 时，当增加 epoch 时权重值如何变化。
- en: 'The following charts are obtained when we modify the corresponding learning
    rate value in *step 5* and execute *step 6* (the code to generate the following
    charts is the same as the code we learned earlier, with a change in the learning
    rate value, and is available in the associated notebook in GitHub):'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们修改相应的学习率值在*步骤 5*中并执行*步骤 6*时，获得以下图表：
- en: '![Chart  Description automatically generated](img/B18457_01_19.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表说明](img/B18457_01_19.png)'
- en: 'Figure 1.19: (Left) Weight value over increasing epochs when learning rate
    is 0.1 (Right) Weight value over increasing epochs when learning rate is 1'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19：（左）学习率为 0.1 时随着 epoch 增加的权重值（右）学习率为 1 时随着 epoch 增加的权重值
- en: Notice that when the learning rate was very small (0.01), the weight value moved
    slowly (over a higher number of epochs) toward the optimal value. However, with
    a slightly higher learning rate (0.1), the weight value oscillated initially and
    then quickly saturated (in fewer epochs) to the optimal value. Finally, when the
    learning rate was high (1), the weight value spiked to a very high value and was
    not able to reach the optimal value.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当学习率非常小时（0.01），权重值缓慢移动（在较多的 epoch 中）向最佳值靠近。然而，稍高的学习率（0.1）时，权重值起初振荡，然后迅速饱和（在较少的
    epoch 中）到达最佳值。最后，当学习率很高（1）时，权重值急剧上升到非常高的值，并且无法达到最佳值。
- en: The reason the weight value did not spike by a large amount when the learning
    rate was low is that we restricted the weight update by an amount that was equal
    to the *gradient * learning rate*, essentially resulting in a small amount of
    weight update when the learning rate was small. However, when the learning rate
    was high, the weight update was high, after which the change in loss (when the
    weight was updated by a small value) was so small that the weight could not achieve
    the optimal value.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习率较低时，权重值没有大幅波动的原因在于我们通过*梯度 *学习率*限制了权重更新的幅度，从本质上讲，当学习率较小时，权重更新量较小。然而，当学习率较高时，权重更新量较大，在权重更新量较小时，损失的变化很小，以至于权重无法达到最佳值。
- en: 'To have a deeper understanding of the interplay between the gradient value,
    learning rate, and weight value, let’s run the `update_weights` function only
    for 10 epochs. Furthermore, we will print the following values to understand how
    they vary over increasing epochs:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解梯度值、学习速率和权重值之间的相互作用，让我们仅运行`update_weights`函数10个周期。此外，我们将打印以下数值以了解它们随着周期增加的变化：
- en: Weight value at the start of each epoch
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个周期开始时的权重值
- en: Loss prior to weight update
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重更新前的损失
- en: Loss when the weight is updated by a small amount
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当权重稍作调整时的损失
- en: Gradient value
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度值
- en: 'We modify the `update_weights` function to print the preceding values as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改`update_weights`函数，打印如下前述数值：
- en: '[PRE37]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The lines highlighted in bold font in the preceding code are where we modified
    the `update_weights` function from the previous section, where, first, we are
    checking whether we are currently working on the weight parameter by checking
    if (`i % 2 == 0`) as the other parameter corresponds to the bias value, and then
    we are printing the original weight value (`original_weights[i][index]`), loss
    (`org_loss`), updated loss value (`_loss_plus`), gradient (`grad`), and the resulting
    updated weight value (`updated_weights`).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中加粗显示的行是我们从前一节中修改`update_weights`函数的地方，首先我们通过检查 (`i % 2 == 0`) 来确认当前是否在处理权重参数，因为另一个参数对应于偏差值，然后我们打印原始权重值
    (`original_weights[i][index]`)、损失值 (`org_loss`)、更新后的损失值 (`_loss_plus`)、梯度 (`grad`)
    和更新后的权重值 (`updated_weights`)。
- en: Let’s now understand how the preceding values vary over increasing epochs across
    the three different learning rates that we are considering.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解在我们考虑的三种不同学习速率下，随着周期增加，前述数值如何变化。
- en: Learning rate of 0.01
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习速率为0.01的情景
- en: 'We will check the values using the following code:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码检查数值：
- en: '[PRE38]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code results in the following output:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![A picture containing line, plot, diagram, slope  Description automatically
    generated](img/B18457_01_20.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![包含线条、图表、图示、斜率的图片 自动生成的描述](img/B18457_01_20.png)'
- en: 'Figure 1.20: Weight & Loss values over increasing epochs when learning rate
    is 0.01'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.20：当学习速率为0.01时，随着周期增加权重和损失值的变化
- en: Note that, when the learning rate was 0.01, the loss value decreased slowly,
    and also the weight value updated slowly toward the optimal value. Let’s now understand
    how the preceding varies when the learning rate is 0.1.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当学习速率为0.01时，损失值下降缓慢，权重值向最优值更新速度也慢。现在让我们了解学习速率为0.1时的前述变化。
- en: Learning rate of 0.1
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习速率为0.1
- en: 'The code remains the same as in the learning rate of 0.01 scenario; however,
    the learning rate parameter would be 0.1 in this scenario. The output of running
    the same code with the changed learning rate parameter value is as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与学习速率为0.01的情景保持一致；然而，在这种情况下，学习速率参数将为0.1。更改学习速率参数值后运行相同代码的输出如下：
- en: '![A picture containing line, plot, diagram, slope  Description automatically
    generated](img/B18457_01_21.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![包含线条、图表、图示、斜率的图片 自动生成的描述](img/B18457_01_21.png)'
- en: 'Figure 1.21: Weight & loss values over increasing epochs when learning rate
    is 0.1'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.21：当学习速率为0.1时，随着周期增加权重和损失值的变化
- en: 'Let’s contrast the learning rate scenarios of 0.01 and 0.1 – the major difference
    between the two is as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对比学习速率为0.01和0.1的情景——两者之间的主要区别如下：
- en: '*When the learning rate was 0.01, the weight updated much slower when compared
    to a learning rate of 0.1 (from 0 to 0.45 in the first epoch when the learning
    rate was 0.01, to 4.5 when the learning rate was 0.1). The reason for the slower
    update is the lower learning rate as the weight is updated by the gradient times
    the learning rate.*'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '*当学习速率为0.01时，与学习速率为0.1相比，权重更新速度较慢（在第一个周期中，学习速率为0.01时从0更新到0.45，而学习速率为0.1时更新到4.5）。更新速度较慢的原因是学习速率较低，因为权重是通过梯度乘以学习速率进行更新。*'
- en: In addition to the weight update magnitude, we should note the direction of
    the weight update. *The gradient is negative when the weight value is smaller
    than the optimal value and it is positive when the weight value is larger than
    the optimal value. This phenomenon helps in updating weight values in the right
    direction.*
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权重更新幅度，我们还应注意权重更新的方向。*当权重值小于最优值时，梯度为负；当权重值大于最优值时，梯度为正。这种现象有助于权重值向正确方向更新。*
- en: Finally, we will contrast the preceding with a learning rate of 1.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将以学习速率为1进行对比。
- en: Learning rate of 1
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习速率为1
- en: 'The code remains the same as in the learning rate of 0.01 scenario; however,
    the learning rate parameter would be 1 in this scenario. The output of running
    the same code with the changed learning rate parameter is as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与学习率为 0.01 的情况下保持不变；但在这种情况下，学习率参数为 1。更改学习率参数后运行相同代码的输出如下：
- en: '![A picture containing line, plot, diagram, text  Description automatically
    generated](img/B18457_01_22.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![包含线条、图形、图表、文本的图片 自动生成的描述](img/B18457_01_22.png)'
- en: 'Figure 1.22: Weight & loss value over increasing epochs when learning rate
    is 1'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22：在学习率为 1 时，随着周期的增加，权重和损失值的变化
- en: From the preceding diagram, we can see that the weight has deviated to a very
    high value (as at the end of the first epoch, the weight value is 45, which further
    deviated to a very large value in later epochs). In addition to that, the weight
    value moved to a very large amount, so that a small change in the weight value
    hardly results in a change in the gradient, and hence the weight got stuck at
    that high value.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中我们可以看到，权重已偏向一个非常高的值（例如在第一个周期结束时，权重值为 45，在后续周期中进一步偏向一个非常大的值）。此外，权重值已移动到一个非常大的数值，因此权重值的微小变化几乎不会导致梯度变化，从而使得权重困在这个高值上。
- en: '**Note**'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: In general, it is better to have a low learning rate. This way, the model is
    able to learn slowly but will adjust the weights toward an optimal value. Typical
    learning rate parameter values range between 0.0001 and 0.01.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，采用较低的学习率效果更好。这样一来，模型学习得更慢，但会朝着最优值调整权重。典型的学习率参数值范围在 0.0001 到 0.01 之间。
- en: Now that we have learned about the building blocks of a neural network – feedforward
    propagation, backpropagation, and learning rate – in the next section, we will
    summarize a high-level overview of how these three are put together to train a
    neural network.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了神经网络的构建模块——前向传播、反向传播和学习率——在接下来的部分，我们将总结如何将这三者结合起来训练神经网络的高级概述。
- en: Summarizing the training process of a neural network
  id: totrans-366
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结神经网络的训练过程
- en: Training a neural network is a process of coming up with optimal weights for
    a neural network architecture by repeating the two key steps, forward propagation
    and backpropagation with a given learning rate.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络是通过重复两个关键步骤——前向传播和反向传播——来找到神经网络架构的最优权重的过程，给定一个学习率。
- en: In forward propagation, we apply a set of weights to the input data, pass it
    through the defined hidden layers, perform the defined non-linear activation on
    the hidden layers’ output, and then connect the hidden layer to the output layer
    by multiplying the hidden layer node values with another set of weights to estimate
    the output value. Finally, we calculate the overall loss corresponding to the
    given set of weights. For the first forward propagation, the values of the weights
    are initialized randomly.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，我们将一组权重应用于输入数据，通过定义的隐藏层传递，对隐藏层输出执行定义的非线性激活，然后通过将隐藏层节点值与另一组权重相乘连接到输出层，以估算输出值。最后，我们计算与给定权重集合对应的整体损失。对于第一次前向传播，权重值是随机初始化的。
- en: In backpropagation, we decrease the loss value (error) by adjusting weights
    in a direction that reduces the overall loss. Furthermore, the magnitude of the
    weight update is the gradient times the learning rate.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，我们通过调整权重以减少总体损失的方向来减少损失值（误差）。此外，权重更新的幅度是梯度乘以学习率。
- en: The process of feedforward propagation and backpropagation is repeated until
    we achieve as minimal a loss as possible. This implies that, at the end of the
    training, the neural network has adjusted its weights ![](img/B18457_01_015.png)
    such that it predicts the output that we want it to predict. In the preceding
    toy example, after training, the updated network will predict a value of 0 as
    output when *{1,1}* is fed as input as it is trained to achieve that.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播和前向传播的过程重复进行，直到尽可能地减少损失。这意味着，在训练结束时，神经网络已经调整了其权重 ![](img/B18457_01_015.png)
    以预测我们希望其预测的输出。在上述示例中，训练后，更新的网络在输入 *{1,1}* 时将预测输出值为 0，因为它被训练成这样。
- en: Summary
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we understood the need for a single network that performs both
    feature extraction and classification in a single shot, before we learned about
    the architecture and the various components of an artificial neural network. Next,
    we learned about how to connect the various layers of a network before implementing
    feedforward propagation to calculate the loss value corresponding to the current
    weights of the network. We next implemented backpropagation to learn about the
    way to optimize weights to minimize the loss value and learned how the learning
    rate plays a role in achieving optimal weights for a network. In addition, we
    implemented all the components of a network – feedforward propagation, activation
    functions, loss functions, the chain rule, and gradient descent to update weights
    in NumPy from scratch so that we have a solid foundation to build upon in the
    next chapters.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解到需要一个单一网络来同时执行特征提取和分类，然后学习了人工神经网络的架构和各个组件。接下来，我们学习了如何连接网络的各个层，并实现了前馈传播来计算网络当前权重对应的损失值。然后，我们实现了反向传播来优化权重以最小化损失值，并学习了学习率在实现网络的最优权重中的作用。此外，我们从头开始使用NumPy实现了网络的所有组件——前馈传播、激活函数、损失函数、链式法则和梯度下降来更新权重，以便在接下来的章节中建立坚实的基础。
- en: Now that we understand how a neural network works, we’ll implement one using
    PyTorch in the next chapter, and dive deep into the various other components (hyperparameters)
    that can be tweaked in a neural network in the third chapter.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了神经网络的工作原理，接下来将在下一章中使用PyTorch实现一个神经网络，并深入探讨可以在神经网络中调整的各种其他组件（超参数）。
- en: Questions
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the various layers in a neural network?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中的各种层有哪些？
- en: What is the output of feedforward propagation?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈传播的输出是什么？
- en: How is the loss function of a continuous dependent variable different from that
    of a binary dependent variable or a categorical dependent variable?
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续依赖变量的损失函数与二元依赖变量或分类依赖变量的损失函数有何不同？
- en: What is stochastic gradient descent?
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是随机梯度下降？
- en: What does a backpropagation exercise do?
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播练习的作用是什么？
- en: How does the update of all the weights across layers happen during backpropagation?
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播期间如何更新跨层所有权重？
- en: Which functions are used within each epoch of training a neural network?
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练神经网络的每个epoch中使用了哪些函数？
- en: Why is training a network on a GPU faster when compared to training it on a
    CPU?
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在GPU上训练网络比在CPU上训练更快？
- en: What is the impact of the learning rate when training a neural network?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练神经网络时学习率的影响是什么？
- en: What is the typical value of the learning rate parameter?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率参数的典型值是多少？
- en: Learn more on Discord
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多信息
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
