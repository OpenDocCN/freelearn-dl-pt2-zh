- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Teaching Networks to Generate Digits
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教授网络生成数字
- en: In the previous chapter, we covered the building blocks of neural network models.
    In this chapter, our first project will recreate one of the most groundbreaking
    models in the history of deep learning, **Deep Belief Network** (**DBN**). DBN
    was one of the first multi-layer networks for which a feasible learning algorithm
    was developed. Besides being of historical interest, this model is connected to
    the topic of this book because the learning algorithm makes use of a generative
    model in order to pre-train the neural network weights into a reasonable configuration
    prior to backpropagation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们涵盖了神经网络模型的构建基块。在这一章中，我们的第一个项目将重新创建深度学习历史上最具突破性的模型之一- **深度信念网络**（**DBN**）。DBN
    是一个最早的多层网络，为其开发了一个可行的学习算法。除了具有历史意义外，该模型与本书主题有关，因为学习算法利用生成模型来预先将神经网络权重调整到合理配置，然后进行反向传播。
- en: 'In this chapter, we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: How to load the **Modified National Institute of Standards and Technology**
    (**MNIST**) dataset and transform it using TensorFlow 2's Dataset API.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何加载**修改的国家标准技术研究所**（**MNIST**）数据集并使用 TensorFlow 2 的数据集 API 进行转换。
- en: How a **Restricted Boltzmann Machine** (**RBM**) – a simple neural network –
    is trained by minimizing an "energy" equation that resembles formulas from physics
    to generate images.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过最小化类似于物理公式的“能量”方程来训练**受限玻尔兹曼机**（**RBM**）- 一个简单的神经网络- 以生成图像。
- en: How to stack several RBMs to make a DBN and apply forward and backward passes
    to pre-train this network to generate image data.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何堆叠多个 RBM 来生成 DBN 并应用前向和后向传递来预训练此网络以生成图像数据。
- en: How to implement an end-to-end classifier by combining this pre-training with
    backpropagation "fine-tuning" using the TensorFlow 2 API.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过将这种预训练与使用 TensorFlow 2 API 的反向传播“微调”相结合来实现端到端的分类器。
- en: The MNIST database
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据库
- en: 'In developing the DBN model, we will use a dataset that we have discussed before
    – the MNIST database, which contains digital images of hand-drawn digits from
    0 to 9¹. This database is a combination of two sets of earlier images from the
    **National Institute of Standards and Technology** (**NIST**): Special Database
    1 (digits written by US high school students) and Special Database 3 (written
    by US Census Bureau employees),² the sum of which is split into 60,000 training
    images and 10,000 test images.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '在开发 DBN 模型时，我们将使用之前讨论过的数据集 - MNIST 数据库，其中包含手绘数字 0 到 9 的数字图像¹。该数据库是两组早期图像的组合，分别来自**国家标准技术研究所**（**NIST**）:
    特殊数据库1（由美国高中学生书写）和特殊数据库3（由美国人口普查局员工书写）²，总共分为 60,000 个训练图像和 10,000 个测试图像。'
- en: The original images in the dataset were all black and white, while the modified
    dataset normalized them to fit into a 20x20-pixel bounding box and removed jagged
    edges using anti-aliasing, leading to intermediary grayscale values in cleaned
    images; they are padded for a final resolution of 28x28 pixels.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集中的图像全部为黑白，而修改后的数据集将其标准化以适应20x20像素的边界框，并使用抗锯齿技术去除锯齿状边缘，导致清洁图像中间灰度值；它们被填充以获得最终分辨率为28x28像素。
- en: In the original NIST dataset, all the training images came from Bureau employees,
    while the test dataset came from high school students, and the modified version
    mixes the two groups in the training and test sets to provide a less biased population
    for training machine learning algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 NIST 数据集中，所有的训练图像都来自局务员，而测试数据集来自高中学生，修改后的版本将这两组人群混合在训练和测试集中，以为机器学习算法提供一个更少偏见的人口。
- en: '![](img/B16176_04_01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_01.png)'
- en: 'Figure 4.1: Digits from the NIST dataset (left)³ and MNIST (right)⁴'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：NIST 数据集中的数字（左）³ 和 MNIST（右）⁴
- en: An early application of **Support Vector Machines** (**SMVs**) to this dataset
    yielded an error rate of 0.8%,⁵ while the latest deep learning models have shown
    error rates as low as 0.23%.⁶ You should note that these figures were obtained
    due to not only the discrimination algorithms used but also "data augmentation"
    tricks such as creating additional translated images where the digit has been
    shifted by several pixels, thus increasing the number of data examples for the
    algorithm to learn from. Because of its wide availability, this dataset has become
    a benchmark for many machine learning models, including Deep Neural Networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将**支持向量机**（**SVMs**）早期应用于此数据集的结果显示出了0.8%的错误率，⁵而最新的深度学习模型的错误率低至0.23%。⁶ 你应该注意到，这些数字的获得不仅是由于使用的判别算法，还有"数据增强"技巧，如创建额外的翻译图像，其中数字已经偏移了几个像素，从而增加了算法学习的数据示例数量。由于其广泛的可用性，这个数据集已经成为许多机器学习模型的基准，包括深度神经网络。
- en: The dataset was also the benchmark for a breakthrough in training multi-layer
    neural networks in 2006, in which an error rate of 1.25% was achieved (without
    image translation, as in the preceding examples).⁷ In this chapter, we will examine
    in detail how this breakthrough was achieved using a generative model, and explore
    how to build our own DBN that can generate MNIST digits.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集也是2006年多层神经网络训练突破的基准，该突破实现了1.25%的错误率（与前述示例不同，没有图像翻译）。⁷ 在本章中，我们将详细研究如何使用生成模型实现这一突破，并探讨如何构建我们自己的DBN，以生成MNIST数字。
- en: Retrieving and loading the MNIST dataset in TensorFlow
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索和加载TensorFlow中的MNIST数据集
- en: The first step in training our own DBN is to construct our dataset. This section
    will show you how to transform the MNIST data into a convenient format that allows
    you to train a neural network, using some of TensorFlow 2's built-in functions
    for simplicity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自己的DBN的第一步是构造我们的数据集。本节将向您展示如何将MNIST数据转换为一种方便的格式，以便您可以使用一些TensorFlow 2的内置函数来训练神经网络，以简化操作。
- en: 'Let''s start by loading the MNIST dataset in TensorFlow. As the MNIST data
    has been used for many deep learning benchmarks, TensorFlow 2 already has convenient
    utilities for loading and formatting this data. To do so, we need to first install
    the `tensorflow-datasets` library:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从TensorFlow中加载MNIST数据集开始。由于MNIST数据已经用于许多深度学习基准测试，TensorFlow 2已经为加载和格式化此数据提供了方便的实用程序。为此，我们首先需要安装`tensorflow-datasets`库：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After installing the package, we need to import it along with the required
    dependencies:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完软件包后，我们需要导入它以及所需的依赖项：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we can download the MNIST data locally from **Google Cloud Storage** (**GCS**)
    using the builder functionality:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用构建器功能从**Google Cloud Storage**（**GCS**）本地下载MNIST数据：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The dataset will now be available on disk on our machine. As noted earlier,
    this data is divided into a training and test dataset, which you can verify by
    taking a look at the `info` command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集将在我们的计算机磁盘上可用。正如前面所述，这些数据被分为训练数据集和测试数据集，您可以通过查看`info`命令来验证：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives the following output:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see, the test dataset has 10,000 examples, the training dataset has
    60,000 examples, and the images are 28x28 pixels with a label from one of 10 classes
    (0 to 9).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，测试数据集有10,000个示例，训练数据集有60,000个示例，图像为28x28像素，具有10个类别中的一个标签（0到9）。
- en: 'Let''s start by taking a look at the training dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先来看看训练数据集：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can visually plot some examples using the `show_examples` function:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`show_examples`函数可视化绘制一些示例：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives the following figure:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下图表：
- en: '![](img/B16176_04_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_02.png)'
- en: 'Figure 4.2: MNIST digit examples from the TensorFlow dataset'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：来自TensorFlow数据集的MNIST数字示例
- en: You can also see more clearly here the grayscale edges on the numbers where
    the anti-aliasing was applied to the original dataset to make the edges seem less
    jagged (the colors have also been flipped from the original example in *Figure
    4.1*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您还可以更清楚地看到应用了抗锯齿处理的灰度边缘，以使原始数据集的边缘看起来不那么锯齿状（颜色也已从*图4.1*中的原始示例翻转）。
- en: 'We can also plot an individual image by taking one element from the dataset,
    reshaping it to a 28x28 array, casting it as a 32-bit float, and plotting it in
    grayscale:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过从数据集中取一个元素，将其重新塑形为28x28数组，将其强制转换为32位浮点数，并以灰度形式绘制来绘制单个图像：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This gives the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下图表：
- en: '![](img/B16176_04_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_03.png)'
- en: 'Figure 4.3: A MNIST digit in TensorFlow'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：TensorFlow中的MNIST数字
- en: 'This is nice for visual inspection, but for our experiments in this chapter,
    we will actually need to flatten these images into a vector. To do so, we can
    use the `map()` function, and verify that the dataset is now flattened; note that
    we also need to cast to a float for use in the RBM later. The RBM also assumes
    binary (0 or 1) inputs, so we need to rescale the pixels, which range from 0 to
    256 to the range 0 to 1:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于视觉检查很好，但是在本章的实验中，我们实际上需要将这些图像展平成向量。为了做到这一点，我们可以使用`map（）`函数，并验证数据集现在已经被展平；请注意，我们还需要将其转换为浮点数以便稍后在RBM中使用。RBM还假设输入是二进制（0或1），所以我们需要重新缩放像素，范围从0到256到范围从0到1：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This gives a 784x1 vector, which is the "flattened" version of the pixels of
    the digit "4":'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这得到了一个784x1的向量，这是数字"4"的"展平"版本的像素：
- en: '![](img/B16176_04_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_04.png)'
- en: 'Figure 4.4: Flattening the MNIST digits in TensorFlow'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.4: 在TensorFlow中将MNIST数字展平'
- en: Now that we have the MNIST data as a series of vectors, we are ready to start
    implementing an RBM to process this data and ultimately create a model capable
    of generating new images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将MNIST数据处理成一系列向量，我们准备开始实现一个RBM来处理这些数据，最终创建一个能够生成新图像的模型。
- en: 'Restricted Boltzmann Machines: generating pixels with statistical mechanics'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机：用统计力学生成像素
- en: The neural network model that we will apply to the MNIST data has its origins
    in earlier research on how neurons in the mammalian brain might work together
    to transmit signals and encode patterns as memories. By using analogies to statistical
    mechanics in physics, this section will show you how simple networks can "learn"
    the distribution of image data and be used as building blocks for larger networks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用于MNIST数据的神经网络模型的起源可以追溯到对哺乳动物大脑中的神经元如何一起传递信号并编码模式作为记忆的早期研究。通过使用物理学中的统计力学类比，本节将向您展示简单的网络如何"学习"图像数据的分布，并且可以用作更大网络的构建模块。
- en: Hopfield networks and energy equations for neural networks
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络和神经网络的能量方程
- en: As we discussed in *Chapter 3*, *Building Blocks of Deep Neural Networks*, Hebbian
    Learning states, "Neurons that fire together, wire together",⁸ and many models,
    including the multi-layer perceptron, made use of this idea in order to develop
    learning rules. One of these models was the **Hopfield network**, developed in
    the 1970-80s by several researchers^(9 10). In this network, each "neuron" is
    connected to every other by a symmetric weight, but no self-connections (there
    are only connections between neurons, no self-loops).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第3章*讨论的*深度神经网络的基本组成部分*中所提到的，**赫布学习法**陈述："发射的神经元会产生联系"⁸，并且许多模型，包括多层感知器，都利用了这个想法来开发学习规则。其中一个模型就是**霍普菲尔德网络**，由几位研究人员在1970-80年代开发^(9
    10)。在这个网络中，每个"神经元"都通过对称权重与其他所有神经元相连，但没有自连接（只有神经元之间的连接，没有自环）。
- en: Unlike the multi-layer perceptrons and other architectures we studied in *Chapter
    3*, *Building Blocks of Deep Neural Networks,* the Hopfield network is an undirected
    graph, since the edges go "both ways."
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在*第3章*学习的多层感知器和其他架构不同，霍普菲尔德网络是一个无向图，因为边是"双向的"。
- en: '![](img/Chapter_04_05.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Chapter_04_05.png)'
- en: 'Figure 4.5: The Hopfield network'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.5: 霍普菲尔德网络'
- en: 'The neurons in the Hopfield network take on binary values, either (-1, 1) or
    (0, 1), as a thresholded version of the tanh or sigmoidal activation function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络中的神经元采用二进制值，要么是(-1, 1)，要么是(0, 1)，作为双曲正切或Sigmoid激活函数的阈值版本：
- en: '![](img/B16176_04_001.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_001.png)'
- en: 'The threshold values (sigma) never change during training; to update the weights,
    a "Hebbian" approach is to use a set of *n* binary patterns (configurations of
    all the neurons) and update as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值（sigma）在训练过程中不会发生变化；为了更新权重，可以使用"**赫布学习法**"来使用一组*n*个二进制模式（所有神经元的配置）进行更新：
- en: '![](img/B16176_04_002.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_002.png)'
- en: where *n* is the number of patterns, and *e* is the binary activations of neurons
    *i* and *j* in a particular configuration. Looking at this equation, you can see
    that if the neurons share a configuration, the connection between them is strengthened,
    while if they are opposite signs (one neuron has a sign of +1, the other -1),
    it is weakened. Following this rule to iteratively strengthen or weaken a connection
    leads the network to converge to a stable configuration that resembles a "memory"
    for a particular activation of the network, given some input. This represents
    a model for associative memory in biological organisms – the kind of memory that
    links unrelated ideas, just as the neurons in the Hopfield network are linked
    together^(11 12).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*n*是模式数，*e*是特定配置中神经元*i*和*j*的二进制激活。观察这个方程，你会发现如果神经元共享一个配置，它们之间的连接会被加强，而如果它们是相反的符号（一个神经元的符号为+1，另一个的符号为-1），它们之间的连接就会被削弱。按照这个规则迭代地加强或削弱连接，导致网络收敛到一个稳定的配置，类似于网络的特定激活的“记忆”，给定一些输入。这代表了生物有机体中的联想记忆模型——将不相关的思想链接在一起的记忆，就像Hopfield网络中的神经元被链接在一起一样。^(11
    12)
- en: 'Besides representing biological memory, Hopfield networks also have an interesting
    parallel to electromagnetism. If we consider each neuron as a particle or "charge,"
    we can describe the model in terms of a "free energy" equation that represents
    how the particles in this system mutually repulse/attract each other and where
    on the distribution of potential configurations the system lies relative to equilibrium:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表示生物记忆外，Hopfield网络还与电磁学有一个有趣的相似点。如果我们将每个神经元视为粒子或“电荷”，我们可以用一个“自由能”方程描述该模型，表示该系统中的粒子如何相互排斥/吸引，以及系统在潜在配置分布上相对于平衡点的位置：
- en: '![](img/B16176_04_003.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_003.png)'
- en: 'where w is the weights between neurons *i* and *j*, *s* is the "states" of
    those neurons (either 1, "on," or -1, "off"), and sigma is the threshold of each
    neuron (for example, the value that its total inputs must exceed to set it to
    "on"). When the Hopfield network is in its final configuration, it also minimizes
    the value of the energy function computed for the network, which is lowered by
    units with an identical state(s) being connected strongly (*w*). The probability
    associated with a particular configuration is given by the **Gibbs measure**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，w是神经元*i*和*j*之间的权重，*s*是这些神经元的“状态”（要么是1，“开”，要么是-1，“关”），sigma是每个神经元的阈值（例如，它的总输入必须超过的值，才能将其设置为“开”）。当Hopfield网络处于其最终配置中时，它还最小化了为网络计算的能量函数的值，其中具有相同状态的单元通过强连接（*w*）连接。与特定配置相关联的概率由**Gibbs测度**给出：
- en: '![](img/B16176_04_004.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_004.png)'
- en: 'Here, *Z(B)* is a normalizing constant that represents all possible configurations
    of the network, in the same respect as the normalizing constant in the Bayesian
    probability function you saw in *Chapter 1*, *An Introduction to Generative AI:
    "Drawing" Data from Models*.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Z(B)*是一个归一化常数，表示与*“Chapter 1”，生成AI的介绍：“从模型中“绘制”数据* 中的贝叶斯概率函数中的归一化常数相同，表示网络的所有可能配置。
- en: Also notice in the energy function definition that the state of a neuron is
    only affected by local connections (rather than the state of every other neuron
    in the network, regardless of if it is connected); this is also known as the **Markov
    property**, since the state is "memoryless," depending only on its immediate "past"
    (neighbors). In fact, the *Hammersly-Clifford theorem* states that any distribution
    having this same memoryless property can be represented using the Gibbs measure.^(13)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意能量函数的定义中，神经元的状态只受到本地连接的影响（而不是受到所有网络中其他神经元的状态影响，无论它是否连接）；这也被称为**马尔科夫性质**，因为状态是“无记忆”的，仅取决于其立即“过去”（邻居）。实际上，*Hammersly-Clifford定理*表明，任何具有相同无记忆属性的分布都可以使用Gibbs测度来表示。^(13)
- en: Modeling data with uncertainty with Restricted Boltzmann Machines
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用受限玻尔兹曼机建模不确定性数据
- en: What other kinds of distributions might we be interested in? While useful from
    a theoretical perspective, one of the shortcomings of the Hopfield network is
    that it can't incorporate the kinds of uncertainty seen in actual physical or
    biological systems; rather than deterministically turning on or off, real-world
    problems often involve an element of chance – a magnet might flip polarity, or
    a neuron might fire at random.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能对其他种类的分布感兴趣吗？虽然Hopfield网络从理论角度来看很有用，但其缺点之一是无法纳入实际物理或生物系统中存在的不确定性；与确定性的打开或关闭不同，现实世界的问题通常涉及一定程度的偶然性
    - 磁铁可能会翻转极性，或者神经元可能会随机发射。
- en: This uncertainty, or *stochasticity*, is reflected in the *Boltzmann machine*,^(14)
    a variant of the Hopfield network in which half the neurons (the "visible" units)
    receive information from the environment, while half (the "hidden" units) only
    receive information from the visible units.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不确定性，或者*随机性*，反映在*Boltzmann机器*中^(14)——这是Hopfield网络的变体，其中一半的神经元（“可见”单元）从环境接收信息，而另一半（“隐藏”单元）只从可见单元接收信息。
- en: '![](img/Chapter_04_06.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Chapter_04_06.png)'
- en: 'Figure 4.6: The Boltzmann machine'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：Boltzmann机器
- en: The Boltzmann machine randomly turns on (1) or off (0) each neuron by sampling,
    and over many iterations converges to a stable state represented by the minima
    of the energy function. This is shown schematically in *Figure 4.6*, in which
    the white nodes of the network are "off," and the blue ones are "on;" if we were
    to simulate the activations in the network, these values would fluctuate over
    time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann机器通过抽样随机打开（1）或关闭（0）每个神经元，并在许多迭代中收敛到能量函数的最小值所代表的稳定状态。这在*图4.6*中以示意图的形式显示，网络的白色节点为“关闭”，蓝色节点为“开启”；如果我们模拟网络中的激活，这些值将随时间波动。
- en: In theory, a model like this could be used, for example, to model the distribution
    of images, such as the MNIST data using the hidden nodes as a "barcode" that represents
    an underlying probability model for "activating" each pixel in the image. In practice,
    though, there are problems with this approach. Firstly, as the number of units
    in the Boltzmann network increases, the number of connections increases exponentially
    (for example, the number of potential configurations that has to be accounted
    for in the Gibbs measure's normalization constant explodes), as does the time
    needed to sample the network to an equilibrium state. Secondly, weights for units
    with intermediate activate probabilities (not strongly 0 or 1) will tend to fluctuate
    in a random walk pattern (for example, the probabilities will increase or decrease
    randomly but never stabilize to an equilibrium value) until the neurons converge,
    which also prolongs training.^(15)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，像这样的模型可以用来模拟图像的分布，例如使用隐藏节点作为表示图像中每个像素的基础概率模型的“条形码”。然而，在实践中，这种方法存在问题。首先，随着Boltzmann网络中单元的数量增加，连接的数量呈指数增长（例如，必须在Gibbs测度的归一化常数中考虑的潜在配置数量激增），同样需要采样网络到平衡状态所需的时间也随之增加。其次，具有中间激活概率的单元的权重往往会呈现随机行走模式（例如，概率会随机增加或减少，但永远不会稳定到平衡值），直到神经元收敛，这也延长了训练时间。^(15)
- en: 'A practical modification is to remove some of the connections in the Boltzmann
    machine, namely those between visible units and between hidden units, leaving
    only connections between the two types of neurons. This modification is known
    as the RBM, shown in *Figure 4.7*^(16):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实用的修改是删除Boltzmann机器中的一些连接，即可见单元之间的连接和隐藏单元之间的连接，仅保留两种类型神经元之间的连接。这种修改称为RBM，如*图4.7*所示^(16)：
- en: '![](img/Chapter_04_07.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Chapter_04_07.png)'
- en: 'Figure 4.7: RBM'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：RBM
- en: Imagine as described earlier that the visible units are input pixels from the
    MNIST dataset, and the hidden units are an encoded representation of that image.
    By sampling back and forth to convergence, we could create a generative model
    for images. We would just need a learning rule that would tell us how to update
    the weights to allow the energy function to converge to its minimum; this algorithm
    is **contrastive divergence** (**CD**). To understand why we need a special algorithm
    for RBMs, it helps to revisit the energy equation and how we might sample to get
    equilibrium for the network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前描述的那样，可见单元是来自MNIST数据集的输入像素，而隐藏单元是该图像的编码表示。通过来回采样直到收敛，我们可以创建一个图像的生成模型。我们只需要一个学习规则，告诉我们如何更新权重以使能量函数收敛到其最小值；这个算法就是**对比散度**（**CD**）。为了理解为什么我们需要一个特殊的算法来处理RBM，有助于重新思考能量方程以及我们如何采样获得网络的平衡。
- en: 'Contrastive divergence: Approximating a gradient'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对比散度：梯度的近似
- en: 'If we refer back to *Chapter 1*, *An Introduction to Generative AI: "Drawing"
    Data from Models*, creating a generative model of images using an RBM essentially
    involves finding the probability distribution of images, using the energy equation^(17):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾*第1章* *生成式人工智能简介：从模型中“生成”数据*，使用RBM创建图像的生成模型本质上涉及找到图像的概率分布，使用能量方程^(17)：
- en: '![](img/B16176_04_005.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_005.png)'
- en: 'where *x* is an image, theta is the parameters of the model (the weights and
    biases), and *Z* is the partition function:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是一个图像，theta是模型的参数（权重和偏置），*Z*是分区函数：
- en: '![](img/B16176_04_006.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_006.png)'
- en: 'In order to find the parameters that optimize this distribution, we need to
    maximize the likelihood (product of each datapoint''s probability under a density
    function) based on data:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到优化这个分布的参数，我们需要基于数据最大化似然（每个数据点在密度函数下的概率乘积）：
- en: '![](img/B16176_04_007.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_007.png)'
- en: 'In practice, it''s a bit easier to use the negative log likelihood, as this
    is represented by a sum:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用负对数似然稍微容易一些，因为它表示为一个和：
- en: '![](img/B16176_04_008.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_008.png)'
- en: If the distribution *f* has a simple form, then we can just take the derivative
    of *E* with respect to parameters of *f*. For example, if *f* is a single normal
    distribution, then the values that maximize *E* with respect to mu (the mean)
    and sigma (the standard deviation) are, respectively, the sample mean and standard
    deviation; the partition function *Z* doesn't affect this calculation because
    the integral is 1, a constant, which becomes 0 once we take the logarithm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分布*f*的形式简单，那么我们可以对*f*的参数进行导数。例如，如果*f*是一个单一的正态分布，那么最大化*E*关于mu（平均值）和sigma（标准差）的值分别是样本均值和标准差；分区函数*Z*不会影响这个计算，因为积分是1，一个常数，一旦我们取了对数，它就变成了0。
- en: If the distribution is instead a sum of *N* normal distributions, then the partial
    derivative of *mu(i)* (one of these distributions) with respect to *f* (the sum
    of all the *N* normal distributions) involves the mu and sigma of each other distribution
    as well. Because of this dependence, there is no closed-form solution (for example,
    a solution equation we can write out by rearranging terms or applying algebraic
    transformations) for the optimal value; instead, we need to use a gradient search
    method (such as the backpropagation algorithm we discussed in *Chapter 3*, *Building
    Blocks of Deep Neural Networks*) to iteratively find the optimal value of this
    function. Again, the integral of each of these *N* distributions is 1, meaning
    the partition function is the constant *log(N)*, making the derivative 0.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分布代替一个正态分布的总和，则*mu(i)*（这些分布中的一个）关于*f*（所有*N*个正态分布的总和）的偏导数同样涉及到每个其他分布的mu和sigma。由于这种依赖关系，对于最优值没有封闭形式解法（例如，我们可以通过重新排列项或应用代数转换写出的解方程）；相反，我们需要使用梯度搜索方法（例如我们在*第3章*
    *深度神经网络的构建基块*中讨论的反向传播算法）迭代地找到这个函数的最优值。同样，每个这些*N*个分布的积分都是1，意味着分区函数是常数*log(N)*，使得导数为0。
- en: What happens if the distribution *f* is a product, instead of a sum, of normal
    distributions? The partition function *Z* is now no longer a constant in this
    equation with respect to theta, the parameters; the value will depend on how and
    where these functions overlap when computing the integral – they could cancel
    each other out by being mutually exclusive (0) or overlapping (yielding a value
    greater than 1). In order to evaluate gradient descent steps, we would need to
    be able to compute this partition function using numerical methods. In the RBM
    example, this partition function for the configuration of 28x28 MNIST digits would
    have 784 logistic units, and a massive number (2^(784)) of possible configurations,
    making it unwieldy to evaluate every time we want to take a gradient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分布*f*是正态分布的乘积而不是和，会发生什么？对于参数θ来说，分区函数*Z*不再是该方程中的常数；其值将取决于这些函数在计算积分时如何重叠和在何处重叠——它们可能通过相互排斥（0）或重叠（产生大于1的值）相互抵消。为了评估梯度下降步骤，我们需要能够使用数值方法计算此分区函数。在RBM示例中，这种28x28
    MNIST数字配置的分区函数将具有784个逻辑单元和大量可能的配置（2^(784)），使其在每次我们想要进行梯度计算时评估变得不方便。
- en: 'Is there any other way we could optimize the value of this energy equation
    without taking a full gradient? Returning to the energy equation, let''s write
    out the gradient explicitly:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了采用完整梯度之外，我们还能优化此能量方程的值吗？回到能量方程，让我们明确地写出梯度：
- en: '![](img/B16176_04_009.png)![](img/B16176_04_010.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_009.png)![](img/B16176_04_010.png)'
- en: 'The partition function *Z* can be further written as a function of the integral
    involving *X* and the parameters of *f*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 分区函数*Z*还可以进一步写成涉及*X*和*f*的参数的积分函数：
- en: '![](img/B16176_04_011.png)![](img/B16176_04_012.png)![](img/B16176_04_013.png)![](img/B16176_04_014.png)![](img/B16176_04_015.png)![](img/B16176_04_016.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_011.png)![](img/B16176_04_012.png)![](img/B16176_04_013.png)![](img/B16176_04_014.png)![](img/B16176_04_015.png)![](img/B16176_04_016.png)'
- en: where *< >* represents an average over the observed data sampled from the distribution
    of *x*. In other words, we can approximate the integral by sampling from the data
    and computing the average, which allows us to avoid computing or approximating
    high-dimensional integrals.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*< >*表示对从*x*的分布中采样的观察数据的平均值。换句话说，我们可以通过从数据中进行采样并计算平均值来近似积分，这使我们能够避免计算或近似高维积分。
- en: While we can't directly sample from *p(x)*, we can use a technique known as
    **Markov Chain Monte Carlo** (**MCMC**) sampling to generate data from the target
    distribution *p(x')*. As was described in our discussion on Hopfield networks,
    the "Markov" property means that this sampling only uses the last sample as input
    in determining the probability of the next datapoint in the simulation – this
    forms a "chain" in which each successive sampled datapoint becomes input to the
    next.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不能直接从*p(x)*中采样，但我们可以使用一种称为**马尔可夫链蒙特卡洛**（**MCMC**）采样的技术从目标分布*p(x')*生成数据。正如我们在讨论Hopfield网络时所描述的那样，“马尔可夫”属性意味着此采样仅使用上一个样本作为模拟中下一个数据点的概率的输入——这形成了一个“链”，其中每个连续采样的数据点成为下一个数据点的输入。
- en: 'The "Monte Carlo" in the name of this technique is a reference to a casino
    in the principality of Monaco, and denotes that, like the outcomes of gambling,
    these samples are generated through a random process. By generating these random
    samples, you can use *N* MCMC steps as an approximation of the average of a distribution
    that is otherwise difficult or impossible to integrate. When we put all of this
    together, we get the following gradient equation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术名称中的“蒙特卡罗”是指摩纳哥公国的一个赌场，并表示，与赌博的结果一样，这些样本是通过随机过程生成的。通过生成这些随机样本，您可以使用*N*个MCMC步骤作为对难以或不可能积分的分布的平均值的近似。当我们把所有这些都放在一起时，我们得到以下梯度方程：
- en: '![](img/B16176_04_017.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_017.png)'
- en: where *X* represents the data at each step in the MCMC chain, with *X*⁰ being
    the input data. While in theory you might think it would take a large number of
    steps for the chain to converge, in practice it has been observed that even *N=1*
    steps is enough to get a decent gradient approximation.^(18)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*X*表示MCMC链中每一步的数据，其中*X*⁰是输入数据。尽管在理论上您可能会认为需要大量步骤才能使链收敛，但实践中观察到，甚至*N=1*步就足以得到一个不错的梯度近似。^(18)
- en: Notice that the end result is a *contrast* between the input data and the sampled
    data; thus, the method is named **contrastive divergence** as it involves the
    difference between two distributions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最终结果是输入数据和抽样数据之间的*对比*；因此，该方法被命名为**对比散度**，因为它涉及两个分布之间的差异。
- en: 'Applying this to our RBM example, we can follow this recipe to generate the
    required samples:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一方法应用于我们的RBM示例中，我们可以按照以下步骤生成所需的样本：
- en: Take an input vector *v*
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取输入向量*v*
- en: Compute a "hidden" activation *h*
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算“隐藏”激活*h*
- en: Use the activation from (*2*) to generate a sampled visible state *v'*
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用（*2*）中的激活生成一个抽样的可见状态*v'*
- en: Use (*3*) to generate a sampled hidden state *h'*
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用（*3*）生成一个抽样的隐藏状态*h'*
- en: Compute the updates, which are simply the correlations of the visible and hidden
    units:![](img/B16176_04_018.png)![](img/B16176_04_019.png)![](img/B16176_04_020.png)
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算更新，这仅仅是可见和隐藏单元的相关性：![](img/B16176_04_018.png)![](img/B16176_04_019.png)![](img/B16176_04_020.png)
- en: where *b* and *c* are the bias terms of visible and hidden units, respectively,
    and *e* is the learning rate.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*b*和*c*分别是可见单元和隐藏单元的偏置项，*e*是学习率。
- en: This sampling is known as **Gibbs sampling**, a method in which we sample one
    unknown parameter of a distribution at a time while holding all others constant.
    Here we hold the visible or the hidden fixed and sample units in each step.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽样被称为**吉布斯抽样**，这是一种方法，在这种方法中，我们一次只对分布的一个未知参数进行抽样，而将其他所有参数保持不变。在这里，我们在每一步中保持可见或隐藏的固定，并对单元进行抽样。
- en: With CD, we now have a way to perform gradient descent to learn the parameters
    of our RBM model; as it turns out, we can potentially compute an even better model
    by stacking RBMs in what is called a DBN.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CD，我们现在有了一种方法来执行梯度下降以学习我们的RBM模型的参数；事实证明，通过堆叠RBM，我们可以潜在地计算出一个更好的模型，这就是所谓的DBN。
- en: 'Stacking Restricted Boltzmann Machines to generate images: the Deep Belief
    Network'
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠受限玻尔兹曼机以生成图像：深度信念网络
- en: You have seen that an RBM with a single hidden layer can be used to learn a
    generative model of images; in fact, theoretical work has suggested that with
    a sufficiently large number of hidden units, an RBM can approximate *any* distribution
    with binary values.^(19) However, in practice, for very large input data, it may
    be more efficient to add additional layers, instead of a single large layer, allowing
    a more "compact" representation of the data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到，具有单个隐藏层的RBM可用于学习图像的生成模型；事实上，理论工作表明，具有足够多的隐藏单元，RBM可以近似表示*任何*具有二进制值的分布。^(19)然而，在实践中，对于非常大的输入数据，添加额外的层可能比添加单个大层更有效，这允许对数据进行更“紧凑”的表示。
- en: Researchers who developed DBNs also noted that adding additional layers can
    only lower the log likelihood of the lower bound of the approximation of the data
    reconstructed by the generative model.^(20) In this case, the hidden layer output
    *h* of the first layer becomes the input to a second RBM; we can keep adding other
    layers to make a deeper network. Furthermore, if we wanted to make this network
    capable of learning not only the distribution of the image (*x*) but also the
    label – which digit it represents from 0 to 9 (*y*) – we could add yet another
    layer to a stack of connected RBMs that is a probability distribution (softmax)
    over the 10 possible digit classes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 开发DBNs的研究人员还注意到，添加额外的层只会降低由生成模型重构的数据近似的下界的对数似然性。^(20)在这种情况下，第一层的隐藏层输出*h*成为第二个RBM的输入；我们可以继续添加其他层来构建一个更深的网络。此外，如果我们希望使此网络能够学习不仅图像（*x*）的分布，还包括标签
    - 它代表从0到9的哪个数字（*y*）-我们可以将另一个层添加到连接的RBM堆栈中，这是10个可能数字类的概率分布（softmax）。
- en: 'A problem with training a very deep graphical model such as stacked RBMs is
    the "explaining-away effect" that we discussed in *Chapter 3*, *Building Blocks
    of Deep Neural Networks*. Recall that the dependency between variables can complicate
    inference of the state of hidden variables:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非常深的图形模型，如堆叠RBM，存在一个问题，即我们在第3章“深度神经网络的基本构件”中讨论过的“解释效果”。请注意，变量之间的依赖关系可能会使对隐藏变量状态的推断变得复杂：
- en: '![](img/Chapter_04_08.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Chapter_04_08.png)'
- en: 'Figure 4.8: The explaining-away effect in a Bayesian network^(21)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：贝叶斯网络中的解释效果^(21)
- en: 'In *Figure 4.8*, the knowledge that the pavement is wet can be explained by
    a sprinkler being turned on, to the extent that the presence or absence of rain
    becomes irrelevant, meaning we can''t meaningfully infer the probability that
    it is raining. This is equivalent to saying that the posterior distribution (*Chapter
    1*, *An Introduction to Generative AI: "Drawing" Data from Models*) of the hidden
    units cannot be tractably computed, since they are correlated, which interferes
    with easily sampling the hidden states of the RBM.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4.8*中，知道路面潮湿可以被解释为打开了洒水器，以至于下雨与否变得无关紧要，这意味着我们无法有意义地推断下雨的概率。这相当于说隐藏单元的后验分布（*第
    1 章*，*生成式人工智能简介："从模型中抽取"数据*）无法被可计算，因为它们是相关的，这会干扰对 RBM 的隐藏状态进行轻松抽样。
- en: One solution is to treat each of the units as independent in the likelihood
    function, which is known as *variational inference*; while this works in practice,
    it is not a satisfying solution given that we know that these units are in fact
    correlated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是在似然函数中将每个单元视为独立的，这被称为*变分推断*；虽然这在实践中有效，但鉴于我们知道这些单元实际上是相关的，这并不是一个令人满意的解决方案。
- en: But where does this correlation come from? If we sample the state of the visible
    units in a single-layer RBM, we set the states of each hidden unit randomly since
    they are independent; thus the *prior distribution* over the hidden units is independent.
    Why is the posterior then correlated? Just as the knowledge (data) that the pavement
    is wet causes a correlation between the probabilities of a sprinkler and rainy
    weather, the correlation between pixel values causes the posterior distribution
    of the hidden units to be non-independent. This is because the pixels in the images
    aren't set randomly; based on which digit the image represents, groups of pixels
    are more or less likely to be bright or dark. In the 2006 paper *A Fast Learning
    Algorithm for Deep Belief Nets*,^(22) the authors hypothesized that this problem
    could be solved by computing a *complementary prior* that has exactly the opposite
    correlation to the likelihood, thus canceling out this dependence and making the
    posterior also independent.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种相关性是从哪里来的呢？如果我们在单层 RBM 中对可见单元的状态进行抽样，我们会将每个隐藏单元的状态随机设置，因为它们是独立的；因此，隐藏单元的*先验分布*是独立的。那么后验为何会相关呢？正如知道（数据）路面潮湿会导致洒水器和下雨天气的概率之间存在相关性一样，像素值之间的相关性导致隐藏单元的后验分布不是独立的。这是因为图像中的像素并非随机设置；根据图像代表的数字，像素组更有可能是明亮或黑暗的。在
    2006 年的论文*A Fast Learning Algorithm for Deep Belief Nets*中，作者假设可以通过计算一个*互补先验*来解决这个问题，该先验与似然完全相反，从而抵消这种相关性，并使后验也独立。
- en: 'To compute this *complementary prior*, we could use the posterior distribution
    over hidden units in a higher layer. The trick to generating such distributions
    is in a greedy, layer-wise procedure for "priming" the network of stacked RBMs
    in a multi-layer generative model, such that the weights can then be fine-tuned
    as a classification model. For example, let''s consider a three-layer model for
    the MNIST data (*Figure 4.9*):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算这个*互补先验*，我们可以使用一个更高层次的隐藏单元的后验分布。生成这种分布的技巧在一个贪婪的、逐层的程序中，用于在多层生成模型中“初始化”堆叠的
    RBM 网络，从而可以将权重微调为分类模型。例如，让我们考虑一个用于 MNIST 数据的三层模型（*图 4.9*）：
- en: '![](img/B16176_04_09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_09.png)'
- en: 'Figure 4.9: DBN architecture based on "A fast learning algorithm for deep belief
    nets" by Hinton et al.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9：基于 "A fast learning algorithm for deep belief nets" 的 DBN 架构由 Hinton 等人提出。
- en: The two 500-unit layers form representations of the MNIST digits, while the
    2000- and 10-unit layers are "associative memory" that correlates labels with
    the digit representation. The two first layers have directed connections (different
    weights) for upsampling and downsampling, while the top layers have undirected
    weights (the same weight for forward and backward passes).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 500 单元层形成了 MNIST 数字的表示，而 2000 和 10 单元层是“关联记忆”，将标签与数字表示相关联。前两层具有定向连接（不同的权重）用于上采样和下采样，而顶层具有无向权重（前向和后向传递使用相同的权重）。
- en: This model could be learned in stages. For the first 500-unit RBM, we would
    treat it as an undirected model by enforcing that the forward and backward weights
    are equal; we would then use CD to learn the parameters of this RBM. We would
    then fix these weights and learn a *second* (500-unit) RBM that uses the hidden
    units from the first layer as input "data," and repeat for the 2000-layer unit.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以分阶段学习。对于第一个500单元RBM，我们会将其视为一个无向模型，强制前向和反向权重相等；然后我们将使用CD来学习这个RBM的参数。然后，我们会固定这些权重，并学习一个*第二个*（500单元）RBM，它使用第一层的隐藏单元作为输入“数据”，然后重复这个过程，直到2000层。
- en: After we have "primed" the network, we no longer need to enforce that the weights
    in the bottom layers are tied, and can fine-tune the weights using an algorithm
    known as "wake-sleep."^(23)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们“启动”网络之后，我们就不再需要强制底层的权重是绑定的，并且可以使用称为“wake-sleep”的算法来微调权重。^(23)
- en: Firstly, we take input data (the digits) and compute the activations of the
    other layers all the way up until the connections between the 2000- and 10-unit
    layers. We compute updates to the "generative weights" (those that compute the
    activations that yield image data from the network) pointing downward using the
    previously given gradient equations. This is the "wake" phase because if we consider
    the network as resembling a biological sensory system, then it receives input
    from the environment through this forward pass.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，我们接受输入数据（数字）并计算其他层的激活，一直到2000个单元和10个单元层之间的连接。我们使用先前给出的梯度方程计算指向下的“生成权重”（计算从网络生成图像数据的激活）的更新。这是“唤醒”阶段，因为如果我们将网络视为类似生物感知系统，则它通过这个前向传递从环境中接收输入。 '
- en: For the 2000- and 10-unit layers, we use the sampling procedure for CD using
    the second 500-unit layer's output as "data" to update the undirected weights.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于2000个单元和10个单元的层，我们使用CD的采样过程，使用第二个500单元层的输出作为“数据”来更新无向权重。
- en: We then take the output of the 2000-layer unit and compute activations downward,
    updating the "recognition weights" (those that compute activations that lead to
    the classification of the image into one of the digit classes) pointing upward.
    This is called the "sleep" phase because it displays what is in the "memory" of
    the network, rather than taking data from outside.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取2000层单元的输出并向下计算激活，更新指向上的“识别权重”（计算激活以将图像分类为数字类别之一的权重）。这被称为“睡眠”阶段，因为它显示的是网络的“记忆”，而不是从外部获取数据。
- en: We then repeat *these steps* until convergence.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重复*这些步骤*直到收敛。
- en: Note that in practice, instead of using undirected weights in the top layers
    of the network, we could replace the last layer with directed connections and
    a softmax classifier. This network would then technically no longer be a DBN,
    but rather a regular Deep Neural Network that we could optimize with backpropagation.
    This is an approach we will take in our own code, as we can then leverage TensorFlow's
    built-in gradient calculations, and it fits into the paradigm of the Model API.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在实践中，我们可以在网络的顶层替换最后一层的无向权重为有向连接和softmax分类器。这个网络在技术上就不再是DBN，而是一个可以用反向传播优化的普通深度神经网络。这是我们在自己的代码中要采取的方法，因为我们可以利用TensorFlow内置的梯度计算，并且它符合模型API的范例。
- en: Now that we have covered the theoretical background to understand how a DBN
    is trained and how the pre-training approach resolves issues with the "explaining-away"
    effect, we will implement the whole model in code, showing how we can leverage
    TensorFlow 2's gradient tape functionality to implement CD as a custom learning
    algorithm.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了DBN的训练方式以及预训练方法如何解决“解释”效应的问题的理论背景，我们将在代码中实现整个模型，展示如何利用TensorFlow 2的梯度带功能来实现CD作为自定义学习算法。
- en: Creating an RBM using the TensorFlow Keras layers API
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow Keras层API创建RBM
- en: Now that you have an appreciation of some of the theoretical underpinnings of
    the RBM, let's look at how we can implement it using the TensorFlow 2.0 library.
    For this purpose, we will represent the RBM as a custom layer type using the Keras
    layers API.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了RBM的一些理论基础，让我们看看如何使用TensorFlow 2.0库实现它。为此，我们将使用Keras层API将RBM表示为自定义层类型。
- en: Code in this chapter was adapted to TensorFlow 2 from the original Theano (another
    deep learning Python framework) code from deeplearning.net.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码是从deeplearning.net的原始Theano代码转换到TensorFlow 2的。
- en: 'Firstly, we extend `tf.keras.layer`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们扩展`tf.keras.layer`：
- en: '[PRE9]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We input a number of hidden units, visible units, a learning rate for CD updates,
    and the number of steps to take with each CD pass. For the layers API, we are
    only required to implement two functions: `build()` and `call()`. `build()` is
    executed when we call `model.compile()`, and is used to initialize the weights
    of the network, including inferring the right size of the weights given the input
    dimensions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入一定数量的隐藏单元、可见单元、用于 CD 更新的学习率以及每次 CD 传递中采取的步骤数。对于层 API，我们只需要实现两个函数：`build()`
    和 `call()`。当我们调用 `model.compile()` 时执行 `build()`，并用于初始化网络的权重，包括根据输入维度推断权重的正确大小：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We also need a way to perform both forward and reverse samples from the model.
    For the forward pass, we need to compute sigmoidal activations from the input,
    and then stochastically turn the hidden units on or off based on the activation
    probability between 1 and 0 given by that sigmoidal activation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一种方法来执行模型的前向和反向采样。对于前向传播，我们需要从输入计算 S 型激活，然后根据由该 S 型激活给出的介于 1 和 0 之间的激活概率，随机打开或关闭隐藏单元：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Likewise, we need a way to sample in reverse for the visible units:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们需要一种方式来为可见单元进行反向采样：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We also implement `call()` in the RBM class, which provides the forward pass
    we would use if we were to use the `fit()` method of the Model API for backpropagation
    (which we can do for fine-tuning later in our deep belief model):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 RBM 类中实现了 `call()`，它提供了我们将在深度信念模型的微调中使用 `fit()` 方法时使用的前向传播：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To actually implement CD learning for each RBM, we need to create some additional
    functions. The first calculates the free energy, as you saw in the Gibbs measure
    earlier in this chapter:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要为每个受限玻尔兹曼机实际实现 CD 学习，我们需要创建一些额外的函数。第一个函数计算自由能，就像你在本章前面看到的 Gibbs 测度那样：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note here that we could have used the Bernoulli distribution from `tensorflow_probability`
    in order to perform this sampling, using the sigmoidal activations as the probabilities;
    however, this is slow and would cause performance issues when we need to repetitively
    sample during CD learning. Instead, we use a speedup in which we sample an array
    of uniform random numbers the same size as the sigmoidal array and then set the
    hidden unit as 1 if it is greater than the random number. Thus, if a sigmoidal
    activation is 0.9, it has a 90% probability of being greater than a randomly sampled
    uniform number, and is set to "on." This has the same behavior as sampling a Bernoulli
    variable with a probability of 0.9, but is computationally much more efficient.
    The reverse and visible samples are computed similarly. Finally, putting these
    together allows us to perform both forward and reverse Gibbs samples:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们本可以使用 `tensorflow_probability` 中的伯努利分布来执行此采样，使用 S 型激活作为概率；然而，这样做速度很慢，在进行
    CD 学习时会导致性能问题。相反，我们使用了一种加速方法，在这种方法中，我们对与 S 型数组大小相同的均匀随机数数组进行采样，然后如果隐藏单元大于随机数，则将其设置为
    1。因此，如果 S 型激活为 0.9，则它有 90% 的概率大于随机抽样的均匀数，并被设置为“打开”。这与以概率 0.9 采样伯努利变量的行为相同，但在计算上要高效得多。反向和可见样本的计算方式类似。最后，将这些放在一起允许我们执行前向和后向
    Gibbs 采样：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To perform the CD updates, we make use of TensorFlow 2''s eager execution and
    the `GradientTape` API you saw in *Chapter 3*, *Building Blocks of Deep Neural
    Networks*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行 CD 更新，我们利用 TensorFlow 2 的即时执行和 *第 3 章*，*深度神经网络的构建块* 中看到的 `GradientTape`
    API：
- en: '[PRE16]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We perform one or more sample steps, and compute the cost using the difference
    between the free energy of the data and the reconstructed data (which is cast
    as a constant using `tf.constant` so that we don''t treat it as a variable during
    autogradient calculation). We then compute the gradients of the three weight matrices
    and update their values, before returning our reconstruction cost as a way to
    monitor progress. The reconstruction cost is simply the cross-entropy loss between
    the input and reconstructed data:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行一步或多步样本，并使用数据的自由能与重建数据之间的差异计算成本（使用 `tf.constant` 将其转换为常数，以便在自动梯度计算期间不将其视为变量）。然后，我们计算三个权重矩阵的梯度并更新其值，然后返回我们的重建成本作为监视进度的一种方式。重建成本只是输入和重建数据之间的交叉熵损失：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'which represents the formula:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表着公式：
- en: '![](img/B16176_04_021.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_021.png)'
- en: where *y* is the target label, *y-hat* is the estimated label from the softmax
    function, and *N* is the number of elements in the dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 *y* 是目标标签，*y-hat* 是从 softmax 函数估计的标签，*N* 是数据集中元素的数量。
- en: Note that we enforce the weights being equal by copying over the transposed
    value of the updated (recognition) weights into the generative weights. Keeping
    the two sets of weights separate will be useful later on when we perform updates
    only on the recognition (forward) or generative (backward) weights during the
    wake-sleep procedure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过将更新（识别）权重的转置值复制到生成权重中来强制权重相等。在后续的唤醒-睡眠过程中，保持两组权重分开将会很有用，因为我们只会对识别（前向）或生成（反向）权重进行更新。
- en: 'Putting it all together, we can initialize an RBM with 500 units like in Hinton''s
    paper24, call `build()` with the shape of the flattened MNIST digits, and run
    successive epochs of training:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们可以像在 Hinton 的论文24 中那样初始化一个具有 500 个单位的 RBM，调用 `build()` 并传递 MNIST
    数字的扁平化形状，并运行连续的训练周期：
- en: '[PRE18]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After ~25 steps, the model should converge, and we can inspect the results.
    One parameter of interest is the weight matrix *w*; the shape is 784 (28x28) by
    500, so we could see each "column" as a 28x28 filter, similar to the kernels in
    the convolutional networks we studied in *Chapter 3*, *Building Blocks of Deep
    Neural Networks*. We can visualize a few of these to see what kinds of patterns
    they are recognizing in the images:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 大约 25 步后，模型应该会收敛，我们可以检查结果。一个感兴趣的参数是权重矩阵 *w*；形状为 784（28x28）乘以 500，因此我们可以将每个“列”看作是一个
    28x28 的滤波器，类似于我们在 *第 3 章*，*深度神经网络的构建块* 中学习的卷积网络中的卷积核。我们可以可视化其中一些，看看它们在图像中识别出了什么样的模式：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This provides a set of filters:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一组滤波器：
- en: '![](img/B16176_04_10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_10.png)'
- en: 'Figure 4.10: DBN filters after training'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：训练后的 DBN 滤波器
- en: 'We can see that these filters appear to represent different shapes that we
    would find in a digit image, such as curves or lines. We can also observe the
    reconstruction of the images by sampling from our data:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这些滤波器似乎代表了我们在数字图像中找到的不同形状，比如曲线或线条。我们还可以通过从我们的数据中进行采样观察图像的重建：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/B16176_04_11.png)![](img/B16176_04_12.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_04_11.png)![](img/B16176_04_12.png)'
- en: 'Figure 4.11: Original (right) and reconstructed (left) digits from DBN'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：DBN 中的原始（右）和重建（左）数字
- en: We can see in *Figure 4.11* that the network has nicely captured the underlying
    data distribution, as our samples represent a recognizable binary form of the
    input images. Now that we have one layer working, let's continue by combining
    multiple RBMs in layers to create a more powerful model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 *图 4.11* 中看到，网络已经很好地捕捉到了底层的数据分布，因为我们的样本代表了输入图像的可识别的二进制形式。现在我们已经有了一个工作层，让我们继续将多个
    RBM 结合在一起以创建一个更强大的模型。
- en: Creating a DBN with the Keras Model API
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 模型 API 创建 DBN
- en: You have now seen how to create a single-layer RBM to generate images; this
    is the building block required to create a full-fledged DBN. Usually, for a model
    in TensorFlow 2, we only need to extend `tf.keras.Model` and define an initialization
    (where the layers are defined) and a `call` function (for the forward pass). For
    our DBN model, we also need a few more custom functions to define its behavior.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到如何创建一个单层 RBM 来生成图像；这是创建一个完整的 DBN 所需的基本模块。通常情况下，对于 TensorFlow 2 中的模型，我们只需要扩展
    `tf.keras.Model` 并定义一个初始化（其中定义了层）和一个 `call` 函数（用于前向传播）。对于我们的 DBN 模型，我们还需要一些自定义函数来定义其行为。
- en: 'First, in the initialization, we need to pass a list of dictionaries that contain
    the parameters for our RBM layers (`number_hidden_units`, `number_visible_units`,
    `learning_rate`, `cd_steps`):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在初始化中，我们需要传递一个包含我们的 RBM 层参数的字典列表（`number_hidden_units`、`number_visible_units`、`learning_rate`、`cd_steps`）：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Note at the same time that we also initialize a set of sigmoidal dense layers
    with a softmax at the end, which we can use for fine-tuning through backpropagation
    once we''ve trained the model using the generative procedures outlined earlier.
    To train the DBN, we begin a new code block to start the generative learning process
    for the stack of RBMs:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时请注意，我们还初始化了一组带有 softmax 的 sigmoid 密集层，我们可以在使用之前概述的生成过程训练模型后通过反向传播进行微调。要训练
    DBN，我们开始一个新的代码块来启动 RBM 堆栈的生成学习过程：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Notice that for computational efficiency, we generate the input for each layer
    past the first by passing every datapoint through the prior layer in a forward
    pass using the `map()` function for the Dataset API, instead of having to generate
    these forward samples repeatedly. While this takes more memory, it greatly reduces
    the computation required. Each layer in the pre-training loop calls back to the
    CD loop you saw before, which is now a member function of the DBN class:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算效率，我们通过使用 Dataset API 中的 `map()` 函数，在前向传递中将每个数据点通过前一层传递以生成除第一层以外每一层的输入，而不是反复生成这些前向样本。尽管这需要更多的内存，但大大减少了所需的计算量。预训练循环中的每一层都会回调到你之前看到的
    CD 循环，它现在是 DBN 类的成员函数：
- en: '[PRE23]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have pre-trained in a greedy manner, we can proceed to the wake-sleep
    step. We start with the upward pass:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们以贪婪的方式进行了预训练，我们就可以进行`wake-sleep`步骤。我们从向上传递开始：
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Again, note that we gather a list of the transformed forward passes at each
    stage so that we have the necessary inputs for the update formula. We''ve now
    added a function, `wake_update`, to the RBM class, which will compute updates
    only for the generative (downward) weights, in every layer except the last (the
    associate, undirected connections):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，我们收集了在每个阶段转换的前向传递的列表，以便我们具有更新公式所需的必要输入。我们现在已经向 RBM 类添加了一个函数，`wake_update`，它将仅为生成（向下）权重计算更新，即除了最后一层（关联的，无向连接）之外的每一层：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is almost identical to the CD update, except that we are only updating
    the generative weights and the visible unit bias terms. Once we compute the forward
    pass, we then perform a contrastive update on the associate memory in the top
    layer:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 CD 更新几乎相同，不同之处在于我们仅更新生成权重和可见单元偏置项。一旦我们计算了前向传递，然后对顶层的关联内存执行对比更新：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then need to compute the data for the `reverse` pass of the wake-sleep algorithm;
    we do this by again applying a mapping to the last layer input:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要计算`wake-sleep`算法的逆向传递数据；我们通过再次对最后一层的输入应用映射来实现这一点：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For the sleep pass, we need to traverse the RBM in reverse, updating only the
    non-associative (undirected) connections. We first need to map the required input
    for each layer in reverse:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于睡眠传递，我们需要反向遍历 RBM，仅更新非关联（无向）连接。我们首先需要逆向映射每一层所需的输入：
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then we perform a backward traversal of the layers, only updating the non-associative
    connections:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对层进行反向遍历，仅更新非关联连接：
- en: '[PRE29]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once we are satisfied with the training progress, we can tune the model further
    using normal backpropagation. The last step in the wake-sleep procedure is to
    set all the dense layers with the results of the trained weights from the RBM
    layers:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对训练进展满意，我们就可以使用常规反向传播进一步调整模型。`wake-sleep`过程中的最后一步是将所有稠密层设置为来自 RBM 层的训练权重的结果：
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We have included a forward pass for a neural network in the DBN class using
    the call `function()`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 DBN 类中使用 `function()` 调用包含了神经网络的前向传递：
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This can be used in the `fit()` call in the TensorFlow API:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在 TensorFlow API 中的 `fit()` 调用中使用：
- en: '[PRE32]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This begins to train the now pre-trained weights using backpropagation, to fine-tune
    the discriminative power of the model. One way to conceptually understand this
    fine-tuning is that the pre-training procedure guides the weights to a reasonable
    configuration that captures the "shape" of the data, which backpropagation can
    then tune for a particular classification task. Otherwise, starting from a completely
    random weight configuration, the parameters are too far from capturing the variation
    in the data to be efficiently navigated to an optimal configuration through backpropagation
    alone.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这开始使用反向传播来训练现在预先训练过的权重，以微调模型的判别能力。概念上理解这种微调的一种方式是，预训练过程引导权重到一个合理的配置，以捕捉数据的“形状”，然后反向传播可以调整这些权重以适应特定的分类任务。否则，从完全随机的权重配置开始，参数距离捕捉数据中的变化太远，无法通过单独的反向传播有效地导航到最佳配置。
- en: You have seen how to combine multiple RBMs in layers to create a DBN, and how
    to run a generative learning process on the end-to-end model using the TensorFlow
    2 API; in particular, we made use of the gradient tape to allow us to record and
    replay the gradients using a non-standard optimization algorithm (for example,
    not one of the default optimizers in the TensorFlow API), allowing us to plug
    a custom gradient update into the TensorFlow framework.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了如何将多个 RBM 结合在层中创建 DBN，并如何使用 TensorFlow 2 API 在端到端模型上运行生成式学习过程；特别是，我们利用梯度磁带允许我们记录和重放梯度使用非标准优化算法（例如，不是
    TensorFlow API 中的默认优化器之一），使我们能够将自定义梯度更新插入 TensorFlow 框架中。
- en: Summary
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about one of the most important models from the
    beginnings of the deep learning revolution, the DBN. You saw that DBNs are constructed
    by stacking together RBMs, and how these undirected models can be trained using
    CD.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了深度学习革命开始时最重要的模型之一，即 DBN。您看到 DBN 是通过堆叠 RBM 构建的，以及如何使用 CD 训练这些无向模型。
- en: The chapter then described a greedy, layer-wise procedure for priming a DBN
    by sequentially training each of a stack of RBMs, which can then be fine-tuned
    using the wake-sleep algorithm or backpropagation. We then explored practical
    examples of using the TensorFlow 2 API to create an RBM layer and a DBN model,
    illustrating the use of the `GradientTape` class to compute updates using CD.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节随后描述了一种贪婪的逐层过程，通过逐个训练一堆 RBM 来启动 DBN，然后可以使用唤醒-睡眠算法或反向传播进行微调。然后，我们探讨了使用 TensorFlow
    2 API 创建 RBM 层和 DBN 模型的实际示例，说明了使用`GradientTape`类计算使用 CD 更新的方法。
- en: You also learned how, following the wake-sleep algorithm, we can compile the
    DBN as a normal Deep Neural Network and perform backpropagation for supervised
    training. We applied these models to MNIST data and saw how an RBM can generate
    digits after training converges, and has features resembling the convolutional
    filters described in *Chapter 3*, *Building Blocks of Deep Neural Networks*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您还学习了如何根据唤醒-睡眠算法，将 DBN 编译为正常的深度神经网络，并对其进行监督训练的反向传播。我们将这些模型应用于 MNIST 数据，并看到 RBM
    在训练收敛后如何生成数字，并具有类似于第 3 章*深度神经网络的构建基块*中描述的卷积滤波器的特征。
- en: While the examples in the chapter involved significantly extending the basic
    layer and model classes of the TensorFlow Keras API, they should give you an idea
    of how to implement your own low-level alternative training procedures. Going
    forward, we will mostly stick to using the standard `fit()` and `predict()` methods,
    starting with our next topic, Variational Autoencoders, a sophisticated and computationally
    efficient way to generate image data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章中的示例显著扩展了 TensorFlow Keras API 的基本层和模型类，但它们应该能够让你了解如何实现自己的低级替代训练过程。未来，我们将主要使用标准的`fit()`和`predict()`方法，从我们的下一个主题开始，变分自动编码器，这是一种复杂且计算效率高的生成图像数据的方式。
- en: References
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). *Gradient-
    Based Learning Applied to Document Recognition*. Proceedings of the IEEE. 86 (11):
    2278–2324'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). *基于梯度的学习应用于文档识别*。IEEE
    会议录。86 (11): 2278–2324'
- en: LeCun, Yann; Corinna Cortes; Christopher J.C. Burges. *MNIST handwritten digit
    database, Yann LeCun, Corinna Cortes, and Chris Burges*
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LeCun, Yann; Corinna Cortes; Christopher J.C. Burges。 *MNIST 手写数字数据库，Yann LeCun，Corinna
    Cortes 和 Chris Burges*
- en: 'NIST''s original datasets: [https://www.nist.gov/system/files/documents/srd/nistsd19.pdf](https://www.nist.gov/system/files/documents/srd/nistsd19.pdf)'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NIST 的原始数据集：[https://www.nist.gov/system/files/documents/srd/nistsd19.pdf](https://www.nist.gov/system/files/documents/srd/nistsd19.pdf)
- en: '[https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png)'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png](https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/MnistExamples.png/440px-MnistExamples.png)'
- en: 'LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). *Gradient-Based
    Learning Applied to Document Recognition*. Proceedings of the IEEE. 86 (11): 2278–2324'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). *基于梯度的学习应用于文档识别*。IEEE
    会议录。86 (11): 2278–2324'
- en: D. Ciregan, U. Meier and J. Schmidhuber, (2012) *Multi-column deep neural networks
    for image classification*, 2012 IEEE Conference on Computer Vision and Pattern
    Recognition, pp. 3642-3649\. [https://ieeexplore.ieee.org/document/6248110](https://ieeexplore.ieee.org/document/6248110)
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Ciregan, U. Meier和J. Schmidhuber, (2012) *用于图像分类的多列深度神经网络*，2012年IEEE计算机视觉和模式识别会议，pp.
    3642-3649\. [https://ieeexplore.ieee.org/document/6248110](https://ieeexplore.ieee.org/document/6248110)
- en: Hinton GE, Osindero S, Teh YW. (2006) *A fast learning algorithm for deep belief
    nets*. Neural Comput. 18(7):1527-54\. [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton GE, Osindero S, Teh YW (2006) *深度信念网络的快速学习算法*。神经计算。18(7):1527-54\. [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
- en: 'Hebb, D. O. (1949). *The Organization of Behavior: A Neuropsychological Theory*.
    New York: Wiley and Sons'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hebb, D. O. (1949). *行为的组织：一个神经心理学理论*。纽约：Wiley and Sons
- en: Gurney, Kevin (2002). *An Introduction to Neural Networks*. Routledge
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gurney, Kevin (2002). *神经网络简介*. Routledge
- en: Sathasivam, Saratha (2008). *Logic Learning in Hopfield Networks*.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sathasivam, Saratha (2008). *霍普菲尔德网络中的逻辑学习*.
- en: 'Hebb, D. O.. *The organization of behavior: A neuropsychological theory*. Lawrence
    Erlbaum, 2002.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hebb, D. O.。*行为的组织：一个神经心理学理论*。劳伦斯埃尔巴姆出版，2002年
- en: Suzuki, Wendy A. (2005). *Associative Learning and the Hippocampus*. Psychological
    Science Agenda. American Psychological Association. [https://www.apa.org/science/about/psa/2005/02/suzuki](https://www.apa.org/science/about/psa/2005/02/suzuki)
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Suzuki, Wendy A. (2005). *联想学习与海马体*. 心理科学日程。美国心理协会。[https://www.apa.org/science/about/psa/2005/02/suzuki](https://www.apa.org/science/about/psa/2005/02/suzuki)
- en: 'Hammersley, J. M.; Clifford, P. (1971), Markov fields on finite graphs and
    lattices; Clifford, P. (1990), *Markov random fields in statistics*, in Grimmett,
    G. R.; Welsh, D. J. A. (eds.), *Disorder in Physical Systems: A Volume in Honour
    of John M. Hammersley*, Oxford University Press, pp. 19–32'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hammersley, J. M.; Clifford, P. (1971)，有限图和晶格上的马尔可夫场；Clifford, P. (1990)，*统计学中的马尔可夫随机场*，在Grimmett,
    G. R.; Welsh, D. J. A. (eds.)，*物理系统中的无序：纪念约翰M. Hammersley专著*，牛津大学出版社，pp. 19-32
- en: 'Ackley, David H; Hinton, Geoffrey E; Sejnowski, Terrence J (1985), *A learning
    algorithm for Boltzmann machines* (PDF), Cognitive Science, 9 (1): 147–169'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ackley, David H; Hinton, Geoffrey E; Sejnowski, Terrence J (1985)，*玻尔兹曼机的学习算法*
    (PDF)，认知科学，9(1)：147–169
- en: '*Boltzmann machine*. Wikipedia. Retrieved April, 26, 2021 from [https://en.wikipedia.org/wiki/Boltzmann_machine](https://en.wikipedia.org/wiki/Boltzmann_machine)'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*玻尔兹曼机*. 维基百科. 检索日期：2021年4月26日，来自[https://en.wikipedia.org/wiki/Boltzmann_machine](https://en.wikipedia.org/wiki/Boltzmann_machine)'
- en: 'Smolensky, Paul (1986). *Chapter 6: Information Processing in Dynamical Systems:
    Foundations of Harmony Theory* (PDF). In Rumelhart, David E.; McLelland, James
    L. (eds.). *Parallel Distributed Processing: Explorations in the Microstructure
    of Cognition*, Volume 1: Foundations. MIT Press. pp. 194–281'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Smolensky, Paul (1986). *第六章：动力系统中的信息处理：谐和理论的基础* (PDF). 在Rumelhart，David E.;
    McLelland, James L. (eds.) *平行分布式处理：认知微结构探索*，第1卷：基础。麻省理工学院出版社。pp.194–281
- en: Woodford O. *Notes on Contrastive Divergence*. [http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf](http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Woodford O. *对比散度笔记*. [http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf](http://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf)
- en: Hinton, G E. (2000). *Training Products of Experts by Minimizing Contrastive
    Divergence*. [http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf](http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, G E. (2000). *通过最小化对比散度来训练专家乘积*. [http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf](http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)
- en: Roux, N L., Bengio, Y. (2008). *Representational Power of Restricted Boltzmann
    Machines and Deep Belief Networks*. in Neural Computation, vol. 20, no. 6, pp.
    1631-1649\. [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf)
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Roux, N L.，Bengio, Y. (2008). *受限玻尔兹曼机和深度信念网络的表示能力*. 在神经计算，卷20，第6期，pp. 1631-1649\.
    [https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/representational_power.pdf)
- en: Hinton, G E. (2000). *Training Products of Experts by Minimizing Contrastive
    Divergence*. [http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf](http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, G E. (2000). *通过最小化对比散度来训练专家乘积*. [http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf](http://www.cs.utoronto.ca/~hinton/absps/nccd.pdf)
- en: Pearl J., Russell S. (2000). *BAYESIAN NETWORKS*. [https://ftp.cs.ucla.edu/pub/stat_ser/r277.pdf](https://ftp.cs.ucla.edu/pub/stat_ser/r277.pdf)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pearl J., Russell S. (2000). *贝叶斯网络*. [https://ftp.cs.ucla.edu/pub/stat_ser/r277.pdf](https://ftp.cs.ucla.edu/pub/stat_ser/r277.pdf)
- en: Hinton GE, Osindero S, Teh YW. (2006) *A fast learning algorithm for deep belief
    nets*. Neural Comput. 18(7):1527-54\. [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton GE, Osindero S, Teh YW. (2006) *深信念网络的快速学习算法*. Neural Comput. 18(7):1527-54\.
    [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
- en: Hinton GE, Osindero S, Teh YW. (2006) *A fast learning algorithm for deep belief
    nets*. Neural Comput. 18(7):1527-54\. [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton GE, Osindero S, Teh YW. (2006) *深信念网络的快速学习算法*. Neural Comput. 18(7):1527-54\.
    [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
- en: Hinton GE, Osindero S, Teh YW. (2006) *A fast learning algorithm for deep belief
    nets*. Neural Comput. 18(7):1527-54\. [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton GE, Osindero S, Teh YW. (2006) *深信念网络的快速学习算法*. Neural Comput. 18(7):1527-54\.
    [https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)
