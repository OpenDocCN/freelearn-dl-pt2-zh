- en: Transfer Learning with Modern Network Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用现代网络架构进行迁移学习
- en: In the previous chapter, we explored how deep learning algorithms can be used
    to create artistic images, create new images based on existing datasets, and generate
    text. In this chapter, we will introduce you to different network architectures
    that power modern computer vision applications and natural language systems. We
    will also cover how transfer learning can be incorporated into these models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了如何利用深度学习算法创建艺术图像、基于现有数据集生成新图像以及生成文本。在本章中，我们将介绍驱动现代计算机视觉应用和自然语言系统的不同网络架构。我们还将覆盖如何在这些模型中应用迁移学习。
- en: Transfer learning is a method in machine learning where a model that's been
    developed for a particular task is reused for another. If, for example, we wanted
    to learn how to drive a motorbike but we already know how to drive a car, we would
    transfer our knowledge about what driving a car involves to the new task rather
    than starting from scratch.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是机器学习中的一种方法，其中一个为特定任务开发的模型被重用于另一个任务。例如，如果我们想学习如何驾驶摩托车，但我们已经知道如何驾驶汽车，我们会将关于驾驶汽车的知识转移到新任务，而不是从头开始。
- en: To transfer such knowledge from one task to another, some of the layers in the
    network need to be frozen. Freezing a layer means that the weights of the layer
    won't update during training. The benefit of transfer learning is that it can
    speed up the time that's taken to develop and train a new model by reusing what
    was learned by the pretrained model, thereby helping to accelerate the results.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这种知识从一个任务转移到另一个任务，网络中的一些层需要被冻结。冻结一层意味着在训练期间不会更新该层的权重。迁移学习的好处在于，它可以通过重复使用预训练模型所学到的知识来加快开发和训练新模型的时间，从而加速结果的产生。
- en: 'Some of the architectures that we will look at in this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论的一些架构如下：
- en: '**Residual network** (**ResNet**)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差网络** (**ResNet**)'
- en: Inception
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception
- en: DenseNet
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet
- en: Encoder-decoder architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: 'The following topics will be covered in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Modern network architectures
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代网络架构
- en: Densely connected convolutional networks – DenseNet
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集连接卷积网络 – DenseNet
- en: Model ensembling
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型集成
- en: Encoder-decoder architecture
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: Modern network architectures
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代网络架构
- en: One of the best things we do when the deep learning model fails to learn is
    add more layers to the model. As you add layers, the model's accuracy improves
    and then starts saturating. However, adding more layers beyond a certain number
    will introduce certain challenges, such as vanishing or exploding gradients. This
    is partially solved by carefully initializing weights and introducing intermediate
    normalizing layers. Modern architectures, such as ResNet and Inception, try to
    solve this problem by introducing different techniques, such as residual connections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当深度学习模型无法学习时，我们最好的做法之一是向模型添加更多的层。随着层数的增加，模型的准确性会提高，然后开始饱和。然而，超过一定数量的层会引入一些挑战，例如梯度消失或梯度爆炸。通过精心初始化权重和引入中间的规范化层，部分解决了这个问题。现代架构，如ResNet和Inception，通过引入不同的技术，如残差连接，试图解决这个问题。
- en: ResNet
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet
- en: 'ResNet was first introduced in 2015 in a paper called *Deep Residual Learning
    for Image Recognition* by Kaiming He and et al. ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)).
    It makes it possible for us to train thousands of layers and achieve high performance.
    The core concept of ResNet is to introduce an identity shortcut connection that
    skips one or more of the layers. The following diagram depicts how ResNet works:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet首次在2015年由Kaiming He等人在名为《深度残差学习用于图像识别》的论文中提出（[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)）。它使我们能够训练成千上万层并实现高性能。ResNet的核心概念是引入一个跳过一个或多个层的身份快捷连接。下图展示了ResNet的工作原理：
- en: '![](img/ec6af942-3b2b-4c91-8133-7850c3bc892b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec6af942-3b2b-4c91-8133-7850c3bc892b.png)'
- en: 'This identity mapping doesn''t have any parameters. It is simply there to add
    the output from the previous layer to the next layer. However, sometimes, x and
    F(x) will not have the same dimensions. A convolution operation typically shrinks
    the spatial resolution of an image. For example, a 3 x 3 convolution on a 32 x
    32 image results in a 30 x 30 image. This identity mapping is multiplied by a
    linear projection, *W*, in order to expand the channels of the shortcut to match
    the residual. As such, the input, x, and F(x) need to be combined to create the
    input for the next layer:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此身份映射没有任何参数。它只是将前一层的输出添加到下一层的输入中。然而，有时候，x和F(x)将不具有相同的维度。卷积操作通常会缩小图像的空间分辨率。例如，对32
    x 32图像进行3 x 3卷积会得到一个30 x 30图像。此身份映射被线性投影*W*乘以，以扩展捷径的通道以匹配残差。因此，需要将输入x和F(x)结合起来创建下一层的输入：
- en: '![](img/19c4f7c5-4ffa-48e4-8f02-0f182357dad8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19c4f7c5-4ffa-48e4-8f02-0f182357dad8.png)'
- en: 'The following code demonstrates what a simple ResNet block would look like
    in PyTorch:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了PyTorch中简单ResNet块的样子：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `ResNetBasicBlock` class contains an `init` method that initializes all
    the different layers, such as the convolution layer and batch normalization. The
    forward method is almost identical to what we have seen up until now, except that
    the input is added to the layer's output just before it is returned.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResNetBasicBlock`类包含一个`init`方法，用于初始化各种层次，如卷积层和批量归一化。前向方法几乎与我们到目前为止看到的相同，只是在返回之前将输入添加到层次的输出中。'
- en: 'The PyTorch `torchvision` package provides an out-of-the-box ResNet model with
    different layers. Some of the different models that are available are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`torchvision`包提供了一个即插即用的ResNet模型，具有不同的层次结构。以下是一些可用的不同模型：
- en: ResNet-18
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-18
- en: ResNet-34
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-34
- en: ResNet-50
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-50
- en: ResNet-101
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-101
- en: ResNet-152
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet-152
- en: 'We can also use any of these models for transfer learning. The `torchvision`
    instance allows us to simply create one of these models and use it as shown in
    the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将这些模型之一用于迁移学习。`torchvision`实例允许我们简单地创建其中一个模型并像以下代码中所示使用它：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following diagram shows what a 34-layer ResNet model would look like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了一个34层的ResNet模型的样子：
- en: '![](img/70fe2fcf-af96-4e53-96a1-172475925d0d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70fe2fcf-af96-4e53-96a1-172475925d0d.png)'
- en: Here, we can see that this network consists of multiple ResNet blocks. A key
    advantage of these modern networks is that they need very few parameters compared
    to models such as VGG since they avoid using fully connected layers that need
    lots of parameters to train.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到该网络由多个ResNet块组成。与VGG等模型相比，这些现代网络的一个关键优势是它们需要很少的参数，因为它们避免使用需要大量参数训练的全连接层。
- en: 'Now, we will train a ResNet model on the dogs and cats dataset. We will use
    the data that we used in [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml),
    *Diving Deep into Neural Networks*, and will quickly train a model based on the
    features that have been calculated from the ResNet. As usual, we will follow these
    steps to train the model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在狗和猫的数据集上训练一个ResNet模型。我们将使用在[第三章](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml)中使用的数据，*深入神经网络*，并将基于从ResNet计算得到的特征快速训练一个模型。像往常一样，我们将按照以下步骤训练模型：
- en: Create the PyTorch datasets.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建PyTorch数据集。
- en: Create the loaders for training and validation.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和验证加载器。
- en: Create the ResNet model.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建ResNet模型。
- en: Extract the convolutional features.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取卷积特征。
- en: Create a custom PyTorch dataset class for the pre-convoluted features and loader.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为预卷积特征创建自定义PyTorch数据集类和加载器。
- en: Create a simple linear model.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个简单的线性模型。
- en: Train and validate the model.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和验证模型。
- en: Once done, we are going to repeat these steps for Inception and DenseNet. Finally,
    we will explore the ensembling technique, where we combine these powerful models
    to build a new model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们将重复这些步骤用于Inception和DenseNet。最后，我们将探索集成技术，将这些强大的模型组合成一个新模型。
- en: Creating PyTorch datasets
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建PyTorch数据集
- en: 'First, we need to create a transformation object that contains all the basic
    transformations required and use the `ImageFolder` function to load the images
    from the data directory that we created in [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml),
    *Diving Deep into Neural Networks*. In the following code, we create the datasets:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一个包含所有基本变换的变换对象，并使用`ImageFolder`函数从我们在[第三章](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml)中创建的数据目录加载图像。在以下代码中，我们创建数据集：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By now, most of the preceding code will be self-explanatory.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，前面大部分代码将是不言自明的。
- en: Creating loaders for training and validation
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于训练和验证的加载器
- en: 'We use PyTorch loaders to load the data provided by the dataset in the form
    of batches, along with all of its advantages, such as shuffling the data and using
    multithreads, to speed up the process. The following code demonstrates this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 PyTorch 加载器加载数据集提供的批量数据，以及其所有优势，如数据洗牌和使用多线程，以加快进程速度。以下代码展示了这一点：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We need to maintain the exact sequence of the data while calculating the pre-convoluted
    features. When we allow the data to be shuffled, we won't be able to maintain
    the labels. So, ensure that `shuffle` is `False`; otherwise, the required logic
    needs to be handled inside the code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算预卷积特征时需要保持数据的确切顺序。当允许数据被洗牌时，我们将无法保持标签。因此，请确保`shuffle`为`False`；否则，需要在代码内部处理所需的逻辑。
- en: Creating a ResNet model
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个 ResNet 模型
- en: 'Here, we will consider a coded example for creating a ResNet model. First,
    we initiate the pretrained `resnet34` model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将考虑一个创建 ResNet 模型的编码示例。首先，我们初始化预训练的`resnet34`模型：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we discard the last linear layer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们丢弃最后一个线性层：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the model has been created, we set the `requires_grad` parameter to `False` so
    that PyTorch doesn''t have to maintain any space for holding gradients:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，我们将`requires_grad`参数设为`False`，这样 PyTorch 就不必维护用于保存梯度的任何空间：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Extracting convolutional features
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取卷积特征
- en: 'Here, we pass the data from the train and validation data loaders through the
    model and store the results in a list for further computation. By calculating
    the pre-convoluted features, we can save a lot of time in training the model as
    we won''t be calculating these features in every iteration:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过模型传递来自训练和验证数据加载器的数据，并将结果存储在列表中以供进一步计算。通过计算预卷积特征，我们可以节省大量训练模型的时间，因为我们不会在每次迭代中计算这些特征：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Iterate through the train data and store the calculated features and the labels
    using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代通过训练数据，并使用以下代码存储计算得到的特征和标签：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For validation data, iterate through the validation data and store the calculated
    features and the labels:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证数据，迭代通过验证数据，并使用以下代码存储计算得到的特征和标签：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Creating a custom PyTorch dataset class for the pre-convoluted features and
    loader
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于预卷积特征的自定义 PyTorch 数据集类和加载器
- en: 'Now that we have calculated the pre-convoluted features, we need to create
    a custom dataset that can select data from them. Here, we will create a custom
    dataset and loader for the pre-convoluted features:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出了预卷积特征，我们需要创建一个自定义数据集，以便从中选择数据。在这里，我们将为预卷积特征创建一个自定义数据集和加载器：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the custom dataset for the pre-convoluted features has been created, we
    can use the `DataLoader` function, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了用于预卷积特征的自定义数据集，我们可以使用`DataLoader`函数，如下所示：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This will create a data loader for training and validation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为训练和验证创建一个数据加载器。
- en: Creating a simple linear model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个简单的线性模型
- en: 'Now, we need to create a simple linear model that will map the pre-convoluted
    features to the respective categories. In this example, there are two categories
    (dogs and cats):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个简单的线性模型，将预卷积特征映射到相应的类别。在这个例子中，有两个类别（狗和猫）：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, we are ready to train our new model and validate the dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备训练我们的新模型并验证数据集。
- en: Training and validating the model
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: 'The following code shows how we can train the model. Note that the `fit` function
    is the same as the one discussed in [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml),
    *Diving Deep into Neural Networks*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们如何训练模型。请注意，`fit`函数与 [第3章](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml)
    中讨论的 *深入神经网络* 的相同。
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Inception
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception
- en: 'The Inception network was an important milestone in the development of CNN
    classifiers as it was shown to improve both speed and accuracy. There have been
    a number of versions of Inception, with some of the most noteworthy being the
    following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络是 CNN 分类器发展中的一个重要里程碑，因为它在速度和准确性上都有所提高。Inception 有许多版本，其中一些最著名的版本包括以下几种：
- en: Inception v1 ([https://arxiv.org/pdf/1409.4842v1.pdf](https://arxiv.org/pdf/1409.4842v1.pdf)),
    commonly known as GoogLeNet
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception v1 ([https://arxiv.org/pdf/1409.4842v1.pdf](https://arxiv.org/pdf/1409.4842v1.pdf))，通常称为
    GoogLeNet
- en: Inception v2 and v2 ([https://arxiv.org/pdf/1512.00567v3.pdf](https://arxiv.org/pdf/1512.00567v3.pdf))
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception v2 和 v2 ([https://arxiv.org/pdf/1512.00567v3.pdf](https://arxiv.org/pdf/1512.00567v3.pdf))
- en: Inception v4 and Inception-ResNet ([https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf))
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception v4 和 Inception-ResNet ([https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf))
- en: 'The following diagram shows how the naive Inception network is structured (v1):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了朴素 Inception 网络的结构（v1）：
- en: '![](img/7243507d-5680-4082-af07-6fca7a80b89c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7243507d-5680-4082-af07-6fca7a80b89c.png)'
- en: 'Image source: [https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)
- en: 'Here, the convolution of different sizes is applied to the input, and the outputs
    of all these layers are concatenated. This is the simplest version of an Inception
    module. There is another variant of an Inception block where we pass the input
    through a 1 x 1 convolution before passing it through 3 x 3 and 5 x 5 convolutions.
    A 1 x 1 convolution is used for dimensionality reduction. It helps in solving
    computational bottlenecks. A 1 x 1 convolution looks at one value at a time and
    across the channels. For example, using a 10 x 1 x 1 filter on an input size of
    100 x 64 x 64 would result in 10 x 64 x 64\. The following diagram shows the Inception
    block with dimensionality reductions:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，不同大小的卷积应用于输入，并且所有这些层的输出被连接在一起。这是一个 Inception 模块的最简单版本。还有另一种变体的 Inception
    块，我们在通过 3 x 3 和 5 x 5 卷积之前会先通过 1 x 1 卷积来降低维度。1 x 1 卷积用于降低计算瓶颈。1 x 1 卷积一次查看一个值，跨通道。例如，在输入大小为
    100 x 64 x 64 上使用 10 x 1 x 1 的滤波器将导致 10 x 64 x 64 的输出。以下图表显示了带有降维的 Inception 块：
- en: '![](img/43677ad4-6c85-43cb-817a-29b9f9a21663.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43677ad4-6c85-43cb-817a-29b9f9a21663.png)'
- en: 'Image source: [https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)
- en: 'Now, let''s look at a PyTorch example of what the preceding Inception block
    would look like:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 PyTorch 中上述 Inception 块的示例：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code contains two classes: `BasicConv2d` and `InceptionBasicBlock`.
    `BasicConv2d` acts like a custom layer that applies a two-dimensional convolution
    layer, batch normalization, and a ReLU layer to the input that is passed through.
    It is good practice to create a new layer when we have a repeating code structure,
    in order to make the code look elegant.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码包含两个类：`BasicConv2d` 和 `InceptionBasicBlock`。`BasicConv2d` 充当自定义层，将二维卷积层、批量归一化和
    ReLU 层应用于输入。当我们有重复的代码结构时，创建新的层是很好的做法，以使代码看起来更优雅。
- en: 'The `InceptionBasicBlock` class implements what we have in the second Inception
    diagram. Let''s go through each smaller snippet and try to understand how it is
    implemented:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`InceptionBasicBlock` 类实现了第二个 Inception 图表中的内容。让我们逐个查看每个较小片段，并试图理解其如何实现：'
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code transforms the input by applying a 1 x 1 convolution block:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，通过应用一个 1 x 1 卷积块来转换输入：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block, followed by a 5 x 5 convolution block:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们通过应用一个 1 x 1 卷积块，然后是一个 5 x 5 卷积块来转换输入：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block, followed by a 3 x 3 convolution block:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们通过应用一个 1 x 1 卷积块，然后是一个 3 x 3 卷积块来转换输入：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, we apply an average pool, along with a 1 x 1 convolution
    block. Finally, we concatenate all the results together. An Inception network
    consists of several Inception blocks. The following diagram shows what an Inception
    architecture would look like:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们应用了平均池化以及一个 1 x 1 卷积块。最后，我们将所有结果连接在一起。一个 Inception 网络由多个 Inception
    块组成。以下图表显示了 Inception 架构的外观：
- en: '![](img/b6f49a1f-4121-458d-bbee-f7e4034f24b6.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6f49a1f-4121-458d-bbee-f7e4034f24b6.png)'
- en: The Inception architecture
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception 架构
- en: 'The `torchvision` package has an Inception network that can be used in the
    same way as we used the ResNet network. Many improvements were made to the initial
    Inception block, and the current implementation that''s available from PyTorch
    is Inception v3\. Let''s look at how we can use the Inception v3 model from `torchvision`
    to calculate pre-computed features. We won''t go through the data loading process
    as we will be using the same data loaders from the *Creating a ResNet model* section.
    We will look at the following important topics:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision` 包含一个可以像使用 ResNet 网络一样使用的 Inception 网络。对初始 Inception 块进行了许多改进，PyTorch
    提供的当前实现是 Inception v3。让我们看看如何从 `torchvision` 使用 Inception v3 模型来计算预计算特征。我们不会再次介绍数据加载过程，因为我们将使用与
    *创建一个 ResNet 模型* 部分相同的数据加载器。我们将查看以下重要主题：'
- en: Creating an Inception model
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 Inception 模型
- en: Extracting convolutional features using `register_forward_hook`
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `register_forward_hook` 提取卷积特征
- en: Creating a new dataset for the convoluted features
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为卷积特征创建一个新的数据集
- en: Creating a fully connected model
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个全连接模型
- en: Training and validating the model
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: Creating an Inception model
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个 Inception 模型
- en: 'The Inception v3 model has two branches, each of which generates an output,
    and in the original model training, we would merge the losses, just like we did
    for style transfer. At the moment, we are interested in using only one branch
    to calculate pre-convoluted features using Inception. Going into the details of
    this is outside the scope of this book. If you are interested in finding out more
    about how this works, then going through the paper and the source code ([https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py))
    of the Inception model would help. We can disable one of the branches by setting
    the `aux_logits` parameter to `False`. The following code explains how to create
    a model and how to set the `aux_logits` parameter to `False`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v3 模型有两个分支，每个分支生成一个输出，在原始模型训练中，我们会合并损失，就像风格迁移一样。目前，我们只关心使用一个分支计算 Inception
    的预卷积特征。详细说明超出本书的范围。如果您有兴趣了解更多如何工作的内容，则查阅论文和 Inception 模型的源代码 ([https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py))
    会有所帮助。我们可以通过将 `aux_logits` 参数设置为 `False` 来禁用其中一个分支。下面的代码解释了如何创建模型以及如何将 `aux_logits`
    参数设置为 `False`：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Extracting the convolution features from the Inception model isn't straightforward,
    so we will use the `register_forward_hook` function to extract the activations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Inception 模型中提取卷积特征并不简单，因此我们将使用 `register_forward_hook` 函数来提取激活值。
- en: Extracting convolutional features using register_forward_hook
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `register_forward_hook` 提取卷积特征
- en: 'We will be using the same techniques that we used to calculate activations
    for style transfer. The following is the `LayerActivations` class with some minor
    modifications since we are only interested in extracting the outputs of a particular
    layer:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与计算风格迁移激活值相同的技术。以下是 `LayerActivations` 类的代码，进行了一些小的修改，因为我们只关心提取特定层的输出：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Apart from the `hook` function, the rest of the code is similar to what we
    used for style transfer. Since we are capturing the outputs of all the images
    and storing them, we won''t be able to hold the data on the **graphics processing
    unit** (**GPU**) memory. Therefore, we need to extract the tensors from the GPU
    and CPU and just store the tensors instead of `Variable`. We are converting them
    back into tensors since the data loaders will only work with tensors. In the following
    code, we''re using the objects of `LayerActivations` to extract the output of
    the Inception model from the last layer, excluding the average pooling layer,
    the dropout layer, and the linear layer. We are skipping the average pooling layer
    to avoid losing useful information in the data:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `hook` 函数外，其余代码与我们用于风格迁移的代码类似。因为我们捕获了所有图像的输出并将它们存储起来，所以不能将数据保存在**图形处理单元**（**GPU**）内存中。因此，我们需要从
    GPU 和 CPU 提取张量并仅存储张量而不是 `Variable`。我们将它们重新转换为张量，因为数据加载器只能处理张量。在以下代码中，我们使用 `LayerActivations`
    对象从 Inception 模型的最后一层提取输出，跳过了平均池化层、dropout 层和线性层。我们跳过平均池化层以避免在数据中丢失有用信息：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let's create the datasets and loaders that are required for the new convoluted
    features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建所需的新卷积特征数据集和加载器。
- en: Creating a new dataset for the convoluted features
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于卷积特征的新数据集
- en: 'We can use the same `FeaturesDataset` class to create the new dataset and data
    loaders. In the following code, we''re creating the datasets and the loaders:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的`FeaturesDataset`类来创建新的数据集和数据加载器。在以下代码中，我们正在创建数据集和加载器：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let's create a new model that we can train on the pre-convoluted features.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新模型，我们可以在预卷积特征上训练。
- en: Creating a fully connected model
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个全连接模型
- en: 'A simple model may end up overfitting, so let''s include dropout in the model.
    Dropout will help us avoid overfitting. In the following code, we are creating
    our model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的模型可能会导致过拟合，因此让我们在模型中包含dropout。Dropout将帮助我们避免过拟合。在以下代码中，我们正在创建我们的模型：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once the model has been created, we can train the model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型创建完成，我们就可以开始训练模型。
- en: Training and validating the model
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: 'Here, we will use the same fit and training logic that we used in our ResNet
    example. We''re only going to look at the training code and the results it returns:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用与我们的ResNet示例中相同的拟合和训练逻辑。我们只会看一下训练代码和它返回的结果：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will result in the following output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Looking at the results, the Inception model achieves 99% accuracy on the training
    dataset and 97.8% accuracy on the validation dataset. Since we are precomputing
    and holding all the features in memory, it takes less than a few minutes to train
    the models. If you are running out of memory when you run the program on your
    machine, then you may need to avoid holding the features in memory.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果，Inception模型在训练数据集上达到了99%的准确率，在验证数据集上达到了97.8%的准确率。由于我们预先计算并保存了所有特征在内存中，所以训练模型只需不到几分钟。如果您在运行程序时遇到内存不足的问题，则可能需要避免在内存中保存特征。
- en: In the next section, we will look at another interesting architecture, DenseNet,
    which has become very popular in the last year.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到另一个有趣的架构，DenseNet，这在过去一年中变得非常流行。
- en: Densely connected convolutional networks – DenseNet
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集连接卷积网络 – DenseNet
- en: 'Some of the most successful and popular architectures, such as ResNet and Inception,
    have shown the importance of deeper and wider networks. ResNet uses shortcut connections
    to build deeper networks. DenseNet has taken this to a whole new level by allowing
    connections to be made from each layer to the subsequent layers, that is, the
    layers where we can receive all the feature maps from the previous layers. Symbolically,
    this would look as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最成功和最流行的架构，如ResNet和Inception，显示了更深更宽网络的重要性。ResNet使用快捷连接来构建更深的网络。DenseNet通过允许从每一层到后续层的连接，即我们可以接收来自前一层的所有特征映射的层，将这一点推到了一个全新的水平。符号上看，它会如下所示：
- en: '![](img/adc8b7d8-c009-4882-83bf-55122b79de47.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adc8b7d8-c009-4882-83bf-55122b79de47.png)'
- en: 'The following diagram describes what a five-layer dense block would look like:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了一个五层密集块的外观：
- en: '![](img/4491f48d-c888-46ae-acfe-ae908a038f43.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4491f48d-c888-46ae-acfe-ae908a038f43.png)'
- en: 'Image source: [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)
- en: There is also a DenseNet implementation of `torchvision` ([https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)).
    Let's look at two of its major functionalities, that is, `_DenseBlock` and `_DenseLayer`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`还有一个DenseNet的实现([https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py))。让我们看一下它的两个主要功能，即`_DenseBlock`和`_DenseLayer`。'
- en: The _DenseBlock object
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`_DenseBlock`对象'
- en: 'Let''s look at the code for `_DenseBlock` and then walk through it:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`_DenseBlock`的代码，然后逐步解析它：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`_DenseBlock` is a sequential module where we add layers in a sequential order.
    Based on the number of layers (`number_layers`) in the block, we add that number
    of `_DenseLayer` objects, along with a name, to it. All the magic happens inside
    the `_DenseLayer` object. Let''s look at what goes on inside the `DenseLayer`
    object.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`_DenseBlock`是一个顺序模块，在这里我们按顺序添加层。基于块中的层数(`number_layers`)，我们添加相应数量的`_DenseLayer`对象，并赋予一个名称。所有的魔法都发生在`_DenseLayer`对象内部。让我们看看`DenseLayer`对象内部发生了什么。'
- en: The _DenseLayer object
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`_DenseLayer`对象'
- en: 'One way to learn how a particular network works is to look at the source code.
    PyTorch has a very clean implementation and most of the time it is easily readable.
    Let''s look at the `_DenseLayer` implementation:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 学习特定网络工作方式的一种方法是查看源代码。PyTorch 的实现非常干净，大多数情况下很容易阅读。让我们来看看 `_DenseLayer` 的实现：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you are new to inheritance in Python, then the preceding code may not look
    intuitive. The `_DenseLayer` object is a subclass of `nn.Sequential`; let's look
    at what goes on inside each method.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对 Python 中的继承还不熟悉，那么前面的代码可能看起来不直观。`_DenseLayer` 对象是 `nn.Sequential` 的子类；让我们看看每个方法内部发生了什么。
- en: In the `__init__` method, we add all the layers that the input data needs to
    be passed to. It is quite similar to all the other network architectures we have
    seen.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `__init__` 方法中，我们添加了所有需要传递给输入数据的层。这与我们之前看到的所有其他网络架构非常相似。
- en: 'The magic happens in the `forward` method. We pass the input to the `forward`
    method of the super class, which is `nn.Sequential`. Let''s look at what happens
    in the `forward` method of the sequential class ([https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py](https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py)):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 方法中的魔法发生在这里。我们将输入传递给超类 `nn.Sequential` 的 `forward` 方法。让我们看看序列类 `forward`
    方法中发生了什么（[https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py](https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py))：'
- en: '[PRE28]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The input is passed through all the layers that were previously added to the
    sequential block and the output is concatenated to the input. This process is
    repeated for the required number of layers in a block.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通过之前添加到序列块中的所有层，并将输出连接到输入。这个过程在块中所需的层数中重复进行。
- en: Now that we understand how a DenseNet block works, let's explore how we can
    use DenseNet to calculate pre-convoluted features and build a classifier model
    on top of it. At a high level, the DenseNet implementation is similar to the VGG
    implementation. The DenseNet implementation also has a features module, which
    contains all the dense blocks, and a classifier module, which contains the fully
    connected model. We will be going through the following steps for building the
    model in this section but will be skipping most of the parts that are similar
    to what we have seen for Inception and ResNet, such as creating the data loader
    and datasets.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了 DenseNet 块的工作原理，让我们探索如何使用 DenseNet 计算预卷积特征并在其上构建分类器模型。在高层次上，DenseNet
    的实现类似于 VGG 的实现。DenseNet 的实现还有一个特征模块，其中包含所有的稠密块，以及一个分类器模块，其中包含全连接模型。在本节中，我们将按照以下步骤构建模型，但将跳过与
    Inception 和 ResNet 相似的大部分部分，例如创建数据加载器和数据集。
- en: 'We will discuss the following steps in detail:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讨论以下步骤：
- en: Creating a DenseNet model
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 DenseNet 模型
- en: Extracting DenseNet features
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取 DenseNet 特征
- en: Creating a dataset and loaders
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据集和加载器
- en: Creating a fully connected model and training it
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建全连接模型并对其进行训练
- en: By now, most of the code will be self-explanatory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，大多数代码都是不言自明的。
- en: Creating a DenseNet model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 DenseNet 模型
- en: 'Torchvision has a pretrained DenseNet model with different layer options (121,
    169, 201, and 161). Here, we have chosen the model with 121 layers. As we mentioned
    previously, the DenseNet model has two modules: `features` (containing the dense
    blocks) and `classifier` (fully connected block). Since we are using DenseNet
    as an image feature extractor, we will only use the `features` module:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Torchvision 提供了预训练的 DenseNet 模型，具有不同的层选项（121、169、201 和 161）。在这里，我们选择了具有 121
    层的模型。正如我们之前提到的，DenseNet 模型有两个模块：`features`（包含稠密块）和 `classifier`（全连接块）。由于我们将 DenseNet
    用作图像特征提取器，我们只会使用 `features` 模块：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let's extract the DenseNet features from the images.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从图像中提取 DenseNet 特征。
- en: Extracting DenseNet features
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取 DenseNet 特征
- en: 'This process is similar to what we did for Inception, except we aren''t using
    `register_forward_hook` to extract features. The following code shows how the
    DenseNet features are extracted:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程类似于我们对 Inception 所做的操作，只是我们没有使用 `register_forward_hook` 来提取特征。以下代码展示了如何从图像中提取
    DenseNet 特征：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The preceding code is similar to what we have seen for Inception and ResNet.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码与我们看到的 Inception 和 ResNet 类似。
- en: Creating a dataset and loaders
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据集和加载器
- en: 'We will use the same `FeaturesDataset` class that we created for ResNet and
    use it to create data loaders for the train and validation datasets. We will use
    the following code to do so:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们为ResNet创建的相同`FeaturesDataset`类，并用它来为训练和验证数据集创建数据加载器。我们将使用以下代码来实现：
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, it's time to create the model and train it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候创建模型并训练它了。
- en: Creating a fully connected model and training it
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个全连接模型并训练它
- en: 'Now, we will use a simple linear model, similar to what we used in ResNet and
    Inception. The following code shows the network architecture that we will be using
    to train the model:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用一个简单的线性模型，类似于我们在ResNet和Inception中使用的模型，来训练模型。以下代码展示了我们将用于训练模型的网络架构：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We will use the same `fit` method to train the preceding model. The following
    code snippet shows the training code, along with the results:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的`fit`方法来训练前述模型。下面的代码片段显示了训练代码及其结果：
- en: '[PRE33]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The result of the preceding code is as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的结果如下：
- en: '[PRE34]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding algorithm was able to achieve a maximum training accuracy of 99%,
    and 99% validation accuracy. Your results may be different since the validation
    dataset you will have created may have different images.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 前述算法能够达到最高99%的训练精度和99%的验证精度。由于您可能创建的验证数据集可能具有不同的图像，因此您的结果可能会有所不同。
- en: 'Some of the advantages of DenseNet are as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet的一些优点如下：
- en: It substantially reduces the number of parameters required.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它显著减少了所需的参数数量。
- en: It alleviates the vanishing gradient problem.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它缓解了梯度消失问题。
- en: It encourages feature reuse.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它鼓励特征重用。
- en: In this next section, we will explore how we can build a model that combines
    the advantages of the convoluted features we've computed using ResNet, Inception,
    and DenseNet.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨如何构建一个模型，结合使用ResNet、Inception和DenseNet计算的卷积特征的优势。
- en: Model ensembling
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型集成
- en: There could be times when we need to try to combine multiple models to build
    a very powerful model. There are many techniques we can use to build an ensemble
    model. In this section, we will learn how to combine outputs using the features
    generated by three different models (ResNet, Inception, and DenseNet) to build
    a powerful model. We will be using the same dataset that we used for the other
    examples in this chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要尝试结合多个模型来构建一个非常强大的模型。我们可以使用许多技术来构建集成模型。在本节中，我们将学习如何使用由三个不同模型（ResNet、Inception和DenseNet）生成的特征来组合输出，以构建一个强大的模型。我们将使用本章中其他示例中使用的相同数据集。
- en: 'The architecture for the ensemble model is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型的架构如下：
- en: '![](img/80ace9cb-9435-4e05-8590-af55a99dd66c.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80ace9cb-9435-4e05-8590-af55a99dd66c.png)'
- en: 'The preceding diagram shows what we are going to do in the ensemble model,
    which can be summarized in the following steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了我们将在集成模型中执行的操作，可以总结如下步骤：
- en: Create three models.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个模型。
- en: Extract the image features using the created models.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用创建的模型提取图像特征。
- en: Create a custom dataset that returns features of all the three models, along
    with the labels.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个自定义数据集，返回所有三个模型的特征以及标签。
- en: Create a model that's similar to the architecture shown in the preceding diagram.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个与前面图表中显示的架构类似的模型。
- en: Train and validate the model.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和验证模型。
- en: Let's explore each of these steps in detail.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨每个步骤。
- en: Creating models
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模型
- en: Let's create all the three required models, as shown in the following code blocks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建所有三个所需的模型，如以下代码块所示。
- en: 'The code for creating the ResNet model is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 创建ResNet模型的代码如下：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The code for creating the Inception model is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Inception模型的代码如下：
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The code for creating the DenseNet model is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet模型的代码如下：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that we have all the models, let's extract the features from the images.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的模型，让我们从图像中提取特征。
- en: Extracting the image features
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取图像特征
- en: Here, we will combine all the logic that we have seen individually for the algorithms
    in this chapter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将结合我们在本章中各个算法中单独看到的所有逻辑。
- en: 'The code for ResNet is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet的代码如下：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The code for Inception is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Inception的代码如下：
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The code for DenseNet is as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet的代码如下：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now, we have created image features using all the models. If you are facing
    memory issues, then you can either remove one of the models or stop storing features
    that are slow to train in memory. If you are running this on a CUDA instance,
    then you can go for a more powerful instance.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用所有模型创建了图像特征。如果您遇到内存问题，则可以删除一个模型或停止存储训练速度较慢的特征。如果您在运行CUDA实例，则可以选择更强大的实例。
- en: Creating a custom dataset, along with data loaders
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义数据集及其数据加载器
- en: 'We won''t be able to use the `FeaturesDataset` class as it is since it was
    developed to pick from the output of only one model. Due to this, the following
    implementation contains minor changes that have been made to the `FeaturesDataset`
    class so that we can accommodate all three generated features:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `FeaturesDataset` 类仅开发用于挑选来自单个模型的输出，因此我们无法使用它。由于这一点，以下实现包含对 `FeaturesDataset`
    类所做的轻微更改，以便我们可以容纳所有三个生成的特征：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Here, we have made changes to the `__init__` method so that we can store all
    the features that are generated from the different models. We also changed the `__getitem__`
    method so that we can retrieve the features and labels of an image. Using the
    `FeatureDataset` class, we created dataset instances for the training and validation
    data. Once the dataset has been created, we can use the same data loader for batching
    data, as shown in the following code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对 `__init__` 方法进行了更改，以便我们可以存储从不同模型生成的所有特征。我们还改变了 `__getitem__` 方法，以便我们可以检索图像的特征和标签。使用
    `FeatureDataset` 类，我们为训练和验证数据创建了数据集实例。创建数据集后，我们可以使用相同的数据加载器来批处理数据，如下面的代码所示：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Creating an ensembling model
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建集成模型
- en: 'Now, we need to create a model that works like the architecture diagram we
    saw previously. The following code implements this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个像我们之前看到的架构图一样工作的模型。以下代码实现了这一点：
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the preceding code, we created three linear layers that take the features
    that will be generated by the different models. We sum up all the outputs from
    these three linear layers and pass them to another linear layer, which maps them
    to the required categories. To prevent the model from overfitting, we used dropouts.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们创建了三个线性层，这些层将由不同模型生成的特征作为输入。我们将这三个线性层的所有输出相加，并将它们传递到另一个线性层，将它们映射到所需的类别。为了防止模型过拟合，我们使用了dropout。
- en: Training and validating the model
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和验证模型
- en: 'We need to make some minor changes to the `fit` method to accommodate the three
    input values we generated from the data loader. The following code implements
    the new `fit` function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对 `fit` 方法进行一些小的更改，以适应从数据加载器生成的三个输入值。以下代码实现了新的 `fit` 函数：
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you can see, most of the code remains the same, except that the loader returns
    three inputs and one label. Therefore, we have to make changes to the function,
    which is self-explanatory.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，大部分代码保持不变，除了加载器返回三个输入和一个标签。因此，我们必须对功能进行更改，这是不言自明的。
- en: 'The following is the training code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练代码：
- en: '[PRE45]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The result of the preceding code is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果如下：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The ensemble model achieves 99.6% training accuracy and a validation accuracy
    of 99.3%. Though ensemble models are powerful, they are computationally expensive.
    They are good techniques to use when you are solving problems in competitions
    such as Kaggle.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型实现了99.6%的训练准确率和99.3%的验证准确率。虽然集成模型功能强大，但计算成本高昂。它们是解决Kaggle等竞赛中问题时的好技术。
- en: Encoder-decoder architecture
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: 'Almost all the deep learning algorithms we have seen in this book are good
    at learning how to map training data to their corresponding labels. We cannot
    use them directly for tasks where the model needs to learn from a sequence and
    generate another sequence or an image. Some example applications are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们看到的几乎所有深度学习算法都擅长于学习如何将训练数据映射到其对应的标签。我们不能直接将它们用于需要从序列学习并生成另一个序列或图像的任务。一些示例应用如下：
- en: Language translation
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Image captioning
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕
- en: Image generation (`seq2img`)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成 (`seq2img`)
- en: Speech recognition
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Question answering
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: Most of these problems can be seen as forms of sequence-to-sequence mapping,
    and these can be solved using a family of architectures called encoder-decoder
    architectures. In this section, we will learn about the intuition behind these
    architectures. We will not be looking at the implementation of these networks
    as they need to be studied in more detail.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些问题可以看作是序列到序列映射的形式，并且可以使用一类称为编码器-解码器架构的家族来解决。在本节中，我们将了解这些架构背后的直觉。我们不会深入研究这些网络的实现，因为它们需要更详细的学习。
- en: 'At a high level, the encoder-decoder architecture looks as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，编码器-解码器架构如下所示：
- en: '![](img/e7dc27de-9ac7-4f14-9531-ef4ec4be7236.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7dc27de-9ac7-4f14-9531-ef4ec4be7236.png)'
- en: 'An encoder is usually a **recurrent neural network** (**RNN**) (for sequential
    data) or a **convolutional neural network** (**CNN**) (for images) that takes
    in an image or a sequence and converts it into a fixed-length vector that encodes
    all the information. The decoder is another RNN or CNN, which learns to decode
    the vector that was generated by the encoder and generates a new sequence of data.
    The following diagram shows what the encoder-decoder architecture looks like for
    an image captioning system:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器通常是一个**循环神经网络**（**RNN**）（用于序列数据）或者**卷积神经网络**（**CNN**）（用于图像），接收图像或序列并将其转换为一个固定长度的向量，编码了所有信息。解码器是另一个RNN或CNN，它学习解码编码器生成的向量，并生成新的数据序列。以下图表显示了图像字幕系统中编码器-解码器架构的外观：
- en: '![](img/1dc80559-4b82-4a28-884d-5ec94e46ac29.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1dc80559-4b82-4a28-884d-5ec94e46ac29.png)'
- en: 'Image source: [https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf)
- en: Now, let's look at what happens inside an encoder and a decoder architecture
    for an image captioning system.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看图像字幕系统中编码器和解码器架构内部发生了什么。
- en: Encoder
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器
- en: For an image captioning system, we should use a trained architecture, such as
    ResNet or Inception, to extract features from the image. Like we did for the ensemble
    model, we can output a fixed vector length by using a linear layer and then make
    that linear layer trainable.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像字幕系统，我们应该使用训练好的架构，比如ResNet或Inception，从图像中提取特征。就像我们为集成模型所做的那样，我们可以通过使用线性层输出一个固定长度的向量，然后使该线性层可训练。
- en: Decoder
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码器
- en: A decoder is a **long short-term memory** (**LSTM**) layer that will generate
    a caption for an image. To build a simple model, we can just pass the encoder
    embedding as input to the LSTM. However, this could be quite challenging for the
    decoder to learn; instead, it is common practice to provide the encoder embedding
    at every step of the decoder. Intuitively, a decoder learns to generate a sequence
    of text that best describes the caption of a given image.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器是一个**长短期记忆**（**LSTM**）层，用于为图像生成字幕。为了构建一个简单的模型，我们可以将编码器嵌入作为LSTM的输入。然而，对于解码器来说，学习起来可能会有挑战；因此，常见的做法是在解码器的每个步骤中提供编码器嵌入。直观地说，解码器学习生成一系列文本，最好地描述给定图像的字幕。
- en: Encoder-decoder with attention
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有注意力机制的编码器-解码器
- en: 'In 2017, a paper titled *Attention Is All You Need*, by Ashish Vaswani and
    co. ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)),
    was published, and it incorporates an attention mechanism. At each timestep, the
    attention network computes the weights of the pixels. It considers the sequence
    of words that have been generated so far and outputs what should be described
    next:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，阿希什·瓦斯瓦尼和合作者发表了一篇名为*Attention Is All You Need*的论文（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)），该论文引入了注意力机制。在每个时间步，注意力网络计算像素的权重。它考虑到迄今为止已生成的单词序列，并输出接下来应该描述什么：
- en: '![](img/e74e5dd3-69f8-454c-af67-0880f5004aae.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e74e5dd3-69f8-454c-af67-0880f5004aae.png)'
- en: In the preceding example, we can see that it is the LSTM's ability to retain
    information that can help it to learn that it is logical to write* "is holding
    a dog"* after "*a man"*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，我们可以看到LSTM保留信息的能力可以帮助它学习在“一个男人”之后逻辑地写入“正在抱着一只狗”。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored some modern architectures, such as ResNet, Inception,
    and DenseNet. We also explored how we can use these models for transfer learning
    and ensembling, and also introduced the encoder-decoder architecture, which powers
    a lot of systems, such as language translation systems.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了一些现代架构，如ResNet、Inception和DenseNet。我们还探讨了如何利用这些模型进行迁移学习和集成，并介绍了编码器-解码器架构，这种架构驱动了许多系统，如语言翻译系统。
- en: In the next chapter, we will be diving into deep reinforcement learning and
    learning how models can be applied to solve problems in the real world. We will
    also look at some PyTorch implementations that can help with this.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨深度强化学习，并学习模型如何应用于解决现实世界中的问题。我们还将看看一些PyTorch实现，这些实现可以帮助实现这一目标。
