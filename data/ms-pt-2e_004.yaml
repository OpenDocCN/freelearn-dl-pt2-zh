- en: 5 Hybrid Advanced Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5混合高级模型
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的书籍Discord社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file37.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file37.png)'
- en: In the previous two chapters, we learned extensively about the various convolutional
    and recurrent network architectures available, along with their implementations
    in PyTorch. In this chapter, we will take a look at some other deep learning model
    architectures that have proven to be successful on various machine learning tasks
    and are neither purely convolutional nor recurrent in nature. We will continue
    from where we left off in both *Chapter 3*, *Deep CNN Architectures*, and *Chapter
    4, Deep Recurrent Model Architectures*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们广泛学习了各种卷积和递归网络架构，以及它们在PyTorch中的实现。在本章中，我们将深入研究一些在各种机器学习任务中证明成功的其他深度学习模型架构，它们既不是纯卷积的，也不是循环的。我们将继续探讨*第3章
    深度CNN架构*和*第4章 深度递归模型结构*中的内容。
- en: First, we will explore transformers, which, as we learnt toward the end of *Chapter
    4, Deep Recurrent Model Architectures*, have outperformed recurrent architectures
    on various sequential tasks. Then, we will pick up from the **EfficientNets**
    discussion at the end of *Chapter 3, Deep CNN Architectures*, and explore the
    idea of generating randomly wired neural networks, also known as **RandWireNNs**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探讨transformers，正如我们在*第4章 深度递归模型结构*末尾学到的，它们在各种顺序任务上表现优于循环架构。接着，我们将从*第3章
    深度CNN架构*末尾的**EfficientNets**讨论中继续，并探索生成随机连线神经网络的概念，也称为**RandWireNNs**。
- en: 'With this chapter, we aim to conclude our discussion of different kinds of
    neural network architectures in this book. After completing this chapter, you
    will have a detailed understanding of transformers and how to apply these powerful
    models to sequential tasks using PyTorch. Furthermore, by building your own RandWireNN
    model, you will have hands-on experience of performing a neural architecture search
    in PyTorch. This chapter is broken down into the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们的目标是结束本书中对不同神经网络架构的讨论。完成本章后，您将深入了解transformers及如何使用PyTorch将这些强大的模型应用于顺序任务。此外，通过构建自己的RandWireNN模型，您将有机会在PyTorch中执行神经架构搜索。本章分为以下主题：
- en: Building a transformer model for language modeling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个用于语言建模的transformer模型
- en: Developing a RandWireNN model from scratch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始开发RandWireNN模型
- en: Building a transformer model for language modeling
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立一个用于语言建模的transformer模型
- en: In this section, we will explore what transformers are and build one using PyTorch
    for the task of language modeling. We will also learn how to use some of its successors,
    such as **BERT** and **GPT**, via PyTorch's pretrained model repository. Before
    we start building a transformer model, let's quickly recap what language modeling
    is.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索transformers是什么，并使用PyTorch为语言建模任务构建一个。我们还将学习如何通过PyTorch的预训练模型库使用其后继模型，如**BERT**和**GPT**。在我们开始构建transformer模型之前，让我们快速回顾一下语言建模是什么。
- en: Reviewing language modeling
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾语言建模
- en: '**Language modeling** is the task of figuring out the probability of the occurrence
    of a word or a sequence of words that should follow a given sequence of words.
    For example, if we are given *French is a beautiful _____* as our sequence of
    words, what is the probability that the next word will be *language* or *word*,
    and so on? These probabilities are computed by modeling the language using various
    probabilistic and statistical techniques. The idea is to observe a text corpus
    and learn the grammar by learning which words occur together and which words never
    occur together. This way, a language model establishes probabilistic rules around
    the occurrence of different words or sequences, given various different sequences.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言建模**是一项确定一个词或一系列词出现概率的任务，它们应该跟在给定的一系列词后面。例如，如果我们给定*French is a beautiful
    _____*作为我们的词序列，下一个词将是*language*或*word*的概率是多少？这些概率是通过使用各种概率和统计技术对语言建模来计算的。其思想是通过观察文本语料库并学习语法，学习哪些词汇经常一起出现，哪些词汇从不会一起出现。这样，语言模型建立了关于不同词或序列出现概率的概率规则。'
- en: Recurrent models have been a popular way of learning a language model. However,
    as with many sequence-related tasks, transformers have outperformed recurrent
    networks on this task as well. We will implement a transformer-based language
    model for the English language by training it on the Wikipedia text corpus.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 递归模型一直是学习语言模型的一种流行方法。然而，与许多序列相关任务一样，变压器在这个任务上也超过了递归网络。我们将通过在维基百科文本语料库上训练一个基于变压器的英语语言模型来实现。
- en: Now, let's start training a transformer for language modeling. During this exercise,
    we will demonstrate only the most important parts of the code. The full code can
    be accessed at our github repository [5.1].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练一个变压器进行语言建模。在这个练习过程中，我们将仅演示代码的最重要部分。完整的代码可以在我们的github仓库[5.1]中找到。
- en: We will delve deeper into the various components of the transformer architecture
    in-between the exercise.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在练习之间，我们将深入探讨变压器架构的各个组成部分。
- en: 'For this exercise, we will need to import a few dependencies. One of the important
    `import` statements is listed here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们需要导入一些依赖项。其中一个重要的`import`语句如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Besides importing the regular `torch` dependencies, we must import some modules
    specific to the transformer model; these are provided directly under the torch
    library. We'll also import `torchtext` in order to download a text dataset directly
    from the available datasets under `torchtext.datasets`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了导入常规的`torch`依赖项外，我们还必须导入一些特定于变压器模型的模块；这些模块直接在torch库下提供。我们还将导入`torchtext`，以便直接从`torchtext.datasets`下的可用数据集中下载文本数据集。
- en: In the next section, we will define the transformer model architecture and look
    at the details of the model's components.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将定义变压器模型的架构并查看模型组件的详细信息。
- en: Understanding the transformer model architecture
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解变压器模型架构
- en: This is perhaps the most important step of this exercise. Here, we define the
    architecture of the transformer model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是这个练习中最重要的步骤。在这里，我们定义变压器模型的架构。
- en: 'First, let''s briefly discuss the model architecture and then look at the PyTorch
    code for defining the model. The following diagram shows the model architecture:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们简要讨论模型架构，然后看一下定义模型的PyTorch代码。以下图表显示了模型架构：
- en: '![Figure 5.1 – Transformer model architecture](img/file38.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 变压器模型架构](img/file38.jpg)'
- en: Figure 5.1 – Transformer model architecture
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 变压器模型架构
- en: The first thing to notice is that this is essentially an encoder-decoder based
    architecture, with the **Encoder Unit** on the left (in purple) and the **Decoder
    Unit** (in orange) on the right. The encoder and decoder units can be tiled multiple
    times for even deeper architectures. In our example, we have two cascaded encoder
    units and a single decoder unit. This encoder-decoder setup essentially means
    that the encoder takes a sequence as input and generates as many embeddings as
    there are words in the input sequence (that is, one embedding per word). These
    embeddings are then fed to the decoder, along with the predictions made thus far
    by the model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，这基本上是一个基于编码器-解码器的架构，左侧是**编码器单元**（紫色），右侧是**解码器单元**（橙色）。编码器和解码器单元可以多次平铺以实现更深层的架构。在我们的示例中，我们有两个级联的编码器单元和一个单独的解码器单元。这种编码器-解码器设置的本质是，编码器接受一个序列作为输入，并生成与输入序列中的单词数量相同的嵌入（即每个单词一个嵌入）。然后，这些嵌入与模型迄今为止的预测一起馈送到解码器中。
- en: 'Let''s walk through the various layers in this model:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这个模型中的各个层：
- en: '**Embedding Layer**: This layer is simply meant to perform the traditional
    task of converting each input word of the sequence into a vector of numbers; that
    is, an embedding. As always, here, we use the `torch.nn.Embedding` module to code
    this layer.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**嵌入层**：这一层的作用很简单，即将序列的每个输入单词转换为数字向量，即嵌入。在这里，我们使用`torch.nn.Embedding`模块来实现这一层。'
- en: '**Positional Encoder**: Note that transformers do not have any recurrent layers
    in their architecture, yet they outperform recurrent networks on sequential tasks.
    How? Using a neat trick known as *positional encoding*, the model is provided
    the sense of sequentiality or sequential-order in the data. Basically, vectors
    that follow a particular sequential pattern are added to the input word embeddings.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码器**：请注意，变压器在其架构中没有任何递归层，然而它们在顺序任务上表现出色。怎么做到的？通过一个称为*位置编码*的巧妙技巧，模型能够感知数据中的顺序性或顺序。基本上，按照特定的顺序模式添加到输入单词嵌入中的向量。'
- en: 'These vectors are generated in a way that enables the model to understand that
    the second word comes after the first word and so on. The vectors are generated
    using the `sinusoidal` and `cosinusoidal` functions to represent a systematic
    periodicity and distance between subsequent words, respectively. The implementation
    of this layer for our exercise is as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量是通过一种方法生成的，使模型能够理解第二个词跟在第一个词后面，依此类推。这些向量是使用`正弦`和`余弦`函数生成的，分别用来表示连续单词之间的系统周期性和距离。我们在这个练习中的这一层的实现如下：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, the `sinusoidal` and `cosinusoidal` functions are used alternatively
    to give the sequential pattern. There are many ways to implement positional encoding
    though. Without a positional encoding layer, the model will be clueless about
    the order of the words.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，交替使用`正弦`和`余弦`函数来给出顺序模式。虽然有多种实现位置编码的方式。如果没有位置编码层，模型将对单词的顺序一无所知。
- en: '**Multi-Head Attention**: Before we look at the multi-head attention layer,
    let''s first understand what a **self-attention layer** is. We covered the concept
    of attention in *Chapter 4*, *Deep Recurrent Model Architectures*, with respect
    to recurrent networks. Here, as the name suggests, the attention mechanism is
    applied to self; that is, each word of the sequence. Each word embedding of the
    sequence goes through the self-attention layer and produces an individual output
    that is exactly the same length as the word embedding. The following diagram describes
    the process of this in detail:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**多头注意力**：在我们看多头注意力层之前，让我们先了解什么是**自注意力层**。在*第四章*，*深度递归模型架构*中，我们已经涵盖了关于递归网络中注意力的概念。这里，正如其名称所示，注意机制应用于自身；即序列中的每个单词。序列的每个单词嵌入通过自注意力层，并生成与单词嵌入长度完全相同的单独输出。下面的图解详细描述了这一过程：'
- en: '![Figure 5.2 – Self-attention layer](img/file39.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 自注意力层](img/file39.jpg)'
- en: Figure 5.2 – Self-attention layer
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 自注意力层
- en: As we can see, for each word, three vectors are generated through three learnable
    parameter matrices (**Pq**, **Pk**, and **Pv**). The three vectors are query,
    key, and value vectors. The query and key vectors are dot-multiplied to produce
    a number for each word. These numbers are normalized by dividing the square root
    of the key vector length for each word. The resultant numbers for all words are
    then Softmaxed at the same time to produce probabilities that are finally multiplied
    by the respective value vectors for each word. This results in one output vector
    for each word of the sequence, with the lengths of the output vector and the input
    word embedding being the same.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，对于每个单词，通过三个可学习的参数矩阵（**Pq**，**Pk** 和 **Pv**）生成三个向量。这三个向量是查询、键和值向量。查询和键向量进行点乘，为每个单词产生一个数字。这些数字通过将每个单词的键向量长度的平方根进行归一化来规范化。然后同时对所有单词的结果进行
    Softmax 操作，以产生最终乘以相应值向量的概率。这导致序列的每个单词都有一个输出向量，其长度与输入单词嵌入的长度相同。
- en: 'A multi-head attention layer is an extension of the self-attention layer where
    multiple self-attention modules compute outputs for each word. These individual
    outputs are concatenated and matrix-multiplied with yet another parameter matrix
    (**Pm**) to generate the final output vector, whose length is equal to the input
    embedding vector''s. The following diagram shows the multi-head attention layer,
    along with two self-attention units that we will be using in this exercise:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力层是自注意力层的扩展，其中多个自注意力模块为每个单词计算输出。这些个别输出被串联并与另一个参数矩阵（**Pm**）进行矩阵乘法，以生成最终的输出向量，其长度与输入嵌入向量的长度相等。下图展示了多头注意力层，以及我们在本练习中将使用的两个自注意力单元：
- en: '![Figure 5.3 – Multi-head attention layer with two self-attention units](img/file40.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 具有两个自注意单元的多头注意力层](img/file40.jpg)'
- en: Figure 5.3 – Multi-head attention layer with two self-attention units
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 具有两个自注意力单元的多头注意力层
- en: Having multiple self-attention heads helps different heads focus on different
    aspects of the sequence word, similar to how different feature maps learn different
    patterns in a convolutional neural network. Due to this, the multi-head attention
    layer performs better than an individual self-attention layer and will be used
    in our exercise.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 多个自注意力头有助于不同的头集中于序列单词的不同方面，类似于不同特征映射学习卷积神经网络中的不同模式。因此，多头注意力层的性能优于单个自注意力层，并将在我们的练习中使用。
- en: Also, note that the masked multi-head attention layer in the decoder unit works
    in exactly the same way as a multi-head attention layer, except for the added
    masking; that is, given time step *t* of processing the sequence, all words from
    *t+1* to *n* (length of the sequence) are masked/hidden.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，在解码器单元中，掩码多头注意力层的工作方式与多头注意力层完全相同，除了增加的掩码；也就是说，在处理序列的时间步 *t* 时，从 *t+1*
    到 *n*（序列长度）的所有单词都被掩盖/隐藏。
- en: During training, the decoder is provided with two types of inputs. On one hand,
    it receives query and key vectors from the final encoder as inputs to its (unmasked)
    multi-head attention layer, where these query and key vectors are matrix transformations
    of the final encoder output. On the other hand, the decoder receives its own predictions
    from previous time steps as sequential input to its masked multi-head attention
    layer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，解码器接收两种类型的输入。一方面，它从最终编码器接收查询和键向量作为其（未掩码的）多头注意力层的输入，其中这些查询和键向量是最终编码器输出的矩阵变换。另一方面，解码器接收其自己在前一个时间步的预测作为其掩码多头注意力层的顺序输入。
- en: '**Addition and Layer Normalization**: We discussed the concept of a residual
    connection in *Chapter 3*, *Deep CNN Architectures*, while discussing ResNets.
    In *Figure 5.1*, we can see that there are residual connections across the addition
    and layer normalization layers. In each instance, a residual connection is established
    by directly adding the input word embedding vector to the output vector of the
    multi-head attention layer. This helps with easier gradient flow throughout the
    network and avoiding problems with exploding and vanishing gradients. Also, it
    helps with efficiently learning identity functions across layers.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**加法和层归一化**：我们在 *第三章* *深度CNN架构* 中讨论了残差连接的概念，当讨论ResNets时。在 *图5.1* 中，我们可以看到在加法和层归一化层之间存在残差连接。在每个实例中，通过直接将输入单词嵌入向量加到多头注意力层的输出向量上，建立了一个残差连接。这有助于网络中更容易的梯度流动，避免梯度爆炸和梯度消失问题。此外，它有助于在各层之间有效地学习身份函数。'
- en: Furthermore, layer normalization is used as a normalization trick. Here, we
    normalize each feature independently so that all the features have a uniform mean
    and standard deviation. Please note that these additions and normalizations are
    applied individually to each word vector of the sequence at each stage of the
    network.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此外，层归一化被用作一种规范化技巧。在这里，我们独立地对每个特征进行归一化，以使所有特征具有统一的均值和标准差。请注意，这些加法和归一化逐个应用于网络每个阶段的每个单词向量。
- en: '**Feedforward Layer**: Within both the encoder and decoder units, the normalized
    residual output vectors for all the words of the sequence are passed through a
    common feedforward layer. Due to there being a common set of parameters across
    words, this layer helps with learning broader patterns across the sequence.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈层**：在编码器和解码器单元内部，所有序列中单词的归一化残差输出向量通过一个共同的前馈层传递。由于单词间存在一组共同的参数，这一层有助于学习序列中更广泛的模式。'
- en: '**Linear and Softmax Layer**: So far, each layer is outputting a sequence of
    vectors, one per word. For our task of language modeling, we need a single final
    output. The linear layer transforms the sequence of vectors into a single vector
    whose size is equal to the length of our word vocabulary. The **Softmax** layer
    converts this output into a vector of probabilities summing to `1`. These probabilities
    are the probabilities that the respective words (in the vocabulary) occur as the
    next words in the sequence.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性和Softmax层**：到目前为止，每个层都输出一个单词序列的向量。对于我们的语言建模任务，我们需要一个单一的最终输出。线性层将向量序列转换为一个与我们的单词词汇长度相等的单一向量。**Softmax**层将这个输出转换为一个概率向量，其总和为`1`。这些概率是词汇表中各个单词（在序列中）作为下一个单词出现的概率。'
- en: Now that we have elaborated on the various elements of a transformer model,
    let's look at the PyTorch code for instantiating the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细阐述了变压器模型的各种要素，让我们看一下用于实例化模型的 PyTorch 代码。
- en: Defining a transformer model in PyTorch
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中定义一个变压器模型
- en: 'Using the architecture details described in the previous section, we will now
    write the necessary PyTorch code for defining a transformer model, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面部分描述的架构细节，我们现在将编写必要的 PyTorch 代码来定义一个变压器模型，如下所示：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, in the `__init__` method of the class, thanks to PyTorch's `TransformerEncoder`
    and `TransformerEncoderLayer` functions, we do not need to implement these ourselves.
    For our language modeling task, we just need a single output for the input sequence
    of words. Due to this, the decoder is just a linear layer that transforms the
    sequence of vectors from an encoder into a single output vector. A position encoder
    is also initialized using the definition that we discussed earlier.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，在类的`__init__`方法中，由于PyTorch的`TransformerEncoder`和`TransformerEncoderLayer`函数，我们无需自己实现这些。对于我们的语言建模任务，我们只需要输入单词序列的单个输出。因此，解码器只是一个线性层，它将来自编码器的向量序列转换为单个输出向量。位置编码器也是使用我们之前讨论的定义初始化的。
- en: 'In the `forward` method, the input is positionally encoded and then passed
    through the encoder, followed by the decoder:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward`方法中，输入首先进行位置编码，然后通过编码器，接着是解码器：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have defined the transformer model architecture, we shall load the
    text corpus to train it on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了变压器模型架构，我们将载入文本语料库进行训练。
- en: Loading and processing the dataset
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和处理数据集
- en: 'In this section, we will discuss the steps related to loading a text dataset
    for our task and making it usable for the model training routine. Let''s get started:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论与加载文本数据集和使其适用于模型训练例程有关的步骤。让我们开始吧：
- en: For this exercise, we will be using texts from Wikipedia, all of which are available
    as the `WikiText-2` dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习中，我们将使用维基百科的文本，这些文本都可作为`WikiText-2`数据集使用。
- en: '**Dataset Citation**'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**数据集引用**'
- en: ''
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/.](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/.)'
  id: totrans-59
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/.](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/.)'
- en: 'We''ll use the functionality of `torchtext` to download the training dataset
    (available under `torchtext` datasets) and tokenize its vocabulary:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`torchtext`的功能来下载训练数据集（在`torchtext`数据集中可用）并对其词汇进行标记化：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will then use the vocabulary to convert raw text into tensors for the training,
    validation and testing datasets:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将使用词汇表将原始文本转换为张量，用于训练、验证和测试数据集：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We''ll also define the batch sizes for training and evaluation and declare
    a batch generation function, as shown here:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将为训练和评估定义批量大小，并声明批处理生成函数，如下所示：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we must define the maximum sequence length and write a function that
    will generate input sequences and output targets for each batch, accordingly:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须定义最大序列长度，并编写一个函数，根据每个批次生成输入序列和输出目标：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Having defined the model and prepared the training data, we will now train the
    transformer model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了模型并准备好训练数据后，我们将开始训练变压器模型。
- en: Training the transformer model
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练变压器模型
- en: 'In this section, we will define the necessary hyperparameters for model training,
    define the model training and evaluation routines, and finally execute the training
    loop. Let''s get started:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为模型训练定义必要的超参数，定义模型训练和评估例程，最后执行训练循环。让我们开始吧：
- en: 'In this step, we define all the model hyperparameters and instantiate our transformer
    model. The following code is self-explanatory:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们定义所有模型超参数并实例化我们的变压器模型。以下代码是不言而喻的：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Before starting the model training and evaluation loop, we need to define the
    training and evaluation routines:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始模型训练和评估循环之前，我们需要定义训练和评估例程：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we must run the model training loop. For demonstration purposes, we
    are training the model for `5` epochs, but you are encouraged to run it for longer
    in order to get better performance:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们必须运行模型训练循环。为了演示目的，我们将为模型训练`5`个时代，但建议您训练更长时间以获得更好的性能：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This should result in the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这应产生以下输出：
- en: '![Figure 5.4 – Transformer training logs](img/file41.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 - 变压器训练日志](img/file41.png)'
- en: Figure 5.4 – Transformer training logs
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 - 变压器训练日志
- en: Besides the cross-entropy loss, the perplexity is also reported. **Perplexity**
    is a popularly used metric in natural language processing to indicate how well
    a **probability distribution** (a language model, in our case) fits or predicts
    a sample. The lower the perplexity, the better the model is at predicting the
    sample. Mathematically, perplexity is just the exponential of the cross-entropy
    loss. Intuitively, this metric is used to indicate how perplexed or confused the
    model is while making predictions.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 除了交叉熵损失，还报告了困惑度。**困惑度**是自然语言处理中常用的指标，用于表示一个**概率分布**（在我们的情况下是语言模型）对样本的拟合或预测能力。困惑度越低，模型在预测样本时表现越好。从数学上讲，困惑度只是交叉熵损失的指数。直观地说，这个度量用来指示模型在进行预测时的困惑或混乱程度。
- en: 'Once the model has been trained, we can conclude this exercise by evaluating
    the model''s performance on the test set:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完毕，我们可以通过在测试集上评估模型的性能来完成这个练习：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should result in the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该导致以下输出：
- en: '![Figure 5.5 – Transformer evaluation results](img/file42.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 5.5 – Transformer evaluation results](img/file42.png)'
- en: Figure 5.5 – Transformer evaluation results
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 5.5 – Transformer evaluation results
- en: In this exercise, we built a transformer model using PyTorch for the task of
    language modeling. We explored the transformer architecture in detail and how
    it is implemented in PyTorch. We used the `WikiText-2` dataset and `torchtext`
    functionalities to load and process the dataset. We then trained the transformer
    model for `5` epochs and evaluated it on a separate test set. This shall provide
    us with all the information we need to get started on working with transformers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们使用PyTorch构建了一个用于语言建模任务的变压器模型。我们详细探讨了变压器的架构以及如何在PyTorch中实现它。我们使用了`WikiText-2`数据集和`torchtext`功能来加载和处理数据集。然后我们训练了变压器模型，进行了`5`个epochs的评估，并在一个独立的测试集上进行了评估。这将为我们提供开始使用变压器所需的所有信息。
- en: 'Besides the original transformer model, which was devised in 2017, a number
    of successors have since been developed over the years, especially around the
    field of language modeling, such as the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始的变压器模型，该模型是在2017年设计的，多年来已经开发了许多后续版本，特别是在语言建模领域，例如以下几种：
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**), 2018'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**（**Bidirectional Encoder Representations from Transformers**），2018'
- en: '**Generative Pretrained Transformer** (**GPT**), 2018'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT**（**Generative Pretrained Transformer**），2018'
- en: '**GPT-2**, 2019'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-2**, 2019'
- en: '**Conditional Transformer Language Model** (**CTRL**), 2019'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**CTRL**（**Conditional Transformer Language Model**），2019'
- en: '**Transformer-XL**, 2019'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer-XL**, 2019'
- en: '**Distilled BERT** (**DistilBERT**), 2019'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**DistilBERT**（**Distilled BERT**），2019'
- en: '**Robustly optimized BERT pretraining Approach** (**RoBERTa**), 2019'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa**（**RoBustly optimized BERT pretraining Approach**），2019'
- en: '**GPT-3**, 2020'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT-3**, 2020'
- en: While we will not cover these models in detail in this chapter, you can nonetheless
    get started with using these models with PyTorch thanks to the `transformers`
    library, developed by HuggingFace 5.2 .We will explore HuggingFace in detail in
    Chapter 19\. The transformers library provides pre-trained transformer family
    models for various tasks, such as language modeling, text classification, translation,
    question-answering, and so on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本章不会详细介绍这些模型，但是你仍然可以通过PyTorch和`transformers`库开始使用这些模型。我们将在第19章详细探讨HuggingFace。transformers库为各种任务提供了预训练的变压器系列模型，例如语言建模、文本分类、翻译、问答等。
- en: 'Besides the models themselves, it also provides tokenizers for the respective
    models. For example, if we wanted to use a pre-trained BERT model for language
    modeling, we would need to write the following code once we have installed the
    `transformers` library:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型本身，它还提供了各自模型的分词器。例如，如果我们想要使用预训练的BERT模型进行语言建模，我们需要在安装了`transformers`库后写入以下代码：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we can see, it takes just a couple of lines to get started with a BERT-based
    language model. This demonstrates the power of the PyTorch ecosystem. You are
    encouraged to explore this with more complex variants, such as *Distilled BERT*
    or *RoBERTa*, using the `transformers` library. For more details, please refer
    to their GitHub page, which was mentioned previously.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，仅需几行代码就可以开始使用基于BERT的语言模型。这展示了PyTorch生态系统的强大性。你被鼓励使用`transformers`库探索更复杂的变种，如*Distilled
    BERT*或*RoBERTa*。有关更多详细信息，请参阅它们的GitHub页面，之前已经提到过。
- en: This concludes our exploration of transformers. We did this by both building
    one from scratch as well as by reusing pre-trained models. The invention of transformers
    in the natural language processing space has been paralleled with the ImageNet
    moment in the field of computer vision, so this is going to be an active area
    of research. PyTorch will have a crucial role to play in the research and deployment
    of these types of models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对transformer的探索。我们既通过从头构建一个transformer，也通过重用预训练模型来做到这一点。在自然语言处理领域，transformer的发明与计算机视觉领域的ImageNet时刻齐头并进，因此这将是一个活跃的研究领域。PyTorch将在这些模型的研究和部署中发挥关键作用。
- en: In the next and final section of this chapter, we will resume the neural architecture
    search discussions we provided at the end of *Chapter 3, Deep CNN Architectures*,
    where we briefly discussed the idea of generating optimal network architectures.
    We will explore a type of model where we do not decide what the model architecture
    will look like, and instead run a network generator that will find an optimal
    architecture for the given task. The resultant network is called a **randomly
    wired neural network** (**RandWireNN**) and we will develop one from scratch using
    PyTorch.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一个也是最后一个部分中，我们将恢复我们在*第三章，深度CNN架构*末尾提供的神经架构搜索讨论，那里我们简要讨论了生成最优网络架构的想法。我们将探索一种模型类型，我们不决定模型架构的具体形式，而是运行一个网络生成器，为给定任务找到最优架构。由此产生的网络称为**随机连线神经网络**（**RandWireNN**），我们将使用PyTorch从头开始开发一个。
- en: Developing a RandWireNN model from scratch
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从零开始开发RandWireNN模型
- en: We discussed EfficientNets in *Chapter 3, Deep CNN Architectures*, where we
    explored the idea of finding the best model architecture instead of specifying
    it manually. RandWireNNs, or randomly wired neural networks, as the name suggests,
    are built on a similar concept. In this section, we will study and build our own
    RandWireNN model using PyTorch.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第三章，深度CNN架构*中讨论了EfficientNets，在那里我们探讨了找到最佳模型架构而不是手动指定的想法。RandWireNNs，或随机连线神经网络，顾名思义，是建立在类似概念上的。在本节中，我们将研究并使用PyTorch构建我们自己的RandWireNN模型。
- en: Understanding RandWireNNs
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解RandWireNNs
- en: 'First, a random graph generation algorithm is used to generate a random graph
    with a predefined number of nodes. This graph is converted into a neural network
    by a few definitions being imposed on it, such as the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用随机图生成算法生成一个预定义节点数的随机图。这个图被转换成一个神经网络，通过对其施加一些定义，比如以下定义：
- en: '**Directed**: The graph is restricted to be a directed graph, and the direction
    of edge is considered to be the direction of data flow in the equivalent neural
    network.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有向性**：图被限制为有向图，并且边的方向被认为是等效神经网络中数据流的方向。'
- en: '**Aggregation**: Multiple incoming edges to a node (or neuron) are aggregated
    by weighted sum, where the weights are learnable.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合**：多个入边到一个节点（或神经元）通过加权和进行聚合，其中权重是可学习的。'
- en: '**Transformation**: Inside each node of this graph, a standard operation is
    applied: ReLU followed by 3x3 separable convolution (that is, a regular 3x3 convolution
    followed by a 1x1 pointwise convolution), followed by batch normalization. This
    operation is also referred to as a **ReLU-Conv-BN triplet**.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：在该图的每个节点内部，应用了一个标准操作：ReLU接着3x3可分离卷积（即常规的3x3卷积接着1x1点卷积），然后是批量归一化。这个操作也被称为**ReLU-Conv-BN三重组合**。'
- en: '**Distribution**: Lastly, multiple outgoing edges from each neuron carry a
    copy of the aforementioned triplet operation.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布**：最后，每个神经元有多个出边，每个出边携带前述三重组合的副本。'
- en: One final piece in the puzzle is to add a single input node (source) and a single
    output node (sink) to this graph in order to fully transform the random graph
    into a neural network. Once the graph is realized as a neural network, it can
    be trained for various machine learning tasks.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图中的最后一块是向该图添加一个单一的输入节点（源）和一个单一的输出节点（汇），以完全将随机图转化为神经网络。一旦图被实现为神经网络，它可以被训练用于各种机器学习任务。
- en: In the **ReLU-Conv-BN triplet unit**, the output number of channels/features
    are the same as the input number of channels/features for repeatability reasons.
    However, depending on the type of task at hand, you can stage several of these
    graphs with an increasing number of channels downstream (and decreasing spatial
    size of the data/images). Finally, these staged graphs can be connected to each
    other by connecting the sink of one to the source of the other in a sequential
    manner.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在**ReLU-Conv-BN三元组单元**中，出口通道数/特征与输入通道数/特征相同，出于可重复性考虑。然而，根据手头任务的类型，您可以将几个这样的图阶段性地配置为向下游增加通道数（和减少数据/图像的空间尺寸）。最后，这些分段图可以按顺序连接，将一个的末端连接到另一个的起始端。
- en: Next, in the form of an exercise, we will build a RandWireNN model from scratch
    using PyTorch.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，作为一项练习，我们将使用PyTorch从头开始构建一个RandWireNN模型。
- en: Developing RandWireNNs using PyTorch
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch开发RandWireNN
- en: We will now develop a RandWireNN model for an image classification task. This
    will be performed on the CIFAR-10 dataset. We will start from an empty model,
    generate a random graph, transform it into a neural network, train it for the
    given task on the given dataset, evaluate the trained model, and finally explore
    the resulting model that was generated. In this exercise, we will only show the
    important parts of the code for demonstration purposes. In order to access the
    full code, visit the book’s github repo [5.3] .
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将为图像分类任务开发一个RandWireNN模型。这将在CIFAR-10数据集上执行。我们将从一个空模型开始，生成一个随机图，将其转换为神经网络，为给定的任务在给定的数据集上进行训练，评估训练后的模型，最后探索生成的模型。在这个练习中，我们仅展示代码的重要部分以示范目的。要访问完整的代码，请访问书籍的GitHub仓库[5.3]。
- en: Defining a training routine and loading data
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义训练例程和加载数据
- en: 'In the first sub-section of this exercise, we will define the training function
    that will be called by our model training loop and define our dataset loader,
    which will provide us with batches of data for training. Let''s get started:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习的第一个子部分中，我们将定义训练函数，该函数将由我们的模型训练循环调用，并定义我们的数据集加载器，该加载器将为我们提供用于训练的数据批次。让我们开始吧：
- en: 'First, we need to import some libraries. Some of the new libraries that will
    be used in this exercise are as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入一些库。本练习中将使用的一些新库如下：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we must define the training routine, which takes in a trained model that
    can produce prediction probabilities given an RGB input image:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须定义训练例程，该例程接受一个经过训练的模型，该模型能够根据RGB输入图像产生预测概率：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, we define the dataset loader. We will use the `CIFAR-10` dataset for this
    image classification task, which is a well-known database of 60,000 32x32 RGB
    images labeled across 10 different classes containing 6,000 images per class.
    We will use the `torchvision.datasets` module to directly load the data from the
    torch dataset repository.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义数据集加载器。对于这个图像分类任务，我们将使用`CIFAR-10`数据集，这是一个知名的数据库，包含60,000个32x32的RGB图像，分为10个不同的类别，每个类别包含6,000张图像。我们将使用`torchvision.datasets`模块直接从torch数据集仓库加载数据。
- en: '**Dataset Citation**'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**数据集引用**'
- en: ''
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Learning Multiple Layers of Features from Tiny Images*, Alex Krizhevsky, 2009.'
  id: totrans-124
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*从小图像中学习多层特征*，Alex Krizhevsky，2009年。'
- en: 'The code is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This should give us the following output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给我们以下输出：
- en: '![Figure 5.6 – RandWireNN data loading](img/file43.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – RandWireNN 数据加载](img/file43.jpg)'
- en: Figure 5.6 – RandWireNN data loading
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – RandWireNN 数据加载
- en: We will now move on to designing the neural network model. For this, we will
    need to design the randomly wired graph.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将继续设计神经网络模型。为此，我们需要设计随机连通图。
- en: Defining the randomly wired graph
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义随机连通图
- en: 'In this section, we will define a graph generator in order to generate a random
    graph that will be later used as a neural network. Let''s get started:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义一个图生成器，以生成稍后将用作神经网络的随机图。让我们开始吧：
- en: 'As shown in the following code, we must define the random graph generator class:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如下面的代码所示，我们必须定义随机图生成器类：
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this exercise, we''ll be using a well-known random graph model – the **Watts
    Strogatz (WS)** model. This is one of the three models that was experimented on
    in the original research paper about RandWireNNs. In this model, there are two
    parameters:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用一个众所周知的随机图模型——**Watts Strogatz (WS)** 模型。这是在原始研究论文中对RandWireNN进行实验的三个模型之一。在这个模型中，有两个参数：
- en: The number of neighbors for each node (which should be strictly even), *K*
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点的邻居数（应严格偶数），*K*
- en: A rewiring probability, *P*
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重连概率，*P*
- en: First, all the *N* nodes of the graph are organized in a ring fashion and each
    node is connected to *K/2* nodes to its left and *K/2* to its right. Then, we
    traverse each node clockwise *K/2* times. At the *m*th traversal (*0<m<K/2*),
    the edge between the current node and its *m*th neighbor to the right is *rewired*
    with a probability, *P*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，图的所有*N*个节点按环形排列，每个节点与其左侧的*K/2*个节点和右侧的*K/2*个节点相连。然后，我们顺时针遍历每个节点*K/2*次。在第*m*次遍历（*0<m<K/2*）时，当前节点与其右侧第*m*个邻居之间的边以概率*P*被*重连*。
- en: Here, rewiring means that the edge is replaced by another edge between the current
    node and another node different from itself, as well as the *m*th neighbor. In
    the preceding code, the `make_graph_obj` method of our random graph generator
    class instantiates the WS graph model using the `networkx` library.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，重连意味着将边替换为与当前节点及其不同的另一节点之间的另一条边，以及第*m*个邻居。在前面的代码中，我们的随机图生成器类的`make_graph_obj`方法使用了`networkx`库来实例化WS图模型。
- en: In the preceding code, the make_graph_obj method of our random graph generator
    class instantiates the WS graph model using the networkx library.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们的随机图生成器类的`make_graph_obj`方法使用了`networkx`库来实例化WS图模型。
- en: 'Furthermore, we add a `get_graph_config` method to return the list of nodes
    and edges in the graph. This will come in handy while we''re transforming the
    abstract graph into a neural network. We will also define some graph saving and
    loading methods for caching the generated graph both for reproducibility and efficiency
    reasons:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还添加了一个`get_graph_config`方法来返回图中节点和边的列表。在将抽象图转换为神经网络时，这将非常有用。我们还将为缓存生成的图定义一些图保存和加载方法，以提高可重现性和效率：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we will work on creating the actual neural network model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将开始创建实际的神经网络模型。
- en: Defining RandWireNN model modules
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义RandWireNN模型模块
- en: 'Now that we have the random graph generator, we need to transform it into a
    neural network. But before that, we will design some neural modules to facilitate
    that transformation. Let''s get started:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了随机图生成器，需要将其转换为神经网络。但在此之前，我们将设计一些神经模块来促进这种转换。让我们开始吧：
- en: 'Starting from the lowest level of the neural network, first, we will define
    a separable 2D convolutional layer, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从神经网络的最低级别开始，首先我们将定义一个可分离的2D卷积层，如下所示：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The separable convolutional layer is a cascade of a regular 3x3 2D convolutional
    layer followed by a pointwise 1x1 2D convolutional layer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 可分离卷积层是常规3x3的2D卷积层级联，后跟点态1x1的2D卷积层。
- en: 'Having defined the separable 2D convolutional layer, we can now define the
    ReLU-Conv-BN triplet unit:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了可分离的2D卷积层后，我们现在可以定义ReLU-Conv-BN三元组单元：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we mentioned earlier, the triplet unit is a cascade of a ReLU layer, followed
    by a separable 2D convolutional layer, followed by a batch normalization layer.
    We must also add a dropout layer for regularization.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，三元组单元是ReLU层级联，后跟可分离的2D卷积层，再跟批量归一化层。我们还必须添加一个dropout层进行正则化。
- en: 'With the triplet unit in place, we can now define a node in the graph with
    all of the `aggregation`, `transformation`, and `distribution` functionalities
    we need, as discussed at the beginning of this exercise:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有了三元组单元的存在，我们现在可以定义图中的节点，具备我们在本练习开始时讨论的所有`聚合`、`转换`和`分布`功能：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the `forward` method, we can see that if the number of incoming edges to
    the node is more than `1`, then a weighted average is calculated and these weights
    are learnable parameters of this node. The triplet unit is applied to the weighted
    average and the transformed (ReLU-Conv-BN-ed) output is returned.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward`方法中，如果节点的入边数大于`1`，则计算加权平均值，并使这些权重成为该节点的可学习参数。然后将三元组单元应用于加权平均值，并返回变换后的（ReLU-Conv-BN-ed）输出。
- en: 'We can now consolidate all of our graph and graph node definitions in order
    to define a randomly wired graph class, as shown here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以整合所有图和图节点的定义，以定义一个随机连线图类，如下所示：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the `__init__` method of this class, first, an abstract random graph is generated.
    Its list of nodes and edges are derived. Using the `GraphNode` class, each abstract
    node of this abstract random graph is encapsulated as a neuron of the desired
    neural network. Finally, a source or input node and a sink or an output node are
    added to the network to make the neural network ready for the image classification
    task.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类的 `__init__` 方法中，首先生成一个抽象的随机图。推导出其节点和边缘。使用 `GraphNode` 类，将该抽象随机图的每个抽象节点封装为所需神经网络的神经元。最后，向网络添加一个源或输入节点和一个汇或输出节点，使神经网络准备好进行图像分类任务。
- en: 'The `forward` method is also unconventional, as shown here:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 方法同样非传统，如下所示：'
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: First, a forward pass is run for the source neuron, and then a series of forward
    passes are run for the subsequent neurons based on the `list_of_nodes` for the
    graph. The individual forward passes are executed using `list_of_modules`. Finally,
    the forward pass through the sink neuron gives us the output of this graph.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为源神经元执行前向传递，然后根据图中 `list_of_nodes` 运行一系列后续神经元的前向传递。使用 `list_of_modules` 执行单独的前向传递。最后，通过汇流神经元的前向传递给出了该图的输出。
- en: Next, we will use these defined modules and the randomly wired graph class to
    build the actual RandWireNN model class.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这些定义的模块和随机连接的图类来构建实际的 RandWireNN 模型类。
- en: Transforming a random graph into a neural network
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将随机图转换为神经网络
- en: 'In the previous step, we defined one randomly wired graph. However, as we mentioned
    at the beginning of this exercise, a randomly wired neural network consists of
    several staged randomly wired graphs. The rationale behind that is to have a different
    (increasing) number of channels/features as we progress from the input neuron
    to the output neuron in an image classification task. This would be impossible
    with just one randomly wired graph because the number of channels is constant
    through one such graph, by design. Let''s get started:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一步中，我们定义了一个随机连接的图。然而，正如我们在本练习开始时提到的，随机连接的神经网络由多个分阶段的随机连接图组成。其背后的原理是，在图像分类任务中，从输入神经元到输出神经元的通道/特征数量会随着进展而不同（增加）。这是因为设计上，在一个随机连接的图中，通道的数量是恒定的，这是不可能的。让我们开始吧：
- en: 'In this step, we define the ultimate randomly wired neural network. This will
    have three randomly wired graphs cascaded next to each other. Each graph will
    have double the number of channels compared to the previous graph to help us align
    with the general practice of increasing the number of channels (while downsampling
    spatially) in an image classification task:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们定义了最终的随机连接的神经网络。这将由三个相邻的随机连接的图级联组成。每个图都会比前一个图的通道数量增加一倍，以帮助我们符合图像分类任务中增加通道数量（在空间上进行下采样）的一般实践：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `__init__` method starts with a regular 3x3 convolutional layer, followed
    by three staged randomly wired graphs with channels that double in terms of numbers.
    This is followed by a fully connected layer that flattens the convolutional output
    from the last neuron of the last randomly wired graph into a vector that's `1280`
    in size.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__` 方法以一个常规的 3x3 卷积层开始，然后是三个分阶段的随机连接图，通道数量随着图像分类任务中的增加而增加。接下来是一个完全连接的层，将最后一个随机连接图的卷积输出展平为大小为
    `1280` 的向量。'
- en: 'Finally, another fully connected layer produces a 10-sized vector containing
    the probabilities for the 10 classes, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个完全连接的层产生一个大小为 10 的向量，其中包含 10 个类别的概率，如下所示：
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `forward` method is quite self-explanatory, besides the global average pooling
    that is applied right after the first fully connected layer. This helps reduce
    dimensionality and the number of parameters in the network.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 方法相当不言自明，除了在第一个完全连接层之后应用的全局平均池化。这有助于减少网络中的维度和参数数量。'
- en: At this stage, we have successfully defined the RandWireNN model, loaded the
    datasets, and defined the model training routine. Now, we are all set to run the
    model training loop.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已成功定义了 RandWireNN 模型，加载了数据集，并定义了模型训练流程。现在，我们已经准备好运行模型训练循环。
- en: Training the RandWireNN model
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 RandWireNN 模型
- en: 'In this section, we will set the model''s hyperparameters and train the RandWireNN
    model. Let''s get started:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将设置模型的超参数并训练 RandWireNN 模型。让我们开始吧：
- en: 'We have defined all the building blocks for our exercise. Now, it is time to
    execute it. First, let''s declare the necessary hyperparameters:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了我们练习的所有构建块。现在是执行它的时候了。首先，让我们声明必要的超参数：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Having declared the hyperparameters, we instantiate the RandWireNN model, along
    with the optimizer and loss function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在声明了超参数之后，我们实例化了 RandWireNN 模型，以及优化器和损失函数：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we begin training the model. We''re training the model for `5` epochs
    here for demonstration purposes, but you are encouraged to train for longer to
    see the boost in performance:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们开始训练模型。在这里我们演示目的训练了`5`个epochs，但建议您延长训练时间以提高性能：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This should result in the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '![Figure 5.7 – RandWireNN training logs](img/file44.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – RandWireNN 训练日志](img/file44.png)'
- en: Figure 5.7 – RandWireNN training logs
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – RandWireNN 训练日志
- en: It is evident from these logs that the model is progressively learning as the
    epochs progress. The performance on the validation set seems to be consistently
    increasing, which indicates model generalizability.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些日志中可以明显看出，随着epochs的增加，模型正在逐步学习。验证集上的性能似乎在持续提升，这表明模型具有良好的泛化能力。
- en: With that, we have created a model with no particular architecture in mind that
    can reasonably perform the task of image classification on the CIFAR-10 dataset.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们创建了一个没有特定架构的模型，可以在CIFAR-10数据集上合理地执行图像分类任务。
- en: Evaluating and visualizing the RandWireNN model
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估和可视化 RandWireNN 模型
- en: 'Finally, we will look at this model''s test set performance before briefly
    exploring the model architecture visually. Let''s get started:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在简要探索模型架构之前，我们将查看此模型在测试集上的性能。让我们开始吧：
- en: 'Once the model has been trained, we can evaluate it on the test set:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们可以在测试集上进行评估：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This should result in the following output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生以下输出：
- en: '![Figure 5.8 – RandWireNN evaluation results](img/file45.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – RandWireNN 评估结果](img/file45.jpg)'
- en: Figure 5.8 – RandWireNN evaluation results
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – RandWireNN 评估结果
- en: The best performing model was found at the fourth epoch, with over 67% accuracy.
    Although the model is not perfect yet, we can train it for more epochs to achieve
    better performance. Also, a random model for this task would perform at an accuracy
    of 10% (because of 10 equally likely classes), so an accuracy of 67.73% is still
    promising, especially given the fact that we are using a randomly generated neural
    network architecture.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳表现模型在第四个epoch时找到，准确率超过67%。虽然模型尚未完美，但我们可以继续训练更多epochs以获得更好的性能。此外，对于此任务，一个随机模型的准确率将为10%（因为有10个同等可能的类别），因此67.73%的准确率仍然是有希望的，特别是考虑到我们正在使用随机生成的神经网络架构。
- en: 'To conclude this exercise, let''s look at the model architecture that was learned.
    The original image is too large to be displayed here. You can find the full image
    at our github repository both in .svg format [5.4] and in .pdf format [5.5] .
    In the following figure, we have vertically stacked three parts - the input section,
    a mid section and the output section, of the original neural network:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 结束这个练习之前，让我们看看所学的模型架构。原始图像太大不能在这里显示。您可以在我们的github仓库中找到完整的图像，分别以.svg格式 [5.4]
    和 .pdf 格式 [5.5] 。在下图中，我们垂直堆叠了三部分 - 输入部分，中间部分和输出部分，原始神经网络的：
- en: '![Figure 5.9 – RandWireNN architecture](img/file46.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – RandWireNN 架构](img/file46.png)'
- en: Figure 5.9 – RandWireNN architecture
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – RandWireNN 架构
- en: 'From this graph, we can observe the following key points:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中，我们可以观察到以下关键点：
- en: At the top, we can see the beginning of this neural network, which consists
    of a 64-channel 3x3 2D convolutional layer, followed by a 64-channel 1x1 pointwise
    2D convolutional layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们可以看到这个神经网络的开头，它由一个64通道的3x3的2D卷积层组成，后面是一个64通道的1x1的点卷积2D层。
- en: In the middle section, we can see the transition between the third- and fourth-stage
    random graphs, where we can see the sink neuron, `conv_layer_3`, of the stage
    3 random graph followed by the source neuron `conv_layer_4`, of the stage 4 random
    graph.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间部分，我们可以看到第三阶段和第四阶段随机图之间的过渡，其中我们可以看到第三阶段随机图的汇聚神经元`conv_layer_3`，以及第四阶段随机图的源神经元`conv_layer_4`。
- en: Lastly, the lowermost section of the graph shows the final output layers – the
    sink neuron (a 512-channel separable 2D convolutional layer) of the stage 4 random
    graph, followed by a fully connected flattening layer, resulting in a 1,280-size
    feature vector, followed by a fully connected softmax layer that produces the
    10 class probabilities.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图的最底部显示了最终的输出层 - 随机图第4阶段的汇聚神经元（一个512通道的可分离二维卷积层），接着是一个全连接的展平层，结果是一个1280大小的特征向量，然后是一个全连接的
    softmax 层，产生10个类别的概率。
- en: Hence, we have built, trained, tested, and visualized a neural network model
    for image classification without specifying any particular model architecture.
    We did specify some overarching constraints over the structure, such as the penultimate
    feature vector length (`1280`), the number of channels in the separable 2D convolutional
    layers (`64`), the number of stages in the RandWireNN model (`4`), the definition
    of each neuron (ReLU-Conv-BN triplet), and so on.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经构建、训练、测试和可视化了一个用于图像分类的神经网络模型，未指定任何特定的模型架构。我们确实对结构施加了一些总体约束，比如笔架特征向量的长度（`1280`），可分离二维卷积层中的通道数（`64`），RandWireNN
    模型中的阶段数（`4`），每个神经元的定义（ReLU-Conv-BN 三元组），等等。
- en: However, we didn't specify what the structure of this neural network architecture
    should look like. We used a random graph generator to do this for us, which opens
    up an almost infinite number of possibilities in terms of finding optimal neural
    network architectures.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并没有指定这个神经网络架构应该是什么样的。我们使用了一个随机图生成器来为我们完成这项工作，这为找到最佳神经网络架构开启了几乎无限的可能性。
- en: Neural architecture search is an ongoing and promising area of research in the
    field of deep learning. Largely, this fits in well with the field of training
    custom machine learning models for specific tasks, referred to as AutoML.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 神经架构搜索是深度学习领域一个不断发展且富有前景的研究领域。在很大程度上，这与为特定任务训练定制的机器学习模型的领域相契合，被称为 AutoML。
- en: '**AutoML** stands for **automated machine learning** as it does away with the
    necessity of having to manually load datasets, predefine a particular neural network
    model architecture to solve a given task, and manually deploy models into production
    systems. In *Chapter 16, PyTorch and AutoML*, we will discuss AutoML in detail
    and learn how to build such systems with PyTorch.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**AutoML** 代表**自动化机器学习**，因为它消除了手动加载数据集，预定义特定神经网络模型架构来解决给定任务，并手动将模型部署到生产系统的必要性。在
    *第16章，PyTorch 和 AutoML* 中，我们将详细讨论 AutoML，并学习如何使用 PyTorch 构建这样的系统。'
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at two distinct hybrid types of neural networks.
    First, we looked at the transformer model – the attention-only-based models with
    no recurrent connections that have outperformed all recurrent models on multiple
    sequential tasks. We ran through an exercise where we built, trained, and evaluated
    a transformer model on a language modeling task with the WikiText-2 dataset using
    PyTorch. In the second and final section of this chapter, we took up from where
    we left off in *Chapter 3, Deep CNN Architectures*, where we discussed the idea
    of optimizing for model architectures rather than optimizing for just the model
    parameters while fixing the architecture. We explored one of the approaches to
    do that – using randomly wired neural networks (RandWireNNs) – where we generated
    random graphs, assigned meanings to the nodes and edges of these graphs, and interconnected
    these graphs to form a neural network.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看了两种不同的混合类型的神经网络。首先，我们看了变压器模型 - 基于注意力的模型，没有循环连接，已在多个顺序任务上表现出色。我们进行了一个练习，在这个练习中我们使用
    PyTorch 在 WikiText-2 数据集上构建、训练和评估了一个变压器模型来执行语言建模任务。在本章的第二个也是最后一个部分中，我们接着上一章 *第3章，深度卷积神经网络架构*
    的内容，讨论了优化模型架构而不仅仅是优化模型参数的想法。我们探讨了一种方法 - 使用随机连线神经网络（RandWireNNs）- 在这种网络中我们生成随机图，给这些图的节点和边赋予含义，然后将这些图相互连接形成一个神经网络。
- en: In the next chapter, we will switch gears and move away from model architectures
    and look at some interesting PyTorch applications. We will learn how to generate
    music and text through generative deep learning models using PyTorch.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转变思路，远离模型架构，看看一些有趣的 PyTorch 应用。我们将学习如何使用 PyTorch 通过生成式深度学习模型来生成音乐和文本。
