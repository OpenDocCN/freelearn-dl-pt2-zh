- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Scaling a Deep Learning Pipeline
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展深度学习管道
- en: '**Amazon Web Services** (**AWS**) opens many possibilities in **deep learning**
    (**DL**) model deployments. In this chapter, we will introduce the two most popular
    services designed for deploying a DL model as an inference endpoint: **Elastic
    Kubernetes Service** (**EKS**) and **SageMaker**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚马逊网络服务**（**AWS**）在**深度学习**（**DL**）模型部署方面提供了许多可能性。在本章中，我们将介绍两种最受欢迎的服务，专为将DL模型部署为推理端点而设计：**弹性Kubernetes服务**（**EKS**）和**SageMaker**。'
- en: In the first half, we will describe the EKS-based approach. First, we will discuss
    how to create inference endpoints for **TensorFlow** (**TF**) and PyTorch models
    and deploy them using EKS. We will also introduce the **Elastic Inference** (**EI**)
    accelerator, which can increase the throughput while reducing the cost. EKS clusters
    have pods that host the inference endpoints as web servers. As the last topic
    for EKS-based deployment, we will introduce how the pods can be scaled horizontally
    for the dynamic incoming traffic.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前半部分，我们将描述基于EKS的方法。首先，我们将讨论如何为**TensorFlow**（**TF**）和PyTorch模型创建推理端点，并使用EKS部署它们。我们还将介绍**弹性推理**（**EI**）加速器，它可以提高吞吐量同时降低成本。EKS集群有托管推理端点的pod作为Web服务器。作为基于EKS的部署的最后一个主题，我们将介绍如何根据动态流量扩展这些pod。
- en: In the second half, we will introduce SageMaker-based deployment. We will discuss
    how to create inference endpoints for TF, PyTorch, and ONNX models. Additionally,
    the endpoints will be optimized using **Amazon SageMaker Neo** and EI accelerators.
    Then, we will set up automatic scaling for the inference endpoints running on
    SageMaker. Finally, we will wrap up this chapter by describing how to host multiple
    models in a single SageMaker inference endpoint.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在后半部分，我们将介绍基于SageMaker的部署。我们将讨论如何为TF、PyTorch和ONNX模型创建推理端点。此外，端点将使用**亚马逊SageMaker
    Neo**和EI加速器进行优化。然后，我们将设置在SageMaker上运行的推理端点的自动扩展。最后，我们将通过描述如何在单个SageMaker推理端点中托管多个模型来结束本章。
- en: 'In this chapter, we are going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中，我们将涵盖以下主要主题：
- en: Inferencing using Elastic Kubernetes Service
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用弹性Kubernetes服务进行推理
- en: Inferencing using SageMaker
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SageMaker进行推理
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can download the supplemental material for this chapter from this book’s
    GitHub repository at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的GitHub存储库下载本章的补充材料，网址为[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9)。
- en: Inferencing using Elastic Kubernetes Service
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用弹性Kubernetes服务进行推理
- en: EKS is designed to provide Kubernetes clusters for application deployment by
    simplifying the complex cluster management process ([https://aws.amazon.com/eks](https://aws.amazon.com/eks)).
    The detailed steps for creating an EKS cluster can be found at [https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html).
    In general, an EKS cluster is used to deploy any web service application and scale
    it as necessary. The inference endpoint on EKS is just a web service application
    that handles model inference requests. In this section, you will learn how to
    host a DL model inference endpoint on EKS.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: EKS旨在通过简化复杂的集群管理过程提供用于应用部署的Kubernetes集群（[https://aws.amazon.com/eks](https://aws.amazon.com/eks)）。有关创建EKS集群的详细步骤，请参阅[https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html](https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html)。通常情况下，EKS集群用于部署任何Web服务应用程序，并根据需要进行扩展。EKS上的推理端点只是处理模型推理请求的Web服务应用程序。在本节中，您将学习如何在EKS上托管DL模型推理端点。
- en: A Kubernetes cluster has a control plane and a set of nodes. The control plane
    makes scheduling and scaling decisions based on the volume of incoming traffic.
    With scheduling, the control plane manages which node runs a job at a given point
    in time. With scaling, the control plane increases or decreases the size of the
    pod based on the volume of traffic coming into the endpoints. EKS manages these
    components behind the scenes so that you can focus on hosting your services efficiently
    and effectively.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群有一个控制平面和一组节点。控制平面根据流入流量的大小进行调度和扩展决策。在调度方面，控制平面管理在给定时间点运行作业的节点。在扩展方面，控制平面根据流入端点的流量大小增加或减少pod的大小。EKS在幕后管理这些组件，以便您可以专注于有效和高效地托管您的服务。
- en: This section begins by describing how to set up an EKS cluster. Then, we will
    describe how to create endpoints using TF and PyTorch to handle model inference
    requests on an EKS cluster. Next, we will discuss the EI accelerator, which improves
    the inference performance, along with cost reduction. Finally, we will introduce
    a way to scale the services dynamically based on the volume of incoming traffic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先描述了如何设置 EKS 集群。然后，我们将描述如何使用 TF 和 PyTorch 创建端点，以处理 EKS 集群上的模型推断请求。接下来，我们将讨论
    EI 加速器，该加速器提高了推断性能，并降低了成本。最后，我们将介绍一种根据入站流量量动态扩展服务的方法。
- en: Preparing an EKS cluster
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备 EKS 集群
- en: The first step of model deployment based on EKS is to create a pod of appropriate
    hardware resources. In this section, we will use the GPU Docker images recommended
    by AWS ([https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)).
    These standard images are already registered and available on **Elastic Container
    Registry** (**ECR**), which provides a secure, scalable, and reliable registry
    for Docker images ([https://aws.amazon.com/ecr](https://aws.amazon.com/ecr)).
    Next, we should apply the NVIDIA device plugin to the container. This plugin enables
    **machine learning** (**ML**) operations to exploit the underlying hardware to
    achieve lower latency. For more details on the NVIDIA device plugin, we recommend
    reading [https://github.com/awslabs/aws-virtual-gpu-device-plugin](https://github.com/awslabs/aws-virtual-gpu-device-plugin).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 EKS 的模型部署的第一步是创建适当硬件资源的 pod。在本节中，我们将使用 AWS 建议的 GPU Docker 镜像（[https://github.com/aws/deep-learning-containers/blob/master/available_images.md](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)）。这些标准镜像已在
    Elastic Container Registry（ECR）注册并可用，ECR 提供了一个安全、可扩展和可靠的 Docker 镜像注册表（[https://aws.amazon.com/ecr](https://aws.amazon.com/ecr)）。接下来，我们应该将
    NVIDIA 设备插件应用到容器中。此插件使机器学习（ML）操作能够利用底层硬件以实现较低的延迟。关于 NVIDIA 设备插件的更多详细信息，建议阅读 [https://github.com/awslabs/aws-virtual-gpu-device-plugin](https://github.com/awslabs/aws-virtual-gpu-device-plugin)。
- en: 'In the following code snippet, we will use `kubectl`, the `kubectl`, you need
    to provide a YAML file that consists of information about clusters, users, namespaces,
    and authentication mechanisms ([https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig)).
    The most popular operation is `kubectl apply`, which creates or modifies resources
    in an EKS cluster:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们将使用 `kubectl`，`kubectl` 需要提供一个关于集群、用户、命名空间和认证机制信息的 YAML 文件（[https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig)）。最常见的操作是
    `kubectl apply`，它在 EKS 集群中创建或修改资源：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding use case, the `kubectl apply` command applies the NVIDIA device
    plugin according to the specification specified in the YAML file to the Kubernetes
    cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述用例中，`kubectl apply` 命令根据 YAML 文件中的规范向 Kubernetes 集群应用 NVIDIA 设备插件。
- en: Configuring EKS
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 EKS
- en: 'A YAML file is used to configure both the machines that make up the Kubernetes
    cluster and the application running within the cluster. The configurations in
    the YAML file can be broken down into two parts based on their type: *deployment*
    and *service*. The deployment part controls the application running within the
    pod. In this section, it will be used to create an endpoint from DL models. In
    the EKS context, a set of applications running on one or more pods of a Kubernetes
    cluster is called a service. The service part creates and configures the service
    on the cluster. Throughout the service part, we will create a unique URL for the
    service that external connections can use and configure load balancing for incoming
    traffic.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: YAML 文件用于配置构成 Kubernetes 集群的机器以及集群内运行的应用程序。根据类型，YAML 文件中的配置可以分为两部分：*deployment*
    和 *service*。deployment 部分控制 pod 内运行的应用程序。在本节中，它将用于创建 DL 模型的端点。在 EKS 环境中，运行在一个或多个
    pod 上的一组应用程序称为 service。service 部分在集群上创建和配置服务。在整个 service 部分中，我们将为外部连接创建一个唯一的服务
    URL，并配置入站流量的负载均衡。
- en: 'When managing an EKS cluster, namespaces can be useful as they isolate a group
    of resources within the cluster. To create a namespace, you can simply use the
    `kubectl create namespace` terminal command, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理 EKS 集群时，命名空间可以很有用，因为它们在集群内部隔离了一组资源。要创建命名空间，可以简单地使用 `kubectl create namespace`
    终端命令，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding command, we constructed the `tf-inference` namespace for the
    inference endpoints and services that we will be creating in the following section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述命令中，我们为接下来将在下一节创建的推断端点和服务构建了 `tf-inference` 命名空间。
- en: Creating an inference endpoint using the TensorFlow model on EKS
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 模型在 EKS 上创建推断端点
- en: In this section, we will describe an EKS configuration file (`tf.yaml`) designed
    to host an inference endpoint using a TF model. The endpoint is created by *TensorFlow
    Service*, a system designed for deploying a TF model ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)).
    Since our main focus is on EKS configurations, we will simply assume that a trained
    TF model is already available on S3 as a `.pb` file.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将描述一个设计用于托管 TF 模型推断端点的 EKS 配置文件（`tf.yaml`）。端点由 *TensorFlow Service*
    创建，这是一个用于部署 TF 模型的系统（[https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)）。由于我们的主要重点是在
    EKS 配置上，我们将简单地假设一个训练有素的 TF 模型已经作为 `.pb` 文件存储在 S3 上。
- en: 'First, let’s look at the `Deployment` part of the configuration, which handles
    the endpoint creation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看配置的 `Deployment` 部分，负责处理端点的创建：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we can see, the `Deployment` part of the configuration starts with `kind:
    Deployment`. In this first part of the configuration, we provide some metadata
    about the endpoint and define the system settings by filling in the `spec` section.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们所见，配置的 `Deployment` 部分以 `kind: Deployment` 开始。在配置的第一部分中，我们提供了有关端点的一些元数据，并通过填写
    `spec` 部分定义了系统设置。'
- en: 'The most important configurations for the endpoint are specified under `template`.
    We will create an endpoint that can be accessed using **HyperText Transfer Protocol**
    (**HTTP**) requests, as well as **Remote Procedure Call** (**gRPC**) requests.
    HTTP is the most basic transfer data protocol for web clients and servers. Built
    on top of HTTP, gRPC is an open source protocol for sending requests and receiving
    responses in binary format:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 端点的最重要配置在 `template` 下指定。我们将创建一个端点，可以通过 **超文本传输协议**（**HTTP**）请求以及 **远程过程调用**（**gRPC**）请求进行访问。HTTP
    是用于网络客户端和服务器的最基本的传输数据协议。构建在 HTTP 之上，gRPC 是一个用于以二进制格式发送请求和接收响应的开源协议：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Under the `template` section, we specify an ECR image to use (`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-gpu-py36-cu100-ubuntu18.04`),
    the command to create the TF inference endpoint (`command: /usr/bin/tensorflow_model_server`),
    the arguments for TF serving (`args`), and the ports configuration for containers
    (`ports`).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '在 `template` 部分下，我们指定了要使用的 ECR 镜像（`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-gpu-py36-cu100-ubuntu18.04`）、创建
    TF 推断端点的命令（`command: /usr/bin/tensorflow_model_server`）、TF 服务的参数（`args`）以及容器的端口配置（`ports`）。'
- en: 'The TF serving arguments contains the model’s name (`--model_name=saved_model`),
    the location of the model on S3 (`--model_base_path=s3://mybucket/models`), the
    ports for HTTP access (`--rest_api_port=8500`), and the ports for gRPC access
    (`--port=9000`). The two `ContainerPort` configurations under `ports` are used
    to expose the endpoints to external connections (`containerPort: 8500` and `containerPort:
    9000`).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 'TF 服务参数包含模型名称（`--model_name=saved_model`）、模型在 S3 上的位置（`--model_base_path=s3://mybucket/models`）、用于
    HTTP 访问的端口（`--rest_api_port=8500`）以及用于 gRPC 访问的端口（`--port=9000`）。`ports` 下的两个
    `ContainerPort` 配置用于向外部连接暴露端点（`containerPort: 8500` 和 `containerPort: 9000`）。'
- en: 'Next, let’s look at the second part of the YAML file – that is, the configurations
    for `Service`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下 YAML 文件的第二部分 – `Service` 的配置：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `Service` part of the configuration starts with `kind: Service`. Under
    the `name: http-tf-serving` section, we have `port: 8500`, which refers to the
    port that the TF serving web server is listening to inside the pods for HTTP requests.
    `targetPort` specifies the port that the pods use to expose the corresponding
    port. We have another set of ports configuration for gRPC under the `name: grpc-tf-serving`
    section.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '配置的 `Service` 部分以 `kind: Service` 开始。在 `name: http-tf-serving` 部分下，我们有 `port:
    8500`，指的是用于 HTTP 请求的 TF 服务 Web 服务器在 Pod 内监听的端口。`targetPort` 指定了用于暴露相应端口的 Pod 使用的端口。我们在
    `name: grpc-tf-serving` 部分下有另一组 gRPC 的端口配置。'
- en: To apply the configuration to the underlying cluster, you can simply provide
    this YAML file to the `kubectl apply` command.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要将配置应用于底层集群，您只需将此 YAML 文件提供给 `kubectl apply` 命令。
- en: Next, we will create an endpoint for a PyTorch model on EKS.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 EKS 上为 PyTorch 模型创建一个端点。
- en: Creating an inference endpoint using a PyTorch model on EKS
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在EKS上使用PyTorch模型创建推断端点
- en: In this section, you will learn how to create a PyTorch model inference endpoint
    on EKS. First, we would like to introduce *TorchServe*, an open source model serving
    framework for PyTorch ([https://pytorch.org/serve](https://pytorch.org/serve)).
    It is designed to simplify the process of PyTorch model deployment at scale. EKS
    configurations for PyTorch model deployment are very similar to what we have described
    for deploying a TF model in the previous section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何在EKS上创建PyTorch模型推断端点。首先，我们想介绍*TorchServe*，这是一个面向PyTorch的开源模型服务框架（[https://pytorch.org/serve](https://pytorch.org/serve)）。它旨在简化大规模部署PyTorch模型的过程。用于PyTorch模型部署的EKS配置与前一节描述的TF模型部署非常相似。
- en: 'First, a PyTorch model `.pth` file needs to be converted into a `.mar` file,
    which is the format required by TorchServe ([https://github.com/pytorch/serve/blob/master/model-archiver/README.md](https://github.com/pytorch/serve/blob/master/model-archiver/README.md)).
    The conversion can be achieved using the `torch-model-archiver` package. TorchServe
    and `torch-model-archiver` can be downloaded and installed through `pip`, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要将PyTorch模型的`.pth`文件转换为`.mar`文件，这是TorchServe所需的格式（[https://github.com/pytorch/serve/blob/master/model-archiver/README.md](https://github.com/pytorch/serve/blob/master/model-archiver/README.md)）。可以使用`torch-model-archiver`包实现此转换。TorchServe和`torch-model-archiver`可以通过以下方式使用`pip`下载和安装：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The conversion, when using the `torch-model-archiver` command, is shown in
    the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`torch-model-archiver`命令进行转换时，代码如下所示：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, the `torch-model-archiver` command takes in `model-name`
    (the name of the output `.mar` file, which is `archived_model`), `version` (PyTorch
    version 1.0), `serialized-file` (the input PyTorch `.pth` file, which is `model.pth`),
    and `handler` (the name of the file that defines TorchServe inference logic; that
    is, `run_inference`, which indicates the file named `run_inference.py`). The command
    will generate an `archived_model.mar` file, which will be uploaded to an S3 bucket
    for endpoint hosting through EKS.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`torch-model-archiver`命令接受`model-name`（输出`.mar`文件的名称，即`archived_model`）、`version`（PyTorch版本1.0）、`serialized-file`（输入的PyTorch
    `.pth`文件，即`model.pth`）和`handler`（定义TorchServe推断逻辑的文件名；即`run_inference`，指的是名为`run_inference.py`的文件）。该命令将生成一个`archived_model.mar`文件，该文件将通过EKS上传到S3存储桶以供端点托管。
- en: 'Another command we would like to introduce before discussing EKS configuration
    is `mxnet-model-server`. This command is available in a DLAMI instance, allowing
    you to host a web server that runs PyTorch inference for the incoming requests:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论EKS配置之前，我们还想介绍另一个命令，即`mxnet-model-server`。该命令在DLAMI实例中可用，允许您托管一个运行PyTorch推断的Web服务器以处理传入请求：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding example, the `mxnet-model-server` command, with the `start`
    parameter, creates an endpoint for the model provided through the `models` parameter.
    As you can see, the `models` parameter points to the location of the model on
    S3 (`archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar`).
    The input arguments for the model are specified in the `/home/model-server/config.properties`
    file, which is passed to the command through the `mms-config` parameter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，`mxnet-model-server`命令使用`start`参数创建了通过`models`参数提供的模型的端点。正如您所看到的，`models`参数指向了位于S3上模型的位置（`archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar`）。模型的输入参数在传递给命令的`mms-config`参数的`/home/model-server/config.properties`文件中指定。
- en: 'Now, we will discuss how the `Deployment` part of the EKS configuration must
    be filled in. Every component can stay similar to the version for the TF model.
    The main difference comes from the `template` section, as shown in the following
    code snippet:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论必须填写EKS配置中的`Deployment`部分。每个组件可以保持与TF模型版本类似。主要区别在于`template`部分，如下面的代码片段所示：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code, we are using a different Docker image that has PyTorch
    installed (`image: "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-gpu-py36-cu101-ubuntu16.04"`).
    The configuration takes in the `mxnet-model-server` command to create an inference
    endpoint. The port we will be using for this endpoint is `8080`. The only change
    we made for the `Service` part can be found in the `Ports` section; we must ensure
    that an external port is assigned and connected to port `8080` – that is, the
    port that the endpoint is hosted on. Again, you can use the `kubectl apply` command
    to apply the changes.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在上述代码中，我们使用了一个不同的Docker镜像，其中已安装了PyTorch（`image: "763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-gpu-py36-cu101-ubuntu16.04"`）。配置使用`mxnet-model-server`命令创建推理端点。我们将在此端点上使用的端口是`8080`。我们对`Service`部分所做的唯一更改可以在`Ports`部分找到；我们必须确保分配了外部端口并连接到端口`8080`，即端点托管的端口。同样，您可以使用`kubectl
    apply`命令来应用更改。'
- en: In the next section, we will describe how to interact with the endpoint hosted
    by the EKS cluster.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将描述如何与由EKS集群托管的端点进行交互。
- en: Communicating with an endpoint on EKS
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与EKS上端点的通信
- en: 'Now that we have an endpoint running, we will explain how you can send a request
    and retrieve an inference result. First, we need to identify the IP address of
    the service using `kubectl get services`, as shown in the following code snippet:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个运行中的端点，我们将解释如何发送请求并检索推理结果。首先，我们需要使用`kubectl get services`命令来识别服务的IP地址，如下面的代码片段所示：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding command will return a list of services and their external IP
    address:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将返回服务及其外部IP地址的列表：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this example, we will make use of the `tf-inference` service we created
    in the *Creating an inference endpoint using the TensorFlow model on EKS* section.
    From the sample output of `kubectl get services`, we can see that the service
    is running with an external IP address of `104.198.xxx.xx`. To access the service
    via HTTP, you need to append the port for HTTP to the IP address: `http://104.198.xxx.xx:8500`.
    If you are interested in creating an explicit URL for the IP address, please go
    to [https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster](https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将使用在*在EKS上使用TensorFlow模型创建推理端点*部分中创建的`tf-inference`服务。从`kubectl get
    services`的示例输出中，我们可以看到该服务正在运行，并且具有`104.198.xxx.xx`的外部IP地址。要通过HTTP访问该服务，您需要将HTTP的端口附加到IP地址：`http://104.198.xxx.xx:8500`。如果您有兴趣为IP地址创建显式URL，请访问[https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster](https://aws.amazon.com/premiumsupport/knowledge-center/eks-kubernetes-services-cluster)。
- en: 'To send a prediction request to the endpoint and receive an inference result,
    you need to make a POST-typed HTTP request. If you want to send a request from
    the terminal, you can use the `curl` command as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要向端点发送预测请求并接收推理结果，您需要进行POST类型的HTTP请求。如果您想从终端发送请求，您可以使用以下`curl`命令：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding command, we are sending JSON data (`demo_input.json`) to the
    endpoint (`http://104.198.xxx.xx:8500/v1/models/demo:predict`). The input JSON
    file, `demo_input.json`, consists of the following code snippet:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述命令中，我们将JSON数据（`demo_input.json`）发送到端点（`http://104.198.xxx.xx:8500/v1/models/demo:predict`）。输入JSON文件`demo_input.json`包含以下代码片段：
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The response data we will receive from the endpoint also consists of JSON data
    that looks as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从端点收到的响应数据也由以下的JSON数据组成：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A detailed explanation of the input and output JSON data structures can be
    found in the official documentation: [https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方文档中找到关于输入和输出JSON数据结构的详细解释：[https://www.tensorflow.org/tfx/serving/api_rest](https://www.tensorflow.org/tfx/serving/api_rest)。
- en: If you are interested in using gRPC instead of HTTP, you can find the details
    at [https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework](https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣使用gRPC而不是HTTP，您可以在[https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework](https://aws.amazon.com/blogs/opensource/the-versatility-of-grpc-an-open-source-high-performance-rpc-framework)找到详细信息。
- en: Congratulations! You have successfully created an endpoint for your model that
    your application can access over the network. Next, we will introduce Amazon EI
    accelerator, which can reduce the inference latency and EKS costs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功为您的模型创建了一个应用程序可以通过网络访问的端点。接下来，我们将介绍 Amazon EI 加速器，它可以降低推理延迟和 EKS 成本。
- en: Improving EKS endpoint performance using Amazon Elastic Inference
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高 EKS 端点性能使用 Amazon 弹性推理
- en: In this section, we will describe how to create an EKS cluster with the EI accelerator,
    a low-cost GPU-powered acceleration. The EI accelerator can be linked to Amazon
    EC2 and Sagemaker instances or `eia2.*`-typed instances. The complete description
    of eia2.* instances can be found at [https://aws.amazon.com/machine-learning/elastic-inference/pricing](https://aws.amazon.com/machine-learning/elastic-inference/pricing).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何使用 EI 加速器创建 EKS 集群，这是一种低成本的 GPU 加速。EI 加速器可以链接到 Amazon EC2 和 Sagemaker
    实例或 `eia2.*` 类型的实例。eia2.* 实例的完整描述可以在 [https://aws.amazon.com/machine-learning/elastic-inference/pricing](https://aws.amazon.com/machine-learning/elastic-inference/pricing)
    找到。
- en: To make the most out of AWS resources, you also need to compile your model using
    *AWS Neuron* ([https://aws.amazon.com/machine-learning/neuron](https://aws.amazon.com/machine-learning/neuron)).
    The advantage of Neuron models comes from the fact that they can utilize Amazon
    EC2 Inf1 instances. These types of machines consist of *AWS Inferentia*, a custom
    chip designed by AWS for ML in the cloud ([https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用 AWS 资源，您还需要使用 *AWS Neuron* ([https://aws.amazon.com/machine-learning/neuron](https://aws.amazon.com/machine-learning/neuron))
    编译您的模型。Neuron 模型的优势在于它们可以利用 Amazon EC2 Inf1 实例。这些类型的机器包含 *AWS Inferentia*，这是 AWS
    专为云中的 ML 设计的定制芯片 ([https://aws.amazon.com/machine-learning/inferentia](https://aws.amazon.com/machine-learning/inferentia))。
- en: The AWS Neuron SDK is pre-installed in AWS DL containers and **Amazon Machine
    Images** (**AMI**). In this section, we will focus on TF models. However, PyTorch
    model compilation goes through the same process. The detailed steps for TF can
    be found at [https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html)
    and the steps for PyTorch can be found at [https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Neuron SDK 预安装在 AWS DL 容器和 **Amazon Machine Images** (**AMI**) 中。在本节中，我们将重点放在
    TF 模型上。然而，PyTorch 模型的编译过程与此相同。有关 TF 的详细步骤可以在 [https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-tf-neuron.html)
    找到，PyTorch 的步骤可以在 [https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-pytorch-neuron.html)
    找到。
- en: 'Compiling a TF model into a Neuron model can be achieved by using `tf.neuron.saved_model.compile`
    function of TF:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将 TF 模型编译为 Neuron 模型可以通过使用 TF 的 `tf.neuron.saved_model.compile` 函数来实现。
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For this function, we simply need to provide where the input model is located
    (`tf_model_dir`) and where we want to store the output Neuron model (`neuron_model_dir`).
    Just as we upload a TF model to an S3 bucket for endpoint creation, we need to
    move the Neuron model to an S3 bucket as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此功能，我们只需提供输入模型的位置 (`tf_model_dir`) 和我们想要存储输出 Neuron 模型的位置 (`neuron_model_dir`)。正如我们将
    TF 模型上传到 S3 存储桶以进行端点创建一样，我们还需要将 Neuron 模型移动到 S3 存储桶。
- en: 'Again, the changes you need to make to the EKS configuration only need to be
    done in the `template` section of the `Deployment` part. The following code snippet
    describes the updated sections of the configuration:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提到，您需要对 EKS 配置进行的更改仅需要在 `Deployment` 部分的 `template` 部分完成。以下代码片段描述了配置的更新部分：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first thing we notice from the preceding configuration is that it is very
    similar to the one we described in the *Creating an inference endpoint using the
    TensorFlow model on EKS* section. The difference mainly comes from the `image`,
    `command`, and `args` sections. First, we need to use a DL container with AWS
    Neuron and TensorFlow Serving applications (`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04`).
    Next, the entry point script for the model artifact file is passed through the
    `command` key: `/usr/local/bin/entrypoint.sh`. The entry point script is used
    to start the web server using `args`. To create an endpoint from a Neuron model,
    we must specify the S3 bucket where the target Neuron model is stored as a `model_base_path`
    parameter (`--model_base_path=s3://mybucket/neuron_model/`).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '从上述配置中我们注意到的第一件事是，它与我们在 *在 EKS 上使用 TensorFlow 模型创建推断端点* 部分描述的非常相似。区别主要来自于 `image`、`command`
    和 `args` 部分。首先，我们需要使用带有 AWS Neuron 和 TensorFlow Serving 应用程序的 DL 容器（`image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04`）。接下来，通过
    `command` 键传递模型文件的入口点脚本：`/usr/local/bin/entrypoint.sh`。入口点脚本用于使用 `args` 启动 Web
    服务器。要从 Neuron 模型创建端点，必须指定存储目标 Neuron 模型的 S3 存储桶作为 `model_base_path` 参数（`--model_base_path=s3://mybucket/neuron_model/`）。'
- en: To apply the changes to the cluster, you can simply pass the updated YAML file
    to the `kubectl apply` command.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要将更改应用到集群，只需将更新后的 YAML 文件传递给 `kubectl apply` 命令。
- en: Lastly, we will look at the autoscaling feature of EKS to increase the stability
    of the endpoint.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将看一下 EKS 的自动缩放功能，以提高端点的稳定性。
- en: Resizing EKS cluster dynamically using autoscaling
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态调整 EKS 集群大小使用自动缩放
- en: An EKS cluster can automatically adjust the size of the cluster based on the
    volume of traffic. The idea of horizontal pod autoscaling is to scale up the number
    of running applications by increasing the number of pods as the number of incoming
    requests increases. Similarly, some pods will be freed up when the volume of the
    incoming traffic decreases.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: EKS 集群可以根据流量量自动调整集群大小。水平 Pod 自动缩放的理念是根据传入请求的增加来增加运行应用程序的 Pod 数量。类似地，当传入流量减少时，一些
    Pod 将被释放。
- en: 'Once an application has been deployed through the `kubectl apply` command,
    autoscaling can be set up using the `kubectl autoscale` command, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `kubectl apply` 命令部署应用程序后，可以使用 `kubectl autoscale` 命令设置自动缩放，如下所示：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As shown in the preceding example, the `kubectl autoscale` command takes in
    the name of the application specified in the `Deployment` part of the YAML file,
    `cpu-percent` (the cut-off CPU percentage that is used to scale up or down the
    cluster size), `min` (the minimum number of pods to keep), and `max` (the maximum
    number of pods to spin up). To summarize, the example command will run the service
    using 1 to 10 pods, depending on the volume of the traffic, keeping the CPU usage
    at 60%.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，`kubectl autoscale` 命令接受 YAML 文件的 `Deployment` 部分中指定的应用程序名称，`cpu-percent`（用于调整集群大小的
    CPU 百分比阈值），`min`（要保留的最小 Pod 数），以及 `max`（要启动的最大 Pod 数）。总结一下，示例命令将根据流量量在 1 到 10
    个 Pod 中运行服务，保持 CPU 使用率在 60%。
- en: Things to remember
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事情
- en: a. EKS is designed to provide Kubernetes clusters for application deployment
    by simplifying the complex cluster management for dynamic traffic.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: a. EKS 旨在通过简化动态流量的复杂集群管理，为应用程序部署提供 Kubernetes 集群。
- en: b. A YAML file is used to configure both the machines that make up the Kubernetes
    cluster and the application running within the cluster. The two parts of the configuration,
    `Deployment` and `Service`, control the application running within the pod and
    configure the service for the underlying target cluster, respectively.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: b. 使用 YAML 文件配置组成 Kubernetes 集群的机器和集群内运行的应用程序。配置的两个部分，`Deployment` 和 `Service`，分别控制
    Pod 内运行的应用程序，并为底层目标集群配置服务。
- en: c. It is possible to create and host inference endpoints using TF and PyTorch
    models on an EKS cluster.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: c. 可以使用 TF 和 PyTorch 模型在 EKS 集群上创建和托管推断端点。
- en: d. By exploiting the EI accelerator with a model compiled using AWS Neuron,
    it is possible to improve the inference latency while saving the operating cost
    of the EKS cluster.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: d. 利用使用 AWS Neuron 编译的模型来利用 EI 加速器，可以提高推断延迟，同时节省 EKS 集群的运营成本。
- en: b. An EKS cluster can be configured to resize itself dynamically based on the
    volume of the traffic.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: b. 可以配置 EKS 集群根据流量量动态调整大小。
- en: 'In this section, we discussed EKS-based DL model deployment for TF and PyTorch
    models. We described how the AWS Neuron model and the EI accelerator can be used
    to improve service performance. Finally, we covered autoscaling to utilize the
    available resources more effectively. In the next section, we will look at another
    AWS service for hosting inference endpoints: SageMaker.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们讨论了基于EKS的TF和PyTorch模型部署。我们描述了如何使用AWS Neuron模型和EI加速器来提高服务性能。最后，我们介绍了自动扩展以更有效地利用可用资源。在下一部分中，我们将看看另一个AWS服务用于托管推理端点：SageMaker。
- en: Inferencing using SageMaker
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker进行推断
- en: In this section, you will learn how to create an endpoint using SageMaker instead
    of the EKS cluster. First, we will describe framework-independent ways of creating
    inference endpoints (the `Model` class). Then, we will look at creating TF endpoints
    using `TensorFlowModel` and the TF-specific `Estimator` class. The next section
    will focus on endpoint creation for PyTorch models using the `PyTorchModel` class
    and the PyTorch-specific `Estimator` class. Furthermore, we will introduce how
    to build an endpoint from an ONNX model. At this point, we should have a service
    running model prediction for incoming requests. After that, we will describe how
    to improve the quality of a service using *AWS SageMaker Neo* and the EI accelerator.
    Finally, we will cover autoscaling and describe how to host multiple models on
    a single endpoint.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何使用SageMaker而不是EKS集群创建端点。首先，我们将描述创建推理端点的与框架无关的方法（即`Model`类）。然后，我们将查看使用`TensorFlowModel`和TF特定的`Estimator`类创建TF端点的方法。接下来的部分将重点介绍使用`PyTorchModel`类和PyTorch特定的`Estimator`类创建PyTorch模型的端点。此外，我们还将介绍如何从ONNX模型构建端点。到此为止，我们应该有一个正在为传入请求运行模型预测的服务。之后，我们将描述如何使用*AWS
    SageMaker Neo*和EI加速器来提高服务质量。最后，我们将介绍自动扩展并描述如何在单个端点上托管多个模型。
- en: As described in the *Utilizing SageMaker for ETL* section in [*Chapter 5*](B18522_05.xhtml#_idTextAnchor106),
    *Data Preparation in the Cloud*, SageMaker provides a built-in notebook environment
    called SageMaker Studio. The code snippets we have included in this section are
    meant to be executed in this notebook.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如在《第5章》的*在云中利用SageMaker进行ETL*部分中所述，SageMaker提供了一个名为SageMaker Studio的内置笔记本环境。我们在本节中包含的代码片段是为在这个笔记本中执行而准备的。
- en: Setting up an inference endpoint using the Model class
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Model类设置推理端点
- en: In general, SageMaker provides three different classes for endpoint creation.
    The most basic one is the `Model` class, which supports models from various DL
    frameworks. The other option is to use a framework-specific `Model` class. The
    last option is to use the `Estimator` class. In this section, we will look at
    the first option, which is the `Model` class.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，SageMaker提供三种不同的类来创建端点。最基本的是`Model`类，支持各种DL框架的模型。另一个选项是使用特定于框架的`Model`类。最后一个选项是使用`Estimator`类。在本节中，我们将看看第一种选项，即`Model`类。
- en: 'Before we dive into the endpoint creation process, we need to make sure the
    necessary components have been prepared appropriately; the right IAM role must
    be configured for SageMaker, and the trained model should be available on S3\.
    The IAM role can be prepared in the notebook as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入端点创建过程之前，我们需要确保已经适当地准备了必要的组件；为SageMaker配置了正确的IAM角色，并且训练好的模型应该在S3上可用。IAM角色可以在笔记本中如下准备：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the preceding code, the IAM access role and default bucket have been set
    up. To load the current IAM role of the SageMaker notebook, you can use the `sagemaker.get_execution_role`
    function. To create a SageMaker session, you need to create an instance for the
    `Session` class. The `default_bucket` method of the `Session` instance will create
    a default bucket with its name in `sagemaker-{region}-{aws-account-id}` format.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，已设置了IAM访问角色和默认存储桶。要加载SageMaker笔记本的当前IAM角色，您可以使用`sagemaker.get_execution_role`函数。要创建一个SageMaker会话，您需要为`Session`类创建一个实例。`Session`实例的`default_bucket`方法将创建一个以`sagemaker-{region}-{aws-account-id}`格式命名的默认存储桶。
- en: 'Before uploading the model to an S3 bucket, the model needs to be compressed
    as a `.tar` file. The following code snippet describes how to compress the model
    and upload the compressed model to the target bucket within the notebook:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型上传到S3存储桶之前，模型需要被压缩为`.tar`文件。以下代码片段描述了如何压缩模型并将压缩后的模型上传到笔记本中的目标存储桶：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code snippet, the compression is performed using the `tarfile`
    library. The `upload_data` method of the `Session` instance is used to upload
    the compiled model to the S3 bucket linked with the SageMaker session.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，使用`tarfile`库执行压缩。`Session`实例的`upload_data`方法用于将编译后的模型上传到与SageMaker会话链接的S3存储桶。
- en: 'Now, we are ready to create an instance of the `Model` class. In this particular
    example, we will assume that the model has been trained with TF:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备创建一个`Model`类的实例。在这个特定的示例中，我们将假设该模型已经使用TF训练完成：
- en: '[PRE19]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As shown in the preceding code, the constructor of the `Model` class takes in
    `model_data` (the S3 path where the compressed model file is located), `framework_version`
    (a version of TF), and `role` (the IAM role for the notebook). The `deploy` method
    of the `Model` instance handles the actual endpoint creation. It takes in `initial_instance_count`
    (the number of instances to start the endpoint with) and `instance_type` (the
    EC2 instance type to use).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，`Model`类的构造函数接受`model_data`（压缩模型文件位于的S3路径）、`framework_version`（TF的版本）和`role`（笔记本的IAM角色）。`Model`实例的`deploy`方法处理实际的端点创建。它接受`initial_instance_count`（启动端点的实例数）和`instance_type`（要使用的EC2实例类型）。
- en: Additionally, you can provide a defined `image` and drop `framework_version`.
    In this case, the endpoint will be created with the Docker image specified for
    the `image` parameter. It should be pointing at an image on ECR.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您可以提供一个指定的`image`和删除`framework_version`。在这种情况下，端点将使用为`image`参数指定的Docker镜像创建。它应指向ECR上的一个镜像。
- en: 'Next, we will discuss how to trigger a model inference from the notebook using
    the created endpoint. The `deploy` method will return a `Predictor` instance.
    As shown in the following code snippet, you can achieve this through the `predict`
    function of the `Predictor` instance. All you need to pass to this function is
    some JSON data representing the input:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何使用创建的端点从笔记本触发模型推理。`deploy`方法将返回一个`Predictor`实例。如下代码片段所示，您可以通过`Predictor`实例的`predict`函数实现这一点。您只需传递表示输入的一些JSON数据：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output of the `predict` function, `results`, consists of JSON data that,
    in our example, looks as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`函数的输出`results`是JSON数据，我们的示例中如下所示：'
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `predict` function supports data of different formats such as JSON, CSV,
    and multidimensional array. If you need to use a type other than JSON, you can
    refer to [https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`函数支持不同格式的数据，例如JSON、CSV和多维数组。如果您需要使用除JSON以外的类型，请参考[https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#tensorflow-serving-input-and-output)。'
- en: 'Another option for triggering model inference is to use the `SageMaker.Client`
    class from the `boto3` library. The `SageMaker.Client` class is a low-level client
    representing Amazon SageMaker Service. In the following code snippet, we are creating
    an instance of `SageMaker.Client` and demonstrating how to access the endpoint
    using the `invoke_endpoint` method:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 触发模型推理的另一种选项是使用`boto3`库中的`SageMaker.Client`类。`SageMaker.Client`类是代表Amazon SageMaker服务的低级客户端。在以下代码片段中，我们正在创建一个`SageMaker.Client`实例，并演示如何使用`invoke_endpoint`方法访问端点：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As shown in the preceding code snippet, the `invoke_endpoint` method takes in
    `EndpointName` (the name of the endpoint; that is, `run_model_prediction`), `ContentType`
    (the type of the input data; that is, `"text/csv"`), and `Body` (the input data
    for model prediction; that is, `payload`).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，`invoke_endpoint`方法接受`EndpointName`（端点名称；即`run_model_prediction`）、`ContentType`（输入数据类型；即`"text/csv"`）和`Body`（用于模型预测的输入数据；即`payload`）。
- en: In reality, many companies utilize Amazon API Gateway ([https://aws.amazon.com/api-gateway](https://aws.amazon.com/api-gateway))
    and AWS Lambda ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda))
    along with SageMaker endpoints, to communicate with the deployed model in a serverless
    architecture. For the detailed setup, please refer to [https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多公司利用 Amazon API Gateway ([https://aws.amazon.com/api-gateway](https://aws.amazon.com/api-gateway))
    和 AWS Lambda ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda))
    与 SageMaker 端点一起在无服务器架构中通信，以与部署的模型进行交互。有关详细设置，请参阅 [https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda)。
- en: Next, we will explain framework-specific approaches to creating an endpoint.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将解释创建端点的框架特定方法。
- en: Setting up a TensorFlow inference endpoint
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 设置推断端点
- en: In this section, we will describe a `Model` class designed specifically for
    TF – the `TensorFlowModel` class. Then, we will explain how to use the TF-specific
    `Estimator` class for endpoint creation. The complete versions of the code snippets
    in this section can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述专为 TF 设计的 `Model` 类 - `TensorFlowModel` 类。然后，我们将解释如何使用 TF 特定的 `Estimator`
    类创建端点。本节代码片段的完整版本可以在 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_9/sagemaker)
    找到。
- en: Setting up a TensorFlow inference endpoint using the TensorFlowModel class
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `TensorFlowModel` 类设置 TensorFlow 推断端点
- en: 'The `TensorFlowModel` class is a `Model` class that is designed for TF models.
    As shown in the following code snippet, the class can be imported from the `sagemaker.tensorflow`
    module and its usage is identical to the `Model` class:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorFlowModel` 类是为 TF 模型设计的 `Model` 类。如下代码片段所示，该类可以从 `sagemaker.tensorflow`
    模块导入，并且其使用方式与 `Model` 类相同：'
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The constructor of the `TensorFlowModel` class takes in the same parameters
    as the constructor of the `Model` class: the S3 path of the uploaded model (`model_s3_path`),
    the TF framework version (`Tf_framework_version`), and the IAM role for SageMaker
    (`role`). In addition, you can provide a Python script for pre- and post-processing
    the input and output of the model inference by providing `entry_point`. In this
    case, the script needs to be named `inference.py`. For more details, please refer
    to [https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorFlowModel` 类的构造函数接受与 `Model` 类相同的参数：上传模型的 S3 路径 (`model_s3_path`)、TF
    框架版本 (`Tf_framework_version`) 和 SageMaker 的 IAM 角色 (`role`)。此外，您可以通过提供 `entry_point`
    来提供用于模型推断输入和输出的预处理和后处理的 Python 脚本。在这种情况下，脚本需要命名为 `inference.py`。更多详情，请参阅 [https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html#providing-python-scripts-for-pre-post-processing)。'
- en: Being a child class of `Model`, `TensorFlowModel` also provides a `Predictor`
    instance through the `deploy` method. Its usage is identical to what we described
    in the preceding section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorFlowModel` 类作为 `Model` 的子类，通过 `deploy` 方法也提供了一个 `Predictor` 实例。其使用方式与我们在前一节中描述的相同。'
- en: Next, you will learn how to deploy your model using the `Estimator` class, which
    we have already introduced for the model training on SageMaker in [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133),
    *Efficient Model Training*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习如何使用 `Estimator` 类部署您的模型，我们已经在[*第 6 章*](B18522_06.xhtml#_idTextAnchor133)，*高效模型训练*
    中介绍了这一类用于 SageMaker 模型训练。
- en: Setting up a TensorFlow inference endpoint using the Estimator class
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Estimator 类设置 TensorFlow 推断端点
- en: 'As introduced in the *Training a TensorFlow model using SageMaker* section
    of [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133), *Efficient Model Training*,
    SageMaker provides the `Estimator` class, which supports model training on SageMaker.
    The same class can be used to create and deploy an inference endpoint. In the
    following code snippet, we are making use of the `Estimator` class that’s been
    designed for TF, `sagemaker.tensorflow.estimator.TensorFlow`, to train a TF model
    and deploy an endpoint using a trained model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[*Chapter 6*](B18522_06.xhtml#_idTextAnchor133)，*Efficient Model Training*的*Training
    a TensorFlow model using SageMaker*部分介绍的那样，SageMaker提供了支持在SageMaker上进行模型训练的`Estimator`类。同一类也可用于创建和部署推断端点。在下面的代码片段中，我们使用了为TF设计的`Estimator`类，即`sagemaker.tensorflow.estimator.TensorFlow`，来训练一个TF模型，并使用训练后的模型部署一个端点：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding code snippet, the `sagemaker.tensorflow.estimator.TensorFlow`
    class takes in the following parameters: `entry_point` (the script that handles
    the training; that is, `"tf-train.py"`), `instance_count` (the number of instances
    to use; that is, `1`), `instance_type` (the type of the instance; that is, `"ml.c4.xlarge"`),
    `framework_version` (a PyTorch version; that is, `"2.2"`), and `py_version` (a
    Python version; that is, `"py37"`). The `fit` method of the `Estimator` instance
    performs the model training. The key method for creating and deploying an endpoint
    is the `deploy` method, which creates and hosts an endpoint for the model it trained
    based on the conditions provided: the `initial_instance_count` (`1`) instances
    of `instance_type` (`"ml.c5.xlarge"`). The `deploy` method of the `Estimator`
    class returns a `Predictor` instance as in the case of the `Model` class.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，`sagemaker.tensorflow.estimator.TensorFlow`类接受以下参数：`entry_point`（处理训练的脚本，即`"tf-train.py"`）、`instance_count`（使用的实例数，即`1`）、`instance_type`（实例的类型，即`"ml.c4.xlarge"`）、`framework_version`（PyTorch的版本，即`"2.2"`）和`py_version`（Python的版本，即`"py37"`）。`Estimator`实例的`fit`方法执行模型训练。用于创建和部署端点的关键方法是`deploy`方法，它基于提供的条件创建和托管一个端点：`initial_instance_count`（`1`）实例，`instance_type`（`"ml.c5.xlarge"`）。`Estimator`类的`deploy`方法返回一个`Predictor`实例，就像`Model`类的情况一样。
- en: In this section, we explained how to create an endpoint for a TF model on SageMaker.
    In the next section, we will look at how SageMaker supports PyTorch models.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们解释了如何在SageMaker上为TF模型创建端点。在下一节中，我们将探讨SageMaker如何支持PyTorch模型。
- en: Setting up a PyTorch inference endpoint
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置一个PyTorch推断端点
- en: 'This section is designed to cover different ways of creating and hosting an
    endpoint from a PyTorch model on SageMaker. First, we will introduce a `Model`
    class designed for PyTorch models: the `PyTorchModel` class. Then, we will describe
    an `Estimator` class for the PyTorch model. The complete implementations for the
    code snippets in this section can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节旨在介绍在SageMaker上创建和托管PyTorch模型端点的不同方法。首先，我们将介绍为PyTorch模型设计的`Model`类：`PyTorchModel`类。接着，我们将描述PyTorch模型的`Estimator`类。本节中代码片段的完整实现可以在[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_9/sagemaker/pytorch-inference.ipynb)找到。
- en: Setting up a PyTorch inference endpoint using the PyTorchModel class
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用`PyTorchModel`类设置PyTorch推断端点
- en: 'Similar to the `TensorFlowModel` class, there exists a `Model` class designed
    specifically for a PyTorch model, `PyTorchModel`. It can be instantiated as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`TensorFlowModel`类，专门为PyTorch模型设计了一个`Model`类，即`PyTorchModel`。可以按以下方式实例化它：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As shown in the preceding code snippet, the constructor takes in `entry_point`,
    which defines custom pre- and post-processing logic for the data, `source_dir`
    (the S3 path of the entry point script), `role` (the IAM role for SageMaker),
    `model_data` (the S3 path of the model), `framework_version` (the version of PyTorch),
    and `py_version` (the version of Python).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，构造函数接受`entry_point`（定义数据的自定义前后处理逻辑）、`source_dir`（入口点脚本的S3路径）、`role`（SageMaker的IAM角色）、`model_data`（模型的S3路径）、`framework_version`（PyTorch的版本）和`py_version`（Python的版本）作为参数。
- en: Since the `PyTorchModel` class inherits the `Model` class, it provides the `deploy`
    function, which creates and deploys an endpoint, as described in the *Setting
    up a PyTorch inference endpoint using the Model class* section.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`PyTorchModel`类继承自`Model`类，它提供了`deploy`函数，用于创建和部署端点，如*使用 Model 类设置 PyTorch
    推断端点*章节中所述。
- en: Next, we will introduce an `Estimator` class designed for PyTorch models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍专为 PyTorch 模型设计的`Estimator`类。
- en: Setting up a PyTorch inference endpoint using the Estimator class
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Estimator 类设置 PyTorch 推断端点
- en: 'If a trained PyTorch model is not available, the `sagemaker.pytorch.estimator.PyTorch`
    class can be used to train and deploy a model. The training can be achieved with
    the `fit` method, as described in the *Training a PyTorch model using SageMaker*
    section of [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133), *Efficient Model Training*.
    Being an `Estimator` class, the `sagemaker.pytorch.estimator.PyTorch` class provides
    the same features as `sagemaker.tensorflow.estimator.TensorFlow`, which we covered
    in the *Setting up a TensorFlow inference endpoint using the Estimator class*
    section. In the following code snippet, we are creating an `Estimator` instance
    for a PyTorch model, training the model, and creating an endpoint:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有训练好的 PyTorch 模型可用，可以使用`sagemaker.pytorch.estimator.PyTorch`类来训练和部署模型。训练可以通过`fit`方法来完成，如[*Chapter
    6*](B18522_06.xhtml#_idTextAnchor133)章节*使用 SageMaker 训练 PyTorch 模型*中所述，*高效的模型训练*。作为`Estimator`类，`sagemaker.pytorch.estimator.PyTorch`类提供了与`TensorFlow`中的`Estimator`类相同的功能，我们在*使用
    Estimator 类设置 TensorFlow 推断端点*章节中有所涉及。在下面的代码片段中，我们正在为 PyTorch 模型创建一个`Estimator`实例，训练模型，并创建一个端点：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As shown in the preceding code snippet, the constructor of `sagemaker.pytorch.estimator.PyTorch`
    takes in the same set of parameters as the `Estimator` class designed for TF:
    `entry_point` (the script that handles the training; that is, `"pytorch-train.py"`),
    `instance_count` (the number of instances to use; that is, `1`), `instance_type`
    (the type of the EC2 instance; that is, `"ml.c4.xlarge"`), `framework_version`
    (the PyTorch version; that is, `"1.11.0"`), and `py_version` (the Python version;
    that is, `"py37"`). The model training (the `fit` method) and deployment (the
    `deploy` method) are achieved the same way as in the previous example in the *Setting
    up a TensorFlow inference endpoint using the Estimator class* section.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，`sagemaker.pytorch.estimator.PyTorch`的构造函数接受与为 TF 设计的`Estimator`类相同的参数集：`entry_point`（处理训练的脚本；即`"pytorch-train.py"`）、`instance_count`（要使用的实例数量；即`1`）、`instance_type`（EC2
    实例的类型；即`"ml.c4.xlarge"`）、`framework_version`（PyTorch 版本；即`"1.11.0"`）和`py_version`（Python
    版本；即`"py37"`）。模型训练（`fit`方法）和部署（`deploy`方法）的方式与*使用 Estimator 类设置 TensorFlow 推断端点*章节中的前例相同。
- en: 'In this section, we covered how to deploy a PyTorch model in two different
    ways: using the `PyTorchModel` class and using the `Estimator` class. Next, we
    will learn how to create an endpoint for an ONNX model on SageMaker.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何以两种不同的方式部署 PyTorch 模型：使用`PyTorchModel`类和`Estimator`类。接下来，我们将学习如何在
    SageMaker 上为 ONNX 模型创建端点。
- en: Setting up an inference endpoint from an ONNX model
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立一个来自 ONNX 模型的推断端点
- en: As mentioned in the previous chapter, [*Chapter 8*](B18522_08.xhtml#_idTextAnchor175),
    *Simplifying Deep Learning Model Deployment*, DL models are often transformed
    into **open neural network exchange** (**ONNX**) models for deployment. In this
    section, we will describe how to deploy an ONNX model on SageMaker.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章节所述，[*Chapter 8*](B18522_08.xhtml#_idTextAnchor175)章节*简化深度学习模型部署*中，DL 模型通常会转换为**开放神经网络交换**（**ONNX**）模型进行部署。在本节中，我们将描述如何在
    SageMaker 上部署 ONNX 模型。
- en: 'The most standard approach is to use the base `Model` class. As mentioned in
    the *Setting up a TensorFlow inference endpoint using the Model class* section,
    the `Model` class supports DL models of various types. Fortunately, it provides
    built-in support for ONNX models as well:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最标准的方法是使用基础的`Model`类。如*使用 Model 类设置 TensorFlow 推断端点*章节中所述，`Model`类支持各种类型的 DL
    模型。幸运的是，它也为 ONNX 模型提供了内置支持：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding example, we have a trained ONNX model on S3\. The key in the
    `Model` instance creation comes from `framework="onnx"`. We also need to provide
    an ONNX framework version to `framework_version`. In this example, we are using
    the ONNX framework version 1.4.0\. Everything else is almost identical to the
    previous examples. Again, the `deploy` function is designed for creating and deploying
    an endpoint; a `Predictor` instance will be returned for model prediction.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们在 S3 上有一个训练好的 ONNX 模型。在 `Model` 实例创建中，关键部分来自于 `framework="onnx"`。我们还需要为
    `framework_version` 提供 ONNX 框架版本。在本例中，我们使用的是 ONNX 框架版本 1.4.0。其他所有内容几乎与之前的示例完全相同。再次强调，`deploy`
    函数用于创建和部署端点；将返回一个 `Predictor` 实例用于模型预测。
- en: 'It is also common to use the `TensorFlowModel` and `PyTorchModel` classes for
    creating an endpoint from an ONNX model. The following code snippet demonstrates
    such use cases:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 常见做法还包括使用 `TensorFlowModel` 和 `PyTorchModel` 类来从 ONNX 模型创建端点。以下代码片段展示了这些用例：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code snippets are self-explanatory. Both classes take in a ONNX
    model path (`model_data`), an inference script (`entry_point`), an IAM role (`role`),
    a Python version (`py_version`), and versions for each framework (`framework_version`).
    Like how the `Model` class deploys an endpoint, the `deploy` method will create
    and host an endpoint from each model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段不言自明。这两个类都接受一个 ONNX 模型路径 (`model_data`)，一个推断脚本 (`entry_point`)，一个 IAM
    角色 (`role`)，一个 Python 版本 (`py_version`)，以及每个框架的版本 (`framework_version`)。与 `Model`
    类如何部署端点一样，`deploy` 方法将从每个模型创建并托管一个端点。
- en: While endpoints allow us to get the model predictions at any point in time for
    dynamic input data, there are cases where you need to perform inference on the
    whole input data stored on an S3 bucket instead of feeding each of them one by
    one. Therefore, we will look at how we can leverage Batch Transform for this requirement.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管端点允许我们随时获取动态输入数据的模型预测结果，但有些情况下，您需要对存储在 S3 存储桶中的整个输入数据进行推断，而不是逐个馈送它们。因此，我们将看看如何利用
    Batch Transform 来满足这一需求。
- en: Handling prediction requests in batches using Batch Transform
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Batch Transform 处理批量预测请求
- en: 'We can use the Batch Transform feature of SageMaker ([https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html))
    to run inference on a large dataset in one queue. Using the `sagemaker.transformer.Transformer`
    class, you can perform model prediction in batches for any dataset on S3 without
    a persistent endpoint. The details are included in the following code snippet:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用 SageMaker 的 Batch Transform 功能（[https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)）在一个队列中对大型数据集进行推断。使用
    `sagemaker.transformer.Transformer` 类，您可以在 S3 上对任何数据集进行批量模型预测，而无需持久化端点。具体细节包含在以下代码片段中：
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As shown in the preceding code, the `sagemaker.transformer.Transformer` class
    takes in `base_transformer_job_name` (a job name for the transformer job), `model_name`
    (the name of the model that holds the inference pipeline), `max_payload` (the
    maximum payload in MB allowed), `instance_count` (the number of EC2 instances
    to start with), `instance_type` (the type of EC2 instance), and `output_path`
    (an S3 path where the output will be stored). The `transformer` method will trigger
    the model prediction on the dataset specified. It takes in the following parameters:
    `input_location` (the S3 path where the input data is located), `content_type`
    (the content of the input data; that is, `"text/csv"`), and `split_type` (this
    controls how to split the input data; `"Line"` is used to feed each line of the
    data as an individual input to the model). In reality, many companies also utilize
    SageMaker processing jobs ([https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html))
    to perform batch inference, but we will not talk about this in detail.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如上述代码所示，`sagemaker.transformer.Transformer` 类接受 `base_transformer_job_name`（转换作业的作业名称）、`model_name`（保存推断流水线的模型名称）、`max_payload`（允许的最大有效载荷，以MB为单位）、`instance_count`（要启动的
    EC2 实例数）、`instance_type`（EC2 实例的类型）和 `output_path`（存储输出的 S3 路径）。`transformer`
    方法将触发指定数据集上的模型预测。它接受以下参数：`input_location`（输入数据所在的 S3 路径）、`content_type`（输入数据的内容类型；即
    `"text/csv"`）、以及 `split_type`（控制如何分割输入数据；使用 `"Line"` 将数据的每一行作为模型的单独输入）。实际上，许多公司还利用
    SageMaker 处理作业（[https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ProcessingJob.html)）执行批量推断，但我们不会详细讨论此事。
- en: So far, we have looked at how SageMaker supports hosting an inference endpoint
    for handling live prediction requests and running model predictions in batches
    for a static dataset available on S3\. In the next section, we will describe how
    to use **AWS SageMaker Neo** to further improve the inference latency of the deployed
    model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了 SageMaker 如何支持托管推断终端以处理实时预测请求，并对在 S3 上静态数据集进行模型预测。接下来，我们将描述如何使用
    **AWS SageMaker Neo** 进一步提高部署模型的推断延迟。
- en: Improving SageMaker endpoint performance using AWS SageMaker Neo
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升 SageMaker 终端性能，利用 AWS SageMaker Neo
- en: In this section, we will explain how SageMaker can further improve the performance
    of the application by exploiting the underlying hardware resources (EC2 instances
    or mobile devices). The idea is to compile the trained DL model using **AWS SageMaker
    Neo** ([https://aws.amazon.com/sagemaker/neo](https://aws.amazon.com/sagemaker/neo)).
    After the compilation, the generated Neo model can utilize the underlying device
    better, thus reducing the inference latency. AWS SageMaker Neo supports models
    of different frameworks (TF, PyTorch, MxNet, and ONNX) and various types of hardware
    (OS, chip, architecture, and accelerator). The complete list of supported resources
    can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释 SageMaker 如何通过利用底层硬件资源（EC2 实例或移动设备）进一步提升应用程序的性能。其思想是使用 **AWS SageMaker
    Neo** 编译经过训练的 DL 模型（[https://aws.amazon.com/sagemaker/neo](https://aws.amazon.com/sagemaker/neo)）。编译后，生成的
    Neo 模型可以更好地利用底层设备，从而降低推断延迟。AWS SageMaker Neo 支持不同框架的模型（TF、PyTorch、MxNet 和 ONNX），以及各种类型的硬件（操作系统、芯片、架构和加速器）。支持的完整资源列表详见
    [https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo-supported-devices-edge-devices.html)。
- en: 'Neo model generation can be achieved using the `compile` method of the `Model`
    class. The `compile` method returns an `Estimator` instance that supports endpoint
    creation. Let’s look at the following example for the details:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 可通过 `Model` 类的 `compile` 方法生成 Neo 模型。`compile` 方法返回一个支持终端创建的 `Estimator` 实例。让我们看下面的示例以获取详细信息：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In the preceding code, we start with a `Model` instance called `sm_model`.
    We trigger the `compile` method to compile the loaded model into a Neo model.
    The following list describes the parameters:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们从名为 `sm_model` 的 `Model` 实例开始。我们调用 `compile` 方法将加载的模型编译为 Neo 模型。以下是参数列表的描述：
- en: '`target_instance_family`: The EC2 instance type that the model will be optimized
    for'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_instance_family`：优化模型的 EC2 实例类型'
- en: '`input_shape`: The input data shape'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape`：输入数据的形状'
- en: '`job_name`: The name of the compilation job'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`job_name`：编译作业的名称'
- en: '`role`: The IAM role of the compiled model output'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`role`：编译模型输出的IAM角色'
- en: '`framework`: A DL framework such as TF or PyTorch'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework`：如TF或PyTorch的DL框架'
- en: '`framework_version`: The version of the framework to use'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`framework_version`：要使用的框架版本'
- en: '`output_path`: The output S3 path where the compiled model will be stored'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_path`：编译模型将存储的输出S3路径'
- en: The `Estimator` instance consists of a `deploy` function that creates the endpoint.
    The output is a `Predictor` instance that you can use to run the model prediction.
    In the preceding example, we optimized our model to perform the best on instances
    of the `ml_c5` type.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`Estimator`实例包括一个`deploy`函数，用于创建端点。输出是一个`Predictor`实例，您可以使用它来运行模型预测。在上述示例中，我们优化了我们的模型，以在`ml_c5`类型的实例上表现最佳。'
- en: Next, we will describe how to integrate the EI accelerator into the endpoints
    running on SageMaker.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将描述如何将EI加速器集成到运行在SageMaker上的端点中。
- en: Improving SageMaker endpoint performance using Amazon Elastic Inference
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon Elastic Inference改善SageMaker端点性能
- en: In the *Improving EKS endpoint performance using Amazon Elastic Inference* section,
    we described how an EI accelerator can reduce the operating cost for an inference
    endpoint while improving the inference latency by exploiting the available GPU
    devices. In this section, we will cover EI accelerator integration for SageMaker.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用Amazon Elastic Inference改善EKS端点性能*部分，我们描述了如何利用EI加速器来降低推断端点的运行成本，同时通过利用可用的GPU设备来提高推断延迟。在本节中，我们将涵盖SageMaker的EI加速器集成。
- en: 'The necessary change is fairly simple; you just need to provide `accelerator_type`
    when triggering the `deploy` method of a `Model` instance:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 必要的更改非常简单；您只需在触发`Model`实例的`deploy`方法时提供`accelerator_type`即可：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code, the `deploy` method creates an endpoint for the given
    `Model` instance. To attach an EI accelerator to the endpoint, you need to specify
    the type of accelerator you want (`accelerator_type`) on top of the default parameters
    (`initial_instance_count` and `instance_type`). For the complete description of
    using EI for the SageMaker endpoint, please look at [https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`deploy`方法为给定的`Model`实例创建端点。要将EI加速器附加到端点上，您需要在默认参数（`initial_instance_count`和`instance_type`）之上指定所需的加速器类型（`accelerator_type`）。有关在SageMaker端点中使用EI的完整说明，请参阅[https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html)。
- en: In the following section, we will look at the autoscaling feature of SageMaker,
    which allows us to handle the changes in the incoming traffic better.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨SageMaker的自动扩展功能，这使我们能够更好地处理传入流量的变化。
- en: Resizing SageMaker endpoints dynamically using autoscaling
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动扩展动态调整SageMaker端点的大小
- en: 'Similar to how the EKS cluster supports autoscaling to automatically scale
    up or down the endpoints based on the changes in the traffic, SageMaker also provides
    the autoscaling feature. Configuring autoscaling involves configuring the scaling
    policy, which defines when the scaling takes place and how many resources are
    created and destroyed at the time of scaling. The scaling policy for the SageMaker
    endpoint can be configured from the SageMaker web console. The following steps
    describe how you can configure autoscaling for the inference endpoints created
    from a SageMaker notebook:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于EKS集群支持自动扩展根据流量变化自动调整端点的大小，SageMaker也提供了自动扩展功能。配置自动扩展涉及配置扩展策略，该策略定义了何时进行扩展以及在扩展时创建和销毁多少资源。可以从SageMaker
    Web控制台配置SageMaker端点的扩展策略。以下步骤描述了如何在从SageMaker笔记本创建的推断端点上配置自动扩展：
- en: Visit the SageMaker web console, [https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/),
    and click **Endpoints** under **Inference** in the navigation panel on the left-hand
    side. You may need to provide your credentials to log in.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问SageMaker Web控制台，[https://console.aws.amazon.com/sagemaker/](https://console.aws.amazon.com/sagemaker/)，并在左侧导航面板中的**推断**下点击**端点**。您可能需要提供凭据以登录。
- en: Next, you must choose the endpoint name you want to configure. Under the **Endpoint
    runtime** settings, choose the model variant that requires the configuration.
    This feature allows you to deploy multiple versions of a model in a single endpoint,
    spinning up one container per version. The details on this feature can be found
    at [https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html).
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您必须选择要配置的端点名称。在**端点运行时**设置下，选择需要配置的模型变体。此功能允许您在单个端点中部署多个模型版本，每个版本都会启动一个容器。有关此功能的详细信息，请参见
    [https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_runtime_InvokeEndpoint.html)。
- en: 'Under the **Endpoint runtime** settings, select **Configure auto scaling**.
    This will take you to the **Configure variant automatic scaling** page:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**端点运行时**设置下，选择**配置自动扩展**。这将带您进入**配置变体自动扩展**页面：
- en: '![Figure 9.1 – The Configure variant automatic scaling page of the SageMaker
    web console'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – SageMaker Web 控制台的配置变体自动扩展页面'
- en: '](img/B18522_09_01.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_09_01.jpg)'
- en: Figure 9.1 – The Configure variant automatic scaling page of the SageMaker web
    console
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – SageMaker Web 控制台的配置变体自动扩展页面
- en: Type the minimum number of instances to maintain in the **Minimum instance count**
    field. The minimum value is 1\. This value defines the minimum instance number
    that will be kept at all times.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**最小实例数**字段中输入要维护的最小实例数。最小值为 1。此值定义将始终保留的最小实例数量。
- en: Type the maximum number of instances of the scaling policy to maintain in the
    **Maximum instance count** field. This value defines the maximum number of instances
    allowed at peak traffic.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**最大实例数**字段中输入要维护的扩展策略的最大实例数。此值定义了在峰值流量时允许的最大实例数量。
- en: Fill in the **SageMakerVariantInvocationsPerInstance** field. Each endpoint
    can have multiple models (or model versions) deployed in a single endpoint hosted
    across one or more EC2 instances. **SageMakerVariantInvocationsPerInstance** defines
    the maximum number of invocations allowed per minute for each model variant. This
    value is used for load balancing. Details on calculating the right number for
    this field can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写**SageMakerVariantInvocationsPerInstance**字段。每个端点可以在一个或多个 EC2 实例上托管的单个端点中部署多个模型（或模型版本）。**SageMakerVariantInvocationsPerInstance**定义了每个模型变体每分钟允许的最大调用次数。此值用于负载均衡。有关计算此字段正确数量的详细信息，请参见
    [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html)。
- en: Fill in the scale-in cooldown and scale-out cooldown. These indicate how long
    SageMaker will wait before it checks for another round of scaling.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写缩放缓冲时间和扩展缓冲时间。这些指示 SageMaker 在检查下一轮缩放之前等待多长时间。
- en: Select the **Disable scale in** checkbox. During an increase in traffic, more
    instances are started as part of the scale-out process. But these instances can
    be quickly deleted during the scale-in process if the traffic slows down right
    after the increase. To avoid a newly created instance from being released as soon
    as it gets created, this checkbox must be selected.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**禁用缩放**复选框。在流量增加时，作为扩展过程的一部分会启动更多实例。但是，如果在增加后立即减少流量，这些实例可能会在缩小过程中被快速删除。为避免新创建的实例在创建后立即释放，必须选择此复选框。
- en: Click the **Save** button to apply the configuration.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击**保存**按钮以应用配置。
- en: The scaling will be applied to the selected model variant as soon as you click
    the **Save** button. SageMaker will increase and decrease the number of instances
    based on the incoming traffic. For more details on auto-scaling, please take a
    look at [https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦单击**保存**按钮，将会将扩展应用于所选的模型变体。SageMaker 将根据传入的流量增加和减少实例数量。有关自动扩展的更多详细信息，请参阅 [https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-instance-termination.html)。
- en: As the last topic for SageMaker-based endpoints, we will describe how to deploy
    multiple models through a single endpoint.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基于 SageMaker 的端点的最后一个主题，我们将描述如何通过单个端点部署多个模型。
- en: Hosting multiple models on a single SageMaker inference endpoint
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在单个SageMaker推断端点上托管多个模型
- en: SageMaker supports deploying multiple models on a single endpoint through **Multimodal
    Endpoints** (**MME**). There are a couple of things you must keep in mind before
    setting up MME. First, it’s recommended to set up multiple endpoints if you want
    to keep the low latency. Second, the container can only deploy models from the
    same DL framework. For those who are interested in hosting models from different
    frameworks, we recommend reading [https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html](https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html).
    MEE works best when the models are similar in size and expected to perform with
    similar latencies.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 支持通过 **多模态端点** (**MME**) 部署多个模型在一个端点上。在设置 MME 之前，有几件事情需要牢记。首先，建议如果希望保持低延迟，则设置多个端点。其次，容器只能部署来自同一
    DL 框架的模型。对于希望托管来自不同框架的模型的用户，建议阅读 [https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html](https://docs.amazonaws.cn/en_us/sagemaker/latest/dg/multi-container-direct.html)。当模型大小和预期表现相似时，MME
    的效果最佳。
- en: 'The following steps describe how to set up MME:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了如何设置 MME：
- en: Visit the SageMaker web console at [https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker)
    with your AWS credentials.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的 AWS 凭证访问 SageMaker Web 控制台 [https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker)。
- en: Choose **Models** under the **Inference** section of the left navigation panel.
    Then, click the **Create Model** button at the top right.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航面板的 **推理** 部分下选择 **模型**。然后，点击右上角的 **创建模型** 按钮。
- en: Enter a value for the **Model Name** field. This will be used to uniquely identify
    the target model in the context of SageMaker.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 **模型名称** 字段输入一个值。这将用于在 SageMaker 上下文中唯一标识目标模型。
- en: Choose an IAM role with the **AmazonSageMakerFullAccess** IAM policy.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个具有 **AmazonSageMakerFullAccess** IAM 策略的 IAM 角色。
- en: 'Under the **Container definition** section, choose the **Multiple models**
    option and provide the location of the inference code image and the location of
    the model artifacts (see *Figure 9.2*):'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**容器定义**部分，选择**多个模型**选项，并提供推理代码镜像的位置以及模型工件的位置（参见 *图 9.2*）：
- en: '![Figure 9.2 – The Multi-modal endpoint configuration page of the SageMaker
    web console'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – SageMaker Web 控制台的多模态端点配置页面'
- en: '](img/B18522_09_02.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_09_02.jpg)'
- en: Figure 9.2 – The Multi-modal endpoint configuration page of the SageMaker web
    console
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – SageMaker Web 控制台的多模态端点配置页面
- en: The former field is used to deploy your models with a custom Docker image ([https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)).
    In this field, you should provide the image registry path where the images are
    located within Amazon ECR. The latter field specifies the S3 path where the model
    artifacts reside.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个字段用于使用自定义 Docker 镜像部署您的模型（[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html)）。在此字段中，您应提供镜像注册路径，其中镜像位于
    Amazon ECR 内。后一个字段指定了模型工件所在的 S3 路径。
- en: Additionally, fill in the **Container host name** field. This specifies details
    about the host where the inference code image will be created.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，填写 **容器主机名** 字段。这指定了推理代码镜像将被创建的主机的详细信息。
- en: Choose the **Create Model** button at the end.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后选择 **创建模型** 按钮。
- en: 'Once SageMaker has been configured with MME, we can test the endpoint using
    `SageMaker.Client` from the `boto3` library as shown in the following code snippet:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 配置了 MME 后，可以使用 `boto3` 库中的 `SageMaker.Client` 来测试端点，如下面的代码片段所示：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code, the `invoke_endpoint` function of the `SageMaker.Client`
    instance sends a request to the created endpoint. The `invoke_endpoint` function
    takes in `EndpointName` (the name of the created endpoint), `ContentType` (the
    type of data in the request body), `TargetModel` (the compressed model file in
    `.tar.gz` format; this is used to specify the target model which the request will
    be invoking), and `Body` (the input data in `ContentType`). The `response` variable
    that’s returned from the call consists of the prediction results. For the complete
    description of communicating with the endpoints, please look at [https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html](https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`SageMaker.Client`实例的`invoke_endpoint`函数向创建的端点发送请求。`invoke_endpoint`函数接受`EndpointName`（创建的端点的名称）、`ContentType`（请求主体中的数据类型）、`TargetModel`（以`.tar.gz`格式压缩的模型文件；用于指定请求将调用的目标模型）和`Body`（`ContentType`中的输入数据）。从调用返回的`response`变量包含预测结果。有关与端点通信的完整描述，请参阅[https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html](https://docs.aws.amazon.com/sagemaker/latest/dg/invoke-multi-model-endpoint.html)。
- en: Things to remember
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 记住的事情
- en: 'a. SageMaker supports endpoint creation through its built-in `Model` class
    and the `Estimator` class. These classes support models that have been trained
    with various DL frameworks, including TF, PyTorch, and ONNX. `Model` classes designed
    specifically for TF and PyTorch frameworks also exist: `TensorFlowModel` and `PyTorchModel`.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: a. SageMaker通过其内置的`Model`类和`Estimator`类支持端点创建。这些类支持使用各种DL框架（包括TF、PyTorch和ONNX）训练的模型。专为TF和PyTorch框架设计的`Model`类也存在：`TensorFlowModel`和`PyTorchModel`。
- en: b. Once a model has been compiled using AWS SageMaker Neo, the model can exploit
    the underlying hardware resources better, demonstrating greater inference performance.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: b. 使用AWS SageMaker Neo编译模型后，模型可以更好地利用底层硬件资源，表现出更高的推理性能。
- en: c. SageMaker can be configured to use an EI accelerator, reducing the operating
    cost for inference endpoints while improving the inference latency.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: c. SageMaker可以配置为使用EI加速器，从而降低推理端点的运行成本并提高推理延迟。
- en: d. SageMaker includes an autoscaling feature that scales the endpoints up and
    down dynamically based on the volume of incoming traffic.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: d. SageMaker包含自动扩展功能，根据传入流量的量动态调整端点的规模。
- en: e. SageMaker supports deploying multiple models on a single endpoint through
    MME.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: e. SageMaker支持通过MME在单个端点上部署多个模型。
- en: Throughout this section, we have described various features that SageMaker provides
    for deploying a DL model as an inference endpoint.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了SageMaker为部署DL模型作为推理端点提供的各种功能。
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we described the two most popular AWS services designed for
    deploying a DL model as an inference endpoint: EKS and SageMaker. For both options,
    we started with the simplest setting: creating an inference endpoint from TF,
    PyTorch, or ONNX models. Then, we explained how to improve the performance of
    an inference endpoint using the EI accelerator, AWS Neuron, and AWS SageMaker
    Neo. We also covered how to set up autoscaling to handle the changes in the traffic
    more effectively. Finally, we discussed the MME feature of SageMaker that is used
    to host multiple models on a single inference endpoint.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了两种最受欢迎的AWS服务，专为将DL模型部署为推理端点设计：EKS和SageMaker。对于这两个选项，我们从最简单的设置开始：使用TF、PyTorch或ONNX模型创建推理端点。然后，我们解释了如何使用EI加速器、AWS
    Neuron和AWS SageMaker Neo来提高推理端点的性能。我们还介绍了如何设置自动扩展以更有效地处理流量变化。最后，我们讨论了SageMaker的MME功能，用于在单个推理端点上托管多个模型。
- en: 'In the next chapter, we will look at various model compression techniques:
    network quantization, weight sharing, network pruning, knowledge distillation,
    and network architecture search. These techniques will increase the inference
    efficiency even further.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨各种模型压缩技术：网络量化、权重共享、网络修剪、知识蒸馏和网络架构搜索。这些技术将进一步提高推理效率。
