- en: '*Chaper 2*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*'
- en: Building Blocks of Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的基本构建模块
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章结束时，您将能够：
- en: Identify the advantages and disadvantages of neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别神经网络的优缺点
- en: Distinguish between different components in the anatomy of neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分神经网络解剖学中的不同组成部分
- en: Recognize the most popular neural network architectures and understand what
    they are mainly used for
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别最流行的神经网络架构并了解它们主要用于什么
- en: Use techniques to prepare data to be fed into a neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用技术准备数据输入到神经网络中
- en: Solve a regression problem using a simple architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用简单的架构解决回归问题
- en: Improve the performance of a model by addressing high bias or high variance
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过解决高偏差或高方差来提高模型的性能
- en: In this chapter, we'll look at the basic building blocks of neural networks.
    We'll explore the different architectures to solve a wide variety of tasks. Finally,
    we'll learn how to build a neural network using PyTorch.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨神经网络的基本构建模块。我们将探索不同的架构来解决各种任务。最后，我们将学习如何使用PyTorch构建神经网络。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: 'Although neural network theory was developed several decades ago, and the concept
    evolved from the notion of the perceptron, different architectures have been created
    to solve different data problems in recent times. This is mainly due to the different
    data formats that can be found in real-life data problems, such as text, audio,
    and images. The purpose of this chapter is to introduce the topic of neural networks
    and their main advantages and disadvantages in order to better understand when
    and how to use them. Then, the chapter will move on to explain the building blocks
    of the most popular neural network architectures: **artificial neural networks**
    (**ANNs**), **convolutional neural networks** (**CNNs**), and **recurrent neural
    networks** (**RNNs**).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络理论几十年前就已经发展，概念起源于感知机，但是近年来已经创建了不同的架构来解决不同的数据问题。这主要是由于现实数据问题中可以找到的不同数据格式，如文本、音频和图像。本章的目的是介绍神经网络的主题及其主要优缺点，以便更好地理解何时以及如何使用它们。接下来，本章将详细解释最流行的神经网络架构的构建模块：**人工神经网络**（**ANNs**）、**卷积神经网络**（**CNNs**）和**循环神经网络**（**RNNs**）。
- en: Following this, the process of building an effective model will be explained
    by solving a real-life regression problem. This includes the preparation of the
    data to be fed to the neural network (also known as data preprocessing), the definition
    of the neural network architecture to be used, and finally, the evaluation of
    the performance of the model, with the objective of determining how it can be
    improved to achieve an optimal solution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，通过解决实际的回归问题来解释构建有效模型的过程。这包括准备输入到神经网络中的数据（也称为数据预处理）、定义要使用的神经网络架构，最后评估模型的性能，以确定如何改进以实现最佳解决方案。
- en: The aforementioned process will be done using one of the neural network architectures
    learned in the previous chapter, taking into consideration that the solution for
    each data problem should be carried out using the architecture that performs best
    for the data type in question. The other architectures will be used in subsequent
    chapters to solve more complicated data problems that involve using images and
    sequences of text as input data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 前述过程将使用前一章学习的神经网络架构之一来完成，考虑到每个数据问题的解决方案应使用最适合该数据类型的架构。其他架构将在后续章节中用于解决涉及使用图像和文本序列作为输入数据的更复杂的数据问题。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As a reminder, the GitHub repository containing all the code used in this chapter
    can be found at the following link:[https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch
    )
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，包含本章中使用的所有代码的GitHub存储库可以在以下链接找到：[https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch
    )
- en: Introduction to Neural Networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介神经网络
- en: 'Developed several decades ago, neural networks need to learn from training
    data, rather than being programmed to solve a particular task following a set
    of rules. The learning process can follow one of the following methodologies:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年前发展起来的神经网络需要从训练数据中学习，而不是按照一套规则编程来解决特定任务。学习过程可以遵循以下方法之一：
- en: '**Supervised learning**: This is the simplest form of learning as it consists
    of a labeled dataset, where the neural network needs to find patterns that explain
    the relationship between the features and the target. The iterations during the
    learning process aim to minimize the difference between the predicted value and
    the ground truth. One example of this would be classifying a plant based on the
    attributes of its leaves.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：这是学习的最简单形式，因为它包括一个带标签的数据集，神经网络需要找到解释特征与目标之间关系的模式。学习过程中的迭代旨在最小化预测值与真实值之间的差异。一个例子是基于叶子属性对植物进行分类。'
- en: '**Unsupervised learning**: In contrast with the preceding methodology, unsupervised
    learning consists of training a model with unlabeled data (meaning that there
    is no target value). The purpose of this is to arrive at a better understanding
    of the input data, where, generally, networks take input data, encode it, and
    then reconstruct the content from the encoded version, ideally keeping the relevant
    information. For instance, given a paragraph, the neural network can map the words
    in order to output those words that are actually key, which can be used as tags
    to describe the paragraph.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：与前述方法相比，无监督学习包括使用未标记数据（即没有目标值）训练模型。其目的是更好地理解输入数据，通常网络会接收输入数据，对其进行编码，然后从编码版本中重建内容，理想情况下保留相关信息。例如，给定一段文字，神经网络可以映射单词以输出实际关键的单词，这些单词可以用作描述段落的标签。'
- en: '**Reinforcement learning**: This methodology consists of learning from the
    data at hand, with the main objective of maximizing a reward function in the long
    run. Hence, decisions are not made based on the immediate reward, but on the accumulation
    of it in the entire learning process, such as allocating resources to different
    tasks, with the objective of minimizing bottlenecks that would slow down general
    performance.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：这种方法论包括利用手头的数据进行学习，其主要目标是在长期内最大化奖励函数。因此，决策不是基于即时奖励，而是基于整个学习过程中的累积奖励，例如将资源分配给不同任务，以期减少减缓整体性能的瓶颈。'
- en: Note
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: From the learning methodologies mentioned, the most commonly used is supervised
    learning, which is the one that will be mainly used in subsequent sections. This
    means that most exercises, activities, and examples will use a labeled dataset
    as input data.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从提到的学习方法中，最常用的是监督学习，这也是后续章节中主要使用的学习方法。这意味着大多数练习、活动和示例将使用带标签的数据集作为输入数据。
- en: What Are Neural Networks?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络是什么？
- en: Put simply, neural networks are a type of machine learning algorithm modeled
    on the anatomy of the human brain, which use mathematical equations to learn a
    pattern from the observation of training data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，神经网络是一种机器学习算法，其模型建立在人脑解剖学基础上，利用数学方程从训练数据的观察中学习模式。
- en: However, to actually understand the logic behind the training process that neural
    networks typically follow, it is important to first understand the concept of
    perceptrons.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要真正理解神经网络通常遵循的训练过程背后的逻辑，首先理解感知器的概念非常重要。
- en: Developed during the 1950s by Frank Rosenblatt, a perceptron is an artificial
    neuron that, similar to neurons in the human brain, takes several inputs and produces
    a binary output, which becomes the input of a subsequent neuron. They are the
    essential building blocks of a neural network (just like neurons are the building
    blocks of the human brain).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器在20世纪50年代由Frank Rosenblatt开发，是一种人工神经元，类似于人脑中的神经元，接收多个输入并产生二进制输出，成为后续神经元的输入。它们是神经网络的基本构建块（就像神经元是人脑的构建块一样）。
- en: '![Figure 2.1: Diagram of a perceptron'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：感知器示意图'
- en: '](img/02_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_01.jpg)'
- en: 'Figure 2.1: Diagram of a perceptron'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.1：感知器示意图
- en: Here, X1, X2, X3, and X4 represent the different inputs that the perceptron
    receives, and there could be any number of these. The gray circle is the perceptron,
    where the processing of the inputs occurs to arrive at an outcome.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，X1、X2、X3 和 X4 表示感知器接收的不同输入，可能有任意数量的输入。灰色圆圈表示感知器，在这里处理输入以得出结果。
- en: 'Rosenblatt also introduced the concept of weights (w1, w2, …, wn), which are
    numbers that express the importance of each input. The output can be either 0
    or 1, and it depends on whether the weighted sum of the inputs is above or below
    a given threshold that can be set as a parameter of the perceptron, as can be
    seen here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 罗森布拉特还引入了权重（w1, w2, …, wn）的概念，这些是表达每个输入重要性的数字。输出可以是0或1，这取决于输入的加权和是高于还是低于给定的阈值，可以作为感知器的参数设置，如下所示：
- en: '![Figure 2.2: Equation of output for perceptrons'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2：感知器输出方程'
- en: '](img/02_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_02.jpg)'
- en: 'Figure 2.2: Equation of output for perceptrons'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.2：感知器输出方程
- en: 'Exercise 2: Performing the Calculations of a Perceptron'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2：执行感知器的计算
- en: The following exercise does not require programming of any kind; instead, it
    consists of simple calculations to help you understand the notion of the perceptron.
    To perform the calculations, consider the following scenario.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的练习不需要任何编程；相反，它包含简单的计算，以帮助你理解感知器的概念。要执行这些计算，请考虑以下情景。
- en: 'There is a music festival in your town next Friday, but you are ill and trying
    to decide whether to go (where 0 means you are going and 1 means your aren''t
    going). To make the decision, you decide to consider three factors:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下周五你的城里有个音乐节，但你病了，正考虑是否去参加（其中0表示你会去，1表示你不会去）。为了做出决定，你决定考虑三个因素：
- en: Will there be good weather? (X1)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气会好吗？（X1）
- en: Do you have someone to go with? (X2)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有人可以一起去吗？（X2）
- en: Is the music to your liking? (X3)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你喜欢这里的音乐吗？（X3）
- en: 'For the preceding factors, we will use 1 if the answer to the question is yes,
    and 0 if the answer is no. Additionally, as you are very sick, the factor related
    to the weather is highly relevant, and you decide to give this factor a weight
    twice as big as the other 2 factors. Hence, the weights for the factors are 4
    (w1), 2 (w2), and 2 (w3). Now, consider a threshold of 5:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 针对上述因素，如果问题的答案是是，则我们将使用1，如果答案是否，则使用0。另外，由于你病得很厉害，天气因素相关性很高，你决定给这个因素的权重是其他两个因素的两倍。因此，因素的权重为4（w1）、2（w2）和2（w3）。现在，考虑一个阈值为5：
- en: 'With the information given, calculate the output of the perceptron, considering
    that the weather is not good next Friday, but you both have someone to go with
    and like the music at the festival:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下周五天气不好，但你有人可以一起去音乐节，并且喜欢那里的音乐：
- en: '![Figure 2.3: Output of the perceptron'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3：感知器的输出'
- en: '](img/02_03.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_03.jpg)'
- en: 'Figure 2.3: Output of the perceptron'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.3：感知器的输出
- en: Considering that the output is less than the threshold, the final result will
    be equal to 1, meaning that you should ynot go to the festival to avoid the risk
    of getting even more sicker.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到输出低于阈值，最终结果将等于1，意味着你不应该去音乐节，以避免病情加重的风险。
- en: Congratulations! You have successfully performed the calculations of a perceptron.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你成功地执行了感知器的计算。
- en: Multi-Layer Perceptron
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'Considering the aforementioned, the notion of a multi-layered network consists
    of a network of multiple perceptrons stacked together (also known as nodes or
    neurons), such as the one shown here:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到前述情况，多层网络的概念包括多个感知器（也称为节点或神经元）堆叠在一起的网络，如此处所示：
- en: '![Figure 2.4: Diagram of a multi-layer perceptron'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4：多层感知器的图示'
- en: '](img/02_04.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_04.jpg)'
- en: 'Figure 2.4: Diagram of a multi-layer perceptron'
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.4：多层感知器的图示
- en: Note
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: The notation to refer to the layers in a neural network is as follows:The first
    layer is also known as the input layer, the last layer is also known as the output
    layer, and all layers in between are known as hidden layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中用于指代层次的符号如下：第一层也称为输入层，最后一层也称为输出层，中间的所有层称为隐藏层。
- en: Here, again, a set of inputs is used to train the model, but instead of feeding
    a single perceptron, they are fed to all perceptrons (neurons) in the first layer.
    Next, the outputs obtained from this layer are used as inputs for the perceptrons
    in the subsequent layer, and so on until a final layer is reached, which is in
    charge of outputting a result.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，再次使用一组输入来训练模型，但不是将输入馈送到单个感知器，而是馈送到第一层中的所有感知器（神经元）。接下来，从该层获得的输出被用作下一层感知器的输入，依此类推，直到达到最终层，负责输出结果。
- en: It is important to mention that the first layer of a perceptron handles a simple
    decision process by weighting the inputs, while the subsequent layer can handle
    more complex and abstract decisions based on the output of the previous layer,
    hence the state-of-the-art performance of deep neural networks (networks that
    use many layers) for complex data problems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的是，感知器的第一层通过加权输入来处理简单的决策过程，而后续层可以根据前一层的输出处理更复杂和抽象的决策，因此深度神经网络（使用许多层的网络）在处理复杂数据问题时表现出色。
- en: Different to traditional perceptrons, neural networks have evolved to be able
    to have one or multiple nodes in the output layer, in order to be able to present
    the result either as binary or multiclass.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的感知器不同，神经网络已经演变为能够在输出层具有一个或多个节点，以便将结果呈现为二进制或多类别。
- en: The Learning Process of a Neural Network
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的学习过程
- en: In general terms, neural networks are a connection of multiple neurons, where
    each neuron computes a linear function along with an activation function to arrive
    at an output based on some inputs. This output is tied to a weight, which represents
    its level of importance, to be used for calculations in the following layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，神经网络是多个神经元的连接，其中每个神经元计算一个线性函数以及一个激活函数，以便根据一些输入得出输出。此输出与权重相关联，代表其重要性水平，以供后续层次的计算使用。
- en: Moreover, these calculations are carried out throughout the entire architecture
    of the network, where a final output is reached. This output is used to determine
    the performance of the network in comparison to the ground truth, which is then
    used to adjust the different parameters of the network to start the calculation
    process over again.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这些计算是在整个网络架构中进行的，达到最终输出。此输出用于确定网络性能与真实情况的比较，然后用于调整网络的不同参数，以重新开始计算过程。
- en: 'Considering this, the training process of a neural network can be seen as an
    iterative process that goes forward and backward through the layers of the network
    to arrive at an optimal result, which can be seen in the following figure and
    will be explained in detail:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，神经网络的训练过程可以被视为一个迭代过程，通过网络的各层前进和后退，以达到一个最优结果，如下图所示，并将详细解释：
- en: '![Figure 2.5: Diagram of the learning process of a neural network'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：神经网络学习过程的图示'
- en: '](img/02_05.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_05.jpg)'
- en: 'Figure 2.5: Diagram of the learning process of a neural network'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.5：神经网络学习过程的图示
- en: '**Forward Propagation**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向传播**'
- en: This is the process of going from left to right through the architecture of
    the network, while performing calculations using the input data to arrive at a
    prediction that can be compared to the ground truth. This means that every neuron
    in the network will transform the input data (the initial data or data received
    from the previous layer) according to the weights and biases that it has associated
    to it and send the output to the subsequent layer, until a final layer is reached
    and a prediction is made.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过网络架构从左到右进行的过程，同时使用输入数据进行计算，以得出可以与真实情况进行比较的预测。这意味着网络中的每个神经元都会根据其关联的权重和偏置转换输入数据（初始数据或来自前一层的数据），并将输出发送到下一层，直到达到最终层并进行预测。
- en: 'The calculations performed in each neuron include a linear function that multiplies
    the input data to some weights plus a bias, which is then passed through an activation
    function. The main purpose of the activation function is to break the linearity
    of the model, which is crucial considering that most real-life data problems solved
    using neural networks are not defined by a line, but rather by a complex function.
    The formulas can be found here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元中进行的计算包括一个线性函数，该函数将输入数据乘以一些权重再加上偏置，然后通过激活函数传递。激活函数的主要目的是打破模型的线性性，这在考虑到大多数使用神经网络解决的现实生活数据问题不是线性定义，而是由复杂函数定义时非常关键。相关公式可以在这里找到：
- en: '![Figure 2.6: Calculations performed by each neuron'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6：每个神经元执行的计算'
- en: '](img/02_06.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_06.jpg)'
- en: 'Figure 2.6: Calculations performed by each neuron'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.6：每个神经元执行的计算
- en: Here, as mentioned before, X refers to the input data, W is the weights that
    determine the level of importance of the input data, b is the bias value, and
    sigma (![](img/02_32.png)) represents the activation function applied over the
    linear function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此处，如前所述，X 指输入数据，W 是确定输入数据重要性水平的权重，b 是偏置值，sigma（![](img/02_32.png)）表示应用于线性函数的激活函数。
- en: 'The activation serves the purpose of introducing non-linearity to the model.
    There are different activation functions to choose from, and a list of the most
    commonly used nowadays is as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的作用在于向模型引入非线性。现今常用的激活函数如下：
- en: '**Sigmoid**: This is S-shaped, and it basically converts values into simple
    probabilities between 0 and 1, where most of the outputs obtained by the sigmoid
    function will be close to the extremes of 0 and 1:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：它呈 S 形，基本上将值转换为介于 0 和 1 之间的简单概率，sigmoid 函数得到的大多数输出将接近 0 和 1 的极端值：'
- en: '![Figure 2.7: Sigmoid activation function'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7：Sigmoid 激活函数'
- en: '](img/02_07.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_07.jpg)'
- en: 'Figure 2.7: Sigmoid activation function'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.7：Sigmoid 激活函数
- en: '![Figure 2.8: Graphical representation of the sigmoid activation function'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：Sigmoid 激活函数的图形表示'
- en: '](img/02_08.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_08.jpg)'
- en: 'Figure 2.8: Graphical representation of the sigmoid activation function'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.8：Sigmoid 激活函数的图形表示
- en: '**Softmax**: Similar to the sigmoid function, it calculates the probability
    distribution of an event over n events, meaning that its output is not binary.
    In simple terms, this function calculates the probability of the output being
    one of the target classes in comparison to the other classes:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Softmax**：与 sigmoid 函数类似，它计算一个事件在 n 个事件中的概率分布，这意味着其输出不是二元的。简单来说，该函数计算输出属于目标类别之一的概率，相对于其他类别：'
- en: '![Figure 2.9: Softmax activation function'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：Softmax 激活函数'
- en: '](img/02_09.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_09.jpg)'
- en: 'Figure 2.9: Softmax activation function'
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.9：Softmax 激活函数
- en: Considering that its output is a probability, this activation function is often
    found in the output layer of classification networks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到其输出为概率，这种激活函数通常出现在分类网络的输出层中。
- en: '**Tanh**: This function represents the relationship between the hyperbolic
    sine and the hyperbolic cosine, and the result is between -1 and 1\. The main
    advantage of this activation function is that negative values can be dealt with
    more easily:'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tanh**：该函数表示双曲正弦和双曲余弦之间的关系，其结果在 -1 到 1 之间。该激活函数的主要优势在于能更轻松地处理负值：'
- en: '![Figure 2.10: Tanh activation function.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10：tanh 激活函数。'
- en: '](img/02_10.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_10.jpg)'
- en: 'Figure 2.10: Tanh activation function.'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.10：tanh 激活函数。
- en: '![Figure 2.11: Graphical representation of the tanh activation function'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11：tanh 激活函数的图形表示'
- en: '](img/02_11.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_11.jpg)'
- en: 'Figure 2.11: Graphical representation of the tanh activation function'
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.11：tanh 激活函数的图形表示
- en: '**Rectified Linear Function (ReLU)**: This basically activates a node given
    that the output of the linear function is above 0, otherwise its output will be
    0\. If the output of the linear function is above 0, the result from this activation
    function will be the raw number it received as input:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修正线性单元函数（ReLU）**：基本上激活一个节点，条件是线性函数的输出大于 0，否则其输出将为 0。如果线性函数的输出大于 0，则该激活函数的结果将是其接收的原始数字：'
- en: '![Figure 2.12: ReLU activation function'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12：ReLU 激活函数的图形表示'
- en: '](img/02_12.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_12.jpg)'
- en: 'Figure 2.12: ReLU activation function'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.12：ReLU 激活函数
- en: 'Conventionally, this activation function is used for all hidden layers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，该激活函数用于所有隐藏层：
- en: '![Figure 2.13: Graphical representation of the ReLU activation function'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13：ReLU 激活函数的图形表示'
- en: '](img/02_13.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_13.jpg)'
- en: 'Figure 2.13: Graphical representation of the ReLU activation function'
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.13：ReLU 激活函数的图形表示
- en: '**The Calculation of Loss Functions**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数的计算**'
- en: Once the forward propagation is complete, the next step in the training process
    is to calculate a loss function to estimate the error of the model by comparing
    how good or bad the prediction is in relation to the ground truth value. Considering
    this, the ideal value to be reached is 0, which would mean that there is no divergence
    between the two values.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦前向传播完成，训练过程的下一步是通过比较预测结果与真实值来计算损失函数，以估计模型的误差。考虑到这一点，理想的值应为 0，这意味着两个值之间没有偏差。
- en: This means that the goal in each iteration of the training process would be
    to minimize the loss function by changing the parameters (weights and biases)
    used to perform the calculations during the forward pass.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在训练过程的每个迭代中，目标是通过改变参数（权重和偏置）来最小化损失函数，在前向传播期间执行计算。
- en: 'Again, there are multiple loss functions to choose from. However, the most
    commonly used loss functions for regression and classification tasks are explained
    here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，有多种损失函数可供选择。然而，用于回归和分类任务的最常用的损失函数在此有解释：
- en: '**Mean squared error (MSE)**: Widely used to measure the performance of regression
    models, the MSE function calculates the sum of the distance between the ground
    truth and the prediction values:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差（MSE）**：广泛用于衡量回归模型性能，MSE函数计算实际值和预测值之间距离的平方和：'
- en: '![Figure 2.14: Mean squared error loss function'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14: 均方误差损失函数'
- en: '](img/02_14.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_14.jpg)'
- en: 'Figure 2.14: Mean squared error loss function'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.14: 均方误差损失函数'
- en: Here, *n* refers to the number of samples, ![](img/02_30.png) is the ground
    truth values, and ![](img/02_31.png) is the predicted value.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n* 是样本数，![](img/02_30.png) 是实际值，而 ![](img/02_31.png) 是预测值。
- en: '**Cross entropy/multi-class cross entropy**: This function is conventionally
    used for binary or multi-class classification models. It measures the divergence
    between two probability distributions; a large loss function will represent a
    large divergence. Hence, the objective here is to minimize the loss function as
    well:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉熵/多类交叉熵**：这个函数通常用于二元或多类分类模型。它衡量两个概率分布之间的差异；一个较大的损失函数将代表较大的差异。因此，这里的目标是尽量减少损失函数：'
- en: '![Figure 2.15: Cross entropy loss function'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15: 交叉熵损失函数'
- en: '](img/02_15.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_15.jpg)'
- en: 'Figure 2.15: Cross entropy loss function'
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.15: 交叉熵损失函数'
- en: Again, n refers to the number of samples. ![](img/02_301.png) and ![](img/02_311.png)
    are the ground truth and the predicted value, respectively.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，n 代表样本数。![](img/02_301.png) 和 ![](img/02_311.png) 分别是实际值和预测值。
- en: '**Backward Propagation (Backpropagation)**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播（反向传递）**'
- en: The final step in the training process consists of going from right to left
    in the architecture of the network to calculate the partial derivatives of the
    loss function in respect to the weights and biases in each layer, in order to
    update these parameters (weights and biases) so that, in the next iteration step,
    the loss function is lower.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的最后一步是沿着网络架构从右到左计算损失函数对每一层权重和偏置的偏导数，以便更新这些参数（权重和偏置），以便在下一次迭代步骤中减少损失函数。
- en: 'Moreover, the final objective of the optimization algorithm is to find the
    global minima where the loss function has reached the least possible value, as
    shown in the following figure:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，优化算法的最终目标是找到损失函数达到可能的最小值的全局极小值，如下图所示：
- en: Note
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As a reminder, a local minima refers to the smallest value within a section
    of the function domain. On the other hand, a global minima refers to the smallest
    value of the entire domain of the function.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，局部极小值指的是函数定义域中的最小值。另一方面，全局极小值指的是整个函数定义域的最小值。
- en: '![Figure 2.16: Loss function optimization through the iteration steps.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16: 通过迭代步骤优化损失函数。'
- en: Two-dimensional space.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 二维空间。
- en: '](img/02_16.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_16.jpg)'
- en: 'Figure 2.16: Loss function optimization through the iteration steps. Two-dimensional
    space.'
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 2.16: 通过迭代步骤优化损失函数。二维空间。'
- en: Here, the dot furthest to the left (A) is the initial value of the loss function,
    before any optimization. The dot furthest to the right (B), at the bottom of the
    curve, is the loss function after several iteration steps, where its value has
    been minimized. The process of going from one dot to another is called **step**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，最左边的点（A）是损失函数的初始值，在任何优化之前。最右边最底部的点（B）是经过多次迭代步骤后损失函数被最小化的值。从一个点到另一个点的过程称为**步骤**。
- en: However, it is important to mention that the loss function is not always as
    smooth as the preceding one, which can introduce the risk of reaching a local
    minima during the optimization process.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要提到，损失函数并不总是像前面的那样平滑，这可能在优化过程中引入达到局部极小值的风险。
- en: This process is also called optimization, and there are different algorithms
    that vary in methodology to achieve the same objective. The most commonly used
    optimization algorithm will be explained next.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程也称为优化，有不同的算法以不同的方法达到相同的目标。接下来将解释最常用的优化算法。
- en: '**Gradient Descent**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度下降**'
- en: Gradient descent is the optimization algorithm that's most widely used among
    data scientists, and it is the basis of many other optimization algorithms. After
    the gradients for each neuron are calculated, the weights and biases are updated
    in the opposite direction of the gradient, which should be multiplied by a learning
    rate (used to control the size of the steps taken in each optimization), as seen
    in the following equation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是数据科学家中最广泛使用的优化算法，也是许多其他优化算法的基础。在计算每个神经元的梯度后，权重和偏置会朝梯度的相反方向进行更新，更新步骤的大小由学习率控制（用于控制每次优化中的步骤大小），如下方程式所示。
- en: The learning rate is crucial during the training process as it prevents the
    updates of the weights and biases from over/undershooting, which may prevent the
    model from reaching convergence or delay the training process, respectively.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，学习率至关重要，因为它可以防止权重和偏置的更新过度/不足，这可能会阻止模型达到收敛或延迟训练过程。
- en: 'The optimization of weights and biases in the gradient descent algorithm is
    as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降算法中，权重和偏置的优化如下所示：
- en: '![Figure 2.17: Optimization of parameters in the gradient descent algorithm'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.17: 梯度下降算法中参数的优化](img/02_17.jpg)'
- en: '](img/02_17.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 2.17: 梯度下降算法中参数的优化](img/02_17.jpg)'
- en: 'Figure 2.17: Optimization of parameters in the gradient descent algorithm'
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.17：梯度下降算法中参数的优化
- en: Here, α refers to the learning rate and dw/db represent the gradients of the
    weights or biases in a given neuron. The product of the two values is subtracted
    from the original value of the weight or bias in order to penalize the higher
    values, which are contributing to computing a large loss function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α 表示学习率，dw/db 表示给定神经元中权重或偏置的梯度。这两个值的乘积从权重或偏置的原始值中减去，以惩罚那些导致计算大损失函数的较高值。
- en: An improvement to the gradient descent algorithm is called Stochastic gradient
    descent, and it basically follows the same process, with the distinction that
    it takes the input data in random batches instead of in one chunk, which improves
    the training times while reaching outstanding performance. Moreover, this approach
    allows the use of larger datasets, because by using small batches of the dataset
    as inputs we are no longer limited by computational resources.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对梯度下降算法的改进称为随机梯度下降，它基本上遵循相同的过程，但不同之处在于它以随机批次方式获取输入数据，而不是一次性获取整个数据块，这可以提高训练时间同时达到卓越的性能。此外，这种方法允许使用更大的数据集，因为通过使用数据集的小批次作为输入，我们不再受到计算资源的限制。
- en: Advantages and Disadvantages
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优势和劣势
- en: The following is an explanation of the advantages and disadvantages of neural
    networks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是神经网络的优势和劣势的解释。
- en: '**Advantages**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势**'
- en: 'Neural networks have become increasingly popular in the last few years for
    four main reasons:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年中，神经网络因四个主要原因变得越来越受欢迎：
- en: '**Data**: Neural networks are widely known for their ability to capitalize
    on large amounts of data, and thanks to the advances in hardware and software,
    the recollection and storage of massive databases is now possible. This has allowed
    neural networks to show their real potential as more data is fed into them.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：神经网络以其利用大量数据的能力而广为人知，多亏了硬件和软件的进步，现在可以回忆和存储大量数据库。这使得神经网络在输入更多数据时展现出了其真正的潜力。'
- en: '**Complex data problems**: As has been explained before, neural networks are
    excellent for solving complex data problems that cannot be tackled by other machine
    learning algorithms. This is mainly due to their capability to process large datasets
    and uncover complex patterns.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂数据问题**：正如前面所述，神经网络非常适合解决其他机器学习算法无法解决的复杂数据问题。这主要是因为它们能够处理大规模数据集并揭示复杂的模式。'
- en: '**Computational power**: Advances in technology have also increased the computational
    power available these days, which is crucial for training neural network models
    that use millions of pieces of data.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算能力**：技术的进步也提升了当今可用的计算能力，这对于训练使用数百万数据的神经网络模型至关重要。'
- en: '**Academic research**: Thanks to the preceding three points, a proliferation
    of academic research on the topic is available on the internet, which not only
    facilitates the immersion of new research each day, but also helps to keep the
    algorithms and hardware/software requirements up to date.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学术研究**：由于前述三点，关于这一主题的大量学术研究可在互联网上找到，这不仅促进了每天新研究的涌现，还有助于保持算法及硬件/软件需求的最新性。'
- en: '**Disadvantages**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**'
- en: Just because there are a lot of advantages to using a neural network, it does
    not mean that every data problem should be solved this way. This is a mistake
    that is commonly made. There is no one algorithm that will perform well for all
    data problems, and the selection of the algorithm should depend on the resources
    available, as well as the data problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 只因为使用神经网络有很多优点，并不意味着每个数据问题都应该用这种方式解决。这是一个常见的错误。没有一种算法适用于所有数据问题，选择算法应该依赖于可用的资源以及数据问题本身。
- en: And, although neural networks are thought to outperform almost any machine learning
    algorithm, it is crucial to consider their disadvantages as well, to weigh up
    what matters most for the data problem.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络被认为能够胜任几乎所有的机器学习算法，但重要的是也要考虑它们的缺点，以权衡对数据问题最为重要的因素。
- en: '**Black box**: This is one of the most commonly-known disadvantages of neural
    networks. It basically means that how and why a neural network reached a certain
    output is unknown. For instance, when a neural network wrongly predicts a cat
    picture as a dog, it is not possible to know what the cause of the error was.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**黑盒子**：这是神经网络最常见的缺点之一。它基本上意味着神经网络如何以及为什么会得出特定的输出是未知的。例如，当神经网络错误地将一张猫的图片预测为狗时，无法知道错误的原因是什么。'
- en: '**Data requirements**: The vast amounts of data that they require to achieve
    optimal results can be equally an advantage and a disadvantage. Neural networks
    require more data than traditional machine learning algorithms, which can be the
    main reason to choose between them and other algorithms for some data problems.
    This becomes a greater issue when the task at hand is supervised, which means
    that the data needs to be labeled.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据需求**：为了达到最佳结果，神经网络需要大量的数据，这既是优点也是缺点。神经网络需要比传统机器学习算法更多的数据，这可能是某些数据问题中选择它们与其他算法之间的主要原因。当任务是有监督学习时，即数据需要被标记，这一问题变得尤为突出。'
- en: '**Training times**: Tied to the preceding disadvantage, the need for vast amounts
    of data also makes the training process last longer than traditional machine learning
    algorithms, which in some cases is not an option. Training times can be reduced
    through the use of GPUs, which speed up computation.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：与前述的缺点相关联，对大量数据的需求也使得训练过程比传统机器学习算法更长，而在某些情况下这是不可选的。通过使用GPU可以缩短训练时间，因为GPU能加速计算过程。'
- en: '**Computationally expensive**: Again, the training process of neural networks
    is computationally expensive. While one neural network could take weeks to converge,
    other machine learning algorithms can take hours or minutes to be trained. The
    amount of computational resources needed depends on the quantity of data at hand,
    as well as the complexity of the network; deeper neural networks take a longer
    time to train.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本高昂**：再次强调，神经网络的训练过程是计算成本高昂的。虽然一个神经网络可能需要数周才能收敛，其他机器学习算法则可能只需几小时或几分钟即可完成训练。所需的计算资源取决于手头数据的量以及网络的复杂性；更深层次的神经网络需要更长的训练时间。'
- en: Note
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There are a wide variety of neural network architectures. Three of the most
    commonly used ones will be explained in this chapter, along with their practical
    implementation in subsequent chapters. However, if you wish to learn about other
    architectures, visit [http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/).
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经网络的架构种类繁多。本章将解释其中三种最常用的架构，并在后续章节中介绍它们的实际实现。然而，如果你希望了解其他架构，请访问[http://www.asimovinstitute.org/neural-network-zoo/](http://www.asimovinstitute.org/neural-network-zoo/)。
- en: Introduction to Artificial Neural Networks
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人工神经网络简介
- en: '**Artificial neural networks** (**ANNs**), also known as multi-layer perceptrons,
    are a collection of multiple perceptrons, as explained before. Here, it is important
    to mention that the connection between perceptrons occurs through layers, where
    one layer can have as many perceptrons as desired, and they are all connected
    to all the other perceptrons in the preceding and subsequent layer.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**），也称为多层感知器，是多个感知器的集合，如前所述。这里重要的是提到，感知器之间的连接发生在层之间，其中一个层可以拥有任意多的感知器，并且它们都与前后层的所有其他感知器相连接。'
- en: Networks can have one or more layers. Networks with over four layers are considered
    to be deep neural networks and are commonly used to solve complex and abstract
    data problems.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 网络可以有一个或多个层。具有超过四层的网络被认为是深度神经网络，并且通常用于解决复杂和抽象的数据问题。
- en: 'ANNs are typically composed of three main elements, which were explained in
    detail earlier, and can also be seen in *Figure 2.18*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ANN通常由三个主要元素组成，在前面已经详细解释过，也可以在*图2.18*中看到：
- en: '**Input layer**: This is the first layer of the network, conventionally located
    furthest left in the graphical representation of a network. It receives the input
    data before any calculation is performed, and completes the first set of calculations,
    where the most generic patterns are uncovered.'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入层**：这是网络的第一层，传统上位于网络图表的最左侧。它在执行任何计算之前接收输入数据，并完成第一组计算，在这里最通用的模式被揭示。'
- en: For supervised learning problems, the input data consists of a pair of features
    and targets. The job of the network is to uncover the correlation or dependency
    between the input and output.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于监督学习问题，输入数据由特征和目标值的一对组成。网络的任务是揭示输入和输出之间的相关性或依赖关系。
- en: '**Hidden layers**: Next, the hidden layers can be found. A neural network can
    have as many hidden layers as possible. The more layers it has, the more complex
    data problems it can tackle, but it will also take longer to train. There are
    also neural network architectures that do not contain hidden layers at all, which
    is the case with single-layer networks.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐藏层**：接下来，可以找到隐藏层。神经网络可以拥有尽可能多的隐藏层。层数越多，它可以处理的数据问题越复杂，但训练时间也会更长。也有一些神经网络架构完全不包含隐藏层，这在单层网络中是适用的情况。'
- en: In each layer, a computation is performed based on the information received
    as input from the previous layer, to output a prediction that will become the
    input of the subsequent layer.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每一层中，基于从前一层接收到的输入信息进行计算，输出一个预测结果，成为后续层的输入。
- en: '**Output layer**: This is the last layer of the network, located at the far
    right of the graphical representation of the network. It receives data after being
    processed by all the neurons in the network to make and display a final prediction.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输出层**：这是网络的最后一层，位于网络图表的最右侧。它在所有神经元处理数据后接收数据，以生成和显示最终预测。'
- en: The output layer can have one or more neurons. The former refers to models where
    the solution is binary, in the form of 0s or 1s. On the other hand, the latter
    case consists of models that output the probability of an instance to belong to
    each of the possible class labels (targets), meaning that the layer will have
    as many neurons as there are class labels.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出层可以有一个或多个神经元。前者是指解决方案是二进制形式的模型，即0或1的形式。另一方面，后者包括输出实例属于每个可能类标签（目标）的概率，这意味着该层将具有与类标签数量相同的神经元。
- en: '![Figure 2.18: Architecture of a neural network with two hidden layers'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.18：具有两个隐藏层的神经网络架构'
- en: '](img/02_18.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_18.jpg)'
- en: 'Figure 2.18: Architecture of a neural network with two hidden layers'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.18：具有两个隐藏层的神经网络架构
- en: Introduction to Convolutional Neural Networks
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络介绍
- en: Convolutional neural networks (CNNs) are mostly used in the field of computer
    vision, where, in recent decades, machines have achieved levels of accuracy that
    surpass human ability, which has made them increasingly popular.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）主要用于计算机视觉领域，在这个领域，近几十年来，机器已经达到甚至超过人类能力的准确性水平，因此它们变得越来越受欢迎。
- en: Inspired by human brains, CNNs search to create models that use different groups
    of neurons to recognize different aspects of an image. These groups should be
    able to communicate with each other so that, together, they can form the big picture.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类大脑启发，CNN 搜索创建模型，利用不同组的神经元识别图像的不同方面。这些组应该能够相互通信，以便共同形成整体图像。
- en: Considering this, layers in the architecture of a CNN divide their recognition
    tasks. The first layers focus on trivial patterns, and the layers at the end of
    the network use that information to uncover more complex patterns.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，CNN 架构中的层分割它们的识别任务。第一层专注于简单的模式，网络末端的层使用这些信息揭示更复杂的模式。
- en: For instance, when recognizing human faces in pictures, the first couple of
    layers focus on finding edges that separate one feature from another. Next, the
    subsequent layers emphasize certain features of the face, such as the nose. Finally,
    the last couple of layers use this information to put together the entire face
    of the person.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在识别图片中的人脸时，前几层专注于查找将一个特征与另一个分开的边缘。接下来的层强调人脸的某些特征，如鼻子。最后，最后几层使用这些信息将整个人脸拼合在一起。
- en: 'This idea of using a group of neurons to activate when certain features are
    encountered is achieved through the use of filters or kernels, which are one of
    the main building blocks of the architecture of convolutional neural networks.
    However, they are not the only element present in the architecture, which is why
    a brief explanation of all the components of CNNs will be provided:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用滤波器或核心来激活一组神经元以在遇到特定特征时激活的想法，这是通过使用卷积神经网络结构的主要构建块之一来实现的。然而，它们并不是结构中唯一存在的元素，这就是为什么将提供对
    CNN 的所有组成部分的简要解释：
- en: Note
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: The concepts of padding and stride, which you might have heard of when using
    CNNs, will be explained in subsequent sections of this book.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 CNN 时可能听说过的填充和步幅的概念，将在本书的后续章节中进行解释。
- en: '**Convolutional layers**: In these layers, a convolutional computation occurs
    between an image (represented as a matrix of pixels) and a filter. This computation
    produces a feature map as an output that ultimately serves as input for the next
    layer.'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积层**：在这些层中，图像（表示为像素矩阵）与滤波器之间进行卷积计算。这种计算产生特征图作为输出，最终作为下一层的输入。'
- en: 'The computation takes a subsection of the image matrix of the same shape of
    the filter and performs a multiplication of the values. Then, the sum of the product
    is set as the output for that section of the image, as shown in the following
    figure:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算取图像矩阵的一个子部分，并执行值的乘法。然后，乘积的总和被设定为该图像部分的输出，如下图所示：
- en: '![Figure 2.19: Convolution operation between image and filter'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.19：图像与滤波器之间的卷积操作'
- en: '](img/02_19.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/02_19.jpg)'
- en: 'Figure 2.19: Convolution operation between image and filter'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.19：图像与滤波器之间的卷积操作
- en: 'Here, the matrix to the left is the input data, the matrix in the middle is
    the filter, and the matrix to the right is the output from the computation. The
    computation that occurred with the values highlighted by the red box can be seen
    here:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，左侧的矩阵是输入数据，中间的矩阵是滤波器，右侧的矩阵是计算的输出。可以在这里看到通过红框突出显示的值进行的计算：
- en: '![Figure 2.20: Convolution of the first section of the image'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.20：图像第一部分的卷积'
- en: '](img/02_20.jpg)'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/02_20.jpg)'
- en: 'Figure 2.20: Convolution of the first section of the image'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.20：图像第一部分的卷积
- en: 'This convolutional multiplication is done for all subsections of the image.
    Figure 2.21 shows another convolution step for the same example:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种卷积乘法对图像的所有子部分进行。图 2.21 展示了相同示例的另一个卷积步骤：
- en: '![Figure 2.21: Further step in the convolution operation'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.21：卷积操作的进一步步骤'
- en: '](img/02_21.jpg)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/02_21.jpg)'
- en: 'Figure 2.21: Further step in the convolution operation'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.21：卷积操作的进一步步骤
- en: An important notion of convolutional layers is that they are invariant in such
    a way that each filter will have a specific function, which does not vary during
    the training process. For instance, a filter in charge of detecting ears will
    only specialize in that function throughout the training process.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卷积层的一个重要概念是，它们是不变的，每个滤波器都有一个特定的功能，在训练过程中不会变化。例如，负责检测耳朵的滤波器在整个训练过程中只专注于这个功能。
- en: Moreover, a convolutional neural network will typically have several convolutional
    layers, considering that each of them will focus on identifying a particular feature
    of the image, depending on the filter used. Additionally, it is important to mention
    that, commonly, there is one pooling layer in between two convolutional layers.
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，卷积神经网络通常会有多个卷积层，每个层都会根据所使用的滤波器专注于识别图像的特定特征。此外，需要指出的是，在两个卷积层之间通常有一个池化层。
- en: '**Pooling layers**: Although convolutional layers are capable of extracting
    relevant features from images, their results can become enormous when analyzing
    complex geometrical shapes, which would make the training process impossible in
    terms of computational power. Hence the invention of pooling layers.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**池化层**：尽管卷积层能够从图像中提取相关特征，但当分析复杂的几何形状时，其结果可能会变得庞大，这会使得训练过程在计算能力方面变得不可能。因此，池化层的发明就显得尤为重要。'
- en: These layers not only accomplish the goal of reducing the output of the convolutional
    layers, but also achieve the removal of noise present in the features extracted,
    which ultimately helps the accuracy of the model.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些层不仅达到了减少卷积层输出的目标，而且实现了去除特征中存在的噪声，从而最终提高了模型的准确性。
- en: 'There are two main types of pooling layers that can be applied, and the idea
    behind them is to detect the areas that express a stronger influence in the image
    so that the other areas can be overlooked:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可应用两种主要类型的池化层，并且它们的理念在于检测在图像中表达出更强烈影响的区域，从而可以忽略其他区域：
- en: '**Max pooling**: This operation consists of taking a subsection of the matrix
    of a given size and taking the maximum number in that subsection as the output
    of the max pooling operation.'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**：此操作包括取矩阵中给定大小的子段，并将该子段中的最大数作为最大池化操作的输出。'
- en: '![Figure 2.22: Max pooling operation'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.22：最大池化操作'
- en: '](img/02_22.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_22.jpg)'
- en: 'Figure 2.22: Max pooling operation'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.22：最大池化操作
- en: In the preceding figure, by using a 3x3 max pooling filter, the result on the
    right is achieved. Here, the yellow section (top-left corner) has a maximum number
    of 4, while the orange section (top-right corner) has a maximum number of 5.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，通过使用3x3的最大池化滤波器，得到了右侧的结果。在这里，黄色区域（左上角）的最大值为4，而橙色区域（右上角）的最大值为5。
- en: '**Average pooling**: Similarly, the average pooling operation takes subsections
    of the matrix and takes the number that meets the rule as output, which in this
    case is the average of all the numbers in the subsection in question.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均池化**：类似地，平均池化操作会取矩阵的子段并输出符合规则的数字，这种情况下是该子段中所有数字的平均值。'
- en: '![Figure 2.23: Average pooling operation'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.23：平均池化操作'
- en: '](img/02_23.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_23.jpg)'
- en: 'Figure 2.23: Average pooling operation'
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.23：平均池化操作
- en: Here, using a 3x3 filter, we get that 8.6 is the average of all the numbers
    in the yellow section (top-left corner), while 9.6 is the average for the ones
    in the orange section (top-right corner).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用3x3的滤波器，我们得到了黄色区域（左上角）所有数字的平均值为8.6，而橙色区域（右上角）的平均值为9.6。
- en: '**Fully connected layers**: Finally, and considering that the network would
    be of no use if it was only capable of detecting a set of features without having
    the capability of classifying them into a class label, fully connected layers
    are used at the end of CNNs to take the features detected by the previous layer
    (known as the features map) and output the probability of that group of features
    of belonging to a class label, which is used to make the final prediction.'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全连接层**：最后，考虑到如果网络只能检测一组特征而无法将其分类到类别标签中，那么网络将毫无用处，因此在CNN的末端使用全连接层，将前一层（称为特征图）检测到的特征输出，并输出这些特征组属于类别标签的概率，这被用于进行最终预测。'
- en: Like artificial neural networks, fully connected layers use perceptrons to calculate
    an output based on a given input. Moreover, it is crucial to mention that convolutional
    neural networks typically have more than one fully connected layer at the end
    of the architecture.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像人工神经网络一样，全连接层利用感知器根据给定的输入计算输出。此外，需要提到的是，卷积神经网络通常在架构末端有多个全连接层。
- en: 'By combining all of these concepts, the conventional architecture of convolutional
    neural networks is obtained, where there can be as many layers of each type as
    desired and each convolutional layer can have as many filters as desired (each
    for a particular task), and the pooling layer should have the same number of filters,
    as shown in the following figure:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 结合所有这些概念，可以得到卷积神经网络的传统结构，其中每种类型可以有任意多层，每个卷积层可以有任意多个滤波器（每个用于特定任务），池化层应具有相同数量的滤波器，如下图所示：
- en: '![Figure 2.24: Diagram of convolutional neural network architecture'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.24：卷积神经网络结构图'
- en: '](img/02_24.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_24.jpg)'
- en: 'Figure 2.24: Diagram of convolutional neural network architecture'
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.24：卷积神经网络结构图
- en: Introduction to Recurrent Neural Networks
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归神经网络简介
- en: The main limitation of the aforementioned neural networks is that they learn
    only by considering the current event (the input that is being processed) without
    taking into account previous or following events, which is inconvenient considering
    that we humans do not think that way. For instance, when reading a book, you can
    understand each sentence better by considering the context from the previous paragraph
    or more.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 前述神经网络的主要限制在于它们仅通过考虑当前事件（正在处理的输入）来学习，而不考虑先前或随后的事件，这在我们人类的思考方式中是不方便的。例如，在阅读一本书时，通过考虑上一段落或更多的上下文，你可以更好地理解每个句子。
- en: Due to this, and taking into account that neural networks aim to optimize several
    processes traditionally done by humans, it is crucial to think of a network able
    to consider a sequence of inputs and outputs, hence the creation of recurrent
    neural networks (RNNs). They are a robust type of neural network that allow the
    solution of complex data problems through the use of an internal memory.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到神经网络的优化过程通常由人类完成，设计能够考虑输入和输出序列的网络变得至关重要，因此递归神经网络（RNNs）应运而生。它们是一种强大的神经网络类型，通过使用内部记忆解决复杂的数据问题。
- en: Put simply, these networks contain loops in them that allow for the information
    to remain in their memory for longer periods, even when a subsequent set of information
    is being processed. This means that a perceptron in an RNN not only passes over
    the output to the following perceptron, but it also passes a bit of information
    to itself, which can be useful for analyzing the next bit of information. This
    memory-keeping capability allows them to be very accurate in predicting what is
    coming next.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这些网络包含了其中的循环，允许信息在其内存中保留更长时间，即使正在处理后续信息。这意味着RNN中的感知器不仅将输出传递给下一个感知器，还会向自身传递一些信息，这对于分析下一个信息片段是有用的。这种记忆保持能力使得它们在预测接下来会发生什么方面非常准确。
- en: The learning process of a recurrent neural network, similar to other networks,
    tries to map the relationship between an input (x) and an output (y), with the
    difference being that these models also take into consideration the entire or
    partial history of previous inputs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络的学习过程，类似于其他网络，试图映射输入（x）和输出（y）之间的关系，不同之处在于这些模型还考虑了全部或部分先前输入的历史。
- en: 'RNNs allow the processing of sequences of data in the form of a sequence of
    inputs, a sequence of outputs, or even both at the same time, as shown in the
    following figure:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs允许处理数据序列，可以是输入序列、输出序列，甚至同时处理两者，如下图所示：
- en: '![Figure 2.25: Sequence of data handled by RNNs'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.25：递归神经网络处理的数据序列'
- en: '](img/02_25.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_25.jpg)'
- en: 'Figure 2.25: Sequence of data handled by RNNs'
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.25：递归神经网络处理的数据序列
- en: Here, each box is a matrix and the arrows represent a function that occurs.
    The bottom boxes are the inputs, the top boxes are the outputs, and the middle
    boxes represent the state of the RNNs at that point, which holds the memory of
    the network.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个框都是一个矩阵，箭头表示发生的函数。底部框是输入，顶部框是输出，中间框表示该点的RNN状态，保存了网络的记忆。
- en: 'From left to right, the preceding diagrams are explained here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右，以下是前述图表的解释：
- en: A typical model that does not require an RNN to be solved. It has a fixed input
    and a fixed output. This can refer to image classification, for instance.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 典型的模型不需要RNN解决。它具有固定的输入和固定的输出。例如，这可以是图像分类。
- en: This model takes in an input and yields a sequence of outputs. Take, for instance,
    a model that receives an image as input and the output should be an image caption.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个模型接受一个输入并产生一系列的输出。例如，一个接收图像作为输入并输出图像标题的模型。
- en: Contrary to the above, this model takes a sequence of inputs and yields a single
    outcome. This type of architecture can be seen on sentiment analysis problems,
    where the input is the sentence to be analyzed and the output is the predicted
    sentiment behind the sentence.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与上述相反，这个模型接受一系列的输入并生成一个单一的输出。这种类型的架构可以在情感分析问题中看到，其中输入是要分析的句子，输出是句子背后的预测情感。
- en: The final two models take a sequence of inputs and return a sequence of outputs
    with the difference being that the first one first analyzes the entire set of
    inputs, to then generate the set of outputs. For instance, for language translations,
    where the entire sentence in one language needs to be understood before proceeding
    with the actual translation. On the other hand, the second many-to-many model
    analyzes the inputs and generates the outputs at the same time. For example, when
    each frame of a video is being labeled.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后两个模型接受一系列的输入并返回一系列的输出，不同之处在于第一个模型首先分析整个输入集，然后生成输出集。例如，在语言翻译中，需要先完全理解一种语言中的整个句子，然后再进行实际翻译。另一方面，第二个多对多模型同时分析输入并同时生成输出。例如，标记视频每一帧时。
- en: Data Preparation
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: The first step in the development of any deep learning model, after gathering
    the data, of book, should be the preparation of the data. This is crucial to understand
    the data at hand, and hence, be able to outline the scope of the project correctly.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据之后，任何深度学习模型的开发的第一步，应该是数据的准备。这对于正确理解手头的数据并能够正确界定项目范围至关重要。
- en: Many data scientists fail to do so, which results in models that perform poorly,
    and even models that are useless as they do not answer the data problem to begin
    with.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学家未能这样做，导致模型性能不佳，甚至模型无用，因为它们根本不解决数据问题。
- en: 'The process of preparing the data can be divided into three main tasks: 1)
    Understanding the data and dealing with any potential issues, 2) Rescaling the
    features to make sure no bias is introduced by mistake, and 3) Splitting the data
    to be able to measure performance accurately. All three tasks will be further
    explained in the next section.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据的过程可以分为三个主要任务：1）理解数据并处理任何潜在问题，2）重新缩放特征以确保不会由于错误引入偏差，以及3）拆分数据以能够准确地衡量性能。所有这三个任务将在下一节中进一步解释。
- en: Note
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All of the tasks explained previously are pretty much the same for applying
    any machine learning algorithm, considering that they refer to the techniques
    required to prepare data beforehand.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前解释的任务在应用任何机器学习算法时基本相同，因为它们涉及到预先准备数据所需的技术。
- en: Dealing with Messy Data
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理混乱数据
- en: This task mainly consists of performing **exploratory data analysis** (**EDA**)
    to understand the data available, as well as to detect potential issues that may
    affect the development of the model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这项任务主要包括执行**探索性数据分析**（**EDA**）以理解可用的数据，并检测可能影响模型开发的潜在问题。
- en: 'The EDA process is useful as it helps the developer to uncover information
    crucial to the definition of the book of action. This information is explained
    here:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: EDA过程非常有用，因为它帮助开发人员发现对行动计划定义至关重要的信息。这些信息在这里解释：
- en: '**Quantity of data**: This refers both to the number of instances and the number
    of features. The former is crucial for determining whether it is necessary or
    even possible to solve the data problem using a neural network, or even a deep
    neural network, considering that such models require vast amounts of data to achieve
    high levels of accuracy. The latter, on the other hand, is useful to determine
    whether it would be a good practice to develop some feature selection methodologies
    beforehand in order to reduce the number of features, to simplify the model, and
    eliminate any redundant information.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据量**：这既涉及实例的数量，也涉及特征的数量。前者对于确定是否有必要甚至可能使用神经网络或深度神经网络解决数据问题至关重要，考虑到这类模型需要大量数据才能达到高精度水平。而后者则有助于确定是否在开发之前采用某些特征选择方法，以减少特征数量、简化模型并消除任何冗余信息。'
- en: '**The target feature**: For supervised models, data needs to be labeled. Considering
    this, it is highly important to select the target feature (the objective that
    we want to achieve by building the model) in order to assess whether the feature
    has many missing or outlier values. Additionally, this helps determine the objective
    of the development, which should be in line with the data available.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**目标特征**：对于监督模型，数据需要被标记。考虑到这一点，选择目标特征（建立模型时希望达到的目标）非常重要，以评估特征是否存在许多缺失或异常值。此外，这有助于确定开发的目标，该目标应与可用数据保持一致。'
- en: '**Noisy data/outliers**: Noisy data refers to values that are visibly incorrect,
    for instance, a person who is 200 years old. On the other hand, outliers refer
    to values that, although they may be correct, are very far from the mean, for
    instance, a 10-year-old college student.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**噪声数据/异常值**：噪声数据指的是明显不正确的数值，例如年龄为200岁的人。另一方面，异常值指的是虽然可能是正确的数值，但离均值很远，例如10岁的大学生。'
- en: There is not an exact science to detecting outliers, but there are some methodologies
    that are commonly accepted. Assuming a normally distributed dataset, one of the
    most popular ones is determining as an outlier any value that is about 3-6 standard
    deviations away from the mean of all values, in both directions.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测异常值并没有确切的科学方法，但有些方法学是被普遍接受的。假设一个正态分布的数据集，其中最流行的方法之一是将任何偏离所有数值均值约3-6个标准偏差的值定义为异常值，无论是正方向还是负方向。
- en: An equally valid approach to identifying outliers is to select those values
    at the 99th and 1st percentile.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别异常值的一个同样有效的方法是选择处于99分位和1分位的数值。
- en: It is very important to handle such values when they represent over 5% of the
    data for a feature because failing to do so may introduce bias to the model. The
    way to handle these values, as with any other machine learning algorithm, is to
    either delete the outlier values or assign new values using mean or regression
    imputation techniques.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理此类数值非常重要，特别是当它们代表特征数据的5%以上时，因为不处理可能会引入模型偏差。处理这些数值的方法与任何其他机器学习算法一样，要么删除异常值，要么使用均值或回归插补技术赋予新值。
- en: '**Missing values**: Similar to the aforementioned, a dataset with many missing
    values can introduce bias to the model, considering that different models will
    make different assumptions about those values. Again, when missing values represent
    over 5% of the values of a feature, they should be handled by eliminating or replacing
    them, again using the mean or regression imputation techniques.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缺失值**：与前述情况类似，数据集中存在许多缺失值可能会引入模型偏差，考虑到不同模型会对这些值做出不同的假设。同样，当缺失值占特征值的5%以上时，应通过删除或替换它们的方式进行处理，同样可以使用均值或回归插补技术。'
- en: '**Qualitative features**: Finally, checking whether the dataset contains qualitative
    data is also a key step considering that removing or encoding data may result
    in more accurate models.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定性特征**：最后，检查数据集是否包含定性数据也是一个关键步骤，因为移除或编码数据可能会导致更准确的模型。'
- en: Additionally, in many research developments, several algorithms are tested on
    the same data in order to determine which one performs better, and some of these
    algorithms do not tolerate the use of qualitative data, hence the importance of
    converting or encoding them to be able to feed all algorithms the same data.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，在许多研究开发中，会在同一数据上测试多个算法，以确定哪一个表现更好，而其中一些算法不能容忍使用定性数据，因此转换或编码它们以能够将所有算法用同样的数据进行输入显得尤为重要。
- en: 'Exercise 3: Dealing with Messy Data'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3：处理混乱数据
- en: Note
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All of the exercises in this chapter will be done using the `Appliances energy
    prediction Dataset`, from the UC Irvine Machine Learning Repository, which can
    be downloaded using the following URL, at the `Data Folder` hyperlink:[https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction
    )
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所有的练习将使用UC Irvine机器学习库中的`Appliances energy prediction Dataset`进行，可以通过以下URL下载，使用`Data
    Folder`超链接：[https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction)
- en: 'In this exercise, we will use one of Python''s favorite packages to explore
    the data at hand and learn how to detect missing values, outliers, and qualitative
    values:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用Python的一个受欢迎的包来探索手头的数据，并学习如何检测缺失值、异常值和定性值：
- en: Note
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For the exercises and activities within this chapter, you will need to have
    Python 3.6, Jupyter, NumPy, and Pandas (at least version 0.21).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中的练习和活动，您需要安装Python 3.6、Jupyter、NumPy和Pandas（至少版本0.21）。
- en: Open a Jupyter notebook to implement this exercise.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Jupyter笔记本以实施这个练习。
- en: 'Open your cmd or terminal, navigate to the desired path, and use the following
    command to open a Jupyter notebook: `jupyter notebook`'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打开您的cmd或终端，导航到所需路径，并使用以下命令打开Jupyter笔记本：`jupyter notebook`
- en: 'Import the pandas library:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入pandas库：
- en: '[PRE0]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Use pandas to read the CSV file containing the dataset previously downloaded
    from the UC Irvine Machine Learning Repository site.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas读取先前从UC Irvine机器学习库站点下载的包含数据集的CSV文件。
- en: Next, drop the column named `date` as we do not want to consider it for the
    following exercises.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，删除名为`date`的列，因为我们不打算在接下来的练习中考虑它。
- en: 'Finally, print the head of the DataFrame:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，打印DataFrame的头部：
- en: '[PRE1]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Check for categorical features in your dataset:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据集中的分类特征：
- en: '[PRE2]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The resulting list is empty, which indicates that there are no categorical features
    to deal with.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果列表为空，这表明没有分类特征需要处理。
- en: 'Use Python''s `isnull()` and `sum()` functions to find out whether there are
    any missing values in each column of the dataset:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python的`isnull()`和`sum()`函数来查找数据集每一列中是否有缺失值：
- en: '[PRE3]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command counts the number of null values in each column. For the dataset
    in use, there should not be any missing values.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令计算每列中的空值数量。对于正在使用的数据集，不应该有任何缺失值。
- en: 'Use three standard deviations as the measure to detect outliers for all features
    in the dataset:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用三个标准差作为测量值，以检测数据集中所有特征的异常值：
- en: '[PRE4]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The resulting dictionary displays a list of all the features in the dataset,
    along with the percentage of outliers. From these results, it is possible to conclude
    that there is no need to deal with the outlier values, considering that they account
    for less than 5%.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果字典显示了数据集中所有特征的列表，以及异常值的百分比。从这些结果可以得出结论，由于异常值比例低于5%，因此无需处理。
- en: Congratulations! You have successfully explored the dataset and dealt with potential
    issues.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功探索了数据集并处理了潜在问题。
- en: Data Rescaling
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据重缩放
- en: Although data does not need to be rescaled to be fed to an algorithm for training,
    it is an important step to improve a model's accuracy. This is basically because
    having different scales for each feature may result in the model assuming a feature
    that is more important than others due to having higher numerical values.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据不需要重新缩放以供算法训练，但这是提高模型准确性的重要步骤。基本上是因为每个特征具有不同的比例可能会导致模型认为某个特征比其他特征更重要，因为它具有更高的数值。
- en: 'Take, for instance, two features: one measuring the number of kids a person
    has and another stating the age of the person. Even though the age feature may
    have higher numerical values, in a study for recommending schools, the number
    of kids feature may be more important.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，考虑两个特征：一个测量一个人有几个孩子，另一个说明这个人的年龄。尽管年龄特征可能有更高的数值，但在推荐学校的研究中，孩子数量特征可能更重要。
- en: Considering this, if all features are scaled equally, the model can actually
    give higher weights to those features that matter the most in respect to the target
    feature, and not the numerical values that they have. Moreover, it can also help
    accelerate the training process by removing the need for the model to learn from
    the invariance of the data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，如果所有特征被等比例缩放，模型实际上可以根据目标特征的重要性给予更高的权重，而不是它们具有的数值。此外，它还可以通过消除模型学习数据的不变性来加速训练过程。
- en: There are two main rescaling methodologies popular among data scientists, and
    although there is no rule for selecting one or the other, it is important to highlight
    that they are to be used individually (one or the other).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家中有两种主要的重新缩放方法，虽然没有选择其中一种的规则，但重要的是要强调它们应该单独使用（一种或另一种）。
- en: 'A brief explanation of both of these methodologies can be found here:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在这里找到这两种方法的简要说明：
- en: '**Normalization**: This consists of rescaling the values so that all values
    of all features are between zero and one, using the following equation:'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化**：这包括重新调整值，使得所有特征的所有值都在0到1之间，使用以下方程：'
- en: '![Figure 2.26: Data normalization'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.26：数据归一化'
- en: '](img/02_26.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_26.jpg)'
- en: 'Figure 2.26: Data normalization'
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.26：数据归一化
- en: '**Standardization**: In contrast, this rescaling methodology converts all values
    so that their mean is 0 and their standard deviation is equal to 1 using the following
    equation:'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准化**: 相反，这种缩放方法通过以下方程将所有值转换为均值为0、标准差为1。'
- en: '![Figure 2.27: Data standardization'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.27: 数据标准化'
- en: '](img/02_27.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_27.jpg)'
- en: 'Figure 2.27: Data standardization'
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图2.27: 数据标准化'
- en: 'Exercise 4: Rescaling Data'
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习4: 数据重新缩放'
- en: 'In this exercise, we will rescale the data from the previous exercise:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将对上一个练习中的数据进行重新缩放：
- en: Note
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Use the same Jupyter notebook that you used in the previous exercise.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与上一个练习中相同的Jupyter笔记本。
- en: 'Separate the features from the target. This is done to only rescale the features
    data:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征与目标分开。这样做是为了只对特征数据进行重新缩放：
- en: '[PRE5]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Rescale the features data by using the normalization methodology. Display the
    head of the resulting DataFrame to verify the result:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用归一化方法对特征数据进行重新缩放。显示结果DataFrame的前几行以验证结果:'
- en: '[PRE6]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Congratulations! You have successfully rescaled a dataset.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功地对数据集进行了重新缩放。
- en: Splitting the Data
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'The purpose of splitting the dataset into three subsets is so that the model
    can be trained, fine-tuned, and measured appropriately, without the introduction
    of bias. Here is an explanation of each set:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '将数据集分为三个子集的目的是，使模型可以适当地进行训练、微调和测量，而不引入偏差。以下是每个集合的解释:'
- en: '**Training set**: As its name suggests, this set is fed to the neural network
    to be trained. For supervised learning, it consists of the features and the target
    values. This is typically the largest set out of the three, considering that neural
    networks require large amounts of data to be trained, as mentioned previously.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**: 如其名称所示，此集合被馈送到神经网络进行训练。对于监督学习来说，它包括特征和目标值。考虑到神经网络需要大量数据进行训练，因此这通常是三个集合中最大的集合。'
- en: '**Validation set (dev set)**: This set is used mainly to measure the performance
    of the model in order to make adjustments to the hyperparameters to improve performance.
    This fine-tuning process is done to arrive at the configuration of hyperparameters
    that achieve the best results.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集（开发集）**: 主要用于衡量模型的性能，以便调整超参数以提高性能。这个微调过程是为了找到能够获得最佳结果的超参数配置。'
- en: Although the model is not trained on this data, it indirectly has an effect
    on it, which is why the final measure of performance should not be done on it,
    as it may be a biased measure.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管模型没有在这些数据上进行训练，但它间接影响了这些数据，这就是为什么不应该在这些数据上进行最终的性能评估，因为它可能会产生偏差。
- en: '**Testing set**: This set does not have an effect on the model, which is why
    it is used to perform a final evaluation of the model on unseen data, which becomes
    a guideline of how well the model will perform on future datasets.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**: 这个集合对模型没有影响，因此用于对未见数据进行最终评估，这成为衡量模型在未来数据集上表现的指导。'
- en: There is no actual science on the perfect ratio for splitting data into the
    three sets mentioned, considering that every data problem is different, and developing
    deep learning solutions usually requires a trial-and-error methodology. Nevertheless,
    it is widely known that larger datasets (hundreds of thousands and millions of
    instances) should have a split ratio of 98%/1%/1% for each set, considering that
    it is crucial to use as much data as possible for the training set. For a smaller
    dataset, the conventional split ratio is 60%/20%/20%.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 没有关于将数据分成所提到的三个集合的完美比例的实际科学，考虑到每个数据问题都不同，并且开发深度学习解决方案通常需要试错方法。尽管如此，众所周知，较大的数据集（数十万和数百万个实例）的分割比例应为98%/1%/1%，因为对于训练集来说，尽可能使用尽可能多的数据至关重要。对于较小的数据集，传统的分割比例是60%/20%/20%。
- en: 'Exercise 5: Splitting a Dataset'
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '练习5: 分割数据集'
- en: 'In this exercise, we will split the dataset from the previous exercise into
    three subsets. For the purpose of learning, we will explore two different approaches.
    First, the dataset will be split using indexing. Next, scikit-learn''s `train_test_split()`
    function will be used for the same purpose, achieving the same result with both
    approaches:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '在本练习中，我们将从上一个练习中的数据集中分割出三个子集。为了学习目的，我们将探索两种不同的方法。首先，将使用索引来分割数据集。接下来，将使用scikit-learn的`train_test_split()`函数来实现相同的目的，使用这两种方法都可以达到相同的结果:'
- en: Note
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Use the same Jupyter notebook that you used in the previous exercise.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与上一个练习中相同的Jupyter笔记本。
- en: Print the shape of the dataset in order to determine the split ration to be
    used.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据集的形状，以确定要使用的拆分比例。
- en: '[PRE7]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output from this operation should be `(19735, 28)`. This means that it is
    possible to use a split ratio of 60%/20%/20% for the training, validation, and
    testing sets.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此操作的输出应为`(19735, 28)`。这意味着可以使用60%/20%/20%的拆分比例用于训练、验证和测试集。
- en: 'Get the value to use as the bottom bound of the training and validation sets.
    This will be used to split the dataset using indexing:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取用作训练集和验证集底限的值。这将用于使用索引拆分数据集：
- en: '[PRE8]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Shuffle the dataset:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集进行洗牌：
- en: '[PRE9]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Use indexing to split the shuffled dataset into the three sets, for both the
    features and the target data:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用索引将洗牌后的数据集拆分为三个集合，分别用于特征和目标数据：
- en: '[PRE10]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Print the shapes of all three sets:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印所有三个集合的形状：
- en: '[PRE11]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result from the preceding operation should be as follows:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面操作的结果应如下所示：
- en: '[PRE12]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Import the `train_test_split()` function from scikit-learn''s `model_selection`
    module:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 的`model_selection`模块导入`train_test_split()`函数：
- en: '[PRE13]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Split the shuffled dataset:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拆分洗牌后的数据集：
- en: '[PRE14]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first line of code performs an initial split. The function takes as arguments
    both datasets to be split (X and Y), `test_size`, which is the percentage of instances
    to be contained in the test set, and `random_state` to ensure the reproducibility
    of the results. The result from this line of code is the division of each of the
    datasets (X and Y) into two subsets.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一行代码执行初始拆分。该函数接受两个要拆分的数据集（X 和 Y）、`test_size`（测试集中包含的实例百分比）以及`random_state`（确保结果可重现）。该代码的结果是将每个数据集（X
    和 Y）分为两个子集。
- en: In order to create an additional set (the validation set), we will perform a
    second split. The second line of the preceding code is in charge of determining
    the `test_size` to be used for the second split so that both the testing and validation
    sets have the same shape.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了创建一个额外的集合（验证集），我们将执行第二次拆分。前面代码的第二行负责确定用于第二次拆分的`test_size`，以便测试集和验证集具有相同的形状。
- en: Finally, the last line of code performs the second split, using the value calculated
    previously as the `test_size`.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，代码的最后一行执行第二次拆分，使用先前计算的值作为`test_size`。
- en: 'Print the shape of all three sets:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印所有三个集合的形状：
- en: '[PRE15]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The result from the preceding operation should be as follows:'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面操作的结果应如下所示：
- en: '[PRE16]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As can be seen, the resulting sets from both approaches have the same shapes.
    Using one approach or the other is a matter of preference.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以看到，两种方法得到的结果集具有相同的形状。使用其中一种方法还是另一种方法是一种个人偏好。
- en: Congratulations! You have successfully split the dataset into three subsets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功将数据集分割为三个子集。
- en: 'Activity 2: Performing Data Preparation'
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动2：执行数据准备
- en: For the following activity, we will prepare a dataset containing a list of songs,
    each with several attributes that help determine the year in which it was released.
    This data preparation step is crucial for subsequent activities within this chapter.
    Let's look at the following scenario.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的活动，我们将准备一个包含多个属性的歌曲列表数据集，这些属性有助于确定歌曲发布的年份。这一数据准备步骤对本章节内后续活动至关重要。让我们看看以下场景。
- en: You work at a music record company and they want to uncover the details that
    characterize records from different time periods, which is why they have put together
    a dataset that contains data on 515,345 records, with release years ranging from
    1922 to 2011\. They have tasked you with preparing the dataset so that it is ready
    to be fed to a neural network.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 您在音乐唱片公司工作，他们想要揭示区分不同时间段唱片的细节，因此他们已经整理了一个包含数据的数据集，该数据集包含了 515,345 条记录，发布年份从
    1922 年到 2011 年不等。他们委托您准备数据集，以便输入神经网络使用。
- en: Note
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'To download the dataset, visit the following UC Irvine Machine Learning Repository
    URL: [https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载数据集，请访问以下 UC Irvine Machine Learning Repository 的网址：[https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)
- en: Import the required libraries.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: Using pandas, load the text file. Since the previously downloaded text file
    has the same formatting as a CSV file, you can read it using the `read_csv()`
    function. Make sure to turn the header argument to `None`.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas，加载文本文件。由于先前下载的文本文件与 CSV 文件的格式相同，因此可以使用`read_csv()`函数进行读取。确保将头部参数设置为`None`。
- en: Verify whether any qualitative data is present in the dataset.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证数据集中是否存在任何定性数据。
- en: Check for missing values.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查缺失值。
- en: If you add an additional `sum()` function to the line of code previously used
    for this purpose, you will get the sum of missing values in the entire dataset,
    without discriminating by column.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您在先前用于此目的的代码行中添加额外的`sum()`函数，您将得到整个数据集中缺失值的总和，而不是按列区分。
- en: Check for outliers.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查异常值。
- en: Separate the features from the target data.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征与目标数据分开。
- en: Rescale the data using the standardization methodology.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准化方法重新调整数据。
- en: 'Split the data into three sets: training, validation, and testing. Use the
    approach of your preference.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成三组：训练集、验证集和测试集。使用您喜欢的方法。
- en: Note
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 188.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正解可以在第188页找到。
- en: Building a Deep Neural Network
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建深度神经网络
- en: Building a neural network, in general terms, can be achieved either on a very
    simple level, using libraries such as scikit-learn (not suitable for deep learning)
    that perform all the math for you, without much flexibility; or on a very complex
    level, by coding every single step of the training process from scratch, or by
    using a more robust framework, which allows great flexibility.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般角度来看，构建神经网络可以通过非常简单的方式实现，使用像 scikit-learn 这样的库（不适用于深度学习），它会为您完成所有数学运算，但灵活性不高；或者通过非常复杂的方式实现，从头编写训练过程的每一个步骤，或者使用更强大的框架，它可以在同一个地方允许两者的近似。如前所述，它有一个
    nn 模块，专门用于使用顺序容器轻松预定义简单架构的实现，同时允许创建引入灵活性的自定义模块，以构建非常复杂的架构过程。
- en: PyTorch, on the other hand, was built considering the input of many developers
    in the field and has the advantage of allowing both approximations in the same
    place. As mentioned previously, it has an nn module, built to allow easy predefined
    implementations of simple architectures using the sequential container, while
    at the same time allowing the creation of custom modules that introduce flexibility
    to the process of building very complex architectures.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，PyTorch 是考虑了领域内许多开发者的输入构建的，具有允许在同一地方进行两者近似的优势。如前所述，它有一个 nn 模块，专门用于使用顺序容器轻松预定义简单架构的实现，同时允许创建引入灵活性的自定义模块，以构建非常复杂的架构过程。
- en: In the present section, we will further discuss the use of the sequential container
    for developing deep neural networks, in order to demystify their complexity. Nevertheless,
    in later sections of this book, we will move on to explore more complex and abstract
    applications, which can also be achieved with very little effort.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步讨论使用顺序容器开发深度神经网络的使用，以揭开它们的复杂性。然而，在本书的后续章节中，我们将继续探讨更复杂和抽象的应用，这些应用也可以通过极少的努力实现。
- en: As mentioned previously, the sequential container is a module built to contain
    sequences of modules that follow an order. Each of the modules that it contains
    will apply some computation to a given input to arrive at an outcome.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，顺序容器是一个模块，用于包含按顺序跟随的模块序列。它包含的每个模块都会对给定的输入应用一些计算，以得出结果。
- en: 'Some of the most popular modules (layers) that can be used inside the sequential
    container to develop regular classification models are explained here:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在顺序容器内使用的一些最受欢迎的模块（层）以开发常规分类模型在这里进行解释：
- en: Note
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The modules used for other types of architectures, such as convolutional neural
    networks and recurrent neural networks, will be explained in subsequent chapters.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 用于其他类型架构（如卷积神经网络和循环神经网络）的模块将在接下来的章节中进行解释。
- en: '`True` by default) as arguments.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`True` 默认情况下）作为参数。'
- en: '`False` by default.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 默认情况下。'
- en: '**Tanh**: This applies the element-wise tanh function to the tensor containing
    the input data. It does not take any arguments.'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Tanh**：将逐元素 tanh 函数应用于包含输入数据的张量。它不接受任何参数。'
- en: '**Sigmoid**: This applies the previously explained sigmoid function to the
    tensor containing the input data. It does not take any arguments.'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Sigmoid**：将之前解释过的 sigmoid 函数应用于包含输入数据的张量。它不接受任何参数。'
- en: '**Softmax**: This applies the softmax function to an n-dimensional tensor containing
    the input data. The output is rescaled so that the elements of the tensor lie
    in a range between zero and one, and sum to one. It takes as argument the dimension
    along which the softmax function should be computed.'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Softmax**：将 softmax 函数应用于包含输入数据的 n 维张量。输出经过重新缩放，使得张量元素位于介于零到一之间的范围内，并且总和为一。它接受作为参数应计算
    softmax 函数的维度。'
- en: '`False` by default. This technique is commonly used for dealing with overfitted
    models, which will be further explained later.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`False` 默认情况下。这种技术通常用于处理过拟合模型，稍后将进一步解释。'
- en: '**Normalization layer**: There are different methodologies that can be used
    to add a normalization layer in the sequential container. Some of them are BatchNorm1d,
    BatchNorm2d, and BatchNorm3d. The idea behind this is to normalize the output
    from the previous layer, which ultimately achieves similar accuracy levels at
    lower training times.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化层**：有不同的方法可以用来在顺序容器中添加归一化层。其中一些方法包括BatchNorm1d、BatchNorm2d和BatchNorm3d。其背后的想法是对前一层的输出进行归一化，最终在较短的训练时间内达到类似的准确性水平。'
- en: 'Exercise 6: Building a Deep Neural Network Using PyTorch'
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6：使用PyTorch构建深度神经网络
- en: 'In this exercise, we will use the PyTorch library to define the architecture
    of a deep neural network of four layers, which then will be trained with the dataset
    prepared previously:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用PyTorch库定义一个四层深度神经网络的架构，然后使用之前准备好的数据集进行训练：
- en: Note
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: Use the same Jupyter notebook that you used in the previous exercise.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前练习中使用的相同的Jupyter笔记本。
- en: 'Import the PyTorch library, called `torch`, as well as the `nn` module from
    PyTorch:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从PyTorch库中导入所需的库，称为`torch`，以及来自PyTorch的`nn`模块：
- en: '[PRE17]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: Although the different packages and libraries are being imported as they are
    needed for practical learning purposes, it is always a good practice to import
    them at the beginning of your code.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管不同的包和库根据实际学习目的在需要时被导入，但是始终将它们导入到代码的开头是一个良好的实践。
- en: 'Separate the feature columns from the target, for each of the sets created
    in the previous exercise. Additionally, convert the final DataFrames into tensors:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从之前的练习中分离出特征列和目标列，对于每个创建的数据集。另外，将最终的数据框转换为张量：
- en: '[PRE18]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Define the network architecture using the `sequential()` container. Make sure
    to create a four-layer network.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sequential()`容器定义网络架构。确保创建一个四层网络。
- en: Use ReLU activation functions for the first three layers, and leave the last
    layer without an activation function, considering that we are dealing with a regression
    problem.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于前三层使用ReLU激活函数，并且考虑到我们处理的是回归问题，最后一层不使用激活函数。
- en: 'The number of units for each layer should be: 100, 50, 25, and 1:'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每层的单元数应为：100、50、25和1：
- en: '[PRE19]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the loss function as the mean squared error:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失函数定义为均方误差：
- en: '[PRE20]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the optimizer algorithm as the Adam optimizer:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器算法定义为Adam优化器：
- en: '[PRE21]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Use a `for` loop to train the network over the training data for 100 iteration
    steps:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`for`循环在训练数据上进行100次迭代步骤训练网络：
- en: '[PRE22]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To test out the model, perform a prediction on the first instance of the testing
    set, and compare it to the ground truth (target value):'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试模型，对测试集的第一个实例进行预测，并将其与基准值（目标值）进行比较：
- en: '[PRE23]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: From this result, it is possible to see that the model is not performing well
    considering that the target value greatly differs from the predicted value. In
    subsequent sections of this book, you will learn how to improve the performance
    of your model.
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这个结果可以看出，模型表现不佳，因为目标值与预测值差异很大。在本书的后续部分，您将学习如何提高模型的性能。
- en: Congratulations! You have successfully created and trained a deep neural network
    to solve a regression problem.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 祝贺！您已成功创建并训练了一个深度神经网络，以解决回归问题。
- en: 'Activity 3: Developing a Deep Learning Solution for a Regression Problem'
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动3：为回归问题开发深度学习解决方案
- en: 'For the following activity, we will create and train a four hidden layer neural
    network to solve the regression problem mentioned in the previous activity. Let''s
    look at the following scenario:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，我们将创建并训练一个四层隐藏层神经网络，以解决之前活动中提到的回归问题。让我们看一下以下的场景：
- en: 'You continue to work at the music record company and after seeing the great
    job you did in preparing the dataset, they have trusted you with the task of defining
    the network''s architecture and code, as well as training it with the prepared
    dataset:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 您继续在音乐唱片公司工作，看到您在准备数据集方面做得很出色后，他们信任您来定义网络的架构和代码，并使用准备好的数据集进行训练：
- en: Note
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: Use the same Jupyter notebook that you used in the previous activity.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前活动中使用的相同的Jupyter笔记本。
- en: Import the required libraries.
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: Split the features from the targets for all three sets of data created in the
    previous activity. Convert the DataFrames into tensors.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从之前活动中分离特征和目标，针对所有三组数据。将数据框转换为张量。
- en: Define the architecture of the network. Feel free to try different combinations
    of the number of layers and the number of units per layer.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络的架构。可以尝试不同的层数和每层的单元数组合。
- en: Define the loss function and the optimizer algorithm.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和优化算法。
- en: Use a `for` loop to train the network for 100 iteration steps.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`for`循环训练网络进行100次迭代。
- en: Test your model by performing a prediction on the first instance of the testing
    set and comparing it to the ground truth.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在测试集的第一个实例上进行预测并将其与真实值进行比较来测试您的模型。
- en: 'Your output should look similar to this:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 您的输出应该与以下类似：
- en: '![Figure 2.28: Output of the activity'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.28：活动的输出'
- en: '](img/02_28.jpg)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02_28.jpg)'
- en: 'Figure 2.28: Output of the activity'
  id: totrans-394
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.28：活动的输出
- en: Note
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 190.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这项活动的解决方案可以在第190页找到。
- en: Summary
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概要
- en: The theory that gave birth to neural networks was developed decades ago by Frank
    Rosenblatt. It started with the definition of the perceptron, a unit inspired
    by the human neuron, that takes data as an input to perform a transformation on
    it. It consisted of assigning weights to input data to perform a calculation,
    so that the end result would be either one thing or the other, depending on the
    outcome.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年前，由Frank Rosenblatt开发的理论孕育了神经网络。它始于感知器的定义，这是一个受人类神经元启发的单元，它接收数据作为输入并对其进行转换。它包括为输入数据分配权重以进行计算，以便最终结果要么是一种结果，要么是另一种，取决于结果。
- en: The most widely known form of neural networks is the one created from a succession
    of perceptrons, stacked together in layers, where the output from one column of
    perceptrons (layer) is the input for the following one.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络最广为人知的形式是由一系列感知器组成的，这些感知器堆叠在一起形成层，其中一列感知器（层）的输出是下一列的输入。
- en: 'According to this, the typical learning process for a neural network was explained.
    On this subject, there are three main process to consider: forward propagation,
    the calculation of the loss function, and backward propagation.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一点，解释了神经网络的典型学习过程。在这个主题上，有三个主要过程需要考虑：前向传播、损失函数的计算和反向传播。
- en: The end goal of this procedure is to minimize the loss function by updating
    the weights and biases that accompany each of the input values along the neural
    network. This is achieved through an iterative process that can take minutes,
    hours, or even weeks, depending on the nature of the data problem.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的最终目标是通过更新伴随着神经网络每个输入值的权重和偏差来最小化损失函数。这通过一个迭代过程实现，可能需要几分钟、几小时，甚至几周，具体取决于数据问题的性质。
- en: 'The main architecture of three types of neural networks was also discussed:
    the artificial neural network, the convolutional neural network, and the recurrent
    neural network. The first is used to solve traditional classification problems,
    the second one is widely popular for its capacity to solve computer vision problems
    (image classification), and the last one, capable of processing data in sequence,
    is useful for tasks such as language translation.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 三种类型的神经网络的主要架构也进行了讨论：人工神经网络、卷积神经网络和循环神经网络。第一种用于解决传统分类问题，第二种因其解决计算机视觉问题（图像分类）的能力而广受欢迎，而最后一种能够处理序列数据，对于诸如语言翻译之类的任务非常有用。
