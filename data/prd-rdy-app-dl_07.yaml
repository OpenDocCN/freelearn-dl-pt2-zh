- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Data Preparation in the Cloud
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云中的数据准备
- en: 'In this chapter, we will learn how data preparation can be set up in the cloud
    by leveraging various AWS cloud services. Considering the importance of **extract,
    transform, and load** (**ETL**) operations within data preparation, we will take
    a deeper look into setting up and scheduling ETL jobs in a cost-efficient manner.
    We will cover four different setups: ETL running on a single-node EC2 instance
    and an EMR cluster, and then utilizing Glue and SageMaker for ETL jobs. This chapter
    will also introduce Apache Spark, the most popular framework for ETL. By completing
    this chapter, you will be able to leverage the different advantages of the presented
    setups and select the right set of tools for your project.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何通过利用各种AWS云服务在云中设置数据准备。考虑到在数据准备中的抽取、转换和加载（ETL）操作的重要性，我们将深入研究如何以成本效益的方式设置和调度ETL作业。我们将涵盖四种不同的设置：在单节点EC2实例和EMR集群上运行ETL，然后利用Glue和SageMaker进行ETL作业。本章还将介绍Apache
    Spark，这是最流行的ETL框架。通过完成本章，您将能够利用所提供的不同设置的优势，并为项目选择合适的工具集。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Data processing in the cloud
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云中的数据处理
- en: Introduction to Apache Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark简介
- en: Setting up a single-node EC2 instance for ETL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置单节点EC2实例用于ETL
- en: Setting up an EMR cluster for ETL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置用于ETL的EMR集群
- en: Creating a Glue job for ETL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于ETL的Glue作业
- en: Utilizing SageMaker for ETL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用SageMaker进行ETL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can download the supplemental material for this chapter from this book’s
    GitHub repository: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的GitHub存储库下载本章的补充材料：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5)。
- en: Data processing in the cloud
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云中的数据处理
- en: The success of **deep learning** (**DL**) projects depends on the quality and
    the quantity of data. Therefore, the systems for data preparation must be stable
    and scalable enough to process terabytes and petabytes of data efficiently. This
    often requires more than a single machine; a cluster of machines running a powerful
    ETL engine must be set up for the data process so that it can store and process
    a large amount of data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）项目的成功取决于数据的质量和数量。因此，用于数据准备的系统必须稳定且可扩展，以有效地处理TB和PB级数据。这通常需要不止一台机器；必须设置一组运行强大的ETL引擎的机器集群，以便存储和处理大量数据。
- en: First, we would like to introduce ETL, the core concept in data processing in
    the cloud. Next, we will provide an overview of a distributed system setup for
    data processing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想介绍ETL，即云中数据处理的核心概念。接下来，我们将概述用于数据处理的分布式系统设置。
- en: Introduction to ETL
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ETL简介
- en: '*Throughout the ETL process, data will be collected from one or more sources,
    get transformed into different forms as necessary, and get saved in data storage.*
    In short, ETL itself covers the overall data processing pipeline. ETL interacts
    with three different types of data throughout: **structured**, **unstructured**,
    and **semi-structured**. While structured data represents a set of data with a
    schema (for example, a table), unstructured data does not have an explicit schema
    defined (for example, text, image, or PDF files). Semi-structured data has partial
    structures within the data itself (for example, HTML or emails).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*在整个ETL过程中，数据将从一个或多个来源收集，根据需要转换为不同的形式，并保存在数据存储中。* 简而言之，ETL本身涵盖了整个数据处理管道。ETL在整个过程中与三种不同类型的数据交互：**结构化**、**非结构化**和**半结构化**。结构化数据表示具有模式的数据集（例如表），非结构化数据没有明确定义的显式模式（例如文本、图像或PDF文件），而半结构化数据在数据本身内部具有部分结构（例如HTML或电子邮件）。'
- en: Popular ETL frameworks include **Apache Hadoop** ([https://hadoop.apache.org](https://hadoop.apache.org/)),
    **Presto** ([https://prestodb.io](https://prestodb.io/)), **Apache Flink** ([https://flink.apache.org](https://flink.apache.org/)),
    and **Apache Spark** ([https://spark.apache.org](https://spark.apache.org/)).
    Hadoop is one of the earliest data processing engines to take advantage of distributed
    processing. Presto is specialized in processing data in SQL, and Apache Flink
    is built to process streaming data. Out of these four frameworks, Apache Spark
    is the most popular tool as it can process every data type. *Apache Spark exploits
    in-memory data processing to increase its throughput* and provides much more scalable
    data processing solutions than Hadoop. Furthermore, it can easily be integrated
    with other ML and DL tools. For such reasons, we will mostly focus on Spark in
    this book.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的ETL框架包括**Apache Hadoop** ([https://hadoop.apache.org](https://hadoop.apache.org/))，**Presto**
    ([https://prestodb.io](https://prestodb.io/))，**Apache Flink** ([https://flink.apache.org](https://flink.apache.org/))和**Apache
    Spark** ([https://spark.apache.org](https://spark.apache.org/))。Hadoop是最早利用分布式处理优势的数据处理引擎之一。Presto专门用于处理SQL中的数据，而Apache
    Flink则专注于处理流数据。在这四个框架中，Apache Spark是最流行的工具，因为它可以处理各种数据类型。*Apache Spark利用内存数据处理来增加吞吐量*，并提供比Hadoop更可扩展的数据处理解决方案。此外，它可以轻松集成其他ML和DL工具。因此，我们在本书中主要关注Spark。
- en: Data processing system architecture
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理系统架构
- en: Setting up a system for data processing is not a trivial task because it involves
    procuring high-end machines periodically, linking various data processing software
    correctly, and making sure data is not lost when a failure occurs. Therefore,
    many companies utilize cloud services, a wide range of software services that
    are delivered on demand over the internet. While many companies provide various
    cloud services, **Amazon Web Services** (**AWS**) stands out the most with its
    stable and easy-to-use services.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为数据处理系统设置环境并不是一项简单的任务，因为它涉及定期获取高端机器、正确链接各种数据处理软件，并确保在发生故障时不丢失数据。因此，许多公司利用云服务，这是一种通过互联网按需提供的各种软件服务。虽然许多公司提供各种云服务，但**亚马逊云服务**
    (**AWS**)以其稳定且易于使用的服务脱颖而出。
- en: 'To give you a broader picture of how complex a data processing system can be
    in real life, let’s look at a sample system architecture based on AWS services.
    The *core component* of this system is open sourced *Apache Spark* carrying out
    the main ETL logic. A typical system also contains components for scheduling individual
    jobs, storing data, and visualizing the processed data:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您对现实生活中的数据处理系统有一个更广泛的了解，让我们看一个基于AWS服务的样例系统架构。这个系统的*核心组件*是开源的*Apache Spark*，执行主要的ETL逻辑。一个典型的系统还包括用于调度个别作业、存储数据和可视化处理后数据的组件：
- en: '![Figure 5.1 – A generic architecture for data processing pipelines'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 – 数据处理管道的通用架构'
- en: along with visualization and experimentation platforms
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 还包括可视化和实验平台
- en: '](img/B18522_05_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_01.jpg)'
- en: Figure 5.1 – A generic architecture for data processing pipelines along with
    visualization and experimentation platforms
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 数据处理管道的通用架构，同时包括可视化和实验平台
- en: 'Let’s look at each of these components:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个看看这些组件：
- en: '**Data Storage**: Data storage is responsible for keeping data and relevant
    metadata:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储**：数据存储负责保存数据和相关的元数据：'
- en: '**Hadoop Distributed File System** (**HDFS**): Open-sourced HDFS is a *distributed
    filesystem that can scale on demand* ([https://hadoop.apache.org](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html)).
    HDFS has been the traditional pick for data storage because Apache Spark and Apache
    Hadoop demonstrate the best performance on HDFS.'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop分布式文件系统** (**HDFS**)：开源的HDFS是一个*可按需扩展的分布式文件系统* ([https://hadoop.apache.org](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html))。HDFS一直是数据存储的传统选择，因为Apache
    Spark和Apache Hadoop在HDFS上表现最佳。'
- en: '**Amazon Simple Storage Service (S3)**: This is a *data storage service provided
    by AWS* ([https://aws.amazon.com/s3](https://aws.amazon.com/s3/)). S3 uses the
    concept of objects and buckets, where an object refers to individual files and
    a bucket refers to a container for objects. For each project or submodule, you
    can create a bucket and configure the permission differently for reading and writing
    operations. Buckets can also apply versioning to the data, keeping track of the
    changes.'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Simple Storage Service (S3)**: 这是AWS提供的*数据存储服务*（[https://aws.amazon.com/s3](https://aws.amazon.com/s3/)）。S3使用对象和存储桶的概念，其中对象指单个文件，存储桶指对象的容器。对于每个项目或子模块，您可以创建一个存储桶，并为读写操作配置不同的权限。存储桶还可以对数据应用版本控制，跟踪更改记录。'
- en: '`ml`, and costs are about 30 to 40% higher than the other EC2 instances ([https://aws.amazon.com/sagemaker/pricing](https://aws.amazon.com/sagemaker/pricing/)).
    In the *Utilizing SageMaker for ETL* section, we will describe how to set up a
    SageMaker for ETL process on EC2 instances.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ml`，成本大约比其他EC2实例高30%到40%（[https://aws.amazon.com/sagemaker/pricing](https://aws.amazon.com/sagemaker/pricing/)）。在*利用SageMaker进行ETL*部分，我们将描述如何在EC2实例上设置SageMaker进行ETL流程。'
- en: Considering the amount of data that needs to be processed, a correctly chosen
    ETL service, along with an appropriate data storage selection, can improve the
    pipeline’s efficiency significantly. The key factors to consider include the source
    of the data, the volume of the data, the available hardware resources, and scalability,
    to name a few.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到需要处理的数据量，正确选择的ETL服务以及适当的数据存储选择可以显著提高管道的效率。需要考虑的关键因素包括数据源、数据量、可用的硬件资源和可伸缩性等。
- en: '**Scheduling**: Often, ETL jobs must be periodically run (for example, daily,
    weekly, or monthly) and hence require a scheduler:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scheduling**: 经常需要定期运行ETL作业（例如每天、每周或每月），因此需要调度器：'
- en: '**AWS Lambda functions**: Lambda functions ([https://aws.amazon.com/lambda](https://aws.amazon.com/lambda/))
    are designed to run jobs on EMR without provisioning or managing infrastructure.
    Execution time can be configured dynamically; the job can run right away or can
    be scheduled to run at different times. *The AWS Lambda function runs the code
    in a serverless manner so that it does not require maintenance*. If there is any
    error during the execution, the EMR cluster will shut down automatically.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lambda functions**: Lambda函数（[https://aws.amazon.com/lambda](https://aws.amazon.com/lambda/)）旨在在EMR上运行作业，无需提供或管理基础设施。执行时间可以动态配置；作业可以立即运行，也可以计划在不同时间运行。*AWS
    Lambda函数以无服务器方式运行代码，因此无需维护*。如果执行期间出现错误，EMR集群将自动关闭。'
- en: '**Airflow**: Schedulers play an important role in automating the ETL process.
    Airflow ([https://airflow.apache.org](https://airflow.apache.org/)) *is one of
    the most popular scheduler frameworks used by data engineers*. Airflow’s **Directed
    Acyclic Graph** (**DAG**) can be used to schedule a pipeline periodically. Airflow
    is more common than AWS Lambda functions for running Spark jobs periodically because
    Airflow makes it easy to backfill the data when any of the preceding executions
    failed.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Airflow**: 调度器在自动化ETL过程中发挥重要作用。Airflow（[https://airflow.apache.org](https://airflow.apache.org/)）*是数据工程师使用的最流行的调度框架之一*。Airflow的**有向无环图**（**DAG**）可用于定期调度管道。Airflow比AWS
    Lambda函数更常见，用于定期运行Spark作业，因为Airflow在前面的执行失败时可以轻松地回填数据。'
- en: '**Build**: Build is the process of deploying a code package to an AWS computing
    resource (such as EMR or EC2) or setting up a set of AWS services based on pre-defined
    specifications:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Build**: Build是将代码包部署到AWS计算资源（如EMR或EC2）或根据预定义规范设置一组AWS服务的过程：'
- en: '**CloudFormation**: CloudFormation templates ([https://aws.amazon.com/cloudformation](https://aws.amazon.com/cloudformation/))
    *help provision cloud infrastructure as code*. CloudFormation typically does a
    particular task in setting up a system, such as creating an EMR cluster, preparing
    an S3 bucket with a particular specification, or terminating a running EMR cluster.
    It helps to standardize recurring tasks.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CloudFormation**: CloudFormation模板（[https://aws.amazon.com/cloudformation](https://aws.amazon.com/cloudformation/)）*帮助以代码形式配置云基础设施*。CloudFormation通常用于执行特定任务，比如创建EMR集群、准备具体规格的S3存储桶或终止正在运行的EMR集群。它有助于标准化重复性任务。'
- en: '**Jenkins**: Jenkins ([https://www.jenkins.io](https://www.jenkins.io/)) builds
    executables written in Java and Scala. We use *Jenkins to build Spark pipeline
    artifacts (for example, .jar files) and deploy them to EMR nodes*. Jenkins also
    makes use of CloudFormation templates to execute a task in a standardized way.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jenkins**：Jenkins ([https://www.jenkins.io](https://www.jenkins.io/)) 构建用Java和Scala编写的可执行文件。我们使用Jenkins构建Spark流水线工件（例如.jar文件）并部署到EMR节点。Jenkins还利用CloudFormation模板以标准化方式执行任务。'
- en: '**Database**: The key difference between data storage and databases is that
    databases are used to store structured data. Here, we will discuss two popular
    types of databases: *relational databases* and *key-value storage databases*.
    We will describe how they are different and explain appropriate use cases:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据库（Database）**：数据存储与数据库的关键区别在于数据库用于存储结构化数据。在这里，我们将讨论两种流行的数据库类型：*关系数据库*和*键-值存储数据库*。我们将描述它们的区别，并解释适当的使用案例。'
- en: '**Relational databases**: *Relational databases store structured data with
    a schema in table format*. The main benefit of storing data in a structured manner
    comes from data management; the value of the data being stored is strictly controlled,
    keeping the values in a consistent format. This allows the database to make additional
    optimizations when storing and retrieving particular sets of data. ETL jobs generally
    read the data from one or more data storage services, process the data, and store
    the processed data in relational databases such as **MySQL** ([https://www.mysql.com](https://www.mysql.com/)),
    and **PostgreSQL** ([https://www.postgresql.org](https://www.postgresql.org/)).
    AWS provides a relational database service as well: **Amazon RDS** ([https://aws.amazon.com/rds](https://aws.amazon.com/rds/)).'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系数据库（Relational databases）**：*关系数据库以表格格式存储带有模式的结构化数据*。以结构化方式存储数据的主要优点来自于数据管理；存储的数据值受到严格控制，保持值的一致格式。这使得数据库能够在存储和检索特定数据集时进行额外的优化。ETL作业通常从一个或多个数据存储服务中读取数据，处理数据，并将处理后的数据存储在关系数据库中，例如**MySQL**
    ([https://www.mysql.com](https://www.mysql.com/)) 和 **PostgreSQL** ([https://www.postgresql.org](https://www.postgresql.org/))。AWS还提供关系数据库服务，例如**Amazon
    RDS** ([https://aws.amazon.com/rds](https://aws.amazon.com/rds/))。'
- en: '**Key-value storage databases**: Unlike the traditional relational databases,
    these are *databases that are optimized for a high volume of read and write operations*.
    Such databases store data in a distinct key-value pair fashion. In general, data
    consists of a set of keys and a set of values that hold attributes for each key.
    Many of the databases support schemas, but their main advantage comes from the
    fact that they also support unstructured data. In other words, you can store any
    data, even though each of them has a different structure. Popular databases of
    this type include **Cassandra** ([https://cassandra.apache.org](https://cassandra.apache.org/))
    and **MongoDB** ([https://www.mongodb.com](https://www.mongodb.com/)). Interestingly,
    AWS provides a key-value storage database known as **DynamoDB** as a service ([https://aws.amazon.com/dynamodb](https://aws.amazon.com/dynamodb)).'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键-值存储数据库（Key-value storage databases）**：与传统的关系数据库不同，这些是*专为高频率读写操作优化的数据库*。这些数据库以独特的键-值对方式存储数据。一般来说，数据由一组键和一组值组成，每个键持有各自的属性。许多数据库支持模式，但它们的主要优势在于它们也支持非结构化数据。换句话说，您可以存储任何数据，即使每个数据具有不同的结构。这类数据库的流行例子包括**Cassandra**
    ([https://cassandra.apache.org](https://cassandra.apache.org/)) 和 **MongoDB**
    ([https://www.mongodb.com](https://www.mongodb.com/))。有趣的是，AWS提供了一个称为**DynamoDB**的键-值存储数据库服务
    ([https://aws.amazon.com/dynamodb](https://aws.amazon.com/dynamodb))。'
- en: '**Metastore**: In some cases, the initial set of data that’s collected and
    made available in data storage may not consist of any information about itself:
    for example, it may be missing column types or details about the source. Such
    information often helps engineers when they are managing and processing the data.
    Therefore, engineers have introduced the concept of the *metastore*, which *is
    a repository for metadata*. The metadata, which is stored as a table, provides
    the location, schema, and update history of the data it points to.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据存储库（Metastore）**：在某些情况下，最初收集和存储的数据集可能缺少关于自身的任何信息：例如，可能缺少列类型或关于数据源的详细信息。当工程师们管理和处理数据时，这些信息通常对他们有所帮助。因此，工程师们引入了*元数据存储库*的概念。这是一个存储元数据的仓库。存储为表格的元数据提供了数据指向的位置、模式以及更新历史。'
- en: In the case of AWS, **Glue Data Catalog** plays the role of metastore to provide
    built-in support for S3\. Hive ([https://hive.apache.org](https://hive.apache.org/)),
    on the other hand, is an open-sourced metastore for HDFS. The main advantage of
    Hive comes from data querying, summarization, and analysis, which comes naturally
    as it provides interaction based on SQL-like language.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS的情况下，**Glue Data Catalog** 充当元数据存储库的角色，为S3提供内置支持。而Hive ([https://hive.apache.org](https://hive.apache.org/))
    则是一个针对HDFS的开源元数据存储库。Hive的主要优势来自于数据查询、汇总和分析，这些功能天然支持基于类SQL语言的交互。
- en: '**Application programming interface** (**API**) **services**: *API endpoints
    allow data scientists and engineers to interact with the data efficiently*. For
    example, API endpoints can be set up to allow easy access to the data stored in
    the S3 bucket. Many frameworks have been designed for API services. For example,
    the **Flask API** ([https://flask.palletsprojects.com](https://flask.palletsprojects.com/))
    and **Django** ([https://www.djangoproject.com](https://www.djangoproject.com/))
    frameworks are based on Python, while the **Play** framework ([https://www.playframework.com](https://www.playframework.com/))
    is often used for projects in Scala.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序编程接口** (**API**) **服务**：*API端点允许数据科学家和工程师有效地与数据进行交互*。例如，可以设置API端点以便轻松访问存储在S3存储桶中的数据。许多框架专为API服务而设计。例如，**Flask
    API** ([https://flask.palletsprojects.com](https://flask.palletsprojects.com/))
    和 **Django** ([https://www.djangoproject.com](https://www.djangoproject.com/))
    框架基于Python，而 **Play** 框架 ([https://www.playframework.com](https://www.playframework.com/))
    则经常用于Scala项目。'
- en: '**Experimental platforms**: Evaluating system performance in production is
    often achieved by a popular user experience research methodology known as A/B
    testing. *By deploying two different versions of the system and comparing the
    user experiences, A/B testing allows us to understand whether the recent change
    have made a positive impact on the system or not*. In general, setting up A/B
    testing involves two components:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验平台**：在生产中评估系统性能通常通过一种称为A/B测试的流行用户体验研究方法来实现。*通过部署系统的两个不同版本并比较用户体验，A/B测试使我们能够了解最近的更改是否对系统产生了积极影响*。一般来说，设置A/B测试涉及两个组成部分：'
- en: '**Rest API**: *A Rest API provides greater flexibility in handling a request
    with different parameters and returning data in a processed manner*. Hence, it
    is common to set up a Rest API service that aggregates necessary data from databases
    or data storage for analytical purposes and provides data in JSON format to A/B
    experimentation platforms.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rest API**：*Rest API在处理带有不同参数的请求和返回经过处理的数据方面提供了更大的灵活性*。因此，通常会设置一个Rest API服务，从数据库或数据存储中聚合必要的数据以供分析目的，并以JSON格式提供数据给A/B实验平台。'
- en: '**A/B experimentation platform**: Data scientists often use an application
    with a **graphical user interface** (**GUI**) to schedule various A/B testing
    experiments and visualize the aggregated data intuitively for analysis. GrowthBook
    ([https://www.growthbook.io](https://www.growthbook.io/)) is an open source example
    of such a platform.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B实验平台**：数据科学家通常使用一个带有**图形用户界面** (**GUI**) 的应用程序安排各种A/B测试实验，并直观地可视化聚合数据进行分析。GrowthBook
    ([https://www.growthbook.io](https://www.growthbook.io/)) 是这类平台的开源示例。'
- en: '**Data visualization tools**: There are a few different teams and groups within
    a company (for example, marketing, sales, and executives), who can benefit from
    intuitively visualizing the data. Data visualization tools often support custom
    dashboard creation, which helps with the data analysis process. Tableau ([https://www.tableau.com](https://www.tableau.com/))
    is a popular tool among project leaders, but it’s proprietary software. On the
    other hand, Apache Superset ([https://superset.apache.org](https://superset.apache.org/))
    is an open-sourced data visualization tool that supports most of the standard
    databases. If the management cost is a concern, Apache Superset can be configured
    to read and plot visualizations using data stored in serverless databases such
    as AWS Athena ([https://aws.amazon.com/athena](https://aws.amazon.com/athena/)).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可视化工具**: 公司内的几个团队和组别（例如市场营销、销售和高管团队）可以从直观地可视化数据中受益。数据可视化工具通常支持创建自定义仪表板，有助于数据分析过程。Tableau
    ([https://www.tableau.com](https://www.tableau.com/)) 是项目领导者中广受欢迎的工具，但它是专有软件。另一方面，Apache
    Superset ([https://superset.apache.org](https://superset.apache.org/)) 是一款开源数据可视化工具，支持大多数标准数据库。如果担心管理成本，可以配置
    Apache Superset 来读取和绘制使用无服务器数据库（例如 AWS Athena ([https://aws.amazon.com/athena](https://aws.amazon.com/athena/))）存储的数据的可视化图表。'
- en: '**Identity Access Management** (**IAM**): IAM is a permission system that regulates
    access to AWS resources. Through IAM, it is possible to control a set of resources
    that users can access and a set of operations that they can conduct on the provided
    resources. More details about IAM can be found at [https://aws.amazon.com/iam](https://aws.amazon.com/iam).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份访问管理** (**IAM**): IAM 是一种权限系统，用于管理对 AWS 资源的访问。通过 IAM，可以控制用户可以访问的一组资源以及他们可以对提供的资源执行的一组操作。有关
    IAM 的更多详细信息，请访问 [https://aws.amazon.com/iam](https://aws.amazon.com/iam)。'
- en: Things to remember
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 记住的事情
- en: a. Throughout an ETL process, data will be collected from one or more sources,
    transformed into different forms as necessary, and get saved into data storage
    or a database.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: a. 在整个 ETL 过程中，数据将从一个或多个源收集，根据需要转换为不同的形式，并保存到数据存储或数据库中。
- en: 'b. Apache Spark is an open source ETL engine that’s widely used for processing
    large amounts of data of various types: structured, unstructured, and semi-structured.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: b. Apache Spark 是一款开源的 ETL 引擎，广泛用于处理各种类型的大量数据：结构化、非结构化和半结构化数据。
- en: c. A typical system that’s been set up for a data processing job consists of
    various components, including a data store, databases, ETL engines, data visualization
    tools, and experimental platforms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: c. 为数据处理作业设置的典型系统包括各种组件，包括数据存储、数据库、ETL 引擎、数据可视化工具和实验平台。
- en: d. ETL engines can run in several settings – on a single machine, a cluster,
    a fully managed ETL service in the cloud, and on end-to-end services designed
    for DL projects.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: d. ETL 引擎可以在多种设置中运行 - 单机器、集群、完全托管的云端 ETL 服务以及为深度学习项目设计的端到端服务。
- en: In the next section, we will cover key programming concepts in Apache Spark,
    the most popular tool for ETL.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍 Apache Spark 的关键编程概念，这是最流行的 ETL 工具。
- en: Introduction to Apache Spark
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 简介
- en: 'Apache Spark is an open-sourced data analytics engine that is used for data
    processing. The most popular use case is ETL. As an introduction to Spark, we
    will cover the key concepts surrounding Spark and some common Spark operations.
    Specifically, we will start by introducing **resilient distributed datasets**
    (**RDDs**) and DataFrames. Then, we will discuss Spark basics that you need to
    know about for ETL tasks: how to load a set of data from data storage, apply various
    transformations, and store the processed data. Spark applications can be implemented
    using multiple programming languages: Scala, Java, Python, and R. In this book,
    we will use Python so that we are aligned with the other implementations. The
    code snippets in this section can be found in this book’s GitHub repository: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/spark](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/spark).
    The datasets we will use in our examples include Google Scholar and the COVID
    datasets that we crawled in [*Chapter 2*](B18522_02.xhtml#_idTextAnchor034), *Data
    Preparation for Deep Learning* *Projects*, and another COVID dataset provided
    by the New York Times ([https://github.com/nytimes/covid-19-data](https://github.com/nytimes/covid-19-data)).
    We will refer to the last dataset as NY Times COVID.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个开源的数据分析引擎，用于数据处理。最流行的用例是 ETL。作为 Spark 的介绍，我们将涵盖围绕 Spark 的关键概念以及一些常见的
    Spark 操作。具体来说，我们将从介绍**弹性分布式数据集**（**RDDs**）和 DataFrames 开始。然后，我们将讨论用于 ETL 任务的 Spark
    基础知识：如何从数据存储加载数据集合，应用各种转换，并存储处理后的数据。Spark 应用可以使用多种编程语言实现：Scala、Java、Python 和 R。在本书中，我们将使用
    Python，以便与其他实现保持一致。本节中的代码片段可以在本书的 GitHub 代码库中找到：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/spark](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/spark)。我们在示例中使用的数据集包括
    Google Scholar 和我们在 [*第二章*](B18522_02.xhtml#_idTextAnchor034) *深度学习项目的数据准备* 中爬取的
    COVID 数据集，以及由纽约时报提供的另一个 COVID 数据集（[https://github.com/nytimes/covid-19-data](https://github.com/nytimes/covid-19-data)）。我们将最后一个数据集称为
    NY Times COVID。
- en: Resilient distributed datasets and DataFrames
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性分布式数据集和 DataFrames
- en: The unique advantage of Spark comes from RDDs, immutable distributed collections
    of data objects. By exploiting RDDs, Spark can efficiently process data that exploits
    parallelism. The built-in parallel processing of Spark operating on RDDs helps
    with data processing, even when one or more of its processors fails. When a Spark
    job is triggered, the RDD representation of the input data gets split into multiple
    partitions and distributed to each node for transformations, maximizing the throughput.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 的独特优势来自于 RDDs，即不可变的分布式数据对象集合。通过利用 RDDs，Spark 能够高效处理利用并行性的数据。Spark 内置的操作基于
    RDDs 的并行处理有助于数据处理，即使其中一个或多个处理器失败。当触发 Spark 作业时，输入数据的 RDD 表示会被分割成多个分区，并分发到每个节点进行转换，从而最大化吞吐量。
- en: Like pandas DataFrames, Spark also has the concept of DataFrames, which represent
    tables in a relational database with named columns. A DataFrame is also an RDD,
    so the operations that we describe in the next section can be applied as well.
    A DataFrame can be created from data structured as tables, such as CSV data, a
    table in Hive, or existing RDDs. DataFrames come with schemas that an RDD does
    not provide. As a result, an RDD is used for unstructured and semi-structured
    data, while a DataFrame is used for structured data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 pandas 的 DataFrames，Spark 也有 DataFrames 的概念，它们表示关系数据库中的表，具有命名列。DataFrame
    也是一个 RDD，因此我们在下一节描述的操作也可以应用于它们。DataFrame 可以从结构化为表格的数据创建，例如 CSV 数据、Hive 中的表或现有的
    RDDs。DataFrame 包含 RDD 不提供的模式。因此，RDD 用于非结构化和半结构化数据，而 DataFrame 用于结构化数据。
- en: Converting between RDDs and DataFrames
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 RDDs 和 DataFrames 之间转换
- en: 'The first step for any Spark operation is to create a `SparkSession` object.
    Specifically, the `SparkSession` module from `pyspark.sql` is used to create a
    `SparkSession` object. The `getOrCreate` function from the module is used to create
    the session object, as shown here. A `SparkSession` object is the entry point
    of a Spark application. It provides a way to interact with the Spark application
    under different contexts, such as the Spark context, Hive context, and SQL context:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 Spark 操作的第一步是创建一个 `SparkSession` 对象。具体来说，使用 `pyspark.sql` 中的 `SparkSession`
    模块创建 `SparkSession` 对象。如下所示，使用该模块中的 `getOrCreate` 函数创建会话对象。`SparkSession` 对象是
    Spark 应用程序的入口点。它提供了在不同上下文（如 Spark 上下文、Hive 上下文和 SQL 上下文）下与 Spark 应用程序交互的方式：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Converting an RDD into a DataFrame is simple. Given that an RDD does not have
    any schema, you can create a DataFrame without any schema, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将RDD转换为DataFrame很简单。鉴于RDD没有任何模式，因此可以如下创建一个没有模式的DataFrame：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To convert an RDD into a DataFrame with a schema, you need to use the `StructType`
    class, which is part of the `pyspark.sql.types` module. Once a schema has been
    created using the `StructType` method, the `createDataFrame` method of the Spark
    session object can be used to convert an RDD into a DataFrame:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要将RDD转换为具有模式的DataFrame，您需要使用`pyspark.sql.types`模块中的`StructType`类。一旦使用`StructType`方法创建了模式，就可以使用Spark会话对象的`createDataFrame`方法将RDD转换为DataFrame：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have learned how to set up a Spark environment in Python, let’s
    learn how to load a dataset as an RDD or a DataFrame.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会如何在Python中设置Spark环境，让我们学习如何将数据集加载为RDD或DataFrame。
- en: Loading data
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'Spark can load data of different formats that’s stored in various forms of
    data storage. Loading data stored in CSV format is a basic operation of Spark.
    This can easily be achieved using the `spark_session.read.csv` function. It reads
    a CSV file located locally or in the cloud, such as in an S3 bucket, as a DataFrame.
    In the following code snippet, we are loading Google Scholar data stored in S3\.
    The `header` option can be used to indicate that the CSV file has a header:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 'Spark可以加载存储在各种数据存储中的不同格式的数据。加载存储在CSV格式中的数据是Spark的基本操作。可以使用`spark_session.read.csv`函数轻松实现这一点。它将本地或云端（如S3桶中）的CSV文件读取为DataFrame。在下面的代码片段中，我们正在加载存储在S3中的Google
    Scholar数据。可以使用`header`选项指示CSV文件包含标题行：  '
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following figure shows the results of `df_gs.show(n=3)`. The `show` function
    prints the first *n* rows, along with the column headings:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了`df_gs.show(n=3)`的结果。`show`函数打印了前*n*行以及列标题：
- en: '![Figure 5.2 – A sample DataFrame created by loading a CSV file'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 – 通过加载CSV文件创建的样本DataFrame'
- en: '](img/B18522_05_02.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_02.jpg)'
- en: Figure 5.2 – A sample DataFrame created by loading a CSV file
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 通过加载CSV文件创建的样本DataFrame
- en: 'Similarly, a JSON file from data storage can be read using the `read.json`
    function of the `SparkSession` module:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，可以使用`SparkSession`模块的`read.json`函数读取数据存储中的JSON文件：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the next section, we will learn how to process loaded data using Spark operations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用Spark操作处理加载的数据。
- en: Processing data using Spark operations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark操作处理数据
- en: Spark provides a set of operations that transforms an RDD into an RDD of a different
    structure. Implementing a Spark application is the process of chaining a set of
    Spark operations on an RDD to transform the data into the target format. In this
    section, we will discuss the most commonly used – that is, `filter`, `map`, `flatMap`,
    `reduceByKey`, `take`, `groupBy`, and `join`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了一组操作，可以将RDD转换为不同结构的RDD。实现Spark应用程序是在RDD上链接一组Spark操作以将数据转换为目标格式的过程。在本节中，我们将讨论最常用的操作
    – 即`filter`、`map`、`flatMap`、`reduceByKey`、`take`、`groupBy`和`join`。
- en: filter
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤器
- en: 'In most cases, filters are often applied first to drop unnecessary data. Applying
    the `filter` method to a DataFrame can help you choose the rows of interest from
    the given DataFrame. In the following code snippet, we are using this method to
    only keep the rows where `research_interest` is not `None`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，通常首先应用过滤器以丢弃不必要的数据。对DataFrame应用`filter`方法可以帮助您从给定的DataFrame中选择感兴趣的行。在下面的代码片段中，我们使用这种方法仅保留`research_interest`不为`None`的行：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: map
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: map
- en: 'Like the `map` function in other programming languages, the `map` operation
    in Spark applies the given function to each data entry. Here, we are using the
    `map` function to only keep the `research_interest` column:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他编程语言中的`map`函数类似，Spark中的`map`操作将给定函数应用于每个数据条目。在这里，我们使用`map`函数仅保留`research_interest`列：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: flatMap
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: flatMap
- en: 'The `flatMap` function flattens the RDD after applying the given function to
    every entry and returns the new RDD. In this example, the `flatMap` operation
    splits each data entry with the `##` separator and then creates a pair of `research_interest`
    and a default frequency with a value of `1`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap`函数在对每个条目应用给定函数后展开RDD，并返回新的RDD。在本例中，`flatMap`操作使用`##`分隔符拆分每个数据条目，然后创建具有值为`1`的默认频率的`research_interest`对：'
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: reduceByKey
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: reduceByKey
- en: '`reduceByKey` groups the input RDD based on its key. Here, we are using `reduceByKey`
    to sum the frequencies to understand the number of occurrences for each `research_interest`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`基于其键对输入RDD进行分组。在这里，我们使用`reduceByKey`来对频率进行求和，以了解每个`research_interest`的出现次数：'
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: take
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`take`'
- en: 'One of the basic operations of Spark is `take`. This function is used to get
    the first *n* elements from an RDD:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的基本操作之一是`take`。此函数用于从RDD中获取前*n*个元素：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Grouping operations
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分组操作
- en: The idea of grouping is to collect identical data entries within a DataFrame
    into groups and perform aggregation (for example, average or summation) on the
    groups.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 分组的概念是将DataFrame中相同的数据条目收集到组中，并对这些组执行聚合（例如平均值或求和）。
- en: 'As an example, let’s employ the Moderna COVID dataset to get the average number
    of doses allocated per jurisdiction (state) using the `groupby` operation. Here,
    we are using the `sort` function to sort the state-wise average number of doses.
    The `toDF` and `alias` functions can help add a name for the new DataFrame:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们使用Moderna COVID数据集通过`groupby`操作获取每个司法管辖区（州）分配的平均剂量数。在这里，我们使用`sort`函数对州级平均剂量进行排序。`toDF`和`alias`函数可以帮助为新DataFrame添加名称：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'While applying `groupby`, multiple aggregations (`sum` and `avg`) can be applied
    in a single command. The columns that get created from aggregated functions such
    as `F.avg` or `F.sum` can be renamed using `alias`. In the following example,
    aggregations are being performed on the Moderna COVID dataset to get the average
    number and sum of the first and second doses:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用`groupby`时，可以在单个命令中应用多个聚合（`sum`和`avg`）。从聚合函数（如`F.avg`或`F.sum`）创建的列可以使用`alias`重命名。在以下示例中，正在对Moderna
    COVID数据集执行聚合操作，以获取第一剂和第二剂的平均数和总数：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The calculation is performed at the state level using the `groupby` function.
    This dataset contains 63 states in total, including certain entities (federal
    agencies) as a state.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`groupby`函数在州级别执行计算。该数据集总共包含63个州，包括某些实体（联邦机构）作为州。
- en: join
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`join`'
- en: The `join` functionality helps combine rows from two DataFrames.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`join`功能有助于组合来自两个DataFrame的行。'
- en: 'To demonstrate how `join` can be used, we will join the Moderna COVID dataset
    with the NY Times COVID dataset. Before we explain any `join` operations, we must
    apply aggregation to the NY Times COVID dataset, just like how we processed the
    Moderna COVID dataset previously. In the following code snippet, the `groupby`
    operation is being applied at the state level to get the aggregated (`sum`) value
    representing the total number of deaths and the total number of cases:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何使用`join`，我们将Moderna COVID数据集与NY Times COVID数据集进行连接。在解释任何`join`操作之前，我们必须像之前处理Moderna
    COVID数据集一样，在NY Times COVID数据集上应用聚合。在以下代码片段中，正在应用`groupby`操作以州级别获取聚合（`sum`）值，代表总死亡人数和总病例数：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Figure 5.3* shows the results of the `df_cases.show(n=3)` operation, which
    visualizes the top three rows of the processed DataFrame:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*Figure 5.3* 显示了`df_cases.show(n=3)`操作的结果，可视化处理后DataFrame的前三行：'
- en: '![Figure 5.3 – The top three rows of the aggregated results'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.3 – 使用df_inner.show(n=3)操作的输出结果'
- en: '](img/B18522_05_03.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.3 – 聚合结果的前三行'
- en: Figure 5.3 – The top three rows of the aggregated results
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 5.3 – 可视化处理后DataFrame的前三行结果
- en: 'We are now ready to demonstrate the two types of join: equi-join and left join.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备演示两种类型的连接：equi-join和左连接。
- en: Equi-join (inner-join)
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Equi-join（内连接）
- en: Equi-join, also called an inner-join, is the default `join` operation in Spark.
    An inner join is used to join two DataFrames on common column values. The rows
    where the keys don’t match will get dropped in the final DataFrame. In this example,
    equi-join will be applied to the `state` column as a common column between the
    Moderna COVID dataset and the NY Times COVID dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`Equi-join`，也称为内连接，是 Spark 中默认的`join`操作。内连接用于在两个DataFrame之间基于共同列值进行连接。在最终的DataFrame中，键不匹配的行将被丢弃。在本例中，将应用equi-join到`state`列，作为Moderna
    COVID数据集和NY Times COVID数据集之间的共同列。'
- en: 'The first step is to create aliases for the DataFrames using `alias`. Then,
    we call the `join` function on one DataFrame while passing the other DataFrame
    that defines the column relationship and the type of join:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是使用`alias`为DataFrame创建别名。然后，在一个DataFrame上调用`join`函数，同时传递另一个DataFrame来定义列关系和连接类型：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following is the output of the `df_inner.show(n=3)` operation:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`df_inner.show(n=3)`操作的输出：
- en: '![Figure 5.4 – The output of using the df_inner.show(n=3) operation'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.4 – 使用df_inner.show(n=3)操作的输出结果'
- en: '](img/B18522_05_04.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_04.jpg)'
- en: Figure 5.4 – The output of using the df_inner.show(n=3) operation
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 使用`df_inner.show(n=3)`操作的输出
- en: Now, let’s look at the other type of join, left join.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一种类型的连接，左连接。
- en: Left join
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 左连接
- en: A left join is another popular `join` operation for data analysis. A left join
    returns all the rows from one DataFrame, regardless of the matches found on the
    other DataFrame. When the `join` expression does not match, it assigns `null`
    for the missing entries.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 左连接是另一种用于数据分析的`join`操作。左连接返回来自一个DataFrame的所有行，不管在另一个DataFrame上是否找到匹配项。当`join`表达式不匹配时，它会为缺失的条目分配`null`。
- en: 'The left join syntax is like that of equi-join. The only difference is that
    you need to pass the `left` keyword when specifying the join type instead of `inner`.
    The left join takes all the values of the mentioned column (`df_m.state`) in the
    first DataFrame mentioned (`df_m`). Then, it tries to match entries with the DataFrame
    mentioned second (`df_ny`) on the column mentioned (`df_ny.state`). In this example,
    if a particular state appears on both DataFrames, the output of the `join` operation
    will be the state, along with values from both DataFrames. If a particular state
    is only available in the first DataFrame (`df_m`) but not in the second (`df_ny`),
    then it will add the state with the values for the first DataFrame only, keeping
    the other entry as `null`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 左连接语法类似于等值连接。唯一的区别在于，在指定连接类型时，您需要使用`left`关键字而不是`inner`。左连接获取第一个DataFrame（`df_m`）中提到的指定列（`df_m.state`）的所有值。然后，它试图在第二个提到的DataFrame（`df_ny`）中的指定列（`df_ny.state`）上匹配条目。在本例中，如果某个特定状态在两个DataFrame中都出现，则`join`操作的输出将是该状态及来自两个DataFrame的值。如果某个特定状态仅在第一个DataFrame（`df_m`）中可用，而不在第二个DataFrame（`df_ny`）中，则它将添加该状态及第一个DataFrame的值，保留其他条目为`null`：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output of `df_left.show(n=3`) is shown here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`df_left.show(n=3`)命令的输出如下所示：
- en: '![Figure 5.5 – The output of the df_inner.show(n=3) operation'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5 – 使用`df_inner.show(n=3)`操作的输出'
- en: '](img/B18522_05_05.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_05.jpg)'
- en: Figure 5.5 – The output of the df_inner.show(n=3) operation
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 使用`df_inner.show(n=3)`操作的输出
- en: Even though Spark provides a wide range of operations that cover vastly different
    cases, you may find building a custom operation more useful due to the complexity
    of your logic.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Spark提供了广泛的操作来涵盖不同的用例，但由于逻辑复杂性，构建自定义操作可能更有用。
- en: Processing data using user-defined functions
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用用户定义函数处理数据
- en: A **user-defined function** (**UDF**) *is a reusable custom function that performs
    a transformation on an RDD*. A UDF function can be reused on several DataFrames.
    In this section, we will provide a complete code example for processing the Google
    Scholar dataset using UDF.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**用户定义函数**（**UDF**）*是一种可重复使用的自定义函数，用于对RDD执行转换*。UDF函数可以在多个DataFrame上重复使用。在本节中，我们将提供一个完整的代码示例，用于使用UDF处理Google
    Scholar数据集。'
- en: 'First of all, we would like to introduce the `pyspark.sql.function` module,
    which allows you to define a UDF with the `udf` method and provides various column-wise
    operations. `pyspark.sql.function` also includes functions for aggregations such
    as `avg` or `sum` for computing the average and total, respectively:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想介绍`pyspark.sql.function`模块，它允许您使用`udf`方法定义UDF，并提供各种基于列的操作。`pyspark.sql.function`还包括用于聚合的函数，如用于计算平均值和总和的`avg`或`sum`：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the Google Scholar dataset, `data_science`, `artificial_intelligence`, and
    `machine_learning` all refer to the same field of `research_interest` data and
    check if any of the data can be categorized as AI. If matches are found, it puts
    a value of `1` in a new column. It will assign `0` otherwise. The results of the
    UDF are stored in a new column called `is_artificial_intelligence` using the `withColumn`
    method. In the following code snippet, the `@F.udf` annotation informs Spark that
    the function is a UDF. The `col` method from `pyspark.sql.functions` is often
    used to pass a column as an argument for UDF. Here, `F.col("research_interest")`
    has been passed to the UDF `is_ai` method, indicating which column that UDF should
    operate on:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Scholar数据集中，`data_science`，`artificial_intelligence`和`machine_learning`都指向相同的`research_interest`数据领域，并检查是否可以将任何数据归类为AI。如果找到匹配项，则在新列中放置`1`值。否则，将分配`0`。UDF的结果使用`withColumn`方法存储在名为`is_artificial_intelligence`的新列中。在以下代码片段中，`@F.udf`注解通知Spark该函数是一个UDF。`pyspark.sql.functions`中的`col`方法经常用于将列作为UDF的参数传递。在这里，`F.col("research_interest")`已传递给UDF
    `is_ai`方法，指示该UDF应操作的列：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After processing the raw data, we want to store it in data storage so that we
    can reuse it for other purposes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理原始数据后，我们希望将其存储在数据存储中，以便可以为其他目的重复使用。
- en: Exporting data
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导出数据
- en: In this section, we will learn how to save a DataFrame into an S3 bucket. In
    the case of RDD, it must be converted into a DataFrame to be saved appropriately.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何将 DataFrame 保存到 S3 存储桶中。对于 RDD，必须将其转换为 DataFrame 才能适当保存。
- en: Typically, data analysts want to write the aggregated data as a CSV file for
    the following operations. To export a DataFrame as a CSV file, you must use the
    `df.write.csv` function. In the case of text values, we recommend that you use
    `option("quoteAll", True)`, which will encapsulate each value with quotes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据分析师希望将聚合数据写入 CSV 文件以进行后续操作。要将 DataFrame 导出为 CSV 文件，必须使用 `df.write.csv`
    函数。对于文本值，建议使用 `option("quoteAll", True)`，这将用引号括起每个值。
- en: 'In the following example, we are providing an S3 path to generate a CSV file
    in an S3 bucket. `coalesce(1)` is used to write a single CSV file instead of multiple
    CSV files:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们提供了一个 S3 路径来生成 S3 存储桶中的 CSV 文件。使用 `coalesce(1)` 以写入单个 CSV 文件而不是多个 CSV
    文件：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If you want to save the DataFrame as a JSON file, you can use `write.json`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要将 DataFrame 保存为 JSON 文件，可以使用 `write.json`：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: At this point, you should see that a file is stored in the S3 bucket.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该看到一个文件已存储在 S3 存储桶中。
- en: Things to remember
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 需记住的事项
- en: a. An RDD is an immutable distributed collection of sets that gets split into
    multiple partitions and computed in different nodes of a cluster.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: a. RDD 是一个不可变的分布式集合，被分割成多个分区并在集群的不同节点上计算。
- en: b. A Spark DataFrame is equivalent to a table in a relational database with
    named columns.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: b. Spark DataFrame 相当于关系数据库中的表，具有命名列。
- en: c. Spark provides a set of operations that transforms an RDD into an RDD that
    has a different structure. Implementing a Spark application is the process of
    chaining a set of Spark operations on an RDD to transform the data into the target
    format. You can build a custom Spark operation using UDF.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: c. Spark 提供了一系列操作，可以将一个 RDD 转换为具有不同结构的另一个 RDD。实现 Spark 应用程序是在 RDD 上链接一系列 Spark
    操作，将数据转换为目标格式的过程。您可以使用 UDF 构建自定义的 Spark 操作。
- en: In this section, we described the basics of Apache Spark, which is the most
    common tool for ETL. Starting from the next section, we will talk about how to
    set up a Spark job in the cloud for ETL. First, let's look at how to run ETL on
    a single EC2 instance.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了 Apache Spark 的基础知识，这是最常用的 ETL 工具。从下一节开始，我们将讨论如何在云中为 ETL 设置 Spark
    作业。首先，让我们看看如何在单个 EC2 实例上运行 ETL。
- en: Setting up a single-node EC2 instance for ETL
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 ETL 设置单节点 EC2 实例
- en: 'EC2 instances can have various combinations of CPU/GPU, memory, storage, and
    network capacity. You can find configurable options for EC2 in the official documentation:
    [https://aws.amazon.com/ec2/instance-types](https://aws.amazon.com/ec2/instance-types).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: EC2 实例可以具有各种 CPU/GPU、内存、存储和网络容量的组合。您可以在官方文档中找到 EC2 的可配置选项：[https://aws.amazon.com/ec2/instance-types](https://aws.amazon.com/ec2/instance-types)。
- en: 'When creating an EC2 instance, you can choose a Docker image to run which has
    been predefined for various projects. These are called **Amazon Machine Images**
    (**AMIs**). For example, there’s an image with TF version 2 installed for DL projects
    and an image with Anaconda set up for generic ML projects, as shown in the following
    screenshot. For the complete list of AMIs, please refer to [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 EC2 实例时，可以选择预定义的 Docker 镜像来运行各种项目。这些称为**Amazon Machine Images**（**AMIs**）。例如，有一个安装了
    TF 版本 2 用于 DL 项目的镜像，以及一个为通用 ML 项目设置了 Anaconda 的镜像，如下截图所示。有关完整的 AMI 列表，请参阅 [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)：
- en: '![Figure 5.6 – Selecting an AMI for an EC2 instance'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6 – 选择 EC2 实例的 AMI'
- en: '](img/B18522_05_06.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_06.jpg)'
- en: Figure 5.6 – Selecting an AMI for an EC2 instance
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 选择 EC2 实例的 AMI
- en: AWS offers **Deep Learning AMIs** (**DLAMIs**), which are AMIs that are created
    for DL projects; images utilize different CPU and GPU configurations and different
    compute architectures ([https://docs.aws.amazon.com/dlami/latest/devguide/options.html](https://docs.aws.amazon.com/dlami/latest/devguide/options.html)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 提供了用于 DL 项目的**Deep Learning AMIs**（**DLAMIs**），这些 AMIs 是专为 DL 项目创建的；这些镜像利用不同的
    CPU 和 GPU 配置以及不同的计算架构（[https://docs.aws.amazon.com/dlami/latest/devguide/options.html](https://docs.aws.amazon.com/dlami/latest/devguide/options.html)）。
- en: 'As mentioned in [*Chapter 1*](B18522_01.xhtml#_idTextAnchor014), *Effective
    Planning of Deep Learning-Driven Projects*, many data scientists make use of EC2
    instances to develop their algorithms, exploiting the flexibility in dynamic resource
    allocation. The steps for creating an EC2 instance and installing Spark are as
    follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [*第 1 章*](B18522_01.xhtml#_idTextAnchor014) 中提到的 *深度学习驱动项目的有效规划*，许多数据科学家利用
    EC2 实例开发他们的算法，利用动态资源分配的灵活性。创建 EC2 实例并安装 Spark 的步骤如下：
- en: Create a **Virtual Private Network** (**VPN**) to restrict access to the EC2
    instance for security purposes.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 **虚拟专用网络**（**VPN**），以限制访问 EC2 实例以增强安全性。
- en: Create a `.pem` key with an EC2 key pair. A `.pem` file is used to perform authentication
    when a user attempts to log into the EC2 instance from a terminal.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 EC2 密钥对创建一个 `.pem` 密钥。 `.pem` 文件用于在用户尝试从终端登录 EC2 实例时执行身份验证。
- en: Create an EC2 instance from a Docker image with the necessary tools and packages.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包含所需工具和包的 Docker 镜像创建 EC2 实例。
- en: Add an inbound rule that enables access to the new instances from your local
    terminal.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个入站规则，允许从本地终端访问新实例。
- en: Use SSH to access the EC2 instance with the `.pem` file that was created in
    *Step 2*.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 SSH 访问 EC2 实例，并使用在 *第 2 步* 中创建的 `.pem` 文件。
- en: Initiate the Spark shell.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Spark shell。
- en: We have included detailed descriptions for each step, along with screenshots,
    at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/ec2](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/ec2).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个步骤提供了详细的描述和屏幕截图，位于 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/ec2](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/ec2)。
- en: Things to remember
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事情
- en: a. An EC2 instance can have various combinations of CPU/GPU, memory, storage,
    and network capacity
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: a. 一个 EC2 实例可以具有不同的 CPU/GPU、内存、存储和网络容量组合。
- en: b. An EC2 instance can be created from a predefined Docker image (AMI) with
    a couple of clicks on the AWS web console
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: b. 可以从 AWS Web 控制台上的预定义 Docker 镜像（AMI）中创建 EC2 实例。
- en: Next, we will learn how to set up a cluster that runs a set of Spark workers
    as a group.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何设置一个运行一组 Spark 工作节点的集群。
- en: Setting up an EMR cluster for ETL
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 ETL 设置 EMR 集群
- en: 'In the case of DL, the computational power of a single EC2 instance may not
    be sufficient for model training or data processing. Therefore, a group of EC2
    instances is often put together to increase the throughput. AWS has a dedicated
    service for this purpose: **Amazon Elastic MapReduce** (**EMR**). It is a fully
    managed cluster platform that provides distributed systems for big data frameworks
    such as Apache Spark and Hadoop. In general, an EMR cluster that’s been set up
    for ETL reads data from AWS storage (Amazon S3), processes the data, and writes
    it back to AWS storage. Spark jobs are often used to handle the ETL logic that
    interacts with S3\. EMR provides an interesting feature named **Workspace** that
    helps organize notebooks by developers and shares them with other EMR users for
    collaborative work.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DL 的情况下，单个 EC2 实例的计算能力可能不足以进行模型训练或数据处理。因此，通常将一组 EC2 实例放在一起以增加吞吐量。AWS 为此提供了专门的服务：**Amazon
    Elastic MapReduce**（**EMR**）。它是一个完全托管的集群平台，提供用于大数据框架（如 Apache Spark 和 Hadoop）的分布式系统。通常，为
    ETL 设置的 EMR 集群从 AWS 存储（Amazon S3）读取数据，处理数据，然后将数据写回 AWS 存储。Spark 作业通常用于处理与 S3 交互的
    ETL 逻辑。EMR 提供了一个名为 **Workspace** 的有趣功能，帮助开发人员组织笔记本，并与其他 EMR 用户共享以进行协作工作。
- en: A typical EMR setup contains a master node and a few core nodes. In the case
    of a multi-node cluster, there must be at least one core node. A master node manages
    a cluster that runs the distributed application (for example, Spark or Hadoop).
    Core nodes are managed by the master node and run data processing tasks and store
    data in data storage (for example, S3 or HDFS).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的 EMR 设置包括一个主节点和几个核心节点。在多节点集群中，必须至少有一个核心节点。主节点管理运行分布式应用程序（例如 Spark 或 Hadoop）的集群。核心节点由主节点管理，运行数据处理任务并将数据存储在数据存储中（例如
    S3 或 HDFS）。
- en: Task nodes are managed by the master node and are optional. They increase the
    throughput of the distributed application running on the cluster by introducing
    another parallelism during computation. They run data processing tasks but do
    not store data in data storage.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 任务节点由主节点管理，是可选的。它们通过在计算过程中引入另一种并行性，提高了集群上运行的分布式应用程序的吞吐量。它们运行数据处理任务，但不将数据存储在数据存储中。
- en: 'The following screenshot shows the EMR cluster creation page. Throughout the
    form, we need to provide the cluster’s name, launch mode, EMR release, applications
    (for example, Apache Spark for data processing and Jupyter for notebooks) to run
    on the cluster, and specifications of the EC2 instances. Data processing with
    DL often needs instances of high computational power. In the other cases, you
    can construct a cluster with increased memory limits:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了 EMR 集群创建页面。在整个表单中，我们需要提供集群名称、启动模式、EMR 版本、要在集群上运行的应用程序（例如用于数据处理的 Apache
    Spark 和笔记本的 Jupyter）以及 EC2 实例的规格。DL 的数据处理通常需要高计算能力的实例。在其他情况下，您可以构建具有增加内存限制的集群：
- en: '![Figure 5.7 – EMR cluster creation'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – EMR 集群创建](img/B18522_05_07.jpg)'
- en: '](img/B18522_05_07.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – EMR 集群创建](img/B18522_05_07.jpg)'
- en: Figure 5.7 – EMR cluster creation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – EMR 集群创建
- en: 'The detailed steps are as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 详细步骤如下：
- en: '**Step 1: Software and Steps**: Here, you must choose the software-related
    configuration – that is, the EMR release and applications (Spark, JupyterHub,
    and so on).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1：软件和步骤**：在这里，您必须选择与软件相关的配置 – 即 EMR 版本和应用程序（Spark、JupyterHub 等）。'
- en: '**Step 2: Hardware**: Here, you must choose the hardware-related configuration
    – that is, the instance type, number of instances, and the VPN network.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2：硬件**：在这里，您必须选择与硬件相关的配置 – 即实例类型、实例数量和 VPN 网络。'
- en: '**Step 3: General Cluster Setting**: Choose the cluster name and the S3 bucket
    path for operational logs.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 3：通用集群设置**：选择集群名称和用于操作日志的 S3 存储桶路径。'
- en: '`.pem` file:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.pem` 文件：'
- en: '`.pem` file is only needed if you want to log in to the EC2 master node and
    work on the Spark shell as in the case of a single EC2 instance.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.pem` 文件只在您想要登录到 EC2 主节点并在 Spark shell 上工作时才需要，就像在单个 EC2 实例的情况下一样。'
- en: After following these steps, you will need to wait for a few minutes until the
    state of the cluster changes to `running`. Then, you can navigate to the endpoint
    provided by the EMR cluster to open a Jupyter notebook. The username is `jovyan`
    and the password is `jupyter`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，您需要等待几分钟，直到集群状态变为`running`。然后，您可以导航到 EMR 集群提供的端点以打开 Jupyter 笔记本。用户名为`jovyan`，密码为`jupyter`。
- en: Our GitHub repository provides step-by-step instructions for this process, along
    with screenshots ([https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/emr](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/emr)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 GitHub 存储库提供了这一过程的逐步说明，以及屏幕截图（[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/emr](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/emr)）。
- en: Things to remember
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 需记住的事项
- en: a. EMR is a fully managed cluster platform that runs big data ETL frameworks
    such as Apache Spark
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: a. EMR 是一个完全托管的集群平台，运行大数据 ETL 框架，如 Apache Spark。
- en: b. You can create an EMR cluster with various EC2 instances through the AWS
    web console
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: b. 您可以通过 AWS Web 控制台创建具有各种 EC2 实例的 EMR 集群。
- en: The downside of EMR comes from the fact that it needs to be managed explicitly.
    An organization often has a group of developers dedicated to handling issues related
    to EMR clusters. Unfortunately, this can be a difficult thing to do if the organization
    is small. In the next section, we will introduce Glue, which doesn’t require any
    explicit cluster management.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: EMR 的缺点在于需要明确管理。一个组织通常有专门处理与 EMR 集群相关问题的开发人员小组。不幸的是，如果组织很小，这可能会很难做到。在接下来的部分中，我们将介绍
    Glue，它不需要任何显式的集群管理。
- en: Creating a Glue job for ETL
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建用于 ETL 的 Glue 作业
- en: AWS Glue ([https://aws.amazon.com/glue](https://aws.amazon.com/glue/)) supports
    data processing in a serverless fashion. The computational resource of Glue is
    managed by AWS, so less effort is needed for maintenance, unlike in the case of
    dedicated clusters (for example, EMR). Other than the minimal maintenance effort
    for the resources, Glue provides additional features such as a built-in scheduler
    and Glue Data Catalog, which will be discussed later.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue（[https://aws.amazon.com/glue](https://aws.amazon.com/glue/)）支持以无服务器方式进行数据处理。Glue
    的计算资源由 AWS 管理，因此与专用集群（例如 EMR）相比，维护工作量较少。除了资源的最小维护工作外，Glue 还提供额外功能，如内置调度器和 Glue
    数据目录，稍后将进行讨论。
- en: 'First, let’s learn how to set up data processing jobs using Glue. Before you
    start defining the logic for data processing, you must create a Glue Data Catalog
    that contains the schema for the data in S3\. Once a Glue Data Catalog has been
    defined for the input data, you can use the Glue Python editor to define the details
    of the data processing logic (*Figure 5.8*). The editor provides a basic setup
    for your application to reduce the difficulties in setting up a Glue job: [https://docs.aws.amazon.com/glue/latest/dg/edit-script.html](https://docs.aws.amazon.com/glue/latest/dg/edit-script.html).
    On top of this template code, you will read in the Glue Data Catalog as an input,
    process it, and store the processed output. Since Glue Data Catalog has a nice
    integration for Spark, the operations within a Glue job are often achieved using
    Spark:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们学习如何使用 Glue 设置数据处理作业。在开始定义数据处理逻辑之前，您必须创建一个包含 S3 中数据架构的 Glue 数据目录。一旦为输入数据定义了
    Glue 数据目录，您可以使用 Glue Python 编辑器定义数据处理逻辑的细节（*图 5.8*）。该编辑器为您的应用提供了一个基本设置，以减少设置 Glue
    作业时的困难：[https://docs.aws.amazon.com/glue/latest/dg/edit-script.html](https://docs.aws.amazon.com/glue/latest/dg/edit-script.html)。在这个模板代码的基础上，您将读取
    Glue 数据目录作为输入，对其进行处理，并存储处理后的输出。由于 Glue 数据目录与 Spark 高度集成，Glue 作业内的操作通常使用 Spark
    完成：
- en: '![Figure 5.8 – AWS Glue job script editor'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8 – AWS Glue 作业脚本编辑器'
- en: '](img/B18522_05_08.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_08.jpg)'
- en: Figure 5.8 – AWS Glue job script editor
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – AWS Glue 作业脚本编辑器
- en: In the following sections, you will learn how to set up a Glue job using the
    Google Scholar dataset, which is stored in an S3 bucket. The complete implementation
    can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/glue](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/glue).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将学习如何使用存储在 S3 存储桶中的 Google Scholar 数据集设置 Glue 作业。完整的实现可以在 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/glue](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_5/glue)
    找到。
- en: Creating a Glue Data Catalog
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Glue 数据目录
- en: 'First, we will create a Glue Data Catalog (see *Figure 5.9*). Glue can only
    read a set of data where the metadata is stored in the Glue Data Catalog. Data
    Catalog consists of databases, which are collections of metadata in the form of
    a table. Glue provides a feature called a **crawler**, which *creates metadata
    for the data files present in data storage* (for example, an S3 bucket):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个 Glue 数据目录（见 *图 5.9*）。Glue 只能读取数据集，其中元数据存储在 Glue 数据目录中。数据目录由数据库组成，这些数据库是以表格形式存储的元数据集合。Glue
    提供了一个称为 **crawler** 的功能，*用于创建存储在数据存储中的数据文件的元数据*（例如，一个 S3 存储桶）：
- en: '![Figure 5.9 – The first step of setting up a crawler'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9 – 设置爬虫的第一步'
- en: '](img/B18522_05_09.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_09.jpg)'
- en: Figure 5.9 – The first step of setting up a crawler
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 设置爬虫的第一步
- en: The preceding screenshot shows the first step of creating a crawler. Details
    of each step can be found at [https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 上述截图显示了创建爬虫的第一步。每个步骤的详细信息可以在 [https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)
    找到。
- en: Setting up a Glue context
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Glue 上下文
- en: 'If you look at the template code provided by AWS for Glue, you will find that
    some key packages are already imported. `getResolvedOptions` from the `awsglue.utils`
    module helps utilize the arguments that are passed to the Glue script during runtime:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看 AWS 为 Glue 提供的模板代码，您会发现已经导入了一些关键包。`awsglue.utils` 模块中的 `getResolvedOptions`
    帮助利用在运行时传递给 Glue 脚本的参数：
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For a Glue job with Spark, a Spark context must be created and passed to `GlueContext`.
    A Spark session object can be accessed from a Glue context. A Glue job can be
    instantiated using the `awsglue.job` module by passing a Glue context object:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 Spark 的 Glue 作业，必须创建一个 Spark 上下文并将其传递给 `GlueContext`。可以从 Glue 上下文中访问 Spark
    会话对象。可以通过传递 Glue 上下文对象来实例化使用 `awsglue.job` 模块的 Glue 作业：
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we will learn how to read data from Glue Data Catalog.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何从 Glue 数据目录中读取数据。
- en: Reading data
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取数据
- en: In this section, you will learn how to read data located in an S3 bucket within
    the Glue context after creating a Glue table catalog.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何在创建 Glue 表目录后，在 Glue 上下文中读取位于 S3 存储桶中的数据。
- en: '*The data in Glue passes from transform to transform using a specific data
    structure called a DynamicFrame*, which is an extension of an Apache Spark DataFrame.
    DynamicFrame, with its self-describing nature, does not require any schema. This
    additional property of a DynamicFrame helps accommodate the data that does not
    conform to a fixed schema, unlike in Spark DataFrames. The required library can
    be imported from `awsglue.dynamicframe`. This package makes converting a DynamicFrame
    into a Spark DataFrame easy:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*在 Glue 中，数据通过称为 DynamicFrame 的特定数据结构从转换到转换传递*，它是 Apache Spark DataFrame 的扩展。DynamicFrame
    具有自描述特性，不需要任何模式。与 Spark DataFrame 不同，DynamicFrame 的这一额外属性有助于容纳不符合固定模式的数据。可以从 `awsglue.dynamicframe`
    导入所需的库。该包可以轻松地将 DynamicFrame 转换为 Spark DataFrame：'
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the following example, we are creating a Glue Data Catalog table named `google_authors`
    in a database named `google_scholar`. Once the database is available, `glueContext.create_dynamic_frame.from_catalog`
    can be used to read the `google_authors` table in the `google_scholar` database
    and load it as a Glue DynamicFrame:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们正在创建一个名为 `google_authors` 的 Glue 数据目录表，存储在名为 `google_scholar` 的数据库中。一旦数据库可用，可以使用
    `glueContext.create_dynamic_frame.from_catalog` 读取 `google_scholar` 数据库中的 `google_authors`
    表，并将其加载为 Glue DynamicFrame：
- en: '[PRE22]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A Glue DynamicFrame can be converted into a Spark DataFrame using the `toDF`
    method. This conversion is required to apply Spark operations to the data:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `toDF` 方法将 Glue DynamicFrame 转换为 Spark DataFrame。此转换需要将 Spark 操作应用于数据：
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now, let’s define the data processing logic.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义数据处理逻辑。
- en: Defining the data processing logic
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义数据处理逻辑
- en: 'Basic transformations that you can perform on a Glue DynamicFrame are provided
    by the `awsglue.transforms` module. These transformations include `join`, `filter`,
    `map`, and many others ([https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html](https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html)).
    You can use them similarly to what was presented in the *Introduction to Apache
    Spark* section:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Glue DynamicFrame 可以执行的基本转换由 `awsglue.transforms` 模块提供。这些转换包括 `join`、`filter`、`map`
    等等（[https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html](https://docs.aws.amazon.com/glue/latest/dg/built-in-transforms.html)）。您可以类似于*Apache
    Spark简介*部分中所介绍的方式来使用它们：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Additionally, every Spark operation described in the *Processing data using
    Spark operations* section can be applied to data in Glue if the Glue DynamicFrame
    has already been converted into a Spark DataFrame.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果 Glue DynamicFrame 已转换为 Spark DataFrame，则可以将*使用 Spark 操作处理数据*部分中描述的每个 Spark
    操作应用于 Glue 数据。
- en: Writing data
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 写入数据
- en: In this section, we will learn how to write data in Glue DynamicFrame to an
    S3 bucket.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何将 Glue DynamicFrame 中的数据写入 S3 存储桶。
- en: 'Given a Glue DynamicFrame, you can store the data in the given S3 path using
    `write_dynamic_frame.from_options` of a Glue context. You need to call the `commit`
    method of a job at the end to perform individual operations:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 Glue DynamicFrame，您可以使用 Glue 上下文的 `write_dynamic_frame.from_options` 将数据存储在指定的
    S3 路径中。您需要在作业结束时调用 `commit` 方法来执行各个操作：
- en: '[PRE25]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the case of a Spark DataFrame, you must convert it into a DynamicFrame before
    you can store the data. The `DynamicFrame.fromDF` function takes in a Spark DataFrame
    object, a Glue context object, and the name of the new DynamicFrame:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Spark DataFrame，必须在存储数据之前将其转换为 DynamicFrame。`DynamicFrame.fromDF` 函数接受 Spark
    DataFrame 对象、Glue 上下文对象和新 DynamicFrame 的名称：
- en: '[PRE26]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, you can use both Spark operations and Glue transformations to process your
    data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以同时使用 Spark 操作和 Glue 转换来处理您的数据。
- en: Things to remember
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事项
- en: a. AWS Glue is a fully managed service designed for ETL operations
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: a. AWS Glue 是一个专为 ETL 操作设计的完全托管服务
- en: b. AWS Glue is a serverless architecture, which means the underlying servers
    will be maintained by AWS
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: b. AWS Glue 是一个无服务器架构，这意味着底层服务器将由 AWS 维护
- en: c. AWS Glue provides a built-in editor with Python boilerplate code. In this
    editor, you can define your ETL logic and also leverage Spark
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: c. AWS Glue 提供了一个带有 Python 样板代码的内置编辑器。在此编辑器中，您可以定义您的 ETL 逻辑，并利用 Spark。
- en: As the last setting for ETL, we will look at SageMaker.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 ETL 的最后设置，我们将看一下 SageMaker。
- en: Utilizing SageMaker for ETL
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 SageMaker 进行 ETL
- en: In this section, we will describe how to set up an ETL process using SageMaker
    (the following screenshot shows the web console for SageMaker). *The main advantage
    of SageMaker comes from the fact that it is a fully managed infrastructure for
    building, training, and deploying ML models*. The downside is the fact that it
    is more expensive than EMR and Glue.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述如何使用SageMaker设置ETL流程（下图显示了SageMaker的Web控制台）。*SageMaker的主要优势在于它是一个完全托管的基础设施，用于构建、训练和部署ML模型*。缺点是它比EMR和Glue更昂贵。
- en: 'SageMaker Studio is a web-based development environment for SageMaker. SageMaker
    has been introduced with the philosophy that it’s an all-in-one place for a data
    analytics pipeline. Every phase of an ML pipeline can be achieved using SageMaker
    Studio: data processing, algorithm design, scheduling jobs, experiment management,
    developing and training models, creating inference endpoints, detecting data drift,
    and visualizing model performance. SageMaker Studio notebooks can also be connected
    to EMR for computations with some restrictions; only limited Docker images (such
    as `Data Science` or `SparkMagic`) can be used ([https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-cluster.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-cluster.html)):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Studio是面向SageMaker的基于Web的开发环境。SageMaker的理念是提供一个集成的数据分析管道。使用SageMaker
    Studio可以完成ML管道的每个阶段：数据处理、算法设计、作业调度、实验管理、模型开发和训练、创建推断端点、检测数据漂移以及可视化模型性能。SageMaker
    Studio的笔记本也可以连接到EMR进行计算，但有一些限制；只能使用部分限定的Docker镜像（如`Data Science`或`SparkMagic`）（[https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-cluster.html](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-notebooks-emr-cluster.html)）：
- en: '![Figure 5.10 – The SageMaker web console'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.10 – The SageMaker web console'
- en: '](img/B18522_05_10.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_10.jpg)'
- en: Figure 5.10 – The SageMaker web console
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – SageMaker Web控制台
- en: 'SageMaker provides various predefined development environments as Docker images.
    Popular environments are those for DL projects that have PyTorch, TF, and Anaconda
    installed already. A notebook can easily be attached to any of these images from
    the web-based development environment, as shown in the following screenshot:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供各种预定义的开发环境作为Docker镜像。流行的环境包括已经安装了PyTorch、TF和Anaconda的DL项目环境。可以轻松地从Web开发环境将笔记本附加到任何这些镜像中，如下面的截图所示：
- en: '![Figure 5.11 – Updating the development environment dynamically for a SageMaker
    notebook'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11 – 动态更新SageMaker笔记本的开发环境'
- en: '](img/B18522_05_11.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_11.jpg)'
- en: Figure 5.11 – Updating the development environment dynamically for a SageMaker
    notebook
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 动态更新SageMaker笔记本的开发环境
- en: 'The process of creating an ETL job can be broken down into four steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 创建ETL作业的过程可以分解为四个步骤：
- en: Create a user within SageMaker Studio.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SageMaker Studio内创建一个用户。
- en: Create a notebook under the user by selecting the right Docker image.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择正确的Docker镜像在用户下创建一个笔记本。
- en: Define data processing logic.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据处理逻辑。
- en: Schedule a job.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安排一个作业。
- en: '*Steps 1* and *2* are one click away in the SageMaker web console. *Step 3*
    can be set up using Spark. To schedule a job (*Step 4*), first, you need to install
    the `run-notebook` command-line utility via the `pip` command:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker Web控制台中，*步骤1* 和 *步骤2* 只需一键即可完成。*步骤3* 可以使用Spark设置。要安排作业（*步骤4*），首先需要通过`pip`命令安装`run-notebook`命令行实用程序：
- en: '[PRE27]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Before looking at the `run-notebook` command for scheduling a notebook, we
    will briefly discuss the `cron` command, which defines the format for a schedule.
    As shown in the following diagram, six numbers are used to represent a timestamp.
    For example, `45 22 ** 6*` represents a schedule for 10:45 P.M. every Saturday.
    The `*` (asterisk) wildcard represents every value of the corresponding unit:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论安排笔记本的`run-notebook`命令之前，我们将简要介绍`cron`命令，它定义了一个调度的格式。如下图所示，使用六个数字表示时间戳。例如，`45
    22 ** 6*`表示每周六晚上10:45的调度。`*`（星号）通配符表示相应单位的每个值：
- en: '![Figure 5.12 – Cron schedule format'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12 – Cron调度格式'
- en: '](img/B18522_05_12.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_12.jpg)'
- en: Figure 5.12 – Cron schedule format
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – Cron调度格式
- en: 'The `run-notebook` command takes in a schedule represented with `cron` and
    a notebook. In the following example, `notebook.ipynb` has been scheduled to run
    at 8 A.M. every day in 2021:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`run-notebook`命令接受用`cron`表示的调度和一个笔记本。在以下示例中，`notebook.ipynb`已安排在2021年每天上午8点运行：'
- en: '[PRE28]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We have provided a set of screenshots for each step in our GitHub repository:
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_5/sagemaker/sagemaker_studio.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_5/sagemaker/sagemaker_studio.md).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在GitHub仓库的每个步骤中都提供了一组截图：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_5/sagemaker/sagemaker_studio.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_5/sagemaker/sagemaker_studio.md)。
- en: In the remaining sections, we will take a deeper look at how to utilize the
    SageMaker notebook to run a data processing job.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究如何利用SageMaker笔记本运行数据处理作业。
- en: Creating a SageMaker notebook
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建SageMaker笔记本
- en: 'A notebook instance is an ML compute instance that runs the Jupyter notebook
    application. SageMaker will create this instance, along with the associated resources.
    The Jupyter notebook is used to process data, train models, and deploy and validate
    the model. A notebook instance can be created in a few steps. The complete description
    can be found at [https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本实例是运行Jupyter笔记本应用程序的ML计算实例。SageMaker将创建此实例以及相关资源。Jupyter笔记本用于处理数据、训练模型、部署和验证模型。可以在几个步骤内创建笔记本实例。详细描述可在[https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html](https://docs.aws.amazon.com/sagemaker/latest/dg/howitworks-create-ws.html)找到：
- en: 'Go to the SageMaker web console: [https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker).
    Please note that you will need to log in with AWS credentials.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到SageMaker Web控制台：[https://console.aws.amazon.com/sagemaker](https://console.aws.amazon.com/sagemaker)。请注意，您需要使用AWS凭据登录。
- en: Under **Notebook instances**, choose **Create notebook instance**.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**笔记本实例**下，选择**创建笔记本实例**。
- en: 'On the `pip install tensorflow`) on each new notebook. Various examples of
    this can be found at [https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts](https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts):'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个新笔记本上进行`pip install tensorflow`）。可以在[https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts](https://github.com/aws-samples/amazon-sagemaker-notebook-instance-lifecycle-config-samples/tree/master/scripts)找到各种示例：
- en: '![Figure 5.13 – Life cycle configuration script for a SageMaker notebook'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.13 – SageMaker笔记本的生命周期配置脚本'
- en: '](img/B18522_05_13.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_05_13.jpg)'
- en: Figure 5.13 – Life cycle configuration script for a SageMaker notebook
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – SageMaker笔记本的生命周期配置脚本
- en: While running a set of operations directly from the SageMaker notebook is an
    option, the SageMaker notebook supports running a data processing job defined
    explicitly outside of the notebook to increase throughput and reusability. Let’s
    look at how we can run a Spark job from a notebook.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然直接从SageMaker笔记本运行一组操作是一种选择，但SageMaker笔记本支持运行明确定义在笔记本外部的数据处理作业，以增加吞吐量和重用性。让我们看看如何从笔记本运行Spark作业。
- en: Running a Spark job through a SageMaker notebook
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过SageMaker笔记本运行Spark作业
- en: 'Once a notebook is ready, you can configure a Spark job using the `sagemaker.processing`
    module and execute it using a set of computational resources. SageMaker provides
    the `PySparkProcessor` class, which provides a handle for the Spark job ([https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#data-processing-with-spark](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#data-processing-with-spark)).
    Its constructor takes in basic setup details, such as the job’s name and Python
    version. It takes in three parameters – `framework_version`, `py_version`, and
    `container_version` – which are used to pin the pre-built Spark containers to
    run the processing job. A custom image can be registered and made available on
    the `image_uri` parameter. `image_uri` will override the `framework_version`,
    `py_version`, and `container_version` parameters:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦笔记本准备就绪，您可以使用`sagemaker.processing`模块配置Spark作业，并使用一组计算资源执行它。SageMaker提供了`PySparkProcessor`类，它提供了一个处理Spark作业的句柄([https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#data-processing-with-spark](https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html#data-processing-with-spark))。其构造函数接受基本的设置详细信息，如作业的名称和Python版本。它接受三个参数
    - `framework_version`、`py_version`和`container_version`，这些参数用于固定预构建的Spark容器以运行处理作业。可以注册并在`image_uri`参数上提供自定义映像。`image_uri`将覆盖`framework_version`、`py_version`和`container_version`参数：
- en: '[PRE29]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the preceding code, a `PySparkProcessor` class has been used to create a
    Spark instance. It takes in `base_job_name` (job name: `my-sparkjob`), `framework_version`
    (the TensorFlow framework version: `2.0`), `py_version` (the Python version: `py37`),
    `container_version` (the container version: `1`), `role` (the IAM role for SageMaker:
    `myiamrole`), `instance_count` (the number of EC2 instances: `2`), `instance_type`
    (the EC2 instance type: `ml.c5.xlarge`), `max_runtime_in_second` (the maximum
    runtime in seconds before timeout: `1200`), and `image_url` (the URL of the Docker
    image: `ecr_image_uri`).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，使用了 `PySparkProcessor` 类来创建一个 Spark 实例。它接受 `base_job_name`（作业名称：`my-sparkjob`）、`framework_version`（TensorFlow
    框架版本：`2.0`）、`py_version`（Python 版本：`py37`）、`container_version`（容器版本：`1`）、`role`（SageMaker
    的 IAM 角色：`myiamrole`）、`instance_count`（EC2 实例数：`2`）、`instance_type`（EC2 实例类型：`ml.c5.xlarge`）、`max_runtime_in_second`（超时前的最大运行时间秒数：`1200`）和
    `image_url`（Docker 镜像的 URL：`ecr_image_uri`）。
- en: 'Next, we will discuss the `run` method of `PySparkProcessor`, which starts
    the provided script through Spark:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论 `PySparkProcessor` 的 `run` 方法，它通过 Spark 启动提供的脚本：
- en: '[PRE30]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, the `run` method of `PySparkProcessor` executes the given
    script, along with the arguments provided. It takes in `submit_app` (a data processing
    job written in Python) and arguments. In this example, we have defined where the
    input data is located and where the output should be stored.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`PySparkProcessor` 的 `run` 方法执行给定的脚本和提供的参数。它接受 `submit_app`（用 Python
    编写的数据处理作业）和参数。在此示例中，我们已经定义了输入数据的位置和输出应存储的位置。
- en: Running a job from a custom container through a SageMaker notebook
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 SageMaker 笔记本从自定义容器运行作业
- en: 'In this section, we will discuss how to run a data processing job from a custom
    image. SageMaker provides the `Processor` class as part of the `sagemaker.processing`
    module for this purpose. In this example, we will use the `ProcessingInput` and
    `ProcessingOutput` classes to create input and output objects, respectively. These
    objects will be passed to the `run` method of the `Processor` instance. The `run`
    method executes the data processing job:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将讨论如何从自定义镜像运行数据处理作业。为此，SageMaker 提供了 `sagemaker.processing` 模块中的 `Processor`
    类。在本例中，我们将使用 `ProcessingInput` 和 `ProcessingOutput` 类来分别创建输入和输出对象。这些对象将传递给 `Processor`
    实例的 `run` 方法。`run` 方法执行数据处理作业：
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the preceding code, first, we create a `Processor` instance. It takes in
    `image_uri` (the ECR image’s URL path: `ecr_image_uri`), `role` (the IAM role
    that has access to the ECR image: `myiamrole`), `instance_count` (the EC2 instance
    count: `1`), and `instance_type` (the EC2 instance type: `ml.m5.xlarge`). The
    `run` method of the `Processor` instance can execute the job. It takes in `inputs`
    (the input data passed as a `ProcessingInput` object) and `outputs` (the output
    data passed as a `ProcessingOutput` object). While `Processor` provides a similar
    set of methods to `PySparkProcessor`, the main difference comes from what the
    `run` function takes in; `PySparkProcessor` takes in a Python script that runs
    Spark operations, while `Processor` takes in a Docker image that supports various
    types of data processing jobs.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，首先创建了一个 `Processor` 实例。它接受 `image_uri`（ECR 镜像的 URL 路径：`ecr_image_uri`）、`role`（具有访问
    ECR 镜像权限的 IAM 角色：`myiamrole`）、`instance_count`（EC2 实例计数：`1`）和 `instance_type`（EC2
    实例类型：`ml.m5.xlarge`）。`Processor` 实例的 `run` 方法可以执行作业。它接受 `inputs`（作为 `ProcessingInput`
    对象传递的输入数据）和 `outputs`（作为 `ProcessingOutput` 对象传递的输出数据）。虽然 `Processor` 提供了与 `PySparkProcessor`
    类似的一组方法，但主要区别在于 `run` 函数接受的内容；`PySparkProcessor` 接受运行 Spark 操作的 Python 脚本，而 `Processor`
    接受支持各种数据处理作业的 Docker 镜像。
- en: For those who are willing to dig into the details, we recommend reading [https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些愿意深入了解的人，我们建议阅读 [构建自定义处理容器](https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html)。
- en: Things to remember
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 记住的事情
- en: a. SageMaker is a fully managed infrastructure for building, training, and deploying
    ML models.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: a. SageMaker 是一个完全托管的基础设施，用于构建、训练和部署 ML 模型。
- en: b. SageMaker provides a set of predefined development environments that users
    can change dynamically based on their needs.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: b. SageMaker 提供一组预定义的开发环境，用户可以根据需要动态更改。
- en: c. SageMaker notebooks support data processing jobs defined outside of the notebook
    through the `sagemaker.processing` module.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: c. SageMaker 笔记本支持通过 `sagemaker.processing` 模块定义笔记本之外的数据处理作业。
- en: Having gone through the four most popular ETL tools in AWS, let’s compare the
    four options side by side.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 经过对 AWS 中四种最流行的 ETL 工具的介绍，让我们并排比较这四个选项。
- en: Comparing the ETL solutions in AWS
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 AWS 中的 ETL 解决方案
- en: So far, we have looked at four different ways of setting up ETL pipelines using
    AWS. In this section, we will summarize the four setups in a single table (*Table
    5.1*). Some of the comparison points include support for serverless architecture,
    the availability of a built-in scheduler, and variety in terms of the supported
    EC2 instance types.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了四种使用 AWS 设置 ETL 管道的不同方式。在本节中，我们将把这四种设置总结在一张表中（*表 5.1*）。比较点包括支持无服务器架构、内置调度器的可用性以及支持的
    EC2 实例类型的多样性。
- en: '| **Supports** | **Single-Node****EC2 Instance** | **Glue** | **EMR** | **SageMaker**
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| **支持** | **单节点 EC2 实例** | **Glue** | **EMR** | **SageMaker** |'
- en: '| Support for serverless architecture | No | Yes | No | No |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 支持无服务器架构的可用性 | 否 | 是 | 否 | 否 |'
- en: '| Availability of a built-in workspace for collaboration among developers |
    No | No | Yes | No |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 内置用于开发人员协作的工作空间的可用性 | 否 | 否 | 是 | 否 |'
- en: '| Variety of EC2 instance types | More | Less | More | More |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 多种 EC2 实例类型 | 更多 | 更少 | 更多 | 更多 |'
- en: '| Availability of a built-in scheduler | No | Yes | No | Yes |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 内置调度器的可用性 | 否 | 是 | 否 | 是 |'
- en: '| Availability of a built-in job monitoring UI | No | Yes | No | Yes |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 内置作业监控 UI 的可用性 | 否 | 是 | 否 | 是 |'
- en: '| Availability of a built-in model monitoring | No | No | No | Yes |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 内置模型监控的可用性 | 否 | 否 | 否 | 是 |'
- en: '| Support for a fully managed service from model development to deployment
    | No | No | No | Yes |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 提供从模型开发到部署的全托管服务 | 否 | 否 | 否 | 是 |'
- en: '| Availability of a built-in visualizer for analyzing the processed data |
    No | No | No | Yes |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 内置可视化器以分析处理过的数据的可用性 | 否 | 否 | 否 | 是 |'
- en: '| Availability of a predefined environment for ETL logic development | Yes
    | No | Yes | Yes |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 内置用于 ETL 逻辑开发的预定义环境的可用性 | 是 | 否 | 是 | 是 |'
- en: Table 5.1 – A comparison of the various data processing setups – a single-node
    EC2 instance, Glue, EMR, and SageMaker
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – 各种数据处理设置的比较 – 单节点 EC2 实例、Glue、EMR 和 SageMaker
- en: The right setup depends on both technical and non-technical factors, including
    the source of the data, the amount of data, the availability of MLOps, and the
    cost.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的设置取决于技术和非技术因素，包括数据来源、数据量、MLOps 的可用性和成本。
- en: Things to remember
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. The four ETL setups we described in this chapter have distinct advantages.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: a. 我们在本章描述的四种 ETL 设置具有独特的优势。
- en: 'b. When selecting a particular setup, various factors must be considered: the
    source of the data, the amount of data, the availability of MLOps, and the cost.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: b. 在选择特定设置时，必须考虑各种因素：数据来源、数据量、MLOps 的可用性和成本。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: One of the difficulties with DL projects arises from the amount of data. Since
    a large amount of data is necessary to train a DL model, data processing steps
    can take up a lot of resources. Therefore, in this chapter, we learned how to
    utilize the most popular cloud service, AWS, to process terabytes and petabytes
    of data efficiently. The system includes a scheduler, data storage, databases,
    visualization, as well as a data processing tool for running the ETL logic.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习项目中的一个困难来自于数据量的大小。由于训练深度学习模型需要大量数据，因此数据处理步骤可能会占用大量资源。因此，在本章中，我们学习了如何利用最流行的云服务
    AWS 高效处理数千兆字节和百万兆字节的数据。该系统包括调度器、数据存储、数据库、可视化以及用于运行 ETL 逻辑的数据处理工具。
- en: We have spent extra time looking at ETL since it plays a major role in data
    processing. We introduced Spark, which is the most popular tool for ETL, and described
    four different ways of setting up ETL jobs using AWS. The four settings include
    using a single-node EC2 instance, an EMR cluster, Glue, and SageMaker. Each setup
    has distinct advantages, and the right one may differ based on the situation.
    This is because you need to consider both technical and non-technical aspects
    of the project.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们额外花费了时间研究 ETL，因为它在数据处理中起着重要作用。我们介绍了 Spark，这是最流行的 ETL 工具，并描述了使用 AWS 设置 ETL
    作业的四种不同方式。这四种设置包括使用单节点 EC2 实例、EMR 集群、Glue 和 SageMaker。每种设置都有独特的优势，正确的选择可能因情况而异。这是因为您需要考虑项目的技术和非技术方面。
- en: Similar to how the amount of data becomes an issue for processing data, it also
    introduces multiple issues when training a model. In the next chapter, you will
    learn how to train models efficiently using a distributed system.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于数据量对数据处理的影响，当训练模型时也会引入多个问题。在下一章中，您将学习如何使用分布式系统高效训练模型。
