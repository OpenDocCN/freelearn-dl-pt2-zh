- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Training Models Faster
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更快地训练模型
- en: In the last chapter, we learned the factors that contribute to increasing the
    computational burden of the training process. Those factors have a direct influence
    on the complexity of the training phase and, hence, on the execution time.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了增加训练过程的计算负担的因素。这些因素直接影响训练阶段的复杂性，从而影响执行时间。
- en: Now, it is time to learn how to accelerate this process. In general, we can
    improve performance by changing something in the software stack or increasing
    the number of computing resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候学习如何加快这一过程了。一般来说，我们可以通过改变软件堆栈中的某些内容或增加计算资源的数量来提高性能。
- en: In this chapter, we will start to understand both of these options. Next, we
    will learn what can be modified in the application and environment layers.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始理解这两个选项。接下来，我们将学习可以在应用程序和环境层面进行修改的内容。
- en: 'Here is what you will learn as part of this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的学习内容：
- en: Understanding the approaches to accelerate the training process
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解加速训练过程的方法
- en: Knowing the layers of the software stack used to train a model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道用于训练模型的软件堆栈的层次
- en: Learning the difference between vertical and horizontal scaling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习垂直和水平扩展的区别
- en: Understanding what can be changed in the application layer to accelerate the
    training process.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解可以在应用程序层面修改以加速训练过程的内容。
- en: Understanding what can be changed in the environment layer to improve the performance
    of the training phase
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解可以在环境层面进行修改以提高训练阶段性能的内容
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the complete code of the examples mentioned in this chapter in
    the book’s GitHub repository at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章提到的示例的完整代码，网址为 [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main)。
- en: You can access your favorite environments to execute this notebook, such as
    Google Colab or Kaggle.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问您喜爱的环境来执行此笔记本，例如Google Colab或Kaggle。
- en: What options do we have?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们有哪些选择？
- en: 'Once we have decided to accelerate the training process of a model, we can
    take two directions, as illustrated in *Figure 2**.1*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定加速模型训练过程，我们可以采取两个方向，如*图2.1*所示：
- en: '![Figure 2.1 – Approaches to accelerating the training phase](img/B20959_02_1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 2.1 – 加速训练阶段的方法](img/B20959_02_1.jpg)'
- en: Figure 2.1 – Approaches to accelerating the training phase
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 加速训练阶段的方法
- en: In the first option (**Modify the software stack**), we go through each layer
    of the software stack used to train a model to seek opportunities to improve the
    training process. In simpler words, we can change the application code, install
    and use a specialized library, or enable a special capability regarding the operating
    system or container environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个选项（**修改软件堆栈**）中，我们将遍历用于训练模型的每一层软件堆栈，寻找改进训练过程的机会。简而言之，我们可以更改应用程序代码，安装和使用专门的库，或者在操作系统或容器环境中启用特殊功能。
- en: This first approach relies on having profound knowledge of performance tuning
    techniques. In addition, it demands a high sense of investigation to identify
    bottlenecks and apply the most suitable solution to overcome them. Thus, this
    approach is about harnessing the most hardware and software resources by extracting
    the maximum performance of the computing system.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一种方法依赖于对性能调整技术的深刻了解。此外，它要求具有高度的调查意识，以识别瓶颈并应用最合适的解决方案来克服它们。因此，这种方法是通过提取计算系统的最大性能来利用最多的硬件和软件资源。
- en: Nevertheless, remark that depending on the environment we are running the training
    process in, we may not have the required privileges to change the lower layers
    of the software stack. For example, suppose we are running the training process
    in a notebook provided by a third-party environment such as **Kaggle** or **Google
    Colab**. In this case, we cannot change operating system parameters or modify
    the container image because this environment is controlled and restricted. We
    can still change the application code, but it may not be enough to accelerate
    the training process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，根据我们运行训练过程的环境，我们可能没有必要权限来更改软件堆栈的较低层。例如，假设我们在像**Kaggle**或**Google Colab**这样的第三方环境提供的笔记本中运行训练过程。在这种情况下，我们无法更改操作系统参数或修改容器镜像，因为这个环境是受控制和限制的。我们仍然可以更改应用程序代码，但这可能不足以加速训练过程。
- en: When changing things in the software stack is impossible or does not provide
    the expected performance gain, we can go towards the second option (**Increase
    computing resources**) to train the model. So, we can increase the number of processors
    and the amount of main memory, use accelerator devices, or spread the training
    process across multiple machines. Naturally, we may need to spend money to bring
    this option to life.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当在软件堆栈中无法改变事物或未提供预期性能增益时，我们可以转向第二个选项（**增加计算资源**）来训练模型。因此，我们可以增加处理器数量和主内存量，使用加速器设备，或将训练过程分布在多台机器上。自然地，我们可能需要花费金钱来实现这个选项。
- en: Notice that this approach is easier in the cloud than for on-premises infrastructures.
    When using the cloud, we can easily contract a machine endowed with accelerator
    devices or add more machines to our setup. We can get these resources ready to
    use with a few clicks. On the other hand, we may face some constraints when adding
    new computing resources to the on-premises infrastructure, such as physical space
    and energy capacity restrictions. It is not impossible, though; it might only
    be more challenging to do.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在云端比在本地基础设施中采用这种方法更容易。使用云时，我们可以轻松地合同一台配备加速器设备的机器或在我们的设置中添加更多机器。我们可以通过几次点击准备好这些资源以供使用。另一方面，在本地基础设施中添加新的计算资源时可能会遇到一些约束，例如物理空间和能源容量限制。尽管如此，这并非不可能，只是可能更具挑战性。
- en: Furthermore, we also have another scenario in which our infrastructure, either
    on the cloud or on-premises, already has those computing resources. In this case,
    we just need to start using them to accelerate the training process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还有另一种情况，即我们的基础设施，无论是云端还是本地，已经拥有这些计算资源。在这种情况下，我们只需开始使用它们来加速训练过程。
- en: So, if we have money to buy or contract these resources, or if they are already
    available to us in our environment, the problem is solved, right? Not necessarily.
    Unfortunately, there is no guarantee that using additional resources in the training
    process will automatically improve performance. As we will discuss in this book,
    the performance bottleneck is not always overcome by adding more computing resources
    without rethinking the whole process, adjusting the code, and so on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们有资金购买或合同这些资源，或者如果它们已经在我们的环境中可用，问题解决了，对吗？并非一定如此。不幸的是，使用额外的资源在训练过程中不能自动提高性能的保证。正如本书将讨论的那样，性能瓶颈并非总是通过增加计算资源而得以克服，而需要重新思考整个流程、调整代码等。
- en: 'This last assertion gives us a valuable lesson: we must see these two approaches
    as a cycle, not as two isolated options. This means we must go back and forth
    on both methods as often as necessary to reach the desired improvement, as shown
    in *Figure 2**.2*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后的断言给了我们一个宝贵的教训：我们必须将这两种方法视为一个循环，而不是两个孤立的选择。这意味着我们必须根据需要反复使用这两种方法，以达到所需的改进，如*图2**.2*所示。
- en: '![Figure 2.2 – Continuous improvement cycle](img/B20959_02_2.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – 持续改进循环](img/B20959_02_2.jpg)'
- en: Figure 2.2 – Continuous improvement cycle
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 持续改进循环
- en: Let’s see more details about both approaches in the following sections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的几节中详细了解这两种方法的更多细节。
- en: Modifying the software stack
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改软件堆栈
- en: The software stack used to train a model can vary depending on the environment
    we use to execute this process. For the sake of simplicity, we will consider in
    this book a software stack seen from the point of view of data scientists, i.e.,
    as users of a computing service or environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练模型的软件堆栈可以根据我们用于执行此过程的环境而异。为简单起见，在本书中，我们将从数据科学家的角度考虑软件堆栈，即作为计算服务或环境的用户。
- en: 'In general, we can say the software stack looks like the layers shown in *Figure
    2**.3*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，软件堆栈看起来像是图 *2**.3* 所示的层次结构：
- en: '![Figure 2.3 – Software stack used to train a model](img/B20959_02_3.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 用于训练模型的软件堆栈](img/B20959_02_3.jpg)'
- en: Figure 2.3 – Software stack used to train a model
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 用于训练模型的软件堆栈
- en: 'From the top to the bottom, we have the following layers:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部到底部，我们有以下层次：
- en: '**Application**: The model-building program occupies this layer. This program
    can be written in any programming language capable of building neural networks,
    such as R and Julia, but Python is the language primarily used for this purpose.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用程序**：该层包含建模程序。该程序可以用任何能够构建神经网络的编程语言编写，如R和Julia，但Python是主要用于此目的的语言。'
- en: '**Environment**: The machine learning framework used to build the application,
    libraries, and tools used to support this framework lie in this layer. Some examples
    of machine learning frameworks are **PyTorch**, **TensorFlow**, **Keras**, and
    **MxNet**. Concerning the set of libraries, we can cite **Nvidia Collective Communication
    Library** (**NCCL**), for efficient communication among GPUs, and **jemalloc**,
    for optimized memory allocation.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境**：用于构建应用程序的机器学习框架，支持此框架的库和工具位于此层。一些机器学习框架的例子包括**PyTorch**、**TensorFlow**、**Keras**和**MxNet**。关于库的集合，我们可以提到**Nvidia
    Collective Communication Library**（**NCCL**），用于GPU之间的高效通信，以及**jemalloc**，用于优化内存分配。'
- en: '**Execution**: This layer is responsible for supporting the execution of the
    environment and application layers. Therefore, a container solution or bare metal
    operating system belongs to this layer. Although the components of the upper layers
    can be executed directly on the operating system, nowadays, it is common to use
    a container to wrap up the entire application and its environment. Although Docker
    is the most famous container solution, it is preferable to adopt a more suitable
    option to run machine learning workloads such as **Apptainer** and **Enroot**.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行**：此层负责支持环境和应用层的执行。因此，容器解决方案或裸金属操作系统属于此层。虽然上层组件可以直接在操作系统上执行，但如今通常使用容器来封装整个应用程序及其环境。尽管Docker是最著名的容器解决方案，但更适合运行机器学习工作负载的选择是采用**Apptainer**和**Enroot**。'
- en: At the bottom of the software stack, there is a box representing all the hardware
    resources needed to execute the upper software layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件堆栈的底部，有一个框代表执行上层软件所需的所有硬件资源。
- en: 'To obtain a practical understanding of this software stack representation,
    let’s see a couple of examples, as illustrated in *Figure 2**.4*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际理解这种软件堆栈表示，让我们看几个示例，如图 *2**.4* 所示：
- en: '![Figure 2.4 – Examples of software stacks](img/B20959_02_4.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 软件堆栈示例](img/B20959_02_4.jpg)'
- en: Figure 2.4 – Examples of software stacks
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 软件堆栈示例
- en: All scenarios described in *Figure 2**.4* use an application written in Python.
    As stated before, the application can be a program coded in C++ or a script written
    in R. It does not matter. The important note to keep in mind is that the application
    layer represents the location of our code. In examples **A**, **B**, and **D**,
    we have scenarios using PyTorch as the machine learning framework. Cases **A**
    and **B** have the support of additional libraries, namely **OpenMP** and **Intel
    One API**. This means PyTorch is relying on these libraries to empower tasks and
    operations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *2**.4* 中描述的所有场景都使用Python编写的应用程序。如前所述，应用程序可以是用C++编写的程序或用R编写的脚本。这并不重要。需要牢记的重要提示是应用层代表我们代码的位置。在示例
    **A**、**B** 和 **D** 中，我们有使用PyTorch作为机器学习框架的场景。情况 **A** 和 **B** 还依赖于额外的库，即**OpenMP**和**Intel
    One API**。这意味着PyTorch依赖于这些库来增强任务和操作。
- en: Finally, the execution layer of scenarios **B** and **C** uses a container solution
    to execute the upper layers, whereas the upper layers of examples **A** and **D**
    run directly on the operating system. Furthermore, notice that the hardware resources
    in scenarios **B** and **C** are endowed with GPU accelerators, whereas the others
    have only a CPU.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，场景 **B** 和 **C** 的执行层使用容器解决方案来执行上层，而示例 **A** 和 **D** 的上层直接在操作系统上运行。此外，请注意，场景
    **B** 和 **C** 的硬件资源配备了GPU加速器，而其他情况只有CPU。
- en: Important note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Remark that we are abstracting the type of infrastructure used to run the software
    stack since it is irrelevant to our discussion at this moment. Therefore, you
    can consider the software stack hosted in a cloud or on-premises infrastructure.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在抽象化用于运行软件堆栈的基础设施类型，因为在当前讨论阶段这是无关紧要的。因此，您可以考虑将软件堆栈托管在云端或本地基础设施中。
- en: Except for the case where we are using an environment provided by our own resources,
    we probably will not have the administrative rights to modify or add configurations
    in the execution layer. In most cases, we use the computing environments provisioned
    by our companies. Thus, we do not possess the privilege to alter anything in the
    container or operating system layers. Usually, we address these modifications
    to the IT infrastructure team.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们使用自己资源提供的环境，否则我们可能没有权限修改或添加执行层中的配置。在大多数情况下，我们使用公司配置的计算环境。因此，我们没有特权修改容器或操作系统层中的任何内容。通常，我们会将这些修改提交给IT基础设施团队。
- en: For this reason, we will focus on the application and environment layers, where
    we have the power to change and perform additional configurations.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将专注于应用和环境层面，在这些层面上我们有能力进行修改和执行额外的配置。
- en: The second part of this book focuses on teaching you how to change the software
    stack in such a way that we can accelerate the training process with the available
    resources.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第二部分专注于教授如何改变软件堆栈，以便我们可以利用现有资源加速训练过程。
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: There are exciting configurations we can make in the execution layer to improve
    performance. However, they are out of the scope of this book. As data scientists
    are the primary audience of this material, we focus on the layers that these professionals
    have the necessary access to modify and customize by themselves.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行层面，有一些令人兴奋的配置可以提升性能。然而，它们超出了本书的范围。鉴于数据科学家是本材料的主要受众，我们将重点放在这些专业人士有权访问并自行修改和定制的层面上。
- en: Modifying the software stack to accelerate the training process has a limit.
    We get stuck on performance improvement, no matter how deep and advanced the techniques
    we use are. When we reach that limit, the one way to speed up the training phase
    is by using additional computing resources, as explained in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 修改软件堆栈以加速训练过程有其局限性。无论我们采用多么深入和先进的技术，性能改进都会受到限制。当我们达到这个限制时，加速训练阶段的唯一方法是使用额外的计算资源，如下一节所述。
- en: Increasing computing resources
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加计算资源
- en: 'There are two approaches to increasing computing resources in an existing environment:
    vertical and horizontal scaling. In vertical scaling, we augment the computing
    resources of a single machine, whereas, in horizontal scaling, we add more machines
    to the pool of equipment used to train the model.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有环境中增加计算资源有两种方法：垂直扩展和水平扩展。在垂直扩展中，我们增加单台机器的计算资源，而在水平扩展中，我们将更多机器添加到用于训练模型的设备池中。
- en: 'In practical terms, **vertical scaling** allows for equipping the machine with
    an accelerator device to increase main memory, add more processor cores, and so
    on, as exemplified in *Figure 2**.5*. After doing this scaling, we obtain an empowered
    machine with higher resources:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，**垂直扩展**允许为机器配备加速器设备，增加主存储器，添加更多处理器核心等，正如*图 2**.5*所示的例子。进行这种扩展后，我们获得了一台资源更强大的机器：
- en: '![Figure 2.5 – Example of vertical scaling](img/B20959_02_5.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 垂直扩展示例](img/B20959_02_5.jpg)'
- en: Figure 2.5 – Example of vertical scaling
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 垂直扩展示例
- en: 'Horizontal scaling is related to the increase in the number of machines used
    by our application. If we originally used one machine to execute the training
    process, we can apply horizontal scaling and use two machines to work together
    to train the model, as shown in the example of *Figure 2**.6*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 水平扩展与我们的应用使用的机器数量增加有关。如果我们最初使用一台机器来执行训练过程，我们可以应用水平扩展，使用两台机器共同训练模型，正如在*图 2**.6*的示例中所示：
- en: '![Figure 2.6 – Example of horizontal scaling](img/B20959_02_6.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 水平扩展示例](img/B20959_02_6.jpg)'
- en: Figure 2.6 – Example of horizontal scaling
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 水平扩展示例
- en: Regardless of the type of scaling, we need to know how to harness these additional
    resources to improve performance. Depending on the kind of resources added to
    our setup, we need to adjust the code in many different parts. In other situations,
    the machine learning framework can automatically deal with the increase in resources
    without requiring any additional modification.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 无论扩展的类型如何，我们都需要知道如何利用这些额外的资源来提高性能。根据我们设置的资源类型，我们需要在许多不同的部分调整代码。在其他情况下，机器学习框架可以自动处理资源增加，而无需任何额外的修改。
- en: As we learned in this section, the first step to accelerate the training process
    relies on modifying the application layer. Follow me to the next section to know
    how to do it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节学到的，加速训练过程的第一步依赖于修改应用层。跟我来到下一节，了解如何操作。
- en: Modifying the application layer
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改应用层
- en: The application layer is the starting point of the performance improvement journey.
    As we have complete control of the application code, we can change it without
    depending on anyone else. Thus, there is no better way to start the performance
    optimization process than working independently.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 应用层是性能提升旅程的起点。因为我们完全控制应用代码，所以可以独立进行修改，不依赖于任何其他人。因此，开始性能优化过程的最佳方式就是独立工作。
- en: What can we change in the application layer?
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们可以在应用层中做出哪些改变？
- en: You may wonder how we can modify the code to improve performance. Well, we can
    reduce model complexity, increase the batch size to optimize memory usage, compile
    the model to fuse operations and disable profiling functions to eliminate extra
    overhead in the training process.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道我们如何修改代码以改善性能。好吧，我们可以减少模型复杂性，增加批量大小以优化内存使用，编译模型以融合操作，并禁用分析函数以消除训练过程中的额外开销。
- en: Regardless of the changes applied to the application layer, we cannot sacrifice
    model accuracy in favor of performance improvement since this does not make sense.
    As the primary goal of a neural network is to solve problems, it would be meaningless
    to accelerate the building process of a useless model. Then, we must pay attention
    to model quality when modifying the code to reduce the training phase time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 无论应用层所作的变更如何，我们不能以牺牲模型准确性为代价来改善性能，因为这毫无意义。由于神经网络的主要目标是解决问题，加速无用模型的构建过程就显得毫无意义。因此，在修改代码以减少训练阶段时间时，我们必须注意模型质量。
- en: 'In *Figure 2**.7*, we can see the sort of changes we can make in the application
    layer to speed up the training phase:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 2**.7* 中，我们可以看到我们可以在应用层中进行的改变类型：
- en: '![Figure 2.7 – Changes in the application layer to accelerate the training
    process](img/B20959_02_7.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7 – 修改应用层以加快训练过程](img/B20959_02_7.jpg)'
- en: Figure 2.7 – Changes in the application layer to accelerate the training process
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 修改应用层以加快训练过程
- en: 'Let’s look at each of the changes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每个变化：
- en: '**Change model definition**: Modify the neural network architecture to reduce
    the number of layers, weights, and operations executed on each layer'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改模型定义**：修改神经网络架构以减少每层的层数、权重和执行的操作'
- en: '**Adjust hyperparameters**: Change hyperparameters such as batch size, the
    number of epochs, and the optimizer'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整超参数**：更改超参数，如批量大小、迭代次数和优化器'
- en: '**Use a framework capability**: Take advantage of a framework capability such
    as kernel fusion, automatic mixed precision, and model compiling'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用框架的能力**：充分利用像内核融合、自动混合精度和模型编译等框架能力'
- en: '**Disable unnecessary functions**: Get rid of undue burdens such as computing
    the gradient on the validation phase'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁用不必要的功能**：摆脱不必要的负担，如在验证阶段计算梯度'
- en: Important note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Some framework capabilities rely on making changes in the environment layer,
    such as installing an additional tool or library or even upgrading the framework
    version.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一些框架能力依赖于在环境层进行的变更，比如安装额外的工具或库，甚至升级框架版本。
- en: Naturally, these categories do not cover all possibilities for performance improvement
    in the application layer; their purpose is to give you a clear mental model of
    what we can effectively do to the code to accelerate the training phase.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些类别并不涵盖应用层性能改进的所有可能性；它们的目的是为您提供一个明确的心理模型，了解我们可以有效地对代码做些什么以加速训练阶段。
- en: Getting hands-on
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际操作
- en: Let’s see a practical example of performance improvement by changing only the
    application code. Our guinea pig is the CNN model introduced in the previous chapter,
    which was used to classify the images of the Fashion-MNIST dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过仅更改应用代码来看一个性能改进的实际示例。我们的实验对象是前一章介绍的CNN模型，用于对Fashion-MNIST数据集中的图像进行分类。
- en: Important note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Details about the computing environment used in this experiment are irrelevant
    at this time. What truly matters is the speedup achieved with these modifications,
    considering the same environment and conditions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此实验中使用的计算环境的详细信息在此时无关紧要。真正重要的是这些修改所实现的加速效果，在相同环境和条件下考虑。
- en: 'This model has two convolutional layers and two fully connected layers, resulting
    in **1,630,090** weights. With the number of epochs equal to 10 and the batch
    size equal to 64, the training phase took 148 seconds to complete. The trained
    model achieved 83.99% accuracy when tested against 10,000 images from the test
    dataset, as you can see here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型有两个卷积层和两个全连接层，共**1,630,090**个权重。当训练阶段的批量大小为64，训练时长为148秒。训练后的模型对来自测试数据集的10,000张图像的准确率达到了83.99%，如您所见：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/baseline.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/baseline.ipynb).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/baseline.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/baseline.ipynb)找到。
- en: 'By making only one simple modification to the code, we can reduce the training
    time of this model by 15% while keeping the same accuracy achieved with the baseline
    code. The improved code took 125 seconds to complete, and the trained model reached
    an accuracy equal to 83.76%:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅对代码进行一个简单的修改，我们可以将该模型的训练时间减少15%，同时保持与基线代码相同的准确性。改进后的代码完成时间为125秒，训练后的模型达到了83.76%的准确率：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important note
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-bias.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-bias.ipynb).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-bias.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-bias.ipynb)找到。
- en: 'We improved performance by disabling the bias parameter on the two convolutional
    and two fully connected layers. The following piece of code shows the use of the
    `bias` parameter to disable the bias weight on the function’s `Conv2d` and `Linear`
    layers:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过禁用两个卷积层和两个全连接层的偏差参数来提高性能。下面的代码片段展示了如何使用`bias`参数来禁用函数的`Conv2d`和`Linear`层上的偏差权重：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This modification reduced the number of weights from 1,630,090 to 1,629,472,
    representing a decrease of only 0.04% in the total number of neural network weights.
    As we can see, this change in the number of weights did not affect the model’s
    accuracy since it achieved practically the same efficiency as before. Therefore,
    we trained the model 15% faster with almost no additional effort.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这一修改将神经网络权重数量从1,630,090减少到1,629,472，仅降低了总体权重的0.04%。正如我们所看到的，权重数量的变化并未影响模型的准确性，因为它的效率几乎与之前相同。因此，我们以几乎没有额外工作的情况下，将模型的训练速度提高了15%。
- en: What if we change the batch size?
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果我们改变批量大小会怎样？
- en: 'If we double the batch size from 64 to 128, we achieve an even better performance
    gain than disabling the bias:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将批量大小从64增加到128，则性能提升比禁用偏差要好得多：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Important note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-batchsize.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-batchsize.ipynb).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-batchsize.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/application_layer-batchsize.ipynb)找到。
- en: We trained the model 54% faster by doubling the batch size. As we learned in
    [*Chapter 1*](B20959_01.xhtml#_idTextAnchor016), *Deconstructing the Training
    Process*, the batch size dictates the number of steps in the training phase. Because
    we increased the batch size from 64 to 128, we obtained a fewer number of steps
    per epoch, i.e., the number of steps passed from 938 to 469\. As a consequence,
    the learning algorithm executes half of the phases needed to complete an epoch.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将批量大小加倍来使模型训练速度提高了 54%。正如我们在[*第 1 章*](B20959_01.xhtml#_idTextAnchor016)中学到的，*解构训练过程*，批量大小决定了训练阶段的步骤数。因为我们将批量大小从
    64 增加到 128，每个 epoch 的步骤数减少了，即从 938 减少到 469。因此，学习算法执行了完成一个 epoch 所需阶段的一半。
- en: 'However, such modification came with a price: the accuracy was reduced from
    83.99% to 82.14%. This happens because the learning algorithm executes the optimization
    phase per each training step. Since the number of steps reduced and the number
    of epochs remained the same, the learning algorithm executed a fewer number of
    optimization phases, which consequently decreases its opportunity to reduce the
    training cost.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样的修改是有代价的：准确率从 83.99% 降低到了 82.14%。这是因为学习算法在每个训练步骤中执行优化阶段。由于步骤数量减少而 epoch
    数量保持不变，学习算法执行的优化阶段数量减少了，因此降低了减少训练成本的机会。
- en: 'Just out of curiosity, let’s see what happens when changing the batch size
    to `256`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 只是出于好奇，让我们看看将批量大小更改为 `256` 时会发生什么：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The training time was reduced even more, though not as significantly as when
    we changed from 64 to 128\. On the other hand, the model efficiency fell to 80%.
    We can also observe an increase in the loss per epoch when compared to the previous
    test.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管与从 64 改为 128 相比，训练时间缩短得更多，但效果并不明显。另一方面，模型效率降至 80%。与之前的测试相比，我们还可以观察到每个 epoch
    的损失增加。
- en: In short, we have to find the balance between training speedup and model efficiency
    when adjusting the batch size. The ideal batch size value depends on the model
    architecture, dataset characteristics, and the hardware resource used to train
    the model. Thus, the best way to define it is by doing some experiments before
    starting the training process for real.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，在调整批量大小时，我们必须在训练加速和模型效率之间找到平衡。理想的批量大小取决于模型架构、数据集特征以及用于训练模型的硬件资源。因此，在真正开始训练过程之前，通过一些实验来定义最佳批量大小是最好的方法。
- en: These simple examples showed is possible to accelerate the training process
    by making direct modifications to the code. In the next section, we will see what
    kind of changes we can make in the environment layer to speed up model training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的例子表明，通过直接修改代码可以加速训练过程。在接下来的部分中，我们将看看在环境层可以进行哪些更改以加速模型训练。
- en: Modifying the environment layer
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改环境层
- en: The environment layer comprises the machine learning framework and all the software
    needed to support its execution, such as libraries, compilers, and auxiliary tools.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 环境层包括机器学习框架及其执行所需的所有软件，例如库、编译器和辅助工具。
- en: What can we change in the environment layer?
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们可以在环境层做哪些改变呢？
- en: As we discussed before, we may not have the necessary permission to change anything
    in the environment layer. This restriction depends on the type of environment
    we use to train the model. In third-party environments, such as notebook’s online
    services, we do not have the flexibility to make advanced configurations, such
    as downloading, compiling, and installing a specialized library. We can upgrade
    a package or install a new library, but nothing beyond that.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面讨论过的，我们可能没有必要修改环境层的权限。这种限制取决于我们用于训练模型的环境类型。在第三方环境中，例如笔记本的在线服务中，我们没有灵活性进行高级配置，如下载、编译和安装专门的库。我们可以升级包或安装新的库，但不能做更多的事情。
- en: To overcome this restriction, we commonly use **containers**. Containers allow
    us to configure anything we need to run our application without requiring the
    support or permission of everyone else. Obviously, we are talking about the environment
    layer and not about the execution layer. As we discussed previously, making changes
    to the execution layer requires administrative privileges, which would be out
    of our hands in most environments we usually use.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这种限制，我们通常使用**容器**。容器允许我们配置运行应用程序所需的任何内容，而无需得到所有其他人的支持或权限。显然，我们讨论的是环境层，而不是执行层。正如我们之前讨论过的，修改执行层需要管理员权限，这在我们通常使用的大多数环境中是超出我们能力范围的。
- en: Important note
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/environment_layer.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/environment_layer.ipynb).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/environment_layer.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter02/environment_layer.ipynb)找到。
- en: 'In the case of the environment layer, we can modify these sorts of things:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于环境层，我们可以修改这些内容：
- en: '**Install and use a specialized library**: The machine learning framework comes
    with everything we need to train a model. However, we can speed up the training
    process by using libraries specialized in tasks such as memory allocation, math
    operations, and collective communication.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装和使用专用库**：机器学习框架提供了训练模型所需的一切。但是，我们可以通过使用专门用于内存分配、数学运算和集体通信等任务的库来加快训练过程。'
- en: '**Control libraries’ behavior through environment variables**: The default
    behavior of libraries cannot be the best one for a given scenario or specific
    setup. In that case, we can change it directly through environment variables from
    the application code.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过环境变量控制库的行为**：库的默认行为可能不适合特定情景或特定设置。在这种情况下，我们可以通过应用程序代码直接修改它们的环境变量。'
- en: '**Upgrade framework and libraries to new versions**: This may sound silly,
    but upgrading the machine learning framework and libraries to new versions can
    raise the performance of the training process much more than we think.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级框架和库到新版本**：这听起来可能很傻，但将机器学习框架和库升级到新版本可以比我们想象中提升训练过程的性能。'
- en: We are going to learn many of these things throughout this book. For now, let’s
    jump to the next section to see performance improvement in practice.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的进程中学习到许多这类事情。现在，让我们跳到下一节，看看实际中的性能改进。
- en: Getting hands-on
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实战
- en: As we did in the last section, we will use the baseline code here to assess
    the performance gain from modifying the environment layer. Remember that the training
    process of our baseline code took 148 seconds to run. The environment layer used
    for that execution is composed of PyTorch 2.0 (2.0.0+cpu) as the machine learning
    framework.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在上一节中所做的那样，我们将在这里使用基准代码来评估从修改环境层中获得的性能增益。请记住，我们的基准代码的训练过程花费了148秒。用于执行的环境层由PyTorch
    2.0（2.0.0+cpu）作为机器学习框架。
- en: 'After making two modifications to the environment layer, we got a performance
    improvement near 40%, with the model’s accuracy being practically the same as
    before, as you can see:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在对环境层进行两次修改后，我们获得了接近40%的性能改进，同时模型的准确性几乎与之前相同，正如您所见：
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We made only one change to accelerate the training process of the baseline
    model by almost 40%: the installation and configuration of Intel OpenMP version
    2023.1.0\. We have configured the behavior of this library by setting three environment
    variables:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只进行了一个更改，将基准模型的训练过程加速了将近40%：安装和配置了Intel OpenMP 2023.1.0版本。我们通过设置三个环境变量配置了该库的行为：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In short, these parameters control the way Intel Open launches and orchestrates
    the threads, besides determining the number of threads created by the library.
    We should configure these parameters, taking into account the characteristics
    of the training burden and hardware resources. Notice that setting up these parameters
    in the code is part of modifying the environment layer and not the application
    layer. Even though we are changing the code, those modifications are related to
    environment control rather than model definition.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这些参数控制了Intel Open启动和编排线程的方式，并确定了库创建的线程数量。我们应该根据训练负担的特性和硬件资源来配置这些参数。请注意，在代码中设置这些参数属于修改环境层而不是应用程序层。即使我们在更改代码，这些修改也与环境控制相关，而不是模型定义。
- en: Important note
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Do not worry about how to install and enable the Intel OpenMP library and what
    each of the environment variables used in this test means. We will cover this
    topic in detail in [*Chapter 4*](B20959_04.xhtml#_idTextAnchor060), *Using* *Specialized
    Libraries*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 不必担心如何安装和启用Intel OpenMP库，以及本次测试中使用的每个环境变量的含义。我们将在[*第4章*](B20959_04.xhtml#_idTextAnchor060)，*使用专用库*，中详细介绍这个主题。
- en: Although the PyTorch package installed from PIP comes with the GNU OpenMP library
    by default, the Intel version tends to provide better results in machines endowed
    with Intel CPUs. As the hardware machine used in this test possesses an Intel
    CPU, it is recommended to use the Intel version of OpenMP instead of the implementation
    provided by the GNU project.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过PIP安装的PyTorch包默认包含GNU OpenMP库，但在装有Intel CPU的机器上，Intel版本往往会提供更好的结果。由于本次测试所用的硬件机器配备了Intel
    CPU，建议使用Intel版本的OpenMP而不是GNU项目提供的实现。
- en: We can see that changing a few things in the environment layer can result in
    a relevant performance gain without consuming much time or effort to implement.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在环境层中进行少量更改可以在不消耗大量时间或精力的情况下获得显著的性能提升。
- en: The next section provides some questions to help you retain what you have learned
    in this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节提供了一些问题，帮助你巩固本章学到的内容。
- en: Quiz time!
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验时间！
- en: Let’s review what we have learned in this chapter by answering a few questions.
    At first, try to answer these questions without consulting the material.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答一些问题来回顾我们在本章学到的内容。起初，试着在不查阅材料的情况下回答这些问题。
- en: Important note
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The answers to all these questions are available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter02-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter02-answers.md).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题的答案都可以在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter02-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter02-answers.md)找到。
- en: Before starting the quiz, remember that it is not a test at all! This section
    aims to complement your learning process by revising and consolidating the content
    covered in this chapter.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始测验之前，请记住这根本不是一次测试！本节旨在通过复习和巩固本章涵盖的内容来补充你的学习过程。
- en: 'Choose the correct answers for the following questions:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为以下问题选择正确答案：
- en: After running the training process using two GPUs in a single machine, we decided
    to add two extra GPUs to accelerate the training process. In this case, we tried
    to improve the performance of the training process by applying which of the following?
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在单台机器上使用两个GPU运行训练过程后，我们决定添加两个额外的GPU来加速训练过程。在这种情况下，我们试图通过以下哪种方式来提高训练过程的性能？
- en: Horizontal scaling.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 水平扩展。
- en: Vertical scaling.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 纵向扩展。
- en: Transversal scaling.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 横向扩展。
- en: Distributed scaling.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式扩展。
- en: The training process of a simple model is taking a long time to finish. After
    adjusting the batch size and cutting of one of the convolutional layers, we could
    train the model faster while achieving the same accuracy. In this case, we improve
    the performance of the training process by changing which of the following layers
    of the software stack?
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单模型的训练过程花费了很长时间才能完成。调整批量大小并删减一个卷积层后，我们可以在保持相同精度的情况下更快地训练模型。在这种情况下，我们通过更改以下软件栈的哪个层来改善训练过程的性能？
- en: Application layer.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用层。
- en: Hardware layer.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件层。
- en: Environment layer.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境层。
- en: Execution layer.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行层。
- en: Which of the following changes is applied to the environment layer?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪种变化应用于环境层？
- en: Modify the hyperparameters.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改超参数。
- en: Adopt another network architecture.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采用另一种网络架构。
- en: Update the framework’s version.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新框架的版本。
- en: Set a parameter in the operating system.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在操作系统中设置参数。
- en: Which one of the following components lies in the execution layer?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个组件位于执行层？
- en: OpenMP.
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenMP。
- en: PyTorch.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch。
- en: Apptainer.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Apptainer。
- en: NCCL.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: NCCL。
- en: As users of a given environment, we usually do not modify anything at the execution
    layer. What is the reason for that?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为特定环境的用户，我们通常不修改执行层的任何内容。这是什么原因呢？
- en: We usually do not have administrative rights to change anything at the execution
    layer.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通常没有管理权限来更改执行层的任何内容。
- en: There is no change at the execution layer that could accelerate the training
    process.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行层没有任何变化可以加快训练过程。
- en: The execution and application layers are almost the same thing. So, there is
    no difference between changing one or another layer.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行层和应用层几乎是相同的。因此，在更改其中一个层和另一个层之间没有区别。
- en: As we usually execute the training process on containers, there is no change
    on the execution layer that could improve the training process.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们通常在容器上执行训练过程，所以在执行层没有任何变化可以改善训练过程。
- en: We have accelerated the training process of a given model by using two additional
    machines and applying a given capability provided by the machine learning framework.
    In this case, which of the following actions have we taken to improve the training
    process?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用两台额外的机器和应用机器学习框架提供的特定能力，我们加速了给定模型的训练过程。在这种情况下，我们采取了哪些措施来改进训练过程？
- en: We have performed horizontal and vertical scaling.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们进行了水平和垂直扩展。
- en: We have performed horizontal scaling and increased the number of resources.
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经进行了水平扩展并增加了资源数量。
- en: We have performed horizontal scaling and applied changes to the environment
    layer.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经进行了水平扩展并应用了对环境层的变更。
- en: We have performed horizontal scaling and applied changes to the execution layer.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经进行了水平扩展并应用了对执行层的变更。
- en: Controlling the behavior of a library through environment variables is a change
    that is applied in which of the following layers?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过环境变量控制库的行为是应用在以下哪个层次上的变更？
- en: Application layer.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用层。
- en: Environment layer.
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境层。
- en: Execution layer.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行层。
- en: Hardware layer.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件层。
- en: Increasing the batch size can improve the performance of the training process.
    However, it can also present which of the following side effects?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加批量大小可以提高训练过程的性能。但它也可能导致以下哪些副作用？
- en: Reduce the number of samples.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少样本数量。
- en: Reduce the number of operations.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少操作数量。
- en: Reduce the number of training steps.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少训练步骤的数量。
- en: Reduce model accuracy.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低模型精度。
- en: Let’s summarize what we’ve covered in this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章中涵盖的内容。
- en: Summary
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We reached the end of the introductory part of the book. We started this chapter
    by learning the approaches we can take to reduce the training time. Next, we learned
    what kind of modifications we can perform in the application and environment layers
    to accelerate the training process.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了书的介绍部分。我们从学习如何减少训练时间的方法开始了本章。接下来，我们了解了可以在应用和环境层中进行的修改，以加速训练过程。
- en: We have experienced, in practice, how changing a few things in the code or environment
    can result in impressive performance improvements.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实践中经历了如何在代码或环境中进行少量更改，从而实现令人印象深刻的性能改进。
- en: 'You are ready to move on in the performance journey! In the next chapter, you
    will learn how to apply one of the most exciting capabilities provided by PyTorch
    2.0: model compilation.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经准备好在性能之旅中继续前进！在下一章中，您将学习如何应用 PyTorch 2.0 提供的最令人兴奋的功能之一：模型编译。
