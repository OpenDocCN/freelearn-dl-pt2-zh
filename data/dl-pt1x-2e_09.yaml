- en: Implementing Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自编码器
- en: This chapter addresses the notion of semi-supervised learning algorithms through
    the introduction of autoencoders, and then moves on to **restricted Boltzmann
    machines** (**RBMs**) and **deep belief networks** (**DBNs**) in order to understand
    the probability distribution of data. The chapter will give you an overview as
    to how these algorithms have been applied to some real-world problems. Coded examples
    implemented in PyTorch will also be provided.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了半监督学习算法的概念，通过引入自编码器，然后进入**受限玻尔兹曼机**（**RBMs**）和**深度信念网络**（**DBNs**），以理解数据的概率分布。本章将概述这些算法如何应用于一些实际问题。还将提供在
    PyTorch 中实现的编码示例。
- en: Autoencoders are an unsupervised learning technique. They can take an unlabeled
    dataset and task it with reconstructing the original input by modeling it using
    an unsupervised learning problem as opposed to a supervised one. The goal of the
    autoencoder is for the input to be as similar as possible to the output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种无监督学习技术。它可以接收无标签的数据集，并通过建模来重建原始输入，将问题建模为无监督学习，而不是监督学习。自编码器的目标是使输入与输出尽可能相似。
- en: 'Specifically, the following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主题：
- en: An overview of autoencoders and their applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器及其应用概述
- en: Bottleneck and loss functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓶颈和损失函数
- en: Different types of autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的自编码器
- en: Restricted Boltzmann machines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: Deep belief networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: Applications of autoencoders
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器的应用
- en: 'Autoencoders fall under representational learning and are used to find a compressed
    representation of the inputs. They are composed of an encoder and a decoder. The
    following diagram shows the structure of an autoencoder:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器属于表征学习，用于找到输入的压缩表示。它们由编码器和解码器组成。以下图示显示了自编码器的结构：
- en: '![](img/693d5531-ef4e-45a0-aa19-a9c0854da1ec.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/693d5531-ef4e-45a0-aa19-a9c0854da1ec.png)'
- en: 'Examples of applications of autoencoders include the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的应用示例包括以下几种：
- en: Data denoising
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据去噪
- en: Dimensionality reduction for data visualization
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化的降维
- en: Image generation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像生成
- en: Interpolating text
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插值文本
- en: Bottleneck and loss functions
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 瓶颈和损失函数
- en: 'Autoencoders impose a bottleneck on the network, enforcing a compressed knowledge
    representation of the original input. The network would simply learn to memorize
    the input values if the bottleneck were not present. As such, this would mean
    that the model wouldn''t generalize well on unseen data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器对网络施加了一个瓶颈，强制使原始输入的知识表示被压缩。如果没有瓶颈的话，网络将简单地学会记忆输入值。因此，这意味着模型在未见数据上的泛化能力不会很好：
- en: '![](img/a862aa43-423e-4616-b93e-6c317b509ee9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a862aa43-423e-4616-b93e-6c317b509ee9.png)'
- en: 'In order for the model to detect a signal, we need it to be sensitive to the
    input but not so much that it simply memorizes them and doesn''t predict well
    on unseen data. In order to determine the optimal trade-off, we need to construct
    a loss/cost function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型能够检测到信号，我们需要它对输入具有敏感性，但不能简单地记住它们，而在未见数据上预测效果不佳。为了确定最优权衡，我们需要构建一个损失/成本函数：
- en: '![](img/8e2bfc83-5110-405d-9e0a-74b6c8312912.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e2bfc83-5110-405d-9e0a-74b6c8312912.png)'
- en: There are some commonly used autoencoder architectures for imposing these two
    constraints and ensuring there is an optimal trade-off between the two.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些常用的自编码器架构，用于施加这两个约束条件，并确保在两者之间有最优的权衡。
- en: Coded example – standard autoencoder
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - 标准自编码器
- en: 'In this example, we will show you how to compile an autoencoder model in PyTorch:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将展示如何在 PyTorch 中编译一个自编码器模型：
- en: 'Firstly, import the relevant libraries:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入相关的库：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, define the model parameters:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义模型参数：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, initiate a function to transform the images in the MNIST dataset:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，初始化一个函数来转换 MNIST 数据集中的图像：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the autoencoder class in which to feed the data and initiate the model:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义自编码器类，用于提供数据并初始化模型：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define a function that will output the images from the model after each epoch:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，它将在每个 epoch 后从模型输出图像：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now run the model over each epoch and review the results of the reconstructed
    images:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在在每个 epoch 上运行模型并查看重建图像的结果：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will give the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![](img/b9ecb6f8-ef83-48aa-8e49-e29b3a0f6bca.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9ecb6f8-ef83-48aa-8e49-e29b3a0f6bca.png)'
- en: 'And the following image shows the output of the autoencoder at each epoch:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图片显示了每个 epoch 的自编码器输出：
- en: '![](img/ab6c7350-99d9-4cb1-bde0-602292f98856.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab6c7350-99d9-4cb1-bde0-602292f98856.png)'
- en: The more epochs that pass, the clearer the images become as the model continues
    to learn.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着经过的 epoch 越来越多，图像变得越来越清晰，因为模型继续学习。
- en: Convolutional autoencoders
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: Autoencoders can be used with convolutions instead of fully connected layers.
    This can be done using 3D vectors instead of 1D vectors. In the context of images,
    downsampling the image forces the autoencoder to learn a compressed version of
    it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以使用卷积而不是全连接层。这可以通过使用 3D 向量而不是 1D 向量来实现。在图像的背景下，对图像进行下采样迫使自编码器学习其压缩版本。
- en: Coded example – convolutional autoencoder
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 – 卷积自编码器
- en: 'In this example, we will show you how to compile a convolutional autoencoder:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将展示如何编译一个卷积自编码器：
- en: 'As before, you obtain the train and test datasets from the MNIST dataset and
    define the model parameters:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与以前一样，您从 MNIST 数据集获取训练和测试数据集，并定义模型参数：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From here, initiate the model for the convolutional autoencoder:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里开始，启动卷积自编码器模型：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, run the model over each epoch while saving the output images for reference:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在每个 epoch 运行模型同时保存输出图像以供参考：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can look at the saved images after every epoch in the folder that is mentioned
    in the code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在代码中提到的文件夹中，每个 epoch 后查看保存的图像。
- en: Denoising autoencoders
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: 'Denoising encoders deliberately add noise to the input of the network. These
    autoencoders essentially create a corrupted copy of the data. In doing so, this
    helps the encoder to learn the latent representation in the input data, making
    it more generalizable:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪编码器故意向网络的输入添加噪声。这些自编码器实质上创建了数据的损坏副本。通过这样做，这有助于编码器学习输入数据中的潜在表示，使其更具普适性：
- en: '![](img/7fb53730-c36b-4900-be3b-39f76e76f65e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fb53730-c36b-4900-be3b-39f76e76f65e.png)'
- en: 'This corrupted image is fed into the network in the same way as other standard
    autoencoders:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损坏的图像与其他标准自编码器一样被送入网络：
- en: '![](img/0c46e5ba-3ff4-4584-b559-6f772c705114.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c46e5ba-3ff4-4584-b559-6f772c705114.png)'
- en: As we can see, noises were added in the original input and the encoder encodes
    the input and sends it to the decoder, which then decodes the noisy input into
    the cleaned output. Thus, we have looked at various applications that autoencoders
    can be used for. We will now look at a specific type of autoencoder, which is
    a **variational autoencoder** (**VAE**).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，原始输入中添加了噪声，编码器对输入进行编码并将其发送到解码器，解码器然后将嘈杂的输入解码为清理后的输出。因此，我们已经看过自编码器可以用于的各种应用。现在我们将看看一种特定类型的自编码器，即**变分自编码器**（**VAE**）。
- en: Variational autoencoders
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: VAEs are different from the standard autoencoders we have considered so far
    as they describe an observation in latent space in a probabilistic manner rather
    than deterministic. A probability distribution for each latent attribute is output,
    rather than a single value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs 与我们迄今考虑过的标准自编码器不同，因为它们以概率方式描述潜在空间中的观察结果，而不是确定性方式。每个潜在属性的概率分布被输出，而不是单个值。
- en: Standard autoencoders have somewhat limited applications in the real world as
    they are only really useful when you want to replicate the data that has been
    put into it. Since VAEs are generative models, they can be applied to cases where
    you don't want to output data that is the same as the input.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 标准自编码器在现实世界中的应用有些受限，因为它们只在您想要复制输入的数据时才真正有用。由于 VAEs 是生成模型，它们可以应用于您不希望输出与输入相同的数据的情况。
- en: 'Let''s consider this in a real-world context. When training an autoencoder
    model on a dataset of faces, you would hope that it would learn latent attributes,
    such as whether the person is smiling, their skin tone, whether they are wearing
    glasses, and more:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在现实世界的背景下考虑这个问题。当在面部数据集上训练自编码器模型时，您希望它能学习潜在属性，比如一个人是否微笑，他们的肤色，是否戴眼镜等等：
- en: '![](img/4cc640bb-841f-438d-a6cc-79837d7e85dd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc640bb-841f-438d-a6cc-79837d7e85dd.png)'
- en: As we can see in the preceding diagram, standard autoencoders represent these
    latent attributes as discrete values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的图中所示，标准自编码器将这些潜在属性表示为离散值。
- en: 'If we allow each feature to be within a range of possible values rather than
    a single value, we can use VAEs to describe the attributes in probabilistic terms:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们允许每个特征在可能值的范围内而不是单个值内，我们可以使用 VAEs 以概率术语描述属性：
- en: '![](img/76947b8a-b694-4db9-b32a-498f4972ba45.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76947b8a-b694-4db9-b32a-498f4972ba45.png)'
- en: The preceding diagram depicts how we can represent whether the person is smiling
    as either a discrete value or as a probability distribution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示了我们如何将一个人是否微笑表示为离散值或概率分布。
- en: 'The distribution of each latent attribute is sampled from the image in order
    to generate the vector that is used as the input for the decoder model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 每个潜在属性的分布是从图像中采样的，以生成用作解码器模型输入的向量：
- en: '![](img/07851c42-ae8a-4888-a8d8-39704d6948da.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07851c42-ae8a-4888-a8d8-39704d6948da.png)'
- en: 'Two vectors are output, as shown in the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，输出两个向量：
- en: '![](img/d6e926bd-3db3-4433-969d-9237af0bf3a0.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6e926bd-3db3-4433-969d-9237af0bf3a0.png)'
- en: One describes the mean and the other describes the variance of the distributions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个描述平均值，另一个描述分布的方差。
- en: Training VAEs
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练VAE
- en: During training, we calculate the relationship of each parameter in the network
    with respect to the overall loss using a process called **backpropagation**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们使用反向传播计算网络中每个参数与整体损失的关系。
- en: 'Standard autoencoders use backpropagation in order to reconstruct the loss
    value across the weights of the network. As the sampling operation in VAEs is
    not differentiable, the gradients cannot be propagated from the reconstruction
    error. The following diagram explains this further:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 标准自动编码器使用反向传播来在网络权重上重建损失值。由于VAE中的采样操作不可微，不能从重构误差中传播梯度。以下图表进一步解释了这一点：
- en: '![](img/12949e89-c9f1-491d-a822-7e82befa9fa2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12949e89-c9f1-491d-a822-7e82befa9fa2.png)'
- en: 'In order to overcome this limitation, the reparameterization trick can be used.
    The reparameterization trick samples ε from a unit normal distribution, shifts
    it by the mean 𝜇 of the latent attribute, and, then scales it by the latent attribute''s
    variance 𝜎:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一限制，可以使用重参数化技巧。重参数化技巧从单位正态分布中采样ε，将其平移至潜在属性的均值𝜇，并按潜在属性的方差𝜎进行缩放：
- en: '![](img/6376b4ec-ebd3-4464-adcd-4c39f59e1313.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6376b4ec-ebd3-4464-adcd-4c39f59e1313.png)'
- en: 'This removes the sampling process from the flow of gradients as it is now outside
    of the network. As such, the sampling process doesn''t depend on anything in the
    network. We can now optimize the parameters of the distribution while maintaining
    the ability to randomly sample from it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这将采样过程从梯度流中移除，因为现在它位于网络之外。因此，采样过程不依赖于网络中的任何东西。现在我们可以优化分布的参数，同时保持从中随机采样的能力：
- en: '![](img/0727f2d1-a7f3-4436-a031-7c6a717b2192.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0727f2d1-a7f3-4436-a031-7c6a717b2192.png)'
- en: 'We can transform with a mean, 𝜇, and covariance matrix ∑ as per the following,
    as the distribution of each attribute is Gaussian:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过均值𝜇和协方差矩阵∑对其进行变换，因为每个属性的分布是高斯分布：
- en: '![](img/08421e67-3535-434a-9586-d98bf7e661a4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08421e67-3535-434a-9586-d98bf7e661a4.png)'
- en: Here, ε ~ N(0,1).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，ε ~ N(0,1)。
- en: 'We can now train the model using simple backpropagation with the introduction
    of the reparameterization trick:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用简单的反向传播来训练模型，并引入重参数化技巧：
- en: '![](img/20d8edd5-d1d0-4190-942c-710c7cd089cb.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20d8edd5-d1d0-4190-942c-710c7cd089cb.png)'
- en: As seen in the preceding diagram, we have trained the autoencoder to smooth
    out the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图表所示，我们已经训练了自动编码器以平滑图像。
- en: Coded example – VAE
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - VAE
- en: 'In order to code a VAE in PyTorch, we can load the libraries and dataset like
    we did in the previous examples. From here, we can define the VAE class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PyTorch中编写VAE，我们可以像在之前的示例中那样加载库和数据集。从这里，我们可以定义VAE类：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then define the loss function with the help of KL divergence and initiate
    the model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用KL散度来定义损失函数，并初始化模型：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From here, we can run the model over each epoch and save the output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以运行模型的每个时期并保存输出：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have seen the various autoencoders and how to compile them, let
    us learn how to implement them in recommender systems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过各种自动编码器及其如何编译它们，让我们学习如何在推荐系统中实现它们。
- en: Restricted Boltzmann machines
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: An **RBM** is an algorithm that has been widely used for tasks such as collaborative
    filtering, feature extraction, topic modeling, and dimensionality reduction. They
    can learn patterns in a dataset in an unsupervised fashion.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**RBM**是一种广泛用于协同过滤、特征提取、主题建模和降维等任务的算法。它们可以无监督地学习数据集中的模式。'
- en: For example, if you watch a movie and say whether you liked it or not, we could
    use an RBM to help us determine the reason why you made this decision.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你观看电影并说出你是否喜欢它，我们可以使用一个**RBM**来帮助我们确定你做出这个决定的原因。
- en: 'The goal of RBM is to minimize energy defined by the following formula, which
    depends on the configurations of visible/input states, hidden states, weights,
    and biases:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RBM的目标是最小化能量，由以下公式定义，其依赖于可见/输入状态、隐藏状态、权重和偏置的配置：
- en: '![](img/b2d5c938-993c-4018-8312-747bdca6ea70.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2d5c938-993c-4018-8312-747bdca6ea70.png)'
- en: 'RBMs are two-layer networks that are the fundamental building blocks of a DBN.
    The first layer of an RBM is a visible/input layer of neurons and the second is
    the hidden layer of neurons:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是DBN的基本构建块的两层网络。RBM的第一层是神经元的可见/输入层，第二层是隐藏层的神经元：
- en: '![](img/e6352c09-c581-41b7-858f-b5bf82a3f269.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6352c09-c581-41b7-858f-b5bf82a3f269.png)'
- en: The RBM translates the inputs from the visible layer and translates them into
    a set of numbers. Through several forward and backward passes, the number is then
    translated back to reconstruct the inputs. The restriction in the RBM is such
    that nodes in the same layer are not connected.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: RBM将输入从可见层翻译成一组数字。通过几次前向和后向传递，该数字然后被翻译回重构输入。在RBM中的限制是同一层中的节点不连接。
- en: 'A low-level feature is fed into each node of the visible layer from the training
    dataset. In the case of image classification, each node would receive one pixel
    value for each pixel in an image:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练数据集中的每个节点的低级特征被馈送到可见层的每个节点。在图像分类的情况下，每个节点将为图像中每个像素接收一个像素值：
- en: '![](img/a3310e3c-b99d-4ff3-97bf-aa1ff08491dc.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3310e3c-b99d-4ff3-97bf-aa1ff08491dc.png)'
- en: 'Following one pixel through the network, the input *x* is multiplied by the
    weight from the hidden layer and a bias is then added. From here, this is then
    fed into an activation function, which produces the output, which is essentially
    the strength of the signal passing through it given the input *x*, as shown in
    the following diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网络跟踪一个像素，输入*x*被隐藏层的权重乘以，然后加上偏置。然后，这被输入到激活函数中，产生输出，这实质上是通过它传递的信号强度，给定输入*x*，如下图所示：
- en: '![](img/02467675-a93e-4d99-81ca-01aa85c3ac72.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02467675-a93e-4d99-81ca-01aa85c3ac72.png)'
- en: 'At each node in the hidden layer, x from each pixel value is multiplied by
    a separate weight. The products are then summed, and a bias is added. The output
    of this is then passed through an activation function, producing the output at
    that single node:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在隐藏层的每个节点，来自每个像素值的*x*被单独的权重乘以。然后将这些乘积求和，并添加偏置。然后将其输出通过激活函数，产生该单个节点的输出：
- en: '![](img/00ee7d87-14b2-4e4a-8776-4c879004bb8b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00ee7d87-14b2-4e4a-8776-4c879004bb8b.png)'
- en: 'At each point in time, the RBM is in a certain state, which refers to the values
    of the neurons in the visible *v* and hidden *h* layers. The probability of such
    a state can be given by the following joint distribution function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时刻，RBM处于某种状态，这指的是可见*v*和隐藏*h*层中神经元的值。这种状态的概率可以由以下联合分布函数给出：
- en: '![](img/125757b1-9b57-487d-9133-726cc7addde4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/125757b1-9b57-487d-9133-726cc7addde4.png)'
- en: Here, Z is the partition function that is the summation over all possible pairs
    of visible and hidden vectors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，Z是分区函数，是对所有可能的可见和隐藏向量对的求和。
- en: Training RBMs
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RBM
- en: 'There are two main steps an RBM carries out during training:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，RBM执行两个主要步骤：
- en: '**Gibbs sampling**: The first step in the training process uses Gibbs sampling,
    which repeats the following process *k* times:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**吉布斯采样**：训练过程的第一步使用吉布斯采样，它重复以下过程*k*次：'
- en: Probability of hidden vector given the input vector; prediction of the hidden
    values.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定输入向量的隐藏向量的概率；预测隐藏值。
- en: Probability of the input vector given the hidden vector; prediction of the input
    values. From this, we obtain another input vector, which was recreated from the
    original input values.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定隐藏向量的输入向量的概率；预测输入值。从这里，我们获得另一个输入向量，该向量是从原始输入值重新创建的。
- en: '**Contrastive divergence**: RBMs adjust their weights through contrastive divergence.
    During this process, weights for visible nodes are randomly generated and used
    to generate hidden nodes. The hidden nodes then use the same weights to reconstruct
    visible nodes. The weights used to reconstruct the visible nodes are the same
    throughout. However, the generated nodes are not the same because they aren''t
    connected to each other.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**对比散度**：RBM通过对比散度调整它们的权重。在此过程中，可见节点的权重是随机生成的，并用于生成隐藏节点。然后，隐藏节点再使用相同的权重重构可见节点。用于重构可见节点的权重在整个过程中是相同的。但是生成的节点不同，因为它们之间没有连接。'
- en: 'Once an RBM has been trained, it is essentially able to express two things:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦RBM训练完成，它基本上能够表达两件事情：
- en: The interrelationship between the features of the input data
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据特征之间的相互关系
- en: Which features are the most important when identifying patterns
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在识别模式时哪些特征最重要
- en: Theoretical example – RBM recommender system
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论示例 - RBM推荐系统
- en: In the context of movies, we can use RBMs to uncover a set of latent factors
    that represent their genre, and consequently determine which genre of film a person
    likes. For example, if we were to ask someone to tell us which movies they have
    watched and whether they liked them or not, we can then represent them as binary
    inputs (1 or 0) to the RBM. For those movies they haven't seen or haven't told
    us about, we need to assign a value of -1 so that the network can identify those
    during training and ignore their associated weights.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在电影的背景下，我们可以使用RBM揭示一组代表它们类型的潜在因素，从而确定一个人喜欢哪种电影类型。例如，如果我们要求某人告诉我们他们看过哪些电影以及是否喜欢，我们可以将它们表示为二进制输入（1或0）到RBM中。对于那些他们没看过或没告诉我们的电影，我们需要分配一个值为-1，这样网络在训练时可以识别并忽略它们的关联权重。
- en: 'Let''s consider an example where a user likes *Mrs. Doubtfire*, *The Hangover*,
    and *Bridesmaids*, does not like *Scream* or *Psycho*, and has not yet seen *The
    Hobbit*. Given these inputs, the RBM may identify three hidden factors: comedy,
    horror, and fantasy, which correspond to the genres of the films:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个示例，用户喜欢*老妈妈，我来了*，*宿醉*和*伴娘*，不喜欢*尖叫*或*心理*，还没有看过*霍比特人*。根据这些输入，RBM可能识别出三个隐藏因子：喜剧、恐怖和奇幻，这些因子对应于电影的类型：
- en: '![](img/6d8e29ec-2944-4f90-a8c8-aaab69ca68c5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d8e29ec-2944-4f90-a8c8-aaab69ca68c5.png)'
- en: For each hidden neuron, the RBM assigns a probability of the hidden neuron given
    the input neuron. The final binary values of the neurons are obtained by sampling
    from the Bernoulli distribution.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个隐藏神经元，RBM分配了给定输入神经元的隐藏神经元的概率。神经元的最终二进制值是通过从伯努利分布中抽样得到的。
- en: In the preceding example, the only hidden neuron that represents the comedy genre becomes
    active. As such, given the movie ratings input into the RBM, it predicts that
    the user likes comedy films the most.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，代表喜剧类型的唯一隐藏神经元变得活跃。因此，给定输入到RBM的电影评分，它预测用户最喜欢喜剧电影。
- en: For the trained RBM to make predictions on movies the user has not yet seen,
    based on their preference, the RBM uses the probability of the visible neurons
    given the hidden neurons. It samples from the Bernoulli distribution to find out
    which one of the visible neurons can then become active.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已训练的RBM来说，要预测用户尚未看过的电影，基于他们的喜好，RBM使用可见神经元给定隐藏神经元的概率。它从伯努利分布中进行抽样，以确定哪个可见神经元可以变为活跃状态。
- en: Coded example – RBM recommender system
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码示例 - RBM推荐系统
- en: Continuing in the context of movies, we will now give an example of how how
    we can build an RBM recommender system using the PyTorch library. The goal of
    the example is to train a model to determine whether a user will like a movie
    or not.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 继续在电影的背景下，我们将展示如何使用PyTorch库构建一个RBM推荐系统的示例。该示例的目标是训练一个模型来确定用户是否会喜欢一部电影。
- en: 'In this example, we use the MovieLens dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/))
    with 1 million ratings, which was created by the GroupLens Research Group at the
    University of Minnesota:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了MovieLens数据集（[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)），包含100万条评分，这个数据集由明尼苏达大学的GroupLens研究组创建：
- en: 'Firstly, download the datasets. This can be done through terminal commands
    as follows:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，下载数据集。可以通过终端命令完成如下操作：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now import the libraries that we will use:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在导入我们将要使用的库：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then import the data:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后导入数据：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following screenshot illustrates the structure of our dataset:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了我们数据集的结构：
- en: '![](img/272925c4-059f-410c-b3f1-0833472cddf1.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/272925c4-059f-410c-b3f1-0833472cddf1.png)'
- en: 'Prepare the testing and training datasets:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备测试和训练数据集：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we need to prepare a matrix with the users'' ratings. The matrix will have
    users as rows and movies as columns. Zeros are used to represent cases where a
    user didn''t rate a particular movie. We define the `no_users` and `no_movies`
    variables then consider the `max` value in the training and testing datasets as
    follows:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要准备一个包含用户评分的矩阵。该矩阵将以用户为行，电影为列。零用于表示用户未对特定电影评分的情况。我们定义`no_users`和`no_movies`变量，然后考虑训练和测试数据集中的最大值如下：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we define a function named `convert_dataset` that converts the dataset
    into a matrix. It does so by creating a loop that will run through the dataset
    and fetch all of the movies rated by a specific user along with the ratings by
    that same user. We firstly create a matrix of zeros since there are movies the
    user didn''t rate:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们定义一个名为 `convert_dataset` 的函数，将数据集转换为矩阵。它通过创建一个循环来运行数据集，并获取特定用户评分的所有电影及该用户的评分。因为用户没有评级过的电影有许多，所以我们首先创建一个全零矩阵：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we convert the data into Torch tensors by using the `FloatTensor` utility.
    This will convert the dataset into PyTorch arrays:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用 `FloatTensor` 实用程序将数据转换为 Torch 张量。这将把数据集转换为 PyTorch 数组：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this example, we want to make a binary classification, which is whether
    the user will like the movie or not. As such, we convert the ratings into zeros
    and ones. First, however, we replace the existing zeros with -1 in order to represent
    movies that a user never rated:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们想要进行二元分类，即用户是否喜欢这部电影。因此，我们将评分转换为零和一。但是首先，我们将现有的零替换为 -1，以表示用户从未评级过的电影：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we need to create a class in order to define the architecture of the RBM.
    The class initializes the weight and bias by using a random normal distribution.
    Two types of biases are also defined, where `a` is the probability of the hidden
    nodes given the visible nodes and `b` is the probability of the visible nodes
    given the hidden nodes. The class creates a `sample_hidden_nodes` function that
    takes `x` as an argument and represents the visible neurons. From here, we compute
    the probability of `h` given `v`, where `h` and `v` represent the hidden and visible
    nodes respectively. This represents the sigmoid activation function. It is computed
    as the product of the vector of the weights and `x` plus the bias `a`. Since we
    are considering the model for binary classification, we return Bernoulli samples
    of the hidden neurons. From here, we create a `sample_visible_function` function that
    will sample the visible nodes. Finally, we create the training function. It takes
    the input vector containing the movie ratings, the visible nodes obtained after
    *k* samplings, the vector of probabilities, and the probabilities of the hidden
    nodes after *k* samplings:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个类来定义 RBM 的架构。该类通过使用随机正态分布初始化权重和偏置。还定义了两种类型的偏置，其中 `a` 是给定可见节点时隐藏节点的概率，`b`
    是给定隐藏节点时可见节点的概率。该类创建了一个 `sample_hidden_nodes` 函数，它以 `x` 作为参数并表示可见神经元。从这里，我们计算给定
    `v` 的 `h` 的概率，其中 `h` 和 `v` 分别表示隐藏和可见节点。这代表了 S 型激活函数。它计算为权重向量和 `x` 的乘积加上偏置 `a`。由于我们考虑的是二元分类模型，我们返回隐藏神经元的伯努利样本。从这里，我们创建一个
    `sample_visible_function` 函数，它将对可见节点进行采样。最后，我们创建训练函数。它接受包含电影评分的输入向量、*k* 次采样后获得的可见节点、概率向量以及
    *k* 次采样后的隐藏节点的概率：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we define our model parameters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们定义我们的模型参数：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From here, we can train the model for each epoch:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，我们可以为每个 epoch 训练模型：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can plot the error across epochs during training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练过程中绘制跨 epoch 的错误：
- en: '![](img/c613abe5-4900-4baa-bf32-656a44fdf51a.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c613abe5-4900-4baa-bf32-656a44fdf51a.png)'
- en: This can help us to determine how many epochs we should run the training for.
    It shows that after six epochs, the improved performance rate drops and hence
    we should consider stopping the training at this stage.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以帮助我们确定应该运行多少个 epoch 进行训练。显示在六个 epoch 后，改进的性能率下降，因此我们应该考虑在这个阶段停止训练。
- en: We have seen the coded example of implementing a recommender system in RBM,
    now let's briefly walk through a DBN architecture.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了在 RBM 中实现推荐系统的编码示例，现在让我们简要地浏览一下 DBN 架构。
- en: DBN architecture
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBN 架构
- en: 'A DBN is a multilayer belief network where each layer is an RBM stacked against
    one another. Apart from the first and final layers of the DBN, each layer serves
    as both a hidden layer to the nodes before it and as the input layer to the nodes
    that come after it:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: DBN 是一个多层信念网络，每一层都是一个叠加的 RBM。除了 DBN 的第一层和最后一层之外，每一层既作为其前面节点的隐藏层，又作为其后节点的输入层：
- en: '![](img/476edcd1-4181-4f6b-8111-adf963439fc9.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/476edcd1-4181-4f6b-8111-adf963439fc9.png)'
- en: 'Two layers in the DBN are connected by a matrix of weights. The top two layers
    of a DBN are undirected, which gives a symmetric connection between them, forming
    an associative memory. The lower two layers have direct connections to the layers
    above. The sense of direction converts associative memory into observed variables:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: DBN中的两个层通过权重矩阵连接。DBN的顶部两层是无向的，它们之间形成对称连接，形成联想存储器。较低的两层直接连接到上面的层。方向感将联想存储器转换为观察变量：
- en: '![](img/884509b6-a309-48a5-a398-c1a3f3cde68f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/884509b6-a309-48a5-a398-c1a3f3cde68f.png)'
- en: 'The two most significant properties of DBNs are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DBN的两个最显著特性如下：
- en: A DBN learns top-down, generative weights via an efficient, layer-by-layer procedure.
    These weights determine how the variables in one layer depend on the layer above.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN通过高效的逐层过程学习自顶向下的生成权重。这些权重决定了一个层中的变量如何依赖于上面的层。
- en: Once training is complete, the values of the hidden variables in each layer
    can be inferred by a single bottom-up pass. The pass begins with a visible data
    vector in the lower layer and uses its generative weights in the opposite direction.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完成后，可以通过单个自下而上的传递推断每层隐藏变量的值。传递从底层的可见数据向量开始，并使用其生成权重相反方向。
- en: 'The probability of a joint configuration network over both visible and hidden
    layers depends on the joint configuration network''s energy compared with the
    energy of all other joint configuration networks:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 联合配置网络的概率在可见层和隐藏层之间的联合配置网络的能量依赖于所有其他联合配置网络的能量：
- en: '![](img/9b151fd9-0ff8-4e58-b605-3c292d38b857.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b151fd9-0ff8-4e58-b605-3c292d38b857.png)'
- en: Once the pretraining phase of the DBN has been completed by the RBM stack, a
    feedforward network can then be used for the fine-tuning phase in order to create
    a classifier or simply help cluster unlabeled data in an unsupervised learning
    scenario.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦RBMs堆栈完成了DBN的预训练阶段，就可以使用前向网络进行微调阶段，从而创建分类器或在无监督学习场景中简单地帮助聚类无标签数据。
- en: Fine-tuning
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: Fine-tuning aims to find the optimal values of the weights between the layers.
    It tweaks the original features in order to obtain more precise boundaries of
    the classes. In order to help the model associate patterns and features to the
    datasets, a small labeled dataset is used.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的目标是找到层间权重的最优值。它微调原始特征，以获得更精确的类边界。为了帮助模型将模式和特征关联到数据集，使用了一个小的标记数据集。
- en: Fine-tuning can be applied as a stochastic bottom-up pass and then used to adjust
    the top-down weights. Once the top is reached, recursion is applied to the top
    layer. In order to fine-tune further, we can do a stochastic top-down pass and
    adjust the bottom-up weights.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可以作为随机的自下而上传递应用，然后用于调整自上而下的权重。一旦达到顶层，递归被应用于顶层。为了进一步微调，我们可以进行随机的自上而下传递，并调整自下而上的权重。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explained what autoencoders are and different variations
    of them. Throughout the chapter, we gave some coded examples of how how they can
    be applied to the MNIST dataset. We later introduced RBMs and explained how these
    can be developed into a DBN along with some additional examples.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解释了自编码器及其不同的变体。在整个章节中，我们提供了一些编码示例，展示它们如何应用于MNIST数据集。后来我们介绍了受限玻尔兹曼机，并解释了如何将其开发成深度玻尔兹曼机，同时提供了额外的示例。
- en: In the next chapter, we will introduce generative adversarial networks. We will
    show how they can be used to generate both images and text.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍生成对抗网络，并展示它们如何用于生成图像和文本。
- en: Further reading
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following for further information:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的信息请参考以下内容：
- en: '*Tutorial on Variational Autoencoders*: [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变分自编码器教程*: [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)'
- en: '*CS598LAZ – Variational Autoencoders*: [http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf](http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CS598LAZ – 变分自编码器*: [http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf](http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf)'
- en: '*Auto-Encoding Variational Bayes*: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*自编码变分贝叶斯*: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)'
- en: '*Deep Learning Book*: [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习书籍*: [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)'
- en: '*A Fast Learning Algorithm for Deep Belief Nets*: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度信念网快速学习算法*: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
- en: '*Training restricted Boltzmann machines: An introduction*: [https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495](https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练受限玻尔兹曼机：简介*: [https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495](https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495)'
- en: '*Deep Boltzmann Machines*: [http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度玻尔兹曼机*: [http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)'
- en: '*A Practical Guide to Training Restricted Boltzmann Machines*: [https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练受限玻尔兹曼机实用指南*: [https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)'
- en: '*Deep Belief Networks*: [https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8](https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度信念网络*: [https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8](https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8)'
- en: '*Hands-On Neural Networks:* [https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/](https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实战神经网络:* [https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/](https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/)'
