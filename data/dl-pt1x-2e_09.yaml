- en: Implementing Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°è‡ªç¼–ç å™¨
- en: This chapter addresses the notion of semi-supervised learning algorithms through
    the introduction of autoencoders, and then moves on to **restricted Boltzmann
    machines** (**RBMs**) and **deep belief networks** (**DBNs**) in order to understand
    the probability distribution of data. The chapter will give you an overview as
    to how these algorithms have been applied to some real-world problems. Coded examples
    implemented in PyTorch will also be provided.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« è®¨è®ºäº†åŠç›‘ç£å­¦ä¹ ç®—æ³•çš„æ¦‚å¿µï¼Œé€šè¿‡å¼•å…¥è‡ªç¼–ç å™¨ï¼Œç„¶åè¿›å…¥**å—é™ç»å°”å…¹æ›¼æœº**ï¼ˆ**RBMs**ï¼‰å’Œ**æ·±åº¦ä¿¡å¿µç½‘ç»œ**ï¼ˆ**DBNs**ï¼‰ï¼Œä»¥ç†è§£æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒã€‚æœ¬ç« å°†æ¦‚è¿°è¿™äº›ç®—æ³•å¦‚ä½•åº”ç”¨äºä¸€äº›å®é™…é—®é¢˜ã€‚è¿˜å°†æä¾›åœ¨
    PyTorch ä¸­å®ç°çš„ç¼–ç ç¤ºä¾‹ã€‚
- en: Autoencoders are an unsupervised learning technique. They can take an unlabeled
    dataset and task it with reconstructing the original input by modeling it using
    an unsupervised learning problem as opposed to a supervised one. The goal of the
    autoencoder is for the input to be as similar as possible to the output.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨æ˜¯ä¸€ç§æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯ã€‚å®ƒå¯ä»¥æ¥æ”¶æ— æ ‡ç­¾çš„æ•°æ®é›†ï¼Œå¹¶é€šè¿‡å»ºæ¨¡æ¥é‡å»ºåŸå§‹è¾“å…¥ï¼Œå°†é—®é¢˜å»ºæ¨¡ä¸ºæ— ç›‘ç£å­¦ä¹ ï¼Œè€Œä¸æ˜¯ç›‘ç£å­¦ä¹ ã€‚è‡ªç¼–ç å™¨çš„ç›®æ ‡æ˜¯ä½¿è¾“å…¥ä¸è¾“å‡ºå°½å¯èƒ½ç›¸ä¼¼ã€‚
- en: 'Specifically, the following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: An overview of autoencoders and their applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨åŠå…¶åº”ç”¨æ¦‚è¿°
- en: Bottleneck and loss functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç“¶é¢ˆå’ŒæŸå¤±å‡½æ•°
- en: Different types of autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸åŒç±»å‹çš„è‡ªç¼–ç å™¨
- en: Restricted Boltzmann machines
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å—é™ç»å°”å…¹æ›¼æœº
- en: Deep belief networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ·±åº¦ä¿¡å¿µç½‘ç»œ
- en: Applications of autoencoders
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨çš„åº”ç”¨
- en: 'Autoencoders fall under representational learning and are used to find a compressed
    representation of the inputs. They are composed of an encoder and a decoder. The
    following diagram shows the structure of an autoencoder:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨å±äºè¡¨å¾å­¦ä¹ ï¼Œç”¨äºæ‰¾åˆ°è¾“å…¥çš„å‹ç¼©è¡¨ç¤ºã€‚å®ƒä»¬ç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆã€‚ä»¥ä¸‹å›¾ç¤ºæ˜¾ç¤ºäº†è‡ªç¼–ç å™¨çš„ç»“æ„ï¼š
- en: '![](img/693d5531-ef4e-45a0-aa19-a9c0854da1ec.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/693d5531-ef4e-45a0-aa19-a9c0854da1ec.png)'
- en: 'Examples of applications of autoencoders include the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨çš„åº”ç”¨ç¤ºä¾‹åŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼š
- en: Data denoising
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®å»å™ª
- en: Dimensionality reduction for data visualization
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®å¯è§†åŒ–çš„é™ç»´
- en: Image generation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒç”Ÿæˆ
- en: Interpolating text
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ’å€¼æ–‡æœ¬
- en: Bottleneck and loss functions
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç“¶é¢ˆå’ŒæŸå¤±å‡½æ•°
- en: 'Autoencoders impose a bottleneck on the network, enforcing a compressed knowledge
    representation of the original input. The network would simply learn to memorize
    the input values if the bottleneck were not present. As such, this would mean
    that the model wouldn''t generalize well on unseen data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨å¯¹ç½‘ç»œæ–½åŠ äº†ä¸€ä¸ªç“¶é¢ˆï¼Œå¼ºåˆ¶ä½¿åŸå§‹è¾“å…¥çš„çŸ¥è¯†è¡¨ç¤ºè¢«å‹ç¼©ã€‚å¦‚æœæ²¡æœ‰ç“¶é¢ˆçš„è¯ï¼Œç½‘ç»œå°†ç®€å•åœ°å­¦ä¼šè®°å¿†è¾“å…¥å€¼ã€‚å› æ­¤ï¼Œè¿™æ„å‘³ç€æ¨¡å‹åœ¨æœªè§æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ä¸ä¼šå¾ˆå¥½ï¼š
- en: '![](img/a862aa43-423e-4616-b93e-6c317b509ee9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a862aa43-423e-4616-b93e-6c317b509ee9.png)'
- en: 'In order for the model to detect a signal, we need it to be sensitive to the
    input but not so much that it simply memorizes them and doesn''t predict well
    on unseen data. In order to determine the optimal trade-off, we need to construct
    a loss/cost function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿæ£€æµ‹åˆ°ä¿¡å·ï¼Œæˆ‘ä»¬éœ€è¦å®ƒå¯¹è¾“å…¥å…·æœ‰æ•æ„Ÿæ€§ï¼Œä½†ä¸èƒ½ç®€å•åœ°è®°ä½å®ƒä»¬ï¼Œè€Œåœ¨æœªè§æ•°æ®ä¸Šé¢„æµ‹æ•ˆæœä¸ä½³ã€‚ä¸ºäº†ç¡®å®šæœ€ä¼˜æƒè¡¡ï¼Œæˆ‘ä»¬éœ€è¦æ„å»ºä¸€ä¸ªæŸå¤±/æˆæœ¬å‡½æ•°ï¼š
- en: '![](img/8e2bfc83-5110-405d-9e0a-74b6c8312912.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e2bfc83-5110-405d-9e0a-74b6c8312912.png)'
- en: There are some commonly used autoencoder architectures for imposing these two
    constraints and ensuring there is an optimal trade-off between the two.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€äº›å¸¸ç”¨çš„è‡ªç¼–ç å™¨æ¶æ„ï¼Œç”¨äºæ–½åŠ è¿™ä¸¤ä¸ªçº¦æŸæ¡ä»¶ï¼Œå¹¶ç¡®ä¿åœ¨ä¸¤è€…ä¹‹é—´æœ‰æœ€ä¼˜çš„æƒè¡¡ã€‚
- en: Coded example â€“ standard autoencoder
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - æ ‡å‡†è‡ªç¼–ç å™¨
- en: 'In this example, we will show you how toÂ compile an autoencoder model in PyTorch:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åœ¨ PyTorch ä¸­ç¼–è¯‘ä¸€ä¸ªè‡ªç¼–ç å™¨æ¨¡å‹ï¼š
- en: 'Firstly, import the relevant libraries:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå¯¼å…¥ç›¸å…³çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, define the model parameters:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå®šä¹‰æ¨¡å‹å‚æ•°ï¼š
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, initiate a function to transform the images in the MNIST dataset:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œåˆå§‹åŒ–ä¸€ä¸ªå‡½æ•°æ¥è½¬æ¢ MNIST æ•°æ®é›†ä¸­çš„å›¾åƒï¼š
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the autoencoder class in which to feed the data and initiate the model:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰è‡ªç¼–ç å™¨ç±»ï¼Œç”¨äºæä¾›æ•°æ®å¹¶åˆå§‹åŒ–æ¨¡å‹ï¼š
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define a function that will output the images from the model after each epoch:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°†åœ¨æ¯ä¸ª epoch åä»æ¨¡å‹è¾“å‡ºå›¾åƒï¼š
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now run the model over each epoch and review the results of the reconstructed
    images:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨åœ¨æ¯ä¸ª epoch ä¸Šè¿è¡Œæ¨¡å‹å¹¶æŸ¥çœ‹é‡å»ºå›¾åƒçš„ç»“æœï¼š
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will give the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†äº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š
- en: '![](img/b9ecb6f8-ef83-48aa-8e49-e29b3a0f6bca.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9ecb6f8-ef83-48aa-8e49-e29b3a0f6bca.png)'
- en: 'And the following imageÂ shows the output of the autoencoder at each epoch:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾ç‰‡æ˜¾ç¤ºäº†æ¯ä¸ª epoch çš„è‡ªç¼–ç å™¨è¾“å‡ºï¼š
- en: '![](img/ab6c7350-99d9-4cb1-bde0-602292f98856.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab6c7350-99d9-4cb1-bde0-602292f98856.png)'
- en: The more epochs that pass, the clearer the images become as the model continues
    to learn.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ç»è¿‡çš„ epoch è¶Šæ¥è¶Šå¤šï¼Œå›¾åƒå˜å¾—è¶Šæ¥è¶Šæ¸…æ™°ï¼Œå› ä¸ºæ¨¡å‹ç»§ç»­å­¦ä¹ ã€‚
- en: Convolutional autoencoders
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·ç§¯è‡ªç¼–ç å™¨
- en: Autoencoders can be used with convolutions instead of fully connected layers.
    This can be done using 3D vectors instead of 1D vectors. In the context of images,
    downsampling the image forces the autoencoder to learn a compressed version of
    it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªç¼–ç å™¨å¯ä»¥ä½¿ç”¨å·ç§¯è€Œä¸æ˜¯å…¨è¿æ¥å±‚ã€‚è¿™å¯ä»¥é€šè¿‡ä½¿ç”¨ 3D å‘é‡è€Œä¸æ˜¯ 1D å‘é‡æ¥å®ç°ã€‚åœ¨å›¾åƒçš„èƒŒæ™¯ä¸‹ï¼Œå¯¹å›¾åƒè¿›è¡Œä¸‹é‡‡æ ·è¿«ä½¿è‡ªç¼–ç å™¨å­¦ä¹ å…¶å‹ç¼©ç‰ˆæœ¬ã€‚
- en: Coded example â€“ convolutional autoencoder
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ â€“ å·ç§¯è‡ªç¼–ç å™¨
- en: 'In this example, we will show you how to compile a convolutional autoencoder:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ç¼–è¯‘ä¸€ä¸ªå·ç§¯è‡ªç¼–ç å™¨ï¼š
- en: 'As before, you obtain the train and test datasets from the MNIST dataset and
    define the model parameters:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ä»¥å‰ä¸€æ ·ï¼Œæ‚¨ä» MNIST æ•°æ®é›†è·å–è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ï¼Œå¹¶å®šä¹‰æ¨¡å‹å‚æ•°ï¼š
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'From here, initiate the model for the convolutional autoencoder:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œå¼€å§‹ï¼Œå¯åŠ¨å·ç§¯è‡ªç¼–ç å™¨æ¨¡å‹ï¼š
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, run the model over each epoch while saving the output images for reference:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨æ¯ä¸ª epoch è¿è¡Œæ¨¡å‹åŒæ—¶ä¿å­˜è¾“å‡ºå›¾åƒä»¥ä¾›å‚è€ƒï¼š
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can look at the saved images after every epoch in the folder that is mentioned
    in the code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨ä»£ç ä¸­æåˆ°çš„æ–‡ä»¶å¤¹ä¸­ï¼Œæ¯ä¸ª epoch åæŸ¥çœ‹ä¿å­˜çš„å›¾åƒã€‚
- en: Denoising autoencoders
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å»å™ªè‡ªç¼–ç å™¨
- en: 'Denoising encoders deliberately add noise to the input of the network. These
    autoencoders essentially create a corrupted copy of the data. In doing so, this
    helps the encoder to learn the latent representation in the input data, making
    it more generalizable:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å»å™ªç¼–ç å™¨æ•…æ„å‘ç½‘ç»œçš„è¾“å…¥æ·»åŠ å™ªå£°ã€‚è¿™äº›è‡ªç¼–ç å™¨å®è´¨ä¸Šåˆ›å»ºäº†æ•°æ®çš„æŸåå‰¯æœ¬ã€‚é€šè¿‡è¿™æ ·åšï¼Œè¿™æœ‰åŠ©äºç¼–ç å™¨å­¦ä¹ è¾“å…¥æ•°æ®ä¸­çš„æ½œåœ¨è¡¨ç¤ºï¼Œä½¿å…¶æ›´å…·æ™®é€‚æ€§ï¼š
- en: '![](img/7fb53730-c36b-4900-be3b-39f76e76f65e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fb53730-c36b-4900-be3b-39f76e76f65e.png)'
- en: 'This corrupted image is fed into the network in the same way as other standard
    autoencoders:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæŸåçš„å›¾åƒä¸å…¶ä»–æ ‡å‡†è‡ªç¼–ç å™¨ä¸€æ ·è¢«é€å…¥ç½‘ç»œï¼š
- en: '![](img/0c46e5ba-3ff4-4584-b559-6f772c705114.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c46e5ba-3ff4-4584-b559-6f772c705114.png)'
- en: As we can see, noises were added in the original input and the encoder encodes
    the input and sends it to the decoder, which then decodes the noisy input into
    the cleaned output. Thus, we have looked at various applications that autoencoders
    can be used for. We will now look at a specific type of autoencoder, which is
    aÂ **variational autoencoder** (**VAE**).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼ŒåŸå§‹è¾“å…¥ä¸­æ·»åŠ äº†å™ªå£°ï¼Œç¼–ç å™¨å¯¹è¾“å…¥è¿›è¡Œç¼–ç å¹¶å°†å…¶å‘é€åˆ°è§£ç å™¨ï¼Œè§£ç å™¨ç„¶åå°†å˜ˆæ‚çš„è¾“å…¥è§£ç ä¸ºæ¸…ç†åçš„è¾“å‡ºã€‚å› æ­¤ï¼Œæˆ‘ä»¬å·²ç»çœ‹è¿‡è‡ªç¼–ç å™¨å¯ä»¥ç”¨äºçš„å„ç§åº”ç”¨ã€‚ç°åœ¨æˆ‘ä»¬å°†çœ‹çœ‹ä¸€ç§ç‰¹å®šç±»å‹çš„è‡ªç¼–ç å™¨ï¼Œå³**å˜åˆ†è‡ªç¼–ç å™¨**ï¼ˆ**VAE**ï¼‰ã€‚
- en: Variational autoencoders
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å˜åˆ†è‡ªç¼–ç å™¨
- en: VAEsÂ are different from the standard autoencoders we have considered so far
    as they describe an observation in latent space in a probabilistic manner rather
    than deterministic. A probability distribution for each latent attribute is output,
    rather than a single value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs ä¸æˆ‘ä»¬è¿„ä»Šè€ƒè™‘è¿‡çš„æ ‡å‡†è‡ªç¼–ç å™¨ä¸åŒï¼Œå› ä¸ºå®ƒä»¬ä»¥æ¦‚ç‡æ–¹å¼æè¿°æ½œåœ¨ç©ºé—´ä¸­çš„è§‚å¯Ÿç»“æœï¼Œè€Œä¸æ˜¯ç¡®å®šæ€§æ–¹å¼ã€‚æ¯ä¸ªæ½œåœ¨å±æ€§çš„æ¦‚ç‡åˆ†å¸ƒè¢«è¾“å‡ºï¼Œè€Œä¸æ˜¯å•ä¸ªå€¼ã€‚
- en: Standard autoencoders have somewhat limited applications in the real world as
    they are only really useful when you want to replicate the data that has been
    put into it. Since VAEs are generative models, they can be applied to cases where
    you don't want to output data that is the same as the input.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†è‡ªç¼–ç å™¨åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æœ‰äº›å—é™ï¼Œå› ä¸ºå®ƒä»¬åªåœ¨æ‚¨æƒ³è¦å¤åˆ¶è¾“å…¥çš„æ•°æ®æ—¶æ‰çœŸæ­£æœ‰ç”¨ã€‚ç”±äº VAEs æ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒä»¬å¯ä»¥åº”ç”¨äºæ‚¨ä¸å¸Œæœ›è¾“å‡ºä¸è¾“å…¥ç›¸åŒçš„æ•°æ®çš„æƒ…å†µã€‚
- en: 'Let''s consider this in a real-world context. When training an autoencoder
    model on a dataset of faces, you would hope that it would learn latent attributes,
    such as whether the person is smiling, their skin tone, whether they are wearing
    glasses, and more:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨ç°å®ä¸–ç•Œçš„èƒŒæ™¯ä¸‹è€ƒè™‘è¿™ä¸ªé—®é¢˜ã€‚å½“åœ¨é¢éƒ¨æ•°æ®é›†ä¸Šè®­ç»ƒè‡ªç¼–ç å™¨æ¨¡å‹æ—¶ï¼Œæ‚¨å¸Œæœ›å®ƒèƒ½å­¦ä¹ æ½œåœ¨å±æ€§ï¼Œæ¯”å¦‚ä¸€ä¸ªäººæ˜¯å¦å¾®ç¬‘ï¼Œä»–ä»¬çš„è‚¤è‰²ï¼Œæ˜¯å¦æˆ´çœ¼é•œç­‰ç­‰ï¼š
- en: '![](img/4cc640bb-841f-438d-a6cc-79837d7e85dd.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc640bb-841f-438d-a6cc-79837d7e85dd.png)'
- en: As we can see in the preceding diagram, standard autoencoders represent these
    latent attributes as discrete values.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åœ¨å‰é¢çš„å›¾ä¸­æ‰€ç¤ºï¼Œæ ‡å‡†è‡ªç¼–ç å™¨å°†è¿™äº›æ½œåœ¨å±æ€§è¡¨ç¤ºä¸ºç¦»æ•£å€¼ã€‚
- en: 'If we allow each feature to be within a range of possible values rather than
    a single value, we can use VAEs to describe the attributes in probabilistic terms:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å…è®¸æ¯ä¸ªç‰¹å¾åœ¨å¯èƒ½å€¼çš„èŒƒå›´å†…è€Œä¸æ˜¯å•ä¸ªå€¼å†…ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ VAEs ä»¥æ¦‚ç‡æœ¯è¯­æè¿°å±æ€§ï¼š
- en: '![](img/76947b8a-b694-4db9-b32a-498f4972ba45.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76947b8a-b694-4db9-b32a-498f4972ba45.png)'
- en: The preceding diagram depicts how we can represent whether the person is smiling
    as either a discrete value or as a probability distribution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„å›¾ç¤ºäº†æˆ‘ä»¬å¦‚ä½•å°†ä¸€ä¸ªäººæ˜¯å¦å¾®ç¬‘è¡¨ç¤ºä¸ºç¦»æ•£å€¼æˆ–æ¦‚ç‡åˆ†å¸ƒã€‚
- en: 'The distribution of each latent attribute is sampled from the image in order
    to generate the vector that is used as the input for the decoder model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ½œåœ¨å±æ€§çš„åˆ†å¸ƒæ˜¯ä»å›¾åƒä¸­é‡‡æ ·çš„ï¼Œä»¥ç”Ÿæˆç”¨ä½œè§£ç å™¨æ¨¡å‹è¾“å…¥çš„å‘é‡ï¼š
- en: '![](img/07851c42-ae8a-4888-a8d8-39704d6948da.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07851c42-ae8a-4888-a8d8-39704d6948da.png)'
- en: 'Two vectors are output, as shown in the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¾“å‡ºä¸¤ä¸ªå‘é‡ï¼š
- en: '![](img/d6e926bd-3db3-4433-969d-9237af0bf3a0.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6e926bd-3db3-4433-969d-9237af0bf3a0.png)'
- en: One describes the mean and the other describes the variance of the distributions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªæè¿°å¹³å‡å€¼ï¼Œå¦ä¸€ä¸ªæè¿°åˆ†å¸ƒçš„æ–¹å·®ã€‚
- en: Training VAEs
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒVAE
- en: During training, we calculate the relationship of each parameter in the network
    with respect to the overall loss using a process called **backpropagation**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬ä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—ç½‘ç»œä¸­æ¯ä¸ªå‚æ•°ä¸æ•´ä½“æŸå¤±çš„å…³ç³»ã€‚
- en: 'Standard autoencoders use backpropagation in order to reconstruct the loss
    value across the weights of the network. As the sampling operation in VAEs is
    not differentiable, the gradients cannot be propagated from the reconstruction
    error. The following diagram explains this further:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡å‡†è‡ªåŠ¨ç¼–ç å™¨ä½¿ç”¨åå‘ä¼ æ’­æ¥åœ¨ç½‘ç»œæƒé‡ä¸Šé‡å»ºæŸå¤±å€¼ã€‚ç”±äºVAEä¸­çš„é‡‡æ ·æ“ä½œä¸å¯å¾®ï¼Œä¸èƒ½ä»é‡æ„è¯¯å·®ä¸­ä¼ æ’­æ¢¯åº¦ã€‚ä»¥ä¸‹å›¾è¡¨è¿›ä¸€æ­¥è§£é‡Šäº†è¿™ä¸€ç‚¹ï¼š
- en: '![](img/12949e89-c9f1-491d-a822-7e82befa9fa2.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12949e89-c9f1-491d-a822-7e82befa9fa2.png)'
- en: 'In order to overcome this limitation, the reparameterization trick can be used.
    The reparameterization trick samples Îµ from a unit normal distribution, shifts
    it by the mean ğœ‡ of the latent attribute, and, then scales it by the latent attribute''s
    variance ğœ:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…‹æœè¿™ä¸€é™åˆ¶ï¼Œå¯ä»¥ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§ã€‚é‡å‚æ•°åŒ–æŠ€å·§ä»å•ä½æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·Îµï¼Œå°†å…¶å¹³ç§»è‡³æ½œåœ¨å±æ€§çš„å‡å€¼ğœ‡ï¼Œå¹¶æŒ‰æ½œåœ¨å±æ€§çš„æ–¹å·®ğœè¿›è¡Œç¼©æ”¾ï¼š
- en: '![](img/6376b4ec-ebd3-4464-adcd-4c39f59e1313.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6376b4ec-ebd3-4464-adcd-4c39f59e1313.png)'
- en: 'This removes the sampling process from the flow of gradients as it is now outside
    of the network. As such, the sampling process doesn''t depend on anything in the
    network. We can now optimize the parameters of the distribution while maintaining
    the ability to randomly sample from it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†é‡‡æ ·è¿‡ç¨‹ä»æ¢¯åº¦æµä¸­ç§»é™¤ï¼Œå› ä¸ºç°åœ¨å®ƒä½äºç½‘ç»œä¹‹å¤–ã€‚å› æ­¤ï¼Œé‡‡æ ·è¿‡ç¨‹ä¸ä¾èµ–äºç½‘ç»œä¸­çš„ä»»ä½•ä¸œè¥¿ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥ä¼˜åŒ–åˆ†å¸ƒçš„å‚æ•°ï¼ŒåŒæ—¶ä¿æŒä»ä¸­éšæœºé‡‡æ ·çš„èƒ½åŠ›ï¼š
- en: '![](img/0727f2d1-a7f3-4436-a031-7c6a717b2192.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0727f2d1-a7f3-4436-a031-7c6a717b2192.png)'
- en: 'We can transform with a mean, ğœ‡, and covariance matrix âˆ‘ as per the following,
    as the distribution of each attribute is Gaussian:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å‡å€¼ğœ‡å’Œåæ–¹å·®çŸ©é˜µâˆ‘å¯¹å…¶è¿›è¡Œå˜æ¢ï¼Œå› ä¸ºæ¯ä¸ªå±æ€§çš„åˆ†å¸ƒæ˜¯é«˜æ–¯åˆ†å¸ƒï¼š
- en: '![](img/08421e67-3535-434a-9586-d98bf7e661a4.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08421e67-3535-434a-9586-d98bf7e661a4.png)'
- en: Here, Îµ ~ N(0,1).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼ŒÎµ ~ N(0,1)ã€‚
- en: 'We can now train the model using simple backpropagation with the introduction
    of the reparameterization trick:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€å•çš„åå‘ä¼ æ’­æ¥è®­ç»ƒæ¨¡å‹ï¼Œå¹¶å¼•å…¥é‡å‚æ•°åŒ–æŠ€å·§ï¼š
- en: '![](img/20d8edd5-d1d0-4190-942c-710c7cd089cb.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20d8edd5-d1d0-4190-942c-710c7cd089cb.png)'
- en: As seen in the preceding diagram, we have trained the autoencoder to smooth
    out the image.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰é¢çš„å›¾è¡¨æ‰€ç¤ºï¼Œæˆ‘ä»¬å·²ç»è®­ç»ƒäº†è‡ªåŠ¨ç¼–ç å™¨ä»¥å¹³æ»‘å›¾åƒã€‚
- en: Coded example â€“ VAE
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - VAE
- en: 'In order to code a VAEÂ in PyTorch, we can load the libraries and dataset like
    we did in the previous examples. From here, we can define the VAE class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨PyTorchä¸­ç¼–å†™VAEï¼Œæˆ‘ä»¬å¯ä»¥åƒåœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­é‚£æ ·åŠ è½½åº“å’Œæ•°æ®é›†ã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰VAEç±»ï¼š
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then define the loss function with the help of KL divergence and initiate
    the model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨KLæ•£åº¦æ¥å®šä¹‰æŸå¤±å‡½æ•°ï¼Œå¹¶åˆå§‹åŒ–æ¨¡å‹ï¼š
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From here, we can run the model over each epoch and save the output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œæ¨¡å‹çš„æ¯ä¸ªæ—¶æœŸå¹¶ä¿å­˜è¾“å‡ºï¼š
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have seen the various autoencoders and how to compile them, let
    us learn how to implement them in recommender systems.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹è¿‡å„ç§è‡ªåŠ¨ç¼–ç å™¨åŠå…¶å¦‚ä½•ç¼–è¯‘å®ƒä»¬ï¼Œè®©æˆ‘ä»¬å­¦ä¹ å¦‚ä½•åœ¨æ¨èç³»ç»Ÿä¸­å®ç°å®ƒä»¬ã€‚
- en: Restricted Boltzmann machines
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å—é™ç»å°”å…¹æ›¼æœº
- en: AnÂ **RBM**Â is an algorithm that has been widely used for tasks such as collaborative
    filtering, feature extraction, topic modeling, and dimensionality reduction. They
    can learn patterns in a dataset in an unsupervised fashion.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**RBM**æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºååŒè¿‡æ»¤ã€ç‰¹å¾æå–ã€ä¸»é¢˜å»ºæ¨¡å’Œé™ç»´ç­‰ä»»åŠ¡çš„ç®—æ³•ã€‚å®ƒä»¬å¯ä»¥æ— ç›‘ç£åœ°å­¦ä¹ æ•°æ®é›†ä¸­çš„æ¨¡å¼ã€‚'
- en: For example, if you watch a movie and say whether you liked it or not, we could
    use an RBM to help us determine the reason why you made this decision.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœä½ è§‚çœ‹ç”µå½±å¹¶è¯´å‡ºä½ æ˜¯å¦å–œæ¬¢å®ƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ª**RBM**æ¥å¸®åŠ©æˆ‘ä»¬ç¡®å®šä½ åšå‡ºè¿™ä¸ªå†³å®šçš„åŸå› ã€‚
- en: 'The goal of RBM is to minimize energy defined by the following formula, which
    depends on the configurations of visible/input states, hidden states, weights,
    and biases:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RBMçš„ç›®æ ‡æ˜¯æœ€å°åŒ–èƒ½é‡ï¼Œç”±ä»¥ä¸‹å…¬å¼å®šä¹‰ï¼Œå…¶ä¾èµ–äºå¯è§/è¾“å…¥çŠ¶æ€ã€éšè—çŠ¶æ€ã€æƒé‡å’Œåç½®çš„é…ç½®ï¼š
- en: '![](img/b2d5c938-993c-4018-8312-747bdca6ea70.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2d5c938-993c-4018-8312-747bdca6ea70.png)'
- en: 'RBMs are two-layer networks that are the fundamental building blocks of a DBN.
    The first layer of an RBM is a visible/input layer of neurons and the second is
    the hidden layer of neurons:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: RBMæ˜¯DBNçš„åŸºæœ¬æ„å»ºå—çš„ä¸¤å±‚ç½‘ç»œã€‚RBMçš„ç¬¬ä¸€å±‚æ˜¯ç¥ç»å…ƒçš„å¯è§/è¾“å…¥å±‚ï¼Œç¬¬äºŒå±‚æ˜¯éšè—å±‚çš„ç¥ç»å…ƒï¼š
- en: '![](img/e6352c09-c581-41b7-858f-b5bf82a3f269.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6352c09-c581-41b7-858f-b5bf82a3f269.png)'
- en: The RBM translates the inputs from the visible layer and translates them into
    a set of numbers. Through several forward and backward passes, the number isÂ then
    translated back to reconstruct the inputs. The restriction in the RBM is such
    that nodes in the same layer are not connected.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: RBMå°†è¾“å…¥ä»å¯è§å±‚ç¿»è¯‘æˆä¸€ç»„æ•°å­—ã€‚é€šè¿‡å‡ æ¬¡å‰å‘å’Œåå‘ä¼ é€’ï¼Œè¯¥æ•°å­—ç„¶åè¢«ç¿»è¯‘å›é‡æ„è¾“å…¥ã€‚åœ¨RBMä¸­çš„é™åˆ¶æ˜¯åŒä¸€å±‚ä¸­çš„èŠ‚ç‚¹ä¸è¿æ¥ã€‚
- en: 'A low-level feature is fed into each node of the visible layer from the training
    dataset. In the case of image classification, each node would receive one pixel
    value for each pixel in an image:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è®­ç»ƒæ•°æ®é›†ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹çš„ä½çº§ç‰¹å¾è¢«é¦ˆé€åˆ°å¯è§å±‚çš„æ¯ä¸ªèŠ‚ç‚¹ã€‚åœ¨å›¾åƒåˆ†ç±»çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å°†ä¸ºå›¾åƒä¸­æ¯ä¸ªåƒç´ æ¥æ”¶ä¸€ä¸ªåƒç´ å€¼ï¼š
- en: '![](img/a3310e3c-b99d-4ff3-97bf-aa1ff08491dc.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3310e3c-b99d-4ff3-97bf-aa1ff08491dc.png)'
- en: 'Following one pixel through the network, the input *x* is multiplied by the
    weight from the hidden layer and a bias is then added. From here, this is then
    fed into an activation function, which produces the output, which is essentially
    the strength of the signal passing through it given the input *x*, as shown in
    the following diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ç½‘ç»œè·Ÿè¸ªä¸€ä¸ªåƒç´ ï¼Œè¾“å…¥*x*è¢«éšè—å±‚çš„æƒé‡ä¹˜ä»¥ï¼Œç„¶ååŠ ä¸Šåç½®ã€‚ç„¶åï¼Œè¿™è¢«è¾“å…¥åˆ°æ¿€æ´»å‡½æ•°ä¸­ï¼Œäº§ç”Ÿè¾“å‡ºï¼Œè¿™å®è´¨ä¸Šæ˜¯é€šè¿‡å®ƒä¼ é€’çš„ä¿¡å·å¼ºåº¦ï¼Œç»™å®šè¾“å…¥*x*ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](img/02467675-a93e-4d99-81ca-01aa85c3ac72.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02467675-a93e-4d99-81ca-01aa85c3ac72.png)'
- en: 'At each node in the hidden layer, x from each pixel value is multiplied by
    a separate weight. The products are then summed, and a bias is added. The output
    of this is then passed through an activation function, producing the output at
    that single node:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éšè—å±‚çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œæ¥è‡ªæ¯ä¸ªåƒç´ å€¼çš„*x*è¢«å•ç‹¬çš„æƒé‡ä¹˜ä»¥ã€‚ç„¶åå°†è¿™äº›ä¹˜ç§¯æ±‚å’Œï¼Œå¹¶æ·»åŠ åç½®ã€‚ç„¶åå°†å…¶è¾“å‡ºé€šè¿‡æ¿€æ´»å‡½æ•°ï¼Œäº§ç”Ÿè¯¥å•ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºï¼š
- en: '![](img/00ee7d87-14b2-4e4a-8776-4c879004bb8b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00ee7d87-14b2-4e4a-8776-4c879004bb8b.png)'
- en: 'At each point in time, the RBM is in a certain state, which refers to the values
    of the neurons in the visible *v* and hidden *h* layers. The probability of such
    a state can be given by the following joint distribution function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªæ—¶åˆ»ï¼ŒRBMå¤„äºæŸç§çŠ¶æ€ï¼Œè¿™æŒ‡çš„æ˜¯å¯è§*v*å’Œéšè—*h*å±‚ä¸­ç¥ç»å…ƒçš„å€¼ã€‚è¿™ç§çŠ¶æ€çš„æ¦‚ç‡å¯ä»¥ç”±ä»¥ä¸‹è”åˆåˆ†å¸ƒå‡½æ•°ç»™å‡ºï¼š
- en: '![](img/125757b1-9b57-487d-9133-726cc7addde4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/125757b1-9b57-487d-9133-726cc7addde4.png)'
- en: Here, Z is the partition function that is the summation over all possible pairs
    of visible and hidden vectors.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼ŒZæ˜¯åˆ†åŒºå‡½æ•°ï¼Œæ˜¯å¯¹æ‰€æœ‰å¯èƒ½çš„å¯è§å’Œéšè—å‘é‡å¯¹çš„æ±‚å’Œã€‚
- en: Training RBMs
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒRBM
- en: 'There are two main steps an RBM carries out during training:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒæœŸé—´ï¼ŒRBMæ‰§è¡Œä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š
- en: '**Gibbs sampling**: The first step in the training process uses Gibbs sampling,
    which repeats the following process *k* times:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å‰å¸ƒæ–¯é‡‡æ ·**ï¼šè®­ç»ƒè¿‡ç¨‹çš„ç¬¬ä¸€æ­¥ä½¿ç”¨å‰å¸ƒæ–¯é‡‡æ ·ï¼Œå®ƒé‡å¤ä»¥ä¸‹è¿‡ç¨‹*k*æ¬¡ï¼š'
- en: Probability of hidden vector given the input vector; prediction of the hidden
    values.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»™å®šè¾“å…¥å‘é‡çš„éšè—å‘é‡çš„æ¦‚ç‡ï¼›é¢„æµ‹éšè—å€¼ã€‚
- en: Probability of the input vector given the hidden vector; prediction of the input
    values. From this, we obtain another input vector, which was recreated from the
    original input values.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»™å®šéšè—å‘é‡çš„è¾“å…¥å‘é‡çš„æ¦‚ç‡ï¼›é¢„æµ‹è¾“å…¥å€¼ã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬è·å¾—å¦ä¸€ä¸ªè¾“å…¥å‘é‡ï¼Œè¯¥å‘é‡æ˜¯ä»åŸå§‹è¾“å…¥å€¼é‡æ–°åˆ›å»ºçš„ã€‚
- en: '**Contrastive divergence**: RBMs adjust their weights through contrastive divergence.
    During this process, weights for visible nodes are randomly generated and used
    to generate hidden nodes. The hidden nodes then use the same weights to reconstruct
    visible nodes. The weights used to reconstruct the visible nodes are the same
    throughout. However, the generated nodes are not the same because they aren''t
    connected to each other.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¯¹æ¯”æ•£åº¦**ï¼šRBMé€šè¿‡å¯¹æ¯”æ•£åº¦è°ƒæ•´å®ƒä»¬çš„æƒé‡ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œå¯è§èŠ‚ç‚¹çš„æƒé‡æ˜¯éšæœºç”Ÿæˆçš„ï¼Œå¹¶ç”¨äºç”Ÿæˆéšè—èŠ‚ç‚¹ã€‚ç„¶åï¼Œéšè—èŠ‚ç‚¹å†ä½¿ç”¨ç›¸åŒçš„æƒé‡é‡æ„å¯è§èŠ‚ç‚¹ã€‚ç”¨äºé‡æ„å¯è§èŠ‚ç‚¹çš„æƒé‡åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­æ˜¯ç›¸åŒçš„ã€‚ä½†æ˜¯ç”Ÿæˆçš„èŠ‚ç‚¹ä¸åŒï¼Œå› ä¸ºå®ƒä»¬ä¹‹é—´æ²¡æœ‰è¿æ¥ã€‚'
- en: 'Once an RBM has been trained, it is essentially able to express two things:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦RBMè®­ç»ƒå®Œæˆï¼Œå®ƒåŸºæœ¬ä¸Šèƒ½å¤Ÿè¡¨è¾¾ä¸¤ä»¶äº‹æƒ…ï¼š
- en: The interrelationship between the features of the input data
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¾“å…¥æ•°æ®ç‰¹å¾ä¹‹é—´çš„ç›¸äº’å…³ç³»
- en: Which features are the most important when identifying patterns
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¯†åˆ«æ¨¡å¼æ—¶å“ªäº›ç‰¹å¾æœ€é‡è¦
- en: Theoretical example â€“ RBM recommender system
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è®ºç¤ºä¾‹ - RBMæ¨èç³»ç»Ÿ
- en: In the context of movies, we can use RBMs to uncover a set of latent factors
    that represent their genre, and consequently determine which genre of film a person
    likes. For example, if we were to ask someone to tell us which movies they have
    watched and whether they liked them or not, we can then represent them as binary
    inputs (1 or 0) to the RBM. For those movies they haven't seen or haven't told
    us about, we need to assign a value of -1 so that the network can identify those
    during training and ignore their associated weights.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”µå½±çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨RBMæ­ç¤ºä¸€ç»„ä»£è¡¨å®ƒä»¬ç±»å‹çš„æ½œåœ¨å› ç´ ï¼Œä»è€Œç¡®å®šä¸€ä¸ªäººå–œæ¬¢å“ªç§ç”µå½±ç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¦æ±‚æŸäººå‘Šè¯‰æˆ‘ä»¬ä»–ä»¬çœ‹è¿‡å“ªäº›ç”µå½±ä»¥åŠæ˜¯å¦å–œæ¬¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶è¾“å…¥ï¼ˆ1æˆ–0ï¼‰åˆ°RBMä¸­ã€‚å¯¹äºé‚£äº›ä»–ä»¬æ²¡çœ‹è¿‡æˆ–æ²¡å‘Šè¯‰æˆ‘ä»¬çš„ç”µå½±ï¼Œæˆ‘ä»¬éœ€è¦åˆ†é…ä¸€ä¸ªå€¼ä¸º-1ï¼Œè¿™æ ·ç½‘ç»œåœ¨è®­ç»ƒæ—¶å¯ä»¥è¯†åˆ«å¹¶å¿½ç•¥å®ƒä»¬çš„å…³è”æƒé‡ã€‚
- en: 'Let''s consider an example where a user likes *Mrs. Doubtfire*, *The Hangover*,
    and *Bridesmaids*, does not like *Scream* or *Psycho*, and has not yet seen *The
    Hobbit*. Given these inputs, the RBM may identify three hidden factors: comedy,
    horror, and fantasy, which correspond to the genres of the films:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç¤ºä¾‹ï¼Œç”¨æˆ·å–œæ¬¢*è€å¦ˆå¦ˆï¼Œæˆ‘æ¥äº†*ï¼Œ*å®¿é†‰*å’Œ*ä¼´å¨˜*ï¼Œä¸å–œæ¬¢*å°–å«*æˆ–*å¿ƒç†*ï¼Œè¿˜æ²¡æœ‰çœ‹è¿‡*éœæ¯”ç‰¹äºº*ã€‚æ ¹æ®è¿™äº›è¾“å…¥ï¼ŒRBMå¯èƒ½è¯†åˆ«å‡ºä¸‰ä¸ªéšè—å› å­ï¼šå–œå‰§ã€ææ€–å’Œå¥‡å¹»ï¼Œè¿™äº›å› å­å¯¹åº”äºç”µå½±çš„ç±»å‹ï¼š
- en: '![](img/6d8e29ec-2944-4f90-a8c8-aaab69ca68c5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d8e29ec-2944-4f90-a8c8-aaab69ca68c5.png)'
- en: For each hidden neuron, the RBM assigns a probability of the hidden neuron given
    the input neuron. The final binary values of the neurons are obtained by sampling
    from the Bernoulli distribution.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªéšè—ç¥ç»å…ƒï¼ŒRBMåˆ†é…äº†ç»™å®šè¾“å…¥ç¥ç»å…ƒçš„éšè—ç¥ç»å…ƒçš„æ¦‚ç‡ã€‚ç¥ç»å…ƒçš„æœ€ç»ˆäºŒè¿›åˆ¶å€¼æ˜¯é€šè¿‡ä»ä¼¯åŠªåˆ©åˆ†å¸ƒä¸­æŠ½æ ·å¾—åˆ°çš„ã€‚
- en: In the preceding example, the only hidden neuron that represents the comedyÂ genreÂ becomes
    active. As such, given the movie ratings input into the RBM, it predicts that
    the user likes comedy films the most.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œä»£è¡¨å–œå‰§ç±»å‹çš„å”¯ä¸€éšè—ç¥ç»å…ƒå˜å¾—æ´»è·ƒã€‚å› æ­¤ï¼Œç»™å®šè¾“å…¥åˆ°RBMçš„ç”µå½±è¯„åˆ†ï¼Œå®ƒé¢„æµ‹ç”¨æˆ·æœ€å–œæ¬¢å–œå‰§ç”µå½±ã€‚
- en: For the trained RBM to make predictions on movies the user has not yet seen,
    based on their preference, the RBM uses the probability of the visible neurons
    given the hidden neurons. It samples from the Bernoulli distribution to find out
    which one of the visible neurons can then become active.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå·²è®­ç»ƒçš„RBMæ¥è¯´ï¼Œè¦é¢„æµ‹ç”¨æˆ·å°šæœªçœ‹è¿‡çš„ç”µå½±ï¼ŒåŸºäºä»–ä»¬çš„å–œå¥½ï¼ŒRBMä½¿ç”¨å¯è§ç¥ç»å…ƒç»™å®šéšè—ç¥ç»å…ƒçš„æ¦‚ç‡ã€‚å®ƒä»ä¼¯åŠªåˆ©åˆ†å¸ƒä¸­è¿›è¡ŒæŠ½æ ·ï¼Œä»¥ç¡®å®šå“ªä¸ªå¯è§ç¥ç»å…ƒå¯ä»¥å˜ä¸ºæ´»è·ƒçŠ¶æ€ã€‚
- en: Coded example â€“ RBM recommender system
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç ç¤ºä¾‹ - RBMæ¨èç³»ç»Ÿ
- en: Continuing in the context of movies, we willÂ now give an example of how how
    we can build an RBM recommender system using the PyTorch library. The goal of
    the example is to train a model to determine whether a user will like a movie
    or not.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ç»­åœ¨ç”µå½±çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨PyTorchåº“æ„å»ºä¸€ä¸ªRBMæ¨èç³»ç»Ÿçš„ç¤ºä¾‹ã€‚è¯¥ç¤ºä¾‹çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥ç¡®å®šç”¨æˆ·æ˜¯å¦ä¼šå–œæ¬¢ä¸€éƒ¨ç”µå½±ã€‚
- en: 'In this example, we use the MovieLens dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/))
    with 1 million ratings, which was created by the GroupLens Research Group at the
    University of Minnesota:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†MovieLensæ•°æ®é›†ï¼ˆ[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)ï¼‰ï¼ŒåŒ…å«100ä¸‡æ¡è¯„åˆ†ï¼Œè¿™ä¸ªæ•°æ®é›†ç”±æ˜å°¼è‹è¾¾å¤§å­¦çš„GroupLensç ”ç©¶ç»„åˆ›å»ºï¼š
- en: 'Firstly, download the datasets. This can be done through terminal commands
    as follows:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¸‹è½½æ•°æ®é›†ã€‚å¯ä»¥é€šè¿‡ç»ˆç«¯å‘½ä»¤å®Œæˆå¦‚ä¸‹æ“ä½œï¼š
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now import the libraries that we will use:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨å¯¼å…¥æˆ‘ä»¬å°†è¦ä½¿ç”¨çš„åº“ï¼š
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then import the data:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åå¯¼å…¥æ•°æ®ï¼š
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following screenshot illustrates the structure of our dataset:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æˆªå›¾å±•ç¤ºäº†æˆ‘ä»¬æ•°æ®é›†çš„ç»“æ„ï¼š
- en: '![](img/272925c4-059f-410c-b3f1-0833472cddf1.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/272925c4-059f-410c-b3f1-0833472cddf1.png)'
- en: 'Prepare the testing and training datasets:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‡†å¤‡æµ‹è¯•å’Œè®­ç»ƒæ•°æ®é›†ï¼š
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we need to prepare a matrix with the users'' ratings. The matrix will have
    users as rows and movies as columns. Zeros are used to represent cases where a
    user didn''t rate a particular movie. We define the `no_users` and `no_movies`
    variablesÂ then consider the `max`Â value in the training and testing datasets as
    follows:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€ä¸ªåŒ…å«ç”¨æˆ·è¯„åˆ†çš„çŸ©é˜µã€‚è¯¥çŸ©é˜µå°†ä»¥ç”¨æˆ·ä¸ºè¡Œï¼Œç”µå½±ä¸ºåˆ—ã€‚é›¶ç”¨äºè¡¨ç¤ºç”¨æˆ·æœªå¯¹ç‰¹å®šç”µå½±è¯„åˆ†çš„æƒ…å†µã€‚æˆ‘ä»¬å®šä¹‰`no_users`å’Œ`no_movies`å˜é‡ï¼Œç„¶åè€ƒè™‘è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¸­çš„æœ€å¤§å€¼å¦‚ä¸‹ï¼š
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we define a function named `convert_dataset` that converts the dataset
    into a matrix. It does so by creating a loop that will run through the dataset
    and fetch all of the movies rated by a specific user along with the ratings by
    that same user. We firstly create a matrix of zeros since there are movies the
    user didn''t rate:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåä¸º `convert_dataset` çš„å‡½æ•°ï¼Œå°†æ•°æ®é›†è½¬æ¢ä¸ºçŸ©é˜µã€‚å®ƒé€šè¿‡åˆ›å»ºä¸€ä¸ªå¾ªç¯æ¥è¿è¡Œæ•°æ®é›†ï¼Œå¹¶è·å–ç‰¹å®šç”¨æˆ·è¯„åˆ†çš„æ‰€æœ‰ç”µå½±åŠè¯¥ç”¨æˆ·çš„è¯„åˆ†ã€‚å› ä¸ºç”¨æˆ·æ²¡æœ‰è¯„çº§è¿‡çš„ç”µå½±æœ‰è®¸å¤šï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªå…¨é›¶çŸ©é˜µï¼š
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we convert the data into Torch tensors by using the `FloatTensor` utility.
    This will convert the dataset into PyTorch arrays:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ `FloatTensor` å®ç”¨ç¨‹åºå°†æ•°æ®è½¬æ¢ä¸º Torch å¼ é‡ã€‚è¿™å°†æŠŠæ•°æ®é›†è½¬æ¢ä¸º PyTorch æ•°ç»„ï¼š
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this example, we want to make a binary classification, which is whether
    the user will like the movie or not. As such, we convert the ratings into zeros
    and ones. First, however, we replace the existing zeros with -1 in order to represent
    movies that a user never rated:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦è¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼Œå³ç”¨æˆ·æ˜¯å¦å–œæ¬¢è¿™éƒ¨ç”µå½±ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¯„åˆ†è½¬æ¢ä¸ºé›¶å’Œä¸€ã€‚ä½†æ˜¯é¦–å…ˆï¼Œæˆ‘ä»¬å°†ç°æœ‰çš„é›¶æ›¿æ¢ä¸º -1ï¼Œä»¥è¡¨ç¤ºç”¨æˆ·ä»æœªè¯„çº§è¿‡çš„ç”µå½±ï¼š
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we need to create a class in order to define the architecture of the RBM.
    The class initializes the weight and bias by using a random normal distribution.
    Two types of biases are also defined, where `a` is the probability of the hidden
    nodes given the visible nodes and `b` is the probability of the visible nodes
    given the hidden nodes. The class creates a `sample_hidden_nodes`Â functionÂ that
    takes `x` as an argument and represents the visible neurons. From here, we compute
    the probability of `h` given `v`, where `h` and `v` represent the hidden and visible
    nodes respectively. This represents the sigmoid activation function. It is computed
    as the product of the vector of the weights and `x` plus the bias `a`. Since we
    are considering the model for binary classification, we return Bernoulli samples
    of the hidden neurons. From here, we create a `sample_visible_function` functionÂ that
    will sample the visible nodes. Finally, we create the training function. It takes
    the input vector containing the movie ratings, the visible nodes obtained after
    *k* samplings, the vector of probabilities, and the probabilities of the hidden
    nodes after *k* samplings:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªç±»æ¥å®šä¹‰ RBM çš„æ¶æ„ã€‚è¯¥ç±»é€šè¿‡ä½¿ç”¨éšæœºæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡å’Œåç½®ã€‚è¿˜å®šä¹‰äº†ä¸¤ç§ç±»å‹çš„åç½®ï¼Œå…¶ä¸­ `a` æ˜¯ç»™å®šå¯è§èŠ‚ç‚¹æ—¶éšè—èŠ‚ç‚¹çš„æ¦‚ç‡ï¼Œ`b`
    æ˜¯ç»™å®šéšè—èŠ‚ç‚¹æ—¶å¯è§èŠ‚ç‚¹çš„æ¦‚ç‡ã€‚è¯¥ç±»åˆ›å»ºäº†ä¸€ä¸ª `sample_hidden_nodes` å‡½æ•°ï¼Œå®ƒä»¥ `x` ä½œä¸ºå‚æ•°å¹¶è¡¨ç¤ºå¯è§ç¥ç»å…ƒã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—ç»™å®š
    `v` çš„ `h` çš„æ¦‚ç‡ï¼Œå…¶ä¸­ `h` å’Œ `v` åˆ†åˆ«è¡¨ç¤ºéšè—å’Œå¯è§èŠ‚ç‚¹ã€‚è¿™ä»£è¡¨äº† S å‹æ¿€æ´»å‡½æ•°ã€‚å®ƒè®¡ç®—ä¸ºæƒé‡å‘é‡å’Œ `x` çš„ä¹˜ç§¯åŠ ä¸Šåç½® `a`ã€‚ç”±äºæˆ‘ä»¬è€ƒè™‘çš„æ˜¯äºŒå…ƒåˆ†ç±»æ¨¡å‹ï¼Œæˆ‘ä»¬è¿”å›éšè—ç¥ç»å…ƒçš„ä¼¯åŠªåˆ©æ ·æœ¬ã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª
    `sample_visible_function` å‡½æ•°ï¼Œå®ƒå°†å¯¹å¯è§èŠ‚ç‚¹è¿›è¡Œé‡‡æ ·ã€‚æœ€åï¼Œæˆ‘ä»¬åˆ›å»ºè®­ç»ƒå‡½æ•°ã€‚å®ƒæ¥å—åŒ…å«ç”µå½±è¯„åˆ†çš„è¾“å…¥å‘é‡ã€*k* æ¬¡é‡‡æ ·åè·å¾—çš„å¯è§èŠ‚ç‚¹ã€æ¦‚ç‡å‘é‡ä»¥åŠ
    *k* æ¬¡é‡‡æ ·åçš„éšè—èŠ‚ç‚¹çš„æ¦‚ç‡ï¼š
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we define our model parameters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ï¼š
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From here, we can train the model for each epoch:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ª epoch è®­ç»ƒæ¨¡å‹ï¼š
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can plot the error across epochs during training:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç»˜åˆ¶è·¨ epoch çš„é”™è¯¯ï¼š
- en: '![](img/c613abe5-4900-4baa-bf32-656a44fdf51a.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c613abe5-4900-4baa-bf32-656a44fdf51a.png)'
- en: This can help us to determine how many epochs we should run the training for.
    It shows that after six epochs, the improved performance rate drops and hence
    we should consider stopping the training at this stage.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç¡®å®šåº”è¯¥è¿è¡Œå¤šå°‘ä¸ª epoch è¿›è¡Œè®­ç»ƒã€‚æ˜¾ç¤ºåœ¨å…­ä¸ª epoch åï¼Œæ”¹è¿›çš„æ€§èƒ½ç‡ä¸‹é™ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥è€ƒè™‘åœ¨è¿™ä¸ªé˜¶æ®µåœæ­¢è®­ç»ƒã€‚
- en: We have seen the coded example of implementing a recommender system in RBM,
    now let's briefly walk through a DBN architecture.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†åœ¨ RBM ä¸­å®ç°æ¨èç³»ç»Ÿçš„ç¼–ç ç¤ºä¾‹ï¼Œç°åœ¨è®©æˆ‘ä»¬ç®€è¦åœ°æµè§ˆä¸€ä¸‹ DBN æ¶æ„ã€‚
- en: DBN architecture
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBN æ¶æ„
- en: 'A DBNÂ is a multilayer belief network where each layer is an RBM stacked against
    one another. Apart from the first and final layers of the DBN, each layer serves
    as both a hidden layer to the nodes before it and as the input layer to the nodes
    that come after it:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: DBN æ˜¯ä¸€ä¸ªå¤šå±‚ä¿¡å¿µç½‘ç»œï¼Œæ¯ä¸€å±‚éƒ½æ˜¯ä¸€ä¸ªå åŠ çš„ RBMã€‚é™¤äº† DBN çš„ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚ä¹‹å¤–ï¼Œæ¯ä¸€å±‚æ—¢ä½œä¸ºå…¶å‰é¢èŠ‚ç‚¹çš„éšè—å±‚ï¼Œåˆä½œä¸ºå…¶åèŠ‚ç‚¹çš„è¾“å…¥å±‚ï¼š
- en: '![](img/476edcd1-4181-4f6b-8111-adf963439fc9.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/476edcd1-4181-4f6b-8111-adf963439fc9.png)'
- en: 'Two layers in the DBN are connected by a matrix of weights. The top two layers
    of a DBN are undirected, which gives a symmetric connection between them, forming
    an associative memory. The lower two layers have direct connections to the layers
    above. The sense of direction converts associative memory into observed variables:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: DBNä¸­çš„ä¸¤ä¸ªå±‚é€šè¿‡æƒé‡çŸ©é˜µè¿æ¥ã€‚DBNçš„é¡¶éƒ¨ä¸¤å±‚æ˜¯æ— å‘çš„ï¼Œå®ƒä»¬ä¹‹é—´å½¢æˆå¯¹ç§°è¿æ¥ï¼Œå½¢æˆè”æƒ³å­˜å‚¨å™¨ã€‚è¾ƒä½çš„ä¸¤å±‚ç›´æ¥è¿æ¥åˆ°ä¸Šé¢çš„å±‚ã€‚æ–¹å‘æ„Ÿå°†è”æƒ³å­˜å‚¨å™¨è½¬æ¢ä¸ºè§‚å¯Ÿå˜é‡ï¼š
- en: '![](img/884509b6-a309-48a5-a398-c1a3f3cde68f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/884509b6-a309-48a5-a398-c1a3f3cde68f.png)'
- en: 'The two most significant properties of DBNs are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: DBNçš„ä¸¤ä¸ªæœ€æ˜¾è‘—ç‰¹æ€§å¦‚ä¸‹ï¼š
- en: A DBN learns top-down, generative weights via an efficient, layer-by-layer procedure.
    These weights determine how the variables in one layer depend on the layer above.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBNé€šè¿‡é«˜æ•ˆçš„é€å±‚è¿‡ç¨‹å­¦ä¹ è‡ªé¡¶å‘ä¸‹çš„ç”Ÿæˆæƒé‡ã€‚è¿™äº›æƒé‡å†³å®šäº†ä¸€ä¸ªå±‚ä¸­çš„å˜é‡å¦‚ä½•ä¾èµ–äºä¸Šé¢çš„å±‚ã€‚
- en: Once training is complete, the values of the hidden variables in each layer
    can be inferred by a single bottom-up pass. The pass begins with a visible data
    vector in the lower layer and uses its generative weights in the opposite direction.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥é€šè¿‡å•ä¸ªè‡ªä¸‹è€Œä¸Šçš„ä¼ é€’æ¨æ–­æ¯å±‚éšè—å˜é‡çš„å€¼ã€‚ä¼ é€’ä»åº•å±‚çš„å¯è§æ•°æ®å‘é‡å¼€å§‹ï¼Œå¹¶ä½¿ç”¨å…¶ç”Ÿæˆæƒé‡ç›¸åæ–¹å‘ã€‚
- en: 'The probability of a joint configuration network over both visible and hidden
    layers depends on the joint configuration network''s energy compared with the
    energy of all other joint configuration networks:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è”åˆé…ç½®ç½‘ç»œçš„æ¦‚ç‡åœ¨å¯è§å±‚å’Œéšè—å±‚ä¹‹é—´çš„è”åˆé…ç½®ç½‘ç»œçš„èƒ½é‡ä¾èµ–äºæ‰€æœ‰å…¶ä»–è”åˆé…ç½®ç½‘ç»œçš„èƒ½é‡ï¼š
- en: '![](img/9b151fd9-0ff8-4e58-b605-3c292d38b857.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b151fd9-0ff8-4e58-b605-3c292d38b857.png)'
- en: Once the pretraining phase of the DBN has been completed by the RBM stack, a
    feedforward network can then be used for the fine-tuning phase in order to create
    a classifier or simply help cluster unlabeled data in an unsupervised learning
    scenario.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦RBMså †æ ˆå®Œæˆäº†DBNçš„é¢„è®­ç»ƒé˜¶æ®µï¼Œå°±å¯ä»¥ä½¿ç”¨å‰å‘ç½‘ç»œè¿›è¡Œå¾®è°ƒé˜¶æ®µï¼Œä»è€Œåˆ›å»ºåˆ†ç±»å™¨æˆ–åœ¨æ— ç›‘ç£å­¦ä¹ åœºæ™¯ä¸­ç®€å•åœ°å¸®åŠ©èšç±»æ— æ ‡ç­¾æ•°æ®ã€‚
- en: Fine-tuning
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾®è°ƒ
- en: Fine-tuning aims to find the optimal values of the weights between the layers.
    It tweaks the original features in order to obtain more precise boundaries of
    the classes. In order to help the model associate patterns and features to the
    datasets, a small labeled dataset is used.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒçš„ç›®æ ‡æ˜¯æ‰¾åˆ°å±‚é—´æƒé‡çš„æœ€ä¼˜å€¼ã€‚å®ƒå¾®è°ƒåŸå§‹ç‰¹å¾ï¼Œä»¥è·å¾—æ›´ç²¾ç¡®çš„ç±»è¾¹ç•Œã€‚ä¸ºäº†å¸®åŠ©æ¨¡å‹å°†æ¨¡å¼å’Œç‰¹å¾å…³è”åˆ°æ•°æ®é›†ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå°çš„æ ‡è®°æ•°æ®é›†ã€‚
- en: Fine-tuning can be applied as a stochastic bottom-up pass and then used to adjust
    the top-down weights. Once the top is reached, recursion is applied to the top
    layer. In order to fine-tune further, we can do a stochastic top-down pass and
    adjust the bottom-up weights.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒå¯ä»¥ä½œä¸ºéšæœºçš„è‡ªä¸‹è€Œä¸Šä¼ é€’åº”ç”¨ï¼Œç„¶åç”¨äºè°ƒæ•´è‡ªä¸Šè€Œä¸‹çš„æƒé‡ã€‚ä¸€æ—¦è¾¾åˆ°é¡¶å±‚ï¼Œé€’å½’è¢«åº”ç”¨äºé¡¶å±‚ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¾®è°ƒï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œéšæœºçš„è‡ªä¸Šè€Œä¸‹ä¼ é€’ï¼Œå¹¶è°ƒæ•´è‡ªä¸‹è€Œä¸Šçš„æƒé‡ã€‚
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, we explained what autoencoders are and different variations
    of them. Throughout the chapter, we gave some coded examples of how how they can
    be applied to the MNIST dataset. We later introduced RBMs and explained how these
    can be developed into a DBN along with some additional examples.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬è§£é‡Šäº†è‡ªç¼–ç å™¨åŠå…¶ä¸åŒçš„å˜ä½“ã€‚åœ¨æ•´ä¸ªç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€äº›ç¼–ç ç¤ºä¾‹ï¼Œå±•ç¤ºå®ƒä»¬å¦‚ä½•åº”ç”¨äºMNISTæ•°æ®é›†ã€‚åæ¥æˆ‘ä»¬ä»‹ç»äº†å—é™ç»å°”å…¹æ›¼æœºï¼Œå¹¶è§£é‡Šäº†å¦‚ä½•å°†å…¶å¼€å‘æˆæ·±åº¦ç»å°”å…¹æ›¼æœºï¼ŒåŒæ—¶æä¾›äº†é¢å¤–çš„ç¤ºä¾‹ã€‚
- en: In the next chapter, we will introduce generative adversarial networks. We will
    show how they can be used to generate both images and text.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼Œå¹¶å±•ç¤ºå®ƒä»¬å¦‚ä½•ç”¨äºç”Ÿæˆå›¾åƒå’Œæ–‡æœ¬ã€‚
- en: Further reading
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: 'Refer to the following for further information:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥çš„ä¿¡æ¯è¯·å‚è€ƒä»¥ä¸‹å†…å®¹ï¼š
- en: '*Tutorial on Variational Autoencoders*: [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å˜åˆ†è‡ªç¼–ç å™¨æ•™ç¨‹*: [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)'
- en: '*CS598LAZ â€“ Variational Autoencoders*: [http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf](http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CS598LAZ â€“ å˜åˆ†è‡ªç¼–ç å™¨*: [http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf](http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf)'
- en: '*Auto-Encoding Variational Bayes*: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è‡ªç¼–ç å˜åˆ†è´å¶æ–¯*: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)'
- en: '*Deep Learning Book*: [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦å­¦ä¹ ä¹¦ç±*: [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)'
- en: '*A Fast Learning Algorithm for Deep Belief Nets*: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦ä¿¡å¿µç½‘å¿«é€Ÿå­¦ä¹ ç®—æ³•*: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
- en: '*Training restricted Boltzmann machines: An introduction*: [https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495](https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è®­ç»ƒå—é™ç»å°”å…¹æ›¼æœºï¼šç®€ä»‹*: [https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495](https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495)'
- en: '*Deep Boltzmann Machines*: [http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦ç»å°”å…¹æ›¼æœº*: [http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)'
- en: '*A Practical Guide to Training Restricted Boltzmann Machines*: [https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*è®­ç»ƒå—é™ç»å°”å…¹æ›¼æœºå®ç”¨æŒ‡å—*: [https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)'
- en: '*Deep Belief Networks*: [https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8](https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æ·±åº¦ä¿¡å¿µç½‘ç»œ*: [https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8](https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8)'
- en: '*Hands-On Neural Networks:* [https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/](https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å®æˆ˜ç¥ç»ç½‘ç»œ:* [https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/](https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/)'
