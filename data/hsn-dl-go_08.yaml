- en: Object Recognition with Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行物体识别
- en: Now it's time to get to some computer vision or image classification problems
    that are a little more general than our earlier MNIST handwriting example. A lot
    of the same principles apply, but we will be using some new types of operations
    to build **Convolutional Neural** **Networks** (**CNNs**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候处理一些比我们之前的MNIST手写例子更一般的计算机视觉或图像分类问题了。许多相同的原则适用，但我们将使用一些新类型的操作来构建**卷积神经**
    **网络**（**CNNs**）。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to CNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN简介
- en: Building an example CNN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个示例CNN
- en: Assessing the results and making improvements
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估结果和进行改进
- en: Introduction to CNNs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN简介
- en: CNNs are a class of deep neural networks—they are well suited to data with several
    channels and are sensitive to the locality of the information contained within
    the inputs fed into the network. This makes CNNs well suited for tasks associated
    with computer vision such as facial recognition, image classification, scene labeling,
    and more.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是一类深度神经网络，它们非常适合处理具有多个通道的数据，并对输入中包含的信息局部性敏感。这使得CNN非常适合与计算机视觉相关的任务，例如人脸识别、图像分类、场景标记等。
- en: What is a CNN?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是CNN？
- en: CNNs, also known as **ConvNets**, are a class or a category of neural networks
    that are generally accepted to be very good at image classification, that is to
    say, they are very good at distinguishing cats from dogs, cars from planes, and
    many other common classification tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，也称为**ConvNets**，是一类被普遍认为在图像分类方面非常出色的神经网络，也就是说，它们非常擅长区分猫和狗、汽车和飞机等常见分类任务。
- en: A CNN typically consists of convolution layers, activation layers, and pooling
    layers. However, it has been structured specifically to take advantage of the
    fact that the inputs are typically images, and take advantage of the fact that
    some parts of the image are very likely to be next to each other.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常由卷积层、激活层和池化层组成。然而，它们被特别构造以利用输入通常为图像的事实，并利用图像中某些部分极有可能紧邻彼此的事实。
- en: They are actually fairly similar implementation wise to the feedforward networks
    that we have covered in earlier chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现上，它们与我们在早期章节中介绍的前馈网络非常相似。
- en: Normal feedforward versus ConvNet
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普通前馈与ConvNet
- en: In general, a neural network receives a single vector as input (such as our
    MNIST example in [Chapter 3](200c9784-4718-47d4-84ce-95e41854a151.xhtml), *Beyond
    Basic Neural Networks – Autoencoders and RBMs*) and then goes through several
    hidden layers, before arriving at the end with our inference for the result. This
    is fine for images that aren't that big; when our images become larger, however,
    as they usually are in most real-life applications, we want to ensure that we
    aren't building immensely large hidden layers to process them correctly.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，神经网络接收一个单独的向量作为输入（例如我们在[第3章](200c9784-4718-47d4-84ce-95e41854a151.xhtml)中的MNIST示例，*超越基本神经网络—自编码器和RBM*），然后经过几个隐藏层，在最后得到我们推断的结果。这对于图像不是很大的情况是可以的；然而，当我们的图像变得更大时，通常是大多数实际应用中的情况，我们希望确保我们不会建立极其庞大的隐藏层来正确处理它们。
- en: 'Of course, one of the convenient features that is present in our ideas with
    tensors is the fact that we don''t actually have to feed a vector into the model;
    we can feed something a little more complicated and with more dimensions. Basically,
    what we want to do with a CNN is that we want to have neurons arranged in three
    dimensions: height, width, and depth—what we mean by depth here is the number
    of colors in our color system, in our case being red, green, and blue.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们在张量理念中的一个方便特性是，事实上我们并不需要将一个向量馈送到模型中；我们可以馈送一个稍微复杂且具有更多维度的东西。基本上，我们想要用CNN做的是将神经元按三维排列：高度、宽度和深度——这里所说的深度是指我们彩色系统中的颜色数量，在我们的情况下是红色、绿色和蓝色。
- en: Instead of trying to connect every neuron in a layer together, we will try to
    reduce it so that it is more manageable and less likely to be overfitted for our
    sample size, as we won't be trying to train every single pixel of the input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再试图将每个层中的每个神经元连接在一起，而是试图减少它，使其更易管理，减少对我们的样本大小过拟合的可能性，因为我们不会尝试训练输入的每个像素。
- en: Layers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层
- en: 'Of course, CNNs use layers, and we will need to talk about some of these layers
    in more detail, because we haven''t discussed them yet; in general, there are
    three main layers in a CNN: convolutional layers, pooling layers, and fully connected
    layers (these are the ones you''ve already seen).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，CNN 使用层，我们需要更详细地讨论其中的一些层，因为我们还没有讨论它们；一般来说，CNN 中有三个主要层：卷积层、池化层和全连接层（这些您已经见过）。
- en: Convolutional layer
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: Convolutional layers are part of the name of this neural network and form a
    very important part of the neural network architecture. It can be broadly explained
    as scanning across the image to find certain features. We create a small filter,
    which we then slide across the entire image according to our desired stride.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层是这种神经网络的一部分，是神经网络架构中非常重要的组成部分。它可以广义地解释为在图像上进行滑动来寻找特定的特征。我们创建一个小型滤波器，然后根据我们想要的步幅在整个图像上滑动。
- en: 'So, for example, the first cell of the output would be calculated by finding
    the **Dot Product** of our 3 x 3 filter with the top-left corner of our **Image**,
    as shown in the following diagram:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，例如，输出的第一个单元格将通过计算我们的 3 x 3 滤波器与图像的左上角的**点积**来得出，如下图所示：
- en: '![](img/c91d32a0-1c57-4c34-b655-ec010737ac00.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c91d32a0-1c57-4c34-b655-ec010737ac00.png)'
- en: 'And if your stride was one, it would shift one column right and continue, as
    shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果步幅为一，那么将向右移动一列并继续，如下图所示：
- en: '![](img/bc423070-20d8-4603-859f-3818d9aacb32.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc423070-20d8-4603-859f-3818d9aacb32.png)'
- en: This would then continue until we had our entire output.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就可以继续，直到获得整个输出。
- en: Pooling layer
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: Pooling layers are commonly put in between convolutional layers; they are meant
    to reduce the volume of data being passed around, therefore reducing the number
    of parameters, as well as reducing the amount of computation required by the network.
    In this case, we are *pooling* numbers together by taking the maximum over a given
    region of numbers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层通常放置在卷积层之间；它们的作用是减少传递的数据量，从而减少参数数量，以及减少网络所需的计算量。在这种情况下，我们通过在给定区域内取最大值来进行*池化*操作。
- en: These layers also work similarly to the convolutional layers; they apply on
    a predetermined grid and perform the pooling operation. In this case, it is the
    maximum operation, so it will take the highest value within the grid.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层的工作方式与卷积层类似；它们在预定的网格上应用并执行池化操作。在这种情况下，它是最大化操作，因此它将在网格内取最高值。
- en: 'For example, in a max pooling operation on a 2 x 2 grid, the first cell of
    output will come from the top left, as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个 2 x 2 网格上进行最大池化操作时，第一个输出的单元格将来自左上角，如下所示：
- en: '![](img/f819e869-6747-45e1-91de-27b878f7ef45.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f819e869-6747-45e1-91de-27b878f7ef45.png)'
- en: 'And with a stride of two, the second will come from the grid shifted right
    two rows, as shown here:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 并且使用步幅为两，第二个将来自向右移动两行的网格，如下图所示：
- en: '![](img/911e2c7d-0498-4763-9c0a-828918037283.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/911e2c7d-0498-4763-9c0a-828918037283.png)'
- en: Basic structure
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本结构
- en: 'Now that you understand the layers, let''s talk about the basic structure of
    a CNN. A CNN consists broadly of the following: an input layer, and then several
    layers of convolutional layers, activation layers, and pooling layers, before
    ending in a fully connected layer at the end to get to our final results.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您理解了层次，让我们来谈谈 CNN 的基本结构。一个 CNN 主要包括以下几部分：一个输入层，然后是若干层卷积层、激活层和池化层，最后以一个全连接层结束，以获得最终的结果。
- en: 'The basic structure looks a little like the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基本结构看起来像下面这样：
- en: '![](img/a840475f-4055-40b9-b2ef-34e9126359a8.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a840475f-4055-40b9-b2ef-34e9126359a8.png)'
- en: Building an example CNN
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个示例 CNN
- en: To illustrate how a CNN works in practice, we will be building a model to recognize
    whether an object in a photo is a cat or not. The dataset we are using has more
    depth than this, but it would take a rather long time to train it to correctly
    classify everything. It is fairly trivial to extend the example to classify everything,
    but we would rather not be sitting there for a week waiting for the model to train.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明 CNN 在实践中的工作原理，我们将构建一个模型来识别照片中的物体是否是猫。我们使用的数据集比这更加复杂，但训练它以正确分类一切会花费相当长的时间。将示例扩展到分类一切都是相当简单的，但我们宁愿不花一周时间等待模型训练。
- en: 'For our example, we will be using the following structure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将使用以下结构：
- en: '![](img/1773604f-ee5a-4e45-a98e-1931d0252d8f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1773604f-ee5a-4e45-a98e-1931d0252d8f.png)'
- en: CIFAR-10
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CIFAR-10
- en: We are using CIFAR-10 for our example this time instead of MNIST. As such, we
    do not have the convenience of using the already convenient MNIST loader. Let's
    quickly go through what it takes to load this new dataset!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们的示例中使用 CIFAR-10 而不是 MNIST。因此，我们不能方便地使用已有的 MNIST 加载器。让我们快速浏览一下加载这个新数据集所需的步骤！
- en: 'We will be using the binary format for CIFAR-10, which you can download here:
    [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 CIFAR-10 的二进制格式，你可以在这里下载：[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)。
- en: 'This dataset was put together by Alex Krizhevsky, Vinod Nair, and Geoffrey
    Hinton. It consists of 60,000 tiny images 32 pixels high by 32 pixels wide. The
    binary format of CIFAR-10 is laid out as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集由 Alex Krizhevsky、Vinod Nair 和 Geoffrey Hinton 组成。它包含 60,000 张 32 像素高、32
    像素宽的小图像。CIFAR-10 的二进制格式如下所示：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It should be noted that it is not delimited or does not have any other information
    for validation of the file; as such, you should ensure that the MD5 checksum for
    the file that you have downloaded matches that on the website. As the structure
    is relatively simple, we can just pull the binary file straight into Go and parse
    it accordingly.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意，它没有分隔符或任何其他验证文件的信息；因此，你应该确保你下载的文件的 MD5 校验和与网站上的匹配。由于结构相对简单，我们可以直接将二进制文件导入
    Go 并相应地解析它。
- en: The 3,072 pixels are actually three layers of red, green, and blue values from
    0 to 255, over a 32 x 32 grid in row-major order, so this gives us our image data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这 3,072 个像素实际上是红、绿、蓝三层值，从 0 到 255，按行主序在 32 x 32 网格中排列，因此这为我们提供了图像数据。
- en: 'The label is a number from **0** to **9**, representing one of the following
    categories respectively:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是从 **0** 到 **9** 的数字，分别表示以下各类之一：
- en: '![](img/5dbee2fd-efdf-4ec2-815f-7db63d1ab4eb.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5dbee2fd-efdf-4ec2-815f-7db63d1ab4eb.png)'
- en: 'CIFAR-10 comes in six files, five training set files of 10,000 images each
    and one test set file of 10,000 images:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10 有六个文件，包括五个每个包含 10,000 张图像的训练集文件和一个包含 10,000 张图像的测试集文件：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Importing this in Go is easy—open the file and read the raw bytes. As every
    single underlying value is an 8-bit integer within a single byte, we can just
    cast it to whatever we want. If you wanted the single integer values, you could
    just convert them all into unsigned 8-bit integers; this is useful for when you
    want to convert the data into an image. You''ll find, however, we''ve made some
    slightly different decisions in the code, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Go 中导入这个很容易——打开文件并读取原始字节。由于每个底层值都是单字节内的 8 位整数，我们可以将其转换为任何我们想要的类型。如果你想要单个整数值，你可以将它们全部转换为无符号
    8 位整数；这在你想要将数据转换为图像时非常有用。然而，正如下面的代码所示，你会发现我们在代码中做了一些稍微不同的决定：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we are interested in using this data for our deep learning algorithm, it
    is prudent to not stray too far from our happy medium between `0` and `1`. We''re
    reusing pixel weight from the MNIST example, as shown here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有兴趣将这些数据用于我们的深度学习算法，因此最好不要偏离我们在 `0` 到 `1` 之间的中间点。我们正在重用来自 MNIST 示例的像素权重，如下所示：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will convert all our pixel values from 0 to 255 to a range between `0.1`
    and `1.0`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有像素值从 0 到 255 转换为 `0.1` 到 `1.0` 的范围。
- en: 'Similarly, for our labels, we will be using one-hot encoding again, encoding
    the desired label at `0.9` and everything else at `0.1`, as shown in the following
    code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于我们的标签，我们将再次使用一位有效编码，将期望的标签编码为 `0.9`，其他所有内容编码为 `0.1`，如下所示：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We''ve packaged this into a convenient `Load` function so we can call it from
    our code. It''ll return two conveniently shaped tensors for us to work with. This
    gives us a function that can import both the train and test sets:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将其打包为一个便利的 `Load` 函数，这样我们就可以从我们的代码中调用它。它将为我们返回两个方便形状的张量供我们使用。这为我们提供了一个可以导入训练集和测试集的函数：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This allows us to load the data in my `main` by calling the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许我们通过在 `main` 中调用以下方式来加载数据：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Epochs and batch size
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Epochs 和批处理大小
- en: 'We''ll choose `10` epochs for this example so that the code can be trained
    in less than an hour. It should be noted that 10 epochs will only get us to around
    20% accuracy, so do not be alarmed if you find the resulting model does not appear
    accurate; you will need to train it for much longer, maybe even around 1,000 epochs.
    On a modern computer, an epoch takes around three minutes to complete; for the
    sake of not requiring three days to complete this example, we''ve chosen to abbreviate
    the training process and will leave it as an exercise to assess the results of
    more epochs, as shown here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择`10`个epochs作为本例子的训练周期，这样代码可以在不到一个小时内完成训练。需要注意的是，仅仅进行10个epochs只能使我们达到约20%的准确率，因此如果发现生成的模型看起来不准确，不必惊慌；你需要更长时间的训练，甚至可能需要大约1,000个epochs。在现代计算机上，一个epoch大约需要三分钟来完成；为了不让这个例子需要三天的时间才能完成，我们选择了缩短训练过程，并留给你练习评估更多epochs的结果，如下所示：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that this model will consume a fairly large amount of memory; a `batchsize`
    of `100` can still mean you will need around 4 GB of memory. If you don't have
    this amount available without resorting to swapping memory, you may want to lower
    the batch size to make the code perform better on your computer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个模型将消耗相当大的内存；`batchsize`设为`100`仍可能需要大约4 GB的内存。如果你没有足够的内存而不得不使用交换内存，你可能需要降低批处理大小，以便代码在你的计算机上执行得更好。
- en: Accuracy
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率
- en: 'As this model takes much longer to converge, we should also add a rudimentary
    metric to track our accuracy. In order to do this, we must first extract our labels
    from the data - which we can do as below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个模型需要更长时间来收敛，我们还应该添加一个简单的度量来跟踪我们的准确性。为了做到这一点，我们必须首先从数据中提取我们的标签 - 我们可以像下面这样做：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We must then get our prediction from the output data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们需要从输出数据中获取我们的预测：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can then use this to update our accuracy metric. The amount by which it is
    updated is scaled by the number of examples - so that our output will be a percentage
    figure.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个来更新我们的准确性度量。更新的量将按示例的数量进行缩放，因此我们的输出将是一个百分比数字。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This give us a broad *accuracy* metric that we can use to gauge our training
    progress.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个广泛的*准确性*度量指标，可以用来评估我们的训练进展。
- en: Constructing the layers
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建层
- en: 'We can think of our layer structure having four parts. We are going to have
    three convolutional layers and one fully connected layer. Our first two layers
    are extremely similar - they follow the convolution-ReLU-MaxPool-dropout structure
    that we''ve described previously:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的层结构考虑为有四个部分。我们将有三个卷积层和一个全连接层。我们的前两层非常相似 - 它们遵循我们之前描述的卷积-ReLU-MaxPool-dropout结构：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our following layer is similar - we just need to join it to the output of our
    previous one:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的层类似 - 我们只需要将它连接到前一个输出：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following layer is essentially the same, but there is a slight change to
    prepare it for the change to the fully connected layer:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的层本质上是相同的，但为了准备好连接到全连接层，有些细微的改变：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`Layer 3` is something we''re already very familiar with—the fully connected
    layer—here, we have a fairly simple structure. We can certainly add more tiers
    to this layer (and this has been done by many different architectures before as
    well, with differing levels of success). This layer is demonstrated in the following
    code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`Layer 3`是我们已经非常熟悉的全连接层 - 在这里，我们有一个相当简单的结构。我们当然可以向这个层添加更多的层级（许多不同的架构之前也已经这样做过，成功的程度不同）。这个层的代码如下所示：'
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Loss function and solver
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数和求解器
- en: 'We will be using the ordinary cross-entropy loss function here, which can be
    implemented as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用普通的交叉熵损失函数，其实现如下：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Together with that, we will be using the Gorgonia tape machine and the RMSprop
    solver, as shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我们将使用Gorgonia的计算机器和RMSprop求解器，如下所示：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Test set output
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试集输出
- en: At the end of our training, we should pit our model against the test set.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练结束时，我们应该将我们的模型与测试集进行比较。
- en: 'First, we should import our test data as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该导入我们的测试数据如下：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we need recalculate our batches as the test set is sized differently
    from the train set:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要重新计算我们的批次，因为测试集的大小与训练集不同：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We then need to just add a quick way to track our results and output our results
    for later inspection by inserting the following code into the accuracy metric
    calculation code described earlier in the chapter:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要一种快速的方法来跟踪我们的结果，并将我们的结果输出以便稍后检查，将以下代码插入前述章节中描述的准确度度量计算代码中：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And finally, at the end of our run through the entire test set - write the
    data out to text files:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们运行整个测试集的最后时刻 - 将数据写入文本文件：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let's now assess the results.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们评估结果。
- en: Assessing the results
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估结果
- en: 'As mentioned previously, the example trained over 10 epochs is not particularly
    accurate. You will need to train it over many epochs to get better results. If
    you have been watching the cost and accuracy of the model, you''ll find that cost
    will stay relatively flat as accuracy increased over the number of epochs, as
    shown in the following graph:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，例子训练了 10 个 epochs 并不特别准确。您需要训练多个 epochs 才能获得更好的结果。如果您一直关注模型的成本和准确性，您会发现随着
    epochs 数量的增加，成本会保持相对稳定，准确性会增加，如下图所示：
- en: '![](img/56e90fa4-1342-4b15-8d55-a1287eb2cba9.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56e90fa4-1342-4b15-8d55-a1287eb2cba9.png)'
- en: 'It is still useful to explore the results to see how the model is performing;
    we''ll specifically look at cats:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有用地探索结果以查看模型的表现；我们将特别关注猫：
- en: '![](img/235068fc-b8a5-4209-aa8c-ebef1b32bb49.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/235068fc-b8a5-4209-aa8c-ebef1b32bb49.png)'
- en: As we can see, it currently appears to do much better with cats in very specific
    positions. Obviously, we need to find a solution to train it faster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，目前似乎在非常具体的位置上猫的表现要好得多。显然，我们需要找到一个训练更快的解决方案。
- en: GPU acceleration
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU 加速
- en: 'Convolution and its associated operations tend to do very well on GPU acceleration.
    You saw earlier that our GPU acceleration had minimal impact, but it is extremely
    useful for building CNNs. All we need to do is add the magical `''cuda''` build
    tag, as shown here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积及其相关操作在 GPU 加速上表现非常出色。您之前看到我们的 GPU 加速影响很小，但对于构建 CNNs 非常有用。我们只需添加神奇的 `'cuda'`
    构建标签，如下所示：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we tend to be more memory constrained on GPUs, be aware that the same batch
    size may not work on your GPU. The model as mentioned previously uses around 4
    GB of memory, so you will probably want to reduce the batch size if you have less
    than 6 GB of GPU memory (because presumably, you will be using about 1 GB for
    your normal desktop). If your model is running very slowly, or the CUDA version
    of your executable just fails, it would be prudent to check if being out of memory
    is the issue. You can do this using the NVIDIA SMI utility and getting it to check
    your memory every second, as shown here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPU 内存受限，同样的批次大小可能不适用于您的 GPU。如前所述，该模型使用约 4 GB 内存，因此如果您的 GPU 内存少于 6 GB（因为假设您正常桌面使用约
    1 GB），则可能需要减少批次大小。如果您的模型运行非常缓慢或者 CUDA 版本的可执行文件执行失败，最好检查是否存在内存不足的问题。您可以使用 NVIDIA
    SMI 实用程序，并让其每秒检查您的内存，如下所示：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will tend to produce the following report every second; watching it while
    your code runs will tell you broadly how much GPU memory your code is consuming:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致每秒生成以下报告；在代码运行时观察它将告诉您大致消耗了多少 GPU 内存：
- en: '![](img/4c24d016-134d-4fe8-b69c-397847f0af81.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c24d016-134d-4fe8-b69c-397847f0af81.png)'
- en: 'Let''s quickly compare the performance between CPU and GPU versions of our
    code. The CPU version takes broadly around three minutes per epoch, as shown in
    the following code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速比较 CPU 和 GPU 版本代码的性能。CPU 版本每个 epoch 大致需要三分钟，如下所示的代码：
- en: '[PRE23]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The GPU version takes around two minutes thirty seconds per epoch, as shown
    in the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 版本每个 epoch 大约需要两分钟三十秒，如下所示的代码：
- en: '[PRE24]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'A future version of Gorgonia will also include support for better operations;
    this is currently in testing, and you can use it by importing `gorgonia.org/gorgonia/ops/nn`
    and replacing your `Conv2d`, `Rectify`, `MaxPool2D`, and `Dropout` calls from
    their Gorgonia versions with their `nnops` version, An example of a slightly different
    `Layer 0` is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的 Gorgonia 版本还将包括对更好操作的支持；目前正在测试中，您可以通过导入 `gorgonia.org/gorgonia/ops/nn` 并将
    Gorgonia 版本的 `Conv2d`、`Rectify`、`MaxPool2D` 和 `Dropout` 调用替换为它们的 `nnops` 版本来使用它。稍有不同的
    `Layer 0` 示例如下：
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As an exercise, replace all the necessary operations and run it to see how it
    is different.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，替换所有必要的操作并运行以查看它的不同之处。
- en: CNN weaknesses
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 的弱点
- en: 'CNNs actually have a fairly major weakness: they are not orientation invariant,
    which means that if you were to feed the same image in, but upside down, the network
    is likely to not recognize it at all. One of the ways we can ensure this is not
    the case is to train the model with different rotations; however, there are better
    architectures that can solve this problem, which we will discuss later in this
    book.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CNN实际上有一个相当严重的弱点：它们不具备方向不变性，这意味着如果你把同一图像倒过来输入，网络很可能完全无法识别。我们可以确保这不是问题的一种方法是训练模型使用不同的旋转；然而，有更好的架构可以解决这个问题，我们稍后会在本书中讨论。
- en: They are also not scale invariant. Feeding it the same image much smaller or
    much larger makes it likely to fail. If you think back to why this is the case,
    it's because we are building the model based on a filter of a very specific size
    on a very specific group of pixels.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它们也不是尺度不变的。如果输入一张比较小或比较大的同一图像，模型很可能会失败。如果你回想一下为什么会这样，那是因为我们基于一个非常特定大小的过滤器在一个非常特定的像素组上构建模型。
- en: You have also seen that the model is very slow to train in general, especially
    on the CPU. We can get around this somewhat by using the GPU instead, but overall,
    it is an expensive process and can take several days to complete.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你也已经看到，通常情况下模型训练非常缓慢，特别是在CPU上。我们可以通过使用GPU来部分解决这个问题，但总体而言，这是一个昂贵的过程，可能需要几天的时间来完成。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: You have now learned how to build a CNN and how to tune some of the hyperparameters
    (such as the number of epochs and batch sizes) in order to get the desired result
    and get it running smoothly on different computers.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经学会了如何构建CNN以及如何调整一些超参数（如epoch数量和batch大小），以便获得期望的结果并在不同的计算机上顺利运行。
- en: As an exercise, you should try training this model to recognize MNIST digits,
    and even change around the structure of the convolutional layers; try Batch Normalization,
    and perhaps even more weights in the fully connected layer.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，你应该尝试训练这个模型以识别MNIST数字，甚至改变卷积层的结构；尝试批量归一化，也许甚至在全连接层中加入更多的权重。
- en: The next chapter will give an introduction to reinforcement learning and Q-learning
    and how to build a DQN and solve a maze.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍强化学习和Q学习的基础知识，以及如何构建一个DQN并解决迷宫问题。
- en: Further reading
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Character-Level Convolutional Networks for Text Classification* by *Xiang
    Zhang, Junbo Zhao* and *Yann LeCun*'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字符级卷积网络用于文本分类* 由 *张翔，赵军波* 和 *杨立昆* '
- en: '*U-Net: Convolutional Networks for Biomedical Image Segmentation* by *Olaf
    Ronneberger*, *Philipp Fischer*, and *Thomas Brox*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*U-Net：用于生物医学图像分割的卷积网络* 由 *Olaf Ronneberger*，*Philipp Fischer* 和 *Thomas Brox* '
- en: '*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks*
    by *Shaoqing Ren*, *Kaiming He*, *Ross Girshick*, and *Jian Sun*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更快的R-CNN：基于区域建议网络实现实时目标检测* 由 *任少卿*，*何凯明*，*Ross Girshick* 和 *孙剑* '
- en: '*Long-term Recurrent Convolutional Networks for Visual Recognition and Description*
    by *Jeff Donahue*, *Lisa Anne Hendricks*, *Marcus Rohrbach*, *Subhashini Venugopalan*,
    *Sergio Guadarrama*, *Kate Saenko*, and *Trevor Darrell*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*长期递归卷积网络用于视觉识别和描述* 由 *Jeff Donahue*，*Lisa Anne Hendricks*，*Marcus Rohrbach*，*Subhashini
    Venugopalan*，*Sergio Guadarrama*，*Kate Saenko* 和 *Trevor Darrell* '
