- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Interpreting Black Box Transformer Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释黑匣子变压器模型
- en: Million- to billion-parameter transformer models seem like huge black boxes
    that nobody can interpret. As a result, many developers and users have sometimes
    been discouraged when dealing with these mind-blowing models. However, recent
    research has begun to solve the problem with innovative, cutting-edge tools.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 百万到十亿参数的变压器模型看起来像是无法解释的巨大黑匣子。 因此，当处理这些令人震惊的模型时，许多开发者和用户有时会感到沮丧。 然而，最近的研究已经开始用创新的前沿工具解决了这个问题。
- en: It is beyond the scope of this book to describe all of the explainable AI methods
    and algorithms. So instead, this chapter will focus on ready-to-use visual interfaces
    that provide insights for transformer model developers and users.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 描述所有可解释的AI方法和算法超出了本书的范围。 因此，本章将专注于提供给变压器模型开发者和用户洞察力的即用型视觉界面。
- en: The chapter begins by installing and running `BertViz` by *Jesse Vig*. Jesse
    did quite an excellent job of building a visual interface that shows the activity
    in the attention heads of a BERT transformer model. BertViz interacts with the
    BERT models and provides a well-designed interactive interface.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先通过安装和运行*Jesse Vig*的`BertViz`来开始。 Jesse在构建显示BERT变压器模型中注意力头活动的可视界面方面做得相当出色。
    BertViz与BERT模型交互，并提供了一个设计良好的交互界面。
- en: We will continue to focus on visualizing the activity of transformer models
    with the **Language Interpretability Tool** (**LIT**). LIT is a non-probing tool
    that can use PCA or UMAP to represent transformer model predictions. We will go
    through PCA and use UMAP as well.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续专注于使用**语言可解释工具**（**LIT**）来可视化变压器模型的活动。 LIT是一个非侵入性工具，可以使用PCA或UMAP来表示变压器模型的预测。
    我们将介绍PCA并使用UMAP。
- en: Finally, we will visualize a transformer’s journey through the layers of a BERT
    model with dictionary learning. **Local Interpretable Model-agnostic Explanations**
    (**LIME**) provides practical functions to visualize how a transformer learns
    how to understand language. The method shows that transformers often begin by
    learning a word, then the word in the sentence context, and finally long-range
    dependencies.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用字典学习可视化变压器穿越BERT模型层的旅程。 **局部可解释的模型不可知解释**（**LIME**）提供了可视化变压器学习如何理解语言的实用函数。
    该方法显示变压器通常首先学习一个词，然后学习句子上下文中的词，最后是长距离依赖关系。
- en: By the end of the chapter, you will be able to interact with users to show visualizations
    of the activity of transformer models. BertViz, LIT, and visualizations through
    dictionary learning still have a long way to go. However, these nascent tools
    will help developers and users understand how transformer models work.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够与用户交互，以展示变压器模型的活动可视化。 BertViz，LIT和通过字典学习进行的可视化仍有很长的路要走。 然而，这些新生工具将帮助开发者和用户理解变压器模型的工作方式。
- en: 'This chapter covers the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Installing and running BertViz
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和运行BertViz
- en: Running BertViz’s interactive interface
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行BertViz的交互界面
- en: The difference between probing and non-probing methods
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探究和非侵入性方法的区别
- en: A **Principal Component Analysis** (**PCA**) reminder
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）提醒'
- en: Running LIT to analyze transformer outputs
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行LIT分析变压器输出
- en: Introducing LIME
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍LIME
- en: Running transformer visualization through dictionary learning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过字典学习运行变压器可视化
- en: Word-level polysemy disambiguation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词级多义消歧
- en: Visualizing low-level, mid-level, and high-level dependencies
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化低级，中级和高级依赖性
- en: Visualizing key transformer factors
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化关键变压器因素
- en: Our first step will begin by installing and using BertViz.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将从安装和使用BertViz开始。
- en: Transformer visualization with BertViz
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BertViz进行变压器可视化
- en: '*Jesse Vig*’s article, *A Multiscale Visualization of Attention in the Transformer
    Model*, 2019, recognizes the effectiveness of transformer models. However, *Jesse
    Vig* explains that deciphering the attention mechanism is challenging. The paper
    describes the process of BertViz, a visualization tool.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jesse Vig*的文章，*变压器模型中注意力的多尺度可视化*，2019年，认识到了变压器模型的有效性。 但是，*Jesse Vig*解释了解密注意力机制是具有挑战性的。
    该论文描述了BertViz的过程，即可视化工具。'
- en: BertViz can visualize attention head activity and interpret a transformer model’s
    behavior.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BertViz可以可视化注意力头活动并解释变压器模型的行为。
- en: BertViz was first designed to visualize BERT and GPT-3 models. In this section,
    we will visualize the activity of a BERT model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: BertViz最初是为了可视化BERT和GPT-3模型而设计的。 在本节中，我们将可视化BERT模型的活动。
- en: Let’s now install and run BertViz.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们安装并运行BertViz。
- en: Running BertViz
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行BertViz
- en: It only takes five steps to visualize transformer attention heads and interact
    with them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化变形器注意头并与之交互只需五个步骤。
- en: Open the `BertViz.ipynb` notebook in the `Chapter14` directory in the GitHub
    repository of this book.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub仓库的`Chapter14`目录中打开`BertViz.ipynb`笔记本。
- en: The first step is to install `BertViz` and the requirements.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装`BertViz`和必要的依赖。
- en: 'Step 1: Installing BertViz and importing the modules'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤1：安装BertViz并导入模块
- en: 'The notebook installs `BertViz`, Hugging Face transformers, and the other basic
    requirements to implement the program:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本安装了`BertViz`、Hugging Face transformers和其他实现程序所需的基本要求：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The head view and model view libraries are now imported. We will now load the
    BERT model and tokenizer.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 头视图和模型视图库现在已被导入。我们现在将加载BERT模型和分词器。
- en: 'Step 2: Load the models and retrieve attention'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤2：加载模型并检索注意力
- en: 'BertViz supports BERT, GPT-2, RoBERTa, and other models. You can consult BertViz
    on GitHub for more information: [https://github.com/jessevig/BertViz](https://github.com/jessevig/BertViz).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: BertViz支持BERT、GPT-2、RoBERTa和其他模型。您可以在GitHub上查阅BertViz获取更多信息：[https://github.com/jessevig/BertViz](https://github.com/jessevig/BertViz)。
- en: 'In this section, we will run a `bert-base-uncased` model and a pretrained tokenizer:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将运行一个`bert-base-uncased`模型和一个预训练的分词器：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now enter our two sentences. You can try different sequences to analyze
    the behavior of the model. `sentence_b_start` will be necessary for *Step: 5 Model
    view*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在输入两个句子。您可以尝试不同的序列以分析模型的行为。`sentence_b_start`将在*步骤5：模型视图*中必需：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And that’s it! We are ready to interact with the visualization interface.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们已经准备好与可视化界面进行交互。
- en: 'Step 3: Head view'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤3：头视图
- en: 'We just have one final line to add to activate the visualization of the attention
    heads:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需添加一个最后一行来激活注意头的可视化：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The words of the first layer (layer 0) are not the actual tokens, but the interface
    is educational. The 12 attention heads of each layer are displayed in different
    colors. The default view is set to layer 0, as shown in *Figure 14.1*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层（layer 0）中的单词并非实际标记，但界面是教育性的。每一层的12个注意头以不同的颜色显示。默认视图设置为层0，如*图14.1*所示：
- en: '![](img/B17948_14_01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_01.png)'
- en: 'Figure 14.1: The visualization of attention heads'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：注意头的可视化
- en: We are now ready to explore attention heads.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备探索注意头。
- en: 'Step 4: Processing and displaying attention heads'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤4：处理和显示注意头
- en: Each color above the two columns of tokens represents an attention head of the
    layer number. Choose a layer number and click on an attention head (color).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上面两列标记中的每种颜色表示该层编号的一个注意头。选择一个层编号并单击一个注意头（颜色）。
- en: The words in the sentences are broken down into tokens in the attention. However,
    in this section, the word `tokens` loosely refers to `words` to help us understand
    how the transformer heads work.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的单词在注意中被分解成标记。然而，在本节中，单词`tokens`宽泛指`words`，以帮助我们了解变形器头是如何工作的。
- en: 'I focused on the word `animals` in *Figure 14.2*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我专注于*图14.2*中的单词`animals`：
- en: '![](img/B17948_14_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_02.png)'
- en: 'Figure 14.2: Selecting a layer, an attention head, and a token'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：选择一个层、一个注意头和一个标记
- en: '`BertViz` shows that the model made a connection between `animals` and several
    words. This is normal since we are only at layer 0.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`BertViz`显示模型在`animals`和几个单词之间建立了连接。这是正常的，因为我们只在层0。'
- en: 'Layer 1 begins to isolate words `animals` is related to, as shown in *Figure
    14.3*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Layer 1开始单独分离与之相关的单词`animals`，如*图14.3*所示：
- en: '![](img/B17948_14_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_03.png)'
- en: 'Figure 14.3: Visualizing the activity of attention head 11 of layer 1'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：可视化第1层中注意头11的活动
- en: Attention head 11 makes a connection between `animals`, `people`, and `adopt`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意头11在`animals`、`people`和`adopt`之间建立了一个连接。
- en: 'If we click on `cats`, some interesting connections are shown in *Figure 14.4*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们点击`cats`，一些有趣的连接将显示在*图14.4*中：
- en: '![](img/B17948_14_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_04.png)'
- en: 'Figure 14.4: Visualizing the connections between cats and other tokens'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：可视化猫与其他标记之间的连接
- en: The word `cats` is now associated with `animals`. This connection shows that
    the model is learning that cats are animals.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，单词`cats`与`animals`相关联。这种关联表明模型正在学习猫是动物。
- en: You can change the sentences and then click on the layers and attention heads
    to visualize how the transformer makes connections. You will find limits, of course.
    The good and bad connections will show you how transformers work and fail. Both
    cases are valuable for explaining how transformers behave and why they require
    more layers, parameters, and data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以更改句子，然后单击图层和注意力头，以可视化变压器如何建立连接。当然，您会发现限制。好的和坏的连接将向您展示变压器的工作原理和失败情况。这两种情况对于解释变压器的行为以及为什么它们需要更多层、参数和数据都是有价值的。
- en: Let’s see how `BertViz` displays the model view.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `BertViz` 如何显示模型视图。
- en: 'Step 5: Model view'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 5：模型视图
- en: 'It only takes one line to obtain the model view of a transformer with `BertViz`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 只需一行代码就可以获得一个带有 `BertViz` 的变压器的模型视图：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`BertViz` displays all of the layers and heads in one view, as shown in the
    view excerpt in *Figure 14.5*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`BertViz` 在一个视图中显示所有的层和头部，如 *图 14.5* 中所示：'
- en: '![Une image contenant bâtiment  Description générée automatiquement](img/B17948_14_05.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含建筑物的图像，自动生成的描述](img/B17948_14_05.png)'
- en: 'Figure 14.5: Model view mode of BertViz'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5：BertViz 的模型视图模式
- en: 'If you click on one of the heads, you will obtain a head view with word-to-word
    and sentence-to-sentence options. You can then go through the attention heads
    to see how the transformer model makes better representations as it progresses
    through the layers. For example, *Figure 14.6* shows the activity of an attention
    head in the first layers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您单击其中一个头部，将获得一个带有单词对单词和句子对句子选项的头部视图。然后，您可以浏览注意力头，以查看变压器模型在通过层时如何生成更好的表示。例如，*图
    14.6* 显示了第一层中一个注意力头的活动：
- en: '![Une image contenant texte, noir, laser  Description générée automatiquement](img/B17948_14_06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、黑色、激光的图像，自动生成的描述](img/B17948_14_06.png)'
- en: 'Figure 14.6: Activity of an attention head in the lower layers of the model'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6：模型较低层的注意力头活动
- en: Sometimes, the representation makes connections between the separator, `[SEP]`,
    and words, which does not make much sense. However, sometimes tokens are not activated
    in every attention head of every layer. Also, the level of training of a transformer
    model limits the quality of the interpretation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，表示会在分隔符 `[SEP]` 和单词之间建立连接，这并没有太多意义。但是，有时标记不会在每个层的每个注意力头中激活。此外，变压器模型的训练水平限制了解释的质量。
- en: In any case, `BertViz` remains an interesting educational tool and interpretability
    tool for transformer models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，`BertViz` 仍然是一个有趣的教育工具和变压器模型的可解释性工具。
- en: Let’s now run the intuitive LIT tool.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行直观的 LIT 工具。
- en: LIT
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LIT
- en: LIT’s visual interface will help you find examples that the model processes
    incorrectly, dig into similar examples, see how the model behaves when you change
    a context, and more language issues related to transformer models.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LIT 的可视化界面将帮助您找到模型处理不正确的示例，深入研究类似示例，查看在更改上下文时模型的行为以及与变压器模型相关的更多语言问题。
- en: LIT does not display the activities of the attention heads like `BertViz` does.
    However, it’s worth analyzing why things went wrong and trying to find solutions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: LIT 不像 `BertViz` 那样显示注意力头的活动。然而，值得分析为什么事情出错了并尝试找到解决方案。
- en: You can choose a **Uniform Manifold Approximation and Projection** (**UMAP**)
    visualization or a PCA projector representation. PCA will make more linear projections
    in specific directions and magnitude. UMAP will break its projections down into
    mini-clusters. Both approaches make sense depending on how far you want to go
    when analyzing the output of a model. You can run both and obtain different perspectives
    of the same model and examples.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择**均匀流形逼近与投影**（**UMAP**）可视化或 PCA 投影仪表示。PCA 将在特定方向和幅度上进行更线性的投影。UMAP 将其投影分解为小型聚类。根据您分析模型输出的深入程度，这两种方法都是有意义的。您可以同时运行并获得相同模型和示例的不同视角。
- en: This section will use PCA to run LIT. Let’s begin with a brief reminder of how
    PCA works.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用 PCA 运行 LIT。让我们从简要回顾 PCA 的工作原理开始。
- en: PCA
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: PCA takes data and represents it at a higher level.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 将数据提升到更高的层次进行表示。
- en: Imagine you are in your kitchen. Your kitchen is a 3D cartesian coordinate system.
    The objects in your kitchen are all at specific *x*, *y*, *z* coordinates too.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下您在厨房里。您的厨房是一个 3D 笛卡尔坐标系。您厨房里的物体也都有特定的 *x*、*y*、*z* 坐标。
- en: You want to cook a recipe and gather the ingredients on your kitchen table.
    Your kitchen table is a higher-level representation of the recipe in your kitchen.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要烹饪一道菜，并将配料摆放在厨房桌上。您的厨房桌是您厨房里菜谱的较高级别表示。
- en: The kitchen table is using a cartesian coordinate system too. But when you extract
    the *main features* of your kitchen to represent the recipe on your kitchen table,
    you are performing PCA. This is because you have displayed the principal components
    that fit together to make a specific recipe.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 厨房桌也在使用笛卡尔坐标系。但是当您提取厨房的*主要特征*以在厨房桌上表示菜谱时，您正在执行 PCA。这是因为您展示了一起适合制作特定菜谱的主要组件。
- en: The same representation can be applied to NLP. For example, a dictionary is
    a list of words. But the words that mean something together constitute a representation
    of the principal components of a sequence.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的表现可以应用于 NLP。例如，字典是一个词语列表。但是一起有意义的词语构成序列主成分的表示。
- en: The PCA representation of sequences in LIT will help visualize the outputs of
    a transformer.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LIT 中序列的 PCA 表示将有助于可视化变压器的输出。
- en: 'The main steps to obtain an NLP PCA representation are:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 获得 NLP PCA 表示的主要步骤是：
- en: '**Variance**: The numerical variance of a word in a dataset; the frequency
    and frequency of its meaning, for example.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差**：数据集中一个词语的数值方差；其含义的频率和频率，例如。'
- en: '**Covariance**: The variance of more than one word is related to that of another
    word in the dataset.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协方差**：多个单词的方差与数据集中另一个单词的相关性。'
- en: '**Eigenvalues and eigenvectors**: To obtain a representation in the cartesian
    system, we need the vectors and magnitudes representation of the covariances.
    The eigenvectors will provide the direction of the vectors. The eigenvalues will
    provide their magnitudes.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征值和特征向量**：要在笛卡尔系统中获得表示，我们需要协方差的向量和幅角表示。特征向量将提供向量的方向。特征值将提供它们的幅角。'
- en: '**Deriving the data**: The last step is to apply the feature vectors to the
    original dataset by multiplying the row feature vector by the row data:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**派生数据**：最后一步是通过将行特征向量乘以行数据来将特征向量应用于原始数据集：'
- en: '**Data to display** = row of feature vector * row of data'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**要显示的数据** = 行特征向量 * 行数据'
- en: PCA projections provide a clear linear visualization of the data points to analyze.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 投影提供了清晰的数据点线性可视化以进行分析。
- en: Let’s now run LIT.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行 LIT。
- en: Running LIT
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 LIT
- en: 'You can run LIT online or open it in a Google Colaboratory notebook. Click
    on the following link to access both options:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在线运行 LIT 或在 Google Colaboratory 笔记本中打开它。单击以下链接以访问两个选项：
- en: '[https://pair-code.github.io/lit/](https://pair-code.github.io/lit/)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pair-code.github.io/lit/](https://pair-code.github.io/lit/)'
- en: 'The tutorial page contains several types of NLP tasks to analyze:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 教程页面包含几种类型的 NLP 任务进行分析：
- en: '[https://pair-code.github.io/lit/tutorials/](https://pair-code.github.io/lit/tutorials/)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pair-code.github.io/lit/tutorials/](https://pair-code.github.io/lit/tutorials/)'
- en: 'In this section, we will run LIT online and explore a sentiment analysis classifier:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在线运行 LIT 并探索情感分析分类器：
- en: '[https://pair-code.github.io/lit/tutorials/sentiment/](https://pair-code.github.io/lit/tutorials/sentiment/)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://pair-code.github.io/lit/tutorials/sentiment/](https://pair-code.github.io/lit/tutorials/sentiment/)'
- en: 'Click on **Explore this demo yourself**, and you will enter the intuitive LIT
    interface. The transformer model is a small transformer model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 单击**自己探索这个演示**，您将进入直观的 LIT 接口。变压器模型是一个小型变压器模型：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_14_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant texte  Description générée automatiquement](img/B17948_14_07.png)'
- en: 'Figure 14.7: Selecting a model'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：选择模型
- en: 'You can change the model by clicking on the model. You can test this type of
    model and similar ones directly on Hugging Face on its hosted API page:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过单击模型更改模型。您可以直接在 Hugging Face 提供的托管 API 页面上测试这种类型的模型和类似模型：
- en: '[https://huggingface.co/sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english)'
- en: The NLP models might change on LIT’s online version based on subsequent updates.
    The concepts remain the same, just the models change.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 模型可能会根据 LIT 在线版本的后续更新而发生变化。概念保持不变，只是模型变化。
- en: 'Let’s begin by selecting the PCA projector and the binary `0` or `1` classification
    label of the sentiment analysis of each example:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从选择PCA投影仪和每个示例的二元`0`或`1`分类标签开始：
- en: '![](img/B17948_14_08.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17948_14_08.png)'
- en: 'Figure 14.8: Selecting the projector and type of label'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8：选择投影仪和标签类型
- en: 'We then go to the data table and click on a **sentence** and its classification
    **label**:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们转到数据表，点击一个**句子**及其分类**标签**：
- en: '![](img/B17948_14_09.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17948_14_09.png)'
- en: 'Figure 14.9: Selecting a sentence'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9：选择一句话
- en: The algorithm is stochastic so the output can vary from one run to another.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法是随机的，因此输出可能会在不同运行中有所变化。
- en: 'The sentence will also appear in the datapoint editor:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据点编辑器中也会出现这个句子：
- en: '![](img/B17948_14_10.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17948_14_10.png)'
- en: 'Figure 14.10: Datapoint editor'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.10：数据点编辑器
- en: The datapoint editor allows you to change the context of the sentence. For example,
    you might want to find out what went wrong with a counterfactual classification
    that should have been in one class but ended up in another one. You can change
    the context of the sentence until it appears in the correct class to understand
    how the model works and why it made a mistake.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点编辑器允许您改变句子的上下文。例如，您可能想要找出为什么一个反事实的分类本应属于一个类别，但最终属于另一个类别。您可以改变句子的上下文，直到它出现在正确的类别中，以了解模型的工作原理及为何出现错误。
- en: 'The sentence will appear in the PCA projector with its classification:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这个句子将与它的分类一起出现在PCA投影仪中：
- en: '![](img/B17948_14_11.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17948_14_11.png)'
- en: 'Figure 14.11: PCA projector in a positive cluster'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：正向聚类中的PCA投影仪
- en: You can click the data points in the PCA projector, and the sentences will appear
    in the datapoint editor under the sentence you selected. That way, you can compare
    results.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在PCA投影仪中点击数据点，所选句子将出现在数据点编辑器中。这样，您可以比较结果。
- en: LIT contains a wide range of interactive functions you can explore and use.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LIT包含许多您可以探索和使用的交互功能。
- en: The results obtained in LIT are not always convincing. However, LIT provides
    valuable insights in many cases. Also, it is essential to get involved in these
    emerging tools and techniques.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: LIT中得到的结果并不总是令人信服。然而，在许多情况下，LIT提供了有价值的见解。同时，参与这些新兴的工具和技术是至关重要的。
- en: Let’s now visualize transformer layers through dictionary learning.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过词典学习来可视化变压器层。
- en: Transformer visualization via dictionary learning
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过词典学习的变压器可视化
- en: Transformer visualization via dictionary learning is based on transformer factors.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过词典学习的变压器可视化是基于变压器因子的。
- en: Transformer factors
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器因子
- en: A transformer factor is an embedding vector that contains contextualized words.
    A word with no context can have many meanings, creating a polysemy issue. For
    example, the word `separate` can be a verb or an adjective. Furthermore, `separate`
    can mean disconnect, discriminate, scatter, and has many other definitions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器因子是包含上下文单词的嵌入向量。一个没有上下文的单词可以有许多意义，从而产生多义问题。例如，单词`separate`可以是动词或形容词。此外，`separate`还可以意味着断开、区分、分散，以及许多其他定义。
- en: '*Yun* et al., 2021, thus created an embedding vector with contextualized words.
    A word embedding vector can be constructed with sparse linear representations
    of word factors. For example, depending on the context of the sentences in a dataset,
    `separate` can be represented as:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*Yun*等人在2021年创造了一个包含上下文单词的嵌入向量。单词的嵌入向量可以由单词因子的稀疏线性表示构建。例如，根据数据集中句子的上下文，`separate`可以被表示为：'
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To ensure that a linear representation remains sparse, we don’t add 0 factors
    that would create huge matrices with 0 values. Thus, we do not include useless
    information such as:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保线性表示保持稀疏，我们不添加会创建大量0值矩阵的0因子。因此，我们不包括无用的信息，比如：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The whole point is to keep the representation sparse by forcing the coefficients
    of the factors to be greater than 0.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 整个关键是通过强制因子的系数大于0来保持表示的稀疏性。
- en: 'The hidden state for each word is retrieved for each layer. Since each layer
    progresses in its understanding of the representation of the word in the dataset
    of sentences, the latent dependencies build up. This sparse linear superposition
    of transformer factors becomes a dictionary matrix with a sparse vector of coefficients
    to be inferred that we can sum up as:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词的隐藏状态在每一层中都被检索出来。由于每一层在理解数据集中句子中单词的表示方面都有所进展，潜在的依赖关系逐渐增加。这种稀疏线性叠加的变压器因子变成了一个带有稀疏系数向量的词典矩阵，我们可以将其总结为：
- en: '![](img/B17948_14_001.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_001.png)'
- en: 'In which:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中：
- en: '![](img/B17948_14_002.png) (phi) is the dictionary matrix'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B17948_14_002.png)（phi）是词典矩阵。'
- en: '![](img/B17948_14_003.png) is the sparse vector of coefficients to be inferred'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B17948_14_003.png)是待推断的稀疏系数向量'
- en: '*Yun* et al., 2021, added, ![](img/B17948_14_004.png), Gaussian noise samples
    to force the algorithm to search for deeper representations.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*Yun*等人在2021年补充说，![](img/B17948_14_004.png)，添加高斯噪声样本以强制算法搜索更深层的表示。'
- en: Also, to ensure that the representation remains sparse, the equation must be
    written s.t. (such that) ![](img/B17948_14_005.png).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，为了确保表示保持稀疏，方程必须写成，使得（such that）![](img/B17948_14_005.png)。
- en: The authors refer to *X* as the set of hidden states of the layers and *x* as
    a sparse linear superposition of transformer factors that belongs to *X*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将*X*指代为层的隐藏状态集合，将*x*指代为属于*X*的变换因子的稀疏线性叠加。
- en: 'They beautifully sum up their sparse dictionary learning model as:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将自己稀疏字典学习模型总结为：
- en: '![](img/B17948_14_006.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_006.png)'
- en: In the dictionary matrix, ![](img/B17948_14_007.png):,c refers to a column of
    the dictionary matrix and contains a transformer factor.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在词典矩阵中，![](img/B17948_14_007.png):,c 指的是词典矩阵的一列，其中包含一个变换因子。
- en: '![](img/B17948_14_008.png):,c is divided into three levels:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B17948_14_008.png):,c 分为三个级别：'
- en: '**Low-level** transformer factors to solve polysemy problems through word-level
    disambiguation'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低级**变换因子用于通过词级消歧解决多义问题'
- en: '**Mid-level** transformer factors take us further into sentence-level patterns
    that will bring vital context to the low level'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中级**变换因子将我们进一步带入句子级模式，这将为低级模式带来重要的上下文。'
- en: '**High-level** transformer patterns that will help understand long-range dependencies'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级**变换模式帮助理解长程依赖关系'
- en: The method is innovative, exciting, and seems efficient. However, there is no
    visualization functionality at this point. Therefore, *Yun* et al., 2021, created
    the necessary information for LIME, a standard interpretable AI method to visualize
    their findings.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法是创新的、令人兴奋的，看起来效率很高。然而，目前还没有可视化功能。因此，*Yun*等人于2021年为LIME创造了必要的信息，这是一种标准的可解释人工智能方法，可以用于可视化他们的发现。
- en: The interactive transformer visualization page is thus based on LIME for its
    outputs. The following section is a brief introduction to LIME.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式变换器可视化页面基于LIME进行输出。以下部分是对LIME的简要介绍。
- en: Introducing LIME
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍LIME
- en: '**LIME** stands for **Local Interpretable Model-Agnostic Explanations**. The
    name of this explainable AI method speaks for itself. It is *model-agnostic*.
    Thus, we can draw immediate consequences about the method of transformer visualization
    via dictionary learning:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**LIME**代表**局部可解释的模型-不可知解释**。这一可解释的人工智能方法的名称说明了它的特点。它是*模型不可知*的。因此，我们可以立即得出有关通过词典学习进行变换器可视化方法的结论：'
- en: This method does not dig into the matrices, weights, and matrix multiplications
    of transformer layers.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法不会深入研究变换器层的矩阵、权重和矩阵乘法。
- en: The method does not explain how a transformer model works, as we did in *Chapter
    2*, *Getting Started with the Architecture of the Transformer Model*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法不会解释变换器模型的工作原理，就像我们在*第二章*中所做的那样，*开始使用变换器模型的架构*。
- en: In this chapter, the method peeks into the mathematical outputs provided by
    the sparse linear superpositions of transformer factors.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，该方法窥视了由变换因子的稀疏线性叠加提供的数学输出。
- en: LIME does not try to parse all of the information in a dataset. Instead, LIME
    finds out whether a model is *locally reliable* by examining the features around
    a prediction.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: LIME并不试图解析数据集中的所有信息。相反，LIME通过检查预测周围的特征来判断模型是否*局部可靠*。
- en: LIME does not apply to the model globally. It focuses on the local environment
    of a prediction.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: LIME不适用于全局模型。它专注于预测的局部环境。
- en: This is particularly efficient when dealing with NLP because LIME explores the
    context of a word, providing invaluable information on the model’s output.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这在处理自然语言处理时尤为有效，因为LIME探索了一个词的上下文，为模型的输出提供了宝贵的信息。
- en: 'In visualization via dictionary learning, an instance *x* can be represented
    as:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过词典学习进行可视化时，一个实例*x*可以表示为：
- en: '![](img/B17948_14_009.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_009.png)'
- en: 'The interpretable representation of this instance is a binary vector:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此实例的可解释表示是一个二进制向量：
- en: '![](img/B17948_14_010.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_010.png)'
- en: The goal is to determine the local presence or absence of a feature or several
    features. In NLP, the features are tokens that can be reconstructed into words.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是确定特征或多个特征的局部存在或缺失。在NLP中，这些特征是可以重建为单词的令牌。
- en: 'For LIME, *g* represents a transformer model or any other machine learning
    model. *G* represents a set of transformer models containing *g*, among other
    models:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LIME来说，*g*代表一个变压器模型或任何其他机器学习模型。*G*代表一组包含*g*在内的变压器模型，以及其他模型：
- en: '![](img/B17948_14_011.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_011.png)'
- en: LIME’s algorithm can thus be applied to any transformer model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LIME的算法可以应用于任何变压器模型。
- en: 'At this point, we know that:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们知道：
- en: LIME targets a word and searches the local context for other words
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME以一个词为目标，并搜索其他词的局部上下文
- en: LIME thus provides the local context of a word to explain why that word was
    predicted and not another one
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，LIME提供了一个词的局部上下文，以解释为什么会预测这个词，而不是其他词
- en: Exploring explainable AI such as LIME is not in the scope of this book on transformers
    for NLP. However, for more on LIME, see the *References* section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 探索LIME等可解释AI并不在这本关于NLP的变压器书籍的范围内。不过，有关LIME的更多信息，请参见*参考*部分。
- en: Let’s now see how LIME fits in the method of transformer visualization via dictionary
    learning.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看LIME是如何将自己融入通过字典学习的变压器可视化方法中的。
- en: Let’s now explore the visualization interface.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索可视化界面。
- en: The visualization interface
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化界面
- en: 'Visit the following site to access the interactive transformer visualization
    page: [https://transformervis.github.io/transformervis/](https://transformervis.github.io/transformervis/).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 访问以下网站以访问交互式的变压器可视化页面：[https://transformervis.github.io/transformervis/](https://transformervis.github.io/transformervis/)。
- en: 'The visualization interface provides intuitive instructions to start analyzing
    a transformer factor of a specific layer in one click, as shown in *Figure 14.12*:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化界面提供了直观的说明，只需点击一次即可开始分析特定层的变压器因素，如*图 14.12*所示：
- en: '![](img/B17948_14_12.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_12.png)'
- en: 'Figure 14.12: Selecting a transformer factor'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12：选择一个变压器因素
- en: 'Once you have chosen a factor, you can click on the layer you wish to visualize
    for this factor:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了一个因素，您可以点击您想要为这个因素可视化的层：
- en: '![](img/B17948_14_13.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_13.png)'
- en: 'Figure 14.13: Visualize function per layer'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13：每层的函数可视化
- en: 'The first visualization shows the activation of the factor layer by layer:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个可视化展示了按层激活因素的情况：
- en: '![](img/B17948_14_14.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_14.png)'
- en: 'Figure 14.14: Importance of a factor for each layer'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.14：每层因素的重要性
- en: 'Factor `421` focuses on the lexical field of `separate`, as shown in the lower
    layers:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因素`421`着眼于“separate”的词汇领域，正如下层所示：
- en: '![](img/B17948_14_15.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_15.png)'
- en: 'Figure 14.15: The representation of “separate” in the lower layers'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.15：在下层中对“separate”的表示
- en: 'As we visualize higher layers, longer-range representations emerge. Factor
    `421` began with the representation of `separate`. But at higher levels, the transformer
    began to form a deeper understanding of the factor and associated `separate` with
    `distinct`, as shown in *Figure 14.16*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们可视化更高的层级，会出现更长范围的表示。因素`421`从“separate”的表示开始。但在更高的层次上，变压器开始形成对因素的更深层次的理解，并将“separate”与“distinct”相关联，如*图
    14.16*所示：
- en: '![](img/B17948_14_16.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_14_16.png)'
- en: 'Figure 14.16: The higher-layer representations of a transformer factor'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.16：变压器要素的高层表示
- en: Try several transformer factors to visualize how transformers expand their perception
    and understanding of language, layer by layer.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用几个变压器因素来可视化，以便逐层扩展他们对语言的理解和感知。
- en: You will find many good examples and also poor results. Focus on the good examples
    to understand how a transformer makes its way through language learning. Use the
    poor results to understand why it made a mistake. Also, the transformer model
    used for the visualization interface is not the most powerful or well-trained
    one.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现许多好例子，也会发现很多糟糕的结果。专注于好例子，以了解变压器如何通过语言学习的方式。利用糟糕的结果来理解它为什么会犯错误。此外，可视化界面所使用的变压器模型并非最强大或经过充分训练的模型。
- en: In any case, get involved and stay in the loop of this ever-evolving field!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，参与其中并保持对这个不断发展的领域的关注！
- en: For example, you can explore `Understanding_GPT_2_models_with_Ecco.ipynb`, which
    is in the GitHub repository of this book for this chapter. It shows you how transformers
    generate candidates before choosing a token. It is self-explanatory.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以探索 `Understanding_GPT_2_models_with_Ecco.ipynb`，这是本章的GitHub存储库中的内容。它演示了变压器在选择标记之前如何生成候选项。这是不言自明的。
- en: 'In this section, we saw how transformers learn the meaning of words layer by
    layer. A transformer generates candidates before making a choice. As shown in
    the notebook, a transformer model is stochastic and as such chooses among several
    top probabilities. Consider the following sentence:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了变压器如何逐层学习单词的含义。一个变压器在做出选择之前会生成候选项。正如笔记本所示，变压器模型是随机的，因此会在几个最高概率中进行选择。考虑以下句子：
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: What word would you choose at the end of the sentence? We all hesitate. So do
    transformers!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 您会选择什么词放在句子的结尾？我们都会犹豫。这也是变压器所做的！
- en: 'In this case, the GPT-2 model chooses the word `sky`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，GPT-2模型选择了“sky”这个词：
- en: '![Graphical user interface  Description automatically generated](img/B17948_14_17.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成描述](img/B17948_14_17.png)'
- en: 'Figure 14.17: Completing a sequence'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.17：完成一个序列
- en: 'But there are other candidates the GPT-2 model may choose in another run, as
    shown in *Figure 14.18*:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 但是GPT-2模型可能会在另一次运行中选择其他候选项，就像*图14.18*中显示的：
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_14_18.png)Figure
    14.18: The other candidates for the completion'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面，应用程序 自动生成描述](img/B17948_14_18.png)图14.18：其他的完成候选项'
- en: We can see that `sky` appears in rank first. However, `morning` appears in rank
    second and could fit as well. If we run the model several times, we may obtain
    different outputs because the model is stochastic.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到“sky”出现在第一名。但“morning”也出现在第二名，并且也可以合适。如果我们多次运行模型，可能会得到不同的输出，因为模型是随机的。
- en: It seems that the domain of AI and transformers is complete.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来AI和变压器的领域已经完备了。
- en: However, let’s see why humans still have a lot of work to do before we go.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们继续之前，让我们看看为什么人们在这个领域仍然有很多工作要做。
- en: Exploring models we cannot access
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索我们无法访问的模型
- en: The visual interfaces explored in this chapter are fascinating. However, there
    is still a lot of work to do!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨的可视化界面非常引人入胜。但仍然有很多工作要做！
- en: For example, OpenAI’s GPT-3 model runs online or through an API. Thus, we cannot
    access the weights of some **Software as a Service** (**SaaS**) transformer models.
    This trend will increase and expand in the years to come. Corporations that spend
    millions of dollars on research and computer power will tend to provide pay-as-you-go
    services, not open-source applications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI的GPT-3模型可以在线运行或通过API运行。因此，我们无法访问一些**软件即服务**（**SaaS**）变压器模型的权重。这种趋势将在未来数年内增加和扩展。那些花费数百万美元用于研究和计算机动力的公司更倾向于提供按需付费的服务，而不是开源应用程序。
- en: Even if we had access to the source code or output weights of a GPT-3 model,
    using a visual interface to analyze the 9,216 attention heads (96 layers x 96
    heads) would be quite challenging.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们能够访问GPT-3模型的源代码或输出权重，使用可视化界面来分析9,216个注意力头（96层x 96头）也将是相当具有挑战性的。
- en: Finding what is wrong will still require some human involvement in many cases.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在很多情况下，寻找错误仍然需要一些人的参与。
- en: For example, the polysemy issue of the word `coach` in English to French translation
    often represents a problem. In English, a coach can be a person who trains people,
    or a bus. The word `coach` exists in French, but it only applies to a person who
    trains people.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在英语到法语翻译中，“coach”这个词的多义性经常会造成问题。在英语中，coach可以是一个训练别人的人，也可以是一辆公共汽车。法语中也有“coach”这个词，但它只适用于一个训练别人的人。
- en: If you go to OpenAI AI GPT-3 playground, [https://openai.com/](https://openai.com/),
    and translate sentences containing the word `coach`, you might obtain mixed results.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您去OpenAI AI GPT-3游乐场，[https://openai.com/](https://openai.com/)，并翻译含有“coach”这个词的句子，您可能会得到混合的结果。
- en: 'Sentence 1 is translated correctly by the OpenAI engine:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 句子1被OpenAI引擎正确翻译了：
- en: '[PRE8]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`coach` is translated as a bus, which is fine. More context would be required.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: “coach”被翻译成了巴士，这是正确的。但需要更多的语境。
- en: The outputs are stochastic, so the translation might be correct one time and
    false the time after.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是随机的，所以翻译可能一次正确，下一次错误。
- en: 'However, Sentence 2 is mistranslated:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，第二句被误译了：
- en: '[PRE9]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This time, the GPT-3 engine missed the fact that `coach` meant a person, not
    a bus. The same stochastic runs will provide unstable outputs.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，GPT-3引擎错过了`coach`指的是一个人，而不是一辆公共汽车的事实。相同的随机运行将提供不稳定的输出。
- en: 'If we modify sentence 2 by adding context, we will obtain the proper translation:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过添加上下文来修改句子2，我们将获得正确的翻译：
- en: '[PRE10]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The translation now contains the French word `coach` for the same definition
    of the English word `coach` in this sentence. More context was added.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在翻译中包含了法语单词`coach`，表示与本句英语单词`coach`相同的含义。添加了更多上下文。
- en: OpenAI’s solutions, AI in general, and transformer models in particular, are
    continuously progressing. Furthermore, most Industry 4.0 AI-driven micro-decisions
    do not require the level of sophistical of NLP or translation tasks and are effective.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的解决方案，AI总体上，尤其是变压器模型，都在不断进步。此外，大多数工业4.0的AI驱动微决策不需要如此复杂的NLP或翻译任务，并且非常有效。
- en: However, human intervention and development at the Cloud AI API level will still
    remain necessary for quite a long time!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在Cloud AI API级别的人为干预和发展仍然需要相当长的时间！
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Transformer models are trained to resolve word-level polysemy disambiguation
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型被训练来解决词级别的一词多义消歧
- en: low-level, mid-level, and high-level dependencies. The process is achieved by
    connecting training million- to trillion-parameter models. The task of interpreting
    these giant models seems daunting. However, several tools are emerging.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 低层、中层和高层的依赖关系。通过连接训练百万到万亿参数模型来实现这一过程。解释这些巨型模型的任务似乎令人生畏。然而，一些工具正在涌现。
- en: We first installed `BertViz`. We learned how to interpret the computations of
    the attention heads with an interactive interface. We saw how words interacted
    with other words for each layer.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装了`BertViz`。我们学会了如何使用交互式界面解释注意头的计算。我们看到了每一层的单词是如何与其他单词互动的。
- en: The chapter continued by defining the scope of probing and non-probing tasks.
    Probing tasks such as NER provide insights into how a transformer model represents
    language. However, non-probing methods analyze how the model makes predictions.
    For example, LIT plugged a PCA project and UMAP representations into the outputs
    of a BERT transformer model. We could then analyze clusters of outputs to see
    how they fit together.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本章继续定义了探查任务和非探查任务的范围。诸如NER之类的探查任务提供了有关变压器模型如何表示语言的见解。然而，非探查方法分析模型如何做出预测。例如，LIT将PCA项目和UMAP表示插入BERT变压器模型的输出中。然后，我们可以分析输出的集群以查看它们如何组合在一起。
- en: Finally, we ran transformer visualization via dictionary learning. A user can
    choose a transformer factor to analyze and visualize the evolution of its representation
    from the lower layers to the higher layers of the transformer. The factor will
    progressively go from polysemy disambiguation to sentence context analysis and
    finally to long-term dependencies.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过词典学习运行了变压器可视化。用户可以选择一个变压器因子来分析并可视化其在变压器的低层到高层的表示演变。该因子将逐渐从一词多义的消歧到句子上下文分析，最终到长期依赖。
- en: The tools of this chapter will evolve along with other techniques. However,
    the key takeaway of this chapter is that transformer model activity can be visualized
    and interpreted in a user-friendly manner. In the next chapter, we will discover
    new transformer models. We will also go through risk management methods to choose
    the best implementations for a transformer model project.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的工具将随着其他技术的发展而不断发展。然而，本章的关键重点是变压器模型的活动可以以用户友好的方式进行可视化和解释。在下一章中，我们将发现新的变压器模型。我们还将通过风险管理方法选择变压器模型项目的最佳实现。
- en: Questions
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: BertViz only shows the output of the last layer of the BERT model. (True/False)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BertViz只显示了BERT模型最后一层的输出。（真/假）
- en: BertViz shows the attention heads of each layer of a BERT model. (True/False)
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BertViz显示了BERT模型每一层的注意头。（真/假）
- en: BertViz shows how the tokens relate to each other. (True/False)
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BertViz展示了标记之间的关系。（真/假）
- en: LIT shows the inner workings of the attention heads like BertViz. (True/False)
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LIT显示了类似BertViz的注意头的内部工作。（真/假）
- en: Probing is a way for an algorithm to predict language representations. (True/False)
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探查是算法预测语言表示的一种方式。（真/假）
- en: NER is a probing task. (True/False)
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NER是一个探查任务。（真/假）
- en: PCA and UMAP are non-probing tasks. (True/False)
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA和UMAP都不是探查任务。（真/假）
- en: LIME is model agnostic. (True/False)
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LIME是与模型无关的。（真/假）
- en: Transformers deepen the relationships of the tokens layer by layer. (True/False)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformers通过逐层深化令牌之间的关系。（True/False）
- en: Visual transformer model interpretation adds a new dimension to interpretable
    AI. (True/False)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视觉Transformer模型解释为可解释的人工智能增添了一个新的维度。（True/False）
- en: References
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'BertViz: *Jesse Vig,2019, A Multiscale Visualization of Attention in the Transformer
    Model,2019,* [https://aclanthology.org/P19-3007.pdf](https://aclanthology.org/P19-3007.pdf)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BertViz：*Jesse Vig，2019，Transformer模型中的注意力的多尺度可视化，2019*，[https://aclanthology.org/P19-3007.pdf](https://aclanthology.org/P19-3007.pdf)
- en: 'BertViz: [https://github.com/jessevig/BertViz](https://github.com/jessevig/BertViz)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BertViz：[https://github.com/jessevig/BertViz](https://github.com/jessevig/BertViz)
- en: 'LIT, explanation of sentiment analysis representations: [https://pair-code.github.io/lit/tutorials/sentiment/](https://pair-code.github.io/lit/tutorials/sentiment/)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIT，情感分析表示的解释：[https://pair-code.github.io/lit/tutorials/sentiment/](https://pair-code.github.io/lit/tutorials/sentiment/)
- en: 'LIT: [https://pair-code.github.io/lit/](https://pair-code.github.io/lit/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIT：[https://pair-code.github.io/lit/](https://pair-code.github.io/lit/)
- en: '*Transformer visualization via dictionary learning*: *Zeyu Yun*, *Yubei Chen*,
    *Bruno A Olshausen*, *Yann LeCun*, 2021, Transformer visualization via dictionary
    learning: contextualized embedding as a linear superposition of transformer factors,
    [https://arxiv.org/abs/2103.15949](https://arxiv.org/abs/2103.15949)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过字典学习进行Transformer可视化*：*Zeyu Yun*，*Yubei Chen*，*Bruno A Olshausen*，*Yann
    LeCun*，2021，通过字典学习进行Transformer可视化：上下文嵌入作为Transformer因子的线性叠加，[https://arxiv.org/abs/2103.15949](https://arxiv.org/abs/2103.15949)'
- en: 'Transformer visualization via dictionary learning: [https://transformervis.github.io/transformervis/](https://transformervis.github.io/transformervis/)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过字典学习的Transformer可视化：[https://transformervis.github.io/transformervis/](https://transformervis.github.io/transformervis/)
- en: Join our book’s Discord space
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作空间，与作者进行每月的*问我任何事*会话：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
