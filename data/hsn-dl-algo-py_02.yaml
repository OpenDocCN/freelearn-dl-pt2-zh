- en: Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Deep learning is a subset of machine learning inspired by the neural networks
    in the human brain. It has been around for a decade, but the reason it is so popular
    right now is due to the computational advancements and availability of the huge
    volume of data. With a huge volume of data, deep learning algorithms outperform
    classic machine learning. It has already been transfiguring and extensively used
    in several interdisciplinary scientific fields such as computer vision, **natural
    language processing** (**NLP**), speech recognition, and many others.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，灵感来自人类大脑中的神经网络。它已经存在了十年，但现在之所以如此流行，是由于计算能力的提升和大量数据的可用性。有了大量的数据，深度学习算法表现优于经典的机器学习。它已经在计算机视觉、**自然语言处理**（**NLP**）、语音识别等多个跨学科科学领域得到广泛应用和变革。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Fundamental concepts of deep learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的基本概念
- en: Biological and artificial neurons
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生物和人工神经元
- en: Artificial neural network and its layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络及其层次
- en: Activation functions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Forward and backward propagation in ANN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络中的前向和后向传播
- en: Gradient checking algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度检查算法
- en: Building an artificial neural network from scratch
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始构建人工神经网络
- en: What is deep learning?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: Deep learning is just a modern name for artificial neural networks with many
    layers. What is *deep* in deep learning though? It is basically due to the structure
    of the **artificial neural network** (**ANN**). ANN consists of some *n* number
    of layers to perform any computation. We can build an ANN with several layers
    where each layer is responsible for learning the intricate patterns in the data.
    Due to the computational advancements, we can build a network even with 100s or
    1000s of layers deep. Since the ANN uses deep layers to perform learning we call
    it as deep learning and when ANN uses deep layers to learn we call it as a deep
    network. We have learned that deep learning is a subset of machine learning. How
    does deep learning differ from machine learning? What makes deep learning so special
    and popular?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习其实只是具有多层的人工神经网络的现代称谓。那么深度学习中的“深”指的是什么？基本上是由于**人工神经网络**（**ANN**）的结构。ANN由若干层组成，用于执行任何计算。我们可以建立一个具有数百甚至数千层深度的网络。由于计算能力的提升，我们可以构建一个具有深层次的网络。由于ANN使用深层进行学习，我们称之为深度学习；当ANN使用深层进行学习时，我们称之为深度网络。我们已经了解到，深度学习是机器学习的一个子集。深度学习与机器学习有何不同？什么使深度学习如此特别和流行？
- en: The success of machine learning lies in the right set of features. Feature engineering
    plays a crucial role in machine learning. If we handcraft the right set of features
    to predict a certain outcome, then the machine learning algorithms can perform
    well, but finding and engineering the right set of features is not an easy task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的成功取决于正确的特征集。特征工程在机器学习中起着至关重要的作用。如果我们手工制作了一组合适的特征来预测某一结果，那么机器学习算法可以表现良好，但是找到和设计合适的特征集并非易事。
- en: With deep learning, we don't have to handcraft such features. Since deep ANNs
    employ several layers, it learns the complex intrinsic features and multi-level
    abstract representation of data by itself. Let's explore this a bit with an analogy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们无需手工制作这些特征。由于深层次的人工神经网络采用了多层，它可以自动学习复杂的内在特征和多层次的抽象数据表示。让我们通过一个类比来探讨这个问题。
- en: Let's suppose we want to perform an image classification task. Say, we are learning
    to recognize whether an image contains a dog or not. With machine learning, we
    need to handcraft features that help the model to understand whether the image
    contains a dog. We send these handcrafted features as inputs to machine learning
    algorithms which then learn a mapping between the features and the label (dog).
    But extracting features from an image is a tedious task. With deep learning, we
    just need to feed in a bunch of images to the deep neural networks, and it will
    automatically act as a feature extractor by learning the right set of features.
    As we have learned, ANN uses multiple layers; in the first layer, it will learn
    the basic features of the image that characterize the dog, say, the body structure
    of the dog, and, in the succeeding layers, it will learn the complex features.
    Once it learns the right set of features, it will look for the presence of such
    features in the image. If those features are present then it says that the given
    image contains a dog. Thus, unlike machine learning, with DL, we don't have to
    manually engineer the features, instead, the network will itself learns the correct
    set of features required for the task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想执行一个图像分类任务。比如说，我们要学习识别一张图像是否包含狗。使用机器学习时，我们需要手工设计能帮助模型理解图像是否包含狗的特征。我们将这些手工设计的特征作为输入发送给机器学习算法，然后它们学习特征与标签（狗）之间的映射关系。但是从图像中提取特征是一项繁琐的任务。使用深度学习，我们只需将一堆图像输入到深度神经网络中，它将自动充当特征提取器，通过学习正确的特征集。正如我们所学到的，人工神经网络使用多个层次；在第一层中，它将学习表征狗的基本特征，例如狗的体态，而在后续层次中，它将学习复杂的特征。一旦学习到正确的特征集，它将查找图像中是否存在这些特征。如果这些特征存在，则说明给定的图像包含狗。因此，与机器学习不同的是，使用深度学习时，我们不必手动设计特征，而是网络自己学习任务所需的正确特征集。
- en: Due to this interesting aspect of deep learning, it is substantially used in
    unstructured datasets where extracting features are difficult, such as speech
    recognition, text classification, and many more. When we have a fair amount of
    huge datasets, deep learning algorithms are good at extracting features and mapping
    the extracted features to their labels. Having said that, deep learning is not
    just throwing a bunch of data points to a deep network and getting results. It's
    not that simple either. We would have numerous hyperparameters that act as a tuning
    knob to obtain better results which we will explore in the upcoming sections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习的这一有趣特性，在提取特征困难的非结构化数据集中，例如语音识别、文本分类等领域中，它被广泛应用。当我们拥有大量的大型数据集时，深度学习算法擅长提取特征并将这些特征映射到它们的标签上。话虽如此，深度学习并不仅仅是将一堆数据点输入到深度网络中并获得结果。这也并非那么简单。我们将会探索的下一节内容中会讨论到，我们会有许多超参数作为调节旋钮，以获得更好的结果。
- en: Although deep learning performs better than conventional machine learning models,
    it is not recommended to use DL for smaller datasets. When we don't have enough
    data points or the data is very simple, then the deep learning algorithms can
    easily overfit to the training dataset and fail to generalize well on the unseen
    dataset. Thus, we should apply deep learning only when we have a significant amount
    of data points.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习比传统的机器学习模型表现更好，但不建议在较小的数据集上使用DL。当数据点不足或数据非常简单时，深度学习算法很容易对训练数据集过拟合，并且在未见过的数据集上泛化能力不佳。因此，我们应仅在有大量数据点时应用深度学习。
- en: The applications of deep learning are numerous and almost everywhere. Some of
    the interesting applications include automatically generating captions to the
    image, adding sound to the silent movies, converting black-and-white images to
    colored images, generating text, and many more. Google's language translate, Netflix,
    Amazon, and Spotify's recommendations engines, and self-driving cars are some
    of the applications powered by deep learning. There is no doubt that deep learning
    is a disruptive technology and has achieved tremendous technological advancement
    in the past few years.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的应用是多种多样且几乎无处不在的。一些有趣的应用包括自动生成图像标题，为无声电影添加声音，将黑白图像转换为彩色图像，生成文本等等。谷歌的语言翻译、Netflix、亚马逊和Spotify的推荐引擎以及自动驾驶汽车都是由深度学习驱动的应用程序。毫无疑问，深度学习是一项颠覆性技术，在过去几年取得了巨大的技术进步。
- en: In this book, we will learn from the basic deep learning algorithms as to the
    state of the algorithms by building some of the interesting applications of deep
    learning from scratch, which includes image recognition, generating song lyrics,
    predicting bitcoin prices, generating realistic artificial images, converting
    photographs to paintings, and many more. Excited already? Let's get started!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将通过从头开始构建一些有趣的深度学习应用来学习基本的深度学习算法，这些应用包括图像识别、生成歌词、预测比特币价格、生成逼真的人工图像、将照片转换为绘画等等。已经兴奋了吗？让我们开始吧！
- en: Biological and artificial neurons
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物和人工神经元
- en: Before going ahead, first, we will explore what are neurons and how neurons
    in our brain actually work, and then we will learn about artificial neurons.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，首先我们将探讨什么是神经元以及我们的大脑中的神经元实际工作原理，然后我们将了解人工神经元。
- en: A **neuron** can be defined as the basic computational unit of the human brain.
    Neurons are the fundamental units of our brain and nervous system. Our brain encompasses
    approximately 100 billion neurons. Each and every neuron is connected to one another
    through a structure called a **synapse**, which is accountable for receiving input
    from the external environment, sensory organs for sending motor instructions to
    our muscles, and for performing other activities.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经元**可以被定义为人类大脑的基本计算单位。神经元是我们大脑和神经系统的基本单元。我们的大脑大约有1000亿个神经元。每一个神经元通过一个称为**突触**的结构相互连接，突触负责从外部环境接收输入，从感觉器官接收运动指令到我们的肌肉，以及执行其他活动。'
- en: A neuron can also receive inputs from the other neurons through a branchlike
    structure called a **dendrite**. These inputs are strengthened or weakened; that
    is, they are weighted according to their importance and then they are summed together
    in the cell body called the **soma**. From the cell body, these summed inputs
    are processed and move through the **axons** and are sent to the other neurons.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元还可以通过称为**树突**的分支结构从其他神经元接收输入。这些输入根据它们的重要性加强或减弱，然后在称为**细胞体**的细胞体内汇总在一起。从细胞体出发，这些汇总的输入被处理并通过**轴突**发送到其他神经元。
- en: 'The basic single biological neuron is shown in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单个生物神经元的基本结构如下图所示：
- en: '![](img/c34c9841-5d50-4d12-ba6b-8a3437187c69.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c34c9841-5d50-4d12-ba6b-8a3437187c69.png)'
- en: 'Now, let''s see how artificial neurons work. Let''s suppose we have three inputs [![](img/dbdfdc31-7b1f-4278-8390-b1e98e50047b.png)],
    [![](img/99fdffc8-a9fe-4134-af1d-48fdef7ee8d7.png)], and [![](img/3799a7fd-21c3-4d2d-bbdb-252177cfa98a.png)],
    to predict output [![](img/c4d040ed-3f79-4281-8103-a5408f345d6e.png)]. These inputs
    are multiplied by weights [![](img/50e7081f-b6b0-4249-949c-349b0f28ff13.png)],
    [![](img/92e8cf78-f1fc-48f7-8a84-cd4ea925be6c.png)], and [![](img/12920197-2d25-4066-b8b0-e9bed69b3db6.png)]
    and are summed together as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看人工神经元是如何工作的。假设我们有三个输入[![](img/dbdfdc31-7b1f-4278-8390-b1e98e50047b.png)],
    [![](img/99fdffc8-a9fe-4134-af1d-48fdef7ee8d7.png)], 和 [![](img/3799a7fd-21c3-4d2d-bbdb-252177cfa98a.png)],
    来预测输出[![](img/c4d040ed-3f79-4281-8103-a5408f345d6e.png)]。这些输入分别乘以权重[![](img/50e7081f-b6b0-4249-949c-349b0f28ff13.png)],
    [![](img/92e8cf78-f1fc-48f7-8a84-cd4ea925be6c.png)], 和 [![](img/12920197-2d25-4066-b8b0-e9bed69b3db6.png)]，并按以下方式求和：
- en: '![](img/3d90597b-20ae-4167-9118-36496338db66.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d90597b-20ae-4167-9118-36496338db66.png)'
- en: 'But why are we multiplying these inputs by weights? Because all of the inputs
    are not equally important in calculating the output ![](img/799d4419-1f3d-4337-97a3-a3f0a9f6a394.png).
    Let''s say that ![](img/ff460794-7422-40a7-b2a9-57947337bd5b.png) is more important
    in calculating the output compared to the other two inputs. Then, we assign a
    higher value to [![](img/d174252e-29c4-44ba-96e6-5d7020ed6bd5.png)] than the other
    two weights. So, upon multiplying weights with inputs, [![](img/906e8174-801b-4eef-b5db-5a3142c3333a.png)]
    will have a higher value than the other two inputs. In simple terms, weights are
    used for strengthening the inputs. After multiplying inputs with the weights,
    we sum them together and we add a value called bias, ![](img/96616fe5-e29e-404a-aced-6dfe47191b40.png):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么我们要用权重乘以这些输入呢？因为在计算输出时，并不是所有的输入都同等重要！![](img/799d4419-1f3d-4337-97a3-a3f0a9f6a394.png)。假设![](img/ff460794-7422-40a7-b2a9-57947337bd5b.png)在计算输出时比其他两个输入更重要。那么，我们给[![](img/d174252e-29c4-44ba-96e6-5d7020ed6bd5.png)]赋予比其他两个权重更高的值。因此，在将权重与输入相乘后，[![](img/906e8174-801b-4eef-b5db-5a3142c3333a.png)]将比其他两个输入具有更高的值。简单来说，权重用于加强输入。在用权重乘以输入之后，我们将它们加在一起，再加上一个叫做偏置的值，![](img/96616fe5-e29e-404a-aced-6dfe47191b40.png)：
- en: '![](img/62b24d74-3072-4188-bcb0-402c2b2c8221.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62b24d74-3072-4188-bcb0-402c2b2c8221.png)'
- en: 'If you look at the preceding equation closely, it may look familiar? Doesn''t
    [![](img/79f6816d-0766-498b-b832-1b334c9c0323.png)] look like the equation of
    linear regression? Isn''t it just the equation of a straight line? We know that
    the equation of a straight line is given as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察前面的方程式，它看起来可能很熟悉？[![](img/79f6816d-0766-498b-b832-1b334c9c0323.png)]
    看起来像线性回归的方程式吗？难道它不就是一条直线的方程式吗？我们知道直线的方程式如下所示：
- en: '![](img/90183881-f6c9-4d97-addd-29b3d0efce9b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90183881-f6c9-4d97-addd-29b3d0efce9b.png)'
- en: Here *m* is the weights (coefficients), *x* is the input, and *b* is the bias
    (intercept).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*m* 是权重（系数），*x* 是输入，*b* 是偏置（截距）。
- en: 'Well, yes. Then, what is the difference between neurons and linear regression?
    In neurons, we introduce non-linearity to the result, ![](img/79f6816d-0766-498b-b832-1b334c9c0323.png),
    by applying a function ![](img/2de94421-2982-487b-ba76-37fcbd4d91d6.png) called
    the **activation** or **transfer function**. Thus, our output becomes:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，是的。那么，神经元和线性回归之间有什么区别呢？在神经元中，我们通过应用称为**激活**或**传递函数**的函数 ![](img/2de94421-2982-487b-ba76-37fcbd4d91d6.png)，引入非线性到结果
    ![](img/79f6816d-0766-498b-b832-1b334c9c0323.png)。因此，我们的输出变为：
- en: '![](img/bcc6610a-e632-404d-b3e0-69abb61ef764.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcc6610a-e632-404d-b3e0-69abb61ef764.png)'
- en: 'A single artificial neuron is shown in the following diagram:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了单个人工神经元：
- en: '![](img/4cc8f0a1-8bf3-4fad-9844-d322c2056852.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc8f0a1-8bf3-4fad-9844-d322c2056852.jpg)'
- en: So, a neuron takes the input, *x*, multiples it by weights, *w,* and adds bias,
    *b,* forms ![](img/31ba413e-23d8-4ad1-8c7e-5e2b3f0f5d45.png), and then we apply
    the activation function on ![](img/3aa2453f-17dc-49ca-ab0a-f844a21daf3d.png) and
    get the output, ![](img/c70627ba-a0d8-4c88-b471-4e09c1c94af5.png).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个神经元接收输入 *x*，将其乘以权重 *w*，加上偏置 *b*，形成 ![](img/31ba413e-23d8-4ad1-8c7e-5e2b3f0f5d45.png)，然后我们在
    ![](img/3aa2453f-17dc-49ca-ab0a-f844a21daf3d.png) 上应用激活函数，并得到输出 ![](img/c70627ba-a0d8-4c88-b471-4e09c1c94af5.png)。
- en: ANN and its layers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN及其层
- en: While neurons are really cool, we cannot just use a single neuron to perform
    complex tasks. This is the reason our brain has billions of neurons, stacked in
    layers, forming a network. Similarly, artificial neurons are arranged in layers.
    Each and every layer will be connected in such a way that information is passed
    from one layer to another.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经元非常强大，但我们不能仅使用一个神经元执行复杂任务。这就是为什么我们的大脑有数十亿个神经元，排列成层，形成一个网络的原因。同样，人工神经元也被排列成层。每一层都将以信息从一层传递到另一层的方式连接起来。
- en: 'A typical ANN consists of the following layers:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的人工神经网络由以下层组成：
- en: Input layer
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layer
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Output layer
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: Each layer has a collection of neurons, and the neurons in one layer interact
    with all the neurons in the other layers. However, neurons in the same layer will
    not interact with one another. This is simply because neurons from the adjacent
    layers have connections or edges between them; however, neurons in the same layer
    do not have any connections. We use the term **nodes** or **units** to represent
    the neurons in the artificial neural network.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都有一组神经元，而且一层中的神经元会与其他层中的所有神经元进行交互。然而，同一层中的神经元不会相互作用。这是因为相邻层的神经元之间有连接或边缘；然而，同一层中的神经元之间没有任何连接。我们使用术语**节点**或**单元**来表示人工神经网络中的神经元。
- en: 'A typical ANN is shown in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了典型的人工神经网络（ANN）：
- en: '![](img/ada614a6-bc63-4e6c-8faa-d4289e2bd97f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ada614a6-bc63-4e6c-8faa-d4289e2bd97f.png)'
- en: Input layer
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入层
- en: The **input layer** is where we feed input to the network. The number of neurons
    in the input layer is the number of inputs we feed to the network. Each input
    will have some influence on predicting the output. However, no computation is
    performed in the input layer; it is just used for passing information from the
    outside world to the network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**是我们向网络提供输入的地方。输入层中的神经元数量等于我们向网络提供的输入数量。每个输入都会对预测输出产生一定影响。然而，输入层不进行任何计算；它只是用于将信息从外部世界传递到网络中。'
- en: Hidden layer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Any layer between the input layer and the output layer is called a **hidden
    layer**. It processes the input received from the input layer. The hidden layer
    is responsible for deriving complex relationships between input and output. That
    is, the hidden layer identifies the pattern in the dataset. It is majorly responsible
    for learning the data representation and for extracting the features.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层之间的任何层称为**隐藏层**。它处理从输入层接收到的输入。隐藏层负责推导输入和输出之间的复杂关系。也就是说，隐藏层识别数据集中的模式。它主要负责学习数据表示并提取特征。
- en: There can be any number of hidden layers; however, we have to choose a number
    of hidden layers according to our use case. For a very simple problem, we can
    just use one hidden layer, but while performing complex tasks such as image recognition,
    we use many hidden layers, where each layer is responsible for extracting important
    features. The network is called a **deep neural network** when we have many hidden
    layers.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层可以有任意数量；然而，我们根据使用情况选择隐藏层的数量。对于非常简单的问题，我们可以只使用一个隐藏层，但在执行像图像识别这样的复杂任务时，我们使用许多隐藏层，每个隐藏层负责提取重要特征。当我们有许多隐藏层时，网络被称为**深度神经网络**。
- en: Output layer
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出层
- en: After processing the input, the hidden layer sends its result to the output
    layer. As the name suggests, the output layer emits the output. The number of
    neurons in the output layer is based on the type of problem we want our network
    to solve.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理输入后，隐藏层将其结果发送到输出层。顾名思义，输出层发出输出。输出层的神经元数量基于我们希望网络解决的问题类型。
- en: If it is a binary classification, then the number of neurons in the output layer
    is one that tells us which class the input belongs to. If it is a multi-class
    classification say, with five classes, and if we want to get the probability of
    each class as an output, then the number of neurons in the output layer is five,
    each emitting the probability. If it is a regression problem, then we have one
    neuron in the output layer.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是二元分类，则输出层的神经元数量为1，告诉我们输入属于哪个类别。如果是多类分类，比如五类，并且我们想要得到每个类别的概率作为输出，则输出层的神经元数量为五个，每个神经元输出概率。如果是回归问题，则输出层有一个神经元。
- en: Exploring activation functions
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索激活函数
- en: An **activation function**, also known as a **transfer function**, plays a vital
    role in neural networks. It is used to introduce non-linearity in neural networks.
    As we learned before, we apply the activation function to the input, which is
    multiplied by weights and added to the bias, that is, ![](img/9cf85797-e709-49e7-a4c8-d7a84c9b78a2.png),
    where *z = (input * weights) + bias* and ![](img/d14376f3-d671-4abb-8956-7db1284efec4.png)
    is the activation function. If we do not apply the activation function, then a
    neuron simply resembles the linear regression. The aim of the activation function
    is to introduce a non-linear transformation to learn the complex underlying patterns
    in the data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**，也称为**传递函数**，在神经网络中起着至关重要的作用。它用于在神经网络中引入非线性。正如我们之前学到的，我们将激活函数应用于输入，该输入与权重相乘并加上偏置，即，![](img/9cf85797-e709-49e7-a4c8-d7a84c9b78a2.png)，其中*z
    = (输入 * 权重) + 偏置*，![](img/d14376f3-d671-4abb-8956-7db1284efec4.png)是激活函数。如果不应用激活函数，则神经元简单地类似于线性回归。激活函数的目的是引入非线性变换，以学习数据中的复杂潜在模式。'
- en: Now let's look at some of the interesting commonly used activation functions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一些有趣的常用激活函数。
- en: The sigmoid function
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'The **sigmoid function** is one of the most commonly used activation functions.
    It scales the value between 0 and 1\. The sigmoid function can be defined as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid函数**是最常用的激活函数之一。它将值缩放到0到1之间。Sigmoid函数可以定义如下：'
- en: '![](img/85674218-0ed8-4c46-a215-d0ad662c3dac.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85674218-0ed8-4c46-a215-d0ad662c3dac.png)'
- en: 'It is an S-shaped curve shown as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如下所示的S形曲线：
- en: '![](img/7f9d30e5-1892-4249-8ae0-c82e1254e33a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f9d30e5-1892-4249-8ae0-c82e1254e33a.png)'
- en: It is differentiable, meaning that we can find the slope of the curve at any
    two points. It is **monotonic**, which implies it is either entirely non-increasing
    or non-decreasing. The sigmoid function is also known as a **logistic** function.
    As we know that probability lies between 0 and 1 and since the sigmoid function
    squashes the value between 0 and 1, it is used for predicting the probability
    of output.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它是可微的，意味着我们可以找到任意两点处曲线的斜率。它是**单调**的，这意味着它要么完全非递减，要么非递增。Sigmoid 函数也称为**logistic**函数。由于我们知道概率介于
    0 和 1 之间，由于 Sigmoid 函数将值压缩在 0 和 1 之间，因此用于预测输出的概率。
- en: 'The sigmoid function can be defined in Python as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中可以定义 sigmoid 函数如下：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The tanh function
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: 'A **hyperbolic tangent (tanh)** function outputs the value between -1 to +1
    and is expressed as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**双曲正切（tanh）**函数将值输出在 -1 到 +1 之间，表达如下：'
- en: '![](img/2fd28ae8-16bc-4064-9f34-1f7ac917673f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fd28ae8-16bc-4064-9f34-1f7ac917673f.png)'
- en: 'It also resembles the S-shaped curve. Unlike a sigmoid function which is centered
    on 0.5, the tanh function is 0 centered, as shown in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 它也类似于 S 形曲线。与 Sigmoid 函数以 0.5 为中心不同，tanh 函数是以 0 为中心的，如下图所示：
- en: '![](img/c6f8b6b5-e378-469d-97f4-8598b7859e13.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6f8b6b5-e378-469d-97f4-8598b7859e13.png)'
- en: 'Similar to the sigmoid function, it is also a differentiable and monotonic
    function. The tanh function is implemented as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与 sigmoid 函数类似，它也是一个可微且单调的函数。tanh 函数的实现如下：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Rectified Linear Unit function
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矫正线性单元函数
- en: 'The **Rectified Linear Unit** (**ReLU**) function is another one of the most
    commonly used activation functions. It outputs a value from o to infinity. It
    is basically a **piecewise** function and can be expressed as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**矫正线性单元（ReLU）**函数是最常用的激活函数之一。它输出从零到无穷大的值。它基本上是一个**分段函数**，可以表达如下：'
- en: '![](img/4b1830c9-1f13-4bed-ba70-be53b28c4a5e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b1830c9-1f13-4bed-ba70-be53b28c4a5e.png)'
- en: 'That is, ![](img/9b9b4d0d-62e5-4f60-9a70-b9b830397fd2.png) returns zero when
    the value of *x* is less than zero and ![](img/d8c4dc45-d54f-4539-89f5-bcd67c4e4e6b.png)
    returns *x* when the value of *x* is greater than or equal to zero. It can also
    be expressed as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 即当 *x* 的值小于零时，![](img/9b9b4d0d-62e5-4f60-9a70-b9b830397fd2.png) 返回零，当 *x* 的值大于或等于零时，![](img/d8c4dc45-d54f-4539-89f5-bcd67c4e4e6b.png)
    返回 *x*。也可以表达如下：
- en: '![](img/54c73360-a603-4ca4-b6de-9848f839e2ba.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54c73360-a603-4ca4-b6de-9848f839e2ba.png)'
- en: 'The ReLU function is shown in the following figure:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数如下图所示：
- en: '![](img/9c3ebd42-c9ff-4ba7-9d14-df25c70a9125.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c3ebd42-c9ff-4ba7-9d14-df25c70a9125.png)'
- en: 'As we can see in the preceding diagram, when we feed any negative input to
    the ReLU function, it converts it to zero. The snag for being zero for all negative
    values is a problem called **dying ReLU**, and a neuron is said to be dead if
    it always outputs zero. A ReLU function can be implemented as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，当我们将任何负输入传入 ReLU 函数时，它将其转换为零。所有负值为零的这种情况被称为**dying ReLU**问题，如果神经元总是输出零，则称其为死神经元。ReLU
    函数的实现如下：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The leaky ReLU function
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 函数
- en: '**Leaky ReLU** is a variant of the ReLU function that solves the dying ReLU
    problem. Instead of converting every negative input to zero, it has a small slope
    for a negative value as shown:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**泄漏 ReLU**是 ReLU 函数的一个变种，可以解决 dying ReLU 问题。它不是将每个负输入转换为零，而是对负值具有小的斜率，如下所示：'
- en: '![](img/b7cbffba-0634-45c9-9227-8ce07581d3d6.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7cbffba-0634-45c9-9227-8ce07581d3d6.png)'
- en: 'Leaky ReLU can be expressed as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 可以表达如下：
- en: '![](img/7698bd93-47b2-412a-a6f8-09f95f80e988.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7698bd93-47b2-412a-a6f8-09f95f80e988.png)'
- en: 'The value of ![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png) is typically
    set to 0.01\. The leaky ReLU function is implemented as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png) 的值通常设置为 0.01。泄漏 ReLU 函数的实现如下：'
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Instead of setting some default values to ![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png),
    we can send them as a parameter to a neural network and make the network learn
    the optimal value of ![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png). Such an
    activation function can be termed as a **Parametric ReLU** function. We can also
    set the value of ![](img/21bbeeeb-1cb8-497e-851b-064b57f8e1cc.png) to some random
    value and it is called as **Rando****mized ReLU** function.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将一些默认值设置为 ![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png)，我们可以将它们作为神经网络的参数发送，并使网络学习
    ![](img/a29d974a-56fe-4c04-9929-20eba2e714b4.png) 的最优值。这样的激活函数可以称为**Parametric
    ReLU**函数。我们也可以将 ![](img/21bbeeeb-1cb8-497e-851b-064b57f8e1cc.png) 的值设置为某个随机值，这被称为**Randomized
    ReLU**函数。
- en: The Exponential linear unit function
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指数线性单元函数
- en: '**Exponential linear unit** (**ELU**), like Leaky ReLU, has a small slope for
    negative values. But instead of having a straight line, it has a log curve, as
    shown in the following diagram:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**指数线性单元** (**ELU**)，类似于 Leaky ReLU，在负值时具有小的斜率。但是它不是直线，而是呈现对数曲线，如下图所示：'
- en: '![](img/5b5db978-888c-4ed6-9281-03376d1f6737.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b5db978-888c-4ed6-9281-03376d1f6737.png)'
- en: 'It can be expressed as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表达如下：
- en: '![](img/511a1bb4-db49-4bc2-a77e-a5567616c7e9.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511a1bb4-db49-4bc2-a77e-a5567616c7e9.png)'
- en: 'The `ELU` function is implemented in Python as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`ELU` 函数在 Python 中的实现如下：'
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The Swish function
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Swish 函数
- en: 'The **Swish** function is a recently introduced activation function by Google.
    Unlike other activation functions, which are monotonic, Swish is a non-monotonic
    function, which means it is neither always non-increasing nor non-decreasing.
    It provides better performance than ReLU. It is simple and can be expressed as
    follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**Swish** 函数是 Google 最近引入的激活函数。与其他激活函数不同，Swish 是非单调函数，即它既不总是非递减也不总是非增加。它比 ReLU
    提供更好的性能。它简单且可以表达如下：'
- en: '![](img/f188d07c-c687-470f-b22a-b652e77a8d0a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f188d07c-c687-470f-b22a-b652e77a8d0a.png)'
- en: 'Here, ![](img/b347a90c-363c-4b47-b023-147ee0110f4e.png) is the sigmoid function.
    The Swish function is shown in the following diagram:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/b347a90c-363c-4b47-b023-147ee0110f4e.png) 是 sigmoid 函数。Swish 函数如下图所示：
- en: '![](img/2352091f-84a6-4451-96f1-7a0a18ddce2f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2352091f-84a6-4451-96f1-7a0a18ddce2f.png)'
- en: 'We can also reparametrize the Swish function and express it as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以重新参数化 Swish 函数，并将其表达如下：
- en: '![](img/661a40be-cffb-4345-9e47-ea0801a30812.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/661a40be-cffb-4345-9e47-ea0801a30812.png)'
- en: When the value of ![](img/165eb486-3642-4860-96ae-5d53b69eea23.png) is 0, then
    we get the identity function ![](img/22835e77-9093-4d13-94c3-1abd677b0b28.png).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/165eb486-3642-4860-96ae-5d53b69eea23.png) 的值为 0 时，我们得到恒等函数 ![](img/22835e77-9093-4d13-94c3-1abd677b0b28.png)。
- en: 'It becomes a linear function and, when the value of ![](img/165eb486-3642-4860-96ae-5d53b69eea23.png) tends
    to infinity, then ![](img/f199e52d-ee6f-4cdf-a12f-384e23beba5b.png) becomes ![](img/65683175-77c3-4cd5-b79d-e8770b79f073.png),
    which is basically the ReLU function multiplied by some constant value. So, the
    value of ![](img/165eb486-3642-4860-96ae-5d53b69eea23.png) acts as a good interpolation
    between a linear and a nonlinear function. The swish function can be implemented
    as shown:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它变成了一个线性函数，当 ![](img/165eb486-3642-4860-96ae-5d53b69eea23.png) 的值趋于无穷大时，![](img/f199e52d-ee6f-4cdf-a12f-384e23beba5b.png)
    变成了 ![](img/65683175-77c3-4cd5-b79d-e8770b79f073.png)，这基本上是 ReLU 函数乘以某个常数值。因此，![](img/165eb486-3642-4860-96ae-5d53b69eea23.png)
    的值在线性和非线性函数之间起到了很好的插值作用。Swish 函数的实现如下所示：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The softmax function
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax 函数
- en: The **softmax function** is basically the generalization of the sigmoid function.
    It is usually applied to the final layer of the network and while performing multi-class
    classification tasks. It gives the probabilities of each class for being output
    and thus, the sum of softmax values will always equal 1.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**Softmax 函数** 基本上是 sigmoid 函数的一般化。它通常应用于网络的最后一层，并在执行多类别分类任务时使用。它给出每个类别的概率作为输出，因此
    softmax 值的总和始终等于 1。'
- en: 'It can be represented as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表示如下：
- en: '![](img/e3f8ae71-6850-4e09-985a-310b056c3c02.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3f8ae71-6850-4e09-985a-310b056c3c02.png)'
- en: 'As shown in the following diagram, the softmax function converts their inputs
    to probabilities:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，softmax 函数将它们的输入转换为概率：
- en: '![](img/f7adea0b-6ffc-4df5-ae4e-422765495182.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7adea0b-6ffc-4df5-ae4e-422765495182.png)'
- en: 'The `softmax` function can be implemented in Python as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 Python 中如下实现 `softmax` 函数：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Forward propagation in ANN
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络中的前向传播
- en: 'In this section, we will see how an ANN learns where neurons are stacked up
    in layers. The number of layers in a network is equal to the number of hidden
    layers plus the number of output layers. We don''t take the input layer into account
    when calculating the number of layers in a network. Consider a two-layer neural
    network with one input layer, ![](img/0ca38542-0bbd-4c1d-aab1-041856efa069.png),
    one hidden layer, ![](img/23c5a8d4-df59-46d0-bad7-4fa3019edf96.png), and one output
    layer, ![](img/414c1121-e805-4f7f-a1b4-be8e0bc102a6.png), as shown in the following
    diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到神经网络学习的过程，其中神经元堆叠在层中。网络中的层数等于隐藏层数加上输出层数。在计算网络层数时，我们不考虑输入层。考虑一个具有一个输入层
    ![](img/0ca38542-0bbd-4c1d-aab1-041856efa069.png)、一个隐藏层 ![](img/23c5a8d4-df59-46d0-bad7-4fa3019edf96.png)
    和一个输出层 ![](img/414c1121-e805-4f7f-a1b4-be8e0bc102a6.png) 的两层神经网络，如下图所示：
- en: '![](img/9324b53f-3c67-42a9-ac50-02d9561c0e8b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9324b53f-3c67-42a9-ac50-02d9561c0e8b.png)'
- en: Let's consider we have two inputs, ![](img/e576bc39-3496-4769-83f1-0da92a9fd56b.png)
    and ![](img/d98cc1ae-745a-4be3-b5c7-7d172de62d5b.png), and we have to predict
    the output, ![](img/00c11656-9e19-47ee-b758-117b6358ae91.png). Since we have two
    inputs, the number of neurons in the input layer will be two. We set the number
    of neurons in the hidden layer to four, and, the number of neurons in the output
    layer to one. Now, the inputs will be multiplied by weights, and then we add bias
    and propagate the resultant value to the hidden layer where the activation function
    will be applied.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个输入， ![](img/e576bc39-3496-4769-83f1-0da92a9fd56b.png) 和 ![](img/d98cc1ae-745a-4be3-b5c7-7d172de62d5b.png)，我们需要预测输出，![](img/00c11656-9e19-47ee-b758-117b6358ae91.png)。由于有两个输入，输入层中的神经元数为两个。我们设置隐藏层中的神经元数为四个，输出层中的神经元数为一个。现在，输入将与权重相乘，然后加上偏置，并将结果传播到隐藏层，在那里将应用激活函数。
- en: Before that, we need to initialize the weight matrix. In the real world, we
    don't know which input is more important than the other so that we can weight
    them and compute the output. Therefore, we will randomly initialize weights and
    the bias value. The weight and the bias value between the input to the hidden
    layer are represented by ![](img/9439f056-b74c-40ae-b855-01f76b5f3de0.png) and
    ![](img/97fdd9d3-3eeb-416c-855c-ba075b92e1f5.png), respectively. What about the
    dimensions of the weight matrix? The dimensions of the weight matrix must be *number
    of neurons in the current layer* x *number of neurons in the next layer*. Why
    is that?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，我们需要初始化权重矩阵。在现实世界中，我们不知道哪些输入比其他输入更重要，因此我们会对它们进行加权并计算输出。因此，我们将随机初始化权重和偏置值。输入层到隐藏层之间的权重和偏置值分别由
    ![](img/9439f056-b74c-40ae-b855-01f76b5f3de0.png) 和 ![](img/97fdd9d3-3eeb-416c-855c-ba075b92e1f5.png)
    表示。那么权重矩阵的维度是多少呢？权重矩阵的维度必须是 *当前层中的神经元数* x *下一层中的神经元数*。为什么呢？
- en: 'Because it is a basic matrix multiplication rule. To multiply any two matrices,
    *AB*, the number of columns in matrix *A* must be equal to the number of rows
    in matrix *B*. So, the dimension of the weight matrix, ![](img/1ae8dad5-6ffd-49e3-9c63-ad8f7a727b82.png),
    should be *number of neurons in the input layer* x *number of neurons in the hidden
    layer*, that is, 2 x 4:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是基本的矩阵乘法规则。要将任意两个矩阵 *AB* 相乘，矩阵 *A* 的列数必须等于矩阵 *B* 的行数。因此，权重矩阵 ![](img/1ae8dad5-6ffd-49e3-9c63-ad8f7a727b82.png)
    的维度应为 *输入层中的神经元数* x *隐藏层中的神经元数*，即 2 x 4：
- en: '![](img/f9845f24-bfaa-4722-b430-631c893abb1c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9845f24-bfaa-4722-b430-631c893abb1c.png)'
- en: 'The preceding equation represents, ![](img/0a21b4e7-70c8-4efb-b08c-e55457170f0b.png).
    Now, this is passed to the hidden layer. In the hidden layer, we apply an activation
    function to ![](img/ccd2fedf-a381-487e-8e37-b3dedd059c0b.png). Let''s use the
    sigmoid ![](img/4d1ee602-8255-4022-b0eb-d52c6dca7c54.png) activation function.
    Then, we can write:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程表示，![](img/0a21b4e7-70c8-4efb-b08c-e55457170f0b.png)。现在，这将传递到隐藏层。在隐藏层中，我们对
    ![](img/ccd2fedf-a381-487e-8e37-b3dedd059c0b.png) 应用激活函数。让我们使用 sigmoid ![](img/4d1ee602-8255-4022-b0eb-d52c6dca7c54.png)
    激活函数。然后，我们可以写成：
- en: '![](img/0864106a-31ca-4524-b408-11e3a81c346e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0864106a-31ca-4524-b408-11e3a81c346e.png)'
- en: 'After applying the activation function, we again multiply result ![](img/5c8475b6-c81e-4e05-abfb-3c0856c26635.png) by
    a new weight matrix and add a new bias value that is flowing between the hidden
    layer and the output layer. We can denote this weight matrix and bias as ![](img/e460863e-35ab-41d3-855a-fdaa58c780c8.png)
    and ![](img/910bb4a3-6b59-441d-a624-a2aec21c2190.png), respectively. The dimension
    of the weight matrix, ![](img/c86587a7-4639-4ad9-9777-0ce185b1dbf4.png), will
    be the *number of neurons in the hidden layer* x *the number of neurons in the
    output layer*. Since we have four neurons in the hidden layer and one neuron in
    the output layer, the ![](img/5268e1a5-2ffe-478b-b375-69e7bee62e29.png) matrix
    dimension will be 4 x 1\. So, we multiply ![](img/5c8475b6-c81e-4e05-abfb-3c0856c26635.png)
    by the weight matrix,![](img/9bbf9460-67f9-4cb8-8302-da3866a28c8c.png), and add
    bias, ![](img/8030beb2-9a44-4823-99e8-438002a73a4d.png), and pass the result ![](img/97c26ba4-4188-4326-870c-47d83b9d3535.png)
    to the next layer, which is the output layer:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 应用激活函数后，我们再次将结果 ![](img/5c8475b6-c81e-4e05-abfb-3c0856c26635.png) 乘以一个新的权重矩阵，并添加一个在隐藏层和输出层之间流动的新偏置值。我们可以将这个权重矩阵和偏置表示为
    ![](img/e460863e-35ab-41d3-855a-fdaa58c780c8.png) 和 ![](img/910bb4a3-6b59-441d-a624-a2aec21c2190.png)，分别。权重矩阵
    ![](img/c86587a7-4639-4ad9-9777-0ce185b1dbf4.png) 的维度将是 *隐藏层神经元的数量* x *输出层神经元的数量*。由于我们隐藏层有四个神经元，输出层有一个神经元，因此
    ![](img/5268e1a5-2ffe-478b-b375-69e7bee62e29.png) 矩阵的维度将是 4 x 1。所以，我们将 ![](img/5c8475b6-c81e-4e05-abfb-3c0856c26635.png)
    乘以权重矩阵，![](img/9bbf9460-67f9-4cb8-8302-da3866a28c8c.png)，并添加偏置，![](img/8030beb2-9a44-4823-99e8-438002a73a4d.png)，然后将结果
    ![](img/97c26ba4-4188-4326-870c-47d83b9d3535.png) 传递到下一层，即输出层：
- en: '![](img/d6c4af2f-65ac-4d56-a396-74add1443848.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6c4af2f-65ac-4d56-a396-74add1443848.png)'
- en: 'Now, in the output layer, we apply a sigmoid function to ![](img/97c26ba4-4188-4326-870c-47d83b9d3535.png),
    which will result an output value:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在输出层，我们对 ![](img/97c26ba4-4188-4326-870c-47d83b9d3535.png) 应用sigmoid函数，这将产生一个输出值：
- en: '![](img/a2b2f8dc-6cac-4959-bfad-c41c887d1c77.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2b2f8dc-6cac-4959-bfad-c41c887d1c77.png)'
- en: 'This whole process from the input layer to the output layer is known as **forward
    propagation**. Thus, in order to predict the output value, inputs are propagated
    from the input layer to the output layer. During this propagation, they are multiplied
    by their respective weights on each layer and an activation function is applied
    on top of them. The complete forward propagation steps are given as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入层到输出层的整个过程被称为 **前向传播**。因此，为了预测输出值，输入被从输入层传播到输出层。在这个传播过程中，它们在每一层上都乘以各自的权重，并在其上应用一个激活函数。完整的前向传播步骤如下所示：
- en: '![](img/4bd12cf0-2cc6-48ba-8f5a-d38dc68ded6b.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bd12cf0-2cc6-48ba-8f5a-d38dc68ded6b.png)'
- en: '![](img/0864106a-31ca-4524-b408-11e3a81c346e.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0864106a-31ca-4524-b408-11e3a81c346e.png)'
- en: '![](img/32e542ad-cb13-401d-92b4-40e1dae62ea7.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32e542ad-cb13-401d-92b4-40e1dae62ea7.png)'
- en: '![](img/a2b2f8dc-6cac-4959-bfad-c41c887d1c77.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2b2f8dc-6cac-4959-bfad-c41c887d1c77.png)'
- en: 'The preceding forward propagation steps can be implemented in Python as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播的步骤可以在Python中实现如下：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Forward propagation is cool, isn''t it? But how do we know whether the output
    generated by the neural network is correct? We define a new function called the
    **co****st function** (![](img/5bb1b6b9-cd5d-44b2-8a3a-896a26d1e80f.png)), also
    known as the **loss function** (![](img/fbc4098e-0480-492b-8742-2e055c6fbb8e.png)),
    which tells us how well our neural network is performing. There are many different
    cost functions. We will use the mean squared error as a cost function, which can
    be defined as the mean of the squared difference between the actual output and
    the predicted output:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播很酷，不是吗？但是我们如何知道神经网络生成的输出是否正确？我们定义一个称为 **代价函数** (![](img/5bb1b6b9-cd5d-44b2-8a3a-896a26d1e80f.png))
    或者 **损失函数** (![](img/fbc4098e-0480-492b-8742-2e055c6fbb8e.png)) 的新函数，这个函数告诉我们神经网络的表现如何。有许多不同的代价函数。我们将使用均方误差作为一个代价函数，它可以定义为实际输出和预测输出之间平方差的均值：
- en: '![](img/c488ce70-38e6-4e74-b409-3334dfdf65ad.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c488ce70-38e6-4e74-b409-3334dfdf65ad.png)'
- en: Here, ![](img/50e54b3e-22b0-4fe5-9a09-4ec202021219.png) is the number of training
    samples, ![](img/cf05ae33-0d55-44f2-b238-b3e2cef84419.png) is actual output, and
    ![](img/2e0c94cb-54ad-4658-b9c7-f13cb993c739.png) is the predicted output.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/50e54b3e-22b0-4fe5-9a09-4ec202021219.png) 是训练样本的数量，![](img/cf05ae33-0d55-44f2-b238-b3e2cef84419.png)
    是实际输出，![](img/2e0c94cb-54ad-4658-b9c7-f13cb993c739.png) 是预测输出。
- en: Okay, so we learned that a cost function is used for assessing our neural network;
    that is, it tells us how good our neural network is at predicting the output.
    But the question is where is our network actually learning? In forward propagation,
    the network is just trying to predict the output. But how does it learn to predict
    the correct output? In the next section, we will examine this.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们学到了成本函数用于评估我们的神经网络；也就是说，它告诉我们我们的神经网络在预测输出方面表现如何。但问题是我们的网络实际上是在哪里学习的？在前向传播中，网络只是尝试预测输出。但是它如何学习预测正确的输出呢？在接下来的部分，我们将探讨这个问题。
- en: How does ANN learn?
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN 如何学习？
- en: If the cost or loss is very high, then it means that our network is not predicting
    the correct output. So, our objective is to minimize the cost function so that
    our neural network predictions will be better. How can we minimize the cost function?
    That is, how can we minimize the loss/cost? We learned that the neural network
    makes predictions using forward propagation. So, if we can change some values
    in the forward propagation, we can predict the correct output and minimize the
    loss. But what values can we change in the forward propagation? Obviously, we
    can't change input and output. We are now left with weights and bias values. Remember
    that we just initialized weight matrices randomly. Since the weights are random,
    they are not going to be perfect. Now, we will update these weight matrices (![](img/3e007dfd-6705-4a5f-8d63-9b8e1663031f.png)
    and ![](img/1c5358ea-4e8c-49b0-9e12-af1c8d04c16e.png) ) in such a way that our
    neural network gives a correct output. How do we update these weight matrices?
    Here comes a new technique called **gradient descent**.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成本或损失非常高，那么意味着我们的网络没有预测出正确的输出。因此，我们的目标是最小化成本函数，这样我们神经网络的预测就会更好。我们如何最小化成本函数呢？也就是说，我们如何最小化损失？我们学到神经网络使用前向传播进行预测。因此，如果我们能在前向传播中改变一些值，我们就可以预测出正确的输出并最小化损失。但是在前向传播中可以改变哪些值呢？显然，我们不能改变输入和输出。我们现在只剩下权重和偏置值。请记住，我们只是随机初始化了权重矩阵。由于权重是随机的，它们不会是完美的。现在，我们将更新这些权重矩阵（![](img/3e007dfd-6705-4a5f-8d63-9b8e1663031f.png)
    和 ![](img/1c5358ea-4e8c-49b0-9e12-af1c8d04c16e.png) ），使得我们的神经网络能够给出正确的输出。我们如何更新这些权重矩阵呢？这就是一个新技术，称为**梯度下降**。
- en: With gradient descent, the neural network learns the optimal values of the randomly
    initialized weight matrices. With the optimal values of weights, our network can
    predict the correct output and minimize the loss.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过梯度下降，神经网络学习随机初始化权重矩阵的最优值。有了最优的权重值，我们的网络可以预测正确的输出并最小化损失。
- en: Now, we will explore how the optimal values of weights are learned using gradient
    descent. Gradient descent is one of the most commonly used optimization algorithms.
    It is used for minimizing the cost function, which allows us to minimize the error
    and obtain the lowest possible error value. But how does gradient descent find
    the optimal weights? Let's begin with an analogy.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨如何使用梯度下降学习最优的权重值。梯度下降是最常用的优化算法之一。它用于最小化成本函数，使我们能够最小化误差并获得可能的最低误差值。但是梯度下降如何找到最优权重呢？让我们用一个类比来开始。
- en: Imagine we are on top of a hill, as shown in the following diagram, and we want
    to reach the lowest point on the hill. There could be many regions that look like
    the lowest points on the hill, but we have to reach the lowest point that is actually
    the lowest of all.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们站在一个山顶上，如下图所示，我们想要到达山上的最低点。可能会有很多区域看起来像山上的最低点，但我们必须到达实际上是所有最低点中最低的点。
- en: 'That is, we should not be stuck at a point believing it is the lowest point
    when the global lowest point exists:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 即，我们不应该固守在一个点上，认为它是最低点，当全局最低点存在时：
- en: '![](img/99ca846c-c054-412d-8798-0485964e6051.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99ca846c-c054-412d-8798-0485964e6051.png)'
- en: 'Similarly, we can represent our cost function as follows. It is a plot of cost
    against weights. Our objective is to minimize the cost function. That is, we have
    to reach the lowest point where the cost is the minimum. The solid dark point
    in the following diagram shows the randomly initialized weights. If we move this
    point downward, then we can reach the point where the cost is the minimum:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以将成本函数表示如下。它是成本对权重的绘图。我们的目标是最小化成本函数。也就是说，我们必须到达成本最低的点。下图中的实心黑点显示了随机初始化的权重。如果我们将这一点向下移动，那么我们可以到达成本最低的点：
- en: '![](img/28e3cddf-581f-4795-ba7b-5621bdb76849.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28e3cddf-581f-4795-ba7b-5621bdb76849.png)'
- en: But how can we move this point (initial weight) downward? How can we descend
    and reach the lowest point? Gradients are used for moving from one point to another.
    So, we can move this point (initial weight) by calculating a gradient of the cost
    function with respect to that point (initial weights), which is ![](img/cdaa7865-fa7e-4521-b9ac-6e2029df1e56.png).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何将这个点（初始权重）向下移动？我们如何下降并达到最低点？梯度用于从一个点移动到另一个点。因此，我们可以通过计算成本函数相对于该点（初始权重）的梯度来移动这个点（初始权重），即
    ![](img/cdaa7865-fa7e-4521-b9ac-6e2029df1e56.png)。
- en: 'Gradients are the derivatives that are actually the slope of a tangent line
    as illustrated in the following diagram. So, by calculating the gradient, we descend
    (move downward) and reach the lowest point where the cost is the minimum. Gradient
    descent is a first-order optimization algorithm, which means we only take into
    account the first derivative when performing the updates:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是实际上是切线斜率的导数，如下图所示。因此，通过计算梯度，我们向下移动并达到成本最低点。梯度下降是一种一阶优化算法，这意味着在执行更新时只考虑一阶导数：
- en: '![](img/cc6ee8c1-f612-4953-8e0d-41520c38fae7.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc6ee8c1-f612-4953-8e0d-41520c38fae7.png)'
- en: Thus, with gradient descent, we move our weights to a position where the cost
    is minimum. But, still, how do we update the weights?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过梯度下降，我们将权重移动到成本最小的位置。但是，我们如何更新权重呢？
- en: 'As a result of forward propagation, we are in the output layer. We will now
    **backpropagate** the network from the output layer to the input layer and calculate
    the gradient of the cost function with respect to all the weights between the
    output and the input layer so that we can minimize the error. After calculating
    gradients, we update our old weights using the weight update rule:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前向传播的结果，我们处于输出层。现在我们将从输出层向输入层进行**反向传播**，并计算成本函数对输出层和输入层之间所有权重的梯度，以便最小化误差。在计算梯度之后，我们使用权重更新规则来更新旧权重：
- en: '![](img/c038f863-5459-4ada-a1a6-6406f93fee39.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c038f863-5459-4ada-a1a6-6406f93fee39.png)'
- en: This implies *weights = weights -α * gradients*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 *weights = weights -α * gradients*。
- en: What is ![](img/8179ea37-437e-40e1-a563-98346e3be569.png)? It is called the
    **learning rate**. As shown in the following diagram, if the learning rate is
    small, then we take a small step downward and our gradient descent can be slow.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 ![](img/8179ea37-437e-40e1-a563-98346e3be569.png)？它被称为**学习率**。如下图所示，如果学习率很小，那么我们将小步向下移动，我们的梯度下降可能会很慢。
- en: 'If the learning rate is large, then we take a large step and our gradient descent
    will be fast, but we might fail to reach the global minimum and become stuck at
    a local minimum. So, the learning rate should be chosen optimally:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率很大，那么我们会迈大步，梯度下降会很快，但我们可能无法达到全局最小值，并在局部最小值处卡住。因此，学习率应该被选择得最优：
- en: '![](img/975346d3-1711-4973-bacf-8bd77141ff5b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/975346d3-1711-4973-bacf-8bd77141ff5b.png)'
- en: This whole process of backpropagating the network from the output layer to the
    input layer and updating the weights of the network using gradient descent to
    minimize the loss is called **backpropagation**. Now that we have a basic understanding
    of backpropagation, we will strengthen our understanding by learning about this
    in detail, step-by-step. We are going to look at some interesting math, so put
    on your calculus hats and follow the steps.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出层到输入层反向传播网络并使用梯度下降更新网络权重以最小化损失的整个过程称为**反向传播**。现在我们对反向传播有了基本理解，我们将通过逐步学习详细了解，加深理解。我们将看一些有趣的数学内容，所以戴上你们的微积分帽子，跟随这些步骤。
- en: 'So, we have two weights, one ![](img/ca5dfcf7-41c2-4a8a-8883-9b30519930bd.png),input
    to hidden layer weights, and the other ![](img/9576ab5d-840e-4382-8130-bd525f556023.png),
    hidden to output layer weights. We need to find the optimal values for these two
    weights that will give us the fewest errors. So, we need to calculate the derivative
    of the cost function ![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png) with respect
    to these weights. Since we are backpropagating, that is, going from the output
    layer to the input layer, our first weight will be![](img/487d3133-d88e-46eb-abdf-cc4c577a5fe4.png).
    So, now we need to calculate the derivative of ![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png)
    with respect to ![](img/caea536c-bf53-4095-83b8-56981f591219.png). How do we calculate
    the derivative? First, let''s recall our cost function, ![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个权重，一个是![](img/ca5dfcf7-41c2-4a8a-8883-9b30519930bd.png)，输入到隐藏层的权重，另一个是![](img/9576ab5d-840e-4382-8130-bd525f556023.png)，隐藏到输出层的权重。我们需要找到这两个权重的最优值，以减少误差。因此，我们需要计算损失函数![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png)对这些权重的导数。因为我们正在反向传播，即从输出层到输入层，我们的第一个权重将是![](img/487d3133-d88e-46eb-abdf-cc4c577a5fe4.png)。所以，现在我们需要计算![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png)对![](img/caea536c-bf53-4095-83b8-56981f591219.png)的导数。我们如何计算导数？首先，让我们回顾一下我们的损失函数，![](img/a803a8a5-9812-4b93-a811-5e1a6c4e8c54.png)：
- en: '![](img/89657612-2da7-480d-9adc-4ba544bec713.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89657612-2da7-480d-9adc-4ba544bec713.png)'
- en: We cannot calculate the derivative directly from the preceding equation since
    there is no
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述方程式中我们无法直接计算导数，因为没有
- en: '![](img/5f7cfa6f-9a88-4dfe-834b-f8a0e432d721.png) term. So, instead of calculating
    the derivative directly, we calculate the partial derivative. Let''s recall our
    forward propagation equation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/5f7cfa6f-9a88-4dfe-834b-f8a0e432d721.png)项。因此，我们不是直接计算导数，而是计算偏导数。让我们回顾一下我们的前向传播方程式：'
- en: '![](img/cfff4ced-f916-459e-91ec-436bb2571df5.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfff4ced-f916-459e-91ec-436bb2571df5.png)'
- en: '![](img/35a873f3-75f8-4c90-9429-dfba3073c530.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35a873f3-75f8-4c90-9429-dfba3073c530.png)'
- en: 'First, we will calculate a partial derivative with respect to [![](img/34155884-cff2-46fe-8289-ec304f441a2e.png)],
    and then from [![](img/28d2e95e-c6cc-4fa3-bbf5-2570bd4df04d.png)] we will calculate
    the partial derivative with respect to [![](img/1096ac8b-40a8-400d-904a-0dce7783570f.png)].
    From [![](img/f2071a3b-05b8-4315-8f6b-0cb42f3bfb29.png)], we can directly calculate
    our derivative [![](img/0895be9d-fcbd-4237-9b14-3db88a268320.png)]. It is basically
    the chain rule. So, the derivative of [![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)]
    with respect to ![](img/fc832c43-00ac-4825-8cc2-945ab173d55e.png) becomes as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算对[![](img/34155884-cff2-46fe-8289-ec304f441a2e.png)]的偏导数，然后从[![](img/28d2e95e-c6cc-4fa3-bbf5-2570bd4df04d.png)]中计算对[![](img/1096ac8b-40a8-400d-904a-0dce7783570f.png)]的偏导数。从[![](img/f2071a3b-05b8-4315-8f6b-0cb42f3bfb29.png)]中，我们可以直接计算我们的导数[![](img/0895be9d-fcbd-4237-9b14-3db88a268320.png)]。这基本上是链式法则。因此，对于[![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)]对![](img/fc832c43-00ac-4825-8cc2-945ab173d55e.png)的导数如下所示：
- en: '![](img/2e4c510e-c439-4877-8a81-5dffd3972c41.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e4c510e-c439-4877-8a81-5dffd3972c41.png)'
- en: 'Now, we will compute each of the terms in the preceding equation:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算前述方程式中的每一项：
- en: '![](img/74146337-3cbe-4c45-95b8-7043c21022bd.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74146337-3cbe-4c45-95b8-7043c21022bd.png)'
- en: '![](img/2fb19f9c-87bc-4e67-ab23-54d2e2553a41.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fb19f9c-87bc-4e67-ab23-54d2e2553a41.png)'
- en: 'Here, ![](img/10c7ef2e-5667-46f5-a0f7-c200f50165ed.png) is the derivative of
    our sigmoid activation function. We know that the sigmoid function is [![](img/92e46632-0b6b-4fa7-80f2-80f925d761ad.png)],
    so the derivative of the sigmoid function would be [![](img/032ca043-1110-4ca5-9b24-7f617d042364.png)]:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/10c7ef2e-5667-46f5-a0f7-c200f50165ed.png)是我们的 sigmoid 激活函数的导数。我们知道
    sigmoid 函数是[![](img/92e46632-0b6b-4fa7-80f2-80f925d761ad.png)]，因此 sigmoid 函数的导数为[![](img/032ca043-1110-4ca5-9b24-7f617d042364.png)]：
- en: '![](img/51925f25-3fc2-4e4d-90d5-f1e6ecb8b5a4.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51925f25-3fc2-4e4d-90d5-f1e6ecb8b5a4.png)'
- en: 'Thus, substituting all the preceding terms in equation *(1)* we can write:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将方程式*(1)*中的所有前述项代入，我们可以写成：
- en: '![](img/23f8268d-7224-44e9-9a8c-ac130056191d.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23f8268d-7224-44e9-9a8c-ac130056191d.png)'
- en: Now we need to compute a derivative of [![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)]
    with respect to our next weight,[![](img/913e2324-d25d-4230-b0ca-dea93a027c15.png)].
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要计算对于下一个权重[![](img/913e2324-d25d-4230-b0ca-dea93a027c15.png)]的导数[![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)]。
- en: 'Similarly, we cannot calculate the derivative of ![](img/092f60bc-0ed1-4a0a-887a-960a2768b4e2.png)
    directly from ![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png) as we don''t have
    any ![](img/83bd97fb-7a2b-4b55-bb8e-cc8f00c7c295.png) terms in ![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png).
    So, we need to use the chain rule. Let''s recall the forward propagation steps
    again:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们无法直接从 ![](img/092f60bc-0ed1-4a0a-887a-960a2768b4e2.png) 对 ![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)
    进行导数计算，因为我们在 ![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png) 中没有任何 ![](img/83bd97fb-7a2b-4b55-bb8e-cc8f00c7c295.png)
    项。所以，我们需要使用链式法则。让我们再次回顾前向传播的步骤：
- en: '![](img/1301fd01-bc90-4a01-aeb3-53298fbf474c.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1301fd01-bc90-4a01-aeb3-53298fbf474c.png)'
- en: '![](img/6cbad05b-da62-4fb0-a8da-1612262b3ba9.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cbad05b-da62-4fb0-a8da-1612262b3ba9.png)'
- en: '![](img/49fbf478-b44b-4544-a88d-fa38af76c837.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49fbf478-b44b-4544-a88d-fa38af76c837.png)'
- en: '![](img/dd27058f-3ed2-42c9-85f0-708b0e63134c.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd27058f-3ed2-42c9-85f0-708b0e63134c.png)'
- en: 'Now, according to the chain rule, the derivative of ![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png)
    with respect to![](img/0b948307-d86f-476f-b942-96b67cc16136.png) is given as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据链式法则，![](img/c93c3c69-6d15-4f81-a049-d6c7095c5ab3.png) 对 ![](img/0b948307-d86f-476f-b942-96b67cc16136.png)
    的导数如下：
- en: '![](img/aa8ae1ab-3d15-43a4-bbd8-96a0b90c9357.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa8ae1ab-3d15-43a4-bbd8-96a0b90c9357.png)'
- en: 'We have already seen how to compute the first terms in the preceding equation;
    now, we will see how to compute the rest of the terms:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何计算前述方程式中的第一项；现在，让我们看看如何计算其余的项：
- en: '![](img/238243e0-d20f-441c-b5ba-2d78d2c0f265.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/238243e0-d20f-441c-b5ba-2d78d2c0f265.png)'
- en: '![](img/f6fb1473-f05d-4beb-b75d-f9a06d398cb1.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6fb1473-f05d-4beb-b75d-f9a06d398cb1.png)'
- en: '![](img/40c69666-cd5e-49e2-995d-c7a662f86659.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40c69666-cd5e-49e2-995d-c7a662f86659.png)'
- en: 'Thus, substituting all the preceding terms in equation *(3)* we can write:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将前述所有项代入方程 *(3)*，我们可以写成：
- en: '![](img/552fd026-1f25-4a95-a947-90f8eb1a0301.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/552fd026-1f25-4a95-a947-90f8eb1a0301.png)'
- en: 'After we have computed gradients for both weights, ![](img/3d784cf7-f536-456e-8515-2eb49e30e4b8.png)
    and ![](img/7a0fa5e1-2c18-4cb8-97dc-1f4c265608e5.png), we will update our initial
    weights according to the weight update rule:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了 ![](img/3d784cf7-f536-456e-8515-2eb49e30e4b8.png) 和 ![](img/7a0fa5e1-2c18-4cb8-97dc-1f4c265608e5.png)
    的梯度之后，我们将根据权重更新规则更新我们的初始权重：
- en: '![](img/7d4f9ae6-6194-43ad-b261-a74a597d85b6.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d4f9ae6-6194-43ad-b261-a74a597d85b6.png)'
- en: '![](img/ca8087d7-8cdd-47d7-ac17-260216c22fb7.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca8087d7-8cdd-47d7-ac17-260216c22fb7.png)'
- en: That's it! This is how we update the weights of a network and minimize the loss.
    If you don't understand gradient descent yet, no worries! In [Chapter 3](28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml),
    *Gradient Descent and Its Variants*, we will go into the basics and learn gradient
    descent and several variants of gradient descent in more detail. Now, let's see
    how to implement the backpropagation algorithm in Python.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这就是我们如何更新网络的权重并最小化损失。如果你还不理解梯度下降，别担心！在[第3章](28ee30be-bf81-4b2b-be0f-08ec3b03a9a7.xhtml)，*梯度下降及其变体*，我们将更详细地讨论基础知识并学习梯度下降及其多种变体。现在，让我们看看如何在Python中实现反向传播算法。
- en: 'In both the equations *(2)* and *(4)*, we have the term ![](img/e8bea459-13b0-41fa-84e4-506d4731e466.png),
    so instead of computing them again and again, we just call them `delta2`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 *(2)* 和 *(4)* 中，我们有项 ![](img/e8bea459-13b0-41fa-84e4-506d4731e466.png)，所以我们不需要反复计算它们，只需称之为
    `delta2`：
- en: '[PRE8]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we compute the gradient with respect to ![](img/cbc989b7-173e-4463-967d-fff236c07ffc.png).
    Refer to equation *(2)*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算相对于 ![](img/cbc989b7-173e-4463-967d-fff236c07ffc.png) 的梯度。参考方程 *(2)*：
- en: '[PRE9]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We compute the gradient with respect to ![](img/0ca26aa5-e066-454a-b0ac-a0042b545572.png).
    Refer to equation *(4)*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算相对于 ![](img/0ca26aa5-e066-454a-b0ac-a0042b545572.png) 的梯度。参考方程 *(4)*：
- en: '[PRE10]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will update the weights according to our weight update rule equation *(5)*
    and *(6)* as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据我们的权重更新规则方程 *(5)* 和 *(6)* 更新权重如下：
- en: '[PRE11]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The complete code for the backpropagation is given as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 后向传播的完整代码如下：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Before moving on, let''s familiarize ourselves with some of the frequently
    used terminologies in neural networks:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们熟悉神经网络中一些经常使用的术语：
- en: '**Forward pass**: Forward pass implies forward propagating from the input layer
    to the output layer.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向传播**：前向传播意味着从输入层向输出层进行前向传播。'
- en: '**Backward pass**: Backward pass implies backpropagating from the output layer
    to the input layer.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后向传播**：后向传播意味着从输出层向输入层进行反向传播。'
- en: '**Epoch**: The epoch specifies the number of times the neural network sees
    our whole training data. So, we can say one epoch is equal to one forward pass
    and one backward pass for all training samples.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮次**：轮次指定了神经网络看到我们整个训练数据的次数。因此，我们可以说一轮次等于所有训练样本的一次前向传播和一次反向传播。'
- en: '**Batch size**: The batch size specifies the number of training samples we
    use in one forward pass and one backward pass.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：批量大小指定了在一次前向传播和一次反向传播中使用的训练样本数。'
- en: '**Number of iterations**: The number of iterations implies the number of passes
    where *one pass = one forward pass + one backward pass*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代次数**：迭代次数意味着传递的次数，其中*一次传递 = 一次前向传播 + 一次反向传播*。'
- en: Say that we have 12,000 training samples and that our batch size is 6,000\.
    It will take us two iterations to complete one epoch. That is, in the first iteration,
    we pass the first 6,000 samples and perform a forward pass and a backward pass;
    in the second iteration, we pass the next 6,000 samples and perform a forward
    pass and a backward pass. After two iterations, our neural network will see the
    whole 12,000 training samples, which makes it one epoch.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有12,000个训练样本，并且我们的批量大小为6,000。我们将需要两次迭代才能完成一个轮次。也就是说，在第一次迭代中，我们传递前6,000个样本，并执行一次前向传播和一次反向传播；在第二次迭代中，我们传递接下来的6,000个样本，并执行一次前向传播和一次反向传播。两次迭代后，我们的神经网络将看到全部12,000个训练样本，这构成了一个轮次。
- en: Debugging gradient descent with gradient checking
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度检查调试梯度下降
- en: We just learned how gradient descent works and how to code the gradient descent
    algorithm from scratch for a simple two-layer network. But implementing gradient
    descent for complex neural networks is not a simple task. Apart from implementing,
    debugging a gradient descent for complex neural network architecture is again
    a tedious task. Surprisingly, even with some buggy gradient descent implementations,
    the network will learn something. However, apparently, it will not perform well
    compared to the bug-free implementation of gradient descent.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了梯度下降的工作原理，以及如何为简单的两层网络从头开始编写梯度下降算法。但是，实现复杂神经网络的梯度下降并不是一件简单的任务。除了实现之外，调试复杂神经网络架构的梯度下降又是一项繁琐的任务。令人惊讶的是，即使存在一些有缺陷的梯度下降实现，网络也会学到一些东西。然而，显然，与无缺陷实现的梯度下降相比，它的表现不会很好。
- en: If the model does not give us any errors and learns something even with buggy
    implementations of the gradient descent algorithm, how can we evaluate and ensure
    that our implementation is correct? That is why we use the gradient checking algorithm.
    It will help us to validate our implementation of gradient descent by numerically
    checking the derivative.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型没有给出任何错误并且即使在梯度下降算法的有缺陷实现下也能学到东西，那么我们如何评估和确保我们的实现是正确的呢？这就是我们使用梯度检查算法的原因。它将通过数值检查导数来验证我们的梯度下降实现是否正确。
- en: Gradient checking is basically used for debugging the gradient descent algorithm
    and to validate that we have a correct implementation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查主要用于调试梯度下降算法，并验证我们是否有正确的实现。
- en: Okay. So, how does gradient checking works? In gradient checking, we basically
    compare the numerical and analytical gradients. Wait! What are numerical and analytical
    gradients?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 好了。那么，梯度检查是如何工作的呢？在梯度检查中，我们基本上比较数值梯度和解析梯度。等等！什么是数值梯度和解析梯度？
- en: '**Analytical gradient** implies the gradients we calculated through backpropagation.
    **Numerical gradients** are the numerical approximation to the gradients. Let''s
    explore this with an example. Assume we have a simple square function, ![](img/2b8960b3-4692-488f-a07a-ef6e3fc3e048.png).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**解析梯度**意味着我们通过反向传播计算的梯度。**数值梯度**是梯度的数值近似。让我们通过一个例子来探讨这个问题。假设我们有一个简单的平方函数，![图片](img/2b8960b3-4692-488f-a07a-ef6e3fc3e048.png)。'
- en: 'The analytical gradient for the preceding function is computed using the power
    rule as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数的解析梯度使用幂次法则计算如下：
- en: '![](img/421425f3-93c8-4fce-b526-90e13997f24c.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/421425f3-93c8-4fce-b526-90e13997f24c.png)'
- en: Now, let's see how to approximate the gradient numerically. Instead of using
    the power rule to calculate gradients, we calculate gradients using a definition
    of the gradients. We know that the gradient or slope of a function basically gives
    us the steepness of the function.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何数值近似梯度。我们不使用幂次法则来计算梯度，而是使用梯度的定义来计算梯度。我们知道，函数的梯度或斜率基本上给出了函数的陡峭程度。
- en: 'Thus, the gradient or slope of a function is defined as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，函数的梯度或斜率定义如下：
- en: '![](img/0d706589-2d5e-4491-8114-39a488a68528.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d706589-2d5e-4491-8114-39a488a68528.png)'
- en: 'A gradient of a function can be given as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的梯度可以表示如下：
- en: '![](img/e8bc90bc-d88e-4c15-b986-c292d0c6c2e2.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8bc90bc-d88e-4c15-b986-c292d0c6c2e2.png)'
- en: 'We use the preceding equation and approximate the gradients numerically. It
    implies that we are calculating the slope of the function manually, instead of
    using power rule as shown in the following diagram:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述方程并在数值上近似计算梯度。这意味着我们手动计算函数的斜率，而不是像以下图表中显示的幂次法则一样：
- en: '![](img/91c3dccd-2d04-46ff-9f7e-7c088c80f5ab.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91c3dccd-2d04-46ff-9f7e-7c088c80f5ab.png)'
- en: Computing gradients through power rule *(7)* and approximating the gradients
    numerically *(8)* essentially gives us the same value. Let's see how *(7)* and
    *(8)* give the same value in Python.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通过幂次法则 *(7)* 计算梯度，并在 Python 中近似计算梯度 *(8)* 本质上给出了相同的值。让我们看看如何在 Python 中 *(7)*
    和 *(8)* 给出相同的值。
- en: 'Define the square function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 定义平方函数：
- en: '[PRE13]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the epsilon and input value:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 epsilon 和输入值：
- en: '[PRE14]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Compute the analytical gradient:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 计算解析梯度：
- en: '[PRE15]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Compute the numerical gradient:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数值梯度：
- en: '[PRE16]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you may have noticed, computing numerical and analytical gradients of the
    square function essentially gave us the same value, which is `6` when *x =3*.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的那样，计算平方函数的数值和解析梯度本质上给出了相同的值，即当 *x =3* 时为 `6`。
- en: While backpropagating the network, we compute the analytical gradients to minimize
    the cost function. Now, we need to make sure that our computed analytical gradients
    are correct. So, let's validate that we approximate the numerical gradients of
    the cost function.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播网络时，我们计算解析梯度以最小化成本函数。现在，我们需要确保我们计算的解析梯度是正确的。因此，让我们验证我们近似计算成本函数数值梯度。
- en: 'The gradients of ![](img/b89b62f6-a88e-4d9d-9293-b4c8e2cb0f69.png) with respect
    to ![](img/3de54252-f200-48b3-8ade-0c6e5fc5e2b3.png) can be numerically approximated
    as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于 ![](img/3de54252-f200-48b3-8ade-0c6e5fc5e2b3.png)，![](img/b89b62f6-a88e-4d9d-9293-b4c8e2cb0f69.png)
    的梯度可以按如下数值近似：
- en: '![](img/176aded0-ffbb-43c7-b899-b15888bcd4a3.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/176aded0-ffbb-43c7-b899-b15888bcd4a3.png)'
- en: 'It is represented as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 它的表示如下：
- en: '![](img/2d6e039e-148f-4c8b-a60c-03c190eef054.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d6e039e-148f-4c8b-a60c-03c190eef054.png)'
- en: We check whether the analytical gradients and approximated numerical gradients
    are the same; if not, then there is an error in our analytical gradient computation.
    We don't want to check whether the numerical and analytical gradients are exactly
    the same; since we are only approximating the numerical gradients, we check the
    difference between the analytical and numerical gradients as an error. If the
    difference is less than or equal to a very small number say, *1e-7*, then our
    implementation is fine. If the difference is greater than *1e-7*, then our implementation
    is wrong.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查解析梯度和近似数值梯度是否相同；如果不是，则我们的解析梯度计算存在错误。我们不想检查数值和解析梯度是否完全相同；因为我们只是近似计算数值梯度，我们检查解析梯度和数值梯度之间的差异作为错误。如果差异小于或等于一个非常小的数字，比如
    *1e-7*，那么我们的实现是正确的。如果差异大于 *1e-7*，那么我们的实现是错误的。
- en: 'Instead of calculating the error directly as the difference between the numerical
    gradient and the analytical gradient, we calculate the relative error. It can
    be defined as the ratio of differences to the ratio of an absolute value of the
    gradients:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是直接计算数值梯度和解析梯度之间的差异作为错误，我们计算相对误差。它可以定义为差异的比率与梯度绝对值的比率：
- en: '![](img/b4c41974-fd9c-45ea-9fa8-49b7eca8c7a9.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4c41974-fd9c-45ea-9fa8-49b7eca8c7a9.png)'
- en: When the value of relative error is less than or equal to a small threshold
    value, say, *1e-7*, then our implementation is fine. If the relative error is
    greater than *1e-7*, then our implementation is wrong. Now let's see how to implement
    gradient checking algorithm in Python step-by-step.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当相对误差的值小于或等于一个小的阈值值，比如 *1e-7*，那么我们的实现是正确的。如果相对误差大于 *1e-7*，那么我们的实现是错误的。现在让我们逐步看看如何在
    Python 中实现梯度检查算法。
- en: First, we calculate the weights. Refer equation *(9):*
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算权重。参考方程 *(9)*：
- en: '[PRE17]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Compute `J_plus` and `J_minus`. Refer equation *(9):*
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 `J_plus` 和 `J_minus`。参考方程 *(9)*：
- en: '[PRE18]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can compute the numerical gradient as given in *(9)* as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按 *(9)* 给出的方式计算数值梯度如下：
- en: '[PRE19]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Analytical gradients can be obtained through backpropagation:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过反向传播获得解析梯度：
- en: '[PRE20]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Compute the relative error as given in equation *(10)* as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 计算相对误差，如方程 *(10)* 所示：
- en: '[PRE21]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If the relative error is less than a small threshold value, say `1e-7`, then
    our gradient descent implementation is correct; otherwise, it is wrong:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果相对误差小于一个小的阈值，比如`1e-7`，则我们的梯度下降实现是正确的；否则，是错误的：
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Thus, with the help of gradient checking, we make sure that our gradient descent
    algorithm is bug-free.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，借助梯度检查，我们确保我们的梯度下降算法没有错误。
- en: Putting it all together
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'Putting all the concepts we have learned so far together, we will see how to
    build a neural network from scratch. We will understand how the neural network
    learns to perform the XOR gate operation. The XOR gate returns 1 only when exactly
    only one of its inputs is 1, else it returns 0 as shown in the following table:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们迄今为止学到的所有概念综合起来，我们将看到如何从头开始构建一个神经网络。我们将了解神经网络如何学习执行XOR门操作。XOR门只在其输入中恰好有一个为1时返回1，否则返回0，如下表所示：
- en: '![](img/27147219-0adc-49ba-a3d4-40df55825d41.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27147219-0adc-49ba-a3d4-40df55825d41.png)'
- en: Building a neural network from scratch
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始构建神经网络
- en: 'To perform the XOR gate operation, we build a simple two-layer neural network,
    as shown in the following diagram. As you can see, we have an input layer with
    two nodes: a hidden layer with five nodes and an output layer comprising one node:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行XOR门操作，我们构建了一个简单的两层神经网络，如下图所示。您可以看到，我们有一个具有两个节点的输入层，一个具有五个节点的隐藏层和一个包含一个节点的输出层：
- en: '![](img/584c586c-8500-4150-907b-f32a004254ca.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/584c586c-8500-4150-907b-f32a004254ca.png)'
- en: 'We will understand step-by-step how a neural network learns the XOR logic:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步理解神经网络如何学习XOR逻辑：
- en: 'First, import the libraries:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入库：
- en: '[PRE23]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Prepare the data as shown in the preceding XOR table:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前面的XOR表准备数据：
- en: '[PRE24]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the number of nodes in each layer:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义每层中的节点数：
- en: '[PRE25]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Initialize the weights and bias randomly. First, we initialize the input to
    hidden layer weights:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重和偏置。首先，我们初始化输入到隐藏层的权重：
- en: '[PRE26]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we initialize the hidden to output layer weights:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将隐藏层到输出层的权重初始化：
- en: '[PRE27]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define the sigmoid activation function:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义sigmoid激活函数：
- en: '[PRE28]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the derivative of the sigmoid function:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义sigmoid函数的导数：
- en: '[PRE29]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the forward propagation:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义前向传播：
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the backward propagation:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义反向传播：
- en: '[PRE31]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the cost function:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义成本函数：
- en: '[PRE32]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Set the learning rate and the number of training iterations:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习率和训练迭代次数：
- en: '[PRE33]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s start training the network with the following code:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们用以下代码开始训练网络：
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Plot the cost function:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制成本函数：
- en: '[PRE35]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As you can observe in the following plot, the loss decreases over the training
    iterations:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以在下面的图中观察到的那样，损失随着训练迭代次数的增加而减少：
- en: '![](img/8491d0d9-3387-4789-944b-8315befbc019.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8491d0d9-3387-4789-944b-8315befbc019.png)'
- en: Thus, in this chapter, we got an overall understanding of artificial neural
    network and how they learn.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们对人工神经网络及其学习方式有了全面的了解。
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started off the chapter by understanding what deep learning is and how it
    differs from machine learning. Later, we learned how biological and artificial
    neurons work, and then we explored what is input, hidden, and output layer in
    the ANN, and also several types of activation functions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解深度学习是什么及其与机器学习的区别开始本章。后来，我们学习了生物和人工神经元的工作原理，然后探讨了ANN中的输入、隐藏和输出层，以及几种激活函数。
- en: Going ahead, we learned what forward propagation is and how ANN uses forward
    propagation to predict the output. After this, we learned how ANN uses backpropagation
    for learning and optimizing. We learned an optimization algorithm called gradient
    descent that helps the neural network to minimize the loss and make correct predictions.
    We also learned about gradient checking, a technique that is used to evaluate
    the gradient descent. At the end of the chapter, we implemented a neural network
    from scratch to perform the XOR gate operation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了前向传播是什么，以及ANN如何使用前向传播来预测输出。在此之后，我们学习了ANN如何使用反向传播来学习和优化。我们学习了一种称为梯度下降的优化算法，帮助神经网络最小化损失并进行正确预测。我们还学习了梯度检查，一种用于评估梯度下降的技术。在本章的结尾，我们实现了一个从头开始的神经网络来执行XOR门操作。
- en: In the next chapter, we will learn about one of the most powerful and popularly
    used deep learning libraries called **TensorFlow**.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一个名为**TensorFlow**的最强大和最广泛使用的深度学习库。
- en: Questions
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our newly acquired knowledge by answering the following questions:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们新获得的知识：
- en: How does deep learning differ from machine learning?
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习与机器学习有何不同？
- en: What does the word *deep* mean in deep learning?
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*deep* 在深度学习中是什么意思？'
- en: Why do we use the activation function?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么使用激活函数？
- en: Explain dying ReLU problem.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释 dying ReLU 问题。
- en: Define forward propagation.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义前向传播。
- en: What is back propagation?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是反向传播？
- en: Explain gradient checking.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释梯度检查。
- en: Further reading
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'You can also check out some of these resources for more information:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看以下一些资源以获取更多信息：
- en: 'Understand more about gradient descent from this amazing video: [https://www.youtube.com/watch?v=IHZwWFHWa-w](https://www.youtube.com/watch?v=IHZwWFHWa-w)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这个精彩的视频了解更多关于梯度下降的知识：[https://www.youtube.com/watch?v=IHZwWFHWa-w](https://www.youtube.com/watch?v=IHZwWFHWa-w)
- en: 'Learn about implementing a neural network from scratch to recognize handwritten
    digits: [https://github.com/sar-gupta/neural-network-from-scratch](https://github.com/sar-gupta/neural-network-from-scratch)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何从头开始实现一个神经网络来识别手写数字：[https://github.com/sar-gupta/neural-network-from-scratch](https://github.com/sar-gupta/neural-network-from-scratch)
