- en: Natural Language Processing with Sequence Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列数据的自然语言处理
- en: In this chapter, we will be looking at different representations of text data
    that are useful for building deep learning models. We will be helping you to understand
    **recurrent neural networks** (**RNNs**). This chapter will cover different implementations
    of RNNs, such as **long short-term memory** (**LSTM**) and **gated recurrent unit**
    (**GRU**), which power most of the deep learning models for text and sequential
    data. We will be looking at different representations of text data and how they
    are useful for building deep learning models. In addition, this chapter will address
    one-dimensional convolutions that can be used for sequential data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到不同的文本数据表示形式，这些形式对构建深度学习模型非常有用。我们将帮助您理解**循环神经网络**（**RNNs**）。本章将涵盖不同的RNN实现，如**长短期记忆**（**LSTM**）和**门控循环单元**（**GRU**），它们支持大多数文本和序列数据的深度学习模型。我们将研究文本数据的不同表示及其对构建深度学习模型的用处。此外，本章还将讨论可用于序列数据的一维卷积。
- en: 'Some of the applications that can be built using RNNs are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用RNN构建的一些应用程序包括：
- en: '**Document classifiers**: Identifying the sentiment of a tweet or review, classifying
    news articles'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分类器**：识别推文或评论的情感，分类新闻文章'
- en: '**Sequence-to-sequence learning**: For tasks such as language translations,
    converting English to French'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到序列学习**：用于诸如语言翻译、将英语转换为法语等任务'
- en: '**Time-series forecasting**: Predicting the sales of a store when given details
    about previous days'' sales records'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列预测**：根据前几天的销售记录预测商店的销售情况'
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Working with text data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: Training embedding by building a sentiment classifier
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构建情感分类器训练嵌入
- en: Using pretrained word embeddings
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的词嵌入
- en: Recursive neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: Solving text classification problem using LSTM
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM解决文本分类问题
- en: Convolutional network on sequence data
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列数据上的卷积网络
- en: Language modeling
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言建模
- en: Working with text data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: 'Text is one of the most commonly used sequential data types. Text data can
    be seen as either a sequence of characters or a sequence of words. It is common
    to see text as a sequence of words for most problems. Deep learning sequential
    models such as RNNs and its variants are able to learn important patterns from
    text data that can solve problems in areas such as the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是最常用的序列数据类型之一。文本数据可以看作是字符序列或单词序列。对于大多数问题，将文本视为单词序列是很常见的。深度学习的顺序模型，如RNN及其变种，能够从文本数据中学习重要的模式，以解决以下领域的问题：
- en: Natural language understanding
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: Document classification
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类
- en: Sentiment classification
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分类
- en: These sequential models also act as important building blocks for various systems,
    such as **question and answer **(**QA**) systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些顺序模型也是各种系统的重要构建块，例如**问答**（**QA**）系统。
- en: Though these models are highly useful in building these applications, they do
    not have an understanding of human language, due to its inherent complexities.
    These sequential models are able to successfully find useful patterns that are
    then used for performing different tasks. Applying deep learning to text is a
    fast-growing field, and a lot of new techniques arrive every month. We will cover
    the fundamental components that power most modern-day deep learning applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型在构建这些应用程序中非常有用，但由于其固有的复杂性，它们并不理解人类语言。这些顺序模型能够成功地发现有用的模式，然后用于执行不同的任务。将深度学习应用于文本是一个快速增长的领域，每个月都会出现许多新技术。我们将涵盖大多数现代深度学习应用程序的基本组件。
- en: 'Deep learning models, like any other machine learning model, do not understand
    text, so we need to convert text into numerical representations. The process of
    converting text into numerical representations is called **vectorization** and
    can be done in different ways, as outlined here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型与其他机器学习模型一样，并不理解文本，因此我们需要将文本转换为数值表示。将文本转换为数值表示的过程称为**向量化**，可以通过以下不同方式完成：
- en: Convert text into words and represent each word as a vector
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为单词，并将每个单词表示为向量
- en: Convert text into characters and represent each character as a vector
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为字符，并将每个字符表示为向量
- en: Create n-grams of words and represent them as vectors
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建单词的n-gram，并将它们表示为向量
- en: 'Text data can be broken down into one of these representations. Each smaller
    unit of text is called a token, and the process of breaking text into tokens is
    called **tokenization**. There are a lot of powerful libraries available in Python
    that can help us in tokenization. Once we convert the text data into tokens, we
    then need to map each token to a vector. One-hot encoding and word embedding are
    the two most popular approaches for mapping tokens to vectors. The following diagram
    summarizes the steps for converting text into vector representations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据可以被分解为这些表示之一。文本的每个较小单元称为标记，将文本分解为标记的过程称为**分词**。Python中有许多强大的库可以帮助我们进行分词。一旦我们将文本数据转换为标记，我们接下来需要将每个标记映射到一个向量。独热编码和词嵌入是将标记映射到向量的两种最流行的方法。以下图表总结了将文本转换为向量表示的步骤：
- en: '![](img/73e7d9d7-448e-467e-864f-20f9f0fd4e44.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73e7d9d7-448e-467e-864f-20f9f0fd4e44.png)'
- en: Let's look in more detail at tokenization, n-gram representation, and vectorization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解分词、n-gram表示和向量化。
- en: Tokenization
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Given a sentence, splitting it into either characters or words is called tokenization.
    There are libraries, such as spaCy, that offer complex solutions to tokenization.
    Let's use simple Python functions such as `split` and `list` to convert the text
    into tokens.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个句子，将其分割为字符或单词称为分词。有一些库，比如spaCy，提供了复杂的分词解决方案。让我们使用简单的Python函数如`split`和`list`来将文本转换为标记。
- en: 'To demonstrate how tokenization works on characters and words, let''s consider
    a small review of the movie *Toy Story*. We will work with the following text:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示分词在字符和单词上的工作方式，让我们考虑一部电影*Toy Story*的简短评论。我们将使用以下文本：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Converting text into characters
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转换为字符
- en: 'The Python `list` function takes a string and converts it into a list of individual
    characters. This does the job of converting the text into characters. The following
    code block shows the code used and the results:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`list`函数接受一个字符串并将其转换为单个字符的列表。这完成了将文本转换为字符的任务。以下代码块展示了所使用的代码及其结果：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The result is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This result shows how our simple Python function has converted text into tokens.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果展示了我们简单的Python函数如何将文本转换为标记。
- en: Converting text into words
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转换为单词
- en: 'We will use the `split` function available in the Python string object to break
    the text into words. The `split` function takes an argument, and based on this,
    it splits the text into tokens. For our example, we will use spaces as delimiters.
    The following code block demonstrates how we can convert text into words using
    the Python `split` function:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python字符串对象中提供的`split`函数来将文本分割成单词。`split`函数接受一个参数，基于这个参数将文本分割成标记。在我们的示例中，我们将使用空格作为分隔符。以下代码块演示了如何使用Python的`split`函数将文本转换为单词：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code, we did not use any separator; by default, the `split`
    function splits on white spaces.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们没有使用任何分隔符；默认情况下，`split`函数在空格上分割。
- en: N-gram representation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-gram表示
- en: 'We have seen how text can be represented as characters and words. Sometimes,
    it is useful to look at two, three, or more words together. **N-grams** are groups
    of words extracted from the given text. In an n-gram, *n* represents the number
    of words that can be used together. Let''s look at an example of what a bigram
    (*n=2*) looks like. We used the Python `nltk` package to generate a bigram for
    `toy_story_review`. The following code block shows the result of the bigram and
    the code used to generate it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到文本如何表示为字符和单词。有时，查看两个、三个或更多单词一起的情况非常有用。**N-grams**是从给定文本中提取的单词组。在一个n-gram中，*n*表示可以一起使用的单词数。让我们看一个双字母词（*n=2*）的例子。我们使用Python的`nltk`包为`toy_story_review`生成了一个双字母词。以下代码块展示了双字母词的结果以及生成它的代码：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `ngrams` function accepts a sequence of words as its first argument and
    the number of words to be grouped as the second argument. The following code block
    shows how a trigram representation would look, and the code used for it:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`ngrams`函数接受一个单词序列作为其第一个参数，以及要分组的单词数作为第二个参数。以下代码块展示了三元组表示的样例以及用于生成它的代码：'
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This results in the following output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The only thing that changed in the preceding code is the *n* value, the second
    argument to the function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中唯一改变的是*n*值，即函数的第二个参数。
- en: Many supervised machine learning models, for example, Naive Bayes, use n-grams
    to improve their feature space. N-grams are also used for spelling correction
    and text summarization tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 许多监督学习机器学习模型，例如朴素贝叶斯，使用 n-gram 来改进其特征空间。n-gram 也用于拼写校正和文本摘要任务。
- en: One challenge with n-gram representation is that it loses the sequential nature
    of text. It is often used with shallow machine learning models. This technique
    is rarely used in deep learning, as architectures such as RNN and Conv1D, learn
    these representations automatically.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram 表示的一个挑战是失去文本的顺序性质。它通常与浅层机器学习模型一起使用。这种技术在深度学习中很少使用，因为像 RNN 和 Conv1D 这样的架构会自动学习这些表示。
- en: Vectorization
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化
- en: There are two popular approaches to mapping the generated tokens to vectors
    of numbers, called one-hot encoding and word embedding. Let's understand how tokens
    can be converted to these vector representations by writing a simple Python program.
    We will also discuss the various pros and cons of each method.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常用方法将生成的标记映射到数字向量，称为 one-hot 编码和词嵌入。让我们通过编写一个简单的 Python 程序来理解如何将标记转换为这些向量表示。我们还将讨论每种方法的各种优缺点。
- en: One-hot encoding
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: One-hot 编码
- en: 'In one-hot encoding, each token is represented by a vector of length *N*, where
    *N* is the size of the vocabulary. The vocabulary is the total number of unique
    words in the document. Let''s take a simple sentence and observe how each token
    would be represented as one-hot encoded vectors. The following is the sentence
    and its associated token representation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在 one-hot 编码中，每个标记由长度为 *N* 的向量表示，其中 *N* 是词汇表的大小。词汇表是文档中唯一单词的总数。让我们以一个简单的句子为例，观察每个标记如何表示为
    one-hot 编码向量。以下是句子及其相关标记表示：
- en: '*An apple a day keeps doctor away said the doctor.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*每天一个苹果，医生远离我说道医生。*'
- en: 'One-hot encoding for the preceding sentence can be represented in a tabular
    format as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以前面的句子为例，其 one-hot 编码可以表示为以下表格格式：
- en: '| An | 100000000 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| An | 100000000 |'
- en: '| apple | 10000000 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| apple | 10000000 |'
- en: '| a | 1000000 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| a | 1000000 |'
- en: '| day | 100000 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| day | 100000 |'
- en: '| keeps | 10000 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| keeps | 10000 |'
- en: '| doctor | 1000 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| doctor | 1000 |'
- en: '| away | 100 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| away | 100 |'
- en: '| said | 10 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| said | 10 |'
- en: '| the | 1 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| the | 1 |'
- en: 'This table describes the tokens and their one-hot encoded representation. The
    vector length is nine, as there are nine unique words in the sentence. A lot of
    machine learning libraries have eased the process of creating one-hot encoding
    variables. We will write our own implementation to make it easier to understand,
    and we can use the same implementation to build other features required for later
    examples. The following code contains a `Dictionary` class, which contains functionality
    to create a dictionary of unique words along with a function to return a one-hot
    encoded vector for a particular word. Let''s take a look at the code and then
    walk through each functionality:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该表格描述了标记及其 one-hot 编码表示。向量长度为九，因为句子中有九个唯一单词。许多机器学习库已经简化了创建 one-hot 编码变量的过程。我们将编写自己的实现以便更容易理解，并可以使用相同的实现来构建后续示例所需的其他特性。以下代码包含一个`Dictionary`类，其中包含创建唯一单词字典的功能，以及返回特定单词的
    one-hot 编码向量的函数。让我们看一下代码，然后逐个功能进行解释：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code provides three important functionalities:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码提供了三个重要功能：
- en: The initialization, `init`, function creates a `word2index` dictionary, which
    will store all unique words along with the index. The `index2word` list stores
    all the unique words and the `length` variable contains the total number of unique
    words in our document.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化函数 `init` 创建一个 `word2index` 字典，它将存储所有唯一单词及其索引。`index2word` 列表存储所有唯一单词，`length`
    变量包含我们文档中的唯一单词总数。
- en: The `add_word` function takes a word and adds it to `word2index` and `index2word`,
    and increases the length of the vocabulary, provided the word is unique.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_word` 函数接收一个单词并将其添加到 `word2index` 和 `index2word` 中，并增加词汇表的长度，前提是该单词是唯一的。'
- en: The `onehot_encoded` function takes a word and returns a vector of length *N*
    with zeros throughout, except at the index of the word. If the index of the passed
    word is two, then the value of the vector at index two will be one, and all the
    remaining values will be zeros.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onehot_encoded` 函数接收一个单词并返回一个长度为 *N* 的向量，全零，除了单词的索引处为一。如果传递的单词索引为二，则向量在索引为二处的值为一，其余所有值为零。'
- en: 'As we have defined our `Dictionary` class, let''s use it on our `toy_story_review`
    data. The following code demonstrates how the `word2index` dictionary is built
    and how we can call our `onehot_encoded` function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义了我们的`Dictionary`类后，让我们在我们的`toy_story_review`数据上使用它。下面的代码演示了如何构建`word2index`字典以及如何调用我们的`onehot_encoded`函数：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: One of the challenges with one-hot representation is that the data is too sparse,
    and the size of the vector quickly grows as the number of unique words in the
    vocabulary increases. Another limitation is that one-hot has no representation
    of internal relations between words. For these reasons, one-hot is rarely used
    with deep learning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单热表示的一个挑战是数据过于稀疏，而且随着词汇表中唯一单词数量的增加，向量的大小会迅速增长。另一个局限是单热没有表现词语之间的内部关系。因此，单热表示在深度学习中很少使用。
- en: Word embedding
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词嵌入
- en: Word embedding is a very popular way of representing text data in problems that
    are solved by deep learning algorithms. Word embedding provides a dense representation
    of a word filled with floating numbers. The vector dimension varies according
    to the vocabulary size. It is common to use a word embedding of dimension size
    50, 100, 256, 300, and sometimes 1,000\. The dimension size is a hyperparameter
    that we need to play with during the training phase.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 单词嵌入是在由深度学习算法解决的问题中代表文本数据的一种非常流行的方式。单词嵌入提供了填充有浮点数的单词的密集表示。向量维度根据词汇量的大小而变化。在训练阶段通常使用维度大小为50、100、256、300或者有时1000的单词嵌入。维度大小是一个我们需要在训练阶段调整的超参数。
- en: If we are trying to represent a vocabulary of size 20,000 in one-hot representation,
    then we will end up with 20,000 x 20,000 numbers, most of which will be zero.
    The same vocabulary can be represented in a word embedding as 20,000 x dimension
    size, where the dimension size could be 10, 50, 300, and so on.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试用单热表示表示大小为20,000的词汇表，那么我们最终会得到20,000 x 20,000个数字，其中大多数将为零。相同的词汇表可以用20,000
    x 维度大小的单词嵌入表示，其中维度大小可以是10、50、300等等。
- en: 'One way to create word embeddings is to start with dense vectors for each token
    containing random numbers and then train a model, such as a document classifier,
    for sentiment classification. The floating point numbers, which represent the
    tokens, will get adjusted in such a way that semantically closer words will have
    similar representation. To understand it, let''s look at the following diagram,
    where we plotted the word embedding vectors on a two-dimensional plot of five
    movies:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 创建单词嵌入的一种方法是为每个标记开始时使用随机数生成密集向量，然后训练一个模型，如文档分类器，用于情感分类。代表标记的浮点数将被调整，以便语义上更接近的单词具有类似的表示。为了理解它，让我们看下面的图表，我们在其中绘制了五部电影的二维图上的单词嵌入向量：
- en: '![](img/cd68a722-b8a3-4dfb-bd5e-d753a5d2c3e4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd68a722-b8a3-4dfb-bd5e-d753a5d2c3e4.png)'
- en: The preceding diagram shows how the dense vectors are tuned in order to have
    smaller distances for words that are semantically similar. Since movie titles
    such as *Finding Nemo*, *Toy Story*, and *The Incredibles* are fictional movies
    with cartoons, the embedding for such words is closer. On the other hand, the
    embedding for the movie *Titanic* is far from the cartoons and closer to the movie
    title *Notebook*, since they are romantic movies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了如何调整密集向量，以便使语义上相似的单词之间的距离更小。由于电影标题如*寻找尼莫*、*玩具总动员*和*超人特工队*都是虚构的卡通电影，因此这些单词的嵌入更接近。另一方面，电影*泰坦尼克号*的嵌入远离卡通片，更接近电影*恋恋笔记本*，因为它们都是浪漫电影。
- en: Learning word embedding may not be feasible when you have too little data, and
    in those cases, we can use word embeddings that are trained by some other machine
    learning algorithm. An embedding generated from another task is called a pretrained
    word embedding. We will learn how to build our own word embeddings and use pretrained
    word embeddings.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据量太少时，学习单词嵌入可能不可行，在这些情况下，我们可以使用由其他机器学习算法训练的单词嵌入。从其他任务生成的嵌入称为预训练单词嵌入。我们将学习如何构建自己的单词嵌入并使用预训练单词嵌入。
- en: Training word embedding by building a sentiment classifier
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过构建情感分类器来训练单词嵌入
- en: In the last section, we briefly learned about word embedding without implementing
    it. In this section, we will download a dataset called IMDb, which contains reviews,
    and build a sentiment classifier that calculates whether a review's sentiment
    is positive, negative, or unknown. In the process of building, we will also train
    word embedding for the words present in the IMDb dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要介绍了词嵌入而没有实现它。在本节中，我们将下载一个名为 IMDb 的数据集，其中包含评论，并构建一个情感分类器，计算评论情感是积极的、消极的还是未知的。在构建过程中，我们还将对
    IMDb 数据集中出现的单词进行词嵌入训练。
- en: 'We will use a library called `torchtext`, which makes a lot of processes such
    as downloading, text vectorization, and batching, much easier. Training a sentiment
    classifier will involve the following steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为 `torchtext` 的库，它使得许多过程（如下载、文本向量化和批处理）更加简单。训练情感分类器将涉及以下步骤：
- en: Downloading IMDb data and performing text tokenization
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 IMDb 数据并进行文本标记化
- en: Building a vocabulary
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建词汇表
- en: Generating batches of vectors
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成向量批次
- en: Creating a network model with embeddings
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用嵌入创建网络模型
- en: Training the model
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: We shall see these steps in more detail in the following sections.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中详细介绍这些步骤。
- en: Downloading IMDb data and performing text tokenization
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载 IMDb 数据并进行文本标记化
- en: 'For applications related to computer vision, we used the `torchvision` library,
    which provides us with a lot of utility functions, helping to build computer vision
    applications. In the same way, there is a library called `torchtext`, which is
    built to work with PyTorch and eases a lot of activities related to **natural
    language processing** (**NLP**) by providing different data loaders and abstractions
    for text. At the time of writing, `torchtext` does not come with the standard
    PyTorch installation and requires a separate installation. You can run the following
    code in the command line of your machine to get `torchtext` installed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与计算机视觉相关的应用程序，我们使用了 `torchvision` 库，它为我们提供了许多实用函数，帮助构建计算机视觉应用程序。同样，还有一个名为
    `torchtext` 的库，它专为与 PyTorch 一起工作而构建，通过提供不同的文本数据加载器和抽象来简化与**自然语言处理**（**NLP**）相关的许多活动。在撰写本文时，`torchtext`
    并不随标准的 PyTorch 安装一起提供，需要单独安装。您可以在机器的命令行中运行以下代码来安装 `torchtext`：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once it is installed, we will be able to use it. The `torchtext` download provides
    two important modules called `torchtext.data` and `torchtext.datasets`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完成，我们将能够使用它。 `torchtext` 下载提供了两个重要的模块，称为 `torchtext.data` 和 `torchtext.datasets`。
- en: 'We can download the IMDb Movies dataset from the following link:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从以下链接下载 IMDb 电影数据集：
- en: '[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)'
- en: Tokenizing with torchtext.data
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `torchtext.data` 进行标记化
- en: 'The `torchtext.data` instance defines a `Field` class, which helps us to define
    how the data has to be read and tokenized. Let''s look at the following example,
    which we will use for preparing our IMDb dataset:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext.data` 实例定义了一个 `Field` 类，它帮助我们定义数据的读取和标记化方式。让我们看下面的例子，我们将用它来准备我们的
    IMDb 数据集：'
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we define two `Field` objects, one for actual text and
    another for the label data. For actual text, we expect `torchtext` to lowercase
    all the text, tokenize the text, and trim it to a maximum length of 20\. If we
    are building an application for a production environment, we may fix the length
    to a much larger number. However, for the toy example, this works well. The `Field`
    constructor also accepts another argument called `tokenize`, which by default
    uses the `str.split` function. We can also specify `spaCy` as the argument, or
    any other tokenizer. For our example, we will stick with `str.split`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们定义了两个 `Field` 对象，一个用于实际文本，另一个用于标签数据。对于实际文本，我们希望 `torchtext` 将所有文本转换为小写，标记化文本并将其修剪到最大长度为
    20。如果我们正在构建用于生产环境的应用程序，可能会将长度固定为更大的数字。但是，对于示例，这样做效果很好。`Field` 构造函数还接受另一个名为 `tokenize`
    的参数，默认使用 `str.split` 函数。我们也可以指定 `spaCy` 或任何其他分词器作为参数。对于我们的示例，我们将坚持使用 `str.split`。
- en: Tokenizing with torchtext.datasets
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `torchtext.datasets` 进行标记化
- en: 'The `torchtext.datasets` instance provides wrappers for using different datasets
    such as IMDb, TREC (question classification), language modeling (WikiText-2),
    and a few other datasets. We will use `torch.datasets` to download the IMDb dataset
    and split it into train and test datasets. The following code does that and when
    you run it for the first time, it could take several minutes (depending on your
    broadband connection) as it downloads the IMDb dataset from the internet:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext.datasets`实例提供了使用不同数据集的包装器，如IMDb、TREC（问题分类）、语言建模（WikiText-2）和其他几个数据集。我们将使用`torch.datasets`下载IMDb数据集并将其分割为训练集和测试集。以下代码执行此操作，第一次运行时可能需要几分钟（取决于您的宽带连接），因为它从互联网下载IMDb数据集：'
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The previous dataset''s `IMDB` class abstracts away all the complexity involved
    in downloading, tokenizing, and splitting the database into train and test datasets.
    The `train.fields` download contains a dictionary where `TEXT` is the key and
    `LABEL` is the value. Let''s look at `train.fields` and what each element of `train`
    contains:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 先前数据集的`IMDB`类抽象了下载、分词和将数据库分为训练集和测试集的所有复杂性。`train.fields`下载包含一个字典，其中`TEXT`是键，`LABEL`是值。让我们看看`train.fields`及其每个元素`train`包含的内容：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Similarly, the variance for the training dataset is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，训练数据集的方差如下：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see from these results that a single element contains a field and text,
    along with all the tokens representing the text, and a `label` field that contains
    the label of the text. Now we have the IMDb dataset ready for batching.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些结果中看到，单个元素包含一个字段和文本，以及表示文本的所有标记，以及一个包含文本标签的`label`字段。现在我们已经准备好进行IMDb数据集的批处理。
- en: Building vocabulary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建词汇表
- en: 'When we created one-hot encoding for `toy_story_review`, we created a `word2index`
    dictionary, which is referred to as the vocabulary since it contains all the details
    of the unique words in the documents. The `torchtext` instance makes that easier
    for us. Once the data is loaded, we can call `build_vocab` and pass the necessary
    arguments that will take care of building the vocabulary for the data. The following
    code shows how the vocabulary is built:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为`toy_story_review`创建了一种一热编码时，我们创建了一个`word2index`字典，它被称为词汇表，因为它包含了文档中唯一单词的所有详细信息。`torchtext`实例使这一切变得更加容易。一旦数据加载完成，我们可以调用`build_vocab`并传递必要的参数来构建数据的词汇表。以下代码展示了词汇表是如何构建的：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we pass in the `train` object on which we need to build
    the vocabulary, and we also ask it to initialize vectors with pretrained embeddings
    of dimensions `300`. The `build_vocab` object just downloads and creates the dimension
    that will be used later when we train the sentiment classifier using pretrained
    weights. The `max_size` instance limits the number of words in the vocabulary,
    and `min_freq` removes any word that has not occurred more than 10 times, where
    `10` is configurable.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们传递了需要构建词汇表的`train`对象，并要求它使用维度为`300`的预训练嵌入来初始化向量。`build_vocab`对象只是下载并创建稍后在使用预训练权重训练情感分类器时将使用的维度。`max_size`实例限制了词汇表中单词的数量，而`min_freq`删除了任何出现次数不到10次的单词，其中`10`是可配置的。
- en: 'Once the vocabulary is built, we can obtain different values such as frequency,
    word index, and the vector representation for each word. The following code demonstrates
    how to access these values:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦词汇表建立完成，我们可以获取不同的值，如频率、词索引和每个单词的向量表示。以下代码演示了如何访问这些值：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in the following output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following code demonstrates how to access the results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何访问结果：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This results in the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Similarly, we will print the values for a dictionary containing words and their
    indexes as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将打印包含单词及其索引的字典的值如下：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `stoi` value gives access to a dictionary containing words and their indexes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`stoi`值提供了访问包含单词及其索引的字典。'
- en: Generating batches of vectors
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成向量的批次
- en: 'The `torchtext` download provides `BucketIterator`, which helps in batching
    all the text and replacing the words with the index number of the words. The `BucketIterator`
    instance comes with a lot of useful parameters such as `batch_size`, `device`
    (GPU or CPU), and `shuffle` (whether data has to be shuffled). The following code
    demonstrates how to create iterators that generate batches for the train and test
    datasets:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext`下载提供了`BucketIterator`，它有助于批处理所有文本并用单词的索引号替换单词。`BucketIterator`实例带有许多有用的参数，例如`batch_size`、`device`（GPU或CPU）和`shuffle`（数据是否需要洗牌）。以下代码演示了如何创建为训练和测试数据集生成批次的迭代器：'
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code gives a `BucketIterator` object for both train and test
    datasets. The following code shows how to create a batch and display the results
    of the batch:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码为训练和测试数据集提供了一个`BucketIterator`对象。下面的代码展示了如何创建一个批次并显示批次的结果：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This results in the following output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will print the labels as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下方式打印标签：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: From the results in the preceding code block, we can see how the text data is
    converted into a matrix of size `(batch_size` * `fix_len`), which is (128 x 20).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面代码块的结果，我们可以看到文本数据如何转换为大小为`(batch_size` * `fix_len`)的矩阵，即(128 x 20)。
- en: Creating a network model with embedding
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个带有嵌入的网络模型
- en: 'We discussed word embeddings briefly earlier. In this section, we create word
    embeddings as part of our network architecture and train the entire model to predict
    the sentiment of each review. At the end of the training, we will have a sentiment
    classifier model and also the word embeddings for the IMDB datasets. The following
    code demonstrates how to create a network architecture to predict the sentiment
    using word embeddings:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要讨论了词嵌入。在本节中，我们将词嵌入作为网络架构的一部分创建，并训练整个模型以预测每个评论的情感。在训练结束时，我们将得到一个情感分类器模型以及IMDB数据集的词嵌入。以下代码演示了如何创建一个网络架构来使用词嵌入来预测情感：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, `EmbeddingNetwork` creates the model for sentiment classification.
    Inside the `_init_` function, we initialize an object of the `nn.Embedding` class,
    which takes two arguments, namely, the size of the vocabulary and the dimensions
    that we wish to create for each word. As we have limited the number of unique
    words, the vocabulary size will be 10,000 and we can start with a small embedding
    size of 10\. For running the program quickly, a small embedding size is useful,
    but when you are trying to build applications for production systems, use embeddings
    of a large size. We also have a linear layer that maps the word embeddings to
    the category (positive, negative, or unknown).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`EmbeddingNetwork`创建了情感分类的模型。在`_init_`函数内部，我们初始化了`nn.Embedding`类的一个对象，它接受两个参数，即词汇量的大小和我们希望为每个单词创建的维度。由于我们限制了唯一单词的数量，词汇量大小将为10,000，并且我们可以从一个小的嵌入大小10开始。在快速运行程序时，使用小的嵌入大小是有用的，但是当您尝试构建用于生产系统的应用程序时，请使用较大的嵌入。我们还有一个线性层，将单词嵌入映射到类别（积极、消极或未知）。
- en: The forward function determines how the input data is processed. For a batch
    size of 32 and sentences of a maximum length of 20 words, we will have inputs
    of the shape 32 x 20\. The first embedding layer acts as a lookup table, replacing
    each word with the corresponding embedding vector. For an embedding dimension
    of 10, the output becomes 32 x 20 x 10 as each word is replaced with its corresponding
    embedding. The `view()` function will flatten the result from the embedding layer.
    The first argument passed to view will keep that dimension intact.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前向函数确定如何处理输入数据。对于批次大小为32和句子最大长度为20个单词，我们将会得到形状为32 x 20的输入。第一个嵌入层充当查找表，用相应的嵌入向量替换每个单词。对于嵌入维度为10，输出变为32
    x 20 x 10，因为每个单词被其对应的嵌入所替换。`view()`函数将会展平来自嵌入层的结果。传递给view的第一个参数将保持该维度不变。
- en: In our case, we do not want to combine data from different batches, so we preserve
    the first dimension and flatten the rest of the values in the tensor. After the
    `view()` function is applied, the tensor shape changes to 32 x 200\. A dense layer
    maps the flattened embeddings to the number of categories. Once the network is
    defined, we can train the network as usual.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们不希望将来自不同批次的数据合并，因此我们保留第一维并展平张量中的其余值。应用`view()`函数后，张量形状变为32 x 200。一个密集层将展平的嵌入映射到类别的数量。一旦网络定义好了，我们可以像往常一样训练网络。
- en: Remember that in this network, we lose the sequential nature of the text and
    we just use the text as a bag of words.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个网络中，我们失去了文本的顺序性，我们只是将文本视为一袋词语。
- en: Training the model
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Training the model is very similar to what we saw for building image classifiers,
    so we will be using the same functions. We pass batches of data through the model,
    calculate the outputs and losses, and then optimize the model weights, which includes
    the embedding weights. The following code does this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型非常类似于我们构建图像分类器时所看到的，因此我们将使用相同的函数。我们通过模型传递数据批次，计算输出和损失，然后优化模型权重，包括嵌入权重。以下代码执行此操作：
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we iterate over the dataset:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据集进行迭代：
- en: '[PRE32]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'From here, we can train the model over each epoch:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以在每个epoch上训练模型：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we call the `fit` method by passing the `BucketIterator`
    object that we created for batching the data. The iterator, by default, does not
    stop generating batches, so we have to set the repeat variable of the `BucketIterator`
    object to `False`. If we don't set the repeat variable to `False`, then the `fit`
    function will run indefinitely. Training the model for around 10 epochs gives
    a validation accuracy of approximately 70%. Now that you've learned how to train
    word embedding by building sentiment classifier, let us learn how to use pretrained
    word embeddings in the next section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们通过传递用于批处理数据的`BucketIterator`对象来调用`fit`方法。默认情况下，迭代器不会停止生成批次，因此我们必须将`BucketIterator`对象的repeat变量设置为`False`。如果不将repeat变量设置为`False`，那么`fit`函数将无限运行。在大约10个epoch的训练后，模型达到了约70%的验证准确率。现在您已经学会了通过构建情感分类器训练词嵌入，让我们在下一节中学习如何使用预训练词嵌入。
- en: Using pretrained word embeddings
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练词嵌入
- en: 'Pretrained word embeddings are useful when we are working in specific domains,
    such as medicine and manufacturing, where we have a lot of data to train the embeddings.
    When we have little data and we cannot meaningfully train the embeddings, we can
    use embeddings that are trained on different data corpuses such as Wikipedia,
    Google News, and Twitter tweets. A lot of teams have open source word embeddings
    trained using different approaches. In this section, we will explore how `torchtext`
    makes it easier to use different word embeddings, and how to use them in our PyTorch
    models. It is similar to transfer learning, which we use in computer vision applications.
    Typically, using pretrained embedding would involve the following steps:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练词嵌入在我们工作于特定领域时非常有用，例如医学和制造业，在这些领域中我们有大量数据可以用来训练嵌入。当我们的数据很少且不能有意义地训练嵌入时，我们可以使用在不同数据语料库（如维基百科、Google新闻和Twitter推文）上训练的嵌入。许多团队都有使用不同方法训练的开源词嵌入。在本节中，我们将探讨`torchtext`如何使使用不同词嵌入更加容易，以及如何在我们的PyTorch模型中使用它们。这类似于在计算机视觉应用中使用的迁移学习。通常，使用预训练嵌入涉及以下步骤：
- en: Downloading the embeddings
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载嵌入
- en: Loading the embeddings in the model
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型中的嵌入
- en: Freezing the embedding layer weights
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结嵌入层权重
- en: Let's explore in detail how each step is implemented.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨每个步骤的实现方式。
- en: Downloading the embeddings
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载嵌入
- en: 'The `torchtext` library abstracts away a lot of complexity involved in downloading
    the embeddings and mapping them to the right word. The `torchtext` library provides
    three classes in the `vocab` module, namely, GloVe, FastText, CharNGram, which
    ease the process of downloading embeddings and mapping them to our vocabulary.
    Each of these classes provides different embeddings trained on different datasets
    and using different techniques. Let''s look at some of the different embeddings
    provided:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext`库在下载嵌入和将其映射到正确单词时，抽象掉了许多复杂性。`torchtext`库在`vocab`模块中提供了三个类，即GloVe、FastText、CharNGram，它们简化了下载嵌入和映射到我们词汇表的过程。每个类都提供了在不同数据集上训练的不同嵌入，并使用不同技术。让我们看一些不同的提供的嵌入：'
- en: '`charngram.100d`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`charngram.100d`'
- en: '`fasttext.en.300d`'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fasttext.en.300d`'
- en: '`fasttext.simple.300d`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fasttext.simple.300d`'
- en: '`glove.42B.300d`'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.42B.300d`'
- en: '`glove.840B.300d`'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.840B.300d`'
- en: '`glove.twitter.27B.25d`'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.25d`'
- en: '`glove.twitter.27B.50d`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.50d`'
- en: '`glove.twitter.27B.100d`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.100d`'
- en: '`` `glove.twitter.27B.200d` ``'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `glove.twitter.27B.200d` ``'
- en: '`glove.6B.50d`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.50d`'
- en: '`glove.6B.100d`'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.100d`'
- en: '`glove.6B.200d`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.200d`'
- en: '`glove.6B.300d`'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.300d`'
- en: 'The `build_vocab` method of the `Field` object takes in an argument for the
    embeddings. The following code explains how we download the embeddings:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`Field`对象的`build_vocab`方法接受一个用于嵌入的参数。以下代码解释了如何下载嵌入：'
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The value to the argument vector denotes what embedding class is to be used.
    The `name` and `dim` arguments determine which embeddings can be used. We can
    easily access the embeddings from the `vocab` object. The following code demonstrates
    it, along with a view of how the results will look:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 参数向量的值表示要使用的嵌入类。`name` 和 `dim` 参数确定可以使用哪些嵌入。我们可以轻松地从 `vocab` 对象中访问嵌入。下面的代码演示了它，以及结果将如何显示：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This results in the following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE36]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now we have downloaded and mapped the embeddings to our vocabulary. Let's understand
    how we can use them with a PyTorch model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载并将嵌入映射到我们的词汇表中。让我们了解如何在PyTorch模型中使用它们。
- en: Loading the embeddings in the model
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在模型中加载嵌入
- en: 'The `vectors` variable returns a torch tensor of shape `vocab_size` x `dimensions`
    containing the pretrained embeddings. We have to store the embeddings to the weights
    of our embedding layer. We can assign the weights of the embeddings by accessing
    the weights of the embeddings layer as demonstrated by the following code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`vectors` 变量返回一个形状为 `vocab_size` x `dimensions` 的torch张量，其中包含预训练的嵌入。我们必须将嵌入分配给我们嵌入层的权重。我们可以通过访问嵌入层的权重来赋值嵌入的权重，如以下代码所示：'
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `model` download represents the object of our network, and `embedding`
    represents the embedding layer. As we are using the embedding layer with new dimensions,
    there will be a small change in the input to the linear layer that comes after
    the embedding layer. The following code has the new architecture, which is similar
    to the previously used architecture where we trained our embeddings:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`model` 下载代表我们网络的对象，`embedding` 代表嵌入层。由于我们使用了新维度的嵌入层，线性层的输入将会有所变化。下面的代码展示了新的架构，与我们之前训练嵌入时使用的架构类似：'
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Once the embeddings are loaded, we have to ensure that, during training, we
    do not change the embedding weights. Let's discuss how to achieve that.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了嵌入，我们必须确保在训练期间不改变嵌入权重。让我们讨论如何实现这一点。
- en: Freezing the embedding layer weights
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冻结嵌入层的权重
- en: 'It is a two-step process to tell PyTorch not to change the weights of the embedding
    layer:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要告诉PyTorch不要改变嵌入层的权重是一个两步过程：
- en: Set the `requires_grad` attribute to `False`, which instructs PyTorch that it
    does not need gradients for these weights.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `requires_grad` 属性设置为 `False`，这告诉PyTorch不需要这些权重的梯度。
- en: Remove the passing of the embedding layer parameters to the optimizer. If this
    step is not done, then the optimizer throws an error, as it expects all the parameters
    to have gradients.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除嵌入层参数传递给优化器。如果不执行此步骤，则优化器会抛出错误，因为它期望所有参数都有梯度。
- en: 'The following code demonstrates how easy it is to freeze the embedding layer
    weights and instruct the optimizer not to use those parameters:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码演示了冻结嵌入层权重以及指导优化器不使用这些参数是多么简单：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We generally pass all the model parameters to the optimizer, but in the previous
    code, we passed parameters that have `requires_grad` as `True`.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将所有模型参数传递给优化器，但在之前的代码中，我们传递了 `requires_grad` 为 `True` 的参数。
- en: We can train the model using this exact code and should achieve similar accuracy.
    All these model architectures fail to take advantage of the sequential nature
    of the text. In the next section, we explore two popular techniques, namely, RNN
    and Conv1D, which take advantage of the sequential nature of the data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这段代码训练模型，并且应该达到类似的准确性。所有这些模型架构都未能充分利用文本的顺序特性。在下一节中，我们探讨了两种流行的技术，即RNN和Conv1D，它们利用了数据的顺序特性。
- en: Recursive neural networks
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: RNNs are among the most powerful models that enable us to take on applications
    such as classification, labeling of sequential data, generating sequences of text
    (such as with the SwiftKey Keyboard app, which predicts the next word), and converting
    one sequence to another, such as when translating a language (for example, from
    French to English). Most of the model architectures, such as feedforward neural
    networks, do not take advantage of the sequential nature of data. For example,
    we need the data to present the features of each example in a vector, say all
    the tokens that represent a sentence, paragraph, or documents. Feedforward networks
    are designed just to look at all the features once and map them to output. Let's
    look at a text example that shows why the order, or sequential nature of text,
    is important. *I had cleaned my car* and *I had my car cleaned* are two English
    sentences with the same set of words, but they mean different things when we consider
    the order of the words.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 是我们能够应对分类、序列数据标记、生成文本序列（例如 SwiftKey 键盘应用程序，预测下一个单词）、以及将一种序列转换为另一种序列（比如从法语翻译成英语）等应用中最强大的模型之一。大多数模型架构，如前馈神经网络，并不利用数据的序列性质。例如，我们需要数据来呈现每个示例的特征，以向量形式表示，比如表示句子、段落或文档的所有标记。前馈网络仅设计为一次性查看所有特征并将其映射到输出。让我们看一个文本示例，说明为什么文本的顺序或序列性质很重要。*I
    had cleaned my car* 和 *I had my car cleaned* 是两个英文句子，它们有相同的单词集合，但考虑单词顺序时它们的含义不同。
- en: 'In most modern languages, humans make sense of text data by reading words from
    left to right and building a powerful model that kind of understands all the different
    things the text says. RNN works similarly by looking at one word in text at a
    time. RNN is also a neural network that has a special layer in it, which loops
    over the data instead of processing all at once. As RNNs can process data in sequence,
    we can use vectors of different lengths and generate outputs of different lengths.
    Some of the different representations are provided in the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数现代语言中，人类通过从左到右阅读单词并构建一个强大的模型来理解文本数据的各种内容。RNN 类似地通过逐次查看文本中的每个单词来工作。RNN 也是一种神经网络，其中有一个特殊的层，该层循环处理数据而不是一次性处理所有数据。由于
    RNN 可以处理序列数据，我们可以使用不同长度的向量并生成不同长度的输出。以下图表提供了一些不同的表示方式：
- en: '![](img/b5536b92-c0fc-446f-bdbc-b1e77ee93f23.jpeg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5536b92-c0fc-446f-bdbc-b1e77ee93f23.jpeg)'
- en: The previous diagram is from one of the famous blogs on RNN ([http://karpathy.github.
    io/2015/05/21/rnn-effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness)),
    in which the author, Andrej Karpathy, writes about how to build an RNN from scratch
    with Python and using it as a sequence generator.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图是来自有关 RNN 的著名博客之一（[http://karpathy.github. io/2015/05/21/rnn-effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness)），作者
    Andrej Karpathy 在其中讲述了如何使用 Python 从零开始构建 RNN 并将其用作序列生成器。
- en: Understanding how RNN works with an example
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例理解 RNN 的工作方式
- en: Let's start with the assumption that we have an RNN model already built and
    let's try to understand what functionality it provides. Once we understand what
    an RNN does, let's then explore what happens inside an RNN.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们已经构建了一个 RNN 模型，并试图理解它提供的功能。一旦我们理解了 RNN 的功能，我们再探讨 RNN 内部发生的事情。
- en: 'Let''s consider the *Toy Story* review as input to the RNN model. The example
    text we are looking at is *Just perfect. Script, character, animation....this
    manages to break free....*. We start by passing the first word, *just* to our
    model, and the model generates two different things: a **State Vector** and an
    **Output** vector. The **State Vector** is passed to the model when it processes
    the next word in the review, and a new **State Vector** is generated. We just
    consider the **Output** of the model generated during the last sequence. The following
    diagram summarizes it:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 *Toy Story* 的评论作为 RNN 模型的输入。我们正在查看的示例文本是 *Just perfect. Script, character,
    animation....this manages to break free....*。我们从将第一个单词 *just* 传递给我们的模型开始，模型生成两种不同的东西：一个**状态向量**和一个**输出**向量。**状态向量**在模型处理评论中的下一个单词时被传递，并生成一个新的**状态向量**。我们只考虑模型在最后一个序列期间生成的**输出**。以下图表总结了这一点：
- en: '![](img/7f19b5d5-4e41-410e-bf89-0d4057d2b74c.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f19b5d5-4e41-410e-bf89-0d4057d2b74c.png)'
- en: 'The preceding diagram demonstrates the following:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示演示了以下内容：
- en: How RNN works by unfolding the text input and the image
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过展开文本输入和图像来理解 RNN 的工作方式
- en: How the state is recursively passed to the same model
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态如何递归地传递给同一个模型
- en: 'By now, you will have an idea of what RNN does, but not how it works. Before
    we get into how it works, let''s look at a code snippet that showcases in more
    detail what we have learned. We will still view RNN as a black box:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经对 RNN 的工作有了一定了解，但不知道其具体工作原理。在我们深入研究其工作原理之前，让我们看一下展示我们所学内容更详细的代码片段。我们仍将视
    RNN 为一个黑箱：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the preceding code, the `hidden` variable represents the state vector, sometimes
    called the **hidden state**. By now, we should have an idea of how RNN is used.
    Now, let''s look at the code that implements RNN and understand what happens inside
    the RNN. The following code contains the `RNN` class:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`hidden` 变量表示状态向量，有时称为**隐藏状态**。现在，我们应该对 RNN 的使用有所了解了。接下来，让我们看一下实现 RNN
    并理解 RNN 内部发生了什么的代码。以下代码包含 `RNN` 类：
- en: '[PRE41]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Except for the word RNN in the preceding code, everything else would sound pretty
    similar to what we have used in the previous chapters, as PyTorch hides a lot
    of complexity of backpropagation. Let's walk through the `__init__` function and
    the `forward` function to understand what is happening.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述代码中的 RNN 一词外，其他内容听起来与我们在前几章中使用的内容非常相似，因为 PyTorch 隐藏了很多反向传播的复杂性。让我们详细查看 `__init__`
    函数和 `forward` 函数，了解其中发生了什么。
- en: The `__init__` function initializes two linear layers, one for calculating the
    output and the other for calculating the state or hidden vector.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__` 函数初始化了两个线性层，一个用于计算输出，另一个用于计算状态或隐藏向量。'
- en: The `forward` function combines the input vector and the hidden vector and passes
    it through the two linear layers, which generates an output vector and a hidden
    state. For the output layer, we apply a `log_softmax` function.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 函数将输入向量和隐藏向量组合，并通过两个线性层传递，生成输出向量和隐藏状态。对于输出层，我们应用 `log_softmax` 函数。'
- en: 'The `initHidden` function helps in creating hidden vectors with no state for
    calling RNN the very first time. Let''s take a visual look into what the RNN class
    does in the following diagram:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`initHidden` 函数有助于在第一次调用 RNN 时创建没有状态的隐藏向量。让我们通过下面的图示来直观了解 RNN 类的功能：'
- en: '![](img/488bd1e1-0128-4c9d-8375-0ca89bb15457.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/488bd1e1-0128-4c9d-8375-0ca89bb15457.png)'
- en: The preceding diagram shows how an RNN works.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了 RNN 的工作原理。
- en: 'The concepts of RNN are sometimes tricky to understand when you meet them for
    the first time, so I would strongly recommend some of the amazing blogs provided
    in the following links: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    and [http://colah.github.io/posts/2015-08-Understanding-LSTMs/.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的概念有时在第一次接触时可能难以理解，因此我强烈推荐阅读以下链接提供的一些令人惊叹的博客：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    和 [http://colah.github.io/posts/2015-08-Understanding-LSTMs/.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: In the next section, we will learn how to use a variant of RNN called LSTM to
    build a sentiment classifier on the IMDB dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用称为 LSTM 的 RNN 变体构建 IMDB 数据集上的情感分类器。
- en: Solving text classification problem using LSTM
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LSTM 解决文本分类问题
- en: RNNs are quite popular in building real-world applications, such as language
    translation, text classification, and many more sequential problems. However,
    in reality, we would rarely use a vanilla version of RNN, such as the one we saw
    in the previous section. The vanilla version of RNN has problems, such as vanishing
    gradients and gradient explosion when dealing with large sequences. In most of
    the real-world problems, variants of RNN such as LSTM or GRU are used, which solve
    the limitations of plain RNN and also have the ability to handle sequential data
    better. We will try to understand what happens in LSTM and build a network based
    on LSTM to solve the text classification problem on the IMDB datasets.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在构建实际应用中非常流行，例如语言翻译、文本分类等多种顺序问题。然而，在现实中，我们很少使用简单版本的 RNN，比如我们在前一节中看到的那种。简单版本的
    RNN 存在问题，如处理大序列时的梯度消失和梯度爆炸。在大多数实际问题中，使用诸如 LSTM 或 GRU 等 RNN 变体，这些变体解决了普通 RNN 的限制，并且能更好地处理顺序数据。我们将尝试理解
    LSTM 的工作原理，并基于 LSTM 构建网络，解决 IMDB 数据集上的文本分类问题。
- en: Long-term dependency
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期依赖
- en: 'RNNs, in theory, should learn all the dependency required from the historical
    data to build a context of what happens next. Say, for example, we are trying
    to predict the last word in the sentence *The clouds are in the sky*. RNN would
    be able to predict it, as the information (clouds) is just a few words behind.
    Let''s take another long paragraph where the dependency need not be that close,
    and we want to predict the last word in it. The sentence is: *I am born in Chennai
    a city in Tamilnadu. Did schooling in different states of India and I speak...*
    The vanilla version of RNN, in practice, finds it difficult to remember the context
    that happened in the earlier parts of sequences. LSTMs, and other different variants
    of RNN, solve this problem by adding different neural networks inside the LSTM,
    which later decide how much, or what data can be remembered.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，RNN应该从历史数据中学习所有必需的依赖关系，以建立下一个事件的上下文。例如，我们试图预测句子“The clouds are in the sky.”中的最后一个单词。RNN可以预测，因为信息（clouds）仅在几个单词之后。让我们再来看一个长段落，依赖关系不需要那么紧密，我们想要预测其中的最后一个单词。这个句子是：“I
    am born in Chennai a city in Tamilnadu. Did schooling in different states of India
    and I speak...”在实践中，传统的RNN版本很难记住前面序列中发生的上下文。LSTMs及其他RNN的不同变体通过在LSTM内部添加不同的神经网络来解决这个问题，稍后这些网络会决定可以记住多少或者可以记住什么数据。
- en: LSTM networks
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络
- en: LSTMs are a special kind of RNN, capable of learning long-term dependency. They
    were introduced in 1997 and got popular in the last few years with advancements
    in available data and hardware. They work tremendously well on a large variety
    of problems and are widely used.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs是一种特殊类型的RNN，能够学习长期依赖关系。它们于1997年引入，并在最近几年因可用数据和硬件的进步而变得流行。它们在各种问题上表现出色，并被广泛应用。
- en: LSTMs are designed to avoid long-term dependency problems by having a design
    by which it is natural to remember information for a long period of time. In RNNs,
    we saw how they repeat themselves over each element of the sequence. In standard
    RNNs, the repeating module will have a simple structure like a single linear layer.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs通过设计来避免长期依赖问题，自然而然地记住信息长时间。在RNN中，我们看到它们在序列的每个元素上重复自己。在标准RNN中，重复模块将具有类似于单个线性层的简单结构。
- en: 'The following diagram shows how a simple RNN repeats itself:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个简单的循环神经网络是如何重复自身的：
- en: '![](img/c492b55a-f731-4b2e-a0a0-015848b3a8cb.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c492b55a-f731-4b2e-a0a0-015848b3a8cb.png)'
- en: 'Inside LSTM, instead of using a simple linear layer, we have smaller networks
    inside the LSTM, which do an independent job. The following diagram showcases
    what happens inside an LSTM:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM内部，我们没有使用简单的线性层，而是在LSTM内部有更小的网络，这些网络执行独立的工作。下图展示了LSTM内部的情况：
- en: '![](img/5122dbe6-0dd9-4680-91a4-6d898baa8669.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5122dbe6-0dd9-4680-91a4-6d898baa8669.png)'
- en: 'Image source: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    (diagram by Christopher Olah)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)（由Christopher
    Olah绘制的图表）
- en: Each of the small rectangular (yellow) boxes in the second box in the preceding
    diagram represent a PyTorch layer, the circles represent an element matrix or
    vector addition, and the merging lines represent that the two vectors are being
    concatenated. The good part is, we need not implement all of this manually. Most
    of the modern deep learning frameworks provide an abstraction that will take care
    of what happens inside an LSTM. PyTorch provides abstraction of all the functionality
    inside the `nn.LSTM` layer, which we can use like any other layer.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中第二个框中，每个小矩形（黄色）框代表一个PyTorch层，圆圈代表一个元素矩阵或向量的加法，而合并线表示两个向量正在被串联。好处在于，我们无需手动实现所有这些。大多数现代深度学习框架提供了一个抽象，可以处理LSTM内部的所有功能。PyTorch提供了`nn.LSTM`层内部所有功能的抽象，我们可以像使用任何其他层一样使用它。
- en: 'The most important thing in the LSTM is the cell state that passes through
    all the iterations, represented by the horizontal line across the cells in the
    preceding diagram. Multiple networks inside LSTM control what information travels
    across the cell state. The first step in LSTM (a small network represented by
    the symbol σ) is to decide what information is going to be thrown away from the
    cell state. This network is called a **forget gate** and has a sigmoid as an activation
    function, which outputs values between 0 and 1 for each element in the cell state.
    The network (PyTorch layer) is represented using the following formula:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 中最重要的是单元状态，它通过前面图表中的所有迭代表示为跨单元的水平线。 LSTM 内的多个网络控制信息如何在单元状态之间传播。 LSTM 中的第一步（由符号
    σ 表示的小网络）是决定从单元状态中丢弃哪些信息。该网络称为**遗忘门**，并且具有 sigmoid 作为激活函数，输出每个元素在单元状态中的取值介于 0
    和 1 之间。该网络（PyTorch 层）用以下公式表示：
- en: '![](img/39818084-8346-4d7a-8d4a-0eb898c16dd5.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39818084-8346-4d7a-8d4a-0eb898c16dd5.png)'
- en: 'The values from the network decide which values are to be held in the cell
    state and which are to be thrown away. The next step is to decide what information
    we are going to add to the cell state. This has two parts; a sigmoid layer, called
    the input gate, which decides what values to be updated, and a *tanh* layer, which
    creates new values to be added to the cell state. The mathematical representation
    looks like this:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的值决定了哪些值将保留在单元状态中，哪些将被丢弃。下一步是决定我们将添加到单元状态中的信息。这有两部分组成：一个称为输入门的 sigmoid 层，它决定要更新的值，以及一个创建新值添加到单元状态的
    *tanh* 层。数学表示如下：
- en: '![](img/70fea098-0372-4393-a059-4c4b8a510564.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70fea098-0372-4393-a059-4c4b8a510564.png)'
- en: 'In the next step, we combine the two values generated by the input gate and
    *tanh*. Now we can update the cell state by doing an element-wise multiplication
    between the forget gate and the sum of the product of it and *Ct*, as represented
    by the following formula:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将输入门和 *tanh* 生成的两个值组合起来。现在，我们可以通过遗忘门与其和 *Ct* 乘积之和的逐元素乘法来更新单元状态，如下公式所示：
- en: '![](img/8e447e5e-118b-4001-9a99-d5140f016bdb.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e447e5e-118b-4001-9a99-d5140f016bdb.png)'
- en: Finally, we need to decide on the output, which will be a filtered version of
    the cell state. There are different versions of LSTM available and most of them
    work on similar principles. As developers or data scientists, we rarely need to
    worry about what goes on inside LSTM.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要决定输出，这将是单元状态的筛选版本。 LSTM 有不同的版本，大多数都采用类似的原理。作为开发人员或数据科学家，我们很少需要担心 LSTM
    内部发生了什么。
- en: If you want to learn more about them, go through the following blog links, which
    cover a lot of theory in a very intuitive way.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想更深入了解它们，请阅读以下博客链接，以非常直观的方式涵盖了许多理论内容。
- en: Look at Christopher Olah's amazing blog on LSTM ([http://colah.github.io/posts/2015-
    08-Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs)),
    and another blog from Brandon Rohrer ([https://brohrer.github.io/how_rnns_lstm_work.html](https://brohrer.github.io/how_rnns_lstm_work.html))
    where he explains LSTM in a nice video.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Christopher Olah 的关于 LSTM 的精彩博客（[http://colah.github.io/posts/2015-08-Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs)），以及
    Brandon Rohrer 的另一篇博客（[https://brohrer.github.io/how_rnns_lstm_work.html](https://brohrer.github.io/how_rnns_lstm_work.html)），他在一个很棒的视频中解释了
    LSTM。
- en: 'Since we understand LSTM, let''s implement a PyTorch network that we can use
    to build a sentiment classifier. As usual, we will follow these steps for creating
    the classifier:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们理解了 LSTM，让我们实现一个 PyTorch 网络，用于构建情感分类器。像往常一样，我们将遵循以下步骤来创建分类器：
- en: Preparing the data
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Creating the batches
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建批次
- en: Creating the network
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建网络
- en: Training the model
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练
- en: We will see the steps in detail in the following section.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节详细讨论这些步骤。
- en: Preparing the data
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'We use the same `torchtext` library for downloading, tokenizing, and building
    vocabulary for the IMDB dataset. When creating the `Field` object, we leave the
    `batch_first` argument at `False`. RNNs expect the data to be in the form of `sequence_length`,
    `batch_size`, and `features.` The following is used for preparing the dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的 `torchtext` 库来下载、分词化和构建 IMDB 数据集的词汇表。在创建 `Field` 对象时，我们将 `batch_first`
    参数保留为 `False`。RNN 需要数据的形式为 `sequence_length`、`batch_size` 和 `features.` 用于准备数据集的步骤如下：
- en: '[PRE42]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Creating batches
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建批次
- en: We use the `torchtext BucketIterator` function for creating batches, and the
    size of the batches will be sequence length and batches. For our case, the size
    will be [200, 32], where 200 is the sequence length and 32 is the batch size.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `torchtext BucketIterator` 函数创建批次，批次的大小将是序列长度和批次大小。对于我们的情况，大小将是 [200, 32]，其中
    200 是序列长度，32 是批次大小。
- en: 'The following is the code used for batching:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于批处理的代码：
- en: '[PRE43]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Creating the network
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网络
- en: 'Let''s look at the code and then walk through it. You may be surprised at how
    familiar the code looks:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下代码，然后逐步理解。您可能会对代码看起来多么熟悉感到惊讶：
- en: '[PRE44]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `init` method creates an embedding layer of the size of the vocabulary and
    `hidden_size`. It also creates an LSTM and a linear layer. The last layer is a
    `LogSoftmax` layer for converting the results from the linear layer to probabilities.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`init` 方法创建一个大小为词汇表大小和 `hidden_size` 的嵌入层。它还创建了一个 LSTM 和一个线性层。最后一层是一个 `LogSoftmax`
    层，用于将线性层的结果转换为概率。'
- en: In the `forward` function, we pass the input data of size [200, 32], which gets
    passed through the embedding layer and each token in the batch gets replaced by
    embedding and the size turns to [200, 32, 100], where 100 is the embedding dimensions.
    The LSTM layer takes the output of the embedding layer along with two hidden variables.
    The hidden variables should be the same type of the embeddings output, and their
    size should be [`num_layers`, `batch_size`, `hidden_size`]. The LSTM processes
    the data in a sequence and generates the output of the shape [`Sequence_length`,
    `batch_size`, `hidden_size`], where each sequence index represents the output
    of that sequence. In this case, we just take the output of the last sequence,
    which is of the shape [`batch_size`, `hidden_dim`], and pass it on to a linear
    layer to map it to the output categories. Since the model tends to overfit, add
    a dropout layer. You can play with the dropout probabilities.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward`函数中，我们传入大小为 [200, 32] 的输入数据，经过嵌入层处理，批次中的每个标记都被嵌入取代，大小变为 [200, 32,
    100]，其中 100 是嵌入维度。LSTM 层接收嵌入层的输出和两个隐藏变量。这些隐藏变量应与嵌入输出的类型相同，它们的大小应为 [`num_layers`,
    `batch_size`, `hidden_size`]。LSTM 按顺序处理数据，并生成形状为 [`Sequence_length`, `batch_size`,
    `hidden_size`] 的输出，其中每个序列索引表示该序列的输出。在这种情况下，我们只取最后一个序列的输出，其形状为 [`batch_size`, `hidden_dim`]，并将其传递给线性层，将其映射到输出类别。由于模型容易过拟合，添加一个
    dropout 层。您可以调整 dropout 的概率。
- en: Training the model
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Once the network is created, we can train the model using the same code as
    seen in the previous examples. The following is the code for training the model:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 网络创建完成后，我们可以使用与之前示例中相同的代码训练模型。以下是训练模型的代码：
- en: '[PRE45]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following is the result of the training model:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是训练模型的结果：
- en: '[PRE46]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Training the model for four epochs gave an accuracy of 84%. Training for more
    epochs resulted in an overfitted model, as the loss started increasing. We can
    try some of the techniques that we tried such as decreasing the hidden dimensions,
    increasing sequence length, and training with smaller learning rates to further
    improve the accuracy.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行四个 epoch 的训练得到了 84% 的准确率。再训练更多 epoch 导致过拟合，因为损失开始增加。我们可以尝试一些技术，如减小隐藏维度、增加序列长度和使用较小的学习率来进一步提高准确性。
- en: We will also explore how we can use one-dimensional convolutions for training
    on sequence data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何在序列数据上使用一维卷积。
- en: Convolutional network on sequence data
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列数据上的卷积网络
- en: We learned how CNNs solve problems in computer vision by learning features from
    the images in [Chapter 4](bfebc11a-90af-4c67-ab9a-3118061abaf3.xhtml), *Deep Learning
    for Computer Vision*. In images, CNNs work by convolving across height and width.
    In the same way, time can be treated as a convolutional feature. One-dimensional
    convolutions sometimes perform better than RNNs and are computationally cheaper.
    In the last few years, companies such as Facebook have shown success in audio
    generation and machine translation. In this section, we will learn how CNNs can
    be used to build a text classification solution.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习 [第 4 章](bfebc11a-90af-4c67-ab9a-3118061abaf3.xhtml) *深度学习在计算机视觉中的应用*
    中图像中 CNN 如何通过学习图像特征来解决计算机视觉问题。在图像中，CNN 通过在高度和宽度上进行卷积来工作。同样地，时间可以被视为卷积特征。一维卷积有时比
    RNN 更好，并且计算成本更低。在过去几年中，像 Facebook 这样的公司展示了在音频生成和机器翻译方面的成功。在本节中，我们将学习如何使用 CNN 构建文本分类解决方案。
- en: Understanding one-dimensional convolution for sequence data
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解序列数据的一维卷积
- en: 'In [Chapter 4](bfebc11a-90af-4c67-ab9a-3118061abaf3.xhtml), *Deep Learning
    for Computer Vision*, we have seen how two-dimensional weights are learned from
    the training data. These weights move across the image to generate different activations.
    In the same way, one-dimensional convolution activations are learned during the
    training of our text classifier, where these weights learn patterns by moving
    across the data. The following diagram explains how one-dimensional convolutions
    will work:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第四章](bfebc11a-90af-4c67-ab9a-3118061abaf3.xhtml)，*计算机视觉的深度学习*，我们已经看到如何从训练数据中学习二维权重。这些权重在图像上移动以生成不同的激活。同样，一维卷积激活在训练我们的文本分类器时也是通过移动这些权重来学习模式。以下图示解释了一维卷积的工作原理：
- en: '![](img/712620f7-e5b3-4826-b33e-8fce255942d6.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/712620f7-e5b3-4826-b33e-8fce255942d6.png)'
- en: For training a text classifier on the IMDB dataset, we will follow the same
    steps as we followed for building the classifier using LSTMs. The only thing that
    changes is that we use `batch_first =` `True`, unlike in our LSTM network. So,
    let's look at the network, the training code, and its results.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对IMDB数据集上的文本分类器进行训练时，我们将按照使用LSTMs构建分类器时遵循的相同步骤进行操作。唯一改变的是，我们使用`batch_first =`
    `True`，而不像我们的LSTM网络那样。所以，让我们看看网络、训练代码以及其结果。
- en: Creating the network
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网络
- en: 'Let''s look at the network architecture and then walk through the code:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看网络架构，然后逐步看代码：
- en: '[PRE47]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, instead of an LSTM layer, we have a `Conv1d` layer and
    an `AdaptiveAvgPool1d` layer. The convolution layer accepts the sequence length
    as its input size, and the output size to the hidden size, and the kernel size
    as three. Since we have to change the dimensions of the linear layer, every time
    we try to run it with different lengths, we use an `AdaptiveAvgpool1d` layer,
    which takes input of any size and generates an output of the given size. So, we
    can use a linear layer whose size is fixed. The rest of the code is similar to
    what we have seen in most of the network architectures.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们不再使用LSTM层，而是使用了`Conv1d`层和`AdaptiveAvgPool1d`层。卷积层接受序列长度作为其输入大小，输出大小为隐藏大小，核大小为三。由于我们必须改变线性层的维度，所以每次我们尝试使用不同长度运行时，我们使用`AdaptiveAvgpool1d`层，它接受任何大小的输入并生成给定大小的输出。因此，我们可以使用一个大小固定的线性层。代码的其余部分与大多数网络架构中看到的相似。
- en: Training the model
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'The training steps for the model are the same as the previous example. Let''s
    just look at the code to call the `fit` method and the results it generated:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练步骤与前面的示例相同。让我们看看调用`fit`方法和生成的结果的代码：
- en: '[PRE48]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We ran the model for four epochs, which gave approximately 83% accuracy. Here
    are the results of running the model:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行了四个epoch的训练，得到了大约83%的准确率。以下是运行模型的结果：
- en: '[PRE49]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Since the validation loss started increasing after three epochs, I stopped running
    the model. A couple of things that we could try to improve the results are using
    pretrained weights, adding another convolution layer, and trying a `MaxPool1d`
    layer between the convolutions. I leave it to you to try this and see if that
    helps improve the accuracy. Now that we have learned all about the various neural
    networks to process sequence data, let us see language modeling in the next section.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 自从三个epoch后验证损失开始增加，我停止了模型的运行。我们可以尝试几件事来改进结果，例如使用预训练权重、添加另一个卷积层以及在卷积之间尝试使用`MaxPool1d`层。我把这些尝试留给你来测试是否有助于提高准确性。现在我们已经学习了处理序列数据的各种神经网络，让我们在下一节中看看语言建模。
- en: Language modeling
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言建模
- en: 'Language modeling is the task of predicting the next word in a piece of text
    given the previous words. The ability to generate this sequential data has applications
    in many different areas such as the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模是在给定前几个单词的情况下预测下一个单词的任务。生成这种顺序数据的能力在许多不同领域都有应用，如下所示：
- en: Image captioning
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕
- en: Speech recognition
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Language translation
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Automatic email reply
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动邮件回复
- en: Writing stories, news articles, poems, and so on
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写故事、新闻文章、诗歌等
- en: The focus in this area was initially held by RNNs, in particular, LSTMs. However,
    after the introduction of the Transformer architecture in 2017 ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))
    it became prevalent in NLP tasks. A number of modifications of the transformer
    have since appeared, some of which we will cover in this chapter.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这一领域的关注点主要集中在RNNs，特别是LSTMs上。然而，自2017年引入Transformer架构（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）后，在NLP任务中变得普遍。此后出现了许多Transformer的修改版本，其中一些我们将在本章中介绍。
- en: Pretrained models
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练模型
- en: In recent times there has been a lot of interest in the use of pretrained models
    for NLP tasks. A key benefit to using pretrained language models is that they
    are able to learn with significantly less data. These models are particularly
    beneficial for languages where labeled data is scarce as they only require labeled
    data.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预训练模型在NLP任务中的使用引起了广泛关注。使用预训练语言模型的一个关键优势是它们能够用更少的数据学习。这些模型特别适用于标记数据稀缺的语言，因为它们只需要标记数据。
- en: 'Pretrained models for sequence learning were first proposed in 2015 by A. M.
    Dai and Q. V. Le, in a paper titled *Semi-supervised Sequence Learning* ([http://arxiv.org/abs/1511.01432](http://arxiv.org/abs/1511.01432)).
    However, it is only recently that they have shown to be beneficial across a broad
    range of tasks. We will now consider some of the most noteworthy advances in this
    field in recent times, which include, but are by no means limited to the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，戴安哲和Q.V.勒在题为*半监督序列学习*的论文中首次提出了用于序列学习的预训练模型（[http://arxiv.org/abs/1511.01432](http://arxiv.org/abs/1511.01432)）。然而，直到最近，它们才被证明在广泛的任务中具有益处。现在我们将考虑近年来这一领域中一些值得注意的最新进展，其中包括但不限于以下内容：
- en: '**Embeddings from Language Models** (**ELMo**)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言模型的嵌入**（**ELMo**）'
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向编码器表示来自Transformers**（**BERT**）'
- en: '**Generative Pretrained Transformer 2** (**GPT**-**2**)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成预训练变压器2**（**GPT**-**2**）'
- en: Embeddings from language models
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的嵌入
- en: 'In February 2018, the *Deep contextualized word representations* paper by M.
    Peters and others ([https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365))
    introduced ELMo. It essentially demonstrated that language model embeddings can
    be used as features in a target model as shown in the following diagram:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年2月，M. Peters等人发表了*深度上下文化的单词表示*论文（[https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365)），介绍了ELMo。本质上，它证明了语言模型嵌入可以作为目标模型中的特征，如下图所示：
- en: '![](img/03d95256-f743-459d-8efb-63b6e8df13b5.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03d95256-f743-459d-8efb-63b6e8df13b5.png)'
- en: ELMo uses a bidirectional language model to learn both word and context. The
    internal states of both the forward and the backward pass are concatenated at
    each word to produce an intermediate vector. It is the bidirectional nature of
    the model that gives it information about both the next word in the sentence and
    the words before it.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo使用双向语言模型来学习单词和上下文。正向和反向传递的内部状态在每个单词处被串联起来，以产生一个中间向量。正是模型的双向性质使其获得关于句子中下一个单词和之前单词的信息。
- en: Bidirectional Encoder Representations from Transformers
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向编码器表示来自Transformers
- en: 'In a later paper published by Google in November 2018 ([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)),
    **Bidirectional Encoder Representations from Transformers** (**BERT**) was proposed,
    which incorporates an attention mechanism that learns the contextual relations
    between words and text:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在2018年11月发布的后续论文（[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)）中提出了**双向编码器表示来自Transformers**（**BERT**），它融入了一个注意力机制，学习词与文本之间的上下文关系：
- en: '![](img/f0368534-50c8-4c93-b718-3080f02282c6.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0368534-50c8-4c93-b718-3080f02282c6.png)'
- en: Unlike ELMo where the text input is read sequentially (left to right or right
    to left), with BERT, the entire sequence of words is read at once. Essentially,
    BERT is a trained transformer encoder stack.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 与ELMo不同，文本输入是按顺序（从左到右或从右到左）读取的，但是BERT会一次性读取整个单词序列。本质上，BERT是一个经过训练的Transformer编码器堆栈。
- en: Generative Pretrained Transformer 2
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成预训练变压器2
- en: 'At the time of writing, OpenAI''s GTP-2 is one of the most state-of-the-art
    language models designed to improve on the realism and coherence of generated
    text. It was introduced in the paper *Language Models are Unsupervised Multi-task
    Learners* ([https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))
    in February 2019\. It was trained to predict the next word on 8 million web pages
    (which totalled 40 GB of text) with 1.5 billion parameters, which is four times
    as many as BERT. Here is what OpenAI has to say about GPT-2:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI的GPT-2是设计用于提高生成文本的逼真度和连贯性的最先进的语言模型之一。它是在2019年2月的论文*Language Models
    are Unsupervised Multi-task Learners*（[https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)）中介绍的。它被训练用于预测800万个网页（总共40GB的文本），参数达到15亿个，是BERT的四倍多。以下是OpenAI关于GPT-2的说法：
- en: '*GPT-2 g**enerates coherent paragraphs of text, achieves state-of-the-art*
    *performance on many language modeling benchmarks,* *and performs rudimentary
    reading comprehension, machine translation,* *question answering, and summarization—all
    without task-specific training.*'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPT-2生成连贯的段落文本，在许多语言建模基准上取得了最先进的性能，并在基本阅读理解、机器翻译、问题回答和摘要等方面表现出色，所有这些都没有经过特定任务的训练。*'
- en: 'Initially, OpenAI said they would not release the dataset, code, or the full
    GPT-2 model weights. The reason for this was due to concerns about them being
    used to generate deceptive, biased, or abusive language at scale. Examples of
    some of the applications of these models for malicious purposes are as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，OpenAI表示他们不会发布数据集、代码或完整的GPT-2模型权重。这是因为他们担心这些内容会被用于大规模生成欺骗性、偏见性或滥用性语言。这些模型应用于恶意目的的示例如下：
- en: Realistic fake news articles
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逼真的假新闻文章
- en: Realistically impersonate others online
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线实际模仿其他人
- en: Abusive or faked content that could be posted on social media
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能发布在社交媒体上的滥用或伪造内容
- en: Automate the production of spam or phishing content
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动生产垃圾邮件或钓鱼内容
- en: The team then decided to do a staged release of the model in order to give people
    time to assess the societal implications and evaluate the impacts of release after
    each stage.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后团队决定分阶段发布模型，以便让人们有时间评估社会影响并在每个阶段发布后评估其影响。
- en: PyTorch implementations
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch实现
- en: 'There is a popular GitHub repository from the developer Hugging Face that has
    implemented BERT and GPT-2 in PyTorch. It can be found at the following web link:
    [https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT).
    The repository was first made available in November 2018 and permits the user
    to generate sentences from their own data. It also includes a variety of classes
    that can be used to test different models when applied to different tasks, such
    as question answering, token classification, and sequence classification.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个来自开发者Hugging Face的流行GitHub仓库，其中实现了基于PyTorch的BERT和GPT-2。可以在以下网址找到该仓库：[https://github.com/huggingface/pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT)。该仓库最早于2018年11月发布，并允许用户从自己的数据中生成句子。它还包括多种可用于测试不同模型在不同任务（如问题回答、标记分类和序列分类）中应用效果的类。
- en: 'The following code snippet demonstrates how the code from the GitHub repository
    can be used to generate text from the GPT-2 model. Firstly, we import the associated
    libraries and initialize the pretrained information as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段演示了如何从GitHub仓库中的代码使用GPT-2模型生成文本。首先，我们导入相关的库并初始化预训练信息如下：
- en: '[PRE50]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'In this example, we feed the model the `''We like unicorns because they''`
    sentence and it generates words from there like so:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将模型提供 `'We like unicorns because they'` 这个句子，然后它生成如下所示的词语：
- en: '[PRE51]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following is the output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/287e51be-3416-478c-91a1-83ef17291737.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/287e51be-3416-478c-91a1-83ef17291737.png)'
- en: GPT-2 playground
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-2游乐场
- en: 'There is another useful GitHub repository from developer ilopezfr, which can
    be found at the following link: [https://github.com/ilopezfr/gpt-2](https://github.com/ilopezfr/gpt-2).
    It also provides a notebook in Google Colab that allows the user to play around
    and experiment with the OpenAI GPT-2 model ([https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb](https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb)).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个有用的GitHub存储库来自开发者ilopezfr，可以在以下链接找到：[https://github.com/ilopezfr/gpt-2](https://github.com/ilopezfr/gpt-2)。它还提供了一个Google
    Colab的笔记本，允许用户与OpenAI GPT-2模型进行交互和实验（[https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb](https://colab.research.google.com/github/ilopezfr/gpt-2/blob/master/gpt-2-playground_.ipynb)）。
- en: 'The following are some examples from different sections of the playground:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是游乐场不同部分的一些示例：
- en: 'The **Text Completion** section:'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本完成**部分：'
- en: '![](img/13cc1515-9720-464e-8a64-a3ad0d5a4a74.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13cc1515-9720-464e-8a64-a3ad0d5a4a74.png)'
- en: 'The **Question-Answering** section:'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**部分：'
- en: '![](img/b0350b09-fe81-4365-9650-abe2f0181f09.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0350b09-fe81-4365-9650-abe2f0181f09.png)'
- en: 'The **Translation** section:'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译**部分：'
- en: '![](img/49c7641e-839d-4f53-8aef-8af2f70b7107.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49c7641e-839d-4f53-8aef-8af2f70b7107.png)'
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned different techniques to represent text data in deep
    learning. We learned how to use pretrained word embeddings and our own trained
    embeddings when working on a different domain. We built a text classifier using
    LSTMs and one-dimensional convolutions. We also learned about how to generate
    text using state-of-the-art language modeling architectures.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了不同的技术来表示深度学习中的文本数据。我们学习了如何在处理不同领域时使用预训练的词嵌入和我们自己训练的嵌入。我们使用LSTM和一维卷积构建了文本分类器。我们还了解了如何使用最先进的语言建模架构生成文本。
- en: In the next chapter, we will learn how to train deep learning algorithms to
    generate stylish images, and new images, and to generate text.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何训练深度学习算法来生成时尚图像、新图像，并生成文本。
