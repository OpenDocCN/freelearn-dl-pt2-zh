- en: Learning More about GANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关于GAN的学习
- en: We learned what **Generative Adversarial Networks** (**GANs**) are and how different
    types of GANs are used to generate images in [Chapter 8](b71eb1cb-af20-41ea-9e3d-26c7d0b956ba.xhtml),
    *Generating Images Using GANs*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了**生成对抗网络**（**GANs**）是什么，以及如何使用不同类型的GAN生成图像，参见[第8章](b71eb1cb-af20-41ea-9e3d-26c7d0b956ba.xhtml)，*使用GAN生成图像*。
- en: In this chapter, we will uncover various interesting different types of GANs.
    We've learned that GANs can be used to generate new images but we do not have
    any control over the images that they generate. For instance, if we want our GAN
    to generate a human face with specific traits how do we tell this information
    to the GAN? We can't because we have no control over the images generated by the
    generator.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将揭示各种有趣的不同类型的GAN。我们已经了解到GAN可以用来生成新的图像，但我们无法控制它们生成的图像。例如，如果我们希望我们的GAN生成具有特定特征的人脸，我们如何向GAN传达这些信息？我们做不到，因为我们无法控制生成器生成的图像。
- en: To resolve this, we use a new type of GAN called a **Conditional GAN** (**CGAN**)
    where we can condition the generator and discriminator by specifying what we want
    to generate. We will start off the chapter by comprehending how CGANs can be used
    to generate images of our interest and then we learn how to implement CGANs using
    **TensorFlow**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们使用一种称为**Conditional GAN**（**CGAN**）的新型GAN，可以通过指定我们要生成的内容来条件化生成器和判别器。我们将从理解CGAN如何生成我们感兴趣的图像开始本章，然后学习如何使用**TensorFlow**实现CGAN。
- en: We then understand about the **InfoGANs** which is an unsupervised version of
    a CGAN. We will understand what InfoGANs are and how they differ from CGANs, and
    how can we implement them using TensorFlow to generate new images.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接着我们了解**InfoGANs**，这是CGAN的无监督版本。我们将了解InfoGANs是什么，它们与CGAN有何不同，以及如何使用TensorFlow实现它们来生成新的图像。
- en: Then, we shall learn about **CycleGANs**, which are a very intriguing type of
    GAN. They try to learn the mapping from the distribution of images in one domain
    to the distribution of images in another domain. For instance, to convert a grayscale
    image to a colored image, we train the CycleGAN to learn the mapping between grayscale
    and colored images, which means they learn to map from one domain, to another
    and the best part is, unlike other architectures, they even don't require a paired
    dataset. We will investigate how exactly they learn these mappings and their architecture
    in detail. We will explore how to implement CycleGAN to convert real pictures
    to paintings.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习关于**CycleGANs**的内容，这是一种非常有趣的GAN类型。它们试图学习从一个域中图像的分布到另一个域中图像的映射。例如，将灰度图像转换为彩色图像，我们训练CycleGAN学习灰度图像和彩色图像之间的映射，这意味着它们学会了从一个域映射到另一个域，最好的部分是，与其他架构不同，它们甚至不需要成对的数据集。我们将深入探讨它们如何学习这些映射以及它们的架构细节。我们将探索如何实现CycleGAN以将真实图片转换为绘画作品。
- en: At the end of the chapter, we will explore, **StackGAN**, which can convert
    the text description to a photo-realistic image. We will perceive how StackGANs
    do this by gaining a deeper understanding of their architecture in detail.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将探索**StackGAN**，它可以将文本描述转换为逼真的图片。我们将通过深入理解其架构细节来理解StackGAN如何实现这一点。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: Conditional GANs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conditional GANs
- en: Generating specific digits using CGAN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CGAN生成特定数字
- en: InfoGAN
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InfoGAN
- en: Architecture of InfoGAN
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InfoGAN的架构
- en: Constructing InfoGAN using TensorFlow
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建InfoGAN
- en: CycleGAN
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN
- en: Converting pictures to paintings using CycleGAN
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CycleGAN将图片转换为绘画
- en: StackGAN
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StackGAN
- en: Conditional GANs
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Conditional GANs
- en: We know that the generator generates new images by learning the real data distribution,
    while the discriminator examines whether the image generated by the generator
    is from the real data distribution or fake data distribution.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器通过学习真实数据分布生成新的图像，而鉴别器则检查生成器生成的图像是来自真实数据分布还是伪数据分布。
- en: However, the generator has the capability to generate new and interesting images
    by learning the real data distribution. We have no control or influence over the
    images generated by the generator. For instance, let's say our generator is generating
    human faces; how can we tell the generator to generate a human face with certain
    features, say big eyes and a sharp nose?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，生成器有能力通过学习真实数据分布生成新颖有趣的图像。我们无法控制或影响生成器生成的图像。例如，假设我们的生成器正在生成人脸图像；我们如何告诉生成器生成具有某些特征的人脸，比如大眼睛和尖鼻子？
- en: We can't! Because we have no control over the images that are being generated
    by the generator.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能！因为我们无法控制生成器生成的图像。
- en: To overcome this, we introduce a small variant of a GAN called a **CGAN**, which
    imposes a condition to both the generator and the discriminator. This condition
    tells the GAN that what image we want our generator to generate. So, both of our
    components—the discriminator and the generator—act upon this condition.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一点，我们引入了 GAN 的一个小变体称为 **CGAN**，它对生成器和鉴别器都施加了一个条件。这个条件告诉 GAN 我们希望生成器生成什么样的图像。因此，我们的两个组件——鉴别器和生成器——都会根据这个条件进行操作。
- en: Let's consider a simple example. Say we are generating handwritten digits using
    CGAN with the MNIST dataset. Let's assume that we are more focused on generating
    digit 7 instead of other digits. Now, we need to impose this condition to both
    of our generators and discriminators. How do we do that?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子。假设我们正在使用 MNIST 数据集和 CGAN 生成手写数字。我们假设我们更专注于生成数字 7 而不是其他数字。现在，我们需要将这个条件强加给我们的生成器和鉴别器。我们如何做到这一点？
- en: The generator takes the noise ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)
    as an input and generates an image. But along with ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png),
    we also pass an additional input, ![](img/92bb425b-a16a-4c8a-a295-243ecfdb386b.png).
    This ![](img/9f1aaca1-ad1d-4079-9a27-8f12401d51fe.png) is a one-hot encoded class
    label. As we are interested in generating digit 7, we set the seventh index to
    1 and set all other indices to 0, that is, [0,0,0,0,0,0,0,1,0,0].
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器以噪声 ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png) 作为输入，并生成一幅图像。但除了 ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)
    外，我们还传入了额外的输入，即 ![](img/92bb425b-a16a-4c8a-a295-243ecfdb386b.png)。这个 ![](img/9f1aaca1-ad1d-4079-9a27-8f12401d51fe.png)
    是一个独热编码的类标签。由于我们希望生成数字 7，我们将第七个索引设置为 1，其余索引设置为 0，即 [0,0,0,0,0,0,0,1,0,0]。
- en: We concatenate the latent vector, ![](img/35a24ed1-1817-4456-ae54-3ab000857bcc.png),
    and the one-hot encoded conditional variable, ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png),
    and pass that as an input to the generator. Then, the generator starts generating
    the digit 7.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将潜在向量 ![](img/35a24ed1-1817-4456-ae54-3ab000857bcc.png) 和独热编码的条件变量 ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)
    连接起来，并将其作为输入传递给生成器。然后，生成器开始生成数字 7。
- en: What about the discriminator? We know that the discriminator takes image ![](img/b867ddbc-f08c-4126-b54a-2141fa745659.png)
    as an input and tells us whether the image is a real or fake image. In CGAN, We
    want the discriminator to discriminate based on the condition, which means it
    has to identify whether the generated image is a real digit 7 or a fake digit
    7\. So, along with passing input ![](img/e7df7672-8219-4515-8446-49fa00b78885.png),
    we also pass the conditional variable ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)
    to the discriminator by concatenating ![](img/cb3b4dc9-80cf-4374-8cdb-a5e075850686.png)
    and ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器呢？我们知道鉴别器以图像 ![](img/b867ddbc-f08c-4126-b54a-2141fa745659.png) 作为输入，并告诉我们图像是真实的还是伪造的。在
    CGAN 中，我们希望鉴别器基于条件进行鉴别，这意味着它必须判断生成的图像是真实的数字 7 还是伪造的数字 7。因此，除了传入输入 ![](img/e7df7672-8219-4515-8446-49fa00b78885.png)
    外，我们还通过连接 ![](img/cb3b4dc9-80cf-4374-8cdb-a5e075850686.png) 和 ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)
    将条件变量 ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png) 传递给鉴别器。
- en: 'As you can see in the following figure, we are passing ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)
    and ![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png) to the generator:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在以下图中所看到的，我们正在传入生成器 ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png) 和 ![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png)：
- en: '![](img/d46bd418-6cb7-416f-acb2-42e463616a17.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d46bd418-6cb7-416f-acb2-42e463616a17.png)'
- en: The generator is conditioned on information imposed on **![](img/cd509d39-205f-4d4d-ad26-bc085d848f0b.png)**.
    Similarly, along with passing real and fake images to the discriminator, we also
    pass ![](img/4fcaa738-2b08-4797-864f-5b47e6446731.png) to the discriminator. So,
    the generator generates digit 7 and the discriminator learns to discriminate between
    the real 7 and the fake 7.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是基于 **![](img/cd509d39-205f-4d4d-ad26-bc085d848f0b.png)** 引入的信息条件。类似地，除了将真实和伪造图像传递给鉴别器外，我们还向鉴别器传递了
    ![](img/4fcaa738-2b08-4797-864f-5b47e6446731.png)。因此，生成器生成数字 7，而鉴别器学会区分真实的 7 和伪造的
    7。
- en: We've just learned how to generate a specific digit using CGAN, and yet the
    applications of CGAN do not end here. Assume we need to generate a digit with
    a specific width and height. We can also impose this condition on ![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png)
    and make the GAN to generate any desired image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了如何使用CGAN生成特定数字，但是CGAN的应用并不仅限于此。假设我们需要生成具有特定宽度和高度的数字。我们也可以将这个条件加到![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png)，并让GAN生成任何期望的图像。
- en: Loss function of CGAN
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CGAN的损失函数
- en: As you may have noticed, there is not much difference between our vanilla GAN
    and CGAN except that in CGANs, we are concatenating the additional input, which
    is the conditioning variable ![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png)
    with the inputs of the generator and the discriminator. So, the loss function
    for both generator and discriminator is the same as the vanilla GAN with the exception
    that it is conditioned on ![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的那样，我们的普通GAN和CGAN之间没有太大区别，只是在CGAN中，我们将额外输入（即条件变量![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png)）与生成器和鉴别器的输入连接在一起。因此，生成器和鉴别器的损失函数与普通GAN相同，唯一的区别是它是有条件的![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png)。
- en: 'Thus, the loss function of the discriminator is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，鉴别器的损失函数如下所示：
- en: '![](img/e6a23fb2-a1aa-4d68-a8de-abf2f794ee16.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6a23fb2-a1aa-4d68-a8de-abf2f794ee16.png)'
- en: 'The loss function of the generator is given as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失函数如下所示：
- en: '![](img/3daefbdb-8cb3-4431-924b-2856bf8d53ad.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3daefbdb-8cb3-4431-924b-2856bf8d53ad.png)'
- en: The CGAN learns by minimizing the loss function using gradient descent.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降最小化损失函数来学习CGAN。
- en: Generating specific handwritten digits using CGAN
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CGAN生成特定手写数字
- en: We just learned how CGAN works and the architecture of CGAN. To strengthen our
    understanding, now we will learn how to implement CGAN in TensorFlow for generating
    an image of specific handwritten digit, say digit 7.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了CGAN的工作原理和结构。为了加强我们的理解，现在我们将学习如何在TensorFlow中实现CGAN，以生成特定手写数字的图像，比如数字7。
- en: 'First, Load the required libraries:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载所需的库：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the MNIST dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 加载MNIST数据集：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Defining the generator
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器
- en: 'Generator, *G,* takes the noise, ![](img/3fba73ea-bd4f-4352-8535-f23286d101b8.png),
    and also the conditional variable, ![](img/2076de9c-04e1-47b2-93f1-8d9cd520e9a0.png),
    as an input and returns an image. We define the generator as a simple two- layer
    feed-forward network:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器*G*接受噪声，![](img/3fba73ea-bd4f-4352-8535-f23286d101b8.png)，以及条件变量，![](img/2076de9c-04e1-47b2-93f1-8d9cd520e9a0.png)，作为输入，并返回图像。我们将生成器定义为简单的两层前馈网络：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize the weights:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化权重：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Concatenate the noise, ![](img/c017b2d2-3c5d-4fe5-a6de-29cf18af21b0.png), and
    the conditional variable, ![](img/c5a5b62a-728c-4a2b-a938-24c2b58fbb0c.png):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 连接噪声，![](img/c017b2d2-3c5d-4fe5-a6de-29cf18af21b0.png)，和条件变量，![](img/c5a5b62a-728c-4a2b-a938-24c2b58fbb0c.png)：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the first layer:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一层：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the second layer and compute the output with the `tanh` activation function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二层，并使用`tanh`激活函数计算输出：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Defining discriminator
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义鉴别器
- en: 'We know that the discriminator, ![](img/92dd0d61-61e9-4979-bef8-5fa25b08d846.png),
    returns the probability; that is, it will tell us the probability of the given
    image being real. Along with the input image, ![](img/21e9ebf0-fbab-4938-9890-29594707d2ba.png),
    it also takes the conditional variable, ![](img/c9b15d4e-d63d-4776-bbce-9bf5d1544b81.png),
    as an input. We define the discriminator also as a simple two-layer feed-forward
    network:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道鉴别器，![](img/92dd0d61-61e9-4979-bef8-5fa25b08d846.png)，返回概率；也就是说，它会告诉我们给定图像为真实图像的概率。除了输入图像，![](img/21e9ebf0-fbab-4938-9890-29594707d2ba.png)，它还将条件变量，![](img/c9b15d4e-d63d-4776-bbce-9bf5d1544b81.png)，作为输入。我们将鉴别器定义为简单的两层前馈网络：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Initialize the weights:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化权重：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Concatenate input, ![](img/6b91c2b8-d0ad-4fdd-93c7-2027143d3a78.png) and the
    conditional variable, ![](img/f16638c8-eebc-417a-8500-983bb5984248.png):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 连接输入，![](img/6b91c2b8-d0ad-4fdd-93c7-2027143d3a78.png) 和条件变量，![](img/f16638c8-eebc-417a-8500-983bb5984248.png)：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the first layer:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一层：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the second layer and compute the output with the `sigmoid` activation
    function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二层，并使用`sigmoid`激活函数计算输出：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the placeholder for the input, ![](img/6b8eef0e-1ec8-4195-8e3b-acb7878fc080.png),
    conditional variable, ![](img/03fe94eb-54c9-4a6c-ac36-6613626a2b9d.png), and the
    noise, ![](img/aa946952-416d-46e4-8f61-b4525a754412.png):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入占位符，![](img/6b8eef0e-1ec8-4195-8e3b-acb7878fc080.png)，条件变量，![](img/03fe94eb-54c9-4a6c-ac36-6613626a2b9d.png)，和噪声，![](img/aa946952-416d-46e4-8f61-b4525a754412.png)：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Start the GAN!
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动GAN！
- en: 'First, we feed the noise, ![](img/7a9ce465-36be-4928-a62e-93fad76038e5.png)
    and the conditional variable, ![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png),
    to the generator, and it will output the fake image, that is, ![](img/718a88c0-6582-4fe1-bd16-212f9204db72.png):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将噪声，![](img/7a9ce465-36be-4928-a62e-93fad76038e5.png) 和条件变量，![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png)，输入到生成器中，它将输出伪造的图像，即，![](img/718a88c0-6582-4fe1-bd16-212f9204db72.png)：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we feed the real image ![](img/f8310e0e-b680-4cf7-862f-050b010071da.png)
    along with conditional variable, ![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png),
    to the discriminator, ![](img/ce7b8ed9-614f-4ddc-b414-da102cdc0e64.png), and get
    the probability of them being real:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将真实图像![](img/f8310e0e-b680-4cf7-862f-050b010071da.png)一同与条件变量，![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png)，输入到鉴别器，
    ![](img/ce7b8ed9-614f-4ddc-b414-da102cdc0e64.png)，并得到它们是真实的概率：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, we feed the fake image, `fake_x`, and the conditional variable,
    ![](img/6adb17cf-a37e-4c68-9ecf-c9da309fa5fb.png), to the discriminator, ![](img/c4d9de79-ac6a-45f8-afa6-c46ffd2933ab.png),
    and get the probability of them being real:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将伪造的图像，`fake_x` 和条件变量，![](img/6adb17cf-a37e-4c68-9ecf-c9da309fa5fb.png)，输入到鉴别器，
    ![](img/c4d9de79-ac6a-45f8-afa6-c46ffd2933ab.png)，并得到它们是真实的概率：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Computing the loss function
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失函数
- en: Now we will see how to compute the loss function. It is essentially the same
    as the Vanilla GAN except that we add a conditional variable.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看如何计算损失函数。它与普通GAN基本相同，只是我们添加了一个条件变量。
- en: Discriminator loss
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鉴别器损失
- en: 'The discriminator loss is given as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器损失如下所示：
- en: '![](img/6c621f81-f100-4980-9d7a-7d9012be17e1.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c621f81-f100-4980-9d7a-7d9012be17e1.png)'
- en: 'First, we will implement the first term, that is, ![](img/c5c53cc8-1af4-4ec4-b1df-779b67a80b69.png):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现第一项，即，![](img/c5c53cc8-1af4-4ec4-b1df-779b67a80b69.png)：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we will implement the second term, ![](img/4c06f526-38fe-4aeb-9332-e3ace5521540.png):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现第二项，![](img/4c06f526-38fe-4aeb-9332-e3ace5521540.png)：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The final loss can be written as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失可以写为：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Generator loss
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器损失
- en: 'The generator loss is given as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失如下所示：
- en: '![](img/b294fdb0-97f2-463a-82aa-cf250794a4bf.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b294fdb0-97f2-463a-82aa-cf250794a4bf.png)'
- en: 'Generator loss can be implemented as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失可以实现如下：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Optimizing the loss
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化损失
- en: 'We need to optimize our generator and discriminator. So, we collect the parameters
    of the discriminator and generator as `theta_D` and `theta_G` respectively:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要优化我们的生成器和鉴别器。因此，我们将鉴别器和生成器的参数分别收集为`theta_D`和`theta_G`：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Optimize the loss using the Adam optimizer:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Adam优化器优化损失：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Start training the CGAN
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始训练CGAN
- en: 'Start the TensorFlow session and initialize the variables:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 开始TensorFlow会话并初始化变量：
- en: '[PRE22]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Define the `batch_size:`
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 定义`batch_size:`
- en: '[PRE23]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the number of epochs and the number of classes:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 定义迭代次数和类别数：
- en: '[PRE24]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the images and labels:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 定义图像和标签：
- en: '[PRE25]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Generate the handwritten digit, 7
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成手写数字7
- en: 'We set the digit (`label`) to generate as `7`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要生成的数字（`标签`）设置为`7`：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Set the number of iterations:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 设置迭代次数：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Sample images based on the batch size:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于批处理大小采样图像：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Sample the condition that is, digit we want to generate:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 随机采样条件，即我们要生成的数字：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Sample noise:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 采样噪声：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Train the generator and compute the generator loss:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练生成器并计算生成器损失：
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Train the discriminator and compute the discriminator loss:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 训练鉴别器并计算鉴别器损失：
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Randomly sample noise:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 随机采样噪声：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Select the digit we want to generate:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 选择我们想要生成的数字：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Convert the selected digit into a one-hot encoded vector:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 将选择的数字转换为一个独热编码向量：
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Feed the noise and one hot encoded condition to the generator and generate
    the fake image:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将噪声和独热编码条件输入到生成器中并生成伪造图像：
- en: '[PRE36]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Print the loss of generator and discriminator and plot the generator image:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 打印生成器和鉴别器的损失并绘制生成器图像：
- en: '[PRE37]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As you can see following plot, the generator has now learned to generate the
    digit 7 instead of generating other digits randomly:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，生成器现在已经学会生成数字7，而不是随机生成其他数字：
- en: '![](img/4816e841-0f8b-4f5c-829a-c9c066a3bca1.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4816e841-0f8b-4f5c-829a-c9c066a3bca1.png)'
- en: Understanding InfoGAN
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解InfoGAN
- en: InfoGAN is an unsupervised version of CGAN. In CGAN, we learned how to condition
    the generator and discriminator to generate the image we want. But how can we
    do that when we have no labels in the dataset? Assume we have an MNIST dataset
    with no labels; how can we tell the generator to generate the specific image that
    we are interested in? Since the dataset is unlabeled, we do not even know about
    the classes present in the dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN是CGAN的无监督版本。在CGAN中，我们学习如何条件生成器和判别器以生成我们想要的图像。但是当数据集中没有标签时，我们如何做到这一点？假设我们有一个没有标签的MNIST数据集，我们如何告诉生成器生成我们感兴趣的特定图像？由于数据集是无标签的，我们甚至不知道数据集中存在哪些类别。
- en: We know that generators use noise *z* as an input and generate the image. Generators
    encapsulate all the necessary information about the image in the *z* and it is
    called **entangled representation**. It is basically learning the semantic representation
    of the image in *z*. If we can disentangle this vector, then we can discover interesting
    features of our image.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器使用噪声*z*作为输入并生成图像。生成器在*z*中封装了关于图像的所有必要信息，这被称为**纠缠表示**。它基本上学习了*z*中图像的语义表示。如果我们能解开这个向量，我们就可以发现图像的有趣特征。
- en: 'So, we will split this *z* into two:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将把*z*分为两部分：
- en: Usual noise
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规噪声
- en: Code *c*
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码*c*
- en: What is the code? The code *c* is basically interpretable disentangled information.
    Assuming we have MNIST data, then, code *c1* implies the digit label, code *c2*
    implies the width, *c3* implies the stroke of the digit, and so on. We collectively
    represent them with the term *c*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是代码？代码*c*基本上是可解释的分离信息。假设我们有MNIST数据，那么代码*c1*暗示了数字标签，代码*c2*暗示了数字的宽度，*c3*暗示了数字的笔画，依此类推。我们用术语*c*来统称它们。
- en: Now that we have *z* and *c*, how can we learn meaningful code *c*? Can we learn
    meaningful code with the image generated from the generator? Say a generator generates
    the image of 7\. Now we can say code *c1 is 7* as we know c1 implies the digit
    label.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有*z*和*c*，我们如何学习有意义的代码*c*？我们能从生成器生成的图像学习有意义的代码吗？假设生成器生成了数字7的图像。现在我们可以说代码*c1*是7，因为我们知道*c1*暗示了数字标签。
- en: But since code can mean anything, say, a label, a width of the digit, stroke,
    rotation angle, and so on—how can we learn what we want? The code *c* will be
    learned based on the choice of the prior. For instance, if we chose a multinomial
    prior for *c*, then our InfoGAN might assign a digit label for *c*. Say, we assign
    a Gaussian prior, then it might assign a rotation angle, and so on. We can also
    have more than one prior.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于代码可以意味着任何东西，比如标签、数字的宽度、笔画、旋转角度等等，我们如何学习我们想要的东西？代码*c*将根据先验选择进行学习。例如，如果我们为*c*选择了一个多项先验，那么我们的InfoGAN可能会为*c*分配一个数字标签。假设我们选择了一个高斯先验，那么它可能会分配一个旋转角度等等。我们也可以有多个先验。
- en: The distribution for prior *c* can be anything. InfoGAN assigns different properties
    according to the distribution. In InfoGAN, the code *c* is inferred automatically
    based on the generator output, unlike CGAN, where we explicitly specify the *c*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 先验*c*的分布可以是任何形式。InfoGAN根据分布分配不同的属性。在InfoGAN中，代码*c*是根据生成器输出自动推断的，不像CGAN那样我们需要显式指定*c*。
- en: In a nutshell, we are inferring ![](img/0c7149c6-3be8-4775-b309-45e3be1b8545.png)
    based on the generator output, ![](img/90798245-af2f-48c9-97b5-4f0e5a14b3e3.png).
    But how exactly we are inferring ![](img/4191b48d-7b3c-4407-88fa-23a8fa0dbbda.png)?
    We use a concept from information theory called **mutual information**.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们基于生成器输出推断![](img/0c7149c6-3be8-4775-b309-45e3be1b8545.png)，![](img/90798245-af2f-48c9-97b5-4f0e5a14b3e3.png)。但我们究竟是如何推断![](img/4191b48d-7b3c-4407-88fa-23a8fa0dbbda.png)的呢？我们使用信息理论中的一个概念，称为**互信息**。
- en: Mutual information
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互信息
- en: 'Mutual information between two random variables tells us the amount of information
    we can obtain from one random variable through another. Mutual information between
    two random variables *x* and *y* can be given as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 两个随机变量之间的互信息告诉我们可以通过一个随机变量获取另一个随机变量的信息量。两个随机变量*x*和*y*之间的互信息可以表示如下：
- en: '![](img/cd64a09b-fa5f-407f-b4e1-a0b07db0e49a.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd64a09b-fa5f-407f-b4e1-a0b07db0e49a.png)'
- en: It is basically the difference between the entropy of *y* and the conditional
    entropy of *y* given *x*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上是*y*的熵与给定*x*的条件熵之间的差异。
- en: Mutual information between code ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    and the generator output ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) tells
    us how much information we can obtain about ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    through ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) . If the mutual information
    *c* and ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) is high, then we can
    say knowing the generator output helps us to infer *c*. But if the mutual information
    is low, then we cannot infer *c* from the generator output. Our goal is to maximize
    the mutual information.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png) 和生成器输出 ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)
    之间的互信息告诉我们通过 ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) 我们可以获取多少关于 ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    的信息。如果互信息 *c* 和 ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) 很高，那么我们可以说知道生成器输出有助于推断
    *c*。但如果互信息很低，则无法从生成器输出推断 *c*。我们的目标是最大化互信息。
- en: 'The mutual information between code ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    and the generator output, ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png), can
    be given as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 代码 ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png) 和生成器输出 ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)
    之间的互信息可以表示为：
- en: '![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)'
- en: 'Let''s look at the elements of the formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看公式的元素：
- en: '![](img/77b4d2e1-3669-4218-9056-fcdd47af1e82.png) is the entropy of the code'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/77b4d2e1-3669-4218-9056-fcdd47af1e82.png) 是代码的熵'
- en: '![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png) is the conditional entropy
    of the code c given the generator output ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png) 是给定生成器输出 ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)
    条件下代码 *c* 的条件熵。'
- en: 'But the problem is, how do we compute ![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png)?
    Because to compute this value, we need to know the posterior, ![](img/ea74a871-385f-4fd1-a422-29c2dbe4076b.png),
    which we don''t know yet. So, we estimate the posterior with the auxiliary distribution,
    ![](img/8197d00c-71c7-43b0-bb9f-bde673eeb1d2.png):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题是，我们如何计算 ![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png)？因为要计算这个值，我们需要知道后验分布
    ![](img/ea74a871-385f-4fd1-a422-29c2dbe4076b.png)，而这是我们目前不知道的。因此，我们用辅助分布 ![](img/8197d00c-71c7-43b0-bb9f-bde673eeb1d2.png)
    来估计后验分布：
- en: '![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)'
- en: 'Let''s say ![](img/7cf8508c-5680-44f2-9436-4809e8b458dd.png), then we can deduce
    mutual information as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/7cf8508c-5680-44f2-9436-4809e8b458dd.png)，那么我们可以推导出互信息如下：
- en: '![](img/77307ccf-295e-499c-8c4f-3413deffc80f.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77307ccf-295e-499c-8c4f-3413deffc80f.png)'
- en: 'Thus, we can say:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说：
- en: '![](img/504b15ff-8dbe-4e7b-8f33-f85da19dc791.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/504b15ff-8dbe-4e7b-8f33-f85da19dc791.png)'
- en: Maximizing mutual information, ![](img/04461b06-9679-4dfd-8a03-3a8c0dbce1b7.png)
    basically implies we are maximizing our knowledge about *c* given the generated
    output, that is, knowing about one variable through another.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化互信息，![](img/04461b06-9679-4dfd-8a03-3a8c0dbce1b7.png) 基本上意味着我们在生成输出中最大化了关于
    *c* 的知识，也就是通过另一个变量了解一个变量。
- en: Architecture of the InfoGAN
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InfoGAN 的架构
- en: 'Okay. What is really going on here? why are we doing this? To put it in simple
    terms, we split the input to the generator into two: *z* and *c*. Since both *z*
    and *c* are used for generating the image, they capture the semantic meaning of
    the image. The code *c* gives us the interpretable disentangled information about
    the image. So we try to find *c* given the generator output. However, we can''t
    do this easily since we don''t know the posterior, ![](img/1a09fb91-ad1e-4640-9ee1-7858444eeb75.png),
    so we use an auxiliary distribution, ![](img/59e766d6-4103-4d50-a928-6a24384de828.png),
    to learn *c*.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。这里到底发生了什么？为什么我们要这样做？简单地说，我们把输入分成了两部分：*z* 和 *c*。因为 *z* 和 *c* 都用于生成图像，它们捕捉了图像的语义含义。代码
    *c* 给出了我们关于图像的可解释解耦信息。因此，我们试图在生成器输出中找到 *c*。然而，我们不能轻易地做到这一点，因为我们还不知道后验分布 ![](img/1a09fb91-ad1e-4640-9ee1-7858444eeb75.png)，所以我们使用辅助分布
    ![](img/59e766d6-4103-4d50-a928-6a24384de828.png) 来学习 *c*。
- en: This auxiliary distribution is basically another neural network; let's call
    this network as *Q* network. The role of the *Q* network is to predict the likelihood
    of *c* given a generator image *x* and is given by ![](img/8bdc985a-0401-4abe-88ab-4a6af6ac2097.png).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个辅助分布基本上是另一个神经网络；让我们称这个网络为 *Q* 网络。*Q* 网络的作用是预测给定生成器图像 *x* 的代码 *c* 的可能性，表示为
    ![](img/8bdc985a-0401-4abe-88ab-4a6af6ac2097.png)。
- en: First, we sample *c* from a prior, *p(c)*. Then we concatenate *c* and *z* and
    feed them to the generator. Next, we feed the generator result given by ![](img/36423d4e-7ade-4f6e-87b3-b71551f5b61c.png)
    to the discriminator. We know that the role of the discriminator is to output
    the probability of the given image being real. So, it takes the image generated
    by the generator and returns the probability. Also, the *Q* network takes the
    generated image and returns the estimates of *c* given the generated image.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从先验分布 *p(c)* 中采样 *c*。然后，我们将 *c* 和 *z* 连接起来，并将它们输入生成器。接下来，我们将由生成器给出的结果 ![](img/36423d4e-7ade-4f6e-87b3-b71551f5b61c.png)
    输入鉴别器。我们知道鉴别器的作用是输出给定图像为真实图像的概率。此外，*Q* 网络接收生成的图像，并返回给定生成图像的 *c* 的估计。
- en: 'Both the discriminator *D* and *Q* networks take the generator image and return
    the output so they both share some layers. Since they both share some layers,
    we attach the *Q* network to the discriminator, as shown in the following diagram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器 *D* 和 *Q* 网络都接受生成器图像并返回输出，因此它们共享一些层。由于它们共享一些层，我们将 *Q* 网络附加到鉴别器上，如下图所示：
- en: '![](img/cbc47881-40df-4e43-a508-96ffd57dfa4f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbc47881-40df-4e43-a508-96ffd57dfa4f.png)'
- en: 'Thus, the discriminator returns two outputs:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，鉴别器返回两个输出：
- en: The probability of the image being real
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像为真实图像的概率
- en: The estimates of *c,* which is the probability of *c* given the generator image
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* 的估计，即给定生成器图像的 *c* 的概率'
- en: We add the mutual information term to our loss function.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将互信息项添加到我们的损失函数中。
- en: 'Thus, the loss function of the discriminator is given as:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，鉴别器的损失函数定义为：
- en: '![](img/2a5568d6-8ba3-495a-a04d-3adf366f9206.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a5568d6-8ba3-495a-a04d-3adf366f9206.png)'
- en: 'The loss function of the generator is given as:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失函数定义为：
- en: '![](img/0c69ce3b-afea-432e-a4b8-e52ac1ee4a26.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c69ce3b-afea-432e-a4b8-e52ac1ee4a26.png)'
- en: Both of the preceding equations implies we are minimizing the loss of GAN along
    with maximizing the mutual information. Still confused about InfoGANs? Don't worry!
    We will learn about InfoGANs step by step better by implementing them in TensorFlow.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个方程表明我们正在最小化GAN的损失，并同时最大化互信息。对InfoGAN还有疑惑？别担心！我们将通过在TensorFlow中逐步实现它们来更好地学习InfoGAN。
- en: Constructing an InfoGAN in TensorFlow
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中构建InfoGAN：
- en: We will better understand InfoGANs by implementing them in TensorFlow step by
    step. We will use the MNIST dataset and learn how the InfoGAN infers the code
    ![](img/285c7e36-2c3b-451b-9d4c-ec3e14821693.png) automatically based on the generator
    output. We build an Info-DCGAN; that is, we use convolutional layers in the generator
    and discriminator instead of a vanilla neural network.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在TensorFlow中逐步实现InfoGAN来更好地理解它们。我们将使用MNIST数据集，并学习InfoGAN如何基于生成器输出自动推断出代码
    ![](img/285c7e36-2c3b-451b-9d4c-ec3e14821693.png)。我们构建一个Info-DCGAN；即，在生成器和鉴别器中使用卷积层而不是简单的神经网络。
- en: 'First, we will import all the necessary libraries:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所有必要的库：
- en: '[PRE38]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the MNIST dataset:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 加载MNIST数据集：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the leaky ReLU activation function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 定义泄漏ReLU激活函数：
- en: '[PRE40]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Defining generator
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器：
- en: 'Generator ![](img/be85bd9d-7482-49d4-a48e-ec8024aeea1c.png) which takes the
    noise, ![](img/315bac8e-4528-42d4-b2db-971178e21db7.png), and also a variable,
    ![](img/8562e754-dcfe-4432-b70f-d9825a801f78.png), as an input and returns an
    image. Instead of using a fully connected layer in the generator, we use a deconvolutional
    network, just like when we studied DCGANs:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器 ![](img/be85bd9d-7482-49d4-a48e-ec8024aeea1c.png)，它接受噪声 ![](img/315bac8e-4528-42d4-b2db-971178e21db7.png)
    和变量 ![](img/8562e754-dcfe-4432-b70f-d9825a801f78.png) 作为输入，并返回一个图像。与在生成器中使用全连接层不同，我们使用了一个反卷积网络，就像我们学习DCGAN时一样：
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'First, concatenate the noise, *z,* and the variable, ![](img/e6c9289c-bc98-4d2e-9085-3d0dad48a5b8.png):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，连接噪声 *z* 和变量 ![](img/e6c9289c-bc98-4d2e-9085-3d0dad48a5b8.png)：
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the first layer, which is a fully connected layer with batch normalization
    and ReLU activations:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一层，这是一个包括批归一化和ReLU激活的全连接层：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the second layer, which is also fully connected with batch normalization
    and ReLU activations:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第二层，它也是全连接层，包括批归一化和ReLU激活：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Flatten the result of the second layer:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 展平第二层的结果：
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The third layer consists of **deconvolution**; that is, a transpose convolution
    operation and it is followed by batch normalization and ReLU activations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第三层是**反卷积**，即转置卷积操作，紧随其后是批归一化和ReLU激活：
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The fourth layer is another transpose convolution operation:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第四层是另一个转置卷积操作：
- en: '[PRE47]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Apply sigmoid function to the result of the fourth layer and get the output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对第四层的结果应用 sigmoid 函数并获得输出：
- en: '[PRE48]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Defining the discriminator
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义鉴别器
- en: 'We learned that both the discriminator ![](img/ef5d0704-9421-4960-99c5-7a189b57d943.png)
    and *Q* network take the generator image and return the output so they both share
    some layers. Since they both share some layers, we attach the *Q* network to the
    discriminator, as we learned in the architecture of InfoGAN. Instead of using
    fully connected layers in the discriminator, we use a convolutional network, as
    we learned in the discriminator of DCGAN:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到鉴别器 ![](img/ef5d0704-9421-4960-99c5-7a189b57d943.png) 和 *Q* 网络都接受生成器图像并返回输出，因此它们共享一些层。由于它们共享一些层，我们像在
    InfoGAN 的架构中学到的那样，将 *Q* 网络附加到鉴别器上。在鉴别器中，我们使用卷积网络，而不是全连接层，正如我们在 DCGAN 的鉴别器中学到的：
- en: '[PRE49]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define the first layer, which performs the convolution operation followed by
    a leaky ReLU activation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第一层，执行卷积操作，随后是泄漏 ReLU 激活：
- en: '[PRE50]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We also perform convolution operation in the second layer, which is followed
    by batch normalization and a leaky ReLU activation:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二层中，我们还执行卷积操作，随后进行批归一化和泄漏 ReLU 激活：
- en: '[PRE51]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Flatten the result of the second layer:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 展平第二层的结果：
- en: '[PRE52]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Feed the flattened result to a fully connected layer which is the third layer
    and it is followed by batch normalization and leaky ReLU activation:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 将展平的结果馈送到全连接层，这是第三层，随后进行批归一化和泄漏 ReLU 激活：
- en: '[PRE53]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Compute the discriminator output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 计算鉴别器输出：
- en: '[PRE54]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As we learned that we attach the *Q* network to the discriminator. Define the
    first layer of the *Q* network that takes the final layer of the discriminator
    as inputs:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们学到的，我们将 *Q* 网络附加到鉴别器。定义 *Q* 网络的第一层，它以鉴别器的最终层作为输入：
- en: '[PRE55]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Define the second layer of the *Q* network:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 *Q* 网络的第二层：
- en: '[PRE56]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Estimate *c*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 估计 *c*：
- en: '[PRE57]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Return the discriminator `logits` and the estimated *c* value as output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 返回鉴别器 `logits` 和估计的 *c* 值作为输出：
- en: '[PRE58]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Define the input placeholders
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义输入占位符
- en: 'Now we define the placeholder for the input, ![](img/1198ce65-9257-470f-9ebe-fa1dd5af2b41.png),
    the noise, ![](img/47af0b1c-6f7b-4b5e-8963-0f17cbdcca52.png), and the code, ![](img/7f206bce-181b-4732-a826-72a1001e9921.png):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们为输入 ![](img/1198ce65-9257-470f-9ebe-fa1dd5af2b41.png)、噪声 ![](img/47af0b1c-6f7b-4b5e-8963-0f17cbdcca52.png)
    和代码 ![](img/7f206bce-181b-4732-a826-72a1001e9921.png) 定义占位符：
- en: '[PRE59]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Start the GAN
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动 GAN
- en: 'First, we feed the noise, ![](img/541c0403-7faf-4e90-904f-3fabb2287695.png)
    and the code, ![](img/19dc097f-0ef5-4373-9c71-2c13948ea07a.png) to the generator,
    and it will output the fake image according to the equation ![](img/d4bb1f15-c827-442a-b4a3-2e2706b728a9.png):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将噪声 ![](img/541c0403-7faf-4e90-904f-3fabb2287695.png) 和代码 ![](img/19dc097f-0ef5-4373-9c71-2c13948ea07a.png)
    提供给生成器，它将根据方程输出假图像 ![](img/d4bb1f15-c827-442a-b4a3-2e2706b728a9.png)：
- en: '[PRE60]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now we feed the real image, ![](img/c3d7298c-8e7f-4dec-a5ff-4c36a29078c8.png),
    to the discriminator, ![](img/19be0dee-d6ea-4c21-a901-3acd2320a789.png), and get
    the probability that the image being real. Along with this, we also obtain the
    estimate of ![](img/235ed536-2d0f-48d3-88b9-3013d10a46d1.png) for the real image:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将真实图像 ![](img/c3d7298c-8e7f-4dec-a5ff-4c36a29078c8.png) 提供给鉴别器 ![](img/19be0dee-d6ea-4c21-a901-3acd2320a789.png)，并得到图像为真实的概率。同时，我们还获得了对于真实图像的估计
    ![](img/235ed536-2d0f-48d3-88b9-3013d10a46d1.png)：
- en: '[PRE61]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Similarly, we feed the fake image to the discriminator and get the probability
    of the image being real and also the estimate of ![](img/5e52ecb7-3193-45c3-b47f-1c66192759cb.png)
    for the fake image:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将假图像提供给鉴别器，并得到图像为真实的概率，以及对于假图像的估计 ![](img/5e52ecb7-3193-45c3-b47f-1c66192759cb.png)：
- en: '[PRE62]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Computing loss function
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失函数
- en: Now we will see how to compute the loss function.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何计算损失函数。
- en: Discriminator loss
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鉴别器损失
- en: 'The discriminator loss is given as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器损失如下：
- en: '![](img/4835bff6-b8a1-4f22-b460-2011ba5ed03d.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4835bff6-b8a1-4f22-b460-2011ba5ed03d.png)'
- en: 'As the discriminator loss of an InfoGAN is same as with a CGAN, implementing
    the discriminator loss is the same as what we learned in the CGAN section:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 InfoGAN 的鉴别器损失与 CGAN 相同，实现鉴别器损失与我们在 CGAN 部分学到的相同：
- en: '[PRE63]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Generator loss
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器损失
- en: 'The loss function of the generator is given as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失函数如下所示：
- en: '![](img/82e62838-ebe0-4eab-9f5b-823ec4f35519.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82e62838-ebe0-4eab-9f5b-823ec4f35519.png)'
- en: 'Generator loss is implemented as:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失的实现为：
- en: '[PRE64]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Mutual information
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互信息
- en: 'We subtract **mutual information** from both the discriminator and the generator
    loss. So, the final loss function of discriminator and generator is given as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从鉴别器和生成器的损失中减去**互信息**。因此，鉴别器和生成器的最终损失函数如下所示：
- en: '![](img/60d35e00-184e-445d-9314-985adea697e2.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60d35e00-184e-445d-9314-985adea697e2.png)'
- en: '![](img/6969ec81-922a-4d37-a3c4-d395a2bee81a.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6969ec81-922a-4d37-a3c4-d395a2bee81a.png)'
- en: 'The mutual information can be calculated as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 可以计算互信息如下：
- en: '![](img/288708fe-7ab4-4eb1-84b2-a7b46b2eb6eb.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/288708fe-7ab4-4eb1-84b2-a7b46b2eb6eb.png)'
- en: 'First, we define a prior for ![](img/3db44084-161b-4594-95d0-a12b9a741445.png):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们为 ![](img/3db44084-161b-4594-95d0-a12b9a741445.png) 定义一个先验：
- en: '[PRE65]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The entropy of ![](img/db8e2ca2-24ae-4c99-aeee-2fd273971ec8.png) is represented
    as ![](img/2bbf0f0f-c47b-4f20-aaa0-c2dfcb91e0ce.png). We know that the entropy
    is calculated as ![](img/b02529c2-01a4-4ae3-ad88-d9ac0311d711.png):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/84c1b4e1-f67d-4298-8f91-860d6ec8ac2f.png) 给定时，![](img/db8e2ca2-24ae-4c99-aeee-2fd273971ec8.png)
    的熵表示为 ![](img/2bbf0f0f-c47b-4f20-aaa0-c2dfcb91e0ce.png)。我们知道熵的计算如下所示：
- en: '[PRE66]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The conditional entropy of ![](img/00a39cdd-ada4-44a6-a583-2dcf3de42643.png)
    when ![](img/84c1b4e1-f67d-4298-8f91-860d6ec8ac2f.png) is given is ![](img/68c0e69b-b302-47f4-bb8d-ba72a7fefaad.png).
    The code for the conditional entropy is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当给定 ![](img/84c1b4e1-f67d-4298-8f91-860d6ec8ac2f.png) 时，![](img/00a39cdd-ada4-44a6-a583-2dcf3de42643.png)
    的条件熵为 ![](img/68c0e69b-b302-47f4-bb8d-ba72a7fefaad.png)。条件熵的代码如下：
- en: '[PRE67]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The mutual information is given as ![](img/fd79fef3-96a8-4c96-94f5-3d0b50ce42e7.png):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息表示为 ![](img/fd79fef3-96a8-4c96-94f5-3d0b50ce42e7.png)：
- en: '[PRE68]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The final loss of the discriminator and the generator is given as:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器和生成器的最终损失如下所示：
- en: '[PRE69]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Optimizing the loss
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化损失
- en: 'Now we need to optimize our generator and discriminator. So, we collect the
    parameters of the discriminator and generator as ![](img/ae3bf55a-b765-41d3-8a7f-2040f62b651d.png)
    and ![](img/00357b65-480a-4f02-8551-88c1d95a4c81.png) respectively:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要优化我们的生成器和鉴别器。因此，我们收集鉴别器和生成器的参数分别为 ![](img/ae3bf55a-b765-41d3-8a7f-2040f62b651d.png)
    和 ![](img/00357b65-480a-4f02-8551-88c1d95a4c81.png)：
- en: '[PRE70]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Optimize the loss using the Adam optimizer:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Adam 优化器优化损失：
- en: '[PRE71]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Beginning training
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始训练
- en: 'Define the batch size and the number of epochs and initialize all the TensorFlow
    variables:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 定义批量大小和轮数，并初始化所有 TensorFlow 变量：
- en: '[PRE72]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Define a helper function for visualizing results:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个辅助函数来可视化结果：
- en: '[PRE73]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Generating handwritten digits
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成手写数字
- en: 'Start training and generate the image. For every `100` iterations, we print
    the image generated by the generator:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练并生成图像。每`100`次迭代，我们打印生成器生成的图像：
- en: '[PRE74]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Sample the images:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 样本图像：
- en: '[PRE75]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Sample the value of *c*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 样本值 *c*：
- en: '[PRE76]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Sample noise *z*:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 样本噪声 *z*：
- en: '[PRE77]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Optimize the loss of the generator and the discriminator:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 优化生成器和鉴别器的损失：
- en: '[PRE78]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Print the generator image for every 100^(th) iteration:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 每 100^(th) 次迭代打印生成器图像：
- en: '[PRE79]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We can see how the generator is evolving on each iteration and generating better
    digits:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到生成器在每次迭代中是如何发展和生成更好的数字的：
- en: '![](img/302b0cac-d211-4e5c-b999-7fc25215d834.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/302b0cac-d211-4e5c-b999-7fc25215d834.png)'
- en: Translating images using a CycleGAN
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CycleGAN 翻译图像
- en: We have learned several types of GANs, and the applications of them are endless.
    We have seen how the generator learns the distribution of real data and generates
    new realistic samples. We will now see a really different and very innovative
    type of GAN called the **CycleGAN**.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了几种类型的 GAN，并且它们的应用是无穷无尽的。我们已经看到生成器如何学习真实数据的分布并生成新的逼真样本。现在我们将看到一个非常不同且非常创新的
    GAN 类型，称为**CycleGAN**。
- en: Unlike other GANs, the CycleGAN maps the data from one domain to another domain,
    which implies that here we try to learn the mapping from the distribution of images
    from one domain to the distribution of images in another domain. To put it simply,
    we translate images from one domain to another.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 不像其他 GAN，CycleGAN 将数据从一个域映射到另一个域，这意味着这里我们试图学习从一个域的图像分布到另一个域的图像分布的映射。简单地说，我们将图像从一个域翻译到另一个域。
- en: What does this mean? Assume we want to convert a grayscale image to a colored
    image. The grayscale image is one domain and the colored image is another domain.
    A CycleGAN learns the mapping between these two domains and translates between
    them. This means that given a grayscale image, a CycleGAN converts the image into
    a colored one.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着什么？假设我们想要将灰度图像转换为彩色图像。灰度图像是一个域，彩色图像是另一个域。CycleGAN 学习这两个域之间的映射并在它们之间进行转换。这意味着给定一个灰度图像，CycleGAN
    将其转换为彩色图像。
- en: 'Applications of CycleGANs are numerous, such as converting real photos to artistic
    pictures, season transfer, photo enhancement, and many more. As shown in the following
    figure, you can see how a CycleGAN converts images between different domains:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的应用非常广泛，例如将真实照片转换为艺术图片、季节转换、照片增强等。如下图所示，您可以看到CycleGAN如何在不同域之间转换图像：
- en: '![](img/24bfb704-2d29-46b4-ad20-2bfbb147825c.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24bfb704-2d29-46b4-ad20-2bfbb147825c.png)'
- en: 'But what is so special about CycleGANs? It''s their ability to convert images
    from one domain to another without any paired examples. Let''s say we are converting
    photos (source) to paintings (target). In a normal image-to-image translation,
    how do we that? We prepare the training data by collecting some photos and also
    their corresponding paintings in pairs, as shown in the following image:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 但CycleGAN有什么特别之处？它们能够在没有配对示例的情况下将图像从一个域转换到另一个域。假设我们要将照片（源）转换为绘画（目标）。在普通的图像到图像的转换中，我们如何做到这一点？我们通过收集一些照片及其对应的绘画，如下图所示，准备训练数据：
- en: '![](img/f0f516ad-affa-420a-9e51-175f6100a9b8.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0f516ad-affa-420a-9e51-175f6100a9b8.png)'
- en: Collecting these paired data points for every use case is an expensive task,
    and we might not have many records or pairs. Here is where the biggest advantage
    of a CycleGAN lies. It doesn't require data in aligned pairs. To convert from
    photos to paintings, we just need a bunch of photos and a bunch of paintings.
    They don't have to map or align with each other.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 收集每个用例的这些配对数据点是一项昂贵的任务，我们可能没有太多记录或配对。这就是CycleGAN的最大优势所在。它不需要对齐的数据对。要将照片转换为绘画，我们只需一堆照片和一堆绘画。它们不需要进行映射或对齐。
- en: 'As shown in the following figure, we have some photos in one column and some
    paintings in another column; as you can see, they are not paired with each other.
    They are completely different images:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们有一些照片在一列，一些绘画在另一列；您可以看到它们并不互相配对。它们是完全不同的图像：
- en: '![](img/caffee85-96ab-46a7-aef3-6779d717a2cf.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/caffee85-96ab-46a7-aef3-6779d717a2cf.png)'
- en: Thus, to convert images from any source domain to the target domain, we just
    need a bunch of images in both of the domains and it does not have to be paired.
    Now let's see how they work and how they learn the mapping between the source
    and the target domain.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要将图像从任何源域转换为目标域，我们只需一堆这两个域中的图像，而它们不需要成对。现在让我们看看它们是如何工作以及它们如何学习源与目标域之间的映射。
- en: Unlike other GANs, the CycleGAN consists of two generators and two discriminators.
    Let's represent an image in the source domain by ![](img/e6cbd39c-20ee-4b2e-96c5-b068afd8734d.png)
    and target domain by ![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png). We need
    to learn the mapping between ![](img/519f9f4e-65c0-4e32-a83c-9bf20af36f52.png)
    and ![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他生成对抗网络（GAN）不同，CycleGAN 包含两个生成器和两个判别器。我们用![](img/e6cbd39c-20ee-4b2e-96c5-b068afd8734d.png)来代表源域中的图像，用![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png)来代表目标域中的图像。我们需要学习![](img/519f9f4e-65c0-4e32-a83c-9bf20af36f52.png)到![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png)的映射关系。
- en: 'Let''s say we are learning to convert a real picture, ![](img/60960d1b-d4e6-4b9c-87e1-8ee02750cd75.png),
    to a painting, ![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png), as shown in
    the following figure:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要学习将真实图片![](img/60960d1b-d4e6-4b9c-87e1-8ee02750cd75.png)转换为绘画![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png)，如下图所示：
- en: '![](img/57884722-f016-4486-adea-5228202949a5.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57884722-f016-4486-adea-5228202949a5.png)'
- en: Role of generators
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器的角色
- en: 'We have two generators, ![](img/f2387305-924d-40a4-9d6a-3a575d4c4a91.png) and
    ![](img/f26d3b48-e48a-47e2-9b52-3b893a2ca07e.png). The role of ![](img/4ae0c2af-662a-4c7c-8d91-3049e76f238e.png)
    is to learn the mapping from ![](img/c74866d7-29b8-4604-b4cc-282c077598f3.png)
    to ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png). As mentioned above, the
    role of ![](img/884f31f8-643c-4b27-845f-0b4d6996a1b5.png) is to learn to translate
    photos to paintings, as shown in the following figure:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个生成器，![](img/f2387305-924d-40a4-9d6a-3a575d4c4a91.png)和![](img/f26d3b48-e48a-47e2-9b52-3b893a2ca07e.png)。![](img/4ae0c2af-662a-4c7c-8d91-3049e76f238e.png)的作用是学习从![](img/c74866d7-29b8-4604-b4cc-282c077598f3.png)到![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)的映射。如上所述，![](img/884f31f8-643c-4b27-845f-0b4d6996a1b5.png)的作用是学习将照片转换为绘画，如下图所示：
- en: '![](img/591401f1-c8e0-4d69-ba80-a805d400bbd5.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/591401f1-c8e0-4d69-ba80-a805d400bbd5.png)'
- en: 'It tries to generate a fake target image, which implies it takes a source image,
    ![](img/39408d7b-f87b-4bd0-b3ed-d9f192d9d5cd.png), as input and generates a fake
    target image, ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 它试图生成一个虚假的目标图像，这意味着它将一个源图像，![](img/39408d7b-f87b-4bd0-b3ed-d9f192d9d5cd.png)，作为输入，并生成一个虚假的目标图像，![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)：
- en: '![](img/1a62a24b-575c-4bf3-8cb5-5aaecba7e7e3.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a62a24b-575c-4bf3-8cb5-5aaecba7e7e3.png)'
- en: 'The role of the generator, ![](img/1b2e1c2d-6249-4a04-b264-4ff7d01d97ff.png),
    is to learn the mapping from ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)
    to ![](img/a4872b2a-894e-4f82-9517-fbf12be37e3a.png) and learn to translate from
    the painting to a real picture, as shown in following figure:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的角色，![](img/1b2e1c2d-6249-4a04-b264-4ff7d01d97ff.png)，是学习从![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)到![](img/a4872b2a-894e-4f82-9517-fbf12be37e3a.png)的映射，并学会将绘画转化为真实图片，如下图所示：
- en: '![](img/f2572f4c-4e50-405a-aff3-7f8b4eeb3799.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2572f4c-4e50-405a-aff3-7f8b4eeb3799.png)'
- en: 'It tries to generate a fake source image, which implies it takes a target image,
    ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png), as input and generates a fake
    source image, ![](img/a92caa9a-5f73-4ac8-b8cc-b1d5fa0df87c.png):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 它试图生成一个虚假的源图像，这意味着它将一个目标图像，![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)，作为输入，并生成一个虚假的源图像，![](img/a92caa9a-5f73-4ac8-b8cc-b1d5fa0df87c.png)：
- en: '![](img/545b880a-c12b-48db-95a9-66280805e4ef.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/545b880a-c12b-48db-95a9-66280805e4ef.png)'
- en: Role of discriminators
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器的角色
- en: Similar to the generators, we have two discriminators, ![](img/80d3f113-745d-4b0f-b80f-be4a1819cd4f.png)
    and ![](img/96eaeabb-208c-4166-b365-7573fc3e077f.png). The role of the discriminator ![](img/ff3805fa-fe28-45ba-91b5-e4a966d8e749.png)
    is to discriminate between the real source image, ![](img/155da74d-cd57-4190-b1f6-a24d9214df6a.png),
    and the fake source image ![](img/24043c10-de75-4b46-bba4-dec621e86dc9.png). We
    know that the fake source image is generated by the generator ![](img/d7a67cf6-df09-41fb-8099-9b079109065d.png).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成器类似，我们有两个判别器，![](img/80d3f113-745d-4b0f-b80f-be4a1819cd4f.png)和![](img/96eaeabb-208c-4166-b365-7573fc3e077f.png)。判别器![](img/ff3805fa-fe28-45ba-91b5-e4a966d8e749.png)的角色是区分真实源图像![](img/155da74d-cd57-4190-b1f6-a24d9214df6a.png)和假源图像![](img/24043c10-de75-4b46-bba4-dec621e86dc9.png)。我们知道假源图像是由生成器![](img/d7a67cf6-df09-41fb-8099-9b079109065d.png)生成的。
- en: 'Given an image to discriminator ![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png),
    it returns the probability of the image being a real source image:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个图像给判别器![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png)，它返回图像为真实源图像的概率：
- en: '![](img/e2370055-f57b-4877-9ee3-222b56144bda.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2370055-f57b-4877-9ee3-222b56144bda.png)'
- en: 'The following figure shows the discriminator ![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png),
    as you can observe it takes the real source image x and the fake source image
    generated by the generator F as inputs and returns the probability of the image
    being a real source image:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了判别器![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png)，您可以观察到它将真实源图像x和生成器F生成的假源图像作为输入，并返回图像为真实源图像的概率：
- en: '![](img/61193748-e404-42e8-bc74-7a5fe90a4e30.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61193748-e404-42e8-bc74-7a5fe90a4e30.png)'
- en: 'The role of discriminator ![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png)
    is to discriminate between the real target image, ![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png),
    and the fake target image, ![](img/e9cf6ebe-aea1-4a4f-a472-11f5fd14d49f.png).
    We know that fake target image is generated by the generator, ![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png).
    Given an image to discriminator ![](img/88040175-da0f-4cfa-ba2e-3f20aaa8e923.png),
    it returns the probability of the image being a real target image:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png)的角色是区分真实目标图像![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png)和假目标图像![](img/e9cf6ebe-aea1-4a4f-a472-11f5fd14d49f.png)。我们知道假目标图像是由生成器![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png)生成的。给定一个图像给判别器![](img/88040175-da0f-4cfa-ba2e-3f20aaa8e923.png)，它返回图像为真实目标图像的概率：
- en: '![](img/927703eb-3676-40a3-afd4-5c1640434ff7.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/927703eb-3676-40a3-afd4-5c1640434ff7.png)'
- en: 'The following figure shows the discriminator ![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png),
    as you can observe it takes the real target image, ![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png)
    and the fake target image generated by the generator, ![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png)
    as inputs and returns the probability of the image being a real target image:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了判别器![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png)，您可以观察到它将真实目标图像![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png)和生成器生成的假目标图像![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png)作为输入，并返回图像为真实目标图像的概率：
- en: '![](img/6c26c935-7df9-4449-9337-db63fb930635.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c26c935-7df9-4449-9337-db63fb930635.png)'
- en: Loss function
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: In CycleGANs, we have two generators and two discriminators. Generators learn
    to translate images from one domain to another and a discriminator tries to discriminate
    between the translated images.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CycleGAN 中，我们有两个生成器和两个鉴别器。生成器学习将图像从一个域转换到另一个域，鉴别器试图区分转换后的图像。
- en: 'So, we can say the loss function of discriminator ![](img/df0890f7-556a-4a6b-acde-ba0debf5b68d.png)
    can be represented as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说鉴别器的损失函数 ![](img/df0890f7-556a-4a6b-acde-ba0debf5b68d.png) 可以表示如下：
- en: '![](img/447e0f80-e9ca-4e2a-b5df-31ee82a9250e.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/447e0f80-e9ca-4e2a-b5df-31ee82a9250e.png)'
- en: 'Similarly, the loss function of discriminator ![](img/dbc8900b-9e03-420c-b703-d61ecdabd2cb.png)
    can be represented as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，鉴别器 ![](img/dbc8900b-9e03-420c-b703-d61ecdabd2cb.png) 的损失函数可以表示如下：
- en: '![](img/9d48690d-01f6-42fc-a996-5c8e1f47103e.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d48690d-01f6-42fc-a996-5c8e1f47103e.png)'
- en: 'The loss function of generator ![](img/171a85e6-72fc-44ca-813e-50e907dd78aa.png)
    can be represented as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器 ![](img/171a85e6-72fc-44ca-813e-50e907dd78aa.png) 的损失函数可以表示如下：
- en: '![](img/e50dae88-b189-429d-9e31-ef711f415e4a.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e50dae88-b189-429d-9e31-ef711f415e4a.png)'
- en: 'The loss function of generator ![](img/5666a423-e13a-481e-9f41-42277b0ee5b4.png)
    can be given as follows:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器 ![](img/5666a423-e13a-481e-9f41-42277b0ee5b4.png) 的损失函数可以表示如下：
- en: '![](img/6b7edf58-4bda-435f-b4e3-87cdfd384ec2.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b7edf58-4bda-435f-b4e3-87cdfd384ec2.png)'
- en: 'Altogether, the final loss can be written as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，最终损失可以写成如下形式：
- en: '![](img/cabb8433-6bad-4d41-95ce-c60c6ab50bff.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cabb8433-6bad-4d41-95ce-c60c6ab50bff.png)'
- en: Cycle consistency loss
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环一致性损失
- en: The adversarial loss alone does not ensure the proper mapping of the images.
    For instance, a generator can map the images from the source domain to a random
    permutation of images in the target domain which can match the target distribution.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过对抗损失并不能确保图像的正确映射。例如，生成器可以将源域中的图像映射到目标域中图像的随机排列，这可能匹配目标分布。
- en: So, to avoid this, we introduce an additional loss called **cycle consistent
    lo****ss**. It enforces both generators *G* and *F* to be cycle-consistent.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免这种情况，我们引入了一种额外的损失，称为**循环一致性损失**。它强制生成器 *G* 和 *F* 都是循环一致的。
- en: 'Let''s recollect the function of the generators:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下生成器的功能：
- en: '**Generator *G***: Converts *x* to *y*'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器 *G***：将 *x* 转换为 *y*'
- en: '**Generator *F***: Converts *y* to *x*'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器 *F***：将 *y* 转换为 *x*'
- en: We know that generator *G* takes the source image *x* and converts it to a fake
    target image *y*. Now if we feed this generated fake target image *y* to generator
    *F*, it has to return the original source image *x*. Confusing, right?
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道生成器 *G* 接受源图像 *x* 并将其转换为虚假的目标图像 *y*。现在，如果我们将这个生成的虚假目标图像 *y* 输入生成器 *F*，它应该返回原始的源图像
    *x*。有点混乱，对吧？
- en: 'Look at the following figure; we have a source image, *x*. First, we feed this
    image to generator *G*, and it returns the fake target image. Now we take this
    fake target image, *y*, and feed it to generator *F*, which has to return the
    original source image:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的图示；我们有一个源图像 *x*。首先，我们将这个图像输入生成器 *G*，它会返回一个虚假的目标图像 *y*。现在我们将这个虚假的目标图像 *y*
    输入到生成器 *F*，它应该返回原始的源图像：
- en: '![](img/2acdf8f9-b887-4af7-a557-2ecf88bb42fe.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2acdf8f9-b887-4af7-a557-2ecf88bb42fe.png)'
- en: 'The above equation can be represented as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程可以表示如下：
- en: '![](img/16f10554-ff0c-45c7-bb4e-726914dd8b4f.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16f10554-ff0c-45c7-bb4e-726914dd8b4f.png)'
- en: 'This is called the **forward consistency loss** and can be represented as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**前向一致性损失**，可以表示如下：
- en: '![](img/b3cde238-f5f1-4b51-ab2e-45b3b7e229ec.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3cde238-f5f1-4b51-ab2e-45b3b7e229ec.png)'
- en: 'Similarly, we can specify backward consistent loss, as shown in the following
    figure. Let''s say we have an original target image, *y*. We take this *y* and
    feed it to discriminator *F*, and it returns the fake source image *x*. Now we
    feed this fake source image *x* to the generator *G*, and it has to return the
    original target image *y*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以指定后向一致性损失，如下图所示。假设我们有一个原始的目标图像 *y*。我们将这个 *y* 输入鉴别器 *F*，它返回一个虚假的源图像 *x*。现在我们将这个生成的虚假源图像
    *x* 输入生成器 *G*，它应该返回原始的目标图像 *y*：
- en: '![](img/28cc50f4-c98d-4edb-868b-32c46c7169d9.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28cc50f4-c98d-4edb-868b-32c46c7169d9.png)'
- en: 'The preceding equation can be represented as:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程可以表示为：
- en: '![](img/e90a057d-6622-4304-b62f-6544beced8e6.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e90a057d-6622-4304-b62f-6544beced8e6.png)'
- en: 'The backward consistency loss can be represented as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 后向一致性损失可以表示如下：
- en: '![](img/9527c35d-b8c7-48cd-8cb1-b004a7228ca9.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9527c35d-b8c7-48cd-8cb1-b004a7228ca9.png)'
- en: 'So, together with a backward and forward consistent loss, we can write the
    cycle consistency loss as:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，结合反向和正向一致性损失，我们可以将循环一致性损失写成：
- en: '![](img/921caae3-cc6c-4005-8cc1-87399e37c099.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/921caae3-cc6c-4005-8cc1-87399e37c099.png)'
- en: '![](img/3fc60b22-7713-48c1-a1a0-9ab1596b4486.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fc60b22-7713-48c1-a1a0-9ab1596b4486.png)'
- en: 'We want our generators to cycle consistent so, we multiply their loss with
    the cycle consistent loss. So, the final loss function can be given as:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的生成器保持循环一致，因此，我们将它们的损失与循环一致性损失相乘。因此，最终损失函数可以表示为：
- en: '![](img/77f55660-3c8a-42e3-a6d8-1eea3b607e54.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77f55660-3c8a-42e3-a6d8-1eea3b607e54.png)'
- en: Converting photos to paintings using a CycleGAN
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CycleGAN 将照片转换为绘画作品
- en: 'Now we will learn how to implement a CycleGAN in TensorFlow. We will see how
    to convert pictures to paintings using a CycleGAN:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将学习如何在 TensorFlow 中实现 CycleGAN。我们将看到如何使用 CycleGAN 将图片转换为绘画作品：
- en: '![](img/c98cbbab-07be-4a1e-ada3-198683726f4a.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c98cbbab-07be-4a1e-ada3-198683726f4a.png)'
- en: The dataset used in this section can be downloaded from [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip).
    Once you have downloaded the dataset, unzip the archive; it will consist of four
    folders, `trainA`, `trainB`, `testA`, and `testB`, with training and testing images.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用的数据集可以从 [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip)
    下载。下载数据集后，请解压缩存档；它包括四个文件夹，`trainA`、`trainB`、`testA` 和 `testB`，其中包含训练和测试图像。
- en: The `trainA` folder consists of paintings (Monet) and the `trainB` folder consists
    of photos. Since we are mapping photos (*x*) to the paintings (*y*), the `trainB`
    folder, which consists of photos, will be our source image, ![](img/1b539a3e-4e00-4e5a-9f26-6f8824318419.png),
    and the `trainA`, which consists of paintings, will be our target image, ![](img/413d8fcb-4439-4bc7-952e-21ec209ba5b7.png).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainA` 文件夹包含绘画作品（莫奈），而 `trainB` 文件夹包含照片。由于我们将照片 (*x*) 映射到绘画作品 (*y*)，所以 `trainB`
    文件夹中的照片将作为我们的源图像，![](img/1b539a3e-4e00-4e5a-9f26-6f8824318419.png)，而包含绘画作品的 `trainA`
    将作为目标图像，![](img/413d8fcb-4439-4bc7-952e-21ec209ba5b7.png)。'
- en: The complete code for the CycleGAN with a step-by-step explanation is available
    as a Jupyter Notebook at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的完整代码及其逐步说明可作为 Jupyter Notebook 在 [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python)
    中找到。
- en: Instead of looking at the whole code, we will see only how the CycleGAN is implemented
    in TensorFlow and maps the source image to the target domain. You can also check
    the complete code at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是查看整段代码，我们只会看 TensorFlow 中如何实现 CycleGAN 并将源图像映射到目标领域。您也可以在 [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python)
    查看完整的代码。
- en: 'Define the `CycleGAN` class:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 `CycleGAN` 类：
- en: '[PRE80]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Define the placeholders for the input, `X`, and the output, `Y`:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入占位符 `X` 和输出占位符 `Y`：
- en: '[PRE81]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Define the generator, ![](img/182c4d15-8ba5-40f8-b249-97c18125d781.png), that
    maps ![](img/56fe4f4b-9eda-4272-8872-10529152edae.png) to ![](img/9c32ba4d-98d2-44d7-aeb6-d768786c4630.png):'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器，![](img/182c4d15-8ba5-40f8-b249-97c18125d781.png)，将 ![](img/56fe4f4b-9eda-4272-8872-10529152edae.png)
    映射到 ![](img/9c32ba4d-98d2-44d7-aeb6-d768786c4630.png)：
- en: '[PRE82]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Define the generator, ![](img/36bef03d-9e31-47c0-82c4-5fca2416aad6.png), that
    maps ![](img/5f3eaf60-fe74-4775-a7d9-3cc407a97c35.png) to ![](img/d45bf48f-aada-4436-ac4f-b125d352e8b1.png):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 定义生成器，![](img/36bef03d-9e31-47c0-82c4-5fca2416aad6.png)，将 ![](img/5f3eaf60-fe74-4775-a7d9-3cc407a97c35.png)
    映射到 ![](img/d45bf48f-aada-4436-ac4f-b125d352e8b1.png)：
- en: '[PRE83]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Define the discriminator, ![](img/3a6cab69-81bb-4464-ab61-c35a59115f15.png),
    that discriminates between the real source image and the fake source image:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 定义辨别器，![](img/3a6cab69-81bb-4464-ab61-c35a59115f15.png)，用于区分真实源图像和虚假源图像：
- en: '[PRE84]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Define the discriminator, ![](img/356ef7c9-c99f-4961-b689-ac89e4523e90.png),
    that discriminates between the real target image and the fake target image:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 定义辨别器，![](img/356ef7c9-c99f-4961-b689-ac89e4523e90.png)，用于区分真实目标图像和虚假目标图像：
- en: '[PRE85]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Generate the fake source image:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 生成虚假源图像：
- en: '[PRE86]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Generate the fake target image:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 生成虚假目标图像：
- en: '[PRE87]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Get the `logits`:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 `logits`：
- en: '[PRE88]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We know cycle consistency loss is given as follows:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道循环一致性损失如下所示：
- en: '![](img/1b2a7610-0aa3-445a-a9a5-47650edd03e9.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b2a7610-0aa3-445a-a9a5-47650edd03e9.png)'
- en: 'We can implement the cycle consistency loss as follows:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下实现循环一致性损失：
- en: '[PRE89]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Define the loss for both of our discriminators, ![](img/3e052ee5-2ec7-44d5-a708-309775749f01.png)
    and ![](img/cc26f199-c253-4dde-8238-3cbe052233ed.png).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们两个鉴别器的损失，![](img/3e052ee5-2ec7-44d5-a708-309775749f01.png) 和 ![](img/cc26f199-c253-4dde-8238-3cbe052233ed.png)。
- en: 'We can rewrite our loss function of discriminator with Wasserstein distance
    as:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下重写带有Wasserstein距离的鉴别器损失函数：
- en: '![](img/dfe69197-13a3-4fd4-8cc2-6b43783a27f5.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfe69197-13a3-4fd4-8cc2-6b43783a27f5.png)'
- en: '![](img/112ce76a-1cec-4244-b827-b6779ad589d3.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/112ce76a-1cec-4244-b827-b6779ad589d3.png)'
- en: 'Thus, the loss of both the discriminator is implemented as follows:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，两个鉴别器的损失如下实现：
- en: '[PRE90]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Define the loss for both of the generators, ![](img/5a278307-2f9e-4d77-997b-5ca957c0ddeb.png)
    and ![](img/89c696f8-ad04-4042-a889-c93eb7505a7d.png). We can rewrite our loss
    function of generators with Wasserstein distance as:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们两个生成器的损失，![](img/5a278307-2f9e-4d77-997b-5ca957c0ddeb.png) 和 ![](img/89c696f8-ad04-4042-a889-c93eb7505a7d.png)。我们可以如下重写带有Wasserstein距离的生成器损失函数：
- en: '![](img/433704ef-29de-4c76-b26c-de00e1561b24.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/433704ef-29de-4c76-b26c-de00e1561b24.png)'
- en: '![](img/4a705c83-3217-4ded-8f50-660f415c4073.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a705c83-3217-4ded-8f50-660f415c4073.png)'
- en: 'Thus, the loss of both the generators multiplied with the cycle consistency
    loss, `cycle_loss` is implemented as:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，两个生成器的损失乘以循环一致性损失`cycle_loss`如下实现：
- en: '[PRE91]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Optimize the discriminators and generators using the Adam optimizer:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Adam优化器优化鉴别器和生成器：
- en: '[PRE92]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Once we start training the model, we can see how the loss of discriminators
    and generators decreases over the iterations:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们开始训练模型，我们可以看到鉴别器和生成器的损失随迭代次数而减小：
- en: '[PRE93]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: StackGAN
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StackGAN
- en: Now we will see one of the most intriguing and fascinating types of GAN, which
    is called a **StackGAN**. Can you believe if I say StackGANs can generate photo-realistic
    images just based on the textual descriptions? Well, yes. They can do that. Given
    a text description, they can generate a realistic image.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到一种最引人入胜和迷人的GAN类型，称为**StackGAN**。你能相信如果我说StackGANs可以根据文本描述生成逼真的图像吗？是的，他们可以。给定一个文本描述，他们可以生成一个逼真的图像。
- en: Let's first understand how an artist draws an image. In the first stage, artists
    draw primitive shapes and create a basic outline that forms an initial version
    of the image. In the next stage, they enhance the image by making it more realistic
    and appealing.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解艺术家如何画出一幅图像。在第一阶段，艺术家绘制原始形状，创建图像的基本轮廓，形成图像的初始版本。在接下来的阶段，他们通过使图像更真实和吸引人来增强图像。
- en: StackGANs works in a similar manner. They divide the process of generating images
    into two stages. Just like artists draw pictures, in the first stage, they generate
    a basic outline, primitive shapes, and create a low-resolution version of the
    image, and in the second stage, they enhance the picture generated in the first
    stage by making it more realistic, and then convert them into a high-resolution
    image.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: StackGANs以类似的方式工作。它们将生成图像的过程分为两个阶段。就像艺术家画图一样，在第一阶段，他们生成基础轮廓、原始形状，并创建图像的低分辨率版本；在第二阶段，他们通过使图像更真实和吸引人来增强第一阶段生成的图像，并将其转换为高分辨率图像。
- en: But how do StackGANs do this?
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 但是StackGANs是如何做到的呢？
- en: 'They use two GANs, one for each stage. The GAN in the first stage generates
    a basic image and sends it to the GAN in the next stage, which converts basic
    low-resolution image into a proper high-resolution image. The following figure
    shows how StackGANs generate images in each of the stages based on the text description:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用两个GAN，每个阶段一个。第一阶段的GAN生成基础图像，并将其发送到下一阶段的GAN，后者将基础低分辨率图像转换为合适的高分辨率图像。下图显示了StackGANs如何基于文本描述在每个阶段生成图像：
- en: '![](img/d14ffa08-2b92-42b6-9faf-41852ea42806.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d14ffa08-2b92-42b6-9faf-41852ea42806.png)'
- en: 'Source: https://arxiv.org/pdf/1612.03242.pdf'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://arxiv.org/pdf/1612.03242.pdf
- en: As you can see , in the first stage, we have a low-resolution version of the
    image, but in the second stage, we have good clarity high-resolution image. But,
    still, how StackGAN are doing this? Remember, when we learned with conditional
    GANs that we can make our GAN generate images that we want by conditioning them?
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，在第一阶段，我们有图像的低分辨率版本，但在第二阶段，我们有清晰的高分辨率图像。但是，StackGAN是如何做到的呢？记住，当我们用条件GAN学习时，我们可以通过条件化使我们的GAN生成我们想要的图像。
- en: We just use them in both of the stages. In stage one, our network is conditioned
    based on the text description. With this text description, they generate a basic
    version of an image. In stage II, our network is conditioned based on the image
    generated from stage I and also on the text description.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个阶段都使用它们。在第一阶段，我们的网络基于文本描述进行条件设定。根据这些文本描述，它们生成图像的基本版本。在第二阶段，我们的网络基于从第一阶段生成的图像和文本描述进行条件设定。
- en: But why do we have to have to condition on the text description again in stage
    II? Because in stage I, we miss some details specified in the text description
    to create a basic version of an image. So, in stage II, we again condition on
    the text description to fix the missing information and also to make our image
    more realistic.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么我们在第二阶段又必须要基于文本描述进行条件设定呢？因为在第一阶段，我们错过了文本描述中指定的一些细节，只创建了图像的基本版本。所以，在第二阶段，我们再次基于文本描述进行条件设定，修复缺失的信息，并且使我们的图像更加真实。
- en: With this ability to generate pictures just based on the text, it is used for
    numerous applications. It is heavily used in the entertainment industry, for instance,
    for creating frames just based on descriptions, and it can also be used for generating
    comics and many more.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借这种仅基于文本生成图片的能力，它被广泛应用于许多应用场景。尤其在娱乐行业中大量使用，例如根据描述创建帧，还可以用于生成漫画等等。
- en: The architecture of StackGANs
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StackGAN的架构
- en: Now that we have a basic understanding of how StackGANs work, we will take a
    closer look into their architecture and see how exactly they generate a picture
    from the text.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对StackGAN的工作原理有了基本的了解，我们将更详细地查看它们的架构，看看它们如何从文本生成图片。
- en: 'The complete architecture of a StackGAN is shown in the following diagram:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: StackGAN的完整架构如下图所示：
- en: '![](img/75cf1c71-6b88-4b7f-a1e4-f37f5487a4c6.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75cf1c71-6b88-4b7f-a1e4-f37f5487a4c6.png)'
- en: 'Source: https://arxiv.org/pdf/1612.03242.pdf'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://arxiv.org/pdf/1612.03242.pdf
- en: We will look at each component one by one.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个查看每个组件。
- en: Conditioning augmentation
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件增强
- en: We have a text description as an input to the GAN. Based on these descriptions,
    it has to generate the images. But how do they understand the meaning of the text
    to generate a picture?
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文本描述作为GAN的输入。基于这些描述，它必须生成图像。但它们如何理解文本的含义以生成图片呢？
- en: First, we convert the text into an embedding using an encoder. We represent
    this text embedding by ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png). Can
    we create variations of ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)? By
    creating variations of text embeddings, ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png),
    we can have additional training pairs, and we can also increase the robustness
    to small perturbations.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用编码器将文本转换为嵌入。我们用 ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png) 表示这个文本嵌入。我们能够创建
    ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png) 的变化吗？通过创建文本嵌入的变化，我们可以获得额外的训练对，并增加对小扰动的鲁棒性。
- en: Let ![](img/3ecca366-0d12-43d0-8ba6-d70d5af45f18.png) be mean and ![](img/00891861-b74f-4d4b-849d-aadc3615125b.png)
    be the diagonal covariance matrix of our text embedding, ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png).
    Now we randomly sample an additional conditioning variable, ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png),
    from the independent Gaussian distribution, ![](img/0cde2c39-1d06-4b38-9ee5-95e83080118f.png).
    It helps us create variations of text descriptions with their meanings. We know
    that same text can be written in various ways, so with the conditioning variable,
    ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png), we can have various versions
    of the text mapping to the image.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![](img/3ecca366-0d12-43d0-8ba6-d70d5af45f18.png) 表示均值，![](img/00891861-b74f-4d4b-849d-aadc3615125b.png)
    表示我们文本嵌入的对角协方差矩阵，![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)。现在我们从独立的高斯分布中随机采样一个额外的条件变量，![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)，帮助我们创建具有其含义的文本描述的变化。我们知道同样的文本可以用多种方式书写，因此通过条件变量
    ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)，我们可以得到映射到图像的文本的各种版本。
- en: Thus, once we have the text description, we will extract their embeddings using
    the encoder, and then we compute their mean and covariance. Then, we sample ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)
    from the Gaussian distribution of the text embedding, ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦我们有了文本描述，我们将使用编码器提取它们的嵌入，然后计算它们的均值和协方差。然后，我们从文本嵌入的高斯分布中采样 ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)。
- en: Stage I
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一阶段
- en: OK, now we have a text embedding, ![](img/b98b457b-5113-4138-87a1-5ecc312f9ee4.png),
    and also a conditioning variable, ![](img/2d0a756c-3fa1-4604-8921-217706c768a7.png).
    We will see how it is being used to generate the basic version of the image.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个文本嵌入，![](img/b98b457b-5113-4138-87a1-5ecc312f9ee4.png)，以及一个条件变量，![](img/2d0a756c-3fa1-4604-8921-217706c768a7.png)。我们将看看它是如何被用来生成图像的基本版本。
- en: Generator
  id: totrans-421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: We know that the goal of the generator is to generate a fake image by learning
    the real data distribution. First, we sample noise from a Gaussian distribution
    and create *z*. Then we concatenate *z* with our conditional variable, ![](img/2049be92-cbff-4589-9f92-654889e904da.png),
    and feed this as an input to the generator which outputs a basic version of the
    image.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的目标是通过学习真实数据分布来生成虚假图像。首先，我们从高斯分布中采样噪声并创建*z*。然后，我们将*z*与我们的条件变量，![](img/2049be92-cbff-4589-9f92-654889e904da.png)，连接起来，并将其作为输入提供给生成器，生成图像的基本版本。
- en: 'The loss function of the generator is given as follows:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的损失函数如下：
- en: '![](img/3563799a-e439-4332-81d9-9870c6fb465f.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3563799a-e439-4332-81d9-9870c6fb465f.png)'
- en: 'Let''s examine this formula:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个公式：
- en: '![](img/8fefbb15-57bb-408f-a6f4-18edae972eec.png) implies we sample z from
    the fake data distribution, that is, the noise prior.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8fefbb15-57bb-408f-a6f4-18edae972eec.png)表示我们从虚假数据分布中采样*z*，也就是先验噪声。'
- en: '![](img/aa262917-373b-423e-b8ff-5295ea45fa41.png)implies we sample the text
    description, ![](img/045cf8e4-5c8b-4dcc-a914-8e2e7bcc15ef.png), from the real
    data distribution.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/aa262917-373b-423e-b8ff-5295ea45fa41.png)表示我们从真实数据分布中采样文本描述，![](img/045cf8e4-5c8b-4dcc-a914-8e2e7bcc15ef.png)。'
- en: '![](img/8a6a0eeb-b335-4e97-b511-4fe7c92f8d02.png) implies that the generator
    takes the noise and the conditioning variable returns the image. We feed this
    generated image to the discriminator.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8a6a0eeb-b335-4e97-b511-4fe7c92f8d02.png)表示生成器接受噪声和条件变量返回图像。我们将这个生成的图像馈送给鉴别器。'
- en: '![](img/89856ddf-2b18-4581-a8e3-4d502f407bf8.png) implies the log probability
    of the generated image being fake.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/89856ddf-2b18-4581-a8e3-4d502f407bf8.png)表示生成的图像为虚假的对数概率。'
- en: Along with this loss, we also add a regularizer term, ![](img/fa97f5c1-7daa-4454-b679-108d3f13d2c5.png),
    to our loss function, which implies the KL divergence between the standard Gaussian
    distribution and the conditioning Gaussian distribution. It helps us to avoid
    overfitting.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个损失之外，我们还向损失函数中添加了一个正则化项，![](img/fa97f5c1-7daa-4454-b679-108d3f13d2c5.png)，这意味着标准高斯分布和条件高斯分布之间的KL散度。这有助于我们避免过拟合。
- en: 'So, our final loss function for the generator becomes:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，生成器的最终损失函数如下：
- en: '![](img/f75101c3-d7b2-4b82-b0fb-027f1e3cf0f2.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f75101c3-d7b2-4b82-b0fb-027f1e3cf0f2.png)'
- en: Discriminator
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鉴别器
- en: 'Now we feed this generated image to the discriminator, which returns the probability
    of the image being real. The discriminator loss is given as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这个生成的图像馈送给鉴别器，它返回图像为真实的概率。鉴别器的损失如下：
- en: '![](img/ff6aed90-dd55-40f7-9541-dfd28d64318d.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff6aed90-dd55-40f7-9541-dfd28d64318d.png)'
- en: 'Here:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '![](img/cf8d3d28-dce2-4c91-9fb5-d57329b166e1.png) implies the real image, ![](img/79417429-af4b-4492-86f7-dbe46ae3762a.png),
    conditional on the text description, ![](img/788b3eff-5b35-41c9-b2d4-03cecadd3ba2.png)'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/cf8d3d28-dce2-4c91-9fb5-d57329b166e1.png)表示真实图像，![](img/79417429-af4b-4492-86f7-dbe46ae3762a.png)，条件于文本描述，![](img/788b3eff-5b35-41c9-b2d4-03cecadd3ba2.png)'
- en: '![](img/59e7da3c-1310-432d-8855-869dc1caa9bd.png) implies the generated fake
    image'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/59e7da3c-1310-432d-8855-869dc1caa9bd.png)表示生成的虚假图像'
- en: Stage II
  id: totrans-439
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二阶段
- en: We have learned how the basic version of the image is generated in stage I.
    Now, in stage II, we fix the defects of the image produced in stage I and generate
    a more realistic version of the image. We condition our network with the image
    generated from the previous stage and also on the text embeddings.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了第一阶段如何生成图像的基本版本。现在，在第二阶段，我们修复了第一阶段生成的图像的缺陷，并生成了更真实的图像版本。我们用来条件化网络的图像来自前一个阶段生成的图像，还有文本嵌入。
- en: Generator
  id: totrans-441
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: Instead of taking noise as an input, the generator in stage II takes the image
    generated from the previous stage as an input and it is conditioned on the text
    description.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，生成器不再接受噪声作为输入，而是接受从前一阶段生成的图像作为输入，并且以文本描述作为条件。
- en: Here, ![](img/dfdca6f9-f1c9-4754-bb16-1dd14a2485c4.png) implies that we are
    sampling ![](img/4412e70a-76f9-42cf-a621-441ef7c58ff1.png) from the ![](img/385a1be2-69ec-4ebd-bdd7-eade9473467c.png).
    It basically means that we are sampling the image generated from stage I.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/dfdca6f9-f1c9-4754-bb16-1dd14a2485c4.png) 意味着我们从阶段 I 生成的图像进行采样。
- en: '![](img/532013c8-a184-4d77-b833-45398433ea46.png) implies that we are sampling
    text from the given real data distribution, ![](img/5ebebb89-77fd-4d4b-b3e3-7dfcde29d765.png).'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/532013c8-a184-4d77-b833-45398433ea46.png) 意味着我们从给定的真实数据分布 ![](img/5ebebb89-77fd-4d4b-b3e3-7dfcde29d765.png)
    中抽样文本。'
- en: 'Then the generator loss can be given as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，生成器的损失可以如下给出：
- en: '![](img/5a59d6ec-a0dc-4b5b-a7cc-0b37e179e788.png)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a59d6ec-a0dc-4b5b-a7cc-0b37e179e788.png)'
- en: 'Along with the regularizer, our loss function of the generator becomes:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 除了正则化器外，生成器的损失函数为：
- en: '![](img/ecc0d204-c26a-429a-802d-bca587641079.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecc0d204-c26a-429a-802d-bca587641079.png)'
- en: Discriminator
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鉴别器
- en: 'The goal of the discriminator is to tell us whether the image is from the real
    distribution or the generator distribution. Thus, the loss function of the discriminator
    is given as follows:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的目标是告诉我们图像来自真实分布还是生成器分布。因此，判别器的损失函数如下：
- en: '![](img/f51864d9-cdd0-4a47-a689-6c2f3823d189.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f51864d9-cdd0-4a47-a689-6c2f3823d189.png)'
- en: Summary
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started the chapter by learning about conditional GANs and how they can be
    used to generate our image of interest.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从学习条件 GAN 开始这一章，了解了它们如何用于生成我们感兴趣的图像。
- en: Later, we learned about InfoGANs, where the code *c* is inferred automatically
    based on the generated output, unlike CGAN, where we explicitly specify *c*. To
    infer *c*, we need to find the posterior, ![](img/6681a392-47b2-4fde-b928-ea3e9dd3f0e7.png),
    which we don't have access to. So, we use an auxiliary distribution. We used mutual
    information to maximize the mutual information, ![](img/30eec497-b316-4712-8e24-0cc194eb757e.png),
    to maximize our knowledge about *c* given the generator output.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们学习了 InfoGAN，其中代码 *c* 是根据生成的输出自动推断的，不像 CGAN 那样我们需要显式指定 *c*。要推断 *c*，我们需要找到后验分布
    ![](img/6681a392-47b2-4fde-b928-ea3e9dd3f0e7.png)，而我们无法访问它。因此，我们使用辅助分布。我们使用互信息来最大化
    ![](img/30eec497-b316-4712-8e24-0cc194eb757e.png)，以增强对 *c* 在给定生成器输出的知识。
- en: Then, we learned about CycleGANs, which map the data from one domain to another
    domain. We tried to learn the mapping from the distribution of images from photos
    domain to the distribution of images in paintings domain. Finally, we understood
    how StackGANs generate photorealistic images from a text description.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们学习了 CycleGAN，它将数据从一个域映射到另一个域。我们尝试学习将照片域图像分布映射到绘画域图像分布的映射关系。最后，我们了解了 StackGANs
    如何从文本描述生成逼真的图像。
- en: In the next chapter, we will learn about **autoencoders** and their types.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习**自编码器**及其类型。
- en: Questions
  id: totrans-457
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions to gauge how much you have learned from this
    chapter:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 回答以下问题，以评估你从本章学到了多少内容：
- en: How do conditional GANs differ from vanilla GANs?
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条件 GAN 和普通 GAN 有何不同？
- en: What is the code called in InfoGAN?
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: InfoGAN 中的代码称为什么？
- en: What is mutual information?
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是互信息？
- en: Why do we need auxiliary distribution in InfoGANs?
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 InfoGAN 需要辅助分布？
- en: What is cycle consistency loss?
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是循环一致性损失？
- en: Explain the role of generators in a CycleGAN.
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释 CycleGAN 中生成器的作用。
- en: How do StackGANs convert text descriptions into pictures?
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: StackGANs 如何将文本描述转换为图片？
- en: Further reading
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following links for more information:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下链接：
- en: '*Conditional Generative Adversarial Nets* by Mehdi Mirza and Simon Osindero,
    [https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*条件生成对抗网络*，作者为Mehdi Mirza和Simon Osindero，[https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)'
- en: '*InfoGAN: Interpretable Representation Learning by Information Maximizing Generative
    Adversarial Nets* by Xi Chen et al., [https://arxiv.org/pdf/1606.03657.pdf](https://arxiv.org/pdf/1606.03657.pdf)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*InfoGAN: 通过最大化信息的生成对抗网络进行可解释表示学习*，作者为陈希等人，[https://arxiv.org/pdf/1606.03657.pdf](https://arxiv.org/pdf/1606.03657.pdf)'
- en: '*Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks*
    by Jun-Yan Zhu et al., [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*利用循环一致性对抗网络进行非配对图像到图像翻译*，作者为朱俊彦等人，[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)'
