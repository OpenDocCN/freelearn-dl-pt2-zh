- en: Chapter 2\. Getting Started with OpenAI API
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章。开始使用OpenAI API
- en: Even though GPT-3 is the most sophisticated and complex language model in the
    world, its capabilities are abstracted as a simple "text-in-text-out" interface
    to end users. This chapter will get you started with using that interface, Playground,
    and cover the technical nuances of the OpenAI API because it is always the details
    that reveal the true gems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPT-3是世界上最复杂和复杂的语言模型，但其功能被抽象为简单的“文本输入-文本输出”接口提供给最终用户。本章将帮助您开始使用该接口、Playground，并深入涵盖OpenAI
    API的技术细微差别，因为细节总是揭示真正的宝石。
- en: To work through this chapter, you must sign up for an OpenAI account at [https://beta.openai.com/signup](https://beta.openai.com/signup).
    If you haven’t done that, please do so now.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的学习，您必须在[https：//beta.openai.com/signup](https://beta.openai.com/signup)注册OpenAI帐户。如果您还没有这样做，请现在注册。
- en: OpenAI Playground
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Playground
- en: 'Your OpenAI developer account provides access to the API and infinite possibilities.
    We’ll start with Playground, a private web-based sandbox environment that allows
    you to experiment with the API, learn how its components work, and access developer
    documentation and the OpenAI community. We will then show you how to build robust
    prompts that generate favorable responses for your application. We’ll finish the
    chapter with examples of GPT-3 performing four NLP tasks: classification, named
    entity recognition (NER), summarization, and text generation.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您的OpenAI开发人员帐户提供对API和无限可能性的访问。我们将从Playground开始，这是一个私人基于Web的沙箱环境，让您可以尝试API，了解其组件的工作原理，并访问开发人员文档和OpenAI社区。然后，我们将向您展示如何构建强大的提示，以生成应用程序的良好回应。最后，本章将演示GPT-3执行四项自然语言处理任务的示例：分类、命名实体识别（NER）、摘要和文本生成。
- en: In an interview with Peter Welinder, VP of Product at OpenAI, we asked about
    the key advice on navigating the Playground for first-time users. He told us his
    advice depends on the persona of the user. If the user has a machine learning
    background, Peter encourages them to “start by forgetting the things that they
    already know, and just go to the playground and try to get GPT-3 to do what you
    wanted it to by just asking it”. He suggests users “imagine GPT-3 as a friend
    or a colleague that you're asking to do something. How would you describe the
    task that you want them to do? And then see how GPT-3 responds. And if it doesn't
    respond in the way you want, iterate on your instructions."
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在与OpenAI产品副总裁彼得·韦琳德进行的一次采访中，我们问及了关于首次使用Playground的关键建议。他告诉我们，他的建议取决于用户的角色。如果用户具有机器学习背景，彼得鼓励他们“首先忘记他们已经知道的事情，然后只需前往Playground并尝试让GPT-3做到你要求的事情”。他建议用户“想象GPT-3是你要求做某事的朋友或同事。你会如何描述你想让他们做的任务？然后看看GPT-3如何回应。如果它的回应不符合你的期望，就调整你的指示。”
- en: 'As YouTuber and NLP influencer [Bakz Awan](https://www.youtube.com/user/bakztfuture)
    puts it, “The non-technical people ask: Do I need a degree to use this? Do I need
    to know how to code to use it? Absolutely not. You can use the Playground. You
    don''t need to write a single line of code. You''ll get results instantly. Anybody
    can do this.”'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如YouTube博主和NLP影响者[Bakz Awan](https://www.youtube.com/user/bakztfuture)所说，“非技术人员会问：我需要学位才能使用吗？我需要懂编程才能使用吗？绝对不需要。您可以使用Playground。您无需编写一行代码。您将立即获得结果。任何人都可以做到这一点。”
- en: Before you start using the Playground, we recommend reading OpenAI’s “[Getting
    Started](https://beta.openai.com/docs/developer-quickstart)” guide and the developer
    documentation. You’ll be able to access it with your OpenAI account.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在您开始使用Playground之前，我们建议阅读OpenAI的“[入门指南](https://beta.openai.com/docs/developer-quickstart)”和开发人员文档。您可以使用您的OpenAI帐户访问它。
- en: 'Here are the steps to get started with the Playground:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是开始使用Playground的步骤：
- en: Login at https://openai.com. After you’ve authenticated, navigate to Playground
    from the main menu.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录https://openai.com。身份验证后，从主菜单中转到Playground。
- en: Look at the Playground screen (Figure 2-1).
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看Playground屏幕（图2-1）。
- en: ●        The big text box marked 1 is where you provide text input (prompts).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ● 标记为1的大文本框是您提供文本输入（提示）的位置。
- en: ●        The box marked 2 on the right is the parameter-setting pane, which
    allows you to tweak the parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ● 标记为2的右侧框是参数设置窗格，允许您调整参数。
- en: '●        The box marked 3 allows you to load an existing preset: an example
    prompt and Playground settings. You can provide your training prompt or load an
    existing preset.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ● 标记为3的框允许您加载现有的预设：示例提示和Playground设置。您可以提供您的培训提示或加载现有的预设。
- en: '![](img/image-0-2.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-2.jpg)'
- en: Figure 2-1\. The Playground Interface, screenshot taken on 10th January 2022
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-1\. Playground 界面，截图于2022年1月10日
- en: Select an existing QA preset (marked 3). This will automatically load the training
    prompt along with the associated parameter settings. Click the Generate button
    (marked 4 in Figure 2-1).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个现有的 QA 预设（标记为 3）。这将自动加载训练提示以及关联的参数设置。点击生成按钮（见图2-1中的标记 4）。
- en: The API processes your input and provides a response (called a completion) in
    the same text box. It also shows you the number of tokens utilized. A token is
    a numerical representation of words used to determine the pricing of API calls;
    we’ll discuss them later in this chapter.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: API 处理您的输入，并在同一文本框中提供响应（称为完成）。它还显示您所使用的令牌数量。令牌是用于确定 API 调用定价的单词的数值表示；我们将在本章后面讨论它们。
- en: At the bottom of the screen, you have a token count on the right, and on the
    left you have a Generate button (see Figure 2-2).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕底部，您可以看到右侧的令牌计数，左侧是一个生成按钮（见图2-2）。
- en: '![](img/image-0-3.jpg)Figure 2-2\. Q/A prompt completion along with token count'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/image-0-3.jpg)图2-2\. Q/A 提示完成及标记令牌计数'
- en: Every time you click the Generate button, GPT-3 takes the prompt and completions
    within the text input field (marked as 1) into account and treats them as part
    of your training prompt for the next completion.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次您点击生成按钮时，GPT-3 都会考虑文本输入字段中的提示和完成，并将它们视为下一个完成的一部分。
- en: 'Here is the prompt you can see in figure 2-2:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '这是您在图2-2中可以看到的提示：  '
- en: I am a highly intelligent question-answering bot. If you ask me a question rooted
    in truth, I will give you the answer. If you ask me a question that is nonsense,
    trickery, or has no clear answer, I will respond with "Unknown."
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一个高度智能的问答机器人。如果您问我一个根植于真相的问题，我会给您答案。如果您问我一个荒谬、欺诈或没有明确答案的问题，我会回答“未知”。
- en: 'Q: What is human life expectancy in the United States?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Q：美国的人类预期寿命是多少？
- en: 'A: Human life expectancy in the United States is 78 years.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: A：美国的人类预期寿命为78岁。
- en: 'Q: Who was the president of the United States in 1955?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Q：1955年美国总统是谁？
- en: 'A: Dwight D. Eisenhower was president of the United States in 1955\.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: A：1955年，德怀特·D·艾森豪威尔担任美国总统。
- en: 'Q: Which party did he belong to?'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Q：他属于哪个政党？
- en: 'A: He belonged to the Republican Party.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: A：他属于共和党。
- en: 'Q: What is the square root of a banana?'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Q：香蕉的平方根是多少？
- en: 'A: Unknown'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: A：未知
- en: 'Q: How does a telescope work?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Q：望远镜是如何工作的？
- en: 'A: Telescopes use lenses or mirrors to focus light and make objects appear
    closer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: A：望远镜使用透镜或镜子聚焦光线，使物体看起来更近。
- en: 'Q: Where were the 1992 Olympics held?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Q：1992年奥运会在哪里举行？
- en: 'And here is the completion:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完成：
- en: 'A: The 1992 Olympics were held in Barcelona, Spain.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: A：1992年奥运会在西班牙的巴塞罗那举办。
- en: Now that you understand the basic outline of the Playground, let's get into
    the nitty gritty of prompt engineering and design.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 Playground 的基本概述，让我们深入了解一下提示工程和设计的要点。
- en: Prompt Engineering and Design
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程与设计
- en: The OpenAI API radically changed the way we interact with an AI model, stripping
    out layers and layers of complicated programming languages and frameworks. Andrej
    Karpathy, director of AI at Tesla, said jokingly, as soon as GPT-3 was released,
    that programming 3.0 is all about prompt design (the meme he tweeted is in Figure
    2-3). There is a direct relation between the training prompt you provide and the
    completion quality you get. The structure and arrangement of your words heavily
    influence the output. Understanding prompt design is the key to unlocking GPT-3’s
    true potential.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API 彻底改变了我们与 AI 模型互动的方式，剥离了复杂的编程语言和框架。特斯拉人工智能主管安德烈·卡帕西（Andrej Karpathy）开玩笑地说，GPT-3
    发布后，编程 3.0 全部都是关于提示设计的（他发布的梗见图2-3）。您提供的训练提示与您获得的完成质量之间存在直接关系。您的词汇结构和排列方式会严重影响输出。理解提示设计是释放
    GPT-3 真正潜力的关键。
- en: '![](img/image-0-4.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-4.jpg)'
- en: Figure 2-3\. Meme source unknown; [tweeted by Andrej Karpathy](https://twitter.com/karpathy/status/1273788774422441984/photo/1)
    on June 18, 2020
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-3\. 梗源未知；[安德烈·卡帕西在 2020 年 6 月 18 日推文](https://twitter.com/karpathy/status/1273788774422441984/photo/1)
- en: 'While designing the training prompt, aim for a zero-shot response from the
    model: see if you can get the kind of response you want without priming the model
    with external training examples. If not, move forward by showing it a few examples
    rather than an entire dataset. The standard flow for designing a training prompt
    is to try for zero-shot first, then a few shots, and go for corpus-based fine-tuning
    (described below).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计训练提示时，要以零追问的方式获取模型的响应：看看你是否能在无需向模型提供外部训练例子的情况下获得你想要的响应。如果不能，那么请向模型展示一些例子，而不是整个数据集。设计训练提示的标准流程是首先尝试零追问，然后尝试一些追问，并进行基于语料库的精细调整（如下所述）。
- en: GPT-3 is the first step towards general-purpose artificial intelligence and
    thus has its limitations. It doesn’t know everything and can’t reason on a human
    level, but it’s competent when you know how to talk to it. That’s where the art
    of prompt engineering comes in.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3是通往通用人工智能的第一步，因此它有其局限性。它不知道所有事情，也无法像人类一样思考，但当你知道如何与它交流时，它很有能力。这就是提示工程的艺术所在。
- en: 'GPT-3 isn’t a truth-teller but an exceptional storyteller. It takes in the
    text input and attempts to respond with the text it thinks best completes it.
    If you give it a few lines from your favorite fiction novel, it will try to continue
    in the same style. It works by navigating through the context; without proper
    context, it can generate inconsistent responses. Let’s look at an example to understand
    how GPT-3 processes the input prompt and generates the output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3不是一个讲真话的人，而是一个出色的讲故事者。它接受文本输入并尝试以它认为最好的方式作出回应。如果你给它你最喜欢的小说中的几行文字，它会试图以相同的风格继续故事。它通过理解上下文来进行工作；没有合适的上下文，它可能产生不一致的回应。让我们通过一个例子来了解GPT-3如何处理输入提示并生成输出：
- en: 'Q: What is human life expectancy in the United States?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 美国的人类预期寿命是多少？'
- en: 'A:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'A:'
- en: If you provide a prompt like this to GPT-3 without any context, you ask it to
    look for general answers from its universe of training data. It will result in
    generalized and inconsistent responses as the model doesn’t know which part of
    the training data to answer these questions. [[6]](xhtml-0-12.xhtml#aid_54)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你向GPT-3提供这样的提示而没有任何上下文，你要求它从其训练数据的范围中寻找一般性的答案。这将导致一般化和不一致的回应，因为模型不知道如何从训练数据的哪一部分中回答这些问题。[[6]](xhtml-0-12.xhtml#aid_54)
- en: On the other hand, providing the proper context will exponentially improve the
    quality of responses. It simply limits the universe of training data that the
    model has to examine to answer a question, resulting in more specific and to-the-point
    responses.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，提供正确的上下文将指数级地提高回应的质量。它简单地限制了模型必须检查来回答问题的训练数据的范围，从而产生更具体和点到为止的回应。
- en: I am a highly intelligent question-answering bot. If you ask me a question rooted
    in truth, I will give you the answer. If you ask me a question that is nonsense,
    trickery or has no clear answer, I will respond with "Unknown."
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我是一个非常聪明的答疑机器人。如果你问我一个根植于事实的问题，我会给你答案。如果你问我一个毫无意义、诡计多端或者没有明确答案的问题，我会回答“未知”。
- en: 'Q: What is human life expectancy in the United States?'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 美国的人类预期寿命是多少？'
- en: 'A:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 'A:'
- en: You can think of GPT-3 processing the input similar to the human brain. We tend
    to give random responses when somebody asks us questions without proper context.
    It happens because it’s difficult to get to a precise response without proper
    direction or context. The same is the case with GPT-3\. Its universe of training
    data is so big that it makes it difficult to navigate to a correct response without
    any external context or direction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把GPT-3处理输入的方式想象成类似于人脑。在没有合适上下文的情况下，当有人问我们问题时，我们往往会给出随机的回应。这是因为没有合适的指引或上下文，很难给出精确的回应。GPT-3也是一样的情况。它的训练数据范围非常大，这使得它在没有外部上下文或指引的情况下很难导航到正确的响应。
- en: 'LLMs like GPT-3 can creatively write and answer factual questions in the right
    context. Here is our five-step formula for creating efficient and effective training
    prompts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 像GPT-3这样的语言模型能够在正确的上下文中创造性地写作和回答事实性问题。以下是我们创建高效有效的训练提示的五步公式：
- en: 1\. Define the problem you are trying to solve and what kind of NLP task it
    is, such as classification, Q&A, text generation, or creative writing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 确定你要解决的问题以及这是什么类型的自然语言处理任务，比如分类、问答、文本生成或创意写作。
- en: 2\. Ask yourself if there is a way to get a zero-shot solution. If you need
    external examples to prime the model for your use case, think hard.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 问问自己是否有零追问解决方案。如果你需要外部示例来为你的使用案例引导模型，就要认真思考。
- en: '3\. Now think of how you might encounter the problem in a textual fashion given
    the “text-in, text-out” interface of GPT-3\. Think about all the possible scenarios
    to represent your problem in textual form. For example, you want to build an ad
    copy assistant that can generate creative copy by looking at product name and
    description. To frame this goal in the “text-in, text-out” format, you can define
    the input as the product name and description and the output as the ad copy:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 现在考虑一下，通过 GPT-3 的“文本输入，文本输出”界面，您可能如何以文本方式遇到问题。考虑表示问题的所有可能情景。例如，您想构建一个广告文案助手，它可以通过查看产品名称和描述来生成创意文案。要将此目标框定为“文本输入，文本输出”格式，您可以将输入定义为产品名称和描述，输出为广告文案：
- en: 'Input: Betty’s Bikes, for price-sensitive shoppers'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：贝蒂自行车，适用于价格敏感的购物者
- en: 'Output: Low prices and huge selection. Free and fast delivery. Order online
    today!'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：低价和丰富选择。免费快速送货。今天在线订购！
- en: 4\. If you do end up using external examples, use as few as possible and try
    to incorporate diversity, capturing all the representations to avoid essentially
    overfitting the model or skewing the predictions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 如果您最终使用外部示例，请尽可能少地使用，并尝试包含多样性，捕捉所有表示以避免过度拟合模型或偏倚预测。
- en: These steps will act as a standard framework whenever you create a training
    prompt from scratch. Before you can build an end-to-end solution for your data
    problems, you need to understand more about how the API works. Let's dig deeper
    by looking at its components.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将成为每当您从头开始创建培训提示时的标准框架。在您可以为您的数据问题构建端到端解决方案之前，您需要更多了解 API 的工作方式。让我们通过查看其组件来深入了解。
- en: Breaking Down OpenAI API
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 拆解 OpenAI API
- en: '![](img/image-0-5.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-5.jpg)'
- en: Figure 2-4\. Components of API
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-4\. API 的组件
- en: Table 2-1 shows an overview of the components in the OpenAI API.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2-1 显示了 OpenAI API 中组件的概述。
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Component
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 组件
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Function
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 功能
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Execution engine
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Determines the language model used for execution
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 确定用于执行的语言模型
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Response length
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 响应长度
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Sets a limit on how much text the API includes in its completion
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 API 在其完成中包含的文本量的限制
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Temperature and Top P
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和 Top P
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Temperature controls the randomness of the response, represented as a range
    from 0 to 1.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 温度控制响应的随机性，表示为 0 到 1 的范围。
- en: Top P controls how many random results the model should consider for completion,
    as suggested by
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 控制模型应考虑的随机结果数量，正如
- en: the temperature; it determines the scope of randomness.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 温度；它决定了随机性的范围。
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Frequency penalty and
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚和
- en: Presence penalty
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Frequency penalty decreases the likelihood that the model will repeat the same
    line verbatim by
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 频率惩罚减少了模型完全重复相同行的可能性
- en: “punishing” it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: “惩罚”它。
- en: Presence penalty increases the likelihood that it will talk about new topics.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 存在惩罚增加了它谈论新话题的可能性。
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Best of
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳完成
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Lets you specify the number of completions (n) to generate on the server side
    and returns the best of “n” completions
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 允许您指定要在服务器端生成的完成数量（n），并返回“n”个完成中的最佳完成
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Stop sequence
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 停止序列
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Specifies a set of characters that signals the API to stop generating completions
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 指定一组字符，表示 API 停止生成完成
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Inject start and restart
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注入起始和重启
- en: text
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文本
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Inject start text allows you to insert text at the beginning of the completion.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注入起始文本允许您在完成的开头插入文本。
- en: Inject restart text allows you to insert text at the end of the completion.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注入重启文本允许您在完成的末尾插入文本。
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Show probabilities
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 显示概率
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Lets you debug the text prompt by showing the probability of tokens that the
    model can generate for a given input.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过显示模型可以为给定输入生成的令牌的概率来调试文本提示。
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 2-1\. Components in the OpenAI API
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. OpenAI API 中的组件
- en: Here is an overview of the components of GPT-3 API. We will discuss all of these
    components in more detail throughout the chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 GPT-3 API 组件的概述。我们将在整章中更详细地讨论所有这些组件。
- en: Execution Engine
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎
- en: 'The execution engine determines the language model used for execution. Choosing
    the right engine is the key to determining your model’s capabilities and getting
    the right output. GPT-3 has four execution engines of varying sizes and capabilities:
    Davinci, Ada, Babbage, and Curie. Davinci is the most powerful and the Playground’s
    default.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎决定了用于执行的语言模型。选择正确的引擎是确定模型能力并获得正确输出的关键。GPT-3 具有四种不同尺寸和能力的执行引擎：达芬奇、艾达、巴贝奇和居里。达芬奇是最强大的，也是
    Playground 的默认引擎。
- en: Response Length
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 响应长度
- en: 'The response length limits how much text the API includes in its completion.
    Because OpenAI charges by the length of text generated per API call (as noted,
    this is translated into tokens or numeric representations of words), response
    length is a crucial parameter for anyone on a budget. A higher response length
    will use more tokens and cost more. For example, suppose you do a classification
    task. In that case, it is not a good idea to set the response text dial to 100:
    the API could generate irrelevant texts and use extra tokens that will incur charges
    on your account. The API supports a maximum of 2048 tokens combined in the prompt
    and completion. So, while using the API you need to be careful that prompt and
    expected completion don’t exceed the maximum response length to avoid abrupt completions.
    If your use case involves large text prompts and completions, the workaround is
    to think of creative ways to solve problems within token limits, like condensing
    your prompt, breaking the text into smaller pieces, chaining multiple requests.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 响应长度限制了 API 在其完成中包含的文本量。因为 OpenAI 根据每次 API 调用生成的文本长度收费（值得注意的是，这被转换为标记或单词的数值表示），响应长度对于预算有限的任何人来说都是至关重要的参数。较高的响应长度将使用更多的标记并且成本更高。例如，假设您做一个分类任务。在这种情况下，将响应文本调节器设置为
    100 不是一个好主意：API 可能会生成无关的文本并使用额外的标记，这将导致您的账户产生费用。 API 支持在提示和完成中最多使用 2048 个标记。因此，在使用
    API 时，您需要注意提示和预期完成不要超过最大响应长度，以避免突然的完成。如果您的使用案例涉及大量文本提示和完成，解决方法是想出创造性的方式来在标记限制内解决问题，例如简化您的提示，将文本拆分为较小的部分，链式发送多个请求。
- en: Temperature and Top P
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和 Top P
- en: The temperature dial controls the creativity of the response, represented as
    a range from 0 to 1\. A lower temperature value means the API will predict the
    first thing the model sees, resulting in the correct text, but rather dull, with
    minor variation. On the other hand a higher temperature value means the model
    evaluates possible responses that could fit into the context before predicting
    the result. The generated text will be more diverse, but there is a higher possibility
    of grammar mistakes and the generation of nonsense.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 温度调节器控制响应的创造力，表示为从 0 到 1 的范围。较低的温度值意味着 API 将预测模型所见的第一件事情，导致正确文本，但相当乏味，变化微小。另一方面，较高的温度值意味着模型在预测结果之前评估可能符合上下文的响应。生成的文本将更加多样化，但有更高的语法错误和无意义生成的可能性。
- en: 'Top P controls how many random results the model should consider for completion,
    as suggested by the temperature dial; it determines the scope of randomness. Top
    P’s range is from 0 to 1\. A value close to zero means the random responses will
    be limited to a certain fraction: for example, if the value is 0.1, then only
    10% of the random responses will be considered for completion. This makes the
    engine deterministic, which means that it will always generate the same output
    for a given input text. If the value is set to 1, the API will consider all responses
    for completion, taking risks and coming up with creative responses. A lower value
    limits creativity; a higher value expands horizons.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Top P 控制模型在完成时应考虑多少个随机结果，如温度调节器所示；它确定了随机性的范围。 Top P 的范围从 0 到 1。接近零的值意味着随机响应将受限于某个比例：例如，如果值为
    0.1，则只有 10% 的随机响应将被视为完成。这使得引擎具有确定性，这意味着对于给定的输入文本，它将始终生成相同的输出。如果值设置为 1，则 API 将考虑所有响应以进行完成，承担风险并提出创新性的响应。较低的值限制了创造力；较高的值扩展了视野。
- en: 'Temperature and Top P have a very significant effect on output. It can sometimes
    be confusing to get your head around when and how to use them to get the correct
    output. The two are correlated: changing the value of one will affect the other.
    So by setting Top P to 1, you can allow the model to unleash its creativity by
    exploring the entire spectrum of responses and control the randomness by using
    the temperature dial.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 温度和Top P对输出具有非常显著的影响。有时，要弄清楚何时以及如何使用它们以获得正确的输出会令人困惑。两者是相关的：更改一个值将影响另一个值。因此，通过将Top
    P设置为1，您可以允许模型通过探索完整的响应谱来释放其创造力，并通过使用温度旋钮来控制随机性。
- en: 'TIP: We always advise changing either Top P or temperature and keeping the
    other dial set at 1\.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：我们建议始终更改Top P或温度，并将另一个旋钮保持在1的位置。
- en: Large language models rely on probabilistic approaches rather than conventional
    logic. Depending on how you set the model's parameters, they can generate various
    responses for the same input. The model tries to find the best probabilistic match
    within the universe of data it has been trained on instead of looking for a perfect
    solution every time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型依赖概率方法而不是传统逻辑。根据您设置模型的参数方式，它们可以为相同的输入生成各种响应。模型试图在其被训练的数据宇宙中找到最佳的概率匹配，而不是每次寻求完美的解决方案。
- en: 'As we mentioned in Chapter 1, GPT-3’s universe of training data is vast, consisting
    of a variety of publicly available books, internet forums, and Wikipedia articles
    specially curated by OpenAI, allowing it to generate a wide array of completions
    for a given prompt. That’s where temperature and Top P, sometimes called the “creativity
    dials,” come in: you can tune them to generate more natural or abstract responses
    with an element of playful creativity.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第1章中提到的，GPT-3的大量训练数据宇宙包括各种公开可用的书籍，互联网论坛和由OpenAI专门策划的维基百科文章，使其能够根据提示生成各种完成。这就是温度和Top
    P，有时被称为“创造力旋钮”的地方：您可以调整它们以产生更自然或抽象的响应，并带有一些俏皮的创造力。
- en: 'Let’s say you will use GPT-3 to create names for your startup. You can set
    the temperature dial higher to get the most creative response. When we spent days
    and nights trying to come up with the perfect name for our startup, we dialed
    the temperature. GPT-3 came to the rescue and helped us to arrive at a name we
    love: Kairos Data Labs.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你要使用GPT-3为你的创业公司创建名字。你可以将温度设置更高以获得最有创意的回应。当我们日日夜夜地努力想出我们创业公司的完美名称时，我们拨动了温度。GPT-3挺身而出，帮助我们找到了一款我们喜欢的名字：Kairos
    Data Labs。
- en: 'On other occasions, your task might require little to no creativity: classification
    and question-answering tasks, for example. For these, keep the temperature lower.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，你的任务可能需要很少或没有创造力：例如分类和问答任务。对于这些任务，要将温度设置较低。
- en: Let’s look at a simple classification example that categorizes companies into
    general buckets or categories based on their names.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的分类例子，根据他们的名字将公司分为常规的类别或类别。
- en: '![](img/image-0-6.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-6.jpg)'
- en: Figure 2-5\. Temperature component
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-5. 温度组件
- en: As you can see in Figure 2-5, we have again used temperature to control the
    degree of randomness. You can also do this by changing Top P while keeping the
    temperature dial set to 1.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图2-5中所看到的，我们再次使用温度来控制随机程度。您也可以通过更改Top P来实现此目的，同时将温度旋钮设置为1。
- en: Frequency and Presence Penalties
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 频率和存在惩罚
- en: Like the temperature and Top P dials, the frequency penalty, and presence penalty
    dials consider text prompts (the previous completion plus the new input) instead
    of internal model parameters when deciding on output. The existing text thus influences
    the new completions. The frequency penalty decreases the likelihood of the model
    repeating the same line verbatim by “punishing” it. The presence penalty increases
    the likelihood that it will talk about new topics.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 就像温度和Top P旋钮一样，频率惩罚和存在惩罚旋钮考虑文本提示（先前完成加上新的输入）而不是内部模型参数来决定输出。现有文本因此会影响新的完成。频率惩罚通过“惩罚”来减少模型重复相同行的可能性。存在惩罚增加了它会谈论新主题的可能性。
- en: These come in handy when preventing the exact completion text from being repeated
    across multiple completions. Although these dials are similar, there is one crucial
    distinction. The frequency penalty is applied if the suggested text output is
    repeated (for example, the model used the exact token in previous completions
    or during the same session) and the model chooses an old output over a new one.
    The presence penalty is applied if a token is present in a given text at all.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这在防止完全完成文本在多个完成中重复出现时非常有用。虽然这些刻度类似，但有一个关键区别。如果建议的文本输出重复（例如，模型在以前的完成中使用了相同的标记或在同一会话期间选择旧的输出），则应用频率惩罚。如果一个标记在给定文本中存在，则应用存在惩罚。
- en: Best of
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳
- en: GPT-3 uses the best of feature to generate multiple completions on the server
    side, evaluate them behind the scenes, and then provide you with the best probabilistic
    result. Using the “best of ” parameter, you can specify the number of completions
    (n) to generate on the server side. The model will return the best of n completions
    (the one with the lowest log probability per token).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 使用最佳功能在服务器端生成多个完成，并在幕后评估它们，然后为您提供最佳的概率结果。使用“最佳”参数，您可以指定在服务器端生成的完成次数（n）。模型将返回
    n 次完成中的最佳完成（每个标记的对数概率最低的那个）。
- en: 'This enables you to evaluate multiple prompt completions in a single API call
    rather than calling the API repeatedly to check the quality of different completions
    for the same input. However, using “best of ” is expensive: it costs n times the
    tokens in the prompt. For example, if you set the “best of ” value to 2, then
    you will be charged double the tokens present in the input prompt because on the
    backend the API will generate two completions and show you the best one.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这使您可以在单个 API 调用中评估多个提示完成，而不是重复调用 API 来检查相同输入的不同完成的质量。但是，“最佳”使用是昂贵的：它的成本是输入提示中的标记的
    n 倍。例如，如果您将“最佳”值设置为 2，那么您将被收取输入提示中标记数量的两倍，因为在后台，API 将生成两个完成，并显示给您最佳的一个。
- en: “Best of ” can range from 1 to 20 depending on your use case. If your use case
    serves clients for whom the quality of output needs to be consistent, then you
    can set the “best of ” value to a higher number. On the other hand, if your use
    case involves too many API invocations, then it makes sense to have a lower “best
    of ” value to avoid unnecessary latency and costs. We advise keeping response
    length minimal while generating multiple prompts using the “best of ” parameter
    to avoid additional charges.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳”值可以从 1 到 20 不等，具体取决于您的用例。如果您的用例为客户提供需要一致质量的输出，那么可以将“最佳”值设为更高的数字。另一方面，如果您的用例涉及太多的
    API 调用，那么将“最佳”值设为更低的数字可以避免不必要的延迟和成本。我们建议在使用“最佳”参数生成多个提示时，尽量保持响应长度最小，以避免额外收费。
- en: Stop Sequence
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 停止序列
- en: A stop sequence is a set of characters that signal the API to stop generating
    completions. It helps avoid unnecessary tokens, an essential cost-saving feature
    for regular users.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 停止序列是一组字符，用于通知 API 停止生成完成。这有助于避免不必要的标记，是常规用户不可或缺的节省成本的功能。
- en: You can provide up to 4 sequences for the API to stop generating further tokens.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为 API 提供最多 4 个序列，以停止生成进一步的标记。
- en: 'Let''s look at the example language translation task in Figure 2-6 to understand
    how the stop sequence works. In this example, English phrases are translated into
    French. We use the restart sequence “English:” as a stop sequence: whenever the
    API encounters that phrase, it will stop generating new tokens.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看图 2-6 中的示例语言翻译任务，以了解停止序列的工作原理。在这个示例中，英文短语被翻译成法语。我们使用重新启动序列“English:”作为停止序列：每当
    API 遇到该短语时，它将停止生成新的标记。
- en: '![](img/image-0-7.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-7.jpg)'
- en: Figure 2-6\. Stop Sequence component
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-6\. 停止序列组件
- en: Inject Start Text and Inject Restart Text
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注入开始文本和注入重新启动文本
- en: The inject start text and inject restart text parameters allow you to insert
    text at the beginning or end of the completion, respectively. You can use them
    to keep a desired pattern going. Often, these settings work in tandem with the
    stop sequence, as in our example. The prompt has a pattern where an English sentence
    is provided with the prefix “English:” (the restart text), and the translated
    output is generated with the prefix “French:” (the start text). As a result, anyone
    can easily distinguish between the two and create a training prompt that both
    the model and the user can clearly comprehend.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注入起始文本和注入重启文本参数允许您分别在完成的开头或结尾插入文本。您可以使用它们来保持所需的模式。通常，这些设置与停止序列一起工作，就像我们的示例中一样。提示具有模式，其中提供了一个带有前缀“英语：”（重启文本）的英语句子，并且生成的翻译输出具有前缀“法语：”（起始文本）。因此，任何人都可以轻松区分两者，并创建一个模型和用户都可以清楚理解的训练提示。
- en: Whenever we run the model for such prompts, the model automatically injects
    a start text “French:” before the output and a restart text “English:” before
    the next input, so that this pattern can be sustained.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们为这样的提示运行模型时，模型会自动在输出前注入一个起始文本“法语：”，在下一个输入前注入一个重启文本“英语：”，以便保持这种模式的持续性。
- en: Show Probabilities
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 展示概率
- en: The show probabilities parameter is at the bottom of the Playground settings
    pane. In conventional software engineering, developers use a debugger to troubleshoot
    (debug) a piece of code. You can use the show probabilities parameter to debug
    your text prompt. Whenever you select this parameter, you will see highlighted
    text. Hovering over it with the cursor will show a list of tokens that the model
    can generate for the particular input specified, with their respective probabilities.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: “显示概率”参数位于 Playground 设置窗格的底部。在传统软件工程中，开发人员使用调试器来调试一段代码。您可以使用“显示概率”参数来调试您的文本提示。每当您选择此参数时，将会看到突出显示的文本。将鼠标悬停在上面，将显示一个列表，其中包含模型可以为特定指定输入生成的标记，以及它们各自的概率。
- en: 'You can use this parameter to examine your options. In addition, it can make
    it easier to see alternatives that might be more effective. The “show probabilities”
    parameter has three settings:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此参数来检查您的选项。此外，它可以使更有效的替代方案更容易看到。 “显示概率”参数有三个设置：
- en: Most Likely
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最有可能
- en: Lists the tokens most likely to be considered for completion in decreasing order
    of probability.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 按概率降序列出最可能用于完成的标记。
- en: Least Likely
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最不可能
- en: Lists the tokens least likely to be considered for completion in decreasing
    order of probability.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 按概率降序列出最不可能用于完成的标记。
- en: Full Spectrum
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 完整频谱
- en: Shows the entire universe of tokens that could be selected for completion.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 显示可以选择用于完成的所有标记的完整宇宙。
- en: 'Let’s look at this parameter in the context of a simple prompt. We want to
    start the output sentence with a simple, well-known phrase: “Once upon a time”.
    We provide the API with the prompt “Once upon a” and then check the Most Likely
    option in the show probabilities tab.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在简单提示的上下文中查看此参数。我们想要以简单、众所周知的短语“从前有个人”开始输出句子。我们向 API 提供提示“从前有个” ，然后在显示概率选项卡中选择最有可能选项。
- en: As Figure 2-7 shows, it generates “time” as the response. Because we have set
    the “show probabilities” parameter to “Most Likely,” the API indicates the response
    and a list of possible options along with their probabilities.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 2-7 所示，它生成“时间”作为响应。因为我们将“显示概率”参数设置为“最有可能”，所以 API 指示响应和可能的选项列表以及它们的概率。
- en: Now that you’ve had an overview, let’s look at these components in more detail.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了一个概述，让我们更详细地查看这些组件。
- en: '![](img/image-0-8.jpg)Figure 2-7\. Show Probabilities component showing the
    most likely tokens'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/image-0-8.jpg)图 2-7\. 展示概率组件显示最可能的标记'
- en: Execution Engines
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 执行引擎
- en: As noted in Figure 2-7, the OpenAI API offers four different execution engines,
    differentiated by parameters and performance capabilities. Execution engines power
    OpenAI API. They serve as “autoML” solutions, providing automated ML methods and
    processes to make machine learning available for non-experts. They are easy to
    configure and adapt to a given dataset and task.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 2-7 所示，OpenAI API 提供了四种不同的执行引擎，通过参数和性能能力区分。执行引擎驱动 OpenAI API。它们作为“autoML”解决方案，提供自动化的
    ML 方法和流程，使机器学习对非专家可用。它们易于配置，并适应给定的数据集和任务。
- en: 'The four primary execution engines were named after famous scientists in alphabetical
    order: Ada (named after Ada Lovelace), Babbage (named after Charles Babbage),
    Curie (named after Madame Marie Curie), and Davinci (named after Leonardo da Vinci).
    Let’s deep-dive into each of these execution engines to understand when to use
    which execution engine when working with GPT-3:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个主要的执行引擎按照字母顺序以著名科学家命名：艾达（以艾达·洛夫莱斯命名）、巴贝奇（以查尔斯·巴贝奇命名）、居里（以玛丽·居里夫人命名）和达芬奇（以莱昂纳多·达·芬奇命名）。让我们深入了解每个执行引擎，以便了解在使用
    GPT-3 时何时使用哪个执行引擎：
- en: DaVinci
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 达芬奇
- en: Da Vinci is the largest execution engine and the default when you first open
    the Playground. It can do anything the other engines can, often with fewer instructions
    and better outcomes. However, the trade-off is that it costs more per API call
    and is slower than other engines. You might want to use other engines to optimize
    cost and runtimes.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 达芬奇是最大的执行引擎，在您首次打开 Playground 时是默认选项。它可以做任何其他引擎可以做的事情，通常需要更少的指令并获得更好的结果。然而，这样做的代价是每次
    API 调用的成本更高，而且比其他引擎更慢。您可能希望使用其他引擎来优化成本和运行时间。
- en: 'TIP: We recommend starting with Davinci because of its superior capabilities
    when testing new ideas and prompts. Experimenting with Davinci is a great way
    to determine what the API can do. You can slowly move down the ladder to optimize
    budgets and runtimes as you become comfortable with your problem statement. Once
    you have an idea of what you want to accomplish, you can either stay with Davinci
    (if cost and speed are not a concern) or move on to Curie or other less costly
    engines and try to optimize the output around its capabilities. You can use [OpenAI’s
    Comparison Tool](https://gpttools.com/comparisontool) to generate an Excel spreadsheet
    that compares engines’ outputs, settings, and response times.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：我们建议从达芬奇开始，因为它在测试新想法和提示时具有卓越的能力。尝试达芬奇是了解 API 能做什么的好方法。随着您对问题陈述的理解逐渐加深，您可以逐渐降低预算和运行时间以优化。一旦您确定了想要实现的目标，您可以选择继续使用达芬奇（如果成本和速度不是问题）或转向居里或其他成本较低的引擎，并尝试根据其能力优化输出。您可以使用[OpenAI
    的比较工具](https://gpttools.com/comparisontool)生成一个 Excel 电子表格，比较引擎的输出、设置和响应时间。
- en: Davinci should be your first choice for tasks that require understanding the
    content, like summarizing meeting notes or generating creative ad copy. It’s great
    at solving logic problems and explaining the motives of fictional characters.
    It can write a story. Davinci has also been able to solve some of the most challenging
    AI problems involving cause and effect.[[7]](xhtml-0-12.xhtml#aid_85)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 达芬奇应该是您处理需要理解内容的任务的首选，比如总结会议记录或生成创意广告文案。它擅长解决逻辑问题并解释虚构角色的动机。它可以编写故事。达芬奇也已经能够解决一些涉及因果关系的最具挑战性的
    AI 问题。[[7]](xhtml-0-12.xhtml#aid_85)
- en: Curie
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 居里
- en: Curie aims to find an optimal balance between power and speed that is very important
    for performing high-frequency tasks like classification on an immense scale or
    putting a model into production.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 居里旨在找到功率和速度之间的最佳平衡，这对于执行大规模分类或将模型投入生产等高频任务非常重要。
- en: Curie is also quite good at answering questions, performing Q&As, and serving
    as a general-purpose chatbot. For instance, if you build a customer-support chatbot,
    you might choose Curie to serve high-volume requests faster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 居里也非常擅长回答问题、执行问答和作为通用聊天机器人。例如，如果您构建了一个客服聊天机器人，您可能会选择居里来更快地处理高流量请求。
- en: While Davinci is stronger at analyzing complicated texts, Curie can perform
    with low latency and lightning-fast speed. It is always sensible to figure out
    your use case and do a cost-benefit analysis before choosing Davinci over Curie
    in production.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及分析复杂文本时，达芬奇更擅长，而居里则能以低延迟和闪电般的速度执行任务。在选择在生产中使用达芬奇而不是居里之前，理清您的用例并进行成本效益分析总是明智的。
- en: Babbage
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 巴贝奇
- en: Babbage is faster than Cuire but not capable of performing tasks that involve
    understanding complex intent. However, it is quite capable and is preferable when
    it comes to semantic search rankings and analyzing how well documents match up
    with search queries. It’s less expensive than Curie and Davinci and is preferred
    for simple problems involving frequent API calls.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 巴贝奇比居里更快，但不能执行涉及理解复杂意图的任务。然而，它非常能干，且在语义搜索排名和分析文档与搜索查询匹配程度方面表现出色。它比居里和达芬奇便宜，并且在涉及频繁
    API 调用的简单问题上更受欢迎。
- en: Ada
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 艾达
- en: Ada is the fastest and least expensive of all the engines available. It can
    perform simple tasks that do not require a nuanced understanding of context, like
    parsing text, correcting grammar, or simple classification. It is often possible
    to improve Ada's performance by providing more context with the input. Ada can
    be the preferred model of choice for use cases involving frequent API invocations,
    as it can achieve results similar to bigger models with suitable settings. The
    more you experiment with the API parameters, the more understanding you will gain
    of what settings work for your use case.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Ada 是所有可用引擎中速度最快且价格最低的。它可以执行一些不需要细致理解上下文的简单任务，比如解析文本、纠正语法或简单分类。通过在输入中提供更多上下文，通常可以改善
    Ada 的性能。Ada 可以成为频繁调用 API 的使用案例的首选模型，因为它可以通过合适的设置实现与更大模型类似的结果。您对 API 参数进行的实验越多，就越能理解哪些设置适用于您的使用案例。
- en: Instruct Series
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 指令系列
- en: Based on the four primary models, OpenAI has launched a series of [InstructGPT](https://openai.com/blog/instruction-following/)
    models that are better at understanding the instructions and following them while
    being less toxic and more truthful than the original GPT-3\. They have been developed
    using techniques coming from OpenAI’s alignment research. These models are trained
    with humans in the loop and are now deployed as the default language models on
    OpenAI API.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 基于四个主要模型，OpenAI 推出了一系列[InstructGPT](https://openai.com/blog/instruction-following/)
    模型，它们在理解说明并遵循说明方面比原始的 GPT-3 更好，同时 less toxic and more truthful。它们是使用 OpenAI 对齐研究中的技术进行开发的。这些模型是在人类参与的情况下进行训练的，现在已经作为
    OpenAI API 的默认语言模型部署。
- en: '![](img/image-0-9.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-9.jpg)'
- en: Figure 2-8\. Output generated by InstructGPT Davinci
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-8\. InstructGPT Davinci 生成的输出
- en: '![](img/image-0-10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-10.jpg)'
- en: Figure 2-9\. Output generated by GPT Davinci
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-9\. GPT Davinci 生成的输出
- en: 'Our input:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入：
- en: “Write a story about AI for a 5-year-old.”
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: “为一个 5 岁的孩子写一个关于人工智能的故事。”
- en: 'InstructGPT output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT 输出：
- en: “Robot was very happy. He had just been born, and he was already the smartest
    robot in the world. He could do anything that any other robot could do, but he
    could also think for himself.”
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: “机器人非常快乐。他刚刚出生，他已经是世界上最聪明的机器人了。他可以做任何其他机器人能做的事情，但他也可以为自己思考。”
- en: 'GPT output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 输出：
- en: “This project is an attempt to make an AI that can produce fictional stories.
    It is a work in progress. Fictionality The aim of this project is to create an
    AI that can create fictional stories,”
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: “这个项目是尝试制造一个能够产生虚构故事的 AI。这是一个正在进行中的工作。虚构性这个项目的目标是创建一个可以创作虚构故事的 AI。”
- en: To make the process of building prompts efficient, OpenAI decided to publicly
    launch text-davinci-001, text-curie-001, text-babbage-001, and text-ada-001\.
    With clear instructions, these models can produce better results than their base
    counterparts and are now the [default models of the API](https://help.openai.com/en/articles/5832130-what-s-changed-with-engine-names-and-best-practices).
    This series is important in bridging the gap between how humans think and models
    operate.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使构建提示的过程更加高效，OpenAI 决定公开启动 text-davinci-001、text-curie-001、text-babbage-001
    和 text-ada-001。通过清晰的说明，这些模型可以产生比它们的基本版本更好的结果，并且现在是 [API 的默认模型](https://help.openai.com/en/articles/5832130-what-s-changed-with-engine-names-and-best-practices)
    。这个系列对于弥合人类思维方式和模型操作方式之间的差距具有重要意义。
- en: 'TIP: We recommend using this model series as your default for all text-related
    tasks. The base version of GPT-3 models are available as davinci, curie, babbage,
    and ada and are meant to be used with the fine-tuning, search, classification,
    and answers endpoints.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 'TIP: 我们建议将这个模型系列作为您所有与文本相关的任务的默认选择。GPT-3 模型的基本版本包括 davinci、curie、babbage 和
    ada，它们的使用适用于微调、搜索、分类和答案终端。 '
- en: Endpoints
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 终端点
- en: 'The Playground is a graphical web interface that calls the OpenAI API behind
    the scenes, but there are several other ways to call the API. To do this, you
    will need to get familiar with its endpoints: the remote APIs that communicate
    back and forth when they are called. This section will familiarize you with the
    functionality and usage of six API endpoints.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 游乐场是一个图形化的 web 接口，它在幕后调用 OpenAI API，但还有其他一些调用 API 的方式。为此，您需要熟悉其终端点：即在调用时来回通信的远程
    API。本节将使您熟悉六个 API 终端点的功能和用法。
- en: List Engines
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列出引擎
- en: 'The list engine endpoint, also known as the “metadata API,” provides a list
    of available engines and specific metadata associated with each engine, such as
    owner and availability. To access it, you can hit the following URI with the HTTP
    GET method without passing any request parameters:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列出引擎端点，也称为“元数据 API”，提供了可用引擎的列表以及与每个引擎相关的特定元数据，例如所有者和可用性。要访问它，您可以使用 HTTP GET
    方法命中以下 URI，而无需传递任何请求参数：
- en: GET https://api.openai.com/v1/engines
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: GET https://api.openai.com/v1/engines
- en: Retrieve Engines
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 检索引擎
- en: 'When you provide an engine name to the retrieve engine endpoint, it returns
    detailed metadata information about that engine. To access, hit the following
    URI with the HTTP GET method without passing any request parameters:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当您向检索引擎端点提供引擎名称时，它将返回有关该引擎的详细元数据信息。要访问，请使用 HTTP GET 方法命中以下 URI，而无需传递任何请求参数：
- en: GET [https://api.openai.com/v1/engines/{engine_id](https://api.openai.com/v1/engines/%7Bengine_id)}
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GET [https://api.openai.com/v1/engines/{engine_id](https://api.openai.com/v1/engines/%7Bengine_id)}
- en: Completions
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 完成
- en: 'Completions is GPT-3’s most famous and widely used endpoint. It simply takes
    in the text prompt as input and returns the completed response as output. It uses
    the HTTP POST method and requires an engine ID as part of the URI path. As part
    of the HTTP Body, the Completions endpoint accepts several additional parameters
    discussed in the previous section. Its signature is:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 完成是 GPT-3 最著名和广泛使用的端点。它只需将文本提示作为输入，并将完成的响应作为输出返回。它使用 HTTP POST 方法，并且作为 URI 路径的一部分需要一个引擎
    ID。作为 HTTP Body 的一部分，完成端点接受在前一节中讨论的几个附加参数。其签名是：
- en: POST https://api.openai.com/v1/engines/{engine_id}/completions
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: POST https://api.openai.com/v1/engines/{engine_id}/completions
- en: Semantic Search
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索
- en: The semantic search endpoint enables you to provide a query in natural language
    to search a set of documents, which can be words, sentences, paragraphs, or even
    longer texts. It will score and rank the documents based on how semantically related
    they are to the input query. For example, if you provide the documents ["school",
    "hospital", "park"] and query "the doctor", you’ll get a different similarity
    score for each document.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 语义搜索端点使您能够提供自然语言查询以搜索一组文档，这些文档可以是单词、句子、段落，甚至更长的文本。它将根据文档与输入查询的语义相关性对文档进行评分和排名。例如，如果您提供文档["学校"、"医院"、"公园"]和查询"医生"，则每个文档将获得不同的相似性分数。
- en: The similarity score is a positive score that usually ranges from 0 to 300 (but
    can sometimes go higher), where a score above 200 usually indicates that the document
    is semantically similar to the query. The higher the similarity score, the more
    semantically similar the document is to the query (in this example, "hospital"
    will be most similar to "the doctor"). You can provide up to 200 documents as
    part of your request to the API.[[8]](xhtml-0-12.xhtml#aid_48)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 相似性分数是一个正分数，通常范围从 0 到 300（但有时可能更高），其中分数超过 200 通常表示文档在语义上与查询相似。相似性分数越高，文档与查询的语义相似性就越高（在此示例中，“医院”将与“医生”最相似）。您可以作为
    API 请求的一部分提供最多 200 个文档。
- en: 'Following is the signature for the semantic search endpoint:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是语义搜索端点的签名：
- en: POST [https://api.openai.com/v1/engines/{engine_id}/search](https://api.openai.com/v1/engines/%7Bengine_id%7D/search)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: POST [https://api.openai.com/v1/engines/{engine_id}/search](https://api.openai.com/v1/engines/%7Bengine_id%7D/search)
- en: Files
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 文件
- en: 'The files endpoint can be used across different endpoints like answers, classification,
    and semantic search. It is used to upload documents or files to the OpenAI storage,
    which is accessible through the API features. The same endpoint can be used with
    different signatures to perform the following tasks:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 文件端点可以用于不同的端点，如答案、分类和语义搜索。它用于将文档或文件上传到 OpenAI 存储，该存储可通过 API 功能访问。相同的端点可以使用不同的签名来执行以下任务：
- en: List Files
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 列出文件
- en: It simply returns the list of files that belong to the user's organization or
    that are linked to a particular user’s account. It's an HTTP GET call that doesn’t
    require any parameters to be passed with the request.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 它简单地返回属于用户组织或链接到特定用户帐户的文件列表。这是一个 HTTP GET 调用，不需要传递任何请求参数。
- en: GET https://api.openai.com/v1/files
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: GET https://api.openai.com/v1/files
- en: Upload Files
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上传文件
- en: It is used to upload a file that contains documents to be used across various
    endpoints. It uploads the documents to the already allocated internal space by
    OpenAI for the users' organization. It is an HTTP POST call that requires the
    file path to be added to the API request.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 用于上传包含要在各种端点中使用的文档的文件。它将文档上传到 OpenAI 为用户组织分配的内部空间。它是一个需要将文件路径添加到 API 请求中的 HTTP
    POST 调用。
- en: ​POST https://api.openai.com/v1/files
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ​POST https://api.openai.com/v1/files
- en: Retrieve File
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 检索文件
- en: 'It returns the information about a specific file by just providing the file
    id as the request parameter. Following is the signature for the Retrieve endpoint:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过提供文件 ID 作为请求参数即可返回关于特定文件的信息。以下是 Retrieve 端点的签名：
- en: ​GET https://api.openai.com/v1/files/{file_id}
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ​GET https://api.openai.com/v1/files/{file_id}
- en: Delete File
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 删除文件
- en: 'It deletes the specific file by providing the file as the request parameter.
    Following is the signature for the Delete endpoint:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供文件作为请求参数来删除特定文件。以下是 Delete 端点的签名：
- en: ​DELETE https://api.openai.com/v1/files/{file_id}
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ​DELETE https://api.openai.com/v1/files/{file_id}
- en: Embeddings
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入
- en: Another experimental endpoint of the API is Embeddings. Embeddings are the core
    of any machine learning model and allow to capture of semantics from the text
    by converting it into high-dimensional vectors. Currently, developers tend to
    use open-source models like the BERT series to create embeddings for their data
    that can be used for a variety of tasks like recommendation, topic modeling, semantic
    search, etc.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: API 的另一个实验性端点是嵌入。嵌入是任何机器学习模型的核心，并允许通过将其转换为高维向量从文本中捕获语义。目前，开发人员倾向于使用开源模型，如 BERT
    系列，为其数据创建嵌入，这些数据可以用于各种任务，如推荐、主题建模、语义搜索等。
- en: 'OpenAI realized that GPT-3 holds great potential to power embedding-driven
    use cases and come up with state-of-the-art results. Generating embeddings for
    the input data is very straightforward and wrapped in the form of an API call.
    To create an embedding vector representing the input text, you can use the following
    API signature:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 发现 GPT-3 具有驱动基于嵌入的用例并产生最先进结果的巨大潜力。为输入数据生成嵌入向量非常简单，只需通过 API 调用即可包装。要创建代表输入文本的嵌入向量，您可以使用以下
    API 签名：
- en: POST [https://api.openai.com/v1/engines/{engine_id}/embeddings](https://api.openai.com/v1/engines/%7Bengine_id%7D/embeddings)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: POST [https://api.openai.com/v1/engines/{engine_id}/embeddings](https://api.openai.com/v1/engines/%7Bengine_id%7D/embeddings)
- en: To invoke the embeddings endpoint, you can choose the type of engine depending
    on your use case by referring to the [embeddings document](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings).
    Each engine has its specific dimensions of embedding, with Davinci being the biggest
    and Ada being the smallest. All the embedding engines are derived from the four
    base models and classified based on the use cases to allow efficient and cost-friendly
    usage.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用嵌入端点，您可以根据您的用例选择引擎类型，参考[嵌入文档](https://beta.openai.com/docs/guides/embeddings/what-are-embeddings)。每个引擎都有其特定的嵌入维度，Davinci
    是最大的，而 Ada 是最小的。所有嵌入引擎都是从四个基本模型派生出来的，并根据用例进行分类，以允许高效和经济实惠的使用。
- en: Customizing GPT-3
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 定制 GPT-3
- en: OpenAI’s research paper “[Process for Adapting Language Models to Society (PALMS)
    with Values-Targeted Datasets](https://cdn.openai.com/palms.pdf)” by Irene Solaiman
    and Christy Dennison (June 2021) led the company to launch a first-of-its-kind
    fine-tuning endpoint that allows you to get more out of GPT-3 than was previously
    possible by customizing the model for your particular use case. Customizing GPT-3
    improves the performance of any natural language task GPT-3 is capable of performing
    for your specific use case.[[9]](xhtml-0-12.xhtml#aid_36)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的研究论文“[适应社会的语言模型过程（PALMS）与面向价值的数据集](https://cdn.openai.com/palms.pdf)”由
    Irene Solaiman 和 Christy Dennison（2021 年 6 月）领导公司推出了一个首创的微调端点，该端点允许您定制模型以符合您特定的用例，从而使您能够从
    GPT-3 中获得比以前更多的东西。定制 GPT-3 可以提高 GPT-3 能够执行的任何自然语言任务在您特定用例中的性能。
- en: Let us explain how that works first.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先解释它是如何工作的。
- en: OpenAI pre-trained GPT-3 on a [specially prepared dataset](https://arxiv.org/pdf/2005.14165.pdf)
    in a semi-supervised fashion. When given a prompt with just a few examples, it
    can often intuit what task you are trying to perform and generate a plausible
    completion. This is called "few-shot learning," as you learned in chapter 1.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI以半监督方式在一个 [特别准备的数据集](https://arxiv.org/pdf/2005.14165.pdf) 上对GPT-3进行了预训练。当只提供了几个示例的提示时，它通常能直觉到你试图执行的任务，并生成一个合理的完成。这被称为"少样本学习"，正如你在第1章中学到的。
- en: By fine-tuning GPT-3 on their own data, users can create a custom version of
    the model that is tailored to their specific project needs. This customization
    allows GPT-3 to be more reliable and efficient in a variety of use cases. Fine-tuning
    the model involves adjusting it so that it consistently performs in the desired
    way. This can be done using an existing dataset of any size or by incrementally
    adding data based on user feedback.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在自己的数据上微调GPT-3，用户可以创建一个根据其特定项目需求定制的模型版本。这种定制化使GPT-3在各种用例中更可靠和高效。微调模型涉及调整它以便始终以期望的方式执行。这可以使用任何大小的现有数据集完成，也可以根据用户反馈逐步添加数据完成。
- en: The process of fine-tuning will focus the knowledge and capabilities of the
    model on the contents and semantics of the data used for training, which will
    in turn limit the range of topics and creativity it can generate. This can be
    useful for downstream tasks that require specialized knowledge, such as classifying
    internal documents or dealing with internal jargon. Fine-tuning the model also
    focuses its attention on the specific data used for training, limiting its overall
    knowledge base.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 调整模型的过程将专注于使模型的知识和能力集中于用于训练的数据的内容和语义，这将限制它可以生成的主题范围和创造力。对于需要专业知识的后续任务，如分类内部文件或处理内部行话，这可能很有用。对模型进行微调还将注意力集中在用于训练的具体数据上，限制其整体知识库。
- en: Once a model has been fine-tuned, it no longer requires examples in the prompt,
    which saves costs and improves the speed and quality of the output. Customizing
    GPT-3 in this way appears to be more effective than using prompt design alone,
    as it allows for the use of more training examples.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过微调，就不再需要提示中的示例，这可以节省成本，并提高输出的速度和质量。以这种方式定制化GPT-3似乎比仅使用提示设计更有效，因为它可以使用更多的训练示例。
- en: With less than 100 examples you can start seeing the benefits of fine-tuning
    the model. Its performance continues to improve as you add more data. In the PALMS
    research paper, OpenAI showed how fine-tuning with less than 100 examples can
    improve the model’s performance on a number of tasks. They’ve also found that
    doubling the number of examples tends to improve the quality of the output linearly.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 少于100个示例，你就可以开始看到微调模型的好处。随着添加更多数据，其性能将继续提高。在PALMS研究论文中，OpenAI展示了如何使用少于100个示例进行微调可以提高模型在许多任务上的性能。他们还发现，将示例数量翻倍往往会线性提高输出的质量。
- en: Customizing GPT-3 improves the reliability of its outputs and gives more consistent
    results that you can take to production use cases. Existing OpenAI API customers
    found that customizing GPT-3 can dramatically reduce the frequency of unreliable
    outputs - there is a growing group of customers that can vouch for it with their
    performance numbers.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定制化GPT-3可以提高其输出的可靠性，并提供更一致的结果，可应用于生产用例。现有的OpenAI API客户发现，定制化GPT-3可以大大减少不可靠输出的频率
    - 有越来越多的客户可以凭借他们的性能数据为此作证。
- en: Apps Powered by Customized GPT-3 Models
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序由定制化GPT-3模型提供支持
- en: Keeper Tax helps independent contractors and freelancers with their taxes. It
    uses various models to extract text and classify transactions and then identifies
    easy-to-miss tax write-offs to help customers file their taxes directly from the
    app. By customizing GPT-3, Keeper Tax experienced an increasing accuracy from
    85% to 93%. And it continuously improves thanks to adding 500 new training examples
    to their model once a week, which is leading to about a 1% accuracy improvement
    a week.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Keeper Tax帮助独立承包商和自由职业者处理他们的税务。它使用各种模型来提取文本并分类交易，然后识别易被忽视的税务减免，从而帮助客户直接从应用程序中申报税款。通过定制化GPT-3，Keeper
    Tax的准确度由85%提高到93%。并且由于每周向他们的模型添加500个新的训练示例，所以它不断改进，导致每周约1%的准确度提高。
- en: Viable helps companies get insights from their customer feedback. By customizing
    GPT-3, Viable is able to transform massive amounts of unstructured data into readable
    natural language reports. Customizing GPT-3 has increased the reliability of Viable’s
    reports. By using a customized version of GPT-3, accuracy in summarizing customer
    feedback has improved from 66% to 90%. For an in-depth insight into Viable’s journey,
    refer to our interview with Viable’s CEO in chapter 4\.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Viable帮助公司从客户反馈中获得洞察。通过定制GPT-3，Viable能够将海量非结构化数据转化为可读的自然语言报告。定制GPT-3提高了Viable报告的可靠性。通过使用定制版GPT-3，总结客户反馈的准确性从66%提高到90%。要深入了解Viable的旅程，请参阅我们在第4章对Viable首席执行官的采访。
- en: Sana Labs is a global leader in the development and application of AI to learning.
    Their platform powers personalized learning experiences for businesses by leveraging
    the latest ML breakthroughs to personalize content. By customizing GPT-3 with
    their data, Sana’s question and content generation went from grammatically correct
    but general responses to highly accurate ones. This yielded a 60% improvement,
    enabling more personalized experiences for their users.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Sana Labs是AI在学习领域开发和应用方面的全球领先者。他们的平台利用最新的机器学习突破技术为企业提供个性化学习体验。通过使用他们的数据定制GPT-3，Sana的问题和内容生成从语法正确但一般化的回答变得高度准确。这带来了60%的改进，为他们的用户提供了更个性化的体验。
- en: 'Elicit is an AI research assistant that helps directly answer research questions
    using findings from academic papers. The assistant finds the most relevant abstracts
    from a large corpus of research papers, then applies GPT-3 to generate the claim
    that the paper makes about the question. A custom version of GPT-3 outperformed
    prompt design and led to improvement in three areas: results were 24% easier to
    understand, 17% more accurate, and 33% better overall.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Elicit是一个AI研究助手，可以使用学术论文的发现直接回答研究问题。助手从大量研究论文中找到最相关的摘要，然后应用GPT-3生成有关问题的论文所做声明。定制版GPT-3在三个方面优于提示设计，分别是结果更易于理解24%，准确性提高了17%，整体表现提高了33%。
- en: How to customize GPT-3 for your application
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如何为您的应用定制GPT-3
- en: To begin, simply use the OpenAI command line tool with a file of your choosing.
    Your personalized version will begin training and will be accessible through our
    API right away.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，只需使用OpenAI命令行工具和您选择的文件。您的个性化版本将立即开始训练，并可立即通过我们的API访问。
- en: 'At a very high level, customizing GPT-3 for your application involves the following
    three steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从非常高的层面来看，为您的应用定制GPT-3包括以下三个步骤：
- en: ●        Prepare new training data and upload it to the OpenAI server
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ●       准备新的训练数据并将其上传到OpenAI服务器
- en: ●        Fine-tune the existing models with the new training data
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ●       使用新的训练数据对现有模型进行微调
- en: ●        Use the fine-tuned model
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ●       使用经过微调的模型
- en: Prepare and upload training data
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 准备并上传训练数据
- en: Training data is what the model takes in as input for fine-tuning. Your training
    data must be a JSONL document, where each line is a prompt-completion pair corresponding
    to a training example. For model fine-tuning, you can provide an arbitrary number
    of examples where it is highly recommended to create a value-targeted dataset
    to provide the model with quality data and wide representation. Fine-tuning improves
    performance with more examples, so the more examples you provide, the better the
    outcome.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据是模型用作微调的输入。您的训练数据必须是一个JSONL文档，其中每一行都是一个与训练示例对应的提示-完成对。对于模型的微调，您可以提供任意数量的示例，强烈建议创建一个以价值为目标的数据集，以向模型提供优质且广泛代表性的数据。通过提供更多示例进行微调可以改进性能，因此您提供的示例越多，结果就会更好。
- en: 'Your JSONL document should look something like this:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 您的JSONL文档应该长这样：
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '{"prompt": "<提示文本>", "completion": "<理想生成的文本>"}'
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '{"prompt": "<提示文本>", "completion": "<理想生成的文本>"}'
- en: '{"prompt": "<prompt text>", "completion": "<ideal generated text>"}'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '{"prompt": "<提示文本>", "completion": "<理想生成的文本>"}'
- en: …
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: Where the prompt text should include the exact prompt text you want to complete,
    and the ideal generated text should include an example of desired completion text
    that you want GPT-3 to generate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示文本中，应包括您要完成的确切提示文本，而理想生成的文本应包括您希望GPT-3生成的理想完成文本的示例。
- en: 'You can use OpenAI''s CLI data preparation tool to easily convert your data
    into this file format. CLI data preparation tool accepts files in different formats,
    with the only requirement that they contain a prompt and a completion column/key.
    You can pass a CSV, TSV, XLSX, JSON, or JSONL file, and it will save the output
    into a JSONL file ready for fine-tuning. To do that, you can use the following
    command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 OpenAI 的 CLI 数据准备工具轻松将数据转换为此文件格式。CLI 数据准备工具接受不同格式的文件，唯一的要求是它们包含一个提示和一个完成列/键。您可以传递
    CSV、TSV、XLSX、JSON 或 JSONL 文件，它将把输出保存到一个 JSONL 文件中，准备好进行精调。为此，您可以使用以下命令：
- en: openai tools fine_tunes.prepare_data -f <LOCAL_FILE>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: openai tools fine_tunes.prepare_data -f <LOCAL_FILE>
- en: Where LOCAL_FILE is the file you prepared for conversion.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: LOCAL_FILE 是您为转换准备的文件。
- en: Train a new fine-tuned model
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个新的精调模型
- en: 'Once you prepare your training data as described above, you can move on to
    the fine-tuning job with the help of the OpenAI CLI. For that, you need the following
    command:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您按上述描述准备好培训数据，您就可以借助 OpenAI CLI 进行精调作业的帮助。为此，您需要以下命令：
- en: openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>
- en: 'Where BASE_MODEL is the name of the base model you''re starting from (ada,
    babbage, or curie). Running this command does several things:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: BASE_MODEL 是您从中开始的基础模型的名称（ada、babbage 或 curie）。运行此命令会执行几个操作：
- en: ●        Uploads the file using the files endpoint (as discussed earlier in
    this chapter);
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ●  使用文件端点上传文件（如本章前述）；
- en: ●        Fine-tunes the model using the request configuration from the command;
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ●  使用命令中的请求配置精调模型；
- en: ●        Streams the event logs until the fine-tuning job is completed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ●  流式传输事件日志，直到完成精调作业为止。
- en: Log streaming is helpful to understand what’s happening in real-time and to
    respond to any incidents/failures as and when they happen. The streaming may take
    from minutes to hours depending on the number of jobs in the queue and the size
    of your dataset.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 日志流式传输有助于实时了解发生的情况，并在发生任何事件/故障时做出响应。流式传输可能需要几分钟到几小时，具体取决于队列中的作业数量和您的数据集大小。
- en: Using the fine-tuned model
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用精调模型
- en: Once the model is successfully fine-tuned, you can start using it! You can now
    specify this model as a parameter to the Completion Endpoint, and make requests
    to it using the Playground.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型成功进行了精调，您就可以开始使用它！您现在可以将此模型指定为完成端点的参数，并使用 Playground 向其发出请求。
- en: 'TIP: After the fine-tuning job completes, it may take several minutes for your
    model to become ready to handle requests. If completion requests to your model
    time out, it is likely because your model is still being loaded. If this happens,
    try again in a few minutes.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：在精调作业完成后，您的模型可能需要几分钟才能准备好处理请求。如果向您的模型发出的完成请求超时，则可能是因为您的模型仍在加载中。如果出现此情况，请稍后再试。
- en: 'You can start making requests by passing the model name as the model parameter
    of a completion request using the following command:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下命令将模型名称作为完成请求的模型参数来开始发出请求：
- en: openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>
- en: Where FINE_TUNED_MODEL is the name of your model and YOUR_PROMPT is the prompt
    you want to complete in this request.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: FINE_TUNED_MODEL 是您的模型名称，YOUR_PROMPT 是您想要在此请求中完成的提示。
- en: You can continue to use all the Completion Endpoint parameters that were discussed
    in this chapter, like temperature, frequency_penalty, presence_penalty, etc.,
    on these requests to the newly fine-tuned model as well.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以继续在这些请求中使用本章讨论的所有完成端点参数，如温度、频率惩罚、存在惩罚等，也可以将这些请求用于新精调的模型。
- en: NOTE:No engine is specified on these requests. This is the intended design and
    something that OpenAI plans on standardizing across other API endpoints in the
    future.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这些请求中未指定引擎。这是预期的设计，也是 OpenAI 打算在将来标准化其他 API 端点的内容。
- en: For more information, please refer to OpenAI’s [fine-tuning documentation](https://beta.openai.com/docs/guides/fine-tuning).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅 OpenAI 的[精调文档](https://beta.openai.com/docs/guides/fine-tuning)。
- en: Tokens
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Tokens
- en: Before diving deeper into how different prompts consume tokens, let's look more
    closely at what is a token.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解不同提示如何消耗令牌之前，让我们更仔细地看一下令牌是什么。
- en: We’ve told you that tokens are numerical representations of words or characters.
    Using tokens as a standard measure, GPT-3 can handle training prompts from a few
    words to entire documents.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们告诉过你，标记是单词或字符的数值表示。使用标记作为标准度量，GPT-3 可以处理从几个词到整个文档的训练提示。
- en: For regular English text, 1 token consists of approximately 4 characters. It
    translates to roughly ¾ of a word, so for 100 tokens there will be approximately
    75 words. As a point of reference, the collected works of Shakespeare consist
    of about 900,000 words, roughly translating to 1.2M tokens.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 对于普通英文文本，1 个标记大约包含 4 个字符。这大致相当于 ¾ 个单词，因此 100 个标记大约包含 75 个单词。作为参考，莎士比亚的全部作品大约有
    900,000 个词，大致翻译为 1.2M 个标记。
- en: To maintain the latency of API calls, OpenAI imposes a limit of 2,048 tokens
    (approximately ~1,500 words) for prompts and completions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持 API 调用的延迟，OpenAI 对提示和完成设置了 2,048 个标记（约 ~1,500 个词）的限制。
- en: To further understand how the tokens are calculated/consumed in the context
    of GPT-3 and to stay within the limits set by the API let us walk you through
    the following ways you can measure the token count.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解标记在 GPT-3 上是如何计算/消耗的，并且要在 API 设置的限制内，让我们为你介绍以下几种衡量标记计数的方式。
- en: In the Playground, as you enter text into the interface, you can see the token
    count update in real-time in the footer at the bottom right. It displays the number
    of tokens that will be consumed by the text prompt after hitting the submit button.
    You can use it to monitor your token consumption every time you interact with
    the Playground (see Figure 2-10).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Playground 中，当你在界面中输入文本时，你可以在右下角的页脚实时看到标记计数的更新。它显示在点击提交按钮后文本提示将要消耗的标记数量。你可以使用它来监视每次与
    Playground 交互时的标记消耗（参见图 2-10）。
- en: '![](img/image-0-11.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-11.jpg)'
- en: Figure 2-10\. Token Count in the Playground
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-10\. Playground 中的 Token 计数
- en: The other way to measure the consumption of tokens is by using the explicit
    GPT-3 Tokenizer tool (see Figure 2-11) that lets you visualize the formation of
    tokens from the word characters. You can interact with the Tokenizer tool via
    a simple text box where you write the prompt text and Tokenizer will show you
    the token and character counts along with the detailed visualization.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量标记消耗的另一种方法是使用显式的 GPT-3 分词器工具（见图 2-11），它让你可以可视化地看到标记是如何从单词字符中形成的。你可以通过一个简单的文本框与分词器工具交互，在那里你写下提示文本，分词器会显示给你标记和字符的计数以及详细的可视化。
- en: '![](img/image-0-12.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-12.jpg)'
- en: Figure 2-11\. Tokenizer by OpenAI
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-11\. OpenAI 的分词器
- en: For integrating the token count metric in your API calls to different endpoints,
    you can patch the logprobs and echo attribute along with the API request to get
    the full list of tokens consumed.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 API 调用中集成标记计数度量到不同端点，你可以在 API 请求中一并传递 logprobs 和 echo 属性，以获取已消耗的标记的完整列表。
- en: In the next section, we will cover how tokens are priced based on the different
    execution engines.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将介绍如何基于不同执行引擎定价标记。
- en: Pricing
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 定价
- en: In the last section we talked about tokens, which are the smallest fungible
    unit used by OpenAI to determine the pricing for API calls. Tokens allow greater
    flexibility than measuring the number of words or sentences used in the training
    prompt and due to the token’s sheer granularity tokens can be easily processed
    and used to measure the pricing for a wide range of training prompts.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们谈论了标记，它是 OpenAI 用来确定 API 调用定价的最小可交换单位。标记比测量训练提示中使用的单词或句子数量具有更大的灵活性，由于标记的极端细粒度，因此可以轻松处理并用于测量广泛范围的训练提示的定价。
- en: Every time you call the API from either the Playground or programmatically,
    behind the scenes the API calculates the number of tokens used in the training
    prompt along with the generated completion and charges each call on the basis
    of the total number of tokens used.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 每次从 Playground 或以编程方式调用 API 时，API 在后台计算训练提示和生成的完成中使用的标记数，并根据使用的标记总数收取每次调用的费用。
- en: OpenAI generally charges a flat fee per 1,000 tokens, with the fee depending
    on the execution engine used in the API call. Davinci is the most powerful and
    expensive, while Curie, Babbage, and Ada are cheaper and faster.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 通常按每 1,000 个标记收取固定费用，费用取决于 API 调用中使用的执行引擎。Davinci 是最强大和最昂贵的，而 Curie、Babbage
    和 Ada 则更便宜和更快。
- en: Table 2-2 shows the pricing for the various API engines at the time this chapter
    was written (December 2022).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-2 显示了本章撰写时（2022年12月）各种 API 引擎的定价。
- en: '|'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Model
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Price Per 1k Tokens
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 每 1k 令牌的价格
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Davinci (most powerful)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 达芬奇（最强大的）
- en: '|'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: $0.0200
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: $0.0200
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Curie
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 居里
- en: '|'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: $0.0020
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: $0.0020
- en: '|'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Babbage
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 巴贝奇
- en: '|'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: $0.0005
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: $0.0005
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Ada (fastest)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 艾达（最快的）
- en: '|'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: $0.0004
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: $0.0004
- en: '|'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 2-2\. Model Pricing
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-2\. 模型定价
- en: The company works on the cloud pricing model of “pay as you go”. For updated
    pricing please check the [online pricing schedule](https://beta.openai.com/pricing).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 该公司采用“按需付费”的云计价模式。有关最新价格，请查看[在线定价计划](https://beta.openai.com/pricing)。
- en: Instead of monitoring the tokens for each of the API calls, OpenAI provides
    a [reporting dashboard](https://beta.openai.com/account/usage) to monitor daily
    cumulative token usage. Depending on your usage, it may look something like Figure
    2-12.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供了[报告仪表板](https://beta.openai.com/account/usage)，用于监控每日累积令牌使用情况，而不是监视每个
    API 调用的令牌。根据你的使用情况，它可能看起来像图 2-12。
- en: '![](img/image-0-13.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-13.jpg)'
- en: Figure 2-12\. API Usage Dashboard
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-12\. API 使用仪表板
- en: In Figure 2-12 you can see a bar graph showing the daily token consumption from
    the API usage. The dashboard helps you to monitor the token usage and pricing
    for your organization. This helps you to regulate the API usage and stay within
    your budget. There is also an option to monitor the cumulative usage and breakdown
    of token count per API call. This should give you enough flexibility to create
    policies around token consumption and pricing for your organization. Now that
    you understand the ins and outs of the Playground and the API, we will take a
    look at GPT-3’s performance on typical language modeling tasks.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 2-12 中，你可以看到一个条形图，显示了来自 API 使用的每日令牌消耗量。该仪表板帮助你监控组织的令牌使用情况和定价。这有助于你调节 API
    使用量，并保持预算内。还有一个选项可以监控累积使用量和每个 API 调用的令牌计数的细分。这应该给你足够的灵活性来围绕令牌消耗和定价为你的组织制定政策。现在你已经了解了
    Playground 和 API 的方方面面，我们将看一下 GPT-3 在典型语言建模任务中的表现。
- en: 'TIP: For beginners who have just started with GPT-3 and find it hard to wrap
    their heads around token consumption. Many users enter too long prompt texts,
    which leads to the overuse of credits, followed by unplanned fees. To avoid this,
    during your initial days try to use the API dashboard to observe the number of
    tokens consumed and see how the length of prompts and completions affect token
    usage. It can help you to prevent the uncontrolled use of credits and keep everything
    within the budget.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：对于刚开始使用 GPT-3 并且发现很难理解令牌消耗的初学者。许多用户输入了过长的提示文本，导致信用额度的过度使用，随后是不计划的费用。为了避免这种情况，在你的初始阶段尝试使用
    API 仪表板观察令牌的消耗数量，并查看提示文本和完成的长度如何影响令牌使用情况。这可以帮助你防止信用额度的不受控制使用，并保持一切在预算范围内。
- en: GPT-3’s Performance on Standard NLP Tasks
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 在标准 NLP 任务上的表现
- en: 'GPT-3 is a highly advanced and sophisticated successor to the NLP field, built
    and trained using the core NLP approaches and deep neural networks. For any AI-based
    modeling approach, the model performance is evaluated in the following way: First,
    you train the model for a specific task (like classification, Q/A, text generation,
    etc) on training data; then you verify the model performance using the test data
    (unseen data).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 是 NLP 领域的高度先进和复杂的继任者，使用核心 NLP 方法和深度神经网络构建和训练。对于任何基于 AI 的建模方法，模型性能都是通过以下方式评估的：首先，你使用训练数据为特定任务（如分类、问答、文本生成等）训练模型；然后，你使用测试数据（未见数据）验证模型性能。
- en: In a similar way, there is a standard set of NLP benchmarks for evaluating the
    performance of NLP models and coming up with a relative model ranking or comparison.
    This comparison, or relative ranking, allows you to pick and choose the best model
    for a specific NLP task (business problem).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，有一套标准的 NLP 基准用于评估 NLP 模型的性能，并提供相对模型排名或比较。这种比较或相对排名，允许你为特定的 NLP 任务（业务问题）选择最佳模型。
- en: In this section we will discuss the performance of GPT-3 on some standard NLP
    tasks as seen in Figure 2-13 and compare it with the performance of similar models
    on respective NLP tasks.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 GPT-3 在一些标准 NLP 任务中的表现，如图 2-13 所示，并将其与相似模型在相应 NLP 任务中的表现进行比较。
- en: '![](img/image-0-14.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-14.jpg)'
- en: Figure 2-13\. Conventional NLP Tasks
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-13\. 传统的 NLP 任务
- en: Text Classification
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类
- en: NLP-powered text classification involves using algorithms to automatically analyze
    text and assign it to predefined categories or tags based on its context. This
    process helps to organize and categorize text into relevant groups.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 强力文本分类涉及使用算法自动分析文本，并根据其上下文将其分配到预定义的类别或标签中。这个过程有助于组织和分类文本到相关的组。
- en: Text classification involves analyzing the text provided as input and assigning
    it a label, score, or another attribute that characterizes the text. Some common
    examples of text classification are sentiment analysis, topic labeling, intent
    detection, etc. You can use a number of approaches to get GTP-3 to classify text,
    again ranging from zero-shot classification (where you don't give any examples
    to the model) to single-shot and few-shot classification (where you show some
    examples to the model).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类涉及分析提供的文本并为其分配标签、分数或其他表征文本的属性。一些常见的文本分类例子包括情感分析、主题标注、意图检测等。你可以采用多种方法让 GPT-3
    对文本进行分类，从零样本分类（不向模型提供任何示例）到单样本和少样本分类（向模型展示一些示例）都可以。
- en: Zero-Shot Classification
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本分类
- en: Modern artificial intelligence has long aimed to develop models that can perform
    predictive functions on data it has never seen before. This important research
    area is called zero-shot learning. Similarly, a zero-shot classification is a
    classification task where no prior training and fine-tuning on labeled data is
    required for the model to classify a piece of text.GPT-3 currently produces results
    for unseen data that are either better or at par with state-of-the-art AI models
    fine-tuned for that specific purpose. In order to perform zero-shot classification
    with GPT-3, we must provide it with a compatible prompt. In chapter 2, we will
    discuss prompt engineering.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能长期以来一直致力于开发能够在从未见过的数据上执行预测功能的模型。这一重要的研究领域被称为零样本学习。同样，零样本分类是一种分类任务，模型在对文本进行分类时无需进行先前的训练和在标记数据上的微调。GPT-3
    目前为未见过的数据生成的结果要么优于要么与针对该特定目的进行微调的最先进的 AI 模型持平。为了使用 GPT-3 进行零样本分类，我们必须提供一个兼容的提示。在第二章中，我们将讨论提示工程。
- en: Here is an example of a zero-shot classification where the goal is to perform
    a fact-checking analysis to determine if the information included in the tweet
    is correct or incorrect. Figure 2-14 shows a pretty impressive information correctness
    classification result based on a zero-shot example.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个零样本分类的例子，目标是执行事实核对分析，以确定推文中包含的信息是否正确。图 2-14 展示了基于零样本示例的相当令人印象深刻的信息正确性分类结果。
- en: '![](img/image-0-15.jpg)Figure 2-14\. Zero-shot classification example'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/image-0-15.jpg)图 2-14。零样本分类示例'
- en: 'And here is our prompt:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: Analyse the tweet in terms of information correctness.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以信息正确性为标准分析推文。
- en: 'Tweet: “More than 50%of global scientists don’t believe in climate change.”'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 推文：“全球超过 50% 的科学家不相信气候变化。”
- en: 'Analysis:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 分析：
- en: 'And the output:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 并且输出为：
- en: The tweet is incorrect.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这条推文是错误的。
- en: Single-shot and Few Shot Text Classification
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 单样本和少样本文本分类
- en: The other approach to text classification is via fine-tuning an AI model on
    a single or few training examples also known as a single shot or few shots text
    classification. When you provide examples of how to classify text, the model can
    learn information about the object categories based on the samples you provide.
    This is a superset of zero-shot classification that allows you to classify text
    by providing the model with three to four diversified examples. This can be useful
    specifically for downstream use cases, which require some level of context setting.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的另一种方法是通过在少量或单个训练示例上对 AI 模型进行微调，也称为单样本或少样本文本分类。当你提供如何对文本进行分类的示例时，模型可以根据你提供的样本学习对象类别的信息。这是零样本分类的超集，允许你通过向模型提供三到四个多样化的示例来对文本进行分类。这特别适用于需要一定程度上下文设置的下游用例。
- en: 'Let’s look at the following example of a few-shot classification. We are asking
    the model to perform a tweet sentiment analysis classification and giving it three
    tweet examples to illustrate each of the possible labels: positive, neutral, and
    negative. As you can see in Figure 2-15, the model equipped with such a detailed
    context based on a few examples, is able to very easily perform the sentiment
    analysis of the next tweet.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个少样本分类的例子。我们要求模型执行推文情感分析分类，并给出三个推文示例来说明每个可能的标签：积极的、中立的和消极的。正如你在图 2-15
    中所看到的，配备了这样详细的上下文的模型，基于少量示例，能够非常轻松地执行下一个推文的情感分析。
- en: 'Note: when you recreate prompt examples from the book or create your own, make
    sure to have adequate line spacing in your prompt. An additional line after a
    paragraph can result in a very different outcome, so you want to play with that
    and see what works best for you.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当你从书中重新创建提示示例或创建你自己的示例时，确保在你的提示中有足够的行间距。段落之后的额外一行可能会导致完全不同的结果，因此你需要尝试并看看哪种方式最适合你。
- en: '![](img/image-0-16.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-16.jpg)'
- en: Figure 2-15\. Few-shot classification example
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-15\. few-shot 分类示例
- en: 'Here is our prompt:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: Analyse the tweet in terms of its sentiment. Depending on the sentiment, classify
    it as positive, neutral, or negative.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 根据情感分析 tweet。根据情感，将它分类为积极、中性或消极。
- en: 'Tweet: "I''m seriously worried that super intelligent AI will be disappointed
    in humans."'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："我真的很担心超智能人工智能会对人类感到失望。"
- en: 'Sentiment analysis(positive, neutral, negative): negative'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：消极
- en: 'Tweet: "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："我迫不及待地希望超智能人工智能出现，并加深我们对宇宙的理解。"
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：积极
- en: 'Tweet: "I think it is neither super likely nor super unlikely that the super
    intelligent AI will emerge one day."'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："我认为超智能人工智能出现的可能性既不是很大，也不是很小。"
- en: 'Sentiment analysis(positive, neutral, negative): neutral'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：中性
- en: 'Tweet: "Super intelligent AI is going to be the most exciting discovery in
    human history."'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："超智能人工智能将是人类历史上最激动人心的发现。"
- en: 'Sentiment analysis(positive, neutral, negative):'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：
- en: 'And the output:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: positive
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 积极
- en: Batch Classification
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 批量分类
- en: After understanding the few-shot classification with GPT-3, let's dive deeper
    into the batch classification which allows you to classify input samples in batches
    in a single API call instead of just classifying one example per API call. It
    is suitable for applications where you want to classify multiple examples in a
    single go, just like the tweet sentiment analysis task we examined, but analyzing
    a few tweets in a row.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了 GPT-3 的 few-shot 分类之后，让我们更深入地探讨批量分类，它允许你以单个 API 调用的方式对批量输入样本进行分类，而不是仅仅对单个示例进行分类。它适用于你想一次性对多个示例进行分类的应用，就像我们所检验的
    tweet 情感分析任务一样，但是分析一系列 tweet。
- en: As with a few shots classification, you want to provide enough context for the
    model to achieve the desired result but in a batch configuration format. Here,
    we define the different categories of tweet sentiment classification using various
    examples in the batch configuration format. Then we ask the model to analyze the
    next batch of tweets.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 与 few shots 分类一样，你希望为模型提供足够的上下文以达到期望的结果，但是在批量配置格式中。在这里，我们使用批量配置格式定义了 tweet 情感分类的不同类别，然后要求模型分析下一批
    tweet。
- en: '![](img/image-0-17.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-17.jpg)'
- en: Figure 2-16\. Batch classification example (Part-1)
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-16\. 批量分类示例（第一部分）
- en: '![](img/image-0-18.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-18.jpg)'
- en: Figure 2-17\. Batch-classification example (Part-2)
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-17\. 批量分类示例（第二部分）
- en: 'Here is our prompt:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: Analyse tweets in terms of their sentiment. Depending on their sentiment, classify
    them as positive, neutral, or negative.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 根据它们的情感分析 tweet。根据情感，将它们分类为积极、中性或消极。
- en: 'Tweet: "I''m seriously worried that super intelligent AI will be disappointed
    in humans."'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："我真的很担心超智能人工智能会对人类感到失望。"
- en: 'Sentiment analysis(positive, neutral, negative): negative'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：消极
- en: '###'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '###'
- en: 'Tweet: "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："我迫不及待地希望超智能人工智能出现，并加深我们对宇宙的理解。"
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：积极
- en: '###'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '###'
- en: 'Tweet: "I think it is neither super likely nor super unlikely that the super
    intelligent AI will emerge one day."'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："我认为超智能人工智能出现的可能性既不是很大，也不是很小。"
- en: 'Sentiment analysis(positive, neutral, negative): neutral'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：中性
- en: '###'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '###'
- en: 'Tweet: "Super intelligent AI is going to be the most exciting discovery in
    human history."'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet："超智能人工智能将是人类历史上最激动人心的发现。"
- en: 'Sentiment analysis(positive, neutral, negative): positive'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析（积极、中性、消极）：积极
- en: '###'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '###'
- en: 'Tweet:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Tweet：
- en: 1\. "I'm seriously worried that super intelligent AI will be disappointed in
    humans."
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. "我真的很担心超智能人工智能会对人类感到失望。"
- en: 2\. "I cannot wait for super intelligent AI to emerge and deepen our understanding
    of the Universe."
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. "我迫不及待地等待超智能人工智能的出现，深化我们对宇宙的理解。"
- en: 3\. "I think it is neither super likely nor super unlikely that the super intelligent
    AI will emerge one day."
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. "我认为超智能人工智能出现是既不太可能也不太不可能的。"
- en: 4\. "Super intelligent AI is going to be the most exciting discovery in human
    history."
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. "超智能人工智能将是人类历史上最令人兴奋的发现"
- en: 5\. "This is the latest report on the state of the AI"
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. "这是AI状况的最新报告"
- en: 'Tweet sentiment:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 推文情感：
- en: 1\. negative
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 负面
- en: 2\. positive
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 积极
- en: 3\. neutral
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 中性
- en: 4\. positive
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 积极
- en: 5\. neutral
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 中性
- en: 'Tweet:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 推文：
- en: 1\. "I can't stand bad techno music"
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. "我受不了糟糕的电子音乐"
- en: 2\. "This is a tweet"
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. "这是一条推文"
- en: 3\. "I can't wait for going to the Moon!!!"
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. "我迫不及待地去月球！！！"
- en: 4\. "AI is super cute ❤️"
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. "AI太可爱了 ❤️"
- en: 5\. "Got VERY ANGRY now!!! ��"
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. "现在非常生气了！​​​​ ��"
- en: 'Tweet sentiment:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 推文情感：
- en: 1\.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 1\.
- en: 'And the output:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 并且输出：
- en: 1\. negative
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 负面
- en: 2\. neutral
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 中性
- en: 3\. positive
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 积极
- en: 4\. positive
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 积极
- en: 5\. negative
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 负面
- en: As you can see, the model recreated the batch sentiment analysis format and
    classified the tweets successfully. Now let’s move on to see how it performs at
    the Named Entity Recognition tasks.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，该模型还原了批量情感分析格式，并成功分类了推文。现在让我们继续看看它在命名实体识别任务中的表现如何。
- en: Named Entity Recognition
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Named entity recognition (NER) is an information extraction task that involves
    identifying and categorizing named entities mentioned in unstructured text. These
    entities may include people, organizations, locations, dates, quantities, monetary
    values, and percentages. This task is useful for extracting important information
    from text.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是一项涉及在非结构化文本中识别和分类命名实体的信息提取任务。这些实体可能包括人、组织、地点、日期、数量、货币值和百分比。这项任务有助于从文本中提取重要信息。
- en: NER helps to make the responses more personalized and relevant but the current
    state-of-the-art approaches require massive amounts of data for training before
    you even start with the prediction. GPT-,3 on the other hand, can work out of
    the box for recognizing general entities like people, places, and organizations
    without humans providing even a single training example.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: NER有助于使回复更加个性化和相关，但当前的最先进方法要求在开始预测之前进行大量的训练数据。另一方面，GPT-3可以直接识别人、地点和组织等一般实体，而无需人类提供任何训练示例。
- en: 'In the following example we used a davinci-instruct-series version of the model
    that was in beta at the time of writing this book and the model gathers prompts
    to train and improve the future OpenAI API models. We gave it a simple task: to
    extract contact information from an example email. It successfully completed the
    task on the first attempt (Figure 2-18).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们使用了一种处于beta测试阶段的davinci-instruct-series版本的模型，该模型在撰写本书时处于beta测试阶段，并且该模型获取提示来训练和改进未来的OpenAI
    API模型。我们给了它一个简单的任务：从示例电子邮件中提取联系信息。它第一次尝试就成功完成了任务（图2-18）。
- en: '![](img/image-0-19.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-19.jpg)'
- en: Figure 2-18\. NER example
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-18\. NER示例
- en: 'Here is our input:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输入：
- en: 'Extract the name and mailing address from this email:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 从这封电子邮件中提取姓名和邮寄地址：
- en: Shubham,
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: Shubham，
- en: It was great to talk to you the other day!
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴前几天和你交谈！
- en: I'm SO looking forward to start working on our book.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常期待开始我们的书的工作。
- en: Here's my address 1307 Roosevelt Street, San Francisco CA​94107
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的地址：1307 Roosevelt Street, San Francisco CA​94107
- en: Best,
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 最好，
- en: Sandra Kublik
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Sandra Kublik
- en: 'Name and mailing address:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 姓名和邮寄地址：
- en: 'And the output:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 并且输出：
- en: Sandra Kublik
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: Sandra Kublik
- en: 1307 Roosevelt Street, San Francisco CA 94107
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 1307 Roosevelt Street, San Francisco CA 94107
- en: Text Summarization
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要
- en: The goal of text summarization is to create a shortened version of a lengthy
    text while still accurately representing the original content and maintaining
    its overall meaning. This is done by identifying and highlighting the most important
    information in the text.GPT-3 based text summarization aims to transform lengthy
    pieces of texts into their condensed tl;dr[[10]](xhtml-0-12.xhtml#aid_41) versions.
    Such tasks are generally difficult and costly to accomplish manually. With GPT-3,
    it is a matter of a single input and a few seconds!
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要的目标是在准确表示原始内容并保持其整体含义的同时创建原始文本的缩短版本。这是通过识别和突出文本中最重要的信息来实现的。基于GPT-3的文本摘要旨在将长篇文本转化为简化的tl;dr[[10]](xhtml-0-12.xhtml#aid_41)版本。这样的任务通常难以且成本高昂。而GPT-3只需要一个输入和几秒钟！
- en: NLP models can be trained to comprehend documents and identify the sections
    that convey important facts and information before producing the required summarized
    texts. However, such models need a large amount of training samples before they
    can learn the context and start summarizing the unseen input.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: NLP模型可以训练以理解文档，并确定传达重要事实和信息的部分，然后才生成所需的摘要文本。然而，这样的模型在能够学习上下文并开始总结未见输入之前，需要大量的训练样本。
- en: GPT-3's abstractive summarization is the key to solving the problem of information
    extraction. By producing summaries instead of merely extracting key information,
    GPT-3 can provide a more comprehensive and accurate understanding of the text.
    It uses zero shot or few-shot approach towards text summarization making it useful
    for a variety of use cases. With GPT-3 there are multiple ways you can go about
    summarizing the text involving basic summaries, one-line summaries, and grade-level
    summaries depending on your use case. Let’s have a quick walkthrough of these
    approaches.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的抽象摘要是解决信息提取问题的关键。通过产生摘要而不仅仅提取关键信息，GPT-3可以更全面和准确地理解文本。它采用零射击或少射击方法对文本进行摘要，使其适用于各种用例。使用GPT-3，您可以根据您的用例以多种方式对文本进行摘要，包括基本摘要、一行摘要和年级摘要。让我们快速浏览这些方法。
- en: Most of the time the model is able to generate decent results in the form of
    a review summary, but sometimes it can output irrelevant results depending on
    the prior context. To avoid the problem of getting unwanted results, you can set
    the Best of parameter to 3 which will always give you the best of three results
    generated by the API. In the example shown in Figure 2-18, after a few tries and
    minor parameter tweaking, we got decent results.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，该模型能够产生出色的审查摘要，但有时根据先前的上下文可能会输出不相关的结果。为了避免获得不想要的结果，您可以将Best of参数设置为3，这将始终给您API生成的三个最佳结果之一。在图2-18所示的示例中，经过几次尝试和微小参数调整，我们得到了不错的结果。
- en: 'Here is our prompt:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: The problem is that when it works, it works, but when it doesn’t, it really
    doesn’t. Luckily, the game’s low points like the horrible binary stealth system
    and the bugs and lack of QoL are either fixable or vastly outweighed by the good
    bits, and the overall tally still ends up being much higher than the average of
    many other games out there. That’s helped a lot by the moment-to-moment gameplay,
    which pays off just as much as the story beats; the movement system lets you climb
    almost any short object, vehicles are weighty to drive, and have fantastic interior
    and exterior designs, and guns feel properly loud and heavy. Its story and narrative
    kept me glued to the screen, and everything – from sex scenes and romance options
    to Keanu Reeves trying to kill you or help you throughout the game – are remarkably
    well done. Cyberpunk 2077 is a game crafted with love, and it shows.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于它工作时确实很有效，但当它不起作用时，它确实不起作用。幸运的是，游戏的低点，比如可怕的二进制潜行系统以及错误和缺乏QoL，要么可以修复，要么被好东西远远超过，并且总体得分仍远高于其他许多游戏的平均水平。这在很大程度上得到了实时游戏性的帮助，这与故事情节一样有益;移动系统让您几乎可以爬上任何短物体，车辆驾驶沉重，并具有出色的内部和外部设计，枪支声音真实而沉重。它的故事和叙事让我沉迷于屏幕，而一切——从性场景和浪漫选择到基努·里维斯试图杀死你或在整个游戏中帮助你——都做得非常出色。《赛博朋克2077》是一款用爱心打造的游戏，这一点是显而易见的。
- en: 'tl;dr:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之：
- en: 'And the output:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: Cyberpunk 2077 is a game crafted with love, and it shows. The story is great,
    the gameplay is great, the worldbuilding is great. It's not perfect by any means,
    but it's still one of the best games I've played in years.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 《赛博朋克2077》是一款用爱心打造的游戏，这一点是显而易见的。故事很棒，游戏很棒，世界构建很棒。它并不完美，但仍是我玩过的最好的游戏之一。
- en: 'The Good:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 优点：
- en: '- Great story and characters'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 了不起的故事和角色
- en: '- Amazing worldbuilding'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '- 令人惊叹的世界构建'
- en: '- Excellent gunplay and movement system'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '- 出色的枪战和移动系统'
- en: 'The Bad:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点：
- en: '- Binary stealth system (sometimes you''re seen no matter what) - Bugs'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '- 二进制潜行系统（有时无论如何都会被发现）-错误'
- en: '![](img/image-0-20.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-20.jpg)'
- en: Figure 2-19\. Text summarization example
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-19\. 文本摘要示例
- en: Since many people spend hours per day reading and writing emails, summarizing
    them is a widely desired use case for GPT-3\. Let’s see how GPT-3 does with summarizing
    a three-paragraph email into one crisp line (Figure 2-19).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多人每天花费数小时阅读和写邮件，对其进行摘要是GPT-3广泛期待的用例。让我们看看GPT-3如何将三段邮件总结成一条简洁的句子（图2-19）。
- en: '![](img/image-0-21.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-21.jpg)'
- en: Figure 2-20\. Email summarization example
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-20. 电子邮件摘要示例
- en: To achieve the desired result, we pasted the full email and then simply added
    “one-sentence summary:'' at the end. We also included a “.” stop sequence to tell
    the model that it should stop its summary generation after a single sentence.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到预期的结果，我们复制粘贴了完整的电子邮件，然后简单地在末尾添加了“一句总结：”，我们还包括了一个“。”停止序列，告诉模型在生成一句摘要后应该停止生成。
- en: 'Our prompt:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示：
- en: Hey Greg,
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，格雷格，
- en: We are Sandra and Shubham. We are the co-authors of a GPT-3 based book that
    aims to be a go-to manual for API users. It will provide the readers with the
    knowledge to get started with GPT-3 as well as the context of the growing ecosystem
    surrounding it. The book will be released by O’Reilly publishing house by early
    2022\.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是桑德拉和舒布汉。我们是一本以GPT-3为基础的书的合著者，旨在成为API用户的必备手册。它将为读者提供开始使用GPT-3以及围绕其不断增长的生态系统背景的知识。这本书将由O’Reilly出版社于2022年初出版。
- en: We’d like to connect and explore the possibility of collaboration with OpenAI
    to evangelize access to API for people from all backgrounds and enable them to
    build exciting new solutions using GPT-3\. We would appreciate your input to ensure
    we accurately represent OpenAI's stance on GPT-3 and inform our readers on the
    upcoming advancements/features of the API.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望能够与OpenAI建立联系，探讨与各种背景的人士推广API访问以及让他们能够利用GPT-3构建令人兴奋的新解决方案的可能性。我们将感谢您的意见，以确保我们准确地代表OpenAI对GPT-3的立场，并告知我们的读者API的即将推出的进展/功能。
- en: It will be great if we can catch up sometime this week to discuss it further?
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能在本周某个时候见面讨论一下，那就太好了？
- en: Best,
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 致敬，
- en: S&S
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: S&S
- en: 'one-sentence summary:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 一句总结：
- en: 'And the output:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 并且输出：
- en: We’re writing a book on GPT-3 and would like to connect with OpenAI to get a
    better understanding of the API and the future developments.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在撰写一本关于GPT-3的书，并希望与OpenAI建立联系，以更好地了解API和未来的发展。
- en: Text Generation
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成
- en: Before the introduction of GPT-3, there was a general understanding that AI
    models are capable of carrying a short conversation with humans that answer specific
    questions or handle specific tasks. However the models were not sophisticated
    enough to handle complicated text generation tasks and started to lose track whenever
    they encountered something complex or abstract.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入GPT-3之前，人们普遍认为AI模型能够与人类进行短暂对话，回答特定问题或处理特定任务。然而，这些模型还不够复杂，无法处理复杂的文本生成任务，并且一旦遇到复杂或抽象的内容就开始失去跟踪。
- en: In the complicated world of natural language generation, GPT-3 has shaken the
    notion of language models being limited to trivial tasks. Text generation is the
    greatest strength of GPT-3\. It is capable of generating textual content that
    is almost indistinguishable from human-written text. GPT-3 is trained on billions
    of words from the training dataset to generate text in response to a variety of
    prompts. It generates an average of 4.5 billion words per day, [according to OpenAI](https://openai.com/blog/gpt-3-apps/).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在复杂的自然语言生成世界中，GPT-3颠覆了语言模型仅限于琐碎任务的观念。文本生成是GPT-3的最大优势。它能够生成几乎和人类写的文本一样的文本内容。GPT-3受训于来自训练数据集的数十亿字，以对各种提示生成文本。根据OpenAI的说法，它每天平均生成45亿字。
- en: In the next two examples, we experiment with using GPT-3 to create content for
    a personal productivity app start-up and social media posts. We give the model
    only minimal context, and it generates many of the responses in Figures 2-21 and
    2-22 on the first take.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个示例中，我们尝试使用GPT-3为个人生产力应用创业公司和社交媒体发布内容。我们只给模型提供了最少的上下文，它在第一次尝试中就生成了图2-21和图2-22中许多响应中的大部分。
- en: Article Generation
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 文章生成
- en: '![](img/image-0-22.jpg)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-22.jpg)'
- en: Figure 2-21\. Article generation example
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图2-21. 文章生成示例
- en: 'Our prompt:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的提示：
- en: ​​Here are the top 5 tips for personal productivity.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是个人生产力的前5个提示。
- en: 'And the output:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 并且输出：
- en: 1\. Develop a routine
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 建立例行程序
- en: Work can be overwhelming. You don’t need to remember every little detail. By
    having a routine, you will be able to work more efficiently.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 工作可能会让人感到不堪重负。你不需要记住每一个小细节。通过建立一个例行程序，你将能够更有效地工作。
- en: 2\. Avoid multitasking
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 避免同时处理多项任务
- en: If you are multitasking, you are bound to miss something. It is better to work
    on one task at a time.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在同时做多项任务，你很可能会错过一些事情。最好一次只处理一个任务。
- en: 3\. Take breaks
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 休息一下
- en: It is true that if you do not take breaks, you will be more productive. However,
    if you take breaks too often you will get tired and stressed. If you work more
    effectively, you will shorten the time needed to complete a task.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，如果不休息，你会更有效率。然而，如果你过于频繁地休息，你会感到疲劳和压力。如果你工作更有效率，你可以缩短完成任务所需的时间。
- en: 4\. Learn to delegate
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 学习委派
- en: If you have a lot of work to do, you should not do everything on your own. Delegate
    some of the work to others.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有很多工作要做，你不应该自己做完所有事情。将一些工作委派给他人。
- en: 5\. Prioritize
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 优先排序
- en: Prioritize what you have to do and do the most important tasks first. Don’t
    worry about the small things.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 优先处理必须做的事情，并首先完成最重要的任务。不要担心小事情。
- en: Social Media Post Generation
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 社交媒体帖子生成
- en: '![](img/image-0-23.jpg)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image-0-23.jpg)'
- en: Figure 2-22\. Social media post generation example
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2-22. 社交媒体帖子生成示例
- en: 'Here is our prompt:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的提示：
- en: Write a social media post for Facebook, Twitter, and Instagram about launching
    a new beta version of the personal productivity app.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Facebook、Twitter 和 Instagram 编写关于发布个人生产力应用程序新测试版本的社交媒体帖子。
- en: 'Facebook post:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook 帖子：
- en: 'And the output:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: “After extensive user testing, we are happy to announce that our latest update
    is available for download! Our team has worked hard to create the best version
    of our app yet. Download it now for free at www.app.com.”
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: “经过广泛的用户测试，我们很高兴宣布我们的最新更新现已可以下载！我们的团队努力创造了迄今为止我们应用程序的最佳版本。立即免费下载 www.app.com。”
- en: 'Twitter post:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 推特帖子：
- en: '“We''re rolling out an updated version of your favorite productivity app! Now
    available on iOS and Android. #productivity #beta”'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '“我们正在推出您最喜爱的生产力应用程序的更新版本！现在在 iOS 和 Android 上推出。#productivity #beta”'
- en: 'Instagram post:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: Instagram 帖子：
- en: '“We just released a new beta version of your favorite productivity app! Download
    it today for free on iOS and Android. #productivity #beta”'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '“我们刚刚发布了您最喜爱的生产力应用程序的新测试版本！今天在 iOS 和 Android 上免费下载。#productivity #beta”'
- en: Conclusion
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter we have covered the OpenAI playground, prompt engineering and
    the different components of the OpenAI API followed by playground examples covering
    the major NLP tasks. By now, you have an understanding of how the API works in
    tandem with different components and how to use Playground as the base to design
    and experiment with different training prompts.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 OpenAI Playground、提示工程以及 OpenAI API 的不同组件，然后通过覆盖主要 NLP 任务的 Playground
    示例。现在，您已经了解了 API 如何与不同组件协同工作，以及如何使用 Playground 作为基础来设计和尝试不同的训练提示。
- en: In the next chapter, we will walk you through how to use GPT-3 with different
    programming languages to integrate the API within your product or build a completely
    new application from scratch.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将带您了解如何使用 GPT-3 与不同编程语言协同工作，以将 API 集成到您的产品中或从头开始构建一个全新的应用程序。
