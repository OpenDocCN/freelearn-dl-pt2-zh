- en: Natural Language Processing
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '**Natural language processing** (**NLP**) is about analyzing texts and designing
    algorithms to process texts, making predictions from texts, or generating more
    text. NLP covers anything related to language, often including speech similar
    to what we saw in the *R**ecognizing voice commands* recipe in [Chapter 9](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml), *Deep
    Learning in Audio and Speech*. You might also want to refer to the *Battling algorithmic
    bias* recipe in [Chapter 2](bca59029-1915-4856-b47d-6041d7b10a0a.xhtml), *Advanced
    Topics in Supervised Machine Learning*, or the *Representing for similarity search* recipe
    in [Chapter 3](424f3988-2d11-4098-9c52-beb685a6ed27.xhtml), *Patterns, Outliers,
    and Recommendations*, for more traditional approaches. Most of this chapter will
    deal with the deep learning models behind the breakthroughs in recent years.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理** (**NLP**) 是关于分析文本并设计处理文本的算法，从文本中进行预测或生成更多文本。NLP 涵盖了与语言相关的任何内容，通常包括类似于我们在[第9章](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml)中看到的*识别语音命令*配方，*深度学习中的音频和语音*。您可能还想参考[第2章](bca59029-1915-4856-b47d-6041d7b10a0a.xhtml)中*对抗算法偏差*配方或[第3章](424f3988-2d11-4098-9c52-beb685a6ed27.xhtml)中*用于相似性搜索的表示*配方，以了解更传统的方法。本章大部分内容将涉及近年来突破性进展背后的深度学习模型。'
- en: Language is often seen to be intrinsically linked to human intelligence, and
    machines mastering communication capabilities have long been seen as closely intertwined
    with the goal of achieving **Artificial General Intelligence** (**AGI**). Alan
    Turing, in his 1950 article *Computing Machinery and Intelligence*, suggested
    a test, since then called the **Turing test**, in which interrogators have to
    find out whether their interlocutor (in a different room) is a computer or a human.
    It has been argued, however, that successfully tricking interrogators into thinking
    they are dealing with humans is not a proof of true understanding (or intelligence),
    but rather of manipulating symbols (the **Chinese room argument**; John Searle, *Minds,
    Brains, and Programs*, 1980). Whichever is the case, in recent years, with the
    availability of parallel computing devices such as GPUs, NLP has been making impressive
    progress in many benchmark tests, for example, in text classification: [http://nlpprogress.com/english/text_classification.html](http://nlpprogress.com/english/text_classification.html).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 语言通常被视为与人类智能密切相关，并且机器掌握沟通能力长期以来一直被认为与实现**人工通用智能** (**AGI**) 的目标密切相关。艾伦·图灵在他1950年的文章*计算机与智能*中建议了一个测试，后来称为**图灵测试**，在这个测试中，询问者必须找出他们的交流对象（在另一个房间里）是计算机还是人类。然而，有人认为，成功地欺骗询问者以为他们在与人类交流并不是真正理解（或智能）的证据，而是符号操纵的结果（**中文房间论证**；约翰·西尔，*思想、大脑和程序*，1980年）。不管怎样，在最近几年里，随着像GPU这样的并行计算设备的可用性，NLP
    在许多基准测试中取得了令人瞩目的进展，例如在文本分类方面：[http://nlpprogress.com/english/text_classification.html](http://nlpprogress.com/english/text_classification.html)。
- en: We'll first do a simple supervised task, where we determine the sentiment of
    paragraphs, then we'll set up an Alexa-style chatbot that responds to commands.
    Next, we'll translate a text using sequence-to-sequence models. Finally, we'll
    attempt to write a popular novel using state-of-the-art text generation models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将完成一个简单的监督任务，确定段落的情感，然后我们将设置一个响应命令的Alexa风格聊天机器人。接下来，我们将使用序列到序列模型翻译文本。最后，我们将尝试使用最先进的文本生成模型写一本流行小说。
- en: 'In this chapter, we''ll be doing these recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将进行以下配方：
- en: Classifying newsgroups
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对新闻组进行分类
- en: Chatting to users
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与用户聊天
- en: Translating a text from English to German
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本从英语翻译成德语
- en: Writing a popular novel
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 写一本流行小说
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: As in most chapters so far, we'll try both PyTorch and TensorFlow-based models.
    We'll apply different, more specialized libraries in each recipe.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与迄今为止大多数章节一样，我们将尝试基于PyTorch和TensorFlow的模型。我们将在每个配方中应用不同的更专业的库。
- en: As always, you can find the recipe notebooks on GitHub: [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter10](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter10).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，您可以在GitHub上找到配方笔记本：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter10](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter10)。
- en: Classifying newsgroups
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对新闻组进行分类
- en: 'In this recipe, we''ll do a relatively simple supervised task: based on texts,
    we''ll train a model to determine what an article is about, from a selection of
    topics. This is a relatively common task with NLP; we''ll try to give an overview
    of different ways to approach this.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将做一个相对简单的监督任务：基于文本，我们将训练一个模型来确定文章的主题，从一系列话题中选择。这是一个在 NLP 中相对常见的任务；我们将试图概述不同的解决方法。
- en: You might also want to compare the *Battling algorithmic bias* recipe in [Chapter
    2](bca59029-1915-4856-b47d-6041d7b10a0a.xhtml), *Advanced Topics in Supervised
    Machine Learning*, on how to approach this problem using a bag-of-words approach
    (`CountVectorizer` in scikit-learn). In this recipe, we'll be using approaches
    with word embeddings and deep learning models using word embeddings.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还想比较在《第2章》中 *算法偏差对抗* 的配方中，使用词袋法（在 scikit-learn 中的 `CountVectorizer`）来解决这个问题的方法。在这个配方中，我们将使用词嵌入和使用词嵌入的深度学习模型。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we''ll be using scikit-learn and TensorFlow (Keras), as in
    so many other recipes of this book. Additionally, we''ll use word embeddings that
    we''ll have to download, and we''ll use utility functions from the Gensim library
    to apply them in our machine learning pipeline:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用 scikit-learn 和 TensorFlow（Keras），正如本书的许多其他配方一样。此外，我们将使用需要下载的词嵌入，并且我们将使用
    Gensim 库的实用函数在我们的机器学习管道中应用它们：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We''ll be using a dataset from scikit-learn, but we still need to download
    the word embeddings. We''ll use Facebook''s fastText word embeddings trained on
    Wikipedia:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自 scikit-learn 的数据集，但我们仍然需要下载词嵌入。我们将使用 Facebook 的 fastText 词嵌入，该词嵌入是在
    Wikipedia 上训练的：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please note that the download can take a while and should take around 6 GB of
    disk space. If you are running on Colab, you might want to put the embedding file
    into a directory of your Google Drive, so you don't have to download it again
    when you restart your notebook.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，下载可能需要一些时间，并且需要大约 6 GB 的磁盘空间。如果您在 Colab 上运行，请将嵌入文件放入 Google Drive 的目录中，这样当您重新启动笔记本时就不需要重新下载。
- en: How to do it...
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: The newsgroups dataset is a collection of around 20,000 newsgroup documents
    divided into 20 different groups. The 20 newsgroups collection is a popular dataset
    for testing machine learning techniques in NLP, such as text classification and
    text clustering.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 新闻组数据集是大约 20,000 个新闻组文档的集合，分为 20 个不同的组。20 个新闻组集合是在 NLP 中测试机器学习技术（如文本分类和文本聚类）的流行数据集。
- en: We'll be classifying a selection of newsgroups into three different topics,
    and we'll be approaching this task with three different techniques that we can
    compare. We'll first get the dataset, and then apply a bag-of-words technique,
    using word embeddings, training custom word embeddings in a deep learning model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把一组新闻组分类为三个不同的主题，并且我们将使用三种不同的技术来解决这个任务，以便进行比较。首先获取数据集，然后应用词袋法技术，使用词嵌入，在深度学习模型中训练定制的词嵌入。
- en: 'First, we''ll download the dataset using scikit-learn functionality. We''ll
    download the newgroup dataset in two batches, for training and testing, respectively:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用 scikit-learn 的功能下载数据集。我们将新闻组数据集分两批下载，分别用于训练和测试。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This conveniently gives us training and test datasets, which we can use in the
    three approaches.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这方便地为我们提供了训练和测试数据集，我们可以在这三种方法中使用。
- en: Let's begin with covering the first one, using a bag-of-words approach.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始覆盖第一个方法，使用词袋法。
- en: Bag-of-words
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词袋法
- en: 'We''ll build a pipeline of counting words and reweighing them according to
    their frequency. The final classifier is a random forest. We train this on our
    training dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个单词计数和根据它们的频率重新加权的管道。最终的分类器是一个随机森林。我们在训练数据集上训练这个模型：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`CountVectorizer` counts tokens in texts and `tfidfTransformer` reweighs the
    counts. We''ll discuss the **term frequency-inverse document frequency** (**TFIDF**)
    reweighting in the *How it works...* section.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 计算文本中的标记，而 `tfidfTransformer` 重新加权这些计数。我们将在 *工作原理...* 部分讨论**词项频率-逆文档频率**（**TFIDF**）重新加权。'
- en: 'After the training, we can test the accuracy on the test dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以在测试数据集上测试准确率：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We get an accuracy of about 0.805\. Let's see how our other two methods will
    do. Using word embeddings is next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率约为 0.805。让我们看看我们的另外两种方法表现如何。下一步是使用词嵌入。
- en: Word embeddings
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: 'We''ll load up our previously downloaded word embeddings:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载我们之前下载的词嵌入：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The most straightforward strategy to vectorize a text of several words is to
    average word embeddings across words. This works usually at least reasonably well
    for short texts:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本的最简单策略向量化是对单词嵌入进行平均。对于短文本，这通常至少效果还不错：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We''ll apply this vectorization to our dataset and then train a random forest
    classifier on top of these vectors:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将这种向量化应用于我们的数据集，然后在这些向量的基础上训练一个随机森林分类器：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can then test the performance of our approach:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以测试我们方法的性能：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We get an accuracy of about 0.862.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的准确率约为0.862。
- en: Let's see whether our last method does any better than this. We'll build customized
    word embeddings using Keras' embedding layer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的最后一种方法是否比这个更好。我们将使用Keras的嵌入层构建定制的词嵌入。
- en: Custom word embeddings
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义词嵌入
- en: 'An embedding layer is a way to create customized word embeddings on the fly
    in neural networks:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层是在神经网络中即时创建自定义词嵌入的一种方式：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have to tell the embedding layer how many words you want to store, how many
    dimensions your word embeddings should have, and how many words are in each text.
    We feed in arrays of integers that each refer to words in a dictionary. We can
    delegate the job of creating the input for the embedding layer to TensorFlow utility
    functions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须告诉嵌入层希望存储多少单词，词嵌入应该具有多少维度，以及每个文本中有多少单词。我们将整数数组输入嵌入层，每个数组引用字典中的单词。我们可以将创建嵌入层输入的任务委托给TensorFlow实用函数：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This creates the dictionary. Now we need to tokenize the text and pad sequences
    to the right length:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了字典。现在我们需要对文本进行分词并将序列填充到适当的长度：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we are ready to build our neural network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备构建我们的神经网络：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our model contains half a million parameters. Approximately half of them sit
    in the embedding, and the other half in the feedforward fully connected layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型包含50万个参数。大约一半位于嵌入层，另一半位于前馈全连接层。
- en: 'We fit our networks for a few epochs, and then we can test our accuracy on
    the test data:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对网络进行了几个epoch的拟合，然后可以在测试数据上测试我们的准确性：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We get about 0.902 accuracy. We haven't tweaked the model architecture yet.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得约为0.902的准确率。我们还没有调整模型架构。
- en: This concludes our newsgroup classification using bag-of-words, pre-trained
    word embeddings, and custom word embeddings. We'll now come to some background.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们使用词袋模型、预训练词嵌入和自定义词嵌入进行新闻组分类的工作。现在我们来探讨一些背景。
- en: How it works...
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'We''ve classified texts based on three different approaches of featurization:
    bag-of-words, pre-trained word embeddings, and custom word embeddings. Let''s
    briefly delve into word embeddings and TFIDF.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经根据三种不同的特征化方法对文本进行了分类：词袋模型、预训练词嵌入和自定义词嵌入。让我们简要地深入研究词嵌入和TFIDF。
- en: We've already talked about the Skipgram and the **Continuous Bag of Words**
    (**CBOW**) algorithms in the *Making decisions based on knowledge *recipe in [Chapter
    5](146f9a36-b2f6-4853-9fed-229537c08052.xhtml), *Heuristic Search Techniques and
    Logical Inference* (within the *Graph embedding with Walklets *subsection).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](146f9a36-b2f6-4853-9fed-229537c08052.xhtml)的*基于知识做决策*配方中，我们已经讨论了*Skipgram*和**Continuous
    Bag of Words**（**CBOW**）算法，在*启发式搜索技术与逻辑推理*（在*使用Walklets进行图嵌入*子节中）。
- en: Very briefly, word vectors are a simple machine learning model that can predict
    the next word based on the context (the CBOW algorithm) or can predict the context
    based on a single word (the Skipgram algorithm). Let's quickly look at the CBOW
    neural network.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，词向量是一个简单的机器学习模型，可以根据上下文（CBOW算法）预测下一个词，或者可以根据一个单词预测上下文（Skipgram算法）。让我们快速看一下CBOW神经网络。
- en: The CBOW algorithm
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CBOW算法
- en: 'The CBOW algorithm is a two-layer feedforward neural network that predicts
    words (rather, the sparse index vector) from their context:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW算法是一个两层前馈神经网络，用于从上下文中预测单词（更确切地说是稀疏索引向量）：
- en: '![](img/e1ff8dab-e14b-4ecd-8223-865fec0b449a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1ff8dab-e14b-4ecd-8223-865fec0b449a.png)'
- en: 'This illustration shows how, in the CBOW model, words are predicted based on
    the surrounding context. Here, words are represented as bag-of-words vectors.
    The hidden layer is composed of a weighted average of the context (linear projection).
    The output word is a prediction based on the hidden layer. This is adapted from
    an image on the French-language Wikipedia page on word embeddings: [https://fr.wikipedia.org/wiki/Word_embedding](https://fr.wikipedia.org/wiki/Word_embedding).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅插图展示了在CBOW模型中，基于周围上下文预测单词的方式。在这里，单词被表示为词袋向量。隐藏层由上下文的加权平均值组成（线性投影）。输出单词是基于隐藏层的预测。这是根据法语维基百科关于词嵌入的页面上的一幅图片进行调整的：[https://fr.wikipedia.org/wiki/Word_embedding](https://fr.wikipedia.org/wiki/Word_embedding)。
- en: 'What we haven''t talked about is the implication of these word embeddings,
    which created such a stir when they came out. The embeddings are the network activations
    for a single word, and they have a compositional property that gave a title to
    many talks and a few papers. We can combine vectors to do semantic algebra or
    make analogies. The best-known example of this is the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有讨论这些词嵌入的含义，这些词嵌入在它们出现时引起了轰动。这些嵌入是单词的网络激活，并具有组合性质，这为许多演讲和少数论文赋予了标题。我们可以结合向量进行语义代数或进行类比。其中最著名的例子是以下内容：
- en: '![](img/9c86828c-52e3-4874-90d7-981bb90d8792.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c86828c-52e3-4874-90d7-981bb90d8792.png)'
- en: Intuitively, a king and a queen are similar societal positions, only one is
    taken up by a man, the other by a woman. This is reflected in the embedding space
    learned on billions of words. Starting with the vector of king, subtracting the
    vector of man, and finally adding the vector of woman, the closest word that we
    end up at is queen.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，国王和王后是相似的社会职位，只是一个由男人担任，另一个由女人担任。这在数十亿个单词学习的嵌入空间中得到了反映。从国王的向量开始，减去男人的向量，最后加上女人的向量，我们最终到达的最接近的单词是王后。
- en: 'The embedding space can tell us a lot about how we use language, some of it
    a bit concerning, such as when the word vectors exhibit gender stereotypes:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入空间可以告诉我们关于我们如何使用语言的很多信息，其中一些信息有些令人担忧，比如当单词向量展现出性别刻板印象时。
- en: '![](img/1efd0ca5-7f8b-44d1-a001-c2c8a6b5389a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1efd0ca5-7f8b-44d1-a001-c2c8a6b5389a.png)'
- en: This can actually be corrected to some degree using affine transformations as
    shown by Tolga Bolukbasi and others (*Man* *is to Computer Programmer as Woman
    is to Homemaker? Debiasing Word Embeddings*, 2016; [https://arxiv.org/abs/1607.06520](https://arxiv.org/abs/1607.06520)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过仿射变换在一定程度上进行校正，如Tolga Bolukbasi等人所示（*Man* *is to Computer Programmer as
    Woman is to Homemaker? Debiasing Word Embeddings*, 2016; [https://arxiv.org/abs/1607.06520](https://arxiv.org/abs/1607.06520))。
- en: Let's have a quick look at the reweighting employed in the bag-of-words approach
    of this recipe.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看看在这个方法的词袋法中采用的重新加权。
- en: TFIDF
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TFIDF
- en: In the *Bag-of-words* section, we counted words using `CountVectorizer`. This
    gives us a vector of ![](img/44e2f2df-1941-479d-ab2d-16eeb139b557.png), where
    ![](img/fa33ae63-50e8-44fb-9a40-4d3bc9a20d99.png) is the number of words in the
    vocabulary. The vocabulary has to be created during the `fit()` stage of `CountVectorizer`,
    before `transform()` will be able to create the (sparse) vector based on the positions
    of tokens (words) in the vocabulary.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在*词袋模型*部分，我们使用`CountVectorizer`来计数单词。这给了我们一个形状为![](img/44e2f2df-1941-479d-ab2d-16eeb139b557.png)的向量，其中![](img/fa33ae63-50e8-44fb-9a40-4d3bc9a20d99.png)是词汇表中的单词数量。词汇表必须在`fit()`阶段之前创建，然后`transform()`可以基于词汇表中标记（单词）的位置创建（稀疏）向量。
- en: By applying `CountVectorizer` for a number of documents, we get a sparse matrix
    of shape ![](img/43576faf-0270-40f4-bccc-87fb8526b49d.png), where ![](img/dcc5aab4-b1da-471e-bad9-e651b6474bb6.png) is
    the corpus (the collection of documents), and ![](img/647ead55-3682-4f3d-928d-44af1d8bfcd0.png) the
    number of documents. Each position in this matrix accounts for the number of times
    a certain token occurs in a document. In this recipe, a token corresponds to a
    word, however, it can equally be a character or any collection of characters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在多篇文档上应用`CountVectorizer`，我们可以得到一个形状为![](img/43576faf-0270-40f4-bccc-87fb8526b49d.png)的稀疏矩阵，其中![](img/dcc5aab4-b1da-471e-bad9-e651b6474bb6.png)是语料库（文档集合），而![](img/647ead55-3682-4f3d-928d-44af1d8bfcd0.png)是文档的数量。这个矩阵中的每个位置表示某个标记在文档中出现的次数。在这个方法中，一个标记对应一个单词，但它同样可以是字符或任何一组字符。
- en: Some words might occur in every document; others might occur only in a small
    subset of documents, suggesting they are more specific and precise. That's the
    intuition of TFIDF, where the importance of counts (columns in the matrix) is
    raised if a word frequency across the corpus (the collection of documents) is
    low.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有些词可能出现在每个文档中；其他词可能只出现在文档的一个小子集中，这表明它们更为特定和精确。这就是TFIDF的直觉，即如果一个词在语料库（文档集合）中的频率低，则提升计数（矩阵中的列）的重要性。
- en: 'The inverse-term given a term for a set of documents ![](img/8562cff9-aa50-416a-9def-f79755765a60.png)
    is defined as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组文档 ![](img/8562cff9-aa50-416a-9def-f79755765a60.png) 的反向术语 ![](img/8562cff9-aa50-416a-9def-f79755765a60.png) 的定义如下：
- en: '![](img/04c10278-22df-4bc1-b90b-539e0648ba00.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04c10278-22df-4bc1-b90b-539e0648ba00.png)'
- en: Here ![](img/2686c059-3f84-4c5f-b5f5-efe4a9bae38c.png) is the count of a term
    ![](img/8fc9090d-e14e-4dda-954e-c91c0ba9792b.png) in a document ![](img/14778b5a-a60b-4c39-8904-05e8fb17293d.png),
    and ![](img/9ae48337-e38e-4601-839a-c867bec58164.png) is the number of documents
    where ![](img/3b2b845a-66fa-4808-819c-5b008263395b.png) appears. You should see
    that the TFIDF value decreases with ![](img/e2a4a99e-9466-4b5a-a8ac-78a50e58af85.png). As
    a term occurs in more documents, the logarithm and the TFIDF value approach 0.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![](img/2686c059-3f84-4c5f-b5f5-efe4a9bae38c.png) 是术语的计数 ![](img/8fc9090d-e14e-4dda-954e-c91c0ba9792b.png) 在文档 ![](img/14778b5a-a60b-4c39-8904-05e8fb17293d.png) 中，以及 ![](img/9ae48337-e38e-4601-839a-c867bec58164.png) 是术语 ![](img/3b2b845a-66fa-4808-819c-5b008263395b.png) 出现的文档数。您应该看到TFIDF值随 ![](img/e2a4a99e-9466-4b5a-a8ac-78a50e58af85.png) 的增加而减少。随着术语出现在更多文档中，对数和TFIDF值接近0。
- en: In the next recipes of this chapter, we'll go beyond the encodings of single
    words and study more complex language models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一个示例中，我们将超越单词的编码，研究更复杂的语言模型。
- en: There's more...
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We''ll briefly look at learning your own word embeddings using Gensim, building
    more complex deep learning models, and using pre-trained word embeddings in Keras:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍如何使用Gensim学习自己的词嵌入，构建更复杂的深度学习模型，并在Keras中使用预训练的词嵌入：
- en: We can easily train our own word embeddings on texts in Gensim.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在Gensim上轻松地训练自己的词嵌入。
- en: 'Let''s read in a text file in order to feed it as the training dataset for
    fastText:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读入一个文本文件，以便将其作为fastText的训练数据集：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This can be useful for transfer learning, search applications, or for cases
    when learning the embeddings would take too long. Using Gensim, this is only a
    few lines of code (adapted from the Gensim documentation).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于迁移学习、搜索应用或在学习嵌入过程中花费过长时间的情况非常有用。在Gensim中，这只需几行代码（改编自Gensim文档）。
- en: 'The training itself is straightforward and, since our text file is small, relatively
    quick:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 训练本身很简单，并且由于我们的文本文件很小，所以相对快速：
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can find the *Crime and Punishment* novel at Project Gutenberg, where there
    are many more classic novels: [http://www.gutenberg.org/ebooks/2554](http://www.gutenberg.org/ebooks/2554).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Project Gutenberg找到《罪与罚》等经典小说：[http://www.gutenberg.org/ebooks/2554](http://www.gutenberg.org/ebooks/2554)。
- en: 'You can retrieve vectors from the trained model like this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以像这样从训练好的模型中检索向量：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Gensim comes with a lot of functionality, and we recommend you have a read through
    some of its documentation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim具有丰富的功能，我们建议您阅读一些其文档。
- en: 'Building more complex deep learning models: for more difficult problems, we
    can use stacked `conv1d` layers on top of the embedding, as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建更复杂的深度学习模型：对于更困难的问题，我们可以在嵌入层之上使用堆叠的`conv1d`层，如下所示：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Convolutional layers come with very few parameters, which is another advantage
    of using them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层具有非常少的参数，这是使用它们的另一个优点。
- en: 'Using pretrained word embeddings in a Keras model: If we want to use downloaded
    (or previously customized word embeddings), we can do this as well. We first need
    to create a dictionary, which we can easily do ourselves after loading them in,
    for example, with Gensim:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras模型中使用预训练的词嵌入：如果我们想要使用已下载的（或之前定制的）词嵌入，我们也可以这样做。首先，我们需要创建一个字典，在加载它们后我们可以很容易地自己完成，例如，使用Gensim：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can then feed these vectors into the embedding layer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将这些向量馈送到嵌入层中：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For training and testing, you have to feed in the word indices by looking them
    up in our new dictionary and pad them to the same length as we've done before.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练和测试，您必须通过在我们的新词典中查找它们来提供单词索引，并像之前一样将它们填充到相同的长度。
- en: 'This concludes our recipe on classifying newsgroups. We''ve applied three different
    types of featurization: bag-of-words, a pre-trained word embedding, and a custom
    word embedding in a simple neural network.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于新闻组分类的配方。我们应用了三种不同的特征化方法：词袋模型、预训练词嵌入以及简单神经网络中的自定义词嵌入。
- en: See also
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'We used word embeddings in this recipe. A lot of different embedding methods
    have been introduced, and quite a few word embedding matrices have been published
    that were trained on hundreds of billions of words from many millions of documents. Such
    large-scale training could cost as much as hundreds of thousands of dollars if
    done on rented hardware. The most popular word embeddings are the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用了词嵌入。已经介绍了许多不同的嵌入方法，并且已经发布了许多训练自数百亿字词和数百万文档的词嵌入矩阵。如果在租用的硬件上进行大规模训练，这可能会耗费数十万美元。最流行的词嵌入包括以下内容：
- en: GloVe: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
- en: fastText: [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastText: [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html)
- en: Word2vec: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2vec: [https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)
- en: 'Popular libraries for dealing with word embeddings are these:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 处理词嵌入的流行库包括以下内容：
- en: Gensim: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gensim: [https://radimrehurek.com/gensim/](https://radimrehurek.com/gensim/)
- en: fastText: [https://fasttext.cc/](https://fasttext.cc/)
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: fastText: [https://fasttext.cc/](https://fasttext.cc/)
- en: spaCy: [https://spacy.io/](https://spacy.io/)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy: [https://spacy.io/](https://spacy.io/)
- en: '`Kashgari` is a library built on top of Keras for text labeling and text classification
    and includes Word2vec and more advanced models such as BERT and GPT2 language
    embeddings: [https://github.com/BrikerMan/Kashgari](https://github.com/BrikerMan/Kashgari).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`Kashgari` 是建立在 Keras 之上用于文本标注和文本分类的库，包括 Word2vec 和更高级的模型，如BERT和GPT2语言嵌入：[https://github.com/BrikerMan/Kashgari](https://github.com/BrikerMan/Kashgari)。'
- en: The Hugging Face transformer library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers))
    includes many advanced architectures and pre-trained weights for many transformer
    models that can be used for text embedding. These models can achieve state-of-the-art
    performance in many NLP tasks. For instance, companies such as Google have moved
    many of their language applications to the BERT architecture. We'll learn more
    about transformer architectures in the *Translating a text from English to German*
    recipe in this chapter.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 变压器库（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）包含许多先进的架构和许多变压器模型的预训练权重，可用于文本嵌入。这些模型可以在许多自然语言处理任务中实现最先进的性能。例如，谷歌等公司已将许多语言应用转移到
    BERT 架构上。我们将在本章的*将英语翻译为德语的文本*配方中学习更多有关变压器架构的信息。
- en: fast.ai provides a compendium of tutorials and courses on deep learning with
    PyTorch; it includes many resources about NLP as well: [https://nlp.fast.ai/](https://nlp.fast.ai/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: fast.ai 提供了关于使用 PyTorch 进行深度学习的许多教程和课程的综合信息；它还包含许多有关自然语言处理的资源：[https://nlp.fast.ai/](https://nlp.fast.ai/)。
- en: Finally, in NLP, often there can be thousands or even millions of different
    labels in classification tasks. This has been termed an **eXtreme MultiLabel** (**XML**)
    scenario. You can find a notebook tutorial on XML here: [https://github.com/ppontisso/Extreme-Multi-Label-Classification](https://github.com/ppontisso/Extreme-Multi-Label-Classification).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在自然语言处理中，分类任务中常常涉及成千上万甚至数百万个不同的标签。这种情况被称为**eXtreme MultiLabel**（**XML**）场景。你可以在这里找到关于
    XML 的笔记本教程：[https://github.com/ppontisso/Extreme-Multi-Label-Classification](https://github.com/ppontisso/Extreme-Multi-Label-Classification)。
- en: Chatting to users
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与用户聊天
- en: 'In 1966, Joseph Weizenbaum published an article about his chatbot ELIZA, called *ELIZA—a
    computer program for the study of natural language communication between man and
    machine*. Created with a sense of humor to show the limitations of technology,
    the chatbot employed simplistic rules and vague, open-ended questions as a way
    of giving an impression of empathic understanding in the conversation, and was
    in an ironic twist often seen as a milestone of artificial intelligence. The field
    has moved on, and today, AI assistants are around us: you might have an Alexa,
    a Google Echo, or any of the other commercial home assistants in the market.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 1966年，约瑟夫·韦伊岑鲍姆发表了一篇关于他的聊天机器人ELIZA的文章，名为*ELIZA - 人与机器之间自然语言交流研究的计算机程序*。以幽默的方式展示技术的局限性，该聊天机器人采用简单的规则和模糊的开放性问题，以表现出对话中的移情理解，并以具有讽刺意味的方式经常被视为人工智能的里程碑。该领域已经发展，今天，AI助手就在我们身边：您可能有Alexa、Google
    Echo或市场上其他任何商业家庭助手。
- en: In this recipe, we'll be building an AI assistant. The difficulty with this
    is that there are an infinite amount of ways for people to express themselves
    and that it is simply impossible to anticipate everything your users might say.
    In this recipe, we'll be training a model to infer what they want and we'll respond
    accordingly in consequence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将构建一个AI助手。这其中的困难在于，人们表达自己的方式有无数种，而且根本不可能预料到用户可能说的一切。在这个教程中，我们将训练一个模型来推断他们想要什么，并且我们会相应地做出回应。
- en: Getting ready
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we''ll be using a framework developed by Fariz Rahman called **Eywa**.
    We''ll install it with `pip` from GitHub:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个教程，我们将使用Fariz Rahman开发的名为**Eywa**的框架。我们将从GitHub使用`pip`安装它：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Eywa has the main capabilities of what's expected from a conversational agent,
    and we can look at its code for some of the modeling that's behind its power.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Eywa具有从对话代理中预期的主要功能，我们可以查看其代码，了解支撑其功能的建模。
- en: 'We are also going to be using the OpenWeatherMap Web API through the `pyOWM`
    library, so we''ll install this library as well:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过`pyOWM`库使用OpenWeatherMap Web API，因此我们也将安装这个库：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With this library, we can request weather data in response to user requests
    as part of our chatbot functionality. If you want to use this in your own chatbot,
    you should register a free user account and get your API key on [OpenWeatherMap.org](https://openweathermap.org/)
    for up to 1,000 requests a day.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个库，我们可以作为我们聊天机器人功能的一部分响应用户请求并请求天气数据。如果您想在自己的聊天机器人中使用此功能，您应该在[OpenWeatherMap.org](https://openweathermap.org/)注册一个免费用户账户并获取您的API密钥，每天最多可请求1,000次。
- en: Let's see how we implement this.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何实现这一点。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Our agent will process sentences by the user, interpret them, and respond accordingly.
    It will first predict the intent of user queries, and then extract entities in
    order to know more precisely what the query is about, before returning an answer:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理将处理用户输入的句子，解释并相应地回答。它将首先预测用户查询的意图，然后提取实体，以更准确地了解查询的内容，然后返回答案：
- en: 'Let''s start with the intent classes – based on a few samples of phrases each,
    we''ll define intents such as `greetings`, `taxi`, `weather`, `datetime`, and
    `music`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从意图类开始 - 基于一些短语示例，我们将定义诸如`greetings`、`taxi`、`weather`、`datetime`和`music`等意图：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We''ve created a classifier based on conversation samples. We can quickly test
    how this works using the following code block:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了基于对话样本的分类器。我们可以使用以下代码块快速测试其工作原理：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can successfully predict whether the required action is regarding the weather,
    a hotel booking, music, or about the time.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以成功预测所需操作是否涉及天气、酒店预订、音乐或时间。
- en: 'As the next step, we need to understand whether there''s something more specific
    to the intent, such as the weather in London versus the weather in New York, or
    playing the Beatles versus Kanye West. We can use `eywa` entity extraction for
    this purpose:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为下一步，我们需要了解意图是否有更具体的内容，例如伦敦的天气与纽约的天气，或者播放披头士与坎耶·韦斯特。我们可以使用`eywa`实体提取来实现此目的：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This is to check for a specific place for the weather prediction. We can test
    the entity extraction for the weather as well:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为了检查天气预测的特定位置。我们也可以测试天气的实体提取：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We ask for the weather in London, and, in fact, our entity extraction successfully
    comes back with the place name:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们询问伦敦的天气，并且我们的实体提取成功地返回了地点名称：
- en: '[PRE26]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We also need to code the functionality of our conversational agent, such as
    looking up the weather forecast. Let''s first do a weather request:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ELIZA
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can request weather forecasts given a location with the Python OpenWeatherMap
    library (`pyOWM`). Calling the new function, `get_weather_forecast()`, with `London`
    as its argument results in this as of in the time of writing:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍中提到的原始ELIZA有许多语句-响应对，例如以下内容：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Please note that you need to use your own (free) OpenWeatherMap API key if you
    want to execute this.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 没有问候和日期，没有一个聊天机器人是完整的：
- en: 'No chatbot is complete without a greeting and the date:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在匹配正则表达式的情况下，会随机选择一种可能的响应，如果需要，动词会进行转换，包括使用如下逻辑进行缩写：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let''s create some interaction based on the classifier and entity extraction.
    We''ll write a response function that can greet, tell the date, and give a weather
    forecast:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “您今天还有其他问题或疑虑需要我帮助您吗？”
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We are leaving out functionality for calling taxis or playing music:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Eywa，一个用于对话代理的框架，具有三个主要功能：
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The `question_and_answer()` function answers a user query.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们详细讨论这些内容之前，看一下介绍中提到的ELIZA聊天机器人可能会很有趣。这将希望我们了解需要理解更广泛语言集的改进。
- en: 'We can now have a limited conversation with our agent if we ask it questions:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是Jez Higgins在GitHub上的ELIZA仿制品的摘录：[https://github.com/jezhiggins/eliza.py](https://github.com/jezhiggins/eliza.py)。
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This wraps up our recipe. We've implemented a simple chatbot that first predicts
    intent and then extracts entities. Based on intent and entities, a user query
    is answered based on rules.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**实体提取器** - 从句子中提取命名实体'
- en: You should be able to ask for the date and the weather in different places,
    however, it will tell you to upgrade your software if you ask for taxis or music. You
    should be able to implement and extend this functionality by yourself if you are
    interested.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够询问不同地方的日期和天气情况，但如果你询问出租车或音乐，它会告诉你需要升级你的软件。
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <问候>
- en: We've implemented a very simple, though effective, chatbot for basic tasks.
    It should be clear how this can be extended and customized for more or other tasks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器而言，在开始阶段，硬编码一些规则会更容易，但如果您想处理更多复杂性，您将构建解释意图和位置等参考的模型。
- en: Before we go through some of this, it might be of interest to look at the ELIZA
    chatbot mentioned in the introduction of this recipe. This will hopefully shed
    some light on what improvements we need to understand a broader set of languages.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: How did ELIZA work?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了我们的配方。我们实现了一个简单的聊天机器人，首先预测意图，然后基于规则提取实体回答用户查询。
- en: ELIZA
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们省略了呼叫出租车或播放音乐的功能：
- en: 'The original ELIZA mentioned in the introduction has many statement-response
    pairs, such as the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: “感谢您的来电，我的名字是_。今天我能为您做什么？”
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Given a match of the regular expression, one of the possible responses is chosen
    randomly, while verbs are transformed if necessary, including contractions, using
    logic like this, for example:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python OpenWeatherMap库（`pyOWM`）请求给定位置的天气预报。在撰写本文时，调用新功能`get_weather_forecast()`，将`London`作为参数传入，结果如下：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: These are excerpts from Jez Higgins' ELIZA knock-off on GitHub: [https://github.com/jezhiggins/eliza.py](https://github.com/jezhiggins/eliza.py).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ELIZA是如何工作的？
- en: 'Sadly perhaps experiences with call centers might seem similar. They often
    employ scripts as well, such as the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣，你应该能够自己实现和扩展这个功能。
- en: <Greeting>
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于分类器和实体提取创建一些互动。我们将编写一个响应函数，可以问候，告知日期和提供天气预报：
- en: '"Thank you for calling, my name is _. How can I help you today?"'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Eywa
- en: '...'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您想执行此操作，您需要使用您自己的（免费）OpenWeatherMap API密钥。
- en: '"Do you have any other questions or concerns that I can help you with today?"'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为基本任务实现了一个非常简单但有效的聊天机器人。很明显，这可以扩展和定制以处理更多或其他任务。
- en: While for machines, in the beginning, it is easier to hardcode some rules, if
    you want to handle more complexity, you'll be building models that interpret intentions
    and references such as locations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`question_and_answer()`函数回答用户查询。'
- en: Eywa
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不幸的是，可能与呼叫中心的经历看起来很相似。它们通常也使用脚本，例如以下内容：
- en: 'Eywa, a framework for conversational agents, comes with three main functionalities:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何运作的…
- en: '**A classifier** – to decide what class the user input belongs to from a choice
    of a few'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们问它问题，我们现在可以与我们的代理进行有限的对话：
- en: '**An entity extractor** – to extract named entities from sentences'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要编码我们对话代理的功能，例如查找天气预报。让我们首先进行天气请求：
- en: '**Pattern matching** – for variable matching based on parts of speech and semantic
    meaning'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式匹配** – 基于词性和语义意义进行变量匹配'
- en: 'All three are very simple to use, though quite powerful. We''ve seen the first
    two functionalities in action in the *How to do it...* section. Let''s see the
    pattern matching for food types based on semantic context:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这三者使用起来非常简单，但功能强大。我们在“如何做...”部分看到了前两者的功能。让我们看看基于语义上下文的食品类型模式匹配：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We create a variable food with sample values: `pizza`, `banana`, `yogurt`,
    and `kebab`. Using food terms in similar contexts will match our variables. The
    expression should return this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个名为 food 的变量，并赋予样本值：`pizza`、`banana`、`yogurt` 和 `kebab`。在类似上下文中使用食品术语将匹配我们的变量。这个表达式应该返回这个：
- en: '[PRE36]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The usage looks very similar to regular expressions, however, while regular
    expressions are based on words and their morphology, `eywa.nlu.Pattern` works
    semantically, anchored in word embeddings.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用看起来与正则表达式非常相似，但正则表达式基于单词及其形态学，`eywa.nlu.Pattern` 则在语义上锚定在词嵌入中工作。
- en: 'A **regular expression** (short: regex) is a sequence of characters that define
    a search pattern. It was first formalized by Steven Kleene and implemented by
    Ken Thompson and others in Unix tools such as QED, ed, grep, and sed in the 1960s.
    This syntax has entered the POSIX standard and is therefore sometimes referred
    to as **POSIX regular expressions**. A different standard emerged in the late
    1990s with the Perl programming language, termed **Perl Compatible Regular Expressions** (**PCRE**),
    which has been adopted in different programming languages, including Python.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则表达式**（简称：regex）是定义搜索模式的字符序列。它由 Steven Kleene 首次形式化，并由 Ken Thompson 和其他人在
    Unix 工具（如 QED、ed、grep 和 sed）中实现于 1960 年代。这种语法已进入 POSIX 标准，因此有时也称为**POSIX 正则表达式**。在
    1990 年代末，随着 Perl 编程语言的出现，出现了另一种标准，称为**Perl 兼容正则表达式**（**PCRE**），已在包括 Python 在内的不同编程语言中得到采用。'
- en: How do these models work?
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型如何工作？
- en: 'First of all, the `eywa` library relies on sense2vec word embeddings from explosion.ai.
    Sense2vec word embeddings were introduced by Andrew Trask and others (*sense2vec
    – A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings*,
    2015). This idea was taken up by explosion.ai, who trained part-of-speech disambiguated
    word embeddings on Reddit discussions. You can read up on these on the explosion.ai
    website: [https://explosion.ai/blog/sense2vec-reloaded](https://explosion.ai/blog/sense2vec-reloaded).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`eywa` 库依赖于来自 explosion.ai 的 sense2vec 词嵌入。sense2vec 词嵌入由 Andrew Trask 和其他人引入（*sense2vec
    – A Fast and Accurate Method for Word Sense Disambiguation In Neural Word Embeddings*,
    2015）。这个想法被 explosion.ai 接受，他们在 Reddit 讨论中训练了词性消歧的词嵌入。您可以在 explosion.ai 的网站上阅读更多信息：[https://explosion.ai/blog/sense2vec-reloaded](https://explosion.ai/blog/sense2vec-reloaded)。
- en: The classifier goes through the stored conversational items and picks out the
    match with the highest similarity score based on these embeddings. Please note
    that `eywa` has another model implementation based on recurrent neural networks.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器通过存储的对话项目并根据这些嵌入选择具有最高相似度分数的匹配项。请注意，`eywa` 还有另一个基于递归神经网络的模型实现。
- en: See also
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Libraries and frameworks abound for creating chatbots with different ideas
    and integrations:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 创建聊天机器人的库和框架非常丰富，包括不同的想法和集成方式：
- en: ParlAI is a library for training and testing dialog models. It comes with more
    than 80 dialog datasets out of the box as well as, integration with Facebook Messenger
    and Mechanical Turk: [https://github.com/facebookresearch/ParlAI](https://github.com/facebookresearch/ParlAI).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ParlAI 是一个用于训练和测试对话模型的库。它自带了超过 80 个对话数据集，并且可以与 Facebook Messenger 和 Mechanical
    Turk 集成：[https://github.com/facebookresearch/ParlAI](https://github.com/facebookresearch/ParlAI)。
- en: 'NVIDIA has its own toolkit for conversational AI applications and comes with
    many modules providing additional functionality such as automatic speech recognition
    and speech synthesis: [https://github.com/NVIDIA/NeMo](https://github.com/NVIDIA/NeMo).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA 拥有自己的用于对话 AI 应用的工具包，并提供许多模块，如自动语音识别和语音合成：[https://github.com/NVIDIA/NeMo](https://github.com/NVIDIA/NeMo)。
- en: Google Research open sourced their code for an open-domain dialog system: [https://github.com/google-research/google-research/tree/master/meena](https://github.com/google-research/google-research/tree/master/meena).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌研究开源了他们用于开放域对话系统的代码：[https://github.com/google-research/google-research/tree/master/meena](https://github.com/google-research/google-research/tree/master/meena)。
- en: 'Rasa incorporates feedback on every interaction to improve the chatbot: [https://rasa.com/](https://rasa.com/).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rasa在每次交互中都会整合反馈以改进聊天机器人：[https://rasa.com/](https://rasa.com/)。
- en: 'Chatterbot, a spaCy-based library: [https://spacy.io/universe/project/Chatterbot](https://spacy.io/universe/project/Chatterbot).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chatterbot，基于spaCy的库：[https://spacy.io/universe/project/Chatterbot](https://spacy.io/universe/project/Chatterbot)。
- en: Translating a text from English to German
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本从英语翻译成德语
- en: In this recipe, we'll be implementing a transformer network from scratch, and
    we'll be training it for translation tasks from English to German. In the *How
    it works...* section, we'll go through a lot of the details.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将从头开始实现一个Transformer网络，并将其用于从英语到德语的翻译任务。在*它是如何工作的...*部分，我们将详细介绍很多细节。
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We recommend using a machine with a **GPU**. The Colab environment is highly
    recommended, however, please make sure you are using a runtime with GPU enabled.
    If you want to check that you have access to a GPU, you can call the NVIDIA System
    Management Interface:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议使用带有**GPU**的机器。强烈推荐使用Colab环境，但请确保您正在使用启用了GPU的运行时。如果您想检查是否可以访问GPU，可以调用NVIDIA系统管理接口：
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You should see something like this:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似这样的东西：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This tells you you are using an NVIDIA Tesla T4 with 0 MB of about 1.5 GB used
    (1 MiB corresponds to approximately 1.049 MB).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉您正在使用NVIDIA Tesla T4，已使用1.5GB的0MB（1MiB大约相当于1.049MB）。
- en: 'We''ll need a relatively new version of `torchtext`, a library with text datasets
    and utilities for `pytorch`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个相对较新版本的`torchtext`，这是一个用于`pytorch`的文本数据集和实用工具库。
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'For the part in the *There''s more...* section, you might need to install an
    additional dependency:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*还有更多...*部分，您可能需要安装额外的依赖项：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We are using spaCy for tokenization. This comes preinstalled in Colab. In other
    environments, you might have to `pip-install` it. We do need to install the German
    core functionality, such as tokenization for `spacy`, which we''ll rely on in
    this recipe:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用spaCy进行标记化。这在Colab中预先安装。在其他环境中，您可能需要`pip-install`它。我们确实需要安装德语核心功能，例如`spacy`的标记化，在这个食谱中我们将依赖它：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We'll load up this functionality in the main part of the recipe.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在食谱的主要部分加载此功能。
- en: How to do it...
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this recipe, we'll be implementing a transformer model from scratch, and
    we'll be training it for a translation task. We've adapted this notebook from Ben
    Trevett's excellent tutorials on implementing a transformer sequence-to-sequence
    model with PyTorch and TorchText: [https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将从头开始实现一个Transformer模型，并且将其用于翻译任务的训练。我们从Ben Trevett关于使用PyTorch和TorchText实现Transformer序列到序列模型的优秀教程中适应了这个笔记本：[https://github.com/bentrevett/pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)。
- en: 'We''ll first prepare the dataset, then implement the transformer architecture,
    then we''ll train, and finally test:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先准备数据集，然后实现Transformer架构，接着进行训练，最后进行测试：
- en: 'Preparing the dataset – let''s import all the required modules upfront:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据集 - 让我们预先导入所有必需的模块：
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The dataset we'll be training on is the Multi30k dataset. This is a dataset
    of about 30,000 parallel English, German, and French short sentences.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要训练的数据集是Multi30k数据集。这是一个包含约30,000个平行英语、德语和法语短句子的数据集。
- en: 'We''ll load the `spacy` functionality and we''ll implement functions to tokenize
    German and English text, respectively:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载`spacy`功能，实现函数来标记化德语和英语文本：
- en: '[PRE43]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: These functions tokenize German and English text from a string into a list of
    strings.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数将德语和英语文本从字符串标记化为字符串列表。
- en: '`Field` defines operations for converting text to tensors. It provides interfaces
    to common text processing tools and holds a `Vocab` that maps tokens or words
    to a numerical representation. We are passing our preceding tokenization methods:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`Field`定义了将文本转换为张量的操作。它提供了常见文本处理工具的接口，并包含一个`Vocab`，将标记或单词映射到数值表示。我们正在传递我们的前面的标记化方法：'
- en: '[PRE44]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We''ll create a train-test-validation split from the dataset. The `exts` parameter
    specifies which languages to use as the source and target, and `fields` specifies
    which fields to feed. After that, we build the vocabulary from the training dataset:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从数据集中创建一个训练-测试-验证拆分。`exts`参数指定要用作源和目标的语言，`fields`指定要提供的字段。之后，我们从训练数据集构建词汇表：
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then we can define our data iterator over the train, validation, and test datasets:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以定义我们的数据迭代器，覆盖训练、验证和测试数据集：
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We can build our transformer architecture now before we train it with this dataset.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在训练此数据集之前构建我们的变压器架构。
- en: 'When implementing the transformer architecture, important parts are the multi-head
    attention and the feedforward connections. Let''s define them, first starting
    with the attention:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实施变压器架构时，重要部分是多头注意力和前馈连接。让我们先定义它们，首先从注意力开始：
- en: '[PRE47]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The feedforward layer is just a single forward pass with a non-linear activation,
    and a dropout, and a linear read-out. The first projection is much larger than
    the original hidden dimension. In our case, we use a hidden dimension of 512 and
    a `pf` dimension of 2048:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈层只是一个带有非线性激活、dropout和线性读出的单向传递。第一个投影比原始隐藏维度大得多。在我们的情况下，我们使用隐藏维度为512和`pf`维度为2048：
- en: '[PRE48]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We'll need `Encoder` and `Decoder` parts, each with their own layers. Then we'll
    connect these two with the `Seq2Seq` model.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要`Encoder`和`Decoder`部分，每个部分都有自己的层。然后我们将这两者连接成`Seq2Seq`模型。
- en: 'This is how the encoder looks:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这是编码器的外观：
- en: '[PRE49]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'It consists of a number of encoder layers. These look as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 它由多个编码器层组成。它们看起来如下所示：
- en: '[PRE50]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The decoder is not too different from the encoder, however, it comes with two
    multi-head attention layers. The decoder looks like the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与编码器并没有太大的不同，但是它附带了两个多头注意力层。解码器看起来像这样：
- en: '[PRE51]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In sequence, the decoder layer does the following tasks:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列中，解码器层执行以下任务：
- en: Self-attention with masking
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带掩码的自注意力
- en: Feedforward
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈
- en: Dropout
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退出率
- en: Residual connection
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Normalization
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化
- en: The mask in the self-attention layer is to avoid the model including the next
    token in its prediction (which would be cheating).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层中的掩码是为了避免模型在预测中包含下一个标记（这将是作弊）。
- en: 'Let''s implement the decoder layer:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现解码器层：
- en: '[PRE52]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, it all comes together in the `Seq2Seq` model:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`Seq2Seq`模型中一切都汇聚在一起：
- en: '[PRE53]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can now instantiate our model with actual parameters:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用实际参数实例化我们的模型：
- en: '[PRE54]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: This whole model comes with 9,543,087 trainable parameters.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 整个模型共有9,543,087个可训练参数。
- en: 'Training the translation model, we can initialize the parameters using Xavier
    uniform normalization:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练翻译模型时，我们可以使用Xavier均匀归一化来初始化参数：
- en: '[PRE55]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We need to set the learning rate much lower than the default:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将学习率设置得比默认值低得多：
- en: '[PRE56]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In our loss function, `CrossEntropyLoss`, we have to make sure to ignore padded
    tokens:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的损失函数`CrossEntropyLoss`中，我们必须确保忽略填充的标记：
- en: '[PRE57]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Our training function looks like this:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练函数如下所示：
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The training is then performed in a loop:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在循环中执行训练：
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We are slightly simplifying things here. You can find the full notebook on GitHub.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里略微简化了事情。您可以在GitHub上找到完整的笔记本。
- en: This trains for 10 epochs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型训练了10个时期。
- en: Testing the model, we'll first have to write functions to encode a sentence
    for the model and decode the model output back to get a sentence. Then we can
    run some sentences and have a look at the translations. Finally, we can calculate
    a metric of the translation performance across the test set.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型时，我们首先必须编写函数来为模型编码一个句子，并将模型输出解码回句子。然后我们可以运行一些句子并查看翻译。最后，我们可以计算测试集上的翻译性能指标。
- en: 'In order to translate a sentence, we have to encode it numerically using the
    source vocabulary created before and append stop tokens before feeding this into
    our model. The model output then has to be decoded from the target vocabulary:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了翻译一个句子，我们必须使用之前创建的源词汇表将其数值化编码，并在将其馈送到我们的模型之前附加停止标记。然后，必须从目标词汇表中解码模型输出：
- en: '[PRE60]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can look at an example pair and check the translation:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看一个示例对并检查翻译：
- en: '[PRE61]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We get the following pair:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下对：
- en: '[PRE62]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can compare this with the translation we get from our model:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其与我们模型获得的翻译进行比较：
- en: '[PRE63]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This is our translated sentence:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的翻译句子：
- en: '[PRE64]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Our translation looks actually better than the original translation. A purse
    is not really a wallet (`geldbörse`), but a small bag (`handtasche`).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的翻译实际上比原始翻译好看。钱包（`geldbörse`）不是真正的钱包，而是一个小包（`handtasche`）。
- en: 'We can then calculate a metric, the BLEU score, of our model versus the gold
    standard:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算我们模型与黄金标准的BLEU分数的指标：
- en: '[PRE65]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We get a BLEU score of 33.57, which is not bad while training fewer parameters
    and the training finishes in a matter of a few minutes.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了33.57的BLEU分数，这个分数还不错，同时训练参数更少，训练时间只需几分钟。
- en: In translation, a useful metric is the **Bilingual Evaluation Understudy** (**BLEU**) score,
    where 1 is the best possible value. It is the ratio of parts in the candidate
    translation over parts in a reference translation (gold standard), where parts
    can be single words or a sequence of words (**n-grams**).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在翻译中，一个有用的度量标准是**双语评估助手**（**BLEU**）分数，其中1是最佳值。它是候选翻译部分与参考翻译（黄金标准）部分的比率，其中部分可以是单个词或一个词序列（**n-grams**）。
- en: This wraps up our translation model. We can see it's actually not that hard
    to create a translation model. However, there's quite a lot of theory, part of
    which we'll cover in the next section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的翻译模型了。我们可以看到实际上创建一个翻译模型并不是那么困难。然而，其中有很多理论知识，我们将在下一节中进行介绍。
- en: How it works...
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: In this recipe, we trained a transformer model from scratch for an English to
    German translation task. Let's look a bit into what a transformer is and how it
    works.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们为英语到德语的翻译任务从头开始训练了一个变压器模型。让我们稍微了解一下变压器是什么，以及它是如何工作的。
- en: Until not long ago, **Long Short-Term Memory networks** (**LSTMs**) had been
    the prevalent choice for deep learning models, however, since words are processed
    sequentially, training can take a long time to converge. We have seen in previous
    recipes how recurrent neural networks can be used for sequence processing (please
    compare it with the *Generating melodies *recipe in [Chapter 9](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml),
    *Deep Learning in Audio and Speech*). In yet other recipes, for example, the *Recognizing
    voice commands* recipe in [Chapter 9](270a18b0-4bf4-4bb3-8c39-a9bab3fe38e1.xhtml),
    *Deep Learning in Audio and Speech*, we discussed how convolutional models have
    been replacing these recurrent networks with an advantage in speed and prediction
    performance. In NLP, convolutional networks have been tried as well (for example, Jonas
    Gehring and others, *Convolutional Sequence to Sequence Learning*, 2017) with
    improvements in speed and prediction performance with regard to recurrent models,
    however, the transformer architecture proved more powerful and still faster.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之前，**长短期记忆网络**（**LSTMs**）一直是深度学习模型的主要选择，然而，由于单词是按顺序处理的，训练可能需要很长时间才能收敛。在前面的示例中，我们已经看到递归神经网络如何用于序列处理（请与*第9章*中的*生成旋律*食谱进行比较，*音频和语音中的深度学习*）。在其他示例中，例如在*第9章*中的*识别语音命令*食谱中，我们讨论了卷积模型如何替代这些递归网络，以提高速度和预测性能。在自然语言处理中，卷积网络也已经尝试过（例如，Jonas
    Gehring 等人的*卷积序列到序列学习*，2017年），相较于递归模型，速度和预测性能有所改善，然而，变压器架构证明更加强大且更快。
- en: The transformer architecture was originally created for machine translation
    (Ashish Vaswani and others, *Attention is All you Need*, 2017). Dispensing with
    recurrence and convolutions, transformer networks are much faster to train and
    predict since words are processed in parallel. Transformer architectures provide
    universal language models that have pushed the envelope in a broad set of tasks
    such as **Neural Machine Translation** (**NMT**), **Question Answering** (**QA**),
    **Named-Entity Recognition** (**NER**), **Textual Entailment** (**TE**), abstractive
    text summarization, and other tasks. Transformer models are often taken off the
    shelf and fine-tuned for specific tasks in order to profit from general language
    understanding acquired through a long and expensive training process.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构最初是为机器翻译而创建的（Ashish Vaswani 等人的*注意力机制就是你所需要的*，2017年）。变压器网络摒弃了递归和卷积，训练和预测速度大大加快，因为单词可以并行处理。变压器架构提供了通用的语言模型，在许多任务中推动了技术发展，如**神经机器翻译**（**NMT**），**问答系统**（**QA**），**命名实体识别**（**NER**），**文本蕴涵**（**TE**），抽象文本摘要等。变压器模型通常被拿来即插即用，并针对特定任务进行微调，以便从长期和昂贵的训练过程中获得的通用语言理解中受益。
- en: 'Transformers come in two parts, similar to an autoencoder:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 由两部分组成，类似于自动编码器：
- en: '**An encoder** – it encodes the input into a series of context vectors (aka
    hidden states).'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个编码器** – 它将输入编码为一系列上下文向量（也称为隐藏状态）。'
- en: '**A decoder** – it takes the context vector and decodes it into a target representation.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个解码器** – 它接收上下文向量并将其解码为目标表示。'
- en: 'The differences between the implementation in our recipe and the original transformer
    implementation (Ashish Vaswani and others, *Attention is All you Need*, 2017) is
    the following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例中实现与原始变压器实现（Ashish Vaswani 等人的*注意力机制就是你所需要的*，2017年）之间的差异如下：
- en: We use a learned positional encoding instead of a static one.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了学习的位置编码而不是静态的编码。
- en: We use a fixed learning rate (no warm-up and cool-down steps).
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用固定的学习率（没有预热和冷却步骤）。
- en: We don't use label smoothing.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不使用标签平滑处理。
- en: These changes are in sync with modern transformers such as BERT.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化与现代转换器（如BERT）保持同步。
- en: First, the input is passed through an embedding layer and a positional embedding
    layer in order to encode the positions of tokens in the sequence. Token embeddings
    are scaled by ![](img/35f98b55-821b-45c8-a75a-22519eddd6e9.png), the square root
    of the size of the hidden layers, and added to positional embeddings. Finally,
    dropout is applied for regularization.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，输入通过嵌入层和位置嵌入层传递，以编码序列中令牌的位置。令牌嵌入通过缩放为![](img/35f98b55-821b-45c8-a75a-22519eddd6e9.png)（隐藏层大小的平方根），并添加到位置嵌入中。最后，应用dropout进行正则化。
- en: The encoder then passes through stacked modules, each consisting of attention,
    feedforward fully connected layers, and normalization. Attention layers are linear
    combinations of scaled multiplicative (dot product) attention layers (**Multi-Head
    Attention**).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然后编码器通过堆叠模块传递，每个模块包括注意力、前馈全连接层和归一化。注意力层是缩放的乘法（点积）注意力层的线性组合（**多头注意力**）。
- en: 'Some transformer architectures only contain one of the two parts. For example,
    the OpenAI GPT transformer architecture (Alec Radfor and others, *Improving Language
    Understanding by Generative Pre-Training*, 2018), which generates amazingly coherent
    texts and consists of stacked decoders, while Google''s BERT architecture (Jacob
    Devlin and others, *BERT: Pre-training of Deep Bidirectional Transformers for
    Language Understanding*, 2019) also consists of stacked encoders.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '一些转换器架构只包含其中的一部分。例如，OpenAI GPT转换器架构（Alec Radfor等人，《通过生成预训练改进语言理解》，2018年），生成了非常连贯的文本，由堆叠的解码器组成，而Google的BERT架构（Jacob
    Devlin等人，《BERT: 深度双向转换器的预训练用于语言理解》，2019年），也由堆叠的编码器组成。'
- en: There's more...
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Both Torch and TensorFlow have a repository for pretrained models. We can download
    a translation model from the Torch hub and use it straight away. This is what
    we''ll quickly show. For the `pytorch` model, we need to have a few dependencies
    installed first:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Torch和TensorFlow都有预训练模型的存储库。我们可以从Torch Hub下载一个翻译模型并立即使用它。这就是我们将快速展示的内容。对于`pytorch`模型，我们首先需要安装一些依赖项：
- en: '[PRE66]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'After this, we can download the model. It is quite big, which means it''ll
    take up a lot of disk space:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 完成之后，我们可以下载模型。它非常大，这意味着它会占用大量磁盘空间：
- en: '[PRE67]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We should get an output like this:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到这样的输出：
- en: '[PRE68]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This model (Nathan Ng and others, *Facebook FAIR's WMT19 News Translation Task
    Submission*, 2019) is state-of-the-art for translation. It even outperforms human
    translations in precision (BLEU score). `fairseq` comes with tutorials for training
    translation models on your own datasets.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型（Nathan Ng等人，《Facebook FAIR的WMT19新闻翻译任务提交》，2019年）在翻译方面处于技术领先地位。它甚至在精度（BLEU分数）上超越了人类翻译。`fairseq`附带了用于在您自己的数据集上训练翻译模型的教程。
- en: The Torch hub provides a lot of different translation models, but also generic
    language models.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Torch Hub提供了许多不同的翻译模型，还有通用语言模型。
- en: See also
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: You can find a guide about the transformer architecture complete with PyTorch
    code (and an explanation on positional embeddings) on the Harvard NLP group website,
    which can also run on Google Colab: [http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在哈佛大学自然语言处理组的网站上找到关于转换器架构的指南，包括PyTorch代码（以及关于位置编码的解释），它还可以在Google Colab上运行：[http://nlp.seas.harvard.edu/2018/04/03/attention.html](http://nlp.seas.harvard.edu/2018/04/03/attention.html)。
- en: 'Lilian Weng of OpenAI has written about language modeling and transformer models,
    and provides a concise and clear overview:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的Lilian Weng已经写过关于语言建模和转换器模型的文章，并提供了简明清晰的概述：
- en: 'Generalized Language Models – about the history of language and **Neural Machine
    Translation Models** (**NMTs**): [https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化语言模型 - 关于语言历史和**神经机器翻译模型**（**NMTs**）的文章：[https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)
- en: 'The Transformer Family – about the history of transformer models: [https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于变压器模型家族的历史：[https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html)。
- en: 'As for libraries supporting translation tasks, both `pytorch` and `tensorflow`
    provide pre-trained models, and support architectures useful in translation:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 至于支持翻译任务的库，`pytorch` 和 `tensorflow` 都提供预训练模型，并支持在翻译中有用的架构：
- en: '`fairseq` is a library for sequence-to-sequence models in PyTorch: [https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fairseq` 是 PyTorch 中用于序列到序列模型的库：[https://github.com/pytorch/fairseq](https://github.com/pytorch/fairseq)。'
- en: You can find tutorials on TensorFlow by Google Research on GitHub: [https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt).
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上找到由 Google Research 提供的 TensorFlow 教程：[https://github.com/tensorflow/nmt](https://github.com/tensorflow/nmt)。
- en: Finally, OpenNMT is a framework based on PyTorch and TensorFlow for translation
    tasks with many tutorials and pre-trained models: [https://opennmt.net/](https://opennmt.net/).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，OpenNMT 是一个基于 PyTorch 和 TensorFlow 的框架，用于翻译任务，拥有许多教程和预训练模型：[https://opennmt.net/](https://opennmt.net/)。
- en: Writing a popular novel
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 撰写一本流行小说
- en: We've mentioned the Turing test before as a way of finding out whether a computer
    is intelligent enough to trick an interrogator into thinking it is human. Some
    text generation tools generate essays that could possibly make sense, however,
    contain no intellectual merit behind the appearance of scientific language.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到过图灵测试，用于判断计算机是否足够智能以欺骗审问者认为它是人类。一些文本生成工具生成的文章可能在外表上看起来有意义，但在科学语言背后缺乏智力价值。
- en: 'The same could be said of some human essays and utterances, however. Nassim
    Taleb, in his book *Fooled by Randomness*, argued a person should be called unintelligent
    if their writing could not be distinguished from an artificially generated one
    (a **reverse Turing test**). In a similar vein, Alan Sokal''s 1996 hoax article
    *Transgressing the Boundaries: Towards a Transformative Hermeneutics of Quantum
    Gravity*, accepted by and published in a well-known social science journal, was
    a deliberate attempt by the university professor of physics to expose a lack of
    intellectual rigor and the misuse of scientific terminology without understanding.
    A possible conclusion could be that imitating humans might not be the way forward
    toward intellectual progress.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人类的文章和言论也可能如此。纳西姆·塔勒布在他的书《随机漫步的傻子》中认为，如果一个人的写作无法与人工生成的文章区分开来（一种**逆图灵测试**），则可以称其为不聪明。类似地，艾伦·索卡尔在1996年的恶作剧文章《超越边界：走向量子引力的转变诠释学》被一位物理学教授故意编造，以揭露缺乏思维严谨和对科学术语的误用。一个可能的结论是，模仿人类可能不是智力进步的正确方向。
- en: OpenAI GPT-3, with 175 billion parameters, has pushed the field of language
    models considerably, having learned facts in physics, being able to generate programming
    code based on descriptions, and being able to compose entertaining and funny prose.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT-3 拥有1750亿个参数，显著推动了语言模型领域的发展，学习了物理学的事实，能够基于描述生成编程代码，并能够撰写娱乐性和幽默性的散文。
- en: Millions of fans across the world have been waiting for more than 200 years
    to know how the story of *Pride and Prejudice* continues with Elizabeth and Mr
    Darcy. In this recipe, we'll be generating *Pride and Prejudice 2* using a transformer-based
    model.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 数百万全球粉丝已经等待200多年，想知道伊丽莎白和达西先生的《傲慢与偏见》故事如何继续。在这个配方中，我们将使用基于变压器的模型生成《傲慢与偏见2》。
- en: Getting ready
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Project Gutenberg is a digital library of (mostly) public domain e-books hosting
    more than 60,000 books in different languages and in formats such as plain text,
    HTML, PDF, EPUB, MOBI, and Plucker. Project Gutenberg also lists the most popular
    downloads: [http://www.gutenberg.org/browse/scores/top](http://www.gutenberg.org/browse/scores/top).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Project Gutenberg 是一个数字图书馆（大部分为公有领域电子书），拥有超过60,000本书籍，以纯文本、HTML、PDF、EPUB、MOBI
    和 Plucker 等格式提供。Project Gutenberg 还列出了最受欢迎的下载：[http://www.gutenberg.org/browse/scores/top](http://www.gutenberg.org/browse/scores/top)。
- en: 'At the time of writing, Jane Austen''s romantic early-19th century novel *Pride
    and Prejudice* had by far the most downloads over the last 30 days (more than
    47,000). We''ll download the book in plain text format:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，简·奥斯汀的浪漫19世纪初的小说*傲慢与偏见*在过去30天内下载量最高（超过47,000次）。我们将以纯文本格式下载这本书：
- en: '[PRE69]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We save the text file as `pride_and_prejudice.txt`.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文本文件保存为`pride_and_prejudice.txt`。
- en: We'll be working in Colab, where you'll have access to Nvidia T4 or Nvidia K80
    GPUs. However, you can use your own computer as well, using either GPUs or even
    CPUs.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Colab中工作，您将可以访问Nvidia T4或Nvidia K80 GPU。但是，您也可以使用自己的计算机，使用GPU甚至CPU。
- en: If you are working in Colab, you'll need to upload your text file to your Google
    Drive ([https://drive.google.com](https://drive.google.com)), where you can access
    it from Colab.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Colab中工作，您需要将您的文本文件上传到您的Google Drive ([https://drive.google.com](https://drive.google.com))，这样您可以从Colab访问它。
- en: 'We''ll be using a wrapper library for OpenAI''s GPT-2 that''s called `gpt-2-simple`,
    which is created and maintained by Max Woolf, a data scientist at BuzzFeed:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个称为`gpt-2-simple`的OpenAI GPT-2的包装库，由BuzzFeed的数据科学家Max Woolf创建和维护：
- en: '[PRE70]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This library will make it easy to fine-tune the model to new texts and show
    us text samples along the way.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 此库将使模型对新文本进行微调并在训练过程中显示文本样本变得更加容易。
- en: 'We then have a choice of the size of the GPT-2 model. Four sizes of GPT-2 have
    been released by OpenAI as pretrained models:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以选择GPT-2模型的大小。OpenAI已经发布了四种大小的预训练模型：
- en: '**Small** (124 million parameters; occupies 500 MB)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小**（124百万参数；占用500 MB）'
- en: '**Medium** (355 million parameters; 1.5 GB)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中等**（355百万参数；1.5 GB）'
- en: '**Large** (774 million)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大**（774百万）'
- en: '**Extra large** (1,558 million)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超大**（1,558百万）'
- en: The large model cannot currently be fine-tuned in Colab, but can generate text
    from the pretrained model. The extra large model is too large to load into memory
    in Colab, and can therefore neither be fine-tuned nor generate text. While bigger
    models will achieve better performance and have more knowledge, they will take
    longer to train and to generate text.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型目前无法在Colab中进行微调，但可以从预训练模型生成文本。超大模型太大以至于无法加载到Colab的内存中，因此既不能进行微调也不能生成文本。尽管较大的模型会取得更好的性能并且具有更多知识，但它们需要更长时间来训练和生成文本。
- en: 'We''ll choose the small model:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择小模型：
- en: '[PRE71]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Let's get started!
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it...
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We''ve downloaded the text of a popular novel, *Pride and Prejudice*, and we''ll
    first fine-tune the model, then we''ll generate similar text to *Pride and Prejudice*:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经下载了一本流行小说*傲慢与偏见*的文本，并将首先对模型进行微调，然后生成类似*傲慢与偏见*的文本：
- en: 'Fine-tuning the model: We''ll load a pre-trained model and fine-tune it for
    our texts.'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调模型：我们将加载一个预训练模型，并对我们的文本进行微调。
- en: 'We''ll mount Google Drive. The `gpt-2-simple` library provides a utility function:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将挂载Google Drive。`gpt-2-simple`库提供了一个实用函数：
- en: '[PRE72]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'At this point, you''d need to authorize the Colab notebook to have access to
    your Google Drive. We''ll use the *Pride and Prejudice* text file that we uploaded
    to our Google Drive before:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您需要授权Colab笔记本访问您的Google Drive。我们将使用之前上传到Google Drive的*傲慢与偏见*文本文件：
- en: '[PRE73]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We can then start fine-tuning based on our downloaded text:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以基于我们下载的文本开始微调：
- en: '[PRE74]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We should see the training loss going down over the span of at least a couple
    of hours. We see samples of generated text during training such as this one:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到训练损失在至少几个小时内下降。我们在训练过程中会看到生成文本的样本，例如这个：
- en: '[PRE75]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The `gpt-2-simple` library is really making it easy to train and continue training.
    All model checkpoints can be stored on Google Drive, so they aren''t lost when
    the runtime times out. We might have to restart several times, so it''s good to
    always store backups on Google Drive:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpt-2-simple`库确实使得训练和继续训练变得非常容易。所有模型检查点都可以存储在Google Drive上，因此在运行时超时时它们不会丢失。我们可能需要多次重启，因此始终在Google
    Drive上备份是个好习惯：'
- en: '[PRE76]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'If we want to continue training after Colab has restarted, we can do this as
    well:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望在Colab重新启动后继续训练，我们也可以这样做：
- en: '[PRE77]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We can now generate our new novel.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成我们的新小说了。
- en: 'Writing our new bestseller: We might need to get the model from Google Drive
    and load it up into the GPU:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写我们的新畅销书：我们可能需要从Google Drive获取模型并将其加载到GPU中：
- en: '[PRE78]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Please note that you might have to restart your notebook (Colab) again so that
    the TensorFlow variables don't clash.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可能需要再次重启笔记本（Colab），以避免TensorFlow变量冲突。
- en: 'Now we can call a utility function in `gpt-2-simple` to generate the text into
    a file. Finally, we can download the file:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以调用`gpt-2-simple`中的一个实用函数将文本生成到文件中。最后，我们可以下载该文件：
- en: '[PRE79]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The `gpt_2_simple.generate()` function takes an optional `prefix` parameter,
    which is the text that is to be continued.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '`gpt_2_simple.generate()` 函数接受一个可选的`prefix`参数，这是要继续的文本。'
- en: '*Pride and Prejudice* – the saga continues; reading the text, there are sometimes
    some obvious flaws in the continuity, however, some passages are captivating to
    read. We can always generate a few samples so that we have a choice of how our
    novel continues.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '*傲慢与偏见*——传奇继续；阅读文本时，有时可以看到一些明显的连续性缺陷，然而，有些段落令人着迷。我们总是可以生成几个样本，这样我们就可以选择我们小说的继续方式。'
- en: How it works...
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we've used the GPT-2 model to generate text. This is called **neural
    story generation** and is a subset of **neural text generation**. Simply put,
    neural text generation is the process of building a statistical model of a text
    or of a language and applying this model to generate more text.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了 GPT-2 模型来生成文本。这被称为**神经故事生成**，是**神经文本生成**的一个子集。简而言之，神经文本生成是构建文本或语言的统计模型，并应用该模型生成更多文本的过程。
- en: 'XLNet, OpenAI''s GPT and GPT-2, Google''s Reformer, OpenAI''s Sparse Transformers,
    and other transformer-based models have one thing in common: they are generative
    because of a modeling choice – they are autoregressive rather than auto-encoding.
    This autoregressive language generation is based on the assumption that the probability
    of a token can be predicted given a context sequence of length ![](img/3660a617-b11e-4ed3-8a5c-b76d66950499.png)
    like this:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet、OpenAI的GPT和GPT-2、Google的Reformer、OpenAI的Sparse Transformers以及其他基于变换器的模型有一个共同点：它们是由于建模选择而具有生成性——它们是自回归而不是自编码的。这种自回归语言生成基于这样的假设：在给定长度为![](img/3660a617-b11e-4ed3-8a5c-b76d66950499.png)的上下文序列时，可以预测令牌的概率：
- en: '![](img/0bfcdb76-4980-4b66-8a88-c9c0788b5dea.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bfcdb76-4980-4b66-8a88-c9c0788b5dea.png)'
- en: This can be approximated via minimizing the cross-entropy of the predicted token
    versus the actual token. LSTMs, **G****enerative Adversarial Networks** (**GANs**),
    or autoregressive transformer architectures have been used for this.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过最小化预测令牌与实际令牌的交叉熵来近似这一过程。例如，LSTM、**生成对抗网络**（**GANs**）或自回归变换器架构已经用于此目的。
- en: 'One major choice we have to make in our text generation is how to sample, and
    we have a few choices:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本生成中，我们需要做出的一个主要选择是如何抽样，我们有几种选择：
- en: Greedy search
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪搜索
- en: Beam search
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 束搜索
- en: Top-k sampling
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Top-k 抽样
- en: Top-p (nucleus) sampling
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Top-p（核心）抽样
- en: 'In greedy search, we take the highest rated choice each time, ignoring other
    choices. In contrast, rather than taking a high-scoring token, beam search tracks
    the scores of several choices in parallel in order to take the highest-scored
    sequence. Top-k sampling was introduced by Angela Fan and others (*Hierarchical
    Neural Story Generation*, 2018). In top-k sampling, all but the *k* most likely
    words are discarded. Conversely, in top-p (also called: nucleus sampling), the
    highest-scoring tokens surpassing probability threshold *p* are chosen, while
    the others are discarded. Top-k and top-p can be combined in order to avoid low-ranking
    words.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在贪婪搜索中，每次选择评分最高的选择，忽略其他选择。相比之下，束搜索（beam search）并行跟踪几个选择的分数，以选择最高分序列，而不是选择高分令牌。Top-k
    抽样由Angela Fan等人提出（*Hierarchical Neural Story Generation*, 2018）。在 top-k 抽样中，除了最可能的*k*个词语外，其他词语都被丢弃。相反，在
    top-p（也称为核心抽样）中，选择高分词汇超过概率阈值*p*，而其他词语则被丢弃。可以结合使用 top-k 和 top-p 以避免低排名词汇。
- en: While the `huggingface transformers` library gives us all of these choices, with `gpt-2-simple`,
    we have the choice of top-k sampling and top-p sampling.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`huggingface transformers`库为我们提供了所有这些选择，但是使用`gpt-2-simple`时，我们可以选择使用 top-k
    抽样和 top-p 抽样。
- en: See also
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: There are many fantastic libraries that make training a model or applying an
    off-the-shelf model much easier. First of all, perhaps `Hugging Face transformers`,
    which is a library for language understanding and language generation supporting
    architectures and pretrained models for BERT, GPT-2, RoBERTa, XLM, DistilBert,
    XLNet, T5, and CTRL, among others: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多出色的库可以使模型训练或应用现成模型变得更加容易。首先，也许是`Hugging Face transformers`，这是一个语言理解和生成库，支持BERT、GPT-2、RoBERTa、XLM、DistilBert、XLNet、T5和CTRL等架构和预训练模型：[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)。
- en: The `Hugging Face transformers` library comes with a few pre-trained transformer
    models, including a distilled GPT-2 model, which provides performance at the level
    of GPT-2, but with about 30% fewer parameters, bringing advantages of higher speed
    and lower resource demands in terms of memory and processing power. You can find
    a few notebooks linked from the Hugging Face GitHub repository that describe text
    generation and the fine-tuning of transformer models: [https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks](https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hugging Face transformers` 库提供了一些预训练的变换器模型，包括精简版的GPT-2模型，该模型在性能上接近GPT-2，但参数数量减少了约30%，带来更高的速度和更低的内存及处理需求。你可以从Hugging
    Face的GitHub仓库中找到几篇链接的笔记本，描述了文本生成和变换器模型的微调：[https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks](https://github.com/huggingface/transformers/tree/master/notebooks#community-notebooks).'
- en: Additionally, Hugging Face provides a website called *Write with Transformers* that
    – according to their slogan – can *autocomplete your thoughts*: [https://transformer.huggingface.co/](https://transformer.huggingface.co/).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Hugging Face还提供了一个名为*Write with Transformers*的网站，根据他们的口号，该网站可以*自动补全你的思路*：[https://transformer.huggingface.co/](https://transformer.huggingface.co/).
- en: You can find a tutorial on text generation with **recurrent neural networks **in
    the TensorFlow documentation: [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow文档中，你可以找到关于**循环神经网络**文本生成的教程：[https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation).
- en: Such models are also prepackaged in libraries such as textgenrnn: [https://github.com/minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型还预装在诸如textgenrnn这样的库中：[https://github.com/minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn).
- en: More complex, transformer-based models are also available from TensorFlow Hub,
    as demonstrated in another tutorial: [https://www.tensorflow.org/hub/tutorials/wiki40b_lm](https://www.tensorflow.org/hub/tutorials/wiki40b_lm).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的基于变换器的模型也可以从TensorFlow Hub获取，正如另一个教程所示：[https://www.tensorflow.org/hub/tutorials/wiki40b_lm](https://www.tensorflow.org/hub/tutorials/wiki40b_lm).
