- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Generative Adversarial Networks for Synthesizing New Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络用于合成新数据
- en: In the previous chapter, we focused on **recurrent neural networks** for modeling
    sequences. In this chapter, we will explore **generative adversarial networks**
    (**GANs**) and see their application in synthesizing new data samples. GANs are
    considered to be one of the most important breakthroughs in deep learning, allowing
    computers to generate new data (such as new images).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们专注于用于建模序列的**循环神经网络**。在本章中，我们将探讨**生成对抗网络**（**GANs**）及其在合成新数据样本中的应用。GAN被认为是深度学习中最重要的突破之一，允许计算机生成新数据（如新图像）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing generative models for synthesizing new data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入生成模型用于合成新数据
- en: Autoencoders, variational autoencoders, and their relationship to GANs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器、变分自编码器及其与生成对抗网络（GANs）的关系
- en: Understanding the building blocks of GANs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解GAN的构建模块
- en: Implementing a simple GAN model to generate handwritten digits
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个简单的GAN模型来生成手写数字
- en: Understanding transposed convolution and batchnormalization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解转置卷积和批归一化
- en: 'Improving GANs: deep convolutional GANs and GANs using the Wasserstein distance'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进GAN：深度卷积GAN和使用Wasserstein距离的GAN
- en: Introducing generative adversarial networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍生成对抗网络
- en: Let’s first look at the foundations of GAN models. The overall objective of
    a GAN is to synthesize new data that has the same distribution as its training
    dataset. Therefore, GANs, in their original form, are considered to be in the
    unsupervised learning category of machine learning tasks, since no labeled data
    is required. It is worth noting, however, that extensions made to the original
    GAN can lie in both the semi-supervised and supervised domains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看GAN模型的基础。GAN的总体目标是合成具有与其训练数据集相同分布的新数据。因此，GAN在其原始形式中被认为是机器学习任务中无监督学习类别的一部分，因为不需要标记数据。然而，值得注意的是，对原始GAN的扩展可以同时属于半监督和监督领域。
- en: The general GAN concept was first proposed in 2014 by Ian Goodfellow and his
    colleagues as a method for synthesizing new images using deep **neural networks**
    (**NNs**) (*Generative Adversarial Nets*, in *Advances in Neural Information Processing
    Systems* by *I. Goodfellow*, *J. Pouget-Abadie*, *M. Mirza*, *B. Xu*, *D. Warde-Farley*,
    *S. Ozair*, *A. Courville*, and *Y. Bengio*, pp. 2672-2680, 2014). While the initial
    GAN architecture proposed in this paper was based on fully connected layers, similar
    to multilayer perceptron architectures, and trained to generate low-resolution
    MNIST-like handwritten digits, it served more as a proof of concept to demonstrate
    the feasibility of this new approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）的一般概念最早由**伊恩·古德费洛**及其同事于2014年提出，作为利用深度**神经网络**（**NNs**）合成新图像的方法（*生成对抗网络*，见*I.
    Goodfellow*, *J. Pouget-Abadie*, *M. Mirza*, *B. Xu*, *D. Warde-Farley*, *S. Ozair*,
    *A. Courville*, and *Y. Bengio*，《神经信息处理系统进展》，第2672-2680页，2014年）。尽管该论文中最初的GAN架构基于全连接层，类似于多层感知器结构，并训练生成低分辨率的类似MNIST手写数字，但它更像是一个概念验证，旨在展示这种新方法的可行性。
- en: However, since its introduction, the original authors, as well as many other
    researchers, have proposed numerous improvements and various applications in different
    fields of engineering and science; for example, in computer vision, GANs are used
    for image-to-image translation (learning how to map an input image to an output
    image), image super-resolution (making a high-resolution image from a low-resolution
    version), image inpainting (learning how to reconstruct the missing parts of an
    image), and many more applications. For instance, recent advances in GAN research
    have led to models that are able to generate new, high-resolution face images.
    Examples of such high-resolution images can be found on [https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/),
    which showcases synthetic face images generated by a GAN.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自其引入以来，原始作者及许多其他研究人员已提出了许多改进以及不同领域工程和科学中的各种应用。例如，在计算机视觉中，GAN被用于图像到图像的转换（学习如何将输入图像映射到输出图像）、图像超分辨率（从低分辨率版本生成高分辨率图像）、图像修补（学习如何重构图像中丢失的部分）等多种应用。例如，最近GAN研究的进展导致了能够生成新的高分辨率人脸图像的模型。此类高分辨率图像的例子可以在[https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/)找到，展示了由GAN生成的合成人脸图像。
- en: Starting with autoencoders
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从自编码器开始
- en: Before we discuss how GANs work, we will first start with autoencoders, which
    can compress and decompress training data. While standard autoencoders cannot
    generate new data, understanding their function will help you to navigate GANs
    in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论 GAN 的工作原理之前，我们首先从自编码器开始，它可以压缩和解压训练数据。虽然标准自编码器不能生成新数据，但理解它们的功能将有助于你在下一节中理解
    GAN。
- en: 'Autoencoders are composed of two networks concatenated together: an **encoder**
    network and a **decoder** network. The encoder network receives a *d*-dimensional
    input feature vector associated with example **x** (that is, ![](img/B17582_17_001.png))
    and encodes it into a *p*-dimensional vector, **z** (that is, ![](img/B17582_17_002.png)).
    In other words, the role of the encoder is to learn how to model the function
    **z** = *f*(**x**). The encoded vector, **z**, is also called the **latent vector**,
    or the latent feature representation. Typically, the dimensionality of the latent
    vector is less than that of the input examples; in other words, *p* < *d*. Hence,
    we can say that the encoder acts as a data compression function. Then, the decoder
    decompresses ![](img/B17582_17_003.png) from the lower-dimensional latent vector,
    **z**, where we can think of the decoder as a function, ![](img/B17582_17_004.png).
    A simple autoencoder architecture is shown in *Figure 17.1*, where the encoder
    and decoder parts consist of only one fully connected layer each:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器由两个串联的网络组成：一个**编码器**网络和一个**解码器**网络。编码器网络接收与示例**x**相关的*d*维输入特征向量（即 ![](img/B17582_17_001.png)），并将其编码成*p*维向量**z**（即
    ![](img/B17582_17_002.png)）。换句话说，编码器的作用是学习如何建模函数**z** = **f**(**x**)。编码后的向量**z**也称为**潜在向量**或潜在特征表示。通常，潜在向量的维度小于输入示例的维度；换句话说，*p*
    < *d*。因此，我们可以说编码器充当数据压缩函数。然后，解码器从低维潜在向量**z**中解压出 ![](img/B17582_17_003.png)，我们可以将解码器视为一个函数，![](img/B17582_17_004.png)。*图17.1*展示了一个简单的自编码器架构，其中编码器和解码器部分只包含一个完全连接的层：
- en: '![](img/B17582_17_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_01.png)'
- en: 'Figure 17.1: The architecture of an autoencoder'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：自编码器的架构
- en: '**The connection between autoencoders and dimensionality reduction**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器与降维的联系**'
- en: In *Chapter 5*, *Compressing Data via Dimensionality Reduction*, you learned
    about dimensionality reduction techniques, such as **principal component analysis**
    (**PCA**) and **linear discriminant analysis** (**LDA**). Autoencoders can be
    used as a dimensionality reduction technique as well. In fact, when there is no
    nonlinearity in either of the two subnetworks (encoder and decoder), then the
    autoencoder approach is *almost identical* to PCA.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第5章*，*通过降维压缩数据*，你学习了一些降维技术，比如**主成分分析**（**PCA**）和**线性判别分析**（**LDA**）。自编码器也可以作为一种降维技术。事实上，当两个子网络（编码器和解码器）中没有非线性时，自编码器方法与
    PCA *几乎完全相同*。
- en: 'In this case, if we assume the weights of a single-layer encoder (no hidden
    layer and no nonlinear activation function) are denoted by the matrix *U*, then
    the encoder models **z** = **U**^T**x**. Similarly, a single-layer linear decoder
    models ![](img/B17582_17_005.png). Putting these two components together, we have
    ![](img/B17582_17_006.png). This is exactly what PCA does, with the exception
    that PCA has an additional orthonormal constraint: **UU**^T = **I**[n][×][n].'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们假设单层编码器的权重（无隐藏层和非线性激活函数）用矩阵*U*表示，则编码器模型为**z** = **U**^T**x**。类似地，单层线性解码器模型为
    ![](img/B17582_17_005.png)。将这两个组件放在一起，我们有 ![](img/B17582_17_006.png)。这正是 PCA 所做的事情，唯一的区别在于
    PCA 有一个额外的正交规范约束：**UU**^T = **I**[n][×][n]。
- en: While *Figure 17.1* depicts an autoencoder without hidden layers within the
    encoder and decoder, we can, of course, add multiple hidden layers with nonlinearities
    (as in a multilayer NN) to construct a deep autoencoder that can learn more effective
    data compression and reconstruction functions. Also, note that the autoencoder
    mentioned in this section uses fully connected layers. When we work with images,
    however, we can replace the fully connected layers with convolutional layers,
    as you learned in *Chapter 14*, *Classifying Images with Deep Convolutional Neural
    Networks*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*图17.1*描绘了一个没有隐藏层的自编码器，在编码器和解码器中，我们当然可以添加多个带有非线性的隐藏层（如多层神经网络），以构建一个能够学习更有效数据压缩和重构函数的深度自编码器。此外，注意到本节提到的自编码器使用全连接层。然而，在处理图像时，我们可以用卷积层替换全连接层，正如你在*第14章*，*使用深度卷积神经网络分类图像*中学到的那样。
- en: '**Other types of autoencoders based on the size of latent space**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于潜在空间大小的其他类型的自编码器**'
- en: As previously mentioned, the dimensionality of an autoencoder’s latent space
    is typically lower than the dimensionality of the inputs (*p* < *d*), which makes
    autoencoders suitable for dimensionality reduction. For this reason, the latent
    vector is also often referred to as the “bottleneck,” and this particular configuration
    of an autoencoder is also called **undercomplete**. However, there is a different
    category of autoencoders, called **overcomplete**, where the dimensionality of
    the latent vector, *z*, is, in fact, greater than the dimensionality of the input
    examples (*p* > *d*).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，自编码器的潜在空间的维度通常比输入的维度低（*p* < *d*），这使得自编码器适用于降维。因此，潜在向量也经常被称为“瓶颈”，并且这种特定的自编码器配置也称为**欠完备**。然而，还有一种不同类别的自编码器，称为**过完备**，在这种情况下，潜在向量*z*的维度实际上大于输入示例的维度（*p* > *d*）。
- en: When training an overcomplete autoencoder, there is a trivial solution where
    the encoder and the decoder can simply learn to copy (memorize) the input features
    to their output layer. Obviously, this solution is not very useful. However, with
    some modifications to the training procedure, overcomplete autoencoders can be
    used for *noise reduction*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，当训练一个过完备的自编码器时，存在一个平凡的解决方案，即编码器和解码器可以简单地学习复制（记忆）输入特征到它们的输出层。显然，这种解决方案并不是很有用。然而，通过对训练过程进行一些修改，过完备的自编码器可以用于*噪声减少*。
- en: In this case, during training, random noise, ![](img/B17582_17_007.png), is
    added to the input examples and the network learns to reconstruct the clean example,
    *x*, from the noisy signal, ![](img/B17582_17_008.png). Then, at evaluation time,
    we provide the new examples that are naturally noisy (that is, noise is already
    present such that no additional artificial noise, ![](img/B17582_17_007.png),
    is added) in order to remove the existing noise from these examples. This particular
    autoencoder architecture and training method is referred to as a *denoising autoencoder*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练过程中，随机噪声![](img/B17582_17_007.png)被添加到输入示例中，网络学习从嘈杂的信号![](img/B17582_17_008.png)中重构出干净的例子*x*。然后，在评估时，我们提供自然嘈杂的新例子（即已经存在噪声，因此不需要额外的人工噪声![](img/B17582_17_007.png)）以便从这些例子中去除现有的噪声。这种特殊的自编码器架构和训练方法被称为*去噪自编码器*。
- en: 'If you are interested, you can learn more about it in the research article
    *Stacked denoising autoencoders: Learning useful representations in a deep network
    with a local denoising criterion* by *Pascal Vincent* and colleagues, 2010 ([http://www.jmlr.org/papers/v11/vincent10a.html](http://www.jmlr.org/papers/v11/vincent10a.html)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你感兴趣，你可以通过*Pascal Vincent*和他的同事在2010年发表的研究文章[*Stacked denoising autoencoders:
    Learning useful representations in a deep network with a local denoising criterion*](http://www.jmlr.org/papers/v11/vincent10a.html)了解更多。'
- en: Generative models for synthesizing new data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成模型用于合成新数据
- en: Autoencoders are deterministic models, which means that after an autoencoder
    is trained, given an input, **x**, it will be able to reconstruct the input from
    its compressed version in a lower-dimensional space. Therefore, it cannot generate
    new data beyond reconstructing its input through the transformation of the compressed
    representation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是确定性模型，这意味着在自编码器训练后，给定一个输入**x**，它将能够从其在较低维空间中的压缩版本重新构建输入。因此，它不能在超出重构其输入之外生成新的数据。
- en: 'A generative model, on the other hand, can generate a new example, ![](img/B17582_17_010.png),
    from a random vector, **z** (corresponding to the latent representation). A schematic
    representation of a generative model is shown in the following figure. The random
    vector, **z**, comes from a distribution with fully known characteristics, so
    we can easily sample from such a distribution. For example, each element of **z**
    may come from the uniform distribution in the range [–1, 1] (for which we write
    ![](img/B17582_17_011.png)) or from a standard normal distribution (in which case,
    we write ![](img/B17582_17_012.png)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，一个生成模型可以从一个随机向量**z**（对应于潜在表示）生成一个新的例子，![](img/B17582_17_010.png)。生成模型的示意图如下所示。随机向量**z**来自具有完全已知特性的分布，因此我们可以轻松地从这样的分布中进行抽样。例如，**z**的每个元素可以来自于范围为[–1,
    1]的均匀分布（我们写成![](img/B17582_17_011.png)），或者来自于标准正态分布（这种情况下我们写成![](img/B17582_17_012.png)）：
- en: '![](img/B17582_17_02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_02.png)'
- en: 'Figure 17.2: A generative model'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：一个生成模型
- en: As we have shifted our attention from autoencoders to generative models, you
    may have noticed that the decoder component of an autoencoder has some similarities
    with a generative model. In particular, they both receive a latent vector, **z**,
    as input and return an output in the same space as **x**. (For the autoencoder,
    ![](img/B17582_17_013.png) is the reconstruction of an input, **x**, and for the
    generative model, ![](img/B17582_17_014.png) is a synthesized sample.)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将注意力从自动编码器转向生成模型时，您可能已经注意到自动编码器的解码器部分与生成模型有些相似。特别是它们都接收潜在向量 **z** 作为输入，并返回与
    **x** 相同空间的输出。（对于自动编码器，![](img/B17582_17_013.png) 是输入 **x** 的重构，对于生成模型，![](img/B17582_17_014.png)
    是一个合成的样本。）
- en: However, the major difference between the two is that we do not know the distribution
    of **z** in the autoencoder, while in a generative model, the distribution of
    **z** is fully characterizable. It is possible to generalize an autoencoder into
    a generative model, though. One approach is the **variational autoencoder** (**VAE**).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，两者之间的主要区别在于我们不知道自动编码器中 **z** 的分布，而在生成模型中，**z** 的分布是完全可描述的。虽然可以将自动编码器泛化为生成模型。一种方法是
    **变分自动编码器**（**VAE**）。
- en: 'In a VAE receiving an input example, **x**, the encoder network is modified
    in such a way that it computes two moments of the distribution of the latent vector:
    the mean, ![](img/B17582_17_015.png), and variance, ![](img/B17582_17_016.png).
    During the training of a VAE, the network is forced to match these moments with
    those of a standard normal distribution (that is, zero mean and unit variance).
    Then, after the VAE model is trained, the encoder is discarded, and we can use
    the decoder network to generate new examples, ![](img/B17582_17_010.png), by feeding
    random **z** vectors from the “learned” Gaussian distribution.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收输入示例 **x** 的 VAE 中，编码器网络被修改，以计算潜在向量分布的两个时刻：均值 ![](img/B17582_17_015.png)
    和方差 ![](img/B17582_17_016.png)。在训练 VAE 期间，网络被强制使这些时刻与标准正态分布（即零均值和单位方差）的时刻匹配。然后，在训练
    VAE 模型后，编码器被丢弃，我们可以使用解码器网络通过从“学习到的”高斯分布中提供的随机 **z** 向量来生成新的示例，![](img/B17582_17_010.png)。
- en: Besides VAEs, there are other types of generative models, for example, *autoregressive
    models* and *normalizing flow models*. However, in this chapter, we are only going
    to focus on GAN models, which are among the most recent and most popular types
    of generative models in deep learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 VAE，还有其他类型的生成模型，例如 *自回归模型* 和 *正规化流模型*。然而，在本章中，我们将只关注 GAN 模型，它们是深度学习中最新和最流行的生成模型类型之一。
- en: '**What is a generative model?**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是生成模型？**'
- en: Note that generative models are traditionally defined as algorithms that model
    data input distributions, *p*(*x*), or the joint distributions of the input data
    and associated targets, *p*(*x*, *y*). By definition, these models are also capable
    of sampling from some feature, **x**[i], conditioned on another feature, **x**[j],
    which is known as **conditional inference**. In the context of deep learning,
    however, the term **generative model** is typically used to refer to models that
    generate realistic-looking data. This means that we can sample from input distributions,
    *p*(*x*), but we are not necessarily able to perform conditional inference.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成模型通常被定义为模拟数据输入分布 *p*(*x*) 或输入数据及相关目标的联合分布 *p*(*x*, *y*) 的算法。按照定义，这些模型也能够从某些特征
    **x**[i] 中进行采样，条件是另一特征 **x**[j]，这被称为 **条件推理**。然而，在深度学习的语境中，术语 **生成模型** 通常用来指代能够生成看起来真实的数据的模型。这意味着我们可以从输入分布
    *p*(*x*) 中采样，但不一定能进行条件推理。
- en: Generating new samples with GANs
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GANs 生成新样本
- en: To understand what GANs do in a nutshell, let’s first assume we have a network
    that receives a random vector, **z**, sampled from a known distribution, and generates
    an output image, **x**. We will call this network **generator** (*G*) and use
    the notation ![](img/B17582_17_018.png) to refer to the generated output. Assume
    our goal is to generate some images, for example, face images, images of buildings,
    images of animals, or even handwritten digits such as MNIST.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要简单理解 GANs 的作用，我们首先假设有一个网络，接收来自已知分布的随机向量 **z**，并生成输出图像 **x**。我们将这个网络称为 **生成器**（*G*），并使用符号
    ![](img/B17582_17_018.png) 表示生成的输出。假设我们的目标是生成一些图像，例如人脸图像、建筑物图像、动物图像，甚至是手写数字如 MNIST。
- en: As always, we will initialize this network with random weights. Therefore, the
    first output images, before these weights are adjusted, will look like white noise.
    Now, imagine there is a function that can assess the quality of images (let’s
    call it an *assessor function*).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如以往一样，我们将使用随机权重初始化这个网络。因此，在调整这些权重之前，第一批输出图像看起来像是白噪声。现在，想象一下有一个能够评估图像质量的函数（我们称之为*评估函数*）。
- en: If such a function exists, we can use the feedback from that function to tell
    our generator network how to adjust its weights to improve the quality of the
    generated images. This way, we can train the generator based on the feedback from
    that assessor function, such that the generator learns to improve its output toward
    producing realistic-looking images.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在这样的函数，我们可以利用该函数的反馈告诉生成器网络如何调整其权重以提高生成图像的质量。通过这种方式，我们可以根据评估函数的反馈训练生成器，使其学习改进其输出以生成看起来真实的图像。
- en: While an assessor function, as described in the previous paragraph, would make
    the image generation task very easy, the question is whether such a universal
    function to assess the quality of images exists and, if so, how it is defined.
    Obviously, as humans, we can easily assess the quality of output images when we
    observe the outputs of the network; although, we cannot (yet) backpropagate the
    result from our brain to the network. Now, if our brain can assess the quality
    of synthesized images, can we design an NN model to do the same thing? In fact,
    that’s the general idea of a GAN.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一段所描述的评估函数，如果存在这样一个通用函数来评估图像的质量，那么生成图像的任务将变得非常简单。问题是，是否存在这样一个可以评估图像质量的通用函数，如果存在，它是如何定义的。显然，作为人类，当我们观察网络的输出时，可以轻松评估输出图像的质量；尽管我们目前（还）无法将我们的大脑结果反向传播到网络中。现在，如果我们的大脑能够评估合成图像的质量，那么我们是否可以设计一个神经网络模型来做同样的事情？事实上，这正是GAN的一般想法。
- en: 'As shown in *Figure 17.3*, a GAN model consists of an additional NN called
    **discriminator** (*D*), which is a classifier that learns to detect a synthesized
    image, ![](img/B17582_17_010.png), from a real image, **x**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 17.3*所示，GAN模型包括一个名为**鉴别器**（*D*）的附加神经网络，它是一个分类器，学习如何检测由生成器合成的图像，![](img/B17582_17_010.png)，与真实图像**x**的区别：
- en: '![](img/B17582_17_03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_03.png)'
- en: 'Figure 17.3: The discriminator distinguishes between the real image and the
    one created by the generator'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3：鉴别器区分真实图像和生成器创建的图像
- en: In a GAN model, the two networks, generator and discriminator, are trained together.
    At first, after initializing the model weights, the generator creates images that
    do not look realistic. Similarly, the discriminator does a poor job of distinguishing
    between real images and images synthesized by the generator. But over time (that
    is, through training), both networks become better as they interact with each
    other. In fact, the two networks play an adversarial game, where the generator
    learns to improve its output to be able to fool the discriminator. At the same
    time, the discriminator becomes better at detecting the synthesized images.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN模型中，生成器和鉴别器两个网络一起训练。一开始，初始化模型权重后，生成器创建的图像看起来不太真实。同样，鉴别器在区分真实图像和生成器合成图像方面表现不佳。但随着时间的推移（即训练过程中），这两个网络通过相互作用逐渐提升。事实上，这两个网络在进行对抗训练，生成器学习改进其输出以欺骗鉴别器。与此同时，鉴别器变得更加擅长检测合成图像。
- en: Understanding the loss functions of the generator and discriminator networks
    in a GAN model
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GAN模型中生成器和鉴别器网络的损失函数
- en: 'The objective function of GANs, as described in the original paper *Generative
    Adversarial Nets* by *I. Goodfellow* and colleagues ([https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)),
    is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的目标函数，如*I. Goodfellow*及其同事在原始论文*生成对抗网络*（[https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)）中描述的那样：
- en: '![](img/B17582_17_020.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_020.png)'
- en: 'Here, ![](img/B17582_17_021.png) is called the **value function**, which can
    be interpreted as a payoff: we want to maximize its value with respect to the
    discriminator (*D*), while minimizing its value with respect to the generator
    (*G*), that is, ![](img/B17582_17_022.png). *D*(**x**) is the probability that
    indicates whether the input example, **x**, is real or fake (that is, generated).
    The expression ![](img/B17582_17_023.png) refers to the expected value of the
    quantity in brackets with respect to the examples from the data distribution (distribution
    of the real examples); ![](img/B17582_17_024.png) refers to the expected value
    of the quantity with respect to the distribution of the input, **z**, vectors.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_17_021.png)被称为**值函数**，可以被解释为一种回报：我们希望在鉴别器(*D*)方面最大化其值，同时在生成器(*G*)方面最小化其值，即![](img/B17582_17_022.png)。*D*(**x**)是指示输入示例**x**是真实还是生成的概率（即生成的）。表达式![](img/B17582_17_023.png)指的是对于来自数据分布（真实例子的分布）的示例期望值；![](img/B17582_17_024.png)是指对于输入向量**z**的分布的期望值。
- en: 'One training step of a GAN model with such a value function requires two optimization
    steps: (1) maximizing the payoff for the discriminator and (2) minimizing the
    payoff for the generator. A practical way of training GANs is to alternate between
    these two optimization steps: (1) fix (freeze) the parameters of one network and
    optimize the weights of the other one, and (2) fix the second network and optimize
    the first one. This process should be repeated at each training iteration. Let’s
    assume that the generator network is fixed, and we want to optimize the discriminator.
    Both terms in the value function ![](img/B17582_17_021.png) contribute to optimizing
    the discriminator, where the first term corresponds to the loss associated with
    the real examples, and the second term is the loss for the fake examples. Therefore,
    when *G* is fixed, our objective is to *maximize* ![](img/B17582_17_021.png),
    which means making the discriminator better at distinguishing between real and
    generated images.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GAN模型的一个训练步骤需要两个优化步骤：（1）最大化鉴别器的回报，（2）最小化生成器的回报。训练GAN的一个实际方法是在这两个优化步骤之间交替进行：（1）固定一个网络的参数并优化另一个网络的权重，（2）固定第二个网络并优化第一个网络。这个过程应该在每个训练迭代中重复。假设生成器网络被固定，并且我们想要优化鉴别器。值函数![](img/B17582_17_021.png)中的两项都有助于优化鉴别器，其中第一项对应于真实例子的损失，第二项是虚假例子的损失。因此，当*G*固定时，我们的目标是*最大化*![](img/B17582_17_021.png)，这意味着使鉴别器更好地区分真实和生成的图像。
- en: After optimizing the discriminator using the loss terms for real and fake samples,
    we then fix the discriminator and optimize the generator. In this case, only the
    second term in ![](img/B17582_17_021.png) contributes to the gradients of the
    generator. As a result, when *D* is fixed, our objective is to *minimize* ![](img/B17582_17_021.png),
    which can be written as ![](img/B17582_17_029.png). As was mentioned in the original
    GAN paper by Goodfellow and colleagues, this function, ![](img/B17582_17_030.png),
    suffers from vanishing gradients in the early training stages. The reason for
    this is that the outputs, *G*(**z**), early in the learning process, look nothing
    like real examples, and therefore *D*(*G*(**z**)) will be close to zero with high
    confidence. This phenomenon is called **saturation**. To resolve this issue, we
    can reformulate the minimization objective, ![](img/B17582_17_031.png), by rewriting
    it as ![](img/B17582_17_032.png).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 优化鉴别器使用真实和虚假样本的损失项后，我们固定鉴别器并优化生成器。在这种情况下，只有![](img/B17582_17_021.png)中的第二项对生成器的梯度起作用。因此，当*D*固定时，我们的目标是*最小化*![](img/B17582_17_021.png)，可以写成![](img/B17582_17_029.png)。正如Goodfellow及其同事在原始GAN论文中提到的那样，这个函数![](img/B17582_17_030.png)在早期训练阶段会出现梯度消失的问题。造成这一现象的原因是在学习过程早期，输出*G*(**z**)看起来与真实例子完全不同，因此*D*(*G*(**z**))会非常接近零并且有很高的置信度。这种现象被称为**饱和**。为了解决这个问题，我们可以通过重写将最小化目标![](img/B17582_17_031.png)重新表述为![](img/B17582_17_032.png)来重新制定。
- en: This replacement means that for training the generator, we can swap the labels
    of real and fake examples and carry out a regular function minimization. In other
    words, even though the examples synthesized by the generator are fake and are
    therefore labeled 0, we can flip the labels by assigning label 1 to these examples
    and *minimize* the binary cross-entropy loss with these new labels instead of
    maximizing ![](img/B17582_17_032.png).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种替换意味着在训练生成器时，我们可以交换真实和虚假示例的标签，并执行常规的函数最小化。换句话说，尽管生成器合成的示例是虚假的，因此标记为0，我们可以通过将这些示例分配标签1来反转标签，并*最小化*使用这些新标签的二元交叉熵损失，而不是最大化
    ![](img/B17582_17_032.png)。
- en: 'Now that we have covered the general optimization procedure for training GAN
    models, let’s explore the various data labels that we can use when training GANs.
    Given that the discriminator is a binary classifier (the class labels are 0 and
    1 for fake and real images, respectively), we can use the binary cross-entropy
    loss function. Therefore, we can determine the ground truth labels for the discriminator
    loss as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了训练GAN模型的一般优化过程，让我们探讨在训练GAN时可以使用的各种数据标签。鉴于判别器是二元分类器（虚假和真实图像的类标签分别为0和1），我们可以使用二元交叉熵损失函数。因此，我们可以确定判别器损失的地面真实标签如下：
- en: '![](img/B17582_17_034.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_034.png)'
- en: What about the labels to train the generator? As we want the generator to synthesize
    realistic images, we want to penalize the generator when its outputs are not classified
    as real by the discriminator. This means that we will assume the ground truth
    labels for the outputs of the generator to be 1 when computing the loss function
    for the generator.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，训练生成器的标签如何呢？由于我们希望生成器合成逼真的图像，当鉴别器不将其输出分类为真实图像时，我们希望惩罚生成器。这意味着在计算生成器的损失函数时，我们将假设生成器输出的地面真实标签为1。
- en: 'Putting all of this together, the following figure displays the individual
    steps in a simple GAN model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些内容整合在一起，下图展示了简单GAN模型中的各个步骤：
- en: '![](img/B17582_17_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_04.png)'
- en: 'Figure 17.4: The steps in building a GAN model'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4：构建GAN模型的步骤
- en: In the following section, we will implement a GAN from scratch to generate new
    handwritten digits.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将从零开始实现GAN以生成新的手写数字。
- en: Implementing a GAN from scratch
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始实现GAN
- en: In this section, we will cover how to implement and train a GAN model to generate
    new images such as MNIST digits. Since the training on a normal **central processing
    unit** (**CPU**) may take a long time, in the following subsection, we will cover
    how to set up the Google Colab environment, which will allow us to run the computations
    on **graphics processing units** (**GPUs**).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何实现和训练GAN模型以生成新的图像，如MNIST数字。由于在普通**中央处理单元**（**CPU**）上进行训练可能需要很长时间，因此在下一小节中，我们将介绍如何设置Google
    Colab环境，以便我们可以在**图形处理单元**（**GPU**）上运行计算。
- en: Training GAN models on Google Colab
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Google Colab上训练GAN模型
- en: Some of the code examples in this chapter may require extensive computational
    resources that go beyond a conventional laptop or a workstation without a GPU.
    If you already have an NVIDIA GPU-enabled computing machine available, with CUDA
    and cuDNN libraries installed, you can use that to speed up the computations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的某些代码示例可能需要超出传统笔记本电脑或工作站的常规计算资源。如果您已经有一个安装了CUDA和cuDNN库的NVIDIA GPU计算机，可以使用它来加快计算速度。
- en: However, since many of us do not have access to high-performance computing resources,
    we will use the Google Colaboratory environment (often referred to as Google Colab),
    which is a free cloud computing service (available in most countries).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于许多人无法获得高性能计算资源，我们将使用Google Colaboratory环境（通常称为Google Colab），这是一个免费的云计算服务（在大多数国家都可以使用）。
- en: Google Colab provides Jupyter Notebook instances that run on the cloud; the
    notebooks can be saved on Google Drive or GitHub. While the platform provides
    various different computing resources, such as CPUs, GPUs, and even **tensor processing
    units** (**TPUs**), it is important to highlight that the execution time is currently
    limited to 12 hours. Therefore, any notebook running longer than 12 hours will
    be interrupted.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab提供在云上运行的Jupyter Notebook实例；可以将笔记本保存在Google Drive或GitHub上。虽然该平台提供各种不同的计算资源，如CPU、GPU，甚至**张量处理单元**（**TPU**），但需要强调的是，执行时间目前限制为12小时。因此，任何运行超过12小时的笔记本将被中断。
- en: The code blocks in this chapter will need a maximum computing time of two to
    three hours, so this will not be an issue. However, if you decide to use Google
    Colab for other projects that take longer than 12 hours, be sure to use checkpointing
    and save intermediate checkpoints.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码块最长需要两到三个小时的计算时间，所以这不会成为问题。不过，如果你决定在Google Colab上运行其他超过12小时的项目，请务必使用检查点和保存中间检查点。
- en: '**Jupyter Notebook**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**'
- en: Jupyter Notebook is a graphical user interface (GUI) for running code interactively
    and interleaving it with text documentation and figures. Due to its versatility
    and ease of use, it has become one of the most popular tools in data science.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook是一个用于交互式运行代码、插入文档和图形的图形用户界面（GUI）。由于其多功能性和易用性，它已成为数据科学中最流行的工具之一。
- en: For more information about the general Jupyter Notebook GUI, please view the
    official documentation at [https://jupyter-notebook.readthedocs.io/en/stable/](https://jupyter-notebook.readthedocs.io/en/stable/).
    All the code in this book is also available in the form of Jupyter notebooks,
    and a short introduction can be found in the code directory of the first chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于Jupyter Notebook的一般信息，请查阅官方文档，网址为[https://jupyter-notebook.readthedocs.io/en/stable/](https://jupyter-notebook.readthedocs.io/en/stable/)。本书中所有代码也以Jupyter
    Notebook形式提供，第一章的代码目录中还附有简短介绍。
- en: Lastly, we highly recommend *Adam Rule* et al.’s article *Ten simple rules for
    writing and sharing computational analyses in Jupyter Notebooks* on using Jupyter
    Notebook effectively in scientific research projects, which is freely available
    at [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们强烈推荐*Adam Rule*等人的文章《在Jupyter Notebooks中编写和共享计算分析的十个简单规则》，该文章对在科学研究项目中有效使用Jupyter
    Notebook提供了有价值的建议，可在[https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007)免费获取。
- en: Accessing Google Colab is very straightforward. You can visit [https://colab.research.google.com](https://colab.research.google.com),
    which automatically takes you to a prompt window where you can see your existing
    Jupyter notebooks. From this prompt window, click the **Google Drive** tab, as
    shown in *Figure 17.5*. This is where you will save the notebook on your Google
    Drive.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 访问Google Colab非常简单。您可以访问[https://colab.research.google.com](https://colab.research.google.com)，该链接会自动跳转到一个提示窗口，您可以在其中看到现有的Jupyter笔记本。在这个提示窗口中，点击如*图17.5*所示的**Google
    Drive**选项卡，这是您将笔记本保存到Google Drive的地方。
- en: 'Then, to create a new notebook, click on the **New notebook** link at the bottom
    of the prompt window:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，要创建一个新的笔记本，请点击提示窗口底部的**New notebook**链接：
- en: '![](img/B17582_17_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_05.png)'
- en: 'Figure 17.5: Creating a new Python notebook in Google Colab'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5：在Google Colab中创建一个新的Python笔记本
- en: This will create and open a new notebook for you. All the code examples you
    write in this notebook will be automatically saved, and you can later access the
    notebook from your Google Drive in a directory called **Colab Notebooks**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您创建并打开一个新的笔记本。您在此笔记本中编写的所有代码示例都将自动保存，稍后您可以从名为**Colab Notebooks**的目录中访问笔记本。
- en: 'In the next step, we want to utilize GPUs to run the code examples in this
    notebook. To do this, from the **Runtime** option in the menu bar of this notebook,
    click on **Change runtime type** and select **GPU**, as shown in *Figure 17.6*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们希望利用GPU来运行此笔记本中的代码示例。为此，请在此笔记本菜单栏的**Runtime**选项中，点击**Change runtime
    type**，然后选择**GPU**，如*图17.6*所示：
- en: '![](img/B17582_17_06.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_06.png)'
- en: 'Figure 17.6: Utilizing GPUs in Google Colab'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6：在Google Colab中利用GPU
- en: In the last step, we just need to install the Python packages that we will need
    for this chapter. The Colab Notebooks environment already comes with certain packages,
    such as NumPy, SciPy, and the latest stable version of PyTorch. At the time of
    writing, the latest stable version on Google Colab is PyTorch 1.9.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步中，我们只需安装本章所需的Python包。Colab Notebooks环境已经预装了某些包，如NumPy、SciPy和最新稳定版本的PyTorch。撰写本文时，Google
    Colab的最新稳定版本是PyTorch 1.9。
- en: 'Now, we can test the installation and verify that the GPU is available using
    the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下代码来测试安装并验证GPU是否可用：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Furthermore, if you want to save the model to your personal Google Drive, or
    transfer or upload other files, you need to mount Google Drive. To do this, execute
    the following in a new cell of the notebook:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想要将模型保存到个人谷歌驱动器，或者转移或上传其他文件，你需要挂载谷歌驱动器。要做到这一点，在笔记本的新单元格中执行以下操作：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will provide a link to authenticate the Colab Notebook accessing your
    Google Drive. After following the instructions for authentication, it will provide
    an authentication code that you need to copy and paste into the designated input
    field below the cell you have just executed. Then, your Google Drive will be mounted
    and available at `/content/drive/My Drive`. Alternatively, you can mount it via
    the GUI interface as highlighted in *Figure 17.7*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供一个链接来对Colab笔记本访问你的谷歌驱动器进行身份验证。按照身份验证的说明操作后，它会提供一个身份验证代码，你需要复制并粘贴到刚刚执行的单元格下方的指定输入字段中。然后，你的谷歌驱动器将被挂载，并可在`/content/drive/My
    Drive`路径下访问。或者，你可以通过GUI界面挂载，如*图17.7*所示：
- en: '![Graphical user interface, application, email  Description automatically generated](img/B17582_17_07.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面、应用程序、电子邮件说明](img/B17582_17_07.png)'
- en: 'Figure 17.7: Mounting your Google Drive'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7：挂载你的谷歌驱动器
- en: Implementing the generator and the discriminator networks
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现生成器和判别器网络
- en: 'We will start the implementation of our first GAN model with a generator and
    a discriminator as two fully connected networks with one or more hidden layers,
    as shown in *Figure 17.8*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个包含生成器和判别器的两个全连接网络的第一个GAN模型实现开始，如*图17.8*所示：
- en: '![Diagram  Description automatically generated](img/B17582_17_08.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图解](img/B17582_17_08.png)'
- en: 'Figure 17.8: A GAN model with a generator and discriminator as two fully connected
    networks'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8：一个包含生成器和判别器的GAN模型，两者都是全连接网络
- en: '*Figure 17.8* depicts the original GAN based on fully connected layers, which
    we will refer to as a *vanilla GAN*.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.8* 描述了基于全连接层的原始GAN，我们将其称为*香草GAN*。'
- en: In this model, for each hidden layer, we will apply the leaky ReLU activation
    function. The use of ReLU results in sparse gradients, which may not be suitable
    when we want to have the gradients for the full range of input values. In the
    discriminator network, each hidden layer is also followed by a dropout layer.
    Furthermore, the output layer in the generator uses the hyperbolic tangent (tanh)
    activation function. (Using tanh activation is recommended for the generator network
    since it helps with the learning.)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，对于每个隐藏层，我们将应用泄漏线性整流单元（Leaky ReLU）激活函数。使用ReLU会导致梯度稀疏化，这在我们希望对输入值的全范围有梯度时可能不太合适。在判别器网络中，每个隐藏层之后还跟有一个dropout层。此外，生成器的输出层使用双曲正切（tanh）激活函数。（推荐在生成器网络中使用tanh激活函数，因为它有助于学习。）
- en: The output layer in the discriminator has no activation function (that is, linear
    activation) to get the logits. Alternatively, we can use the sigmoid activation
    function to get probabilities as output.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器的输出层没有激活函数（即线性激活），以获取logits。作为替代，我们可以使用sigmoid激活函数以获取概率作为输出。
- en: '**Leaky rectified linear unit (ReLU) activation function**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**泄漏修正线性单元（ReLU）激活函数**'
- en: In *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, we covered
    different nonlinear activation functions that can be used in an NN model. If you
    recall, the ReLU activation function was defined as ![](img/B17582_17_035.png),
    which suppresses the negative (preactivation) inputs; that is, negative inputs
    are set to zero. Consequently, using the ReLU activation function may result in
    sparse gradients during backpropagation. Sparse gradients are not always detrimental
    and can even benefit models for classification. However, in certain applications,
    such as GANs, it can be beneficial to obtain the gradients for the full range
    of input values, which we can achieve by making a slight modification to the ReLU
    function such that it outputs small values for negative inputs. This modified
    version of the ReLU function is also known as **leaky ReLU**. In short, the leaky
    ReLU activation function permits non-zero gradients for negative inputs as well,
    and as a result, it makes the networks more expressive overall.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第12章*，《使用PyTorch并行化神经网络训练》，我们介绍了可以在NN模型中使用的不同非线性激活函数。如果你还记得，ReLU激活函数被定义为![](img/B17582_17_035.png)，它抑制负（激活前）输入；也就是说，负输入被设置为零。因此，在反向传播过程中使用ReLU激活函数可能导致稀疏梯度。稀疏梯度并不总是有害的，甚至可以使分类模型受益。然而，在某些应用中，如GANs，获得全范围输入值的梯度可能对模型有利。我们可以通过对ReLU函数进行轻微修改来实现这一点，使其对负输入输出小值。这种修改版本的ReLU函数也被称为**泄露的ReLU**。简而言之，泄露的ReLU激活函数允许负输入的梯度非零，并因此使网络整体更具表现力。
- en: 'The leaky ReLU activation function is defined as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 泄露的ReLU激活函数定义如下：
- en: '![A picture containing diagram  Description automatically generated](img/B17582_17_09.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing diagram  Description automatically generated](img/B17582_17_09.png)'
- en: 'Figure 17.9: The leaky ReLU activation function'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.9：泄露的ReLU激活函数
- en: Here, ![](img/B17582_05_018.png) determines the slope for the negative (preactivation)
    inputs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_05_018.png)决定了负（激活前）输入的斜率。
- en: 'We will define two helper functions for each of the two networks, instantiate
    a model from the PyTorch `nn.Sequential` class, and add the layers as described.
    The code is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个网络定义两个辅助函数，从PyTorch的`nn.Sequential`类中实例化模型，并按描述添加层。代码如下：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will specify the training settings for the model. As you will remember
    from previous chapters, the image size in the MNIST dataset is 28×28 pixels. (That
    is only one color channel because MNIST contains only grayscale images.) We will
    further specify the size of the input vector, **z**, to be 20\. Since we are implementing
    a very simple GAN model for illustration purposes only and using fully connected
    layers, we will only use a single hidden layer with 100 units in each network.
    In the following code, we will specify and initialize the two networks, and print
    their summary information:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将指定模型的训练设置。正如您从前几章中记得的那样，MNIST数据集中的图像大小为28×28像素。（因为MNIST只包含灰度图像，所以只有一个颜色通道。）我们还将指定输入向量**z**的大小为20。由于我们只是为了演示目的而实现了一个非常简单的GAN模型，并使用全连接层，我们只会使用每个网络中的一个单隐藏层，每层有100个单元。在下面的代码中，我们将指定和初始化这两个网络，并打印它们的摘要信息：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Defining the training dataset
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义训练数据集
- en: 'In the next step, we will load the MNIST dataset from PyTorch and apply the
    necessary preprocessing steps. Since the output layer of the generator is using
    the tanh activation function, the pixel values of the synthesized images will
    be in the range (–1, 1). However, the input pixels of the MNIST images are within
    the range [0, 255] (with a data type `PIL.Image.Image`). Thus, in the preprocessing
    steps, we will use the `torchvision.transforms.ToTensor` function to convert the
    input image tensors to a tensor. As a result, besides changing the data type,
    calling this function will also change the range of input pixel intensities to
    [0, 1]. Then, we can shift them by –0.5 and scale them by a factor of 0.5 such
    that the pixel intensities will be rescaled to be in the range [–1, 1], which
    can improve gradient descent-based learning:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将从PyTorch加载MNIST数据集并应用必要的预处理步骤。由于生成器的输出层使用tanh激活函数，合成图像的像素值将在（-1，1）范围内。然而，MNIST图像的输入像素在[0，255]范围内（数据类型为`PIL.Image.Image`）。因此，在预处理步骤中，我们将使用`torchvision.transforms.ToTensor`函数将输入图像张量转换为张量。因此，除了更改数据类型外，调用此函数还将改变输入像素强度的范围为[0，1]。然后，我们可以通过将它们移动-0.5并缩放0.5的因子来将它们移动到[-1，1]范围内，从而改善基于梯度下降的学习：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Furthermore, we will also create a random vector, **z**, based on the desired
    random distribution (in this code example, uniform or normal, which are the most
    common choices):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将根据所需的随机分布（在此代码示例中为均匀分布或正态分布，这是最常见的选择之一）创建一个随机向量**z**：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s inspect the dataset object that we created. In the following code, we
    will take one batch of examples and print the array shapes of this sample of input
    vectors and images. Furthermore, in order to understand the overall data flow
    of our GAN model, in the following code, we will process a forward pass for our
    generator and discriminator.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们创建的数据集对象。在接下来的代码中，我们将取一个示例批次并打印这些输入向量和图像的数组形状。此外，为了理解我们的GAN模型的整体数据流，我们将在下面的代码中为我们的生成器和判别器进行前向传递。
- en: 'First, we will feed the batch of input, **z**, vectors to the generator and
    get its output, `g_output`. This will be a batch of fake examples, which will
    be fed to the discriminator model to get the probabilities for the batch of fake
    examples, `d_proba_fake`. Furthermore, the processed images that we get from the
    dataset object will be fed to the discriminator model, which will result in the
    probabilities for the real examples, `d_proba_real`. The code is as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将批量输入向量**z**传递给生成器并获取其输出`g_output`。这将是一批假例子，将被馈送到判别器模型以获取假例子批次`d_proba_fake`的概率。此外，我们从数据集对象获取的处理后的图像将被馈送到判别器模型，这将导致真实例子批次`d_proba_real`的概率。代码如下：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The two probabilities, `d_proba_fake` and `d_proba_real`, will be used to compute
    the loss functions for training the model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 两个概率`d_proba_fake`和`d_proba_real`将用于计算训练模型的损失函数。
- en: Training the GAN model
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练GAN模型
- en: 'As the next step, we will create an instance of `nn.BCELoss` as our loss function
    and use that to calculate the binary cross-entropy loss for the generator and
    discriminator associated with the batches that we just processed. To do this,
    we also need the ground truth labels for each output. For the generator, we will
    create a vector of 1s with the same shape as the vector containing the predicted
    probabilities for the generated images, `d_proba_fake`. For the discriminator
    loss, we have two terms: the loss for detecting the fake examples involving `d_proba_fake`
    and the loss for detecting the real examples based on `d_proba_real`.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，我们将创建一个`nn.BCELoss`的实例作为我们的损失函数，并使用它来计算与我们刚刚处理的批次相关的生成器和判别器的二元交叉熵损失。为此，我们还需要每个输出的地面实况标签。对于生成器，我们将创建一个与包含生成图像预测概率的向量`d_proba_fake`形状相同的1向量。对于判别器损失，我们有两个项：涉及`d_proba_fake`的检测假例的损失和基于`d_proba_real`的检测真实例的损失。
- en: 'The ground truth labels for the fake term will be a vector of 0s that we can
    generate via the `torch.zeros()` (or `torch.zeros_like()`) function. Similarly,
    we can generate the ground truth values for the real images via the `torch.ones()`
    (or `torch.ones_like()`) function, which creates a vector of 1s:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于假术语的地面实况标签将是一个0向量，我们可以通过`torch.zeros()`（或`torch.zeros_like()`）函数生成。类似地，我们可以通过`torch.ones()`（或`torch.ones_like()`）函数生成真实图像的地面实况值，该函数创建一个1向量：
- en: '[PRE7]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The previous code example shows the step-by-step calculation of the different
    loss terms for the purpose of understanding the overall concept behind training
    a GAN model. The following code will set up the GAN model and implement the training
    loop, where we will include these calculations in a `for` loop.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码示例展示了逐步计算不同损失项的过程，以便理解训练GAN模型背后的整体概念。接下来的代码将设置GAN模型并实现训练循环，在其中我们将在`for`循环中包括这些计算。
- en: 'We will start with setting up the data loader for the real dataset, the generator
    and discriminator model, as well as a separate Adam optimizer for each of the
    two models:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从为真实数据集设置数据加载器开始，包括生成器和判别器模型，以及两个模型各自的单独Adam优化器：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In addition, we will compute the loss gradients with respect to the model weights
    and optimize the parameters of the generator and discriminator using two separate
    Adam optimizers. We will write two utility functions for training the discriminator
    and the generator as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们将计算损失相对于模型权重的梯度，并使用两个单独的Adam优化器优化生成器和判别器的参数。我们将编写两个实用函数来训练判别器和生成器，如下所示：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will alternate between the training of the generator and the discriminator
    for 100 epochs. For each epoch, we will record the loss for the generator, the
    loss for the discriminator, and the loss for the real data and fake data respectively.
    Furthermore, after each epoch, we will generate some examples from a fixed noise
    input using the current generator model by calling the `create_samples()` function.
    We will store the synthesized images in a Python list. The code is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在 100 个 epochs 中交替训练生成器和判别器。每个 epoch，我们将记录生成器的损失、判别器的损失以及真实数据和假数据的损失。此外，在每个
    epoch 后，我们将使用当前生成器模型调用 `create_samples()` 函数从固定噪声输入生成一些示例。我们将合成的图像存储在一个 Python
    列表中。代码如下：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using a GPU on Google Colab, the training process that we implemented in the
    previous code block should be completed in less than an hour. (It may even be
    faster on your personal computer if you have a recent and capable CPU and a GPU.)
    After the model training has completed, it is often helpful to plot the discriminator
    and generator losses to analyze the behavior of both subnetworks and assess whether
    they converged.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 上使用 GPU，我们在前面的代码块中实现的训练过程应该在一个小时内完成。（如果你有最新和强大的 CPU 和 GPU，甚至可能更快。）模型训练完成后，通常有助于绘制判别器和生成器的损失，分析两个子网络的行为并评估它们是否收敛。
- en: 'It is also helpful to plot the average probabilities of the batches of real
    and fake examples as computed by the discriminator in each iteration. We expect
    these probabilities to be around 0.5, which means that the discriminator is not
    able to confidently distinguish between real and fake images:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 还有助于绘制由判别器在每次迭代中计算的真实和假例子批次的平均概率。我们期望这些概率在 0.5 左右，这意味着判别器不能自信地区分真实和假的图像：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Figure 17.10* shows the results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17.10* 显示了结果：'
- en: '![](img/B17582_17_10.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_10.png)'
- en: 'Figure 17.10: The discriminator performance'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.10：判别器的性能
- en: As you can see from the discriminator outputs in the previous figure, during
    the early stages of the training, the discriminator was able to quickly learn
    to distinguish quite accurately between the real and fake examples; that is, the
    fake examples had probabilities close to 0, and the real examples had probabilities
    close to 1\. The reason for that was that the fake examples were nothing like
    the real ones; therefore, distinguishing between real and fake was rather easy.
    As the training proceeds further, the generator will become better at synthesizing
    realistic images, which will result in probabilities of both real and fake examples
    that are close to 0.5.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的判别器输出中所见，训练早期，判别器能够快速学习准确区分真实和假例子；也就是说，假例子的概率接近 0，真实例子的概率接近 1。这是因为假例子与真实例子完全不同，因此很容易区分真伪。随着训练的进展，生成器将变得更擅长合成逼真图像，这将导致真实和假例子的概率都接近
    0.5。
- en: 'Furthermore, we can also see how the outputs of the generator, that is, the
    synthesized images, change during training. In the following code, we will visualize
    some of the images produced by the generator for a selection of epochs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以看到生成器输出，即合成图像在训练过程中的变化。在接下来的代码中，我们将可视化生成器为一些 epochs 选择生成的一些图像。
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Figure 17.11* shows the produced images:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17.11* 展示了生成的图像：'
- en: '![](img/B17582_17_11.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_11.png)'
- en: 'Figure 17.11: Images produced by the generator'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.11：生成器生成的图像
- en: As you can see from *Figure 17.11*, the generator network produced more and
    more realistic images as the training progressed. However, even after 100 epochs,
    the produced images still look very different from the handwritten digits contained
    in the MNIST dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从 *图 17.11* 中可以看到的那样，随着训练的进行，生成器网络生成的图像变得越来越逼真。然而，即使经过 100 个 epochs，生成的图像仍然与
    MNIST 数据集中的手写数字非常不同。
- en: In this section, we designed a very simple GAN model with only a single fully
    connected hidden layer for both the generator and discriminator. After training
    the GAN model on the MNIST dataset, we were able to achieve promising, although
    not yet satisfactory, results with the new handwritten digits.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们设计了一个非常简单的 GAN 模型，生成器和判别器仅有一个完全连接的隐藏层。在 MNIST 数据集上训练 GAN 模型后，我们能够获得有希望的，尽管还不完全满意的新手写数字结果。
- en: As we learned in *Chapter 14*, *Classifying Images with Deep Convolutional Neural
    Networks*, NN architectures with convolutional layers have several advantages
    over fully connected layers when it comes to image classification. In a similar
    sense, adding convolutional layers to our GAN model to work with image data might
    improve the outcome. In the next section, we will implement a **deep convolutional
    GAN** (**DCGAN**), which uses convolutional layers for both the generator and
    the discriminator networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在《第14章》“使用深度卷积神经网络对图像进行分类”中学到的，具有卷积层的NN体系结构在图像分类时比全连接层具有几个优势。在类似的情况下，将卷积层添加到我们的GAN模型中以处理图像数据可能会改善结果。在下一节中，我们将实现一个**深度卷积GAN**
    (**DCGAN**)，它将使用卷积层来构建生成器和鉴别器网络。
- en: Improving the quality of synthesized images using a convolutional and Wasserstein
    GAN
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积和Wasserstein GAN来改善合成图像的质量
- en: In this section, we will implement a DCGAN, which will enable us to improve
    the performance we saw in the previous GAN example. Additionally, we will briefly
    talk about an extra key technique, **Wasserstein GAN** (**WGAN**).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将实现一个DCGAN，这将使我们能够提高我们在前面GAN示例中看到的性能。此外，我们还将简要讨论一种额外的关键技术，**Wasserstein
    GAN** (**WGAN**)。
- en: 'The techniques that we will cover in this section will include the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将涵盖以下技术：
- en: Transposed convolution
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转置卷积
- en: Batch normalization (BatchNorm)
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化（BatchNorm）
- en: WGAN
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WGAN
- en: The DCGAN was proposed in 2016 by *A. Radford*, *L. Metz*, and *S. Chintala*
    in their article *Unsupervised representation learning with deep convolutional
    generative adversarial networks*, which is freely available at [https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf).
    In this article, the researchers proposed using convolutional layers for both
    the generator and discriminator networks. Starting from a random vector, **z**,
    the DCGAN first uses a fully connected layer to project **z** into a new vector
    with a proper size so that it can be reshaped into a spatial convolution representation
    (*h*×*w*×*c*), which is smaller than the output image size. Then, a series of
    convolutional layers, known as **transposed convolution**, are used to upsample
    the feature maps to the desired output image size.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN是由*A. Radford*、*L. Metz*和*S. Chintala*在他们的文章《无监督表示学习与深度卷积生成对抗网络》中提出的，该文章可以在[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)免费获取。在这篇文章中，研究人员建议为生成器和鉴别器网络都使用卷积层。从一个随机向量**z**开始，DCGAN首先使用全连接层将**z**投影到一个新的向量中，使其大小适当，以便可以将其重新塑造为空间卷积表示（*h*×*w*×*c*），这个表示比输出图像大小要小。然后，一系列卷积层（称为转置卷积）被用来将特征图上采样到所需的输出图像大小。
- en: Transposed convolution
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转置卷积
- en: In *Chapter 14*, you learned about the convolution operation in one- and two-dimensional
    spaces. In particular, we looked at how the choices for the padding and strides
    change the output feature maps. While a convolution operation is usually used
    to downsample the feature space (for example, by setting the stride to 2, or by
    adding a pooling layer after a convolutional layer), a *transposed convolution*
    operation is usually used for *upsampling* the feature space.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在《第14章》中，你学习了在一维和二维空间中的卷积操作。特别是，我们看到了如何通过选择填充和步幅来改变输出特征图。虽然卷积操作通常用于对特征空间进行下采样（例如，通过将步幅设置为2，或在卷积层后添加池化层），而*转置卷积*操作通常用于对特征空间进行上采样。
- en: To understand the transposed convolution operation, let’s go through a simple
    thought experiment. Assume that we have an input feature map of size *n*×*n*.
    Then, we apply a 2D convolution operation with certain padding and stride parameters
    to this *n*×*n* input, resulting in an output feature map of size *m*×*m*. Now,
    the question is, how we can apply another convolution operation to obtain a feature
    map with the initial dimension *n*×*n* from this *m*×*m* output feature map while
    maintaining the connectivity patterns between the input and output? Note that
    only the shape of the *n*×*n* input matrix is recovered and not the actual matrix
    values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解转置卷积操作，让我们进行一个简单的思想实验。假设我们有一个大小为*n*×*n*的输入特征图。然后，我们对这个*n*×*n*的输入应用一个带有特定填充和步幅参数的2D卷积操作，得到一个大小为*m*×*m*的输出特征图。现在的问题是，我们如何可以应用另一个卷积操作来从这个*m*×*m*的输出特征图中获得一个具有初始维度*n*×*n*的特征图，同时保持输入和输出之间的连接模式？请注意，只恢复了*n*×*n*输入矩阵的形状，而不是实际的矩阵值。
- en: 'This is what transposed convolution does, as shown in *Figure 17.12*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 17.12* 所示，这就是转置卷积的工作原理：
- en: '![](img/B17582_17_12.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_12.png)'
- en: 'Figure 17.12: Transposed convolution'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.12：转置卷积
- en: '**Transposed convolution versus deconvolution**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**转置卷积与反卷积**'
- en: Transposed convolution is also called **fractionally strided convolution**.
    In deep learning literature, another common term that is used to refer to transposed
    convolution is **deconvolution**. However, note that deconvolution was originally
    defined as the inverse of a convolution operation, *f*, on a feature map, **x**,
    with weight parameters, **w**, producing feature map **x′**, *f*[w](**x**) = **x′**.
    A deconvolution function, *f*^(–1), can then be defined as ![](img/B17582_17_037.png).
    However, note that the transposed convolution is merely focused on recovering
    the dimensionality of the feature space and not the actual values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积也称为**分数步幅卷积**。在深度学习文献中，另一个常用的术语来指代转置卷积的是**反卷积**。但是需要注意的是，反卷积最初被定义为对特征映射
    **x** 进行逆卷积操作 *f*，其权重参数为 **w**，生成特征映射 **x′**，*f*[w](**x**) = **x′**。然后可以定义一个反卷积函数
    *f*^(–1)，如 ![](img/B17582_17_037.png)。但需要注意的是，转置卷积仅仅专注于恢复特征空间的维度，而不是实际的数值。
- en: 'Upsampling feature maps using transposed convolution works by inserting 0s
    between the elements of the input feature maps. *Figure 17.13* shows an example
    of applying transposed convolution to an input of size 4×4, with a stride of 2×2
    and kernel size of 2×2\. The matrix of size 9×9 in the center shows the results
    after inserting such 0s into the input feature map. Then, performing a normal
    convolution using the 2×2 kernel with a stride of 1 results in an output of size
    8×8\. We can verify the backward direction by performing a regular convolution
    on the output with a stride of 2, which results in an output feature map of size
    4×4, which is the same as the original input size:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用转置卷积对特征映射进行上采样是通过在输入特征映射的元素之间插入 0 来实现的。*图 17.13* 展示了将转置卷积应用于一个大小为 4×4 的输入的示例，步幅为
    2×2，卷积核大小为 2×2。中间的 9×9 矩阵显示了在输入特征映射中插入这样的 0 后的结果。然后，使用 2×2 卷积核和步幅为 1 进行普通卷积将产生一个大小为
    8×8 的输出。我们可以通过在输出上执行步幅为 2 的常规卷积来验证反向方向，这将产生一个大小为 4×4 的输出特征映射，与原始输入大小相同：
- en: '![](img/B17582_17_13.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_13.png)'
- en: 'Figure 17.13: Applying transposed convolution to a 4×4 input'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.13：将转置卷积应用于一个 4×4 的输入
- en: '*Figure 17.13* shows how transposed convolution works in general. There are
    various cases in which input size, kernel size, strides, and padding variations
    can change the output. If you want to learn more about all these different cases,
    refer to the tutorial *A Guide to Convolution Arithmetic for Deep Learning* by
    *Vincent Dumoulin* and *Francesco Visin*, 2018 ([https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf).)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17.13* 概述了转置卷积的工作原理。输入大小、卷积核大小、步幅和填充方式的不同情况会影响输出结果。如果你想了解更多关于这些不同情况的内容，请参考
    *A Guide to Convolution Arithmetic for Deep Learning*，由 *Vincent Dumoulin* 和 *Francesco
    Visin* 撰写，2018 年 ([https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf).)'
- en: Batch normalization
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批归一化
- en: '**BatchNorm** was introduced in 2015 by Sergey Ioffe and Christian Szegedy
    in the article *Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift*, which you can access via arXiv at [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf).
    One of the main ideas behind BatchNorm is normalizing the layer inputs and preventing
    changes in their distribution during training, which enables faster and better
    convergence.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**BatchNorm** 是由 Sergey Ioffe 和 Christian Szegedy 在文章 *Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift* 中于 2015
    年提出的，您可以通过 arXiv 访问该文章 [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf)。BatchNorm
    的主要思想之一是对层输入进行归一化，并在训练期间防止其分布的变化，从而加快和改善收敛速度。'
- en: 'BatchNorm transforms a mini-batch of features based on its computed statistics.
    Assume that we have the net preactivation feature maps obtained after a convolutional
    layer in a four-dimensional tensor, **Z**, with the shape [*m*×*c*×*h*×*w*], where
    *m* is the number of examples in the batch (i.e., batch size), *h*×*w* is the
    spatial dimension of the feature maps, and *c* is the number of channels. BatchNorm
    can be summarized in three steps, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: BatchNorm 基于其计算出的统计信息来转换一个特征的小批量。假设我们有一个经过卷积层得到的四维张量 **Z** 的网络预激活特征映射，其形状为 [*m*×*c*×*h*×*w*]，其中
    *m* 是批量大小（即批量大小），*h*×*w* 是特征映射的空间维度，*c* 是通道数。BatchNorm 可以总结为三个步骤：
- en: 'Compute the mean and standard deviation of the net inputs for each mini-batch:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个小批次的网络输入的均值和标准差：
- en: '![](img/B17582_17_038.png)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_038.png)'
- en: where ![](img/B17582_17_039.png) and ![](img/B17582_17_040.png) both have size
    *c*.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 ![](img/B17582_17_039.png) 和 ![](img/B17582_17_040.png) 都具有大小 *c*。
- en: 'Standardize the net inputs for all examples in the batch:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对批次中所有示例的网络输入进行标准化：
- en: '![](img/B17582_17_041.png)'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_041.png)'
- en: where ![](img/B17582_17_042.png) is a small number for numerical stability (that
    is, to avoid division by zero).
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 ![](img/B17582_17_042.png) 是为了数值稳定性而设定的一个小数（即避免除以零）。
- en: 'Scale and shift the normalized net inputs using two learnable parameter vectors,
    ![](img/B17582_17_043.png) and ![](img/B17582_17_044.png), of size *c* (number
    of channels):'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用两个可学习参数向量 ![](img/B17582_17_043.png) 和 ![](img/B17582_17_044.png)（大小为 *c*，即通道数）来缩放和偏移标准化的网络输入：
- en: '![](img/B17582_17_045.png)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_045.png)'
- en: '*Figure 17.14* illustrates the process:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 17.14* 描述了这个过程：'
- en: '![](img/B17582_17_14.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_14.png)'
- en: 'Figure 17.14: The process of batch normalization'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.14：批次归一化的过程
- en: In the first step of BatchNorm, the mean, ![](img/B17582_17_039.png), and standard
    deviation, ![](img/B17582_17_047.png), of the mini-batch are computed. Both ![](img/B17582_17_039.png)
    and ![](img/B17582_17_047.png) are vectors of size *c* (where *c* is the number
    of channels). Then, these statistics are used in *step 2* to scale the examples
    in each mini-batch via z-score normalization (standardization), resulting in standardized
    net inputs, ![](img/B17582_17_050.png). As a consequence, these net inputs are
    mean-centered and have *unit variance*, which is generally a useful property for
    gradient descent-based optimization. On the other hand, always normalizing the
    net inputs such that they have the same properties across the different mini-batches,
    which can be diverse, can severely impact the representational capacity of NNs.
    This can be understood by considering a feature, ![](img/B17582_17_051.png), which,
    after sigmoid activation to ![](img/B17582_17_052.png), results in a linear region
    for values close to 0\. Therefore, in step 3, the learnable parameters, ![](img/B17582_17_044.png)
    and ![](img/B17582_17_043.png), which are vectors of size *c* (number of channels),
    allow BatchNorm to control the shift and spread of the normalized features.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BatchNorm 的第一步中，计算小批次的均值 ![](img/B17582_17_039.png) 和标准差 ![](img/B17582_17_047.png)。这两者都是大小为
    *c* 的向量（其中 *c* 是通道数）。然后，这些统计数据在 *第 2 步* 中用于通过 z-score 标准化（标准化）来缩放每个小批次中的示例，得到标准化的网络输入
    ![](img/B17582_17_050.png)。因此，这些网络输入是以均值为中心并具有*单位方差*的，这通常是基于梯度下降的优化中的一个有用特性。另一方面，总是归一化网络输入，使它们在不同的小批次之间具有相同的属性，这些属性可能是多样的，可能会严重影响神经网络的表示能力。这可以通过考虑特征
    ![](img/B17582_17_051.png)，在进行 sigmoid 激活后到达 ![](img/B17582_17_052.png)，对接近 0
    的值具有线性区域来理解。因此，在第 3 步中，可学习的参数 ![](img/B17582_17_044.png) 和 ![](img/B17582_17_043.png)，这些参数是大小为
    *c* 的向量（通道数），允许 BatchNorm 控制标准化特征的偏移和扩展。
- en: During training, the running averages, ![](img/B17582_17_039.png), and running
    variance, ![](img/B17582_17_040.png), are computed, which are used along with
    the tuned parameters, ![](img/B17582_17_044.png) and ![](img/B17582_17_043.png),
    to normalize the test example(s) at evaluation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 训练期间计算运行平均值 ![](img/B17582_17_039.png) 和运行方差 ![](img/B17582_17_040.png)，这些值与调整后的参数
    ![](img/B17582_17_044.png) 和 ![](img/B17582_17_043.png) 一起用于规范化评估时的测试示例。
- en: '**Why does BatchNorm help optimization?**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么 BatchNorm 能帮助优化？**'
- en: Initially, BatchNorm was developed to reduce the so-called **internal covariance
    shift**, which is defined as the changes that occur in the distribution of a layer’s
    activations due to the updated network parameters during training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，BatchNorm 的开发目的是减少所谓的**内部协变量漂移**，即由于训练期间更新的网络参数而导致的层激活分布的变化。
- en: To explain this with a simple example, consider a fixed batch that passes through
    the network at epoch 1\. We record the activations of each layer for this batch.
    After iterating through the whole training dataset and updating the model parameters,
    we start the second epoch, where the previously fixed batch passes through the
    network. Then, we compare the layer activations from the first and second epochs.
    Since the network parameters have changed, we observe that the activations have
    also changed. This phenomenon is called the **internal covariance shift**, which
    was believed to decelerate NN training.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 举个简单例子来解释，考虑一个固定的批次在第1个epoch通过网络。我们记录该批次每层的激活。在遍历整个训练数据集并更新模型参数后，我们开始第二个epoch，之前固定的批次再次通过网络。然后，我们比较第一和第二个epoch的层激活。由于网络参数已更改，我们观察到激活也已更改。这种现象称为**内部协方差转移**，据信会减缓神经网络的训练速度。
- en: However, in 2018, S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry further investigated
    what makes BatchNorm so effective. In their study, the researchers observed that
    the effect of BatchNorm on the internal covariance shift is marginal. Based on
    the outcome of their experiments, they hypothesized that the effectiveness of
    BatchNorm is, instead, based on a smoother surface of the loss function, which
    makes the non-convex optimization more robust.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，2018年，S. Santurkar, D. Tsipras, A. Ilyas和A. Madry进一步研究了BatchNorm如此有效的原因。在他们的研究中，研究人员观察到BatchNorm对内部协方差转移的影响微小。基于他们实验的结果，他们假设BatchNorm的有效性取决于损失函数表面的平滑度，这使得非凸优化更加健壮。
- en: If you are interested in learning more about these results, read through the
    original paper, *How Does Batch Normalization Help Optimization?*, which is freely
    available at [http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf](http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对了解这些结果更感兴趣，请阅读原始论文*How Does Batch Normalization Help Optimization?*，可以在[http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf](http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf)免费获取。
- en: The PyTorch API provides a class, `nn.BatchNorm2d()` (`nn.BatchNorm1d()` for
    1D input), that we can use as a layer when defining our models; it will perform
    all of the steps that we described for BatchNorm. Note that the behavior for updating
    the learnable parameters, ![](img/B17582_17_043.png) and ![](img/B17582_17_044.png),
    depends on whether the model is a training model not. These parameters are learned
    only during training and are then used for normalization during evaluation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch API提供了一个类`nn.BatchNorm2d()`（对于1D输入是`nn.BatchNorm1d()`），我们在定义模型时可以将其用作层；它会执行我们描述的所有BatchNorm步骤。请注意，更新可学习参数![](img/B17582_17_043.png)和![](img/B17582_17_044.png)的行为取决于模型是否处于训练模式。这些参数仅在训练期间学习，然后在评估期间用于归一化。
- en: Implementing the generator and discriminator
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现生成器和鉴别器
- en: At this point, we have covered the main components of a DCGAN model, which we
    will now implement. The architectures of the generator and discriminator networks
    are summarized in the following two figures.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了DCGAN模型的主要组成部分，接下来我们将实现它。生成器和鉴别器网络的架构总结如下两个图示。
- en: The generator takes a vector, **z**, of size 100 as input. Then, a series of
    transposed convolutions using `nn.ConvTranspose2d()` upsamples the feature maps
    until the spatial dimension of the resulting feature maps reaches 28×28\. The
    number of channels is reduced by half after each transposed convolutional layer,
    except the last one, which uses only one output filter to generate a grayscale
    image. Each transposed convolutional layer is followed by BatchNorm and leaky
    ReLU activation functions, except the last one, which uses tanh activation (without
    BatchNorm).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受大小为100的向量**z**作为输入。然后，使用`nn.ConvTranspose2d()`进行一系列转置卷积，直到生成的特征图的空间尺寸达到28×28。每个转置卷积层在BatchNorm和leaky
    ReLU激活函数之后，最后一个仅使用一个输出滤波器以生成灰度图像。每个转置卷积层之后都跟随BatchNorm和leaky ReLU激活函数，最后一个转置卷积层使用tanh激活函数（不使用BatchNorm）。
- en: 'The architecture for the generator (the feature maps after each layer) is shown
    in *Figure 17.15*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的架构（每层后的特征图）如*图17.15*所示：
- en: '![](img/B17582_17_15.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_15.png)'
- en: 'Figure 17.15: The generator network'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.15：生成器网络
- en: 'The discriminator receives images of size 1×28×28, which are passed through
    four convolutional layers. The first three convolutional layers reduce the spatial
    dimensionality by 4 while increasing the number of channels of the feature maps.
    Each convolutional layer is also followed by BatchNorm and leaky ReLU activation.
    The last convolutional layer uses kernels of size 7×7 and a single filter to reduce
    the spatial dimensionality of the output to 1×1×1\. Finally, the convolutional
    output is followed by a sigmoid function and squeezed to one dimension:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器接收大小为1×28×28的图像，这些图像通过四个卷积层。前三个卷积层通过增加特征图的通道数同时减少空间维度。每个卷积层后面还跟有BatchNorm和泄漏ReLU激活函数。最后一个卷积层使用大小为7×7的核和单个滤波器，将输出的空间维度减少到1×1×1。最后，卷积输出通过sigmoid函数并压缩为一维：
- en: '![](img/B17582_17_16.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_16.png)'
- en: 'Figure 17.16: The discriminator network'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.16：鉴别器网络
- en: '**Architecture design considerations for convolutional GANs**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积GAN的架构设计考虑**'
- en: Notice that the number of feature maps follows different trends between the
    generator and the discriminator. In the generator, we start with a large number
    of feature maps and decrease them as we progress toward the last layer. On the
    other hand, in the discriminator, we start with a small number of channels and
    increase it toward the last layer. This is an important point for designing CNNs
    with the number of feature maps and the spatial size of the feature maps in reverse
    order. When the spatial size of the feature maps increases, the number of feature
    maps decreases and vice versa.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，特征图的数量在生成器和鉴别器之间遵循不同的趋势。在生成器中，我们从大量的特征图开始，并随着向最后一层的进展而减少它们。另一方面，在鉴别器中，我们从少量的通道开始，并向最后一层增加它们。这是设计CNN时特征图数量和特征图空间尺寸按相反顺序的重要点。当特征图的空间尺寸增加时，特征图的数量减少，反之亦然。
- en: In addition, note that it’s usually not recommended to use bias units in the
    layer that follows a BatchNorm layer. Using bias units would be redundant in this
    case, since BatchNorm already has a shift parameter, ![](img/B17582_17_061.png).
    You can omit the bias units for a given layer by setting `bias=False` in `nn.ConvTranspose2d`
    or `nn.Conv2d`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，通常不建议在跟随BatchNorm层的层中使用偏置单元。在这种情况下，使用偏置单元将是多余的，因为BatchNorm已经有一个偏移参数，![](img/B17582_17_061.png)。您可以通过在`nn.ConvTranspose2d`或`nn.Conv2d`中设置`bias=False`来省略给定层的偏置单元。
- en: 'The code for the helper function to make the generator and the discriminator
    network class is as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建生成器和鉴别器网络类的辅助函数代码如下：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With the helper function and class, you can build a DCGAN model and train it
    by using the same MNIST dataset object we initialized in the previous section
    when we implemented the simple, fully connected GAN. We can create the generator
    networks using the helper function and print its architecture as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 借助辅助函数和类，您可以使用我们在实现简单全连接GAN时初始化的相同MNIST数据集对象构建DCGAN模型并训练它。我们可以使用辅助函数创建生成器网络并打印其架构如下：
- en: '[PRE14]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, we can generate the discriminator network and see its architecture:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以生成鉴别器网络并查看其架构：
- en: '[PRE15]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Also, we can use the same loss functions and optimizers as we did in the *Training
    the GAN model* subsection:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以像在*训练GAN模型*小节中一样使用相同的损失函数和优化器：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will be making a few small modifications to the training procedure. The
    `create_noise()` function for generating random input must change to output a
    tensor of four dimensions instead of a vector:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对训练过程进行几处小修改。用于生成随机输入的`create_noise()`函数必须更改为输出四维张量而不是向量：
- en: '[PRE17]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `d_train()` function for training the discriminator doesn’t need to reshape
    the input image:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练鉴别器的`d_train()`函数不需要重新调整输入图像：
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we will alternate between the training of the generator and the discriminator
    for 100 epochs. After each epoch, we will generate some examples from a fixed
    noise input using the current generator model by calling the `create_samples()`
    function. The code is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在100个时期内交替训练生成器和鉴别器。每个时期结束后，我们将使用当前生成器模型调用`create_samples()`函数从固定噪声输入生成一些示例。代码如下：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, let’s visualize the saved examples at some epochs to see how the model
    is learning and how the quality of synthesized examples changes over the course
    of learning:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在一些时期可视化保存的示例，以查看模型的学习情况及合成示例的质量如何随学习过程变化：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Figure 17.17* shows the results:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.17*显示了结果：'
- en: '![](img/B17582_17_17.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_17.png)'
- en: 'Figure 17.17: Generated images from the DCGAN'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.17：DCGAN生成的图像
- en: We used the same code to visualize the results as in the section on vanilla
    GAN. Comparing the new examples shows that DCGAN can generate images of a much
    higher quality.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与普通GAN部分相同的代码来可视化结果。比较新的示例表明，DCGAN能够生成质量更高的图像。
- en: 'You may wonder how we can evaluate the results of GAN generators. The simplest
    approach is visual assessment, which involves evaluating the quality of the synthesized
    images in the context of the target domain and the project objective. Furthermore,
    there have been several more sophisticated evaluation methods proposed that are
    less subjective and less limited by domain knowledge. For a detailed survey, see
    *Pros and Cons of GAN Evaluation Measures: New Developments* ([https://arxiv.org/abs/2103.09396](https://arxiv.org/abs/2103.09396)).
    The paper summarizes generator evaluation into qualitative and quantitative measures.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道我们如何评估GAN生成器的结果。最简单的方法是视觉评估，它涉及在目标域和项目目标的背景下评估合成图像的质量。此外，已经提出了几种更复杂的评估方法，这些方法不太主观，并且不受领域知识的限制。有关详细调查，请参阅*GAN评估指标的优缺点：新发展*（[https://arxiv.org/abs/2103.09396](https://arxiv.org/abs/2103.09396)）。该论文总结了生成器评估为定性和定量措施。
- en: There is a theoretical argument that training the generator should seek to minimize
    the dissimilarity between the distribution observed in the real data and the distribution
    observed in synthesized examples. Hence our current architecture would not perform
    very well when using cross-entropy as a loss function.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个理论论点认为，训练生成器应该致力于最小化真实数据观察到的分布与合成示例观察到的分布之间的差异。因此，当使用交叉熵作为损失函数时，我们当前的架构性能不会非常好。
- en: In the next subsection, we will cover WGAN, which uses a modified loss function
    based on the so-called Wasserstein-1 (or earth mover’s) distance between the distributions
    of real and fake images for improving the training performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将介绍WGAN，它使用基于所谓的Wasserstein-1（或地球运动者）距离的修改损失函数，用于改进训练性能。
- en: Dissimilarity measures between two distributions
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两个分布之间的差异度量
- en: We will first see different measures for computing the divergence between two
    distributions. Then, we will see which one of these measures is already embedded
    in the original GAN model. Finally, switching this measure in GANs will lead us
    to the implementation of a WGAN.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先会看到不同的度量方法来计算两个分布之间的差异。然后，我们将看到这些方法中哪些已经嵌入到原始GAN模型中。最后，通过在GAN中切换这种度量，我们将实现WGAN的实现。
- en: As mentioned at the beginning of this chapter, the goal of a generative model
    is to learn how to synthesize new samples that have the same distribution as the
    distribution of the training dataset. Let *P*(*x*) and *Q*(*x*) represent the
    distribution of a random variable, *x*, as shown in the following figure.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头提到的，生成模型的目标是学习如何合成具有与训练数据集分布相同分布的新样本。让*P*(*x*)和*Q*(*x*)代表随机变量*x*的分布，如下图所示。
- en: 'First, let’s look at some ways, shown in *Figure 17.18*, that we can use to
    measure the dissimilarity between two distributions, *P* and *Q*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看一些测量两个分布**P**和**Q**之间差异的方法，如*图17.18*所示：
- en: '![](img/B17582_17_18.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_18.png)'
- en: 'Figure 17.18: Methods to measure the dissimilarity between distributions *P*
    and *Q*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.18：测量分布*P*和*Q*之间差异的方法
- en: The function supremum, *sup*(*S*), used in the **total variation** (**TV**)
    measure, refers to the smallest value that is greater than all elements of *S*.
    In other words, *sup*(*S*) is the least upper bound for *S*. Vice versa, the infimum
    function, *inf*(*S*), which is used in EM distance, refers to the largest value
    that is smaller than all elements of *S* (the greatest lower bound).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在**总变差**（TV）度量中使用的上确界函数*sup*(*S*)，指的是大于*S*中所有元素的最小值。换句话说，*sup*(*S*)是*S*的最小上界。相反，用于EM距离中的下确界函数*inf*(*S*)，指的是小于*S*中所有元素的最大值（最大下界）。
- en: 'Let’s gain an understanding of these measures by briefly stating what they
    are trying to accomplish in simple words:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过简单的话语来了解这些度量试图实现什么：
- en: The first one, TV distance, measures the largest difference between the two
    distributions at each point.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个是总变差（TV）距离，它测量每个点处两个分布之间的最大差异。
- en: The EM distance can be interpreted as the minimal amount of work needed to transform
    one distribution into the other. The infimum function in the EM distance is taken
    over ![](img/B17582_17_062.png), which is the collection of all joint distributions
    whose marginals are *P* or *Q*. Then, ![](img/B17582_17_063.png) is a transfer
    plan, which indicates how we redistribute the earth from location *u* to *v*,
    subject to some constraints for maintaining valid distributions after such transfers.
    Computing EM distance is an optimization problem by itself, which is to find the
    optimal transfer plan, ![](img/B17582_17_063.png).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EM距离可以解释为将一个分布转换为另一个分布所需的最小工作量。在EM距离中，infimum函数取自![](img/B17582_17_062.png)，这是所有边缘为*P*或*Q*的联合分布的集合。然后，![](img/B17582_17_063.png)是一个转移计划，指示我们如何将地球从位置*u*转移到*v*，在进行这些转移后维持有效的分布约束条件。计算EM距离本身就是一个优化问题，即找到最优的转移计划![](img/B17582_17_063.png)。
- en: The **Kullback-Leibler** (**KL**) and **Jensen-Shannon** (**JS**) divergence
    measures come from the field of information theory. Note that KL divergence is
    not symmetric, that is, ![](img/B17582_17_065.png) in contrast to JS divergence.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kullback-Leibler**（**KL**）和**Jensen-Shannon**（**JS**）散度测量来自信息论领域。请注意，KL散度不是对称的，即，![](img/B17582_17_065.png)，相比之下，JS散度是对称的。'
- en: 'The dissimilarity equations provided in *Figure 17.18* correspond to continuous
    distributions but can be extended for discrete cases. An example of calculating
    these different dissimilarity measures with two simple discrete distributions
    is illustrated in *Figure 17.19*:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.18*中提供的不相似度方程对应于连续分布，但可以扩展到离散情况。一个计算两个简单离散分布的不同不相似度测量的示例如*图17.19*所示：'
- en: '![](img/B17582_17_19.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_19.png)'
- en: 'Figure 17.19: An example of calculating the different dissimilarity measures'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*图17.19*：计算不同不相似度测量的示例'
- en: Note that, in the case of the EM distance, for this simple example, we can see
    that *Q*(*x*) at *x* = 2 has the excess value of ![](img/B17582_17_066.png), while
    the value of *Q* at the other two *x*’s is below 1/3\. Therefore, the minimal
    amount of work is when we transfer the extra value at *x* = 2 to *x* = 1 and *x* = 3,
    as shown in *Figure 17.19*. For this simple example, it’s easy to see that these
    transfers will result in the minimal amount of work out of all possible transfers.
    However, this may be infeasible to do for more complex cases.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在EM距离的情况下，对于这个简单的例子，我们可以看到在*x*=2处，*Q*(*x*)具有超过![](img/B17582_17_066.png)的额外值，而在其他两个*x*的值则低于1/3。因此，在这个简单的例子中，将额外值从*x*=2转移到*x*=1和*x*=3会产生最小的工作量，如*图17.19*所示。然而，在更复杂的情况下，这可能是不可行的。
- en: '**The relationship between KL divergence and cross-entropy**'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**KL散度与交叉熵之间的关系**'
- en: 'KL divergence, ![](img/B17582_17_067.png), measures the relative entropy of
    the distribution, *P*, with respect to a reference distribution, *Q*. The formulation
    for KL divergence can be extended as:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度，![](img/B17582_17_067.png)，测量分布*P*相对于参考分布*Q*的相对熵。KL散度的表述可以扩展为：
- en: '![](img/B17582_17_068.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_068.png)'
- en: 'Moreover, for discrete distributions, KL divergence can be written as:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于离散分布，KL散度可以写成：
- en: '![](img/B17582_17_069.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_069.png)'
- en: 'which can be similarly extended as:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 可以类似地扩展为：
- en: '![](img/B17582_17_070.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_070.png)'
- en: Based on the extended formulation (either discrete or continuous), KL divergence
    is viewed as the cross-entropy between *P* and *Q* (the first term in the preceding
    equation) subtracted by the (self-) entropy of *P* (second term), that is, ![](img/B17582_17_071.png).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩展的表述（无论是离散还是连续），KL散度被视为*P*和*Q*之间的交叉熵（上述方程的第一项）减去*P*的（自）熵（第二项），即，![](img/B17582_17_071.png)。
- en: Now, going back to our discussion of GANs, let’s see how these different distance
    measures are related to the loss function for GANs. It can be mathematically shown
    that the loss function in the original GAN indeed *minimizes the JS divergence
    between the distribution of real and fake examples*. But, as discussed in an article
    by *Martin Arjovsky* and colleagues (*Wasserstein Generative Adversarial Networks*,
    [http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf](http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf)),
    JS divergence has problems training a GAN model, and therefore, in order to improve
    the training, the researchers proposed using the EM distance as a measure of dissimilarity
    between the distribution of real and fake examples.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们对GAN的讨论，让我们看看这些不同的距离度量与GAN的损失函数之间的关系。可以在数学上证明，在原始GAN中的损失函数确实*最小化了真实和虚假示例分布之间的JS散度*。但是，正如马丁·阿尔乔夫斯基及其同事在一篇文章中讨论的那样（《Wasserstein生成对抗网络》，[http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf](http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf)），JS散度在训练GAN模型时存在问题，因此为了改善训练，研究人员提出使用EM距离作为衡量真实和虚假示例分布不相似性的度量。
- en: '**What is the advantage of using EM distance?**'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用EM距离的优势是什么？**'
- en: To answer this question, we can consider an example that was given in the previously
    mentioned article by Martin Arjovsky and colleagues. To put it in simple words,
    assume we have two distributions, *P* and *Q*, which are two parallel lines. One
    line is fixed at *x* = 0 and the other line can move across the *x*-axis but is
    initially located at ![](img/B17582_17_072.png), where ![](img/B17582_17_073.png).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们可以考虑马丁·阿尔乔夫斯基及其同事在之前提到的文章中给出的一个例子。简单来说，假设我们有两个分布，*P*和*Q*，它们是两条平行线。一条线固定在*x*
    = 0处，另一条线可以沿着*x*轴移动，但最初位于![](img/B17582_17_072.png)，其中![](img/B17582_17_073.png)。
- en: It can be shown that the KL, TV, and JS dissimilarity measures are ![](img/B17582_17_074.png),
    ![](img/B17582_17_075.png), and ![](img/B17582_17_076.png). None of these dissimilarity
    measures are a function of the parameter ![](img/B17582_17_077.png), and therefore,
    they cannot be differentiated with respect to ![](img/B17582_17_077.png) toward
    making the distributions, *P* and *Q*, become similar to each other. On the other
    hand, the EM distance is ![](img/B17582_17_079.png), whose gradient with respect
    to ![](img/B17582_17_077.png) exists and can push *Q* toward *P*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明KL、TV和JS差异度量分别为![](img/B17582_17_074.png)、![](img/B17582_17_075.png)和![](img/B17582_17_076.png)。这些差异度量都不是参数![](img/B17582_17_077.png)的函数，因此不能相对于![](img/B17582_17_077.png)进行微分，以使分布*P*和*Q*相似。另一方面，EM距离是![](img/B17582_17_079.png)，其相对于![](img/B17582_17_077.png)的梯度存在并可以将*Q*推向*P*。
- en: 'Now, let’s focus our attention on how EM distance can be used to train a GAN
    model. Let’s assume *P*[r] is the distribution of the real examples and *P*[g]
    denotes the distributions of fake (generated) examples. *P*[r] and *P*[g] replace
    *P* and *Q* in the EM distance equation. As was mentioned earlier, computing the
    EM distance is an optimization problem by itself; therefore, this becomes computationally
    intractable, especially if we want to repeat this computation in each iteration
    of the GAN training loop. Fortunately, though, the computation of the EM distance
    can be simplified using a theorem called **Kantorovich-Rubinstein duality**, as
    follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于如何使用EM距离来训练GAN模型。假设*P*[r]是真实示例的分布，*P*[g]表示虚假（生成的）示例的分布。*P*[r]和*P*[g]在EM距离方程中替代*P*和*Q*。正如之前提到的，计算EM距离本身就是一个优化问题；因此，这在GAN训练循环的每次迭代中重复计算变得计算上难以处理。幸运的是，EM距离的计算可以通过称为**Kantorovich-Rubinstein对偶**的定理简化，如下所示：
- en: '![](img/B17582_17_081.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_081.png)'
- en: Here, the supremum is taken over all the *1-Lipschitz* continuous functions
    denoted by ![](img/B17582_17_082.png).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，最高值取自所有*1-Lipschitz*连续函数，表示为![](img/B17582_17_082.png)。
- en: '**Lipschitz continuity**'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lipschitz continuity**'
- en: 'Based on 1-Lipschitz continuity, the function, *f*, must satisfy the following
    property:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 基于1-Lipschitz连续性，函数*f*必须满足以下性质：
- en: '![](img/B17582_17_083.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_083.png)'
- en: Furthermore, a real function, *f*:*R*→*R*, that satisfies the property
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个实函数，*f*:*R*→*R*，满足以下性质
- en: '![](img/B17582_17_084.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_084.png)'
- en: is called **K-Lipschitz continuous**.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为**K-Lipschitz连续**。
- en: Using EM distance in practice for GANs
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在GAN实践中使用EM距离
- en: Now, the question is, how do we find such a 1-Lipschitz continuous function
    to compute the Wasserstein distance between the distribution of real (*P*[r])
    and fake (*P*[g]) outputs for a GAN? While the theoretical concepts behind the
    WGAN approach may seem complicated at first, the answer to this question is simpler
    than it may appear. Recall that we consider deep NNs to be universal function
    approximators. This means that we can simply train an NN model to approximate
    the Wasserstein distance function. As you saw in the previous section, the simple
    GAN uses a discriminator in the form of a classifier. For WGAN, the discriminator
    can be changed to behave as a *critic*, which returns a scalar score instead of
    a probability value. We can interpret this score as how realistic the input images
    are (like an art critic giving scores to artworks in a gallery).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们如何找到一个1-Lipschitz连续函数来计算GAN中真实（*P*[r]）和虚假（*P*[g]）输出分布之间的Wasserstein距离？虽然WGAN方法背后的理论概念乍看起来很复杂，但对于这个问题的答案比看起来要简单得多。回想一下我们将深度神经网络视为通用函数逼近器的观点。这意味着我们可以简单地训练一个神经网络模型来近似Wasserstein距离函数。正如你在前一节中看到的那样，简单的GAN使用的鉴别器是一个分类器。对于WGAN，鉴别器可以改变行为作为一个*评论家*，返回一个标量分数而不是概率值。我们可以将这个分数解释为输入图像的真实性（就像艺术评论家在画廊中为艺术品评分一样）。
- en: To train a GAN using the Wasserstein distance, the losses for the discriminator,
    *D*, and generator, *G*, are defined as follows. The critic (that is, the discriminator
    network) returns its outputs for the batch of real image examples and the batch
    of synthesized examples. We use the notations *D*(**x**) and *D*(*G*(**z**)),
    respectively.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Wasserstein距离训练GAN，定义鉴别器*D*和生成器*G*的损失如下。评论家（即鉴别器网络）返回其对于一批真实图像示例和合成示例的输出。我们使用*D*(**x**)和*D*(*G*(**z**))来表示。
- en: 'Then, the following loss terms can be defined:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以定义以下损失项：
- en: 'The real component of the discriminator’s loss:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器损失的实部：
- en: '![](img/B17582_17_085.png)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_085.png)'
- en: 'The fake component of the discriminator’s loss:'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器损失的虚假部分：
- en: '![](img/B17582_17_086.png)'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_086.png)'
- en: 'The loss for the generator:'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器的损失：
- en: '![](img/B17582_17_087.png)'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_087.png)'
- en: That will be all for the WGAN, except that we need to ensure that the 1-Lipschitz
    property of the critic function is preserved during training. For this purpose,
    the WGAN paper proposes clamping the weights to a small region, for example, [–0.01, 0.01].
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是关于WGAN的所有内容，除非我们需要确保评论家函数的1-Lipschitz性质在训练过程中得到保持。为此，WGAN论文建议将权重夹在一个小范围内，例如[–0.01, 0.01]。
- en: Gradient penalty
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度惩罚
- en: In the paper by Arjovsky and colleagues, weight clipping is suggested for the
    1-Lipschitz property of the discriminator (or critic). However, in another paper
    titled *Improved Training of Wasserstein GANs* by *Ishaan Gulrajani* and colleagues,
    2017, which is freely available at [https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf),
    Ishaan Gulrajani and colleagues showed that clipping the weights can lead to exploding
    and vanishing gradients. Furthermore, weight clipping can also lead to capacity
    underuse, which means that the critic network is limited to learning only some
    simple functions, as opposed to more complex functions. Therefore, rather than
    clipping the weights, Ishaan Gulrajani and colleagues proposed **gradient penalty**
    (**GP**) as an alternative solution. The result is the **WGAN with gradient penalty**
    (**WGAN-GP**).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在Arjovsky和同事的论文中，建议对鉴别器（或评论家）的1-Lipschitz性质进行权重修剪。然而，在另一篇名为*Ishaan Gulrajani*及同事的2017年的文章*Improved
    Training of Wasserstein GANs*中，可以免费获取[https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf)，Ishaan
    Gulrajani及同事指出，修剪权重可能导致梯度爆炸和消失。此外，权重修剪也可能导致能力未被充分利用，这意味着评论家网络仅限于学习一些简单的函数，而不是更复杂的函数。因此，Ishaan
    Gulrajani及同事提出了**梯度惩罚**（**GP**）作为替代解决方案。其结果是带有梯度惩罚的**WGAN**（**WGAN-GP**）。
- en: 'The procedure for the GP that is added in each iteration can be summarized
    by the following sequence of steps:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中添加的GP的过程可以总结如下步骤：
- en: For each pair of real and fake examples ![](img/B17582_17_088.png) in a given
    batch, choose a random number, ![](img/B17582_17_089.png), sampled from a uniform
    distribution, that is, ![](img/B17582_17_090.png).
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定批次中每对真实和虚假示例 ![](img/B17582_17_088.png)，选择从均匀分布中随机采样的随机数 ![](img/B17582_17_089.png)，即
    ![](img/B17582_17_090.png)。
- en: 'Calculate an interpolation between the real and fake examples: ![](img/B17582_17_091.png),
    resulting in a batch of interpolated examples.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算真实和虚假例子之间的插值：![](img/B17582_17_091.png)，得到一批插值例子。
- en: Compute the discriminator (critic) output for all the interpolated examples,
    ![](img/B17582_17_092.png).
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有插值例子的鉴别器（评论者）输出，![](img/B17582_17_092.png)。
- en: Calculate the gradients of the critic’s output with respect to each interpolated
    example, that is, ![](img/B17582_17_093.png).
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论者关于每个插值例子的输出的梯度，即![](img/B17582_17_093.png)。
- en: 'Compute the GP as:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算GP如下：
- en: '![](img/B17582_17_094.png)'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_17_094.png)'
- en: 'The total loss for the discriminator is then as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 那么鉴别器的总损失如下：
- en: '![](img/B17582_17_095.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_095.png)'
- en: Here, ![](img/B17582_03_040.png) is a tunable hyperparameter.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_03_040.png)是一个可调参数。
- en: Implementing WGAN-GP to train the DCGAN model
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施WGAN-GP来训练DCGAN模型
- en: 'We have already defined the helper function and class that create the generator
    and discriminator networks for DCGAN (`make_generator_network()` and `Discriminator()`).
    It is recommended to use layer normalization in WGAN instead of batch normalization.
    Layer normalization normalizes the inputs across features instead of across the
    batch dimension in batch normalization. The code to build the WGAN model is as
    follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了创建DCGAN生成器和鉴别器网络的辅助函数和类（`make_generator_network()`和`Discriminator()`）。建议在WGAN中使用层归一化而不是批归一化。层归一化在特征维度上归一化输入，而不是在批次维度上。
- en: '[PRE21]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can initiate the networks and their optimizers as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按以下方式初始化网络及其优化器：
- en: '[PRE22]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we will define the function to compute the GP component as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义计算GP组件的函数如下：
- en: '[PRE23]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The WGAN version of discriminator and generator training functions are as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN版本的鉴别器和生成器训练函数如下：
- en: '[PRE24]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then we will train the model for 100 epochs and record the generator output
    of a fixed noise input:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将模型训练100个epochs，并记录固定噪声输入的生成器输出：
- en: '[PRE25]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, let’s visualize the saved examples at some epochs to see how the WGAN
    model is learning and how the quality of synthesized examples changes over the
    course of learning. The following figure shows the results, which demonstrate
    slightly better image quality than what the DCGAN model generated:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在一些epochs时可视化保存的例子，看看WGAN模型如何学习以及合成例子的质量在学习过程中如何变化。下图显示了结果，显示出比DCGAN模型生成的图像质量稍好：
- en: '![](img/B17582_17_20.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_20.png)'
- en: 'Figure 17.20: Generated images using WGAN'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.20：使用WGAN生成的图像
- en: Mode collapse
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式崩溃
- en: Due to the adversarial nature of GAN models, it is notoriously hard to train
    them. One common cause of failure in training GANs is when the generator gets
    stuck in a small subspace and learns to generate similar samples. This is called
    **mode collapse**, and an example is shown in *Figure 17.21*.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GAN模型的对抗性质，训练它们非常困难。训练GAN失败的一个常见原因是生成器陷入了一个小的子空间并学会生成类似的样本。这被称为**模式崩溃**，并且在*图17.21*中有一个例子。
- en: 'The synthesized examples in this figure are not cherry-picked. This shows that
    the generator has failed to learn the entire data distribution, and instead, has
    taken a lazy approach focusing on a subspace:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图中的合成例子并非精选。这表明生成器未能学习整个数据分布，而是采取了一种懒惰的方法，集中在一个子空间上：
- en: '![](img/B17582_17_21.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_17_21.png)'
- en: 'Figure 17.21: An example of mode collapse'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.21：模式崩溃的示例
- en: Besides the vanishing and exploding gradient problems that we saw previously,
    there are some further aspects that can also make training GAN models difficult
    (indeed, it is an art). Here are a few suggested tricks from GAN artists.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前看到的梯度消失和梯度爆炸问题外，还有一些其他因素也会使得训练GAN模型变得困难（事实上，这是一门艺术）。以下是一些来自GAN艺术家的建议技巧。
- en: One approach is called **mini-batch discrimination**, which is based on the
    fact that batches consisting of only real or fake examples are fed separately
    to the discriminator. In mini-batch discrimination, we let the discriminator compare
    examples across these batches to see whether a batch is real or fake. The diversity
    of a batch consisting of only real examples is most likely higher than the diversity
    of a fake batch if a model suffers from mode collapse.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法称为 **小批量区分**，它基于一个事实：只有真实或虚假示例组成的批次被分别馈送给鉴别器。在小批量区分中，我们让鉴别器跨这些批次比较示例，以确定一个批次是真实还是虚假。如果模型遭遇模式崩溃，只有真实示例组成的批次的多样性很可能比虚假批次的多样性更高。
- en: Another technique that is commonly used for stabilizing GAN training is *feature
    matching*. In feature matching, we make a slight modification to the objective
    function of the generator by adding an extra term that minimizes the difference
    between the original and synthesized images based on intermediate representations
    (feature maps) of the discriminator. We encourage you to read more about this
    technique in the original article by *Ting-Chun Wang* and colleagues, titled *High
    Resolution Image Synthesis and Semantic Manipulation with Conditional GANs,* which
    is freely available at [https://arxiv.org/pdf/1711.11585.pdf](https://arxiv.org/pdf/1711.11585.pdf).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用于稳定 GAN 训练的技术是 *特征匹配*。在特征匹配中，我们通过向生成器的目标函数添加一个额外项，该项通过鉴别器的中间表示（特征图）来最小化原始图像与合成图像之间的差异。我们鼓励您阅读
    *王廷春* 及其同事撰写的原始文章 *High Resolution Image Synthesis and Semantic Manipulation with
    Conditional GANs*，可在 [https://arxiv.org/pdf/1711.11585.pdf](https://arxiv.org/pdf/1711.11585.pdf)
    免费获取。
- en: During the training, a GAN model can also get stuck in several modes and just
    hop between them. To avoid this behavior, you can store some old examples and
    feed them to the discriminator to prevent the generator from revisiting previous
    modes. This technique is referred to as *experience replay*. Furthermore, you
    can train multiple GANs with different random seeds so that the combination of
    all of them covers a larger part of the data distribution than any single one
    of them.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，GAN 模型可能会陷入多个模式中，并在它们之间跳跃。为了避免这种行为，您可以存储一些旧样本，并将它们馈送给鉴别器，以防止生成器重新访问先前的模式。这种技术被称为
    *经验回放*。此外，您还可以使用不同的随机种子训练多个 GAN 模型，使它们的组合覆盖数据分布的更大部分，而不是任何单个模型能够覆盖的部分。
- en: Other GAN applications
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他 GAN 应用
- en: In this chapter, we mainly focused on generating examples using GANs and looked
    at a few tricks and techniques to improve the quality of synthesized outputs.
    The applications of GANs are expanding rapidly, including in computer vision,
    machine learning, and even other domains of science and engineering. A nice list
    of different GAN models and application areas can be found at [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要关注使用 GAN 生成示例，并探讨了一些技巧和方法来提高合成输出的质量。GAN 的应用正在迅速扩展，包括计算机视觉、机器学习甚至其他科学和工程领域。您可以在
    [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)
    找到一个不错的 GAN 模型和应用领域的列表。
- en: It is worth mentioning that we covered GANs in an unsupervised fashion; that
    is, no class label information was used in the models that were covered in this
    chapter. However, the GAN approach can be generalized to semi-supervised and supervised
    tasks, as well. For example, the **conditional GAN** (**cGAN**) proposed by *Mehdi
    Mirza* and *Simon Osindero* in the paper *Conditional Generative Adversarial Nets*,
    2014 ([https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf))
    uses the class label information and learns to synthesize new images conditioned
    on the provided label, that is, ![](img/B17582_17_097.png)—applied to MNIST. This
    allows us to generate different digits in the range 0-9 selectively. Furthermore,
    conditional GANs allow us to do image-to-image translation, which is to learn
    how to convert a given image from a specific domain to another. In this context,
    one interesting work is the Pix2Pix algorithm, published in the paper *Image-to-Image
    Translation with Conditional Adversarial Networks* by *Philip Isola* and colleagues,
    2018 ([https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf)).
    It is worth mentioning that in the Pix2Pix algorithm, the discriminator provides
    the real/fake predictions for multiple patches across the image as opposed to
    a single prediction for an entire image.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，我们以无监督的方式讨论了 GAN；也就是说，在本章涵盖的模型中没有使用类标签信息。然而，GAN 方法可以推广到半监督和监督任务。例如，*Mehdi
    Mirza* 和 *Simon Osindero* 在论文《Conditional Generative Adversarial Nets》（2014）中提出的**条件
    GAN**（**cGAN**）使用类标签信息，并学习在给定标签条件下合成新图像，即，[https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)—应用于
    MNIST。这使我们能够有选择地生成 0-9 范围内的不同数字。此外，条件 GAN 还允许进行图像到图像的转换，即学习如何将给定域中的图像转换到另一个域中。在这个背景下，一个有趣的工作是
    Pix2Pix 算法，由 *Philip Isola* 和同事在2018年的论文《Image-to-Image Translation with Conditional
    Adversarial Networks》中发布（[https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf)）。值得一提的是，在
    Pix2Pix 算法中，鉴别器为图像中多个补丁提供真/假预测，而不是整个图像的单一预测。
- en: CycleGAN is another interesting GAN model built on top of the cGAN, also for
    image-to-image translation. However, note that in CycleGAN, the training examples
    from the two domains are unpaired, meaning that there is no one-to-one correspondence
    between inputs and outputs. For example, using a CycleGAN, we could change the
    season of a picture taken in summer to winter. In the paper *Unpaired Image-to-Image
    Translation Using Cycle-Consistent Adversarial Networks* by *Jun-Yan Zhu* and
    colleagues, 2020 ([https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)),
    an impressive example shows horses converted into zebras.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 是另一个建立在 cGAN 之上的有趣的 GAN 模型，也用于图像到图像的转换。然而，请注意，在 CycleGAN 中，来自两个域的训练示例是不配对的，这意味着输入和输出之间没有一对一的对应关系。例如，使用
    CycleGAN，我们可以将夏天拍摄的图片改变为冬天的景色。在 *Jun-Yan Zhu* 和同事于2020年的论文《Unpaired Image-to-Image
    Translation Using Cycle-Consistent Adversarial Networks》中展示了一个令人印象深刻的例子，展示了将马转换为斑马的过程（[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)）。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you first learned about generative models in deep learning
    and their overall objective: synthesizing new data. We then covered how GAN models
    use a generator network and a discriminator network, which compete with each other
    in an adversarial training setting to improve each other. Next, we implemented
    a simple GAN model using only fully connected layers for both the generator and
    the discriminator.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您首先学习了深度学习中生成模型及其总体目标：合成新数据。然后，我们讨论了 GAN 模型如何使用生成器网络和鉴别器网络，在对抗训练设置中相互竞争以改进彼此。接下来，我们实现了一个简单的
    GAN 模型，仅使用全连接层作为生成器和鉴别器。
- en: 'We also covered how GAN models can be improved. First, you saw a DCGAN, which
    uses deep convolutional networks for both the generator and the discriminator.
    Along the way, you also learned about two new concepts: transposed convolution
    (for upsampling the spatial dimensionality of feature maps) and BatchNorm (for
    improving convergence during training).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了如何改进 GAN 模型。首先，您看到了 DCGAN，它使用深度卷积网络作为生成器和鉴别器。在此过程中，您还学习了两个新概念：转置卷积（用于上采样特征映射的空间维度）和
    BatchNorm（用于在训练过程中改善收敛性）。
- en: We then looked at a WGAN, which uses the EM distance to measure the distance
    between the distributions of real and fake samples. Finally, we talked about the
    WGAN with GP to maintain the 1-Lipschitz property instead of clipping the weights.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们看了一个WGAN，它使用EM距离来衡量真实样本和假样本分布之间的距离。最后，我们讨论了带GP的WGAN，以维持1-Lipschitz属性，而不是修剪权重。
- en: In the next chapter, we will look at graph neural networks. Previously, we have
    been focused on tabular and image datasets. In contrast, graph neural networks
    are designed for graph-structured data, which allows us to work with datasets
    that are ubiquitous in social sciences, engineering, and biology. Popular examples
    of graph-structure data include social network graphs and molecules consisting
    of atoms connected by covalent bonds.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨图神经网络。之前，我们专注于表格和图像数据集。相比之下，图神经网络是为图结构数据设计的，这使我们能够处理社会科学、工程学和生物学中普遍存在的数据集。图结构数据的流行示例包括社交网络图和由共价键连接的原子组成的分子。
- en: Join our book’s Discord space
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的Discord工作区，每月进行*Ask me Anything*与作者的会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
