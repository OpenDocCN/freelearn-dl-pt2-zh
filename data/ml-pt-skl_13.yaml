- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Going Deeper – The Mechanics of PyTorch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨 – PyTorch的机制
- en: In *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, we covered
    how to define and manipulate tensors and worked with the `torch.utils.data` module
    to build input pipelines. We further built and trained a multilayer perceptron
    to classify the Iris dataset using the PyTorch neural network module (`torch.nn`).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第12章*中，*使用PyTorch并行化神经网络训练*，我们讨论了如何定义和操作张量，并使用`torch.utils.data`模块构建输入管道。我们进一步构建并训练了一个多层感知器，使用PyTorch神经网络模块（`torch.nn`）对鸢尾花数据集进行分类。
- en: Now that we have some hands-on experience with PyTorch neural network training
    and machine learning, it’s time to take a deeper dive into the PyTorch library
    and explore its rich set of features, which will allow us to implement more advanced
    deep learning models in upcoming chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一些关于PyTorch神经网络训练和机器学习的实践经验，是时候深入探索PyTorch库，并探索其丰富的功能集，这将使我们能够在即将到来的章节中实现更高级的深度学习模型。
- en: In this chapter, we will use different aspects of PyTorch’s API to implement
    NNs. In particular, we will again use the `torch.nn` module, which provides multiple
    layers of abstraction to make the implementation of standard architectures very
    convenient. It also allows us to implement custom NN layers, which is very useful
    in research-oriented projects that require more customization. Later in this chapter,
    we will implement such a custom layer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用PyTorch API的不同方面来实现神经网络。特别是，我们将再次使用`torch.nn`模块，它提供了多层抽象，使得实现标准架构非常方便。它还允许我们实现自定义神经网络层，这在需要更多定制的研究项目中非常有用。稍后在本章中，我们将实现这样一个自定义层。
- en: To illustrate the different ways of model building using the `torch.nn` module,
    we will also consider the classic **exclusive or** (**XOR**) problem. Firstly,
    we will build multilayer perceptrons using the `Sequential` class. Then, we will
    consider other methods, such as subclassing `nn.Module` for defining custom layers.
    Finally, we will work on two real-world projects that cover the machine learning
    steps from raw input to prediction.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明使用`torch.nn`模块构建模型的不同方法，我们还将考虑经典的**异或**（**XOR**）问题。首先，我们将使用`Sequential`类构建多层感知器。然后，我们将考虑其他方法，例如使用`nn.Module`子类化来定义自定义层。最后，我们将处理两个涵盖从原始输入到预测的机器学习步骤的真实项目。
- en: 'The topics that we will cover are as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的主题如下：
- en: Understanding and working with PyTorch computation graphs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并操作PyTorch计算图
- en: Working with PyTorch tensor objects
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch张量对象进行操作
- en: Solving the classic XOR problem and understanding model capacity
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决经典的XOR问题并理解模型容量
- en: Building complex NN models using PyTorch’s `Sequential` class and the `nn.Module`
    class
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch的`Sequential`类和`nn.Module`类构建复杂的神经网络模型
- en: Computing gradients using automatic differentiation and `torch.autograd`
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动微分和`torch.autograd`计算梯度
- en: The key features of PyTorch
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch的关键特性
- en: 'In the previous chapter, we saw that PyTorch provides us with a scalable, multiplatform
    programming interface for implementing and running machine learning algorithms.
    After its initial release in 2016 and its 1.0 release in 2018, PyTorch has evolved
    into one of the two most popular frameworks for deep learning. It uses dynamic
    computational graphs, which have the advantage of being more flexible compared
    to its static counterparts. Dynamic computational graphs are debugging friendly:
    PyTorch allows for interleaving the graph declaration and graph evaluation steps.
    You can execute the code line by line while having full access to all variables.
    This is a very important feature that makes the development and training of NNs
    very convenient.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们看到PyTorch为我们提供了一个可扩展的、跨平台的编程接口，用于实现和运行机器学习算法。在2016年的初始发布以及2018年的1.0版本发布之后，PyTorch已经发展成为两个最受欢迎的深度学习框架之一。它使用动态计算图，相比静态计算图具有更大的灵活性优势。动态计算图易于调试：PyTorch允许在图声明和图评估步骤之间交错执行代码。您可以逐行执行代码，同时完全访问所有变量。这是一个非常重要的功能，使得开发和训练神经网络非常方便。
- en: While PyTorch is an open-source library and can be used for free by everyone,
    its development is funded and supported by Facebook. This involves a large team
    of software engineers who expand and improve the library continuously. Since PyTorch
    is an open-source library, it also has strong support from other developers outside
    of Facebook, who avidly contribute and provide user feedback. This has made the
    PyTorch library more useful to both academic researchers and developers. A further
    consequence of these factors is that PyTorch has extensive documentation and tutorials
    to help new users.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 PyTorch 是一个开源库，可以免费使用，但其开发是由 Facebook 提供资金和支持的。这涉及到一个大型的软件工程团队，他们不断扩展和改进这个库。由于
    PyTorch 是一个开源库，它也得到了来自 Facebook 以外其他开发者的强大支持，他们积极贡献并提供用户反馈。这使得 PyTorch 库对学术研究人员和开发者都更加有用。由于这些因素的影响，PyTorch
    拥有广泛的文档和教程，帮助新用户上手。
- en: Another key feature of PyTorch, which was also noted in the previous chapter,
    is its ability to work with single or multiple **graphical processing units**
    (**GPUs**). This allows users to train deep learning models very efficiently on
    large datasets and large-scale systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的另一个关键特性，也在前一章节中提到过的，是其能够与单个或多个**图形处理单元**（**GPU**）一起工作。这使得用户能够在大型数据集和大规模系统上高效训练深度学习模型。
- en: Last but not least, PyTorch supports mobile deployment, which also makes it
    a very suitable tool for production.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，PyTorch 支持移动部署，这也使它成为生产环境中非常合适的工具。
- en: In the next section, we will look at how a tensor and function in PyTorch are
    interconnected via a computation graph.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到在 PyTorch 中张量和函数如何通过计算图相互连接。
- en: PyTorch’s computation graphs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 的计算图
- en: PyTorch performs its computations based on a **directed acyclic graph** (**DAG**).
    In this section, we will see how these graphs can be defined for a simple arithmetic
    computation. Then, we will see the dynamic graph paradigm, as well as how the
    graph is created on the fly in PyTorch.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 根据**有向无环图**（**DAG**）执行其计算。在本节中，我们将看到如何为简单的算术计算定义这些图。然后，我们将看到动态图的范例，以及如何在
    PyTorch 中动态创建图。
- en: Understanding computation graphs
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解计算图
- en: PyTorch relies on building a computation graph at its core, and it uses this
    computation graph to derive relationships between tensors from the input all the
    way to the output. Let’s say that we have rank 0 (scalar) tensors *a*, *b*, and
    *c* and we want to evaluate *z* = 2 × (*a* – *b*) + *c*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的核心是构建计算图，它依赖于这个计算图来推导从输入到输出的张量之间的关系。假设我们有秩为 0（标量）的张量 *a*、*b* 和 *c*，我们想要评估
    *z* = 2 × (*a* – *b*) + *c*。
- en: 'This evaluation can be represented as a computation graph, as shown in *Figure
    13.1*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此评估可以表示为一个计算图，如*图 13.1*所示：
- en: '![](img/B17582_13_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_01.png)'
- en: 'Figure 13.1: How a computation graph works'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：计算图的工作原理
- en: As you can see, the computation graph is simply a network of nodes. Each node
    resembles an operation, which applies a function to its input tensor or tensors
    and returns zero or more tensors as the output. PyTorch builds this computation
    graph and uses it to compute the gradients accordingly. In the next subsection,
    we will see some examples of creating a graph for this computation using PyTorch.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，计算图只是一个节点网络。每个节点类似于一个操作，它对其输入张量或张量应用函数，并根据需要返回零个或多个张量作为输出。PyTorch 构建这个计算图并使用它来相应地计算梯度。在下一小节中，我们将看到如何使用
    PyTorch 为这种计算创建图的一些示例。
- en: Creating a graph in PyTorch
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 PyTorch 中创建图
- en: 'Let’s look at a simple example that illustrates how to create a graph in PyTorch
    for evaluating *z* = 2 × (*a* – *b*) + *c*, as shown in the previous figure. The
    variables *a*, *b*, and *c* are scalars (single numbers), and we define these
    as PyTorch tensors. To create the graph, we can simply define a regular Python
    function with `a`, `b`, and `c` as its input arguments, for example:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个简单的例子，说明如何在 PyTorch 中创建一个用于评估 *z* = 2 × (*a* – *b*) + *c* 的图，如前图所示。变量
    *a*、*b* 和 *c* 是标量（单个数字），我们将它们定义为 PyTorch 张量。为了创建图，我们可以简单地定义一个常规的 Python 函数，其输入参数为
    `a`、`b` 和 `c`，例如：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, to carry out the computation, we can simply call this function with tensor
    objects as function arguments. Note that PyTorch functions such as `add`, `sub`
    (or `subtract`), and `mul` (or `multiply`) also allow us to provide inputs of
    higher ranks in the form of a PyTorch tensor object. In the following code example,
    we provide scalar inputs (rank 0), as well as rank 1 and rank 2 inputs, as lists:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了执行计算，我们可以简单地将此函数与张量对象作为函数参数调用。请注意，PyTorch函数如`add`、`sub`（或`subtract`）、`mul`（或`multiply`）也允许我们以PyTorch张量对象的形式提供更高秩的输入。在以下代码示例中，我们提供了标量输入（秩0），以及秩1和秩2的输入，作为列表：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this section, you saw how simple it is to create a computation graph in PyTorch.
    Next, we will look at PyTorch tensors that can be used for storing and updating
    model parameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，你看到了在PyTorch中创建计算图是多么简单。接下来，我们将看看可以用来存储和更新模型参数的PyTorch张量。
- en: PyTorch tensor objects for storing and updating model parameters
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch张量对象用于存储和更新模型参数
- en: 'We covered tensor objects in *Chapter 12*, *Parallelizing Neural Network Training
    with PyTorch*. In PyTorch, a special tensor object for which gradients need to
    be computed allows us to store and update the parameters of our models during
    training. Such a tensor can be created by just assigning `requires_grad` to `True`
    on user-specified initial values. Note that as of now (mid-2021), only tensors
    of floating point and complex `dtype` can require gradients. In the following
    code, we will generate tensor objects of type `float32`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第12章*《使用PyTorch并行化神经网络训练》中介绍了张量对象。在PyTorch中，需要计算梯度的特殊张量对象允许我们在训练期间存储和更新模型的参数。这样的张量可以通过在用户指定的初始值上简单地将`requires_grad`赋值为`True`来创建。请注意，截至目前（2021年中），只有浮点和复杂`dtype`的张量可以需要梯度。在以下代码中，我们将生成`float32`类型的张量对象：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that `requires_grad` is set to `False` by default. This value can be
    efficiently set to `True` by running `requires_grad_()`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，默认情况下`requires_grad`被设置为`False`。可以通过运行`requires_grad_()`有效地将其设置为`True`。
- en: '`method_()` is an in-place method in PyTorch that is used for operations without
    making a copy of the input.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`method_()`是PyTorch中的一个原地方法，用于执行操作而不复制输入。'
- en: 'Let’s take a look at the following example:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下例子：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You will recall that for NN models, initializing model parameters with random
    weights is necessary to break the symmetry during backpropagation—otherwise, a
    multilayer NN would be no more useful than a single-layer NN like logistic regression.
    When creating a PyTorch tensor, we can also use a random initialization scheme.
    PyTorch can generate random numbers based on a variety of probability distributions
    (see [https://pytorch.org/docs/stable/torch.html#random-sampling](https://pytorch.org/docs/stable/torch.html#random-sampling)).
    In the following example, we will take a look at some standard initialization
    methods that are also available in the `torch.nn.init` module (see [https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得，对于NN模型，使用随机权重初始化模型参数是必要的，以打破反向传播过程中的对称性——否则，一个多层NN将不会比单层NN（如逻辑回归）更有用。在创建PyTorch张量时，我们也可以使用随机初始化方案。PyTorch可以基于各种概率分布生成随机数（参见[https://pytorch.org/docs/stable/torch.html#random-sampling](https://pytorch.org/docs/stable/torch.html#random-sampling)）。在以下例子中，我们将看一些标准的初始化方法，这些方法也可以在`torch.nn.init`模块中找到（参见[https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)）。
- en: 'So, let’s look at how we can create a tensor with Glorot initialization, which
    is a classic random initialization scheme that was proposed by Xavier Glorot and
    Yoshua Bengio. For this, we first create an empty tensor and an operator called
    `init` as an object of class `GlorotNormal`. Then, we fill this tensor with values
    according to the Glorot initialization by calling the `xavier_normal_()` method.
    In the following example, we initialize a tensor of shape 2×3:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看如何使用Glorot初始化创建一个张量，这是一种经典的随机初始化方案，由Xavier Glorot和Yoshua Bengio提出。为此，我们首先创建一个空张量和一个名为`init`的操作符，作为`GlorotNormal`类的对象。然后，通过调用`xavier_normal_()`方法按照Glorot初始化填充这个张量的值。在下面的例子中，我们初始化一个形状为2×3的张量：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Xavier (or Glorot) initialization**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xavier（或Glorot）初始化**'
- en: In the early development of deep learning, it was observed that random uniform
    or random normal weight initialization could often result in poor model performance
    during training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的早期开发中观察到，随机均匀或随机正态的权重初始化通常会导致训练过程中模型表现不佳。
- en: In 2010, Glorot and Bengio investigated the effect of initialization and proposed
    a novel, more robust initialization scheme to facilitate the training of deep
    networks. The general idea behind Xavier initialization is to roughly balance
    the variance of the gradients across different layers. Otherwise, some layers
    may get too much attention during training while the other layers lag behind.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2010年，Glorot和Bengio调查了初始化的效果，并提出了一种新颖、更健壮的初始化方案，以促进深层网络的训练。Xavier初始化背后的主要思想是大致平衡不同层次梯度的方差。否则，一些层可能在训练过程中受到过多关注，而其他层则滞后。
- en: 'According to the research paper by Glorot and Bengio, if we want to initialize
    the weights in a uniform distribution, we should choose the interval of this uniform
    distribution as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Glorot和Bengio的研究论文，如果我们想要在均匀分布中初始化权重，我们应该选择此均匀分布的区间如下：
- en: '![](img/B17582_13_001.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_001.png)'
- en: 'Here, *n*[in] is the number of input neurons that are multiplied by the weights,
    and *n*[out] is the number of output neurons that feed into the next layer. For
    initializing the weights from Gaussian (normal) distribution, we recommend that
    you choose the standard deviation of this Gaussian to be:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n*[in] 是与权重相乘的输入神经元的数量，*n*[out] 是输入到下一层的输出神经元的数量。对于从高斯（正态）分布初始化权重，我们建议您选择这个高斯分布的标准差为：
- en: '![](img/B17582_13_002.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_002.png)'
- en: PyTorch supports Xavier initialization in both uniform and normal distributions
    of weights.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch支持在权重的均匀分布和正态分布中进行**Xavier初始化**。
- en: For more information about Glorot and Bengio’s initialization scheme, including
    the rationale and mathematical motivation, we recommend the original paper (*Understanding
    the difficulty of deep feedforward neural networks*, *Xavier Glorot* and *Yoshua
    Bengio*, 2010), which is freely available at [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Glorot和Bengio初始化方案的更多信息，包括其背景和数学动机，我们建议查阅原始论文（*理解深层前馈神经网络的难度*，*Xavier Glorot*
    和 *Yoshua Bengio*，2010），可以在 [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
    免费获取。
- en: 'Now, to put this into the context of a more practical use case, let’s see how
    we can define two `Tensor` objects inside the base `nn.Module` class:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将其放入更实际的用例背景中，让我们看看如何在基础 `nn.Module` 类内定义两个 `Tensor` 对象：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These two tensors can be then used as weights whose gradients will be computed
    via automatic differentiation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以将这两个张量用作权重，其梯度将通过自动微分计算。
- en: Computing gradients via automatic differentiation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自动微分计算梯度
- en: As you already know, optimizing NNs requires computing the gradients of the
    loss with respect to the NN weights. This is required for optimization algorithms
    such as **stochastic gradient descent** (**SGD**). In addition, gradients have
    other applications, such as diagnosing the network to find out why an NN model
    is making a particular prediction for a test example. Therefore, in this section,
    we will cover how to compute gradients of a computation with respect to its input
    variables.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您已经知道的那样，优化神经网络需要计算损失相对于神经网络权重的梯度。这对于优化算法如**随机梯度下降**（**SGD**）是必需的。此外，梯度还有其他应用，比如诊断网络以找出为什么神经网络模型对测试示例做出特定预测。因此，在本节中，我们将涵盖如何计算计算的梯度对其输入变量的梯度。
- en: Computing the gradients of the loss with respect to trainable variables
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算损失相对于可训练变量的梯度
- en: PyTorch supports *automatic differentiation*, which can be thought of as an
    implementation of the *chain rule* for computing gradients of nested functions.
    Note that for the sake of simplicity, we will use the term *gradient* to refer
    to both partial derivatives and gradients.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch支持*自动微分*，可以将其视为计算嵌套函数梯度的链式规则的实现。请注意，出于简化的目的，我们将使用术语*梯度*来指代偏导数和梯度。
- en: '**Partial derivatives and gradients**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏导数和梯度**'
- en: 'A partial derivative ![](img/B17582_13_003.png) can be understood as the rate
    of change of a multivariate function—a function with multiple inputs, *f*(*x*[1], *x*[2], ...),
    with respect to one of its inputs (here: *x*[1]). The gradient, ![](img/B17582_13_004.png),
    of a function is a vector composed of all the inputs’ partial derivatives, ![](img/B17582_13_005.png).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 部分导数 ![](img/B17582_13_003.png) 可以理解为多变量函数（具有多个输入 *f*(*x*[1], *x*[2], ...）相对于其输入之一（此处为
    *x*[1]）的变化率。函数的梯度，![](img/B17582_13_004.png)，是由所有输入的偏导数 ![](img/B17582_13_005.png)
    组成的向量。
- en: When we define a series of operations that results in some output or even intermediate
    tensors, PyTorch provides a context for calculating gradients of these computed
    tensors with respect to its dependent nodes in the computation graph. To compute
    these gradients, we can call the `backward` method from the `torch.autograd` module.
    It computes the sum of gradients of the given tensor with regard to leaf nodes
    (terminal nodes) in the graph.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义一系列操作以产生某些输出甚至是中间张量时，PyTorch提供了一个计算梯度的上下文，用于计算这些计算张量相对于其在计算图中依赖节点的梯度。要计算这些梯度，我们可以从`torch.autograd`模块调用`backward`方法。它计算给定张量相对于图中叶节点（终端节点）的梯度之和。
- en: 'Let’s work with a simple example where we will compute *z* = *wx* + *b* and
    define the loss as the squared loss between the target *y* and prediction *z*,
    *Loss* = (*y* – *z*)². In the more general case, where we may have multiple predictions
    and targets, we compute the loss as the sum of the squared error, ![](img/B17582_13_006.png).
    In order to implement this computation in PyTorch, we will define the model parameters,
    *w* and *b*, as variables (tensors with the `requires_gradient` attribute set
    to `True`), and the input, *x* and *y*, as default tensors. We will compute the
    loss tensor and use it to compute the gradients of the model parameters, *w* and
    *b*,as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的例子，我们将计算 *z* = *wx* + *b* 并定义损失为目标 *y* 和预测 *z* 之间的平方损失，*Loss* = (*y*
    - *z*)²。在更一般的情况下，我们可能有多个预测和目标，我们将损失定义为平方误差的总和，![](img/B17582_13_006.png)。为了在PyTorch中实现这个计算，我们将定义模型参数
    *w* 和 *b* 为变量（具有`requires_gradient`属性设置为`True`的张量），输入 *x* 和 *y* 为默认张量。我们将计算损失张量并用它来计算模型参数
    *w* 和 *b* 的梯度，如下所示：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Computing the value *z* is a forward pass in an NN. We used the `backward`
    method on the `loss` tensor to compute ![](img/B17582_13_007.png) and ![](img/B17582_13_008.png).
    Since this is a very simple example, we can obtain ![](img/B17582_13_009.png)
    symbolically to verify that the computed gradients match the results we obtained
    in the previous code example:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 计算值*z*是NN中的前向传递。我们在`loss`张量上使用`backward`方法来计算 ![](img/B17582_13_007.png) 和 ![](img/B17582_13_008.png)。由于这是一个非常简单的例子，我们可以通过符号方式获得
    ![](img/B17582_13_009.png) 来验证计算得到的梯度与我们在先前的代码示例中得到的结果是否匹配：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We leave the verification of *b* as an exercise for the reader.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们留下对*b*的验证作为读者的练习。
- en: Understanding automatic differentiation
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解自动微分
- en: 'Automatic differentiation represents a set of computational techniques for
    computing gradients of arbitrary arithmetic operations. During this process, gradients
    of a computation (expressed as a series of operations) are obtained by accumulating
    the gradients through repeated applications of the chain rule. To better understand
    the concept behind automatic differentiation, let’s consider a series of nested
    computations, *y* = *f*(*g*(*h*(*x*))), with input *x* and output *y*. This can
    be broken into a series of steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分表示一组用于计算任意算术操作梯度的计算技术。在这个过程中，通过重复应用链式法则来积累计算（表示为一系列操作）的梯度。为了更好地理解自动微分背后的概念，让我们考虑一系列嵌套计算，*y*
    = *f*(*g*(*h*(*x*)))，其中 *x* 是输入，*y* 是输出。这可以分解为一系列步骤：
- en: '*u*[0] = *x*'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*[0] = *x*'
- en: '*u*[1] = *h*(*x*)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*[1] = *h*(*x*)'
- en: '*u*[2] = *g*(*u*[1])'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*[2] = *g*(*u*[1])'
- en: '*u*[3] = *f*(*u*[2]) = *y*'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*u*[3] = *f*(*u*[2]) = *y*'
- en: 'The derivative ![](img/B17582_13_010.png) can be computed in two different
    ways: forward accumulation, which starts with ![](img/B17582_13_011.png), and
    reverse accumulation, which starts with ![](img/B17582_13_012.png). Note that
    PyTorch uses the latter, reverse accumulation, which is more efficient for implementing
    backpropagation.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 导数 ![](img/B17582_13_010.png) 可以通过两种不同的方式计算：前向累积，从 ![](img/B17582_13_011.png)
    开始，以及反向累积，从 ![](img/B17582_13_012.png) 开始。请注意，PyTorch使用后者，即反向累积，这对于实现反向传播更有效率。
- en: Adversarial examples
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗样本
- en: Computing gradients of the loss with respect to the input example is used for
    generating *adversarial examples* (or *adversarial attacks*). In computer vision,
    adversarial examples are examples that are generated by adding some small, imperceptible
    noise (or perturbations) to the input example, which results in a deep NN misclassifying
    them. Covering adversarial examples is beyond the scope of this book, but if you
    are interested, you can find the original paper by *Christian Szegedy et al.*,
    *Intriguing properties of neural networks* at [https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失相对于输入示例的梯度用于生成*对抗样本*（或*对抗攻击*）。在计算机视觉中，对抗样本是通过向输入示例添加一些微小且难以察觉的噪声（或扰动）生成的示例，导致深度神经网络误分类它们。涵盖对抗样本超出了本书的范围，但如果您感兴趣，可以在
    [https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf) 找到*Christian
    Szegedy et al.*的原始论文*神经网络的有趣属性*。
- en: Simplifying implementations of common architectures via the torch.nn module
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 torch.nn 模块简化常见架构的实现
- en: You have already seen some examples of building a feedforward NN model (for
    instance, a multilayer perceptron) and defining a sequence of layers using the
    `nn.Module` class. Before we take a deeper dive into `nn.Module`, let’s briefly
    look at another approach for conjuring those layers via `nn.Sequential`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到了构建前馈 NN 模型（例如，多层感知器）和使用 `nn.Module` 类定义层序列的一些示例。在我们深入研究 `nn.Module` 之前，让我们简要了解另一种通过
    `nn.Sequential` 配置这些层的方法。
- en: Implementing models based on nn.Sequential
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 `nn.Sequential` 实现模型
- en: 'With `nn.Sequential` ([https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#sequential](https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#sequential)),
    the layers stored inside the model are connected in a cascaded way. In the following
    example, we will build a model with two densely (fully) connected layers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `nn.Sequential` ([https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#sequential](https://pytorch.org/docs/master/generated/torch.nn.Sequential.html#sequential))，模型内部存储的层以级联方式连接。在下面的示例中，我们将构建一个具有两个全连接层的模型：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We specified the layers and instantiated the `model` after passing the layers
    to the `nn.Sequential` class. The output of the first fully connected layer is
    used as the input to the first ReLU layer. The output of the first ReLU layer
    becomes the input for the second fully connected layer. Finally, the output of
    the second fully connected layer is used as the input to the second ReLU layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了层并在将这些层传递给 `nn.Sequential` 类后实例化了 `model`。第一个全连接层的输出作为第一个 ReLU 层的输入。第一个
    ReLU 层的输出成为第二个全连接层的输入。最后，第二个全连接层的输出作为第二个 ReLU 层的输入。
- en: 'We can further configure these layers, for example, by applying different activation
    functions, initializers, or regularization methods to the parameters. A comprehensive
    and complete list of available options for most of these categories can be found
    in the official documentation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用不同的激活函数、初始化器或正则化方法来进一步配置这些层的参数。大多数这些类别的所有可用选项的详细和完整列表可以在官方文档中找到：
- en: 'Choosing activation functions: [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择激活函数：[https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
- en: 'Initializing the layer parameters via `nn.init`: [https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `nn.init` 初始化层参数：[https://pytorch.org/docs/stable/nn.init.html](https://pytorch.org/docs/stable/nn.init.html)
- en: 'Applying L2 regularization to the layer parameters (to prevent overfitting)
    via the parameter `weight_decay` of some optimizers in `torch.optim`: [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在 `torch.optim` 中某些优化器的 `weight_decay` 参数应用 L2 正则化到层参数（以防止过拟合）：[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)
- en: Applying L1 regularization to the layer parameters (to prevent overfitting)
    by adding the L1 penalty term to the loss tensor, which we will implement next
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将 L1 正则化应用于层参数（以防止过拟合），通过将 L1 惩罚项添加到损失张量中实现下一步
- en: 'In the following code example, we will configure the first fully connected
    layer by specifying the initial value distribution for the weight. Then, we will
    configure the second fully connected layer by computing the L1 penalty term for
    the weight matrix:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们将通过指定权重的初始值分布来配置第一个全连接层。然后，我们将通过计算权重矩阵的L1惩罚项来配置第二个全连接层：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we initialized the weight of the first linear layer with Xavier initialization.
    And we computed the L1 norm of the weight of the second linear layer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Xavier初始化来初始化第一个线性层的权重。然后，我们计算了第二个线性层权重的L1范数。
- en: 'Furthermore, we can also specify the type of optimizer and the loss function
    for training. Again, a comprehensive list of all available options can be found
    in the official documentation:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以指定训练的优化器类型和损失函数。再次，您可以在官方文档中找到所有可用选项的全面列表。
- en: 'Optimizers via `torch.optim`: [https://pytorch.org/docs/stable/optim.html#algorithms](https://pytorch.org/docs/stable/optim.html#algorithms)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`torch.optim`优化器：[https://pytorch.org/docs/stable/optim.html#algorithms](https://pytorch.org/docs/stable/optim.html#algorithms)
- en: 'Loss functions: [https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数：[https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)
- en: Choosing a loss function
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择损失函数
- en: Regarding the choices for optimization algorithms, SGD and Adam are the most
    widely used methods. The choice of loss function depends on the task; for example,
    you might use mean square error loss for a regression problem.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化算法的选择，SGD和Adam是最常用的方法。损失函数的选择取决于任务；例如，对于回归问题，您可能会使用均方误差损失。
- en: The family of cross-entropy loss functions supplies the possible choices for
    classification tasks, which are extensively discussed in *Chapter 14*, *Classifying
    Images with Deep Convolutional Neural Networks*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失函数系列提供了分类任务的可能选择，在*第14章*，*使用深度卷积神经网络分类图像*中广泛讨论。
- en: Furthermore, you can use the techniques you have learned from previous chapters
    (such as techniques for model evaluation from *Chapter 6*, *Learning Best Practices
    for Model Evaluation and Hyperparameter Tuning*) combined with the appropriate
    metrics for the problem. For example, precision and recall, accuracy, **area under
    the curve** (**AUC**), and false negative and false positive scores are appropriate
    metrics for evaluating classification models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以结合适用于问题的适当指标，利用您从先前章节学到的技术（如*第6章*中用于模型评估和超参数调优的技术）。例如，精度和召回率、准确率、**曲线下面积**（**AUC**）、假阴性和假阳性分数是评估分类模型的适当指标。
- en: 'In this example, we will use the SGD optimizer, and cross-entropy loss for
    binary classification:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用SGD优化器和交叉熵损失进行二元分类：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we will look at a more practical example: solving the classic XOR classification
    problem. First, we will use the `nn.Sequential()` class to build the model. Along
    the way, you will also learn about the capacity of a model for handling nonlinear
    decision boundaries. Then, we will cover building a model via `nn.Module` that
    will give us more flexibility and control over the layers of the network.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一个更实际的例子：解决经典的XOR分类问题。首先，我们将使用`nn.Sequential()`类来构建模型。在此过程中，您还将了解模型处理非线性决策边界的能力。然后，我们将讨论通过`nn.Module`构建模型，这将为我们提供更多灵活性和对网络层的控制。
- en: Solving an XOR classification problem
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决XOR分类问题
- en: 'The XOR classification problem is a classic problem for analyzing the capacity
    of a model with regard to capturing the nonlinear decision boundary between two
    classes. We generate a toy dataset of 200 training examples with two features
    (*x*[0], *x*[1]) drawn from a uniform distribution between [–1, 1). Then, we assign
    the ground truth label for training example *i* according to the following rule:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: XOR分类问题是分析模型捕捉两类之间非线性决策边界能力的经典问题。我们生成了一个包含200个训练样本的玩具数据集，具有两个特征（*x*[0]，*x*[1]），这些特征从均匀分布[-1,
    1)中抽取。然后，根据以下规则为训练样本*i*分配了地面真实标签：
- en: '![](img/B17582_13_013.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_013.png)'
- en: 'We will use half of the data (100 training examples) for training and the remaining
    half for validation. The code for generating the data and splitting it into the
    training and validation datasets is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一半的数据（100个训练样本）进行训练，剩余一半用于验证。生成数据并将其拆分为训练和验证数据集的代码如下：
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code results in the following scatterplot of the training and validation
    examples, shown with different markers based on their class label:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码生成了训练和验证样本的散点图，根据它们的类别标签使用不同的标记：
- en: '![](img/B17582_13_02.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_02.png)'
- en: 'Figure 13.2: Scatterplot of training and validation examples'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：训练和验证样本的散点图
- en: 'In the previous subsection, we covered the essential tools that we need to
    implement a classifier in PyTorch. We now need to decide what architecture we
    should choose for this task and dataset. As a general rule of thumb, the more
    layers we have, and the more neurons we have in each layer, the larger the capacity
    of the model will be. Here, the model capacity can be thought of as a measure
    of how readily the model can approximate complex functions. While having more
    parameters means the network can fit more complex functions, larger models are
    usually harder to train (and prone to overfitting). In practice, it is always
    a good idea to start with a simple model as a baseline, for example, a single-layer
    NN like logistic regression:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的小节中，我们介绍了在PyTorch中实现分类器所需的基本工具。现在我们需要决定为这个任务和数据集选择什么样的架构。作为一个经验法则，我们拥有的层次越多，每层的神经元越多，模型的容量就越大。在这里，模型容量可以被看作是模型能够逼近复杂函数的能力的一个度量。虽然更多参数意味着网络可以拟合更复杂的函数，但更大的模型通常更难训练（且容易过拟合）。实际操作中，从一个简单模型开始作为基准总是一个好主意，例如单层神经网络如逻辑回归：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After defining the model, we will initialize the cross-entropy loss function
    for binary classification and the SGD optimizer:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型后，我们将初始化用于二元分类的交叉熵损失函数和SGD优化器：
- en: '[PRE13]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will create a data loader that uses a batch size of 2 for the train
    data:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个数据加载器，使用批大小为2来处理训练数据：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we will train the model for 200 epochs and record a history of training
    epochs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练模型200个epoch，并记录训练过程的历史：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that the history of training epochs includes the train loss and validation
    loss and the train accuracy and validation accuracy, which is useful for visual
    inspection after training. In the following code, we will plot the learning curves,
    including the training and validation loss, as well as their accuracies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练epoch的历史包括训练损失和验证损失，以及训练准确率和验证准确率，这对于训练后的可视化检查非常有用。在下面的代码中，我们将绘制学习曲线，包括训练和验证损失，以及它们的准确率。
- en: 'The following code will plot the training performance:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将绘制训练性能：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following figure, with two separate panels for the losses
    and accuracies:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了下图，分别显示了损失和准确率的两个单独面板：
- en: '![](img/B17582_13_03.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_03.png)'
- en: 'Figure 13.3: Loss and accuracy results'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：损失和准确率结果
- en: As you can see, a simple model with no hidden layer can only derive a linear
    decision boundary, which is unable to solve the XOR problem. As a consequence,
    we can observe that the loss terms for both the training and the validation datasets
    are very high, and the classification accuracy is very low.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，一个没有隐藏层的简单模型只能得出线性决策边界，无法解决XOR问题。因此，我们可以观察到，训练集和验证集的损失项都非常高，分类准确率非常低。
- en: To derive a nonlinear decision boundary, we can add one or more hidden layers
    connected via nonlinear activation functions. The universal approximation theorem
    states that a feedforward NN with a single hidden layer and a relatively large
    number of hidden units can approximate arbitrary continuous functions relatively
    well. Thus, one approach for tackling the XOR problem more satisfactorily is to
    add a hidden layer and compare different numbers of hidden units until we observe
    satisfactory results on the validation dataset. Adding more hidden units would
    correspond to increasing the width of a layer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到非线性决策边界，我们可以添加一个或多个通过非线性激活函数连接的隐藏层。普遍逼近定理表明，具有单个隐藏层和相对较大隐藏单元数的前馈神经网络可以相对良好地逼近任意连续函数。因此，更有效地解决XOR问题的一种方法是添加一个隐藏层，并比较不同数量的隐藏单元，直到在验证数据集上观察到满意的结果。增加更多的隐藏单元相当于增加层的宽度。
- en: Alternatively, we can also add more hidden layers, which will make the model
    deeper. The advantage of making a network deeper rather than wider is that fewer
    parameters are required to achieve a comparable model capacity.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以添加更多隐藏层，这会使模型变得更深。将网络变得更深而不是更宽的优势在于，需要的参数更少，即使实现相似的模型容量。
- en: However, a downside of deep (versus wide) models is that deep models are prone
    to vanishing and exploding gradients, which make them harder to train.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与宽模型相比，深模型的一个缺点是，深模型容易出现梯度消失和梯度爆炸，这使得它们更难训练。
- en: 'As an exercise, try adding one, two, three, and four hidden layers, each with
    four hidden units. In the following example, we will take a look at the results
    of a feedforward NN with two hidden layers:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，请尝试添加一层、两层、三层和四层，每层都有四个隐藏单元。在下面的例子中，我们将查看具有两个隐藏层的前馈神经网络的结果：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can repeat the previous code for visualization, which produces the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重复之前的代码进行可视化，产生以下结果：
- en: '![](img/B17582_13_04.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_04.png)'
- en: 'Figure 13.4: Loss and accuracy results after adding two hidden layers'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：添加两个隐藏层后的损失和准确率结果
- en: Now, we can see that the model is able to derive a nonlinear decision boundary
    for this data, and the model reaches 100 percent accuracy on the training dataset.
    The validation dataset’s accuracy is 95 percent, which indicates that the model
    is slightly overfitting.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到该模型能够为这些数据推导出非线性决策边界，并且该模型在训练数据集上达到100%的准确率。验证数据集的准确率为95%，这表明模型略微过拟合。
- en: Making model building more flexible with nn.Module
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用nn.Module使模型构建更加灵活
- en: In the previous example, we used the PyTorch `Sequential` class to create a
    fully connected NN with multiple layers. This is a very common and convenient
    way of building models. However, it unfortunately doesn’t allow us to create more
    complex models that have multiple input, output, or intermediate branches. That’s
    where `nn.Module` comes in handy.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们使用了PyTorch的`Sequential`类来创建一个具有多层的全连接神经网络。这是一种非常常见和方便的建模方式。然而，不幸的是，它不允许我们创建具有多个输入、输出或中间分支的复杂模型。这就是`nn.Module`派上用场的地方。
- en: 'The alternative way to build complex models is by subclassing `nn.Module`.
    In this approach, we create a new class derived from `nn.Module` and define the
    method, `__init__()`, as a constructor. The `forward()` method is used to specify
    the forward pass. In the constructor function, `__init__()`, we define the layers
    as attributes of the class so that they can be accessed via the `self` reference
    attribute. Then, in the `forward()` method, we specify how these layers are to
    be used in the forward pass of the NN. The code for defining a new class that
    implements the previous model is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 构建复杂模型的另一种方法是通过子类化`nn.Module`来实现。在这种方法中，我们创建一个派生自`nn.Module`的新类，并将`__init__()`方法定义为构造函数。`forward()`方法用于指定前向传播。在构造函数`__init__()`中，我们将层定义为类的属性，以便可以通过`self`引用属性访问这些层。然后，在`forward()`方法中，我们指定这些层在神经网络的前向传播中如何使用。定义实现前述模型的新类的代码如下：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Notice that we put all layers in the `nn.ModuleList` object, which is just a
    `list` object composed of `nn.Module` items. This makes the code more readable
    and easier to follow.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将所有层放在`nn.ModuleList`对象中，这只是由`nn.Module`项组成的`list`对象。这样做可以使代码更易读，更易于理解。
- en: 'Once we define an instance of this new class, we can train it as we did previously:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了这个新类的实例，我们就可以像之前一样对其进行训练：
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, besides the train history, we will use the mlxtend library to visualize
    the validation data and the decision boundary.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，除了训练历史记录外，我们将使用mlxtend库来可视化验证数据和决策边界。
- en: 'Mlxtend can be installed via `conda` or `pip` as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式通过`conda`或`pip`安装mlxtend：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To compute the decision boundary of our model, we need to add a `predict()`
    method in the `MyModule` class:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算我们模型的决策边界，我们需要在`MyModule`类中添加一个`predict()`方法：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It will return the predicted class (0 or 1) for a sample.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它将为样本返回预测类（0或1）。
- en: 'The following code will plot the training performance along with the decision
    region bias:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将绘制训练性能以及决策区域偏差：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This results in *Figure 13.5*, with three separate panels for the losses, accuracies,
    and the scatterplot of the validation examples, along with the decision boundary:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致*图 13.5*，其中包括损失、准确率的三个独立面板以及验证示例的散点图，以及决策边界：
- en: '![](img/B17582_13_05.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_05.png)'
- en: 'Figure 13.5: Results, including a scatterplot'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：包括散点图在内的结果
- en: Writing custom layers in PyTorch
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中编写自定义层
- en: In cases where we want to define a new layer that is not already supported by
    PyTorch, we can define a new class derived from the `nn.Module` class. This is
    especially useful when designing a new layer or customizing an existing layer.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们想定义一个PyTorch尚未支持的新层的情况下，我们可以定义一个新类，该类派生自`nn.Module`类。当设计新层或自定义现有层时，这尤其有用。
- en: To illustrate the concept of implementing custom layers, let’s consider a simple
    example. Imagine we want to define a new linear layer that computes ![](img/B17582_13_014.png),
    where ![](img/B17582_13_015.png) refers to a random variable as a noise variable.
    To implement this computation, we define a new class as a subclass of `nn.Module`.
    For this new class, we have to define both the constructor `__init__()` method
    and the `forward()` method. In the constructor, we define the variables and other
    required tensors for our customized layer. We can create variables and initialize
    them in the constructor if the `input_size` is given to the constructor. Alternatively,
    we can delay the variable initialization (for instance, if we do not know the
    exact input shape upfront) and delegate it to another method for late variable
    creation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明实现自定义层的概念，让我们考虑一个简单的例子。假设我们想定义一个新的线性层，计算![](img/B17582_13_014.png)，其中![](img/B17582_13_015.png)表示作为噪声变量的随机变量。为了实现这个计算，我们定义一个新的类作为`nn.Module`的子类。对于这个新类，我们必须定义构造函数`__init__()`方法和`forward()`方法。在构造函数中，我们为自定义层定义变量和其他所需的张量。如果构造函数提供了`input_size`，我们可以在构造函数中创建变量并初始化它们。或者，我们可以延迟变量初始化（例如，如果我们事先不知道确切的输入形状），并将其委托给另一个方法进行延迟变量创建。
- en: 'To look at a concrete example, we are going to define a new layer called `NoisyLinear`,
    which implements the computation ![](img/B17582_13_016.png), which was mentioned
    in the preceding paragraph:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看一个具体的例子，我们将定义一个名为`NoisyLinear`的新层，实现了在前述段落中提到的计算![](img/B17582_13_016.png)：
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Before we go a step further and use our custom `NoisyLinear` layer in a model,
    let’s test it in the context of a simple example.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步使用我们自定义的`NoisyLinear`层将其应用于模型之前，让我们在一个简单示例的背景下测试它。
- en: 'In the following code, we will define a new instance of this layer, and execute
    it on an input tensor. Then, we will call the layer three times on the same input
    tensor:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将定义此层的新实例，并在输入张量上执行它。然后，我们将在相同的输入张量上三次调用该层：
- en: '[PRE24]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that the outputs for the first two calls differ because the `NoisyLinear`
    layer added random noise to the input tensor. The third call outputs [0, 0] as
    we didn’t add noise by specifying `training=False`.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，前两次调用的输出不同，因为`NoisyLinear`层向输入张量添加了随机噪声。第三次调用输出[0, 0]，因为我们通过指定`training=False`未添加噪声。
- en: 'Now, let’s create a new model similar to the previous one for solving the XOR
    classification task. As before, we will use the `nn.Module` class for model building,
    but this time, we will use our `NoisyLinear` layer as the first hidden layer of
    the multilayer perceptron. The code is as follows:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个类似于以前用于解决XOR分类任务的新模型。与之前一样，我们将使用`nn.Module`类构建模型，但这次我们将把我们的`NoisyLinear`层作为多层感知机的第一个隐藏层。代码如下：
- en: '[PRE25]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Similarly, we will train the model as we did previously. At this time, to compute
    the prediction on the training batch, we use `pred = model(x_batch, True)[:, 0]`
    instead of `pred = model(x_batch)[:, 0]`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，我们将像以前一样训练模型。此时，为了在训练批次上计算预测值，我们使用`pred = model(x_batch, True)[:, 0]`而不是`pred
    = model(x_batch)[:, 0]`：
- en: '[PRE26]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After the model is trained, we can plot the losses, accuracies, and the decision
    boundary:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型后，我们可以绘制损失、准确率和决策边界：
- en: '[PRE27]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The resulting figure will be as follows:![](img/B17582_13_06.png)
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果图如下：![](img/B17582_13_06.png)
- en: 'Figure 13.6: Results using NoisyLinear as the first hidden layer'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图13.6：使用NoisyLinear作为第一个隐藏层的结果
- en: Here, our goal was to learn how to define a new custom layer subclassed from
    `nn.Module` and to use it as we would use any other standard `torch.nn` layer.
    Although, with this particular example, `NoisyLinear` did not help to improve
    the performance, please keep in mind that our objective was to mainly learn how
    to write a customized layer from scratch. In general, writing a new customized
    layer can be useful in other applications, for example, if you develop a new algorithm
    that depends on a new layer beyond the existing ones.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的目标是学习如何定义一个新的自定义层，这个层是从`nn.Module`子类化而来，并且如同使用任何其他标准`torch.nn`层一样使用它。虽然在这个特定的例子中，`NoisyLinear`并没有帮助提高性能，请记住我们的主要目标是学习如何从头开始编写一个定制层。一般来说，编写一个新的自定义层在其他应用中可能会很有用，例如，如果您开发了一个依赖于现有层之外的新层的新算法。
- en: Project one – predicting the fuel efficiency of a car
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目一 – 预测汽车的燃油效率
- en: So far, in this chapter, we have mostly focused on the `torch.nn` module. We
    used `nn.Sequential` to construct the models for simplicity. Then, we made model
    building more flexible with `nn.Module` and implemented feedforward NNs, to which
    we added customized layers. In this section, we will work on a real-world project
    of predicting the fuel efficiency of a car in miles per gallon (MPG). We will
    cover the underlying steps in machine learning tasks, such as data preprocessing,
    feature engineering, training, prediction (inference), and evaluation.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们主要集中在`torch.nn`模块上。我们使用`nn.Sequential`来简化模型的构建。然后，我们使用`nn.Module`使模型构建更加灵活，并实现了前馈神经网络，其中我们添加了定制层。在本节中，我们将致力于一个真实世界的项目，即预测汽车的每加仑英里数（MPG）燃油效率。我们将涵盖机器学习任务的基本步骤，如数据预处理、特征工程、训练、预测（推理）和评估。
- en: Working with feature columns
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理特征列
- en: 'In machine learning and deep learning applications, we can encounter various
    different types of features: continuous, unordered categorical (nominal), and
    ordered categorical (ordinal). You will recall that in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*, we covered different types of features
    and learned how to handle each type. Note that while numeric data can be either
    continuous or discrete, in the context of machine learning with PyTorch, “numeric”
    data specifically refers to continuous data of floating point type.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和深度学习应用中，我们可能会遇到各种不同类型的特征：连续、无序分类（名义）和有序分类（序数）。您会回忆起，在*第 4 章*，*构建良好的训练数据集
    – 数据预处理*中，我们涵盖了不同类型的特征，并学习了如何处理每种类型。请注意，虽然数值数据可以是连续的或离散的，但在使用 PyTorch 进行机器学习的上下文中，“数值”数据特指浮点类型的连续数据。
- en: 'Sometimes, feature sets are comprised of a mixture of different feature types.
    For example, consider a scenario with a set of seven different features, as shown
    in *Figure 13.7*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，特征集合由不同类型的特征混合组成。例如，考虑一个具有七种不同特征的情景，如*图 13.7*所示：
- en: '![](img/B17582_13_07.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_07.png)'
- en: 'Figure 13.7: Auto MPG data structure'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7：汽车 MPG 数据结构
- en: The features shown in the figure (model year, cylinders, displacement, horsepower,
    weight, acceleration, and origin) were obtained from the Auto MPG dataset, which
    is a common machine learning benchmark dataset for predicting the fuel efficiency
    of a car in MPG. The full dataset and its description are available from UCI’s
    machine learning repository at [https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示的特征（车型年份、汽缸数、排量、马力、重量、加速度和起源）来自汽车 MPG 数据集，这是一个常见的用于预测汽车燃油效率的机器学习基准数据集。完整数据集及其描述可从UCI的机器学习存储库获取：[https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg)。
- en: We are going to treat five features from the Auto MPG dataset (number of cylinders,
    displacement, horsepower, weight, and acceleration) as “numeric” (here, continuous)
    features. The model year can be regarded as an ordered categorical (ordinal) feature.
    Lastly, the manufacturing origin can be regarded as an unordered categorical (nominal)
    feature with three possible discrete values, 1, 2, and 3, which correspond to
    the US, Europe, and Japan, respectively.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从汽车 MPG 数据集中处理五个特征（汽缸数、排量、马力、重量和加速度），将它们视为“数值”（这里是连续）特征。车型年份可以被视为有序分类（序数）特征。最后，制造地可以被视为无序分类（名义）特征，具有三个可能的离散值，1、2和3，分别对应于美国、欧洲和日本。
- en: 'Let’s first load the data and apply the necessary preprocessing steps, including
    dropping the incomplete rows, partitioning the dataset into training and test
    datasets, as well as standardizing the continuous features:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先加载数据并应用必要的预处理步骤，包括删除不完整的行，将数据集分为训练和测试数据集，以及标准化连续特征：
- en: '[PRE28]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下结果：
- en: '![](img/B17582_13_08.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_08.png)'
- en: 'Figure 13.8: Preprocessed Auto MG data'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：经过预处理的Auto MG数据
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, let’s group the rather fine-grained model year (`ModelYear`) information
    into buckets to simplify the learning task for the model that we are going to
    train later. Concretely, we are going to assign each car into one of four *year*
    buckets, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将相当精细的模型年份（`ModelYear`）信息分组到桶中，以简化我们稍后将要训练的模型的学习任务。具体来说，我们将每辆车分配到四个*年份*桶中，如下所示：
- en: '![](img/B17582_13_019.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_13_019.png)'
- en: 'Note that the chosen intervals were selected arbitrarily to illustrate the
    concepts of “bucketing.” In order to group the cars into these buckets, we will
    first define three cut-off values: [73, 76, 79] for the model year feature. These
    cut-off values are used to specify half-closed intervals, for instance, (–∞, 73),
    [73, 76), [76, 79), and [76, ∞). Then, the original numeric features will be passed
    to the `torch.bucketize` function ([https://pytorch.org/docs/stable/generated/torch.bucketize.html](https://pytorch.org/docs/stable/generated/torch.bucketize.html))
    to generate the indices of the buckets. The code is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，所选的间隔是任意选择的，用于说明“分桶”的概念。为了将车辆分组到这些桶中，我们首先定义三个截断值：[73, 76, 79]，用于模型年份特征。这些截断值用于指定半开区间，例如，(-∞,
    73)，[73, 76)，[76, 79)，和[76, ∞)。然后，原始数值特征将传递给`torch.bucketize`函数（[https://pytorch.org/docs/stable/generated/torch.bucketize.html](https://pytorch.org/docs/stable/generated/torch.bucketize.html)）来生成桶的索引。代码如下：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We added this bucketized feature column to the Python list `numeric_column_names`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此分桶特征列添加到Python列表`numeric_column_names`中。
- en: 'Next, we will proceed with defining a list for the unordered categorical feature,
    `Origin`. In PyTorch, There are two ways to work with a categorical feature: using
    an embedding layer via `nn.Embedding` ([https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)),
    or using one-hot-encoded vectors (also called *indicator*). In the encoding approach,
    for example, index 0 will be encoded as [1, 0, 0], index 1 will be encoded as
    [0, 1, 0], and so on. On the other hand, the embedding layer maps each index to
    a vector of random numbers of the type `float`, which can be trained. (You can
    think of the embedding layer as a more efficient implementation of a one-hot encoding
    multiplied with a trainable weight matrix.)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续定义一个无序分类特征`Origin`的列表。在PyTorch中，处理分类特征有两种方法：使用通过`nn.Embedding`（[https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)）实现的嵌入层，或者使用独热编码向量（也称为*指示器*）。在编码方法中，例如，索引0将被编码为[1,
    0, 0]，索引1将被编码为[0, 1, 0]，依此类推。另一方面，嵌入层将每个索引映射到一组随机数的向量，类型为`float`，可以进行训练。（您可以将嵌入层视为与可训练权重矩阵相乘的一种更有效的独热编码实现。）
- en: When the number of categories is large, using the embedding layer with fewer
    dimensions than the number of categories can improve the performance.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别数量较多时，使用比类别数量少的嵌入层维度可以提高性能。
- en: 'In the following code snippet, we will use the one-hot-encoding approach on
    the categorical feature in order to convert it into the dense format:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们将使用独热编码方法处理分类特征，以便将其转换为密集格式：
- en: '[PRE31]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After encoding the categorical feature into a three-dimensional dense feature,
    we concatenated it with the numeric features we processed in the previous step.
    Finally, we will create the label tensors from the ground truth MPG values as
    follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类特征编码为三维密集特征后，我们将其与前一步骤中处理的数值特征串联起来。最后，我们将从地面实际MPG值创建标签张量如下：
- en: '[PRE32]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In this section, we have covered the most common approaches for preprocessing
    and creating features in PyTorch.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了PyTorch中预处理和创建特征的最常见方法。
- en: Training a DNN regression model
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练DNN回归模型
- en: 'Now, after constructing the mandatory features and labels, we will create a
    data loader that uses a batch size of 8 for the train data:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在构建必需的特征和标签之后，我们将创建一个数据加载器，用于训练数据的批量大小为8：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will build a model with two fully connected layers where one has 8
    hidden units and another has 4:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将建立一个具有两个全连接层的模型，其中一个具有8个隐藏单元，另一个具有4个：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After defining the model, we will define the MSE loss function for regression
    and use stochastic gradient descent for optimization:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型后，我们将为回归定义MSE损失函数，并使用随机梯度下降进行优化：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we will train the model for 200 epochs and display the train loss for every
    20 epochs:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练模型200个epoch，并在每20个epoch显示训练损失：
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After 200 epochs, the train loss was around 5\. We can now evaluate the regression
    performance of the trained model on the test dataset. To predict the target values
    on new data points, we can feed their features to the model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 经过200个epoch后，训练损失约为5。现在我们可以在测试数据集上评估训练模型的回归性能。为了预测新数据点上的目标值，我们可以将它们的特征提供给模型：
- en: '[PRE37]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The MSE on the test set is 9.6, and the **mean absolute error** (**MAE**) is
    2.1\. After this regression project, we will work on a classification project
    in the next section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的MSE为9.6，**平均绝对误差**（**MAE**）为2.1。完成此回归项目后，我们将在下一部分进行分类项目。
- en: Project two – classifying MNIST handwritten digits
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目二 – 对MNIST手写数字进行分类
- en: For this classification project, we are going to categorize MNIST handwritten
    digits. In the previous section, we covered the four essential steps for machine
    learning in PyTorch in detail, which we will need to repeat in this section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个分类项目，我们将对MNIST手写数字进行分类。在前一部分中，我们详细介绍了PyTorch中机器学习的四个基本步骤，我们将在本节中重复这些步骤。
- en: You will recall that in *Chapter 12* you learned the way of loading available
    datasets from the `torchvision` module. First, we are going to load the MNIST
    dataset using the `torchvision` module.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您会记得，在*第12章*中，您学习了如何从`torchvision`模块中加载可用的数据集的方法。首先，我们将使用`torchvision`模块加载MNIST数据集。
- en: 'The setup step includes loading the dataset and specifying hyperparameters
    (the size of the train set and test set, and the size of mini-batches):'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置步骤包括加载数据集并指定超参数（训练集和测试集的大小，以及小批量的大小）：
- en: '[PRE38]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here, we constructed a data loader with batches of 64 samples. Next, we will
    preprocess the loaded datasets.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们构建了一个每批64个样本的数据加载器。接下来，我们将预处理加载的数据集。
- en: We preprocess the input features and the labels. The features in this project
    are the pixels of the images we read from **Step 1**. We defined a custom transformation
    using `torchvision.transforms.Compose`. In this simple case, our transformation
    consisted only of one method, `ToTensor()`. The `ToTensor()` method converts the
    pixel features into a floating type tensor and also normalizes the pixels from
    the [0, 255] to [0, 1] range. In *Chapter 14*, *Classifying Images with Deep Convolutional
    Neural Networks*, we will see some additional data transformation methods when
    we work with more complex image datasets. The labels are integers from 0 to 9
    representing ten digits. Hence, we don’t need to do any scaling or further conversion.
    Note that we can access the raw pixels using the `data` attribute, and don’t forget
    to scale them to the range [0, 1].
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们预处理输入特征和标签。在这个项目中，特征是我们从**第1步**读取的图像的像素。我们使用`torchvision.transforms.Compose`定义了一个自定义转换。在这种简单情况下，我们的转换仅包括一个方法`ToTensor()`。`ToTensor()`方法将像素特征转换为浮点型张量，并将像素从[0, 255]范围归一化到[0, 1]范围。在*第14章*，*使用深度卷积神经网络对图像进行分类*中，当我们处理更复杂的图像数据集时，我们将看到一些额外的数据转换方法。标签是从0到9的整数，表示十个数字。因此，我们不需要进行任何缩放或进一步的转换。请注意，我们可以使用`data`属性访问原始像素，并不要忘记将它们缩放到[0, 1]范围内。
- en: We will construct the model in the next step once the data is preprocessed.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在数据预处理完成后，我们将在下一步构建模型。
- en: 'Construct the NN model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建神经网络模型：
- en: '[PRE39]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that the model starts with a flatten layer that flattens an input image
    into a one-dimensional tensor. This is because the input images are in the shape
    of [1, 28, 28]. The model has two hidden layers, with 32 and 16 units respectively.
    And it ends with an output layer of ten units representing ten classes, activated
    by a softmax function. In the next step, we will train the model on the train
    set and evaluate it on the test set.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，模型以一个展平层开始，将输入图像展平为一维张量。这是因为输入图像的形状是[1, 28, 28]。模型有两个隐藏层，分别为32和16个单元。最后是一个由十个单元组成的输出层，通过softmax函数激活，代表十个类别。在下一步中，我们将在训练集上训练模型，并在测试集上评估模型。
- en: 'Use the model for training, evaluation, and prediction:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型进行训练、评估和预测：
- en: '[PRE40]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We used the cross-entropy loss function for multiclass classification and the
    Adam optimizer for gradient descent. We will talk about the Adam optimizer in
    *Chapter 14*. We trained the model for 20 epochs and displayed the train accuracy
    for every epoch. The trained model reached an accuracy of 96.3 percent on the
    training set and we will evaluate it on the testing set:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了交叉熵损失函数进行多类别分类，使用Adam优化器进行梯度下降。我们将在*第14章*讨论Adam优化器。我们对模型进行了20个epochs的训练，并且在每个epoch显示了训练准确率。训练后的模型在训练集上达到了96.3%的准确率，并且我们将在测试集上进行评估：
- en: '[PRE41]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The test accuracy is 95.6 percent. You have learned how to solve a classification
    problem using PyTorch.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率为95.6%。您已经学会了如何使用PyTorch解决分类问题。
- en: 'Higher-level PyTorch APIs: a short introduction to PyTorch-Lightning'
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更高级别的PyTorch API：简介PyTorch-Lightning
- en: In recent years, the PyTorch community developed several different libraries
    and APIs on top of PyTorch. Notable examples include fastai ([https://docs.fast.ai/](https://docs.fast.ai/)),
    Catalyst ([https://github.com/catalyst-team/catalyst](https://github.com/catalyst-team/catalyst)),
    PyTorch Lightning ([https://www.pytorchlightning.ai](https://www.pytorchlightning.ai)),
    ([https://lightning-flash.readthedocs.io/en/latest/quickstart.html](https://lightning-flash.readthedocs.io/en/latest/quickstart.html)),
    and PyTorch-Ignite ([https://github.com/pytorch/ignite](https://github.com/pytorch/ignite)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，PyTorch社区开发了几个不同的库和API，这些库和API都是基于PyTorch构建的。值得注意的例子包括fastai ([https://docs.fast.ai/](https://docs.fast.ai/))、Catalyst
    ([https://github.com/catalyst-team/catalyst](https://github.com/catalyst-team/catalyst))、PyTorch
    Lightning ([https://www.pytorchlightning.ai](https://www.pytorchlightning.ai))、([https://lightning-flash.readthedocs.io/en/latest/quickstart.html](https://lightning-flash.readthedocs.io/en/latest/quickstart.html))以及PyTorch-Ignite
    ([https://github.com/pytorch/ignite](https://github.com/pytorch/ignite))。
- en: In this section, we will explore PyTorch Lightning (Lightning for short), which
    is a widely used PyTorch library that makes training deep neural networks simpler
    by removing much of the boilerplate code. However, while Lightning’s focus lies
    in simplicity and flexibility, it also allows us to use many advanced features
    such as multi-GPU support and fast low-precision training, which you can learn
    about in the official documentation at [https://pytorch-lightning.rtfd.io/en/latest/](https://pytorch-lightning.rtfd.io/en/latest/).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨PyTorch Lightning（简称Lightning），这是一个广泛使用的PyTorch库，通过消除大量样板代码，使训练深度神经网络变得更加简单。然而，尽管Lightning专注于简单性和灵活性，它也允许我们使用许多高级功能，例如多GPU支持和快速低精度训练，您可以在官方文档中了解更多信息：[https://pytorch-lightning.rtfd.io/en/latest/](https://pytorch-lightning.rtfd.io/en/latest/)。
- en: There is also a bonus introduction to PyTorch-Ignite at [https://github.com/rasbt/machine-learning-book/blob/main/ch13/ch13_part4_ignite.ipynb](https://github.com/rasbt/machine-learning-book/blob/main/ch13/ch13_part4_ignite.ipynb).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个关于PyTorch-Ignite的额外介绍在[https://github.com/rasbt/machine-learning-book/blob/main/ch13/ch13_part4_ignite.ipynb](https://github.com/rasbt/machine-learning-book/blob/main/ch13/ch13_part4_ignite.ipynb)。
- en: In an earlier section, *Project two – classifying MNIST handwritten digits*,
    we implemented a multilayer perceptron for classifying handwritten digits in the
    MNIST dataset. In the next subsections, we will reimplement this classifier using
    Lightning.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一个章节中，*项目二 - 分类MNIST手写数字*，我们实现了一个多层感知器，用于在MNIST数据集中分类手写数字。在接下来的小节中，我们将使用Lightning重新实现这个分类器。
- en: '**Installing PyTorch Lightning**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装PyTorch Lightning**'
- en: 'Lightning can be installed via pip or conda, depending on your preference.
    For instance, the command for installing Lightning via pip is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据喜好通过pip或conda安装Lightning。例如，通过pip安装Lightning的命令如下：
- en: '[PRE42]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following is the command for installing Lightning via conda:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过conda安装Lightning的命令：
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The code in the following subsections is based on PyTorch Lightning version
    1.5, which you can install by replacing `pytorch-lightning` with `pytorch-lightning==1.5`
    in these commands.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节的代码基于PyTorch Lightning 1.5版本，您可以通过在命令中将`pytorch-lightning`替换为`pytorch-lightning==1.5`来安装它。
- en: Setting up the PyTorch Lightning model
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置PyTorch Lightning模型
- en: 'We start by implementing the model, which we will train in the next subsections.
    Defining a model for Lightning is relatively straightforward as it is based on
    regular Python and PyTorch code. All that is required to implement a Lightning
    model is to use `LightningModule` instead of the regular PyTorch module. To take
    advantage of PyTorch’s convenience functions, such as the trainer API and automatic
    logging, we just define a few specifically named methods, which we will see in
    the following code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实现模型，在接下来的子节中将对其进行训练。在 Lightning 中定义模型相对简单，因为它基于常规的 Python 和 PyTorch 代码。要实现
    Lightning 模型，只需使用 `LightningModule` 替代常规的 PyTorch 模块即可。为了利用 PyTorch 的便利函数，如训练器
    API 和自动日志记录，我们只需定义几个特定命名的方法，我们将在接下来的代码中看到：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Let’s now discuss the different methods one by one. As you can see, the `__init__`
    constructor contains the same model code that we used in a previous subsection.
    What’s new is that we added the accuracy attributes such as `self.train_acc =
    Accuracy()`. These will allow us to track the accuracy during training. `Accuracy`
    was imported from the `torchmetrics` module, which should be automatically installed
    with Lightning. If you cannot import `torchmetrics`, you can try to install it
    via `pip install torchmetrics`. More information can be found at [https://torchmetrics.readthedocs.io/en/latest/pages/quickstart.html](https://torchmetrics.readthedocs.io/en/latest/pages/quickstart.html).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐一讨论不同的方法。正如你所见，`__init__` 构造函数包含了我们在之前子节中使用的相同模型代码。新的内容是我们添加了诸如 `self.train_acc
    = Accuracy()` 等准确性属性。这些属性将允许我们在训练过程中跟踪准确性。`Accuracy` 是从 `torchmetrics` 模块导入的，它应该会随着
    Lightning 的自动安装而被安装。如果无法导入 `torchmetrics`，你可以尝试通过 `pip install torchmetrics` 进行安装。更多信息可以在
    [https://torchmetrics.readthedocs.io/en/latest/pages/quickstart.html](https://torchmetrics.readthedocs.io/en/latest/pages/quickstart.html)
    找到。
- en: The `forward` method implements a simple forward pass that returns the logits
    (outputs of the last fully connected layer of our network before the softmax layer)
    when we call our model on the input data. The logits, computed via the `forward`
    method by calling `self(x)`, are used for the training, validation, and test steps,
    which we’ll describe next.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 方法实现了一个简单的前向传递，当我们在输入数据上调用模型时，返回 logit（我们网络中 softmax 层之前的最后一个全连接层的输出）。通过调用
    `self(x)` 的 `forward` 方法计算的 logit 用于训练、验证和测试步骤，我们将在接下来描述这些步骤。'
- en: The `training_step`, `training_epoch_end`, `validation_step`, `test_step`, and
    `configure_optimizers` methods are methods that are specifically recognized by
    Lightning. For instance, `training_step` defines a single forward pass during
    training, where we also keep track of the accuracy and loss so that we can analyze
    these later. Note that we compute the accuracy via `self.train_acc.update(preds,
    y)` but don’t log it yet. The `training_step` method is executed on each individual
    batch during training, and via the `training_epoch_end` method, which is executed
    at the end of each training epoch, we compute the training set accuracy from the
    accuracy values we accumulated via training.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`training_step`、`training_epoch_end`、`validation_step`、`test_step` 和 `configure_optimizers`
    方法是 Lightning 特别识别的方法。例如，`training_step` 定义了训练期间的单次前向传递，我们在此期间跟踪准确性和损失，以便稍后进行分析。请注意，我们通过
    `self.train_acc.update(preds, y)` 计算准确性，但尚未记录。`training_step` 方法在训练过程中每个单独的批次上执行，而
    `training_epoch_end` 方法在每个训练周期结束时执行，我们通过累积的准确性值计算训练集准确性。'
- en: The `validation_step` and `test_step` methods define, analogous to the `training_step`
    method, how the validation and test evaluation process should be computed. Similar
    to `training_step`, each `validation_step` and `test_step` receives a single batch,
    which is why we log the accuracy via respective accuracy attributes derived from
    `Accuracy` of `torchmetric`. However, note that `validation_step` is only called
    in certain intervals, for example, after each training epoch. This is why we log
    the validation accuracy inside the validation step, whereas with the training
    accuracy, we log it after each training epoch, otherwise, the accuracy plot that
    we inspect later will look too noisy.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`validation_step` 和 `test_step` 方法类似于 `training_step` 方法，定义了验证和测试评估过程的计算方式。与
    `training_step` 类似，每个 `validation_step` 和 `test_step` 都接收一个批次数据，这就是为什么我们通过 `torchmetric`
    的相应精度属性来记录准确性。但是，请注意，`validation_step` 仅在特定间隔调用，例如每次训练周期后。这就是为什么我们在验证步骤内记录验证准确性，而在训练准确性方面，我们会在每次训练周期后记录，否则，稍后检查的准确性图表将显得太过嘈杂。'
- en: Finally, via the `configure_optimizers` method, we specify the optimizer used
    for training. The following two subsections will discuss how we can set up the
    dataset and how we can train the model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过 `configure_optimizers` 方法，我们指定用于训练的优化器。接下来的两个小节将讨论如何设置数据集以及如何训练模型。
- en: Setting up the data loaders for Lightning
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 Lightning 设置数据加载器
- en: 'There are three main ways in which we can prepare the dataset for Lightning.
    We can:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种主要方法可以为 Lightning 准备数据集。我们可以：
- en: Make the dataset part of the model
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集作为模型的一部分
- en: Set up the data loaders as usual and feed them to the `fit` method of a Lightning
    Trainer—the Trainer is introduced in the next subsection
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像往常一样设置数据加载器并将它们提供给 Lightning Trainer 的 `fit` 方法——Trainer 将在下一小节中介绍
- en: Create a `LightningDataModule`
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 `LightningDataModule`
- en: 'Here, we are going to use a `LightningDataModule`, which is the most organized
    approach. The `LightningDataModule` consists of five main methods, as we can see
    in the following:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 `LightningDataModule`，这是最有组织的方法。`LightningDataModule` 包含五个主要方法，正如我们在接下来会看到的：
- en: '[PRE45]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the `prepare_data` method, we define general steps, such as downloading the
    dataset. In the `setup` method, we define the datasets used for training, validation,
    and testing. Note that MNIST does not have a dedicated validation split, which
    is why we use the `random_split` function to divide the 60,000-example training
    set into 55,000 examples for training and 5,000 examples for validation.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `prepare_data` 方法中，我们定义了通用步骤，如下载数据集。在 `setup` 方法中，我们定义了用于训练、验证和测试的数据集。请注意，MNIST
    没有专门的验证集拆分，这就是为什么我们使用 `random_split` 函数将包含 60,000 个示例的训练集分为 55,000 个示例用于训练和 5,000
    个示例用于验证。
- en: 'The data loader methods are self-explanatory and define how the respective
    datasets are loaded. Now, we can initialize the data module and use it for training,
    validation, and testing in the next subsections:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器的方法是不言自明的，并定义了如何加载各自的数据集。现在，我们可以初始化数据模块并在接下来的小节中用它来进行训练、验证和测试：
- en: '[PRE46]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Training the model using the PyTorch Lightning Trainer class
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch Lightning Trainer 类来训练模型
- en: 'Now we can reap the rewards from setting up the model with the specifically
    named methods, as well as the Lightning data module. Lightning implements a `Trainer`
    class that makes the training model super convenient by taking care of all the
    intermediate steps, such as calling `zero_grad()`, `backward()`, and `optimizer.step()`
    for us. Also, as a bonus, it lets us easily specify one or more GPUs to use (if
    available):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从设置模型以及 Lightning 数据模块中具体命名的方法中受益。Lightning 实现了一个 `Trainer` 类，通过为我们处理所有中间步骤（如调用
    `zero_grad()`、`backward()` 和 `optimizer.step()`）使得训练模型非常方便。此外，作为一个额外的好处，它让我们可以轻松地指定一个或多个
    GPU（如果可用）来使用：
- en: '[PRE47]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Via the preceding code, we train our multilayer perceptron for 10 epochs. During
    training, we see a handy progress bar that keeps track of the epoch and core metrics
    such as the training and validation losses:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述代码，我们为我们的多层感知器训练了 10 个 epochs。在训练过程中，我们可以看到一个便利的进度条，用于跟踪 epoch 和核心指标，如训练损失和验证损失：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: After the training has finished, we can also inspect the metrics we logged in
    more detail, as we will see in the next subsection.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练结束后，我们还可以更详细地检查我们记录的指标，正如我们将在下一小节中看到的那样。
- en: Evaluating the model using TensorBoard
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 评估模型
- en: In the previous section, we experienced the convenience of the `Trainer` class.
    Another nice feature of Lightning is its logging capabilities. Recall that we
    specified several `self.log` steps in our Lightning model earlier. After, and
    even during training, we can visualize them in TensorBoard. (Note that Lightning
    supports other loggers as well; for more information, please see the official
    documentation at [https://pytorch-lightning.readthedocs.io/en/latest/common/loggers.html](https://pytorch-lightning.readthedocs.io/en/latest/common/loggers.html).)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们体验了 `Trainer` 类的便利性。Lightning 的另一个好处是其日志记录功能。回想一下，我们之前在 Lightning 模型中指定了几个
    `self.log` 步骤。在训练期间，我们可以可视化它们在 TensorBoard 中的展示。 （注意，Lightning 还支持其他日志记录器；更多信息请参阅官方文档：[https://pytorch-lightning.readthedocs.io/en/latest/common/loggers.html](https://pytorch-lightning.readthedocs.io/en/latest/common/loggers.html)。）
- en: '**Installing TensorBoard**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 TensorBoard**'
- en: 'TensorBoard can be installed via pip or conda, depending on your preference.
    For instance, the command for installing TensorBoard via pip is as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 pip 或 conda 安装 TensorBoard，具体取决于您的偏好。例如，通过 pip 安装 TensorBoard 的命令如下：
- en: '[PRE49]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following is the command for installing Lightning via conda:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通过 conda 安装 Lightning 的命令：
- en: '[PRE50]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The code in the following subsection is based on TensorBoard version 2.4, which
    you can install by replacing `tensorboard` with `tensorboard==2.4` in these commands.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节的代码基于TensorBoard版本2.4，您可以通过在这些命令中替换 `tensorboard` 为 `tensorboard==2.4` 来安装它。
- en: 'By default, Lightning tracks the training in a subfolder named `lightning_logs`.
    To visualize the training runs, you can execute the following code in the command-line
    terminal, which will open TensorBoard in your browser:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Lightning将训练结果保存在名为 `lightning_logs` 的子文件夹中。要可视化训练运行结果，您可以在命令行终端中执行以下代码，它将在您的浏览器中打开TensorBoard：
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Alternatively, if you are running the code in a Jupyter notebook, you can add
    the following code to a Jupyter notebook cell to show the TensorBoard dashboard
    in the notebook directly:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您在Jupyter笔记本中运行代码，您可以将以下代码添加到Jupyter笔记本单元格中，以直接显示笔记本中的TensorBoard仪表板：
- en: '[PRE52]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '*Figure 13.9* shows the TensorBoard dashboard with the logged training and
    validation accuracy. Note that there is a `version_0` toggle shown in the lower-left
    corner. If you run the training code multiple times, Lightning will track them
    as separate subfolders: `version_0`, `version_1`, `version_2`, and so forth:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.9* 展示了TensorBoard仪表板上记录的训练和验证准确率。请注意左下角显示的 `version_0` 切换。如果多次运行训练代码，Lightning会将它们作为单独的子文件夹进行跟踪：`version_0`、`version_1`、`version_2`
    等等：'
- en: '![Chart  Description automatically generated](img/B17582_13_09.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_13_09.png)'
- en: 'Figure 13.9: TensorBoard dashboard'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.9: TensorBoard 仪表板'
- en: By looking at the training and validation accuracies in *Figure 13.9*, we can
    hypothesize that training the model for a few additional epochs can improve performance.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察 *图 13.9* 中的训练和验证准确率，我们可以假设再训练几个周期可以提高性能。
- en: 'Lightning allows us to load a trained model and train it for additional epochs
    conveniently. As mentioned previously, Lightning tracks the individual training
    runs via subfolders. In *Figure 13.10*, we see the contents of the `version_0`
    subfolder, which contains log files and a model checkpoint for reloading the model:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Lightning允许我们加载已训练的模型，并方便地再训练多个周期。如前所述，Lightning通过子文件夹追踪各个训练运行。在 *图 13.10* 中，我们看到了
    `version_0` 子文件夹的内容，其中包括日志文件和重新加载模型的模型检查点：
- en: '![Graphical user interface, text, application Description automatically generated](img/B17582_13_10.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图形用户界面、文本、应用描述](img/B17582_13_10.png)'
- en: 'Figure 13.10: PyTorch Lightning log files'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.10: PyTorch Lightning日志文件'
- en: 'For instance, we can use the following code to load the latest model checkpoint
    from this folder and train the model via `fit`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用以下代码从此文件夹加载最新的模型检查点，并通过 `fit` 方法训练模型：
- en: '[PRE53]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Here, we set `max_epochs` to `15`, which trained the model for 5 additional
    epochs (previously, we trained it for 10 epochs).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 `max_epochs` 设置为 `15`，这使得模型总共训练了5个额外的周期（之前训练了10个周期）。
- en: 'Now, let’s take a look at the TensorBoard dashboard in *Figure 13.11* and see
    whether training the model for a few additional epochs was worthwhile:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 *图 13.11* 中的TensorBoard仪表板，看看再训练几个周期是否值得：
- en: '![Chart  Description automatically generated](img/B17582_13_11.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_13_11.png)'
- en: 'Figure 13.11: TensorBoard dashboard after training for five more epochs'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.11: 训练了五个额外周期后的TensorBoard仪表板'
- en: As we can see in *Figure 13.11*, TensorBoard allows us to show the results from
    the additional training epochs (`version_1`) next to the previous ones (`version_0`),
    which is very convenient. Indeed, we can see that training for five more epochs
    improved the validation accuracy. At this point, we may decide to train the model
    for more epochs, which we leave as an exercise to you.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *图 13.11* 中所看到的，TensorBoard允许我们展示额外训练周期（`version_1`）的结果与之前的（`version_0`）对比，这非常方便。确实，我们可以看到再训练五个周期提高了验证准确率。在这一点上，我们可以决定是否继续训练更多周期，这留给您作为练习。
- en: 'Once we are finished with training, we can evaluate the model on the test set
    using the following code:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，我们可以使用以下代码在测试集上评估模型：
- en: '[PRE54]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The resulting test set performance, after training for 15 epochs in total,
    is approximately 95 percent:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在总共训练15个周期后，得到的测试集性能约为95%：
- en: '[PRE55]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Note that PyTorch Lightning also saves the model automatically for us. If you
    want to reuse the model later, you can conveniently load it via the following
    code:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PyTorch Lightning还会自动保存模型。如果您想稍后重用模型，您可以通过以下代码方便地加载它：
- en: '[PRE56]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '**Learn more about PyTorch Lightning**'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**了解更多关于PyTorch Lightning的信息**'
- en: To learn more about Lightning, please visit the official website, which contains
    tutorials and examples, at [https://pytorch-lightning.readthedocs.io](https://pytorch-lightning.readthedocs.io).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多有关 Lightning 的信息，请访问官方网站，其中包含教程和示例，网址为 [https://pytorch-lightning.readthedocs.io](https://pytorch-lightning.readthedocs.io)。
- en: Lightning also has an active community on Slack that welcomes new users and
    contributors. To find out more, please visit the official Lightning website at
    [https://www.pytorchlightning.ai](https://www.pytorchlightning.ai).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Lightning 在 Slack 上也有一个活跃的社区，欢迎新用户和贡献者加入。要了解更多信息，请访问官方 Lightning 网站 [https://www.pytorchlightning.ai](https://www.pytorchlightning.ai)。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we covered PyTorch’s most essential and useful features. We
    started by discussing PyTorch’s dynamic computation graph, which makes implementing
    computations very convenient. We also covered the semantics of defining PyTorch
    tensor objects as model parameters.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了 PyTorch 最重要和最有用的特性。我们首先讨论了 PyTorch 的动态计算图，这使得实现计算非常方便。我们还介绍了定义 PyTorch
    张量对象作为模型参数的语义。
- en: After we considered the concept of computing partial derivatives and gradients
    of arbitrary functions, we covered the `torch.nn` module in more detail. It provides
    us with a user-friendly interface for building more complex deep NN models. Finally,
    we concluded this chapter by solving a regression and classification problem using
    what we have discussed so far.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们考虑了计算任意函数的偏导数和梯度的概念后，我们更详细地讨论了 `torch.nn` 模块。它为我们提供了一个用户友好的接口，用于构建更复杂的深度神经网络模型。最后，我们通过使用迄今为止讨论的内容解决了回归和分类问题，从而结束了本章。
- en: Now that we have covered the core mechanics of PyTorch, the next chapter will
    introduce the concept behind **convolutional neural network** (**CNN**) architectures
    for deep learning. CNNs are powerful models and have shown great performance in
    the field of computer vision.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了 PyTorch 的核心机制，下一章将介绍深度学习中 **卷积神经网络** (**CNN**) 架构的概念。CNN 是强大的模型，在计算机视觉领域表现出色。
- en: Join our book’s Discord space
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的 Discord 工作区，与作者进行每月的 *问答* 会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
