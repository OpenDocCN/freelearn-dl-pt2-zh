- en: '*Chaper 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*'
- en: Style Transfer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风格转移
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章末尾，您将能够：
- en: Load pretrained models from PyTorch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从PyTorch加载预训练模型
- en: Extract the style of an image
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取图像的风格
- en: Obtain contents of an image
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取图像的内容
- en: Create a new image using the style of one image and contents from another
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新图像，使用一张图像的风格和另一张图像的内容
- en: In this chapter, you'll learn how to transfer the artistic style from one picture
    to another. This way, you'll be able to convert your everyday pictures into masterpieces.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何将艺术风格从一张图片转移到另一张图片。这样，您就能够将日常图片转变为艺术杰作。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: The previous chapter explained different building blocks of traditional convolutional
    neural networks (CNNs), as well as some techniques to be able to improve performance
    and reduce training time. The architecture explained there, although typical,
    is not set in stone, and, on the contrary, a proliferation of CNNs architectures
    has emerged to solve different data problems, more commonly in the field of computer
    vision.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章详细解释了传统卷积神经网络（CNNs）的不同构建模块，以及一些技术，以提高性能并减少训练时间。尽管那里解释的架构是典型的，但并非一成不变，相反，已经出现了大量用于解决不同数据问题的CNN架构，更常见的是在计算机视觉领域。
- en: These architectures vary in configuration as well as learning tasks. A very
    popular one nowadays is the VGG architecture created by Oxford's Visual Geometry
    Group. It was developed for object recognition, achieving state-of-the-art performance
    thanks to the massive number of parameters that the network relies on. One of
    the main reasons for its popularity popularity among data scientists is due to
    the availability of the parameters (weights and biases) of the trained model,
    which allows researchers to use it without training, as well as the outstanding
    performance of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构因配置和学习任务而异。如今非常流行的一种架构是由牛津视觉几何组（Visual Geometry Group）创建的VGG架构。它是为了对象识别而开发的，通过依赖大量参数，实现了最先进的性能。其在数据科学家中的流行原因之一是训练模型的参数（权重和偏差）的可用性，这使得研究人员可以在不进行训练的情况下使用它，同时模型的性能也非常出色。
- en: In this chapter, we will use this pretrained model to solve a computer vision
    problem that is particularly famous due to the popularity of social media channels
    specializing in sharing images. It consists of performing style transfer in order
    to improve the appearance of an image with the style (colors and textures) of
    another one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用这个预训练模型来解决一个计算机视觉问题，这个问题因社交媒体频道的普及而变得特别有名，专门用于分享图像。它包括执行风格转移，以改善图像的外观，使其具有另一张图像的风格（颜色和纹理）。
- en: The previous task is performed millions of times every day when applying filters
    over regular images to improve their quality and appeal while posting on social
    media profiles. Although it seems like a simple task when in use, this chapter
    will explain the magic that occurs behind the scenes of these image editing apps.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在每天应用滤镜来提高社交媒体上常规图像质量和吸引力的过程中，进行了数百万次前述任务。尽管在使用时似乎是一个简单的任务，但本章将解释在这些图像编辑应用程序的幕后发生的魔法。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As a reminder, the GitHub repository containing all code used in this chapter
    can be found at [https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，包含本章所有代码的GitHub存储库可以在[https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch)找到。
- en: Style Transfer
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风格转移
- en: 'In simple words, style transfer consists of modifying the style of an image,
    while still preserving its content. For instance, taking an image of an animal
    and transforming the style into a Van Gogh-like painting, as shown in the following
    figure:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，风格转移包括修改图像的风格，同时保留其内容。例如，将动物图像的风格转换为梵高风格的绘画，如下图所示：
- en: '![Figure 5.1: Style transfer inputs and output.Result from the final exercise
    of this chapter.](img/C11865_05_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1：风格转移的输入和输出。本章最终练习的结果。](img/C11865_05_01.jpg)'
- en: 'Figure 5.1: Style transfer inputs and output.Result from the final exercise
    of this chapter.'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.1：风格转移的输入和输出。本章最终练习的结果。
- en: 'According to the preceding figure, there are two inputs to a pretrained model:
    a content image and a style image. The content refers to the objects, while the
    style refers to the colors and textures. As a result, the output from the model
    should be an image containing the objects from the content image and the artistic
    appearance of the style image.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述图示，预训练模型有两个输入：内容图像和样式图像。内容指的是物体，而样式指的是颜色和纹理。因此，模型的输出应该是包含内容图像中物体和样式图像艺术外观的图像。
- en: How Does It Work?
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作原理是怎样的？
- en: Different to solving a traditional computer vision problem, which was explained
    in the previous chapter, style transfer requires to follow a different set of
    steps to effectively take two images as an input and create a new image as the
    output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与解决传统计算机视觉问题不同（如前一章所述），风格转移需要按不同步骤有效地将两个图像作为输入并创建新图像作为输出。
- en: 'The following is a brief explanation of the steps followed when solving a style
    transfer problem:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解决风格转移问题时遵循的步骤的简要解释：
- en: '**Feeding the inputs**: Both the content and the style image are to be fed
    to the model, and they need to be the same shape. A common practice here is to
    resize the style image to be the same shape of the content image.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入数据的提供**：内容图像和样式图像都需要输入模型，并且它们的形状必须相同。在这里的常见做法是将样式图像调整为与内容图像相同的形状。'
- en: '**Loading the model**: The Oxford''s Visual Geometry Group created a model''s
    architecture that performs outstandingly well over style transfer problems, which
    is known as the VGG network. Moreover, they also made the model''s parameters
    available to anyone so that the training process of the model could be shortened
    or skipped.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载模型**：牛津大学视觉几何组创建了一个在风格转移问题上表现出色的模型架构，称为 VGG 网络。此外，他们还提供了模型的参数，以便任何人可以缩短或跳过模型的训练过程。'
- en: Note
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: There are different versions of the VGG network, which use different number
    of layers. To differentiate the different versions, the nomenclature on the matter
    will add a dash and a number at the end of the acronym, which represents the number
    of layers of that particular architecture. For the present chapter, we will use
    the network's 19-layer version, which is known as the VGG-19.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VGG 网络有不同版本，使用不同数量的层。为了区分不同的版本，其命名方式会在缩写后加上一个破折号和数字，代表该特定架构的层数。本章将使用网络的 19 层版本，即被称为
    VGG-19 的版本。
- en: Because of this, and using PyTorch's pretrained models' subpackage, it is possible
    to load the pretrained model in order to perform the style transfer task without
    the need to train the network with a large number of images.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，利用 PyTorch 的预训练模型子包，可以加载预训练模型，以执行风格转移任务，无需使用大量图像训练网络。
- en: '**Determining the layers'' functions**: Given that there are two main tasks
    at hand (recognizing the content of an image and distinguishing the style of another
    one), different layers will have different functions to extract the different
    features; for the style image, the focus should be on colors and textures; while
    for the content image, the focus should be on edges and forms. In this step, the
    different layers are separated into different tasks.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定层的功能**：鉴于有两个主要任务（识别图像内容和区分另一个图像的样式），不同的层将有不同的功能来提取不同的特征；对于样式图像，重点应放在颜色和纹理上；而对于内容图像，则应关注边缘和形式。在这一步骤中，不同的层被分配到不同的任务中。'
- en: '**Defining the optimization problem**: Like any other supervised problem, it
    is necessary to define a loss function, which will have the responsibility of
    measuring the difference between the output and inputs. Unlike other supervised
    problems, it is required to minimize three different loss functions for style
    transfer problems:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义优化问题**：与任何其他监督问题一样，需要定义一个损失函数，它负责衡量输出和输入之间的差异。与其他监督问题不同的是，风格转移问题需要最小化三种不同的损失函数：'
- en: 'Content loss: This measures the distance between the content image and the
    output, only considering features related to content.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内容损失：仅考虑与内容相关的特征，衡量内容图像和输出之间的距离。
- en: 'Style loss: This measures the distance between the style image and the output,
    only considering features related to style.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 风格损失：仅考虑与样式相关的特征，衡量样式图像和输出之间的距离。
- en: 'Total loss: This combines both the content and style loss. Both the content
    and style loss have a weight associated to them, which is used to determine their
    participation in the calculation of the total loss.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总损失：这结合了内容损失和风格损失。内容损失和风格损失都有一个相关的权重，用于确定它们在计算总损失中的贡献。
- en: '**Parameters update**: This step uses gradients to update the different parameters
    of the network.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**参数更新**：此步骤使用梯度来更新网络的不同参数。'
- en: Implementation of Style Transfer Using the VGG-19 Network Architecture
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用VGG-19网络架构实现风格迁移的实施
- en: The VGG-19 is a CNN consisting of 19 layers. It was trained using millions of
    images from the ImageNet database. The network is capable of classifying images
    into 1000 different class labels, including a vast number of animals and different
    tools.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-19是一个包含19层的卷积神经网络。它使用来自ImageNet数据库的数百万图像进行训练。该网络能够将图像分类为1000个不同的类标签，包括大量的动物和各种工具。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'To explore the ImageNet database, use the following URL: [http://www.image-net.org/](http://www.image-net.org/).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 若要探索ImageNet数据库，请使用以下网址：[http://www.image-net.org/](http://www.image-net.org/)。
- en: Considering its depth, the network is able to identify complex features from
    a wide variety of images, which makes it particularly good for style transfer
    problems, where feature extraction is crucial at different stages and for different
    purposes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到其深度，该网络能够从各种图像中识别复杂的特征，这使其特别适合风格迁移问题，其中在不同阶段和不同目的下的特征提取至关重要。
- en: The following section will focus on explaining the process of using a pretrained
    VGG-19 model to perform style transfer. The end purpose of this chapter will be
    to take an image of an animal or a landscape (as the content image) and one of
    a painting from a well-known artist (as the style image) to create a new image
    of a regular object with an artistic style.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将专注于解释如何使用预训练的VGG-19模型进行风格迁移的过程。本章的最终目的是将动物或风景图像（作为内容图像）和知名艺术家的绘画图像（作为风格图像）合成新的带有艺术风格的常规物体图像。
- en: 'However, before diving into the process, the following is an explanation of
    the imports with a brief explanation of their use:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在深入到过程之前，以下是导入的解释及其用途的简要解释：
- en: '**NumPy**: This will be used to transform images to be displayed.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：将用于将图像转换为显示的格式。'
- en: '**torch, torch.nn, and torch.optim**: These will implement the neural network,
    as well as define the optimization algorithm.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**torch, torch.nn 和 torch.optim**：这些将实现神经网络，并定义优化算法。'
- en: '**PIL.Image**: This will load images.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PIL.Image**：这将加载图像。'
- en: '**matplotlib.pyplot**: This will display images.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**matplotlib.pyplot**：这将显示图像。'
- en: '**torchvision.transforms and torchvision.models**: These will convert the images
    into tensors and load the pretrained model.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**torchvision.transforms 和 torchvision.models**：这些将把图像转换为张量并加载预训练模型。'
- en: 'Inputs: Loading and Displaying'
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入：加载和显示
- en: The first step of performing a style transfer consists of loading both the content
    and style images. During this step, the basic pre-processing is handled, where
    images must be equally sized (preferably the size of the images used to train
    the pretrained model), which will be the size of the output image as well. Additionally,
    images are converted into PyTorch tensors and can be normalized if desired.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 执行风格迁移的第一步包括加载内容图像和风格图像。在此步骤中，处理基本的预处理，其中图像必须是相同大小的（最好是用于训练预训练模型的图像大小），这也将是输出图像的大小。此外，图像被转换为PyTorch张量，并且可以根据需要进行归一化。
- en: Furthermore, it is always a good practice to display images that have been loaded
    in order to make sure that they are as desired. Considering images have already
    been converted to tensors and normalized at this point, the tensor should be cloned,
    and a new set of transformations need to be performed in order to be able to display
    them using Matplotlib.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，始终将加载的图像显示出来是一个好习惯，以确保它们如预期一样。考虑到图像已经转换为张量并在此时进行了归一化，应克隆张量，并进行新一组转换，以便使用Matplotlib显示它们。
- en: Defining functions to both load and display images can help save up time, and
    can also make sure that the same process is done over both the content and style
    image. This process will be expanded in the following exercise.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 定义函数来加载和显示图像可以节省时间，并确保内容图像和风格图像上的处理过程相同。此过程将在后续练习中展开。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All exercises of this chapter are to be coded in the same notebook, as together,
    they will perform the style transfer task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有练习都要在同一个笔记本中编写，因为它们将一起执行风格转移任务。
- en: 'Exercise 10: Loading and Displaying Images'
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 10：加载和显示图像
- en: This is the first of four steps to perform a style transfer. The objective of
    this chapter is to load and display images (both content and style) that will
    be used in further exercises.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是进行风格转移的四个步骤中的第一步。本章的目标是加载和显示图像（内容和风格），这些图像将在后续练习中使用。
- en: Note
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Inside the GitHub repository (link shared at the beginning of this chapter),
    you will be able to find different images that will be used throughout this chapter
    in the different exercises and activities.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 仓库（本章开头分享的链接）中，您可以找到将在本章中不同练习和活动中使用的不同图像。
- en: 'Import all packages required to perform style transfer:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有进行风格转移所需的包：
- en: '[PRE0]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Set the image size to be used for both images. Also, set the transformations
    to be performed over images, which should include resizing images, converting
    them to tensors, and normalizing them:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置用于两幅图像的图像大小。此外，设置应在图像上执行的变换，其中包括调整图像大小、将其转换为张量并进行归一化：
- en: '[PRE1]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The VGG network was trained using normalized images, where each channel has
    a mean of 0.485, 0.456, and 0.406, respectively, and a standard deviation of 0.229,
    0.224, and 0.225.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VGG 网络是使用归一化图像训练的，其中每个通道分别具有均值 0.485、0.456 和 0.406，标准差为 0.229、0.224 和 0.225。
- en: 'Define a function that will receive the image path as input and use PIL to
    open the image. Next, it should apply the transformations over the image:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将接收图像路径作为输入，并使用 PIL 打开图像。接下来，它应该对图像应用变换：
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Call the function to load both content and style images. Use the dog image
    as content and the Matisse image as style, both of which are available in the
    GitHub repository:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用函数以加载内容图像和风格图像。将狗图像作为内容，将马蒂斯图像作为风格，这两者都可在 GitHub 仓库中找到：
- en: '[PRE3]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To display the images, convert them back to PIL images and revert the normalization
    process. Define these transformations in a variable:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要显示图像，将它们转换回 PIL 图像并恢复归一化过程。将这些变换定义在一个变量中：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To revert the normalization, it is necessary to use as mean the negative value
    of the mean used for normalizing the data, divided by the standard deviation previously
    used for normalizing the data. Moreover, the new standard deviation should be
    equal to one divided by the standard deviation used to normalize the data before.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要恢复归一化，需要使用与用于数据归一化的均值相反的均值，除以先前用于数据归一化的标准差。此外，新的标准差应等于归一化数据之前使用的标准差的倒数。
- en: 'Create a function that clones the tensor, squeezes it, and finally applies
    transformations to the tensor:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，克隆张量，压缩它，并最终对张量应用变换：
- en: '[PRE5]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Call the function for both images and plot the results:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对两幅图像调用函数并绘制结果：
- en: '[PRE6]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The resulting images should look as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像应如下所示：
- en: '![Figure 5.2: Content image'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：内容图像'
- en: '](img/C11865_05_02.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_02.jpg)'
- en: 'Figure 5.2: Content image'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.2：内容图像
- en: '![Figure 5.3: Style image'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：风格图像'
- en: '](img/C11865_05_03.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_03.jpg)'
- en: 'Figure 5.3: Style image'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.3：风格图像
- en: Congratulations! You have successfully loaded and displayed the content and
    style images to be used for style transfer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功加载并显示用于风格转移的内容和风格图像。
- en: Loading the Model
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载模型
- en: As in many other frameworks, PyTorch has a subpackage that contains different
    models that have been previously trained and made available for public use. This
    is important considering that training a neural network from scratch is time consuming,
    while starting off with a pretrained model can help reduce this training times.
    This means that pretrained models can be loaded in order to use their final parameters
    (which should be those that minimize the loss function) without the need to go
    through an iterative process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他框架一样，PyTorch 拥有一个子包，其中包含之前训练过的不同模型，并已公开供使用。这一点很重要，因为从头开始训练神经网络非常耗时，而使用预训练模型可以帮助减少这些训练时间。这意味着可以加载预训练模型以使用它们的最终参数（应为最小化损失函数的参数），而无需经历迭代过程。
- en: 'As mentioned eariler, the architecture to be used to perform the style transfer
    task is that of the VGG network of 19 layers, also known as VGG-19\. The pretrained
    model is available under the model''s subpackage of `torchvision`. The saved model
    in PyTorch is split into two portions, as mentioned and explained as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，用于执行风格转移任务的架构是 VGG 网络的 19 层，也被称为 VGG-19。预训练模型位于 `torchvision` 的模型子包下。在
    PyTorch 中保存的模型被分成两部分，如下所述和解释的那样：
- en: '**vgg19.features**: This consists of all the convolutional and pooling layers
    of the network along with the parameters. These layers are in charge of extracting
    the features from the images, where some of the layers specialize in style features,
    such as colors, while others specialize in content features, such as edges.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**vgg19.features**：这包括网络的所有卷积和池化层以及其参数。这些层负责从图像中提取特征，其中一些层专门处理风格特征，如颜色，而其他层专门处理内容特征，如边缘。'
- en: '**vgg19.classifier**: This refers to the linear layers (also known as fully
    connected layers) that are located at the end of the network, including their
    parameters. These layers are the ones that perform the classification of the image
    into one of the label classes.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**vgg19.classifier**：这指的是网络末端的线性层（也称为全连接层），包括它们的参数。这些层负责将图像分类为标签类别之一。'
- en: Note
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To explore other pretrained models available in PyTorch, visit [https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html).
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解 PyTorch 中提供的其他预训练模型，请访问 [https://pytorch.org/docs/stable/torchvision/models.html](https://pytorch.org/docs/stable/torchvision/models.html)。
- en: According to the preceding information, only the features portion of the model
    should be loaded in order to extract the necessary features of both the content
    and style images. Loading a model consists of calling the models' subpackage followed
    by the name of the model, making sure that the pretrained argument is set to `True`
    and that only the feature layers are being loaded.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前述信息，应仅加载模型的特征部分，以便提取内容和风格图像的必要特征。加载模型包括调用模型的子包，后跟模型的名称，确保预训练参数设置为 `True`，并且仅加载特征层。
- en: Moreover, the parameters in each layer should be kept unchanged, considering
    that those are the ones that will help detect the desired features. This can be
    achieved by defining that the model does not need to calculate gradients for any
    of these layers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，应保持每层的参数不变，考虑到这些参数将有助于检测所需的特征。可以通过定义模型不需要计算这些层的任何梯度来实现这一点。
- en: 'Exercise 11: Loading a Pretrained Model in PyTorch'
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 11：在 PyTorch 中加载预训练模型
- en: 'Using the same notebook as in the previous exercise, this exercise aims to
    load the pretrained model that will be used in subsequent exercises to perform
    the style transfer task using the images previously loaded:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与前一练习中相同的笔记本，本练习旨在加载预训练模型，该模型将在随后的练习中使用，以执行使用先前加载的图像执行风格转移任务：
- en: Open the notebook from the previous exercise.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开之前练习中的笔记本。
- en: 'Load the VGG-19 pretrained model from PyTorch:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载来自 PyTorch 的 VGG-19 预训练模型：
- en: '[PRE7]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Select the features portion of the model, as explained previously. This will
    give access to all the convolutional and pooling layers of the model, which are
    to be used to perform the extraction of features in subsequent exercises of this
    chapter.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据先前解释的内容选择模型的特征部分。这将允许访问模型的所有卷积和池化层，这些层将用于在本章后续练习中执行特征提取。
- en: 'Perform a `for` loop through the parameters of the previously loaded model.
    Set each parameter to not require gradients calculations:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过之前加载的模型的参数进行 `for` 循环。将每个参数设置为不需要计算梯度：
- en: '[PRE8]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By setting the calculation of gradients to `False`, we ensure that this to not
    require gradients calculations unchanged during the process of creating the target
    image.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过将梯度计算设置为 `False`，我们确保在创建目标图像的过程中不需要对梯度进行计算。
- en: Congratulations! You have successfully loaded a pretrained model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功加载了预训练模型。
- en: Extracting the Features
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取特征
- en: The VGG-19 network, as mentioned before, contains 19 different layers, including
    convolutional, pooling, and fully connected layers. The convolutional layers come
    in stacks before every pooling layer, five being the number of stacks in the entire
    architecture.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，VGG-19 网络包含 19 层不同的层，包括卷积层、池化层和全连接层。每个池化层之前都有卷积层堆叠，整个架构中有五个堆叠。
- en: In the field of style transfer, there have been different papers that have identified
    those layers that are crucial at recognizing relevant features over the content
    and style images. According to this, it is conventionally accepted that the first
    convolutional layer of every stack is capable of extracting style features, while
    only the second convolutional layer of the fourth stack should be used to extract
    content features. From now on, we will refer to the layers that extract the style
    features as `conv1_1`, `conv2_1`, `conv3_1`, `conv4_1`, and `conv5_1`, while the
    layer in charge of extracting the content features will be known as `conv4_2`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在风格转移领域，已经有不同的论文确定了那些识别内容和风格图像中相关特征的关键层。根据这一点，通常认为每个堆栈的第一个卷积层能够提取风格特征，而只有第四个堆栈的第二个卷积层应用于提取内容特征。从现在开始，我们将称提取风格特征的层为
    `conv1_1`、`conv2_1`、`conv3_1`、`conv4_1` 和 `conv5_1`，而负责提取内容特征的层将称为 `conv4_2`。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The paper used as guide for this chapter can be accessed in the following URL:
    [https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的指导文件可以通过以下网址访问：[https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)。
- en: This means that the style image should be passed through five different layers,
    while the content image only needs to go through one layer. The output from each
    of these layers is used to compare the output image to the input images, where
    the objective would be to modify the parameters of the target image to resemble
    the content of the content image and the style of the style image, which can be
    achieved through the optimization of three different loss functions (which will
    be further explained in this chapter).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着风格图像应通过五个不同的层，而内容图像只需通过一个层。每个层的输出用于比较输出图像与输入图像，其目标是修改目标图像的参数，使其类似于内容图像的内容和风格图像的风格，这可以通过优化三个不同的损失函数来实现（这将在本章中进一步解释）。
- en: To determine whether the target image contains the same content as the content
    image, we need to check whether certain features are present in both images. However,
    to check the style representation of the target image and the style image, it
    is necessary to check for correlations and not the strict presence of the features
    on both images. This is because the style features of both images will not be
    exact, but rather an approximation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定目标图像是否包含与内容图像相同的内容，我们需要检查两者中是否存在某些特征。然而，要检查目标图像和风格图像的风格表示，需要检查它们之间的相关性，而不是严格的特征存在。这是因为两者的风格特征不会完全相同，而是近似。
- en: 'To achieve this, the gram matrix is introduced. It consists of the creation
    of a matrix that looks at the correlations of different style features in a given
    layer. This is done by multiplying the vectorized output from the convolutional
    layer by the same transposed vectorized output, as can be seen in the following
    figure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，引入了格拉姆矩阵。它由创建一个矩阵组成，该矩阵查看给定层中不同风格特征的相关性。这是通过将卷积层的向量化输出与相同的转置向量化输出相乘来完成的，如下图所示：
- en: '![Figure 5.4: Calculation of the gram matrix'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：格拉姆矩阵的计算'
- en: '](img/C11865_05_04.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_04.jpg)'
- en: 'Figure 5.4: Calculation of the gram matrix'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：格拉姆矩阵的计算
- en: In the preceding figure, A refers to the input style image with four-by-four
    dimensions (height and width), B represents the output after passing the image
    through a convolutional layer with five filters. Finally, C refers to the calculation
    of the gram matrix, where the image to the left represents the vectorized version
    of B, and the image to the right is its transposed version. From the multiplication
    of the vectorized outputs, a five-by-five gram matrix is created, whose values
    indicate the similarities (correlations) in terms of style features along the
    different channels (filters).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，A 表示输入的风格图像，具有四乘四的尺寸（高度和宽度），B 表示通过五个滤波器的卷积层后的输出。最后，C 表示格拉姆矩阵的计算，其中左侧的图像代表B的向量化版本，右侧的图像是其转置版本。通过向量化输出的乘积，创建了一个五乘五的格拉姆矩阵，其值指示了不同通道（滤波器）中风格特征的相似性（相关性）。
- en: These correlations can be used to determine those features that are relevant
    for the style representation of the image, which can then be used to alter the
    target image. Considering that the style features are obtained in five different
    layers, it is safe to assume that the network is capable of detecting small and
    large features from the style image, considering that a gram matrix has to be
    created for each of the layers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些相关性可以用来确定对图像的风格表示而言重要的特征，随后可用于修改目标图像。考虑到风格特征是从五个不同的层获取的，可以安全地假设网络能够检测到风格图像的小和大特征，因为每个层都必须创建一个Gram矩阵。
- en: 'Exercise 12: Setting up the Feature Extraction Process'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12：设置特征提取过程
- en: 'Using the network architecture from the previous exercise and the image from
    the first exercise of this chapter, we will create a couple of functions capable
    of extracting features from the input images and creating the gram matrix for
    the style features:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前一练习中的网络架构和本章第一次练习中的图像，我们将创建一对函数，能够从输入图像中提取特征并为风格特征创建Gram矩阵：
- en: Open the notebook from the previous exercise.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开前一练习中的笔记本。
- en: 'Print the architecture of the model loaded in the previous exercise. This will
    help identify relevant layers to perform the style transfer task:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印在前一练习中加载的模型的架构。这将有助于识别执行风格迁移任务所需的相关层：
- en: '[PRE9]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create a dictionary mapping the index of the relevant layers (keys) to a name
    (values). This will facilitate the process of calling relevant layers in the future:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个将相关层的索引（键）映射到名称（值）的字典。这将简化未来调用相关层的过程：
- en: '[PRE10]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To create the dictionary, we use the output from the previous step that displays
    each of the layers in the network. There, it is possible to observe that the first
    layer of the first stack is labelled as `0`, while the first layer of the second
    stack is labelled as `5` and so on.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要创建字典，我们使用从上一步的输出，显示网络中每一层的输出。在那里，可以观察到第一个堆栈的第一层标记为`0`，而第二个堆栈的第一层标记为`5`，依此类推。
- en: 'Create a function that will extract the relevant features (features extracted
    from the relevant layers only) from an input image. Name it `features_extractor`
    and make sure it takes as inputs the image, the model, and the dictionary previously
    created:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，从输入图像中提取相关特征（仅从相关层提取的特征）。命名为`features_extractor`，确保它以图像、模型和先前创建的字典作为输入：
- en: '[PRE11]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`model._modules` contains a dictionary holding each layer of the network. By
    performing a `for` loop through the different layers, we pass the image through
    the layers of interest (the ones inside the `layers` dictionary previously created)
    and save the output into the `features` dictionary.'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`model._modules` 包含一个字典，其中存储了网络的每一层。通过对不同层进行`for`循环，我们将图像通过感兴趣的层（之前创建的`layers`字典内的层）并将输出保存到`features`字典中。'
- en: The output dictionary consists of keys containing the name of the layer and
    values containing the output features from that layer.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出字典包含键，其中包含层的名称，值包含该层的输出特征。
- en: 'Call the `features_extractor` function over the content and style images loaded
    in the first exercise of this chapter:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章第一次练习中加载的内容和风格图像上调用`features_extractor`函数：
- en: '[PRE12]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Perform the gram matrix calculation over style features. Consider that the
    style features were obtained from different layers, which is why different gram
    matrices should be created, one for each layer''s output:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对风格特征执行Gram矩阵计算。考虑到风格特征来自不同的层，因此应创建不同的Gram矩阵，每层的输出各一个：
- en: '[PRE13]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create an initial target image. This image will be later compared against the
    content and style images and be changed until the desired similarity is achieved:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个初始目标图像。稍后将与内容图像和风格图像进行比较，并在达到所需相似度之前进行更改：
- en: '[PRE14]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is a good practice to create the initial target image as a copy of the content
    image. Moreover, it is essential to set it to require the calculation of gradients,
    considering that we want to be able to modify it in an iterative process until
    the content is similar to that of the content image and the style to that of the
    style image.
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将初始目标图像创建为内容图像的副本是一种良好的做法。此外，设置为需要计算梯度是至关重要的，因为我们希望能够在迭代过程中修改它，直到内容与内容图像相似，风格与风格图像相似。
- en: 'Using the `tensor2image` function created during the first exercise of this
    chapter, plot the target image, which should look the same as the content image:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用本章第一个练习期间创建的`tensor2image`函数，绘制目标图像，该图像应与内容图像相同：
- en: '[PRE15]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output image is the following:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出图像如下：
- en: '![Figure 5.5: The target Image'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5：目标图像'
- en: '](img/C11865_05_05.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_05.jpg)'
- en: 'Figure 5.5: The target Image'
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.5：目标图像
- en: Congratulations! You have successfully performed feature extraction and the
    calculation of the gram matrix to perform the style transfer task.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功执行特征提取并计算格拉姆矩阵，以执行样式转移任务。
- en: The Optimization Algorithm, Losses, and Parameter Updates
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化算法、损失和参数更新
- en: Although style transfer is performed using a pretrained network where the parameters
    are left unchanged, creating the target image consists of an iterative process
    where three different loss functions are calculated and minimized by updating
    only the parameters related to the target image.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管样式转移是使用预训练网络执行的，其中参数保持不变，但创建目标图像涉及一个迭代过程，其中通过仅更新与目标图像相关的参数来计算并最小化三种不同的损失函数。
- en: 'To achieve this, two different loss functions are calculated (the content and
    style losses), which are then put together to calculate a total loss function
    that is to be optimized to arrive at an appropriate target image. However, considering
    that measuring accuracy in terms of content and style is achieved very differently,
    the following is an explanation of the calculation of both the content and style
    loss functions, as well as a description on how the total loss is calculated:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，计算了两种不同的损失函数（内容损失和样式损失），然后将它们结合在一起计算出总损失函数，以优化得到一个合适的目标图像。然而，考虑到以内容和样式为度量精确度是非常不同的，以下是对计算内容和样式损失函数以及描述如何计算总损失的说明：
- en: '**Content loss**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容损失**'
- en: This consists of a function that, based on the feature map obtained by a given
    layer, calculates the distance between the content image and the target image.
    In the case of the VGG-19 network, the content loss is only calculated based on
    the output from the `conv4_2` layer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括一个函数，根据给定层获得的特征映射计算内容图像和目标图像之间的距离。在VGG-19网络的情况下，仅基于`conv4_2`层的输出计算内容损失。
- en: The main idea behind the content loss function is to minimize the distance between
    the content image and the target image so that the latter highly resembles the
    former one in terms of content.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失函数的主要思想是最小化内容图像和目标图像之间的距离，使得后者在内容上高度类似于前者。
- en: 'The content loss can be calculated as the mean squared difference between the
    feature maps of the content and target images at the relevant layer (`conv4_2`),
    which can be achieved using the following equation:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失可以通过以下方程计算，即内容和目标图像在相关层（`conv4_2`）的特征映射之间的均方差差异来实现：
- en: '![Figure 5.6: The content loss function'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6：内容损失函数'
- en: '](img/C11865_05_06.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_06.jpg)'
- en: 'Figure 5.6: The content loss function'
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.6：内容损失函数
- en: '**Style loss**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**样式损失**'
- en: Similar to the content loss, the style loss is a function that measures the
    distance between the style and the target image in terms of style features (for
    instance, color and texture) by calculating the mean squared difference.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与内容损失类似，样式损失是一个函数，通过计算样式特征（例如颜色和纹理）的均方差差异来衡量样式和目标图像之间的距离。
- en: Contrary to the content loss, instead of comparing the feature maps derived
    from the different layers, it compares the gram matrices calculated based on the
    feature maps of both the style and the target image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与内容损失相反，样式损失不是比较来自不同层的特征映射，而是比较基于样式和目标图像的特征映射计算得到的格拉姆矩阵。
- en: It is important to mention that the style loss has to be calculated for all
    relevant layers (in this case, five layers) using a `for` loop. This will result
    in a loss function that considers simple and complex style representations from
    both images.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提到的是，样式损失必须使用`for`循环来计算所有相关层（在本例中为五层）。这将导致一个损失函数，考虑了来自两幅图像的简单和复杂样式表示。
- en: Furthermore, it is a good practice to weigh the style representation of each
    of these layers between zero to one in order to give more emphasis to the layers
    that extract larger and simpler features over layers that extract very complex
    features. This is achieved by giving higher weights to earlier layers (`conv1_1`
    and `conv2_1`) that extract more generic features from the style image.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，将这些层的样式表示加权在 0 到 1 之间是一个很好的做法，以便更强调从样式图像中提取较大和更简单特征的层。通过给予更早的层（`conv1_1`
    和 `conv2_1`）更高的权重，从而实现这一点，这些层从样式图像中提取更通用的特征。
- en: 'Considering this, the calculation of the style loss can be performed using
    the following equation for each of the relevant layers:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，可以使用以下方程来计算每个相关层的样式损失：
- en: '![Figure 5.7: Style loss calculation'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7：样式损失计算'
- en: '](img/C11865_05_07.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_07.jpg)'
- en: 'Figure 5.7: Style loss calculation'
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：样式损失计算
- en: '**Total loss**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**总损失**'
- en: Finally, the total loss function consists of a combination of both the content
    loss and the style loss. Its value is minimized during the iterative process of
    creating the target image by updating parameters of the target image.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，总损失函数由内容损失和样式损失的组合构成。在创建目标图像的迭代过程中，通过更新目标图像的参数来最小化其值。
- en: Again, it is recommended to assign weights to the content and the style losses
    in order to determine their participation in the final output. This helps determine
    the degree at which the target image will be stylized, while making the content
    still visible. Considering this, it is a good practice to set the weight of the
    content loss as equal to one, whereas the one for the style loss must be much
    higher to achieve the ratio of your preference.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，建议分配内容和样式损失的权重，以确定它们在最终输出中的参与程度。这有助于确定目标图像的风格化程度，同时仍然保持内容的可见性。考虑到这一点，将内容损失的权重设置为1是一个很好的做法，而样式损失的权重必须更高，以实现您喜欢的比例。
- en: The weight assigned to the content loss is conventionally known as alpha, while
    the one given to the style loss is known as beta.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 被分配给内容损失的权重通常被称为α，而被分配给样式损失的权重则被称为β。
- en: 'The final equation to calculate the total loss can be seen as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 计算总损失的最终方程可以如下所示：
- en: '![Figure 5.8: Total loss calculation'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8：总损失计算'
- en: '](img/C11865_05_08.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_08.jpg)'
- en: 'Figure 5.8: Total loss calculation'
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：总损失计算
- en: Once the weights of the losses are defined, it is time to set the number of
    iteration steps, as well as the optimization algorithm which should only affect
    the target image. This means that, in every iteration step, all three losses will
    be calculated to then use the gradients to optimize the parameters associated
    to the target image, until the loss functions are minimized and a target function
    with the desired look is achieved.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了损失的权重，就是设置迭代步数和优化算法的时候了，这只会影响目标图像。这意味着，在每个迭代步中，将计算这三个损失，然后利用梯度来优化与目标图像相关的参数，直到最小化损失函数并实现具有所需外观的目标函数。
- en: 'Like the optimization of previous neural networks, the following are the steps
    followed in each iteration:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前的神经网络优化类似，每次迭代中遵循以下步骤：
- en: Get the features, both in terms of content and style, from the target image.
    In the initial iteration, this image will be an exact copy of the content image.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从目标图像获取内容和样式的特征。在初始迭代中，此图像将是内容图像的精确副本。
- en: Calculate the content loss. This is done comparing the content features map
    of the content and the target images.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算内容损失。这是通过比较内容和目标图像的内容特征图来完成的。
- en: Calculate the average style loss of all relevant layers. This is achieved by
    comparing the gram matrices for all layers of both the style and target image.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有相关层的平均样式损失。这是通过比较样式和目标图像的所有层的格拉姆矩阵来实现的。
- en: Calculate the total loss.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算总损失。
- en: Calculate the partial derivatives of the total loss function in respect to the
    parameters (weights and biases) of the target image.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标图像参数（权重和偏置）的总损失函数的偏导数。
- en: Repeat until the desired number of iterations has been reached.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到达到所需的迭代次数为止重复此过程。
- en: The final output will be an image with content similar to the content image
    and a style similar to the style image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出将是一个内容类似于内容图像且风格类似于样式图像的图像。
- en: 'Exercise 13: Creating the Target Image'
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 13：创建目标图像
- en: 'In the final exercise of this chapter, the tasks of style transfer will be
    achieved. This exercise consists of coding the section in charge of performing
    the different iterations while optimizing the loss functions in order to arrive
    at an ideal target image. To do so, it is crucial to make use of the code bits
    programmed in the previous exercises of this chapter:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一个练习中，将实现风格转移任务。本练习包括编写负责在优化损失函数的同时执行不同迭代的部分的代码，以达到理想的目标图像。为此，关键是利用本章之前编程的代码片段：
- en: Open the notebook from the previous exercise.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开上一个练习中的笔记本。
- en: 'Define a dictionary containing the weights for each of the layers in charge
    of extracting style features:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含每个负责提取风格特征层的权重的字典：
- en: '[PRE16]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Be sure to use the same name that you gave your layers in the previous exercise
    as keys.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保使用与前一章节中给出的层相同的名称作为键。
- en: 'Define the weights associated with the content and style losses:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义与内容损失和风格损失相关联的权重：
- en: '[PRE17]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Define the number of iteration steps, as well as the optimization algorithm.
    We can also set the number of iterations after we want to see a plot of the image
    that has been created to that point.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义迭代步骤的数量以及优化算法。我们也可以设置在特定迭代之后要看到创建图像的情况。
- en: '[PRE18]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The parameters to be updated by this optimization algorithm should be the parameters
    of the target image.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优化算法应更新目标图像的参数。
- en: Note
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Running 2,000 iterations, as per the example in this exercise, will take quite
    some time, depending on your resources. However, to reach an outstanding result
    in style transfer even more iterations are typically required (around 6,000, perhaps).
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如本练习中的示例所示，运行 2,000 次迭代将需要相当长的时间，这取决于您的资源。然而，要达到风格转移的卓越结果，通常需要更多的迭代（大约 6,000
    次）。
- en: To appreciate the changes that occur to the target image from iteration to iteration,
    a couple of iterations will suffice, but you are encouraged to try training longer.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了欣赏从迭代到迭代发生在目标图像上的变化，几次迭代就足够了，但建议您尝试更长时间的训练。
- en: 'Define the `for` loop where all three loss functions will be calculated, and
    the optimization will be performed:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个 `for` 循环，在其中计算所有三个损失函数，并执行优化：
- en: '[PRE19]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Plot both the content and the target image to compare the results:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制内容和目标图像以比较结果：
- en: '[PRE20]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The final image should look similar to the following figure:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终的图像应该看起来类似于以下的图例：
- en: '![Figure 5.9: Comparison between content and target image'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：内容和目标图像的比较'
- en: '](img/C11865_05_09.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C11865_05_09.jpg)'
- en: 'Figure 5.9: Comparison between content and target image'
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.9：内容和目标图像的比较
- en: Congratulations! You have successfully performed the style transfer task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你成功地完成了风格转移任务。
- en: 'Activity 10: Performing Style Transfer'
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 10：执行风格转移
- en: 'In this activity, we will perform the task of style transfer. To do so, we
    will code all the concepts learned throughout this chapter. Let''s look at the
    following scenario:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将执行风格转移任务。为此，我们将编写本章学到的所有概念。让我们看看以下情况：
- en: 'You are a globe trotter and you have decided to create a blog that documents
    your travels. However, you are also passionate about art and would like all your
    images to have an artistic look to it, that of a Monet painting. To be able to
    achieve that, you have decided to create a code that uses a pretrained neural
    network to perform a style transfer task:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个环球旅行者，决定创建一个记录你旅行的博客。然而，你还热衷艺术，并希望所有的图片看起来都像莫奈的画作一样具有艺术感。为了实现这一目标，你决定创建一个使用预训练神经网络执行风格转移任务的代码：
- en: Import the required libraries.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: Specify the transformations to be performed over the input images. Be sure to
    resize them to the same size, convert them to tensor, and normalize them.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定要对输入图像执行的转换。确保将它们调整为相同的大小，转换为张量，并进行归一化。
- en: Define an image loader function. It should open the image and load it. Call
    the image loader function to load both input images.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个图像加载函数。它应该打开并加载图像。调用图像加载函数以加载两个输入图像。
- en: To be able to display the images, set the transformations to revert the normalization
    of the images and to convert the tensors into PIL images.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了能够显示这些图片，设置转换以恢复图片的归一化，并将张量转换为PIL图像。
- en: Create a function capable of performing the previous transformation over the
    tensors. Call the function for both images and plot the results.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个能够在张量上执行先前转换的函数。为两个图像调用该函数并绘制结果。
- en: Load the VGG-19 model.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载VGG-19模型。
- en: Create a dictionary mapping the index of the relevant layers (keys) to a name
    (values). Then, create a function to extract the feature maps of the relevant
    layers. Use them to extract the features of both input images.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个将相关层的索引（键）映射到名称（值）的字典。然后，创建一个函数来提取相关层的特征映射。使用它们来提取两个输入图像的特征。
- en: Calculate the gram matrix for the style features. Also, create the initial target
    image.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算风格特征的Gram矩阵。同时，创建初始目标图像。
- en: Set the weights for the different style layers, as well as the weights for the
    content and style losses.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置不同风格层的权重，以及内容和风格损失的权重。
- en: Run the model for 500 iterations. Define the Adam optimization algorithm before
    starting to train the model using 0.001 as the learning rate.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行500次迭代的模型。在开始训练模型之前，定义Adam优化算法，并使用0.001作为学习率。
- en: Note
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Depending on your resources, the training process may take several hours, considering
    that, to achieve excellent results, it is recommended that you train for thousands
    of iterations. Adding print statements is a good practice to see the progress
    of the training process.
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据您的资源情况，训练过程可能需要数小时，考虑到为了获得出色的结果，建议进行数千次迭代的训练。添加打印语句是查看训练过程进展的良好实践。
- en: According to the previous information, the results of this chapter were achieved
    by running around 30,000 iterations, which will take a long time to run without
    a GPU (this configuration can be found on the GitHub's repository). However, to
    see some minor changes, it will suffice to run it for a couple of hundred iterations,
    as recommended in this activity (500).
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据先前的信息，本章的结果是通过运行大约30,000次迭代实现的，如果没有GPU，运行时间会很长（此配置可以在GitHub的存储库中找到）。然而，为了看到一些细微的变化，仅需运行几百次迭代即可，正如本活动中推荐的那样（500次）。
- en: Plot both the content and target images to compare the results.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制内容图像和目标图像以比较结果。
- en: Note
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 214.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第214页找到。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced style transfers, which is a popular task nowadays that
    can be solved using CNNs. It consists of taking both content and a style images
    as inputs and returning a newly created image as output that keeps the content
    of one of the images and the style of the other. It is typically used to give
    images an artistic look, by combining random regular images with those of the
    paintings of great artists.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了风格转换，这是当今流行的任务之一，可以使用CNN来解决。它包括将内容和风格图像作为输入，并返回一个新创建的图像作为输出，该图像保留了一个图像的内容和另一个图像的风格。通常用于通过将随机常规图像与伟大艺术家的绘画相结合来赋予图像艺术感。
- en: Although style transfer is resolved using CNNs, the process of creating the
    new image is not achieved by training the network conventionally. In this chapter,
    it was explained how, by using pretrained networks, it is possible to consider
    the output of some relevant layers that are especially good at identifying certain
    features.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用CNN解决了风格转换问题，但创建新图像的过程并不是通过传统训练网络来实现的。本章详细解释了如何通过使用预训练网络，考虑到一些特别擅长识别特定特征的相关层的输出。
- en: The chapter explains each of the steps for developing a code capable of performing
    the task of style transfer, where the first step consisted of loading and displaying
    the inputs. As mentioned earlier, there are two inputs to the model (the content
    and style images). Each image is to go through a series of transformations that
    aims to resize the images to equal size, convert them into tensors, and normalize
    them in order for them to be properly processed by the network.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本章解释了开发能够执行风格转换任务的代码的每个步骤，其中第一步是加载和显示输入。如前所述，模型有两个输入（内容和风格图像）。每个图像都要经历一系列转换步骤，目的是将图像调整为相同大小，转换为张量，并进行归一化，以便网络正确处理它们。
- en: Next, the pretrained model was loaded. As mentioned in this chapter, the VGG-19
    is one of the most typically used architectures to solve such tasks. It consists
    of 19 layers, including convolutional, pooling, and fully connected layers, where,
    for the task in question, only some of the convolutional layers are to be used.
    The process of loading the pretrained model is fairly simple, considering PyTorch
    provides a subpackage containing several pretrained network architectures.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载预训练模型。如本章所述，VGG-19是解决此类任务中最常用的体系结构之一。它由19层组成，包括卷积、池化和全连接层，对于所讨论的任务，仅使用其中的一些卷积层。加载预训练模型的过程相当简单，因为PyTorch提供了一个子包，其中包含几种预训练的网络体系结构。
- en: Furthermore, once the network is loaded, it was explained how certain layers
    of the network have been identified as overperformers at detecting certain features
    that are crucial for style transfer. While five different layers have the capability
    to extract features related to the style of an image, such as colors and textures,
    just one of the layers is exceptionally good at extracting content features, such
    as edges and shapes. According to this, it is crucial to define those relevant
    layers, which will be used to extract the information from the input images in
    order to create the desired target image.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一旦加载网络，就解释了如何确定网络的某些层被标识为过度表现者来检测对样式转移至关重要的某些特征。虽然有五个不同的层能够提取与图像样式相关的特征，比如颜色和纹理，但其中一个层在提取边缘和形状等内容特征方面表现异常出色。因此，定义这些相关层是至关重要的，这些层将用于从输入图像中提取信息，以创建所需的目标图像。
- en: Finally, it was time to code the iterative process capable of creating a target
    image with the desired features. To do so, three different losses were calculated.
    One for comparing the difference between the content image and the target image
    in terms of content (content loss), another one for comparing the difference in
    terms of style between the style image and the target image, which is achieved
    by the calculation of the gram matrix (the style loss). Lastly, one that combines
    both the losses (the total loss).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是编写迭代过程的时候，该过程能够创建具有所需特征的目标图像。为此，计算了三种不同的损失。一种是比较内容图像与目标图像在内容方面的差异（内容损失），另一种是比较样式图像与目标图像在样式方面的差异，这是通过计算格拉姆矩阵来实现的（样式损失）。最后一种是结合了这两种损失的总损失。
- en: The target image was then achieved by minimizing the value of the total loss,
    which can be done by updating the parameters related to the target image. Although
    a pretrained network is used, the process of arriving at an ideal target image
    may take several thousand iterations and quite some time.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过减少总损失值的方法实现了目标图像，这可以通过更新与目标图像相关的参数来完成。尽管使用了预训练网络，但到达理想的目标图像可能需要数千次迭代和相当长的时间。
