- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Machine Translation with the Transformer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Transformer 进行机器翻译
- en: Humans master sequence transduction, transferring a representation to another
    object. We can easily imagine a mental representation of a sequence. If somebody
    says *The flowers in my garden are beautiful*, we can easily visualize a garden
    with flowers in it. We see images of the garden, although we might never have
    seen that garden. We might even imagine chirping birds and the scent of flowers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人类精通序列转换，将表示转移到另一个对象上。我们可以轻松想象一个序列的心理表示。如果有人说*我的花园里的花很漂亮*，我们可以轻松地想象出一个有花的花园。我们看到花园的图像，尽管我们可能从未见过那个花园。我们甚至可能想象出鸟鸣和花香。
- en: A machine must learn transduction from scratch with numerical representations.
    Recurrent or convolutional approaches have produced interesting results but have
    not reached significant BLEU translation evaluation scores. Translating requires
    the representation of language *A* transposed into language *B*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 机器必须从头开始学习用数字表示进行转换。循环或卷积方法产生了有趣的结果，但还没有达到显著的 BLEU 翻译评估分数。翻译需要将语言 *A* 的表示转换成语言
    *B*。
- en: The transformer model’s self-attention innovation increases the analytic ability
    of machine intelligence. A sequence in language *A* is adequately represented
    before attempting to translate it into language *B*. Self-attention brings the
    level of intelligence required by a machine to obtain better BLEU scores.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型的自注意创新提高了机器智能的分析能力。在尝试将语言 *A* 的序列翻译成语言 *B* 之前，必须对其进行充分的表示。自注意带来了机器获得更好
    BLEU 分数所需的智能水平。
- en: The seminal *Attention Is All You Need* Transformer obtained the best results
    for English-German and English-French translations in 2017\. Since then, the scores
    have been improved by other transformers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意力全靠你* 变压器在 2017 年的英语-德语和英语-法语翻译中取得了最佳结果。从那以后，其他变压器的得分已经有所提高。'
- en: 'At this point in the book, we have covered the essential aspects of transformers:
    the *architecture* of the Transformer, *training* a RoBERTa model from scratch,
    *fine-tuning* a BERT, *evaluating* a fine-tuned BERT, and exploring *downstream
    tasks* with some transformer examples.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们已经涵盖了变压器的基本方面：Transformer 的 *架构*，从头开始 *训练* 一个 RoBERTa 模型，*微调* 一个
    BERT，*评估* 一个经过微调的 BERT，并探索一些变压器示例中的 *下游任务*。
- en: In this chapter, we will go through machine translation in three additional
    topics. We will first define what machine translation is. We will then preprocess
    a **Workshop on Machine Translation** (**WMT**) dataset. Finally, we will see
    how to implement machine translations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过三个额外的主题来介绍机器翻译。我们首先会定义什么是机器翻译。然后，我们将预处理一个**机器翻译研讨会**（**WMT**）数据集。最后，我们将看到如何实现机器翻译。
- en: 'This chapter covers the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Defining machine translation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义机器翻译
- en: Human transductions and translations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类转换和翻译
- en: Machine transductions and translations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器转换和翻译
- en: Preprocessing a WMT dataset
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理 WMT 数据集
- en: Evaluating machine translation with BLEU
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 BLEU 评估机器翻译
- en: Geometric evaluations
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几何评估
- en: Chencherry smoothing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chencherry 平滑
- en: Introducing Google Translate’s API
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Google 翻译的 API
- en: Initializing the English-German problem with Trax
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Trax 初始化英语-德语问题
- en: Our first step will be to define machine translation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是定义机器翻译。
- en: Defining machine translation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义机器翻译
- en: '*Vaswani* et al. (2017) tackled one of the most difficult NLP problems when
    designing the Transformer. The human baseline for machine translation seems out
    of reach for us human-machine intelligence designers. This did not stop *Vaswani*
    et al. (2017) from publishing the Transformer’s architecture and achieving state-of-the-art
    BLEU results.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*Vaswani* 等人（2017年）在设计 Transformer 时解决了 NLP 中最困难的问题之一。机器翻译的人类基线似乎对我们这些人类-机器智能设计者来说是遥不可及的。这并没有阻止
    *Vaswani* 等人（2017年）发布 Transformer 的架构并取得最先进的 BLEU 结果。'
- en: 'In this section, we will define machine translation. Machine translation is
    the process of reproducing human translation by machine transductions and outputs:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义机器翻译。机器翻译是通过机器转换和输出来复制人类翻译的过程：
- en: '![](img/B17948_06_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_06_01.png)'
- en: 'Figure 6.1: Machine translation process'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.1*：机器翻译过程'
- en: 'The general idea in *Figure 6.1* is for the machine to do the following in
    a few steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.1* 中的一般思想是让机器在几个步骤中执行以下操作：'
- en: Choose a sentence to translate
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择要翻译的句子
- en: Learn how words relate to each other with hundreds of millions of parameters
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习单词如何相互关联以及数以亿计的参数
- en: Learn the many ways in which words refer to each other
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习单词之间相互指代的多种方式
- en: Use machine transduction to transfer the learned parameters to new sequences
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器传导将学习的参数传递给新的序列
- en: Choose a candidate translation for a word or sequence
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为单词或序列选择候选翻译
- en: The process always starts with a sentence to translate from a source language,
    *A*. The process ends with an output containing a translated sentence in language
    *B*. The intermediate calculations involve transductions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程总是以从源语言*A*翻译的句子开始。过程以包含语言*B*中翻译的句子的输出结束。中间计算涉及传导。
- en: Human transductions and translations
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类传导和翻译
- en: A human interpreter at the European Parliament, for instance, will not translate
    a sentence word by word. Word-by-word translations often make no sense because
    they lack the proper *grammatical structure* and cannot produce the right translation
    because the *context* of each word is ignored.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，欧洲议会的人类口译员不会逐字逐句地翻译句子。逐字翻译通常毫无意义，因为它们缺乏适当的*语法结构*，并且无法产生正确的翻译，因为忽略了每个词的*上下文*。
- en: Human transduction takes a sentence in language *A* and builds a cognitive *representation*
    of the sentence’s meaning. An interpreter (oral translations) or a translator
    (written translations) at the European Parliament will only then transform that
    transduction into an interpretation of that sentence in language *B*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人类传导将语言*A*中的一句话构建成一种认知*表征*这句话的含义。欧洲议会的口译员（口头翻译）或翻译员（书面翻译）只有在此之后才会将该传导转化为语言*B*中对该句话的解释。
- en: We will name the translation done by the interpreter or translator in language
    *B* a *reference* sentence.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将口译员或翻译员在语言*B*中完成的翻译称为*参考*句子。
- en: You will notice several references in the *Machine translation process* described
    in *Figure 6.1*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.1*中描述的*机器翻译过程*中，您会注意到几个参考资料。
- en: A human translator will not translate sentence *A* into sentence *B* several
    times but only once in real life. However, more than one translator could translate
    sentence *A* in real life. For example, you can find several French to English
    translations of *Les Essais* by Montaigne. If you take one sentence, *A*, out
    of the original French version, you will thus find several versions of sentence
    *B* noted as references `1` to `n`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人类翻译员在现实生活中不会将句子*A*翻译为句子*B*多次，而只会翻译一次。然而，在现实生活中可能会有不止一个翻译员翻译句子*A*。例如，你可以找到蒙田的《论文》的多个法语到英语的翻译。如果你从原始法语版本中取出句子*A*，你会发现被标注为引用`1`到`n`的句子*B*的几个版本。
- en: 'If you go to the European Parliament one day, you might notice that the interpreters
    only translate for a limited time of two hours, for example. Then another interpreter
    takes over. No two interpreters have the same style, just like writers have different
    styles. Sentence *A* in the source language might be repeated by the same person
    several times in a day but be translated into several reference sentence *B* versions:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一天你去欧洲议会，你可能会注意到口译员只会在有限的时间内翻译两个小时，例如。然后另一个口译员接替。没有两个口译员有相同的风格，就像作家有不同的风格一样。源语言中的句子*A*可能由同一个人在一天中重复几次，但会被翻译成几个参考句子*B*版本：
- en: '*reference* ={*reference 1*, *reference 2*,…*reference n*}'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*参考* = {*参考 1*, *参考 2*,…*参考 n*}'
- en: Machines have to find a way to think the same way as human translators.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 机器必须找到一种以与人类翻译员相同的方式思考的方法。
- en: Machine transductions and translations
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器传导和翻译
- en: The transduction process of the original Transformer architecture uses the encoder
    stack, the decoder stack, and all the model’s parameters to represent a *reference
    sequence*. We will refer to that output sequence as the *reference*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Transformer架构的传导过程使用编码器堆栈、解码器堆栈和所有模型参数来表示一个*参考序列*。我们将称之为*参考*的输出序列。
- en: Why not just say “output prediction”? The problem is that there is no single
    output prediction. The Transformer, like humans, will produce a result we can
    refer to, but that can change if we train it differently or use different transformer
    models!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不直接说“输出预测”？问题在于没有单一的输出预测。Transformer，就像人类一样，将产生一个我们可以参考的结果，但如果我们对其进行不同的训练或使用不同的Transformer模型，结果可能会改变！
- en: We immediately realize that the human baseline of human transduction, representations
    of a language sequence, is quite a challenge. However, much progress has been
    made.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即意识到人类语言序列转换的基线是一个相当大的挑战。然而，已经取得了很大的进步。
- en: An evaluation of machine translation proves that NLP has progressed. To determine
    that one solution is better than another, each NLP challenger, lab, or organization
    must refer to the same datasets for the comparison to be valid.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译的评估证明了自然语言处理的进步。为了确定一个解决方案是否比另一个更好，每个自然语言处理挑战者、实验室或组织必须参考相同的数据集，以便比较是有效的。
- en: Let’s now explore a WMT dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索一个 WMT 数据集。
- en: Preprocessing a WMT dataset
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理一个 WMT 数据集
- en: '*Vaswani* et al. (2017) present the Transformer’s achievements on the WMT 2014
    English-to-German translation task and the WMT 2014 English-to-French translation
    task. The Transformer achieves a state-of-the-art BLEU score. BLEU will be described
    in the *Evaluating machine translation with BLEU* section of this chapter.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*Vaswani* 等人（2017）展示了 Transformer 在 WMT 2014 英译德和 WMT 2014 英译法任务上的成就。Transformer
    实现了最先进的 BLEU 分数。BLEU 将在本章的*使用 BLEU 评估机器翻译*部分进行描述。'
- en: The 2014 WMT contained several European language datasets. One of the datasets
    contained data taken from version 7 of the Europarl corpus. We will be using the
    French-English dataset from the *European Parliament Proceedings Parallel Corpus*,
    1996-2011 ([https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2014 年的 WMT 包含了几个欧洲语言数据集。其中一个数据集包含了从欧洲议会语料库第 7 版中获取的数据。我们将使用来自*欧洲议会会议平行语料库*1996-2011
    年的法语-英语数据集（[https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)）。
- en: 'Once you have downloaded the files and have extracted them, we will preprocess
    the two parallel files:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你下载了文件并解压缩它们，我们将预处理这两个平行文件：
- en: '`europarl-v7.fr-en.en`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`europarl-v7.fr-en.en`'
- en: '`europarl-v7.fr-en.fr`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`europarl-v7.fr-en.fr`'
- en: We will load, clear, and reduce the size of the corpus.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载、清理并缩小语料库的大小。
- en: Let’s start the preprocessing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始预处理。
- en: Preprocessing the raw data
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理原始数据
- en: In this section, we will preprocess `europarl-v7.fr-en.en` and `europarl-v7.fr-en.fr`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将预处理`europarl-v7.fr-en.en`和`europarl-v7.fr-en.fr`。
- en: Open `read.py`, which is in this chapter’s GitHub directory. Ensure that the
    two europarl files are in the same directory as `read.py`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 打开本章的 GitHub 目录中的`read.py`。确保两个 europarl 文件与`read.py`在同一目录中。
- en: 'The program begins using standard Python functions and `pickle` to dump the
    serialized output files:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 程序开始使用标准的 Python 函数和`pickle`来转储序列化输出文件：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we define the function to load the file into memory:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义将文件加载到内存中的函数：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The loaded document is then split into sentences:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 文档加载后被分割成句子：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The shortest and the longest lengths are retrieved:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 检索到最短和最长的长度：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The imported sentence lines must be cleaned to avoid training useless and noisy
    tokens. The lines are normalized, tokenized on white spaces, and converted to
    lowercase. The punctuation is removed from each token, non-printable characters
    are removed, and tokens containing numbers are excluded. The cleaned line is stored
    as a string.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的句子行必须经过清理以避免训练无用和嘈杂的标记。行被规范化，以空格分词，并转换为小写。从每个标记中删除标点符号，删除不可打印的字符，并排除包含数字的标记。清理后的行被存储为字符串。
- en: 'The program runs the cleaning function and returns clean appended strings:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 程序运行清理函数并返回干净的附加字符串：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We have defined the key functions we will call to prepare the datasets. The
    English data is loaded and cleaned first:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了将调用来准备数据集的关键函数。首先加载并清理英文数据：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The dataset is now clean, and `pickle` dumps it into a serialized file named
    `English.pkl`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集现在已经干净，`pickle`将其转储到名为`English.pkl`的序列化文件中：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output shows the key statistics and confirms that `English.pkl` is saved:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示关键统计信息，并确认`English.pkl`已保存：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We now repeat the same process with the French data and dump it into a serialized
    file named `French.pkl`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用相同的过程处理法语数据，并将其转储到名为`French.pkl`的序列化文件中：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output shows the key statistics for the French dataset and confirms that
    `French.pkl` is saved.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示法语数据集的关键统计信息，并确认`French.pkl`已保存。
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The main preprocessing is done. But we still need to make sure the datasets
    do not contain noisy and confusing tokens.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的预处理已经完成。但我们仍需要确保数据集不包含嘈杂和混乱的标记。
- en: Finalizing the preprocessing of the datasets
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成数据集的预处理
- en: 'Now, open `read_clean.py` in the same directory as `read.py`. Our process now
    defines the function that will load the datasets that were cleaned up in the previous
    section and then save them once the preprocessing is finalized:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在与`read.py`相同目录中打开`read_clean.py`。我们的流程现在定义了一个函数，该函数将加载前一部分清理过的数据集，然后在预处理完成后保存它们：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now define a function that will create a vocabulary counter. It is important
    to know how many times a word is used in the sequences we will parse. For example,
    if a word is only used once in a dataset containing two million lines, we will
    waste our energy using precious GPU resources to learn it. Let’s define the counter:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义一个函数，该函数将创建一个词汇计数器。知道我们将解析的序列中每个单词的使用次数非常重要。例如，如果一个词在包含两百万行的数据集中只使用一次，我们将浪费我们的能量使用宝贵的GPU资源来学习它。让我们定义计数器：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The vocabulary counter will detect words with a frequency that is below `min_occurrence`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇计数器将检测频率低于`min_occurrence`的单词：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, `min_occurrence=5` and the words below or equal to this threshold
    have been removed to avoid wasting the training model’s time analyzing them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`min_occurrence=5`，低于或等于此阈值的单词已被移除，以避免浪费训练模型分析它们的时间。
- en: 'We now have to deal with **Out-Of-Vocabulary** (**OOV**) words. OOV words can
    be misspelled words, abbreviations, or any word that does not fit standard vocabulary
    representations. We could use automatic spelling, but it would not solve all of
    the problems. For this example, we will simply replace OOV words with the `unk`
    (unknown) token:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须处理**Out-Of-Vocabulary**（**OOV**）单词。 OOV词可以是拼写错误的单词，缩写词或任何不符合标准词汇表示的单词。我们可以使用自动拼写，但它不会解决所有问题。在本例中，我们将简单地用`unk`（未知）标记替换OOV词：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will now run the functions for the English dataset, save the output, and
    then display `20` lines:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将运行针对英语数据集的函数，保存输出，然后显示`20`行：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output functions first show the vocabulary compression obtained:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出函数首先显示了获得的词汇压缩：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preprocessed dataset is saved. The output function then displays `20` lines,
    as shown in the following excerpt:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的数据集已保存。输出函数然后显示`20`行，如下摘录所示：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s now run the functions for the French dataset, save the output, and then
    display `20` lines:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行针对法语数据集的函数，保存输出，然后显示`20`行：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output functions first show the vocabulary compression obtained:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出函数首先显示了获得的词汇压缩：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preprocessed dataset is saved. The output function then displays `20` lines,
    as shown in the following excerpt:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的数据集已保存。输出函数然后显示`20`行，如下摘录所示：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This section shows how raw data must be processed before training. The datasets
    are now ready to be plugged into a transformer to be trained.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分显示了在训练之前必须处理原始数据的方法。数据集现在已准备好插入变压器进行训练。
- en: Each line of the French dataset is the *sentence* to translate. Each line of
    the English dataset is the *reference* for a machine translation model. The machine
    translation model must produce an *English candidate translation* that matches
    the *reference*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 法语数据集的每一行都是要翻译的*句子*。 英语数据集的每一行都是机器翻译模型的*参考*。 机器翻译模型必须产生一个与*参考*匹配的*英文候选翻译*。
- en: BLEU provides a method to evaluate `candidate` translations produced by machine
    translation models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU提供了一种评估机器翻译模型生成的`候选`翻译的方法。
- en: Evaluating machine translation with BLEU
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BLEU评估机器翻译
- en: '*Papineni* et al. (2002) came up with an efficient way to evaluate a human
    translation. The human baseline was difficult to define. However, they realized
    that we could obtain efficient results if we compared human translation with machine
    translation, word for word.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*Papineni*等人（2002年）提出了一种评估人类翻译的高效方法。定义人类基线很困难。但是，他们意识到，如果我们逐字比较人类翻译和机器翻译，我们可以获得有效的结果。'
- en: '*Papineni* et al. (2002) named their method the **Bilingual Evaluation Understudy
    Score** (**BLEU**).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*Papineni*等人（2002年）将他们的方法命名为**双语评估助手分数**（**BLEU**）。'
- en: 'In this section, we will use the **Natural Language Toolkit** (**NLTK**) to
    implement BLEU:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用**自然语言工具包**（**NLTK**）实现BLEU：
- en: '[http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu)'
- en: We will begin with geometric evaluations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从几何评估开始。
- en: Geometric evaluations
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 几何评估
- en: The BLEU method compares the parts of a candidate sentence to a reference sentence
    or several reference sentences.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU 方法将候选句子的部分与参考句子或多个参考句子进行比较。
- en: Open `BLEU.py`, which is in the chapter directory of the GitHub repository of
    this book.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 打开这本书的 GitHub 存储库的章节目录中的`BLEU.py`。
- en: 'The program imports the `nltk` module:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 程序导入了`nltk`模块：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It then simulates a comparison between a candidate translation produced by the
    machine translation model and the actual translation(s) references in the dataset.
    Remember that a sentence could have been repeated several times and translated
    by different translators in different ways, making it challenging to find efficient
    evaluation strategies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它模拟了机器翻译模型生成的候选翻译与数据集中实际翻译参考之间的比较。 请记住，一句话可能已经多次重复，并且以不同的方式被不同的译者翻译，这使得找到有效的评估策略变得具有挑战性。
- en: 'The program can evaluate one or more references:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序可以评估一个或多个参考：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The score for both examples is `1`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个例子的得分都是`1`：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A straightforward evaluation *P* of the candidate *C*, the reference *R*, and
    the number of correct tokens found in *C (N)* can be represented as a geometric
    function:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 候选*C*、参考*R*以及在*C(N)*中找到的正确令牌数量的简单评估*P*可以表示为几何函数：
- en: '![](img/B17948_06_03.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_06_03.png)'
- en: 'This geometric approach is rigid if you are looking for a 3-gram overlap, for
    example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找 3 克重叠，例如：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is severe if you are looking for 3-gram overlaps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找 3 克重叠，则输出会很严重：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A human can see that the score should be `1` and not `0.7`. The hyperparameters
    can be changed, but the approach remains rigid.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人可以看出得分应该是`1`而不是`0.7`。 超参数可以更改，但方法仍然保持不变。
- en: The warning in the code above is a good one that announces the next section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码中的警告是一个很好的警告，它预告了下一节的内容。
- en: The messages may vary with each version of the program and each run since this
    is a stochastic process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个程序版本和每次运行的消息可能会有所不同，因为这是一个随机过程。
- en: '*Papineni* et al. (2002) came up with a modified unigram approach. The idea
    was to count the word occurrences in the reference sentence and ensure the word
    was not over-evaluated in the candidate sentence.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*Papineni*等人（2002年）提出了一种修改过的一元方法。 思路是统计参考句子中的词出现次数，并确保在候选句子中不会过度评估该词。'
- en: 'Consider the following example explained by *Papineni* et al. (2002):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑由*Papineni*等人（2002年）解释的以下示例：
- en: '`Reference 1: The cat is on the mat.`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`参考 1：地板上有一只猫。`'
- en: '`Reference 2: There is a cat on the mat.`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`参考 2：垫子上有一只猫。`'
- en: 'Now consider the following candidate sequence:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下候选序列：
- en: '`Candidate: the the the the the the the`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`候选：the the the the the the the`'
- en: We now look for the number of words in the candidate sentence (the `7` occurrences
    of the same word “the”) present in the `Reference 1` sentence (`2` occurrences
    of the word “the”).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来找出候选句子中的单词数（相同单词“the”的`7`次出现）在`参考 1`句子中（单词“the”的`2`次出现）的情况。
- en: A standard unigram precision would be `7/7`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的一元精确度将是`7/7`。
- en: The modified unigram precision is `2/7`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的一元精确度为`2/7`。
- en: Note that the BLEU function output warning agrees and suggests using smoothing.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，BLEU 函数的输出警告表示同意并建议使用平滑。
- en: Let’s add smoothing techniques to the BLEU toolkit.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将平滑技术添加到 BLEU 工具包中。
- en: Applying a smoothing technique
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用平滑技术
- en: '*Chen* and *Cherry* (2014) introduced a smoothing technique that improves standard
    BLEU techniques’ geometric evaluation approach.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chen*和*Cherry*（2014年）引入了一种平滑技术，可以改进标准 BLEU 技术的几何评估方法。'
- en: Label smoothing is a very efficient method that improves the performance of
    a transformer model during the training phase. It has a negative impact on perplexity.
    However, it forces the model to be more uncertain. In turn, this has a positive
    effect on accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑是一种非常有效的方法，可以在训练阶段改进变压器模型的性能。 这对困惑度产生了负面影响。 然而，它迫使模型更加不确定。 这反过来对准确性产生积极影响。
- en: 'For example, suppose we have to predict what the masked word is in the following
    sequence:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们必须预测以下序列中的掩码单词是什么：
- en: '`The cat [mask] milk`.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`猫[mask]了牛奶。`'
- en: 'Imagine the output comes out as a softmax vector:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下输出以 softmax 向量的形式出现：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This would be a brutal approach. Label smoothing can make the system more open-minded
    by introducing epsilon = ![](img/B17948_06_001.png).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个残酷的方法。 标签平滑可以通过引入 epsilon = ![](img/B17948_06_001.png) 使系统更开放。
- en: The number of elements of `candidate_softmax` is *k=4*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`candidate_softmax`的元素数量为*k=4*。'
- en: For label smoothing, we can set ![](img/B17948_06_001.png) to `0.25`, for example.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标签平滑，我们可以将 ![](img/B17948_06_001.png) 设置为 `0.25`，例如。
- en: One of the several approaches to label smoothing can be a straightforward function.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑的几种方法之一可以是一个直观的函数。
- en: First, reduce the value of `candidate_one_hot` by ![](img/B17948_06_003.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将 `candidate_one_hot` 的值减小 ![](img/B17948_06_003.png)。
- en: Increase the `0` values by ![](img/B17948_06_004.png).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `0` 值增加 ![](img/B17948_06_004.png)。
- en: 'We obtain the following result if we apply this approach:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应用这种方法，我们将得到以下结果：
- en: '`candidate_smoothed=[0.75,0.083,0.083,0.083]`, making the output open to future
    transformations and changes.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`candidate_smoothed=[0.75,0.083,0.083,0.083]`，使输出对未来的变换和更改开放。'
- en: The transformer uses variants of label smoothing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器使用标签平滑的变体。
- en: A variant of BLEU is chencherry smoothing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU 的一个变体是 chencherry 平滑。
- en: Chencherry smoothing
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Chencherry 平滑
- en: '*Chen* and *Cherry* (2014) introduced an interesting way of smoothing candidate
    evaluations by adding ![](img/B17948_06_001.png) to otherwise `0` values. There
    are several chencherry (Boxing Chen + Colin Cherry) methods: [https://www.nltk.org/api/nltk.translate.html](https://www.nltk.org/api/nltk.translate.html).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chen* 和 *Cherry*（2014）介绍了一种通过将 ![](img/B17948_06_001.png) 添加到否则为 `0` 的值来平滑候选评估的有趣方法。有几种
    chencherry（Boxing Chen + Colin Cherry）方法：[https://www.nltk.org/api/nltk.translate.html](https://www.nltk.org/api/nltk.translate.html)。'
- en: 'Let’s first evaluate a French-English example with smoothing:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先评估一个带有平滑的法语 - 英语示例：
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Although a human could accept the candidate, the output score is weak:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人类可以接受候选者，但输出分数较低：
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let’s add some openminded smoothing to the evaluation:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对评估添加一些开放的平滑：
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The score does not reach human acceptability:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 分数未达到人类可接受程度：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have now seen how a dataset is preprocessed and how BLEU evaluates machine
    translations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了数据集是如何预处理的以及 BLEU 如何评估机器翻译。
- en: Translation with Google Translate
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用谷歌翻译进行翻译
- en: Google Translate, [https://translate.google.com/](https://translate.google.com/),
    provides a ready-to-use interface for translations. Google is progressively introducing
    a transformer encoder into its translation algorithms. In the following section,
    we will implement a transformer model for a translation task with Google Trax.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌翻译，[https://translate.google.com/](https://translate.google.com/)，提供了一个用于翻译的即用界面。谷歌正在逐步将变换器编码器引入其翻译算法中。在接下来的部分中，我们将使用谷歌
    Trax 实现一个翻译任务的变换器模型。
- en: However, an AI specialist may not be required at all.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，可能根本不需要 AI 专家。
- en: 'If we enter the sentence analyzed in the previous section in Google Translate,
    `Levez-vous svp pour cette minute de silence`, we obtain an English translation
    in real time:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在谷歌翻译中输入前一节中分析的句子，`Levez-vous svp pour cette minute de silence`，我们将实时获得英文翻译：
- en: '![](img/B17948_06_02.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_06_02.png)'
- en: 'Figure 6.2: Google Translate'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：谷歌翻译
- en: The translation is correct.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译是正确的。
- en: Does Industry 4.0 still require AI specialists for translation tasks or simply
    a web interface developer?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 工业 4.0 是否仍需要 AI 专家进行翻译任务，或者只需要一个网络界面开发人员？
- en: 'Google provides every service required for translations on their Google Translate
    platform: [https://cloud.google.com/translate](https://cloud.google.com/translate):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在其谷歌翻译平台上提供了翻译所需的每项服务：[https://cloud.google.com/translate](https://cloud.google.com/translate)：
- en: 'A translation API: A web developer can create an interface for a customer'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个翻译 API：一个网络开发人员可以为客户创建一个接口
- en: A media translation API that can translate your streaming content
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以翻译流媒体内容的媒体翻译 API
- en: An AutoML translation service that will train a custom model for a specific
    domain
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 AutoML 翻译服务，将为特定领域训练一个定制模型
- en: A Google translate project requires a web developer for the interfaces, a **Subject
    Matter Expert** (**SME**), and perhaps a linguist. However, an AI specialist is
    not a prerequisite.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一个谷歌翻译项目需要一个网络开发人员来处理界面，一个**专业主题专家**（**SME**），也许还需要一个语言学家。但是，AI 专家不是必需的。
- en: 'Industry 4.0 is going toward AI as a service. So why bother studying AI development
    with transformers? There are two important reasons to become an Industry 4.0 AI
    specialist:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 工业 4.0 正朝着 AI 作为一种服务的方向发展。那么为什么要费心学习具有变换器的 AI 开发呢？成为工业 4.0 AI 专家有两个重要的原因：
- en: In real life, AI projects often run into unexpected problems. For example, Google
    Translate might not fit a specific need no matter how much goodwill was put into
    the project. In that case, Google Trax will come in handy!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实生活中，AI 项目经常遇到意想不到的问题。例如，谷歌翻译可能无论投入多少善意都无法满足特定需求。在这种情况下，谷歌 Trax 将会派上用场！
- en: To use Google Trax for AI, you need to be an AI developer!
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用谷歌Trax进行AI，您需要是一个AI开发者！
- en: You never know! The Fourth Industrial Revolution is connecting everything to
    everything. Some AI projects might run smoothly, and some will require AI expertise
    to solve complex problems. For example, in *Chapter 14*, *Interpreting Black Box
    Transformer Models*, we will show how AI development is sometimes required to
    implement Google Translate.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你永远不知道！第四次工业革命正在将一切与一切连接起来。一些AI项目可能会顺利运行，而一些则需要AI专业知识来解决复杂问题。例如，在*第14章*中，*解释黑匣子变压器模型*，我们将展示有时需要AI开发来实现Google翻译。
- en: We are now ready to implement translations with Trax.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用Trax进行翻译。
- en: Translations with Trax
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Trax进行翻译
- en: Google Brain developed **Tensor2Tensor** (**T2T**) to make deep learning development
    easier. T2T is an extension of TensorFlow and contains a library of deep learning
    models that contains many transformer examploes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌大脑开发了**Tensor2Tensor**（**T2T**）来使深度学习开发更容易。T2T是TensorFlow的一个扩展，包含了一个深度学习模型库，其中包含许多变压器示例。
- en: Although T2T was a good start, Google Brain then produced Trax, an end-to-end
    deep learning library. Trax contains a transformer model that can be applied to
    translations. The Google Brain team presently maintains Trax.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然T2T是一个很好的开始，但谷歌大脑随后推出了Trax，一个端到端的深度学习库。Trax包含一个可以应用于翻译的变压器模型。谷歌大脑团队目前负责维护Trax。
- en: This section will focus on the minimum functions to initialize the English-German
    problem described by *Vaswani* et al. (2017) to illustrate the Transformer’s performance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分将重点介绍初始化由*瓦斯瓦尼*等人（2017年）描述的英德问题所需的最小功能，以说明变压器的性能。
- en: We will be using preprocessed English and German datasets to show that the Transformer
    architecture is language-agnostic.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用预处理的英语和德语数据集来表明变压器架构是语言无关的。
- en: Open `Trax_Translation.ipynb`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Trax_Translation.ipynb`。
- en: We will begin by installing the modules we need.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始安装我们需要的模块。
- en: Installing Trax
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Trax
- en: 'Google Brain has made Trax easy to install and run. We will import the basics
    along with Trax, which can be installed in one line:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌大脑已经使Trax易于安装和运行。我们将导入基础知识和Trax，可以在一行中安装：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Yes, it’s that simple!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，就是这么简单！
- en: Now, let’s create our transformer model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的变压器模型。
- en: Creating the original Transformer model
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建原始的变压器模型
- en: We will create the original Transformer model as described in *Chapter 2*, *Getting
    Started with the Architecture of the Transformer Model*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建原始的变压器模型，就像*第2章*，*开始使用变压器模型架构*中描述的那样。
- en: 'Our Trax function will retrieve a pretrained model configuration in a few lines
    of code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Trax函数将在几行代码中检索预训练模型配置：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The model is the Transformer with an encoder and decoder stack. Each stack contains
    `6` layers and `8` heads. `d_model=512`, as in the architecture of the original
    Transformer.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是一个具有编码器和解码器堆栈的变压器。每个堆栈包含`6`层和`8`个头。`d_model=512`，与原始变压器的架构相同。
- en: The Transformer requires the pretrained weights to run.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器需要预训练的权重才能运行。
- en: Initializing the model using pretrained weights
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练权重初始化模型
- en: The pretrained weights contain the intelligence of the Transformer. The weights
    constitute the Transformer’s representation of language. The weights can be expressed
    as a number of parameters that will produce some form of *machine intelligence
    IQ*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练权重包含变压器的智能。权重构成了变压器对语言的表示。权重可以表示为将产生某种形式的*机器智能IQ*的参数数量。
- en: 'Let’s give life to the model by initializing the weights:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过初始化权重来给模型赋予生命：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The machine configuration and its *intelligence* are now ready to run. Let’s
    tokenize a sentence.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 机器配置及其*智能*现已准备就绪。让我们对一个句子进行分词。
- en: Tokenizing a sentence
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对句子进行分词
- en: Our machine translator is ready to tokenize a sentence. The notebook uses the
    vocabulary preprocessed by `Trax`. The preprocessing method is similar to the
    one described in this chapter’s *Preprocessing a WMT dataset* section.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器翻译器已准备好对句子进行分词。笔记本使用由`Trax`预处理的词汇表。预处理方法类似于本章中描述的*预处理WMT数据集*部分。
- en: 'The sentence will now be tokenized:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个句子将被分词：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The program will now decode the sentence and produce a translation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将解码句子并产生一个翻译。
- en: Decoding from the Transformer
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从变压器解码
- en: The Transformer encodes the sentence in English and will decode it in German.
    The model and its weights constitute its set of abilities.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器将句子编码为英语，然后将其解码为德语。模型及其权重构成了其能力集。
- en: '`Trax` has made the decoding function intuitive to use:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`Trax`已经使解码函数变得直观易用：'
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that higher temperatures will produce diverse results, just as with human
    translators, as explained in this chapter’s *Defining machine translation* section.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，较高的温度会产生多样化的结果，就像人类翻译者一样，在本章的*定义机器翻译*部分有详细解释。
- en: Finally, the program will de-tokenize and display the translation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，程序将解密并显示翻译。
- en: De-tokenizing and displaying the translation
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解密并显示翻译
- en: Google Brain has produced a mainstream, disruptive, and intuitive implementation
    of the Transformer with `Trax`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Google Brain用`Trax`开发了一个主流、颠覆性和直观的Transformer实现。
- en: 'The program now de-tokenizes and displays the translation in a few lines:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将解密并显示翻译的结果：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is quite impressive:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 输出相当令人印象深刻：
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The Transformer translated `machine intelligence` into `Maschinenübersicht`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器将`machine intelligence`翻译为`Maschinenübersicht`。
- en: 'If we deconstruct `Maschinenübersicht` into `Maschin (machine)` + `übersicht
    (intelligence)`, we can see that:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`Maschinenübersicht`拆分为`Maschin（机器）`+ `übersicht（智能）`，我们可以看到：
- en: '`über` literally means “over”'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`über`的字面意思是“over”'
- en: '`sicht` means “sight” or “view”'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sicht`的意思是“视野”或“视图”'
- en: The transformer tells us that although it is a machine, it has vision. Machine
    intelligence is growing through Transformers, but it is not human intelligence.
    Machines learn languages with an intelligence of their own.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器告诉我们，尽管它是一台机器，它能够有视觉。机器智能通过Transformer不断增长，但它不是人类智能。机器用自身的智能学习语言。
- en: That concludes our experiment with Google Trax.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对Google Trax的实验。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through three additional essential aspects of the original
    Transformer.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了原始Transformer的三个重要方面。
- en: We started by defining machine translation. Human translation sets an extremely
    high baseline for machines to reach. We saw that English-French and English-German
    translations imply numerous problems to solve. The transformer tackled these problems
    and set state-of-the-art BLEU records to beat.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了机器翻译。人工翻译为机器设立了一个极高的基准。我们看到，英语-法语和英语-德语翻译意味着需要解决大量问题。Transformer解决了这些问题，并创造了最先进的BLEU记录。
- en: We then preprocessed a WMT French-English dataset from the European Parliament
    that required cleaning. We had to transform the datasets into lines and clean
    the data up. Once that was done, we reduced the dataset’s size by suppressing
    words that occurred below a frequency threshold.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对来自欧洲议会的WMT法语-英语数据集进行了预处理，并进行了数据清理。我们必须将数据集转换成行并清理数据。做完这些之后，我们通过消除低于频率阈值的单词来减小数据集的大小。
- en: Machine translation NLP models require identical evaluation methods. Training
    a model on a WMT dataset requires BLEU evaluations. We saw that geometric assessments
    are a good basis for scoring translations, but even modified BLEU has its limits.
    We thus added a smoothing technique to enhance BLEU.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译NLP模型需要相同的评估方法。在WMT数据集上训练模型需要进行BLEU评估。我们看到了几何评估是打分翻译的一个很好的基础，但即使是修改后的BLEU也有其局限性。因此，我们添加了一种平滑技术来增强BLEU。
- en: We saw that Google Translate provides a standard translation API, a media streaming
    API, and custom AutoML model training services. Implementing Google Translate
    APIs may require no AI development if the project rolls out smoothly. If not,
    we will have to get our hands dirty, like in the old days!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，Google翻译提供了标准的翻译API、媒体流API和自定义的自动机器学习模型训练服务。如果项目顺利进行，实施Google翻译API可能不需要进行AI开发。如果不顺利，我们将不得不像从前一样动手做！
- en: We implemented an English-to-German translation transformer with Trax, Google
    Brain’s end-to-end deep learning library.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Trax，Google Brain的端到端深度学习库，实现了英语到德语的翻译转换器。
- en: 'We have now covered the main building blocks to construct transformers: architecture,
    pretraining, training, preprocessing datasets, and evaluation methods.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了构建转换器的主要构件：架构、预训练、训练、数据预处理和评估方法。
- en: In the next chapter, *The Rise of Suprahuman Transformers with GPT-3 Engines*,
    we will discover mind-blowing ways of implementing transformers with the building
    blocks we explored in the previous chapters.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*GPT-3引擎崛起的超人类Transformer*中，我们将探索使用我们在前几章中探讨的构件实现Transformer的令人惊叹的方式。
- en: Questions
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Machine translation has now exceeded human baselines. (True/False)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译现在已经超过了人类基准。（是/否）
- en: Machine translation requires large datasets. (True/False)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译需要大规模的数据集。（是/否）
- en: There is no need to compare transformer models using the same datasets. (True/False)
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有必要使用相同的数据集比较变压器模型。（True/False）
- en: BLEU is the French word for *blue* and is the acronym of an NLP metric (True/False)
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BLEU是法语单词*蓝色*的缩写，也是NLP度量的首字母缩写。（True/False）
- en: Smoothing techniques enhance BERT. (True/False)
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平滑技术增强了BERT。（True/False）
- en: German-English is the same as English-German for machine translation. (True/False)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 德英与英德的机器翻译相同。（True/False）
- en: The original Transformer multi-head attention sub-layer has 2 heads. (True/False)
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始的Transformer多头注意力子层有2个头。（True/False）
- en: The original Transformer encoder has 6 layers. (True/False)
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始的Transformer编码器有6层。（True/False）
- en: The original Transformer encoder has 6 layers but only 2 decoder layers. (True/False)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始的Transformer编码器有6层，但只有2层解码器。（True/False）
- en: You can train transformers without decoders. (True/False)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以训练没有解码器的转换器。（True/False）
- en: References
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'English-German BLEU scores with reference papers and code: [https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英德BLEU分数与参考论文和代码：[https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)
- en: 'The 2014 **Workshop on Machine Translation** (**WMT**): [https://www.statmt.org/wmt14/translation-task.html](https://www.statmt.org/wmt14/translation-task.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2014年**机器翻译研讨会**（**WMT**）：[https://www.statmt.org/wmt14/translation-task.html](https://www.statmt.org/wmt14/translation-task.html)
- en: '*European Parliament Proceedings Parallel Corpus 1996-2011*, parallel corpus
    French-English: [https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*欧洲议会议事录1996-2011*，法英平行语料库：[https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)'
- en: '*Jason Brownlee*, *Ph.D*., *How to Prepare a French-to-English Dataset for
    Machine Translation*: [https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/](https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jason Brownlee*，*博士*，*如何准备法语至英语的机器翻译数据集*：[https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/](https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/)'
- en: '*Kishore Papineni*, *Salim Roukos*, *Todd Ward*, and *Wei-Jing Zhu*, *2002*,
    *BLEU: a Method for Automatic Evaluation of Machine Translation*: [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kishore Papineni*，*Salim Roukos*，*Todd Ward* 和 *Wei-Jing Zhu*，*2002年*，*BLEU：自动评估机器翻译的方法*：[https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)'
- en: '*Jason Brownlee*, *Ph.D*., *A Gentle Introduction to Calculating the BLEU Score
    for Text in Python*: [https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jason Brownlee*，*博士*，*用Python计算文本的BLEU得分的简介*：[https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)'
- en: '*Boxing Chen* and *Colin Cherry*, 2014, *A Systematic Comparison of Smoothing
    Techniques for Sentence-Level BLEU*: [http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Boxing Chen* 和 *Colin Cherry*，2014年，*句子级BLEU平滑技术的系统比较*：[http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf)'
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, and *Illia Polosukhin*, 2017, *Attention
    Is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ashish Vaswani*，*Noam Shazeer*，*Niki Parmar*，*Jakob Uszkoreit*，*Llion Jones*，*Aidan
    N. Gomez*，*Lukasz Kaiser* 和 *Illia Polosukhin*，2017年，*注意力就是一切*：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
- en: 'Trax repository: [https://github.com/google/trax](https://github.com/google/trax)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trax存储库：[https://github.com/google/trax](https://github.com/google/trax)
- en: 'Trax tutorial: [https://trax-ml.readthedocs.io/en/latest/](https://trax-ml.readthedocs.io/en/latest/)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trax教程：[https://trax-ml.readthedocs.io/en/latest/](https://trax-ml.readthedocs.io/en/latest/)
- en: Join our book’s Discord space
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 加入该书的Discord工作区，与作者进行每月的*Ask me Anything*会话：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
