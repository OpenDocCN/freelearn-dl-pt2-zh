- en: Chapter 8. Deep Learning for Computer Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 深度学习与电脑游戏
- en: The last chapter focused on solving board games. In this chapter, we will look
    at the more complex problem of training AI to play computer games. Unlike with
    board games, the rules of the game are not known ahead of time. The AI cannot
    tell what will happen if it takes an action. It can't simulate a range of button
    presses and their effect on the state of the game to see which receive the best
    scores. It must instead learn the rules and constraints of the game purely from
    watching, playing, and experimenting.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章关注的是解决棋盘游戏问题。在本章中，我们将研究更复杂的问题，即训练人工智能玩电脑游戏。与棋盘游戏不同，游戏规则事先是不知道的。人工智能不能预测它采取行动会发生什么。它不能模拟一系列按钮按下对游戏状态的影响，以查看哪些获得最高分。它必须纯粹通过观察、玩耍和实验来学习游戏的规则和约束。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Q-learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: Experience replay
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验重演
- en: Actor-critic
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员-评论家
- en: Model-based approaches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的方法
- en: A supervised learning approach to games
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏的监督学习方法
- en: The challenge in reinforcement learning is working out a good target for our
    network. We saw one approach to this in the last chapter, policy gradients. If
    we can ever turn a reinforcement learning task into a supervised task problem,
    it becomes a lot easier. So, if our aim is to build an AI agent that plays computer
    games, one thing we might try is to look at how humans play and get our agent
    to learn from them. We can make a recording of an expert human player playing
    a game, keeping track of both the screen image and the buttons the player is pressing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的挑战在于找到我们网络的良好目标。我们在上一章中就一种方法，策略梯度。如果我们能够将强化学习任务转化为监督任务问题，那么问题就会变得容易得多。因此，如果我们的目标是构建一个玩电脑游戏的人工智能代理，我们可能会尝试观察人类的游戏方式，并让我们的代理从他们那里学习。我们可以录制一个专家玩家玩游戏的视频，同时跟踪屏幕图像和玩家按下的按钮。
- en: 'As we saw in the chapter on computer vision, deep neural networks can identify
    patterns from images, so we can train a network that has the screen as input and
    the buttons the human pressed in each frame as the targets. This is similar to
    how AlphaGo was pretrained in the last chapter. This was tried on a range of complex
    3D games, such as Super Smash Bros and Mario Tennis. Convolutional networks were
    used for their image recognition qualities, and LTSMs were used to handle the
    long-term dependencies between frames. Using this approach, a trained network
    for Super Smash Bros could defeat the in-game AI on the hardest difficulty setting:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在计算机视觉章节中所看到的，深度神经网络可以从图像中识别模式，因此我们可以训练一个以屏幕为输入，以每一帧中用户按下的按钮为目标的网络。这类似于上一章中AlphaGo的预训练。这种方法在一系列复杂的3D游戏上进行了尝试，例如《超级大乱斗》和《马里奥网球》。卷积网络用于其图像识别质量，而LTSM用于处理帧之间的长期依赖关系。使用这种方法，一个针对《超级大乱斗》训练过的网络可以在最困难的难度设置下击败游戏内AI：
- en: '![A supervised learning approach to games](img/00285.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![游戏的监督学习方法](img/00285.jpeg)'
- en: Learning from humans is a good starting point, but our aim in doing reinforcement
    learning should be to achieve super-human performance. Also, agents trained in
    this way will always be limited in what they can do, and what we really want are
    agents that can truly learn for themselves. In the rest of this chapter, we'll
    look at approaches that aim to go further than replicating human levels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从人类身上学习是一个很好的起点，但我们进行强化学习的目标应该是实现超越人类的表现。此外，用这种方式训练的智能体将永远受到其能力的限制，而我们真正想要的是能够真正自我学习的智能体。在本章的其余部分，我们将介绍一些旨在超越人类水平的方法。
- en: Applying genetic algorithms to playing games
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用遗传算法玩游戏
- en: 'For a long time, the best results and the bulk of the research in AI''s playing
    in video game environments were around genetic algorithms. This approach involves
    creating a set of modules that take parameters to control the behavior of the
    AI. The range of parameter values are then set by a selection of genes. A group
    of agents would then be created using different combinations of these genes, which
    would be run on the game. The most successful set of agent''s genes would be selected,
    then a new generation of agents would be created using combinations of the successful
    agent''s genes. Those would again be run on the game and so on until a stopping
    criteria is reached, normally either a maximum number of iterations or a level
    of performance in the game. Occasionally, when creating a new generation, some
    of the genes can be mutated to create new genes. A good example of this is *MarI/O*,
    an AI that learnt to play the classic SNES game *Super Mario World* using neural
    network genetic evolution:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，AI 在视频游戏环境中的最佳结果和大部分研究都围绕着遗传算法展开。这种方法涉及创建一组模块，这些模块接受参数以控制 AI 的行为。参数值的范围由一组基因的选择来确定。然后将创建一组代理，使用这些基因的不同组合，然后在游戏上运行。最成功的一组代理基因将被选择，然后将使用成功代理的基因的组合创建一个新的代理一代。这些代理再次在游戏上运行，直到达到停止条件，通常是达到最大迭代次数或游戏中的性能水平。偶尔，在创建新一代时，一些基因可以发生突变以创建新基因。一个很好的例子是
    *MarI/O*，这是一个使用神经网络遗传演化学习玩经典的 SNES 游戏 *超级马里奥世界* 的 AI：
- en: '![Applying genetic algorithms to playing games](img/00286.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![将遗传算法应用于游戏](img/00286.jpeg)'
- en: 'Figure 1: Learning Mario using genetic algorithms (https://www.youtube.com/watch?v=qv6UVOQ0F44)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用遗传算法学习马里奥（https://www.youtube.com/watch?v=qv6UVOQ0F44）
- en: 'The big downside of these approaches is that they require a lot of time and
    computational power to simulate all the variations of parameters. Each member
    of every generation must run through the whole game until the termination point.
    The technique also does not take advantage of any of the rich information in the
    game that a human can use. Whenever a reward or punishment is received, there
    is contextual information around the state and the actions taken, but Genetic
    algorithms only use the final result of a run to determine fitness. They are not
    so much learning as doing trial and error. In recent years, better techniques
    have been found, which take advantage of backpropagation to allow the agent to
    really learn as it plays. Like the last chapter, this one is quite code heavy;
    so if you don''t want to spend your time copying text from the pages, you can
    find all the code in a GitHub repository here: [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的一个很大的缺点是，它们需要大量的时间和计算能力来模拟所有参数的变化。每一代的每个成员都必须运行整个游戏直到终止点。该技术也没有利用游戏中人类可以使用的丰富信息。每当收到奖励或惩罚时，都会有关于状态和采取的行动的上下文信息，但遗传算法只使用运行的最终结果来确定适应度。它们不是那么多的学习而是试错。近年来，已经找到了更好的技术，利用反向传播来允许代理在玩耍时真正学习。与上一章一样，这一章也非常依赖代码；如果您不想花时间从页面上复制文本，您可以在
    GitHub 仓库中找到所有代码：[https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)。
- en: Q-Learning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q 学习
- en: 'Imagine that we have an agent who will be moving through a maze environment,
    somewhere in which is a reward. The task we have is to find the best path for
    getting to the reward as quickly as possible. To help us think about this, let''s
    start with a very simple maze environment:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们有一个代理将在一个迷宫环境中移动，其中某处有一个奖励。我们的任务是尽快找到到达奖励的最佳路径。为了帮助我们思考这个问题，让我们从一个非常简单的迷宫环境开始：
- en: '![Q-Learning](img/00287.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Q 学习](img/00287.jpeg)'
- en: 'Figure 2: A simple maze, the agent can move along the lines to go from one
    state to another. A reward of 4 is received if the agent gets to state D.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：一个简单的迷宫，代理可以沿着线移动从一个状态到另一个状态。如果代理到达状态 D，将获得奖励 4。
- en: In the maze pictured, the agent can move between any of the nodes, in both directions,
    by following the lines. The node the agent is in is its state; moving along a
    line to a different node is an action. There is a reward of **4** if the agent
    gets to the goal in state **D**. We want to come up with the optimum path through
    the maze from any starting node.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在所示的迷宫中，代理可以在任何节点之间来回移动，通过沿着线移动。代理所在的节点是它的状态；沿着线移动到不同的节点是一种行动。如果代理达到状态**D**的目标，就会得到**4**的奖励。我们希望从任何起始节点找到迷宫的最佳路径。
- en: Let's think about this problem for a moment. If moving along a line puts us
    in state **D**, then that will always be the path we want to take as that will
    give us the **4** reward in the next time step. Then going back a step, we know
    that if we get to state **C**, which has a direct route to **D**, we can get that
    4 reward.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下这个问题。如果沿着一条直线移动将我们置于状态**D**，那么这将永远是我们想要采取的路径，因为这将在下一个时间步给我们**4**的奖励。然后退回一步，我们知道如果我们到达状态**C**，它直接通往**D**，我们可以获得那个4的奖励。
- en: 'To pick the best action, we need a function that can give us the reward we
    could expect for the state that action would put us in. The name for this function
    in reinforcement learning is the Q-function:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择最佳行动，我们需要一个能够为行动让我们置于的状态提供预期奖励的函数。在强化学习中，这个函数的名称是Q函数：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As stated before, the reward for getting to state **D** is **4**. What should
    be the reward for getting to state **C**? From state **C**, a single action can
    be taken to move to state **D** and get a reward of **4**, so perhaps we could
    set the reward for **C** as **4**. But if we take a series of random actions in
    our maze pictured, we will always eventually reach state **D**, which would mean
    each action gives equal reward because from any state, we will eventually reach
    the reward of **4** in state **D**.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，到达状态**D**的奖励是**4**。那么到达状态**C**的奖励应该是多少呢？从状态**C**，可以采取一个行动转移到状态**D**并获得**4**的奖励，所以也许我们可以将**C**的奖励设为**4**。但是如果我们在所示的迷宫中采取一系列随机行动，我们最终总是会到达状态**D**，这意味着每个行动都会获得相同的奖励，因为从任何状态，我们最终都会到达状态**D**的**4**奖励。
- en: We want our expected reward to factor in the number of actions it will take
    to get a future reward. We will like this expectation to create the effect that
    when in state **A**, we go to state **C** directly rather than via state **B**,
    which will result in it taking longer to get to **D**. What is needed is an equation
    that factors in a future reward, but at a discount compared with reward gained
    sooner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的预期奖励考虑到获得未来奖励需要的行动数。我们希望这种期望能够产生这样的效果，即当处于状态**A**时，我们直接转移到状态**C**而不是通过状态**B**，这将导致到达**D**需要更长的时间。所需的是一个考虑到未来奖励的方程，但与更早获得的奖励相比打折。
- en: Another way of thinking about this is to think of human behavior towards money,
    which is good proxy for human behavior towards reward, in general. If given a
    choice between receiving $1 one week from now and $1 10 weeks from now, people
    will generally choose receiving the $1 sooner. Living in an uncertain environment,
    we place greater value on rewards we get with less uncertainty. Every moment we
    delay getting our reward is more time when the uncertainty of the world might
    remove our reward.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考这个问题的方式是考虑人们对待金钱的行为，这是对人们对待奖励的行为的良好代理。如果在一周后和十周后选择收到1美元的选择，人们通常会选择尽快收到1美元。生活在不确定的环境中，我们对以较少不确定性获得的奖励更加重视。我们推迟获得奖励的每一刻都是世界不确定性可能消除我们奖励的更多时间。
- en: 'To apply this to our agent, we will use the temporal difference equation for
    valuing reward; it looks as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这个应用于我们的代理，我们将使用用于评估奖励的时间差方程；它如下所示：
- en: '![Q-Learning](img/00288.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![Q-Learning](img/00288.jpeg)'
- en: In this equation, *V* is the reward for a sequence of actions taken, *r* [*t*]
    is the reward received at time *t* in this sequence, and *g* is a constant, where
    *0 < g < 1*, which will mean rewards further in the future are less valuable than
    rewards achieved sooner; this is often referred to as the discount factor. If
    we go back to our maze, this function will give a better reward to actions that
    get to the reward in one move versus those that get to the reward in two or more
    moves. If a value of 1 is used for *g*, the equation becomes simply the sum of
    reward over time. This is rarely used in practice for Q-learning; it can result
    in the agent not converging.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*V* 是采取一系列动作的奖励，*r* [*t*] 是在这个序列中在时间 *t* 收到的奖励，*g* 是一个常数，其中 *0 < g <
    1*，这意味着将来的奖励不如更早获得的奖励有价值；这通常被称为折扣因子。如果我们回到我们的迷宫，这个函数将为在一个动作中到达奖励的动作提供更好的奖励，而不是在两个或更多动作中到达奖励的动作。如果将
    *g* 的值设为 1，方程简化为随时间的奖励总和。这在 Q 学习中很少使用；它可能导致代理不收敛。
- en: Q-function
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q 函数
- en: Now that we can evaluate a path for our agent moving through the maze, how do
    we find the optimal policy? The simple answer for our maze problem is that given
    a choice of actions, we simply want the one leading to the max reward; this is
    not just for the current action but also the max action for the state that we
    would get into after the current action.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以评估代理在迷宫中移动的路径，那么如何找到最优策略呢？对于我们的迷宫问题，简单的答案是，在面临动作选择时，我们希望选择导致最大奖励的动作；这不仅适用于当前动作，还适用于当前动作后我们将进入的状态的最大动作。
- en: 'The name for this function is the Q-function. This function gives us the optimal
    action in any state if we have perfect information; it looks as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的名称是 Q 函数。如果我们有完美的信息，这个函数将给出我们在任何状态下的最优动作；它看起来如下：
- en: '![Q-function](img/00289.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![Q 函数](img/00289.jpeg)'
- en: Here, *s* is a state, *a* is an action that can be taken in that state, and
    *0 < g < 1* is the discount factor. *rewards* is a function that returns the reward
    for taking an action in a state. *actions* is a function that returns the state
    *s'* and that you transition into after taking actions *a* in state *s* and all
    the actions *a'* available in that state.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*s* 是一个状态，*a* 是在该状态下可以采取的动作，而 *0 < g < 1* 是折扣因子。*rewards* 是一个函数，它返回在某个状态下采取某个动作的奖励。*actions*
    是一个函数，它返回在状态 *s* 中采取动作 *a* 后转移到的状态 *s'* 以及在该状态下所有可用的动作 *a'*。
- en: 'Let''s see how the maze looks if we apply the Q-function to the maze with discount
    factor *g=0.5*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们将 Q 函数应用于折扣因子为 *g=0.5* 的迷宫会是什么样子：
- en: '![Q-function](img/00290.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Q 函数](img/00290.jpeg)'
- en: 'Figure 3: Simple maze, now with Q-values. the arrows show the expected reward
    for moving between the two states at each end'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：简单迷宫，现在带有 Q 值。箭头显示了在每个末端两个状态之间移动的预期奖励
- en: 'You will notice that the Q-function as shown is infinitely recursive. It is
    a hypothetical perfect Q-function, so not something we could apply in code. To
    use it in code, one approach is to simply have a maximum number of actions to
    look ahead; then it might look like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到所示的 Q 函数是无限递归的。这是一个假设的完美 Q 函数，所以不是我们可以在代码中应用的东西。为了在代码中使用它，一个方法是简单地预先设定一个最大的动作数；那么它可能是这样的：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `state` is some object that defines the state of the environment. `action`
    is some object that defines a valid action that can be taken in a state. `reward_func`
    is a function that returns the float value reward, given a state. `apply_action_func`
    returns a new state that is the result of applying a given action to a given state.
    `actions_for_state_func` is a function that returns all valid actions given a
    state.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`state` 是定义环境状态的某个对象。`action` 是定义在状态中可以采取的有效动作的某个对象。`reward_func` 是一个函数，它返回给定状态的浮点值奖励。`apply_action_func`
    返回将给定动作应用于给定状态后的新状态。`actions_for_state_func` 是一个函数，它返回给定状态的所有有效动作。
- en: The aforementioned will give good results if we don't have to worry about rewards
    far in the future and our state space is small. It also requires that we can accurately
    simulate forward from the current state to future states as we could for board
    games. But if we want to train an agent to play a dynamic computer game, none
    of these constraints is true. When presented with an image from a computer game,
    we do not know until we try what the image will be after pressing a given button
    or what reward we will receive.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不必担心未来的奖励并且我们的状态空间很小，上述方法将获得良好的结果。它还要求我们能够准确地从当前状态模拟到未来状态，就像我们可以为棋盘游戏做的那样。但是，如果我们想要训练一个代理来玩动态电脑游戏，那么这些约束都不成立。当被提供来自电脑游戏的图像时，我们不知道在按下给定按钮后图像将会变成什么，或者我们将获得什么奖励，直到我们尝试为止。
- en: Q-learning in action
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习的实践
- en: A game may have in the region of 16-60 frames per second, and often rewards
    will be received based on actions taken many seconds ago. Also, the state space
    is vast. In computer games, the state contains all the pixels on the screen used
    as input to the game. If we imagine a screen downsampled to say 80 x 80 pixels,
    all of which are single color and binary, black or white, that is still a 2^6400
    state. This makes a direct map from state to reward impractical.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个游戏可能每秒有 16-60 帧，并且经常会根据许多秒前所采取的动作来获得奖励。此外，状态空间是广阔的。在电脑游戏中，状态包含作为游戏输入的屏幕上的所有像素。如果我们想象一个屏幕被降低到
    80 x 80 像素，所有像素都是单色和二进制，黑色或白色，那仍然是 2^6400 个状态。这使得状态到奖励的直接映射变得不切实际。
- en: 'What we will need to do is learn an approximation of the Q-function. This is
    where neural networks can be used for their universal function approximation ability.
    To train our Q-function approximation, we will store all the game states, rewards,
    and actions our agent took as it plays through the game. The loss function for
    our network will be the square of the difference between its approximation of
    the reward in the previous state and the actual reward it got in the current state,
    plus its approximation of the reward for the current state it reached in the game,
    times the discount factor:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是学习 Q 函数的近似值。这就是神经网络可以应用其通用函数近似能力的地方。为了训练我们的 Q 函数近似值，我们将存储游戏状态、奖励和我们的代理在游戏中采取的行动。我们网络的损失函数将是其对前一状态的奖励的近似值与其在当前状态获得的实际奖励之间的差的平方，加上其对游戏中达到的当前状态的奖励的近似值乘以折扣因子的差的平方：
- en: '![Q-learning in action](img/00291.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning in action](img/00291.jpeg)'
- en: '*s* is the previous state, *a* is the action that was taken in that state,
    and *0 < g < 1* is the discount factor. *rewards* is a function that returns the
    reward for taking an action in a state. *actions* is a function that returns the
    *s''* state and that you transition into after taking actions *a* in state *s*
    and all the actions *a''* available in that state. *Q* is the Q-function presented
    earlier.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*s* 是先前的状态，*a* 是在该状态下采取的动作，而 *0 < g < 1* 是折扣因子。*rewards* 是返回在状态中采取行动的奖励的函数。*actions*
    是返回在状态 *s* 中采取行动后你过渡到的 *s''* 状态和该状态中所有可用的动作 *a''*。*Q* 是先前介绍的 Q 函数。'
- en: By training successive iterations in this manner, our Q-function approximator
    will slowly converge towards the true Q-function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以这种方式训练连续的迭代，我们的 Q 函数逼近器将慢慢收敛到真实的 Q 函数。
- en: 'Let''s start by training the Q-function for the worlds simplest game. The environment
    is a one-dimensional map of states. A hypothetical agent must navigate the maze
    by moving either left or right to maximize its reward. We will set up the rewards
    for each state as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先为世界上最简单的游戏训练 Q 函数。环境是一个一维状态地图。一个假设的代理必须通过向左或向右移动来最大化其奖励来导航迷宫。我们将为每个状态设置奖励如下：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we were to visualize it, it might look something like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要可视化它，它可能会看起来像这样：
- en: '![Q-learning in action](img/00292.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![Q-learning in action](img/00292.jpeg)'
- en: 'Figure 4: Simple maze game, agent can move between connected nodes and can
    get a reward of 1 in the top node.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：简单的迷宫游戏，代理可以在相连节点之间移动，并可以在顶部节点获得奖励1。
- en: 'If we were to put our agent into a space in this "maze" in position 1, he would
    have the option of moving to either position 0 or 2\. We want to build a network
    that learns the value of each state and, by extension, the value of taking an
    action that moves to that state. The first pass of training our network will learn
    just the innate rewards of each state. But on the second pass, it will use the
    information gained from the first pass to improve its estimation of the rewards.
    What we expect to see at the end of training is a pyramid shape, with the most
    value in the 1 reward space and then decreasing value on either side as we move
    away from the center to spaces where you would have to travel further, and thus
    apply more future discounting to get the reward. Here is how this looks in code
    (the full sample is in `q_learning_1d.py` in the Git repo):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把我们的代理放到这个“迷宫”中的第一个位置，他可以选择移动到0或2的位置。我们想要构建一个学习每个状态的价值的网络，并通过此可推断出采取移动到该状态的动作的价值。网络的第一次训练将仅学习每个状态的内在奖励。但在第二次训练中，它将利用从第一次训练中获得的信息来改进奖励的估计。在训练结束时，我们预期看到一个金字塔形状，在1个奖励空间中具有最大的价值，然后在离中心更远的空间上递减价值，因为您必须更进一步旅行，从而应用更多的未来折扣以获得奖励。以下是代码中的示例（完整示例在Git存储库中的`q_learning_1d.py`中）：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We create a list of `states`; the value of each item in the list is the reward
    the agent will get for moving to that position. In this example, it gets a reward
    for getting to the 5th position:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`states`列表；列表中每个项的值是代理移动到该位置时获得的奖励。在这个示例中，它获得到第5个位置的奖励：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This method will take a number and change it into a one-hot encoding for the
    space of our states, for example, 3 becomes [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法将使用一个数字，并将其转换为我们状态空间的独热编码，例如，3变为[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We create a TensorFlow `session` and placeholders for our input and targets;
    the `None` in the arrays is for the mini-batch dimension:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个TensorFlow `session`和用于输入和目标的占位符；数组中的`None`用于小批量维度：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For this simple example, we can accurately value everything just using a linear
    relationship between the state and the action reward, so we will only create an
    `output` layer that is a matrix multiplication of the `weights`. There''s no need
    for a hidden layer or any kind of non-linearity function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个简单的例子，我们可以使用状态和动作奖励之间的线性关系来准确地评估一切，所以我们只需要创建一个`output`层，它是`weights`的矩阵乘法。不需要隐藏层或任何非线性函数：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We use the MSE for the loss and standard gradient descent training. What makes
    this Q-learning is what we will eventually use as the value for the targets:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用均方误差（MSE）作为损失函数和标准梯度下降训练。这就是我们最终将用作目标值的Q-learning的一部分：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We create a `state_batch`, each item of which is each of the states in the
    game, encoded in one hot form. For example, [1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1,
    0, 0, 0, 0, 0, 0, 0], and so on. We will then train the network to approximate
    the value for each state:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`state_batch`，其中的每个项目都是游戏中的每个状态，以独热编码形式表示。例如，[1, 0, 0, 0, 0, 0, 0, 0,
    0]，[0, 1, 0, 0, 0, 0, 0, 0, 0]，以此类推。然后，我们将训练网络来逼近每个状态的值：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For each state, we now get the position we would be in if we took each possible
    action from that state. Note for the example that the states wrap around, so moving
    -1 from position 0 puts you in position 8:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个状态，我们现在获取如果我们从该状态采取每个可能动作后所在的位置。注意，在这个示例中，状态会循环，所以从位置0向-1移动会使您处于位置8：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We use our network, which is our q-function approximator to get the reward
    it thinks we will get if we were to take each of the actions, `minus_action_index`
    and `plus_action_index`, which is the reward the network thinks we would get in
    the states it puts us into:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的网络，即我们的q函数近似器，来获取它认为如果我们采取每个动作（`minus_action_index`和`plus_action_index`），我们将获得的奖励，即网络认为我们在它将我们放入的状态中能够获得的奖励：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, we have the Python version of the now familiar Q-function equation. We
    take the initial reward for moving into a state and add to it the `DISCOUNT_FACTOR`
    times the max reward we could receive for our actions taken in that state:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有了现在常见的Q函数方程的Python版本。我们将移动到一个状态的初始奖励与`DISCOUNT_FACTOR`乘以我们在该状态下采取的动作所能获得的最大奖励相加：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We add these to the `rewards_batch`, which will be used as targets for the
    training operation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些添加到`rewards_batch`中，它将用作训练操作的目标值：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We then run the actual train step once we have the full set of rewards for
    each state. If we run this script and look at the output, we can get a sense of
    how the algorithm iteratively updates. After the first training run, we see this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了每个状态的完整奖励集，我们就会运行实际的训练步骤。如果我们运行此脚本并查看输出，我们可以感受到算法是如何迭代更新的。在第一次训练运行之后，我们看到了这个：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Everything is 0, except the items on either side of the rewarding state. These
    two states now get a reward on the basis that you could move from them to the
    reward square. Go forward a few more steps and you see that the reward has started
    to spread out across the states:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都是 0，除了奖励状态两边的项目。现在这两个状态基于你可以从它们移动到奖励方块上获得奖励。再向前走几步，你会发现奖励开始在状态之间传播：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The eventual output for this program will look something like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序的最终输出将类似于这样：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, the highest reward is in the fifth spot in the array, the position
    we originally set up to have the reward. But the reward we gave was only 1; so
    why is the reward here higher than that? This is because `1.295` is the sum of
    the reward gained for being in the current space plus the reward we can get in
    the future for moving away from this space and coming back again repeatedly, with
    these future rewards reduced by our discount factor, 0.5.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，数组中最高的奖励位于第五个位置，我们最初设置为有奖励的位置。但是我们给出的奖励只有 1；那么为什么这里的奖励比这个要高呢？这是因为 `1.295`
    是在当前空间获得的奖励与我们可以在未来从这个空间移开并反复返回时获得的奖励的总和，这些未来的奖励通过我们的折扣因子 0.5 减少。
- en: 'Learning this kind of future reward to infinity is good, but rewards are often
    learned in the process of doing a task that has a fixed end. For example, the
    task might be stacking objects on a shelf that ends when either the stack collapses
    or all objects are stacked. To add this concept into our simple 1-D game, we need
    to add in terminal states. These will be states where, once reached, the task
    ends; so in contrast to every other state, when evaluating the Q-function for
    it, we would not train by adding a future reward. To make this change, first we
    need an array to define which states are terminal:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 学习这种未来的无限奖励是好的，但是奖励通常是在执行具有固定结束的任务过程中学到的。例如，任务可能是在架子上堆放物体，当堆栈倒塌或所有物体都堆放完毕时结束。要将这个概念添加到我们的简单
    1-D 游戏中，我们需要添加终端状态。这些将是达到后，任务就结束的状态；所以与其他任何状态相比，在评估其 Q 函数时，我们不会通过添加未来奖励来训练。要进行此更改，首先我们需要一个数组来定义哪些状态是终止状态：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This will be set in the fifth state, the one we get the reward from to be terminal.
    Then all we need is to modify our training code to take into account this terminal
    state:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这将设置为第五个状态，我们从中获得奖励的状态为终止状态。然后，我们只需要修改我们的训练代码以考虑这个终止状态：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we run the code again now, the output will settle to this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在再次运行代码，输出将稳定为这样：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Dynamic games
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态游戏
- en: 'Now that we have learned the world''s simplest game, let''s try learning something
    a bit more dynamic. The cart pole task is a classic reinforcement learning problem.
    The agent must control a cart, on which is balanced a pole, attached to the cart
    via a joint. At every step, the agent can choose to move the cart left or right,
    and it receives a reward of 1 every time step that the pole is balanced. If the
    pole ever deviates by more than 15 degrees from upright, then the game ends:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了世界上最简单的游戏，让我们尝试学习一些更加动态的内容。Cart pole 任务是一个经典的强化学习问题。代理必须控制一个小车，上面平衡着一个杆，通过一个关节连接到小车上。在每一步，代理可以选择将小车向左或向右移动，并且每一步平衡杆的时候都会获得奖励
    1。如果杆与竖直方向偏差超过 15 度，游戏就结束：
- en: '![Dynamic games](img/00293.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![动态游戏](img/00293.jpeg)'
- en: 'Figure 5: The cart pole task'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Cart pole 任务
- en: 'To run the cart pole task, we will use OpenAIGym, an open source project set
    up in 2015, which gives a way to run reinforcement learning agents against a range
    of environments in a consistent way. At the time of writing, OpenAIGym has support
    for running a whole range of Atari games and even some more complex games, such
    as doom, with minimum setup. It can be installed using `pip` by running this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Cart pole 任务，我们将使用 OpenAIGym，这是一个于 2015 年创建的开源项目，它以一致的方式提供了一种运行强化学习代理与一系列环境进行交互的方法。在撰写本文时，OpenAIGym
    支持运行一系列 Atari 游戏，甚至还支持一些更复杂的游戏，如 doom，而且设置最少。可以通过运行以下命令来安装它：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Running cart pole in Python can be done as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中运行 Cart pole 可以通过以下方式实现：
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `gym.make` method creates the environment that our agent will run in. Passing
    in the `"CartPole-v0"` string tells OpenAIGym that we want this to be the cart
    pole task. The returned `env` object is used to interact with the cart pole game.
    The `env.reset()` method puts the environment into its initial state, returning
    an array that describes it. Calling `env.render()` will display the current state
    visually, and subsequent calls to `env.step(action)` allow us to interact with
    the environment, returning the new states in response to the actions we call it
    with.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym.make`方法创建了我们的代理将在其中运行的环境。传入`"CartPole-v0"`字符串告诉OpenAIGym我们希望这是车杆任务。返回的`env`对象用于与车杆游戏进行交互。`env.reset()`方法将环境置于其初始状态，并返回描述它的数组。调用`env.render()`将以可视方式显示当前状态，并对`env.step(action)`的后续调用允许我们与环境进行交互，以响应我们调用它的动作返回新的状态。'
- en: 'In what ways will we need to modify our simple 1-D game code in order to learn
    the cart-pole challenge? We no longer have access to a well-defined position;
    instead, the cart pole environment gives us as input an array of four floating
    point values that describe the position and angle of the cart and pole. These
    will be the input into our neural network, which will consist of one hidden layer
    with 20 nodes and a `tanh` activation function, leading to an output layer with
    two nodes. One output node will learn the expected reward for a move left in the
    current state, the other the expected reward for a move right. Here is what that
    code looks like (the full code sample is in `deep_q_cart_pole.py` in the git repo):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要如何修改我们简单的一维游戏代码以学习车杆挑战？我们不再有一个明确定义的位置；相反，车杆环境将一个描述车和杆位置和角度的四个浮点值的数组作为输入给我们。这些将成为我们的神经网络的输入，它将由一个具有20个节点和一个`tanh`激活函数的隐藏层组成，导致一个具有两个节点的输出层。一个输出节点将学习当前状态向左移动的预期奖励，另一个输出节点将学习当前状态向右移动的预期奖励。以下是代码示例（完整的代码示例在git
    repo的`deep_q_cart_pole.py`中）：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Why one hidden layer with 20 nodes? Why use a `tanh` activation function? Picking
    hyperparameters is a dark art; the best answer I can give is that when tried,
    these values worked well. But knowing that they worked well in practice and knowing
    something about what kind of level of complexity is needed to solve the cart pole
    problem, we can make a guess about why that may guide us in picking hyperparameters
    for other networks and tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么使用一个具有20个节点的隐藏层？为什么使用`tanh`激活函数？挑选超参数是一门黑暗的艺术；我能给出的最好答案是，当尝试时，这些值表现良好。但是知道它们在实践中表现良好，以及知道一些关于解决车杆问题需要什么样的复杂程度的信息，我们可以猜测为什么这可能指导我们选择其他网络和任务的超参数。
- en: 'One rule of thumb for the number of hidden nodes in supervised learning is
    that it should be somewhere in between the number of input nodes and the number
    of output nodes. Often two-thirds of the number of inputs is a good region to
    look at. Here, however, we have chosen 20, five times larger than the number of
    input nodes. In general, there are two reasons for favoring fewer hidden nodes:
    the first is computation time, fewer units means our network is quicker to run
    and train. The second is to reduce overfitting and improve generalization. You
    will have learned from the previous chapters about overfitting and how the risk
    of having too complex a model is that it learns the training data exactly, but
    has no ability to generalize to new data points.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于在监督学习中隐藏节点数量的经验法则是，它应该在输入节点数量和输出节点数量之间。通常，输入数量的三分之二是一个不错的区域。然而，在这里，我们选择了20，是输入节点数量的五倍。一般来说，偏爱更少的隐藏节点有两个原因：第一个是计算时间，更少的单元意味着我们的网络运行和训练速度更快。第二个是减少过拟合并提高泛化能力。你已经从前面的章节中了解到了过拟合以及过于复杂模型的风险，即它完全学习了训练数据，但没有能力泛化到新数据点。
- en: In reinforcement learning, neither of these issues is as important. Though we
    care about computation time, often a lot of the bottleneck is time spent running
    the game; so a few extra nodes is of less concern. For the second issue, when
    it comes to generalization, we don't have a division of test set and training
    set, we just have an environment in which an agent gets a reward. So overfitting
    is not something we have to worry about (until we start to train agents that can
    operate across multiple environments). This is also why you often won't see reinforcement
    learning agents use regularizers. The caveat to this is that over the course of
    training, the distribution of our training set may change significantly as our
    agent changes over the course of training. There is always the risk that the agent
    may overfit to the early samples we got from our environment and cause learning
    to become more difficult later.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，这些问题都不那么重要。虽然我们关心计算时间，但通常瓶颈大部分时间花在运行游戏上；因此，额外的几个节点不太重要。对于第二个问题，在泛化方面，我们没有测试集和训练集的划分，我们只有一个环境，一个代理人在其中获得奖励。因此，过度拟合不是我们必须担心的事情（直到我们开始训练能够跨多个环境运行的代理人）。这也是为什么你经常看不到强化学习代理人使用正则化器的原因。这个警告是，随着训练过程的进行，我们的训练集的分布可能会因为我们的代理人在训练过程中的变化而发生显著变化。存在的风险是代理人可能会对我们从环境中获得的早期样本过度拟合，并导致学习在后来变得更加困难。
- en: 'Given these issues, it makes sense to choose an arbitrary large number of nodes
    in the hidden layers in order to give the maximum chances of learning complex
    interactions between inputs. But the only true way to know is testing. *Figure
    6* shows the results of running a neural network with three hidden nodes against
    the cart pole task. As you can see, though it is able to learn eventually, it
    performs a lot worse than with 20 hidden nodes as shown in *Figure 7*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些问题，选择隐藏层中的任意大数量节点是有意义的，以便给予最大可能性学习输入之间的复杂交互。但是唯一真正的方法是测试。*图6*显示了运行具有三个隐藏节点的神经网络与小车杆任务的结果。正如您所看到的，尽管它最终能够学会，但其表现远不及具有20个隐藏节点的情况，如*图7*所示：
- en: '![Dynamic games](img/00294.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![动态游戏](img/00294.jpeg)'
- en: 'Figure 6: Cart pole with three hidden nodes, y = average reward of last 10
    games, x = games played'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：具有三个隐藏节点的小车杆，y = 最近10场比赛的平均奖励，x = 已玩的比赛
- en: Why only one hidden layer? The complexity of the task can help us estimate this.
    If we think about the cart pole task, we know that we care about the inter-relationship
    of input parameters. The position of the pole may be good or bad depending on
    the position of the cart. This level of interaction means that a purely linear
    combination of weights may not be enough. This guess can be confirmed by a quick
    run, which will show that though a network with no hidden layers can learn this
    task better than random, it performs a lot less well than a single hidden layer
    network.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只有一个隐藏层？任务的复杂性可以帮助我们估计这一点。如果我们考虑小车杆任务，我们知道我们关心输入参数的相互关系。杆的位置可能好也可能不好，这取决于小车的位置。这种交互水平意味着仅仅是权重的纯线性组合可能不够。通过快速运行，这个猜测可以得到确认，它将显示出尽管没有隐藏层的网络可以比随机更好地学习这个任务，但它的表现远不如单隐藏层网络。
- en: Would a deeper network be better? Maybe, but for tasks that only have this kind
    of slight complexity, more layers tend not to improve things. Running the network
    will confirm extra hidden layers appear to make little difference. One hidden
    layer gives us the capacity we need to learn the things we want in this task.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更深的网络会更好吗？也许，但是对于只有这种轻微复杂性的任务来说，更多的层次往往不会改善事情。运行网络将确认额外的隐藏层似乎几乎没有什么影响。一个隐藏层为我们提供了我们在这个任务中所需要的容量。
- en: As for the choice of *tanh*, there are a few factors to think about. The reason
    relu activation functions have been popular for deep networks is because of saturation.
    When running a many-layered network with activation functions bounded to a narrow
    range, for example the 0 to 1 of a logistic function, lots of nodes will learn
    to activate at close to the maximum of 1\. They saturate at 1\. But we often want
    something to signal to a greater degree when it has a more extreme input. This
    is why relu has been so popular—it gives non-linearity to a layer while not bounding
    its maximum activation. This is especially important in many layered networks
    because early layers may get extreme activations that it is useful to signal forward
    to future layers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 至于选择*tanh*，有几个因素需要考虑。relu激活函数之所以在深度网络中流行，是因为饱和。当运行一个具有激活函数范围受限的多层网络时，例如 logistic
    函数的 0 到 1，许多节点会学会在接近 1 的最大值处激活。它们在 1 处饱和。但我们经常希望在输入更极端时信号更明显。这就是为什么 relu 如此流行的原因——它给一个层增加了非线性，同时不限制其最大激活。这在许多层网络中尤为重要，因为早期层可能会获得极端的激活，这对于向未来层发出信号是有用的。
- en: With only one layer, this is not a concern, so a sigmoid function makes sense.
    The output layer will be able to learn to scale the values from our hidden layer
    to what they need to be. Is there any reason to favor `tanh` over the logistic
    function? We know that our target will sometimes be negative, and that for some
    combinations of parameters can be either good or bad depending on their relative
    values. That would suggest that the range of -1 to 1 provided by the `tanh` function
    might be preferable to the logistic function, where to judge negative associations,
    the bias would first have to be learned. This is a lot of conjecture and reasoning
    after the fact; the best answer is ultimately that this combination works very
    well on this task, but hopefully, it should give some feeling for where to start
    guessing at the best hyperparameters when presented with other similar problems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个层时，这不是一个问题，所以 sigmoid 函数是合理的。输出层将能够学习将来自我们隐藏层的值缩放到它们需要的值。有没有理由更喜欢 `tanh`
    而不是 logistic 函数呢？我们知道我们的目标有时会是负数，并且对于一些参数组合，这可能是好的或坏的，具体取决于它们的相对值。这表明，提供由 `tanh`
    函数提供的 -1 到 1 的范围可能比 logistic 函数更可取，其中要判断负相关性，首先必须学习偏差。这是事后的大量推测和推理；最好的答案最终是这种组合在这个任务中非常有效，但希望它能够在面对其他类似问题时给出一些最佳超参数的开始猜测的感觉。
- en: 'To get back to the code, here is what our loss and train functions will look
    like for our cart pole task:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要回到代码，我们的损失和训练函数在我们的推车杆任务中将是这样的：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `q_value_for_state_action` variable will be the `q-value` that the network
    predicts for a given state and action. Multiplying `output_layer` by the `action_placeholder`
    vector, which will be 0 for everything except for a 1 for the action we took,
    and then summing across that means that our output will be our neural networks
    approximation for the expected value for just that action:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`q_value_for_state_action` 变量将是网络为给定状态和动作预测的 `q-value`。将 `output_layer` 乘以
    `action_placeholder` 向量，除了我们采取的动作外，其他所有值都将为 0，然后对其求和，这意味着我们的输出将是我们的神经网络对于仅仅那个动作的预期值的近似：'
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Our cost is the difference between what we think is the expected return of the
    state and action and what it should be as defined by the `target_placeholder`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的成本是我们认为的状态和动作的预期回报与由 `target_placeholder` 定义的应该是的回报之间的差异。
- en: One of the downsides to the policy gradient approach described in [Chapter 7](part0046_split_000.html#1BRPS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 7. Deep Learning for Board Games"), *Deep Learning for Board Games*,
    is that all training must be done against the environment. A set of policy parameters
    can only be evaluated by seeing its effect on the environments reward. With Q-learning,
    we are instead trying to learn how to value a state and action. As our ability
    to value specific states improves, we can use that new information to better value
    the previous states we have experienced. So, rather than always training on the
    currently experienced state, we can have our network store a history of states
    and train against those. This is known as **experience replay**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 描述在[第7章](part0046_split_000.html#1BRPS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "第7章。棋盘游戏的深度学习")*棋盘游戏的深度学习*中的策略梯度方法的一个缺点是，所有训练都必须针对环境进行。一组策略参数只能通过观察其对环境奖励的影响来评估。而在Q学习中，我们试图学习如何评估一个状态和动作的价值。随着我们对特定状态价值的理解能力提高，我们可以利用这些新信息更好地评估我们曾经经历过的先前状态。因此，与其总是在当前经历的状态上进行训练，我们可以让我们的网络存储一系列状态，并针对这些状态进行训练。这被称为**经验回放**。
- en: Experience replay
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验回放
- en: Every time we take an action and get into a new state, we store a tuple of `previous_state,
    action_taken, next_reward, next_state`, and `next_terminal`. These five pieces
    of information are all we need to run a q-learning training step. As we play the
    game, we will store this as a list of observations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们采取一个动作并进入一个新状态时，我们都会存储一个元组`previous_state, action_taken, next_reward, next_state`和`next_terminal`。这五个信息片段就足以运行一个Q学习训练步骤。当我们玩游戏时，我们将把这些信息存储为一系列观察。
- en: Another difficulty that experience replay helps solve is that in reinforcement
    learning, it can be very hard for training to converge. Part of the reason for
    this is that the data we train on is very heavily correlated. A series of states
    experienced by a learning agent will be closely related; a time series of states
    and actions leading to a reward if trained on together will have a large impact
    on the weights of the network and can undo a lot of the previous training. One
    of the assumptions of neural networks is that the training samples are all independent
    samples from a distribution. Experience replay helps with this problem because
    we can have our training mini-batches be randomly sampled from our memory, making
    it unlikely that samples are correlated.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个经验回放有助于解决的困难是，在强化学习中，训练很难收敛。部分原因是我们训练的数据之间存在非常强的相关性。学习代理人经历的一系列状态将密切相关；如果一系列状态和行动的时间序列一起训练，会对网络的权重产生很大影响，并可能撤销大部分之前的训练。神经网络的一个假设是训练样本都是来自某个分布的独立样本。经验回放有助于解决这个问题，因为我们可以让训练的小批量样本从内存中随机抽样，这样样本之间就不太可能相关。
- en: A learning algorithm that learns from memories is called an off-line learning
    algorithm. The other approach is on-line learning, in which we are only able to
    adjust the parameters based on direct play of the game. Policy gradients, genetic
    algorithms, and cross-entropy methods are all examples of this.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从记忆中学习的学习算法称为离线学习算法。另一种方法是在线学习，其中我们只能根据直接玩游戏来调整参数。策略梯度、遗传算法和交叉熵方法都是其示例。
- en: 'Here is what the code for running cart pole with experience replay looks like:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 运行带有经验回放的车杆的代码如下：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We start with our `observations` collection. A deque in Python is a queue that
    once hits capacity will start removing items from the beginning of the queue.
    Making the deque here has a maxlen of 20,000, which means we will only store the
    last 20,000 observations. We also create the last action, `np.array`, which will
    store the action we decided on from the previous main loop. It will be a one-hot
    vector:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从我们的`observations`集合开始。在Python中，一个deque是一个队列，一旦达到容量就会开始从队列开头删除项目。在这里创建deque时，其maxlen为20,000，这意味着我们只会存储最近的20,000个观察。我们还创建了最后一个动作，`np.array`，它将存储我们从上一个主循环中决定的动作。它将是一个独热向量：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is the main loop. We will first render the environment, then decide on
    an action to take based on the `last_state` we were in, then take that action
    so as to get the next state:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是主循环。我们首先渲染环境，然后根据我们所处的`last_state`决定采取什么动作，然后采取该动作以获得下一个状态：
- en: '[PRE27]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The cart pole task in OpenAIGym always gives a reward of 1 for every time step.
    We will force giving a negative reward when we hit the terminal state so the agent
    has a signal to learn to avoid it:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIGym 中的小车杆任务始终在每个时间步长给出奖励1。当我们达到终止状态时，我们将强制给予负奖励，以便代理有信号学习避免它：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We store the information for this transition in our observations array. We
    can also start training if we have enough observations stored. It is important
    to only begin training once we have a good number of samples, otherwise a few
    early observations could heavily bias training:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此转换的信息存储在我们的观察数组中。如果我们有足够的观察结果存储，我们也可以开始训练。只有在我们有足够多的样本时才开始训练非常重要，否则少量早期观察结果可能会严重偏倚训练：
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If we are in a terminal state, we need to `reset` our `env` so as to give us
    a fresh state of the game. Otherwise, we can just set `last_state` to be the `current_state`
    for the next training loop. We also now need to decide what action to take based
    on the state. Then here is the actual `train` method, using the same steps as
    our earlier 1-D example, but changed to use samples from our observations:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处于终止状态，我们需要`重置`我们的`env`以便给我们一个新的游戏状态。否则，我们可以将`last_state`设置为下一个训练循环的`current_state`。我们现在还需要根据状态决定采取什么行动。然后是实际的`train`方法，使用与我们之前的1-D示例相同的步骤，但改为使用来自我们观察的样本：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Take 100 random items from our observations; these will be the `mini_batch`
    to train on:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的观察中随机取100个项目；这些将是要进行训练的`mini_batch`：
- en: '[PRE31]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Unpack the `mini_batch` tuples into separate lists for each type of data. This
    is the format we need to feed into our neural network:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将`mini_batch`元组解包为每种类型数据的单独列表。这是我们需要馈入神经网络的格式：
- en: '[PRE32]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Get the reward for each `current_state` as predicted by our neural network.
    Output here will be an array of the size of the `mini_batch`, where each item
    is an array with two elements, the estimate of the Q-value for taking the action
    move left, and the estimate for taking the action move right. We take the max
    of these to get the estimated Q-value for the state. Successive training loops
    will improve this estimate towards the true Q-value:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 获取由我们的神经网络预测的每个`current_state`的奖励。这里的输出将是一个大小为`mini_batch`的数组，其中每个项目都是一个包含两个元素的数组，即采取左移动作的Q值估计和采取右移动作的Q值估计。我们取其中的最大值以获取状态的估计Q值。连续的训练循环将改进此估计值以接近真实的Q值：
- en: '[PRE33]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Augment the rewards we actually got with the rewards our network predicts if
    it''s a non-terminal state:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是非终止状态，将我们实际获得的奖励与我们的网络预测的奖励相结合：
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Finally, run the training operation on the network.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对网络运行训练操作。
- en: Epsilon greedy
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Epsilon 贪心
- en: Another issue Q-learning has is that initially, the network will be very poor
    at estimating the rewards of actions. But these poor action estimations are the
    things that determine the states we move into. Our early estimates may be so bad
    that we may never get into a reward state from which we would be able to learn.
    Imagine if in the cart pole the network weights are initialized so the agent always
    picks to go left and hence fails after a few time steps. Because we only have
    samples of moving left, we will never start to adjust our weights for moving right
    and so will never be able to find a state with better rewards.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的另一个问题是，最初，网络会非常糟糕地估计动作的奖励。但是这些糟糕的动作估计决定了我们进入的状态。我们早期的估计可能非常糟糕，以至于我们可能永远无法从中学习到奖励的状态。想象一下，在小车杆中，网络权重被初始化，使代理始终选择向左移动，因此在几个时间步长后失败。因为我们只有向左移动的样本，所以我们永远不会开始调整我们的权重以向右移动，因此永远无法找到具有更好奖励的状态。
- en: There are a few different solutions to this, such as giving the network a reward
    for getting into novel situations, known as novelty search, or using some kind
    of modification to seek out the actions with the greatest uncertainty.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对此有几种不同的解决方案，例如为网络提供进入新颖情境的奖励，称为新颖性搜索，或者使用某种修改来寻找具有最大不确定性的动作。
- en: 'The simplest solution and one that has been shown to work well is to start
    by choosing actions randomly so as to explore the space, and then over time as
    the network estimations get better and better, replace those random choices with
    actions chosen by the network. This is known as the epsilon greedy strategy and
    it can be used as an easy way to implement exploration for a range of algorithms.
    The epsilon here refers to the variable that is used for choosing whether a random
    action is used, greedy refers to taking the maximum action if not acting randomly.
    In the cart pole example, we will call this epsilon variable `probability_of_random_action`.
    It will start at 1, meaning 0 chance of a random action, and then at each training
    step, we will reduce it by some small amount until it hits 0:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解决方案，也是被证明效果良好的解决方案之一，是从随机选择动作开始，以便探索空间，然后随着网络估计变得越来越好，将这些随机选择替换为网络选择的动作。这被称为
    epsilon 贪心策略，它可以作为一种轻松实现一系列算法的探索方法。这里的 epsilon 是指用于选择是否使用随机动作的变量，贪心是指如果不是随机行动，则采取最大行动。在单车杆示例中，我们将这个
    epsilon 变量称为`probability_of_random_action`。它将从 1 开始，表示 0 的随机动作几率，然后在每个训练步骤中，我们将其减小一些小量，直到它为
    0 为止：
- en: '[PRE35]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the final step, we need the method that changes our neural network output
    into the action of the agent:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们需要的是将我们的神经网络输出转换为智能体动作的方法：
- en: '[PRE36]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Choose an action randomly if a random value comes up less than `probability_of_random_action`;
    otherwise choose the max output of our neural network:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机值小于`probability_of_random_action`，则随机选择一个动作；否则选择我们神经网络的最大输出：
- en: '[PRE37]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here is a graph of training progress against the cart pole task:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练进度与单车杆任务的图表：
- en: '![Epsilon greedy](img/00295.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![Epsilon greedy](img/00295.jpeg)'
- en: 'Figure 7: Cart pole task, y = average length of game over last 10 games x =
    number of games played'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：单车杆任务，y = 过去 10 次游戏的平均长度 x = 所玩游戏的数量
- en: This looks good. Success for the cart pole task is defined as being able to
    last over 200 turns. After 400 games, we beat that comfortably averaging well
    over 300 turns per game. Because we set this learning task up using OpenAIGym,
    it is now easy to set up against other games. All we need to do is change the
    `gym.make` line to take a new input game string as input and then adjust the number
    of inputs and outputs to our network to fit that game. There are a few other interesting
    control tasks in OpenAIGym, such as the pendulum and acrobat, which q-learning
    should also do well on, but as a challenge, let's look at playing some Atari games.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很不错。单车杆任务的成功定义为能够持续超过 200 轮。在 400 次游戏后，我们轻松击败了这个标准，平均每局游戏持续时间远远超过 300 轮。因为我们使用
    OpenAIGym 设置了这个学习任务，现在可以轻松地设置到其他游戏中。我们只需要将`gym.make`行更改为以新输入游戏字符串为输入，然后调整我们网络的输入和输出数量以适应该游戏。在
    OpenAIGym 中还有一些其他有趣的控制任务，例如摆锤和杂技，q-learning 在这些任务上也应该表现良好，但作为挑战，让我们来玩一些 Atari
    游戏。
- en: Atari Breakout
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari Breakout
- en: 'Breakout is a classic Atari game originally released in 1976\. The player controls
    a paddle and must use it to bounce a ball into the colored blocks at the top of
    the screen. Points are scored whenever a block is hit. If the ball travels down
    past the paddle off the bottom of the screen, the player loses a life. The game
    ends either when the all the blocks have been destroyed or if the player loses
    all three lives that he starts with:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Breakout 是一款经典的 Atari 游戏，最初于 1976 年发布。玩家控制一个挡板，必须用它将球弹到屏幕顶部的彩色方块上。每次命中一个方块都会得分。如果球下落到屏幕底部超出挡板范围，玩家将失去一条生命。游戏在所有方块被销毁或玩家失去最初的三条生命后结束：
- en: '![Atari Breakout](img/00296.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![Atari Breakout](img/00296.jpeg)'
- en: 'Figure 8: Atari Breakout'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Atari Breakout
- en: Think about how much harder learning a game like Breakout is compared to the
    cart pole task we just looked at. For cart pole, if a bad move is made that leads
    to the pole tipping over, we will normally receive feedback within a couple of
    moves. In Breakout, such feedback is much rarer. If we position our paddle wrong,
    that can be because of 20 or more moves that went into positioning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 想想学习像 Breakout 这样的游戏比我们刚刚看过的单车杆任务要难多少。对于单车杆来说，如果做出了导致杆倾斜的错误动作，我们通常会在几个动作内收到反馈。在
    Breakout 中，这样的反馈要少得多。如果我们将挡板定位不正确，可能是因为进行了 20 多次移动才导致的。
- en: Atari Breakout random benchmark
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Atari Breakout 随机基准测试
- en: 'Before we go any further, let''s create an agent that will play Breakout by
    selecting moves randomly. That way we will have a benchmark against which to judge
    out a new agent:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探讨之前，让我们创建一个通过随机选择移动来玩 Breakout 的代理程序。这样，我们将有一个基准来评价我们的新代理程序：
- en: '[PRE38]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We select our move randomly; in Breakout, the moves are as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机选择我们的移动方向；在 Breakout 中，移动的方式如下：
- en: '1: Move left'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1：向左移动
- en: '2: Stay still'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2：保持静止
- en: '3: Move right'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3：向右移动
- en: '[PRE39]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If we've come to the end of our game, then we store our score, print it, and
    call `env.reset()` to keep playing. If we let this run for a few hundred games,
    we can see that random breakout tends to score around 1.4 points per game. Let's
    see how much better we can do with Q-learning.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们已经玩了很多游戏，那么我们将存储并打印我们的分数，然后调用`env.reset()`继续玩。通过让这个运行一段时间，我们可以看到随机 Breakout
    倾向于每场比赛得分约为 1.4 分。让我们看看我们能用 Q-learning 做得更好多少。
- en: The first issue we must deal with adapting from our cart pole task is that the
    state space is so much larger. Where the cart pole input was a set of four numbers
    for Breakout, it is the full screen of 210 by 160 pixels, with each pixel containing
    three floats, one for each color. To understand the game, those pixels must be
    related to blocks, paddles, and balls, and then the interaction between those
    things must be on some level computed. To make things even more difficult, a single
    image of the screen is not enough to understand what is going on in the game.
    The ball is moving over time with velocity; to understand the best move, you cannot
    just rely on the current screen image.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须处理的第一个问题是从我们的车杆任务中进行调整的状态空间要大得多。对于车杆输入，是 210 x 160 像素的完整屏幕，每个像素包含三个浮点数，分别是每种颜色的值。要理解游戏，这些像素必须与方块、球拍和球相关联，然后这些物体之间的交互必须在某种程度上被计算。更让事情变得更加困难的是，单个屏幕图像不足以理解游戏正在发生什么。球随时间以某种速度移动；要理解最佳移动，你不能仅仅依赖于当前屏幕图像。
- en: 'There are three approaches to dealing with this: one is to use a recurrent
    neural network that will judge the current state based on the previous output.
    This approach can work, but it is a lot more difficult to train. Another approach
    is to supply the screen input as a delta between the current frame and the last.
    In *Figure 9*, you will see an example of this. Both frames have been converted
    to grayscale because the color is providing us no information in Pong. The image
    of the previous frame has been subtracted from the image of the current frame.
    This allows you to see the path of the ball and the direction in which both paddles
    are moving:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这个问题有三种方法：一种是使用递归神经网络，它将根据先前的输出来判断当前状态。这种方法可以奏效，但训练难度较大。另一种方法是将屏幕输入作为当前帧和上一帧之间的增量。在*图9*中，你会看到一个例子。由于在
    Pong 中颜色并没有提供信息，因此两个帧都已经被转换为灰度。上一帧的图像已从当前帧的图像中减去。这可以让你看到球的路径和两个球拍移动的方向：
- en: '![Atari Breakout random benchmark](img/00297.jpeg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![Atari Breakout 随机基准](img/00297.jpeg)'
- en: 'Figure 9: Pong delta images'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：Pong 游戏的增量图像
- en: This approach can work well for games such as Pong, which are only composed
    of moving elements, but for a game such as Breakout, where blocks are in fixed
    positions, we would be losing important information about the state of the world.
    Indeed, we would only ever be able to see a block for a brief flash when it was
    hit, while blocks we had not yet hit would remain invisible.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法对于只由移动元素组成的游戏（如 Pong）效果很好，但对于像 Breakout 这样的游戏，其中方块的位置是固定的，我们将丢失有关游戏状态的重要信息。事实上，我们只能在方块被击中时看到它的一瞬间，而我们尚未击中的方块将保持不可见。
- en: The third approach that we will take for Breakout is to set the current state
    to be the image of the last *n* states of the game, where *n* is 2 or more. This
    allows the neural network to have all the information it needs to make a good
    judgment about the state of the game. Using an *n* of 4 is a good default value
    for most games; but for Breakout, *n* of 2 has been found to be sufficient. It
    is good to use as low a value for *n* as possible because this reduces the number
    of parameters that our network will need.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Breakout，我们将采取的第三种方法是将当前状态设置为游戏最近 *n* 个状态的图像，其中 *n* 为 2 或更多。这允许神经网络拥有做出对游戏状态的良好判断所需的所有信息。对于大多数游戏来说，默认值
    *n* 为 4 是很好的选择；但对于 Breakout，已经发现 *n* 为 2 就足够了。尽可能使用较低的 *n* 值是很好的，因为这会减少我们的网络所需的参数数量。
- en: Preprocessing the screen
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 屏幕预处理
- en: The full code for this is in `deep_q_breakout.py` in the Git repo. But we will
    go through a few of the important modifications from the cart pole example here.
    The first is the type of neural network. For the cart pole, a network with a single
    hidden layer sufficed. But that involved four values being mapped to just two
    actions. Now we are working with `screen_width * screen_height * color_channels
    * number_of_frames_of_state = 201600` being mapped to three actions, a much higher
    level of complexity.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 全部代码在Git存储库中的`deep_q_breakout.py`中。但我们将在此处逐个讨论与杆平衡示例的一些重要修改。首先是神经网络的类型。对于杆平衡，一个具有单个隐藏层的网络就足够了。但这涉及到将四个值映射到只有两个动作。现在，我们要处理`screen_width
    * screen_height * color_channels * number_of_frames_of_state = 201600`被映射到三个动作，这是一个更高级别的复杂度。
- en: The first thing we can do to make life easier for ourselves is to resize the
    screen to a smaller size. From experimentation, we find that you can still play
    Breakout with a much smaller screen. Scaling down by a factor of 2 still allows
    you to see the ball, the paddles, and all the blocks. Also, a lot of the image
    space is not useful information for the agent. The score at the top, the gray
    patches along the sides and top, and the black space at the bottom can all be
    cropped from the image. This allows us to reduce the 210 * 160 screen into a more
    manageable 72 by 84, reducing the number of parameters by more than three quarters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的第一件事是将屏幕调整大小，以便为自己省点力。经过实验，我们发现可以将屏幕缩小后仍可玩Breakout。缩小两倍仍可看到球，球拍和所有方块。而且，图像空间中大部分都不是对代理有用的信息，顶部的得分、侧面和顶部的灰色区域，以及底部的黑色空间都可以从图像中裁剪掉。这使我们能够将210
    * 160的屏幕缩减为更容易管理的72 * 84，将参数数量减少了四分之三以上。
- en: 'Also in Breakout, the color of the pixels doesn''t contain any useful information,
    so we can replace the three colors with just a single color, which is only ever
    black or white, reducing the number of inputs again to just a third. We are now
    down to 72 by 84 = 6048 bits, and we need two frames of the game to be able to
    learn from. Let''s write a method that does this processing of the Breakout screen:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在Breakout游戏中，像素的颜色不包含任何有用的信息，所以我们可以用单一颜色代替三种颜色，这只有黑色或白色，将输入的数量再次减少到三分之一。现在我们只剩下72
    * 84 = 6048个位，需要两帧的游戏才能学习。我们现在来写一个方法来处理Breakout的屏幕：
- en: '[PRE40]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `screen_image` argument will be the Numpy array that we get from the `env.reset`
    or `env.next_step` operations on OpenAIGym. It will have shape 210 by 160 by 3,
    with each item being an `int` between 0 and 255 representing the value for that
    color:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`screen_image`参数将是我们从OpenAIGym的`env.reset`或`env.next_step`操作中获得的Numpy数组。它的形状为210
    * 160 * 3，每个项都是表示该颜色值的0到255之间的整数：'
- en: '[PRE41]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This operation on the Numpy array crops the image, so we remove the scores
    at the top, black space at the bottom, and gray areas on either side:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对Numpy数组的这个操作裁剪了图像，因此我们去掉了顶部的分数，底部的黑色空间和两侧的灰色区域：
- en: '[PRE42]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `::2` argument to a Python array means we take every second item, which
    conveniently Numpy also supports. The 0 at the end means we just take the red
    color channel, which is fine because we are about to turn it into just black and
    white anyway. `screen_image` will now be of size 72 by 84 by 1:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Python数组的`::2`参数意味着我们取每隔一个项目，幸运的是Numpy也支持这种操作。末尾的0表示我们只取红色通道，这很好，因为我们马上就要把它变成只有黑白两种颜色。`screen_image`现在将被处理成72
    * 84 * 1的大小：
- en: '[PRE43]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This sets everything that isn''t completely black in the image to 1\. This
    may not work for some games where you need precise contrast, but it works fine
    for Breakout:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这将图像中不是完全黑色的一切设为1。这在一些需要精确对比度的游戏中可能行不通，但对于Breakout游戏来说就有效了：
- en: '[PRE44]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, this returns the `screen_image` from the method making sure that the
    type is converted to float. This will save time later when we want to put our
    values into TensorFlow. *Figure 10* shows how the screen looks before and after
    preprocessing. After processing, though it is a lot less pretty, the image still
    contains all the elements you would need to play the game:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个方法返回的`screen_image`确保类型转换成浮点数。这会在以后将值放入TensorFlow时节省时间。*图10*展示了处理前后屏幕的样子。经过处理后，尽管不太美观，图像仍然包含你玩游戏所需的所有元素：
- en: '![Preprocessing the screen](img/00298.jpeg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![处理屏幕](img/00298.jpeg)'
- en: 'Figure 10: Breakout before and after processing'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：处理前后的Breakout样子
- en: This leaves our state as 72*84*2 = 12800 bits, meaning we have *2*^(*12800*)
    possible states that we need to map our three actions to. This sounds like a lot,
    but the problem is made simpler by the fact that though that is the full range
    of states possible in Breakout, only a quite small and predictable set of the
    states will occur. The paddles move horizontally across a fixed area; a single
    pixel will be active for the ball, and some number of blocks will exist across
    the central area. One could easily imagine that there are a small set of features
    that could be extracted from the image that might better relate to the actions
    we want to take—features such as the relative position of our paddle from the
    ball, the velocity of the ball, and so on—the kind of features deep neural networks
    can pick up.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们的状态为 72*84*2 = 12800 位，意味着我们需要将我们的三个动作映射到 *2*^(*12800*) 种可能的状态。这听起来很多，但问题变得更简单了，因为尽管这是打砖块游戏中可能的所有状态的完整范围，但只有一组相当小且可预测的状态会发生。挡板在固定区域水平移动；一个像素将激活球，一些方块将存在于中央区域。可以很容易地想象从图像中提取出一些特征，这些特征可能更好地与我们想要采取的动作相关联
    —— 例如，我们的挡板与球的相对位置、球的速度等 —— 这是深度神经网络可以捕捉到的特征。
- en: Creating a deep convolutional network
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个深度卷积网络
- en: 'Let''s next replace the single hidden layer network from the cart pole example
    with a deep convolutional network. Convolutional networks were first introduced
    in [Chapter 4](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning"), *Unsupervised Feature Learning*.
    A convolutional network makes sense because we are dealing with image data. The
    network we create will have three convolutional layers leading to a single flat
    layer, leading to our output. Having four hidden layers makes some intuitive sense
    because we know we are going to need to detect very abstract invariant representations
    from the pixels, but it has also been shown to work successfully for a range of
    architectures. Because this is a deep network, relu activation functions make
    sense. *Figure 11* shows what the network will look like:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们用一个深度卷积网络来替换小车摆动示例中的单隐藏层网络。卷积网络首次出现在 [第四章](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第四章。无监督特征学习")，“无监督特征学习”。卷积网络是有意义的，因为我们处理的是图像数据。我们创建的网络将有三个卷积层，导致一个单一的平坦层，导致我们的输出。使用四个隐藏层有一定的直观意义，因为我们知道我们将需要从像素中检测非常抽象的不变表示，但也已经被证明对于一系列架构是成功的。因为这是一个深度网络，relu
    激活函数是有意义的。*图 11* 显示了网络的样子：
- en: '![Creating a deep convolutional network](img/00299.jpeg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![创建一个深度卷积网络](img/00299.jpeg)'
- en: 'Figure 11: Architecture for our network that will learn to play breakout.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：我们的网络架构，将学习玩打砖块游戏。
- en: 'Here is the code for creating our deep convolutional network:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建我们的深度卷积网络的代码：
- en: '[PRE45]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'These constants will be used throughout our `create_network` method:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这些常量将在我们的 `create_network` 方法中使用：
- en: '[PRE46]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We define our input to be a product of the height, the width, and the state
    frames; the none dimension will be for batches of states:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的输入定义为高度、宽度和状态帧的乘积；none 维度将用于状态批次：
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The first convolutional layer will be an 8 by 8 widow across the width and
    height, taking in both the state frames. So it will get data on both the current
    8 by 8 section of what the image looks like and what that 8 by 8 patch looked
    like in the previous frame. Each patch will map to 32 convolutions that will be
    the input to the next layer. We give the bias a very slight positive value; this
    can be good for layers with relu activations to reduce the number of dead neurons
    caused by the relu function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个卷积层将是一个 8x8 的窗口，跨越宽度和高度，同时接收状态帧。因此，它将获得关于当前图像的 8x8 部分和上一帧中该 8x8 补丁是什么样子的数据。每个补丁将映射到
    32 个卷积，将成为下一层的输入。我们给偏置一个非常轻微的正值；这对于具有 relu 激活的层来说可能是有好处的，以减少 relu 函数造成的死神经元数量：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We put the weight and bias variables into the convolutional layer. This is
    created by the `tf.nn.conv2d` method. Setting `strides=[1, 4, 4, 1]` means the
    8 by 8 convolutional window will be applied every four pixels across the width
    and height of the image. All the convolutional layers will go through the relu
    activation function:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将权重和偏置变量放入卷积层中。这是通过 `tf.nn.conv2d` 方法创建的。设置 `strides=[1, 4, 4, 1]` 意味着 8x8
    的卷积窗口将在图像的宽度和高度上每四个像素应用一次。所有的卷积层都将通过 relu 激活函数：
- en: '[PRE49]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Creating the next two convolutional layers proceeds in the same way. Our final
    convolutional layer, `hidden_convolutional_layer_3`, must now be connected to
    a flat layer:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 创建接下来的两个卷积层的步骤与之前相同。我们的最后一个卷积层`hidden_convolutional_layer_3`现在必须连接到一个扁平层：
- en: '[PRE50]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This reshapes our convolutional layer, which is of dimensions none, 9, 11,
    64 into a single flat layer:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把我们的卷积层重新整形为单个扁平层，其维度为none，9，11，64：
- en: '[PRE51]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We then create the last two flat layers in the standard way. Note that there
    is no activation function on the final layer because we are learning the value
    of an action in a given state here, and that has an unbounded range.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着按照标准方式创建最后两个扁平层。请注意，最后一层没有激活函数，因为我们在这里学习的是给定状态下动作的价值，它具有无界范围。
- en: 'Our main loop will now need the following code added so that the current state
    is the combination of multiple frames, for breakout `STATE_FRAMES` is set to `2`,
    but higher numbers will also work:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的主循环需要添加以下代码，以便当前状态是多个帧的组合，在打砖块游戏中，`STATE_FRAMES`设置为`2`，但较高的数字也会起效：
- en: '[PRE52]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'If we have no `last_state`, then we construct a new Numpy array that is just
    the current `screen_binary` stacked as many times as we want `STATE_FRAMES`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有`last_state`，那么我们就构造一个新的Numpy数组，它只是当前的`screen_binary`堆叠了我们想要的`STATE_FRAMES`次：
- en: '[PRE53]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Otherwise, we append our new `screen_binary` into the first position in our
    `last_state` to create the new `current_state`. Then we just need to remember
    to re-assign our `last_state` to equal our current state at the end of the main
    loop:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们将新的`screen_binary`添加到我们的`last_state`的第一个位置以创建新的`current_state`。然后我们只需要记住在主循环结束时将我们的`last_state`重新分配为等于我们的当前状态：
- en: '[PRE54]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'One issue we may now run into is that our state space is now a 84*74*2 array,
    and we want to be storing in the order of 1,000,000 of these as our list of past
    observations from which to train. Unless your computer is quite a beast, you may
    start to run into memory issues. Fortunately, a lot of these arrays will be very
    sparse and only contain two states, so one simple solution to this is to use in
    memory compression. This will sacrifice a bit of CPU time to save on memory; so
    before using this, consider which one is more important to you. Implementing it
    in Python is only a few lines:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可能遇到的一个问题是，我们的状态空间现在是一个大小为84*74*2的数组，并且我们想要以100万个这样的数组的顺序作为过去观察的列表，用于训练。除非您的计算机非常强大，否则可能会遇到内存问题。幸运的是，这些数组中很多将是非常稀疏的，并且只包含两种状态，因此可以使用内存压缩来解决这个问题。这将牺牲一些CPU时间来节省内存；因此在使用之前，请考虑哪个对您更重要。在Python中实现它只需要几行代码：
- en: '[PRE55]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here, we compress the data before adding it to our list of observations:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们压缩数据然后将其添加到我们的观察列表中:'
- en: '[PRE56]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Then when sampling from the list, we decompress only our mini batch sample as
    we use it.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当从列表中取样时，我们只在使用时解压我们的小批量样本。
- en: 'Another issue we may run into is that while the cart pole is trained in as
    little as a couple of minutes, for Breakout, training will be measured in days.
    To guard against something going wrong, such as a power failure shutting off the
    computer, we will want to start saving our network weights as we go. In Tensorflow,
    this are only a couple of lines:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能遇到的另一个问题是，尽管推车杆可能只需要几分钟就能训练好，但打砖块的训练时间可能会以天计算。为了防止出现意外情况，比如断电关机，我们希望在训练过程中随时保存我们的网络权重。在Tensorflow中，这只需要几行代码：
- en: '[PRE57]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This can be put at the beginning of the file, just the `session.run( tf.initialize_all_variables())`
    line. Then we just need to run the following command:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以放在文件的开头，就在`session.run(tf.initialize_all_variables())`这一行上面。然后我们只需执行以下命令：
- en: '[PRE58]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This means every couple of thousand training iterations are to have regular
    backups of our network created. Now let''s see what training looks like:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每隔几千次训练迭代都要创建我们网络的定期备份。现在让我们看一下训练的效果如何：
- en: '![Creating a deep convolutional network](img/00300.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![创建深度卷积网络](img/00300.jpeg)'
- en: As we can see after 1.7 million iterations, we are playing at a level well above
    random. This same Q-learning algorithm has been tried on a wide range of Atari
    games, and with good hyper parameter, tuning was able to achieve human level or
    higher performance in, among others, Pong, Space Invaders, and Q*bert.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在170万次迭代后，我们玩的水平远远超出了随机水平。这种相同的Q学习算法已经尝试过多种Atari游戏，并且通过良好的超参数调整，在Pong、Space
    Invaders和Q*bert等游戏中能够达到人类水平或更高的表现。
- en: Convergence issues in Q-learning
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习中的收敛问题
- en: 'But it''s not all plane sailing. Let''s see how agent training is continued
    after the end of the preceding sequence:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，并不是一帆风顺的。让我们看看在前面序列结束后代理的训练如何继续：
- en: '![Convergence issues in Q-learning](img/00301.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习中的收敛问题](img/00301.jpeg)'
- en: As you can see, at a certain point the agent's ability took a massive and prolonged
    drop off before returning to a similar level. The likely reason for this (as much
    as we can ever know the exact reasons) is one of the problems with Q-learning.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在某个时刻，代理的能力出现了巨大且持续的下降，然后回到了类似的水平。这种情况的可能原因（尽管我们很难确切知道原因）之一是 Q 学习的问题之一。
- en: Q-learning is training against its own expectation of how good it thinks a state
    action pair will be. This is a moving target because every time you run a training
    step, the targets change. We hope that they are moving towards a more accurate
    estimation of the reward. But as they head there, small changes in parameters
    can result in quite extreme oscillations.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习是针对其自身对状态动作对的表现期望进行训练的。这是一个移动的目标，因为每次运行训练步骤时，目标都会发生变化。我们希望它们朝着对奖励更准确的估计值的方向移动。但随着它们朝着那里前进，参数的小变化可能会导致相当极端的振荡。
- en: Once we end up in a space where we are doing worse than our previous estimations
    of ability, every single state action evaluation must adjust to this new reality.
    If we're getting an average of 30 points a game and with our new policy, we are
    down to just 20, the whole network must adjust to this.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们陷入了比先前的能力评估更差的状态，每个状态动作评估都必须调整到这个新现实。如果我们每场比赛平均得分为 30 分，而通过我们的新策略，我们只能得到
    20 分，整个网络都必须调整到这个情况。
- en: Target network freezing (Minh et al 2015 Human-level control through deep reinforcement
    learning—Nature) can help reduce this. A second neural network, referred to as
    the target network, is created as a copy of the main training network. During
    training, we use the target network to generate the target values used to train
    the main neural network. In this way, the main network is learning against a more
    fixed point. The target networks weight frozen, but once a set number of iterations
    have passed, or a convergence criterion is reached, the target network is updated
    with the values from the main network. This process has been shown to significantly
    speed up training.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络冻结（Minh等人 2015 年，《通过深度强化学习实现人类水平控制》- 自然）可以帮助减少这种情况。第二个神经网络，称为目标网络，被创建为主训练网络的副本。在训练期间，我们使用目标网络生成用于训练主神经网络的目标值。通过这种方式，主网络正在学习针对更固定的点。目标网络的权重被冻结，但一旦过了一定数量的迭代次数或达到收敛标准，目标网络就会更新为来自主网络的值。已经证明，这个过程可以显著加快训练速度。
- en: Another issue that a lot of reinforcement learning can struggle with pertains
    to games that have quite extreme rewards. Pac-Man, for example, has a very high
    reward for taking the power pill and then eating the ghosts. These extreme rewards
    received can cause problems with the gradients and lead to sub-optimal learning.
    The very easy but unsatisfactory way to fix this is called reward clipping, which
    just involves clipping the rewards received from the environment in some range
    (-1 and +1 are commonly used). For very little effort, this works, but it has
    the problem that the agent has lost the information about these larger rewards.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 很多强化学习可能遇到的另一个问题与具有相当极端奖励的游戏相关。例如，吃下力量丸然后吃掉鬼魂给予了非常高的奖励。这些接收到的极端奖励可能会导致梯度问题，并导致次优学习。修复这个问题的非常简单但不够令人满意的方法叫做奖励剪切，它只是将从环境中接收到的奖励剪切在某个范围内（-1
    和 +1 常用）。这种方法花费很少的精力，但它的问题在于代理已经丢失了关于这些更大奖励的信息。
- en: 'Another approach is what''s called a normalized deep q network (**Hasselt et
    al—learning values across many orders of magnitude, 2016)**. This involves setting
    up the neural network to output the expected reward of the state and action in
    the -1 to 1 range. Put the output into this range; it is put through the following
    equation:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是所谓的归一化深度 Q 网络（**Hasselt等人——跨多个数量级学习价值，2016)**。这涉及将神经网络设置为在 -1 到 1 范围内输出状态和动作的预期奖励。将输出放入此范围后，它通过以下方程进行处理：
- en: '![Convergence issues in Q-learning](img/00302.jpeg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习中的收敛问题](img/00302.jpeg)'
- en: 'Here, *U(s, a)* is the output of the neural network. The parameters σ and µ
    can be worked out by making sure that the scaled output is constant between the
    target and main network, as described in target network freezing:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*U(s, a)* 是神经网络的输出。参数 σ 和 µ 可以通过确保目标网络和主网络之间的缩放输出保持恒定来计算出，如目标网络冻结中所述：
- en: '![Convergence issues in Q-learning](img/00303.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![Q学习中的收敛问题](img/00303.jpeg)'
- en: Using this approach, the neural network gradients will be directed more towards
    learning the relative values of states and actions as opposed to expending energy
    simply learning the scale of the Q-value.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，神经网络梯度将更多地指向学习状态和动作的相对值，而不是简单地消耗精力学习Q值的规模。
- en: Policy gradients versus Q-learning
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度与Q学习
- en: Though we gave an example using policy gradients for learning a board game and
    Q-learning for a computer game, neither technique is limited to that type. Originally,
    Q-learning was considered the better technique overall, but over time and with
    better hyperparameters, tuning policy gradients have often been shown to perform
    better. The world's best performance in backgammon was achieved in 1991 using
    a neural network and Q-learning, and latest research suggests that policy gradients
    are best for most Atari games. So when should you use policy gradients versus
    Q-learning?
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们举了一个例子，使用策略梯度来学习棋盘游戏，使用Q学习来学习计算机游戏，但这两种技术并不局限于此类型。最初，Q学习被认为是更好的技术，但随着时间的推移和更好的超参数调整，策略梯度常常表现更好。1991年利用神经网络和Q学习在博弈中取得了世界最佳表现，最新研究表明策略梯度对大多数雅达利游戏效果最佳。那么何时应该使用策略梯度而不是Q学习呢？
- en: One constraint is that Q-learning can only work for discrete action tasks, whereas
    policy gradients can learn continuous action tasks. Also Q-learning is a deterministic
    algorithm, and for some tasks, the optimum behavior involves having some degree
    of randomness. For example, rock, paper, scissors, where any behavior that deviates
    from purely random can be exploited by an opponent.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一个限制是，Q学习只适用于离散动作任务，而策略梯度可以学习连续动作任务。此外，Q学习是确定性算法，对于某些任务，最佳行为涉及一定程度的随机性。例如，石头、剪刀、布，任何偏离纯随机性的行为都可能被对手利用。
- en: There is also the online versus offline aspect. For many tasks, especially robot-control
    tasks, online learning may be very expensive. The ability to learn from memory
    is needed, so Q-learning is the best option. Unfortunately, the success of both
    Q-learning and policy gradients can vary a lot depending on the task and choice
    of hyperparameters; so when determining the best for a new task, experimentation
    appears to be the best approach.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 也存在在线学习与离线学习的方面。对于许多任务，特别是机器人控制任务，在线学习可能非常昂贵。需要从记忆中学习的能力，因此Q学习是最佳选择。不幸的是，无论是Q学习还是策略梯度的成功都会受到任务和超参数选择的影响很大；因此，在确定新任务的最佳学习方法时，实验似乎是最好的方法。
- en: Policy gradients also have a greater tendency to get stuck in local minima.
    Q-learning has a better chance of finding the global optima, but the cost of this
    is that it is not proven to converge, and performance may oscillate wildly or
    fail completely on its way there.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度也更容易陷入局部最小值。Q学习更有可能找到全局最优解，但这样做的成本是未经证实的收敛，性能可能在达到全局最优解的过程中发生剧烈波动或完全失败。
- en: But there is also another approach that takes some of the best aspects of both.
    These are known as actor-critic methods.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有另一种方法，它兼具两者的优点，这就是演员-评论员方法。
- en: Actor-critic methods
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论员方法
- en: 'Approaches to reinforcement learning can be divided into three broad categories:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习方法可以分为三大类：
- en: '**Value-based learning**: This tries to learn the expected reward/value for
    being in a state. The desirability of getting into different states can then be
    evaluated based on their relative value. Q-learning in an example of value-based
    learning.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于价值的学习**：这个方法试图学习处于某个状态的预期奖励/价值。然后可以根据其相对值来评估进入不同状态的可取性。Q学习就是基于价值的学习的例子。'
- en: '**Policy-based learning**: In this, no attempt is made to evaluate the state,
    but different control policies are tried out and evaluated based on the actual
    reward from the environment. Policy gradients are an example of that.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的学习**：在这种方法中，不尝试评估状态，而是尝试不同的控制策略，并根据环境的实际奖励进行评估。策略梯度就是例子。'
- en: '**Model-based learning**: In this approach, which will be discussed in more
    detail later in the chapter, the agent attempts to model the behavior of the environment
    and choose an action based on its ability to simulate the result of actions it
    might take by evaluating its model.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的学习**：在这种方法中，代理试图对环境的行为进行建模，并选择基于模型模拟其可能采取的行动结果来评估其模型的行为。'
- en: Actor-critic methods all revolve around the idea of using two neural networks
    for training. The first, the critic, uses value-based learning to learn a value
    function for a given state, the expected reward achieved by the agent. Then the
    actor network uses policy-based learning to maximize the value function from the
    critic. The actor is learning using policy gradients, but now its target has changed.
    Rather than being the actual reward received by playing, it is using the critic's
    estimate of that reward.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 演员评论方法都围绕着使用两个神经网络进行训练的想法。第一个，评论者，使用基于价值的学习来学习给定状态的值函数，即代理人实现的预期奖励。然后，演员网络使用基于策略的学习来最大化评论者的值函数。演员正在使用策略梯度进行学习，但现在其目标已经改变。不再是通过游戏获得的实际奖励，而是使用评论者对该奖励的估计。
- en: One of the big problems with Q-learning is that in complex cases, it can be
    very hard for the algorithm to ever converge. As re-evaluations of the Q-function
    change what actions are selected, the actual value rewards received can vary massively.
    For example, imagine a simple maze-walking robot. At the first T-junction it encounters
    in the maze, it initially moves left. Successive iterations of Q-learning eventually
    lead to it determining that right is the preferable way to move. But now because
    its path is completely different, every other state evaluation must now be recalculated;
    the previously learned knowledge is now of little value. Q-learning suffers from
    high variance because small shifts in policy can have huge impacts on reward.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的一个重大问题是，在复杂情况下，算法很难收敛。由于Q函数的重新评估改变了选择的动作，实际的价值奖励可能会有很大的变化。例如，想象一个简单的走迷宫机器人。在迷宫中遇到的第一个T形交叉口，它最初向左移动。
    Q学习的连续迭代最终导致它确定右移是更可取的方式。但现在，因为其路径完全不同，现在必须重新计算每个其他状态评估; 先前学到的知识现在价值很低。 Q学习由于策略的微小变化可能对奖励产生巨大影响而受到高方差的影响。
- en: 'In actor-critic, what the critic is doing is very similar to Q-learning, but
    there is a key difference: instead of learning the hypothetical best action for
    a given state, it is learning the expected reward based on the most likely sub-optimal
    policy that the actor is currently following.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在演员评论中，评论者所做的事情与Q学习非常相似，但存在一个关键区别：不是学习给定状态的假设最佳动作，而是学习基于演员当前遵循的最可能的次优策略的预期奖励。
- en: Conversely, policy gradients have the inverse high variance problem. As policy
    gradients are exploring a maze stochastically, certain moves may be selected,
    which are, in fact, quite good but end up being evaluated as bad because of other
    bad moves being selected in the same rollout. It suffers because though the policy
    is more stable, it has high variance related to evaluating the policy.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，策略梯度存在相反的高方差问题。由于策略梯度是随机地探索迷宫，某些移动可能被选择，实际上相当不错，但由于在同一次试验中选择了其他不良移动而被评估为不良。这是因为尽管策略更稳定，但与评估策略相关的方差很高。
- en: This is where actor critic aims to mutually solve these two problems. The value-based
    learning now has lower variance because the policy is now more stable and predictable,
    while the policy gradient learning is also more stable because it now has a much
    lower variance value function from which to get its gradients.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是演员评论的目标，旨在共同解决这两个问题。基于价值的学习现在方差较低，因为策略现在更加稳定和可预测，而基于策略梯度的学习也更加稳定，因为现在它具有一个从中获取梯度的方差值函数。
- en: Baseline for variance reduction
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方差减少的基线
- en: 'There are a few different variants of actor-critic methods: the first one we
    will look at is the baseline actor critic. Here, the critic tries to learn the
    average performance of the agent from a given position, so its loss function would
    be this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 演员评论方法有几种不同的变体：我们将首先看一下基线演员评论。在这里，评论者试图学习代理人从给定位置的平均表现，因此其损失函数将是这样的：
- en: '![Baseline for variance reduction](img/00304.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![方差减少的基线](img/00304.jpeg)'
- en: 'Here, ![Baseline for variance reduction](img/00305.jpeg) is the output of the
    critic network for the state at time step *t*, and *r*[*t*] is the cumulative
    discounted reward from time step *t*. The actor can then be trained using the
    target:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![方差减少的基线](img/00305.jpeg)是评论者网络在时间步*t*的状态的输出，*r*[*t*]是从时间步*t*开始的累积折现奖励。然后可以使用目标训练演员：
- en: '![Baseline for variance reduction](img/00306.jpeg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![方差减少的基线](img/00306.jpeg)'
- en: Because the baseline is the average performance from this state, this has the
    effect of massively reducing the variance of training. If we run the cart pole
    task once using policy gradients and once using baselines, where we do not use
    batch normalization, we can see that baselines perform much better. But if we
    add in batch normalization, the result is not much different. For more complex
    tasks than cart pole, where the reward may vary a lot more with the state, the
    baselines approach may improve things a lot more. An example of this can be found
    at `actor_critic_baseline_cart_pole.py`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 因为基线是从这个状态的平均表现中得出的，这样做的效果是大幅降低训练的方差。如果我们使用策略梯度一次运行小车杆任务，再使用基线一次，其中我们不使用批量规范化，我们可以看到基线表现更好。但如果我们加入批量规范化，结果并没有太大不同。对于比小车杆更复杂的任务，奖励可能随状态变化而变化很多，基线方法可能会更大程度地改善事物。这方面的一个例子可以在`actor_critic_baseline_cart_pole.py`中找到。
- en: Generalized advantage estimator
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义优势估计器
- en: The baselines approach does a great job at reducing variance, but it is not
    a true actor-critic approach because the actor is not learning the gradient of
    the critic, simply using it to normalize the reward. Generalized advantage estimator
    goes a step further and incorporates the critics gradients into the actor's objective.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 基线方法在减少方差方面做得很好，但它不是真正的演员评论家方法，因为演员不是在学习评论者的梯度，而只是使用它来规范化奖励。广义优势估计器进一步前进，并将评论者的梯度纳入演员的目标中。
- en: 'In order to do this, we need to learn not just the value of the states the
    agent is in, but also of the state action pairs it takes. If *V(s*[*t*]*)* is
    the value of the state, and *Q(s*[*t*]*, a*[*t*]*)* is the value of the state
    action pair, we can define an advantage function like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要学习的不仅仅是代理处于的状态的价值，还有它采取的状态动作对的价值。如果*V(s*[*t*]*)*是状态的价值，*Q(s*[*t*]*,
    a*[*t*]*)*是状态动作对的价值，我们可以这样定义一个优势函数：
- en: '![Generalized advantage estimator](img/00307.jpeg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![广义优势估计器](img/00307.jpeg)'
- en: 'This will give us the difference between how well the action *a*[*t*] did in
    state *s*[*t*] and the average action the agent takes in this position. Moving
    towards the gradient of this function should lead to us maximizing our reward.
    Also, we don''t need another network to estimate *Q(s*[*t*]*, a*[*t*]*)* because
    we can use the fact that we have the value function for the state we reached at
    *s*[*t+1*], and the definition of a Q-function is as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们带来动作*a*[*t*]在状态*s*[*t*]中的表现与代理在这个位置上平均动作之间的差异。向着这个函数的梯度移动应该会使我们最大化我们的奖励。而且，我们不需要另一个网络来估计*Q(s*[*t*]*,
    a*[*t*]*)*，因为我们可以利用我们在*s*[*t+1*]达到的状态的价值函数，而Q函数的定义如下：
- en: '![Generalized advantage estimator](img/00308.jpeg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![广义优势估计器](img/00308.jpeg)'
- en: 'Here, *r* *t* is now the reward for that time step, not the cumulative reward
    as in the baseline equation, and ![Generalized advantage estimator](img/00309.jpeg)
    is the future reward discount factor. We can now substitute that in to give us
    our advantage function purely in terms in *V*:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*r* *t* 现在是该时间步的奖励，而不是基线方程中的累积奖励，![广义优势估计器](img/00309.jpeg) 是未来奖励的折扣因子。我们现在可以将其代入，纯粹地给出我们的优势函数中的*V*项：
- en: '![Generalized advantage estimator](img/00310.jpeg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![广义优势估计器](img/00310.jpeg)'
- en: Again, this gives us a measure of whether the critic thinks a given action improved
    or hurt the value of the position. We replace the cumulative reward in our actor's
    loss function with the result of the advantage function. The full code for this
    is in `actor_critic_advantage_cart_pole.py`. This approach used on the cart-pole
    challenge can complete it, but it may take longer than simply using policy gradients
    with batch normalization. But for more complex tasks such as learning computer
    games, advantage actor-critic can perform the best.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，这给我们提供了一个度量标准，用来判断评论者是否认为给定的动作改善了还是损害了位置的价值。我们将我们的演员损失函数中的累积奖励替换为优势函数的结果。这方面的完整代码在`actor_critic_advantage_cart_pole.py`中。这种方法用于小车杆挑战可以完成，但可能比仅使用批量规范化的策略梯度花费更长的时间。但对于像学习电脑游戏这样更复杂的任务，优势演员-评论家可能表现最好。
- en: Asynchronous methods
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步方法
- en: We have seen a lot of interesting methods in this chapter, but they all suffer
    from the constraint of being very slow to train. This isn't such a problem when
    we are running on basic control problems, such as the cart-pole task. But for
    learning Atari games, or the even more complex human tasks that we might want
    to learn in the future, the days to weeks of training time are far too long.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们看到了许多有趣的方法，但它们都受到训练速度非常慢的限制。当我们在基本控制问题上运行时，例如推车和杆子任务，这并不是什么问题。但是对于学习Atari游戏或者未来可能想要学习的更复杂的人类任务来说，数天到数周的训练时间就太长了。
- en: A big part of the time constraint, for both policy gradients and actor-critic,
    is that when learning online, we can only ever evaluate one policy at a time.
    We can get significant speed improvements by using more powerful GPUs and bigger
    and bigger processors; the speed of evaluating the policy online will always act
    as a hard limit on performance.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于策略梯度和演员-评论家来说，时间限制的一个重要部分是，在在线学习时，我们只能同时评估一个策略。我们可以通过使用更强大的GPU和更大的处理器获得显著的速度提升；在线评估策略的速度将始终作为性能的硬性限制。
- en: This is the problem that asynchronous methods aim to solve. The idea is to train
    multiple copies of the same neural networks across multiple threads. Each neural
    network trains online against a separate instance of the environment running on
    its thread. Instead of updating each neural network per training step, the updates
    are stored across multiple training steps. Every *x* training steps the accumulated
    batch updates from each thread are summed together and applied to all the networks.
    This means network weights are being updated with the average change in parameter
    values across all the network updates.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是异步方法旨在解决的问题。其想法是在多个线程上训练相同的神经网络的多个副本。每个神经网络在线针对其线程上运行的环境的一个单独实例进行训练。不同于对每个训练步骤更新每个神经网络，更新跨多个训练步骤存储。每*x*个训练步骤，来自每个线程的累积批量更新被汇总在一起，并应用于所有网络。这意味着网络权重将根据所有网络更新中参数值的平均变化进行更新。
- en: This approach has been shown to work for policy gradients, actor-critic, and
    Q-learning. It results in a big improvement to training time and even improved
    performance. The best version of asynchronous methods was found to be asynchronous
    advantage actor-critic, which, at the time of writing , is said to be the most
    successful generalized game learning algorithm.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法已经被证明适用于策略梯度、演员-评论家和Q学习。它极大地改善了训练时间，甚至提高了性能。在异步方法的最佳版本中，被认为是最成功的广义游戏学习算法的异步优势演员-评论家方法，在撰写本文时，被认为是最成功的广义游戏学习算法。
- en: Model-based approaches
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于模型的方法
- en: 'The approaches we''ve so far shown can do a good job of learning all kinds
    of tasks, but an agent trained in these ways can still suffer from significant
    limitations:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经展示的方法可以很好地学习各种任务，但是通过这些方法训练出来的智能体仍然可能遭受重大限制：
- en: It trains very slowly; a human can learn a game like Pong from a couple of plays,
    while for Q-learning, it may take millions of playthroughs to get to a similar
    level.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它训练速度非常慢；一个人可以通过几次游玩学会像乒乓球一样的游戏，而对于Q学习，可能需要数百万次游玩才能达到类似的水平。
- en: For games that require long-term planning, all the techniques perform very badly.
    Imagine a platform game where a player must retrieve a key from one side of a
    room to open a door on the other side. There will rarely be a passage of play
    where this occurs, and even then, the chance of learning that it was the key that
    lead to the extra reward from the door is miniscule.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对需要长期规划的游戏，所有技术表现都非常糟糕。想象一个平台游戏，玩家必须从房间的一侧取回一把钥匙，以打开另一侧的门。游戏中很少会发生这种情况，即使发生了，学习到这个钥匙是导致门获得额外奖励的机会也微乎其微。
- en: It cannot formulate a strategy or in any way adapt to a novel opponent. It may
    do well against an opponent it trains against, but when presented with an opponent
    showing some novelty in play, it will take a long time to learn to adapt to this.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法制定策略或以任何方式适应新颖的对手。它可能可以在与训练对手对战时表现良好，但在面对游戏玩法上有新颖性的对手时，学会适应将需要很长时间。
- en: If given a new goal within an environment, it would require retraining. If we
    are training to play Pong as the left paddle and then we're recast as the right
    paddle, we would struggle to reuse the previously learned information. A human
    can do this without even thinking about it.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在环境中给出一个新的目标，就需要重新训练。如果我们正在训练打乒乓球作为左挡板，然后我们改为右挡板，我们将很难重新利用先前学到的信息。一个人可以毫不费力地做到这一点。
- en: All these points could be said to relate to a central problem. Q-learning and
    policy gradients optimize parameters for a reward in a game very successfully,
    but they do not learn to understand a game. Human learning feels different from
    Q-learning in many ways, but one significant one is that when humans learn an
    environment, they are, on some level, learning a model of that environment. They
    can then use that model to make predictions or imagine hypothetical things that
    would happen if they made different actions within the environment.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些观点都可以说与一个中心问题相关。Q学习和策略梯度在游戏中为奖励优化参数非常成功，但它们并没有学习如何理解游戏。人类学习在许多方面与Q学习有所不同，但一个显著的不同是，当人类学习一个环境时，他们在某种程度上正在学习这个环境的模型。然后他们可以使用该模型进行预测或者想象在环境中采取不同行动会发生什么事情。
- en: 'Think of a player learning chess: he can think through what would happen if
    he were to make a certain move. He can imagine what the board would look like
    after this move, what options he would then have in that new position. He might
    even be able to factor his opponent into his model, what kind of personality this
    player has, what kinds of moves he favors, what his mood is.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个玩家学习下棋的情景：他可以思考如果他进行某个特定的移动会发生什么。他可以想象在这一步之后棋盘会呈现什么样子，在那个新的位置他将会有哪些选择。他甚至可以将对手考虑进他的模型中，这个玩家是什么性格，倾向于采取什么样的走法，他的心情如何。
- en: This is what model-based approaches to reinforcement learning aim to do. A model-based
    approach to Pong would aim to build a simulation of the result of different actions
    it might take and try and get that simulation as close to reality as possible.
    Once a good model of an environment has been built up, learning the best action
    becomes a lot simpler as the agent can just treat the current state as the root
    of a Markov chain and use some of the techniques from [Chapter 7](part0046_split_000.html#1BRPS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 7. Deep Learning for Board Games"), *Deep Learning for Board Games*,
    such as MCTS-UCT, to sample from its model to see which actions have the best
    results. It could even go further and use Q-learning or policy gradients trained
    on its own model, rather than the environment.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是基于模型的强化学习方法的目标。基于模型的Pong方法旨在建立一个模拟，模拟出它可能采取的不同行动的结果，并努力使该模拟尽可能接近现实。一旦建立起一个良好的环境模型，学习最佳行动就变得简单得多，因为代理可以将当前状态视为马尔可夫链的根，并利用一些来自[第7章](part0046_split_000.html#1BRPS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第7章 棋盘游戏的深度学习"), *棋盘游戏的深度学习*的技术，比如MCTS-UCT，从其模型中抽样以查看哪些行动有最佳结果。它甚至可以更进一步，使用在自身模型上训练的Q学习或策略梯度，而不是在环境上训练。
- en: Model-based approaches also have the advantage that they might allow the AI
    to adapt much easier to change. If we have learned a model of an environment but
    want to change our goal within it, we can reuse the same model and simply adjust
    our policy within the model. If we are talking about robots, or other AI that
    operate in the physical world, learning using policy gradient by playing millions
    of episodes is completely impractical, especially when you consider that every
    experiment in the real world carries a cost in terms of time, energy, and the
    risk of damage through misadventure. Model-based approaches mitigate a lot of
    these issues.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的方法还有一个优势，那就是它们可能使人工智能更容易适应变化。如果我们已经学会了一个环境模型，但想要在其中改变我们的目标，我们可以重复使用同一个模型，只需简单地调整模型内的策略。如果我们讨论的是机器人或者在物理世界中运作的其他人工智能，通过玩数百万次的情节来学习策略梯度是完全不切实际的，特别是考虑到现实世界中的每次实验都会耗费时间、能量，并且存在着由于意外事件而带来的风险。基于模型的方法可以缓解许多这些问题。
- en: Building a model raises all kinds of questions. If you were building a model-based
    agent to learn Pong, you know that it takes place in a 2D environment with two
    paddles and a ball and very basic physics. You would want all these elements in
    your model for it to be successful. But if you hand craft these, then there is
    no longer much learning going on, and your agent is very far from a generalized
    learning algorithm. What is the right *prior* for a model? How can we build a
    model that is flexible enough to learn the myriad things that can be encountered
    in the world while still being able to successfully learn specifics?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型引发种种问题。如果你正在构建一个基于模型的代理来学习 Pong，你知道它发生在一个二维环境中，有两个球拍和一个球，并且基本的物理规则。你需要这些元素都在你的模型中才能成功。但如果你手工制作这些，那么学习就不会那么多，并且你的代理远离了泛化学习算法。对于模型来说，什么是正确的*先验*？我们如何构建一个足够灵活，可以学习世界中遇到的复杂事物，同时仍能成功学习特定内容的模型？
- en: 'In more formal terms, learning the model can be seen as learning a function
    that gives the next state given the current state and action pair:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，学习模型可以看作是学习一个函数，它给出下一个状态在给定当前状态和动作对的情况下：
- en: '![Model-based approaches](img/00311.jpeg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![基于模型的方法](img/00311.jpeg)'
- en: 'If the environment is stochastic, the function might even return a probability
    distribution of possible next states. A deep neural network would naturally be
    a good choice for the function, and then learning would take the following steps:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '如果环境是随机的，此函数甚至可能返回可能的下一状态的概率分布。一个深度神经网络自然是该函数的一个很好的选择，然后学习将采取以下步骤:'
- en: Build a network where the input is the current state, and an action and output
    are the next state and the reward.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个输入为当前状态，输出为下一个状态和奖励的动作网络。
- en: Gather a collection of state action transitions from the environment following
    an explorative policy. Simply making moves at random might be a good initial choice.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中遵循一种探索性策略，收集一系列状态动作转换。简单地随机行动可能是一个很好的初始选择。
- en: Use the collection of state action transitions to train the network in a supervised
    manner using the next states and state rewards as the targets.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用状态动作转换的集合以监督的方式训练网络，以下一状态和状态奖励作为目标。
- en: Use the trained network transitions to determine the best move using MCTS, policy
    gradients, or Q-learning.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的网络转换来确定使用 MCTS、策略梯度或 Q-learning 的最佳移动。
- en: If we use the cart pole task as an example, using the MSE as our loss function,
    we can find that it is easy to train a deep neural network to accurately predict
    all the state transitions for this environment, including when a new state will
    be terminal. A code sample for this is in the Git repo.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以倒立摆任务为例，并以 MSE 作为损失函数，我们可以发现训练深度神经网络准确预测该环境的所有状态转换很容易，包括新状态何时终止。这个示例代码在
    Git 仓库中。
- en: 'It is even possible to learn models for more complex Atari games, using convolutional
    and recurrent layers. Here is an example of the architecture of the network:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '甚至可以使用卷积和循环层来学习更复杂的 Atari 游戏模型。这是网络架构的一个例子:'
- en: '![Model-based approaches](img/00312.jpeg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![基于模型的方法](img/00312.jpeg)'
- en: 'Source: http://cs231n.stanford.edu/reports2016/116_Report.pdf'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：http://cs231n.stanford.edu/reports2016/116_Report.pdf
- en: A network like this was trained using two convolutional/deconvolutional layers
    and 128 node RNN to learn to predict next frames in Pong. It could do a good job
    of successfully predicting blurry versions of next frames, but the model was found
    to not be robust enough to run an MCTS to predict events beyond a frame or two
    in the future.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一个这样的网络使用了两个卷积/反卷积层和 128 个节点的 RNN 来学习预测 Pong 游戏中的下一帧。它能够成功地预测模糊版本的下一帧，但发现该模型不够稳健，无法运行
    MCTS 来预测未来一两帧的事件。
- en: A modified version of this approach worked a lot better. In this, instead of
    trying to do a deconvolution to predict the next image, the network just tries
    to predict what the input to the RNN will be in the next frame, thus removing
    the need for the deconvolution. This network could learn to play Pong to a high
    enough standard to beat the in-game AI, winning by an average of 2.9 points per
    game after training. This is a long way short of the 20.0, which can be achieved
    by a fully trained Deep Q Network, but it is still a promising result for a very
    new approach. Similar results were also achieved on Breakout.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的修改版本效果好得多。在这种方法中，网络不再尝试进行反卷积来预测下一帧图像，而是仅仅尝试预测 RNN 输入在下一帧中将是什么，从而消除了反卷积的需要。该网络可以学会以足够高的水平玩乒乓球，以击败游戏内的人工智能，训练后平均每场比赛赢得
    2.9 分。这离完全训练的深度 Q 网络可以达到的 20.0 分还有很长的路要走，但对于一种非常新的方法来说，这仍然是一个有希望的结果。类似的结果也在 Breakout
    游戏中实现了。
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we looked at building computer game playing agents using reinforcement
    learning. We went through the three main approaches: policy gradients, Q-learning,
    and model-based learning, and we saw how deep learning can be used with these
    approaches to achieve human or greater level performance. We would hope that the
    reader would come out of this chapter with enough knowledge to be able to use
    these techniques in other games or problems that they may want to solve. Reinforcement
    learning is an incredibly exciting area of research at the moment. Companies such
    as Google, Deepmind, OpenAI, and Microsoft are all investing heavily to unlock
    this future.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了使用强化学习构建计算机游戏代理的方法。我们介绍了三种主要方法：策略梯度、Q学习和基于模型的学习，并展示了如何将深度学习与这些方法结合使用以实现人类或更高水平的表现。我们希望读者能够从本章中获得足够的知识，以便能够将这些技术应用到他们可能想要解决的其他游戏或问题中。强化学习是当前非常令人兴奋的研究领域。谷歌、Deepmind、OpenAI
    和微软等公司都在大力投资以解锁这一未来。
- en: In the next chapter, we will take a look at anomaly detection and how the deep
    learning method can be applied to detect instances of fraud in financial transaction
    data.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨异常检测以及如何应用深度学习方法来检测金融交易数据中的欺诈实例。
