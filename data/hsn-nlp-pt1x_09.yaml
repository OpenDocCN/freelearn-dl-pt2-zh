- en: '*Chapter 6*: Convolutional Neural Networks for Text Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 6 章*：用于文本分类的卷积神经网络'
- en: In the previous chapter, we showed how RNNs can be used to provide sentiment
    classifications for text. However, RNNs are not the only neural network architecture
    that can be used for NLP classification tasks. **Convolutional neural networks**
    (**CNNs**) are another such architecture.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们展示了如何使用 RNNs 为文本提供情感分类。然而，RNNs 并不是唯一可以用于 NLP 分类任务的神经网络架构。**卷积神经网络**（**CNNs**）是另一种这样的架构。
- en: RNNs rely on sequential modeling, maintain a hidden state, and then step sequentially
    through the text word by word, updating the state at each iteration. CNNs do not
    rely on the sequential element of language, but instead try and learn from the
    text by perceiving each word in the sentence individually and learning its relationship
    to the words surrounding it within the sentence.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 依赖于顺序建模，维护隐藏状态，然后逐词遍历文本，每次迭代更新状态。CNNs 不依赖于语言的顺序元素，而是试图通过感知句子中的每个单词并学习其与句子中相邻单词的关系来从文本中学习。
- en: While CNNs are more commonly used for classifying images for the reasons mentioned
    here, they have been shown to be effective at classifying text as well. While
    we do perceive text as a sequence, we also know that the meaning of individual
    words in the sentence depends on their context and the words they appear next
    to. Although CNNs and RNNs learn from text in different ways, they have both shown
    to be effective in text classification, and which one to use in any given situation
    depends on the nature of the task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CNNs 更常用于基于以下原因分类图像，但它们也被证明在文本分类上是有效的。尽管我们把文本视为序列，但我们也知道句子中每个单词的含义取决于它们的上下文及相邻单词。虽然
    CNNs 和 RNNs 从文本中学习的方式不同，但它们都被证明在文本分类中是有效的，而在特定任务中选择哪种取决于任务的性质。
- en: 'In this chapter, we will explore the basic theory behind CNNs, as well as construct
    a CNN from scratch that will be used to classify text. We will cover the following
    topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 CNNs 的基本理论，并从头构建一个 CNN，用于文本分类。我们将涵盖以下主题：
- en: Exploring CNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 CNNs
- en: Building a CNN for text classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用于文本分类的 CNN
- en: Let's get started!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码可以在 [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x)
    找到。
- en: Exploring CNNs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 CNNs
- en: The basis for CNNs comes from the field of computer vision but can conceptually
    be extended to work on NLP as well. The way the human brain processes and understands
    images is not on a pixel-by-pixel basis, but as a holistic map of an image and
    how each part of the image relates to the other parts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的基础来自计算机视觉领域，但在概念上也可以扩展到自然语言处理。人类大脑处理和理解图像的方式并不是基于像素级别，而是将图像视为整体的映射，并理解图像中各部分的关联。
- en: A good analogy of CNNs would be how the human mind processes a picture versus
    how it processes a sentence. Consider the sentence, *This is a sentence about
    a cat*. When you read that sentence you read the first word, followed by the second
    word and so forth. Now, consider a picture of a cat. It would be foolish to assimilate
    the information within the picture by looking at the first pixel, followed by
    the second pixel. Instead, when we look at something, we perceive the whole image
    at once, rather than as a sequence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 的一个好比喻是人类大脑处理图片与处理句子的方式。考虑句子 *This is a sentence about a cat*。当你阅读这句话时，你读取第一个词，然后是第二个词，依此类推。现在，考虑一张猫的图片。通过查看第一个像素，然后是第二个像素来同化图片中的信息是愚蠢的。相反，当我们看东西时，我们一次性地感知整个图像，而不是作为一个序列。
- en: 'For example, if we take a black and white representation of an image (in this
    case, the digit 1), we can see that we can transform this into a vector representation,
    where the color of each pixel is denoted by a 0 or a 1:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将图像的黑白表示（在这种情况下是数字 1），我们可以将其转换为向量表示，其中每个像素的颜色由 0 或 1 表示：
- en: '![Figure 6.1 – Vector representation of an image](img/B12365_06_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 图像的向量表示](img/B12365_06_01.jpg)'
- en: Figure 6.1 – Vector representation of an image
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 图像的向量表示
- en: However, if we think about this in machine learning terms and treat this vector
    as features of our model, does the fact that any single pixel is black or white
    make it more or less likely that the picture is of a given digit? Does a white
    pixel in the top-right corner make the picture more likely to be a four or a seven?
    Imagine if we were trying to detect something more complex, such as whether a
    picture is of a dog or a cat. Does a brown pixel in the middle of the screen make
    the photo more likely to be a cat or a dog? Intuitively, we see that individual
    pixel values do not mean a lot when it comes to image classification. However,
    what we are interested in is the pixel's relationships to one another.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们从机器学习的角度思考并将该向量视为模型的特征，单个像素是黑色还是白色会使照片更有可能是特定数字吗？右上角的白色像素会使照片更有可能是四还是七吗？想象一下，如果我们尝试检测更复杂的事物，比如一张照片是狗还是猫。屏幕中央的褐色像素会使照片更有可能是猫还是狗吗？直觉上，我们看到单个像素值在图像分类方面并不意味着太多。然而，我们感兴趣的是像素之间的关系。
- en: In our case of digit representation, we know that a long vertical line is very
    likely to be a one and that any photo with a closed loop in it is more likely
    to be a zero, six, eight, or nine. By identifying and learning from patterns within
    our images, rather than just looking at individual pixels, we can better understand
    and identify these images. This is exactly what CNNs aim to achieve.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在数字表示的情况下，我们知道一个长竖线很有可能是一个一，而任何带有闭环的照片更有可能是零、六、八或九。通过识别和学习图像中的模式，而不仅仅是查看单个像素，我们可以更好地理解和识别这些图像。这正是
    CNN 的目标所在。
- en: Convolutions
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积
- en: 'The basic concept behind CNNs is that of convolutions. A **convolution** is
    essentially a sliding window function that''s applied to a matrix in order to
    capture information from the surrounding pixels. In the following diagram, we
    can see an example of convolution in action:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的基本概念是卷积。**卷积**本质上是一个应用于矩阵的滑动窗口函数，以捕获周围像素的信息。在以下图表中，我们可以看到卷积的示例运作：
- en: '![Figure 6.2 – Convolution in action'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 卷积的运作'
- en: '](img/B12365_06_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_02.jpg)'
- en: Figure 6.2 – Convolution in action
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 卷积的运作
- en: On the left, we have the image that we are processing, while at the top, we
    have the convolution kernel we wish to apply. For every 3x3 block within our image,
    we multiply this by our kernel to give us our convolution matrix at the bottom.
    We then sum (or average) the convolved matrix to get our single output value for
    this 3x3 block within our initial image. Note that within our 5x5 initial images,
    there are nine possible 3x3 blocks we can overlay onto this. As we apply this
    convolution process for every 3x3 block within our initial image, we are left
    with a final processed convolution that is 3x3\.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，我们有正在处理的图像，而在顶部，我们有希望应用的卷积核。对于我们图像中的每个 3x3 块，我们将其乘以我们的核，得到我们在底部的卷积矩阵。然后我们对卷积矩阵求和（或平均），以获得我们在初始图像中这个
    3x3 块的单个输出值。请注意，在我们的 5x5 初始图像中，我们可以叠加到九种可能的 3x3 块。当我们为初始图像中的每个 3x3 块应用此卷积过程时，我们得到最终处理的卷积结果为
    3x3。
- en: In a large image (or a complex sentence, in the case of NLP), we will also need
    to implement pooling layers. In our preceding example, applying a 3x3 convolution
    to a 5x5 image results in a 3x3 output. However, if we applied a 3x3 convolution
    to a 100x100 pixel image, this would only reduce the output to 98x98\. This hasn't
    reduced the dimensionality of the image enough to perform deep learning effectively
    (as we would have to learn 98x98 parameters per convolutional layer). Therefore,
    we apply a pooling layer to further reduce the dimensionality of the layer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在大图像中（或在自然语言处理中的复杂句子），我们还需要实现池化层。在我们前面的示例中，将 3x3 卷积应用于 5x5 图像会得到 3x3 的输出。但是，如果我们将
    3x3 卷积应用于 100x100 像素图像，则仅将输出减少至 98x98。这并没有降低图像的维度以有效地进行深度学习（因为我们必须为每个卷积层学习 98x98
    个参数）。因此，我们应用池化层来进一步降低层的维度。
- en: 'A pooling layer applies a function (typically, a max function) to the output
    of the convolutional layer in order to reduce its dimensionality. This function
    is applied over a sliding window, similar to how our convolutions are performed,
    except now our convolutions do not overlap. Let''s assume our convolutional layer
    has an output of 4x4 and we apply a 2x2 max pooling function to our output. This
    means that for every smaller 2x2 grid within our layer, we apply a max function
    and keep the resulting output. We can see this in the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层对卷积层的输出应用一个函数（通常是最大函数），以降低其维度。这个函数是在一个滑动窗口上应用的，类似于我们执行卷积的方式，只是现在我们的卷积不重叠。假设我们的卷积层输出为4x4，并且我们对输出应用一个2x2的最大池化函数。这意味着对于我们层内的每个较小的2x2网格，我们应用一个最大函数并保留生成的输出。我们可以在以下图中看到这一点：
- en: '![Figure 6.3 – Pooling layers'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 - 池化层'
- en: '](img/B12365_06_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_03.jpg)'
- en: Figure 6.3 – Pooling layers
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 - 池化层
- en: These pooling layers have been shown to effectively reduce the dimensionality
    of our data, while still retaining much of the essential information from the
    convolutional layer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些池化层已被证明可以有效地降低数据的维度，同时仍保留了卷积层中大部分基本信息。
- en: This combination of convolutions and pooling layers is essentially how CNNs
    learn from images. We can see that by applying many of these convolutional processes
    (also known as **convolutional layers**), we are able to capture information about
    any given pixel's relationship to its neighboring pixels. Within CNNs, the parameters
    we aim to learn are the values of the convolution kernel itself. This means that
    our model effectively learns how it should convolve across an image in order to
    be able to extract the necessary information required to make a classification.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种卷积和池化层的组合基本上是CNN从图像中学习的方式。我们可以看到，通过应用许多这些卷积过程（也称为**卷积层**），我们能够捕捉任给像素与其邻近像素的关系的信息。在CNN中，我们试图学习的参数是卷积核本身的值。这意味着我们的模型有效地学习如何应该在图像上进行卷积以提取必要的信息以进行分类。
- en: There are two main advantages to using convolutions in this context. Firstly,
    we are able to compose a series of low-level features into a higher-level feature;
    that is, a 3x3 patch on our initial image is composed into a single value. This
    effectively acts as a form of feature reduction and allows us to only extract
    the relevant information from our image. The other advantage that using convolutions
    has is that it makes our model location invariant. In our digit detector example,
    we do not care if the number occurs on the right-hand side of the image or the
    left-hand side; we just want to be able to detect it. As our convolutions will
    detect specific patterns within our image (that is, edges), this makes our model
    location invariant as the same features will theoretically be picked up by the
    convolutions, no matter where they occur within the image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用卷积的两个主要优点。首先，我们能够将一系列低级特征组合成一个高级特征；也就是说，我们初始图像上的一个3x3块被组合成一个单个值。这实际上起到了一种特征减少的作用，使我们能够仅从图像中提取相关信息。使用卷积的另一个优点是，它使我们的模型具有位置不变性。在我们的数字检测器示例中，我们不关心数字出现在图像的右侧还是左侧；我们只想要能够检测到它。由于我们的卷积将在图像中检测特定模式（即边缘），这使得我们的模型在任何地方检测到相同的特征都会被理论上通过卷积捕捉到，从而使我们的模型具有位置不变性。
- en: While these principles are useful for understanding how convolutions work in
    image data, they can also be applied to NLP data. We'll look at this in the next
    section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些原则对于理解卷积在图像数据中的工作方式很有用，但也可以应用到自然语言处理数据中。我们将在下一节中进行讨论。
- en: Convolutions for NLP
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理的卷积
- en: As we have seen many times in this book, we can represent individual words numerically
    as vectors, and represent whole sentences and documents as a sequence of vectors.
    When we represent our sentence as a sequence of vectors, we can represent this
    as a matrix. If we have a matrix representation of a given sentence, we notice
    immediately that this is similar to the image we convolved over within our image
    convolutions. Therefore, we can apply convolutions to NLP in a similar fashion
    to images, provided we can represent our text as a matrix.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中很多次看到的，我们可以将单词表示为向量，将整个句子和文档表示为向量序列。当我们将我们的句子表示为向量序列时，我们可以将其表示为一个矩阵。如果我们有一个给定句子的矩阵表示，我们立即注意到这与我们在图像卷积中卷积过的图像相似。因此，我们可以类似地将卷积应用到自然语言处理中，只要我们能够将我们的文本表示为矩阵。
- en: Let's first consider the basis for using this methodology. When we looked at
    n-grams previously, we saw that the context of a word in a sentence depends on
    the words preceding it and the words coming after it. Therefore, if we can convolve
    over a sentence in a way that allows us to capture the relation of one word to
    the words around it, we can theoretically detect patterns in language and use
    this to better classify our sentences.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑使用这种方法的基础。当我们之前研究 n 元组时，我们看到句子中的一个词的上下文取决于它前面的词和后面的词。因此，如果我们能以一种允许我们捕捉单词与周围单词关系的方式对句子进行卷积，我们可以在理论上检测语言中的模式，并用此来更好地分类我们的句子。
- en: It is also worth noting that our method of convolution is slightly different
    to our convolution on images. In our image matrix, we wish to capture the context
    of a single pixel relative to those surrounding it, whereas in a sentence, we
    wish to capture the context of a whole word vector, relative to the other vectors
    around it. Therefore, in NLP, we want to perform our convolutions across whole
    word vectors, rather than within word vectors. This is demonstrated in the following
    diagram.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的卷积方法与图像上的卷积略有不同，值得注意。在我们的图像矩阵中，我们希望捕获单个像素相对于周围像素的上下文，而在句子中，我们希望捕获整个词向量相对于周围向量的上下文。因此，在自然语言处理中，我们希望跨整个词向量执行卷积，而不是在词向量内部执行。以下图表展示了这一点。
- en: 'We first represent our sentence as individual word vectors:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将我们的句子表示为单个词向量：
- en: '![Figure 6.4 – Word vectors'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – 词向量'
- en: '](img/B12365_06_04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_04.jpg)'
- en: Figure 6.4 – Word vectors
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 词向量
- en: 'We then apply a (2 x *n*) convolution over the matrix (where *n* is the length
    of our word vectors; in this instance, *n* = 5). We can convolve four different
    times using a (2 x *n*) filter, which reduces down into four outputs. You will
    notice that this is similar to a bi-gram model, where there are four possible
    bi-grams within a five-word sentence:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在矩阵上应用 (2 x *n*) 的卷积（其中 *n* 是我们词向量的长度；在这个例子中，*n* = 5）。我们可以使用 (2 x *n*) 的滤波器进行四次卷积，从而得到四个输出。您会注意到，这类似于一个二元组模型，在一个五个词的句子中可以有四个可能的二元组：
- en: '![Figure 6.5 – Convolving word vectors into bi-grams'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5 – 将词向量卷积成二元组'
- en: '](img/B12365_06_05.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_05.jpg)'
- en: Figure 6.5 – Convolving word vectors into bi-grams
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 将词向量卷积成二元组
- en: 'Similarly, we can do this for any number of n-grams; for example, *n*=3:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以针对任意数量的 n 元组执行此操作；例如，*n*=3：
- en: '![Figure 6.6 – Convolving word vectors into n-grams'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.6 – 将词向量卷积成 n 元组'
- en: '](img/B12365_06_06.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_06.jpg)'
- en: Figure 6.6 – Convolving word vectors into n-grams
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 将词向量卷积成 n 元组
- en: 'One of the benefits of convolutional models such as this is there is no limit
    to the number of n-grams we can convolve over. We are also able to convolve over
    multiple different n-grams simultaneously. So, to capture both bi-grams and trigrams,
    we could set our model up like so:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这类卷积模型的一个好处是我们可以无限制地对 n 元组进行卷积。我们还能同时对多个不同的 n 元组进行卷积。因此，为了捕获二元组和三元组，我们可以设置我们的模型如下：
- en: '![Figure 6.7 – Convolving word vectors into bi-grams and trigrams'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7 – 将词向量卷积成二元组和三元组'
- en: '](img/B12365_06_07.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_07.jpg)'
- en: Figure 6.7 – Convolving word vectors into bi-grams and trigrams
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 将词向量卷积成二元组和三元组
- en: Although CNNs for NLP have advantages such as those described in the preceding
    sections, they do have their drawbacks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管卷积神经网络在自然语言处理中具有如前文所述的优势，但它们也有其局限性。
- en: In CNNs for images, the assumption that a given pixel is likely related to those
    surrounding it is a reasonable one. When applied to NLP, while this assumption
    is partially correct, words can be semantically related, even if they are not
    in direct proximity to one another. A word at the start of a sentence can pertain
    to the word at the end of a sentence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像的卷积神经网络中，假设某个像素可能与周围像素相关联是合理的。当应用于自然语言处理时，尽管这种假设部分正确，但词语可以在语义上相关，即使它们不直接接近。句子开头的词可能与句子结尾的词相关。
- en: While our RNN models may be able to detect this relationship through longer-term
    memory dependencies, our CNNs may struggle as CNNs only capture the context of
    the words surrounding the target word.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的循环神经网络模型可以通过长期记忆依赖来检测这种关系，但我们的卷积神经网络可能会遇到困难，因为卷积神经网络只能捕获周围单词的上下文。
- en: That being said, CNNs for NLP have been proven to perform very well in some
    tasks, even though our language assumptions do not necessarily hold. Arguably,
    the main advantages of using CNNs for NLP are speed and efficiency. Convolutions
    can be easily implemented on GPUs, allowing for fast parallelized computation
    and training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，尽管我们的语言假设不一定成立，但 CNN 在 NLP 中已被证明在某些任务中表现非常好。可以说，使用 CNN 进行 NLP 的主要优势是速度和效率。卷积可以在
    GPU 上轻松实现，允许进行快速并行计算和训练。
- en: The way that relationships between words are captured is also much more efficient.
    In a true n-gram model, the model must learn individual representations for every
    single n-gram, whereas in our CNN models, we just learn the convolutional kernels,
    which will automatically extract the relationships between given word vectors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉单词之间关系的方式也更加高效。在真正的 n-gram 模型中，模型必须学习每个单独 n-gram 的表示，而在我们的 CNN 模型中，我们只需学习卷积核，它将自动提取给定单词向量之间的关系。
- en: Now that we have defined how our CNN will learn from our data, we can begin
    to code up a model from scratch.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的 CNN 将如何从我们的数据中学习，我们可以开始从头编写一个模型的代码。
- en: Building a CNN for text classification
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为文本分类构建 CNN
- en: Now that we know the basics of CNNs, we can begin to build one from scratch.
    In the previous chapter, we built a model for sentiment prediction, where sentiment
    was a binary classifier; `1` for positive and `0` for negative. However, in this
    example, we will aim to build a CNN for **multi-class text classification**. In
    a multi-class problem, a particular example can only be classified as one of several
    classes. If an example can be classified as many different classes, then this
    is multi-label classification. Since our model is multi-class, this means that
    our model will aim at predicting which one of several classes our input sentence
    is classified as. While this problem is considerably more difficult than our binary
    classification task (as our sentence can now belong to one of many, rather than
    one of two classes), we will show that CNNs can deliver good performance on this
    task. We will first begin by defining our data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 CNN 的基础知识，我们可以开始从头构建一个。在上一章中，我们为情感预测构建了一个模型，其中情感是一个二元分类器；`1`表示积极，`0`表示消极。然而，在这个例子中，我们的目标是构建一个用于**多类文本分类**的
    CNN。在多类问题中，一个特定的例子只能被分类为多个类别之一。如果一个例子可以被分类为许多不同的类别，那么这是多标签分类。由于我们的模型是多类的，这意味着我们的模型将致力于预测我们的输入句子被分类为几个类别中的哪一个。虽然这个问题比我们的二元分类任务要困难得多（因为我们的句子现在可以属于多个，而不是两个类别之一），我们将展示
    CNN 在这个任务上可以提供良好的性能。我们首先开始定义我们的数据。
- en: Defining a multi-class classification dataset
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个多类别分类数据集
- en: 'In the previous chapter, we looked at a selection of reviews and learned a
    binary classification based on whether the review was positive or negative. For
    this task, we will look at data from the TREC ([https://trec.nist.gov/data/qa.html](https://trec.nist.gov/data/qa.html))
    dataset, which is a common dataset used to evaluate the performance of a model''s
    text-classification tasks. The dataset consists of a selection of questions, each
    of which falls into one of six broad semantic categories that our trained model
    will learn to classify. These six categories are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们查看了一些评论，并学习了基于评论是积极的还是消极的二元分类。对于这个任务，我们将查看来自 TREC ([https://trec.nist.gov/data/qa.html](https://trec.nist.gov/data/qa.html))
    数据集的数据，这是一个常用的数据集，用于评估模型文本分类任务的性能。该数据集包含一系列问题，每个问题都属于我们训练模型将学习分类的六个广泛语义类别之一。这六个类别如下：
- en: '![Figure 6.8 – Semantic categories in the TREC dataset'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8 – TREC 数据集中的语义类别'
- en: '](img/B12365_06_08.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_08.jpg)'
- en: Figure 6.8 – Semantic categories in the TREC dataset
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – TREC 数据集中的语义类别
- en: 'This means that unlike our previous classification class, where our model output
    was a single prediction between `0` and `1`, our multi-class prediction model
    now returns a probability for each of the six possible classes. We assume that
    the prediction that''s made is for the class with the highest prediction:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着与我们之前的分类类不同，我们的模型输出不是在`0`和`1`之间的单一预测，而是我们的多类预测模型现在为每个六个可能类别之一返回一个概率。我们假设所做的预测是针对具有最高预测的类别：
- en: '![Figure 6.9 – Prediction values'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9 – 预测值'
- en: '](img/B12365_06_09.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_09.jpg)'
- en: Figure 6.9 – Prediction values
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 预测值
- en: In this way, our model will now be able to perform classification tasks over
    several classes and we are no longer restricted to the 0 or 1 binary classification
    we looked at previously. Models with multiple classes may suffer in terms of predictions
    as there are more different classes to differentiate between.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一来，我们的模型现在能够在多个类别上执行分类任务，不再局限于之前看到的0或1的二元分类。由于多类模型需要区分更多不同的类别，因此在预测方面可能会受到影响。
- en: In a binary classification model, assuming we had a balanced dataset, we would
    expect our model to have an accuracy of 50% if it were just to perform random
    guesses, whereas a multi-class model with five different classes would only have
    a baseline accuracy of 20%. This means that just because a multiclass model has
    an accuracy much lower than 100%, this does not mean the model itself is inherently
    bad at making predictions. This is particularly true when it comes to training
    models that predict from hundreds of different classes. In these cases, a model
    with just 50% accuracy would be considered to be performing very well.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个二分类模型中，假设我们有一个平衡的数据集，如果仅进行随机猜测，我们预期模型的准确率为50%，而具有五个不同类别的多类模型的基准准确率仅为20%。这意味着，仅仅因为多类模型的准确率远低于100%，并不意味着模型本身在进行预测时存在问题。这在训练需要预测数百种不同类别的模型时尤为真实。在这些情况下，准确率仅为50%的模型被认为是表现非常良好的。
- en: Now that we have defined our multi-class classification problem, we need to
    load our data in order to train a model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了我们的多类分类问题，需要加载我们的数据以便训练模型。
- en: Creating iterators to load the data
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于加载数据的迭代器
- en: In our LSTM model in the previous chapter, we simply used a `.csv` file containing
    all the data we used to train our model. We then manually converted this data
    into input tensors and fed them one by one into our network in order to train
    it. While this methodology is perfectly acceptable, it is not the most efficient
    one.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章节的LSTM模型中，我们简单地使用了一个包含所有用于训练模型的数据的`.csv`文件。然后，我们手动将这些数据转换为输入张量，并逐个将它们馈送到网络中进行训练。虽然这种方法完全可以接受，但并不是最有效的方法。
- en: In our CNN model, we will instead look at creating data iterators from our data.
    These iterator objects allow us to easily generate small batches of data from
    our input data, thus allowing us to train our model using mini batches, rather
    than feeding our input data one by one into the network. This means that the gradients
    within our network are calculated across a whole batch of data and that parameter
    adjustments happen after each batch rather than after each individual row of data
    is passed through the network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的CNN模型中，我们将考虑从我们的数据中创建数据迭代器。这些迭代器对象允许我们从输入数据中轻松生成小批量数据，从而允许我们使用小批量而不是将输入数据逐个馈送到网络中进行训练。这意味着网络内部的梯度是跨整个数据批次计算的，并且参数调整发生在每个批次之后，而不是在每个数据行通过网络之后。
- en: For our data, we will take our dataset from the TorchText package. This has
    the advantage of not only containing a number of datasets for model training,
    but it also allows us to easily tokenize and vectorize our sentences using the
    inbuilt functions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，我们将从TorchText包中获取我们的数据集。这不仅包含了用于模型训练的多个数据集，还允许我们使用内置函数轻松地对句子进行标记化和向量化。
- en: 'Follow these steps:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行操作：
- en: 'We first import the data and dataset functions from TorchText:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从TorchText导入数据和数据集函数：
- en: '[PRE0]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we create a field and label field we can use with the `TorchText` package.
    These define the initial processing steps our model will use to process our data:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个字段和标签字段，这些字段可以与`TorchText`包一起使用。这些定义了模型处理数据的初始步骤：
- en: '[PRE1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we set tokenize equal to `spacy` in order to set how our input sentences
    will be tokenized. `TorchText` then uses the `spacy` package to automatically
    tokenize the input sentences. `spacy` consists of an index of the English language,
    so any words are automatically transformed into the relevant tokens. You may need
    to install `spacy` in order for this to work. This can be done within the command
    line by typing the following:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将tokenize设置为`spacy`，以设置如何对输入句子进行标记化。然后，`TorchText`使用`spacy`包自动对输入句子进行标记化。`spacy`包含英语语言的索引，因此任何单词都会自动转换为相关的标记。您可能需要在命令行中安装`spacy`才能使其工作。可以通过输入以下内容来完成这一步骤：
- en: '[PRE2]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This installs `spacy` and downloads the English word index.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将安装`spacy`并下载英语词汇索引。
- en: 'We also define the data type for our labels as floats, which will allow us
    to calculate our losses and gradients. After defining our fields, we can use these
    to split our input data. Using the `TREC` dataset from `TorchText`, we pass this
    our questions and labels fields in order to process the dataset accordingly. We
    then call the `split` function in order to automatically divide our dataset into
    a training set and a validation set:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将我们的标签数据类型定义为浮点数，这将允许我们计算我们的损失和梯度。在定义完我们的字段之后，我们可以使用它们来分割我们的输入数据。使用`TorchText`中的`TREC`数据集，我们将传递问题和标签字段以相应地处理数据集。然后，我们调用`split`函数自动将数据集分成训练集和验证集：
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that, normally, we can view our datasets in Python by simply calling the
    train data:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，通常情况下，我们可以通过简单调用训练数据来查看我们的数据集：
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'However, here, we are dealing with a `TorchText` dataset object, rather than
    a dataset loaded into pandas, as we might be used to seeing. This means that our
    output from the preceding code is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这里，我们处理的是一个`TorchText`数据集对象，而不是像我们可能习惯看到的加载到 pandas 中的数据集。这意味着我们从上述代码得到的输出如下所示：
- en: '![Figure 6.10 – Output of the TorchText object'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10 – TorchText 对象的输出'
- en: '](img/B12365_06_10.png)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_10.png)'
- en: Figure 6.10 – Output of the TorchText object
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – TorchText 对象的输出
- en: 'It is possible for us to view individual data within this dataset object; we
    just need to call the `.examples` parameter. Each of these examples will have
    a text and a label parameter that we can examine like so for the text:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看此数据集对象中的单个数据，只需调用`.examples`参数。每个示例都会有一个文本和一个标签参数，我们可以像这样检查文本：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This returns the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '![Figure 6.11 – Data in the dataset object'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.11 – 数据集对象中的数据'
- en: '](img/B12365_06_11.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_11.jpg)'
- en: Figure 6.11 – Data in the dataset object
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 数据集对象中的数据
- en: 'The label code is run as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 标签代码如下运行：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives us the following output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下输出：
- en: '![Figure 6.12 – Label of the dataset object'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.12 – 数据集对象的标签'
- en: '](img/B12365_06_012.png)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_012.png)'
- en: Figure 6.12 – Label of the dataset object
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 数据集对象的标签
- en: 'So, we can see that our input data consists of a tokenized sentence and that
    our label consists of the category that we wish to classify. We can also check
    the size of our training and validation sets, like so:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到我们的输入数据包括一个标记化的句子，我们的标签包括我们希望分类的类别。我们还可以检查我们的训练集和验证集的大小，如下所示：
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This results in the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 6.13 – Sizes of the training and validation sets'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.13 – 训练集和验证集的大小'
- en: '](img/B12365_06_013.png)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_013.png)'
- en: Figure 6.13 – Sizes of the training and validation sets
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 训练集和验证集的大小
- en: This shows that our training to validation ratio is approximately 70% to 30%.
    It is worth noting exactly how our input sentence has been tokenized, namely that
    punctuation marks are treated as their own tokens.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示我们的训练到验证比例大约为70%到30%。值得注意的是我们的输入句子是如何被标记化的，即标点符号被视为它们自己的标记。
- en: 'Now that we know that our neural network will not take raw text as an input,
    we have to find some way of turning this into some form of embedding representation.
    While it is possible for us to train our own embedding layer, we can instead transform
    our data using the pre-computed `glove` vectors that we discussed in [*Chapter
    3*](B12365_03_Final_JC_ePub.xhtml#_idTextAnchor051)*, Performing Text Embeddings*.
    This also has the added benefit of making our model faster to train as we won''t
    manually need to train our embedding layer from scratch:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们的神经网络不会将原始文本作为输入，我们必须找到一些方法将其转换为某种嵌入表示形式。虽然我们可以训练自己的嵌入层，但我们可以使用我们在[*第
    3 章*](B12365_03_Final_JC_ePub.xhtml#_idTextAnchor051)*，执行文本嵌入*中讨论过的预先计算的`glove`向量来转换我们的数据。这还有一个额外的好处，可以使我们的模型训练更快，因为我们不需要手动从头开始训练我们的嵌入层：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we can see that by using the `build_vocab` function and passing our questions
    and labels as our training data, we can build a vocabulary composed of 200-dimensional
    GLoVe vectors. Note that the TorchText package will automatically download and
    grab the GLoVe vectors, so there is no need to manually install GLoVe in this
    instance. We also define how we wish to treat unknown values within our vocabulary
    (that is, what the model does if it is passed a token that isn't in the pretrained
    vocabulary). In this instance, we choose to treat them as a normal tensor with
    an unspecified value, although we will update this later.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到通过使用 `build_vocab` 函数，并将我们的问题和标签作为我们的训练数据传递，我们可以构建一个由200维GLoVe向量组成的词汇表。请注意，TorchText包将自动下载和获取GLoVe向量，因此在这种情况下无需手动安装GLoVe。我们还定义了我们希望如何处理词汇表中的未知值（即，如果模型传递了一个不在预训练词汇表中的标记时模型将如何处理）。在这种情况下，我们选择将它们视为具有未指定值的普通张量，尽管稍后我们将更新这个值。
- en: 'We can now see that our vocabulary consists of a series of pre-trained 200-dimensional
    GLoVe vectors by calling the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用以下命令，我们现在可以看到我们的词汇表由一系列预训练的200维 GLoVe 向量组成：
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in the following output:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![Figure 6.14 – Tensor contents'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14 – 张量内容'
- en: '](img/B12365_06_014.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_014.jpg)'
- en: Figure 6.14 – Tensor contents
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 张量内容
- en: 'Next, we create our data iterators. We create separate iterators for both our
    training and validation data. We first specify a device so that we are able to
    train our model faster using a CUDA-enabled GPU, if one is available. Within our
    iterators, we also specify the size of our batches to be returned by the iterators,
    which in this case is `64`. You may wish to experiment with using different batch
    sizes for your model as this may affect training speed and how fast the model
    converges into its global optimum:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建我们的数据迭代器。我们为我们的训练和验证数据分别创建单独的迭代器。我们首先指定一个设备，以便在有CUDA启用的GPU时能够更快地训练我们的模型。在我们的迭代器中，我们还指定了由迭代器返回的批次的大小，在这种情况下是`64`。您可能希望尝试使用不同的批次大小来进行模型训练，因为这可能会影响训练速度以及模型收敛到全局最优的速度：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Constructing the CNN model
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建CNN模型
- en: 'Now that we have loaded the data, we are ready to create the model. We will
    use the following steps to do so:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了数据，准备好创建模型了。我们将使用以下步骤来完成：
- en: 'We wish to build the structure of our CNN. We begin as usual by defining our
    model as a class that inherits from `nn.Module`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望构建我们的CNN的结构。我们像往常一样从定义我们的模型作为一个从`nn.Module`继承的类开始：
- en: '[PRE11]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our model is initialized with several inputs, all of which will be covered
    shortly. Next, we individually define the layers within our network, starting
    with our embedding layer:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模型被初始化为几个输入，所有这些输入很快将被覆盖。接下来，我们单独定义网络中的各个层，从我们的嵌入层开始：
- en: '[PRE12]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The embedding layer will consist of embeddings for each possible word in our
    vocabulary, so the size of the layer is the length of our vocabulary and the length
    of our embedding vectors. We are using the 200-dimensional GLoVe vectors, so the
    length will be `200` in this instance. We must also pass the padding index, which
    is the index of our embedding layer that’s used to get the embedding to pad our
    sentences so that they are all the same length. We will manually define this embedding
    later on when we initialize our model.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 嵌入层将包含词汇表中每个可能单词的嵌入，因此层的大小是我们词汇表的长度和我们嵌入向量的长度。我们使用的是200维的GLoVe向量，因此在这个例子中长度将为`200`。我们还必须传递填充索引，该索引是我们嵌入层中用于获取填充我们句子的嵌入的索引，以便它们的长度都相同。当我们初始化我们的模型时，我们稍后将手动定义这个嵌入。
- en: 'Next, we define the actual convolutional layers within our network:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义网络内部的实际卷积层：
- en: '[PRE13]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We start by using `nn.ModuleList` to define a series of convolutional layers.
    `ModuleList` takes a list of modules as input and is used when you wish to define
    a number of separate layers. As we wish to train several different convolutional
    layers of different sizes on our input data, we can use `ModuleList` to do so.
    We could theoretically define each layer separately like so:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先使用 `nn.ModuleList` 来定义一系列卷积层。`ModuleList` 接受一个模块列表作为输入，并且在您希望定义多个单独的层时使用。由于我们希望在输入数据上训练几个不同大小的卷积层，我们可以使用
    `ModuleList` 来实现。我们可以理论上像这样分别定义每一层：
- en: '[PRE14]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the filter sizes are `2` and `3`, respectively. However, it is more efficient
    to do this in a single function. Furthermore, our layers will be automatically
    generated if we pass different filter sizes to the function, rather than having
    to manually define each layer each time we add a new one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，滤波器的尺寸分别为`2`和`3`。然而，将这些操作合并到一个函数中会更加高效。此外，如果我们将不同的滤波器尺寸传递给函数，而不是每次添加新层时手动定义每一层，我们的层将会自动生成。
- en: 'We also define the `out_channels` value as the number of filters we wish to
    train; `kernel_size` will contain the length of our embeddings. Therefore, we
    can pass our `ModuleList` function the lengths of the filters we wish to train
    and the amount of each and it will automatically generate the convolutional layers.
    An example of how this convolution layer might look for a given set of variables
    is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将`out_channels`值定义为我们希望训练的滤波器数量；`kernel_size`将包含我们嵌入的长度。因此，我们可以将我们的`ModuleList`函数传递给我们希望训练的滤波器长度和数量，它将自动生成卷积层。以下是给定一组变量时此卷积层可能的示例：
- en: '![Figure 6.15 – Convolution layer looking for variables'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.15 – 寻找变量的卷积层'
- en: '](img/B12365_06_015.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_015.jpg)'
- en: Figure 6.15 – Convolution layer looking for variables
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 寻找变量的卷积层
- en: 'We can see that our `ModuleList` function adapts to the number of filters and
    the size of the filters we wish to train. Next, within our CNN initialization,
    we define the remaining layers, namely the linear layer, which will classify our
    data, and the dropout layer, which will regularize our network:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的`ModuleList`函数适应于我们希望训练的滤波器数量和大小。接下来，在我们的CNN初始化中，我们定义剩余的层，即线性层，用于分类我们的数据，以及dropout层，用于正则化我们的网络：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note how, in the past, the size of our linear layer has always been `1` as we
    have only ever needed a single output node to perform binary classification. Since
    we are now tackling a multi-class classification problem, we wish to make a prediction
    on each of our potential classes, so our output dimensions are now variable instead
    of being just `1`. When we initialize our network, we will set out output dimensions
    to be `6` since we are predicting which one of the six classes our sentence is
    from.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，过去，我们的线性层的尺寸始终为`1`，因为我们只需要一个单独的输出节点来执行二元分类。由于我们现在正在解决多类分类问题，我们希望对每个潜在类别进行预测，因此我们的输出维度现在是可变的，而不仅仅是`1`。当我们初始化网络时，我们将设置输出维度为`6`，因为我们正在预测句子来自六个类别中的哪一个。
- en: 'Next, as with all our neural networks, we must define our `forward` pass:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，与所有我们的神经网络一样，我们必须定义我们的`forward`传递：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we first pass our input text through our embedding layer to obtain the
    embeddings for all the words in the sentences. Next, for each of the previously
    defined convolutional layers that we passed our embedded sentence into, we apply
    a `relu` activation function and squeeze the results, removing the fourth dimension
    of the resulting output. This is repeated for all of our defined convolutional
    layers so that `conved` consists in a list of the outputs for all of our convolutional
    layers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们首先通过我们的嵌入层将输入文本传递，以获取句子中所有单词的嵌入。接下来，对于我们之前定义的每一个卷积层，我们将嵌入的句子传递，应用一个`relu`激活函数并挤压结果，移除结果输出的第四个维度。这对于我们所有定义的卷积层都是重复的，以便`conved`包含我们所有卷积层输出的列表。
- en: 'For each of these outputs, we apply a pooling function to reduce the dimensionality
    of our convolutional layer outputs, as described earlier. We then concatenate
    all the outputs of our pooling layers together and apply a dropout function before
    passing this to our final fully connected layer, which will make our class predictions.
    After fully defining our CNN class, we create an instance of the model. We define
    our hyperparameters and create an instance of the CNN class using them:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些输出的每一个，我们应用一个池化函数来减少我们卷积层输出的维度，如前所述。然后，我们将所有池化层的输出连接起来，并在传递到我们最终的全连接层之前应用一个dropout函数，这将做出我们的类预测。在完全定义了我们的CNN类之后，我们创建模型的一个实例。我们定义超参数，并使用它们创建CNN类的一个实例：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our input dimensions will always be the length of our vocabulary, while our
    output dimensions will be the number of classes we wish to predict. Here, we are
    predicting from six different classes, so our output vector will have a length
    of `6`. Our embedding dimensions are the length of our GLoVe vectors (in this
    case, `200`). The padding index can be manually taken from our vocabulary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入维度将始终是我们词汇表的长度，而输出维度将是我们希望预测的类别数。在这里，我们预测六种不同的类别，因此我们的输出向量长度将为`6`。我们的嵌入维度是我们的
    GLoVe 向量的长度（在本例中为`200`）。填充索引可以手动从我们的词汇表中获取。
- en: The next three hyperparameters can be manually adjusted, so you may wish to
    experiment with choosing different values to see how this affects your network's
    final output. We pass a list of filter sizes so that our model will train convolutional
    layers using convolutions of size `2`, `3`, and `4`. We will train 100 of these
    filters for each of these filter sizes, so there will be 300 filters in total.
    We also define a dropout percentage of 50% for our network to ensure it is sufficiently
    regularized. This can be raised/lowered if the model seems prone to overfitting
    or underfitting. A general rule of thumb is to try lowering the dropout rate if
    the model is underfitting and raising it if the model appears to be overfitting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三个超参数可以手动调整，因此您可能希望尝试选择不同的值，以查看这如何影响您的网络的最终输出。我们传递一个过滤器大小的列表，以便我们的模型将使用大小为`2`、`3`和`4`的卷积训练卷积层。我们将为每种过滤器大小训练
    100 个这些过滤器，因此总共将有 300 个过滤器。我们还为我们的网络定义了 50% 的丢失率，以确保它足够规范化。如果模型似乎容易过度拟合或欠拟合，可以提高/降低此值。一个一般的经验法则是，如果模型欠拟合，尝试降低丢失率，如果模型过拟合，则尝试提高丢失率。
- en: 'After initializing our model, we need to load our weights into our embedding
    layer. This can be easily done as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化我们的模型之后，我们需要将权重加载到我们的嵌入层中。这可以通过以下简单完成：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 6.16 – Tensor output after lowering the dropout'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.16 – 降低丢失率后的张量输出'
- en: '](img/B12365_06_016.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_016.jpg)'
- en: Figure 6.16 – Tensor output after lowering the dropout
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 降低丢失率后的张量输出
- en: 'Next, we need to define how our model deals with instances when our model accounts
    for unknown tokens that aren''t contained within the embedding layer and how our
    model will apply padding to our input sentences. Fortunately, the easiest way
    to account for both of these scenarios is to use a vector consisting of all zeros.
    We make sure that these zero value tensors are the same length as our embedding
    vectors (in this instance, `200`):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义我们的模型如何处理模型账户中未知的标记，这些标记不包含在嵌入层中，并且我们的模型将如何将填充应用到我们的输入句子中。幸运的是，解决这两种情况的最简单方法是使用一个由全零组成的向量。我们确保这些零值张量与我们的嵌入向量长度相同（在这个实例中为`200`）：
- en: '[PRE19]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we define our optimizer and criterion (loss) function. Notice how
    we choose to use cross-entropy loss instead of binary cross-entropy as our classification
    task is no longer binary. We also use `.to(device)` to train our model using our
    specified device. This means that our training will be done on a CUDA-enabled
    GPU if one is available:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义我们的优化器和准则（损失）函数。请注意，我们选择使用交叉熵损失而不是二元交叉熵，因为我们的分类任务不再是二元的。我们还使用`.to(device)`来使用指定的设备训练我们的模型。这意味着如果有可用的
    CUDA 启用的 GPU，我们的训练将在其上进行：
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that our model's structure has been fully defined, we are ready to start
    training the model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型结构已经完全定义，我们准备开始训练模型。
- en: Training the CNN
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 CNN
- en: Before we define our training process, we need to calculate a performance metric
    to illustrate how our model's performance (hopefully!) increases over time. In
    our binary classification tasks, accuracy was a simple metric we used to measure
    performance. For our multi-classification task, we will again use accuracy, but
    the procedure to calculate it is slightly more complex as we must now work out
    which of the six classes our model predicted and which of the six classes was
    the correct one.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义训练过程之前，我们需要计算一个性能指标，以说明我们希望我们模型的性能（希望如此！）随时间增加。在我们的二元分类任务中，准确度是我们用来衡量性能的一个简单指标。对于我们的多分类任务，我们将再次使用准确度，但是计算它的过程略微复杂，因为我们现在必须弄清楚我们模型预测了哪一个六个类别中的哪一个，并且哪一个六个类别是正确的。
- en: 'First, we define a function called `multi_accuracy` to calculate this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个名为`multi_accuracy`的函数来计算这个：
- en: '[PRE21]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, for our predictions, our model returns the indices with the highest value
    for each prediction using the `torch.max` function for all of them. For each of
    these predictions, if this predicted index is the same as the index of our label,
    it is treated as a correct prediction. We then count all these correct predictions
    and divide them by the total number of predictions to get a measure of multi-class
    accuracy. We can use this function within our training loop to measure accuracy
    at each epoch.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于我们的预测，我们的模型使用 `torch.max` 函数为每个预测返回最高值的索引。对于每个预测，如果此预测的索引与标签的索引相同，则将其视为正确预测。然后我们计算所有这些正确预测的数量，并将其除以总预测数量以得到多类别准确度的度量。我们可以在训练循环中使用此函数来测量每个
    epoch 的准确度。
- en: 'Next, we define our training function. We initially set our loss and accuracy
    for the epoch to be `0` and we call `model.train()` to allow the parameters within
    our model to be updated as we train our model:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的训练函数。我们最初将该 epoch 的损失和准确度设置为 `0`，并调用 `model.train()` 以允许在训练模型时更新模型内部的参数：
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we loop through each batch of data within our iterator and perform the
    training steps. We start by zeroing our gradients to prevent cumulative gradients
    from being calculated from our previous batch. We then use our model''s current
    state to make predictions from our sentences in the current batch, which we then
    compare to our labels to calculate loss. Using the accuracy function we defined
    in the preceding section, we can calculate the accuracy for this given batch.
    We then backpropagate our loss, updating our weights via gradient descent and
    step through our optimizer:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在迭代器中循环每个数据批次并执行训练步骤。我们首先将梯度清零，以防止从先前批次计算的累积梯度。然后，我们使用当前批次中句子的模型当前状态进行预测，然后与我们的标签进行比较以计算损失。使用我们在前面部分定义的准确度函数，我们可以计算给定批次的准确度。然后我们反向传播我们的损失，通过梯度下降更新我们的权重并通过我们的优化器进行步进：
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we add the loss and accuracy from this batch to our total loss and
    accuracy for the whole epoch. After we have looped through all the batches within
    the epoch, we calculate the total loss and accuracy for the epoch and return it:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这一批次的损失和准确度加到整个 epoch 的总损失和准确度上。当我们遍历完整个 epoch 中的所有批次后，我们计算该 epoch 的总损失和准确度，并返回它：
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similarly, we can define a function called `eval` that is called on our validation
    data to calculate our trained model performance on a set of data that our model
    has not been trained on. While this function is almost identical to the training
    function we defined previously, there are two key additions we must make:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以定义一个名为 `eval` 的函数，在我们的验证数据上调用它，以计算我们训练过的模型在我们尚未训练的一组数据上的性能。虽然这个函数与我们之前定义的训练函数几乎相同，但我们必须做两个关键的添加：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: These two steps set our model to evaluation mode, ignore any dropout functions,
    and make sure that the gradients are not calculated and updated. This is because
    we want the weights within our model to be frozen while we evaluate performance,
    as well as to ensure that our model is not trained using our validation data as
    we want this to be kept separate from the data we used to train the model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤将我们的模型设置为评估模式，忽略任何 dropout 函数，并确保不计算和更新梯度。这是因为我们希望在评估性能时冻结模型中的权重，并确保我们的模型不会使用验证数据进行训练，因为我们希望将其与用于训练模型的数据分开保留。
- en: Now, we just need to call our train and evaluate functions in a loop in conjunction
    with our data iterators in order to train the model. We first define the number
    of epochs we wish our model to train for. We also define the lowest validation
    loss our model has achieved so far. This is because we only wish to keep the trained
    model with the lowest validation loss (that is, the best-performing model). This
    means that if our model trains for many epochs and begins overfitting, only the
    best-performing of these models will be kept, meaning there are fewer consequences
    for picking a high number of epochs.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需在与数据迭代器结合的循环中调用我们的训练和评估函数，以训练模型。我们首先定义我们希望模型训练的 epoch 数量。我们还定义到目前为止模型已经达到的最低验证损失。这是因为我们只希望保留验证损失最低的训练模型（即性能最佳的模型）。这意味着如果我们的模型训练了多个
    epoch 并开始过拟合，只会保留这些模型中表现最佳的一个，这样选择较高数量的 epoch 将会减少后果。
- en: 'We initialize our lowest validation loss as infinity to begin with:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最低验证损失初始化为无穷大：
- en: '[PRE26]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we define our training loop, stepping through one epoch at a time. We
    record the start and end times of our training so that we can calculate how long
    each step takes. We then simply call our training function on our model using
    the training data iterator to calculate the training loss and accuracy, updating
    our model as we do so. We then repeat this process using our evaluation function
    on our validation iterator to calculate the loss and accuracy on our validation
    data, without updating our model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的训练循环，一次处理一个epoch。我们记录训练的开始和结束时间，以便计算每个步骤的持续时间。然后，我们简单地使用训练数据迭代器在我们的模型上调用训练函数，计算训练损失和准确率，并在此过程中更新我们的模型。接着，我们使用验证数据迭代器上的评估函数，计算验证数据上的损失和准确率，但不更新我们的模型：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Following this, we determine whether our model, after the current epoch, outperforms
    our best-performing model so far:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们确定我们的模型在当前epoch之后是否优于迄今为止表现最佳的模型：
- en: '[PRE28]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If the loss after this epoch is lower than the lowest validation loss so far,
    we set the validation loss to the new lowest validation loss and save our current
    model weights.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个epoch之后的损失低于迄今为止最低的验证损失，我们将验证损失设置为新的最低验证损失，并保存当前模型权重。
- en: 'Finally, we simply print the results after each epoch. If all is working correctly,
    we should see our training losses fall after every epoch, with our validation
    losses hopefully following suit:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需在每个epoch之后打印结果。如果一切正常，我们应该看到每个epoch后训练损失下降，希望验证损失也跟随下降：
- en: '[PRE29]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This results in the following output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 6.17 – Testing the model'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.17 – 测试模型'
- en: '](img/B12365_06_017.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_017.jpg)'
- en: Figure 6.17 – Testing the model
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 测试模型
- en: Thankfully, we see that this does appear to be the case. Both the training and
    validation loss fall after every epoch and the accuracy rises, showing that our
    model is indeed learning! After many training epochs, we can take our best model
    and use it to make predictions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们看到情况似乎确实如此。每个epoch后，训练损失和验证损失都在下降，准确率上升，显示我们的模型确实在学习！经过多个训练epoch后，我们可以使用最佳模型进行预测。
- en: Making predictions using the trained CNN
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用训练好的CNN进行预测
- en: 'Fortunately, using our fully trained model to make predictions is a relatively
    simple task. We first load our best model using the `load_state_dict` function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，使用我们完全训练好的模型进行预测是一个相对简单的任务。我们首先使用`load_state_dict`函数加载我们的最佳模型：
- en: '[PRE30]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Our model structure has already been defined, so we simply load the weights
    from the file we saved earlier. If this has worked correctly, you will see the
    following output:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型结构已经定义好，所以我们只需从之前保存的文件中加载权重。如果一切正常，您将看到以下输出：
- en: '![Figure 6.18 – Prediction output'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.18 – 预测输出'
- en: '](img/B12365_06_018.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_018.jpg)'
- en: Figure 6.18 – Prediction output
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 预测输出
- en: 'Next, we define a function that will take a sentence as input, preprocess it,
    pass it to our model, and return a prediction:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，该函数将以句子作为输入，对其进行预处理，将其传递给我们的模型，并返回预测：
- en: '[PRE31]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We first pass our input sentence into our tokenizer to get a list of tokens.
    We then add padding to this sentence if it is below the minimum sentence length.
    We then use our vocabulary to obtain the index of all these individual tokens
    before finally creating a tensor consisting of a vector of these indexes. We pass
    this to our GPU if it is available and then unsqueeze the output as our model
    expects a three-dimensional tensor input instead of a single vector.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将输入句子传递给我们的分词器，以获取标记列表。如果句子长度低于最小句子长度，我们然后对这个句子进行填充。然后，我们使用我们的词汇表获取所有这些单个标记的索引，最后创建一个张量，其中包含这些索引的向量。如果GPU可用，我们将其传递给GPU，然后展开输出，因为我们的模型期望三维张量输入而不是单个向量。
- en: 'Next, we make our predictions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行预测：
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We first set our model to evaluation mode (as we did with our evaluation step)
    so that the gradients of our model are not calculated and the weights are not
    adjusted. We then pass our sentence tensor into our model and obtain a prediction
    vector of length `6`, consisting of the individual predictions for each of the
    six classes. We then take the index of the maximum prediction value and use this
    within our label index to return the name of the predicted class.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将模型设置为评估模式（与我们的评估步骤相同），以便不计算模型的梯度并且不调整权重。然后，我们将句子张量传递给我们的模型，并获取长度为`6`的预测向量，其中包含每个六类别的单独预测。然后，我们取最大预测值的索引，并在标签索引中使用此索引返回预测类别的名称。
- en: 'In order to make predictions, we simply call the `predict_class` function on
    any given sentence. Let''s use the following code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行预测，我们只需在任何给定的句子上调用`predict_class`函数。让我们使用以下代码：
- en: '[PRE33]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This returns the following prediction:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下预测：
- en: '![Figure 6.19 – Prediction value'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.19 – 预测值'
- en: '](img/B12365_06_019.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_06_019.jpg)'
- en: Figure 6.19 – Prediction value
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 预测值
- en: This prediction is correct! Our input question contains `How many`, suggesting
    that the answer to this question is a numeric value. This is exactly what our
    model predicts too! You can continue to validate the model on any other questions
    you might wish to test, hopefully with similarly positive results. Congratulations
    – you have now successfully trained a multi-class CNN that can define the category
    of any given question.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预测是正确的！我们的输入问题包含`How many`，表明这个问题的答案是一个数字值。这正是我们的模型也预测到的！你可以继续在任何其他你想测试的问题上验证模型，希望能获得类似的积极结果。恭喜！你现在已经成功训练了一个能够定义任何给定问题类别的多类CNN。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have shown how CNNs can be used to learn from NLP data and
    how to train one from scratch using PyTorch. While the deep learning methodology
    is very different to the methodology used within RNNs, conceptually, CNNs use
    the motivation behind n-gram language models in an algorithmic fashion in order
    to extract implicit information about words in a sentence from the context of
    its neighboring words. Now that we have mastered both RNNs and CNNs, we can begin
    to expand on these techniques in order to construct even more advanced models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了CNN如何从NLP数据中学习，以及如何使用PyTorch从头开始训练CNN。虽然深度学习方法与RNN中使用的方法非常不同，但在概念上，CNN以算法方式使用n-gram语言模型背后的动机，以从上下文中的相邻单词中提取单词的隐含信息。现在我们已经掌握了RNN和CNN，我们可以开始扩展这些技术，以构建更先进的模型。
- en: In the next chapter, we will learn how to build models that utilize elements
    of both convolutional and recurrent neural networks and use them on sequences
    to perform even more advanced functions, such as text translation. These are known
    as sequence-to-sequence networks.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何构建既利用卷积神经网络又利用递归神经网络元素的模型，并将它们用于序列以执行更高级的功能，如文本翻译。这些被称为序列到序列网络。
