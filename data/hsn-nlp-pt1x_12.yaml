- en: '*Chapter 9*: The Road Ahead'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：未来之路'
- en: The field of machine learning is rapidly expanding, with new revelations being
    made almost yearly. The field of machine learning for NLP is no exception, with
    advancements being made rapidly and the performance of machine learning models
    on NLP tasks incrementally increasing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域正在迅速扩展，几乎每年都有新的发现。NLP机器学习领域也不例外，机器学习模型在NLP任务上的表现不断增长。
- en: So far in this book, we have discussed a number of machine learning methodologies
    that allow us to build models to perform NLP tasks such as classification, translation,
    and approximating conversation via a chatbot. However, as we have seen so far,
    the performance of our models has been worse and relative to that of a human being.
    Even using the techniques we have examined so far, including sequence-to-sequence
    networks with attention mechanisms, we are unlikely to train a chatbot model that
    will match or outperform a real person. However, we will see in this chapter that
    recent developments in the field of NLP have been made that bring us one step
    closer to the goal of creating chatbots that are indistinguishable from humans.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经讨论了许多机器学习方法论，使我们能够构建模型来执行诸如分类、翻译和通过聊天机器人逼近对话的NLP任务。然而，正如我们迄今所见，我们的模型性能相对于人类来说仍然较差。即使使用我们迄今所研究的技术，包括带有注意力机制的序列到序列网络，我们也不太可能训练出能够匹敌或超越真人的聊天机器人模型。然而，在本章中，我们将看到NLP领域的最新进展，这些进展使我们更接近创建与人类难以区分的聊天机器人的目标。
- en: In this chapter, we will explore a couple of state-of-the art machine learning
    models for NLP and examine some of the features that result in superior performance.
    We will then turn to look at several other NLP tasks that are currently the focus
    of much research, and how machine learning techniques might be used to solve them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨几种最先进的自然语言处理（NLP）机器学习模型，并分析导致其优越性能的一些特征。然后，我们将转向研究目前受到广泛研究关注的几个其他NLP任务，以及如何利用机器学习技术来解决它们。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Exploring state-of-the-art NLP machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索最先进的NLP机器学习
- en: Future NLP tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未来的NLP任务
- en: Semantic role labeling
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义角色标注
- en: Constituency parsing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成分句法分析
- en: Textual entailment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本蕴含
- en: Machine comprehension
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器理解
- en: Exploring state-of-the-art NLP machine learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索最先进的NLP机器学习
- en: While the techniques we have learned in this book so far are highly useful methodologies
    for training our own machine learning model from scratch, they are far from the
    most sophisticated models being developed globally. Companies and research groups
    are constantly striving to create the most advanced machine learning models that
    will achieve the highest performance on a number of NLP tasks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本书中学到的技术是从头开始训练我们自己的机器学习模型的高度有用的方法，但它们远非全球开发的最复杂模型。公司和研究团体不断努力创建在多个NLP任务上能够达到最高性能的最先进的机器学习模型。
- en: 'Currently, there are two NLP models that have the best performance and could
    be considered state-of-the-art: **BERT** and **GPT-2**. Both models are forms
    of **generalized language models**. We will discuss these in more detail in the
    upcoming sections.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有两个NLP模型具有最佳性能，并且可以被认为是最先进的：**BERT**和**GPT-2**。这两种模型都是通用语言模型的形式。我们将在接下来的章节中详细讨论它们。
- en: BERT
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT
- en: '**BERT**, which stands for **Bidirectional Encoder Representations from Transformers**,
    was developed by Google in 2018 and is widely considered to be the leading model
    in the field of NLP, having achieved leading performance in natural language inference
    and question-answering tasks. Fortunately, this has been released as an open source
    model, so this can be downloaded and used for NLP tasks of your own.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**，全称**双向编码器表示来自Transformers**，由Google于2018年开发，被广泛认为是NLP领域的领先模型，在自然语言推理和问答任务中取得了领先的性能。幸运的是，这已经作为开源模型发布，因此可以下载并用于您自己的NLP任务。'
- en: BERT was released as a pre-trained model, which means users can download and
    implement BERT without the need to retrain the model from scratch each time. The
    pre-trained model is trained on several corpuses, including the whole of Wikipedia
    (consisting of 2.5 billion words) and another corpus of books (which includes
    a further 800 million words). However, the main element of BERT that sets it apart
    from other similar models is the fact that it provides a deep, bidirectional,
    unsupervised language representation, which is shown to provide a more sophisticated,
    detailed representation, thus leading to improved performance in NLP tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: BERT发布为预训练模型，这意味着用户可以下载并实现BERT，而无需每次从头开始重新训练模型。预训练模型是在几个语料库上训练的，包括整个维基百科（包含25亿词）和另一个包含8亿词的书籍语料库。然而，使BERT与其他类似模型不同的主要因素是它提供了深度、双向、无监督的语言表示，据显示在自然语言处理任务中提供了更复杂、更详细的表示，从而导致性能提升。
- en: Embeddings
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入
- en: While traditional embedding layers (such as GLoVe) form a single representation
    of a word that is agnostic to the meaning of the word within the sentence, the
    bidirectional BERT model attempts to form representations based on its context.
    For example, in these two sentences, the word *bat*has two different meanings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统的嵌入层（如GLoVe）形成一个单一的词语表示，与句子中词语的含义无关，但双向BERT模型试图基于其上下文形成表示。例如，在这两个句子中，词语*bat*具有两种不同的含义。
- en: “The bat flew past my window”
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “蝙蝠从我的窗户飞过”
- en: “He hit the baseball with the bat”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: “他用球棒击打了棒球”
- en: 'Although the word *bat* is a noun in both sentences, we can discern that the
    context and meaning of the word is obviously very different, depending on the
    other words around it. Some words may also have different meanings, depending
    on whether they are a noun or verb within the sentence:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这两个句子中，*bat*是名词，但我们可以辨别出其上下文和含义显然是截然不同的，这取决于其周围的其他词语。有些词语可能在句子中也具有不同的含义，这取决于它们在句子中是名词还是动词：
- en: “She used to match to light the fire”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: “她用火柴点火”
- en: “His poor performance meant they had no choice but to fire him”
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: “他的糟糕表现意味着他们别无选择，只能解雇他”
- en: 'Using the bidirectional language model to form context-dependent representations
    of words is what truly makes BERT stand out as a state-of-the-art model. For any
    given token, we obtain its input representation by combining the token, position,
    and segment embeddings:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 利用双向语言模型形成上下文相关的词语表示，这才是使BERT成为一流模型的真正原因。对于任何给定的标记，我们通过结合标记、位置和段落嵌入来获得其输入表示：
- en: '![Figure 9.1 – BERT architecture'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1 – BERT 架构'
- en: '](img/B12365_09_1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_1.jpg)'
- en: Figure 9.1 – BERT architecture
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – BERT 架构
- en: However, it is important to understand how the model arrives at these initial
    context-dependent token-embeddings.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，理解模型如何得出这些初始的上下文相关的标记嵌入是很重要的。
- en: Masked language modeling
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 掩码语言建模
- en: 'In order to create this bidirectional language representation, BERT uses two
    different techniques, the first of which is masked language modeling. This methodology
    effectively hides 15% of the words within the input sentences by replacing them
    with a masking token. The model then tries to predict the true values of the masked
    words, based on the context of the other words in the sentence. This prediction
    is made bidirectionally in order to capture the context of the sentence in both
    directions:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这种双向语言表示，BERT使用了两种不同的技术，第一种是掩码语言建模。这种方法通过用掩码令牌替换输入句子中的15%单词来有效隐藏单词。然后模型试图基于句子中其他词语的上下文来预测掩码词语的真实值。这种预测是双向进行的，以捕获句子的双向上下文：
- en: '**Input**:*We [MASK_1] hide some of the [MASK_2] in the sentence*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：*我们[MASK_1]在句子中[MASK_2]隐藏一些*'
- en: '**Labels**:*MASK_1 = randomly, MASK_2 = words*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签**：*MASK_1 = 随机地，MASK_2 = 词语*'
- en: If our model can learn to predict the correct context-dependent words, then
    we are one step closer to context-dependent representation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型能够学习预测正确的上下文相关词语，那么我们距离上下文相关表示更近了一步。
- en: Next sentence prediction
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下一个句子预测
- en: 'The other technique that BERT uses to learn the language representation is
    next sentence prediction. In this methodology, our model receives two sentences
    and our model learns to predict whether the second sentence is the sentence that
    follows the first sentence; for example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: BERT用于学习语言表示的另一种技术是下一个句子预测。在这种方法中，我们的模型接收两个句子，并学习预测第二个句子是否是紧随第一个句子的句子；例如：
- en: '**Sentence A**: *"I like to drink coffee"*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子 A**:*"我喜欢喝咖啡"*'
- en: '**Sentence B**:*"It is my favorite drink"*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子 B**:*"这是我最喜欢的饮料"*'
- en: '**Is Next Sentence?**:*True*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**是否下一个句子？**:*True*'
- en: '**Sentence A**:*"I like to drink coffee"*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**句子 A**:*"我喜欢喝咖啡"*'
- en: '**Sentence B**:*"The sky is blue"*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sentence B**:*"天空是蓝色的"*'
- en: '**Is Next Sentence?**:*False*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**是否下一个句子？**:*False*'
- en: By passing our model pairs of sentences like this, it can learn to determine
    whether any two sentences are related and follow one another, or whether they
    are just two random, unrelated sentences. Learning these sentence relationships
    is useful in a language model as many NLP-related tasks, such as question-answering,
    require the model to understand the relationship between two sentences. Training
    a model on next sentence prediction allows the model to identify some kind of
    relationship between a pair of sentences, even if that relationship is very basic.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递我们的模型句子对像这样，它可以学习确定任何两个句子是否相关并跟随彼此，或者它们只是两个随机无关的句子。学习这些句子关系在语言模型中是有用的，因为许多自然语言处理相关的任务，如问答，需要模型理解两个句子之间的关系。训练一个模型进行下一个句子预测允许模型识别一对句子之间的某种关系，即使这种关系非常基础。
- en: BERT is trained using both methodologies (masked language modeling and next
    sentence prediction), and the combined loss function of both techniques is minimized.
    By using two different training methods, our language representation is sufficiently
    robust and learns how sentences are formed and structured, as well as how different
    sentences relate to one another.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: BERT使用掩码语言建模和下一个句子预测两种方法进行训练，并最小化两种技术的组合损失函数。通过使用两种不同的训练方法，我们的语言表示足够强大，可以学习句子如何形成和结构化，以及不同句子如何相互关联。
- en: BERT–Architecture
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT–架构
- en: 'The model architecture builds upon many of the principles we have seen in the
    previous chapters to provide a sophisticated language representation using bidirectional
    encoding. There are two different variants of BERT, each consisting of a different
    number of layers and attention heads:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型架构建立在我们在前几章中看到的许多原理之上，使用双向编码提供复杂的语言表示。BERT有两种不同的变体，每种变体由不同数量的层和注意头组成：
- en: '**BERT Base**: 12 transformer blocks (layers), 12 attention heads, ~110 million
    parameters'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT Base**：12 个Transformer块（层），12 个注意头，约 1.1 亿个参数'
- en: '**BERT Large**: 24 transformer blocks (layers), 16 attention heads, ~340 million
    parameters'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT Large**：24 个Transformer块（层），16 个注意头，约 3.4 亿个参数'
- en: While BERT Large is just a deeper version of BERT Base with more parameters,
    we will focus on the architecture of BERT Base.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BERT Large只是BERT Base的深层版本，参数更多，我们将专注于BERT Base的架构。
- en: BERT is built by following the principle of a **transformer**, which will now
    be explained in more detail.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是按照**transformer**的原理构建的，现在将更详细地解释。
- en: Transformers
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformers
- en: 'The model architecture builds upon many of the principles we have seen so far
    in this book. By now, you should be familiar with the concept of encoders and
    decoders, where our model learns an encoder to form a representation of an input
    sentence, and then learns a decoder to decode this representation into a final
    output, whether this be a classification or translation task:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型架构建立在我们在本书中看到的许多原理之上。到目前为止，您应该熟悉编码器和解码器的概念，其中我们的模型学习一个编码器来形成输入句子的表示，然后学习一个解码器来将这个表示解码为最终输出，无论是分类还是翻译任务：
- en: '![Figure 9.2 – Transformer workflow'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – Transformer 工作流程'
- en: '](img/B12365_09_2.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_2.jpg)'
- en: Figure 9.2 – Transformer workflow
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – Transformer 工作流程
- en: 'However, our transformer adds another element of sophistication to this approach,
    where a transformer actually has a stack of encoders and a stack of decoders,
    with each decoder receiving the output of the final encoder as its input:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的Transformer在这种方法中增加了另一个复杂性元素，其中Transformer实际上有一堆编码器和一堆解码器，每个解码器将最终编码器的输出作为其输入：
- en: '![Figure 9.3 – Transformer workflow for multiple encoders'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – 多编码器的Transformer工作流程'
- en: '](img/B12365_09_3.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_3.jpg)'
- en: Figure 9.3 – Transformer workflow for multiple encoders
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 多编码器的Transformer工作流程
- en: 'Within each encoder layer, we find two constituent parts: a self-attention
    layer and a feed-forward layer. The self-attention layer is the layer that receives
    the model''s input first. This layer causes the encoder to examine other words
    within the input sentence as it encodes any received word, making the encoding
    context aware. The output from the self-attention layer is passed forward to a
    feed-forward layer, which is applied independently to each position. This can
    be illustrated diagrammatically like so:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个编码器层内部，我们找到两个组成部分：一个自注意力层和一个前馈层。自注意力层是首先接收模型输入的层。这一层使得编码器在编码任何接收到的单词时能够检查输入句子中的其他单词，从而使得编码上下文感知。自注意力层的输出传递到前馈层，该层独立应用于每个位置。这可以通过如下图示来说明：
- en: '![Figure 9.4 – Feedforward layer'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – 前馈层'
- en: '](img/B12365_09_4.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B12365_09_4.jpg)'
- en: Figure 9.4 – Feedforward layer
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 前馈层
- en: 'Our decoder layers are almost identical in structure to our encoders, but they
    incorporate an additional attention layer. This attention layer helps the decoder
    focus on the relevant part of the encoded representation, similar to how we saw
    attention working within our sequence-to-sequence models:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解码器层在结构上几乎与我们的编码器层相同，但它们包含一个额外的注意力层。这个注意力层帮助解码器专注于编码表示的相关部分，类似于我们在序列到序列模型中看到的注意力工作方式：
- en: '![Figure 9.5 – Attention methodology'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 注意力方法论'
- en: '](img/B12365_09_5.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B12365_09_5.jpg)'
- en: Figure 9.5 – Attention methodology
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 注意力方法论
- en: 'We know that our decoders take input from our final encoder, so one linked
    encoder/decoder might look something like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的解码器从最终编码器接收输入，因此一个链接的编码器/解码器可能看起来像这样：
- en: '![Figure 9.6 – Linked encoder/decoder array'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6 – 链接的编码器/解码器数组'
- en: '](img/B12365_09_6.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B12365_09_6.jpg)'
- en: Figure 9.6 – Linked encoder/decoder array
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 链接的编码器/解码器数组
- en: This should provide you with a useful overview of how the different encoders
    and decoders are stacked up within the larger model. Next, we will examine the
    individual parts in more detail.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该为您提供了关于如何将不同的编码器和解码器堆叠在更大模型中的有用概述。接下来，我们将更详细地研究各个部分。
- en: Encoders
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The unique property of transformers is that words flow through the encoder
    layers individually and each word in each position has its own path. While there
    are some dependencies within the self-attention layer, these don''t exist within
    the feed-forward layer. The vectors for the individual words are obtained from
    an embedding layer and then fed through a self-attention layer before being fed
    through a feed-forward network:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 的独特特性在于单词通过编码器层时是单独处理的，每个位置的每个单词都有自己的路径。虽然自注意力层内部存在一些依赖关系，但在前馈层内部不存在这种依赖。单词的向量是从嵌入层获取的，然后通过自注意力层再通过前馈网络传递：
- en: '![Figure 9.7 – Encoder layout'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 编码器布局'
- en: '](img/B12365_09_7.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B12365_09_7.jpg)'
- en: Figure 9.7 – Encoder layout
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 编码器布局
- en: 'Self-attention is arguably the most complex component of the encoder, so we
    will examine this in more detail first. Let''s say we have a three-word input
    sentence; for example, *"This is fine"*. For each word within this sentence, we
    represent them as a single word vector that was obtained from the embedding layer
    of our model. We then extract three vectors from this single word vector: a query
    vector, a key vector, and a value vector. These three vectors are obtained by
    multiplying our word vector by three different weight matrices that are obtained
    while training the model.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力无疑是编码器中最复杂的组成部分，因此我们将首先更详细地研究它。假设我们有一个由三个单词组成的输入句子；例如，“This is fine”。对于该句子中的每个单词，我们将其表示为来自模型嵌入层的单词向量。然后，我们从这个单词向量中提取三个向量：一个查询向量、一个键向量和一个值向量。这三个向量是通过将我们的单词向量与训练模型时获得的三个不同权重矩阵相乘而获得的。
- en: 'If we call our word embeddings for each word in our input sentence, *Ethis*,
    *Eis*, and *Efine*, we can calculate our query, key, and value vectors like so:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们称我们输入句子中每个单词的词嵌入为 *Ethis*、*Eis* 和 *Efine*，我们可以这样计算我们的查询、键和值向量：
- en: '**Query vectors**:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询向量**：'
- en: '![](img/Formula_09_001.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_001.png)'
- en: '![](img/Formula_09_002.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_002.png)'
- en: '![](img/Formula_09_003.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_003.png)'
- en: '![](img/Formula_09_004.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_004.png)'
- en: '**Key vectors**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**键向量**：'
- en: '![](img/Formula_09_005.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_005.png)'
- en: '![](img/Formula_09_006.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_006.png)'
- en: '![](img/Formula_09_007.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_007.png)'
- en: '![](img/Formula_09_008.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_008.png)'
- en: '**Value vectors**:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**值向量**：'
- en: '![](img/Formula_09_009.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_009.png)'
- en: '![](img/Formula_09_010.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_010.png)'
- en: '![](img/Formula_09_011.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_011.png)'
- en: '![](img/Formula_09_012.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_012.png)'
- en: Now that we know how to calculate each of these vectors, it is important to
    understand what each of them represents. Essentially, each of these is an abstraction
    of a concept within the attention mechanism. This will become apparent once we
    see how they are calculated.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何计算这些向量后，理解它们各自代表的含义就变得很重要。实际上，每一个都是注意力机制中一个概念的抽象。一旦我们看到它们如何计算，这一点就会变得明显。
- en: 'Let''s continue with our working example. We need to consider each word within
    our input sentence in turn. To do this, we calculate a score for each pair of
    query/key vectors in our sentence. This is done by obtaining the dot product of
    each query/key vector pair for each word within our input sentence. For example,
    to calculate the scores for the first word in the sentence, "this", we calculate
    the dot product between the query vector for "this" and the key vector in position
    0\. We repeat this for the key vectors in all other positions within the input
    sentence, so we obtain *n* scores for the first word in our input sentence, where
    *n* is the length of the sentence:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的工作示例。我们需要依次考虑输入句子中的每个单词。为了做到这一点，我们计算每个查询/键向量对在我们句子中的得分。这是通过获得每个输入句子中每个单词的查询向量和位置0处键向量的点积来完成的。我们重复这个过程，对输入句子中所有其他位置的键向量，因此我们得到了第一个单词"this"的*n*个得分，其中*n*是句子的长度：
- en: '**Scores ("this")**:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**分数（"this"）**：'
- en: '![](img/Formula_09_013.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_013.png)'
- en: '![](img/Formula_09_014.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_014.png)'
- en: '![](img/Formula_09_015.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_015.png)'
- en: 'Next, we apply a softmax function to each of these scores so that the value
    of each is now between 0 and 1 (as this helps prevent exploding gradients and
    makes gradient descent more efficient and easily calculable). We then multiply
    each of these scores by the value vectors and sum these all up to obtain a final
    vector, which is then passed forward within the encoder:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对每个得分应用softmax函数，使得每个得分的值现在在0到1之间（这有助于防止梯度爆炸，并使梯度下降更有效和容易计算）。然后，我们将每个得分乘以值向量并将它们全部求和以获得一个最终向量，然后将其向前传递到编码器中：
- en: '**Final vector ("this")**:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终向量（"this"）**：'
- en: '![](img/Formula_09_016.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_016.png)'
- en: '![](img/Formula_09_017.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_017.png)'
- en: '![](img/Formula_09_018.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_018.png)'
- en: '![](img/Formula_09_019.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_019.png)'
- en: We then repeat this procedure for all the words within the input sentence so
    that we obtain a final vector for each word, incorporating an element of self-attention,
    which is then passed along the encoder to the feed-forward network. This self-attention
    process means that our encoder knows where to look within the input sentence to
    obtain the information it needs for the task.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对输入句子中的所有单词重复这个过程，这样我们就为每个单词获得了一个最终向量，其中包含自注意力的元素，然后将其传递到前馈网络中的编码器。这个自注意力过程意味着我们的编码器知道在输入句子中查找需要的信息的位置。
- en: In this example, we only learned a single matrix of weights for our query, key,
    and value vectors. However, we can actually learn multiple different matrices
    for each of these elements and apply these simultaneously across our input sentence
    to obtain our final outputs. This is what's known as **multi-headed attention**
    and allows us to perform more complex attention calculations, relying on multiple
    different learned patterns rather than just a single attention mechanism.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们仅学习了一个权重矩阵来处理我们查询、键和值向量。然而，我们实际上可以为每个元素学习多个不同的矩阵，并同时应用这些矩阵到我们的输入句子，以获得最终的输出。这就是**多头注意力**，它允许我们执行更复杂的注意力计算，依赖于多个不同的学习模式，而不仅仅是单一的注意力机制。
- en: We know that BERT incorporates 12 attention heads, meaning that 12 different
    weight matrices are learned for *Wq*, *Wk*, and *Wv*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道BERT包含12个注意力头，意味着为*Wq*、*Wk*和*Wv*学习了12个不同的权重矩阵。
- en: Finally, we need a way for our encoders to account for the order of words in
    the input sequence. Currently, our model treats each word in our input sequence
    independently, but in reality, the order of the words in the input sequence will
    make a huge difference to the overall meaning of the sentence. To account for
    this, we use **positional encoding**.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一种方法让我们的编码器考虑输入序列中单词的顺序。目前，我们的模型将输入序列中的每个单词视为独立的，但实际上，单词在输入序列中的顺序会对句子的整体意义产生重大影响。为了解决这个问题，我们使用**位置编码**。
- en: 'To apply this, our model takes each input embedding and adds a positional encoding
    vector to each one individually. These positional vectors are learned by our model,
    following a specific pattern to help them determine the position of each word
    in the sequence. In theory, adding these positional vectors to our initial embeddings
    should translate into meaningful distances between our final vectors, once they
    are projected into the individual query, key, and value vectors:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用这一方法，我们的模型获取每个输入嵌入并为每个嵌入单独添加一个位置编码向量。这些位置向量由我们的模型学习，遵循特定的模式以帮助确定序列中每个单词的位置。理论上，将这些位置向量添加到我们的初始嵌入中应该会转化为最终向量中的有意义的距离，一旦它们被投影到单独的查询、键和值向量中：
- en: '*x0 = Raw Embedding*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*x0 = 原始嵌入*'
- en: '*t0 = Positional Encoding*'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*t0 = 位置编码*'
- en: '*E0 = Embedding with Time Signal*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*E0 = 带时间信号的嵌入*'
- en: '*x0 + t0 = E0*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*x0 + t0 = E0*'
- en: 'Our model learns different positional encoding vectors for each position (*t*0,
    *t*1, and so on), which we then apply to each word in our input sentence before
    these even enter our encoder:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型为每个位置学习了不同的位置编码向量（*t*0、*t*1等），然后在这些单词进入编码器之前应用到输入句子中的每个单词上：
- en: '![Figure 9.8 – Adding input to the encoder'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.8 – 添加输入到编码器'
- en: '](img/B12365_09_8.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_8.jpg)'
- en: Figure 9.8 – Adding input to the encoder
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 添加输入到编码器
- en: Now that we have covered the main components of the encoder, it's time to look
    at the other side of the model and see how the decoder is constructed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了编码器的主要组件，是时候看看模型的另一面，了解解码器的构建方式了。
- en: Decoders
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: The components in decoders are much the same of those in encoders. However,
    rather than receiving the raw input sentence like encoders do, the decoders in
    our transformer receive their inputs from the outputs of our encoders.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器中的组件与编码器中的组件大致相同。然而，与编码器接收原始输入句子不同，我们的转换器中的解码器从编码器的输出中获取输入。
- en: 'Our stacked encoders process our input sentence and we are left with a set
    of attention vectors, *K* and *V*, which are used within the encoder-decoder attention
    layer of our decoder. This allows it to focus only on the relevant parts of the
    input sequence:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的堆叠编码器处理我们的输入句子，最终留下一组注意力向量，*K* 和 *V*，这些向量在我们的解码器的编码器-解码器注意力层中使用。这使得解码器能够仅关注输入序列的相关部分：
- en: '![Figure 9.9 – Stacked decoders'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.9 – 堆叠解码器'
- en: '](img/B12365_09_9.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_9.jpg)'
- en: Figure 9.9 – Stacked decoders
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 堆叠解码器
- en: 'At each time step, our decoders use a combination of the previously generated
    words in the sentence and the *K,V* attention vectors to generate the next word
    in the sentence. This process is repeated iteratively until the decoder generates
    an <END> token, indicating that it has completed generating the final output.
    One given time step on the transformer decoder may look like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步骤中，我们的解码器使用前面生成的单词及 *K,V* 注意力向量的组合来生成句子中的下一个单词。这个过程迭代重复，直到解码器生成一个 <END>
    标记，表示它已经完成生成最终输出。在转换器解码器的一个时间步骤中可能如下所示：
- en: '![Figure 9.10 – Transformer decoder'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.10 – 转换器解码器'
- en: '](img/B12365_09_10.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_10.jpg)'
- en: Figure 9.10 – Transformer decoder
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 转换器解码器
- en: It is worth noting here that the self-attention layers within the decoders operate
    in a slightly different way to those found in our encoders. Within our decoder,
    the self-attention layer only focuses on earlier positions within the output sequence.
    This is done by masking any future positions of the sequence by setting them to
    minus infinity. This means that when the classification happens, the softmax calculation
    always results in a prediction of 0.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，解码器中的自注意层与我们的编码器中找到的自注意层的工作方式略有不同。在解码器内部，自注意层仅关注输出序列中较早的位置。通过将序列中的任何未来位置屏蔽为负无穷大来实现这一点。这意味着当分类发生时，softmax
    计算始终导致预测值为0。
- en: The encoder-decoder attention layer works in the same way as the multi-headed
    self-attention layer within our encoder. However, the main difference is that
    it creates a query matrix from the layer below and takes the key and values matrix
    from the output of the encoders.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器注意力层的工作方式与我们的编码器内的多头自注意力层相同。然而，主要区别在于它从下面的层创建一个查询矩阵，并从编码器的输出中获取键和值矩阵。
- en: These encoder and decoder parts comprise our transformer, which forms the basis
    for BERT. Next, we will look at some of the applications of BERT and examine a
    few variations that have shown increased performance at specific tasks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些编码器和解码器部分构成了我们的 transformer，这也是 BERT 的基础。接下来，我们将看一些 BERT 的应用以及几种在特定任务上表现出色的变体。
- en: Applications of BERT
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT 的应用
- en: Being state-of-the-art, BERT obviously has a number of practical applications.
    Currently, it is being used in a number of Google products that you probably use
    on a daily basis; namely, suggested replies and smart compose in Gmail (where
    Gmail predicts your expected sentence based on what you are currently typing)
    and autocomplete within the Google search engine (where you type the first few
    characters you wish to search for and the drop-down list will predict what you
    are going to search for).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最先进的技术，BERT 当然有许多实际应用。目前，它正在被应用于许多您可能每天都在使用的 Google 产品中，比如 Gmail 中的建议回复和智能撰写（Gmail
    根据您当前输入的内容预测您预期的句子），以及 Google 搜索引擎中的自动完成（您输入想要搜索的前几个字符，下拉列表将预测您要搜索的内容）。
- en: As we saw in the previous chapter, chatbots are one of the most impressive things
    NLP deep learning can be used for, and the use of BERT has led to some very impressive
    chatbots indeed. In fact, question-answering is one of the main things that BERT
    excels at, largely due to the fact that it is trained on a large knowledge base
    (Wikipedia) and is able to answer questions in a syntactically correct way (due
    to being trained with next sentence prediction in mind).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章看到的，聊天机器人是自然语言处理深度学习中最令人印象深刻的应用之一，而 BERT 的应用确实带来了一些非常出色的聊天机器人。事实上，问答是
    BERT 擅长的主要任务之一，这主要是因为它是在大量的知识库（如维基百科）上训练的，能够以语法正确的方式回答问题（因为训练时考虑了下一个句子预测）。
- en: We are still not at the stage where chatbots are indistinguishable from conversations
    with real humans, and the ability of BERT to draw from its knowledge base is extremely
    limited. However, some of the results achieved by BERT are promising and, taking
    into account how quickly the field of NLP machine learning is progressing, this
    suggests that this may become a reality very soon.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有达到与真人对话无法区分的聊天机器人的阶段，而 BERT 从其知识库中获取信息的能力非常有限。但是，BERT 取得的一些成果是令人鼓舞的，并且考虑到自然语言处理机器学习领域的快速进展，这表明这一可能性可能很快就会成为现实。
- en: 'Currently, BERT is only able to address a very narrow type of NLP task due
    to the way it is trained. However, there are many variations of BERT that have
    been changed in subtle ways to achieve increased performance at specific tasks.
    These include, but are not limited to, the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，由于训练方式的限制，BERT 只能处理非常特定类型的自然语言处理任务。但是，有许多变体的 BERT 经过微调，以在特定任务上表现出更好的性能。这些变体包括但不限于以下几种：
- en: '**roBERTa**: A variation of BERT, built by Facebook. Removes the next sentence
    prediction element of BERT, but enhances the word masking strategy by implementing
    dynamic masking.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**roBERTa**：这是 Facebook 开发的一种 BERT 变体。去掉了 BERT 的下一个句子预测元素，但通过实施动态掩码来增强了单词掩码策略。'
- en: '**xlm**/**BERT**: Also built by Facebook, this model applies a dual-language
    training mechanism to BERT that allows it to learn relationships between words
    in different languages. This allows BERT to be used effectively for machine translation
    tasks, showing improved performance over basic sequence-to-sequence models.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**xlm**/**BERT**：这个模型也是由 Facebook 开发的，应用了一种双语训练机制，使得 BERT 能够学习不同语言中单词之间的关系。这使得
    BERT 在机器翻译任务中表现出色，比基本的序列到序列模型有了显著的提升。'
- en: '**distilBERT**: A more compact version of BERT, retaining 95% of the original
    but halving the number of learned parameters, reducing the model’s total size
    and training time.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**distilBERT**：这是一个比 BERT 更紧凑的版本，保留了原始模型的 95% 的性能，但是减少了学习参数的数量，从而缩小了模型的总体大小和训练时间。'
- en: '**ALBERT**: This Google trained model uses its own unique training method called
    sentence order prediction. This variation of BERT has been shown to outperform
    the standard BERT across a number of tasks and is now considered state-of-the-art
    ahead of BERT (illustrating just how quickly things can change!).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ALBERT**：这是 Google 训练的模型，采用了自己独特的训练方法——句子顺序预测。这种 BERT 变体在多个任务上表现优异，目前被认为是比标准
    BERT 更先进的技术（这也展示了技术进步的速度是多么快！）。'
- en: While BERT is perhaps the most well known, there are also other transformer-based
    models that are considered state-of-the-art. The major one that is often considered
    a rival to BERT is GPT-2.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BERT也许是最为人知的，但还有其他一些被认为是最先进的基于Transformer的模型。其中一个主要的被视为BERT的竞争对手的模型是GPT-2。
- en: GPT-2
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: GPT-2, while similar to BERT, differs in some subtle ways. While both models
    are based upon the transformer architecture previously outlined, BERT uses a form
    of attention known as self-attention, while GPT-2 uses masked self-attention.
    Another subtle difference between the two is that GPT-2 is constructed in such
    a way that it can to output one token at a time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GPT-2与BERT相似，但在某些细微的方式上有所不同。虽然两种模型都基于先前概述的Transformer架构，BERT使用了一种称为自注意力的注意力形式，而GPT-2使用了掩码自注意力。两者之间的另一个细微差异是，GPT-2被构造成每次只能输出一个标记。
- en: 'This is because GPT-2 is essentially auto-regressive in the way it works. This
    means that when it generates an output (the first word in a sentence), this output
    is added recursively to the input. This input is then used to predict the next
    word in the sentence and is repeated until a complete sentence has been generated.
    You can see this in the following example:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为GPT-2在工作方式上本质上是自回归的。这意味着当它生成一个输出（句子中的第一个单词）时，此输出递归地添加到输入中。然后使用此输入预测句子中的下一个单词，并重复此过程，直到生成完整的句子为止。您可以在以下示例中看到这一点：
- en: '**Step 1:**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**第1步：**'
- en: '**Input**:*What color is the sky?*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：*天空是什么颜色？*'
- en: '**Output**:*The ...*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：*...*'
- en: 'We then add the predicted output to the end of our input and repeat this step:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将预测的输出添加到输入的末尾并重复此步骤：
- en: '**Step 2:**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**第2步：**'
- en: '**Input**:*What color is the sky? The*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：*天空是什么颜色？*'
- en: '**Output**: *sky*'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：*天空*'
- en: 'We repeat this process until we have generated the entire sentence:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复此过程，直到生成整个句子：
- en: '**Step 3:**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**第3步：**'
- en: '**Input**: *What color is the sky? The sky*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：*天空是什么颜色？天空*'
- en: '**Output**: *is*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：*是*'
- en: '**Step 4:**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**第4步：**'
- en: '**Input**:*What color is the sky? The sky is*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：*天空是什么颜色？天空是*'
- en: '**Output**:*blue*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：*蓝色*'
- en: This is one of the key trade-offs in terms of performance between BERT and GPT-2\.
    The fact that BERT is trained bidirectionally means this single-token generation
    is not possible; however, GPT-2 is not bidirectional, so it only considers previous
    words in the sentence when making predictions, which is why BERT outperforms GPT-2
    when predicting missing words within a sentence.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，BERT和GPT-2之间的一个关键权衡之一。BERT是双向训练的，这意味着不可能进行单词级别的生成；然而，GPT-2不是双向的，因此在预测时只考虑句子中的前面单词，这就是为什么在预测句子中缺失单词时，BERT优于GPT-2的原因。
- en: Comparing self-attention and masked self-attention
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较自注意力和掩码自注意力
- en: 'This difference is also apparent in the way the two different models implement
    attention. Since BERT is bidirectional, its attention mechanism is able to consider
    the context of the whole input sentence and determine where exactly in the input
    sentence to look:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异也体现在两种不同模型实现注意力的方式上。由于BERT是双向的，其注意力机制能够考虑整个输入句子的上下文，并确定在输入句子中确切的位置进行查找：
- en: '![Figure 9.11 – Self-Attention mechanism'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.11 – 自注意力机制'
- en: '](img/B12365_09_11.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_11.jpg)'
- en: Figure 9.11 – Self-Attention mechanism
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 自注意力机制
- en: 'On the other hand, GPT-2 is not bidirectional, so the masked self-attention
    mechanism is only able to look at tokens it has already seen and is not able to
    "look ahead":'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，GPT-2不是双向的，因此掩码自注意力机制只能查看它已经看过的标记，并且不能“向前看”：
- en: '![Figure 9.12 – Masked self-attention mechanism'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.12 – 掩码自注意力机制'
- en: '](img/B12365_09_12.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_12.jpg)'
- en: Figure 9.12 – Masked self-attention mechanism
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 掩码自注意力机制
- en: GPT-2 – Ethics
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2 – 伦理
- en: One very interesting consideration of GPT-2 was the author's consideration of
    ethics when considering whether to release the model to the public or not.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑是否将模型发布给公众时，GPT-2的一个非常有趣的考虑是作者在伦理方面的考虑。
- en: 'The language generation capabilities of GPT-2 mean that it is capable of generating
    very realistic and syntactically correct sentences using very few prompts. By
    providing it with just a couple of lines of input to identify a subject matter,
    the model is able to generate paragraphs of detailed information about this subject,
    regardless of whether it is actually true or not! One, now infamous, example was
    the "Unicorn" story, where the model received one sentence input:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的语言生成能力意味着它能够使用非常少的提示生成非常逼真和语法正确的句子。通过提供仅仅几行输入以确定一个主题，模型能够生成关于这个主题的详细信息段落，无论这些信息是否真实！一个臭名昭著的例子是“独角兽”故事，其中模型接收到一句输入：
- en: '**Input**: *"In a shocking finding, scientists discovered a herd of unicorns
    living in a remote, previously unexplored valley, in the Andes Mountains. Even
    more surprising to the researchers was the fact that the unicorns spoke perfect
    English."*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**: *"在一个令人震惊的发现中，科学家们发现了一群生活在安第斯山脉一个偏远、以前未曾探索过的山谷中的独角兽。更让研究人员感到惊讶的是，这些独角兽讲起了完美的英语。"*'
- en: 'The model gave the following as output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出如下：
- en: '**Output**: *The scientist named the population, after their distinctive horn,
    Ovid''s Unicorn. These four-horned, silver-white unicorns were previously unknown
    to science.*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**: *科学家们将这种群体命名为“奥维德的独角兽”，因为它们独特的角。这些四角银白色的独角兽以前对科学界来说是未知的。*'
- en: '*Now, after almost two centuries, the mystery of what sparked this odd phenomenon
    is finally solved.*'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在，近两个世纪之后，这个引发了奇怪现象的谜团终于得以解决。*'
- en: '*Dr. Jorge Pérez, an evolutionary biologist from the University of La Paz,
    and several companions, were exploring the Andes Mountains when they found a small
    valley, with no other animals or humans. Pérez noticed that the valley had what
    appeared to be a natural fountain, surrounded by two peaks of rock and silver
    snow.*'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*Jorge Pérez 博士，来自拉巴斯大学的进化生物学家，和几个同伴，当他们在安第斯山脉探索时，发现了一个没有其他动物或人类的小山谷。Pérez
    注意到山谷中有一个看起来像是自然泉水的地方，周围有两座岩石和银雪覆盖的山峰。*'
- en: This is only the first three paragraphs of a much larger nine-paragraph response
    that the model generated from very little input. What's notable is that the sentences
    all make perfect sense (regardless of the impossible subject matter!), that the
    paragraphs flow together in a logical order, and that the model was able to generate
    all of this from a very small input.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个模型从很少的输入生成的更大的九段回复的第一三段。值得注意的是，这些句子都很有逻辑（无论主题有多不可能！），段落之间的衔接流畅，模型能够从非常少的输入中生成所有这些内容。
- en: While this is extremely impressive in terms of performance and what it's possible
    to achieve from building deep NLP models, it does raise some concerns about the
    ethics of such models and how they can be used (and abused!).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这在性能上非常令人印象深刻，展示了构建深度自然语言处理模型可以实现的可能性，但这也引发了对这些模型伦理问题及其如何使用（和滥用！）的一些担忧。
- en: With the rise of "fake news" and the spread of misinformation using the internet,
    examples like this illustrate how simple it is to generate realistic text using
    these models. Let's consider an example where an agent wishes to generate fake
    news on a number of subjects online. Now, they don't even need to write the fake
    information themselves. In theory, they could train NLP models to do this for
    them, before disseminating this fake information on the internet. The authors
    of GPT-2 paid particular attention to this when training and releasing the model
    to the public, noting that the model had the potential to be abused and misused,
    therefore only releasing the larger more sophisticated models to the public once
    they saw no evidence of misuse of the smaller models.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随着“假新闻”的兴起和利用互联网传播误信息的情况，这些例子说明使用这些模型生成逼真文本是多么简单。让我们考虑一个例子，代理人希望在网络上生成关于多个主题的假新闻。现在，他们甚至不需要自己编写虚假信息。理论上，他们可以训练自然语言处理模型来为他们做这些事情，然后在互联网上传播这些虚假信息。GPT-2的作者在训练和发布模型时特别关注了这一点，指出该模型有被滥用和误用的潜力，因此一旦他们没有看到较小模型被滥用的证据，才向公众发布更大更复杂的模型。
- en: This may become a key focus of NLP deep learning moving forward. As we approach
    chatbots and text generators such as GPT-2 that can approach human levels of sophistication,
    the uses and misuses of these models need to be fully understood. Studies have
    shown that GPT-2 generated text was deemed to be almost as credible (72%) as real
    human-written articles from the New York Times (83%). As we continue to develop
    even more sophisticated deep NLP models in the future, these numbers are likely
    to converge as model-generated text becomes more and more realistic moving forward.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP深度学习前景中，这可能成为关键关注点。随着我们接近像GPT-2这样可以接近人类复杂水平的聊天机器人和文本生成器，需要全面理解这些模型的使用和误用。研究表明，GPT-2生成的文本被认为几乎与纽约时报（83%）的真实人类撰写文章一样可信（72%）。随着我们未来继续开发更复杂的深度NLP模型，这些数字可能会趋于一致，因为模型生成的文本变得越来越逼真。
- en: Furthermore, the authors of GPT-2 also demonstrated that the model can be fine-tuned
    for misuse. By fine-tuning GPT-2 on ideologically extreme positions and generating
    text, it was shown that propaganda text can be generated, which supports these
    ideologies. While it was also shown that counter-models could be trained to detect
    these model-generated texts, we may again face further problems here in the future
    as these models become even more sophisticated.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GPT-2的作者还表明，该模型可以进行误用的精细调整。通过在意识形态极端立场上对GPT-2进行微调并生成文本，表明可以生成支持这些意识形态的宣传文本。尽管还表明可以训练反模型来检测这些模型生成的文本，但在未来，这些模型变得更加复杂时，我们可能会再次面临更多问题。
- en: These ethical considerations are worth keeping in mind as NLP models become
    even more complex and performant over time. While the models you train for your
    own purposes may not have been intended for any misuse, there is always the possibility
    that they could be used for purposes that were unintended. Always consider the
    potential applications of any model that you use.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP模型随着时间的推移变得越来越复杂和高效的同时，这些伦理考虑也值得牢记。虽然您为自己的目的训练的模型可能没有被用于任何误用，但总有可能被用于意外的目的。始终考虑您使用的任何模型的潜在应用。
- en: Future NLP tasks
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的NLP任务
- en: 'While the majority of this book has been focused on text classification and
    sequence generation, there are a number of other NLP tasks that we haven''t really
    touched on. While many of these are more interesting from an academic perspective
    rather than a practical perspective, it''s important to understand these tasks
    as they form the basis of how language is constructed and formed. Anything we,
    as NLP data scientists, can do to better understand the formation of natural language
    will only improve our understanding of the subject matter. In this section, we
    will discuss, in more detail, four key areas of future development in NLP:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本书的大部分内容都集中在文本分类和序列生成上，但还有许多其他NLP任务我们没有真正涉及。虽然其中许多任务更多是从学术角度而不是实际角度来看更加有趣，但理解这些任务很重要，因为它们构成了语言的形成和构建的基础。作为NLP数据科学家，我们能够更好地理解自然语言的形成，这只会改善我们对主题的理解。在本节中，我们将更详细地讨论NLP未来发展的四个关键领域：
- en: Constituency parsing
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成分分析
- en: Semantic role labeling
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义角色标注
- en: Textual entailment
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本蕴涵
- en: Machine comprehension
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器理解
- en: Constituency parsing
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成分分析
- en: Constituency parsing (also known as syntactic parsing) is the act of identifying
    parts of a sentence and assigning a syntactic structure to it. This syntactic
    structure is largely determined by the use of context-free grammars, meaning that
    using syntactic parsing, we can identify the underlying grammatical structure
    of a given sentence and map it out. Any sentence can be broken down into a "parse
    tree," which is a graphical representation of this underlying sentence structure,
    while syntactic parsing is the methodology by which this underlying structure
    is detected and determines how this tree is built.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 成分分析（也称为句法分析）是识别句子部分并为其分配句法结构的行为。这种句法结构主要由无上下文语法确定，这意味着使用句法分析，我们可以识别给定句子的基本语法结构并将其映射出来。任何句子都可以被分解成“解析树”，这是这种基础句子结构的图形表示，而句法分析是检测这种基础结构并确定如何构建此树的方法。
- en: We will begin by discussing this underlying grammatical structure. The idea
    of a "constituency" within a sentence is somewhat of an abstraction, but the basic
    assumption is that a sentence consists of multiple "groups" of words, each one
    of which is a constituency. Grammar, in its basic form, can be said to be an index
    of all possible types of constituencies that can occur within a sentence.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论这种基本的语法结构开始。句子中的“成分”概念在某种程度上是一种抽象，但基本假设是句子由多个“组”组成，每个组都是一个成分。语法，以其基本形式来说，可以说是一个可以出现在句子中的所有可能类型的成分的索引。
- en: 'Let''s first consider the most basic type of constituent, the **noun phrase**.
    Nouns within a sentence are fairly simple to identify as they are words that define
    objects or entities. In the following sentence, we can identify three nouns:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先考虑最基本的成分类型，即**名词短语**。句子中的名词相对简单，因为它们是定义对象或实体的词语。在以下句子中，我们可以识别出三个名词：
- en: “Jeff the chef cooks dinner”
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: “厨师杰夫烹饪晚餐”
- en: Jeff - Proper noun, denotes a name
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 杰夫 - 专有名词，表示一个名字
- en: Chef - A chef is an entity
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 厨师 - 厨师是一个实体
- en: Dinner - Dinner is an object/thing
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 晚餐 - 晚餐是一个对象/事物
- en: 'However, a noun phrase is slightly different as each noun phrase should refer
    to one single entity. In the preceding sentence, even though *Jeff* and *chef*
    are both nouns, the phrase *Jeff the chef* refers to one single person, so this
    can be considered a noun phrase. But how can we determine syntactically that the
    noun phrase refers to a single entity? One simple way is to place the phrase before
    a verb and see if the sentence makes syntactic sense. If it does, then chances
    are, the phrase is a noun phrase:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，名词短语略有不同，因为每个名词短语应该指代一个单一实体。在前面的句子中，即使*杰夫*和*厨师*都是名词，短语*厨师杰夫*指的是一个单一的人，因此这可以被视为一个名词短语。但我们如何从句法上确定名词短语指的是一个单一实体呢？一个简单的方法是将短语放在动词前面，看看句子是否在语法上有意义。如果有意义，那么很可能这个短语就是一个名词短语：
- en: Jeff the chef cooks…
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 厨师杰夫烹饪…
- en: Jeff the chef runs…
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 厨师杰夫运行…
- en: Jeff the chef drinks…
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 杰夫厨师喝…
- en: 'There exist a variety of different phrases that we are able to identify, as
    well as a number of complex grammatical rules that help us to identify them. We
    first identify the individual grammatical features that each sentence can be broken
    down into:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够识别出各种不同的短语，以及帮助我们识别它们的一些复杂语法规则。我们首先确定每个句子可以分解成的单个语法特征：
- en: '![](img/B12365_09_29.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B12365_09_29.jpg)'
- en: 'Now that we know that sentences are composed of constituents, and that constituents
    can be made up of several individual grammars, we can now start to map out our
    sentences based on their structure. For example, take the following example sentence:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道句子由成分组成，而成分可以由多个单个语法组成，我们现在可以根据其结构开始绘制我们的句子。例如，看下面的例句：
- en: “The boy hit the ball”
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: “男孩打了球”
- en: 'We can start by breaking this sentence down into two parts: a noun phrase and
    a verb phrase:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以先将这个句子分成两部分：一个名词短语和一个动词短语：
- en: '![Figure 9.13 Breaking down a sentence into its grammatical components'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.13 将句子分解为其语法组成部分'
- en: '](img/B12365_09_13.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_13.jpg)'
- en: Figure 9.13 – Breaking down a sentence into its grammatical components
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 将句子分解为其语法组成部分
- en: 'We then repeat this process for each of the phrases to split them into even
    smaller grammatical components. We can split this noun phrase into a determiner
    and a noun:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重复这个过程，将每个短语分割成更小的语法组件。我们可以将这个名词短语分割成一个限定词和一个名词：
- en: '![Figure 9.14 Breaking down the noun phrase'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14 分解名词短语'
- en: '](img/B12365_09_14.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_14.jpg)'
- en: Figure 9.14 – Breaking down the noun phrase
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 分解名词短语
- en: 'Again, we do this for the verb phrase to break it down into a verb and another
    noun phrase:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这个动词短语再次分解为动词和另一个名词短语：
- en: '![Figure 9.15 – Breaking down the verb phrase'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.15 – 分解动词短语'
- en: '](img/B12365_09_15.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_15.jpg)'
- en: Figure 9.15 – Breaking down the verb phrase
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – 分解动词短语
- en: 'We can iterate again and again, breaking down the various parts of our sentence
    into smaller and smaller chunks until we are left with a **parse tree**. This
    parse tree conveys the entirety of the syntactic structure of our sentence. We
    can see the parse tree of our example in its entirety here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以一次又一次地迭代，将我们句子的各个部分分解成越来越小的片段，直到我们留下一个**解析树**。这个解析树传达了我们句子的整个句法结构。我们可以在这里完整地看到我们示例的解析树：
- en: '![Figure 9.16 – Parse tree of the sentence'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.16 – 句子的解析树'
- en: '](img/B12365_09_16.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_16.jpg)'
- en: a
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个
- en: Figure 9.16 – Parse tree of the sentence
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – 句子的语法树
- en: While these parse trees allow us to see the syntactic structure of our sentences,
    they are far from perfect. From this structure, we can clearly see that there
    are two noun phrases with a verb taking place. However, from the preceding structure,
    it is not clear what is actually taking place. We have an action between two objects,
    but it is not clear from syntax alone what is taking place. Which party is doing
    the action to whom? We will see that some of this ambiguity is captured by semantic
    role labeling.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些语法树允许我们看到句子的句法结构，但它们还远非完美。从这个结构中，我们可以清楚地看到有两个名词短语和一个动词的发生。然而，从前面的结构中，单凭语法，无法清楚地看出实际发生了什么。我们有两个对象之间的动作，但仅凭语法本身无法确定哪个方面在对谁进行动作。我们将看到，语义角色标注捕捉到了一些这种模糊性。
- en: Semantic role labeling
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义角色标注
- en: Semantic role labeling is the process of assigning labels to words or phrases
    within a sentence that indicates their semantic role within a sentence. In broad
    terms, this involves identifying the predicate of the sentence and determining
    how each of the other terms within the sentence are related to this predicate.
    In other words, for a given sentence, semantic role labeling determines "Who did
    what to whom and where/when?"
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 语义角色标注是将标签分配给句子中的单词或短语的过程，指示它们在句子中的语义角色。广义上讲，这涉及识别句子的谓词，并确定句子中每个其他术语与该谓词的关系如何。换句话说，对于给定的句子，语义角色标注确定了句子中"谁对谁做了什么以及在哪里/何时"。
- en: 'So, for a given sentence, we can generally break down a sentence into its constituent
    parts, like so:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于给定的句子，我们通常可以将句子分解为其组成部分，如下所示：
- en: '![Figure 9.17 Breaking down a sentence into its constituent parts'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.17 将句子分解为其组成部分'
- en: '](img/B12365_09_17.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_17.jpg)'
- en: Figure 9.17 Breaking down a sentence into its constituent parts
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 将句子分解为其组成部分
- en: 'These parts of a sentence have specific semantic roles. The **predicate** of
    any given sentence represents the event occurring within the sentence, while all
    the other parts of the sentence relate back to a given predicate. In this sentence,
    we can label our "Who" as the agent of the predicate. The **agent** is the thing
    that causes the event. We can also label our "Whom" as the theme of our predicate.
    The **theme** is the element of our sentence most affected by the event in question:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 句子的这些部分具有特定的语义角色。任何给定句子的**谓词**表示句子中发生的事件，而句子的所有其他部分与给定的谓词相关联。在这个句子中，我们可以将"Who"标记为谓词的代理者。**代理者**是导致事件发生的事物。我们也可以将"Whom"标记为我们谓词的主题。**主题**是句子中最受事件影响的元素：
- en: '![Figure 9.18 – Breaking down the roles'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.18 – 分解角色'
- en: '](img/B12365_09_18.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_18.jpg)'
- en: Figure 9.18 – Breaking down the roles
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 分解角色
- en: 'In theory, each word or phrase in a sentence can be labeled with its specific
    semantic component. An almost comprehensive table for this is as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，句子中的每个单词或短语都可以用其特定的语义组件标记。一个几乎全面的表格如下所示：
- en: '![](img/B12365_09_Table_01.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B12365_09_Table_01.jpg)'
- en: By performing semantic role labeling, we can assign a specific role to every
    part of a sentence. This is very useful in NLP as it allows a model to "understand"
    a sentence better so that rather than a sentence just being an assortment of roles,
    it is understood as a combination of semantic roles that better convey what is
    actually happening in the event being described by the sentence.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行语义角色标注，我们可以为句子的每个部分分配特定的角色。这在自然语言处理中非常有用，因为它使模型能够更好地"理解"句子，而不是句子仅仅是一堆角色的组合，而是理解成能更好地传达事件实际发生情况的语义角色的组合。
- en: When we read the sentence *"The boy kicked the ball"*, we inherently know that
    there is a boy, there is a ball, and that the boy is kicking the ball. However,
    all the NLP models we have looked at so far would comprehend this sentence by
    looking at the individual words in the sentence and creating some representation
    of them. It is unlikely that the fact that there are two "things" and that object
    one (the boy) is performing some action (kicking) on object two (the ball) would
    be understood by the systems we have seen so far. Introducing an element of semantic
    roles to our models could better help our systems form more realistic representations
    of sentences by defining the subjects of the sentences and the interactions between
    them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们阅读句子“男孩踢了球”时，我们本能地知道有一个男孩，有一个球，而且男孩正在踢球。然而，到目前为止，我们看到的所有自然语言处理模型都是通过查看句子中的单词并为它们创建一些表示来理解这个句子。目前的系统很难理解到这样一个事实，即有两个“事物”，第一个对象（男孩）正在对第二个对象（球）执行某些动作（踢）。通过向我们的模型引入语义角色的元素，我们可以更好地帮助我们的系统通过定义句子的主语和它们之间的交互形成更为现实的表示。
- en: 'One thing that semantic role labeling helps with greatly is the identification
    of sentences that convey the same meaning but are not grammatically or syntactically
    the same; such as the following, for example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 语义角色标注极大地帮助了一个问题，即表达相同意义但语法或句法不同的句子的识别；例如以下句子：
- en: The man bought the apple from the shop
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果由商店卖给了那个男人
- en: The shop sold the man an apple
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 商店卖给了那个男人一个苹果
- en: The apple was bought by the man from the shop
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 男人从商店买了苹果
- en: The apple was sold by the shop to the man
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果是由商店卖给了那个男人
- en: These sentences have essentially the same meaning, although they clearly do
    not contain all the same words in the same order. By applying semantic role labeling
    to these sentences, we can determine that the predicate/agent/theme are all the
    same.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子本质上意思相同，尽管它们显然没有以相同顺序包含完全相同的单词。通过对这些句子应用语义角色标注，我们可以确定谓词/代理/主题都是相同的。
- en: 'We previously saw how constituency parsing/syntactic parsing can be used to
    identify the syntactic structure of a sentence. Here, we can see how we can break
    down the simple sentence "I bought a cat" into its constituent parts – pronoun,
    verb, determinant, and noun:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到了如何使用成分分析/句法分析来识别句子的句法结构。在这里，我们可以看到如何将简单句“我买了一只猫”分解为其组成部分 - 代词、动词、定语和名词：
- en: '![Figure 9.19 – Constituency parsing'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.19 – 成分分析'
- en: '](img/B12365_09_19.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_19.jpg)'
- en: Figure 9.19 – Constituency parsing
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 – 成分分析
- en: However, this does not shed any insight on the semantic role each part of the
    sentence is playing. Is the cat being bought by me or am I being bought by the
    cat? While the syntactic role is useful for understanding the structure of the
    sentence, it doesn't shed as much light on the semantic meaning. A useful analogy
    is that of image captioning. In a model trained to label images, we would hope
    to achieve a caption that describes what is in an image. Semantic labeling is
    the opposite of this, where we take a sentence and try to abstract a mental "image"
    of what action is taking place in the sentence.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这并没有揭示句子中每个部分在语义角色中的作用。是我买了猫还是猫买了我？虽然句法角色有助于理解句子的结构，但它对语义意义的启示不多。一个有用的类比是图像标题。在训练用于标记图像的模型中，我们希望得到一个描述图像内容的标题。语义标注则相反，我们接受一个句子并试图抽象出句子中正在发生的行为的心理“图像”。
- en: But what context is semantic role labeling useful for in NLP? In short, any
    NLP task that requires an element of "understanding" the content of text can be
    enhanced by the addition of roles. This could be anything from document summarization,
    question-answering, or sentence translation. For example, using semantic role
    labeling to identify the predicate of our sentence and the related semantic components,
    we could train a model to identify the components that contribute essential information
    to the sentence and drop those that do not.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，语义角色标注在自然语言处理中有哪些有用的上下文呢？简而言之，任何需要“理解”文本内容的自然语言处理任务都可以通过角色的添加而得到增强。这可以是从文档摘要、问答到句子翻译等任何内容。例如，使用语义角色标注来识别我们句子的谓词和相关的语义组件，我们可以训练一个模型来识别对句子起着重要信息贡献的组件，并丢弃那些不重要的。
- en: Therefore, being able to train models to perform accurate and efficient semantic
    role labeling would have useful applications for the rest of NLP. The earliest
    semantic role labeling systems were purely rule-based, consisting of basic sets
    of rules derived from grammar. These have since evolved to incorporate statistical
    modeling approaches before the recent developments in deep learning, which meant
    that it is possible to train classifiers to identify the relevant semantic roles
    within a sentence.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，能够训练模型以执行精确和高效的语义角色标注将对自然语言处理的其他领域有用。最早的语义角色标注系统是纯粹基于规则的，由从语法中导出的基本规则集组成。这些系统随后演变为整合统计建模方法，然后是近期的深度学习发展，这使得可以训练分类器在句子中识别相关的语义角色。
- en: As with any classification task, this is a supervised learning problem that
    requires a fully annotated sentence in order to train a model that will identify
    the semantic roles of previously unseen sentences. However, the availability of
    such annotated sentences is highly scarce. The gigantic language models, such
    as BERT, that we saw earlier in this chapter are trained on raw sentences and
    do not require labeled examples. However, in the case of semantic role labeling,
    our models require the use of correctly labeled sentences to be able to perform
    this task. While datasets do exist for this purpose, they are not large and versatile
    enough to train a fully comprehensive, accurate model that will perform well on
    a variety of sentences.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何分类任务一样，这是一个需要完全标注句子的监督学习问题，以训练一个能够识别先前未见句子的语义角色的模型。然而，这样标注的句子的可用性非常有限。我们在本章前面看到的巨大语言模型（如BERT）是在原始句子上进行训练的，并不需要标记的例子。然而，在语义角色标注的情况下，我们的模型需要正确标记的句子来执行这一任务。虽然存在此类目的数据集，但它们不足够大且多样化，无法训练出完全全面且准确的模型，以便在各种句子上表现良好。
- en: As you can probably imagine, the latest state-of-the-art approaches to solving
    the semantic role labeling task have all been neural network-based. Initial models
    used LSTMs and bidirectional LSTMs combined with GLoVe embeddings in order to
    perform classification on sentences. There have also been variations of these
    models that incorporate convolutional layers, which have also shown good performance.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能想象的那样，解决语义角色标注任务的最新先进方法都基于神经网络。最初的模型使用了LSTM和双向LSTM，结合了GLoVe嵌入以执行句子的分类。还有一些变体模型结合了卷积层，这些模型也表现出色。
- en: However, it will be no surprise to learn that these state-of-the-art models
    are BERT-based. Using BERT has shown exemplary performance in a whole variety
    of NLP-related tasks, and semantic role labeling is no exception. Models incorporating
    BERT have been trained holistically to predict part-of-speech tags, perform syntactic
    parsing, and perform semantic role labeling simultaneously and have shown good
    results.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不奇怪的是，这些最先进的模型都是基于BERT的。使用BERT在各种自然语言处理相关任务中表现出色，语义角色标注也不例外。整合了BERT的模型被全面训练，以预测词性标签，执行句法分析和同时执行语义角色标注，并展现了良好的结果。
- en: Other studies have also shown that graph convolutional networks are effective
    at semantic labeling. Graphs are constructed with nodes and edges, where the nodes
    within the graph represent semantic constituents and the edges represent the relationships
    between parent and child parts.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其他研究也表明，图卷积网络在语义标注中是有效的。图由节点和边构成，其中图中的节点表示语义成分，边表示父子部分之间的关系。
- en: 'A number of open source models for semantic role labeling are also available.
    The SLING parser from Google is trained to perform semantic annotations of data.
    This model uses a bidirectional LSTM to encode sentences and a transition-based
    recurrent unit for decoding. The model simply takes text tokens as input and outputs
    roles without any further symbolic representation:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多开源模型用于语义角色标注。Google的SLING解析器经过训练，用于执行数据的语义标注。该模型使用双向LSTM编码句子和转移型递归单元进行解码。该模型仅仅将文本标记作为输入，并输出角色，没有进一步的符号表示：
- en: '![Figure 9.20 – Bi-directional LSTM (SLING)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.20 – 双向LSTM (SLING)'
- en: '](img/B12365_09_20.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_20.jpg)'
- en: Figure 9.20 – Bi-directional LSTM (SLING)
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 – 双向LSTM (SLING)
- en: It is worth noting that SLING is still a work in progress. Currently, it is
    not sophisticated enough to extract facts accurately from arbitrary texts. This
    indicates that there is much work to be done in the field before a true, accurate
    semantic role parser can be created. When this is done, a semantic role parser
    could easily be used as part of an ensemble machine learning model to label semantic
    roles within a sentence, which is then used within a wider machine learning model
    to enhance the model's "understanding" of text.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，SLING仍然是一个正在进行中的工作。目前，它还不够复杂，无法从任意文本中准确提取事实。这表明在真正能够创建一个真实且准确的语义角色解析器之前，这个领域还有很多工作要做。完成这项工作后，语义角色解析器可以轻松地作为集成机器学习模型的一部分，用于标记句子中的语义角色，然后在更广泛的机器学习模型中使用，以增强模型对文本的“理解”。
- en: Textual entailment
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本蕴含
- en: Textual entailment is another methodology by which we can train models in an
    attempt to better understand the meaning of a sentence. In textual entailment,
    we attempt to identify a directional relationship between two pieces of text.
    This relationship exists whenever the truth from one piece of text follows from
    another piece of text. This means that, given two texts, if the second text can
    be held to be true by the information within the first text, we can say that there
    is a positive directional relationship between these two texts.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 文本蕴含也是另一种方法，通过这种方法，我们可以训练模型，试图更好地理解句子的含义。在文本蕴含中，我们尝试识别两段文本之间的方向关系。这种关系存在于一个文本的真实性能够从另一个文本中推导出来的情况下。这意味着，给定两段文本，如果第二段文本可以通过第一段文本中的信息来证明是真实的，我们可以说这两段文本之间存在着正向的方向关系。
- en: 'This task is often set up in the following fashion, with our first text labeled
    as text and our second text labeled as our hypothesis:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这项任务通常以以下方式设置，我们的第一段文本标记为文本，我们的第二段文本标记为假设：
- en: '**Text**: *If you give money to charity, you will be happy*'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本**: *如果你给慈善机构捐款，你会感到快乐*'
- en: '**Hypothesis**: *Giving money to charity has good consequences*'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设**: *捐款给慈善机构会产生良好的后果*'
- en: This is an example of **positive textual entailment**. If the hypothesis follows
    from the text, then there can be said to be a directional relationship between
    the two texts. It is important to set up the example with a text/hypothesis as
    this defines the direction of the relationship. The majority of the time, this
    relationship is not symmetrical. For example, in this example, sentence one entails
    sentence two (we can infer sentence two to be true based on the information in
    sentence one). However, we cannot infer that sentence one is true based on the
    information in sentence two. While it is possible that both statements are indeed
    true, if we cannot deduce that there is a directional relationship between the
    two, we cannot infer one from the other.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正文本蕴含的一个例子。如果假设可以从文本中得出结论，那么这两段文本之间就可以说存在方向关系。重要的是通过文本/假设来设置例子，因为这定义了关系的方向。大部分时间，这种关系不是对称的。例如，在这个例子中，第一句蕴含第二句（我们可以根据第一句的信息推断第二句是真实的）。然而，我们不能根据第二句的信息推断第一句是真实的。虽然两个陈述可能都是真实的，但如果我们不能推断这两者之间存在方向关系，我们就不能从一个中推断另一个。
- en: 'There also exists a **negative textual entailment**. This is when the statements
    are contradictory; such as the following, for example:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 也存在**负文本蕴含**。这是当陈述是相互矛盾的时候；例如：
- en: '**Text**: *If you give money to charity, you will be happy*'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本**: *如果你给慈善机构捐款，你会感到快乐*'
- en: '**Hypothesis**: *Giving money to charity has bad consequences*'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设**: *捐款给慈善机构会产生坏后果*'
- en: 'In this example, the text does not entail the hypothesis; instead, the text
    contradicts the hypothesis. Finally, it is also possible to determine that there
    is **no textual entailment** between two sentences if there is no relationship
    between them. This means that the two statements are not necessarily contradictory,
    but rather that the text does not entail the hypothesis:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，文本并不蕴含假设；相反，文本与假设相矛盾。最后，还有可能确定两个句子之间**没有文本蕴含**，如果它们之间没有关系的话。这意味着这两个陈述不一定是矛盾的，而是文本不蕴含假设：
- en: '**Text**: *If you give money to charity, you will be happy*'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本**: *如果你给慈善机构捐款，你会感到快乐*'
- en: '**Hypothesis**: *Giving money to charity will make you relaxed*'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**假设**: *捐款给慈善机构会使你放松*'
- en: The ambiguity of natural language makes this an interesting task from an NLP
    perspective. Two sentences can have a different syntactic structure, a different
    semantic structure, and consist of entirely different words but still have very
    similar meanings. Similarly, two sentences can consist of the same words and entities
    but have very different meanings.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言的歧义性使得这个任务在自然语言处理（NLP）的视角下变得有趣。两个句子可以有不同的句法结构、不同的语义结构，由完全不同的词组成，但仍然可能有非常相似的含义。同样地，两个句子可以由相同的词和实体组成，但含义却截然不同。
- en: This is where using models to be able to quantify the meaning of text is particularly
    useful. Textual entailment is also a unique problem in that two sentences may
    not have exactly the same meaning, yet one can still be inferred from the other.
    This requires an element of linguistic deduction that is not present in most language
    models. By incorporating elements of linguistic deduction in our models going
    forward, we can better capture the meaning of texts, as well as be able to determine
    whether two texts contain the same information, regardless of whether their representations
    are similar.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是使用模型来量化文本含义的地方特别有用。文本蕴涵也是一个独特的问题，因为两个句子可能并不完全具有相同的含义，然而一个句子仍然可以从另一个句子推断出来。这需要一种语言推理的元素，在大多数语言模型中并不存在。通过在我们的模型中引入语言推理的元素，我们可以更好地捕捉文本的含义，以及能够确定两个文本是否包含相同的信息，而不管它们的表现形式是否相似。
- en: Fortunately, simple textual entailment models are not difficult to create, and
    LSTM-based models have been shown to be effective. One setup that may prove effective
    is that of a Siamese LSTM network.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，简单的文本蕴涵模型并不难创建，基于LSTM的模型已被证明是有效的。一个可能有效的设置是Siamese LSTM网络。
- en: 'We set up our model as a multi-class classification problem where two texts
    can be positively or negatively entailed or have no entailment. We feed our two
    texts into a dual-input model, thereby obtaining embeddings for the two texts,
    and pass them through bidirectional LSTM layers. The two outputs are then compared
    somehow (using some tensor operation) before they''re fed through a final LSTM
    layer. Finally, we perform classification on the output using a softmax layer:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的模型设置为一个多类分类问题，其中两个文本可以被积极或消极地蕴涵，或者没有蕴涵。我们将两个文本输入到一个双输入模型中，从而获取两个文本的嵌入，并通过双向LSTM层传递它们。然后对这两个输出进行某种比较（使用某种张量操作），然后它们通过最终的LSTM层。最后，我们使用softmax层对输出进行分类：
- en: '![Figure 9.21 – Siamese LSTM network'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.21 – Siamese LSTM网络'
- en: '](img/B12365_09_21.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_21.jpg)'
- en: Figure 9.21 – Siamese LSTM network
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21 – Siamese LSTM网络
- en: While these models are far from perfect, they represent the first steps toward
    creating a fully accurate textual entailment model and open up the possibilities
    toward integrating this into language models moving forward.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型还远非完美，它们代表了朝着创建完全准确的文本蕴涵模型迈出的第一步，并为将其整合到未来语言模型中打开了可能性。
- en: Machine comprehension
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器理解
- en: So far in this book, we have referred mostly to NLP, but being able to process
    language is just part of the picture. When you or I read a sentence, we not only
    read, observe, and process the individual words, but we also build an inherent
    understanding of what the sentence actually means. Being able to train models
    that not only comprehend a sentence but can also form an understanding of the
    ideas being expressed within it is arguably the next step in NLP. The true definition
    of this field is very loosely defined, but it is often referred to as machine
    comprehension or **natural language understanding** (**NLU**).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本书中，我们主要提到了NLP，但能够处理语言只是一个方面。当你或我阅读一个句子时，我们不仅仅是阅读、观察和处理单个词语，还会建立起对句子实际含义的内在理解。能够训练出不仅仅理解句子，而且能够形成对其中所表达的思想理解的模型，可以说是NLP的下一步。这个领域的真正定义非常宽泛，但通常被称为机器理解或自然语言理解（NLU）。
- en: 'At school, we are taught reading comprehension from a young age. You probably
    learned this skill a long time ago and is something you now take for granted.
    Often, you probably don''t even realize you are doing it; in fact, you are doing
    it right now! Reading comprehension is simply the act of reading a text, understanding
    this text, and being able to answer questions about the text. For example, take
    a look at the following text:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在学校，我们从小就被教导阅读理解。你可能很早就学会了这项技能，现在可能认为这是理所当然的。通常情况下，你可能甚至没有意识到自己在做这件事；事实上，你现在正在做它！阅读理解简单来说就是阅读文本，理解这段文本，并能回答关于文本的问题。例如，请看下面的文本：
- en: As a method of disinfecting water, bringing it to its boiling point at 100 °C
    (212 °F) is the oldest and most effective way of doing this since it does not
    affect its taste. It is effective despite contaminants or particles present in
    it, and is a single step process that eliminates most microbes responsible for
    causing intestine-related diseases. The boiling point of water is 100 °C (212 °F)
    at sea level and at normal barometric pressure.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种消毒水的方法，将水加热至其沸点100 °C（212 °F）是最古老和最有效的方法，因为它不会影响其口感。尽管存在污染物或颗粒物，它仍然有效，并且是一种单步过程，可消灭大多数引起肠道相关疾病的微生物。水的沸点在海平面和常规气压下为100
    °C（212 °F）。
- en: 'Given that you understand this text, you should now be able to answer the following
    questions about it:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你理解了这段文本，现在你应该能够回答以下关于它的问题：
- en: '**Q**: *What is the boiling point of water?*'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q**: *水的沸点是多少？*'
- en: '**A**: *100 °C (212 °F)*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**A**: *100 °C (212 °F)*'
- en: '**Q**: *Does boiling water affect its taste?*'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q**: *沸水会影响其口感吗？*'
- en: '**A**: *No*'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**A**: *否*'
- en: This ability to understand text and answer questions about it form the basis
    for our machine comprehension task. We wish to be able to train a machine learning
    model that can not only form an understanding of a text, but also be able to answer
    questions about it in grammatically correct natural language.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 理解文本并能回答关于文本的问题的能力构成了我们机器理解任务的基础。我们希望能够训练一个机器学习模型，该模型不仅能够理解文本，还能够用语法正确的自然语言回答关于文本的问题。
- en: 'The benefits of this are numerous, but a very intuitive use case would be to
    build a system that acts as a knowledge base. Currently, the way search engines
    work is that we run a search (in Google or a similar search engine) and the search
    engine returns a selection of documents. However, to find a particular piece of
    information, we must still infer the correct information from our returned document.
    The entire process might look something like this:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处是多方面的，但一个非常直观的用例将是构建一个充当知识库的系统。目前，搜索引擎的工作方式是我们运行搜索（在Google或类似的搜索引擎中），搜索引擎返回一些文档。然而，要找到特定的信息，我们仍然必须从返回的文档中推断出正确的信息。整个过程可能看起来像这样：
- en: '![Figure 9.22 – Process of finding information'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.22 – 查找信息的过程'
- en: '](img/B12365_09_22.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_22.jpg)'
- en: Figure 9.22 – Process of finding information
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – 查找信息的过程
- en: In this example, to answer the question *"What is the boiling point of water?",*
    we first formulate our question. Then, we search for the subject matter on a search
    engine. This would probably be some reduced representation of the question; for
    example, *"water boiling point"*. Our search engine would then return some relevant
    documents, most likely the Wikipedia entry for water, which we would then manually
    have to search and use it to infer the answer to our question. While this methodology
    is effective, machine comprehension models would allow this process to be streamlined
    somewhat.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，要回答关于“水的沸点是多少？”的问题，我们首先提出我们的问题。然后，在搜索引擎上搜索主题内容。这可能是一些简化的问题表示形式；例如，“水的沸点”。我们的搜索引擎然后会返回一些相关的文档，很可能是水的维基百科条目，我们随后必须手动搜索并使用它来推断我们问题的答案。虽然这种方法是有效的，但机器理解模型可以使这个过程在某种程度上得到简化。
- en: 'Let''s say we have a perfect model that is able to fully comprehend and answer
    questions on a text corpus. We could train this model on a large source of data
    such as a large text scrape of the internet or Wikipedia and form a model that
    acts as a large knowledge base. By doing this, we would then be able to query
    the knowledge base with real questions and the answers would be returned automatically.
    This removes the knowledge inference step of our diagram as the inference is taken
    care of by the model as the model already has an understanding of the subject
    matter:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个完美的模型，能够完全理解并回答文本语料库中的问题。我们可以训练这个模型，使用像是互联网大量文本抓取或维基百科这样的大数据源，并形成一个充当大型知识库的模型。通过这样做，我们就能够用真实问题查询知识库，答案会自动返回。这样一来，我们的图表中的知识推断步骤就被移除了，因为推断已由模型处理，模型已经对主题有了理解：
- en: '![Figure 9.23 – New process using a model'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.23 – 使用模型的新流程'
- en: '](img/B12365_09_23.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_23.jpg)'
- en: Figure 9.23 – New process using a model
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – 使用模型的新流程
- en: In an ideal world, this would be as simple as typing *"What is the boiling point
    of water?"* into a search engine and receiving *"100 °C (212 °F)"* back as an
    answer.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想的情况下，这将简单到只需在搜索引擎中键入 *"水的沸点是多少？"* ，就会收到 *"100 °C (212 °F)"* 作为答案。
- en: Let's assume we have a simplified version of this model to begin with. Let's
    assume we already know the document that the answer to our asked question appears
    in. So, given the Wikipedia page on water, can we train a model to answer the
    question *"What is the boiling point of water?".* A simple way of doing this to
    begin with, rather than incorporating the elements of a full language model, would
    be to simply return the passage of the Wikipedia page that contains the answer
    to our question.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们有一个简化版本的这个模型。假设我们已经知道包含我们所问问题答案的文档。那么，假设我们有了关于水的维基百科页面，我们能否训练一个模型来回答
    *"水的沸点是多少？"* 的问题。最初的简单做法，而不是整合完整语言模型的元素，是简单地返回包含答案的维基百科页面的段落。
- en: 'An architecture that we could train to achieve this task might look something
    like this:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以训练的架构来实现这个任务可能看起来像这样：
- en: '![Figure 9.24 – Architecture of the model'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.24 – 模型的架构'
- en: '](img/B12365_09_24.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_24.jpg)'
- en: Figure 9.24 – Architecture of the model
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 – 模型的架构
- en: Our model takes our question that we want answered and our document that contains
    our question as inputs. These are then passed through an embedding layer to form
    a tensor-based representation of each, and then an encoding layer to form a further
    reduced vector representation.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型接受我们想要回答的问题和包含我们问题的文档作为输入。然后，这些输入通过嵌入层传递，以形成每个的基于张量的表示，然后通过编码层形成进一步减少的向量表示。
- en: Now that our question and documents are represented as vectors, our matching
    layer attempts to determine where in the document vectors we should look to obtain
    the answer to our question. This is done through a form of attention mechanism
    whereby our question determines what parts of our document vectors we should look
    at in order to answer the question.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的问题和文档被表示为向量后，我们的匹配层尝试确定我们应该查看文档向量中的哪个部分以获取问题的答案。这是通过一种注意力机制形式完成的，我们的问题决定我们应该查看文档向量的哪些部分来回答问题。
- en: Finally, our fusing layer is designed to capture the long-term dependencies
    of our matching layer, combine all the received information from our matching
    layer, and perform a decoding step to obtain our final answers. This layer takes
    the form of a bidirectional RNN that decodes our matching layer output into final
    predictions. We predict two values here – a start point and an endpoint – using
    a multiclass classification. This represents the start and end points within our
    document that contain the answer to our initial question. If our document contained
    100 words and the sentence between word 40 and word 50 contained the answer to
    our question, our model would ideally predict 40 and 50 for the values of the
    start and end points. These values could then be easily used to return the relevant
    passage from the input document.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的融合层旨在捕捉匹配层的长期依赖关系，将来自匹配层的所有接收信息结合起来，并执行解码步骤以获取我们的最终答案。这一层采用双向 RNN 的形式，将匹配层的输出解码为最终预测。我们在这里预测两个值
    – 起点和终点 – 使用多类分类。这代表了在我们的文档中包含回答初始问题的起点和终点。如果我们的文档包含 100 个单词，并且第 40 到第 50 个单词之间的句子包含了我们的答案，我们的模型理想地应该预测起点和终点的值分别为
    40 和 50。这些值随后可以轻松用于返回输入文档中的相关段落。
- en: While returning relevant areas of a target document is a useful model to train,
    it is not the same as a true machine comprehension model. In order to do that,
    we must incorporate elements of a larger language model.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 返回目标文档的相关区域虽然是一个有用的训练模型，但这并不等同于真正的机器理解模型。为了做到这一点，我们必须融入更大语言模型的元素。
- en: 'In any machine comprehension task, there are actually three elements at play.
    We already know that there is a question and answer, but there is also a relevant
    context that may determine the answer to a given question. For example, we can
    ask the following question:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器理解任务中，实际上有三个要素在起作用。我们已经知道有问题和答案，但还有一个相关的上下文可能决定了给定问题的答案。例如，我们可以问以下问题：
- en: '*What day is it today?*'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*今天是星期几？*'
- en: The answer may differ, depending on the context in which the question is asked;
    for example, Monday, Tuesday, March the 6th, Christmas Day.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 答案可能会因提问时的上下文而有所不同；例如，星期一、星期二、3 月 6 日、圣诞节。
- en: 'We must also note that the relationship between the question and answer is
    bidirectional. When given a knowledge base, it is possible for us to generate
    an answer given a question, but it also follows that we are able to generate a
    question given an answer:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须注意问题和答案之间的关系是双向的。在给定知识库的情况下，我们可以根据问题生成答案，但我们也能根据答案生成问题：
- en: '![Figure 9.25 – Relationship between the question and answer'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.25 – 问题和答案之间的关系'
- en: '](img/B12365_09_25.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_25.jpg)'
- en: Figure 9.25 – Relationship between the question and answer
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25 – 问题和答案之间的关系
- en: 'A true machine comprehension may be able to perform **question generation**
    (**QG**), as well as **question-answering** (**QA**). The most obvious solution
    to this is to train two separate models, one for each task, and compare their
    results. In theory, the output of our QG model should equal the input of our QA
    model, so by comparing the two, we can provide simultaneous evaluation:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的机器理解可能能够执行**问题生成**（**QG**），以及**问答**（**QA**）。对此最明显的解决方案是训练两个单独的模型，一个用于每个任务，并比较它们的结果。理论上，我们的
    QG 模型的输出应该等于我们 QA 模型的输入，因此通过比较这两者，我们可以进行同时评估：
- en: '![Figure 9.26 – Comparison between QG and QA models'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.26 – QG 和 QA 模型的比较'
- en: '](img/B12365_09_26.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_26.jpg)'
- en: Figure 9.26 – Comparison between QG and QA models
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26 – QG 和 QA 模型的比较
- en: 'However, a more comprehensive model would be able to perform these two tasks
    simultaneously, thereby generating a question from an answer and answering a question,
    much like humans are able to do:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个更全面的模型可以同时执行这两个任务，从而从答案生成问题并回答问题，就像人类能够做的那样：
- en: '![Figure 9.27 Dual model representation'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.27 双模型表示'
- en: '](img/B12365_09_27.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_27.jpg)'
- en: Figure 9.27 – Dual model representation
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27 – 双模型表示
- en: 'In fact, recent advances in NLU have meant that such models are now a reality.
    By combining many elements, we are able to create a neural network structure that
    is able to perform the function of the dual model, as illustrated previously.
    This is known as the **dual ask-answer network**. In fact, our model contains
    most of the components of neural networks that we have seen in this book so far,
    that is, embedding layers, convolutional layers, encoders, decoders, and attention
    layers. The full architecture of the ask-answer network looks similar to the following:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，自然语言理解的最新进展意味着这些模型现在已成为现实。通过结合多种元素，我们能够创建一个能够执行双重模型功能的神经网络结构，正如之前所示。这被称为**双问答网络**。事实上，我们的模型包含了迄今为止在本书中看到的大多数神经网络组件，即嵌入层、卷积层、编码器、解码器和注意力层。问答网络的完整架构看起来类似于以下内容：
- en: '![Figure 9.28 – Architecture of ask-answer network'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.28 – 双问答网络的架构'
- en: '](img/B12365_09_28.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_09_28.jpg)'
- en: Figure 9.28 – Architecture of ask-answer network
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28 – 双问答网络的架构
- en: 'We can make the following observations here:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这里做出以下观察：
- en: The model’s **inputs** are the question, answer, and context, as previously
    outlined, but also the question and answer shifted right ward.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的**输入**包括问题、答案和上下文，如前所述，还包括右移的问题和答案。
- en: Our **embedding** layer convolves across GLoVe embedded vectors for characters
    and words in order to create a combined representation.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的**嵌入**层通过对字符和单词的GloVe嵌入向量进行卷积，以创建一个合并的表示。
- en: Our **encoders** consist of LSTMs, with applied attention.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的**编码器**由应用了注意力机制的LSTMs组成。
- en: Our **outputs** are also RNN-based and decode our output one word at a time
    to generate final questions and answers.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的**输出**也基于RNN，并逐词解码我们的输出，生成最终的问题和答案。
- en: While pre-trained ask-answer networks exist, you could practice implementing
    your newly acquired PyTorch skills and try building and training a model like
    this yourself.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已存在预训练的问答网络，但你可以练习实现你新学到的PyTorch技能，并尝试自己构建和训练这样的模型。
- en: Language comprehension models like these are likely to be one of the major focuses
    of study within NLP over the coming years, and new papers are likely to be published
    with great frequency moving forward.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的语言理解模型很可能成为未来几年内自然语言处理研究的主要焦点之一，新的论文很可能会频繁出版。
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we first examined several state-of-the-art NLP language models.
    BERT, in particular, seems to have been widely accepted as the industry standard
    state-of-the-art language model, and BERT and its variants are widely used by
    businesses in their own NLP applications.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先研究了几种最先进的自然语言处理语言模型。特别是，BERT似乎已被广泛接受为行业标准的最先进语言模型，BERT及其变体广泛应用于企业自己的自然语言处理应用中。
- en: Next, we examined several areas of focus for machine learning moving forward;
    namely semantic role labeling, constituency parsing, textual entailment, and machine
    comprehension. These areas will likely make up a large percentage of the current
    research being conducted in NLP moving forward.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们详细研究了机器学习未来的几个重点领域；即语义角色标注、成分句法分析、文本蕴涵和机器理解。这些领域很可能占据当前自然语言处理研究的大部分内容。
- en: Now that you have a well-rounded ability and understanding when it comes to
    NLP deep learning models and how to implement them in PyTorch, perhaps you'll
    feel inclined to be a part of this research moving forward. Whether this is in
    an academic or business context, you now hopefully know enough to create your
    own deep NLP projects from scratch and can use PyTorch to create the models you
    need to solve any NLP task you require. By continuing to improve your skills and
    by being aware and keeping up to date with all the latest developments in the
    field, you will surely be a successful, industry leading NLP data scientist!
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你对于NLP深度学习模型及其在PyTorch中的实现有了全面的能力和理解，或许你会有兴趣参与未来的研究。无论是在学术还是商业环境中，你现在应该有足够的知识，从零开始创建你自己的深度NLP项目，并能使用PyTorch创建你需要的模型来解决任何你需要的NLP任务。通过继续提升你的技能，并且保持关注并跟上领域中的最新发展，你必定会成为成功的、业界领先的NLP数据科学家！
