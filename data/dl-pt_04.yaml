- en: Fundamentals of Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: In the previous chapters, we saw practical examples of how to build deep learning
    models to solve classification and regression problems, such as image classification
    and average user view predictions. Similarly, we developed an intuition on how
    to frame a deep learning problem. In this chapter, we will take a look at how
    we can attack different kinds of problems and different tweaks that we will potentially
    end up using to improve our model's performance on our problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到了如何构建深度学习模型来解决分类和回归问题的实际示例，比如图像分类和平均用户观看预测。同样地，我们也形成了如何构建深度学习问题框架的直觉。在本章中，我们将详细讨论如何解决不同类型的问题以及我们可能会使用的各种调整来提高模型在问题上的性能。
- en: 'In this chapter, we will explore:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨：
- en: Other forms of problems beyond classification and regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超出分类和回归的其他问题形式
- en: Problems with evaluation, understanding overfitting, underfitting, and techniques
    to solve them
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估问题，理解过拟合、欠拟合及解决方法的问题
- en: Preparing data for deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为深度学习准备数据
- en: Remember, most of the topics that we discuss in this chapter are common to machine
    learning and deep learning, except for some of the techniques—such as dropout—that
    we use to solve overfitting problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，本章讨论的大多数主题对机器学习和深度学习来说都是常见的，除了一些我们用来解决过拟合问题的技术，比如 dropout。
- en: Three kinds of machine learning problems
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三种机器学习问题
- en: In all our previous examples, we tried to solve either classification (predicting
    cats or dogs) or regression (predicting the average time users spend in the platform)
    problems. All these are examples of supervised learning, where the goal is to
    map the relationship between training examples and their targets and use it to
    make predictions on unseen data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的所有示例中，我们试图解决分类（预测猫或狗）或回归（预测用户在平台上平均花费的时间）问题。所有这些都是监督学习的例子，其目标是映射训练样本和它们的目标之间的关系，并用它来对未见数据进行预测。
- en: 'Supervised learning is just one part of machine learning, and there are other
    different parts of machine learning. There are three different kinds of machine
    learning:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习只是机器学习的一部分，还有其他不同的机器学习部分。机器学习有三种不同的类型：
- en: Supervised learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Let's look in detail at the kinds of algorithms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解各种算法类型。
- en: Supervised learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Most of the successful use cases in the deep learning and machine learning
    space fall under supervised learning. Most of the examples we cover in this book
    will also be part of this. Some of the common examples of supervised learning
    are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和机器学习领域中，大多数成功的用例属于监督学习。本书中我们涵盖的大多数示例也将是其中的一部分。一些常见的监督学习例子包括：
- en: '**Classification problems**: Classifying dogs and cats.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类问题**：对狗和猫进行分类。'
- en: '**Regression problems**: Predicting stock prices, cricket match scores, and
    so on.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归问题**：预测股票价格、板球比赛得分等。'
- en: '**Image segmentation**: Doing a pixel-level classification. For a self-driving
    car, it is important to identify what each pixel belongs to from the photo taken
    by its camera. The pixel could belong to a car, pedestrian, tree, bus, and so
    on.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分割**：进行像素级分类。对于自动驾驶汽车来说，从其摄像头拍摄的照片中识别每个像素属于什么是很重要的。像素可能属于汽车、行人、树木、公共汽车等等。'
- en: '**Speech recognition**: OK Google, Alexa, and Siri are good examples of speech
    recognition.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音识别**：OK Google、Alexa 和 Siri 是语音识别的良好示例。'
- en: '**Language translation**: Translating speech from one language to another language.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：将一种语言的语音翻译成另一种语言。'
- en: Unsupervised learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'When there is no label data, unsupervised learning techniques help in understanding
    the data by visualizing and compressing. The two commonly-used techniques in unsupervised
    learning are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有标签数据时，无监督学习技术通过可视化和压缩帮助理解数据。无监督学习中常用的两种技术是：
- en: Clustering
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Dimensionality reduction
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维
- en: Clustering helps in grouping all similar data points together. Dimensionality
    reduction helps in reducing the number of dimensions, so that we can visualize
    high-dimensional data to find any hidden patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类有助于将所有相似的数据点分组在一起。降维有助于减少维度数量，这样我们可以可视化高维数据以发现任何隐藏的模式。
- en: Reinforcement learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is the least popular machine learning category. It did
    not find its success in real-world use cases. However, it has changed in recent
    years, and teams from Google DeepMind were able to successfully build systems
    based on reinforcement learning and were able to win the AlphaGo game against
    the world champion. This kind of technology advancement, where a computer can
    beat a human in a game, was considered to take more than a few decades for computers
    to achieve. However, deep learning combined with reinforcement learning was able
    to achieve it far sooner than anyone would have anticipated. These techniques
    have started seeing early success, and it could probably take a few years for
    it to become mainstream.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是最不受欢迎的机器学习类别。它在现实世界的使用案例中并没有取得成功。然而，近年来情况发生了变化，Google DeepMind的团队成功地基于强化学习构建系统，并且能够在AlphaGo比赛中击败世界冠军。这种技术进步，即计算机可以在游戏中击败人类，被认为需要几十年的时间才能实现。然而，深度学习结合强化学习比任何人预期的都要早地实现了这一点。这些技术已经开始取得初步的成功，可能需要几年时间才能成为主流。
- en: In this book, we will focus mostly on the supervised techniques and some of
    the unsupervised techniques that are specific to deep learning, such as generative
    networks used for creating images of a particular style called **style transfer**
    and **generative adversarial networks**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将主要关注监督技术，以及一些深度学习中特有的无监督技术，例如用于创建特定风格图像的生成网络，称为**风格转移**和**生成对抗网络**。
- en: Machine learning glossary
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习术语表
- en: 'In the last few chapters, we have used lot of terminology that could be completely
    new to you if you are just entering the machine learning or deep learning space.
    We will list a lot of commonly-used terms in machine learning, which are also
    used in the deep learning literature:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的几章中，我们使用了许多可能对您完全陌生的术语，如果您刚刚进入机器学习或深度学习领域，我们将列出许多在机器学习中常用的术语，这些术语也在深度学习文献中使用：
- en: '**Sample** **or input or** **data point**: These mean particular instances
    of training a set. In our image classification problem seen in the last chapter,
    each image can be referred to as a sample, input, or data point.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本** **或输入** **或** **数据点**：这些表示训练集的特定实例。在我们上一章中看到的图像分类问题中，每个图像可以称为样本、输入或数据点。'
- en: '**Prediction** **or** **output**: The value our algorithm generates as an output.
    For example, in our previous example our algorithm predicted a particular image
    as 0, which is the label given to cat, so the number 0 is our prediction or output.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测** **或** **输出**：我们的算法生成的值作为输出。例如，在我们的上一个例子中，我们的算法预测特定图像为0，这是给猫的标签，所以数字0是我们的预测或输出。'
- en: '**Target** **or label**: The actual tagged label for an image.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标** **或标签**：图像的实际标记标签。'
- en: '**Loss value** **or prediction error**: Some measure of distance between the
    predicted value and actual value. The smaller the value, the better the accuracy.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失值** **或预测误差**：预测值和实际值之间距离的某种度量。值越小，准确性越高。'
- en: '**Classes**: Possible set of values or labels for a given dataset. In the example
    in our previous chapter, we had two classes—cats and dogs.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别**：给定数据集的可能值或标签集。在我们上一章的例子中，我们有两个类别——猫和狗。'
- en: '**Binary classification**: A classification task where each input example should
    be classified as either one of the two exclusive categories.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：一个分类任务，其中每个输入示例应被分类为两个互斥的类别之一。'
- en: '**Multi-class classification**: A classification task where each input example
    can be classified into of more than two different categories.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**：一个分类任务，其中每个输入示例可以被分类为超过两个不同的类别。'
- en: '**Multi-label classification**: An input example can be tagged with multiple
    labels—for example, tagging a restaurant with different types of food it serves
    such as Italian, Mexican, and Indian. Another commonly-used example is object
    detection in an image, where the algorithm identifies different objects in the
    image.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多标签分类**：一个输入示例可以被打上多个标签，例如标记一个餐馆提供的不同类型的食物，如意大利、墨西哥和印度食物。另一个常用的例子是图像中的物体检测，算法可以识别图像中的不同对象。'
- en: '**Scalar regression**: Each input data point will be associated with one scalar
    quality, which is a number. Some examples could be predicting house prices, stock
    prices, and cricket scores.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量回归**：每个输入数据点将与一个标量质量相关联，即一个数字。例如，预测房价、股票价格和板球比分。'
- en: '**Vector regression**: Where the algorithm needs to predict more than one scalar
    quantity. One good example is when you try to identify the bounding box that contains
    the location of a fish in an image. In order to predict the bounding box, your
    algorithm needs to predict four scalar quantities denoting the edges of a square.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vector regression**: 当算法需要预测多个标量量时使用。一个很好的例子是当您尝试识别图像中包含鱼位置的边界框时。为了预测边界框，您的算法需要预测四个标量量，表示正方形的边缘。'
- en: '**Batch**: For most cases, we train our algorithm on a bunch of input samples
    referred to as the batch. The batch size varies generally from 2 to 256, depending
    on the GPU''s memory. The weights are also updated for each batch, so the algorithms
    tend to learn faster than when trained on a single example.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Batch**: 对于大多数情况，我们训练算法时使用一组输入样本，称为批处理。批处理的大小通常从 2 到 256 不等，取决于 GPU 的内存。权重也会在每个批次中更新，因此算法的学习速度比在单个示例上训练时要快。'
- en: '**Epoch**: Running the algorithm through a complete dataset is called an **epoch**.
    It is common to train (update the weights) for several epochs.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Epoch**: 将算法运行完整数据集称为一个**周期**。通常会进行多个周期的训练（更新权重）。'
- en: Evaluating machine learning models
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估机器学习模型
- en: In the example of image classification that we covered in the last chapter,
    we split the data into two different halves, one for training and one for validation.
    It is a good practice to use a separate dataset to test the performance of your
    algorithm, as testing the algorithm on the training set may not give you the true
    generalization power of the algorithm. In most real-world use cases, based on
    the validation accuracy, we often tweak our algorithm in different ways, such
    as adding more layers or different layers, or using different techniques that
    we will cover in the later part of the chapter. So, there is a higher chance that
    your choices for tweaking the algorithm are based on the validation dataset. Algorithms
    trained this way tend to perform well in the training dataset and the validation
    dataset, but fail to generalize well on unseen data. This is due to an information
    leak from your validation dataset, which influences us in tweaking the algorithm.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上一章节讨论的图像分类示例中，我们将数据分成两半，一半用于训练，另一半用于验证。使用单独的数据集来测试算法的性能是一个良好的实践，因为在训练集上测试算法可能不能真正反映算法的泛化能力。在大多数真实应用中，根据验证准确率，我们经常以不同的方式调整算法，例如添加更多层或不同的层，或者使用我们将在本章后部分介绍的不同技术。因此，你对调整算法选择的可能性更高是基于验证数据集。通过这种方式训练的算法通常在训练数据集和验证数据集上表现良好，但在未见数据上泛化能力较差。这是由于验证数据集中的信息泄漏，影响了我们对算法进行调整。
- en: To avoid the problem of an information leak and improve generalization, it is
    often a common practice to split the dataset into three different parts, namely
    a training, validation, and test dataset. We do the training and do all the hyper
    parameter tuning of the algorithm using the training and validation set. At the
    end, when the entire training is done, then you will test the algorithm on the
    test dataset. There are two types of parameters that we talk about. One is the
    parameters or weights that are used inside an algorithm, which are tuned by the
    optimizer or during backpropagation. The other set of parameters, called **hyper
    parameters**, controls the number of layers used in the network, learning rate,
    and other types of parameter that generally change the architecture, which is
    often done manually.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免信息泄漏问题并提高泛化能力，通常的做法是将数据集分为三个不同部分，即训练、验证和测试数据集。我们通过训练和验证集进行算法的所有超参数调整和训练。在整个训练结束时，您将在测试数据集上测试算法。我们讨论的有两种类型的参数。一种是算法内部使用的参数或权重，这些参数通过优化器或反向传播进行调整。另一组参数称为**超参数**，控制网络中使用的层数、学习速率和其他类型的参数，通常需要手动更改架构。
- en: The phenomenon of a particular algorithm performing better in the training set
    and failing to perform on the validation or test set is called **overfitting**,
    or the lack of the algorithm's ability to generalize. There is an opposite phenomenon
    where the algorithm fails to perform for the training set, which is called **underfitting**.
    We will look at different strategies that will help us in overcoming the overfitting
    and underfitting problems.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 特定算法在训练集上表现更好，但在验证或测试集上表现不佳的现象被称为**过拟合**，或者算法泛化能力不足。还有一个相反的现象，算法在训练集上表现不佳，这称为**欠拟合**。我们将看看不同的策略，帮助我们克服过拟合和欠拟合问题。
- en: Let's look at the various strategies available for splitting the dataset before
    looking at overfitting and underfitting.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论过拟合和欠拟合之前，让我们先看看在数据集分割方面的各种策略。
- en: Training, validation, and test split
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练、验证和测试分割
- en: 'It is best practice to split the data into three parts—training, validation,
    and test datasets. The best approach for using the holdout dataset is to:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分成三部分——训练集、验证集和测试集，是最佳实践。使用保留数据集的最佳方法是：
- en: Train the algorithm on the training dataset
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练算法
- en: Perform hyper parameter tuning based on the validation dataset
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于验证数据集执行超参数调优
- en: Perform the first two steps iteratively until the expected performance is achieved
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过迭代执行前两个步骤，直到达到预期的性能
- en: After freezing the algorithm and the hyper parameters, evaluate it on the test
    dataset
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在冻结算法和超参数后，在测试数据集上评估它
- en: 'Avoid splitting the data into two parts, as it may lead to an information leak.
    Training and testing it on the same dataset is a clear no-no as it does not guarantee
    algorithm generalization. There are three popular holdout strategies that can
    be used to split the data into training and validation sets. They are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 避免将数据分割成两部分，因为这可能导致信息泄露。在同一数据集上进行训练和测试是明确禁止的，因为它不能保证算法的泛化。有三种流行的保留策略可用于将数据分割为训练集和验证集。它们如下：
- en: Simple holdout validation
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单保留验证
- en: K-fold validation
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K 折验证
- en: Iterated k-fold validation
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代 k 折验证
- en: Simple holdout validation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单保留验证
- en: 'Set apart a fraction of the data as your test dataset. What fraction to keep
    may be very problem-specific and could largely depend on the amount of data available.
    For problems particularly in the fields of computer vision and NLP, collecting
    labeled data could be very expensive, so to hold out a large fraction of 30% may
    make it difficult for the algorithm to learn, as it will have less data to train
    on. So, depending on the data availability, choose the fraction of it wisely.
    Once the test data is split, keep it apart until you freeze the algorithm and
    its hyper parameters. For choosing the best hyper parameters for the problem,
    choose a separate validation dataset. To avoid overfitting, we generally divide
    available data into three different sets, as shown in following image:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据的一部分作为测试数据集。保留多少数据可能非常依赖于具体问题，并且很大程度上取决于可用的数据量。在计算机视觉和自然语言处理领域，特别是收集标记数据可能非常昂贵，因此保留
    30% 的大部分数据可能会使算法难以学习，因为训练数据较少。因此，根据数据的可用性，明智地选择它的一部分。一旦测试数据分割完成，在冻结算法及其超参数之前保持其独立。为了选择问题的最佳超参数，选择一个单独的验证数据集。为了避免过拟合，我们通常将可用数据分为三个不同的集合，如下图所示：
- en: '![](img/6a1f5457-52c5-4365-bed5-c7e4f4315177.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a1f5457-52c5-4365-bed5-c7e4f4315177.png)'
- en: 'We used a simple implementation of the preceding figure in the last chapter
    to create our validation set. Let''s look at a snapshot of the implementation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章节中使用了上述图示的简单实现来创建我们的验证集。让我们来看一下实现的快照：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is one of the simplest holdout strategies and is commonly used to start
    with. There is a disadvantage of using this with small datasets. The validation
    dataset or test dataset may not be statistically representative of the data at
    hand. We can easily recognize this by shuffling the data before holding out. If
    the results obtained are not consistent, then we need to use a better approach.
    To avoid this issue, we often end up using k-fold or iterated k-fold validation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的保留策略之一，通常用于起步。使用小数据集时会有一个缺点。验证集或测试集可能无法统计代表手头的数据。我们可以通过在保留前对数据进行洗牌来轻松识别这一点。如果获得的结果不一致，则需要使用更好的方法。为了避免这个问题，我们经常使用
    k 折或迭代 k 折验证。
- en: K-fold validation
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K 折验证
- en: 'Keep a fraction of the dataset for the test split, then divide the entire dataset
    into k-folds where k can be any number, generally varying from two to ten. At
    any given iteration, we hold one block for validation and train the algorithm
    on the rest of the blocks. The final score is generally the average of all the
    scores obtained across the k-folds. The following diagram shows an implementation
    of k-fold validation where k is four; that is, the data is split into four parts:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集的一部分保留用于测试拆分，然后将整个数据集分成 k 折，其中 k 可以是任意数字，通常在两到十之间变化。在任何给定的迭代中，我们保留一个块用于验证，并在其余块上训练算法。最终分数通常是在所有
    k 折中获得的所有分数的平均值。以下图示显示了 k 折验证的实现，其中 k 为四；也就是说，数据分为四个部分：
- en: '![](img/52552aac-517c-49b3-8986-04158c3d3e42.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52552aac-517c-49b3-8986-04158c3d3e42.png)'
- en: One key thing to note when using the k-fold validation dataset is that it is
    very expensive, because you run the algorithm several times on different parts
    of the dataset, which can turn out to be very expensive for computation-intensive
    algorithms—particularly in areas of computer vision algorithms, where, sometimes,
    training an algorithm could take anywhere from minutes to days. So, use this technique
    wisely.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 k 折验证数据集时需要注意的一个关键点是它非常昂贵，因为您需要在数据集的不同部分上运行算法多次，这对于计算密集型算法来说可能非常昂贵——特别是在计算机视觉算法的领域，在某些情况下，训练一个算法可能需要从几分钟到几天的时间。因此，明智地使用这种技术。
- en: K-fold validation with shuffling
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有洗牌的 k 折验证
- en: To make things complex and robust, you can shuffle the data every time you create
    your holdout validation dataset. It is very helpful for solving problems where
    a small boost in performance could have a huge business impact. If your case is
    to quickly build and deploy algorithms and you are OK with compromising a few
    percent in performance difference, then this approach may not be worth it. It
    all boils down to what problem you are trying to solve, and what accuracy means
    to you.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要使事情变得复杂和健壮，您可以在每次创建留存验证数据集时对数据进行洗牌。这对于解决那些小幅提升性能可能会产生巨大业务影响的问题非常有帮助。如果您的情况是快速构建和部署算法，并且对性能差异的几个百分点可以妥协，那么这种方法可能不值得。关键在于您试图解决的问题以及准确性对您意味着什么。
- en: 'There are a few other things that you may need to consider when splitting up
    the data, such as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在分割数据时还有一些其他需要考虑的事项，例如：
- en: Data representativeness
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据代表性
- en: Time sensitivity
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间敏感性
- en: Data redundancy
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据冗余
- en: Data representativeness
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据代表性
- en: In the example we saw in our last chapter, we classified images as either dogs
    or cats. Let's take a scenario where all the images are sorted and the first 60%
    of images are dogs and the rest are cats. If we split this dataset by choosing
    the first 80% as the training dataset and the rest as the validation set, then
    the validation dataset will not be a true representation of the dataset, as it
    will only contain cat images. So, in these cases, care should be taken that we
    have a good mix by shuffling the data before splitting or doing a stratified sampling.
    Stratified sampling refers to picking up data points from each category to create
    validation and test datasets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上一章中看到的例子中，我们将图像分类为狗或猫。让我们看一个情况，所有图像都已排序，前 60% 的图像是狗，剩下的是猫。如果我们通过选择前 80%
    作为训练数据集，剩下的作为验证集来拆分这个数据集，那么验证数据集将不是数据集的真实代表，因为它只包含猫的图像。因此，在这些情况下，应该小心地通过在拆分之前对数据进行洗牌或进行分层抽样来确保我们有一个良好的混合数据集。
- en: Time sensitivity
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间敏感性
- en: Let's take the case of predicting stock prices. We have data from January to
    December. In this case, if we do a shuffle or stratified sampling then we end
    up with an information leak, as the prices could be sensitive to time. So, create
    the validation dataset in such a way that there is no information leak. In this
    case, choosing the December data as the validation dataset could make more sense.
    In the case of stock prices it is more complex than this, so domain-specific knowledge
    also comes into play when choosing the validation split.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以预测股票价格为例。我们有从一月到十二月的数据。在这种情况下，如果我们进行洗牌或分层抽样，那么我们最终会出现信息泄漏，因为价格可能对时间敏感。因此，要以不会有信息泄漏的方式创建验证数据集。在这种情况下，选择十二月的数据作为验证数据集可能更合理。在股票价格的情况下，这比较复杂，因此在选择验证拆分时，领域专业知识也会起作用。
- en: Data redundancy
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据冗余
- en: Duplicates are common in data. Care should be taken so that the data present
    in the training, validation, and test sets are unique. If there are duplicates,
    then the model may not generalize well on unseen data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中常见重复。应确保训练、验证和测试集中的数据是唯一的。如果存在重复，则模型可能无法很好地推广到未见过的数据。
- en: Data preprocessing and feature engineering
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理和特征工程
- en: We have looked at different ways to split our datasets to build our evaluation
    strategy. In most cases, the data that we receive may not be in a format that
    can be readily used by us for training our algorithms. In this section, we will
    cover some of the preprocessing techniques and feature engineering techniques.
    Though most of the feature engineering techniques are domain-specific, particularly
    in the areas of computer vision and text, there are some common feature engineering
    techniques that are common across the board, which we will discuss in this chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了不同的方法来分割我们的数据集以建立评估策略。在大多数情况下，我们收到的数据可能不是我们可以直接用于训练算法的格式。在本节中，我们将介绍一些预处理技术和特征工程技术。虽然大部分特征工程技术是领域特定的，特别是在计算机视觉和文本领域，但也有一些通用的特征工程技术是跨领域通用的，我们将在本章讨论。
- en: 'Data preprocessing for neural networks is a process in which we make the data
    more suitable for the deep learning algorithms to train on. The following are
    some of the commonly-used data preprocessing steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 用于神经网络的数据预处理是使数据更适合深度学习算法进行训练的过程。以下是一些常用的数据预处理步骤：
- en: Vectorization
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量化
- en: Normalization
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化
- en: Missing values
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值
- en: Feature extraction
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: Vectorization
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化
- en: Data comes in various formats such as text, sound, images, and video. The very
    first thing that needs to be done is to convert the data into PyTorch tensors.
    In the previous example, we used `torchvision` utility functions to convert **Python
    Imaging Library** (**PIL**) images into a Tensor object, though most of the complexity
    is abstracted away by the PyTorch torchvision libraries. In [Chapter 7](dc189b2e-5166-41a5-8203-aff8b367b53c.xhtml),
    *Generative Networks*, when we deal with **recurrent neural networks** (**RNNs**),
    we will see how text data can be converted into PyTorch tensors. For problems
    involving structured data, the data is already present in a vectorized format;
    all we need to do is convert them into PyTorch tensors.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据以各种格式出现，如文本、声音、图像和视频。首先要做的事情是将数据转换为 PyTorch 张量。在先前的示例中，我们使用了 `torchvision`
    实用函数将**Python Imaging Library** (**PIL**) 图像转换为张量对象，尽管大部分复杂性都被 PyTorch torchvision
    库抽象化了。在[第7章](dc189b2e-5166-41a5-8203-aff8b367b53c.xhtml)，*生成网络*，当我们处理**递归神经网络**
    (**RNNs**) 时，我们将看到如何将文本数据转换为 PyTorch 张量。对于涉及结构化数据的问题，数据已经以向量化格式存在；我们只需将它们转换为 PyTorch
    张量即可。
- en: Value normalization
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值规范化
- en: It is a common practice to normalize features before passing the data to any
    machine learning algorithm or deep learning algorithm. It helps in training the
    algorithms faster and helps in achieving more performance. Normalization is the
    process in which you represent data belonging to a particular feature in such
    a way that its mean is zero and standard deviation is one.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据传递给任何机器学习算法或深度学习算法之前，将特征规范化是一种常见做法。它有助于更快地训练算法，并帮助实现更高的性能。标准化是一种过程，其中您以某种方式表示属于特定特征的数据，使其平均值为零，标准差为一。
- en: In the example of *dogs and cats*, the classification that we covered in the
    last chapter, we normalized the data by using the mean and standard deviation
    of the data available in the `ImageNet` dataset. The reason we chose the `ImageNet`
    dataset's mean and standard deviation for our example is that we are using the
    weights of the ResNet model, which was pretrained on ImageNet. It is also a common
    practice to divide each pixel value by 255 so that all the values fall in the
    range between zero and one, particularly when you are not using pretrained weights.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在*狗和猫*的例子中，我们在上一章中进行了分类，通过使用 `ImageNet` 数据集中可用数据的平均值和标准差来对数据进行标准化。我们选择 `ImageNet`
    数据集的平均值和标准差作为示例的原因是，我们使用了在 ImageNet 上预训练的 ResNet 模型的权重。通常也是一个常见做法，将每个像素值除以 255，以便所有值都落在零到一之间的范围内，特别是当您不使用预训练权重时。
- en: 'Normalization is also applied for problems involving structured data. Say we
    are working on a house price prediction problem—there could be different features
    that could fall in different scales. For example, distance to the nearest airport
    and the age of the house are variables or features that could be in different
    scales. Using them with neural networks as they are could prevent the gradients
    from converging. In simple words, loss may not go down as expected. So, we should
    be careful to apply normalization to any kind of data before training on our algorithms.
    To ensure that the algorithm or model performs better, ensure that the data follows
    the following characteristics:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化也适用于涉及结构化数据的问题。比如，我们正在处理一个房价预测问题，可能存在不同尺度的特征。例如，距离最近的机场和房屋年龄是可能处于不同尺度的变量或特征。直接将它们用于神经网络可能会阻止梯度的收敛。简单来说，损失可能不会按预期降低。因此，在训练算法之前，我们应该注意对任何数据应用标准化，以确保算法或模型表现更好。确保数据遵循以下特性：
- en: '**Take small values**: Typically in a range between zero and one'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**取小值**：通常在0到1的范围内'
- en: '**Same range**: Ensure all the features are in the same range'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相同范围**：确保所有特征都在相同的范围内'
- en: Handling missing values
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Missing values are quite common in real-world machine learning problems. From
    our previous examples of predicting house prices, certain fields for the age of
    the house could be missing. It is often safe to replace the missing values with
    a number that may not occur otherwise. The algorithms will be able to identify
    the pattern. There are other techniques that are available to handle missing values
    that are more domain-specific.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实世界的机器学习问题中，缺失值非常普遍。从我们之前预测房价的例子中可以看出，房屋年龄字段可能缺失。通常可以安全地用一个不会出现的数字替换缺失值。算法将能够识别出模式。还有其他更具领域特定性的技术可用于处理缺失值。
- en: Feature engineering
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'Feature engineering is the process of using domain knowledge about a particular
    problem to create new variables or features that can be passed to the model. To
    understand better, let''s look at a sales prediction problem. Say we have information
    about promotion dates, holidays, competitor''s start date, distance from competitor,
    and sales for a particular day. In the real world, there could be hundreds of
    features that may be useful in predicting the prices of stores. There could be
    certain information that could be important in predicting the sales. Some of the
    important features or derived values are:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是利用关于特定问题的领域知识来创建可以传递给模型的新变量或特征的过程。为了更好地理解，让我们看一个销售预测问题。假设我们有关于促销日期、假期、竞争对手的开始日期、距离竞争对手的距离和某一天销售额的信息。在现实世界中，可能有数百个可能对预测商店价格有用的特征。有些信息可能对预测销售很重要。一些重要的特征或派生值包括：
- en: Days until the next promotion
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离下一个促销活动的天数
- en: Days left before the next holiday
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离下一个假期的天数
- en: Number of days the competitor's business has been open
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞争对手业务开展的天数
- en: 'There could be many more such features that can be extracted that come from
    domain knowledge. Extracting these kinds of features for any machine learning
    algorithm or deep learning algorithm could be quite challenging for the algorithms
    to perform themselves. For certain domains, particularly in the fields of computer
    vision and text, modern deep learning algorithms help us in getting away with
    feature engineering. Except for these fields, good feature engineering always
    helps in the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 可以提取许多来自领域知识的特征。提取这些类型的特征对于任何机器学习算法或深度学习算法来说都可能是相当具有挑战性的。对于某些领域，特别是在计算机视觉和文本领域，现代深度学习算法帮助我们摆脱特征工程的限制。除了这些领域外，良好的特征工程始终有助于以下方面：
- en: The problem can be solved a lot faster with less computational resource.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用更少的计算资源更快地解决问题。
- en: The deep learning algorithms can learn features without manually engineering
    them by using huge amounts of data. So, if you are tight on data, then it is good
    to focus on good feature engineering.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法可以通过使用大量数据来学习特征，而无需手动工程化它们。因此，如果数据紧张，那么专注于良好的特征工程是有益的。
- en: Overfitting and underfitting
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: Understanding overfitting and underfitting is the key to building successful
    machine learning and deep learning models. At the start of the chapter, we briefly
    covered what underfitting and overfitting are; let's take a look at them in detail
    and how we can solve them.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 理解过拟合和欠拟合是构建成功的机器学习和深度学习模型的关键。在本章的开头，我们简要介绍了欠拟合和过拟合的概念；让我们详细看看它们以及我们如何解决它们。
- en: 'Overfitting, or not generalizing, is a common problem in machine learning and
    deep learning. We say a particular algorithm overfits when it performs well on
    the training dataset but fails to perform on unseen or validation and test datasets.
    This mostly occurs due to the algorithm identifying patterns that are too specific
    to the training dataset. In simpler words, we can say that the algorithm figures
    out a way to memorize the dataset so that it performs really well on the training
    dataset and fails to perform on the unseen data. There are different techniques
    that can be used to avoid the algorithm overfitting. Some of the techniques are:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和深度学习中，过拟合或不能泛化是一个常见问题。我们说一个特定的算法过拟合是指它在训练数据集上表现良好，但在未见过的验证和测试数据集上表现不佳。这主要是由于算法识别出的模式过于特定于训练数据集。简单来说，我们可以说算法找到了一种记住数据集的方式，以便在训练数据集上表现非常好，但在未见数据上表现不佳。有不同的技术可以用来避免算法过拟合。其中一些技术包括：
- en: Getting more data
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得更多数据
- en: Reducing the size of the network
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减小网络的大小
- en: Applying weight regularizer
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用权重正则化器
- en: Applying dropout
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用 dropout
- en: Getting more data
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获得更多数据
- en: If you are able to get more data on which the algorithm can train, that can
    help the algorithm to avoid overfitting by focusing on general patterns rather
    than on patterns specific to small data points. There are several cases where
    getting more labeled data could be a challenge.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够获取更多可以训练算法的数据，这将有助于算法避免过拟合，因为它会专注于一般模式而不是小数据点特定的模式。有几种情况可能会使获取更多标记数据成为一个挑战。
- en: There are techniques, such as data augmentation, that can be used to generate
    more training data in problems related to computer vision. Data augmentation is
    a technique where you can adjust the images slightly by performing different actions
    such as rotating, cropping, and generating more data. With enough domain understanding,
    you can create synthetic data too if capturing actual data is expensive. There
    are other ways that can help to avoid overfitting when you are unable to get more
    data. Let's look at them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些技术，比如数据增强，在与计算机视觉相关的问题中可以用来生成更多的训练数据。数据增强是一种技术，你可以通过执行不同的动作如旋转、裁剪来微调图像，并生成更多的数据。有了足够的领域理解，你甚至可以创建合成数据，如果捕获实际数据是昂贵的话。当你无法获取更多数据时，还有其他方法可以帮助避免过拟合。让我们来看看它们。
- en: Reducing the size of the network
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减小网络的大小
- en: 'The size of the network in general refers to the number of layers or the number
    of weight parameters used in a network. In the example of image classification
    that we saw in the last chapter, we used a ResNet model that has 18 blocks consisting
    of different layers inside it. The torchvision library in PyTorch comes with ResNet
    models of different sizes starting from 18 blocks and going up to 152 blocks.
    Say, for example, if we are using a ResNet block with 152 blocks and the model
    is overfitting, then we can try using a ResNet with 101 blocks or 50 blocks. In
    the custom architectures we build, we can simply remove some intermediate linear
    layers, thus preventing our PyTorch models from memorizing the training dataset.
    Let''s look at an example code snippet that demonstrates what it means exactly
    to reduce the network size:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的大小通常指网络中使用的层数或权重参数的数量。在我们上一章节看到的图像分类示例中，我们使用了一个 ResNet 模型，它有 18 个块，包含不同的层。PyTorch
    中的 torchvision 库提供了不同大小的 ResNet 模型，从 18 个块一直到 152 个块。举个例子，如果我们使用一个包含 152 个块的 ResNet
    块并且模型出现了过拟合，那么我们可以尝试使用具有 101 个块或 50 个块的 ResNet。在我们构建的自定义架构中，我们可以简单地删除一些中间线性层，从而防止我们的
    PyTorch 模型记住训练数据集。让我们看一个示例代码片段，展示了如何减小网络大小的具体含义：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding architecture has three linear layers, and let''s say it overfits
    our training data. So, let''s recreate the architecture with reduced capacity:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构有三个线性层，假设它过拟合了我们的训练数据。所以，让我们重新创建一个具有减少容量的架构：
- en: '[PRE2]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding architecture has only two linear layers, thus reducing the capacity
    and, in turn, potentially avoiding overfitting the training dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构只有两个线性层，因此降低了容量，从而潜在地避免了训练数据集的过拟合。
- en: Applying weight regularization
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用权重正则化
- en: 'One of the key principles that helps to solve the problem of overfitting or
    generalization is building simpler models. One technique for building simpler
    models is to reduce the complexity of the architecture by reducing its size. The
    other important thing is ensuring that the weights of the network do not take
    larger values. Regularization provides constraints on the network by penalizing
    the model when the weights of the model are larger. Whenever the model uses larger
    weights, the regularization kicks in and increases the loss value, thus penalizing
    the model. There are two types of regularization possible. They are:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 解决过拟合或泛化问题的一个关键原则是构建更简单的模型。一种构建更简单模型的技术是通过减少其结构的复杂性来降低其大小。另一个重要的事情是确保网络的权重不要取得较大的值。正则化通过对网络施加约束，当模型的权重较大时会对其进行惩罚。正则化有两种可能的类型。它们是：
- en: '**L1 regularization**: The sum of absolute values of weight coefficients are
    added to the cost. It is often referred to as the L1 norm of the weights.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1 正则化**：权重系数的绝对值之和被添加到成本中。通常称为权重的 L1 范数。'
- en: '**L2 regularization**: The sum of squares of all weight coefficients are added
    to the cost. It is often referred to as the L2 norm of the weights.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 正则化**：所有权重系数的平方和被添加到成本中。通常称为权重的 L2 范数。'
- en: 'PyTorch provides an easy way to use L2 regularization by enabling the `weight_decay`
    parameter in the optimizer:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了一种简单的方法来使用 L2 正则化，通过在优化器中启用 `weight_decay` 参数：
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By default, the weight decay parameter is set to zero. We can try different
    values for weight decay; a small value such as `1e-5` works most of the time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，权重衰减参数被设置为零。我们可以尝试不同的权重衰减值；例如 `1e-5` 这样的小值通常效果很好。
- en: Dropout
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: 'Dropout is one of the most commonly used and the most powerful regularization
    techniques used in deep learning. It was developed by Hinton and his students
    at the University of Toronto. Dropout is applied to intermediate layers of the
    model during the training time. Let''s look at an example of how dropout is applied
    on a linear layer''s output that generates 10 values:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是深度学习中最常用且最强大的正则化技术之一。它是由 Hinton 及其在多伦多大学的学生开发的。Dropout 被应用于模型的中间层，在训练时使用。让我们看一个例子，说明如何在生成
    10 个值的线性层输出上应用 dropout：
- en: '![](img/ba017512-24f1-4da2-9703-541556e1da4b.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba017512-24f1-4da2-9703-541556e1da4b.png)'
- en: 'The preceding figure shows what happens when dropout is applied to the linear
    layer output with a threshold value of **0.2**. It randomly masks or zeros 20%
    of data, so that the model will not be dependent on a particular set of weights
    or patterns, thus overfitting. Let''s look at another example where we apply a
    dropout with a threshold value of **0.5**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了当在线性层输出上应用 dropout 时，阈值为 **0.2** 的情况。它随机屏蔽或将数据置零 20%，因此模型不会依赖于特定的权重集或模式，从而避免过拟合。让我们看另一个例子，其中我们使用阈值为
    **0.5** 的 dropout：
- en: '![](img/3976da11-f63f-46f1-9398-e0723c74eb82.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3976da11-f63f-46f1-9398-e0723c74eb82.png)'
- en: 'It is often common to use a threshold of dropout values in the range of 0.2
    to 0.5, and the dropout is applied at different layers. Dropouts are used only
    during the training times, and during the testing values are scaled down by the
    factor equal to the dropout. PyTorch provides dropout as another layer, thus making
    it easier to use. The following code snippet shows how to use a dropout layer
    in PyTorch:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通常使用在 0.2 到 0.5 范围内的 dropout 值的阈值，dropout 应用在不同的层。Dropout 仅在训练时使用，测试时值按 dropout
    的因子进行缩放。PyTorch 提供 dropout 作为另一层，从而更容易使用。下面的代码片段显示了如何在 PyTorch 中使用 dropout 层：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The dropout layer accepts an argument called `training`, which needs to be set
    to `True` during the training phase and false during the validation or test phase.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: dropout 层接受一个名为 `training` 的参数，它在训练阶段需要设置为 `True`，在验证或测试阶段设置为 `False`。
- en: Underfitting
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合
- en: There are times when our model may fail to learn any patterns from our training
    data, which will be quite evident when the model fails to perform well even on
    the dataset it is trained on. One common thing to try when your model underfits
    is to acquire more data for the algorithm to train on. Another approach is to
    increase the complexity of the model by increasing the number of layers or by
    increasing the number of weights or parameters used by the model. It is often
    a good practice not to use any of the aforementioned regularization techniques
    until we actually overfit the dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们的模型可能无法从训练数据中学习到任何模式，这在模型甚至在训练集上表现不佳时会非常明显。当您的模型欠拟合时，一种常见的解决方法是获取更多的数据让算法进行训练。另一种方法是通过增加层次或增加模型使用的权重或参数来增加模型的复杂性。在实际过拟合数据集之前最好不要使用上述任何正则化技术。
- en: Workflow of a machine learning project
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习项目的工作流程
- en: In this section, we will formalize a solution framework that can be used to
    solve any machine learning problem by bringing together the problem statement,
    evaluation, feature engineering, and avoidance of overfitting.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将制定一个解决方案框架，可以通过整合问题陈述、评估、特征工程以及避免过拟合来解决任何机器学习问题。
- en: Problem definition and dataset creation
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义和数据集创建
- en: To define the problem, we need two important things; namely, the input data
    and the type of problem.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义问题，我们需要两个重要的东西；即输入数据和问题类型。
- en: What will be our input data and target labels? For example, say we want to classify
    restaurants based on their speciality—say Italian, Mexican, Chinese, and Indian
    food—from the reviews given by the customers. To start working with this kind
    of problem, we need to manually hand annotate the training data as one of the
    possible categories before we can train the algorithm on it. Data availability
    is often a challenging factor at this stage.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输入数据和目标标签将会是什么？例如，假设我们想根据顾客的评价将餐馆分类为意大利餐厅、墨西哥餐厅、中国餐厅和印度餐厅。在开始处理这类问题之前，我们需要手动为训练数据中的一个可能的类别进行标注，然后才能对算法进行训练。数据的可用性在这个阶段通常是一个具有挑战性的因素。
- en: Identifying the type of problem will help in deciding whether it is a binary
    classification, multi-classification, scalar regression (house pricing), or vector
    regression (bounding boxes). Sometimes, we may have to use some of the unsupervised
    techniques such as clustering and dimensionality reduction. Once the problem type
    is identified, then it becomes easier to determine what kind of architecture,
    loss function, and optimizer should be used.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 确定问题的类型有助于确定它是二元分类、多类分类、标量回归（房价预测）还是向量回归（边界框）。有时，我们可能需要使用一些无监督技术，如聚类和降维。一旦确定了问题类型，就更容易确定应该使用什么样的架构、损失函数和优化器。
- en: 'Once we have the inputs and have identified the type of the problem, then we
    can start building our models with the following assumptions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了输入并且确定了问题的类型，那么我们可以根据以下假设开始构建我们的模型：
- en: There are hidden patterns in the data that can help map the input with the output
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中存在隐藏的模式，可以帮助将输入与输出进行映射
- en: The data that we have is sufficient for the model to learn
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们拥有的数据足以让模型进行学习
- en: As machine learning practitioners, we need to understand that we may not be
    able to build a model with just some input data and target data. Let's take predicting
    stock prices as an example. Let's assume we have features representing historical
    prices, historical performance, and competition details, but we may still fail
    to build a meaningful model that can predict stock prices, as stock prices could
    actually be influenced by a variety of other factors such as the domestic political
    scenario, international political scenario, natural factors such as having a good
    monsoon, and many other factors that may not be represented by our input data.
    So, there is no way that any machine learning or deep learning model would be
    able to identify patterns. So, based on the domain, carefully pick features that
    can be real indicators of the target variable. All these could be reasons for
    the models to underfit.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习从业者，我们需要明白，仅凭一些输入数据和目标数据可能无法构建出一个模型。以预测股票价格为例。假设我们有代表历史价格、历史表现和竞争详情的特征，但我们可能仍然无法构建出一个能够预测股票价格的有意义模型，因为股票价格实际上可能受到多种其他因素的影响，如国内政治形势、国际政治形势、天气因素（例如良好的季风）等，这些因素可能不会被我们的输入数据所代表。因此，没有任何机器学习或深度学习模型能够识别出模式。因此，根据领域的不同，精心选择能够成为目标变量真实指标的特征。所有这些都可能是模型欠拟合的原因。
- en: There is another important assumption that machine learning makes. Future or
    unseen data will be close to the patterns, as described by the historical data.
    Sometimes, our models could fail, as the patterns never existed in the historical
    data, or the data on which the model was trained did not cover certain seasonalities
    or patterns.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习还做了另一个重要的假设。未来或未见过的数据将接近于历史数据所描述的模式。有时，我们的模型可能失败，因为这些模式在历史数据中从未存在过，或者模型训练时的数据未涵盖某些季节性或模式。
- en: Measure of success
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成功的衡量标准
- en: The measure of success will be directly determined by your business goal. For
    example, when trying to predict when the next machine failure will occur in windmills,
    we would be more interested to know how many times the model was able to predict
    the failures. Using simple accuracy can be the wrong metric, as most of the time
    the model will predict correctly when the machine will not fail, as that is the
    most common output. Say we get an accuracy of 98%, and the model was wrong each
    time in predicting the failure rate—such models may not be of any use in the real
    world. Choosing the correct measure of success is crucial for business problems.
    Often, these kinds of problems have imbalanced datasets.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的衡量标准将直接由您的业务目标决定。例如，当尝试预测风车何时会发生下次机器故障时，我们更关心模型能够预测故障的次数。使用简单的准确率可能是错误的度量标准，因为大多数情况下，模型在预测机器不会故障时会预测正确，这是最常见的输出。假设我们获得了98%的准确率，并且模型在预测故障率时每次都错误——这样的模型在现实世界中可能毫无用处。选择正确的成功度量标准对于业务问题至关重要。通常，这类问题具有不平衡的数据集。
- en: For balanced classification problems, where all the classes have a likely accuracy,
    ROC and **Area under the curve** (**AUC**) are common metrics. For imbalanced
    datasets, we can use precision and recall. For ranking problems, we can use mean
    average precision.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于平衡分类问题，所有类别的准确率相似时，ROC和**曲线下面积**（**AUC**）是常见的度量标准。对于不平衡的数据集，我们可以使用精确率和召回率。对于排名问题，可以使用平均精度。
- en: Evaluation protocol
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估协议
- en: 'Once you decide how you are going to evaluate the current progress, it is important
    to decide how you are going to evaluate on your dataset. We can choose from the
    three different ways of evaluating our progress:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了如何评估当前进展，决定如何在数据集上进行评估就变得很重要。我们可以从以下三种不同的评估方式中进行选择：
- en: '**Holdout validation set**: Most commonly used, particularly when you have
    enough data'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**留出验证集**：最常用的方法，特别是在你有足够的数据时。'
- en: '**K-fold cross validation**: When you have limited data, this strategy helps
    you to evaluate on different portions of the data, helping to give us a better
    view of the performance'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K折交叉验证**：当数据有限时，这种策略有助于在数据的不同部分上进行评估，有助于更好地了解性能。'
- en: '**Iterated k-fold validation**: When you are looking to go the extra mile with
    the performance of the model, this approach will help'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复K折验证**：当您希望模型性能更上一层楼时，这种方法会很有帮助。'
- en: Prepare your data
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备你的数据
- en: Bring different formats of available data into tensors through vectorization
    and ensure that all the features are scaled and normalized.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将可用数据的不同格式通过向量化转换为张量，并确保所有特征都被缩放和归一化。
- en: Baseline model
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准模型
- en: Create a very simple model that beats the baseline score. In our previous example
    of dogs and cats, classification, the baseline accuracy should be 0.5 and our
    simple model should be able to beat this score. If we are not able to beat the
    baseline score, then maybe the input data does not hold the necessary information
    required to make the necessary prediction. Remember not to introduce any regularization
    or dropouts at this step.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个非常简单的模型，能够击败基准分数。在我们先前的狗和猫分类的例子中，基准准确率应为0.5，我们的简单模型应能够超过这个分数。如果我们无法击败基准分数，那么可能输入数据不包含进行必要预测所需的信息。请记住，在此步骤中不要引入任何正则化或丢弃。
- en: 'To make the model work, we have to make three important choices:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要使模型工作，我们必须做出三个重要选择：
- en: '**Choice of last layer**: For a regression, it should be a linear layer generating
    a scalar value as output. For a vector regression problem, it would be the same
    linear layer generating more than one scalar output. For a bounding box, it outputs
    four values. For a binary classification, it is often common to use sigmoid, and
    for multi-class classification it is softmax.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最后一层的选择**: 对于回归问题，应该是一个生成标量值作为输出的线性层。对于矢量回归问题，将是生成多个标量输出的相同线性层。对于边界框，它输出四个值。对于二元分类，通常使用sigmoid，而对于多类分类，则使用softmax。'
- en: '**Choice of loss function**: The type of the problem will help you in deciding
    the loss function. For a regression problem, such as predicting house prices,
    we use the mean squared error, and for classification problems we use categorical
    cross entropy.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数的选择**: 问题的类型将帮助您决定损失函数。对于回归问题，如预测房价，我们使用均方误差（MSE），而对于分类问题，我们使用分类交叉熵。'
- en: '**Optimization**: Choosing the right optimization algorithm and some of its
    hyper parameters is quite tricky, and we can find them by experimenting with different
    ones. For most of the use cases, an Adam or RMSprop optimization algorithm works
    better. We will cover some of the tricks that can be used for learning rate selection.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化**: 选择正确的优化算法及其一些超参数相当棘手，我们可以通过尝试不同的算法来找到它们。对于大多数用例，Adam或RMSprop优化算法效果更好。我们将涵盖一些用于学习率选择的技巧。'
- en: 'Let''s summarize what kind of loss function and activation function we would
    use for the last layer of the network in our deep learning algorithms:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下在我们的深度学习算法网络的最后一层中，我们将使用什么样的损失函数和激活函数：
- en: '| **Problem type** | **Activation function** | **Loss function** |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| **问题类型** | **激活函数** | **损失函数** |'
- en: '| Binary classification | Sigmoid activation | `nn.CrossEntropyLoss()` |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 二元分类 | sigmoid激活 | `nn.CrossEntropyLoss()` |'
- en: '| Multi-class classification | Softmax activation | `nn.CrossEntropyLoss()`
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 多类分类 | softmax激活 | `nn.CrossEntropyLoss()` |'
- en: '| Multi-label classification | Sigmoid activation | `nn.CrossEntropyLoss()`
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 多标签分类 | sigmoid激活 | `nn.CrossEntropyLoss()` |'
- en: '| Regression | None | MSE |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 回归 | 无 | 均方误差（MSE） |'
- en: '| Vector regression | None | MSE |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 矢量回归 | 无 | 均方误差（MSE） |'
- en: Large model enough to overfit
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 足够大的模型来过拟合
- en: 'Once you have a model that has enough capacity to beat your baseline score,
    increase your baseline capacity. A few simple tricks to increase the capacity
    of your architecture are as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有一个具有足够容量以打败基准分数的模型，增加您的基准容量。增加架构容量的几个简单技巧如下：
- en: Add more layers to your existing architecture
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向现有架构添加更多层
- en: Add more weights to the existing layers
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向现有层添加更多权重
- en: Train it for more epochs
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其训练更多个周期
- en: We generally train the model for an adequate number of epochs. Stop it when
    the training accuracy keeps increasing and the validation accuracy stops increasing
    and probably starts dropping; that's where the model starts overfitting. Once
    we reach this stage, we need to apply regularization techniques.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会对模型进行充分的训练周期。当训练精度持续增加而验证精度停止增加并可能开始下降时，这就是模型开始过拟合的地方。一旦达到这个阶段，我们需要应用正则化技术。
- en: Remember, the number of layers, size of layers, and number of epochs may change
    from problem to problem. A smaller architecture can work for a simple classification
    problem, but for a complex problem such as facial recognition, we would need enough
    expressiveness in our architecture and the model needs to be trained for more
    epochs than for a simple classification problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，层数、层大小和 epochs 数可能会因问题而异。对于简单的分类问题，较小的架构可以工作，但对于像面部识别这样的复杂问题，我们需要在架构中具有足够的表达能力，并且模型需要进行比简单分类问题更多的
    epochs 训练。
- en: Applying regularization
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用正则化
- en: 'Finding the best way to regularize the model or algorithm is one of the trickiest parts
    of the process, since there are a lot of parameters to be tuned. Some of the parameters
    that we can tune to regularize the model are:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 找到正则化模型或算法的最佳方法是整个过程中最棘手的部分之一，因为有很多参数需要调整。我们可以调整的一些正则化模型的参数包括：
- en: '**Adding dropout**: This can be complex as this can be added between different
    layers, and finding the best place is usually done through experimentation. The
    percentage of dropout to be added is also tricky, as it is purely dependent on
    the problem statement we are trying to solve. It is often good practice to start
    with a small number such as 0.2.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加 dropout**：这可能会很复杂，因为它可以添加在不同的层之间，找到最佳位置通常是通过实验来完成的。要添加的 dropout 百分比也很棘手，因为它完全依赖于我们试图解决的问题陈述。通常的良好做法是从小的数字开始，比如
    0.2。'
- en: '**Trying different architectures**: We can try different architectures, activation
    functions, numbers of layers, weights, or parameters inside the layers.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尝试不同的架构**：我们可以尝试不同的架构、激活函数、层数、权重或层内参数。'
- en: '**Adding L1 or L2 regularization**: We can make use of either one of regularization.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加 L1 或 L2 正则化**：我们可以使用其中一种正则化。'
- en: '**Trying different learning rates**: There are different techniques that can
    be used, which we will discuss in the later sections of the chapter.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尝试不同的学习率**：有不同的技术可以使用，我们将在本章后面的部分讨论这些技术。'
- en: '**Adding more features or more data**: This is probably done by acquiring more
    data or augmenting data.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加更多特征或更多数据**：这可能通过获取更多数据或增强数据来完成。'
- en: We will use our validation dataset to tune all the aforementioned hyper parameters.
    As we keep iterating and tweaking the hyper parameters, we may end up with the
    problem of data leakage. So, we should ensure that we have holdout data for testing.
    If the performance of the model on the test data is good in comparison to the
    training and validation, then there is a good chance that our model will perform
    well on unseen data. But, if the model fails to perform on the test data but performs
    on the validation and training data, then there is a chance that the validation
    data is not a good representation of the real-world dataset. In such scenarios,
    we can end up using k-fold validation or iterated k-fold validation datasets.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用验证数据集来调整所有上述超参数。随着我们不断迭代和调整超参数，我们可能会遇到数据泄漏的问题。因此，我们应确保我们有保留数据用于测试。如果模型在测试数据上的性能比训练和验证数据好，那么我们的模型很可能在未见过的数据上表现良好。但是，如果模型在测试数据上表现不佳，而在验证和训练数据上表现良好，则验证数据可能不是真实世界数据集的良好代表。在这种情况下，我们可以使用
    k 折交叉验证或迭代 k 折交叉验证数据集。
- en: Learning rate picking strategies
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率选择策略
- en: 'Finding the right learning rate for training the model is an ongoing area of
    research where a lot of progress has been made. PyTorch provides some of the techniques
    to tune the learning rate, and they are provided in the `torch.optim.lr_sheduler`
    package. We will explore some of the techniques that PyTorch provides to choose
    the learning rates dynamically:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 找到适合训练模型的正确学习率是一个持续研究的领域，在这个领域取得了很多进展。PyTorch 提供了一些技术来调整学习率，在 `torch.optim.lr_scheduler`
    包中提供了这些技术。我们将探讨一些 PyTorch 提供的动态选择学习率的技术：
- en: '**StepLR**: This scheduler takes two important parameters. One is step size,
    which denotes for what number of epochs the learning rate has to change, and the
    second parameter is gamma, which decides how much the learning rate has to be
    changed.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StepLR**：这个调度程序有两个重要参数。一个是步长，它表示学习率必须变化的 epochs 数，另一个参数是 gamma，它决定学习率要变化多少。'
- en: 'For a learning rate of `0.01`, step size of 10, and gamma size of `0.1`, for
    every 10 epochs the learning rate changes by gamma times. That is, for the first
    10 epochs, the learning rate changes to 0.001, and by the end, on the next 10
    epochs, it changes to 0.0001. The following code explains the implementation of
    `StepLR`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 对于学习率为`0.01`，步长为10，以及`0.1`的gamma值，在每10个epochs，学习率会按gamma倍数变化。也就是说，在前10个epochs中，学习率会变为0.001，在接下来的10个epochs末尾，学习率会变为0.0001。以下代码解释了`StepLR`的实现：
- en: '[PRE5]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**MultiStepLR**: MultiStepLR works similarly to StepLR, except for the fact
    that the steps are not at regular intervals; steps are given as lists. For example,
    it is given as a list of 10, 15, 30, and for each step value, the learning rate
    is multiplied by its gamma value. The following code explains the implementation
    of `MultiStepLR`:'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MultiStepLR**：MultiStepLR的工作方式类似于StepLR，不同之处在于步长不是在规则间隔内的，而是以列表形式给出。例如，给定步长列表为10、15、30，对于每个步长值，学习率将乘以其gamma值。以下代码解释了`MultiStepLR`的实现：'
- en: '[PRE6]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**ExponentialLR**: This sets the learning rate to a multiple of the learning
    rate with gamma values for each epoch.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ExponentialLR**：这将学习率设置为每个epoch的学习率与gamma值的倍数。'
- en: '**ReduceLROnPlateau**: This is one of the commonly used learning rate strategies.
    In this case, the learning rate changes when a particular metric, such as training
    loss, validation loss, or accuracy stagnates. It is a common practice to reduce
    the learning rate by two to 10 times its original value. `ReduceLROnPlateau` can
    be implemented as follows:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReduceLROnPlateau**：这是常用的学习率调整策略之一。在这种情况下，当特定指标（如训练损失、验证损失或准确率）停滞不前时，学习率会进行调整。通常会将学习率降低到其原始值的两到十倍。`ReduceLROnPlateau`的实现如下：'
- en: '[PRE7]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered some of the common and best practices that are used
    in solving machine learning or deep learning problems. We covered various important
    steps such as creating problem statements, choosing the algorithm, beating the
    baseline score, increasing the capacity of the model until it overfits the dataset,
    applying regularization techniques that can prevent overfitting, increasing the
    generalization capacity, tuning different parameters of the model or algorithms,
    and exploring different learning strategies that can be used to train deep learning
    models optimally and faster.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了解决机器学习或深度学习问题中常见和最佳实践。我们涵盖了诸如创建问题陈述、选择算法、击败基准分数、增加模型容量直到过拟合数据集、应用可以防止过拟合的正则化技术、增加泛化能力、调整模型或算法的不同参数以及探索可以优化和加快深度学习模型训练的不同学习策略等各种重要步骤。
- en: In the next chapter, we will cover different components that are responsible
    for building state-of-the-art **Convolutional Neural Networks** (**CNNs**). We
    will also cover transfer learning, which helps us to train image classifiers when
    little data is available. We will also cover techniques that help us to train
    these algorithms more quickly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将涵盖构建最先进的**卷积神经网络**（**CNNs**）所需的不同组件。我们还将涵盖迁移学习，这有助于在数据有限时训练图像分类器。我们还将涵盖帮助我们更快地训练这些算法的技术。
