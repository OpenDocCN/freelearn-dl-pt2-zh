- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Emerging Applications in Generative AI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新兴的生成型人工智能应用
- en: 'In the preceding chapters, we have examined a large number of applications
    using generative AI, from generating pictures and text to even music. However,
    this is a large and ever-expanding field; the number of publications on Google
    Scholar matching a search for "generative adversarial networks" is 27,200, of
    which 16,200 were published in 2020! This is astonishing for a field that essentially
    started in 2014, the exponential growth of which can also be appreciated on the
    Google n-gram viewer (*Figure 13.1*):'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们研究了使用生成型人工智能的大量应用，包括生成图片和文字，甚至音乐。然而，这是一个庞大且不断扩张的领域；在 Google 学术搜索中，与“生成对抗网络”匹配的论文数量为
    27,200 篇，其中有 16,200 篇发表于 2020 年！对于一个从根本上始于 2014 年的领域来说，这是令人惊讶的指数增长，这也可以在 Google
    n-gram 查看器中得到体现（*图 13.1*）：
- en: '![](img/B16176_13_01.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_01.png)'
- en: 'Figure 13.1: Google n-gram of "generative adversarial networks"'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：Google n-gram of "generative adversarial networks"
- en: 'As we saw in this volume, generative adversarial networks are only one class
    of models in the broader field of generative AI, which also includes models such
    as variational autoencoders, BERT, and GPT-3\. As a single book cannot hope to
    cover all of these areas, we conclude this volume with discussion of a number
    of emerging topics in this field: drug discovery and protein folding, solving
    mathematical equations, generating video from images, and generating recipes.
    Interested readers are encouraged to consult the referenced literature for more
    detailed discussion of each topic.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本卷中看到的，生成对抗网络只是更广泛的生成型人工智能领域中一类模型，该领域还包括变分自动编码器、BERT 和 GPT-3 等模型。因为单个书籍无法覆盖所有这些领域，所以我们在这一卷中以讨论该领域中的一些新兴主题来结束本卷：药物发现和蛋白质折叠、求解数学方程、从图像生成视频，以及生成食谱。鼓励感兴趣的读者参考所引用的文献，以获得对每个主题的更详细讨论。
- en: Finding new drugs with generative models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用生成模型发现新药物
- en: 'One field that we have not covered in this volume in which generative AI is
    making a large impact is biotechnology research. We discuss two areas: drug discovery
    and predicting the structure of proteins.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本卷中未涉及的一个领域，即生成型人工智能正在对生物技术研究产生巨大影响。我们讨论两个领域：药物发现和预测蛋白质结构。
- en: Searching chemical space with generative molecular graph networks
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用生成分子图网络搜索化学空间
- en: At its base, a medicine – be it drugstore aspirin or an antibiotic prescribed
    by a doctor – is a *chemical graph* consisting of nodes (atoms) and edges (bonds)
    (*Figure 13.2*). Like the generative models used for textual data (*Chapters 3*,
    *9*, and *10*), graphs have the special property of not being fixed length. There
    are many ways to encode a graph, including a binary representation based on numeric
    codes for the individual fragments (*Figure 13.2*) and "SMILES" strings that are
    linearized representations of 3D molecules (*Figure 13.3*). You can probably appreciate
    that the number of potential features in a chemical graph is quite large; in fact,
    the number of potential chemical structures that are in the same size and property
    range as known drugs has been estimated¹ at 10^(60) – larger even than the number
    of papers on generative models; for reference, the number of molecules in the
    observable universe² is around 10^(78).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在根本上，一种药物——无论是药店里的阿司匹林还是医生开的抗生素——都是由节点（原子）和边（键）组成的*化学图*（*图 13.2*）。与用于文本数据的生成模型（*第
    3、9 和 10 章*）一样，图形具有不固定长度的特殊属性。有许多方法可以对图进行编码，包括基于单个片段的数值代码的二进制表示法（*图 13.2*）以及"SMILES"字符串，它们是
    3D 分子的线性表示法（*图 13.3*）。你可能会意识到，化学图中潜在特征的数量是非常庞大的；事实上，已经估计¹化学结构的潜在数量达到了 10^60，这甚至比生成模型的论文数量还要大；作为参考，可观测宇宙中分子的数量²约为
    10^78。
- en: '![](img/B16176_13_02.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_02.png)'
- en: 'Figure 13.2: The chemical graph³'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：化学图³
- en: One can appreciate, then, that a large challenge of drug discovery – finding
    new drugs for existing and emerging diseases – is the sheer size of the potential
    space one might need to search. While experimental approaches for "drug screening"
    – testing thousands, millions, or even billions of compounds in high-throughput
    experiments to find a chemical needle in a haystack with potential therapeutic
    properties – have been used for decades, the development of computational methods
    such as machine learning has opened the door for "virtual screening" on a far
    larger scale. Using modern computational approaches, scientists can test huge
    libraries of completely virtual compounds for their ability to interact with proteins
    of interest for disease research. How can a large library of such virtual molecules
    be generated?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人们因此可以理解，药物发现的一大挑战——为现有和新出现的疾病寻找新药物——是需要搜索潜在空间的规模之大。虽然实验性方法"药物筛选"——在高通量实验中测试成千上万甚至数十亿的化合物，以寻找具有潜在治疗特性的化学"稻草堆"中的化学针——已经被用了几十年，但是机器学习等计算方法的发展为规模更大的"虚拟筛选"打开了大门。使用现代计算方法，科学家可以测试完全虚拟化合物库，看它们是否具有与疾病研究中感兴趣的蛋白质相互作用的能力。那么，如何生成这样一个大型的虚拟分子库呢？
- en: '![](img/B16176_13_03.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_03.png)'
- en: 'Figure 13.3: A generative model for small molecule generation⁶'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3：小分子的生成模型⁶
- en: Returning to our encoding of the chemical graph, it's probably not surprising
    that we can use a generative model based on recursive neural networks, such as
    LSTMs, to sample from the huge space of possible molecular graph configurations.
    Because molecules follow particular structural motifs, this problem is more complex
    than simply independently sampling sets of atoms, as they must form a coherent
    molecule following chemical structural constraints. *Figure 13.3* illustrates
    what this process might look like; taking a 2D structure, encoding it into binary
    feature vectors (not unlike how we've seen text represented in earlier chapters),
    then running these vectors through a recursive neural network to train a model
    that predicts the next atom or bond based on the prior atoms/bonds in the sequence.
    Once the model is trained on input data, it can be utilized to generate new molecules
    by sampling new structures, one at a time, from this generator. Variational autoencoders
    (*Chapter 5*, *Painting Pictures with Neural Networks using VAEs*) have also been
    used to generate molecules⁴ (*Figure 13.4*), as have generative adversarial networks⁵
    (*Figure 13.5*), which we introduced in *Chapter 6*, *Image Generation with GANs*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回到化学图的编码，也许不足为奇，我们可以使用基于递归神经网络（如LSTM）的生成模型，从可能的分子图构型的巨大空间中采样。因为分子遵循特定的结构主题，这个问题比简单地独立采样原子集合更复杂，因为它们必须按照化学结构约束形成一个连贯的分子。*图13.3*说明了这个过程可能是什么样子；将2D结构编码成二进制特征向量（有点类似我们在早期章节看到的文本表示），然后通过递归神经网络运行这些向量，训练一个根据先前的原子/键预测下一个原子或键的模型。一旦模型在输入数据上训练完成，它就可以被用来通过从这个生成器中一次次采样新结构来生成新的分子。变分自动编码器（*第5章*，*使用VAE画图*）也已经被用来生成分子⁴（*图13.4*），同样生成对抗网络⁵（*图13.5*），我们在*第6章*，*使用GAN生成图像*中介绍过。
- en: '![](img/B16176_13_04.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_04.png)'
- en: 'Figure 13.4: Variational autoencoders for small molecules⁶'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：小分子的变分自动编码器⁶
- en: '![](img/B16176_13_05.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_05.png)'
- en: 'Figure 13.5: Generative adversarial models for small molecules⁶'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：小分子的生成对抗模型⁶
- en: Folding proteins with generative models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用生成模型折叠蛋白质
- en: Once we have the set of virtual molecules from such a model, the next challenge
    is to figure out which, if any, of them have the potential to be drugs. Scientists
    test this through "virtual docking," in which a large library of simulated molecules
    are tested to see if they fit in the pockets of protein structures also represented
    in a computer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从这样的模型得到了虚拟分子集合，下一个挑战是弄清楚它们当中是否有潜在成为药物的可能。科学家们通过"虚拟对接"进行测试，这需要测试一大库模拟分子，看它们是否适配在计算机中表示的蛋白质结构的口袋中。
- en: Molecular dynamics simulations are used to approximate the energetic attraction/repulsion
    between a potential drug and a protein that might affect a disease in the body
    based on the relative chemical structures of the protein and chemical. A challenge
    is that these protein structures against which drugs are "docked" in these simulations
    are typically derived from X-ray crystallography experiments, a painstaking process
    of diffracting X-rays through a protein structure to obtain a representation of
    its 3D form.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分子动力学模拟用于近似评估潜在药物和可能影响体内疾病的蛋白质之间的能量吸引力/排斥力，其基于蛋白质和化学物质的相对化学结构。一个挑战是，在这些模拟中，药物“对接”到的蛋白质结构通常是通过X射线晶体学实验获得的，这是一种通过蛋白质结构衍射X射线以获得其三维形态表示的繁琐过程。
- en: Further, this process is only applicable to a subset of proteins that are stable
    in liquid suspension, which excludes a huge number of molecules relevant to disease
    that are expressed on the surface of the cell, where they are surrounded and stabilized
    by the fat particles in the cell membrane.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个过程仅适用于在液体悬浮液中稳定的一部分蛋白质，这排除了许多与疾病相关的分子，这些分子在细胞表面表达，在那里它们被细胞膜中的脂质粒子包围和稳定。
- en: 'Here, too, generative AI can help: the researchers at DeepMind (a research
    subsidiary of Google that was also responsible for AlphaGo and other breakthroughs)
    recently released a program called **AlphaFold** that can solve the 3D structure
    of proteins directly from their genetic sequence, allowing researchers to bypass
    the painstaking process of experimental crystallography. *Figure 13.6* illustrates
    how this is done: first a network is trained to predict the 3D inter-amino acid
    (the building block of proteins) distances within a protein from its linear sequence
    code, representing the most likely angles and distances between parts in the folded
    structure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，生成式人工智能也可以帮助：DeepMind（谷歌的研究子公司，也负责 AlphaGo 和其他突破性成果）的研究人员最近发布了一个名为**AlphaFold**的程序，它可以直接从蛋白质的遗传序列解决蛋白质的三维结构，使研究人员可以绕过实验晶体学的繁琐过程。*图
    13.6* 说明了这是如何实现的：首先，一个网络被训练来从蛋白质的线性序列代码中预测蛋白质内的三维氨基酸（蛋白质的组成单位）之间的距离，表示折叠结构中各部分之间最可能的角度和距离：
- en: '![](img/B16176_13_07.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_07.png)'
- en: 'Figure 13.6: Training a neural network to predict the distance between amino
    acids in a protein sequence⁷'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：训练神经网络预测蛋白质序列中氨基酸之间的距离⁷
- en: 'Then, for proteins without a structure, AlphaFold proposes new protein fragments
    using a generative model (*Figure 13.7*), and scores which ones have a high likelihood
    of forming a stable 3D structure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于没有结构的蛋白质，AlphaFold 使用生成模型提出新的蛋白质片段（*图 13.7*），并评分哪些片段具有形成稳定三维结构的高可能性：
- en: '![](img/B16176_13_08.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_08.png)'
- en: 'Figure 13.7: Generative model to predict protein conformation⁷'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7：生成模型预测蛋白质构象⁷
- en: These examples show how it is possible to simulate both drugs and the structure
    of their potential targets in the human body using generative AI, taking what
    are difficult or impossible experiments in the real world and harnessing the massive
    computational capacity of new models to potentially discover new medicines.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例展示了如何使用生成式人工智能在人体中模拟药物和其潜在靶点的结构，将现实世界中难以或不可能的实验转化为潜在发现新药物的新模型的大规模计算能力。
- en: Solving partial differential equations with generative modeling
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成建模求解偏微分方程
- en: 'Another field in which deep learning in general, and generative learning in
    particular, have led to recent breakthroughs is the solution of **partial differential
    equations** (**PDEs**), a kind of mathematical model used for diverse applications
    including fluid dynamics, weather prediction, and understanding the behavior of
    physical systems. More formally, a PDE imposes some condition on the partial derivatives
    of a function, and the problem is to find a function that fulfills this condition.
    Usually some set of initial or boundary conditions is placed on the function to
    limit the search space within a particular grid. As an example, consider Burger''s
    equation,⁸ which governs phenomena such as the speed of a fluid at a given position
    and time (*Figure 13.8*):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个深度学习在一般领域以及生成学习在特定领域引领最新突破的领域是**偏微分方程**（**PDEs**），这是一种用于各种应用的数学模型，包括流体动力学、天气预测和理解物理系统行为。更正式地说，偏微分方程对函数的偏导数施加某些条件，问题是找到一个满足这个条件的函数。通常在函数上放置一些初始条件或边界条件，以限制在特定网格内的搜索空间。例如，考虑
    Burger 方程⁸，它控制着诸如给定位置和时间的流体速度之类的现象（*图 13.8*）：
- en: '![](img/B16176_13_001.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_001.png)'
- en: 'Where *u* is speed, *t* is time, *x* is a positional coordinate, and ![](img/B16176_13_006.png)
    is the viscosity ("oiliness") of the fluid. If the viscosity is 0, this simplifies
    to the *inviscid* equation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *u* 是速度，*t* 是时间，*x* 是位置坐标，而 ![](img/B16176_13_006.png) 是流体的黏性（"油腻度"）。如果黏性为
    0，则简化为*无黏*方程：
- en: '![](img/B16176_13_002.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_002.png)'
- en: The solution of this equation depends upon the form of the function *u* and
    its initial condition at *t=0*; for example, if *u* is a linear function, it has
    a closed-form analytical solution (in other words, the solution can be derived
    using only the manipulation of mathematical formulae and variable substitutions,
    rather than requiring numerical methods to minimize an error function for potential
    solutions selection using an algorithm)⁹. In most cases, however, one needs numerical
    methods, and one popular approach is using **finite element methods** (**FEMs**)
    to divide the output (*x*, *t*) into a "grid" and solve for *u* numerically within
    each of those grids. This is analogous to the use of a "basis" set of functions
    such as cosines and sines (Fourier analysis) or wavelets to decompose complex
    functions into a sum of simpler underlying functions.^(10)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此方程的解取决于函数 *u* 的形式及其在 *t=0* 时的初始条件；例如，如果 *u* 是一个线性函数，则它有一个封闭的解析解（换句话说，可以仅使用数学公式的操作和变量替换来导出解，而无需使用数值方法来最小化误差函数，以选择潜在的解算法进行解析）。在大多数情况下，然而，需要数值方法，一个流行的方法是使用**有限元方法**（**FEMs**）将输出（*x*，*t*）划分为一个"网格"，并在每个网格内数值求解
    *u*。这类似于使用一组"基础"函数，如余弦和正弦（傅立叶分析）或小波将复杂函数分解为更简单的基础函数的总和。^(10)
- en: '![](img/B16176_13_09.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_09.png)'
- en: 'Figure 13.8: A visualization of Burger''s equation in two dimensions^(11)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8：二维中 Burger 方程的可视化^(11)
- en: 'However, consider the problem we now have: for a given set of conditions (the
    boundary conditions), we want to output a function value across a grid, creating
    a heatmap of the function value at each point in space and time, not unlike an
    image we''ve seen in prior generative applications! Indeed, convolutional generative
    models have been used to map boundary conditions to an output grid (*Figure 13.9*)^(12):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在考虑我们所面临的问题：对于一组给定条件（边界条件），我们希望在网格上输出一个函数值，从而在空间和时间的每个点上创建一个函数值的热图，不完全像我们之前在生成应用中看到的图像！事实上，卷积生成模型已被用于将边界条件映射到输出网格（*图
    13.9*）^(12)：
- en: '![](img/B16176_13_10.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_10.png)'
- en: 'Figure 13.9: Solution of Burger''s inviscid equation using FEM methods (right),
    and deep learning (left)^(12)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9：使用 FEM 方法解决 Burger 的无黏方程（右）和深度学习（左）^(12)
- en: As described earlier, the typical strategy of using FEM is to solve numerically
    for *u* at each element in a grid, where most of the computational burden is from
    repetitively computing the solution rather than checking it^(12); thus generative
    AI can operate more efficiently by sampling a large set of possible solutions
    (with that set constrained by initial conditions or the numerical range of each
    variable) and checking if they fulfill the conditions of the PDE, completely circumventing
    the need for solving for the function in each element of the grid. *Figure 13.10*
    shows how a set of boundary conditions (*b*) and viscosity variables (*v*) can
    be input into a convolutional network that then generates the solution of the
    PDE.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，使用FEM的典型策略是在网格中对每个元素数值求解*u*，其中大部分的计算负担来自于重复计算解而不是检查解^(12)；因此，生成式AI可以通过抽样大量可能的解（该集合受初始条件或每个变量的数值范围的限制）并检查它们是否满足PDE的条件，完全规避了需要在网格的每个元素中求解函数的需要。*图13.10*显示了将一组边界条件（*b*）和粘度变量（*v*）输入到卷积网络中，然后生成PDE的解。
- en: '![](img/B16176_13_11.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_11.png)'
- en: 'Figure 13.10: DiffNet used to sample potential solutions for Burger''s Equation^(12)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：DiffNet用于对Burger方程的潜在解进行抽样^(12)
- en: The network in this example is termed **DiffNet**. It has two terms in its error
    function; one (*L*[p], equation 4) encourages the generated grids to match the
    PDE surface (low error), while the other forces the solution to reproduce the
    desired boundary conditions on *x* and *t* (*L*[b], equation 5).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例中的网络称为**DiffNet**。其误差函数有两个项；一个（*L*[p]，方程4）鼓励生成的网格与PDE曲面匹配（误差较小），而另一个则强制解在*x*和*t*上再现所需的边界条件（*L*[b]，方程5）。
- en: '![](img/B16176_13_003.png)![](img/B16176_13_004.png)![](img/B16176_13_005.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_003.png)![](img/B16176_13_004.png)![](img/B16176_13_005.png)'
- en: Together, these two constraints resemble the other adversarial generative examples
    we've seen in past chapters; one term tries to minimize error (like the discriminator)
    while another tries to approximate a distribution (here, the boundary conditions
    on the variables).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个约束共同类似于我们在过去章节中看到的其他对抗生成示例；一个项试图最小化误差（如鉴别器），而另一个试图近似一个分布（这里是变量上的边界条件）。
- en: Few shot learning for creating videos from images
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从图像创建视频的少样本学习方式
- en: In prior chapters, we have seen how GANs can generate novel photorealistic images
    after being trained on a group of example photos. This technique can also be used
    to create variations of an image, either applying "filters" or new poses or angles
    of the base image. Extending this approach to its logical limit, could we create
    a "talking head" out of a single or a limited set of images? This problem is quite
    challenging – classical (or deep learning) approaches that apply "warping" transformations
    to a set of images create noticeable artifacts that degrade the realism of the
    output ^(13,14). An alternative approach is to use generative models to sample
    potential angular and positional variations of the input images (*Figure 13.11*),
    as performed by Zakharov et al. in their paper *Few Shot Adversarial Learning
    of Realistic Neural Talking Head Models*.^(15)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到GANs在接受一组示例照片的训练后可以生成新颖的逼真图像。这种技术也可以用来创建图像的变体，无论是应用“滤镜”还是基础图像的新姿势或角度。将这种方法推向逻辑极限，我们是否可以从单个或有限数量的图像中创建一个“说话的头”？这个问题非常具有挑战性——对图像应用“扭曲”转换的传统（或深度学习）方法会产生明显的伪影，从而降低输出的逼真度^(13,14)。另一种替代方法是使用生成模型从输入图像(*图13.11*)中抽样潜在的角度和位置变化，正如Zakharov等人在他们的论文*Few
    Shot Adversarial Learning of Realistic Neural Talking Head Models*中所做的那样。^(15)
- en: '![](img/B16176_13_12.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_12.png)'
- en: 'Figure 13.11: Generative architecture for creating moving frames from single
    images (based on Zakharov et al., Figure 2.)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：用于从单个图像创建运动帧的生成架构（基于Zakharov等人的第2张图。）
- en: This architecture has three networks that process a set of input videos. The
    first generates embeddings of landmark features (a simplified representation of
    the face, such as its outline and the location of important features like the
    eyes and nose), along with the pixels of the original image, into a numerical
    vector. The second generates novel video frames with this embedding as an input
    along with the image and landmark data. Finally, the next frame in the video is
    compared with this generated image via a discriminator.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构有三个处理一组输入视频的网络。第一个生成地标特征的嵌入（面部的简化表示，例如轮廓和重要特征的位置，如眼睛和鼻子的位置），以及原始图像的像素，生成数值向量。第二个使用这个嵌入和图像和地标数据作为输入生成新的视频帧。最后，通过鉴别器将下一帧视频与这个生成的图像进行比较。
- en: This is referred to as *few shot* learning because it uses only a small number
    of video frames to learn to generate new video sequences, which could be unbounded
    in length. This model can then be applied to input data with only a single example,
    such as still photos, generating "talking heads" from historical photos, or even
    paintings such as the Mona Lisa.^(16) Readers may refer to the cited paper for
    sample output.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为*few shot*学习，因为它只使用少量视频帧来学习生成新的视频序列，这些序列的长度可能是无限的。然后，这个模型可以应用于仅有一个示例的输入数据，例如静止照片，从历史照片生成“说话的头像”，甚至是像《蒙娜丽莎》这样的绘画^(16)。读者可以参考引用的论文了解示例输出。
- en: 'These "living portraits" are in some way the evolution of deepfakes – rather
    than copying one image of a moving face to another, lifelike motions are simulated
    from a picture that had no prior motion data associated with it. Like the portrait
    auction we discussed in *Chapter 1*, *An Introduction to Generative AI: "Drawing"
    Data from Models*, this is an example in which generative AI is truly bringing
    new art to life.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些“活肖画像”在某种程度上是deepfakes的演变 - 不是将一个移动的面部图像复制到另一个图像中，而是从没有与之相关的运动数据的图片中模拟出栩栩如生的动作。就像我们在*第1章*中讨论的画像拍卖，*生成AI简介：“从模型中提取”数据*，这是一个真正将生成AI带入新生活的例子。
- en: Generating recipes with deep learning
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习生成食谱
- en: 'A final example we will discuss is related to earlier examples in this book,
    on generating textual descriptions of images using GANs. A more complex version
    of this same problem is to generate a structured description of an image that
    has multiple components, such as the recipe for a food depicted in an image. This
    description is also more complex because it relies on a *particular sequence*
    of these components (instructions) in order to be coherent (*Figure 13.12*):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的最后一个例子与本书中较早的例子有关，即使用GAN生成图像的文本描述。这个问题的一个更复杂的版本是生成图像的结构化描述，该图像具有多个组件，例如图像中所描述的食物的配方。这个描述也更复杂，因为它依赖于这些组件（说明）的*特定顺序*才能连贯（*图13.12*）：
- en: '![](img/B16176_13_13.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_13.png)'
- en: 'Figure 13.12: A recipe generated from an image of food^(17)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图13.12: 从食物图像生成的食谱^(17)'
- en: As *Figure 13.13* demonstrates, this "inverse cooking" problem has also been
    studied using generative models^(17) (Salvador et al.).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图13.13*所示，这个“逆向烹饪”问题也已经使用生成模型^(17)（Salvador等人）进行了研究。
- en: '![](img/B16176_13_14.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_13_14.png)'
- en: 'Figure 13.13: Architecture of a generative model for inverse cooking^(17)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '图13.13: 逆向烹饪生成模型的架构^(17)'
- en: 'Like many of the examples we''ve seen in prior chapters, an "encoder" network
    receives an image as input, and then "decodes" using a sequence model into text
    representations of the food components, which are combined with the image embedding
    to create a set of instructions decoded by a third layer of the network. This
    "instruction decoder" uses the transformer architecture described in *Chapter
    10*, *NLP 2.0: Using Transformers to Generate Text*, in our discussion of BERT,
    to apply weighted relevance to different portions of the image and output ingredient
    list.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '像我们在前几章看到的许多例子一样，“编码器”网络接收图像作为输入，然后使用序列模型“解码”成食物成分的文本表示，这些成分与图像嵌入组合在一起，创建由网络的第三层解码的一组指令。这个“指令解码器”使用了我们在*第10章*中描述的transformer架构，*NLP
    2.0: 使用Transformer生成文本*，在我们讨论BERT时，对图像的不同部分应用加权相关性，并输出成分清单。'
- en: Summary
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we examined a number of emerging applications of generative
    models. One is in the field of biotechnology, where they can be used to create
    large collections of new potential drug structures. Also in the biotechnology
    field, generative models are used to create potential protein folding structures
    that can be used for computational drug discovery.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了许多生成模型的新兴应用。其中之一是在生物技术领域，它们可以用于创建大量新的潜在药物结构。同样在生物技术领域，生成模型被用于创建可以用于计算药物发现的潜在蛋白质折叠结构。
- en: We explored how generative models can be used to solve mathematical problems,
    in particular PDEs, by mapping a set of boundary conditions of a fluid dynamics
    equation to a solution grid. We also examined a challenging problem of generating
    videos from a limited set of input images, and finally generating complex textual
    descriptions (components and sequences of instructions) from images of food to
    create recipes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了生成模型如何可以用于解决数学问题，特别是偏微分方程，通过将流体力学方程组一系列边界条件映射到解决方案网格。我们还研究了一个具有挑战性的问题，即如何从有限的输入图像集生成视频，最后从食物图像生成复杂的文本描述（组件和说明的顺序），用于制作食谱。
- en: 'Well done for reaching the end of the book. As a final summary, let''s recap
    everything we''ve learned:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您到达本书的最后。作为最终总结，让我们回顾一下我们学到的所有内容：
- en: '*Chapter 1*, *An Introduction to Generative AI: "Drawing" Data from Models*:
    What generative models are'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第1章*，*生成式AI简介：从模型“绘制”数据*：生成模型是什么'
- en: '*Chapter 2*, *Setting Up a TensorFlow Lab*: How to set up a TensorFlow 2 environment
    in the cloud'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第2章*，*建立一个TensorFlow实验室*：如何在云端建立TensorFlow 2的环境'
- en: '*Chapter 3*, *Building Blocks of Deep Neural Networks*: The building blocks
    of neural network models used in generative AI'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第3章*，*深度神经网络的基本构件*：生成式AI中使用的神经网络模型的基本构件'
- en: '*Chapter 4*, *Teaching Networks to Generate Digits*: How restricted Boltzmann
    machines (some of the earliest generative models) can generate hand-drawn digits'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第4章*，*教网络生成数字*：限制玻尔兹曼机（最早的生成模型之一）如何生成手写数字'
- en: '*Chapter 5*, *Painting Pictures with Neural Networks Using VAEs*: How variational
    autoencoders can generate images from random data'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第5章*，*使用VAEs利用神经网络绘画图片*：变分自编码器如何从随机数据生成图像'
- en: '*Chapter 6*, *Image Generation with GANs*: The building blocks of GANs and
    how they are used to generate high resolution images from random noise'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第6章*，*使用GAN生成图片*：了解GAN的基本组成以及它们如何用于从随机噪声生成高分辨率图像'
- en: '*Chapter 7*, *Style Transfer with GANs*: How GANs (CycleGAN and pix2pix) can
    be leveraged to transfer style from one domain to another'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第7章*，*使用GAN进行风格迁移*：如何利用GAN（CycleGAN和pix2pix）将风格从一个领域转移到另一个领域'
- en: '*Chapter 8*, *Deepfakes with GANs*: Basic building blocks for deepfakes to
    generate fake photos and videos'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第8章*，*使用GAN进行Deepfakes*：生成深假的基本构件，生成虚假照片和视频'
- en: '*Chapter 9*, *The Rise of Methods for Text Generation*: The basics of language
    generation using deep learning models'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第9章*，*文本生成方法的兴起*：使用深度学习模型进行语言生成的基础知识'
- en: '*Chapter 10*, *NLP 2.0: Using Transformers to Generate Text*: How transformers
    and different state-of-the-art architectures (like GPT-x) have revolutionized
    the language generation and NLP domain in general'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第10章*，*NLP 2.0：使用变形金刚生成文本*：变形金刚以及不同的最先进架构（如GPT-x）如何改变了语言生成和NLP领域的革命'
- en: '*Chapter 11*, *Composing Music with Generative Models*: How generative models
    can be leveraged to generate music from random data'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第11章*，*利用生成模型进行音乐作曲*：如何利用生成模型从随机数据生成音乐'
- en: '*Chapter 12*, *Play Video Games with Generative AI: GAIL*: How generative models
    can be used to train "agents" that can navigate virtual environments through reinforcement
    learning'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第12章*，*用生成式AI玩视频游戏：GAIL*：生成模型如何用于训练可以通过强化学习在虚拟环境中导航的“代理”'
- en: '*Chapter 13*, *Emerging Applications in Generative AI*: Some exciting emerging
    applications of generative AI'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第13章*，*生成式AI的新兴应用*：生成式AI的一些令人兴奋的新兴应用'
- en: We hope this book has demonstrated the varied traditional and cutting-edge use
    cases to which generative modeling is being applied and that you are curious enough
    to learn more by reading the referenced background, or even trying to implement
    some of the models yourself!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望本书展示了传统和尖端用例中生成建模的多样性，并且您对通过阅读引用的背景或甚至尝试自己实施一些模型更加感兴趣！
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: Kirkpatrick, P., & Ellis, C. (2004). *Chemical space*. *Nature* 432, 823\. [https://www.nature.com/articles/432823a](https://www.nature.com/articles/432823a)
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kirkpatrick, P., & Ellis, C. (2004)。*化学空间*。*自然* 432，823。[https://www.nature.com/articles/432823a](https://www.nature.com/articles/432823a)
- en: Villanueva, J.C. (2009, July 30). *How Many Atoms Are There in the Universe?*
    Universe Today. [https://www.universetoday.com/36302/atoms-in-the-universe/#:~:text=At%20this%20level%2C%20it%20is,hundred%20thousand%20quadrillion%20vigintillion%20atoms](https://www.universetoday.com/36302/atoms-in-the-universe/#:~:text=At%20this%20level%2C%20it%20is,hu)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Villanueva, J.C. (2009年7月30日)。*宇宙中有多少个原子？* 《今日宇宙》。[https://www.universetoday.com/36302/atoms-in-the-universe/#:~:text=At%20this%20level%2C%20it%20is,hundred%20thousand%20quadrillion%20vigintillion%20atoms](https://www.universetoday.com/36302/atoms-in-the-universe/#:~:text=At%20this%20level%2C%20it%20is,hu)
- en: Based on a figure from Akutsu, T., & Nagamochi, H. (2013). *Comparison and enumeration
    of chemical graphs*. Computational and Structural Biotechnology Journal, Vol.
    5 Issue 6
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于Akutsu, T., & Nagamochi, H. (2013年)的一幅图。*化学图的比较与枚举*。计算与结构生物技术杂志，Vol. 5 Issue
    6
- en: Gómez-Bombarelli R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling,
    B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., Aspuru-Guzik,
    A. *Automatic chemical design using a data-driven continuous representation of
    molecules*. ACS central science 2018, 4, 268-276.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gómez-Bombarelli R., Wei, J. N., Duvenaud, D., Hernández-Lobato, J. M., Sánchez-Lengeling,
    B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., Aspuru-Guzik,
    A. (2018)。*使用数据驱动的分子连续表示进行自动化学设计*。ACS中央科学 2018，4，268-276。
- en: Blaschke, T., Olivecrona, M., Engkvist, O., Bajorath, J., Chen, H. *Application
    of generative autoencoder in de novo molecular design*. Molecular informatics
    2018, 37, 1700123.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Blaschke, T., Olivecrona, M., Engkvist, O., Bajorath, J., Chen, H.。*应用生成自动编码器进行全新分子设计*。分子信息学
    2018，37，1700123。
- en: 'Bian Y., & Xie, X-Q. (2020). *Generative chemistry: drug discovery with deep
    learning generative models*. arXiv. [https://arxiv.org/abs/2008.09000](https://arxiv.org/abs/2008.09000)'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bian Y., & Xie, X-Q. (2020)。*生成化学：用深度学习生成模型进行药物发现*。arXiv。[https://arxiv.org/abs/2008.09000](https://arxiv.org/abs/2008.09000)
- en: 'Senior, A., Jumper, J., Hassabis, D., & Kohli, P. (2020, January 15). *AlphaFold:
    Using AI for scientific discovery*. DeepMind blog. [https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Senior, A., Jumper, J., Hassabis, D., & Kohli, P. (2020年1月15日)。*AlphaFold：利用AI进行科学发现*。DeepMind博客。[https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery)
- en: Cameron, M. Notes on Burgers's equation. [https://www.math.umd.edu/~mariakc/burgers.pdf](https://www.math.umd.edu/~mariakc/burgers.pdf)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cameron, M. Burgers方程笔记。[https://www.math.umd.edu/~mariakc/burgers.pdf](https://www.math.umd.edu/~mariakc/burgers.pdf)
- en: Chandrasekhar, S. (1943). *On the decay of plane shock waves* (No. 423). Ballistic
    Research Laboratories.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chandrasekhar, S. (1943)。*平面激波衰变论*（No. 423）。弹道研究实验室。
- en: COMSOL Multiphysics Encyclopedia. (2016, March 15). *The Finite Element Method
    (FEM)*. [https://www.comsol.com/multiphysics/finite-element-method](https://www.comsol.com/multiphysics/finite-element-method)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: COMSOL Multiphysics 百科全书。(2016年3月15日)。*有限元法（FEM）*。[https://www.comsol.com/multiphysics/finite-element-method](https://www.comsol.com/multiphysics/finite-element-method)
- en: Wikipedia. (2021, April 14). *Burgers' equation*. [https://en.wikipedia.org/wiki/Burgers%27_equation#%20Inviscid_Burgers'_%20equation](https://en.wikipedia.org/wiki/Burgers%27_equation#%20Inviscid_Burgers'_%20equation)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wikipedia. (2021年4月14日)。*Burgers方程*。[https://en.wikipedia.org/wiki/Burgers%27_equation#%20Inviscid_Burgers'_%20equation](https://en.wikipedia.org/wiki/Burgers%27_equation#%20Inviscid_Burgers'_%20equation)
- en: 'Botelho, S., Joshi, A., Khara, B., Sarkar, S., Hegde, C., Adavani, S., & Ganapathysubramanian,
    B. (2020). *Deep Generative Models that Solve PDEs: Distributed Computing for
    Training Large Data-Free Models*. arXiv. [https://arxiv.org/abs/2007.12792](https://arxiv.org/abs/2007.12792)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Botelho, S., Joshi, A., Khara, B., Sarkar, S., Hegde, C., Adavani, S., & Ganapathysubramanian,
    B. (2020)。*解PDE的深度生成模型：用于训练大型无数据模型的分布式计算*。arXiv。[https://arxiv.org/abs/2007.12792](https://arxiv.org/abs/2007.12792)
- en: Averbuch-Elor, H., Cohen-Or, D., Kopf, J., & Cohen, M.F. (2017). *Bringing portraits
    to life*. ACM Transactions on Graphics (TOG), 36(6):196
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Averbuch-Elor, H., Cohen-Or, D., Kopf, J., & Cohen, M.F. (2017)。*给肖像栩栩如生的魔法*。《ACM图形学交易（TOG）》，36(6):196
- en: 'Ganin, Y., Kononenko, D., Sungatullina, D., & Lempitsky, V. (2016). *DeepWarp:
    Photorealistic Image Resynthesis for Gaze Manipulation*. European Conference on
    Computer Vision, 311-326\. Springer.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ganin, Y., Kononenko, D., Sungatullina, D., & Lempitsky, V. (2016)。*DeepWarp：用于凝视操作的照片逼真图像合成*。欧洲计算机视觉会议，311-326。Springer。
- en: Zakharov, E., Shysheya, A., Burkov, E., & Lempitsky, V. (2019). *Few-Shot Adversarial
    Learning of Realistic Neural Talking Head Models*. arXiv. [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zakharov，E.，Shysheya，A.，Burkov，E.，& Lempitsky，V. (2019)。*Few-Shot Adversarial
    Learning of Realistic Neural Talking Head Models。* arXiv。[https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233)
- en: Hodge, M. (2019, May 24). *REALLY SAYING SOMETHING Samsung’s ‘deepfake’ video
    of a TALKING Mona Lisa painting reveals the terrifying new frontier in fake news*.
    The Sun. [https://www.thesun.co.uk/news/9143575/deepfake-talking-mona-lisa-samsung/](https://www.thesun.co.uk/news/9143575/deepfake-talking-mona-lisa-samsung/)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hodge，M. (2019年5月24日)。*【真正的】谈论三星的'深度伪造'视频显示了假新闻的可怕新前沿。* The Sun。[https://www.thesun.co.uk/news/9143575/deepfake-talking-mona-lisa-samsung/](https://www.thesun.co.uk/news/9143575/deepfake-talking-mona-lisa-samsung/)
- en: 'Salvador, A., Drozdzal, M., Giro-i-Nieto, X., & Romero, A. (2019). *Inverse
    Cooking: Recipe Generation from Food Images*. arXiv. [https://arxiv.org/abs/1812.06164](https://arxiv.org/abs/1812.06164)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Salvador，A.，Drozdzal，M.，Giro-i-Nieto，X.，& Romero，A. (2019)。*反向烹饪：从食物图像生成食谱。*
    arXiv。[https://arxiv.org/abs/1812.06164](https://arxiv.org/abs/1812.06164)
- en: '| **Share your experience**Thank you for taking the time to read this book.
    If you enjoyed this book, help others to find it. Leave a review at [https://www.amazon.com/dp/1800200889](https://www.amazon.com/dp/1800200889).
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **分享您的经验**感谢您抽出时间阅读本书。如果您喜欢这本书，请帮助其他人找到它。在[https://www.amazon.com/dp/1800200889](https://www.amazon.com/dp/1800200889)上留下评论。
    |'
- en: '![](img/Image34250.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Image34250.png)'
- en: '[packt.com](http://packt.com)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[packt.com](http://packt.com)'
- en: Subscribe to our online digital library for full access to over 7,000 books
    and videos, as well as industry leading tools to help you plan your personal development
    and advance your career. For more information, please visit our website.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅我们的在线数字图书馆，以完全访问超过7,000本书籍和视频，以及行业领先的工具，帮助您规划个人发展并推动您的职业发展。有关更多信息，请访问我们的网站。
- en: Why subscribe?
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么订阅？
- en: Spend less time learning and more time coding with practical eBooks and Videos
    from over 4,000 industry professionals
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花更少的时间学习，更多的时间编码，使用来自超过4,000名行业专业人士的实用电子书和视频
- en: Learn better with Skill Plans built especially for you
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过专门为您建立的技能计划学习更好
- en: Get a free eBook or video every month
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每月获取一个免费电子书或视频
- en: Fully searchable for easy access to vital information
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全可搜索，轻松访问重要信息
- en: Copy and paste, print, and bookmark content
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制并粘贴，打印并书签内容
- en: At [www.Packt.com](http://www.Packt.com), you can also read a collection of
    free technical articles, sign up for a range of free newsletters, and receive
    exclusive discounts and offers on Packt books and eBooks.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在[www.Packt.com](http://www.Packt.com)，您还可以阅读一系列免费的技术文章，注册一系列免费的通讯，以及获得Packt图书和电子书的独家折扣和优惠。
