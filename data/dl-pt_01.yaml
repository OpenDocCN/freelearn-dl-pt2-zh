- en: Getting Started with Deep Learning Using PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch入门深度学习
- en: '**Deep learning** (**DL**) has revolutionized industry after industry. It was
    once famously described by Andrew Ng on Twitter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）已经彻底改变了一个又一个行业。安德鲁·吴曾在Twitter上著名地描述过：'
- en: '*Artificial Intelligence is the new electricity!*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能是新的电力！*'
- en: Electricity transformed countless industries; **artificial intelligence** (**AI**) will
    now do the same.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 电力改变了无数行业；**人工智能**（**AI**）现在将做同样的事情。
- en: AI and DL are used like synonyms, but there are substantial differences between
    the two. Let's demystify the terminology used in the industry so that you, as
    a practitioner, will be able to differentiate between signal and noise.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: AI和DL被用作同义词，但两者之间存在重大差异。让我们揭开行业术语的神秘面纱，这样作为从业者的你将能够区分信号和噪音。
- en: 'In this chapter, we will cover the following different parts of AI:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下AI的不同部分：
- en: AI itself and its origination
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI本身及其起源
- en: Machine learning in the real world
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实世界中的机器学习
- en: Applications of deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: Why deep learning now?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么现在是深度学习的时代？
- en: 'Deep learning framework: PyTorch'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习框架：PyTorch
- en: Artificial intelligence
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能
- en: Countless articles discussing AI are published every day. The trend has increased
    in the last two years. There are several definitions of AI floating around the
    web, my favorite being *the automation of intellectual tasks normally performed
    by humans*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每天都有无数篇讨论AI的文章发布。这种趋势在过去两年中有所增加。网络上流传着几种AI的定义，我最喜欢的是*自动执行通常由人类执行的智力任务*。
- en: The history of AI
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI的历史
- en: The term *artificial intelligence* was first coined by John McCarthy in 1956,
    when he held the first academic conference on the subject. The journey of the
    question of whether machines think or not started much earlier than that. In the
    early days of AI, machines were able to solve problems that were difficult for
    humans to solve.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能*这个术语最初由约翰·麦卡锡在1956年首次创造，当时他举办了第一届学术会议。关于机器是否会思考的问题的旅程比这早得多。在AI的早期阶段，机器能够解决人类难以解决的问题。'
- en: For example, the Enigma machine was built at the end of World War II to be used
    in military communications. Alan Turing built an AI system that helped to crack the
    Enigma code. Cracking the Enigma code was a very challenging task for a human,
    and it could take weeks for an analyst to do. The AI machine was able to crack
    the code in hours.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，恩尼格玛机器是在二战结束时建造的，用于军事通信。艾伦·图灵建造了一个AI系统，帮助破译恩尼格玛密码。破译恩尼格玛密码对人类来说是一个非常具有挑战性的任务，分析人员可能需要数周的时间。AI机器能够在几小时内破译该密码。
- en: Computers have a tough time solving problems that are intuitive to us, such
    as differentiating between dogs and cats, telling whether your friend is angry
    at you for arriving late at a party (emotions), differentiating between a truck
    and a car, taking notes during a seminar (speech recognition), or converting notes
    to another language for your friend who does not understand your language (for
    example, French to English). Most of these tasks are intuitive to us, but we were
    unable to program or hard code a computer to do these kinds of tasks. Most of
    the intelligence in early AI machines was hard coded, such as a computer program playing
    chess.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机难以解决我们直觉上理解的问题，比如区分狗和猫，判断朋友是否因为你迟到而对你生气（情感），区分卡车和汽车，参加研讨会时做笔记（语音识别），或为不理解你的语言的朋友转换笔记（例如，从法语到英语）。大多数这些任务对我们来说都很直观，但我们无法编程或硬编码计算机来执行这些任务。早期AI机器的大多数智能是硬编码的，比如一个计算机程序来玩国际象棋。
- en: In the early years of AI, a lot of researchers believed that AI could be achieved
    by hard coding rules. This kind of AI is called **symbolic AI** and was useful
    in solving well-defined, logical problems, but it was almost incapable of solving
    complex problems such as image recognition, object detection, object segmentation,
    language translation, and natural-language-understanding tasks. Newer approaches
    to AI, such as machine learning and DL, were developed to solve these kinds of
    problems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI的早期年代，许多研究人员认为可以通过硬编码规则实现AI。这种类型的AI称为**符号AI**，在解决定义良好的逻辑问题方面很有用，但几乎无法解决复杂的问题，如图像识别、物体检测、物体分割、语言翻译和自然语言理解任务。为了解决这些问题，开发了新的AI方法，如机器学习和深度学习。
- en: 'To better understand the relationships among AI, ML, and DL, let''s visualize
    them as concentric circles with AI—the idea that came first (the largest), then
    machine learning—(which blossomed later), and finally DL—which is driving today’s
    AI explosion (fitting inside both):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解AI、ML和DL之间的关系，让我们将它们想象成同心圆。AI——最先提出的概念（最大的圆），然后是机器学习——稍后发展起来的（位于更大圆的内部），最后是DL——驱动今天AI爆炸的（在两者之内）：
- en: '![](img/5f93757c-b18f-4ffb-b389-0fcfa1cdff41.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f93757c-b18f-4ffb-b389-0fcfa1cdff41.png)'
- en: How AI, machine learning, and DL fit together
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如何AI、机器学习和深度学习相互配合
- en: Machine learning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: '**Machine learning** (**ML**) is a sub-field of AI and has become popular in
    the last 10 years and, at times, the two are used interchangeably. AI has a lot
    of other sub-fields aside from machine learning. ML systems are built by showing
    lots of examples, unlike symbolic AI, where we hard code rules to build the system.
    At a high level, machine learning systems look at tons of data and come up with
    rules to predict outcomes for unseen data:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是AI的一个子领域，在过去10年变得流行，并且有时两者可以互换使用。AI除了机器学习之外还有许多其他子领域。ML系统通过展示大量示例来构建，与符号AI不同，后者在构建系统时硬编码规则。在高层次上，机器学习系统查看大量数据并提出规则，以预测未见数据的结果：'
- en: '![](img/27c1671a-61d3-46e1-ac66-e3dbd5683ab2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27c1671a-61d3-46e1-ac66-e3dbd5683ab2.png)'
- en: Machine learning versus traditional programming
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与传统编程的比较
- en: Most ML algorithms perform well on structured data, such as sales predictions,
    recommendation systems, and marketing personalization. An important factor for
    any ML algorithm is feature engineering and data scientists need to spend a lot
    of time to get the features right for ML algorithms to perform. In certain domains,
    such as computer vision and **natural language processing** (**NLP**), feature
    engineering is challenging as they suffer from high dimensionality.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数ML算法在结构化数据上表现良好，比如销售预测、推荐系统和营销个性化。对于任何ML算法来说，特征工程是一个重要因素，数据科学家需要花费大量时间来正确获取ML算法所需的特征。在某些领域，如计算机视觉和**自然语言处理**（**NLP**），特征工程具有挑战性，因为它们受到高维度的影响。
- en: Until recently, problems like this were challenging for organizations to solve
    using typical machine-learning techniques, such as linear regression, random forest,
    and so on, for reasons such as feature engineering and high dimensionality. Consider
    an image of size 224 x 224 x 3 (height x width x channels), where *3* in the image
    size represents values of red, green, and blue color channels in a color image.
    To store this image in computer memory, our matrix will contain 150,528 dimensions
    for a single image. Assume you want to build a classifier on top of 1,000 images
    of size 224 x 224 x 3, the dimensions will become 1,000 times 150,528\. A special
    branch of machine learning called **deep learning** allows you to handle these
    problems using modern techniques and hardware.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，这类问题对于使用典型的机器学习技术（如线性回归、随机森林等）来解决的组织来说是具有挑战性的，原因包括特征工程和高维度。考虑一幅大小为224 x
    224 x 3（高度 x 宽度 x 通道）的图像，图像尺寸中的*3*代表彩色图像中红色、绿色和蓝色通道的值。要将此图像存储在计算机内存中，我们的矩阵将包含每个图像150,528个维度。假设您想在大小为224
    x 224 x 3的1,000幅图像上构建分类器，维度将变为1,000倍的150,528。一种名为**深度学习**的机器学习特殊分支允许您使用现代技术和硬件处理这些问题。
- en: Examples of machine learning in real life
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生活中机器学习的例子
- en: 'The following are some cool products that are powered by machine learning:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些由机器学习驱动的酷产品：
- en: '**Example 1**: Google Photos uses a specific form of machine learning called
    **deep learning for grouping photos**'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 1**：Google Photos使用一种特定形式的机器学习，称为**深度学习来对照片进行分组**'
- en: '**Example 2**: Recommendation systems, which are a family of ML algorithms,
    are used for recommending movies, music, and products by major companies such
    as Netflix, Amazon, and iTunes'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 2**：推荐系统是ML算法家族的一部分，用于推荐电影、音乐和产品，像Netflix、Amazon和iTunes这样的大公司。'
- en: Deep learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: Traditional ML algorithms use handwritten feature extraction to train algorithms,
    while DL algorithms use modern techniques to extract these features in an automatic
    fashion.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 传统ML算法使用手写特征提取来训练算法，而DL算法使用现代技术以自动方式提取这些特征。
- en: 'For example, a DL algorithm predicting whether an image contains a face or
    not extracts features such as the first layer detecting edges, the second layer
    detecting shapes such as noses and eyes, and the final layer detecting face shapes
    or more complex structures. Each layer trains based on the previous layer''s representation
    of the data. It''s OK if you find this explanation hard to understand, the later
    chapters of the book will help you to intuitively build and inspect such networks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个深度学习算法预测图像是否包含人脸，提取特征如第一层检测边缘，第二层检测形状如鼻子和眼睛，最后一层检测面部形状或更复杂的结构。每一层都基于前一层对数据的表示进行训练。如果你觉得这个解释难以理解，书的后面章节将帮助你直观地构建和检查这样的网络：
- en: '![](img/a2d20614-2bcd-40eb-a797-3382a71f2676.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2d20614-2bcd-40eb-a797-3382a71f2676.png)'
- en: Visualizing the output of intermediate layers (Image source: https://www.cs.princeton.edu/~rajeshr/papers/cacm2011-researchHighlights-convDBN.pdf)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化中间层的输出（图片来源：https://www.cs.princeton.edu/~rajeshr/papers/cacm2011-researchHighlights-convDBN.pdf）
- en: The use of DL has grown tremendously in the last few years with the rise of
    GPUs, big data, cloud providers such as **Amazon Web Services** (**AWS**) and
    Google Cloud, and frameworks such as Torch, TensorFlow, Caffe, and PyTorch. In
    addition to this, large companies share algorithms trained on huge datasets, thus
    helping startups to build state-of-the-art systems on several use cases with little
    effort.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，随着GPU、大数据、云服务提供商如**亚马逊网络服务**（**AWS**）和Google Cloud的兴起，以及Torch、TensorFlow、Caffe和PyTorch等框架，深度学习的应用大幅增长。此外，大公司分享在庞大数据集上训练的算法，从而帮助初创公司在多个用例上轻松构建最先进的系统。
- en: Applications of deep learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的应用
- en: 'Some popular applications that were made possible using DL are as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深度学习实现的一些热门应用包括：
- en: Near-human-level image classification
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近乎人类水平的图像分类
- en: Near-human-level speech recognition
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近乎人类水平的语音识别
- en: Machine translation
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译
- en: Autonomous cars
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车
- en: Siri, Google Voice, and Alexa have become more accurate in recent years
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siri、Google Voice和Alexa近年来变得更加准确
- en: A Japanese farmer sorting cucumbers
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位日本农民正在分类黄瓜
- en: Lung cancer detection
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肺癌检测
- en: Language translation beating human-level accuracy
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过人类水平精度的语言翻译
- en: 'The following screenshot shows a short example of summarization, where the
    computer takes a large paragraph of text and summarizes it in a few lines:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图展示了一个简短的总结示例，计算机将大段文本进行概括，用几行来呈现：
- en: '![](img/4a2ec332-490a-40b0-87ba-2a3290c4aab9.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a2ec332-490a-40b0-87ba-2a3290c4aab9.png)'
- en: Summary of a sample paragraph generated by computer
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由计算机生成的样本段落摘要
- en: In the following image, a computer has been given a plain image without being
    told what it shows and, using object detection and some help from a dictionary,
    you get back an image caption stating **two young girls are playing with lego
    toy**. Isn't it brilliant?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，计算机被给予一张普通的图像，没有告诉它显示的是什么，利用物体检测和字典的帮助，你得到一张图像标题，说**两个年轻女孩正在玩乐高玩具**。这不是很棒吗？
- en: '![](img/25961d1a-6689-4d5b-b3e3-b99c80c5aaa8.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25961d1a-6689-4d5b-b3e3-b99c80c5aaa8.png)'
- en: Object detection and image captioning (Image source: https://cs.stanford.edu/people/karpathy/cvpr2015.pdf)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测和图像标题（图片来源：https://cs.stanford.edu/people/karpathy/cvpr2015.pdf）
- en: Hype associated with deep learning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与深度学习相关的炒作
- en: 'People in the media and those outside the field of AI, or people who are not
    real practitioners of AI and DL, have been suggesting that things like the story
    line of the film *Terminator 2: Judgement Day* could become reality as AI/DL advances.
    Some of them even talk about a time in which we will become controlled by robots,
    where robots decide what is good for humanity. At present, the ability of AI is
    exaggerated far beyond its true capabilities. Currently, most DL systems are deployed
    in a very controlled environment and are given a limited decision boundary.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 媒体和AI领域外的人，或者那些不是真正的AI和深度学习从业者的人，一直在暗示像《终结者2：审判日》的情节可能会随着AI/DL的进步成为现实。他们中的一些人甚至谈论到一个时代，我们将被机器人控制，机器人决定什么对人类有益。目前，AI的能力被夸大到远远超出其真实能力的程度。目前，大多数深度学习系统在非常受控制的环境中部署，并且给出了有限的决策边界。
- en: My guess is that when these systems can learn to make intelligent decisions,
    rather than merely completing pattern matching and, when hundreds or thousands
    of DL algorithms can work together, then maybe we can expect to see robots that
    could probably behave like the ones we see in science fiction movies. In reality,
    we are no closer to general artificial intelligence, where machines can do anything
    without being told to do so. The current state of DL is more about finding patterns
    from existing data to predict future outcomes. As DL practitioners, we need to
    differentiate between signal and noise.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我的猜测是，当这些系统能够学会做出智能决策时，而不仅仅是完成模式匹配时，当数百或数千个深度学习算法能够共同工作时，也许我们可以期待看到像科幻电影中那样的机器人。实际上，我们离得到机器能够在没有被告知的情况下做任何事情的普遍人工智能还很遥远。当前的深度学习状态更多地是关于从现有数据中找到模式以预测未来结果。作为深度学习从业者，我们需要区分信号和噪音。
- en: The history of deep learning
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的历史
- en: 'Though deep learning has become popular in recent years, the theory behind
    deep learning has been evolving since the 1950s. The following table shows some
    of the most popular techniques used today in DL applications and their approximate
    timeline:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管近年来深度学习变得流行，但深度学习背后的理论自20世纪50年代以来一直在发展。以下表格展示了今天在DL应用中使用的一些最流行的技术及其大致时间表：
- en: '| **Techniques** | **Year** |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **年份** |'
- en: '| Neural networks  | 1943 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络 | 1943年 |'
- en: '| Backpropogation | Early 1960s |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 反向传播 | 1960年代早期 |'
- en: '| Convolution Neural Networks | 1979 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 卷积神经网络 | 1979 |'
- en: '| Recurrent neural networks  | 1980 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 循环神经网络 | 1980年 |'
- en: '| Long Short-Term Memory  | 1997 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 长短期记忆 | 1997年 |'
- en: Deep learning has been given several names over the years. It was called **cybernetics**
    in the 1970s, connectionism in the 1980s, and now it is either known as *deep
    learning* or *neural networks*. We will use DL and neural networks interchangeably.
    Neural networks are often referred to as an algorithms inspired by the working
    of human brains. However, as practitioners of DL, we need to understand that it
    is majorly inspired and backed by strong theories in math (linear algebra and
    calculus), statistics (probability), and software engineering.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，深度学习被赋予了几个名字。在1970年代被称为**控制论**，在1980年代被称为*连接主义*，现在则通常称为*深度学习*或*神经网络*。我们将DL和神经网络互换使用。神经网络通常被称为受人类大脑工作启发的算法。然而，作为DL从业者，我们需要理解，它主要受数学（线性代数和微积分）、统计学（概率）和软件工程的强大理论支持。
- en: Why now?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么现在？
- en: 'Why has DL became so popular now? Some of the crucial reasons are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么DL现在如此流行？一些关键原因如下：
- en: Hardware availability
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件可用性
- en: Data and algorithms
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据与算法
- en: Deep learning frameworks
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习框架
- en: Hardware availability
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件可用性
- en: Deep learning requires complex mathematical operations to be performed on millions,
    sometimes billions, of parameters. Existing CPUs take a long time to perform these
    kinds of operations, although this has improved over the last several years. A
    new kind of hardware called a **graphics processing unit** (**GPU**) has completed
    these huge mathematical operations, such as matrix multiplications, orders of
    magnitude faster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习需要在数百万、有时甚至数十亿个参数上执行复杂的数学操作。现有的CPU执行这些操作需要很长时间，尽管在过去几年已有所改进。一种新型的硬件称为**图形处理单元**（**GPU**）能够以数量级更快的速度完成这些大规模的数学运算，如矩阵乘法。
- en: GPUs were initially built for the gaming industry by companies such as Nvidia
    and AMD. It turned out that this hardware is extremely efficient, not only for
    rendering high quality video games, but also to speed up the DL algorithms. One
    recent GPU from Nvidia, the *1080ti*, takes a few days to build an image-classification
    system on top of an `ImageNet` dataset, which previously could have taken around
    a month.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，GPU是由Nvidia和AMD等公司为游戏行业构建的。事实证明，这种硬件不仅在渲染高质量视频游戏时非常有效，还能加速DL算法。最近Nvidia推出的一款GPU，*1080ti*，仅需几天即可在`ImageNet`数据集上构建出图像分类系统，而此前可能需要大约一个月。
- en: If you are planning to buy hardware for running deep learning, I would recommend
    choosing a GPU from Nvidia based on your budget. Choose one with a good amount
    of memory. Remember, your computer memory and GPU memory are two different things.
    The 1080ti comes with 11 GB of memory and it costs around $700.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划购买用于运行深度学习的硬件，我建议根据预算选择一款来自Nvidia的GPU。根据你的预算选择一款内存足够的GPU。请记住，你的计算机内存和GPU内存是两回事。1080ti配备了11
    GB的内存，价格约为700美元。
- en: You can also use various cloud providers such as AWS, Google Cloud, or Floyd
    (this company offers GPU machines optimized for DL). Using a cloud provider is
    economical if you are just starting with DL or if you are setting up machines
    for organization usage where you may have more financial freedom.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用AWS、Google Cloud或Floyd（该公司提供专为深度学习优化的GPU机器）等各种云服务提供商。如果您刚开始学习深度学习或者为组织使用设置机器时具有更多的财务自由度，使用云服务提供商是经济的选择。
- en: Performance could vary if these systems are optimized.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些系统经过优化，性能可能会有所变化。
- en: 'The following image shows some of the benchmarks that compare performance between
    CPUs and GPUs :'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了CPU和GPU之间性能比较的一些基准：
- en: '![](img/c49812cd-7561-4820-8fdf-5d88bd969bbf.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c49812cd-7561-4820-8fdf-5d88bd969bbf.png)'
- en: Performance benchmark of neural architectures on CPUs and GPUs (Image source: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture8.pdf)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU和GPU上神经架构的性能基准（图片来源：http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture8.pdf）
- en: Data and algorithms
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据和算法
- en: Data is the most important ingredient for the success of deep learning. Due
    to the wide adoption of the internet and the growing use of smartphones, several
    companies, such as Facebook and Google, have been able to collect a lot of data
    in various formats, particularly text, images, videos, and audio. In the field
    of computer vision, ImageNet competitions have played a huge role in providing
    datasets of 1.4 million images in 1,000 categories.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是深度学习成功的最重要组成部分。由于互联网的广泛采用和智能手机的增长使用，一些公司（如Facebook和Google）已经能够收集大量数据，包括文本、图片、视频和音频等多种格式。在计算机视觉领域，ImageNet竞赛在提供了包括1,000个类别的1.4百万张图像数据集方面发挥了巨大作用。
- en: 'These categories are hand-annotated and every year hundreds of teams compete.
    Some of the algorithms that were successful in the competition are VGG, ResNet,
    Inception, DenseNet, and many more. These algorithms are used today in industries
    to solve various computer vision problems. Some of the other popular datasets
    that are often used in the deep learning space to benchmark various algorithms
    are as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别是手动注释的，每年有数百个团队参加竞争。在竞赛中成功的一些算法包括VGG、ResNet、Inception、DenseNet等。这些算法今天在行业中用于解决各种计算机视觉问题。在深度学习领域中经常用来对比各种算法性能的其他流行数据集如下：
- en: MNIST
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST
- en: COCO dataset
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: COCO数据集
- en: CIFAR
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CIFAR
- en: The Street View House Numbers
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 街景房屋数字
- en: PASCAL VOC
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PASCAL VOC
- en: Wikipedia dump
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wikipedia的数据集
- en: 20 Newsgroups
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20个新闻组
- en: Penn Treebank
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penn Treebank
- en: Kaggle
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle
- en: The growth of different algorithms such as batch normalization, activation functions,
    skip connections, **Long Short-Term Memory** (**LSTM**), dropouts, and many more
    have made it possible in recent years to train very deep networks faster and more
    successfully. In the coming chapters of this book, we will get into the details
    of each technique and how they help in building better models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年不同算法的发展，如批归一化、激活函数、跳跃连接、**长短期记忆网络（LSTM）**、dropout等，使得能够更快、更成功地训练非常深的网络。在本书的接下来章节中，我们将详细讨论每个技术以及它们如何帮助构建更好的模型。
- en: Deep learning frameworks
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习框架
- en: In the earlier days, people needed to have expertise in C++ and CUDA to implement
    DL algorithms. With a lot of organizations now open sourcing their deep learning
    frameworks, people with knowledge of a scripting language, such as Python, can
    start building and using DL algorithms. Some of the popular deep learning frameworks
    used today in the industry are TensorFlow, Caffe2, Keras, Theano, PyTorch, Chainer,
    DyNet, MXNet, and CNTK.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，人们需要具备C++和CUDA的专业知识来实现深度学习算法。随着许多组织现在开源他们的深度学习框架，具备脚本语言（如Python）知识的人员就可以开始构建和使用深度学习算法。今天在行业中使用的一些流行的深度学习框架包括TensorFlow、Caffe2、Keras、Theano、PyTorch、Chainer、DyNet、MXNet和CNTK。
- en: The adoption of deep learning would not have been this huge if it had not been
    for these frameworks. They abstract away a lot of underlying complications and
    allow us to focus on the applications. We are still in the early days of DL where,
    with a lot of research, breakthroughs are happening every day across companies
    and organizations. As a result of this, various frameworks have their own pros
    and cons.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这些框架的存在，深度学习的采用不可能会如此巨大。它们抽象了很多底层复杂性，使我们能够专注于应用。我们仍处于深度学习的早期阶段，在各公司和组织中每天都有许多研究突破。因此，各种框架都有其优缺点。
- en: PyTorch
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch
- en: 'PyTorch, and most of the other deep learning frameworks, can be used for two
    different things:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch以及大多数其他深度学习框架，可以用于两种不同的目的：
- en: Replacing NumPy-like operations with GPU-accelerated operations
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPU加速操作替代类似NumPy的操作
- en: Building deep neural networks
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度神经网络
- en: What makes PyTorch increasingly popular is its ease of use and simplicity. Unlike
    most other popular deep learning frameworks, which use static computation graphs,
    PyTorch uses dynamic computation, which allows greater flexibility in building
    complex architectures.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使PyTorch日益流行的原因是其易用性和简单性。与大多数其他流行的深度学习框架使用静态计算图不同，PyTorch使用动态计算，允许更大的灵活性来构建复杂的架构。
- en: PyTorch extensively uses Python concepts, such as classes, structures, and conditional
    loops, allowing us to build DL algorithms in a pure object-oriented fashion. Most
    of the other popular frameworks bring their own programming style, sometimes making
    it complex to write new algorithms and it does not support intuitive debugging.
    In the later chapters, we will discuss computation graphs in detail.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch广泛使用Python概念，如类、结构和条件循环，使我们能够以纯面向对象的方式构建深度学习算法。大多数其他流行的框架带来了它们自己的编程风格，有时使编写新算法复杂化，并且不支持直观的调试。在后续章节中，我们将详细讨论计算图。
- en: Though PyTorch was released recently and is still in its beta version, it has
    become immensely popular among data scientists and deep learning researchers for
    its ease of use, better performance, easier-to-debug nature, and strong growing
    support from various companies such as SalesForce.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PyTorch最近发布并且仍处于beta版本阶段，但由于其易用性、更好的性能、易于调试的特性以及来自SalesForce等各种公司的强大支持，它已经在数据科学家和深度学习研究人员中广受欢迎。
- en: As PyTorch was primarily built for research, it is not recommended for production
    usage in certain scenarios where latency requirements are very high. However,
    this is changing with a new project called **Open Neural Network Exchange** (**ONNX**)
    ([https://onnx.ai/](https://onnx.ai/)), which focuses on deploying a model developed
    on PyTorch to a platform like Caffe2 that is production-ready. At the time of
    writing, it is too early to say much about this project as it has only just been
    launched. The project is backed by Facebook and Microsoft.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PyTorch主要是为研究而构建的，在某些对延迟要求非常高的情况下，不推荐用于生产环境。然而，随着一个名为**Open Neural Network
    Exchange**（**ONNX**）的新项目的推出（[https://onnx.ai/](https://onnx.ai/)），情况正在发生变化，该项目致力于将在PyTorch上开发的模型部署到像Caffe2这样的生产就绪平台。在撰写本文时，关于这个项目还为时过早。该项目由Facebook和Microsoft支持。
- en: Throughout the rest of the book, we will learn about the various Lego blocks
    (smaller concepts or techniques) for building powerful DL applications in the
    areas of computer vision and NLP.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们将学习关于在计算机视觉和自然语言处理领域构建强大深度学习应用程序的各种乐高积木（更小的概念或技术）。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this introductory chapter, we explored what artificial intelligence, machine
    learning, and deep learning are and we discussed the differences between all the
    three. We also looked at applications powered by them in our day-to-day lives.
    We dig deeper into why DL is only now becoming more popular. Finally, we gave
    a gentle introduction to PyTorch, which is a deep learning framework.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的介绍中，我们探讨了人工智能、机器学习和深度学习的定义，并讨论了它们之间的区别。我们还看了它们在日常生活中的应用。我们深入探讨了为什么深度学习现在才变得更加流行。最后，我们对PyTorch进行了初步介绍，这是一个深度学习框架。
- en: In the next chapter, we will train our first neural network in PyTorch.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在PyTorch中训练我们的第一个神经网络。
