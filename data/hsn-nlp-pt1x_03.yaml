- en: '*Chapter 2*: Getting Started with PyTorch 1.x for NLP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*：开始使用 PyTorch 1.x 进行自然语言处理'
- en: '**PyTorch** is a Python-based machine learning library. It consists of two
    main features: its ability to efficiently perform tensor operations with hardware
    acceleration (using GPUs) and its ability to build deep neural networks. PyTorch
    also uses dynamic computational graphs instead of static ones, which sets it apart
    from similar libraries such as TensorFlow. By demonstrating how language can be
    represented using tensors and how neural networks can be used to learn from NLP,
    we will show that both these features are particularly useful for natural language
    processing.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch** 是一个基于 Python 的机器学习库。它主要有两个特点：能够高效地使用硬件加速（使用 GPU）进行张量运算，以及能够构建深度神经网络。PyTorch
    还使用动态计算图而不是静态计算图，这使其与 TensorFlow 等类似库有所不同。通过展示如何使用张量表示语言以及如何使用神经网络从自然语言处理中学习，我们将展示这两个特点对于自然语言处理特别有用。'
- en: 'In this chapter, we will show you how to get PyTorch up and running on your
    computer, as well as demonstrate some of its key functionalities. We will then
    compare PyTorch to some other deep learning frameworks, before exploring some
    of the NLP functionality of PyTorch, such as its ability to perform tensor operations,
    and finally demonstrate how to build a simple neural network. In summary, this
    chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示如何在计算机上安装和运行 PyTorch，并演示其一些关键功能。然后，我们将比较 PyTorch 与一些其他深度学习框架，然后探索
    PyTorch 的一些自然语言处理功能，如其执行张量操作的能力，并最后演示如何构建一个简单的神经网络。总之，本章将涵盖以下主题：
- en: Installing PyTorch
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: Comparing PyTorch to other deep learning frameworks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 PyTorch 与其他深度学习框架进行比较
- en: NLP functionality of PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 的自然语言处理功能
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, Python needs to be installed. It is recommended to use the
    latest version of Python (3.6 or higher). It is also recommended to use the Anaconda
    package manager to install PyTorch. A CUDA-compatible GPU is required to run tensor
    operations on a GPU. All the code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要安装 Python。建议使用最新版本的 Python（3.6 或更高版本）。还建议使用 Anaconda 包管理器安装 PyTorch。需要 CUDA
    兼容的 GPU 来在 GPU 上运行张量操作。本章所有代码可以在 [https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x](https://github.com/PacktPublishing/Hands-On-Natural-Language-Processing-with-PyTorch-1.x)
    找到。
- en: Installing and using PyTorch 1.x
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和使用 PyTorch 1.x
- en: 'Like most Python packages, PyTorch is very simple to install. There are two
    main ways of doing so. The first is to simply install it using `pip` in the command
    line. Simply type the following command:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数 Python 包一样，PyTorch 安装非常简单。有两种主要方法。第一种是在命令行中使用 `pip` 直接安装。只需输入以下命令：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'While this installation method is quick, it is recommended to install using
    Anaconda instead, as this includes all the required dependencies and binaries
    for PyTorch to run. Furthermore, Anaconda will be required later to enable training
    models on a GPU using CUDA. PyTorch can be installed through Anaconda by entering
    the following in the command line:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种安装方法很快，但建议改用 Anaconda 安装，因为它包含了 PyTorch 运行所需的所有依赖项和二进制文件。此外，后续需要使用 Anaconda
    来启用 CUDA 在 GPU 上进行模型训练。可以通过在命令行中输入以下内容来通过 Anaconda 安装 PyTorch：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To check that PyTorch is working correctly, we can open a Jupyter Notebook
    and run a few simple commands:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查 PyTorch 是否正确工作，我们可以打开 Jupyter Notebook 并运行几个简单的命令：
- en: 'To define a Tensor in PyTorch, we can do the following:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在 PyTorch 中定义一个 Tensor，我们可以这样做：
- en: '[PRE2]'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in the following output:'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '`![Figure 2.1 – Tensor output  ](img/B12365_02_1.png)`'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`![Figure 2.1 – Tensor output  ](img/B12365_02_1.png)`'
- en: Figure 2.1 – Tensor output
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.1 – 张量输出
- en: This shows that tensors within PyTorch are saved as their own data type (not
    dissimilar to how arrays are saved within NumPy).
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这表明 PyTorch 中的张量被保存为它们自己的数据类型（与 NumPy 中保存数组的方式类似）。
- en: 'We can perform basic operations such as multiplication using standard Python
    operators:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用标准的 Python 运算符进行基本操作，比如乘法：
- en: '[PRE3]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![Figure 2.2 – Tensor multiplication output'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 2.2 – Tensor multiplication output'
- en: '](img/B12365_02_2.jpg)'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_02_2.jpg)'
- en: Figure 2.2 – Tensor multiplication output
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.2 – 张量乘法输出
- en: 'We can also select individual elements from a tensor, as follows:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以按如下方式选择张量中的单个元素：
- en: '[PRE4]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![Figure 2.3 – Tensor selection output'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3 – 张量选择输出'
- en: '](img/B12365_02_3.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_3.jpg)'
- en: Figure 2.3 – Tensor selection output
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 张量选择输出
- en: 'However, note that unlike a NumPy array, selecting an individual element from
    a tensor object returns another tensor. In order to return an individual value
    from a tensor, you can use the `.item()` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但请注意，与NumPy数组不同，从张量对象中选择单个元素会返回另一个张量。为了从张量中返回单个值，您可以使用`.item()`函数：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This results in the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![Figure 2.4 – Output of the .item() function'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4 – .item() 函数输出'
- en: '](img/B12365_02_4.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_4.jpg)'
- en: Figure 2.4 – Output of the .item() function
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – .item() 函数输出
- en: Tensors
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量
- en: 'Before we continue, it is important that you are fully aware of the properties
    of a tensor. Tensors have a property known as an **order**, which essentially
    determines the dimensionality of a tensor. An order one tensor is a tensor with
    a single dimension, which is equivalent to a vector or list of numbers. An order
    2 tensor is a tensor with two dimensions, equivalent to a matrix, whereas a tensor
    of order 3 consists of three dimensions. There is no limit to the maximum order
    a tensor can have within PyTorch:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，您必须充分了解张量的属性是非常重要的。张量有一个称为**阶**的属性，它基本上确定了张量的维数。阶为一的张量是具有单个维度的张量，等同于一个向量或数字列表。阶为2的张量是具有两个维度的张量，相当于矩阵，而阶为3的张量包含三个维度。在PyTorch中，张量的最大阶数没有限制：
- en: '![Figure 2.5 – Tensor matrix'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5 – 张量矩阵'
- en: '](img/B12365_02_5.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_5.jpg)'
- en: Figure 2.5 – Tensor matrix
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 张量矩阵
- en: 'You can check the size of any tensor by typing the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过输入以下内容来检查任何张量的大小：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![Figure 2.6 – Tensor shape output'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6 – 张量形状输出'
- en: '](img/B12365_02_6.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_6.jpg)'
- en: Figure 2.6 – Tensor shape output
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 张量形状输出
- en: This shows that this is a 3x2 tensor (order 2).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示这是一个3x2的张量（阶为2）。
- en: Enabling PyTorch acceleration using CUDA
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA加速PyTorch
- en: One of the main benefits of PyTorch is its ability to enable acceleration through
    the use of a **graphics processing unit** (**GPU**). Deep learning is a computational
    task that is easily parallelizable, meaning that the calculations can be broken
    down into smaller tasks and calculated across many smaller processors. This means
    that instead of needing to execute the task on a single CPU, it is more efficient
    to perform the calculation on a GPU.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的主要好处之一是通过**图形处理单元**（**GPU**）实现加速能力。深度学习是一种易于并行化的计算任务，这意味着可以将计算任务分解为较小的任务，并在许多较小的处理器上计算。这意味着与在单个CPU上执行任务相比，在GPU上执行计算更为高效。
- en: GPUs were originally created to efficiently render graphics, but since deep
    learning has grown in popularity, GPUs have been frequently used for their ability
    to perform multiple calculations simultaneously. While a traditional CPU may consist
    of around four or eight cores, a GPU consists of hundreds of smaller cores. Because
    calculations can be executed across all these cores simultaneously, GPUs can rapidly
    reduce the time taken to perform deep learning tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GPU最初是为高效渲染图形而创建的，但随着深度学习的流行，GPU因其同时执行多个计算的能力而经常被使用。传统CPU可能由大约四到八个核心组成，而GPU由数百个较小的核心组成。由于可以在所有这些核心上同时执行计算，GPU可以快速减少执行深度学习任务所需的时间。
- en: 'Consider a single pass within a neural network. We may take a small batch of
    data, pass it through our network to obtain our loss, and then backpropagate,
    adjusting our parameters according to the gradients. If we have many batches of
    data to do this over, on a traditional CPU, we must wait until batch 1 has completed
    before we can compute this for batch 2:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑神经网络中的单次传递。我们可以取一小批数据，通过网络传递以获取损失，然后进行反向传播，根据梯度调整参数。如果我们有多批数据要处理，在传统CPU上，我们必须等到批次1完成后才能为批次2计算：
- en: '![Figure 2.7 – One pass in a neural network'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7 – 神经网络中的一次传递'
- en: '](img/B12365_02_7.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_7.jpg)'
- en: Figure 2.7 – One pass in a neural network
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – 神经网络中的一次传递
- en: 'However, on a GPU, we can perform all these steps simultaneously, meaning there
    is no requirement for batch 1 to finish before batch 2 can be started. We can
    calculate the parameter updates for all batches simultaneously and then perform
    all the parameter updates in one go (as the results are independent of one another).
    The parallel approach can vastly speed up the machine learning process:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在GPU上，我们可以同时执行所有这些步骤，这意味着在批次1完成之前没有批次2的要求。我们可以同时计算所有批次的参数更新，然后一次性执行所有参数更新（因为结果是彼此独立的）。并行方法可以极大地加速机器学习过程：
- en: '![Figure 2.8 – Parallel approach to perform passes'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8 – 并行执行传递的方法'
- en: '](img/B12365_02_8.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_8.jpg)'
- en: Figure 2.8 – Parallel approach to perform passes
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 并行执行传递的方法
- en: '**Compute Unified Device Architecture** (**CUDA**) is the technology specific
    to Nvidia GPUs that enables hardware acceleration on PyTorch. In order to enable
    CUDA, we must first make sure the graphics card on our system is CUDA-compatible.
    A list of CUDA-compatible GPUs can be found here: [https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus).
    If you have a CUDA-compatible GPU, then CUDA can be installed from this link:
    [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
    We will activate it using the following steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**CUDA（Compute Unified Device Architecture）**是专为Nvidia GPU设计的技术，可以在PyTorch上实现硬件加速。为了启用CUDA，首先必须确保系统上的显卡兼容CUDA。可以在此处找到支持CUDA的GPU列表：[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)。如果您有兼容CUDA的GPU，则可以从此链接安装CUDA：[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)。我们将使用以下步骤来激活它：'
- en: 'Firstly, in order to actually enable CUDA support on PyTorch, you will have
    to build PyTorch from source. Details about how this can be done can be found
    here: https://github.com/pytorch/pytorch#from-source.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，为了在PyTorch上实际启用CUDA支持，您必须从源代码构建PyTorch。有关如何执行此操作的详细信息可以在此处找到：[https://github.com/pytorch/pytorch#from-source](https://github.com/pytorch/pytorch#from-source)。
- en: 'Then, to actually CUDA within our PyTorch code, we must type the following
    into our Python code:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在我们的PyTorch代码中实际使用CUDA，我们必须在Python代码中输入以下内容：
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This sets our default CUDA device's name to `'cuda'`.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将设置我们默认的CUDA设备名称为`'cuda'`。
- en: 'We can then execute operations on this device by manually specifying the device
    argument in any tensor operations:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以通过在任何张量操作中手动指定设备参数来在此设备上执行操作：
- en: '[PRE8]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Alternatively, we can do this by calling the `cuda` method:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，我们可以通过调用`cuda`方法来实现：
- en: '[PRE9]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can then run a simple operation to ensure this is working correctly:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以运行一个简单的操作来确保这个工作正常：
- en: '[PRE10]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following output:'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![Figure 2.9 – Tensor multiplication output using CUDA'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9 – 使用CUDA进行张量乘法输出'
- en: '](img/B12365_02_9.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_9.jpg)'
- en: Figure 2.9 – Tensor multiplication output using CUDA
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – 使用CUDA进行张量乘法输出
- en: The changes in speed will not be noticeable at this stage as we are just creating
    a tensor, but when we begin training models at scale later, we will see the speed
    benefits of parallelizing our computations using CUDA. By training our models
    in parallel, we will be able to reduce the time this takes by a considerable amount.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，由于我们只是创建一个张量，所以速度上的变化并不明显，但当我们稍后开始规模化训练模型时，我们将看到使用CUDA并行化计算可以带来速度上的好处。通过并行训练我们的模型，我们能够大大缩短这个过程所需的时间。
- en: Comparing PyTorch to other deep learning frameworks
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将PyTorch与其他深度学习框架进行比较
- en: 'PyTorch is one of the main frameworks used in deep learning today. There are
    other widely used frameworks available too, such as TensorFlow, Theano, and Caffe.
    While these are very similar in many ways, there are some key differences in how
    they operate. These include the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是今天深度学习中使用的主要框架之一。还有其他广泛使用的框架，如TensorFlow、Theano和Caffe。尽管在许多方面它们非常相似，但它们在操作方式上有一些关键区别。其中包括以下内容：
- en: How the models are computed
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型计算的方式
- en: The way in which the computational graphs are compiled
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图编译的方式
- en: The ability to create dynamic computational graphs with variable layers
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够创建具有可变层的动态计算图的能力
- en: Differences in syntax
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法上的差异
- en: Arguably, the main difference between PyTorch and other frameworks is in the
    way that the models themselves are computed. PyTorch uses an automatic differentiation
    method called **autograd**, which allows computational graphs to be defined and
    executed dynamically. This is in contrast to other frameworks such as TensorFlow,
    which is a static framework. In these static frameworks, computational graphs
    must be defined and compiled before finally being executed. While using pre-compiled
    models may lead to efficient implementations in production, they do not offer
    the same level of flexibility in research and explorational projects.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，PyTorch与其他框架的主要区别在于其模型计算方式的不同。PyTorch使用一种称为**autograd**的自动微分方法，允许动态定义和执行计算图。这与TensorFlow等静态框架形成对比。在这些静态框架中，必须先定义和编译计算图，然后才能最终执行。虽然使用预编译模型可能会导致在生产环境中实现高效，但在研究和探索性项目中，它们不提供同样级别的灵活性。
- en: 'Frameworks such as PyTorch do not need to pre-compile computational graphs
    before the model can be trained. The dynamic computational graphs used by PyTorch
    mean that graphs are compiled as they are executed, which allows graphs to be
    defined on the go. The dynamic approach to model construction is particularly
    useful in the field of NLP. Let''s consider two sentences that we wish to perform
    sentiment analysis on:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如PyTorch之类的框架在模型训练之前不需要预编译计算图。PyTorch使用的动态计算图意味着在执行时编译图形，这允许在执行过程中动态定义图形。在NLP领域，动态模型构建方法尤其有用。让我们考虑两个我们希望进行情感分析的句子：
- en: '![Figure 2.10 – Model construction in PyTorch'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.10 – PyTorch中的模型构建'
- en: '](img/B12365_02_10.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_10.jpg)'
- en: Figure 2.10 – Model construction in PyTorch
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – PyTorch中的模型构建
- en: We can represent each of these sentences as a sequence of individual word vectors,
    which would then form our input to our neural network. However, as we can see,
    each of our inputs is of a different size. Within a fixed computation graph, these
    varying input sizes could be a problem, but for frameworks like PyTorch, models
    are able to adjust dynamically to account for the variation in input structure.
    This is one reason why PyTorch is often preferred for NLP-related deep learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些句子表示为单词向量的序列，这些向量将成为我们神经网络的输入。然而，正如我们所看到的，我们的每个输入大小不同。在固定的计算图内，这些不同的输入大小可能是一个问题，但是对于像PyTorch这样的框架，模型能够动态调整以适应输入结构的变化。这也是为什么PyTorch在与NLP相关的深度学习中经常被优先选择的原因之一。
- en: Another major difference between PyTorch and other deep learning frameworks
    is syntax. PyTorch is often preferred by developers with experience in Python
    as it is considered to be very Pythonic in nature. PyTorch integrates well with
    other aspects of the Python ecosystem and it is very easy to learn if you have
    prior knowledge of Python. We will demonstrate PyTorch syntax now by coding up
    our own neural network from scratch.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch与其他深度学习框架的另一个主要区别在于语法。对于有Python经验的开发者来说，PyTorch通常更受欢迎，因为它在性质上被认为非常符合Python风格。PyTorch与Python生态系统的其他方面集成良好，如果你具备Python的先验知识，学习起来非常容易。现在我们将通过从头开始编写我们自己的神经网络来演示PyTorch的语法。
- en: Building a simple neural network in PyTorch
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中构建简单的神经网络
- en: 'We will now walk through building a neural network from scratch in PyTorch.
    Here, we have a small `.csv` file containing several examples of images from the
    MNIST dataset. The MNIST dataset consists of a collection of hand-drawn digits
    between 0 and 9 that we want to attempt to classify. The following is an example
    from the MNIST dataset, consisting of a hand-drawn digit 1:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将介绍如何在PyTorch中从头开始构建神经网络。这里，我们有一个包含来自MNIST数据集中几个图像示例的小`.csv`文件。MNIST数据集包含一系列手绘的0到9之间的数字，我们希望尝试对其进行分类。以下是来自MNIST数据集的一个示例，其中包含一个手绘的数字1：
- en: '![Figure 2.11 – Sample image from the MNIST dataset'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.11 – MNIST数据集的示例图像'
- en: '](img/B12365_02_11.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_11.jpg)'
- en: Figure 2.11 – Sample image from the MNIST dataset
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – MNIST数据集的示例图像
- en: 'These images are 28x28 in size: 784 pixels in total. Our dataset in `train.csv`
    consists of 1,000 of these images, with each consisting of 784 pixel values, as
    well as the correct classification of the digit (in this case, 1).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像的尺寸为28x28：总共784个像素。我们的训练数据集`train.csv`包含1,000个这样的图像，每个图像由784个像素值组成，以及数字（在本例中为1）的正确分类。
- en: Loading the data
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'We will begin by loading the data, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从加载数据开始，如下所示：
- en: 'First, we need to load our training dataset, as follows:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要加载我们的训练数据集，如下所示：
- en: '[PRE11]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice that we reshaped our input to (`1,` `1,` `28,` `28`), which is a tensor
    of 1,000 images, each consisting of 28x28 pixels.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们将输入重塑为（`1,` `1,` `28,` `28`），这是一个包含1,000个图像的张量，每个图像由28x28像素组成。
- en: 'Next, we convert our training data and training labels into PyTorch tensors
    so they can be fed into the neural network:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将训练数据和训练标签转换为PyTorch张量，以便它们可以被馈送到神经网络中。
- en: '[PRE12]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note the data types of these two tensors. A float tensor comprises 32-bit floating-point
    numbers, while a long tensor consists of 64-bit integers. Our `X` features must
    be floats in order for PyTorch to be able to compute gradients, while our labels
    must be integers within this classification model (as we're trying to predict
    values of 1, 2, 3, and so on), so a prediction of 1.5 wouldn't make sense.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这两个张量的数据类型。一个浮点张量包含32位浮点数，而长张量包含64位整数。我们的`X`特征必须是浮点数，以便PyTorch能够计算梯度，而我们的标签必须是整数，这在这个分类模型中是合理的（因为我们试图预测1、2、3等的值），因此预测1.5没有意义。
- en: Building the classifier
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建分类器
- en: 'Next, we can start to construct our actual neural network classifier:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始构建实际的神经网络分类器：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We build our classifier as if we were building a normal class in Python, inheriting
    from `nn.Module` in PyTorch. Within our `init` method, we define each of the layers
    of our neural network. Here, we define fully connected linear layers of varying
    sizes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建分类器时，就像构建Python中的普通类一样，从PyTorch的`nn.Module`继承。在我们的`init`方法中，我们定义了神经网络的每一层。在这里，我们定义了不同大小的全连接线性层。
- en: Our first layer takes **784** inputs as this is the size of each of our images
    to classify (28x28). We then see that the output of one layer must have the same
    value as the input of the next one, which means our first fully connected layer
    outputs **392** units and our second layer takes **392** units as input. This
    is repeated for each layer, with them having half the number of units each time
    until we reach our final fully connected layer, which outputs **10** units. This
    is the length of our classification layer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一层接受**784**个输入，因为这是每个图像的大小（28x28）。然后我们看到一个层的输出必须与下一个层的输入具有相同的值，这意味着我们的第一个全连接层输出**392**个单元，我们的第二层接受**392**个单元作为输入。这样的过程对每一层都重复进行，每次单元数减半，直到我们达到最终的全连接层，其输出**10**个单元。这是我们分类层的长度。
- en: 'Our network now looks something like this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络现在看起来像这样：
- en: '![Figure 2.12 – Our neural network'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.12 – 我们的神经网络'
- en: '](img/B12365_02_12.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_12.jpg)'
- en: Figure 2.12 – Our neural network
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – 我们的神经网络
- en: Here, we can see that our final layer outputs **10** units. This is because
    we wish to predict whether each image is a digit between 0 and 9, which is 10
    different possible classifications in total. Our output is a vector of length
    **10** and contains predictions for each of the 10 possible values of the image.
    When making a final classification, we take the digit classification that has
    the highest value as the model's final prediction. For example, for a given prediction,
    our model might predict the image is type 1 with a probability of 10%, type 2
    with a probability of 10%, and type 3 with a probability of 80%. We would, therefore,
    take type 3 as the prediction as it was predicted with the highest probability.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的最终层输出**10**个单元。这是因为我们希望预测每个图像是否是0到9之间的数字，总共有10种不同的可能分类。我们的输出是长度为**10**的向量，并包含对图像的每个可能值的预测。在做最终分类时，我们将具有最高值的数字分类作为模型的最终预测。例如，对于给定的预测，我们的模型可能以10%的概率预测图像是类型1，以10%的概率预测图像是类型2，以80%的概率预测图像是类型3。因此，我们将类型3作为预测结果，因为它以最高的概率进行了预测。
- en: Implementing dropout
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施**dropout**
- en: 'Within the `init` method of our `MNISTClassifier` class, we also define a dropout
    method in order to help regularize the network:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`MNISTClassifier`类的`init`方法中，我们还定义了一个dropout方法，以帮助正则化网络。
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Dropout is a way of regularizing our neural networks to prevent overfitting.
    On each training epoch, for each node in a layer that has dropout applied, there
    is a probability (here, defined as *p* = 20%) that each node within the layer
    will not be used in training/backpropagation. This means that when training, our
    network becomes robust toward overfitting since each node will not be used in
    every iteration of the training process. This prevents our network from becoming
    too reliant on predictions from specific nodes within our network.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种正则化神经网络的方法，用于防止过拟合。在每个训练 epoch 中，对于每个应用了 dropout 的层中的节点，存在一定的概率（这里定义为
    *p* = 20%），使得该层中的每个节点在训练和反向传播过程中都不被使用。这意味着在训练过程中，我们的网络变得对过拟合更加健壮，因为每个节点都不会在每次迭代中都被使用。这样一来，我们的网络就不会过度依赖网络中特定节点的预测。
- en: Defining the forward pass
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义前向传播
- en: 'Next, we define the forward pass within our classifier:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在分类器中定义前向传播：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `forward()` method within our classifier is where we apply our activation
    functions and define where dropout is applied within our network. Our `forward`
    method defines the path our input will take through the network. It first takes
    our input, `x,` and reshapes it for use within the network, transforming it into
    a one-dimensional vector. We then pass it through our first fully connected layer
    and wrap it in a `ReLU` activation function to make it non-linear. We also wrap
    it in our dropout, as defined in our `init` method. We repeat this process for
    all the other layers in the network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分类器中的 `forward()` 方法是我们应用激活函数并定义网络中 dropout 的地方。我们的 `forward` 方法定义了输入将如何通过网络。首先接收我们的输入
    `x`，并将其重塑为网络中使用的一维向量。然后，我们通过第一个全连接层，并使用 `ReLU` 激活函数使其非线性化。我们还在 `init` 方法中定义了 dropout。我们将这个过程在网络的所有其他层中重复进行。
- en: For our final prediction layer, we wrap it in a log `softmax` layer. We will
    use this to easily calculate our loss function, as we will see next.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的最终预测层，我们将其包裹在一个对数 `softmax` 层中。我们将使用这个层来轻松计算我们的损失函数，接下来我们会看到。
- en: Setting the model parameters
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置模型参数
- en: 'Next, we define our model parameters:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的模型参数：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We initialize an instance of our `MNISTClassifier` class as a model. We also
    define our loss as a **Negative Log Likelihood Loss**:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `MNISTClassifier` 类实例化为模型的一个实例。我们还将我们的损失定义为 **负对数似然损失**：
- en: Loss(y) = -log(y)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Loss(y) = -log(y)
- en: 'Let''s assume our image is of a number 7\. If we predict class 7 with probability
    1, our loss will be *-log(1) = 0*, but if we only predict class 7 with probability
    0.7, our loss will be *-log(0.7) = 0.3*. This means that our loss approaches infinity
    the further away from the correct prediction we are:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的图像是数字 7。如果我们以概率 1 预测类别 7，我们的损失将是 *-log(1) = 0*，但如果我们只以概率 0.7 预测类别 7，我们的损失将是
    *-log(0.7) = 0.3*。这意味着我们的损失会随着预测偏离正确答案而无限增加：
- en: '![Figure 2.13 – Representation of loss for our network'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13 – 我们网络的损失表示'
- en: '](img/B12365_02_13.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_13.jpg)'
- en: Figure 2.13 – Representation of loss for our network
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13 – 我们网络的损失表示
- en: This is then summed over all the correct classes in our dataset to compute the
    total loss. Note that we defined a log softmax when building the classifier as
    this already applies a softmax function (restricting the predicted output to be
    between 0 and 1) and takes the log. This means that *log(y)* is already calculated,
    so all we need to do to compute the total loss on the network is calculate the
    negative sum of the outputs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对数据集中所有正确类别求和，计算总损失。注意，在构建分类器时，我们定义了对数 softmax 函数，因此已经应用了 softmax 函数（将预测输出限制在
    0 到 1 之间）并取了对数。这意味着 *log(y)* 已经计算好了，所以我们计算网络的总损失只需计算输出的负和。
- en: We will also define our optimizer as an Adam optimizer. An optimizer controls
    the **learning rate** within our model. The learning rate of a model defines how
    big the parameter updates are during each epoch of training. The larger the size
    of the learning rate, the larger the size of the parameter updates during gradient
    descent. An optimizer dynamically controls this learning rate so that when a model
    is initialized, the parameter updates are large. However, as the model learns
    and moves closer to the point where loss is minimized, the optimizer controls
    the learning rate, so the parameter updates become smaller and the local minimum
    can be located more precisely.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将我们的优化器定义为 Adam 优化器。优化器控制模型内的**学习率**。模型的学习率定义了训练的每个周期中参数更新的大小。学习率越大，梯度下降中参数更新的大小越大。优化器动态控制这个学习率，因此当模型初始化时，参数更新很大。但是，随着模型的学习并接近最小化损失的点，优化器控制学习率，使参数更新变小，可以更精确地定位局部最小值。
- en: Training our network
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练我们的网络
- en: 'Finally, we can actually start training our network:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实际开始训练我们的网络：
- en: 'First, create a loop that runs once for each epoch of our training. Here, we
    will run our training loop for 50 epochs. We first take our input tensor of images
    and our output tensor of labels and transform them into PyTorch variables. A `variable`
    is a PyTorch object that contains a `backward()` method that we can use to perform
    backpropagation through our network:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个循环，每个训练周期运行一次。在这里，我们将运行我们的训练循环共 50 个周期。我们首先取出图像的输入张量和标签的输出张量，并将它们转换为
    PyTorch 变量。`variable` 是一个 PyTorch 对象，其中包含一个 `backward()` 方法，我们可以用它来执行网络的反向传播：
- en: '[PRE17]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we call `zero_grad()` on our optimizer to set our calculated gradients
    to zero. Within PyTorch, gradients are calculated cumulatively on each backpropagation.
    While this is useful in some models, such as when training RNNs, for our example,
    we wish to calculate the gradients from scratch after each epoch, so we make sure
    to reset the gradients to zero after each pass:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在我们的优化器上调用 `zero_grad()` 来将计算得到的梯度设置为零。在 PyTorch 中，梯度是在每次反向传播时累积计算的。虽然这对于某些模型（如训练
    RNNs 时）很有用，但对于我们的例子，我们希望在每次通过后从头开始计算梯度，所以确保在每次通过后将梯度重置为零：
- en: '[PRE18]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we use our model''s current state to make predictions on our dataset.
    This is effectively our forward pass as we then use these predictions to calculate
    our loss:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用模型的当前状态在数据集上进行预测。这实际上是我们的前向传递，因为我们使用这些预测来计算我们的损失：
- en: '[PRE19]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using the outputs and the true labels of our dataset, we calculate the total
    loss of our model using the defined loss function, which in this case is the negative
    log likelihood. On calculating this loss, we can then make a `backward()` call
    to backpropagate our loss through the network. We then use `step()` using our
    optimizer in order to update our model parameters accordingly:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据集的输出和真实标签，我们使用定义的损失函数计算我们模型的总损失，本例中为负对数似然。计算完损失后，我们可以调用 `backward()` 来通过网络反向传播我们的损失。然后，我们使用我们的优化器的
    `step()` 方法来相应地更新模型参数：
- en: '[PRE20]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, after each epoch is complete, we print the total loss. We can observe
    this to make sure our model is learning:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在每个周期完成后，我们打印出总损失。我们可以观察这一点以确保我们的模型在学习：
- en: '[PRE21]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In general, we would expect the loss to decrease after every epoch. Our output
    will look something like this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们期望损失在每个周期后都会减少。我们的输出将看起来像这样：
- en: '![Figure 2.14 – Training epochs'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14 – 训练周期'
- en: '](img/B12365_02_14.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_14.jpg)'
- en: Figure 2.14 – Training epochs
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14 – 训练周期
- en: Making predictions
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: 'Now that our model has been trained, we can use this to make predictions on
    unseen data. We begin by reading in our test set of data (which was not used to
    train our model):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经训练好，我们可以用它来对未见过的数据进行预测。我们首先读入我们的测试数据集（这些数据集未用于训练我们的模型）：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we perform the same steps we performed when we loaded our training set
    of data: we reshape our test data and transform it into PyTorch tensors. Next,
    to predict using our trained model, we simply run the following command:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们执行与加载训练数据集时相同的步骤：我们重塑我们的测试数据，并将其转换为 PyTorch 张量。接下来，要使用我们训练过的模型进行预测，我们只需运行以下命令：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In the same way that we calculated our outputs on the forward pass of our training
    data in our model, we now pass our test data through the model and obtain predictions.
    We can view the predictions for one of the images like so:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在模型的前向传播中计算训练数据的输出一样，我们现在通过模型传递测试数据并得到预测。我们可以查看其中一张图像的预测结果如下：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This results in the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 2.15 – Prediction outputs'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15 – 预测输出'
- en: '](img/B12365_02_15.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_15.jpg)'
- en: Figure 2.15 – Prediction outputs
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 – 预测输出
- en: Here, we can see that our prediction is a vector of length 10, with a prediction
    for each of the possible classes (digits between 0 and 9). The one with the highest
    predicted value is the one our model chooses as its prediction. In this case,
    it is the 10th unit of our vector, which equates to the digit 9\. Note that since
    we used log softmax earlier, our predictions are logs and not raw probabilities.
    To convert these back into probabilities, we can just transform them using *x*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的预测是一个长度为10的向量，每个可能类别（0到9之间的数字）有一个预测值。具有最高预测值的那个是我们模型选择作为预测的那个。在这种情况下，它是向量的第10个单元，对应于数字9。请注意，由于我们之前使用了对数
    softmax，我们的预测是对数而不是原始概率。要将其转换回概率，我们可以简单地使用 *x* 进行转换。
- en: 'We can now construct a summary DataFrame containing our true test data labels,
    as well as the labels our model predicted:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建一个包含真实测试数据标签以及我们模型预测标签的总结DataFrame：
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This results in the following output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 2.16 – Prediction table'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16 – 预测表格'
- en: '](img/B12365_02_16.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_16.jpg)'
- en: Figure 2.16 – Prediction table
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 – 预测表格
- en: Note how the `torch.max()` function automatically selects the prediction with
    the highest value. We can see here that, based on a small selection of our data,
    our model appears to be making some good predictions!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`torch.max()` 函数会自动选择具有最高值的预测值。我们可以看到，在我们的数据的小部分选择中，我们的模型似乎在做出一些好的预测！
- en: Evaluating our model
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估我们的模型
- en: 'Now that we have some predictions from our model, we can use these predictions
    to evaluate how good our model is. One rudimentary way of evaluating model performance
    is **accuracy**, as discussed in the previous chapter. Here, we simply calculate
    our correct predictions (where the predicted image label is equal to the actual
    image label) as a percentage of the total number of predictions our model made:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从模型得到了一些预测结果，我们可以用这些预测结果来评估我们模型的好坏。评估模型性能的一个简单方法是**准确率**，正如前一章节讨论的那样。在这里，我们简单地计算我们正确预测的百分比（即预测图像标签等于实际图像标签的情况）：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This results in the following output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![Figure 2.17 – Accuracy score'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17 – 准确率分数'
- en: '](img/B12365_02_17.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_17.jpg)'
- en: Figure 2.17 – Accuracy score
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 – 准确率分数
- en: Congratulations! Your first neural network was able to correctly identify almost
    90% of unseen digit images. As we progress, we will see that there are more sophisticated
    models that may lead to improved performance. However, for now, we have demonstrated
    that creating a simple deep neural network is very simple using PyTorch. This
    can be coded up in just a few lines and leads to performance above and beyond
    what is possible with basic machine learning models such as regression.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你的第一个神经网络能够正确识别近90%的未见数字图像。随着我们的进展，我们将看到更复杂的模型可能会导致性能的提升。然而，目前我们已经证明，使用PyTorch创建简单的深度神经网络非常简单。这可以用几行代码实现，并且能够超越基本的机器学习模型如回归。
- en: NLP for PyTorch
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 的自然语言处理
- en: Now that we have learned how to build neural networks, we will see how it is
    possible to build models for NLP using PyTorch. In this example, we will create
    a basic bag-of-words classifier in order to classify the language of a given sentence.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何构建神经网络，我们将看到如何使用PyTorch为NLP构建模型。在这个例子中，我们将创建一个基本的词袋分类器，以便对给定句子的语言进行分类。
- en: Setting up the classifier
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类器的设置
- en: 'For this example, we''ll take a selection of sentences in Spanish and English:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将选取一些西班牙语和英语的句子：
- en: 'First, we split each sentence into a list of words and take the language of
    each sentence as a label. We take a section of sentences to train our model on
    and keep a small section to one side as our test set. We do this so that we can
    evaluate the performance of our model after it has been trained:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将每个句子拆分为单词列表，并将每个句子的语言作为标签。我们从中取一部分句子来训练我们的模型，并保留一小部分作为测试集。我们这样做是为了在模型训练后评估其性能：
- en: '[PRE27]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that we also transform each word into lowercase, which stops words being
    double counted in our bag-of-words. If we have the word `book` and the word `Book`,
    we want these to be counted as the same word, so we transform these into lowercase.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们还将每个单词转换为小写，这样可以防止在我们的词袋中重复计数。如果我们有单词`book`和单词`Book`，我们希望它们被视为相同的单词，因此我们将它们转换为小写。
- en: 'Next, we build our word index, which is simply a dictionary of all the words
    in our corpus, and then create a unique index value for each word. This can be
    easily done with a short `for` loop:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建我们的词索引，这只是我们语料库中所有单词的字典，并为每个单词创建一个唯一的索引值。这可以通过简短的`for`循环轻松完成：
- en: '[PRE28]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This results in the following output:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![Figure 2.18 – Setting up the classifier'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.18 – 设置分类器'
- en: '](img/B12365_02_18.jpg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_02_18.jpg)'
- en: Figure 2.18 – Setting up the classifier
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.18 – 设置分类器
- en: Note that here, we looped through all our training data and test data. If we
    just created our word index on training data, when it came to evaluating our test
    set, we would have new words that were not seen in the original training, so we
    wouldn't be able to create a true bag-of-words representation for these words.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在这里，我们循环遍历了所有的训练数据和测试数据。如果我们仅在训练数据上创建了我们的词索引，那么在评估测试集时，我们可能会有新的单词，这些单词在原始训练数据中没有出现，因此我们无法为这些单词创建真正的词袋表示。
- en: Now, we build our classifier in a similar fashion to how we built our neural
    network in the previous section; that is, by building a new class that inherits
    from `nn.Module`.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们按照前一节中构建神经网络的方式构建我们的分类器；也就是说，通过构建一个从`nn.Module`继承的新类。
- en: 'Here, we define our classifier so that it consists of a single linear layer
    with log softmax activation functions approximating a logistic regression. We
    could easily extend this to operate as a neural network by adding extra linear
    layers here, but a single layer of parameters will serve our purpose. Pay close
    attention to the input and output sizes of our linear layer:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们定义我们的分类器，使其包含一个具有log softmax激活函数的单个线性层，用来近似逻辑回归。我们可以通过在此处添加额外的线性层轻松扩展为神经网络，但是一个参数的单层将满足我们的目的。请特别注意我们线性层的输入和输出大小：
- en: '[PRE29]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The input is of length `corpus_size`, which is just the total count of unique
    words in our corpus. This is because each input to our model will be a bag-of-words
    representation, consisting of the counts of words in each sentence, with a count
    of 0 if a given word does not appear in our sentence. Our output is of size 2,
    which is our number of languages to predict. Our final predictions will consist
    of a probability that our sentence is English versus the probability that our
    sentence is Spanish, with our final prediction being the one with the highest
    probability.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入的长度为`corpus_size`，这只是我们语料库中唯一单词的总数。这是因为我们模型的每个输入将是一个词袋表示，其中包含每个句子中单词的计数，如果给定单词在我们的句子中不存在，则计数为0。我们的输出大小为2，这是我们要预测的语言数。我们最终的预测将包括一个句子是英语的概率与句子是西班牙语的概率，最终预测将是概率最高的那个。
- en: 'Next, we define some utility functions. We first define `make_bow_vector`,
    which takes the sentence and transforms it into a bag-of-words representation.
    We first create a vector consisting of all zeros. We then loop through them and
    for each word in the sentence, we increment the count of that index within the
    bag-of-words vector by one. We finally reshape this vector using `with .view()`
    for entry into our classifier:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一些实用函数。首先定义`make_bow_vector`，它接受句子并将其转换为词袋表示。我们首先创建一个全零向量。然后循环遍历句子中的每个单词，递增词袋向量中该索引位置的计数。最后，我们使用`with
    .view()`来重塑这个向量以输入到我们的分类器中：
- en: '[PRE30]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Similarly, we define `make_target`, which simply takes the label of the sentence
    (Spanish or English) and returns its relevant index (`0` or `1`):'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，我们定义`make_target`，它简单地接受句子的标签（西班牙语或英语）并返回其相关的索引（`0`或`1`）：
- en: '[PRE31]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can now create an instance of our model, ready for training. We also define
    our loss function as Negative Log Likelihood as we are using a log softmax function,
    and then define our optimizer in order to use standard **stochastic** **gradient**
    **descent** (**SGD**):'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们模型的一个实例，准备进行训练。我们还将我们的损失函数定义为负对数似然，因为我们使用了对数softmax函数，然后定义我们的优化器以使用标准的**随机****梯度****下降**（**SGD**）：
- en: '[PRE32]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now, we are ready to train our model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备训练我们的模型。
- en: Training the classifier
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练分类器
- en: First, we set up a loop consisting of the number of epochs we wish our model
    to run for. In this instance, we will select 100 epochs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置了一个循环，其包含我们希望模型运行的轮数。在这个实例中，我们将选择100轮次。
- en: Within this loop, we first zero our gradients (as otherwise, PyTorch calculates
    gradients cumulatively) and then for each sentence/label pair, we transform each
    into a bag-of-words vector and target, respectively. We then calculate the predicted
    output of this particular sentence pair by making a forward pass of our data through
    the current state of our model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，我们首先将梯度归零（否则，PyTorch会累积计算梯度），然后对于每个句子/标签对，我们分别将其转换为词袋向量和目标。然后，通过当前模型状态的数据进行前向传播，计算出这个特定句子对的预测输出。
- en: 'Using this prediction, we then take our predicted and actual labels and call
    our defined `loss_function` on the two to obtain a measure of loss for this sentence.
    By calling `backward()`, we then backpropagate this loss through our model and
    by calling `step()` on our optimizer, we update our model parameters. Finally,
    we print our loss after every 10 training steps:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 利用此预测，我们接着将预测值和实际标签传入我们定义的`loss_function`，以获取这个句子的损失度量。调用`backward()`来通过我们的模型反向传播这个损失，再调用优化器的`step()`来更新模型参数。最后，在每10个训练步骤后打印出我们的损失：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This results in the following output:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下输出：
- en: '![Figure 2.19 – Training loss'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19 – 训练损失'
- en: '](img/B12365_02_19.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_19.jpg)'
- en: Figure 2.19 – Training loss
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19 – 训练损失
- en: 'Here, we can see that our loss is decreasing over time as our model learns.
    Although our training set in this example is very small, we can still demonstrate
    that our model has learned something useful, as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到随着模型学习，我们的损失随时间递减。尽管这个示例中的训练集非常小，我们仍然可以展示出我们的模型学到了一些有用的东西，如下所示：
- en: We evaluate our model on a couple of sentences from our test data that our model
    was not trained on. Here, we first set `torch.no_grad()`, which deactivates the
    `autograd` engine as there is no longer any need to calculate gradients as we
    are no longer training our model. Next, we take our test sentence and transform
    it into a bag-of-words vector and feed it into our model to obtain predictions.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在一些测试数据的几个句子上评估我们的模型，这些句子我们的模型没有进行训练。在这里，我们首先设置`torch.no_grad()`，这将关闭`autograd`引擎，因为我们不再需要计算梯度，我们不再训练我们的模型。接下来，我们将测试句子转换为词袋向量，并将其馈送到我们的模型中以获得预测。
- en: 'We then simply print the sentence, the true label of the sentence, and then
    the predicted probabilities. Note that we transform the predicted values from
    log probabilities back into probabilities. We obtain two probabilities for each
    prediction, but if we refer back to the label index, we can see that the first
    probability (index 0) corresponds to Spanish, whereas the other one corresponds
    to English:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着我们简单地打印出句子、句子的真实标签，然后是预测的概率。注意，我们将预测值从对数概率转换回概率。对于每个预测，我们得到两个概率，但是如果我们回顾标签索引，可以看到第一个概率（索引0）对应于西班牙语，而另一个对应于英语：
- en: '[PRE34]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This results in the following output:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致了以下输出：
- en: '![Figure 2.20 – Predicted output'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.20 – 预测输出'
- en: '](img/B12365_02_20.jpg)'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B12365_02_20.jpg)'
- en: Figure 2.20 – Predicted output
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.20 – 预测输出
- en: Here, we can see that for both our predictions, our model predicts the correct
    answer, but why is this? What exactly has our model learned? We can see that our
    first test sentence contains the word `estoy`, which was previously seen in a
    Spanish sentence within our training set. Similarly, we can see that the word
    `book` was seen within our training set in an English sentence. Since our model
    consists of a single layer, the parameters on each of our nodes are easy to interpret.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到对于我们的预测，我们的模型预测了正确的答案，但是为什么呢？我们的模型到底学到了什么？我们可以看到，我们的第一个测试句子包含了单词`estoy`，这在我们的训练集中之前出现在一个西班牙语句子中。类似地，我们可以看到单词`book`在我们的训练集中出现在一个英语句子中。由于我们的模型由单层组成，我们每个节点上的参数易于解释。
- en: 'Here, we define a function that takes a word as input and returns the weights
    on each of the parameters within the layer. For a given word, we get the index
    of this word from our dictionary and then select these parameters from the same
    index within the model. Note that our model returns two parameters as we are making
    two predictions; that is, the model''s contribution to the Spanish prediction
    and the model''s contribution to the English prediction:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个函数，该函数以单词作为输入，并返回层内每个参数的权重。对于给定的单词，我们从字典中获取其索引，然后从模型中选择这些参数的同一索引。请注意，我们的模型返回两个参数，因为我们进行了两次预测；即，模型对西班牙语预测的贡献和模型对英语预测的贡献：
- en: '[PRE35]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This results in the following output:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这导致了以下输出：
- en: '![Figure 2.21 – Predicted output for the updated function'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.21 – 更新函数的预测输出'
- en: '](img/B12365_02_21.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_21.jpg)'
- en: Figure 2.21 – Predicted output for the updated function
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21 – 更新函数的预测输出
- en: Here, we can see that for the word `estoy`, this parameter is positive for the
    Spanish prediction and negative for the English one. This means that for each
    count of the word "`estoy`" in our sentence, the sentence becomes more likely
    to be a Spanish sentence. Similarly, for the word `book`, we can see that this
    contributes positively to the prediction that the sentence is English.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到对于单词`estoy`，这个参数对于西班牙语的预测是正的，对于英语则是负的。这意味着在我们的句子中每出现一次单词"`estoy`"，这个句子变得更可能是西班牙语。同样地，对于单词`book`，我们可以看到它对于预测这个句子是英语有正面贡献。
- en: 'We can show that our model has only learned based on what it has been trained
    on. If we try to predict a word the model hasn''t been trained on, we can see
    it is unable to make an accurate decision. In this case, our model thinks that
    the English word "`not"` is Spanish:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示，我们的模型仅基于其训练过的内容进行学习。如果我们尝试预测一个模型未经训练的词汇，我们可以看到它无法做出准确的决定。在这种情况下，我们的模型认为英文单词"`not`"是西班牙语：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This results in the following output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下输出：
- en: '![Figure 2.22 – Final output'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.22 – 最终输出'
- en: '](img/B12365_02_22.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_02_22.jpg)'
- en: Figure 2.22 – Final output
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22 – 最终输出
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced PyTorch and some of its key features. Hopefully,
    you now have a better understanding of how PyTorch differs from other deep learning
    frameworks and how it can be used to build basic neural networks. While these
    simple examples are just the tip of the iceberg, we have illustrated that PyTorch
    is an immensely powerful tool for NLP analysis and learning.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了PyTorch及其一些关键特性。希望现在你对PyTorch与其他深度学习框架的区别有了更好的理解，以及它如何用于构建基本的神经网络。虽然这些简单的例子只是冰山一角，但我们已经说明了PyTorch是NLP分析和学习的强大工具。
- en: In future chapters, we will demonstrate how the unique properties of PyTorch
    can be utilized to build highly sophisticated models for solving very complex
    machine learning tasks.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将展示如何利用PyTorch的独特特性来构建用于解决非常复杂的机器学习任务的高度复杂的模型。
