- en: '*Chapter 8*: Working with Efficient Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第八章*：使用高效的变压器'
- en: So far, you have learned how to design a **Natural Language Processing** (**NLP**)
    architecture to achieve successful task performance with transformers. In this
    chapter, you will learn how to make efficient models out of trained models using
    distillation, pruning, and quantization. Second, you will also gain knowledge
    about efficient sparse transformers such as Linformer, BigBird, Performer, and
    so on. You will see how they perform on various benchmarks, such as memory versus
    sequence length and speed versus sequence length. You will also see the practical
    use of model size reduction.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何设计**自然语言处理**（**NLP**）架构，以实现使用变压器的成功任务执行。在本章中，您将学习如何使用蒸馏、修剪和量化将训练模型变为高效的模型。其次，您还将了解到关于高效稀疏变压器的知识，如
    Linformer、BigBird、Performer 等。您将看到它们在各种基准测试中的表现，例如内存与序列长度和速度与序列长度的比较。您还将看到模型尺寸缩减的实际用途。
- en: The importance of this chapter came to light as it is getting difficult to run
    large neural models under limited computational capacity. It is important to have
    a lighter general-purpose language model such as DistilBERT. This model can then
    be fine-tuned with good performance, like its non-distilled counterparts. Transformers-based
    architectures face complexity bottlenecks due to the quadratic complexity of the
    attention dot product in the transformers, especially for long-context NLP tasks.
    Character-based language models, speech processing, and long documents are among
    the long-context problems. In recent years, we have seen much progress in making
    self-attention more efficient, such as Reformer, Performer, and BigBird, as a
    solution to complexity.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在有限的计算能力下运行大型神经模型变得困难，本章的重要性凸显出来。拥有像 DistilBERT 这样的轻量级通用语言模型是很重要的。然后可以像其未蒸馏的对应物一样对其进行微调以获得良好的性能。基于变压器的架构由于变压器中注意力点积的二次复杂度而面临复杂性瓶颈，尤其是对于长上下文
    NLP 任务。字符级语言模型、语音处理和长文档是长上下文问题的一部分。近年来，我们已经看到了使自注意更加高效的许多进展，例如 Reformer、Performer
    和 BigBird。
- en: 'In short, in this chapter, you will learn about the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在本章中，您将学习以下主题：
- en: Introduction to efficient, light, and fast transformers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍高效、轻量和快速的变压器
- en: Implementation for model size reduction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于模型尺寸缩减的实施
- en: Working with efficient self-attention
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高效的自注意力
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the Jupyter Notebook to run our coding exercises, which require
    Python 3.6+, and the following packages need to be installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Jupyter Notebook 运行我们的编码练习，需要 Python 3.6+，并且需要安装以下软件包：
- en: TensorFlow
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: PyTorch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Transformers >=4.00
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器 >=4.00
- en: Datasets
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: sentence-transformers
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子变压器
- en: py3nvml
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: py3nvml
- en: 'All notebooks with coding exercises are available at the following GitHub link:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所有包含编码练习的笔记本都可以在以下 GitHub 链接中找到：
- en: https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH08
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH08](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH08)'
- en: 'Check out the following link to see Code in Action Video:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看代码演示视频：
- en: '[https://bit.ly/3y5j9oZ](https://bit.ly/3y5j9oZ)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3y5j9oZ](https://bit.ly/3y5j9oZ)'
- en: Introduction to efficient, light, and fast transformers
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍高效、轻量和快速的变压器
- en: 'Transformer-based models have distinctly achieved state-of-the-art results
    in many NLP problems at the cost of quadratic memory and computational complexity.
    We can highlight the issues regarding complexity as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变压器的模型在许多 NLP 问题上已经明显取得了最先进的结果，但代价是二次内存和计算复杂度。我们可以总结如下有关复杂性的问题：
- en: The models are not able to efficiently process long sequences due to their self-attention
    mechanism, which scales quadratically with the sequence length.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于自注意机制的存在，模型无法有效地处理长序列，这是因为其随序列长度呈二次方增长。
- en: An experimental setup using a typical GPU with 16 GB can handle the sentences
    of 512 tokens for training and inference. However, longer entries can cause problems.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用典型 GPU 进行的实验设置，可以处理包含 512 个标记的句子进行训练和推断。然而，较长的条目可能会引起问题。
- en: The NLP models keep growing from the 110 million parameters of BERT-base to
    the 17 billion parameters of Turing-NLG and to the 175 billion parameters of GPT-3\.
    This notion raises concerns about computational and memory complexity.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP 模型从 BERT-base 的 1.1 亿参数增长到 Turing-NLG 的 170 亿参数，再到 GPT-3 的 1750 亿参数。这一概念引起了人们对计算和内存复杂性的担忧。
- en: We also need to care about costs, production, reproducibility, and sustainability.
    Hence, we need faster and lighter transformers, especially on edge devices.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还需要关注成本、生产、可复制性和可持续性。因此，我们需要更快、更轻的转换器，特别是在边缘设备上。
- en: Several approaches have been proposed to reduce computational complexity and
    memory footprint. Some of these approaches focus on changing the architecture
    and some do not alter the original architecture but instead make improvements
    to the trained model or to the training phase. We will divide them into two groups,
    model size reduction and efficient self-attention.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几种方法来减少计算复杂性和内存占用。其中一些方法侧重于改变体系结构，而其他一些方法不改变原始体系结构，而是改进已训练模型或训练阶段。我们将它们分为两组，模型大小缩减和高效自注意力。
- en: 'Model size reduction can be accomplished using three different approaches:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用三种不同的方法来实现模型大小缩减：
- en: Knowledge distillation
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: Pruning
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝
- en: Quantization
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化
- en: Each of these three has its own way of reducing the size model, which we will
    describe in short in the *Implementation for model size reduction* section.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个方法都有各自的方式来缩小模型的大小，我们将在 *模型大小缩减的实现* 部分进行简要描述。
- en: In knowledge distillation, a smaller transformer (student) can transfer the
    knowledge of a big model (teacher). We train the student model so that it can
    mimic the teacher's behavior or produce the same output for the same input. The
    distilled model may underperform the teacher. There is a trade-off between compression,
    speed, and performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在知识蒸馏中，较小的转换器（学生）可以传输大型模型（教师）的知识。我们训练学生模型，使其能够模仿教师的行为或对相同的输入产生相同的输出。蒸馏模型可能性能不如教师模型。压缩、速度和性能之间存在权衡。
- en: Pruning is a model compression technique in machine learning that is used to
    reduce the size of the model by removing a section of the model that contributes
    little to producing results. The most typical example is decision tree pruning,
    which helps to reduce the model complexity and increase the generalization capacity
    of the model. Quantization changes model weight types from higher resolutions
    to lower resolutions. For example, we use a typical floating-point number (`float64`)
    consuming 64 bits of memory for each weight. Instead, we can use `int8` in quantization,
    which consumes 8 bits for each weight, and naturally has less accuracy in presenting
    numbers.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝是机器学习中用于通过删除对产生结果的贡献较小的模型部分来减小模型尺寸的模型压缩技术。最典型的例子是决策树剪枝，它有助于减少模型复杂性并增加模型的泛化能力。量化将模型权重类型从较高分辨率更改为较低分辨率。例如，我们使用典型的浮点数(`float64`)，每个权重占用64位内存。相反，我们可以在量化中使用
    `int8`，每个权重占用8位内存，并且在表示数字时自然精度较低。
- en: Self-attention heads are not optimized for long sequences. In order to solve
    this issue, many different approaches have been proposed. The most efficient approach
    is **Self-Attention Sparsification**, which we will discuss soon. The other most
    widely used approach is **Memory Efficient Backpropagation**. This approach balances
    a trade-off between the caching of intermediate results and re-computing. Intermediate
    activations computed during forward propagation are needed to compute gradients
    during backward propagation. Gradient checkpoints can reduce a substantial amount
    of memory footprint and computation. Another approach is **Pipeline Parallelism
    Algorithms**. Mini-batches are split into micro-batches and the parallelism pipeline
    takes advantage of using the waiting time during the forward and backward operations
    while transferring the batches to deep learning accelerators such as **Graphics
    Processing Unit** (**GPU**) or **Tensor Processing Unit** (**TPU**).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力头不适用于长序列。为了解决这个问题，已经提出了许多不同的方法。最有效的方法是**自注意力稀疏化**，我们将很快讨论。另一个最常用的方法是**内存高效的反向传播**。该方法在缓存中间结果和重新计算之间平衡了权衡。在正向传播期间计算的中间激活在反向传播期间需要用来计算梯度。梯度检查点可以减少内存占用和计算量。另一种方法是**管道并行算法**。小批量被分成微批量，并行管道利用了正向和反向操作期间的等待时间，同时将批量传输到GPU或TPU等深度学习加速器中。
- en: '**Parameter Sharing** can be counted as one of the first approaches towards
    efficient deep learning. The most typical example is RNN, as depicted in [*Chapter
    1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016), *From Bag-of-Words to the Transformers*,
    where the units of unfolded representation use the shared parameters. Hence, the
    number of trainable parameters is not affected by the input size. Some shared
    parameters which are also called weight tying or weight replication, spread the
    network so that the number of trainable parameters is reduced. For instance, Linformer
    shares projection matrices across heads and layers. Reformer shares the query
    and key at the cost of performance loss.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数共享**可以被看作是朝向高效深度学习的第一种方法之一。 最典型的例子是循环神经网络（RNN），如[*第1章*](B17123_01_Epub_AM.xhtml#_idTextAnchor016)中所述，*从词袋模型到Transformer*，在展开表示的单元中使用了共享参数。
    因此，可训练参数的数量不受输入大小的影响。 一些共享的参数，也称为权重绑定或权重复制，会扩展网络，以减少可训练参数的数量。 例如，Linformer在头和层之间共享投影矩阵。
    Reformer通过牺牲性能来共享查询和键。'
- en: Now let's try to understand these issues with corresponding practical examples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们试着用相应的实际例子来理解这些问题。
- en: Implementation for model size reduction
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型大小缩减的实现
- en: 'Even though the transformer-based models achieve state-of-the-art results in
    many aspects of NLP, they usually share the very same problem: they are big models
    and are not fast enough to be used. In business cases where it is necessary to
    embed them inside a mobile application or in a web interface, it seems to be impossible
    if you try to use the original models.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于Transformer的模型在NLP许多方面取得了最先进的成果，它们通常共享同样的问题：它们是庞大的模型，速度不够快无法使用。 在业务案例中，如果需要将它们嵌入到移动应用程序或Web界面中，如果尝试使用原始模型，似乎是不可能的。
- en: 'In order to improve the speed and size of these models, some techniques are
    proposed, which are listed here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高这些模型的速度和大小，提出了一些技术，列举如下：
- en: Distillation (also known as knowledge distillation)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒸馏（又称为知识蒸馏）
- en: Pruning
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精简
- en: Quantization
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化
- en: For each of these techniques, we provide a separate subsection to address the
    technical and theoretical insights.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每种技术，我们提供一个单独的小节来解决技术和理论洞察。
- en: Working with DistilBERT for knowledge distillation
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DistilBERT进行知识蒸馏
- en: The process of transferring knowledge from a bigger model to a smaller one is
    called **knowledge distillation**. In other words, there is a teacher model and
    a student model; the teacher is typically a bigger and stronger model while the
    student is smaller and weaker.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个更大的模型向一个更小的模型转移知识的过程称为**知识蒸馏**。 换句话说，有一个老师模型和一个学生模型；老师通常是一个更大更强大的模型，而学生则更小更弱。
- en: 'This technique is used in various problems, from vision to acoustic models
    and NLP. A typical implementation of this technique is shown in *Figure 8.1*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被用于各种问题，从视觉到声学模型和自然语言处理。 这种技术的典型实现如*图8.1*所示：
- en: '![Figure 8.1 – Knowledge distillation for image classification ](img/B17123_08_001.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 - 用于图像分类的知识蒸馏](img/B17123_08_001.jpg)'
- en: Figure 8.1 – Knowledge distillation for image classification
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 - 用于图像分类的知识蒸馏
- en: DistilBERT is one of the most important models in this field and has got the
    attention of researchers and even the businesses. This model, which tries to mimic
    the behavior of BERT-Base, has 50% fewer parameters and achieves 95% of the teacher
    model's performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT是这一领域最重要的模型之一，引起了研究者甚至企业的关注。 这个模型试图模仿BERT-Base的行为，参数少了50%，但实现了老师模型95%的性能。
- en: 'Some details are given as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 细节如下：
- en: DistilBert is 1.7x compressed and 1.6x faster with 97% relative performance
    (compared to original BERT).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBert压缩了1.7倍，速度提高了1.6倍，相对性能提高了97%（与原始BERT相比）。
- en: Mini-BERT is 6x compressed, 3x faster, and has 98% relative performance.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mini-BERT压缩了6倍，速度提高了3倍，相对性能提高了98%。
- en: TinyBERT is 7.5x compressed, has 9.4x speed, and 97% relative performance.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TinyBERT压缩了7.5倍，速度提高了9.4倍，相对性能提高了97%。
- en: 'The distillation training step that is used to train the model is very simple
    using PyTorch (original description and code available at [https://medium.com/huggingface/distilbert-8cf3380435b5](https://medium.com/huggingface/distilbert-8cf3380435b5)):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练模型的蒸馏训练步骤在PyTorch中非常简单（原始描述和代码可在[https://medium.com/huggingface/distilbert-8cf3380435b5](https://medium.com/huggingface/distilbert-8cf3380435b5)找到）：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This model-supervised training provides us with a smaller model that is very
    similar to the base model in behavior. However, the loss function used here is
    **Kullback-Leibler** loss to ensure that the student model mimics the good and
    bad aspects of the teacher model with no modification of the decision on the last
    softmax logits. This loss function shows how different two distributions are from
    each other; a greater difference means a higher loss value. The reason for using
    this loss function is to make the student model try to completely mimic the behavior
    of the teacher. The GLUE macro scores for BERT and DistilBERT are just 2.8% different.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型监督训练为我们提供了一个行为非常类似于基础模型的较小模型。然而，这里使用的损失函数是**Kullback-Leibler**损失，以确保学生模型模仿了老师模型的好坏方面，并且对最后
    softmax logits 的决策没有修改。这个损失函数显示了两个分布之间的不同程度；差异越大，损失值越高。使用这个损失函数的原因是使学生模型尽可能地模仿老师的行为。BERT
    和 DistilBERT 的 GLUE 宏分数只相差 2.8%。
- en: Pruning transformers
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精简变压器
- en: Pruning includes the process of setting weights at each layer to zero based
    on a pre-specified criterion. For example, a simple pruning algorithm could take
    the weights of each layer and set those that are below a threshold. This method
    eliminates weights that are very low in value and do not affect the results too
    much.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 精简包括根据预先指定的标准在每一层上将权重设置为零的过程。例如，一个简单的精简算法可以获取每层的权重并将低于阈值的权重设置为零。这种方法消除了值非常低且不会对结果产生太大影响的权重。
- en: Likewise, we prune some redundant parts of the transformer network. The pruned
    networks are more likely to generalize better than the original one. We have seen
    a successful pruning operation because the pruning process probably keeps the
    true underlying explanatory factors and discards the redundant subnetwork. But
    we need to still train a large network. The reasonable strategy is that we train
    a neural network as large as possible. Then, the less salient weights or units
    whose removals have a small effect on the model performance are discarded.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们还精简了变压器网络的一些多余部分。精简后的网络更有可能比原始网络更好地泛化。我们看到了成功的精简操作，因为精简过程可能保留了真实的潜在解释因素并且丢弃了多余的子网络。但是我们仍然需要训练一个大型网络。合理的策略是尽可能地训练一个大型神经网络。然后，丢弃一些不太明显的权重或单元，这些权重或单元的移除对模型性能的影响很小。
- en: 'There are two approaches:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法：
- en: '**Unstructured pruning**: where individual weights with a small saliency (or
    the least weight magnitude) are removed no matter which part of the neural network
    they are located in.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非结构化精简**：不管它们位于神经网络的哪个部分，都移除具有较小显著性（或最小权重幅度）的个别权重。'
- en: '**Structured pruning**: this approach prunes heads or layers.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化精简**：此方法精简头部或层。'
- en: However, the pruning process has to be compatible with modern GPUs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，精简过程必须与现代 GPU 兼容。
- en: 'Most libraries such as Torch or TensorFlow come with this capability. We will
    describe how it is possible to prune a model using Torch. There are many different
    methods to use in pruning (magnitude-based or mutual information-based). One of
    the simplest ones to understand and implement is the L1 pruning method. This method
    takes the weights of each layer and zeros out the ones with the lowest L1-norm.
    You can also specify what percentage of your weights must be converted to zero
    after pruning. In order to make this example more understandable and show its
    impact on the model, we''ll use the text representation example from [*Chapter
    7*](B17123_07_Epub_AM.xhtml#_idTextAnchor099), *Text Representation*. We will
    prune the model and see how it performs after pruning:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数库，如 Torch 或 TensorFlow，都具有此功能。我们将描述如何使用 Torch 对模型进行精简。有许多不同的精简方法（基于幅度或基于互信息）。其中一个最容易理解和实现的方法是
    L1 精简方法。此方法获取每一层的权重并将具有最低 L1 范数的权重置为零。您还可以指定在精简后必须将多少百分比的权重转换为零。为了使这个示例更容易理解并展示它对模型的影响，我们将使用
    [*第7章*](B17123_07_Epub_AM.xhtml#_idTextAnchor099) 中的文本表示示例。我们将精简模型并查看精简后的表现：
- en: 'We will use the Roberta model. You can load the model using the following code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 Roberta 模型。您可以使用以下代码加载模型：
- en: '[PRE1]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You will also need to load metrics and datasets for evaluation:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还需要加载指标和数据集进行评估：
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In order to evaluate the model, just like in [*Chapter 7*](B17123_07_Epub_AM.xhtml#_idTextAnchor099),
    *Text Representation*, you can use the following function:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估模型，就像在 [*第7章*](B17123_07_Epub_AM.xhtml#_idTextAnchor099) 中，*文本表示*，您可以使用以下函数：
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And of course, it is required to set labels:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，还必须设置标签：
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And to run the base model with no changes in it:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且运行基础模型而不对其进行任何更改：
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After all these things are done, this is the step where we actually start to
    prune our model:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有这些事情完成之后，这是我们实际开始修剪模型的步骤：
- en: '[PRE6]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The previous code makes a pruning object using L1-norm pruning with 20% of
    the weights in each layer. To apply it to the model, you can use the following
    code:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码使用每层20％的L1范数修剪创建了一个修剪对象。要将其应用于模型，可以使用以下代码：
- en: '[PRE7]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It will iteratively prune all layers that have weight in their name; in other
    words, we will prune all weight layers and not touch the layers that are biased.
    Of course, you can try that too for experimentation purposes.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它将迭代地修剪所有具有权重名称的层；换句话说，我们将修剪所有重量层，而不触及有偏差的层。当然，您也可以出于实验目的尝试这样做。
- en: 'And again, it is good to reload the state dictionary to the model:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次加载状态字典到模型中是很有必要的：
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have done everything, we can test the new model:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经完成了所有事情，可以测试新模型：
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In order to have a good visual representation of the results, you can use the
    following code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了对结果进行良好的可视化表示，可以使用以下代码：
- en: '[PRE10]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following screenshot shows the results:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图显示了结果：
- en: '![Figure 8.2 – Comparison between original and pruned models ](img/B17123_08_002.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 - 原始模型和修剪模型的比较](img/B17123_08_002.jpg)'
- en: Figure 8.2 – Comparison between original and pruned models
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 - 原始模型和修剪模型的比较
- en: But what you did is you eliminated 20% of all weights of the model, reduced
    its size and computation cost, and lost 4% in performance. However, this step
    can be combined with other techniques such as quantization, which is explored
    in the next subsection.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但你所做的是，你消除了模型20%的所有权重，减少了它的大小和计算成本，并且性能下降了4%。但是，这一步骤可以与量化等其他技术结合使用，在下一小节中进行探索。
- en: This type of pruning is applied to some of the weights in a layer; however,
    it is also possible to completely drop some parts or layers of transformer architectures,
    for example, it is possible to drop some of the attention heads and track the
    changes too.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种修剪类型应用于层中的一些权重；但是，也有可能完全丢弃某些部分或层的变压器架构，例如，可以丢弃一部分关注头，并跟踪更改。
- en: There are also other types of pruning algorithms available in PyTorch, such
    as iterative and global pruning, which are worth trying.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了其他类型的修剪算法，例如迭代和全局修剪，值得一试。
- en: Quantization
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化
- en: Quantization is a signal processing and communication term that is generally
    used to emphasize how much accuracy is presented by the data provided. More bits
    mean more accuracy and precision in terms of data resolution. For example, if
    you have a variable that is presented by 4 bits and you want to quantize it to
    2 bits, it means you have to drop the accuracy of your resolution. With 4 bits,
    you can specify 16 different states, while with 2 bits you can distinguish 4 states.
    In other words, by reducing the resolution of your data from 4 to 2 bits, you
    are saving 50% more space and complexity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种信号处理和通信术语，通常用于强调所提供的数据的准确度。更多的位数意味着数据分辨率方面的更高准确度和精度。例如，如果您有一个用4位表示的变量，并且您想将其量化为2位，这意味着您需要降低分辨率的准确度。使用4位，您可以指定16个不同的状态，而使用2位，您只能区分4个状态。换句话说，通过将数据的分辨率从4位降低到2位，您可以节省50％的空间和复杂度。
- en: Many popular libraries, such as TensorFlow, PyTorch, and MXNET, support mixed-precision
    operation. Recall the `fp16` parameter used in the `TrainingArguments` class in
    `chapter 05`. `fP16` increases computational efficiency since modern GPUs offer
    higher efficiency for reduced precision math, but the results are accumulated
    in `fP32`. Mixed-precision can reduce the memory usage required for training,
    which allows us to increase the batch size or model size.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 许多流行的库，例如TensorFlow，PyTorch和MXNET，都支持混合精度运算。回想一下`第05章`中`TrainingArguments`类中使用的`fp16`参数。`fP16`可以提高计算效率，因为现代GPU对降低精度的数学运算提供了更高的效率，但结果是在`fP32`中累积。混合精度可以减少训练所需的内存使用，从而允许我们增加批处理大小或模型大小。
- en: 'Quantization can be applied to model weights to reduce their resolution and
    save computation time, memory, and storage. In this subsection, we will try to
    quantize the model that we pruned in the previous section:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以应用于模型权重，以减少其分辨率并节省计算时间，内存和存储空间。在本小节中，我们将尝试为我们在上一小节中修剪的模型进行量化：
- en: 'In order to do so, you can use the following code to quantize your model in
    8-bit integer representation instead of float:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，使用以下代码以8位整数表示量化模型，而不是浮点数：
- en: '[PRE11]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Afterwards, you can get the evaluation results by using the following code:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用以下代码获得评估结果：
- en: '[PRE12]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And as before, you can view the results:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 而且，你可以看到结果：
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results can be seen as follows:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 8.3 – Comparison between original, pruned, and quantized models ](img/B17123_08_003.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.3 - 原始模型、修剪模型和量化模型的比较 ](img/B17123_08_003.jpg)'
- en: Figure 8.3 – Comparison between original, pruned, and quantized models
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.3 - 原始模型、修剪模型和量化模型的比较
- en: 'Until now, you just used a distilled model, pruned it, and then you quantized
    it to reduce its size and complexity. Let''s see how much space you have saved
    by saving the model:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，你只是使用了一个经蒸馏的模型，对其进行了修剪，然后对其进行了量化，以减少其大小和复杂性。让我们看看通过保存模型你节省了多少空间：
- en: '[PRE14]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Use the following code in order to see the model size:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了查看模型大小，使用以下代码：
- en: '[PRE15]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, it is 191 MB. The initial size of the model was 313 MB, which
    means we managed to decrease the size of the model to 61% of its original size
    and just lost 6%-6.5% in terms of performance. Please note that the `block-size`
    parameter may fail on a Mac, and it is required to use `-lh` instead.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，它是191MB。模型的初始大小为313MB，这意味着我们设法将模型的大小减小到其原始大小的61%，并且在性能方面只损失了6%-6.5%。请注意，`block-size`参数可能在Mac上失败，必须使用`-lh`。
- en: Up to this point, you have learned about pruning and quantization in terms of
    practical model preparation for industrial usage. However, you also gained information
    about the distillation process and how it can be useful. There are many other
    ways to perform pruning and quantization, which can be a good step to go in after
    reading this section. For more information and guides, you can take a look at
    **movement pruning** at [https://github.com/huggingface/block_movement_pruning](https://github.com/huggingface/block_movement_pruning).
    This kind of pruning is a simple and deterministic first-order weight pruning
    approach. It uses the weight changes in training to find out which weights are
    more likely to be unused to have less effect on the result.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 直到这一点，你已经了解了在工业使用中实际模型准备方面的修剪和量化。然而，你也获得了关于蒸馏过程及其如何有用的信息。有许多其他方法可以进行修剪和量化，在阅读本节之后可以迈出的良好一步。有关更多信息和指南，你可以查看[https://github.com/huggingface/block_movement_pruning](https://github.com/huggingface/block_movement_pruning)的**运动修剪**。这种修剪是一种简单且确定性的一阶权重修剪方法。它利用训练中的权重变化来找出哪些权重更有可能未被使用以减少对结果的影响。
- en: Working with efficient self-attention
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用高效的自注意力
- en: Efficient approaches restrict the attention mechanism to get an effective transformer
    model because the computational and memory complexity of a transformer is mostly
    due to the self-attention mechanism. The attention mechanism scales quadratically
    with respect to the input sequence length. For short input, quadratic complexity
    may not be an issue. However, to process longer documents, we need to improve
    the attention mechanism that scales linearly with sequence length.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的方法限制了注意机制以实现有效的变压器模型，因为变压器的计算和存储复杂度大部分都是由自注意机制导致的。关于输入序列长度的注意机制以二次方式进行缩放。对于短输入，二次复杂度可能不是一个问题。但是，要处理更长的文件，我们需要改进能够与序列长度线性缩放的注意机制。
- en: 'We can roughly group the efficient attention solutions into three types:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大致可以将高效的注意力解决方案分为三种类型：
- en: Sparse attention with fixed patterns
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用固定模式的稀疏注意力
- en: Learnable sparse patterns
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可学习的稀疏模式
- en: Low-rank factorization/kernel function
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低秩因式分解/核函数
- en: Let's begin with sparse attention based on a fixed pattern next.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们以基于固定模式的稀疏注意力开始。
- en: Sparse attention with fixed patterns
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用固定模式的稀疏注意力
- en: 'Recall that the attention mechanism is made up of a query, key, and values
    as roughly formulated here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，注意力机制由查询、键和值组成，可能如此粗略地表述：
- en: '![](img/B17123_08_001.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17123_08_001.png)'
- en: Here, the `Score` function, which is mostly softmax, performs QKT multiplication
    that requires O(n2) memory and computational complexity since a token position
    attends to all other token positions in full self-attention mode to build its
    position embeddings. We repeat the same process for all token positions to get
    their embeddings, leading to a quadratic complexity problem. It is a very expensive
    way of learning, especially for long-context NLP problems. It is natural to ask
    the question do we need such a dense interaction or is there a cheaper way to
    do the calculations? Many researchers have addressed this problem and employed
    a variety of techniques to mitigate the complexity burden and to reduce the quadratic
    complexity of the self-attention mechanism. They have mostly made a trade-off
    between performance, computation, and memory, especially for long documents.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`Score`函数，大多数情况下是softmax，执行QKT乘法，需要O(n2)的内存和计算复杂度，因为一个标记位置在完全自注意力模式下会关注所有其他标记位置以构建其位置嵌入。我们对所有标记位置重复相同的过程以获取它们的嵌入，从而导致了二次复杂性问题。这是一种非常昂贵的学习方式，特别是对于长文本NLP问题。自然而然地会问，我们是否需要如此密集的交互，或者是否有更廉价的方法来进行计算？许多研究人员已经解决了这个问题，并采用了各种技术来缓解复杂性负担，并降低自注意机制的二次复杂性。他们在性能、计算和内存之间进行了权衡，特别是对于长文档。
- en: The simplest way of reducing complexity is to sparsify the full self-attention
    matrix or find another cheaper way to approximate full attention. Sparse attention
    patterns formulate how to connect/disconnect certain positions without disturbing
    the flow of information through layers, which helps the model to track long-term
    dependency and to build sentence-level encoding.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 减少复杂性的最简单方法是使完全自注意力矩阵稀疏化，或者找到另一种更廉价的方式来近似完全注意力。稀疏注意力模式制定了如何在不干扰层间信息流的情况下连接/断开某些位置，这有助于模型跟踪长期依赖关系并构建句子级编码。
- en: Full self-attention and sparse attention are depicted in *Figure 8.4* in that
    order, where the rows correspond to output positions and the columns are for the
    inputs. A full self-attention model would directly transfer the information between
    any two positions. On the other hand, in localized sliding window attention, which
    is sparse attention, as shown on the right of the figure, the empty cells mean
    that there is no interaction between the corresponding input-output position.
    The sparse model in the figure is based on fixed patterns that are certain manually
    designed rules. More specifically, it is localized sliding window attention that
    was one of the first proposed methods, also known as the local-based fixed pattern
    approach. The assumption behind it is that useful information is located in each
    position neighbor. Each query token attends to window/2 key tokens to the left
    and window/2 key tokens to the right of that position. In the following example,
    the window size is selected as 4\. This rule applies to every layer in the transformer
    in the same way. In some studies, the window size is increased as it moves towards
    the layers further.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完全自注意力和稀疏注意力依次在*图8.4*中描述，其中行对应输出位置，列对应输入位置。完全自注意力模型会直接在任意两个位置之间传递信息。另一方面，在局部滑动窗口注意力中，也就是稀疏注意力，如图右侧所示，空白单元格意味着相应的输入-输出位置之间没有交互。图中的稀疏模型基于特定手动设计的规则，也就是固定模式。更具体地说，它是局部滑动窗口注意力，是最早提出的方法之一，也被称为基于本地固定模式的方法。其背后的假设是有用的信息位于每个位置的相邻位置。每个查询标记都会关注到其位置左侧和右侧窗口/2个关键标记。在下面的示例中，窗口大小选为4。这个规则在transformer的每一层中都适用。在一些研究中，随着向更深层移动，窗口大小会增加。
- en: 'The following figure simply depicts the difference between full and sparse
    attention:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图简要描述了完全注意力和稀疏注意力之间的区别：
- en: '![Figure 8.4 – Full attention versus sparse attention ](img/B17123_08_004.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 - 完全注意力与稀疏注意力](img/B17123_08_004.jpg)'
- en: Figure 8.4 – Full attention versus sparse attention
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 - 完全注意力与稀疏注意力
- en: In sparse mode, the information is transmitted through connected nodes (non-empty
    cells) in the model. For example, the output position 7 of the sparse attention
    matrix cannot directly attend to the input position 3 (please see the sparse matrix
    at the right of *Figure 8.4*) since the cell (7,3) is seen as empty. However,
    position 7 indirectly attends to position 3 via the token position 5, that is
    (7->5, 5->3 => 7->3). The figure also illustrates that while the full self-attention
    incurs n2 number of active cells (vertex), the sparse model does roughly *5×n*.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏模式下，信息通过模型中的连接节点（非空单元格）传输。例如，稀疏注意力矩阵的输出位置7不能直接关注输入位置3（请参见*Figure 8.4*右侧的稀疏矩阵），因为单元格（7,3）被视为空。但是，位置7间接关注位置3通过标记位置5，即（7->5,
    5->3 => 7->3）。该图还说明了，虽然全自注意会产生n2个活动单元格（顶点），但稀疏模型大致为*5×n*。
- en: 'Another important type is global attention. A few selected tokens or a few
    injected tokens are used as global attention that can attend to all other positions
    and be attended by them. Hence, the maximum path distance between any two token
    positions is equal to 2\. Suppose we have a sentence *[GLB, the, cat, is, very,
    sad]* where **Global** (**GLB**) is an injected global token and the window size
    is 2, which means a token can attend to only its immediate left-right tokens and
    to GLB as well. There is no direct interaction from *cat* to *sad*. But we can
    follow *cat-> GLB, GLB-> sad* interactions, which creates a hyperlink through
    the GLB token. The global tokens can be selected from existing tokens or added
    like *(CLS)*. As shown in the following screenshot, the first two token positions
    are selected as global tokens:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要类型是全局注意力。少量选定的标记或少量注入的标记被用作全局注意力，可以关注所有其他位置，并被它们关注。因此，任意两个标记位置之间的最大路径距离等于2。假设我们有一个句子*[GLB,
    the, cat, is, very, sad]*，其中**Global**（**GLB**）是一个注入的全局标记，窗口大小为2，这意味着一个标记只能关注其直接的左右标记以及GLB。
    *cat* 到 *sad* 之间没有直接交互。但我们可以遵循 *cat-> GLB, GLB-> sad* 交互，这通过GLB标记创建了一个超链接。全局标记可以从现有标记中选择或像*(CLS)*那样添加。如下截图所示，前两个标记位置被选为全局标记：
- en: '![Figure 8.5 – Global attention ](img/B17123_08_005.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 全局注意力](img/B17123_08_005.jpg)'
- en: Figure 8.5 – Global attention
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 全局注意力
- en: By the way, these global tokens don't have to be at the beginning of the sentence
    either. For example, the longformer model randomly selects global tokens in addition
    to the first two tokens..
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，这些全局标记也不必在句子开头。例如，longformer模型除了前两个标记外，还会随机选择全局标记。
- en: 'There are four more widely seen patterns. **Random attention** (the first matrix
    in *Figure 8.6*) is used to ease the flow of information by randomly selecting
    from existing tokens. But most of the time, we employ random attention as part
    of a **combined pattern** (the bottom-left matrix) that consists of a combination
    of other models. **Dilated attention** is similar to the sliding window, but some
    gaps are put in the window as shown at the top right of *Figure 8.6*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种更常见的模式。**随机注意力**（*Figure 8.6*中的第一个矩阵）用于通过从现有标记中随机选择来促进信息流动。但大多数情况下，我们将随机注意力作为**组合模式**（左下角矩阵）的一部分，该模式由其他模型的组合组成。**扩张注意力**类似于滑动窗口，但在窗口中加入了一些间隙，如*Figure
    8.6*右上方所示：
- en: '![Figure 8.6 – Random, Dilated, Combined, and Blockwise ](img/B17123_08_006.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 随机、扩张、组合和分块](img/B17123_08_006.jpg)'
- en: Figure 8.6 – Random, Dilated, Combined, and Blockwise
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 随机、扩张、组合和分块
- en: The **Blockwise Pattern** (the bottom right of *Figure 8.6*) provides a basis
    for other patterns. It chunks the tokens into a fixed number of blocks, which
    is especially useful for long-context problems. For example, when a 4,096x4,096
    attention matrix is chunked using a block size of 512, then 8 (512x512) query
    blocks and key blocks are formed. Many efficient models such as BigBird and Reformer
    mostly chunk tokens into blocks to reduce the complexity.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**分块模式**（*Figure 8.6*右下角）为其他模式提供了基础。它将标记分块成固定数量的块，这对于长文本问题特别有用。例如，当使用块大小为512来分块4096x4096的注意力矩阵时，就形成了8个（512x512）查询块和键块。许多高效的模型，如BigBird和Reformer，大多数将标记分块以减少复杂性。'
- en: It is important to note that the proposed patterns must be supported by the
    accelerators and the libraries. Some attention patterns such as dilated patterns
    require a special matrix multiplication that is not directly supported in current
    deep learning libraries such as PyTorch or TensorFlow as of writing this chapter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，提出的模式必须得到加速器和库的支持。一些注意力模式，如扩张模式，需要特殊的矩阵乘法，在撰写本章时，当前的深度学习库，如PyTorch或TensorFlow，不直接支持。
- en: 'We are ready to run some experiments for efficient transformers. We will proceed
    with models that are supported by the Transformers library and that have checkpoints
    on the HuggingFace platform. **Longformer** is one of the models that use sparse
    attention. It uses a combination of a sliding window and global attention. It
    supports dilated sliding window attention as well:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好进行一些高效transformer的实验。我们将使用Transformers库支持的模型，并具有HuggingFace平台上的检查点。**Longformer**是使用稀疏注意力的模型之一。它使用滑动窗口和全局注意力的组合。它还支持扩张的滑动窗口注意力：
- en: 'Before we start, we need to install the `py3nvml` package for benchmarking.
    Please recall that we already discussed how to apply benchmarking in [*Chapter
    2*](B17123_02_Epub_AM.xhtml#_idTextAnchor034), *A Hands-On Introduction to the
    Subject*:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要安装`py3nvml`包进行基准测试。请记住，我们已经讨论过如何在[*第2章*](B17123_02_Epub_AM.xhtml#_idTextAnchor034)，*对主题的实际介绍*中应用基准测试：
- en: '[PRE16]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We also need to check our devices to ensure that there is no running process:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要检查我们的设备，以确保没有正在运行的进程：
- en: '[PRE17]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.7 – GPU usage ](img/B17123_08_007.jpg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.7 – GPU使用情况 ](img/B17123_08_007.jpg)'
- en: Figure 8.7 – GPU usage
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.7 – GPU使用情况
- en: 'Currently, the Longformer author has shared a couple of checkpoints. The following
    code snippet loads the Longformer checkpoint `allenai/longformer-base-4096` and
    processes a long text:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前，Longformer的作者分享了一些检查点。以下代码片段加载了Longformer检查点`allenai/longformer-base-4096`并处理了一个长文本：
- en: '[PRE18]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As seen, Longformer can process a sequence up to the length of `4096`. When
    we pass a sequence whose length is more than `4096`, which is the limit, you will
    get the error `IndexError: index out of range in self`.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '如图所示，Longformer可以处理长度达`4096`的序列。当我们传递长度超过`4096`的序列时，这是限制时，您将收到错误`IndexError:
    index out of range in self`。'
- en: 'Longformer''s default `attention_window` is `512`, which is the size of the
    attention window around each token. With the following code, we instantiate two
    Longformer configuration objects, where the first one is the default Longformer,
    and the second is a lighter one where we set the window size to a smaller value
    such as 4 so that the model becomes lighter:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Longformer的默认`attention_window`是`512`，这是每个标记周围的关注窗口的大小。通过以下代码，我们实例化了两个Longformer配置对象，第一个是默认Longformer，第二个是一个更轻量级的Longformer，我们将窗口大小设置为较小的值，例如4，这样模型就变得更轻：
- en: 'Please pay attention to the following examples. We will always call `XformerConfig.from_pretrained()`.
    This call does not download the actual weights of the model checkpoint, instead
    only downloading the configuration from the HuggingFace Hub. Throughout this section,
    since we will not fine-tune, we only need the configuration:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意以下示例。我们将始终调用`XformerConfig.from_pretrained()`。这个调用不会下载模型检查点的实际权重，而是仅从HuggingFace
    Hub下载配置。在本节中，由于我们不会进行微调，我们只需要配置：
- en: '[PRE20]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With these configuration instances, you can train your Longformer language
    model with your own datasets passing the configuration object to the Longformer
    model as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些配置实例，您可以将配置对象传递给Longformer模型，使用自己的数据集训练您的Longformer语言模型，如下所示：
- en: '[PRE21]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Other than training a Longformer model, you can also fine-tune a trained checkpoint
    to a downstream task. To do so, you can continue by applying the code as shown
    in `Chapter 03` for language model training and `Chapter 05-06` for fine-tuning.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了训练Longformer模型，您还可以将训练过的检查点微调到下游任务。要这样做，您可以继续应用`第03章`中所示的代码进行语言模型训练，以及`第05-06章`进行微调。
- en: 'We will now compare the time and memory performance of these two configurations
    with various lengths of input [`128, 256, 512, 1024, 2048, 4096`] by utilizing
    `PyTorchBenchmark` as follows:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将利用`PyTorchBenchmark`比较这两种配置在各种输入长度[`128, 256, 512, 1024, 2048, 4096`]下的时间和内存性能，如下所示：
- en: '[PRE22]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.8 – Benchmark results ](img/B17123_08_008.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 基准结果 ](img/B17123_08_008.jpg)'
- en: Figure 8.8 – Benchmark results
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 基准结果
- en: 'Some hints for `PyTorchBenchmarkArguments`: if you like to see the performance
    for training as well as inference, you should set the argument `training` to `True`
    (the default is `False`). You also may want to see your current environment information.
    You can do so by setting `no_env_print` to `False`; the default is `True`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于`PyTorchBenchmarkArguments`的提示：如果你想看到训练和推断的性能，你应该将参数`training`设置为`True`（默认为`False`）。你可能还想看到您当前的环境信息。您可以通过设置`no_env_print`为`False`来做到这一点；默认为`True`。
- en: Let's visualize the performance to be more interpretable. To do so, we define
    a `plotMe()` function since we will need that function for further experiments
    as well. The function plots the inference performance in terms of both running
    time complexity by default or memory footprint properl:.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过可视化性能来使其更易解释。为此，我们定义了一个`plotMe()`函数，因为我们将来在进一步的实验中也将需要该函数。该函数默认情况下绘制推断性能，以及正确限制的运行时间复杂性或内存占用：
- en: 'Here is the function definition:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是函数定义：
- en: '[PRE23]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s see the computational performance of two Longformer configurations,
    as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看两个Longformer配置的计算性能，如下所示：
- en: '[PRE24]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This plots the following chart:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这绘制了以下图表：
- en: '![Figure 8.9 – Speed performance over sequence length (Longformer) ](img/B17123_08_009.jpg)'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.9 – 随序列长度的速度性能（Longformer）](img/B17123_08_009.jpg)'
- en: Figure 8.9 – Speed performance over sequence length (Longformer)
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.9 – 随序列长度的速度性能（Longformer）
- en: In this and the next examples, we see the main differentiation between a heavy
    model and a light model starting from length 512\. The preceding figure shows
    the lighter Longformer model in green (the one with a window length of 4) performs
    better in terms of time complexity as expected. We also see that two Longformer
    models process the input with linear time complexity.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个和后面的例子中，我们看到重型模型和轻型模型之间的主要差异是从长度512开始。前图显示了长子模型（窗口长度为4）在时间复杂度方面如预期的表现更好。我们还看到两个Longformer模型用线性时间复杂度处理输入。
- en: 'Let''s evaluate these two models in terms of memory performance:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从内存性能的角度评估这两个模型：
- en: '[PRE25]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This plots the following:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这绘制了以下内容：
- en: '![Figure 8.10 – Memory performance over sequence length (Longformer) ](img/B17123_08_010.jpg)'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 8.10 – 随序列长度的内存性能（Longformer）](img/B17123_08_010.jpg)'
- en: Figure 8.10 – Memory performance over sequence length (Longformer)
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.10 – 随序列长度的内存性能（Longformer）
- en: Again, up to length 512, there is no substantial differentiation. For the rest,
    we see a similar memory performance to the time performance. It is clear to say
    that the memory complexity of the Longformer self-attention is linear. On the
    other hand, let me bring to your attention that we are not saying anything about
    model task performance yet.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次强调，直到长度512，没有实质性的差异。从长度512开始，我们看到与时间性能类似的内存性能。显然，Longformer自注意力的内存复杂性是线性的。另一方面，我要提醒你的是，我们尚未对模型任务性能做出任何评论。
- en: Thanks to the `PyTorchBenchmark` script, we have cross-checked these models.
    This script is very useful when we choose which configuration the language model
    should be trained with. It will be vital before starting the real language model
    training and fine-tuning.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非常感谢`PyTorchBenchmark`脚本，我们已经交叉检验了这些模型。当我们选择应该用哪种配置来训练语言模型时，这个脚本非常有用。这在开始真正的语言模型训练和微调之前将是至关重要的。
- en: Another best-performing model exploiting sparse attention is BigBird (Zohen
    et al. 2020). The authors claimed that their sparse attention mechanism (they
    called it a generalized attention mechanism) preserves all the functionality of
    the full self-attention mechanism of vanilla transformers in linear time. The
    authors treated the attention matrix as a directed graph so that they leveraged
    graph theory algorithms. They took inspiration from the graph sparsification algorithm,
    which approximates a given graph `G` by graph `G'` with fewer edges or vertices.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个利用稀疏注意力的表现最佳模型是BigBird（Zohen等，2020）。作者声称他们的稀疏注意力机制（他们将其称为广义注意力机制）在线性时间内保留了香草变压器的全自注意机制的所有功能。作者将注意力矩阵视为有向图，因此他们利用了图论算法。他们从图稀疏化算法中汲取灵感，该算法用较少的边或顶点逼近给定图`G`的图`G'`。
- en: BigBird is a block-wise attention model and can handle sequences up to a length
    of `4096`. It first blockifies the attention pattern by packing queries and keys
    together and then defines attention on these blocks. They utilize random, sliding
    window, and global attention.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BigBird 是一个分块注意力模型，可以处理长度达 `4096` 的序列。它首先通过打包查询和密钥一起将注意力模式分块化，然后对这些块定义注意力。他们利用了随机、滑动窗口和全局注意力。
- en: 'Let''s load and use the BigBird model checkpoint configuration just like the
    Longformer transformer model. There are a couple of BigBird checkpoints shared
    by the developers in the HuggingFace Hub. We select the original BigBird model,
    `google/bigbird-roberta-base`, which is warm started from a RoBERTa checkpoint.
    Once again, we''re not downloading the model checkpoint weights but the configuration
    instead. The `BigBirdConfig` implementation allows us to compare full self-attention
    and sparse attention. Thus, we can observe and check whether the sparsification
    will reduce the full-attention O(n^2) complexity to a lower level. Once again,
    up to a length of 512, we do not clearly observe quadratic complexity. We can
    see the complexity from this level on. Setting the attention type to original-full
    will give us a full self-attention model. For comparison, we created two types
    of configurations: the first one is BigBird''s original sparse approach, the second
    is a model that uses the full self-attention model.'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载并使用与Longformer变压器模型相似的BigBird模型检查点配置。HuggingFace Hub的开发人员共享了几个BigBird检查点。我们选择了原始的BigBird模型，`google/bigbird-roberta-base`，它从一个RoBERTa检查点开始预热。再次强调，我们不是下载模型检查点权重，而是下载配置。`BigBirdConfig`实现允许我们比较完全自注意和稀疏注意。因此，我们可以观察和检查稀疏化是否将完全注意的O(n^2)复杂性降低到更低的水平。再次强调，最多达到512的长度，我们并没有清楚地观察到二次复杂性。我们可以从这个级别开始观察复杂性。将注意类型设置为原始全功能将为我们提供完全自注意模型。为了比较，我们创建了两种类型的配置：第一个是BigBird的原始稀疏方法，第二个是使用完全自注意模型的模型。
- en: 'We call them `sparseBird` and `fullBird` in order as follows:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们依次称它们为`sparseBird`和`fullBird`：
- en: '[PRE26]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Please notice that for smaller sequence lengths up to 512, the BigBird model
    works as full self-attention mode due to block-size and sequence-length inconsistency:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，对于最多512个序列长度，由于块大小和序列长度不一致，BigBird模型将作为完全自注意模式运行：
- en: '[PRE27]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.11 – Benchmark results (BigBird) ](img/B17123_08_011.jpg)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.11 – 基准结果（BigBird）](img/B17123_08_011.jpg)'
- en: Figure 8.11 – Benchmark results (BigBird)
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.11 – 基准结果（BigBird）
- en: 'Again, we plot the time performance as follows:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们将时间性能绘制如下：
- en: '[PRE28]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This plots the following:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这绘制了以下内容：
- en: '![Figure 8.12 – Speed performance (BigBird) ](img/B17123_08_012.jpg)'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.12 – 速度性能（BigBird）](img/B17123_08_012.jpg)'
- en: Figure 8.12 – Speed performance (BigBird)
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.12 – 速度性能（BigBird）
- en: To a certain extent, the full self-attention model performs better than a sparse
    model. However, we can observe the quadratic time complexity for `fullBird`. Hence,
    after a certain point, we also see that the sparse attention model abruptly outperforms
    it, when coming to an end.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在一定程度上，完全自注意模型的性能优于稀疏模型。然而，我们可以观察到`fullBird`的二次时间复杂性。因此，在一定程度上，当接近尾声时，我们也看到稀疏注意模型突然超越它。
- en: 'Let''s check the memory complexity as follows:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们按如下方式检查内存复杂性：
- en: '[PRE29]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the output:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 8.13 – Memory performance (BigBird) ](img/B17123_08_013.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图8.13 – 内存性能（BigBird）](img/B17123_08_013.jpg)'
- en: Figure 8.13 – Memory performance (BigBird)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 – 内存性能（BigBird）
- en: In the preceding figure, we can clearly see linear and quadratic memory complexity.
    Once again, up to a certain point (a length of 2,000 in this example), we cannot
    speak of a clear distinction.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以清楚地看到线性和二次内存复杂性。再次强调，在某一点（本例中长度为2,000），我们无法说出清晰的区别。
- en: Next, let's discuss learnable patterns and work with models that can process
    longer input.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论可学习模式，并使用可以处理更长输入的模型。
- en: Learnable patterns
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可学习模式
- en: Learning-based patterns are the alternatives to fixed (predefined) patterns.
    These approaches extract the patterns in an unsupervised data-driven fashion.
    They leverage some techniques to measure the similarity between the queries and
    the keys to properly cluster them. This transformer family learns first how to
    cluster the tokens and then restrict the interaction to get an optimum view of
    the attention matrix.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的模式是固定（预定义）模式的替代方案。这些方法以无监督数据驱动的方式提取模式。它们利用一些技术来衡量查询和键之间的相似性，以正确地对它们进行聚类。这个变压器家族首先学习如何对令牌进行聚类，然后限制交互以获得注意矩阵的最佳视图。
- en: 'Now, we will do some experiments with Reformer as one of the important efficient
    models based on learnable patterns. Before that, let''s address what the Reformer
    model contributes to the NLP field, as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对Reformer进行一些实验，作为基于可学习模式的重要高效模型之一。在此之前，让我们先了解Reformer模型对NLP领域的贡献，如下所示：
- en: It employs `[a,b,c]` and `[d,e,f]`, the token `d` cannot attend to its immediate
    context `c`. As a remedy, Reformer augments each chunk with the parameters that
    control the number of previous neighboring chunks.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它采用了`[a,b,c]`和`[d,e,f]`，标记`d`无法参与到其直接上下文`c`中。为了解决这个问题，Reformer会用参数增加每个块的前面相邻块的数量。
- en: The most important contribution of Reformer is to leverage the **Locality Sensitive
    Hashing** (**LSH**) function, which assigns the same value to similar query vectors.
    Attention could be approximated by only comparing the most similar vectors, which
    helps us reduce the dimensionality and then sparsify the matrix. It is a safe
    operation since the softmax function is highly dominated by large values and can
    ignore dissimilar vectors. Additionally, instead of finding the relevant keys
    to a given query, only similar queries are found and bucked. That is, the position
    of a query can only attend to the positions of other queries to which it has a
    high cosine similarity.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reformer最重要的贡献是利用**局部敏感哈希**（**LSH**）函数，它将相似的查询向量分配相同的值。通过仅比较最相似的向量来近似注意力，可以帮助我们减少维度，然后将矩阵稀疏化。这是一个安全的操作，因为softmax函数受大值的影响很大，可以忽略不相似的向量。此外，与其查找给定查询的相关键，只查找相似的查询并进行桶分配。也就是说，查询的位置只能参与到其他具有高余弦相似度的查询的位置。
- en: To reduce the memory footprint, Reformer uses reversible residual layers, which
    avoids the need to store the activations of all the layers to be reused for backpropagation,
    following the **Reversible Residual Network** (**RevNet**), because the activations
    of any layer can be recovered from the activation of the following layer.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少内存占用，Reformer使用可逆残差层，避免了需要存储所有层的激活以便后续反向传播的需要，遵循**可逆残差网络**（**RevNet**）的方式，因为任意层的激活都可以从后续层的激活中恢复。
- en: 'It is important to note that the Reformer model and many other efficient transformers
    are criticized as, in practice, they are only more efficient than the vanilla
    transformer when the input length is very long (*REF: Efficient Transformers*:
    A Survey, Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler). We made similar
    observations in our earlier experiments (please see the BigBird and Longformer
    experiment) .'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要注意的是，Reformer模型和许多其他高效的transformer在实践中被批评为，只有当输入长度非常长时才比普通transformer更高效（*REF：Efficient
    Transformers*：A Survey，Yi Tay，Mostafa Dehghani，Dara Bahri，Donald Metzler）。我们在早期的实验中也有类似的观察（请参见BigBird和Longformer实验）。
- en: 'Now we will conduct some experiments with Reformer. Thanks to the HuggingFace
    community again, the Transformers library provides us with Reformer implementation
    and its pre-trained checkpoints. We will load the configuration of the original
    checkpoint `google/reformer-enwik8` and also tweak some settings to work in full
    self-attention mode. When we set `lsh_attn_chunk_length` and `local_attn_chunk_length`
    to `16384`, which is the maximum length that Reformer can process, the Reformer
    instance will have no chance of local optimization and will automatically work
    like a vanilla transformer with full attention. We call it `fullReformer`. As
    for the original Reformer, we instantiate it with default parameters from the
    original checkpoint and call it `sparseReformer` as follows:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将对Reformer进行一些实验。再次感谢HuggingFace社区，Transformers库为我们提供了Reformer的实现及其预训练检查点。我们将加载原始检查点`google/reformer-enwik8`的配置，并调整一些设置以使其在完全自注意模式下工作。当我们将`lsh_attn_chunk_length`和`local_attn_chunk_length`设置为`16384`时，这是Reformer可以处理的最大长度，Reformer实例将没有进行局部优化的机会，而是自动以完全注意力的方式工作，我们称之为`fullReformer`。至于原始的Reformer，我们使用来自原始检查点的默认参数实例化它，并称之为`sparseReformer`如下：
- en: '[PRE30]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Please notice that the Reformer model can process sequences up to a length of
    `16384`. But for the full self-attention mode, due to the accelerator capacity
    of our environment, the attention matrix does not fit on GPU, and we get a CUDA
    out of memory warning. Hence, we set the max length as `12000`. If your environment
    is suitable, you can increase it.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，Reformer模型可以处理长度最长为`16384`的序列。但是对于完全自注意模式，由于我们环境的加速器容量限制，注意力矩阵无法适应GPU，并且我们会收到CUDA内存不足的警告。因此，我们将最大长度设置为`12000`。如果您的环境适合，您可以增加它。
- en: 'Let''s run the benchmark experiments as follows:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们进行基准实验如下：
- en: '[PRE31]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.14 – Benchmark results ](img/B17123_08_014.jpg)'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.14 – 基准结果 ](img/B17123_08_014.jpg)'
- en: Figure 8.14 – Benchmark results
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.14 – 基准结果
- en: 'Let''s visualize the time performance result as follows:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将时间性能结果可视化如下：
- en: '[PRE32]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is the following:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 8.15 – Speed performance (Reformer)](img/B17123_08_015.jpg)'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图8.15 – 速度性能（Reformer）](img/B17123_08_015.jpg)'
- en: Figure 8.15 – Speed performance (Reformer)
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.15 – 速度性能（Reformer）
- en: 'We can see the linear and quadratic complexity of the models. We observe similar
    characteristics for the memory footprint by running the following line:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到模型的线性和二次复杂度。通过运行以下行，我们可以观察到内存占用的类似特征：
- en: '[PRE33]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It plots the following:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它绘制了以下内容：
- en: '![Figure 8.16 – Memory usage (Reformer) ](img/B17123_08_016.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图8.16 – 内存使用（Reformer）](img/B17123_08_016.jpg)'
- en: Figure 8.16 – Memory usage (Reformer)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 – 内存使用（Reformer）
- en: Just as expected, Reformer with sparse attention produces a lightweight model.
    However, as was said before, we have difficulty observing the quadratic/linear
    complexity up to a certain length. As all these experiments indicate, efficient
    transformers can mitigate the time and memory complexity for longer text. What
    about task performance? How accurate would they be for classification or summarization
    tasks? To answer this, we will either start an experiment or have a look at the
    performance reports in the relevant articles of the models. For the experiment,
    you can repeat the code in `chapter 04` and `chapter 05` by instantiating an efficient
    model instead of a vanilla transformer. And you can track model performance and
    optimize it by using model tracking tools that we will discuss in detail in [*Chapter
    11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152), *Attention Visualization and Experiment
    Tracking*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，具有稀疏注意力的Reformer产生了一个轻量级模型。然而，正如之前所说的，我们在某个长度上难以观察到二次/线性复杂度。正如所有这些实验所指示的，高效变压器可以减轻长文本的时间和内存复杂度。那么任务性能呢？它们在分类或摘要任务中的准确性如何？为了回答这个问题，我们将开始一个实验或查看相关模型的性能报告中的性能。对于实验，您可以通过实例化一个高效模型而不是一个普通的变压器来重复`chapter
    04`和`chapter 05`中的代码。您可以使用我们将在[*Chapter 11*](B17123_11_Epub_AM.xhtml#_idTextAnchor152)中详细讨论的模型跟踪工具来跟踪模型性能并对其进行优化，*注意可视化和实验跟踪*。
- en: Low-rank factorization, kernel methods, and other approaches
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 低秩分解、核方法和其他方法
- en: The latest trend of the efficient model is to leverage low-rank approximations
    of the full self-attention matrix. These models are considered to be the lightest
    since they can reduce the self-attention complexity from *O(n2) to O(n)* in both
    computational time and memory footprint. Choosing a very small projection dimension
    *k*, such that *k << n*, then the memory and space complexity is highly reduced.
    Linformer and Synthesizer are the models that efficiently approximate the full
    attention with a low-rank factorization. They decompose the dot-product *N×N*
    attention of the original transformer through linear projections.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 高效模型的最新趋势是利用完整自注意力矩阵的低秩近似。这些模型被认为是最轻量级的，因为它们可以将自注意力的复杂度从*O(n2)*降低到*O(n)*，无论是在计算时间还是内存占用上。选择一个非常小的投影维度*k*，使得*k
    << n*，那么内存和空间复杂度会大大降低。Linformer 和 Synthesizer 是能够通过低秩分解高效近似完整注意力的模型。它们通过线性投影来分解原始变压器的点积*
    N×N* 注意力。
- en: Kernel attention is another method family that we have seen lately to improve
    efficiency by viewing the attention mechanism through kernelization. A kernel
    is a function that takes two vectors as arguments and returns the product of their
    projection with a feature map. It enables us to operate in high-dimensional feature
    space without even computing the coordinate of the data in that high-dimensional
    space, because computations within that space become more expensive. This is when
    the kernel trick comes into play. The efficient models based on kernelization
    enable us to re-write the self-attention mechanism to avoid explicitly computing
    the N×N matrix. In machine learning, the algorithm we hear the most about kernel
    methods is Support Vector Machines, where the radial basis function kernel or
    polynomial kernel are widely used, especially for nonlinearity. For transformers,
    the most notable examples are **Performer** and **Linear Transformers**.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 核关注是另一种最近我们看到的方法家族，通过核化来改善效率。核是一个将两个向量作为参数并返回它们的投影与特征映射的乘积的函数。它使我们能够在高维特征空间中操作，甚至不需要计算数据在该高维空间中的坐标，因为在该空间内的计算变得更加昂贵。这就是核技巧发挥作用的时候。基于核化的高效模型使我们能够重写自注意力机制，以避免显式计算*N×N*矩阵。在机器学习中，我们最常听到的有关核方法的算法是支持向量机，其中径向基函数核或多项式核被广泛使用，特别是用于非线性。对于变压器，最引人注目的例子是**Performer**和**线性变压器**。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The importance of this chapter is that we have learned how to mitigate the burden
    of running large models under limited computational capacity. We first discussed
    and implemented how to make efficient models out of trained models using distillation,
    pruning, and quantization. It is important to pre-train a smaller general-purpose
    language model such as DistilBERT. Such light models can then be fine-tuned with
    good performance on a wide variety of problems compared to their non-distilled
    counterparts.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重要性在于我们学会了如何在有限的计算能力下减轻运行大型模型的负担。我们首先讨论并实施了如何使用蒸馏、剪枝和量化使训练模型变得高效的方法。预训练一个较小的通用语言模型，比如
    DistilBERT，是很重要的。这样的轻量化模型可以与非蒸馏的模型相比，在各种问题上都能够取得良好的性能。
- en: Second, we have gained knowledge about efficient sparse transformers that replace
    the full self-attention matrix with a sparse one using approximation techniques
    such as Linformer, BigBird, Performer, and so on. We have seen how they perform
    on various benchmarks such as computational complexity and memory complexity.
    The examples showed us these approaches are able to reduce the quadratic complexity
    to linear complexity without sacrificing the performance.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们了解了如何使用近似技术，例如 Linformer、BigBird、Performer 等，将完整的自注意力矩阵替换为稀疏矩阵的高效稀疏 Transformer。我们已经看到它们在各种基准测试中的性能表现，如计算复杂度和内存复杂度。这些例子向我们展示了这些方法能够将二次复杂度降低到线性复杂度，而不损害性能。
- en: 'In the next chapter, we will discuss other important topics: cross-lingual/multi-lingual
    models.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论其他重要主题：跨语言/多语言模型。
- en: References
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). *DistilBERT, a distilled
    version of BERT: smaller, faster, cheaper and lighter*. arXiv preprint arXiv:1910.01108.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). *DistilBERT，BERT 的精简版：更小、更快、更便宜、更轻量化*.
    arXiv 预印本 arXiv:1910.01108。
- en: Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
    & Weller, A. (2020). *Rethinking attention with performers*. arXiv preprint arXiv:2009.14794.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
    & Weller, A. (2020). *重新思考注意力机制：表演者方法*. arXiv 预印本 arXiv:2009.14794。
- en: 'Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). *Linformer: Self-attention
    with linear complexity*. arXiv preprint arXiv:2006.04768.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). *Linformer：具有线性复杂性的自注意力机制*.
    arXiv 预印本 arXiv:2006.04768。
- en: 'Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S.,
    ... & Ahmed, A. (2020). *Big bird: Transformers for longer sequences*. arXiv preprint
    arXiv:2007.14062.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S.,
    ... & Ahmed, A. (2020). *大鸟：针对更长序列的 Transformer*. arXiv 预印本 arXiv:2007.14062。
- en: 'Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). *Efficient transformers:
    A survey*. arXiv preprint arXiv:2009.06732.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). *高效 Transformer：一项调查*.
    arXiv 预印本 arXiv:2009.06732。
- en: 'Tay, Y., Bahri, D., Metzler, D., Juan, D. C., Zhao, Z., & Zheng, C. (2020).
    *Synthesizer: Rethinking self-attention in transformer models.* arXiv preprint
    arXiv:2005.00743.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay, Y., Bahri, D., Metzler, D., Juan, D. C., Zhao, Z., & Zheng, C. (2020).
    *合成器：重新思考 Transformer 模型中的自注意力机制*. arXiv 预印本 arXiv:2005.00743。
- en: 'Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). *Reformer: The efficient transformer.*
    arXiv preprint arXiv:2001.04451.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). *Reformer：高效 Transformer*. arXiv
    预印本 arXiv:2001.04451。
- en: Fournier, Q., Caron, G. M., & Aloise, D. (2021). *A Practical Survey on Faster
    and Lighter Transformers.* arXiv preprint arXiv:2103.14636.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fournier, Q., Caron, G. M., & Aloise, D. (2021). *关于更快、更轻量化 Transformer 的实用调查*.
    arXiv 预印本 arXiv:2103.14636。
