- en: Solving Multi-armed Bandit Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决多臂老虎机问题
- en: Multi-armed bandit algorithms are probably among the most popular algorithms
    in reinforcement learning. This chapter will start by creating a multi-armed bandit
    and experimenting with random policies. We will focus on how to solve the multi-armed
    bandit problem using four strategies, including epsilon-greedy, softmax exploration,
    upper confidence bound, and Thompson sampling. We will see how they deal with
    the exploration-exploitation dilemma in their own unique ways. We will also work
    on a billion-dollar problem, online advertising, and demonstrate how to solve
    it using a multi-armed bandit algorithm. Finally, we will solve the contextual
    advertising problem using contextual bandits to make more informed decisions in
    ad optimization.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机算法可能是强化学习中最流行的算法之一。本章将从创建多臂老虎机开始，并尝试使用随机策略。我们将专注于如何使用ε-贪心、softmax探索、上置信区间和汤普森抽样等四种策略解决多臂老虎机问题。我们将看到它们如何以独特的方式处理探索与利用的困境。我们还将解决一个价值十亿美元的问题，即在线广告，演示如何使用多臂老虎机算法解决它。最后，我们将使用上下文老虎机解决上下文广告问题，以在广告优化中做出更明智的决策。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下配方：
- en: Creating a multi-armed bandit environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建多臂老虎机环境
- en: Solving multi-armed bandit problems with the epsilon-greedy policy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ε-贪心策略解决多臂老虎机问题
- en: Solving multi-armed bandit problems with softmax exploration
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用softmax探索解决多臂老虎机问题
- en: Solving multi-armed bandit problems with an upper confidence bound algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用上置信区间算法解决多臂老虎机问题
- en: Solving internet advertising problems with the multi-armed bandit
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多臂老虎机解决互联网广告问题
- en: Solving multi-armed bandit problems with the Thompson sampling algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用汤普森抽样算法解决多臂老虎机问题
- en: Solving internet advertising problems with contextual bandits
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用上下文老虎机解决互联网广告问题
- en: Creating a multi-armed bandit environment
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建多臂老虎机环境
- en: Let’s get started with a simple project of estimating the value of π using the
    Monte Carlo method, which is the core of model-free reinforcement learning algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始一个简单的项目，使用蒙特卡洛方法来估计π的值，这是无模型强化学习算法的核心。
- en: '**The** **multi-armed bandit** problem is one of the simplest reinforcement
    learning problems. It is best described as a slot machine with multiple levers
    (arms), and each lever has a different payout and payout probability. Our goal
    is to discover the best lever with the maximum return so that we can keep choosing
    it afterward. Let’s start with a simple multi-armed bandit problem in which the
    payout and payout probability is fixed for each arm. After creating the environment,
    we will solve it using the random policy algorithm.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**多臂老虎机**问题是最简单的强化学习问题之一。它最好被描述为一个有多个杠杆（臂）的老虎机，每个杠杆有不同的支付和支付概率。我们的目标是发现具有最大回报的最佳杠杆，以便在之后继续选择它。让我们从一个简单的多臂老虎机问题开始，其中每个臂的支付和支付概率是固定的。在创建环境后，我们将使用随机策略算法来解决它。'
- en: How to do it...
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s develop the multi-armed bandit environment as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤开发多臂老虎机环境：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The step method executes an action and returns the reward if it pays out, otherwise
    it returns 0.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤方法执行一个动作，并在支付时返回奖励，否则返回 0。
- en: 'Now, we will use a multi-armed bandit as an example and solve it with random
    policy:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将以多臂老虎机为例，并使用随机策略解决它：
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三臂老虎机的支付概率和奖励，并创建老虎机环境的实例：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For example, there is a 10% chance of getting a reward of 4 by choosing arm
    0.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，选择臂 0 获得奖励 4 的概率为 10%。
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要运行的集数，并定义保存通过选择各个臂累积的总奖励、选择各个臂的次数以及各个臂随时间的平均奖励的列表：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the random policy, which randomly selects an arm:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义随机策略，随机选择一个臂：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we run 100,000 episodes. For each episode, we also update the statistics
    of each arm:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们运行 100,000 个集数。对于每个集数，我们还更新每个臂的统计数据：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running 100,000 episodes, we plot the results of average reward over
    time:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行了 100,000 个集数后，我们绘制了随时间变化的平均奖励的结果：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In the example we just worked on, there are three slot machines. Each machine
    has a different payout (reward) and payout probability. In each episode, we randomly
    chose one arm of the machine to pull (one action to execute) and get a payout
    at a certain probability.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚处理的示例中，有三台老虎机。每台机器都有不同的支付（奖励）和支付概率。在每个episode中，我们随机选择一台机器的一个臂来拉（执行一个动作），并以一定的概率获得支付。
- en: 'Run the lines of code in *Step 5*; you will see the following plot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*Step 5*中的代码行，你将看到以下图表：
- en: '![](img/b526f1a1-f16e-4d00-8f6a-c2e08fe8774e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b526f1a1-f16e-4d00-8f6a-c2e08fe8774e.png)'
- en: Arm 1 is the best arm with the largest average reward. Also, the average rewards
    start to saturate round 10,000 episodes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 臂1是平均奖励最高的最佳臂。此外，平均奖励在大约10,000个episode后开始饱和。
- en: This solution seems very naive as we only perform an exploration of all arms.
    We will come up with more intelligent strategies in the upcoming recipes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此解决方案看起来非常幼稚，因为我们仅对所有臂进行了探索。在接下来的配方中，我们将提出更智能的策略。
- en: Solving multi-armed bandit problems with the epsilon-greedy policy
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ε-贪婪策略解决多臂老虎机问题
- en: Instead of exploring solely with random policy, we can do better with a combination
    of exploration and exploitation. Here comes the well-known epsilon-greedy policy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不再仅仅通过随机策略进行探索，我们可以通过探索与利用的结合做得更好。这就是著名的ε-贪婪策略。
- en: 'Epsilon-greedy for multi-armed bandits exploits the best action the majority
    of the time and also keeps exploring different actions from time to time. Given
    a parameter, ε, with a value from 0 to 1, the probabilities of performing exploration
    and exploitation are ε and 1 - ε, respectively:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多臂老虎机的ε-贪婪策略，大部分时间利用最佳动作，同时不时探索不同的动作。给定参数ε，其取值范围为0到1，执行探索和利用的概率分别为ε和1 - ε：
- en: '**Epsilon**: Each action is taken with a probability calculated as follows:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ε**：每个动作的概率如下计算：'
- en: '![](img/f07c030e-3e0d-42ba-a12c-11171c62525d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f07c030e-3e0d-42ba-a12c-11171c62525d.png)'
- en: Here, |A| is the number of possible actions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，|A|是可能动作的数量。
- en: '**Greedy**: The action with the highest state-action value is favored, and
    its probability of being chosen is increased by 1 - ε:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪**：优选具有最高状态-动作值的动作，并且其被选择的概率增加1 - ε：'
- en: '![](img/b76b5ffc-631a-46a9-867d-54c71bc2ab58.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b76b5ffc-631a-46a9-867d-54c71bc2ab58.png)'
- en: How to do it...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We solve the multi-armed bandit problem using the epsilon-greedy policy as
    follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用ε-贪婪策略解决多臂老虎机问题如下：
- en: 'Import the PyTorch and the bandit environment we developed in the previous
    recipe, *Creating a multi-armed bandit environment* (assuming the `BanditEnv`
    class is in a file called `multi_armed_bandit.py`):'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch和我们在之前的配方中开发的老虎机环境，*创建多臂老虎机环境*（假设`BanditEnv`类在名为`multi_armed_bandit.py`的文件中）：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三臂老虎机的支付概率和奖励，并创建一个老虎机环境的实例：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定要运行的episode数，并定义持有通过选择各个臂累积的总奖励、选择各个臂的次数以及每个臂随时间变化的平均奖励的列表：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the epsilon-greedy policy function, specify the value of epsilon, and
    create an epsilon-greedy policy instance:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义ε-贪婪策略函数，指定ε的值，并创建一个ε-贪婪策略实例：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Initialize the `Q` function, which is the average reward obtained by individual
    arms:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`Q`函数，即各个臂获得的平均奖励：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will update the `Q` function over time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随时间更新`Q`函数。
- en: 'Now, we run 100,000 episodes. For each episode, we also update the statistics
    of each arm:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们运行100,000个episode。每个episode，我们还会随时间更新每个臂的统计信息：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行100,000个episode后，我们绘制了随时间变化的平均奖励结果：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Similar to other MDP problems, the epsilon-greedy policy selects the best arm
    with a probability of 1 - ε and performs random exploration with a probability
    of ε. Epsilon manages the trade-off between exploration and exploitation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于其他MDP问题，ε-贪婪策略以1 - ε的概率选择最佳臂，并以ε的概率进行随机探索。ε管理着探索与利用之间的权衡。
- en: 'In *Step 7*, you will see the following plot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Step 7*中，你将看到以下图表：
- en: '![](img/5d2ef11b-8e61-4a96-9fbc-53652af32739.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d2ef11b-8e61-4a96-9fbc-53652af32739.png)'
- en: Arm 1 is the best arm, with the largest average reward at the end. Also, its
    average reward starts to saturate after around 1,000 episodes.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 臂 1 是最佳臂，在最后具有最大的平均奖励。此外，它的平均奖励在大约 1,000 个剧集后开始饱和。
- en: There's more...
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: You may wonder whether the epsilon-greedy policy actually outperforms the random
    policy. Besides the fact that the value for the optimal arm converges earlier
    with the epsilon-greedy policy, we can also prove that, on average, the reward
    we get during the course of training is higher with the epsilon-greedy policy
    than the random policy.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道ε-贪婪策略是否确实优于随机策略。除了在ε-贪婪策略中最优臂的值较早收敛外，我们还可以证明，在训练过程中，通过ε-贪婪策略获得的平均奖励比随机策略更高。
- en: 'We can simply average the reward over all episodes:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地计算所有剧集的平均奖励：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Over 100,000 episodes, the average payout is `0.43718` with the epsilon-greedy
    policy. Repeating the same computation for the random policy solution, we get
    0.37902 as the average payout.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 100,000 个剧集中，使用ε-贪婪策略的平均支付率为 `0.43718`。对随机策略解决方案进行相同计算后，得到平均支付率为 0.37902。
- en: Solving multi-armed bandit problems with the softmax exploration
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 softmax 探索解决多臂赌博问题
- en: In this recipe, we will solve the multi-armed bandit problem using the softmax
    exploration, algorithm. We will see how it differs from the epsilon-greedy policy.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用 softmax 探索算法解决多臂赌博问题。我们将看到它与ε-贪婪策略的不同之处。
- en: 'As we''ve seen with epsilon-greedy, when performing exploration we randomly
    select one of the non-best arms with a probability of ε/|A|. Each non-best arm
    is treated equivalently regardless of its value in the Q function. Also, the best
    arm is chosen with a fixed probability regardless of its value. In **softmax exploration**,
    an arm is chosen based on a probability from the **softmax distribution** of the
    Q function values. The probability is calculated as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在ε-贪婪中看到的，当进行探索时，我们以 ε/|A| 的概率随机选择非最佳臂之一。每个非最佳臂在 Q 函数中的价值不管其值都是等效的。此外，无论其值如何，最佳臂都以固定概率被选择。在
    **softmax 探索** 中，根据 Q 函数值的 softmax 分布选择臂。概率计算如下：
- en: '![](img/8c1bf157-b0ab-4725-841d-7f8299aff6e9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c1bf157-b0ab-4725-841d-7f8299aff6e9.png)'
- en: Here, the τ parameter is the temperature factor, which specifies the randomness
    of the exploration. The higher the value of τ, the closer to equal exploration
    it becomes; the lower the value of τ, the more likely the best arm is chosen.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，τ 参数是温度因子，用于指定探索的随机性。τ 值越高，探索就越接近平等；τ 值越低，选择最佳臂的可能性就越大。
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'We solve the multi-armed bandit problem using the softmax exploration algorithm
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下解决了使用 softmax 探索算法的多臂赌博问题：
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 和我们在第一个示例中开发的赌博环境，*创建多臂赌博环境*（假设 `BanditEnv` 类在名为 `multi_armed_bandit.py`
    的文件中）：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三臂赌博的支付概率和奖励，并创建赌博环境的实例：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要运行的剧集数量，并定义保存通过选择各个臂累积的总奖励、选择各个臂的次数以及每个臂随时间的平均奖励的列表：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the softmax exploration policy function, specify the value of τ, and
    create a softmax exploration policy instance:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 softmax 探索策略函数，指定 τ 的值，并创建 softmax 探索策略实例：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Initialize the Q function, which is the average reward obtained by the individual
    arms:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 函数，即通过各个臂获得的平均奖励：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We will update the Q function over time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随时间更新 Q 函数。
- en: 'Now, we run 100,000 episodes. For each episode, we also update the statistics
    of each arm:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们运行 100,000 个剧集。对于每个剧集，我们还更新每个臂的统计信息：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行了 100,000 个剧集后，我们绘制了随时间变化的平均奖励结果：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: How it works...
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: With the softmax exploration strategy, the dilemma of exploitation and exploration
    is solved with a softmax function based on the Q values. Instead of using a fixed
    pair of probabilities for the best arm and non-best arms, it adjusts the probabilities
    according to the softmax distribution with the τ parameter as a temperature factor.
    The higher the value of τ, the more focus will be shifted to exploration.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用softmax探索策略，利用基于Q值的softmax函数解决了开发与探索的困境。它不是使用最佳臂和非最佳臂的固定概率对，而是根据τ参数作为温度因子的softmax分布调整概率。τ值越高，焦点就会更多地转向探索。
- en: 'In *Step 7*, you will see the following plot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤7*中，您将看到以下绘图：
- en: '![](img/85a2e1aa-29e4-4277-a3ca-811fcffeccec.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85a2e1aa-29e4-4277-a3ca-811fcffeccec.png)'
- en: Arm 1 is the best arm, with the largest average reward at the end. Also, its
    average reward starts to saturate after around 800 episodes in this example.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 臂1是最佳臂，在最后具有最大的平均奖励。此外，在这个例子中，它的平均奖励在大约800个episode后开始饱和。
- en: Solving multi-armed bandit problems with the upper confidence bound algorithm
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用上置信度边界算法解决多臂赌博问题
- en: In the previous two recipes, we explored random actions in the multi-armed bandit
    problem with probabilities that are either assigned as fixed values in the epsilon-greedy
    policy or computed based on the Q-function values in the softmax exploration algorithm.
    In either algorithm, the probabilities of taking random actions are not adjusted
    over time. Ideally, we want less exploration as learning progresses. In this recipe,
    we will use a new algorithm called **upper confidence bound** to achieve this
    goal.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个配方中，我们通过在epsilon-贪婪策略中将概率分配为固定值或者根据Q函数值计算softmax探索算法中的概率，探索了多臂赌博问题中的随机动作。在任一算法中，随机执行动作的概率并不随时间调整。理想情况下，我们希望随着学习的进行减少探索。在本配方中，我们将使用称为**上置信度边界**的新算法来实现这一目标。
- en: 'The **upper confidence bound** (**UCB**) algorithm stems from the idea of the
    confidence interval. In general, the confidence interval is a range of values
    where the true value lies. In the UCB algorithm, the confidence interval for an
    arm is a range where the mean reward obtained with this arm lies. The interval
    is in the form of [lower confidence bound, upper confidence bound] and we only
    use the upper bound, which is the UCB, to estimate the potential of the arm. The
    UCB is computed as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**上置信度边界**（**UCB**）算法源于置信区间的概念。一般来说，置信区间是真值所在的一系列值。在UCB算法中，臂的置信区间是该臂获取的平均奖励所处的范围。该区间的形式为[下置信度边界，上置信度边界]，我们只使用上置信度边界，即UCB，来估计该臂的潜力。UCB的计算公式如下：'
- en: '![](img/5dfdbc84-307b-4249-8a2b-0dd4f636dca4.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5dfdbc84-307b-4249-8a2b-0dd4f636dca4.png)'
- en: Here, t is the number of episodes, and N(a) is the number of times arm a is
    chosen among t episodes. As learning progresses, the confidence interval shrinks
    and becomes more and more accurate. The arm to pull is the one with the highest
    UCB.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，t是episode的数量，N(a)是在t个episode中臂a被选择的次数。随着学习的进行，置信区间收缩并变得越来越精确。应该拉动的臂是具有最高UCB的臂。
- en: How to do it...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We solve the multi-armed bandit problem using the UCB algorithm as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用UCB算法解决多臂赌博问题的步骤如下：
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch和第一个配方中开发的赌博环境，*创建多臂赌博环境*（假设`BanditEnv`类位于名为`multi_armed_bandit.py`的文件中）：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the payout probabilities and rewards for the three-armed bandit and
    create an instance of the bandit environment:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三臂赌博的赔率概率和奖励，并创建赌博环境的一个实例：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要运行的episode数量，并定义保存通过选择不同臂积累的总奖励、选择各个臂的次数以及各个臂随时间的平均奖励的列表：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the UCB policy function, which computes the best arm based on the UCB
    formula:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义UCB策略函数，根据UCB公式计算最佳臂：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Initialize the Q function, which is the average reward obtained with individual
    arms:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化Q函数，它是使用各个臂获取的平均奖励：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will update the Q function over time.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，我们将更新Q函数。
- en: 'Now, we run 100,000 episodes with our UCB policy. For each episode, we also
    update the statistics of each arm:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用我们的UCB策略运行100,000个episode。对于每个episode，我们还更新每个臂的统计信息：
- en: '[PRE26]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行了10万个剧集后，我们绘制了随时间变化的平均奖励结果：
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we solved the multi-armed bandit with the UCB algorithm. It
    adjusts the exploitation-exploration dilemma according to the number of episodes.
    For an action with a few data points, its confidence interval is relatively wide,
    hence, choosing this action is of relatively high uncertainty. With more episodes
    of the action being selected, the confidence interval becomes narrow and shrinks
    to its actual value. In this case, it is of high certainty to choose (or not)
    this action. Finally, the UCB algorithm pulls the arm with the highest UCB in
    each episode and gains more and more confidence over time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了UCB算法解决了多臂赌博机问题。它根据剧集数调整开发-探索困境。对于数据点较少的动作，其置信区间相对较宽，因此选择此动作具有相对较高的不确定性。随着更多的动作剧集被选中，置信区间变窄并收缩到其实际值。在这种情况下，选择（或不选择）此动作是非常确定的。最后，在每个剧集中，UCB算法拉动具有最高UCB的臂，并随着时间的推移获得越来越多的信心。
- en: 'After running the code in *Step 7*, you will see the following plot:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7步*中运行代码后，您将看到以下绘图：
- en: '![](img/34c31cbd-e4b9-4415-aa1f-0a4313d1382d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34c31cbd-e4b9-4415-aa1f-0a4313d1382d.png)'
- en: Arm 1 is the best arm, with the largest average reward in the end.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个臂是最佳的臂，最终平均奖励最高。
- en: There's more...
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: You may wonder whether UCB actually outperforms the epsilon-greedy policy. We
    can compute the average reward over the entire training process, and the policy
    with the highest average reward learns faster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道UCB是否真的优于ε-greedy策略。我们可以计算整个训练过程中的平均奖励，平均奖励最高的策略学习速度更快。
- en: 'We can simply average the reward over all episodes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地平均所有剧集的奖励：
- en: '[PRE28]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Over 100,000 episodes, the average payout is 0.44605 with UCB, which is higher
    than 0.43718 with the epsilon-greedy policy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在10万个剧集中，使用UCB的平均支付率为0.44605，高于ε-greedy策略的0.43718。
- en: See also
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For those who want to brush up on confidence intervals, feel free to check
    out the following: [http://www.stat.yale.edu/Courses/1997-98/101/confint.htm](http://www.stat.yale.edu/Courses/1997-98/101/confint.htm)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些想要了解置信区间的人，请随时查看以下内容：[http://www.stat.yale.edu/Courses/1997-98/101/confint.htm](http://www.stat.yale.edu/Courses/1997-98/101/confint.htm)
- en: Solving internet advertising problems with a multi-armed bandit
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决互联网广告问题的多臂赌博机
- en: 'Imagine you are an advertiser working on ad optimization on a website:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您是一位在网站上进行广告优化的广告商：
- en: There are three different colors of ad background – red, green, and blue. Which
    one will achieve the best click-through rate (CTR)?
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告背景有三种不同的颜色 – 红色，绿色和蓝色。哪种将实现最佳点击率（CTR）？
- en: There are three types of wordings of the ad – *learn …*, *free ...*, and *try
    ...*. Which one will achieve the best CTR?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告有三种不同的文案 – *学习…*，*免费…* 和 *尝试…*。哪一个将实现最佳CTR？
- en: For each visitor, we need to choose an ad in order to maximize the CTR over
    time. How can we solve this?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每位访客，我们需要选择一个广告，以最大化随时间的点击率（CTR）。我们如何解决这个问题？
- en: Perhaps you are thinking about A/B testing, where you randomly split the traffic
    into groups and assign each ad to a different group, and then choose the ad from
    the group with the highest CTR after a period of observation. However, this is
    basically a complete exploration, and we are usually unsure of how long the observation
    period should be and will end up losing a large portion of potential clicks. Besides,
    in A/B testing, the unknown CTR for an ad is assumed to not change over time.
    Otherwise, such A/B testing should be re-run periodically.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 或许您在考虑A/B测试，其中您随机将流量分成几组，并将每个广告分配到不同的组中，然后在观察一段时间后选择具有最高CTR的组中的广告。然而，这基本上是完全的探索，我们通常不确定观察期应该多长，最终会失去大量潜在的点击。此外，在A/B测试中，假设广告的未知CTR不会随时间而变化。否则，这种A/B测试应定期重新运行。
- en: A multi-armed bandit can certainly do better than A/B testing. Each arm is an
    ad, and the reward for an arm is either 1 (click) or 0 (no click).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博机确实可以比A/B测试做得更好。每个臂是一个广告，臂的奖励要么是1（点击），要么是0（未点击）。
- en: Let's try to solve it with the UCB algorithm.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试用UCB算法解决这个问题。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'We can solve the multi-armed bandit advertising problem using the UCB algorithm
    as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用UCB算法解决多臂赌博机广告问题，具体如下：
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 和我们在第一个示例中开发的老虎机环境，《创建多臂老虎机环境》（假设 `BanditEnv` 类位于名为 `multi_armed_bandit.py`
    的文件中）：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the payout probabilities and rewards for the three-armed bandit (three
    ad candidates, for example) and create an instance of the bandit environment:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三臂老虎机（例如三个广告候选项）的支付概率和奖励，并创建老虎机环境的实例：
- en: '[PRE30]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, the true CTR for ad 0 is 1%, for ad 1 1.5%, and for ad 2 3%.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，广告 0 的真实点击率为 1%，广告 1 为 1.5%，广告 2 为 3%。
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要运行的周期数，并定义包含通过选择各个臂累积的总奖励、选择各个臂的次数以及每个臂随时间的平均奖励的列表：
- en: '[PRE31]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the UCB policy function, which computes the best arm based on the UCB
    formula:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 UCB 策略函数，根据 UCB 公式计算最佳臂：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Initialize the Q function, which is the average reward obtained by individual
    arms:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Q 函数，即各个臂获得的平均奖励：
- en: '[PRE33]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We will update the Q function over time.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随时间更新 Q 函数。
- en: 'Now, we run 100,000 episodes with the UCB policy. For each episode, we also
    update the statistics of each arm:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用 UCB 策略运行 100,000 个周期。对于每个周期，我们还更新每个臂的统计信息：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 100,000 个周期后，我们绘制随时间变化的平均奖励结果：
- en: '[PRE35]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How it works...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理…
- en: In this recipe, we solved the ad optimization problem in a multi-armed bandit
    manner. It overcomes the challenges confronting the A/B testing approach. We used
    the UCB algorithm to solve the multi-armed (multi-ad) bandit problem; the reward
    for each arm is either 1 or 0\. Instead of pure exploration and no interaction
    between action and reward, UCB (or other algorithms such as epsilon-greedy and
    softmax exploration) dynamically switches between exploitation and exploration
    where necessarly. For an ad with a few data points, the confidence interval is
    relatively wide, hence, choosing this action is of relatively high uncertainty.
    With more episodes of the ad being selected, the confidence interval becomes narrow
    and shrinks to its actual value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们以多臂老虎机的方式解决了广告优化问题。它克服了 A/B 测试方法所面临的挑战。我们使用 UCB 算法解决多臂（多广告）老虎机问题；每个臂的奖励要么是
    1，要么是 0。UCB（或其他算法如 epsilon-greedy 和 softmax 探索）动态地在开发和探索之间切换。对于数据点较少的广告，置信区间相对较宽，因此选择此动作具有相对高的不确定性。随着广告被选择的次数增多，置信区间变窄，并收敛到其实际值。
- en: 'You can see the resulting plot in *Step 7* as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 *第 7 步* 中看到生成的图表如下：
- en: '![](img/269802e0-7738-422f-aedb-183bdbfb4505.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/269802e0-7738-422f-aedb-183bdbfb4505.png)'
- en: Ad 2 is the best ad with the highest predicted CTR (average reward) after the
    model converges.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 模型收敛后，广告 2 是预测的点击率（平均奖励）最高的广告。
- en: Eventually, we found that ad 2 is the optimal one to choose, which is true.
    Also, the sooner we figure this out the better, because we will lose fewer potential
    clicks. In this example, ad 2 outperformed the others after around 100 episodes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们发现广告 2 是最优选择，这是真实的。而且我们越早发现这一点越好，因为我们会损失更少的潜在点击。在这个例子中，大约在 100 个周期后，广告
    2 表现优于其他广告。
- en: Solving multi-armed bandit problems with the Thompson sampling algorithm
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用汤普森抽样算法解决多臂老虎机问题
- en: In this recipe, we will tackle the exploitation and exploration dilemma in the
    advertising bandits problem using another algorithm, Thompson sampling. We will
    see how it differs greatly from the previous three algorithms.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用另一种算法——汤普森抽样，解决广告老虎机问题中的开发和探索困境。我们将看到它与前三种算法的显著区别。
- en: '**Thompson sampling** (**TS**) is also called Bayesian bandits as it applies
    the Bayesian way of thinking from the following perspectives:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**汤普森抽样**（**TS**）也称为贝叶斯老虎机，因为它从以下角度应用贝叶斯思维：'
- en: It is a probabilistic algorithm.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个概率算法。
- en: It computes the prior distribution for each arm and samples a value from each
    distribution.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它计算每个臂的先验分布并从每个分布中抽样一个值。
- en: It then selects the arm with the highest value and observes the reward.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后选择值最高的臂并观察奖励。
- en: Finally, it updates the prior distribution based on the observed reward. This
    process is called **Bayesian updating**.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，根据观察到的奖励更新先验分布。这个过程称为**贝叶斯更新**。
- en: As we have seen that in our ad optimization case, the reward for each arm is
    either 1 or 0\. We can use **beta distribution** for our prior distribution because
    the value of the beta distribution is from 0 to 1\. The beta distribution is parameterized
    by two parameters, α and β. α represents the number of times we receive the reward
    of 1 and β, indicates the number of times we receive the reward of 0.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在广告优化案例中看到的，每个臂的奖励要么是1要么是0。我们可以使用**贝塔分布**作为我们的先验分布，因为贝塔分布的值在0到1之间。贝塔分布由两个参数α和β参数化。α表示我们获得奖励为1的次数，β表示我们获得奖励为0的次数。
- en: To help you understand the beta distribution better, we will start by looking
    at several beta distributions before we implement the TS algorithm.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你更好地理解贝塔分布，我们将首先看几个贝塔分布，然后再实施TS算法。
- en: How to do it...
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做……
- en: 'Let’s explore the beta distribution through the following steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤来探索贝塔分布：
- en: 'Import PyTorch and matplotlib because we will visualize the shape of the distributions:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch和matplotlib因为我们将可视化分布的形状：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We start by visualizing the shape of the beta distribution with the starting
    positions, α=1 and β=1:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先通过起始位置α=1和β=1来可视化贝塔分布的形状：
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You will see the following plot:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下的绘图：
- en: '![](img/0960d6c4-036e-4345-a1c1-2851ef650ac6.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0960d6c4-036e-4345-a1c1-2851ef650ac6.png)'
- en: Obviously, when α=1 and β=1, it doesn't provide any information about where
    the true value lies in the range of 0 to 1\. Hence, it becomes a uniform distribution.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，当α=1且β=1时，它不提供有关真实值在0到1范围内位置的任何信息。因此，它成为均匀分布。
- en: 'We then visualize the shape of the beta distribution with α=5 and β=1:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随后用α=5和β=1来可视化贝塔分布的形状：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You will see the following plot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下的绘图：
- en: '![](img/bc882317-7591-47f4-a7d9-aec44de5bd29.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc882317-7591-47f4-a7d9-aec44de5bd29.png)'
- en: When α=5 and β=1, this means that there are 4 consecutive rewards of 1 in 4
    experiments. The distribution shifts toward 1.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当α=5且β=1时，这意味着在4次实验中有4次连续的奖励为1。分布向1偏移。
- en: 'Now, let''s experiment with α=1 and β=5:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们实验α=1和β=5：
- en: '[PRE39]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You will see the following plot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下的绘图：
- en: '![](img/dd081d9e-e4bb-4eeb-9605-46f95b53f71c.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd081d9e-e4bb-4eeb-9605-46f95b53f71c.png)'
- en: When α=1 and β=5, this means that there are 4 consecutive rewards of 0 in 4
    experiments. The distribution shifts toward 0.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当α=1且β=5时，这意味着在4次实验中有4次连续的奖励为0。分布向0偏移。
- en: 'Finally, we take a look at the situation when α=5 and β=5:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们看看当α=5且β=5时的情况：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You will see the following plot:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下的绘图：
- en: '![](img/fa2fe6f2-4782-4c9a-aab7-d5a26db84f4f.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa2fe6f2-4782-4c9a-aab7-d5a26db84f4f.png)'
- en: When α=5 and β=5, we observe the same numbers of clicks and no-clicks in 8 rounds.
    The distribution shifts toward the middle point, **0.5**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当α=5且β=5时，在8轮中观察到相同数量的点击和未点击。分布向中间点**0.5**偏移。
- en: 'Now it is time to solve the multi-armed bandit advertising problem using the
    Thompson sampling algorithm:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用汤普森采样算法来解决多臂老虎机广告问题了：
- en: 'Import the bandit environment we developed in the first recipe, *Creating a
    multi-armed bandit environment* (assuming the `BanditEnv` class is in a file called
    `multi_armed_bandit.py`):'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们在第一个示例中开发的老虎机环境，*创建多臂老虎机环境*（假设`BanditEnv`类在名为`multi_armed_bandit.py`的文件中）：
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the payout probabilities and rewards for the three-armed bandit (three
    ad candidates) and create an instance of the bandit environment:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三臂老虎机（三个广告候选项）的支付概率和奖励，并创建一个老虎机环境的实例：
- en: '[PRE42]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms, the number of times individual
    arms are chosen, and the average reward over time for each arm:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要运行的剧集数，并定义包含通过选择各个臂累积的总奖励、选择各个臂的次数以及每个臂的平均奖励随时间变化的列表：
- en: '[PRE43]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the TS function, which samples a value from the beta distribution of
    each arm and selects the arm with the highest value:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义TS函数，从每个臂的贝塔分布中抽样一个值，并选择具有最高值的臂：
- en: '[PRE44]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Initialize α and β for each arm:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个臂初始化α和β：
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that each beta distribution should start with α=β=1.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每个贝塔分布的起始值应为α=β=1。
- en: 'Now, we run 100,000 episodes with the TS algorithm. For each episode, we also
    update α and β of each arm based on the observed reward:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用 TS 算法运行了 100,000 个剧集。对于每个剧集，我们还根据观察到的奖励更新每个臂的 α 和 β：
- en: '[PRE46]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 100,000 个剧集后，我们绘制了随时间变化的平均奖励结果：
- en: '[PRE47]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How it works...
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we solved the ad bandits problem with the TS algorithm. The
    biggest difference between TS and the three other approaches is the adoption of
    Bayesian optimization. It first computes the prior distribution for each possible
    arm, and then randomly draws a value from each distribution. It then picks the
    arm with the highest value and uses the observed outcome to update the prior distribution.
    The TS policy is both stochastic and greedy. If an ad is more likely to receive
    clicks, its beta distribution shifts toward 1 and, hence, the value of a random
    sample tends to be closer to 1.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们使用 TS 算法解决了广告赌博机问题。TS 与另外三种方法的最大区别在于采用贝叶斯优化。它首先计算每个可能臂的先验分布，然后从每个分布中随机抽取一个值。然后选择具有最高值的臂，并使用观察到的结果更新先验分布。TS
    策略既是随机的又是贪婪的。如果某个广告更有可能获得点击，则其贝塔分布向 1 移动，因此随机样本的值趋向于更接近 1。
- en: 'After running the lines of code in *Step 7*, you will see the following plot:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 运行*步骤 7* 中的代码行后，您将看到以下图表：
- en: '![](img/d9200fdd-cbcb-43cf-967e-ff9f581fb1e0.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9200fdd-cbcb-43cf-967e-ff9f581fb1e0.png)'
- en: Ad 2 is the best ad, with the highest predicted CTR (average reward).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 广告 2 是最佳广告，预测的点击率（平均奖励）最高。
- en: See also
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'For those who want to brush up on the beta distribution, feel free to check
    out the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望了解贝塔分布的人，可以随时查看以下链接：
- en: '[https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm)'
- en: '[http://varianceexplained.org/statistics/beta_distribution_and_baseball/](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://varianceexplained.org/statistics/beta_distribution_and_baseball/](http://varianceexplained.org/statistics/beta_distribution_and_baseball/)'
- en: Solving internet advertising problems with contextual bandits
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决互联网广告问题的上下文赌博机
- en: You may notice that in the ad optimization problem, we only care about the ad
    and ignore other information, such as user information and web page information,
    that might affect the ad being clicked on or not. In this recipe, we will talk
    about how we take more information into account beyond the ad itself and solve
    the problem with contextual bandits.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，在广告优化问题中，我们只关心广告本身，而忽略可能影响广告是否被点击的其他信息，例如用户信息和网页信息。在本文中，我们将讨论如何考虑超出广告本身的更多信息，并使用上下文赌博机解决这个问题。
- en: The multi-armed bandit problems we have worked with so far do not involve the
    concept of state, which is very different from MDPs. We only have several actions,
    and a reward will be generated that is associated with the action selected. **Contextual
    bandits** extend multi-armed bandits by introducing the concept of state. State
    provides a description of the environment, which helps the agent take more informed
    actions. In the advertising example, the state could be the user's gender (two
    states, male and female), the user’s age group (four states, for example), or
    page category (such as sports, finance, or news). Intuitively, users of certain
    demographics are more likely to click on an ad on certain pages.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理过的多臂赌博机问题不涉及状态的概念，这与 MDPs 非常不同。我们只有几个动作，并且会生成与所选动作相关联的奖励。**上下文赌博机**通过引入状态的概念扩展了多臂赌博机。状态提供了环境的描述，帮助代理人采取更加明智的行动。在广告示例中，状态可以是用户的性别（两个状态，男性和女性）、用户的年龄组（例如四个状态）或页面类别（例如体育、财务或新闻）。直观地说，特定人口统计学的用户更有可能在某些页面上点击广告。
- en: It is not difficult to understand contextual bandits. A multi-armed bandit is
    a single machine with multiple arms, while contextual bandits are a set of such
    machines (bandits). Each machine in contextual bandits is a state that has multiple
    arms. The learning goal is to find the best arm (action) for each machine (state).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 理解上下文赌博机并不难。一个多臂赌博机是一台具有多个臂的单机，而上下文赌博机是一组这样的机器（赌博机）。上下文赌博机中的每台机器是一个具有多个臂的状态。学习的目标是找到每台机器（状态）的最佳臂（动作）。
- en: We will work with an advertising example with two states for simplicity.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以两个状态的广告示例为例。
- en: How to do it...
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We solve the contextual bandits advertising problem using the UCB algorithm
    as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用UCB算法解决上下文老虎机广告问题如下：
- en: 'Import PyTorch and the bandit environment we developed in the first recipe,
    *Creating a multi-armed bandit environment* (assuming the `BanditEnv` class is
    in a file called `multi_armed_bandit.py`):'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch和我们在第一个示例中开发的老虎机环境，*创建一个多臂老虎机环境*（假设`BanditEnv`类在名为`multi_armed_bandit.py`的文件中）：
- en: '[PRE48]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Define the payout probabilities and rewards for the two three-armed bandits:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义两个三臂老虎机的支付概率和奖励：
- en: '[PRE49]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Here, the true CTR of ad 0 is 1%, of ad 1 is 1.5%, and of ad 2 is 3% for the
    first state, and [2.5%, 1%, and 1.5%] for the second state.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，广告0的真实CTR为1%，广告1为1.5%，广告2为3%适用于第一个状态，以及第二个状态的[2.5%，1%，1.5%]。
- en: 'The number of slot machines in our case is two:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的情况下有两台老虎机：
- en: '[PRE50]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create a list of bandits given the corresponding payout information:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 根据相应的支付信息创建一个老虎机列表：
- en: '[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We specify the number of episodes to run and define the lists holding the total
    rewards accumulated by choosing individual arms in each state, the number of times
    individual arms are chosen in each state, and the average reward over time for
    each arm in each state:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定要运行的剧集数，并定义包含在每个状态下选择各个臂时累计的总奖励、每个状态下选择各个臂的次数以及每个状态下各个臂随时间的平均奖励的列表：
- en: '[PRE52]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Define the UCB policy function, which computes the best arm based on the UCB
    formula:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义UCB策略函数，根据UCB公式计算最佳臂：
- en: '[PRE53]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Initialize the Q function, which is the average reward obtained with individual
    arms for individual states:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化Q函数，这是在各个状态下使用各个臂获得的平均奖励：
- en: '[PRE54]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We will update the Q-function over time.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随时间更新Q函数。
- en: 'Now, we run 100,000 episodes with the UCB policy. For each episode, we also
    update the statistics of each arm in each state:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用UCB策略运行100,000个剧集。对于每个剧集，我们还更新每个状态下每个臂的统计数据：
- en: '[PRE55]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'After running 100,000 episodes, we plot the results of the average reward over
    time for each state:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行100,000个剧集后，我们绘制每个状态随时间变化的平均奖励结果：
- en: '[PRE56]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How it works...
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: In this recipe, we solved the contextual advertising problem with contextual
    bandits using the UCB algorithm.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用UCB算法解决了上下文广告问题的上下文老虎机问题。
- en: Running the lines of code in *Step 7*, you will see the following plot.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 运行*步骤7*中的代码行，您将看到以下绘图。
- en: 'We get this for the first state:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了第一个状态的结果：
- en: '![](img/a9a9590f-38ca-4319-819f-58b8715f2fb4.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9a9590f-38ca-4319-819f-58b8715f2fb4.png)'
- en: 'And we get this for the second state:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了第二个状态的结果：
- en: '![](img/60b4910d-b5cf-41c8-be9e-1ef25c9c1e54.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60b4910d-b5cf-41c8-be9e-1ef25c9c1e54.png)'
- en: Given the first state, ad 2 is the best ad, with the highest predicted CTR.
    Given the second state, ad 0 is the optimal ad, with the highest average reward.
    And these are both true.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 给定第一个状态，广告2是最佳广告，具有最高的预测点击率。给定第二个状态，广告0是最佳广告，具有最高的平均奖励。这两者都是真实的。
- en: Contextual bandits are a set of multi-armed bandits. Each bandit represents
    a unique state of the environment. The state provides a description of the environment,
    which helps the agent take more informed actions. In our advertising example,
    male users might be more likely to click an ad than female users. We simply used
    two slot machines to incorporate two states and searched for the best arm to pull
    given each state.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文老虎机是一组多臂老虎机。每个老虎机代表环境的唯一状态。状态提供了环境的描述，帮助代理者采取更明智的行动。在我们的广告示例中，男性用户可能比女性用户更有可能点击广告。我们简单地使用了两台老虎机来包含两种状态，并在每种状态下寻找最佳的拉杆臂。
- en: One thing to note is that contextual bandits are still different from MDPs,
    although they involve the concept of state. First, the states in contextual bandits
    are not determined by the previous actions or states, but are simply observations
    of the environment. Second, there is no delayed or discounted reward in contextual
    bandits because a bandit episode is one step. However, compared to multi-armed
    bandits, contextual bandits are closer to MDP as the actions are conditional to
    the states in the environment. It is safe to say that contextual bandits are in
    between multi-armed bandits and full MDP reinforcement learning.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管上下文老虎机涉及状态的概念，但它们仍然与MDP有所不同。首先，上下文老虎机中的状态不是由先前的动作或状态决定的，而只是环境的观察。其次，上下文老虎机中没有延迟或折现奖励，因为老虎机剧集是一步。然而，与多臂老虎机相比，上下文老虎机更接近MDP，因为动作是环境状态的条件。可以说上下文老虎机介于多臂老虎机和完整MDP强化学习之间是安全的。
