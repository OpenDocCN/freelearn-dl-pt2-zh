- en: Appendix III — Generic Text Completion with GPT-2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 III — 使用 GPT-2 进行通用文本完成
- en: This appendix is the detailed explanation of the *Generic text completion with
    GPT-2* sectionin *Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3
    Engines*. This section describes how to implement a GPT-2 transformer model for
    generic text complexion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这个附录是*第七章*中*GPT-3 引擎与超人类变压器崛起*中*使用 GPT-2 进行通用文本完成*一节的详细解释。这一节描述了如何实现 GPT-2 变压器模型来完成通用文本。
- en: You can read the usage of this notebook directly in *Chapter 7* or build the
    program and run it in this appendix to get more profound knowledge of how a GPT
    model works.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接在*第七章*中阅读这本笔记本的用法，或者在附录中构建程序并运行它，以更深入地了解 GPT 模型的工作原理。
- en: We will clone the `OpenAI_GPT_2` repository, download the 345M-parameter GPT-2
    transformer model, and interact with it. We will enter context sentences and analyze
    the text generated by the transformer. The goal is to see how it creates new content.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将克隆 `OpenAI_GPT_2` 代码库，下载 345M 参数的 GPT-2 变压器模型，并与其交互。我们将输入上下文句子，并分析变压器生成的文本。目标是看看它如何创建新内容。
- en: This section is divided into nine steps. Open `OpenAI_GPT_2.ipynb` in Google
    Colaboratory. The notebook is in the `AppendixIII` directory of the GitHub repository
    of this book. You will notice that the notebook is also divided into the same
    nine steps and cells as this section.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分分为九个步骤。在 Google Colaboratory 中打开 `OpenAI_GPT_2.ipynb`。这个笔记本在本书的 GitHub 代码库的
    `AppendixIII` 目录中。你会注意到笔记本也被分成了和本节相同的九个步骤和单元格。
- en: Run the notebook cell by cell. The process is tedious, but *the result produced
    by the cloned OpenAI GPT-2 repository is gratifying*. We saw that we could run
    a GPT-3 engine in a few lines. But this appendix gives you the opportunity, even
    if the code is not optimized anymore, to see how GPT-2 models work.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步运行笔记本的单元格。这个过程很枯燥，但*克隆 OpenAI GPT-2 代码库产生的结果是令人满意的*。我们看到我们可以用几行代码运行一个 GPT-3
    引擎。但是这个附录给了你机会，即使代码不再被优化，也可以看到 GPT-2 模型是如何工作的。
- en: Hugging Face has a wrapper that encapsulates GPT-2 models. It’s useful as an
    alternative to the OpenAI API. However, the goal in this appendix is *not* to
    avoid the complexity of the underlying components of a GPT-2 model but to explore
    them!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 有一个封装 GPT-2 模型的包装器。作为 OpenAI API 的一个替代方案很有用。然而，在这个附录中的目标*不是*为了避免
    GPT-2 模型底层组件的复杂性，而是为了探索它们！
- en: Finally, It is important to stress that we are running a low-level GPT-2 model
    and not a one-line call to obtain a result. That is why we are avoiding pre-packaged
    versions (the OpenAI GPT-3 API, Hugging Face wrappers, others). We are getting
    our hands dirty to understand the architecture of GPT-2 from scratch. As a result,
    you might get some deprecation messages. However, the effort is worthwhile to
    become an Industry 4.0 AI guru.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是我们正在运行一个低级别的 GPT-2 模型，而不是一个一行代码调用即可获得结果的简易版本（OpenAI GPT-3 API，Hugging
    Face 封装等）。我们正在从零开始理解 GPT-2 的架构，所以可能会收到一些弃用消息。但这样的努力值得成为 4.0 工业人工智能专家。
- en: Let’s begin by activating the GPU.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始激活 GPU。
- en: 'Step 1: Activating the GPU'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 步：激活 GPU
- en: We must activate the GPU to train our GPT-2 345M-parameter transformer model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须激活 GPU 来训练我们的 GPT-2 345M 参数变压器模型。
- en: 'To activate the GPU, go to the **Runtime** menu in **Notebook settings** to
    get the most out of the VM:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要激活 GPU，请进入 **笔记本设置** 中的 **运行时** 菜单以充分利用 VM：
- en: '![](img/B17948_Appendix_III_01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_III_01.png)'
- en: 'Figure III.1: The GPU hardware accelerator'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 III.1：GPU 硬件加速器
- en: We can see that activating the GPU is a prerequisite for better performance
    that will give us access to the world of GPT transformers. So let’s now clone
    the OpenAI GPT-2 repository.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到激活 GPU 是更好性能的先决条件，这将让我们进入 GPT 变压器的世界。所以现在让我们克隆 OpenAI 的 GPT-2 代码库。
- en: 'Step 2: Cloning the OpenAI GPT-2 repository'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步：克隆 OpenAI GPT-2 代码库
- en: OpenAI still lets us download GPT-2 for now. This may be discontinued in the
    future, or maybe we will get access to more resources. At this point, the evolution
    of transformers and their usage moves so fast that nobody can foresee how the
    market will evolve, even the major research labs themselves.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前 OpenAI 仍然允许我们下载 GPT-2。这种方式可能在将来被停止，或者可能我们会获得更多资源。此时，变压器的发展和使用速度如此之快，以至于没人能预见市场会如何发展，即使是主要的研究实验室自己也不行。
- en: 'We will clone OpenAI’s GitHub directory on our VM:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 VM 上克隆 OpenAI 的 GitHub 目录：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When the cloning is over, you should see the repository appear in the file
    manager:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆结束后，您应该在文件管理器中看到该代码库的出现：
- en: '![](img/B17948_Appendix_III_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_III_02.png)'
- en: 'Figure III.2: Cloned GPT-2 repository'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '图 III.2: 克隆的 GPT-2 存储库'
- en: 'Click on **src**, and you will see that the Python files we need from OpenAI
    to run our model are installed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 **src**，您会看到我们从 OpenAI 安装的运行模型所需的 Python 文件：
- en: '![](img/B17948_Appendix_III_03.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_III_03.png)'
- en: 'Figure III.3: The GPT-2 Python files to run a model'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图 III.3: 运行模型的 GPT-2 Python 文件'
- en: You can see that we do not have the Python training files we need. We will install
    them when we train the GPT-2 model in the *Training a GPT-2 language model* section
    of *Appendix IV*, *Custom Text Completion with GPT-2*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您会发现我们没有需要的 Python 训练文件。在 *附录 IV* 的 *用 GPT-2 训练语言模型* 部分 *自定义文本完成与 GPT-2*中训练
    GPT-2 模型时，我们将安装它们。
- en: Let’s now install the requirements.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们安装所需的内容。
- en: 'Step 3: Installing the requirements'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 3: 安装要求'
- en: 'The requirements will be installed automatically:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要自动安装所需的内容：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When running cell by cell, we might have to restart the VM and thus import `os`
    again.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 逐个单元格运行时，我们可能需要重新启动虚拟机，然后再次导入`os`。
- en: 'The requirements for this notebook are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本的要求是：
- en: '`Fire 0.1.3` to generate **command-line interfaces** (**CLIs**)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fire 0.1.3` 用于生成**命令行界面**（**CLIs**）'
- en: '`regex 2017.4.5` for regex usage'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regex 2017.4.5` 用于正则表达式使用'
- en: '`Requests 2.21.0`, an HTTP library'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Requests 2.21.0`，一个 HTTP 库'
- en: '`tqdm 4.31.1` to display a progress meter for loops'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tqdm 4.31.1` 用于显示循环的进度条'
- en: You may be asked to restart the notebook.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会要求您重新启动笔记本。
- en: '*Do not restart it now*. *Let’s wait until we check the version of TensorFlow*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*现在不要重新启动它*。*让我们等到检查 TensorFlow 的版本*。'
- en: 'Step 4: Checking the version of TensorFlow'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 4: 检查 TensorFlow 的版本'
- en: The GPT-2 345M transformer model provided by OpenAI uses TensorFlow 1.x. This
    will lead to several warnings when running the program. However, we will ignore
    them and run at full speed on the thin ice of training GPT models ourselves with
    our modest machines.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供的 GPT-2 345M 变压器模型使用 TensorFlow 1.x。这将在运行程序时导致几个警告。但是，我们将忽略它们，并在我们的普通机器上以全速运行训练
    GPT 模型的薄冰上。
- en: In the 2020s, GPT models have reached 175 billion parameters, making it impossible
    for us to train them ourselves efficiently without having access to a supercomputer.
    The number of parameters will only continue to increase.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2020 年代，GPT 模型已经达到了 1750 亿个参数，使我们在没有超级计算机的情况下无法高效地进行训练。参数的数量只会继续增加。
- en: The corporate giants’ research labs, such as Facebook AI and OpenAI, and Google
    Research/Brain, are speeding toward super-transformers and are leaving what they
    can for us to learn and understand. But, unfortunately, they do not have time
    to go back and update all the models they share. However, we still have this notebook!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 企业巨头的研究实验室，例如 Facebook AI、OpenAI 和 Google Research/Brain，正在加速转向超级变压器，并且正在留下一些供我们学习和理解的东西。但是，不幸的是，他们没有时间回头更新他们分享的所有模型。然而，我们还有这个笔记本！
- en: TensorFlow 2.x is the latest TensorFlow version. However, older programs can
    still be helpful. This is one reason why Google Colaboratory VMs have preinstalled
    versions of both TensorFlow 1.x and TensorFlow 2.x.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.x 是最新的 TensorFlow 版本。然而，旧程序仍然可能有所帮助。这就是为什么 Google Colaboratory
    VMs 预先安装了 TensorFlow 1.x 和 TensorFlow 2.x 的版本的一个原因。
- en: 'We will be using TensorFlow 1.x in this notebook:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在此笔记本中使用 TensorFlow 1.x：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output should be:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Whether the `tf 1.x` version is displayed or not, rerun the cell to make sure,
    and then restart the VM. *Rerun this cell to make sure before continuing*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否显示`tf 1.x`版本，请重新运行单元格以确保，然后重新启动虚拟机。*在继续之前重新运行此单元格以确保*。
- en: If you encounter a TensforFlow error during the process (ignore the warnings),
    rerun this cell, restart the VM, and rerun to make sure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在过程中遇到 TensforFlow 错误（忽略警告），请重新运行此单元格，重新启动虚拟机，确保重新运行。
- en: Do this every time you restart the VM. The default version of the VM is `tf.2`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每次重新启动虚拟机时都要做这个。虚拟机的默认版本是 `tf.2`。
- en: We are now ready to download the GPT-2 model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备下载 GPT-2 模型。
- en: 'Step 5: Downloading the 345M-parameter GPT-2 model'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 5: 下载 345M 参数 GPT-2 模型'
- en: 'We will now download the trained 345M-parameter GPT-2 model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将下载训练好的 345M 参数 GPT-2 模型：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The path to the model directory is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 模型目录的路径是：
- en: '`/content/gpt-2/models/345M`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`/content/gpt-2/models/345M`'
- en: 'It contains the information we need to run the model:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含我们运行模型所需的信息：
- en: '![](img/B17948_Appendix_III_04.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_III_04.png)'
- en: 'Figure III.4: The GPT-2 Python files of the 345M-parameter model'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图 III.4: 345M 参数模型的 GPT-2 Python 文件'
- en: 'The `hparams.json` file contains the definition of the GPT-2 model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`hparams.json` 文件包含了 GPT-2 模型的定义：'
- en: '`"n_vocab"`: `50257`, the size of the vocabulary of the model'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_vocab"`: `50257`，模型词汇表的大小'
- en: '`"n_ctx"`: `1024`, the context size'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_ctx"`: `1024`，上下文大小'
- en: '`"n_embd"`: `1024`, the embedding size'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_embd"`: `1024`，嵌入大小'
- en: '`"n_head"`: `16`, the number of heads'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_head"`: `16`，头的数量'
- en: '`"n_layer"`: `24`, the number of layers'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"n_layer"`: `24`，层数'
- en: '`encoder.json` and `vacab.bpe` contain the tokenized vocabulary and the BPE
    word pairs. If necessary, take a few minutes to go back and read the *Step 3:
    Training a tokenizer* subsection in *Chapter 4*, *Pretraining a RoBERTa Model
    from Scratch*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`encoder.json` 和 `vacab.bpe` 包含了标记化的词汇表和 BPE 单词对。如有必要，请花几分钟时间返回并阅读*第3步: 训练一个分词器*子节，*第4章*，*从零开始预训练
    RoBERTa 模型*。'
- en: 'The `checkpoint` file contains the trained parameters at a checkpoint. For
    example, it could contain the trained parameters for 1,000 steps, as we will do
    in the *Step 9: Training a GPT-2 model* section of *Appendix IV*, *Custom Text
    Completion with GPT-2*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkpoint` 文件包含了检查点时的训练参数。例如，它可能包含了1,000步的训练参数，就像我们在*第9步: 训练 GPT-2 模型*章节的*附录
    IV*中，*使用 GPT-2 进行自定义文本完成*部分将要做的那样。'
- en: 'The `checkpoint` file is saved with three other important files:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`checkpoint` 文件与其他三个重要文件保存在一起:'
- en: '`model.ckpt.meta` describes the graph structure of the model. It contains `GraphDef`,
    `SaverDef`, and so on. We can retrieve the information with `tf.train.import_meta_graph([path]+''model.ckpt.meta'')`.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.ckpt.meta` 描述了模型的图结构。它包含`GraphDef`，`SaverDef`等。我们可以使用`tf.train.import_meta_graph([path]+''model.ckpt.meta'')`检索信息。'
- en: '`model.ckpt.index` is a string table. The keys contain the name of a tensor,
    and the value is `BundleEntryProto`, which contains the metadata of a tensor.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.ckpt.index` 是一个字符串表。键包含张量的名称，值是`BundleEntryProto`，其中包含张量的元数据。'
- en: '`model.ckpt.data` contains the values of all the variables in a *TensorBundle
    collection*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.ckpt.data` 包含*TensorBundle collection*中所有变量的值。'
- en: We have downloaded our model. We will now go through some intermediate steps
    before activating the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经下载了我们的模型。现在我们将在激活模型之前经历一些中间步骤。
- en: 'Steps 6-7: Intermediate instructions'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 6-7: 中间指令'
- en: In this section, we will go through *Steps 6*, *7*, and *7a*, which are intermediate
    steps leading to *Step 8*, in which we will define and activate the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将经历*步骤 6*、*7*和*7a*，这些是通向*步骤 8*的中间步骤，其中我们将定义和激活模型。
- en: 'We want to print UTF-encoded text to the console when we are interacting with
    the model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '在与模型交互时，我们希望将 UTF 编码的文本打印到控制台:'
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We want to make sure we are in the `src` directory:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要确保我们在`src`目录下：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are ready to interact with the GPT-2 model. We could run it directly with
    a command, as we will do in the *Training a GPT-2 language model* section of *Appendix
    IV*, *Custom Text Completion with GPT-2*. However, in this section, we will go
    through the main aspects of the code.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好与 GPT-2 模型交互。我们可以直接运行它，就像我们在*附录 IV*中的*使用 GPT-2 进行语言模型训练*部分将要做的那样。然而，在本节中，我们将讨论代码的主要方面。
- en: '`interactive_conditional_samples.py` first imports the necessary modules required
    to interact with the model:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`interactive_conditional_samples.py` 首先导入与模型交互所需的必要模块:'
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have gone through the intermediate steps leading to the activation of the
    model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经经历了激活模型的中间步骤。
- en: 'Steps 7b-8: Importing and defining the model'
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '步骤 7b-8: 导入并定义模型'
- en: We will now activate the interaction with the model with `interactive_conditional_samples.py`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用`interactive_conditional_samples.py`激活与模型的交互。
- en: 'We need to import three modules that are also in `/content/gpt-2/src`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '我们需要导入三个同时也在 `/content/gpt-2/src`中的模块:'
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The three programs are:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '这三个程序是:'
- en: '`model.py` defines the model’s structure: the hyperparameters, the multi-attention
    `tf.matmul` operations, the activation functions, and all the other properties.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.py` 定义了模型的结构: 超参数，多头`tf.matmul`操作，激活函数以及所有其他属性。'
- en: '`sample.py` processes the interaction and controls the sample that will be
    generated. It makes sure that the tokens are more meaningful.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample.py` 处理交互并控制将生成的样本。它确保标记更有意义。'
- en: Softmax values can sometimes be blurry, like looking at an image in low definition.
    `sample.py` contains a variable named `temperature` that will make the values
    sharper, increasing the higher probabilities and softening the lower ones.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Softmax 值有时可能模糊不清，就像在低清晰度下查看图像。`sample.py` 包含一个名为`temperature`的变量，将使值更清晰，增加更高的概率并软化更低的概率。
- en: '`sample.py` can activate Top-*k* sampling. Top-*k* sampling sorts the probability
    distribution of a predicted sequence. The higher probability values of the head
    of the distribution are filtered up to the k-*th* token. The tail containing the
    lower probabilities is excluded, preventing the model from predicting low-quality
    tokens.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sample.py` 可以激活Top-*k*采样。Top-*k*采样对预测序列的概率分布进行排序。分布的头部具有较高的概率值，排除掉尾部具有较低概率的部分，以防止模型预测低质量的标记。'
- en: '`sample.py` can also activate Top-*p* sampling for language modeling. Top-*p*
    sampling does not sort the probability distribution. Instead, it selects the words
    with high probabilities until the sum of this subset’s probabilities or the nucleus
    of a possible sequence exceeds *p*.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`sample.py` 也可以激活用于语言建模的Top-*p*采样。Top-*p*采样不对概率分布进行排序。相反，它选择具有较高概率的词，直到此子集的概率之和或可能序列的核心超过
    *p*。'
- en: '`encoder.py` encodes the sample sequence with the defined model, `encoder.json`,
    and `vocab.bpe`. It contains both a BPE encoder and a text decoder.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder.py` 使用定义好的模型 `encoder.json` 和 `vocab.bpe` 对样本序列进行编码。它既包含了一个BPE编码器，又包含了文本解码器。'
- en: You can open these programs to explore them further by double-clicking on them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以双击打开这些程序来进一步探索它们。
- en: '`interactive_conditional_samples.py` will call the functions required to interact
    with the model to initialize the following information: the hyperparameters that
    define the model from `model.py`, and the sample sequence parameters from `sample.py`.
    It will encode and decode sequences with `encode.py`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`interactive_conditional_samples.py` 将调用所需的函数与模型进行交互，以初始化以下信息：来自 `model.py`
    定义模型的超参数，以及来自 `sample.py` 的样本序列参数。它将使用 `encode.py` 进行编码和解码序列。'
- en: '`interactive_conditional_samples.py` will then restore the checkpoint data
    defined in the *Step 5: Downloading the 345M-parameter GPT-2 model* subsection
    of this section.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`interactive_conditional_samples.py` 将恢复本节的 *第5步：下载345M参数GPT-2模型* 子部分中定义的检查点数据。'
- en: 'You can explore `interactive_conditional_samples.py` by double-clicking on
    it and experiment with its parameters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以双击打开`interactive_conditional_samples.py`并尝试调整其参数：
- en: '`model_name` is the model name, such as `"124M"` or `"345M,"` and relies on
    `models_dir`.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name` 是模型名称，如 `"124M"` 或 `"345M,"`，依赖于 `models_dir`。'
- en: '`models_dir` defines the directory containing the models.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models_dir` 定义包含模型的目录。'
- en: '`seed` sets a random integer for random generators. The seed can be set to
    reproduce results.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` 为随机生成器设置一个随机整数。可以设置种子以重现结果。'
- en: '`nsamples` is the number of samples to return. If it is set to `0`, it will
    continue to generate samples until you double-click on the *run* button of the
    cell or press *Ctrl* + *M*.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nsamples` 是要返回的样本数。如果设置为 `0`，它将继续生成样本，直到你双击单元格的 *run* 按钮或按下 *Ctrl* + *M*。'
- en: '`batch_size` determines the size of a batch and has an impact on memory and
    speed.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` 决定了批处理的大小，对内存和速度有影响。'
- en: '`length` is the number of tokens of generated text. If set to `none`, it relies
    on the hyperparameters of the model.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length` 是生成文本的标记数。如果设置为 `none`，则依赖于模型的超参数。'
- en: '`temperature` determines the level of Boltzmann distributions. If the temperature
    is high, the completions will be more random. If the temperature is low, the results
    will become more deterministic.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature` 决定了Boltzmann分布的级别。如果温度很高，完成结果将更加随机。如果温度很低，结果将变得更加确定。'
- en: '`top_k` controls the number of tokens taken into consideration by Top-*k* at
    each step. `0` means no restrictions. `40` is the recommended value.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k` 控制Top-*k*在每一步考虑的标记数。 `0`表示没有限制。推荐值为 `40`。'
- en: '`top_p` controls Top-*p*.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p` 控制Top-*p*。'
- en: 'For the program in this section, the scenario of the parameters we just explored
    will be:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节中的程序，我们刚刚探索的参数场景将是：
- en: '`model_name = "345M"`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model_name = "345M"`'
- en: '`seed = None`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed = None`'
- en: '`nsamples = 1`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nsamples = 1`'
- en: '`batch_size = 1`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size = 1`'
- en: '`length = 300`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`length = 300`'
- en: '`temperature = 1`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature = 1`'
- en: '`top_k = 0`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k = 0`'
- en: '`models_dir = ''/content/gpt-2/models''`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models_dir = ''/content/gpt-2/models''`'
- en: These parameters will influence the model’s behavior, the way it is conditioned
    by the context input, and generate text completion sequences. First, run the notebook
    with the default values. You can then change the code’s parameters by double-clicking
    on the program, editing it, and saving it. The changes will be deleted at each
    restart of the VM. Save the program and reload it if you wish to create interaction
    scenarios.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数将影响模型的行为，它如何受到上下文输入的条件影响，并生成文本完成序列。首先使用默认值运行笔记本。然后，您可以通过双击程序，编辑它并保存它来更改代码的参数。在每次重新启动
    VM 时将删除更改。如果您希望创建交互场景，请保存程序并重新加载它。
- en: The program is now ready to prompt us to interact with it.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在已准备好提示我们与其交互。
- en: 'Step 9: Interacting with GPT-2'
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 9：与 GPT-2 交互
- en: In this section, we will interact with the GPT-2 345M model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将与 GPT-2 345M 模型交互。
- en: There will be more messages when the system runs, but as long as Google Colaboratory
    maintains `tf 1.x`, we will run the model with this notebook. One day, we might
    have to use GPT-3 engines if this notebook becomes obsolete, or we will have to
    use Hugging Face GPT-2 wrappers, for example, which might be deprecated as well
    in the future.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 系统运行时会有更多消息，但只要 Google Colaboratory 保持 `tf 1.x`，我们就会使用此笔记本运行模型。如果这本笔记本过时了，我们可能会有一天不得不使用
    GPT-3 引擎，或者例如使用 Hugging Face GPT-2 包装器，未来它可能也会被弃用。
- en: In the meantime, GPT-2 is still in use so let’s interact with the model!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，GPT-2 仍在使用，所以让我们与模型交互吧！
- en: 'To interact with the model, run the `interact_model` cell:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要与模型交互，请运行 `interact_model` 单元格：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You will be prompted to enter some context:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将提示您输入一些上下文：
- en: '![](img/B17948_Appendix_III_05.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_III_05.png)'
- en: 'Figure III.5: Context input for text completion'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 III.5：用于文本完成的上下文输入
- en: You can try any type of context you wish since this is a standard GPT-2 model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个标准的 GPT-2 模型，您可以尝试任何类型的上下文。
- en: 'We can try a sentence written by Emmanuel Kant:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试以艾曼纽尔·康德（Emmanuel Kant）的句子开头：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Press *Enter* to generate text. The output will be relatively random since the
    GPT-2 model was not trained on our dataset, and we are running a stochastic model
    anyway.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 按下 *Enter* 键生成文本。由于 GPT-2 模型没有在我们的数据集上训练，而且我们正在运行一个随机模型，输出将相对随机。
- en: 'Let’s have a look at the first few lines the GPT model generated at the time
    I ran it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我运行模型时生成的前几行内容：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To stop the cell, double-click on the run button of the cell.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止单元格，请双击单元格的运行按钮。
- en: You can also press *Ctrl* + *M* to stop generating text, but it may transform
    the code into text, and you will have to copy it back into a program cell.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以按 *Ctrl* + *M* 停止生成文本，但它可能会将代码转换为文本，您将不得不将其复制回程序单元格。
- en: 'The output is rich. We can observe several facts:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出内容丰富。我们可以观察到几个事实：
- en: The context we entered *conditioned* the output generated by the model.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们输入的上下文 *条件化* 了模型生成的输出。
- en: The context was a demonstration of the model. It learned what to say from the
    model without modifying its parameters.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文是模型的演示。它从模型中学到了要说的话，而没有修改其参数。
- en: Text completion is conditioned by context. This opens the door to transformer
    models that do not require fine-tuning.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本完成受上下文影响。这为不需要微调的转换器模型打开了大门。
- en: From a semantic perspective, the output could be more interesting.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语义的角度来看，输出可能更有趣。
- en: From a grammatical perspective, the output is convincing.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语法的角度来看，输出是令人信服的。
- en: You can see if we could obtain more impressive results by training the model
    on a customized dataset in *Appendix IV*, *Custom Text Completion with GPT-2*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过阅读 *附录 IV*，*使用 GPT-2 进行自定义文本完成*，看看我们是否可以通过在自定义数据集上训练模型获得更令人印象深刻的结果。
- en: References
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '*OpenAI GPT-2* GitHub repository: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*OpenAI GPT-2* GitHub 仓库：[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
- en: '*N Shepperd’s* GitHub repository: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N Shepperd* 的 GitHub 仓库：[https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)'
- en: Join our book’s Discord space
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，与作者进行每月的 *问我任何* 会话：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
