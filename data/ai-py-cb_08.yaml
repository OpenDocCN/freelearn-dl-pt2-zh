- en: Working with Moving Images
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 处理动态图像
- en: This chapter deals with video applications. While methods applied to images
    can be applied to single frames of videos, this usually comes with a loss of temporal
    consistency. We will try to strike a balance between what's possible on consumer
    hardware and what's interesting enough to show and implement.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及视频应用。虽然应用于图像的方法可以应用于视频的单帧，但通常会导致时间上的一致性损失。我们将尝试在消费者硬件上找到可能性和趣味性之间的平衡，并展示和实施。
- en: Quite a few applications should come to mind when talking about video, such
    as object tracking, event detection (surveillance), deep fake, 3D scene reconstruction,
    and navigation (self-driving cars).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 谈论视频时应该考虑到很多应用程序，例如目标跟踪、事件检测（监视）、深度伪造、3D场景重建和导航（自动驾驶汽车）。
- en: A lot of them require many hours or days of computation. We'll try to strike
    a sensible compromise between what's possible and what's interesting. This compromise
    might be felt more than in other chapters, where computations are not as demanding
    as for video. As part of this compromise, we'll work on videos frame by frame,
    rather than across the temporal domain. Still, as always, we'll try to work on
    problems by giving examples that are either representative of practical real-world
    applications, or that are at least similar.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 很多伪造视频需要耗费数小时或数天的计算时间。我们将尝试在可能性和趣味性之间找到一个合理的平衡。这种平衡可能比其他章节更为明显，因为视频的计算不像单独的图像计算那样耗费资源。作为这种平衡的一部分，我们将逐帧处理视频，而不是跨时间域处理。尽管如此，我们仍将通过提供实际的真实应用示例或至少类似的示例来解决问题。
- en: In this chapter, we'll start with image detection, where an algorithm applies
    an image recognition model to different parts of an image in order to localize
    objects. We'll then give examples of how to apply this to a video feed. We'll
    then create videos using a deep fake model, and reference more related models
    for both creating and detecting deep fakes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从图像检测开始，其中算法将图像识别模型应用于图像的不同部分以定位对象。然后我们将展示如何将其应用到视频流中。然后我们将使用深度伪造模型创建视频，并参考更多相关的模型，用于创建和检测深度伪造。
- en: 'In this chapter, we''ll look at the following recipes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到以下示例：
- en: Localizing objects
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定位对象
- en: Faking videos
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频伪造
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We'll use many standard libraries, including `keras` and `opencv`, but we'll
    see a few more libraries that we'll mention at the beginning of each recipe before
    they'll become relevant.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用许多标准库，包括`keras`和`opencv`，但在每个示例开始之前我们会提到更多的库。
- en: You can find the notebooks for this chapter's recipes on GitHub at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter08](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter08).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本章示例的笔记本，链接为[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter08](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter08)。
- en: Localizing objects
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定位对象
- en: Object detection refers to identifying objects of particular classes in images
    and videos. For example, in self-driving cars, pedestrians and trees have to be
    identified in order to be avoided.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对象检测是指在图像和视频中识别特定类别的对象。例如，在自动驾驶汽车中，必须识别行人和树木以避让。
- en: In this recipe, we'll implement an object detection algorithm in Keras. We'll
    apply it to a single image and then to our laptop camera. In the *How it works...*
    section, we'll discuss the theory and more algorithms for object detection.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将在Keras中实现一个对象检测算法。我们将首先将其应用于单个图像，然后应用于我们的笔记本摄像头。在*工作原理...*部分，我们将讨论理论和更多关于对象检测的算法。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we''ll need the Python bindings for the **Open Computer Vision
    Library** (**OpenCV**) and `scikit-image`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将需要**开放计算机视觉库**（**OpenCV**）和`scikit-image`的Python绑定：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As our example image, we''ll download an image from an object detection toolbox:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的示例图像，我们将从一个对象检测工具箱中下载一张图像：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Please note that any other image will do.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，任何其他图像都可以。
- en: 'We''ll use a code based on the `keras-yolo3` library, which was quick to set
    up with only a few changes. We can quickly download this as well:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用基于`keras-yolo3`库的代码，只需进行少量更改即可快速设置。我们也可以快速下载这个：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, we also need the weights for the `YOLOv3` network, which we can download
    from the darknet open source implementation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还需要`YOLOv3`网络的权重，可以从darknet开源实现中下载：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You should now have the example image, the `yolo3-keras` Python script, and
    the `YOLOv3` network weights in your local directory from which you run your notebook.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该在本地目录中拥有示例图像、`yolo3-keras` Python 脚本以及 `YOLOv3` 网络权重，从中运行你的笔记本。
- en: How to do it...
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现它……
- en: In this section, we'll implement an object detection algorithm with Keras.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Keras 实现一个物体检测算法。
- en: 'We''ll import the `keras-yolo3` library, load the pretrained weights, and then
    perform object detection given images or the video feed from a camera:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将导入 `keras-yolo3` 库，加载预训练的权重，然后对给定的图像或摄像头视频流进行物体检测：
- en: 'Since we have most of the object detection implemented in the `keras-yolo3`
    script, we only need to import it:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们在 `keras-yolo3` 脚本中已实现了大部分物体检测功能，所以我们只需要导入它：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can then load our network with the pretrained weights as follows. Please
    note that the weight files are quite big – they''ll occupy around 237 MB of disk
    space:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以按以下方式加载带有预训练权重的网络。请注意，权重文件相当大 - 它们将占用大约 237 MB 的磁盘空间：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our model is now available as a Keras model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在可作为 Keras 模型使用。
- en: 'We can then perform the object detection on our example image:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以对我们的示例图像执行物体检测：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We should see our example image annotated with labels for each bounding box,
    as can be seen in the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到我们的示例图像标注了每个边界框的标签，如下截图所示：
- en: '![](img/e4a49bb5-e260-4989-a90d-c486244c41ff.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4a49bb5-e260-4989-a90d-c486244c41ff.png)'
- en: We can extend this for videos using the `OpenCV` library. We can capture images
    frame by frame from a camera attached to our computer, run the object detection,
    and show the annotated image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `OpenCV` 库扩展此功能以处理视频。我们可以逐帧捕获连接到计算机的摄像头的图像，运行物体检测，并显示带标注的图像。
- en: Please note that this implementation is not optimized and might run relatively
    slowly. For faster implementations, please refer to the darknet implementation
    linked in the *See also* section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此实现未经优化，可能运行比较慢。要获取更快的实现，请参考*参考资料*部分中链接的 darknet 实现。
- en: 'When you run the following code, please know that you can stop the camera by
    pressing `q`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行以下代码时，请注意你可以按 `q` 键停止摄像头：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We capture our image as grayscale, but then have to convert it back to RGB using
    `scikit-image` by stacking the image. Then we detect objects and show the annotated
    frame.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以灰度方式捕获图像，但随后必须使用 `scikit-image` 将其转换回 RGB，通过堆叠图像来检测对象并显示带标注的帧。
- en: 'This is the image we obtained:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们获得的图像：
- en: '![](img/8d5e5dfc-1b88-4d31-a473-cbdc09e73ae7.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d5e5dfc-1b88-4d31-a473-cbdc09e73ae7.png)'
- en: In the next section, we'll discuss this recipe with some background explanations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论这个配方及其背景解释。
- en: How it works...
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We've implemented an object detection algorithm with Keras. This came out of
    the box with a standard library, but we connected it to a camera and applied it
    to an example image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用 Keras 实现了一个物体检测算法。这是一个标准库的开箱即用功能，但我们将其连接到摄像头，并应用到了一个示例图像上。
- en: 'The main algorithms in terms of image detection are the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图像检测的主要算法如下：
- en: Fast R-CNN (Ross Girshick, 2015)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速 R-CNN (Ross Girshick, 2015)
- en: '**Single Shot MultiBox Detector** (**SSD**); Liu and others, 2015: [https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325))'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单发多框检测器** (**SSD**); Liu 等人，2015: [https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325))'
- en: '**You Only Look Once** (**YOLO**); Joseph Redmon and others*, *2016: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640))'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**你只需看一次** (**YOLO**); Joseph Redmon 等人，2016: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640))'
- en: YOLOv4 (Alexey Bochkovskiy and others, 2020: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934))
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv4 (Alexey Bochkovskiy 等人，2020: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934))
- en: One of the main requirements of object detection is speed – you don't want to
    wait to hit the tree before recognizing it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测的主要需求之一是速度 – 你不希望在识别前等待撞上树。
- en: Image detection is based on image recognition with the added complexity of searching
    through the image for candidate locations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图像检测是基于图像识别的基础上，增加了在图像中搜索候选位置的复杂性。
- en: Fast R-CNN is an improvement over R-CNN by the same author (2014). Each region
    of interest, a rectangular image patch defined by a bounding box, is scale normalized
    by image pyramids. The convolutional network can then process these object proposals
    (from a few thousand to as many as many thousands) through a single forward pass
    of a convolutional neural network. As an implementation detail, Fast R-CNN compresses
    fully connected layers with singular value decomposition for speed.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN 是 R-CNN 的改进（2014 年同一作者）。每个感兴趣区域，即由边界框定义的矩形图像块，通过图像金字塔进行尺度归一化。卷积网络然后可以通过一次前向传递处理这些对象提议（从几千到成千上万）。作为实现细节，Fast
    R-CNN 使用奇异值分解压缩完全连接层以提高速度。
- en: YOLO is a single network that proposed bounding boxes and classes directly from
    images in a single evaluation. It was much faster than other detection methods
    at the time; in their experiments, the author ran different versions of YOLO at
    45 frames per second and 155 frames per second.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 是一个单一网络，直接从图像中提出边界框和类别。在其实验中，作者以每秒 45 帧和 155 帧的速度运行了不同版本的 YOLO。
- en: The SSD is a single-stage model that does away with the need for a separate
    object proposal generation, instead of opting for a discrete set of bounding boxes
    that are passed through a network. Predictions are then combined across different
    resolutions and bounding box locations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SSD 是一种单阶段模型，摒弃了独立对象提议生成的需要，而选择通过网络传递一组离散的边界框。然后在不同分辨率和边界框位置上组合预测结果。
- en: 'As a side note, Joseph Redmon published and maintained several incremental
    improvements of his YOLO architecture, but he has since left academia. The latest
    instantiation of the YOLO series by Bochkovskiy and others is in the same spirit,
    however, and is endorsed on Redmon''s GitHub repository: [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，Joseph Redmon 发表并维护了他的 YOLO 架构的多个增量改进，但他已经离开了学术界。 YOLO 系列的最新实现由 Bochkovskiy
    等人在相同的精神中进行，也在 Redmon 的 GitHub 仓库上得到了认可：[https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)。
- en: YOLOv4 introduces several new network features to their CNN and they exhibit
    fast processing speed, while maintaining a level of accuracy significantly superior
    to YOLOv3 (43.5% **average precision** (**AP**), for the MS COCO dataset at a
    real-time speed of about 65 frames per seconds on a Tesla V100 GPU).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv4 在其 CNN 中引入了几种新的网络特性，展示了快速的处理速度，同时保持了显著优于 YOLOv3 的精度水平（43.5% **平均精度**
    (**AP**)，在 Tesla V100 GPU 上实时速度约为每秒 65 帧，针对 MS COCO 数据集）。
- en: There's more...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: There are different ways of interacting with a web camera, and there are even
    some mobile apps that allow you to stream your camera feed, meaning you can plug
    it into applications that run on the cloud (for example, Colab notebooks) or on
    a server.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与网络摄像头交互的方式有多种，并且甚至有一些移动应用程序允许您流式传输摄像头内容，这意味着您可以将其插入在云上运行的应用程序中（例如 Colab 笔记本）或服务器上。
- en: 'One of the most common libraries is `matplotlib`, and it is also possible to
    live update a matplotlib figure from the web camera, as shown in the following
    code block:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的库之一是`matplotlib`，也可以从网络摄像头实时更新 matplotlib 图形，如下所示的代码块：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is the basic template for initiating your video feed, and showing it in
    a matplotlib subfigure. We can stop by interrupting the kernel.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是初始化视频源并在 matplotlib 子图中显示的基本模板。我们可以通过中断内核来停止。
- en: We'll mention a few more libraries to play with in the next section.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中提到更多的库以供玩耍。
- en: See also
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: We recommend having a look at the YOLOv4 paper available on arxiv: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您查看 YOLOv4 论文，可在 arxiv 上找到：[https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934)。
- en: 'A few libraries are available regarding object detection:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有关对象检测，有几个库可用：
- en: The first author of the YOLOv4 paper is maintaining an open source framework
    supporting object detection, darknet (originally developed by Joseph Redmon) at [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv4 论文的第一作者在维护一个支持对象检测的开源框架，darknet（最初由 Joseph Redmon 开发）：[https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet)。
- en: Detectron2 is a library by Facebook AI Research implementing many algorithms
    for object detection: [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Detectron2 是 Facebook AI Research 提供的一个库，实现了多种物体检测算法：[https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
- en: The Google implementations in TensorFlow for object detection, including the
    recently published SpineNet ([https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html](https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html)),
    are available on GitHub: [https://github.com/tensorflow/models/tree/master/official/vision/detection](https://github.com/tensorflow/models/tree/master/official/vision/detection).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌在 TensorFlow 中实现了多种物体检测模型，包括最近发布的 SpineNet ([https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html](https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html))，可以在
    GitHub 上找到：[https://github.com/tensorflow/models/tree/master/official/vision/detection](https://github.com/tensorflow/models/tree/master/official/vision/detection).
- en: Valkka Live is an open source Python video surveillance platform: [https://elsampsa.github.io/valkka-live/_build/html/index.html](https://elsampsa.github.io/valkka-live/_build/html/index.html).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valkka Live 是一个开源的 Python 视频监控平台：[https://elsampsa.github.io/valkka-live/_build/html/index.html](https://elsampsa.github.io/valkka-live/_build/html/index.html).
- en: MMDetection is an open detection toolbox that covers many popular detection
    methods and comes with pretrained weights for about 200 network models: [https://mmdetection.readthedocs.io/en/latest/](https://mmdetection.readthedocs.io/en/latest/).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MMDetection 是一个开放的检测工具箱，涵盖了许多流行的检测方法，并提供大约 200 个网络模型的预训练权重：[https://mmdetection.readthedocs.io/en/latest/](https://mmdetection.readthedocs.io/en/latest/).
- en: SpineNet is a new model, found using a massive exploration of hyperparameters,
    for object detection in TensorFlow: [https://github.com/tensorflow/models/tree/master/official/vision/detection](https://github.com/tensorflow/models/tree/master/official/vision/detection).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SpineNet 是一种新的模型，通过大规模超参数探索，用于 TensorFlow 中的物体检测：[https://github.com/tensorflow/models/tree/master/official/vision/detection](https://github.com/tensorflow/models/tree/master/official/vision/detection).
- en: PyTracking is a library for object tracking and video object segmentation with
    many powerful, state-of-the-art models based on PyTorch, which can be directly
    plugged on top of the webcam input: [https://github.com/visionml/pytracking](https://github.com/visionml/pytracking).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTracking 是一个基于 PyTorch 的对象追踪和视频对象分割库，具有许多强大的、最先进的模型，可以直接插入网络摄像头输入之上：[https://github.com/visionml/pytracking](https://github.com/visionml/pytracking).
- en: PySlowFast supplies many pretrained models for video classification and detection
    tasks: [https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySlowFast 提供了许多用于视频分类和检测任务的预训练模型：[https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast).
- en: An implementation of models for real-time hand gesture recognition with PyTorch is
    available here: [https://github.com/ahmetgunduz/Real-time-GesRec](https://github.com/ahmetgunduz/Real-time-GesRec).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此处提供了一个用于实时手势识别的 PyTorch 模型实现：[https://github.com/ahmetgunduz/Real-time-GesRec](https://github.com/ahmetgunduz/Real-time-GesRec).
- en: Let's move on to the next recipe!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个步骤吧！
- en: Faking videos
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪造视频
- en: A deep fake is a manipulated video produced by the application of deep learning.
    Potential unethical uses have been around in the media for a while. You can imagine
    how this would end up in the hands of a propaganda mechanism trying to destabilize
    a government. Please note that we are advising against producing deep fakes for
    nefarious purposes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造是通过深度学习应用制作的操纵视频。潜在的不道德用途在媒体上已经存在一段时间。您可以想象这可能如何落入试图破坏政府的宣传机制之手中。请注意，我们建议不要出于不正当目的制作深度伪造视频。
- en: There are ethical applications of the deep fake technology, and some of them
    are a lot of fun. Have you ever wondered how Sylvester Stallone may have looked
    in Terminator? Today you can!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造技术存在一些道德应用，其中一些非常有趣。您是否想过史泰龙可能会在《终结者》中看起来如何？今天您可以实现！
- en: In this recipe, we'll learn how to create a deep fake with Python. We'll download
    public domain videos of two films, and we'll produce a deep fake by replacing
    one face with another. *Charade* was a 1963 film directed by Stanley Donen in
    a style reminiscent of a Hitchcock film. It pairs off Cary Grant in his mid-fifties
    and Audrey Hepburn in her early 30s. We thought we'd make the pairing more age-appropriate.
    After some searching, what we found was Maureen O'Hara in the 1963 John Wayne
    vehicle *McLintock!* to replace Audrey Hepburn.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将学习如何使用 Python 创建深度伪造。我们将下载两部电影的公共领域视频，并通过替换其中一个人物的脸部来制作深度伪造。《Charade》是一部1963年由斯坦利·多南导演的电影，风格类似于希区柯克的影片。影片中50多岁的凯瑞·格兰特与30多岁的奥黛丽·赫本搭档。我们认为让这对配对年龄更合适。在一番搜索后，我们找到了1963年约翰·韦恩主演的《McLintock!》中的莫琳·奥哈拉，以取代奥黛丽·赫本。
- en: Getting ready
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Faceit is a wrapper around the `faceswap` library, which facilitates many of
    the tasks that we'll need to perform for deep fake. We've forked the faceit repository
    at [https://github.com/benman1/facei](https://github.com/benman1/faceit)[t](https://github.com/benman1/faceit).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Faceit`是围绕`faceswap`库的一个包装器，它简化了我们进行深度伪造所需的许多任务。我们已经在[https://github.com/benman1/facei](https://github.com/benman1/faceit)[t](https://github.com/benman1/faceit)上分支了`faceit`仓库。'
- en: What we have to do is download the faceit repository and install the requisite
    library.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是下载`faceit`仓库并安装必需的库。
- en: 'You can download (`clone`) the repository with `git` (add an exclamation mark
    if you are typing this in an `ipython` notebook):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以用`git`克隆（`clone`）这个仓库（如果在`ipython`笔记本中输入此命令，请加上感叹号）：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We found that a Docker container was well-suited for the installation of dependencies
    (for this you need Docker installed). We can create a Docker container like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 Docker 容器非常适合安装依赖项（这需要安装 Docker）。我们可以像这样创建一个 Docker 容器：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This should take a while to build. Please note that the Docker image is based
    on Nvidia's container, so you can use your GPU from within the container.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能需要一些时间来构建。请注意，Docker 镜像基于 Nvidia 的容器，因此您可以在容器内使用 GPU。
- en: Please note that, although there is a lightweight model that we could use, we'd
    highly recommend you run the deep fake on a machine with a GPU.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管有一个轻量级模型可供使用，我们强烈建议您在配备 GPU 的机器上运行深度伪造技术。
- en: 'Finally, we can enter our container as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以这样进入我们的容器：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Inside the container, we can run Python 3.6\. All of the following commands
    assume we are inside the container and in the `/project` directory.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器内部，我们可以运行 Python 3.6\. 以下所有命令都假定我们在容器内并且在 `/project` 目录下。
- en: How to do it...
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We need to define videos and faces as inputs to our deep fake process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将视频和面部定义为深度伪造过程的输入。
- en: 'We define our model in Python (Python 3.6) like this:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以像这样在 Python（Python 3.6）中定义我们的模型：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This makes it clear that we want to replace `hepburn` with `ohara` (this is
    how we name them inside our process). We have to put images inside the data/persons
    `directories` named `hepburn.jpg` and `ohara.jpg`, accordingly. We have provided
    these images for convenience as part of the repository.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明我们想要用`ohara`替换`hepburn`（这是我们在进程内部对它们命名的方式）。我们必须将图像放在`data/persons`目录下分别命名为`hepburn.jpg`和`ohara.jpg`。我们已经在仓库的便利性部分提供了这些图像。
- en: If we don't provide the images, faceit will extract all face images irrespective
    of whom they show. We can then place two of these images for the `persons` directory,
    and delete the directories with faces under `data/processed/`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有提供图像，`faceit`将提取所有的面部图像，不论它们显示的是谁。然后我们可以将其中两张图像放在`persons`目录下，并删除`data/processed/`目录下面部的目录。
- en: We then need to define the videos that we want to use. We have the choice of
    using the complete films or short clips. We didn't find good clips for the *McLintock!*
    film, so we are using the whole film. As for *Charade*, we've focused on the clip
    of a single scene. We have these clips on disk as `mclintock.mp4` and `who_trust.mp4`.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义我们要使用的视频。我们可以选择使用完整的电影或短片。我们没有找到《McLintock!》电影的好短片，因此我们正在使用整部电影。至于《Charade》，我们专注于单场景的片段。我们将这些片段保存在磁盘上，名称为`mclintock.mp4`和`who_trust.mp4`。
- en: 'Please note that you should only download videos from sites that permit or
    don''t disallow downloading, even of public domain videos:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只能从允许下载或不禁止下载的网站下载视频，即使是公共领域的视频：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This defines the data used by our model as a couple of videos. Faceit allows
    an optional third parameter that can be a link to a video, from where it can be
    downloaded automatically. However, before you are downloading videos from YouTube
    or other sites, please make sure this is permitted in their terms of service and
    legal within your jurisdiction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了我们的模型使用的数据为一对视频。Faceit允许一个可选的第三参数，它可以是一个链接到视频的链接，从中可以自动下载。但是，在您从YouTube或其他网站下载视频之前，请确保这在其服务条款中允许，并且在您的司法管辖区内合法。
- en: 'The creation of the deep fake is then initiated by a few more lines of code
    (and a lot of tweaking and waiting):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过几行代码（和大量的调整和等待）启动深度伪造的创建：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preprocess step consists of downloading the videos, extracting all the frames
    as images, and finally extracting the faces. We are providing the faces already,
    so you don't have to perform the preprocess step.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理步骤包括下载视频，提取所有帧作为图像，最后提取面部。我们已经提供了这些面部，因此您不必执行预处理步骤。
- en: 'The following image shows Audrey Hepburn on the left, and Maureen O''Hara playing
    Audrey Hepburn on the right:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图片显示奥黛丽·赫本在左边，莫琳·奥哈拉在右边扮演奥黛丽·赫本：
- en: '![](img/102b8d3f-7123-4445-9ec9-2d1ed37aba2c.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/102b8d3f-7123-4445-9ec9-2d1ed37aba2c.png)'
- en: 'The changes might seem subtle. If you want something clearer, we can use the
    same model to replace Cary Grant with Maureen O''Hara:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变化可能看起来很微妙。如果你想要更清晰的东西，我们可以使用同样的模型将凯瑞·格兰特替换为莫琳·奥哈拉：
- en: '![](img/53694a5a-de71-4c46-967c-67a178423c2f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53694a5a-de71-4c46-967c-67a178423c2f.png)'
- en: In fact, we could produce a film, *Being Maureen O'Hara*, by disabling the face
    filter in the conversion.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以通过在转换中禁用面部过滤器来制作一部名为*《成为莫琳·奥哈拉》*的电影。
- en: We could have used more advanced models, more training to improve the deep fake,
    or we could have chosen an easier scene. However, the result doesn't look bad
    at all sometimes. We've uploaded our fake video to YouTube, where you can view
    it: [https://youtu.be/vDLxg5qXz4k](https://youtu.be/vDLxg5qXz4k).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用更先进的模型，更多的训练来改进深度伪造，或者我们可以选择一个更简单的场景。然而，有时候结果看起来并不差。我们已将我们的伪造视频上传到YouTube，您可以在此观看：[https://youtu.be/vDLxg5qXz4k](https://youtu.be/vDLxg5qXz4k)。
- en: How it works...
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: 'The typical deep fake pipeline consists of a number of steps that we conveniently
    glossed over in our recipe, because of the abstractions afforded in faceit. These
    steps are the following, given person A and person B, where A is to be replaced
    by B:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的深度伪造流程包括我们在配方中方便地忽略的一些步骤，因为Faceit提供了的抽象。这些步骤是以下，给定人物A和人物B，其中A将被B替换：
- en: Choose videos of A and B, and split the videos into frames.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择A和B的视频，并将视频分割成帧。
- en: Extract faces from these frames for A and B using face recognition. These faces,
    or the facial expressions and face postures, should ideally be representative
    of the video you are going to fake. Make sure they are not blurred, not occluded,
    don't show anyone else other than A and B, and that they are not too repetitive.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这些帧中使用人脸识别为A和B提取面部。这些面部，或面部表情和面部姿势，理想情况下应该代表你将制作的视频。确保它们不模糊，不被遮挡，不显示除A和B之外的任何人，并且它们不要太重复。
- en: You can train a model on these faces that can take face A and replace it with
    B. You should let the training run for a while. This can take several days, or
    even weeks or months, depending on the number of faces and the complexity of the
    model.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以训练一个模型处理这些面孔，该模型可以用B替换面孔A。你应该让训练运行一段时间。这可能需要几天，甚至几周或几个月，具体取决于面孔数量和模型复杂性。
- en: We can now convert the video by running the face recognition and the model on
    top of it.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们可以通过运行面部识别和模型在其上进行转换视频。
- en: In our case, the face recognition library (`face-recognition`) has a very good
    performance in terms of detection and recognition. However, it still suffers from
    high false positives, but also false negatives. This can result in a poor experience,
    especially in frames where there are several faces.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，面部识别库（`face-recognition`）在检测和识别方面表现非常出色。然而，它仍然存在高假阳性和假阴性。这可能导致体验不佳，特别是在有几个面部的帧中。
- en: In the current version of the `faceswap` library, we would extract frames from
    our target video in order to get landmarks for all the face alignments. We can
    then use the GUI in order to manually inspect and clean up these alignments in
    order to make sure they contain the right faces. These alignments will then be
    used for the conversion: [https://forum.faceswap.dev/viewtopic.php?t=27#align](https://forum.faceswap.dev/viewtopic.php?t=27#align).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前版本的 `faceswap` 库中，我们将从目标视频中提取帧以获取所有脸部对齐的地标。然后，我们可以使用 GUI 手动检查和清理这些对齐，以确保它们包含正确的脸部。这些对齐将用于转换：[https://forum.faceswap.dev/viewtopic.php?t=27#align](https://forum.faceswap.dev/viewtopic.php?t=27#align).
- en: 'Each of these steps requires a lot of attention. At the heart of the whole
    operation is the model. There can be different models, including a generative
    adversarial autoencoder and others. The original model in faceswap is an autoencoder
    with a twist. We''ve used autoencoders before in [Chapter 7](f386de9e-b56d-4b39-bf36-803860def385.xhtml),
    *Advanced Image Applications.* This one is relatively conventional, and we could
    have taken our autoencoder implementation from there. However, for the sake of
    completeness, we''ll show its implementation, which is based on `keras`/`tensorflow` (shortened):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤都需要非常注意。整个操作的核心是模型。可以有不同的模型，包括生成对抗自编码器和其他模型。faceswap 中的原始模型是带有变化的自编码器。我们之前在[第
    7 章](f386de9e-b56d-4b39-bf36-803860def385.xhtml)中使用过自编码器，*高级图像应用*。这个相对传统，我们可以从那里的自编码器实现中获取我们的自编码器实现。但是为了完整起见，我们将展示其基于
    `keras`/`tensorflow` 的实现（简化）：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This code, in itself, is not terribly interesting. We have two functions, `Decoder()`
    and `Encoder()`, which return decoder and encoder models, respectively. This is
    an encoder-decoder architecture with convolutions. The `PixelShuffle` layer in
    the upscale operation of the decoder rearranges data from depth into blocks of
    spatial data through a permutation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码本身并不是非常有趣。我们有两个函数，`Decoder()` 和 `Encoder()`，分别返回解码器和编码器模型。这是一个卷积编码器-解码器架构。在解码器的放大操作中，`PixelShuffle`
    层通过排列将数据从深度重新排列为空间数据块。
- en: 'Now, the more interesting part of the autoencoder is in how the training is
    performed as two models:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，自编码器更有趣的部分在于训练是如何进行的，作为两个模型：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We have two autoencoders, one to be trained on `A` faces and one on `B` faces.
    Both autoencoders are minimizing the reconstruction error (measured in mean absolute
    error) of output against input. As mentioned, we have a single encoder that forms
    part of the two models, and is therefore going to be trained both on faces `A`
    and faces `B`. The decoder models are kept separate between the two faces. This
    architecture ensures that we have a common latent representation between `A` faces
    and `B` faces. In the conversion, we can take a face from `A`, represent it, and
    then apply the decoder for `B` in order to get a `B` face corresponding to the
    latent representation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个自编码器，一个用于训练 `A` 脸部，另一个用于训练 `B` 脸部。两个自编码器都在最小化输出与输入之间的重建误差（以平均绝对误差度量）。如前所述，我们有一个单一编码器，它是两个模型的一部分，并且因此将在
    `A` 脸部和 `B` 脸部上都进行训练。解码器模型在两个脸部之间是分开的。这种架构确保我们在 `A` 脸部和 `B` 脸部之间有一个共同的潜在表示。在转换中，我们可以从
    `A` 中取出一个脸部，表示它，然后应用 `B` 的解码器以获得对应于潜在表示的 `B` 脸部。
- en: See also
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: We've put together some further references relating to playing around with videos
    and deep fakes, as well as detecting deep fakes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们整理了一些关于玩弄视频和深度伪造以及检测深度伪造的进一步参考资料。
- en: Deep fakes
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度伪造
- en: We've collated a few links relevant to deep fakes and some more links that are
    relevant to the process of creating deep fakes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们整理了一些关于深度伪造以及与创建深度伪造过程相关的链接。
- en: The face recognition library has been used in this recipe to select image regions
    for training and application of the transformations. It is available on GitHub
    at [https://github.com/ageitgey/face_recognition](https://github.com/ageitgey/face_recognition).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，面部识别库已经被用来选择图像区域进行训练和应用变换。该库在 GitHub 上可获得：[https://github.com/ageitgey/face_recognition](https://github.com/ageitgey/face_recognition)。
- en: 'There are some nice examples of simple video faking applications:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些简单视频伪造应用的不错示例：
- en: OpenCV masks can be used to selectively manipulate parts of an image, for example,
    for an invisibility cloak: [https://www.geeksforgeeks.org/invisible-cloak-using-opencv-python-project/](https://www.geeksforgeeks.org/invisible-cloak-using-opencv-python-project/).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV 可以使用掩码来选择性地操作图像的部分，例如隐形斗篷：[https://www.geeksforgeeks.org/invisible-cloak-using-opencv-python-project/](https://www.geeksforgeeks.org/invisible-cloak-using-opencv-python-project/)。
- en: A similar effort has been made to add mustaches based on detected face landmarks: [http://sublimerobots.com/2015/02/dancing-mustaches/](http://sublimerobots.com/2015/02/dancing-mustaches/).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于检测到的面部地标添加胡子的类似尝试：[http://sublimerobots.com/2015/02/dancing-mustaches/](http://sublimerobots.com/2015/02/dancing-mustaches/)。
- en: 'As for more complex video manipulations with deep fakes, quite a few tools
    are available, of which we''ll highlight two:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的视频操作，如深度伪造，有很多可用的工具，我们将重点介绍两个：
- en: The faceswap library has a GUI and even a few guides: [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: faceswap 库有 GUI 和一些指南：[https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap)。
- en: 'DeepFaceLab is a GUI application for creating deep fakes: [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepFaceLab 是一个用于创建深度伪造的 GUI 应用程序：[https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab)。
- en: 'Many different models have been proposed and implemented, including the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 提出并实现了许多不同的模型，包括以下内容：
- en: '**ReenactGAN** *– Learning to Reenact Faces via Boundary Transfer* (2018, [https://arxiv.org/abs/1807.11079](https://arxiv.org/abs/1807.11079))'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReenactGAN** *– 通过边界转移学习重新演绎面部*（2018，[https://arxiv.org/abs/1807.11079](https://arxiv.org/abs/1807.11079)）'
- en: Official implementation: [https://github.com/wywu/ReenactGAN](https://github.com/wywu/ReenactGAN).
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方实现：[https://github.com/wywu/ReenactGAN](https://github.com/wywu/ReenactGAN)。
- en: '**DiscoGAN** *–* Taeksoo Kim and others, *Learning to Discover Cross-Domain
    Relations with Generative Adversarial Networks* (2017, [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192))'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DiscoGAN** *–* Taeksoo Kim 等人，《通过生成对抗网络学习发现跨领域关系》（2017，[https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192)）'
- en: Demonstrated for live video feeds here: [https://github.com/ptrblck/DiscoGAN](https://github.com/ptrblck/DiscoGAN).
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此演示了实时视频源：[https://github.com/ptrblck/DiscoGAN](https://github.com/ptrblck/DiscoGAN)。
- en: A denoising adversarial autoencoder with attention mechanisms for face swapping
    can be seen here: [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有注意机制的去噪对抗自编码器用于面部交换，详见：[https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN)。
- en: The faceit live repository ([https://github.com/alew3/faceit_live](https://github.com/alew3/faceit_live))
    is a fork of faceit that can operate on live video feeds and comes with a hack
    to feed the video back to prank participants in video conferences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: faceit live 仓库（[https://github.com/alew3/faceit_live](https://github.com/alew3/faceit_live)）是
    faceit 的一个分支，可以在实时视频源上运行，并配有一个反馈视频给恶作剧参与者的 hack。
- en: Detection of deep fakes
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度伪造检测
- en: 'The following links are relevant to detecting deep fakes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下链接与检测深度伪造相关：
- en: Yuchen Luo has collected lots of links relating to the detection of deep fakes: [https://github.com/592McAvoy/fake-face-detection](https://github.com/592McAvoy/fake-face-detection).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuchen Luo 收集了许多关于检测深度伪造的链接：[https://github.com/592McAvoy/fake-face-detection](https://github.com/592McAvoy/fake-face-detection)。
- en: Of particular interest is detection via adversarial attacks, as can be found
    here: [https://github.com/natanielruiz/disrupting-deepfakes](https://github.com/natanielruiz/disrupting-deepfakes).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特别感兴趣的是通过对抗攻击进行检测，详情请见：[https://github.com/natanielruiz/disrupting-deepfakes](https://github.com/natanielruiz/disrupting-deepfakes)。
- en: Google, Google Jigsaw, the Technical University of Munich, and the University
    Federico II of Naples provide an extensive dataset of deep fakes for the study
    of detection algorithms: [https://github.com/ondyari/FaceForensics/](https://github.com/ondyari/FaceForensics/).
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google、Google Jigsaw、慕尼黑工业大学和那不勒斯费德里科二世大学为深度伪造研究提供了大量数据集：[https://github.com/ondyari/FaceForensics/](https://github.com/ondyari/FaceForensics/)。
- en: 'The paper *DeepFakes and Beyond: A Survey of **Face Manipulation and Fake Detection* (Ruben
    Tolosana and others, 2020) provides more links and more resources to datasets
    and methods.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '论文《DeepFakes and Beyond: A Survey of **Face Manipulation and Fake Detection*》（Ruben
    Tolosana 等人，2020）提供了更多的链接和数据集资源及方法。'
