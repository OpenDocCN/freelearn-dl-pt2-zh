- en: Diving Deep into Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入研究神经网络
- en: In this chapter, we will explore the different modules of deep learning architectures
    that are used to solve real-world problems. In the previous chapter, we used low-level
    operations of PyTorch to build modules such as a network architecture, a loss
    function, and an optimizer. In this chapter, we will explore some of the important
    components of neural networks required to solve real-world problems, along with
    how PyTorch abstracts away a lot of complexity by providing a lot of high-level
    functions. Towards the end of the chapter, we will build algorithms that solve
    real-world problems such as regression, binary classification, and multi-class
    classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索用于解决真实世界问题的深度学习架构的不同模块。在前一章中，我们使用PyTorch的低级操作来构建模块，如网络架构、损失函数和优化器。在本章中，我们将探讨解决实际问题所需的神经网络的重要组件，以及PyTorch通过提供大量高级功能来抽象掉许多复杂性。在本章末尾，我们将构建解决回归、二元分类和多类分类等真实世界问题的算法。
- en: 'In this chapter, we will go through following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Deep dive into the various building blocks of neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入探讨神经网络的各种构建模块
- en: Exploring higher-level functionalities in PyTorch to build deep learning architectures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索PyTorch中的高级功能，构建深度学习架构
- en: Applying deep learning to a real-world image classification problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度学习应用于真实世界的图像分类问题
- en: Deep dive into the building blocks of neural networks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨神经网络的构建模块
- en: 'As we learned in the previous chapter, training a deep learning algorithm requires
    the following steps:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章学到的，训练深度学习算法需要以下步骤：
- en: Building a data pipeline
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建数据流水线
- en: Building a network architecture
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建网络架构
- en: Evaluating the architecture using a loss function
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用损失函数评估架构
- en: Optimizing the network architecture weights using an optimization algorithm
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用优化算法优化网络架构权重
- en: 'In the previous chapter, the network was composed of a simple linear model
    built using PyTorch numerical operations. Though building a neural architecture
    for a toy problem using numerical operations is easier, it quickly becomes complicated
    when we try to build architectures required to solve complex problems in different
    areas, such as computer vision and **natural language processing** (**NLP**).
    Most of the deep learning frameworks, such as PyTorch, TensorFlow, and Apache
    MXNet, provide higher-level functionalities that abstract a lot of this complexity.
    These higher-level functionalities are called **layers** across the deep learning
    frameworks. They accept input data, apply transformations like the ones we have
    seen in the previous chapter, and output the data. To solve real-world problems,
    deep learning architectures constitute of a number of layers ranging from 1 to
    150, or sometimes more than that. Abstracting the low-level operations and training
    deep learning algorithms would look like the following diagram:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，网络由使用PyTorch数值操作构建的简单线性模型组成。尽管使用数值操作为玩具问题构建神经架构更容易，但当我们尝试构建解决不同领域（如计算机视觉和自然语言处理）复杂问题所需的架构时，情况很快变得复杂。大多数深度学习框架（如PyTorch、TensorFlow和Apache
    MXNet）提供高级功能，抽象掉许多这种复杂性。这些高级功能在深度学习框架中被称为**层**。它们接受输入数据，应用类似于我们在前一章中看到的转换，并输出数据。为了解决真实世界问题，深度学习架构由1到150个层组成，有时甚至更多。通过提供高级函数来抽象低级操作和训练深度学习算法的过程如下图所示：
- en: '![](img/14187b0e-8fa2-415e-a890-6695d3bcae31.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14187b0e-8fa2-415e-a890-6695d3bcae31.png)'
- en: Summarizing the previous diagram, any deep learning training involves getting
    data, building an architecture that in general is getting a bunch of layers together,
    evaluating the accuracy of the model using a loss function, and then optimizing
    the algorithm by optimizing the weights of our network. Before looking at solving
    some of the real-world problems, we will come to understand higher-level abstractions
    provided by PyTorch for building layers, loss functions, and optimizers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 总结前一张图，任何深度学习训练都涉及获取数据，构建一般获取一堆层的架构，使用损失函数评估模型的准确性，然后通过优化权重优化算法。在解决一些真实世界问题之前，我们将了解PyTorch提供的用于构建层、损失函数和优化器的更高级抽象。
- en: Layers – fundamental blocks of neural networks
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层 - 神经网络的基本构建块
- en: 'Throughout the rest of the chapter, we will come across different types of
    layers. To begin, let''s try to understand one of the most important layers, the
    linear layer, which does exactly what our previous network architecture does.
    The linear layer applies a linear transformation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分中，我们将遇到不同类型的层次。首先，让我们尝试理解最重要的层之一，即线性层，它与我们之前的网络架构完全一样。线性层应用线性变换：
- en: '![](img/e6333f93-736c-4658-957a-180b30f69600.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6333f93-736c-4658-957a-180b30f69600.png)'
- en: 'What makes it powerful is that fact that the entire function that we wrote
    in the previous chapter can be written in a single line of code, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其强大之处在于我们在前一章中编写的整个函数可以用一行代码表示，如下所示：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `myLayer` in the preceding code will accept a tensor of size `10` and outputs
    a tensor of size `5` after applying linear transformation. Let''s look at a simple
    example of how to do that:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`myLayer`将接受大小为`10`的张量，并在应用线性变换后输出大小为`5`的张量。让我们看一个如何做到这一点的简单示例：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can access the trainable parameters of the layer using the `weights` and
    `bias` attributes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`weights`和`bias`属性访问层的可训练参数：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Linear layers are called by different names, such as **dense** or **fully connected
    layers** across different frameworks. Deep learning architectures used for solving
    real-world use cases generally contain more than one layer. In PyTorch, we can
    do it in multiple ways, shown as follows.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层在不同框架中被称为**密集层**或**全连接层**。解决实际应用场景的深度学习架构通常包含多个层次。在PyTorch中，我们可以用多种方式实现，如下所示。
- en: 'One simple approach is passing the output of one layer to another layer:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是将一层的输出传递给另一层：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Each layer will have its own learnable parameters. The idea behind using multiple
    layers is that each layer will learn some kind of pattern that the later layers
    will build on. There is a problem in adding just linear layers together, as they
    fail to learn anything new beyond a simple representation of a linear layer. Let's
    see through a simple example of why it does not make sense to stack multiple linear
    layers together.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个层次都有自己可学习的参数。使用多个层次的理念是，每个层次将学习某种模式，后续层次将在此基础上构建。仅将线性层堆叠在一起存在问题，因为它们无法学习超出简单线性层表示的任何新内容。让我们通过一个简单的例子看看为什么堆叠多个线性层没有意义。
- en: 'Let''s say we have two linear layers with the following weights:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个线性层，具有以下权重：
- en: '| **Layers** | **Weight1** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **层次** | **权重1** |'
- en: '| Layer1 | 3.0 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 层1 | 3.0 |'
- en: '| Layer2 | 2.0 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 层2 | 2.0 |'
- en: 'The preceding architecture with two different layers can be simply represented
    as a single layer with a different layer. Hence, just stacking multiple linear
    layers will not help our algorithms to learn anything new. Sometimes, this can
    be unclear, so we can visualize the architecture with the following mathematical
    formulas:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 具有两个不同层的前述架构可以简单地表示为具有不同层的单一层。因此，仅仅堆叠多个线性层并不会帮助我们的算法学习任何新内容。有时这可能不太清晰，因此我们可以通过以下数学公式可视化架构：
- en: '![](img/4556ddc4-d0e1-4e0e-a153-c2ec2c779701.png)![](img/7a28a3ad-2d64-4937-9309-5c062885025f.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4556ddc4-d0e1-4e0e-a153-c2ec2c779701.png)![](img/7a28a3ad-2d64-4937-9309-5c062885025f.png)'
- en: To solve this problem, we have different non-linearity functions that help in
    learning different relationships, rather than only focusing on linear relationships.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们有不同的非线性函数，帮助学习不同的关系，而不仅仅是线性关系。
- en: There are many different non-linear functions available in deep learning. PyTorch
    provides these non-linear functionalities as layers and we will be able to use
    them the same way we used the linear layer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中提供了许多不同的非线性函数。PyTorch将这些非线性功能作为层提供，我们可以像使用线性层一样使用它们。
- en: 'Some of the popular non-linear functions are as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一些流行的非线性函数如下：
- en: Sigmoid
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Tanh
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: ReLU
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU
- en: Leaky ReLU
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Non-linear activations
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性激活函数
- en: Non-linear activations are functions that take inputs and then apply a mathematical
    transformation and produce an output. There are several non-linear operations
    that we come across in practice. We will go through some of the popular non-linear
    activation functions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活是指接受输入并应用数学变换并产生输出的函数。在实践中，我们会遇到几种流行的非线性激活函数。我们将介绍一些常见的非线性激活函数。
- en: Sigmoid
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The sigmoid activation function has a simple mathematical form, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数具有简单的数学形式，如下所示：
- en: '![](img/aabbaeb0-9f60-44df-940e-9b3874c3ece2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aabbaeb0-9f60-44df-940e-9b3874c3ece2.png)'
- en: 'The sigmoid function intuitively takes a real-valued number and outputs a number
    in a range between zero and one. For a large negative number, it returns close
    to zero and, for a large positive number, it returns close to one. The following
    plot represents different sigmoid function outputs:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid函数直观地接受一个实数，并输出一个介于零和一之间的数字。对于一个大的负数，它返回接近零，对于一个大的正数，它返回接近一。以下图表示不同sigmoid函数的输出：
- en: '![](img/beb8c9f1-c75f-4e24-b654-5536c74ca248.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/beb8c9f1-c75f-4e24-b654-5536c74ca248.png)'
- en: The sigmoid function has been historically used across different architectures,
    but in recent times it has gone out of popularity as it has one major drawback.
    When the output of the sigmoid function is close to zero or one, the gradients
    for the layers before the sigmoid function are close to zero and, hence, the learnable
    parameters of the previous layer get gradients close to zero and the weights do
    not get adjusted often, resulting in dead neurons.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid函数在历史上被广泛应用于不同的架构，但近年来已经不那么流行了，因为它有一个主要缺点。当sigmoid函数的输出接近零或一时，前sigmoid函数层的梯度接近零，因此前一层的可学习参数的梯度也接近零，权重很少被调整，导致死神经元。
- en: Tanh
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tanh
- en: 'The tanh non-linearity function squashes a real-valued number in the range
    of -1 and 1\. The tanh also faces the same issue of saturating gradients when
    tanh outputs extreme values close to -1 and 1\. However, it is preferred to sigmoid,
    as the output of tanh is zero centered:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: tanh非线性函数将一个实数压缩到-1到1的范围内。当tanh输出接近-1或1的极值时，也会面临梯度饱和的问题。然而，与sigmoid相比，它更受青睐，因为tanh的输出是零中心化的：
- en: '![](img/eec87fd4-fe4f-408b-b831-4157206a684f.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eec87fd4-fe4f-408b-b831-4157206a684f.png)'
- en: 'Image source: http://datareview.info/article/eto-nuzhno-znat-klyuchevyie-rekomendatsii-po-glubokomu-obucheniyu-chast-2/'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：http://datareview.info/article/eto-nuzhno-znat-klyuchevyie-rekomendatsii-po-glubokomu-obucheniyu-chast-2/
- en: ReLU
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU
- en: 'ReLU has become more popular in the recent years; we can find either its usage
    or one of its variants'' usages in almost any modern architecture. It has a simple
    mathematical formulation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，ReLU变得更加流行；我们几乎可以在任何现代架构中找到它的使用或其变体的使用。它有一个简单的数学公式：
- en: '*f(x)=max(0,x)*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x)=max(0,x)*'
- en: 'In simple words, ReLU squashes any input that is negative to zero and leaves
    positive numbers as they are. We can visualize the ReLU function as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，ReLU将任何负输入压缩为零，并保留正数不变。我们可以将ReLU函数可视化如下：
- en: '![](img/dd8dabd2-67a6-41f6-85bd-99575eadf301.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd8dabd2-67a6-41f6-85bd-99575eadf301.png)'
- en: 'Image source: http://datareview.info/article/eto-nuzhno-znat-klyuchevyie-rekomendatsii-po-glubokomu-obucheniyu-chast-2/'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：http://datareview.info/article/eto-nuzhno-znat-klyuchevyie-rekomendatsii-po-glubokomu-obucheniyu-chast-2/
- en: 'Some of the pros and cons of using ReLU are as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ReLU的一些优缺点如下：
- en: It helps the optimizer in finding the right set of weights sooner. More technically
    it makes the convergence of stochastic gradient descent faster.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它帮助优化器更快地找到正确的权重集。从技术上讲，它加快了随机梯度下降的收敛速度。
- en: It is computationally inexpensive, as we are just thresholding and not calculating
    anything like we did for the sigmoid and tangent functions.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它计算上廉价，因为我们只是进行阈值处理，而不像sigmoid和tanh函数那样进行计算。
- en: ReLU has one disadvantage; when a large gradient passes through it during the
    backward propagation, they often become non-responsive; these are called **dead
    neutrons**, which can be controlled by carefully choosing the learning rate. We
    will discuss how to choose learning rates when we discuss the different ways to
    adjust the learning rate in [Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml),
    *Fundamentals of Machine Learning*.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU有一个缺点；在反向传播过程中，当大梯度通过时，它们往往会变得不响应；这些被称为**死神经元**，可以通过谨慎选择学习率来控制。我们将在讨论不同的学习率调整方法时讨论如何选择学习率，参见[第4章](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml)，《机器学习基础》。
- en: Leaky ReLU
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: Leaky ReLU is an attempt to solve a dying problem where, instead of saturating
    to zero, we saturate to a very small number such as 0.001\. For some use cases,
    this activation function provides a superior performance to others, but it is
    not consistent.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU是解决死亡问题的尝试，它不是饱和为零，而是饱和为一个非常小的数，如0.001。对于某些用例，这种激活函数提供了比其他函数更好的性能，但其表现不一致。
- en: PyTorch non-linear activations
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch非线性激活函数
- en: 'PyTorch has most of the common non-linear activation functions implemented
    for us already and it can be used like any other layer. Let''s see a quick example
    of how to use the `ReLU` function in PyTorch:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch已经为我们实现了大多数常见的非线性激活函数，并且它可以像任何其他层一样使用。让我们快速看一个如何在PyTorch中使用`ReLU`函数的示例：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding example, we take a tensor with two positive values and two
    negative values and apply a `ReLU` on it, which thresholds the negative numbers
    to `0` and retains the positive numbers as they are.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们取一个具有两个正值和两个负值的张量，并对其应用 `ReLU` 函数，这会将负数阈值化为 `0` 并保留正数。
- en: Now we have covered most of the details required for building a network architecture,
    let's build a deep learning architecture that can be used to solve real-world
    problems. In the previous chapter, we used a simple approach so that we could
    focus only on how a deep learning algorithm works. We will not be using that style
    to build our architecture anymore; rather, we will be building the architecture
    in the way it is supposed to be built in PyTorch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了构建网络架构所需的大部分细节，让我们构建一个可以用来解决真实世界问题的深度学习架构。在前一章中，我们使用了一种简单的方法，以便我们可以只关注深度学习算法的工作原理。我们将不再使用那种风格来构建我们的架构；相反，我们将按照PyTorch中应有的方式来构建架构。
- en: The PyTorch way of building deep learning algorithms
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度学习算法的PyTorch方式
- en: 'All the networks in PyTorch are implemented as classes, subclassing a PyTorch
    class called `nn.Module`, and should implement `__init__` and `forward` methods.
    Inside the `init` function, we initialize any layers, such as the `linear` layer,
    which we covered in the previous section. In the `forward` method, we pass our
    input data into the layers that we initialized in our `init` method and return
    our final output. The non-linear functions are often directly used in the `forward`
    function and some use it in the `init` method too. The following code snippet
    shows how a deep learning architecture is implemented in PyTorch:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的所有网络都实现为类，子类化一个名为`nn.Module`的PyTorch类，并且应实现`__init__`和`forward`方法。在`init`函数中，我们初始化任何层，例如我们在前一节中介绍的`linear`层。在`forward`方法中，我们将输入数据传递到我们在`init`方法中初始化的层，并返回最终输出。非线性函数通常直接在`forward`函数中使用，有些也在`init`方法中使用。下面的代码片段显示了如何在PyTorch中实现深度学习架构：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you are new to Python, some of the preceding code could be difficult to understand,
    but all it is doing is inheriting a parent class and implementing two methods
    in it. In Python, we subclass by passing the parent class as an argument to the
    class name. The `init` method acts as a constructor in Python and `super` is used
    to pass on arguments of the child class to the parent class, which in our case
    is `nn.Module`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是Python的新手，一些前面的代码可能难以理解，但它所做的就是继承一个父类并在其中实现两个方法。在Python中，我们通过将父类作为参数传递给类名来子类化。`init`方法在Python中充当构造函数，`super`用于将子类的参数传递给父类，在我们的例子中是`nn.Module`。
- en: Model architecture for different machine learning problems
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同机器学习问题的模型架构
- en: 'The kind of problem we are solving will decide mostly what layers we will use,
    starting from a linear layer to **Long Short-Term Memory** (**LSTM**) for sequential
    data. Based on the type of the problem you are trying to solve, your last layer
    is determined. There are three problems that we generally solve using any machine
    learning or deep learning algorithms. Let''s look at what the last layer would
    look like:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要解决的问题类型将主要决定我们将使用的层，从线性层到**长短期记忆**（**LSTM**）适用于序列数据。根据你尝试解决的问题类型，你的最后一层将被确定。通常使用任何机器学习或深度学习算法解决的问题有三种。让我们看看最后一层会是什么样子：
- en: For a regression problem, such as predicting the price of a t-shirt to sell,
    we would use the last layer as a linear layer with an output of one, which outputs
    a continuous value.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归问题，比如预测一件T恤的价格，我们将使用具有输出为1的线性层作为最后一层，输出一个连续值。
- en: For classifying a given image as t-shirt or shirt, you would use a sigmoid activation
    function, as it outputs values either closer to one or zero, which is generally
    called a **binary classification problem**.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要将给定图像分类为T恤或衬衫，您将使用sigmoid激活函数，因为它输出接近于1或0的值，这通常被称为**二分类问题**。
- en: For a multi-class classification, where we have to classify whether a given
    image is a t-shirt, jeans, shirt, or dress, we would use a softmax layer at the
    end our network. Let's try to understand intuitively what softmax does without
    going into the math of it. It takes inputs from the previous linear layer, for
    example, and outputs the probabilities for a given number of examples. In our
    example, it would be trained to predict four probabilities for each type of image.
    Remember, all these probabilities always add up to one.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多类分类问题，我们需要分类给定图像是否是T恤、牛仔裤、衬衫或连衣裙，我们将在网络的最后使用softmax层。让我们试着直观地理解softmax在没有深入数学的情况下的作用。它从前面的线性层接收输入，例如，为每种图像类型预测四个概率。请记住，所有这些概率始终总和为一。
- en: Loss functions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Loss函数
- en: Once we have defined our network architecture, we are left with two important
    steps. One is calculating how good our network is at performing a particular task
    of regression, classification, and the next is optimizing the weight.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了网络架构，我们还剩下两个重要的步骤。一个是计算我们的网络在执行回归、分类等特定任务时的表现如何，另一个是优化权重。
- en: The optimizer (gradient descent) generally accepts a scalar value, so our `loss`
    function should generate a scalar value that has to be minimized during our training.
    Certain use cases, such as predicting where an obstacle is on the road and classifying
    it to a pedestrian or not, would require two or more loss functions. Even in such
    scenarios, we need to combine the losses to a single scalar for the optimizer
    to minimize. We will discuss examples of combining multiple losses to a single
    scalar in detail with a real-world example in the last chapter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器（梯度下降）通常接受标量值，因此我们的`loss`函数应该生成一个标量值，在训练过程中需要最小化它。在一些特定的应用场景中，比如预测道路上的障碍物并将其分类为行人或其他，可能需要使用两个或更多个`loss`函数。即使在这种情况下，我们也需要将这些损失结合成单一标量值，供优化器最小化。在最后一章节中，我们将详细讨论如何在实际应用中结合多个损失函数的例子。
- en: In the previous chapter, we defined our own `loss` function. PyTorch provides
    several implementations of commonly used `loss` functions. Let's take a look at
    the `loss` functions used for regression and classification.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节中，我们定义了自己的`loss`函数。PyTorch提供了几种常用的`loss`函数的实现。让我们来看看用于回归和分类的`loss`函数。
- en: 'The commonly used `loss` function for regression problems is **mean square
    error** (**MSE**). It is the same `loss` function we implemented in our previous
    chapter. We can use the `loss` function implemented in PyTorch, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题常用的`loss`函数是**均方误差**（**MSE**）。这也是我们在前一章节实现的`loss`函数。我们可以使用PyTorch中实现的`loss`函数，如下所示：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For classification, we use a cross-entropy loss. Before looking at the math
    for cross-entropy, let''s understand what a cross-entropy loss does. It calculates
    the loss of a classification network predicting the probabilities, which should
    sum up to one, like our softmax layer. A cross-entropy loss increases when the
    predicted probability diverges from the correct probability. For example, if our
    classification algorithm predicts 0.1 probability for the following image to be
    a cat, but it is actually a panda, then the cross-entropy loss will be higher.
    If it predicts similar to the actual labels, then the cross-entropy loss will
    be lower:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，我们使用交叉熵损失。在看交叉熵的数学之前，让我们理解一下交叉熵损失的作用。它计算分类网络预测概率的损失，这些概率应该总和为一，就像我们的softmax层一样。当预测的概率与正确概率偏离时，交叉熵损失增加。例如，如果我们的分类算法预测某个图像是猫的概率为0.1，但实际上是熊猫，那么交叉熵损失将会更高。如果它预测与实际标签相似，那么交叉熵损失将会更低：
- en: '![](img/7f8abb0b-31dc-4e78-a06a-1a29631d7706.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f8abb0b-31dc-4e78-a06a-1a29631d7706.png)'
- en: 'Let''s look at a sample implementation of how this actually happens in Python
    code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个实际在Python代码中如何实现的示例：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To use a cross-entropy loss in a classification problem, we really do not need
    to be worried about what happens inside—all we have to remember is that, the loss
    will be high when our predictions are bad and low when predictions are good. PyTorch
    provides us with an implementation of the `loss`, which we can use, as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中使用交叉熵损失，我们实际上不需要担心内部发生了什么——我们只需要记住，当我们的预测很差时，损失会很高，而当预测很好时，损失会很低。PyTorch为我们提供了一个`loss`的实现，我们可以使用，如下所示：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Some of the other `loss` functions that come as part of PyTorch are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供的其他一些`loss`函数如下：
- en: '| L1 loss | Mostly used as a regularizer. We will discuss it further in [Chapter
    4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals of Machine Learning*.
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| L1 loss | 主要用作正则化器。我们将在[第 4 章](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml)，*机器学习基础*中进一步讨论它。
    |'
- en: '| MSE loss | Used as loss function for regression problems. |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MSE loss | 用作回归问题的损失函数。 |'
- en: '| Cross-entropy loss | Used for binary and multi-class classification problems.
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 交叉熵损失 | 用于二元和多类别分类问题。 |'
- en: '| NLL Loss | Used for classification problems and allows us to use specific
    weights to handle imbalanced datasets. |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| NLL Loss | 用于分类问题，并允许我们使用特定权重来处理不平衡的数据集。 |'
- en: '| NLL Loss2d | Used for pixel-wise classification, mostly for problems related
    to image segmentation. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| NLL Loss2d | 用于像素级分类，主要用于与图像分割相关的问题。 |'
- en: Optimizing network architecture
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化网络架构
- en: 'Once we have calculated the loss of our network, we will optimize the weights
    to reduce the loss and thus improving the accuracy of the algorithm. For the sake
    of simplicity, let''s see these optimizers as black boxes that take loss functions
    and all the learnable parameters and move them slightly to improve our performances.
    PyTorch provides most of the commonly used optimizers required in deep learning.
    If you want to explore what happens inside these optimizers and have a mathematical
    background, I would strongly recommend some of the following blogs:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算出网络的损失，我们将优化权重以减少损失，从而提高算法的准确性。为了简单起见，让我们将这些优化器视为黑盒子，它们接受损失函数和所有可学习参数，并轻微地移动它们以提高我们的性能。PyTorch
    提供了深度学习中大多数常用的优化器。如果您想探索这些优化器内部发生了什么，并且具备数学背景，我强烈建议阅读以下一些博客：
- en: '[http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)'
- en: '[http://ruder.io/deep-learning-optimization-2017/](http://ruder.io/deep-learning-optimization-2017/)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://ruder.io/deep-learning-optimization-2017/](http://ruder.io/deep-learning-optimization-2017/)'
- en: 'Some of the optimizers that PyTorch provides are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供的一些优化器包括以下几种：
- en: ADADELTA
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ADADELTA
- en: Adagrad
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: Adam
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam
- en: SparseAdam
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparseAdam
- en: Adamax
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adamax
- en: ASGD
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASGD
- en: LBFGS
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LBFGS
- en: RMSProp
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSProp
- en: Rprop
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rprop
- en: SGD
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD
- en: 'We will get into the details of some of the algorithms in[ Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals
    of Machine Learning*, along with some of the advantages and tradeoffs. Let''s
    walk through some of the important steps in creating any `optimizer`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第 4 章](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml)，*机器学习基础*中详细讨论一些算法，以及一些优缺点。让我们一起走过创建任何`optimizer`的一些重要步骤：
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding example, we created an `SGD` optimizer that takes all the
    learnable parameters of your network as the first argument and a learning rate
    that determines what ratio of change can be made to the learnable parameter. In
    [Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals of Machine
    Learning* we will get into more details of learning rates and momentum, which
    is an important parameter of optimizers. Once you create an optimizer object,
    we need to call `zero_grad()` inside our loop, as the parameters will accumulate
    the gradients created during the previous `optimizer` call:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们创建了一个`SGD`优化器，它将您网络中所有可学习参数作为第一个参数，并且一个学习率，决定可学习参数变化的比例。在[第 4 章](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml)，*机器学习基础*中，我们将更详细地讨论学习率和动量，这是优化器的一个重要参数。一旦创建了优化器对象，我们需要在循环内调用`zero_grad()`，因为参数会累积前一个`optimizer`调用期间创建的梯度：
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once we call `backward` on the `loss` function, which calculates the gradients
    (quantity by which learnable parameters need to change), we call `optimizer.step()`,
    which makes the actual changes to our learnable parameter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在`loss`函数上调用`backward`，计算出梯度（可学习参数需要改变的量），我们调用`optimizer.step()`，实际上对我们的可学习参数进行更改。
- en: Now, we have covered most of the components required to help a computer see/
    recognize images. Let's build a complex deep learning model that can differentiate
    between dogs and cats to put all the theory into practice.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经涵盖了帮助计算机看/识别图像所需的大多数组件。让我们构建一个复杂的深度学习模型，可以区分狗和猫，以便将所有理论付诸实践。
- en: Image classification using deep learning
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类使用深度学习
- en: 'The most important step in solving any real-world problem is to get the data.
    Kaggle provides a huge number of competitions on different data science problems.
    We will pick one of the problems that arose in 2014, which we will use to test
    our deep learning algorithms in this chapter and improve it in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep
    Learning for Computer Vision*, which will be on **Convolution Neural Networks**
    (**CNNs**) and some of the advanced techniques that we can use to improve the
    performance of our image recognition models. You can download the data from [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data).
    The dataset contains 25,000 images of dogs and cats. Preprocessing of data and
    the creation of train, validation, and test splits are some of the important steps
    that need to be performed before we can implement an algorithm. Once the data
    is downloaded, taking a look at it, it shows that the folder contains images in
    the following format:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决任何现实世界问题中最重要的一步是获取数据。Kaggle 提供了大量关于不同数据科学问题的竞赛。我们将挑选2014年出现的一个问题，在本章中用于测试我们的深度学习算法，并在[第五章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)，*计算机视觉的深度学习*
    中进行改进，该章将介绍**卷积神经网络**（**CNNs**）以及一些高级技术，可以用来提升我们的图像识别模型的性能。您可以从 [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)
    下载数据。数据集包含 25,000 张猫和狗的图像。在实施算法之前，需要进行数据的预处理和创建训练、验证和测试集分割等重要步骤。一旦数据下载完毕，查看数据，可以看到文件夹中包含以下格式的图像：
- en: '![](img/c746e70e-bf9a-40be-8f68-7493de1eb7ac.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c746e70e-bf9a-40be-8f68-7493de1eb7ac.png)'
- en: 'Most of the frameworks make it easier to read the images and tag them to their
    labels when provided in the following format. That means that each class should
    have a separate folder of its images. Here, all cat images should be in the `cat`
    folder and dog images in the `dog` folder:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数框架在提供以下格式的图像时，使阅读图像并将其标记为其标签变得更加简单。这意味着每个类别应该有一个单独的文件夹包含其图像。在这里，所有猫的图像应该在
    `cat` 文件夹中，所有狗的图像应该在 `dog` 文件夹中：
- en: '![](img/bf800adb-ed3e-4f0e-b644-de91d3789ce5.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf800adb-ed3e-4f0e-b644-de91d3789ce5.png)'
- en: 'Python makes it easy to put the data into the right format. Let''s quickly
    take a look at the code and, then, we will go through the important parts of it:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Python 让将数据放入正确的格式变得容易。让我们快速查看代码，然后我们将逐步介绍其重要部分：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: All the preceding code does is retrieve all the files and pick 2,000 images
    for creating a validation set. It segregates all the images into the two categories
    of cats and dogs. It is a common and important practice to create a separate validation
    set, as it is not fair to test our algorithms on the same data it is trained on.
    To create a `validation` dataset, we create a list of numbers that are in the range
    of the length of the images in a shuffled order. The shuffled numbers act as an
    index for us to pick a bunch of images for creating our `validation` dataset.
    Let's go through each section of the code in detail.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述代码做的就是检索所有文件，并选择 2,000 张图像来创建验证集。它将所有图像分成两类：猫和狗。创建独立的验证集是一个常见且重要的做法，因为在同一数据上测试我们的算法是不公平的。为了创建一个
    `validation` 数据集，我们创建一个在打乱顺序的图像长度范围内的数字列表。打乱的数字作为索引，帮助我们选择一组图像来创建我们的 `validation`
    数据集。让我们详细地查看代码的每个部分。
- en: 'We create a file using the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码创建一个文件：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `glob` method returns all the files in the particular path. When there are
    a huge number of images, we can also use `iglob`, which returns an iterator, instead
    of loading the names into memory. In our case, we have only 25,000 filenames,
    which can easily fit into memory.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`glob` 方法返回特定路径中的所有文件。当图像数量庞大时，我们可以使用 `iglob`，它返回一个迭代器，而不是将名称加载到内存中。在我们的情况下，只有
    25,000 个文件名，可以轻松放入内存。'
- en: 'We can shuffle our files using the following code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来对文件进行洗牌：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding code returns 25,000 numbers in the range from zero to 25,000 in
    a shuffled order, which we will use as an index for selecting a subset of images
    to create a `validation` dataset.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码以随机顺序返回从零到 25,000 范围内的 25,000 个数字，我们将使用它们作为索引，选择一部分图像来创建 `validation` 数据集。
- en: 'We can create a validation code, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个验证代码，如下所示：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The preceding code creates a `validation` folder and creates folders based on
    categories (cats and dogs) inside `train` and `valid` directories.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码创建了一个 `validation` 文件夹，并在 `train` 和 `valid` 目录内基于类别（cats 和 dogs）创建文件夹。
- en: 'We can shuffle an index with the following code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码对索引进行洗牌：
- en: '[PRE15]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, we use our shuffled index to randomly pick `2000` different
    images for our validation set. We do something similar for the training data to
    segregate the images in the `train` directory.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们使用打乱的索引随机选择了`2000`张不同的图像作为验证集。我们对训练数据做类似的处理，以将图像分离到`train`目录中。
- en: As we have the data in the format we need, let's quickly look at how to load
    the images as PyTorch tensors.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据格式已经符合要求，让我们快速看一下如何将图像加载为 PyTorch 张量。
- en: Loading data into PyTorch tensors
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据加载到 PyTorch 张量中
- en: 'The PyTorch `torchvision.datasets` package provides a utility class called
    `ImageFolder` that can be used to load images along with their associated labels
    when data is presented in the aforementioned format. It is a common practice to
    perform the following preprocessing steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的`torchvision.datasets`包提供了一个名为`ImageFolder`的实用类，可用于加载图像及其相关标签，当数据以前述格式呈现时。通常进行以下预处理步骤是一种常见做法：
- en: Resize all the images to the same size. Most of the deep learning architectures
    expect the images to be of the same size.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有图像调整为相同大小。大多数深度学习架构都期望图像大小相同。
- en: Normalize the dataset with the mean and standard deviation of the dataset.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据集的平均值和标准差对数据进行标准化。
- en: Convert the image dataset to a PyTorch tensor.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像数据集转换为 PyTorch 张量。
- en: 'PyTorch makes a lot of these preprocessing steps easier by providing a lot
    of utility functions in the `transforms` module. For our example, let''s apply
    three transformations:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 通过在`transforms`模块中提供许多实用函数，使这些预处理步骤变得更加容易。对于我们的示例，让我们应用三种转换：
- en: Scale to a 256 x 256 image size
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放为 256 x 256 的图像尺寸
- en: Convert to a PyTorch tensor
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换为 PyTorch 张量
- en: Normalize the data (we will talk about how we arrived at the mean and standard
    deviation in[ Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep Learning
    for Computer Vision*)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化数据（我们将讨论如何在《深度学习计算机视觉》的第5章中得出平均值和标准差）
- en: 'The following code demonstrates how transformation can be applied and images
    are loaded using the `ImageFolder` class:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码演示了如何应用转换并使用`ImageFolder`类加载图像：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `train` object holds all the images and associated labels for the dataset.
    It contains two important attributes: one that gives a mapping between classes
    and the associated index used in the dataset and another one that gives a list
    of classes:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`对象包含数据集中所有图像及其关联标签。它包含两个重要属性：一个给出类别与数据集中使用的相关索引之间的映射，另一个给出类别列表：'
- en: '`train.class_to_idx - {''cat'': 0, ''dog'': 1}`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.class_to_idx - {''cat'': 0, ''dog'': 1}`'
- en: '`train.classes - [''cat'', ''dog'']`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.classes - [''cat'', ''dog'']`'
- en: 'It is often a best practice to visualize the data loaded into tensors. To visualize
    the tensors, we have to reshape the tensors and denormalize the values. The following
    function does that for us:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 将加载到张量中的数据可视化通常是最佳实践。为了可视化张量，我们必须重塑张量并对值进行反标准化。以下函数为我们执行这些操作：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we can pass our tensor to the preceding `imshow` function, which converts
    it into an image:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的张量传递给前面的`imshow`函数，该函数将其转换为图像：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code generates the following output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![](img/6fc5e3ac-369e-4a9f-bcbc-70395c8ed587.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fc5e3ac-369e-4a9f-bcbc-70395c8ed587.png)'
- en: Loading PyTorch tensors as batches
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 PyTorch 张量加载为批次
- en: 'It is a common practice in deep learning or machine learning to batch samples
    of images, as modern **graphics processing units** (**GPUs**) and CPUs are optimized
    to run operations faster on a batch of images. The batch size generally varies
    depending on the kind of GPU we use. Each GPU has its own memory, which can vary
    from 2 GB to 12 GB, and sometimes more for commercial GPUs. PyTorch provides the
    `DataLoader` class, which takes in a dataset and returns us a batch of images.
    It abstracts a lot of complexities in batching, such as the usage of multi-workers
    for applying transformation. The following code converts the previous `train`
    and `valid` datasets into data loaders:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习或机器学习中，将图像样本分批处理是常见做法，因为现代**图形处理单元**（**GPUs**）和CPU在批量图像上运行操作时被优化得更快。批量大小通常根据使用的GPU类型而变化。每个GPU都有自己的内存，从2
    GB到12 GB不等，商用GPU有时甚至更多。PyTorch 提供了`DataLoader`类，它接受一个数据集并返回一个图像批次。它抽象了批处理中的许多复杂性，如使用多个工作进程进行变换应用。以下代码将前述的`train`和`valid`数据集转换为数据加载器：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `DataLoader` class provides us with a lot of options and some of the most
    commonly used ones are as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataLoader`类为我们提供了许多选项，其中一些最常用的选项如下：'
- en: '`shuffle`: When true, this shuffles the images every time the data loader is
    called.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle`: 当为true时，每次调用数据加载器时都会对图像进行洗牌。'
- en: '`num_workers`: This is responsible for parallelization. It is common practice
    to use a number of workers fewer than the number of cores available in your machine.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_workers`: 这个参数负责并行化。通常的做法是使用比您机器上可用的核心数少的工作线程数。'
- en: Building the network architecture
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络架构
- en: 'For most of the real-world use cases, particularly in computer vision, we rarely
    build our own architecture. There are different architectures that can be quickly
    used to solve our real-world problems. For our example, we use a popular deep
    learning algorithm called **ResNet**, which won the first prize in 2015 in different
    competitions, such as ImageNet, related to computer vision. For a simpler understanding,
    let''s assume that this algorithm is a bunch of different PyTorch layers carefully
    tied together and not focus on what happens inside this algorithm. We will see
    some of the key building blocks of the ResNet algorithm in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml),
    *Deep Learning for Computer Vision*, when we learn about CNNs. PyTorch makes it
    easier to use a lot of these popular algorithms by providing them off the shelf
    in the `torchvision.models` module. So, for this example, let''s quickly take
    a look at how to use this algorithm and then walk through each line of code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数现实世界的用例，特别是在计算机视觉领域，我们很少自己构建架构。有不同的架构可以快速用于解决我们的现实世界问题。例如，我们使用一个称为**ResNet**的流行深度学习算法，该算法在2015年赢得了ImageNet等不同比赛的第一名。为了更简单地理解，我们可以假设这个算法是一堆不同的PyTorch层精心连接在一起，而不关注算法内部发生的事情。当我们学习CNN时，在《计算机视觉的深度学习》第5章中，我们将看到ResNet算法的一些关键构建块，详见[第5章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)。PyTorch通过在`torchvision.models`模块中提供这些流行算法，使得使用它们变得更加容易。因此，让我们快速看一下如何使用这个算法，然后逐行分析代码：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `models.resnet18(pertrained = True)` object creates an instance of the
    algorithm, which is a collection of PyTorch layers. We can take a quick look at
    what constitutes the ResNet algorithm by printing `model_ft`. A small portion
    of the algorithm looks like the following screenshot. I am not including the full
    algorithm as it could run for several pages:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`models.resnet18(pertrained = True)`对象创建了算法的一个实例，这是一组PyTorch层。我们可以通过打印`model_ft`来快速查看ResNet算法的构成。算法的一个小部分看起来像以下的屏幕截图。我没有包括完整的算法，因为可能需要运行数页：'
- en: '![](img/09088b50-3dab-4b63-996f-498a5c52bfe2.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09088b50-3dab-4b63-996f-498a5c52bfe2.png)'
- en: As we can see, the ResNet architecture is a collection of layers, namely `Conv2d`,
    `BatchNorm2d`, and `MaxPool2d`, stitched in a particular way. All these algorithms
    will accept an argument called **pretrained**. When `pretrained` is `True`, the
    weights of the algorithm are already tuned for a particular ImageNet classification
    problem of predicting 1,000 different categories, which include cars, ships, fish,
    cats, and dogs. This algorithm is trained to predict the 1,000 ImageNet categories
    and the weights are adjusted to a certain point where the algorithm achieves state-of-art
    accuracy. These weights are stored and shared with the model that we are using
    for the use case. Algorithms tend to work better when started with fine-tuned
    weights, rather than when started with random weights. So, for our use case, we
    start with pretrained weights.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，ResNet架构是一组层，即`Conv2d`、`BatchNorm2d`和`MaxPool2d`，以特定的方式连接在一起。所有这些算法都会接受一个名为**pretrained**的参数。当`pretrained`为`True`时，算法的权重已经针对预测ImageNet分类问题进行了调整，该问题涉及预测包括汽车、船、鱼、猫和狗在内的1000个不同类别。这些权重已经调整到一定程度，使得算法达到了最先进的准确性。这些权重被存储并与我们用于该用例的模型共享。与使用随机权重相比，算法在使用精调权重时往往表现更好。因此，对于我们的用例，我们从预训练权重开始。
- en: 'The ResNet algorithm cannot be used directly, as it is trained to predict one
    of the 1,000 categories. For our use case, we need to predict only one of the
    two categories of dogs and cats. To achieve this, we take the last layer of the
    ResNet model, which is a `linear` layer and change the output features to two,
    as shown in the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet算法不能直接使用，因为它是为了预测1000个类别之一而训练的。对于我们的用例，我们只需要预测狗和猫中的一种。为了实现这一点，我们取ResNet模型的最后一层，这是一个`linear`层，并将输出特征更改为两个，如下面的代码所示：
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you are running this algorithm on a GPU-based machine, then to make the
    algorithm run on a GPU we call the `cuda` method on the model. It is strongly
    recommended that you run these programs on a GPU-powered machine; it is easy to
    spin a cloud instance with a GPU for less than a dollar. The last line in the
    following code snippet tells PyTorch to run the code on the GPU:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在基于GPU的机器上运行此算法，那么为了使算法在GPU上运行，我们需要在模型上调用`cuda`方法。强烈建议您在支持GPU的机器上运行这些程序；在云端实例上租用一个带GPU的实例成本不到一美元。以下代码片段的最后一行告诉PyTorch在GPU上运行代码：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Training the model
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'In the previous sections, we have created `DataLoader` instances and algorithms.
    Now, let''s train the model. To do this we need a `loss` function and an `optimizer`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们创建了`DataLoader`实例和算法。现在，让我们来训练模型。为此，我们需要一个`loss`函数和一个`optimizer`：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding code, we created our `loss` function based on `CrossEntropyLoss`
    and the optimizer based on `SGD`. The `StepLR` function helps in dynamically changing
    the learning rate. We will discuss different strategies available to tune the
    learning rate in [Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals
    of Machine Learning*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们基于`CrossEntropyLoss`创建了我们的`loss`函数，并基于`SGD`创建了优化器。`StepLR`函数有助于动态调整学习率。我们将在《机器学习基础》第四章讨论可用于调整学习率的不同策略。
- en: 'The following `train_model` function takes in a model and tunes the weights
    of our algorithm by running multiple epochs and reducing the loss:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的`train_model`函数接受一个模型，并通过运行多个epochs和减少损失来调整我们算法的权重：
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding function does the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 前述函数执行以下操作：
- en: Passes the images through the model and calculates the loss.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像通过模型并计算损失。
- en: Backpropagates during the training phase. For the validation/testing phase,
    it does not adjust the weights.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练阶段进行反向传播。对于验证/测试阶段，不会调整权重。
- en: The loss is accumulated across batches for each epoch.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失是在每个epoch的各个批次中累积的。
- en: The best model is stored and validation accuracy is printed.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储最佳模型并打印验证准确率。
- en: 'The preceding model, after running for `25` epochs, results in a validation
    accuracy of 87%. The following is the log generated by the preceding `train_model`
    function when run on our `Dogs vs. Cats` dataset; I am just including the result
    of the last few epochs to save space in the book:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行了`25`个epochs后，前述模型的验证准确率达到了87%。以下是在我们的《猫狗大战》数据集上运行`train_model`函数时生成的日志；这里只包含了最后几个epochs的结果，以节省空间。
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the coming chapters, we will learn more advanced techniques that will help
    us in training more accurate models in a much faster way. The preceding model
    took around 30 minutes to run on a Titan X GPU. We will cover different techniques
    that will help in training the model faster.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习更高级的技术，帮助我们以更快的方式训练更精确的模型。前述模型在Titan X GPU上运行大约需要30分钟。我们将介绍不同的技术，这些技术有助于更快地训练模型。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the complete life cycle of a neural network in
    Pytorch, starting from constituting different types of layers, adding activations,
    calculating cross-entropy loss, and finally optimizing network performance (that
    is, minimizing loss), by adjusting the weights of layers using the SGD optimizer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在Pytorch中神经网络的完整生命周期，从构建不同类型的层、添加激活函数、计算交叉熵损失，最终通过SGD优化网络性能（即最小化损失），调整层权重。
- en: We have studied how to apply the popular ResNET architecture to binary or multi-class
    classification problems.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何将流行的ResNET架构应用于二元或多类别分类问题。
- en: While doing this, we have tried to solve the real-world image classification
    problem of classifying a cat image as a cat and a dog image as a dog. This knowledge
    can be applied to classify different categories/classes of entities, such as classifying
    species of fish, identifying different kinds of dogs, categorizing plant seedlings,
    grouping together cervical cancer into Type 1, Type 2, and Type 3, and much more.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们尝试解决真实世界的图像分类问题，如将猫图像分类为猫和狗图像分类为狗。这些知识可以应用于分类不同的实体类别/类别，例如分类鱼类的物种、识别不同品种的狗、分类植物种子苗、将宫颈癌分为类型1、类型2和类型3等等。
- en: In the next chapter, we will go through the fundamentals of machine learning.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨机器学习的基础知识。
