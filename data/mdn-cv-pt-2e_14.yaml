- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Autoencoders and Image Manipulation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器与图像操作
- en: 'In previous chapters, we learned about classifying images, detecting objects
    in an image, and segmenting the pixels corresponding to objects in images. In
    this chapter, we will learn about representing an image in a lower dimension using
    **autoencoders** and then leveraging the lower-dimensional representation of an
    image to generate new images by using **variational autoencoders**. Learning how
    to represent images in a lower number of dimensions helps us manipulate (modify)
    the images to a considerable degree. We will also learn about generating novel
    images that are based on the content and style of two different images. We will
    then explore how to modify images in such a way that the image is visually unaltered;
    however, the class corresponding to the image is changed from one to another when
    the image is passed through an image classification model. Finally, we will learn
    about generating deepfakes: given a source image of person A, we generate a target
    image of person B with a similar facial expression as that of person A.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们学习了如何对图像进行分类、检测图像中的对象以及分割与图像中的对象对应的像素。在本章中，我们将学习如何使用**自编码器**将图像表示为较低维度，然后利用图像的较低维度表示程度较高地操作（修改）图像。我们还将学习如何生成基于两幅不同图像的内容和风格生成新图像。然后，我们将探讨如何以不改变图像外观的方式修改图像，但在将图像通过图像分类模型时，将图像对应的类从一个类更改为另一个类。最后，我们将学习如何生成深度伪造：给定人物A的源图像，我们生成与人物A具有相似面部表情的人物B的目标图像。
- en: 'Overall, we will go through the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，在本章中我们将学习以下主题：
- en: Understanding and implementing autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和实现自编码器
- en: Understanding convolutional autoencoders
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积自编码器
- en: Understanding variational autoencoders
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解变分自编码器
- en: Performing an adversarial attack on images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像执行对抗攻击
- en: Performing neural style transfer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行神经风格迁移
- en: Generating deepfakes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成深度伪造
- en: All code snippets within this chapter are available in the `Chapter11` folder
    of the Github repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有代码片段均可在Github存储库的`Chapter11`文件夹中找到，链接为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Understanding autoencoders
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自编码器
- en: So far, in previous chapters, we have learned about classifying images by training
    a model based on the input image and its corresponding label. Now let’s imagine
    a scenario where we need to cluster images based on their similarity and with
    the constraint of not having their corresponding labels. Autoencoders come in
    handy for identifying and grouping similar images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在之前的章节中，我们已经学习了通过训练基于输入图像及其对应标签的模型来对图像进行分类。现在让我们想象一种情景，即我们需要根据它们的相似性对图像进行聚类，并且不具有它们对应的标签。自编码器在识别和分组相似图像方面非常有用。
- en: How autoencoders work
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器的工作原理
- en: An autoencoder takes an image as input, stores it in a lower dimension, and
    tries to reproduce the same image as output, hence the term **auto** (which, in
    short, means being able to reproduce the input). However, if we just reproduce
    the input in the output, we would not need a network, but a simple multiplication
    of the input by 1 would do. The differentiating aspect of an autoencoder from
    the typical neural network architectures we have learned about so far is that
    it encodes the information present in an image in a lower dimension and then reproduces
    the image, hence the term **encoder**. This way, images that are similar will
    have similar encoding. Further, the **decoder** works toward reconstructing the
    original image from the encoded vector.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器将图像作为输入，将其存储在较低维度中，并尝试生成相同的图像作为输出，因此术语**自**（简而言之，意味着能够重现输入）。然而，如果我们只是在输出中复制输入，那么我们不需要一个网络，只需简单地将输入乘以1即可。自编码器与我们迄今为止所学的典型神经网络架构的区别在于，它将图像中的信息编码到较低维度，然后再生成图像，因此称为**编码器**。这样，相似的图像将具有类似的编码。此外，**解码器**致力于从编码向量中重建原始图像。
- en: 'In order to further understand autoencoders, let’s take a look at the following
    diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解自编码器，让我们看一下以下图表：
- en: '![](img/B18457_11_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_01.png)'
- en: 'Figure 11.1: Typical autoencoder architecture'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：典型自编码器架构
- en: Let’s say the input image is a flattened version of the MNIST handwritten digits
    and the output image is the same as what is provided as input. The middlemost
    layer is the layer of encoding called the **bottleneck** layer. The operations
    happening between the input and the bottleneck layer represent the **encoder**
    and the operations between the bottleneck layer and output represent the **decoder**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输入图像是 MNIST 手写数字的扁平化版本，输出图像与输入相同。中间层是称为 **瓶颈** 层的编码层。在输入和瓶颈层之间发生的操作表示 **编码器**，在瓶颈层和输出之间的操作表示
    **解码器**。
- en: 'Through the bottleneck layer, we can represent an image in a much lower dimension.
    Furthermore, with the bottleneck layer, we can reconstruct the original image.
    We leverage the bottleneck layer to solve the problems of identifying similar
    images as well as generating new images, which we will learn how to do in subsequent
    sections. The bottleneck layer helps in the following ways:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过瓶颈层，我们可以将图像表示为更低维度。此外，借助瓶颈层，我们可以重构原始图像。我们利用瓶颈层来解决识别相似图像和生成新图像的问题，在接下来的部分中我们将学习如何做到这一点。瓶颈层在以下方面帮助：
- en: Images that have similar bottleneck layer values (encoded representations) are
    likely to be similar to each other.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有相似瓶颈层数值（编码表示）的图像可能彼此相似。
- en: By changing the node values of the bottleneck layer, we can change the output
    image.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过更改瓶颈层的节点值，我们可以改变输出图像。
- en: 'With the preceding understanding, let’s do the following in subsequent sections:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的理解基础上，让我们在接下来的部分执行以下操作：
- en: Implement autoencoders from scratch
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始实现自动编码器
- en: Visualize the similarity of images based on bottleneck-layer values
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据瓶颈层数值可视化图像的相似性
- en: In the next section, we will learn about how autoencoders are built and the
    impact of different units in the bottleneck layer on the decoder’s output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解如何构建自动编码器及瓶颈层中不同单元对解码器输出的影响。
- en: Implementing vanilla autoencoders
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基础自动编码器
- en: To understand how to build an autoencoder, let’s implement one on the MNIST
    dataset, which contains images of handwritten digits.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何构建自动编码器，让我们在 MNIST 数据集上实现一个：
- en: You’ll find the code in the `simple_auto_encoder_with_different_latent_size.ipynb`
    file in the `Chapter11` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的 GitHub 仓库的 `Chapter11` 文件夹中的 `simple_auto_encoder_with_different_latent_size.ipynb`
    文件中找到代码，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'You can follow these steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下步骤操作：
- en: 'Import the relevant packages and define the device:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并定义设备：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Specify the transformation that we want our images to pass through:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定我们希望图像通过的转换：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we see that we are converting an image into a tensor,
    normalizing it, and then passing it to the device.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们看到我们正在将图像转换为张量，对其进行归一化，然后将其传递到设备上。
- en: 'Create the training and validation datasets:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和验证数据集：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the dataloaders:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据加载器：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define the network architecture. We define the `AutoEncoder` class constituting
    the encoder and decoder in the `__init__` method, along with the dimension of
    the bottleneck layer, `latent_dim`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络架构。我们在 `__init__` 方法中定义 `AutoEncoder` 类，包括编码器和解码器以及瓶颈层 `latent_dim` 的维度：
- en: '[PRE4]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the `forward` method:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `forward` 方法：
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Visualize the preceding model:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化前述模型：
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This results in the following output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](img/B18457_11_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_02.png)'
- en: 'Figure 11.2: UNet architecture'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：UNet 架构
- en: 'From the preceding output, we can see that the `Linear: 2-5 layer` is the bottleneck
    layer, where each image is represented as a three-dimensional vector. Furthermore,
    the decoder layer reconstructs the original image using the three values in the
    bottleneck layer (`Linear: 2-5 layer`).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '从前述输出中，我们可以看到 `Linear: 2-5 layer` 是瓶颈层，每个图像表示为三维向量。此外，解码器层使用瓶颈层中的三个值 (`Linear:
    2-5 layer`) 重建原始图像。'
- en: 'Define a function named `train_batch` to train the model on a batch of data,
    just like we did in previous chapters:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义名为 `train_batch` 的函数，以便在批处理数据上训练模型，就像我们在前几章中所做的那样：
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the `validate_batch` function to validate the model on a batch of data:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `validate_batch` 函数以验证批处理数据上的模型：
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Define the model, loss criterion, and optimizer. Ensure we use `MSELoss` as
    we are reconstructing the pixel values.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型、损失标准和优化器。确保我们使用 `MSELoss` 因为我们正在重建像素值。
- en: '[PRE9]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Train the model over increasing epochs:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随着epoch的增加训练模型：
- en: '[PRE10]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Visualize the training and validation loss over increasing epochs:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化随着epoch增加的训练和验证损失：
- en: '[PRE11]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/B18457_11_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_03.png)'
- en: 'Figure 11.3: Training and validation loss over increasing epochs'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：随着epoch增加的训练和验证损失
- en: 'Validate the model on the `val_ds` dataset by looking at a few predictions/
    outputs, which were not provided during training:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查看`val_ds`数据集上未在训练期间提供的几个预测/输出来验证模型：
- en: '[PRE13]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![](img/B18457_11_04.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_04.png)'
- en: 'Figure 11.4: Autoencoder generated predictions/outputs'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：自编码器生成的预测/输出
- en: 'We can see that the network can reproduce input with a very high level of accuracy
    even though the bottleneck layer is only three dimensions in size. However, the
    images are not as clear as expected. This is primarily because of the small number
    of nodes in the bottleneck layer. In the following image, we will visualize the
    reconstructed images after training networks with different bottleneck layer sizes:
    2, 3, 5, 10, and 50:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，尽管瓶颈层仅有三个维度，网络仍能以非常高的准确度重现输入。然而，图像的清晰度并不如预期。这主要是由于瓶颈层中节点数量较少造成的。在接下来的图像中，我们将展示在不同瓶颈层大小（2、3、5、10和50）训练网络后的重建图像：
- en: '![](img/B18457_11_05.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_05.png)'
- en: 'Figure 11.5: Autoencoder generated predictions/outputs'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：自编码器生成的预测/输出
- en: It is clear that as the number of vectors in the bottleneck layer increased,
    the clarity of the reconstructed image improved.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随着瓶颈层中向量的数量增加，重建图像的清晰度得到了提高。
- en: In the next section, we will learn about generating clearer images using a **convolutional
    neural network** (**CNN**) and we will learn about grouping similar images.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习如何使用**卷积神经网络**（**CNN**）生成更清晰的图像，并了解如何将相似图像分组。
- en: Implementing convolutional autoencoders
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现卷积自编码器
- en: In the previous section, we learned about autoencoders and implemented them
    in PyTorch. While we have implemented them, one convenience that we had through
    the dataset was that each image had only one channel (each image was represented
    as a black and white image) and the images were relatively small (28 x 28 pixels).
    Hence, the network flattened the input and was able to train on 784 (28*28) input
    values to predict 784 output values. However, in reality, we will encounter images
    that have three channels and are much bigger than a 28 x 28 image.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了自编码器并在PyTorch中实现了它们。尽管我们已经实现了它们，但我们通过数据集的便利之处在于每个图像只有一个通道（每个图像表示为黑白图像），而且图像相对较小（28
    x 28像素）。因此，网络将输入展平，并能够在784（28*28）个输入值上进行训练，以预测784个输出值。然而，在现实中，我们将遇到具有三个通道且远大于28
    x 28像素的图像。
- en: In this section, we will learn about implementing a convolutional autoencoder
    that is able to work on multi-dimensional input images. However, for the purpose
    of comparison with vanilla autoencoders, we will work on the same MNIST dataset
    that we worked on in the previous section, but modify the network in such a way
    that we now build a convolutional autoencoder and not a vanilla autoencoder.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何实现一个能够处理多维输入图像的卷积自编码器。然而，为了与普通自编码器进行比较，我们将继续使用在前一节中使用过的MNIST数据集，但是修改网络结构，使其成为一个卷积自编码器而非普通自编码器。
- en: 'A convolutional autoencoder is represented as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积自编码器的表示如下：
- en: '![](img/B18457_11_06.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_06.png)'
- en: 'Figure 11.6: Convolutional autoencoder'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：卷积自编码器
- en: From the preceding image, we can see that the input image is represented as
    a block in the bottleneck layer that is used to reconstruct the image. The image
    goes through multiple convolutions to fetch the bottleneck representation (which
    is the **Bottleneck** layer that is obtained by passing through the **Encoder**)
    and the bottleneck representation is up-scaled to fetch the original image (the
    original image is reconstructed by passing through the **Decoder**). Note that,
    in a convolutional autoencoder, the number of channels in the bottleneck layer
    can be very high when compared to the input layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图像中，我们可以看到输入图像在用于重建图像的瓶颈层中表示为一个块。图像通过多次卷积来获取瓶颈表示（通过**编码器**获得的**瓶颈**层），并且通过上采样瓶颈表示来获取原始图像（原始图像通过**解码器**重建）。请注意，在卷积自编码器中，与输入层相比，瓶颈层中的通道数量可以非常高。
- en: 'Now that we know how a convolutional autoencoder is represented, let’s implement
    it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了卷积自编码器的表示方式，让我们来实现它：
- en: The following code is available as `conv_auto_encoder.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository at `https://bit.ly/mcvp-2e`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码可以在本书的GitHub代码库`Chapter11`文件夹中的`conv_auto_encoder.ipynb`中找到，网址为`https://bit.ly/mcvp-2e`。
- en: '*Steps* *1* to *4*, which are exactly the same as in the *Implementing vanilla
    autoencoders section*, are as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤* *1* 到 *4*，与*实现普通自编码器部分*完全相同，具体如下：'
- en: '[PRE14]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the class of neural network, `ConvAutoEncoder`, as follows:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络类`ConvAutoEncoder`如下：
- en: 'Define the class and the `__init__` method:'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义类和`__init__`方法：
- en: '[PRE15]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the `encoder` architecture:'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`encoder`架构：
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that in the preceding code, we started with the initial number of channels,
    which is `1`, and increased it to `32`, and then further increased it to `64`
    while reducing the size of the output values by performing `nn.MaxPool2d` and
    `nn.Conv2d` operations.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在前面的代码中，我们从初始通道数`1`开始，增加到`32`，然后进一步增加到`64`，同时通过执行`nn.MaxPool2d`和`nn.Conv2d`操作减少输出值的大小。
- en: 'Define the `decoder` architecture:'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`decoder`架构：
- en: '[PRE17]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the `forward` method:'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`forward`方法：
- en: '[PRE18]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Get the summary of the model using the `summary` method:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary`方法获取模型的摘要：
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code results in the following output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码结果如下：
- en: '![](img/B18457_11_07.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_07.png)'
- en: 'Figure 11.7: Summary of model architecture'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7：模型架构摘要
- en: From the preceding summary, we can see that the `MaxPool2d-6` layer with a shape
    of batch size x 64 x 2 x 2 acts as the bottleneck layer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的总结可以看出，形状为批大小 x 64 x 2 x 2 的`MaxPool2d-6`层充当了瓶颈层。
- en: 'Once we train the model, just like we did in the previous section (in *steps
    6, 7, 8, and 9*), the variation of training and validation loss over increasing
    epochs and the predictions on input images is as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练模型，就像我们在前一节中做的那样（在*步骤 6、7、8 和 9*中），随着增加的时期训练和验证损失的变化以及输入图像的预测如下：
- en: '![](img/B18457_11_08.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_08.png)'
- en: 'Figure 11.8: Variation of loss over epochs and sample predictions'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：随着时期的变化和样本预测的损失变化
- en: From the preceding image, we can see that a convolutional autoencoder is able
    to make much clearer predictions of the image than the vanilla autoencoder. As
    an exercise, we suggest you vary the number of channels in the encoder and decoder
    and then analyze the variation in results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像可以看出，卷积自编码器能够比普通自编码器更清晰地预测图像。作为练习，我们建议您在编码器和解码器中改变通道数量，然后分析结果的变化。
- en: In the next section, we will address the question of grouping similar images
    based on bottleneck-layer values when the labels of images are not present.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，当图像的标签不存在时，我们将讨论基于瓶颈层值分组相似图像的问题。
- en: Grouping similar images using t-SNE
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 t-SNE 对相似图像进行分组
- en: In the previous sections, we represented each image in a much lower dimension
    with the assumption that similar images will have similar embeddings, and images
    that are not similar will have dissimilar embeddings. However, we have not yet
    looked at the image similarity measure or examined embedding representations in
    detail.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们假设每个图像在更低的维度中表示，假设相似的图像将具有相似的嵌入，而不相似的图像将具有不相似的嵌入。然而，我们还没有详细查看图像相似性度量或嵌入表示。
- en: 'In this section, we will plot embedding (bottleneck) vectors in a two-dimensional
    space. We can reduce the 64-dimensional vector of a convolutional autoencoder
    to a two-dimensional space by using a technique called **t-SNE**, which helps
    in compressing information in such a way that similar data points are grouped
    together while dissimilar ones are grouped far away from each other. (More about
    t-SNE is available here: [http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html).)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在二维空间中绘制嵌入（瓶颈）向量。我们可以通过一种称为**t-SNE**的技术将卷积自编码器的64维向量压缩到二维空间中，该技术有助于以一种使得相似数据点聚集在一起而不相似数据点远离彼此的方式压缩信息。
    （关于 t-SNE 的更多信息请参见：[http://www.jmlr.org/papers/v9/vandermaaten08a.html](http://www.jmlr.org/papers/v9/vandermaaten08a.html)。）
- en: 'This way, our understanding that similar images will have similar embeddings
    can be proved, as similar images should be clustered together in the two-dimensional
    plane. We will represent embeddings of all the test images in a two-dimensional
    plane:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以验证类似图像将具有相似的嵌入，因为相似的图像应该在二维平面上聚集在一起。我们将在二维平面上表示所有测试图像的嵌入：
- en: The following code is available as `conv_auto_encoder.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书的GitHub存储库的`Chapter11`文件夹中的`conv_auto_encoder.ipynb`中可用，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Initialize lists so that we store the latent vectors (`latent_vectors`) and
    the corresponding `classes` of images (note that we store the class of each image
    only to verify if images of the same class, which are expected to have a very
    high similarity with each other, are indeed close to each other in terms of representation):'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化列表以存储潜在向量（`latent_vectors`）和图像的对应`classes`（请注意，我们仅存储每个图像的类别，以验证同一类别的图像是否确实在表示上彼此接近）：
- en: '[PRE20]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Loop through the images in the validation dataloader (`val_dl`) and store the
    output of the encoder layer `(model.encoder(im).view(len(im),-1)` and the class
    (`clss`) corresponding to each image (`im`):'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环遍历验证数据加载器中的图像（`val_dl`）并存储编码器层输出的结果（`model.encoder(im).view(len(im),-1)`）和每个图像（`im`）对应的类别（`clss`）：
- en: '[PRE21]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Concatenate the NumPy array of `latent_vectors`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接NumPy数组`latent_vectors`：
- en: '[PRE22]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Import t-SNE (`TSNE`) and specify that each vector is to be converted into
    a two-dimensional vector (`TSNE(2)`) so that we can plot it:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入t-SNE（`TSNE`）并指定将每个向量转换为二维向量（`TSNE(2)`），以便我们可以绘制它：
- en: '[PRE23]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit t-SNE by running the `fit_transform` method on image embeddings (`latent_vectors`):'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`fit_transform`方法来拟合t-SNE到图像嵌入（`latent_vectors`）：
- en: '[PRE24]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Plot the data points after fitting t-SNE:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制t-SNE拟合后的数据点：
- en: '[PRE25]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code provides the following output (you can refer to the digital
    version of the book for the colored image):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下输出（你可以参考书的数字版以查看彩色图片）：
- en: '![](img/B18457_11_09.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_09.png)'
- en: 'Figure 11.9: Data points (images) grouped using t-SNE'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：使用t-SNE分组的数据点（图像）
- en: We can see that images belonging to the same class are clustered together, which
    reinforces our understanding that the bottleneck layer has values in such a way
    that images that look similar will have similar values.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到同一类别的图像被聚集在一起，这进一步证实了瓶颈层的值以这样的方式排列，即看起来相似的图像将具有相似的值。
- en: So far, we have learned about using autoencoders to group similar images together.
    In the next section, we will learn about using autoencoders to generate new images.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了如何使用自编码器将相似图像分组在一起。在下一节中，我们将学习如何使用自编码器生成新图像。
- en: Understanding variational autoencoders
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解变分自编码器
- en: So far, we have seen a scenario where we can group similar images into clusters.
    Furthermore, we have learned that when we take embeddings of images that fall
    in a given cluster, we can re-construct (decode) them. However, what if an embedding
    (a latent vector) falls in between two clusters? There is no guarantee that we
    would generate realistic images. **Variational autoencoders** (**VAEs**) come
    in handy in such a scenario.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一种情况，可以将相似的图像分组到集群中。此外，我们学习到当我们获取落在给定集群中的图像的嵌入时，我们可以重新构建（解码）它们。但是，如果一个嵌入（潜在向量）位于两个集群之间，我们不能保证生成逼真的图像。**变分自编码器**（**VAEs**）在这种情况下非常有用。
- en: The need for VAEs
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAE的必要性
- en: 'Before we dive into understanding and building a VAE, let’s explore the limitations
    of generating images from embeddings that do not fall into a cluster (or in the
    middle of different clusters). First, we generate images by sampling vectors by
    following these steps (available in the `conv_auto_encoder.ipynb` file):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入理解和构建VAE之前，让我们探讨从嵌入中生成图像的局限性，这些图像不属于任何一个集群（或位于不同集群中心）。首先，我们通过以下步骤从采样向量生成图像（可在`conv_auto_encoder.ipynb`文件中找到）：
- en: 'Calculate the latent vectors (embeddings) of the validation images used in
    the previous section:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在前一节中使用的验证图像的潜在向量（嵌入）：
- en: '[PRE26]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Generate random vectors with a column-level mean (`mu`) and a standard deviation
    (`sigma`) and add slight noise to the standard deviation (`torch.randn(1,100)`)
    before creating a vector from the mean and standard deviation. Finally, save them
    in a list (`rand_vectors`):'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列级均值(`mu`)和标准差(`sigma`)生成随机向量，并在创建来自均值和标准差的向量之前，给标准差添加轻微噪声（`torch.randn(1,100)`）。最后，将它们保存在列表中（`rand_vectors`）：
- en: '[PRE27]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the images reconstructed from the vectors obtained in *step 2* and the
    convolutional autoencoder model trained in the previous section:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制从*步骤2*中获取的向量重建的图像和在前一节中训练的卷积自编码器模型：
- en: '[PRE28]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding code results in the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/B18457_11_10.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_10.png)'
- en: 'Figure 11.10: Images generated from the mean and standard deviation of latent
    vectors'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：从潜在向量的均值和标准差生成的图像
- en: We can see from the preceding output that when we plot images that were generated
    from the mean and the noise-added standard deviation of columns of known vectors,
    we get images that are less clear than before. This is a realistic scenario, as
    we would not know beforehand about the range of embedding vectors that would generate
    realistic pictures.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出中我们可以看到，当我们绘制从已知向量的列的均值和加噪标准差生成的图像时，得到的图像不如之前清晰。这是一个现实的情况，因为我们事先不知道会生成逼真图片的嵌入向量的范围。
- en: '**VAEs** help us resolve this problem by generating vectors that have a mean
    of 0 and a standard deviation of 1, thereby ensuring that we generate images that
    have a mean of 0 and a standard deviation of 1.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**VAEs**通过生成具有均值为0和标准差为1的向量来帮助我们解决这个问题，从而确保我们生成的图像具有均值为0和标准差为1。'
- en: In essence, in a VAE, we are specifying that the bottleneck layer should follow
    a certain distribution. In the next sections, we will learn about the strategy
    we adopt with VAEs, and we will also learn about **Kullback-Leibler** (**KL**)
    divergence loss, which helps us fetch bottleneck features that follow a certain
    distribution.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，在VAE中，我们指定瓶颈层应遵循某种分布。在接下来的章节中，我们将学习我们在VAE中采用的策略，还将学习有助于获取遵循特定分布的瓶颈特征的**Kullback-Leibler**（**KL**）散度损失。
- en: How VAEs work
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VAE的工作原理
- en: 'In a VAE, we are building the network in such a way that a random vector that
    is generated from a pre-defined distribution can generate a realistic image. This
    was not possible with a simple autoencoder, as we did not specify the distribution
    of data that generates an image in the network. We enable that with a VAE by adopting
    the following strategy:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在VAE中，我们通过以下方式构建网络，使得从预定义分布生成的随机向量能够生成逼真的图像。简单的自编码器无法做到这一点，因为我们没有在网络中指定生成图像的数据分布。我们通过VAE采用以下策略来实现这一点：
- en: 'The output of the encoder is two vectors for each image:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器的输出是每个图像的两个向量：
- en: One vector represents the mean.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个向量代表均值。
- en: The other represents the standard deviation.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个表示标准差。
- en: From these two vectors, we fetch a modified vector that is the sum of the mean
    and standard deviation (which is multiplied by a random small number). The modified
    vector will be of the same number of dimensions as each vector.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这两个向量中，我们获取一个修改后的向量，该向量是均值和标准差之和（乘以一个随机小数）。修改后的向量将与每个向量具有相同数量的维度。
- en: The modified vector obtained in the previous step is passed as input to the
    decoder to fetch the image.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将上一步得到的修改后的向量作为输入传递给解码器以获取图像。
- en: 'The loss value that we optimize for is a combination of the following:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们优化的损失值是以下几种组合：
- en: 'KL divergence loss: Measures the deviation of the distribution of the mean
    vector and the standard deviation vector from 0 and 1, respectively'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: KL散度损失：衡量均值向量和标准差向量的分布与0和1的偏差，分别为
- en: 'Mean squared loss: Is the optimization we use to re-construct (decode) an image'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方损失：是我们用来重构（解码）图像的优化方法
- en: By specifying that the mean vector should have a distribution centered around
    0 and the standard deviation vector should be centered around 1, we are training
    the network in such a way that when we generate random noise with a mean of 0
    and standard deviation of 1, the decoder will be able to generate a realistic
    image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定均值向量应该具有围绕0中心的分布，标准差向量应该围绕1中心的分布，我们训练网络的方式是，当我们生成均值为0、标准差为1的随机噪声时，解码器能够生成逼真的图像。
- en: Further, note that had we only minimized KL divergence, the encoder would have
    predicted a value of 0 for the mean vector and a standard deviation of 1 for every
    input. Thus, it is important to minimize KL divergence loss and mean squared loss
    together.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，如果我们仅最小化KL散度，编码器将预测均值向量为0，并且每个输入的标准差为1。因此，同时最小化KL散度损失和均方损失是重要的。
- en: In the next section, let’s learn about KL divergence so that we can incorporate
    it into the model’s loss value calculation.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们学习一下KL散度，以便我们可以将其纳入模型损失值的计算中。
- en: KL divergence
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KL散度
- en: KL divergence helps explain the difference between two distributions of data.
    In our specific case, we want our bottleneck feature values to follow a normal
    distribution with a mean of 0 and a standard deviation of 1.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度有助于解释数据两个分布之间的差异。在我们的具体案例中，我们希望我们的瓶颈特征值遵循均值为0、标准差为1的正态分布。
- en: Thus, we use KL divergence loss to understand how different our bottleneck feature
    values are with respect to the expected distribution of values having a mean of
    0 and a standard deviation of 1.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用KL散度损失来理解我们的瓶颈特征值与期望的值分布有多不同，期望的分布具有均值为0和标准差为1。
- en: 'Let’s take a look at how KL divergence loss helps by going through how it is
    calculated:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过详细说明如何计算KL散度损失来看看KL散度损失如何帮助：
- en: '![](img/B18457_11_001.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_001.png)'
- en: In the preceding equation, σ and μ stand for the mean and standard deviation
    values, respectively, of each input image.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，σ和μ分别表示每个输入图像的均值和标准差值。
- en: 'Let’s discuss the preceding equation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论前述方程：
- en: 'Ensure that the mean vector is distributed around 0:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保均值向量分布在0附近：
- en: Minimizing the mean squared error (![](img/B18457_11_002.png)) in the preceding
    equation ensures that ![](img/B18457_11_003.png) is as close to 0 as possible.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化均方误差（![](img/B18457_11_002.png)）在前述方程中确保![](img/B18457_11_003.png)尽可能接近0。
- en: 'Ensure that the standard deviation vector is distributed around 1:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保标准差向量分布在1附近：
- en: The terms in the rest of the equation (except ![](img/B18457_11_004.png)) ensure
    that sigma (the standard deviation vector) is distributed around 1.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的其余项（除了![](img/B18457_11_004.png)）确保σ（标准差向量）分布在1附近。
- en: The preceding loss function is minimized when the mean (µ) is 0 and the standard
    deviation is 1\. Further, by specifying that we are considering the logarithm
    of standard deviation, we are ensuring that sigma values cannot be negative.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当均值（µ）为0且标准差为1时，前述损失函数被最小化。此外，通过指定我们正在考虑标准差的对数，我们确保σ值不能为负。
- en: Now that we understand the high-level strategy of building a VAE and the loss
    function to minimize in order to obtain a pre-defined distribution of encoder
    output, let’s implement a VAE in the next section.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了构建VAE的高级策略以及要最小化的损失函数，以获得编码器输出的预定义分布后，让我们在下一节中实现VAE。
- en: Building a VAE
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建VAE
- en: In this section, we will code up a VAE to generate new images of handwritten
    digits.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写一个VAE来生成手写数字的新图像。
- en: 'The following code is available as `VAE.ipynb` in the `Chapter11` folder of
    this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书GitHub库的`Chapter11`文件夹中的`VAE.ipynb`中可用：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Since we have the same data, all the steps in the *Implementing vanilla autoencoders*
    section remain the same except *steps 5 and 6*, where we define the network architecture
    and train the model respectively. Instead, we define these differently in the
    following code (available in the `VAE.ipynb` file):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们具有相同的数据，*实现基本自动编码器*部分中的所有步骤仍然相同，除了*步骤5和6*，在其中我们分别定义网络架构和训练模型。相反，我们在以下代码中以不同方式定义它们（在`VAE.ipynb`文件中可用）：
- en: '*Step 1* to *step 4*, which are exactly the same as in the *Implementing vanilla
    autoencoders* section, are as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤1* 到 *步骤4*，与*实现基本自动编码器*部分完全相同，具体如下：'
- en: '[PRE29]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the neural network class, `VAE`, as follows:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络类`VAE`如下：
- en: 'Define the layers in the `__init__` method that will be used in the other methods:'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`__init__`方法中定义将在其他方法中使用的层：
- en: '[PRE30]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that the `d1` and `d2` layers will correspond to the encoder section, and
    `d5` and `d6` will correspond to the decoder section. The `d31` and `d32` layers
    are the layers that correspond to the mean and standard deviation vectors, respectively.
    However, for convenience, one assumption we will make is that we will use the
    `d32` layer as a representation of the log of the variance vectors.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意 `d1` 和 `d2` 层对应编码器部分，而 `d5` 和 `d6` 对应解码器部分。`d31` 和 `d32` 层分别对应均值向量和标准差向量。然而，为了方便起见，我们假设将使用
    `d32` 层作为对数方差向量的表示。
- en: 'Define the `encoder` method:'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义编码器方法：
- en: '[PRE31]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that the encoder returns two vectors: one vector for the mean `(self.d31(h))`
    and the other for the log of variance values `(self.d32(h))`.'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意编码器返回两个向量：一个用于均值 `(self.d31(h))`，另一个用于对数方差值 `(self.d32(h))`。
- en: 'Define the method to sample (`sampling`) from the encoder’s outputs:'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义从编码器输出中进行采样的方法 (`sampling`)：
- en: '[PRE32]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that the exponential of *0.5*log_var* (`torch.exp(0.5*log_var)`) represents
    the standard deviation (`std`). Also, we are returning the addition of the mean
    and the standard deviation multiplied by noise generated by a random normal distribution.
    By multiplying by `eps`, we ensure that even with a slight change in the encoder
    vector, we can generate an image.
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意 *0.5*log_var* (`torch.exp(0.5*log_var)`) 的指数代表标准差 (`std`)。此外，我们通过随机正态分布生成的噪声乘以均值和标准差的加法来返回值。通过乘以
    `eps`，我们确保即使在编码器向量轻微变化时，也能生成图像。
- en: 'Define the `decoder` method:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义解码器方法：
- en: '[PRE33]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the `forward` method:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义前向方法：
- en: '[PRE34]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the preceding method, we are ensuring that the encoder returns the mean and
    log of the variance values. Next, we are sampling with the addition of mean with
    epsilon multiplied by the log of the variance and returning the values after passing
    through the decoder.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方法中，我们确保编码器返回均值和对数方差值。接下来，我们通过均值加上乘以对数方差的epsilon进行采样，并通过解码器传递后返回值。
- en: 'Define functions to train the model on a batch and validate it on a different
    batch:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于对一个批次模型进行训练和另一个批次进行验证的函数：
- en: '[PRE35]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the loss function:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数：
- en: '[PRE36]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding code, we are fetching the MSE loss (`RECON`) between the original
    image (`x`) and the reconstructed image (`recon_x`). Next, we are calculating
    the KL divergence loss (`KLD`) based on the formula we defined in the previous
    section. Note that the exponential of the log of the variance is the variance
    value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们获取原始图像 (`x`) 和重构图像 (`recon_x`) 之间的MSE损失 (`RECON`)。接下来，根据我们在前一节定义的公式计算KL散度损失
    (`KLD`)。注意对数方差的指数是方差值。
- en: 'Define the model object (`vae`) and the `optimizer` function:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型对象 (`vae`) 和优化器函数 (`optimizer`)：
- en: '[PRE37]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Train the model over increasing epochs:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加epoch的过程中训练模型：
- en: '[PRE38]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: While the majority of the preceding code is familiar, let’s go over the grid
    image generation process. We are first generating a random vector (`z`) and passing
    it through the decoder (`vae.decoder`) to fetch a sample of images. The `make_grid`
    function plots images (and denormalizes them automatically, if required, before
    plotting).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数前述代码是熟悉的，让我们看一下网格图像生成过程。我们首先生成一个随机向量 (`z`)，然后通过解码器 (`vae.decoder`) 获取图像样本。`make_grid`
    函数会绘制图像（如果需要，会自动进行反归一化）。
- en: 'The output of loss value variations and a sample of images generated is as
    follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值变化的输出和生成图像的样本如下：
- en: '![](img/B18457_11_11.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_11.png)'
- en: 'Figure 11.11: (Left): Variation in loss over increasing epochs. (Right): Images
    generated using VAE'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11：（左）随epoch增加损失的变化。（右）使用VAE生成的图像
- en: We can see that we are able to generate realistic new images that were not present
    in the original image.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们能够生成原始图像中不存在的逼真新图像。
- en: So far, we have learned about generating new images using VAEs. However, what
    if we want to modify images in such a way that a model cannot identify the right
    class? We will learn about the technique leveraged to achieve this in the next
    section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了使用VAE生成新图像的方法。但是，如果我们希望修改图像以使模型无法识别正确的类别，我们将在下一节中学习用于实现此目的的技术。
- en: Performing an adversarial attack on images
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对图像执行对抗攻击：
- en: In the previous section, we learned about generating an image from random noise
    using a VAE. However, it was an unsupervised exercise. What if we want to modify
    an image in such a way that the change is so minimal that it is indistinguishable
    from the original image for a human, but still the neural network model perceives
    the object as belonging to a different class? Adversarial attacks on images come
    in handy in such a scenario.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了使用VAE从随机噪声生成图像。但这是一个无监督的练习。如果我们想以如此微小的方式修改图像，以至于对于人类来说与原始图像几乎无法区分，但神经网络模型仍然会将其识别为属于不同类别的对象，那么对图像进行的对抗攻击就非常有用。
- en: 'Adversarial attacks refer to the changes that we make to input image values
    (pixels) so that we meet a certain objective. This is especially helpful in making
    our models robust so that they are not fooled by minor modifications. In this
    section, we will learn about modifying an image slightly in such a way that the
    pre-trained models now predict them as a different class (specified by the user)
    and not the original class. The strategy we will adopt is as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击是指我们对输入图像值（像素）进行的更改，以便达到特定目标。这在使我们的模型更加健壮以免受轻微修改影响方面特别有帮助。在本节中，我们将学习如何轻微修改图像，使预训练模型将其预测为不同类别（由用户指定），而不是原始类别。我们将采用的策略如下：
- en: Provide an image of an elephant.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一张大象的图像。
- en: Specify the target class corresponding to the image.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定与图像相对应的目标类别。
- en: Import a pre-trained model where the parameters of the model are set so that
    they are not updated (`gradients = False`).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入预训练模型，其中模型参数设置为不更新（`gradients = False`）。
- en: Specify that we calculate gradients on input image pixel values and not on the
    weight values of the network. This is because while training to fool a network,
    we do not have control over the model, but have control only over the image we
    send to the model.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定我们在输入图像像素值上计算梯度，而不是网络权重值。这是因为在训练中欺骗网络时，我们无法控制模型，只能控制发送到模型的图像。
- en: Calculate the loss corresponding to the model predictions and the target class.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算与模型预测和目标类别对应的损失。
- en: Perform backpropagation on the model. This step helps us understand the gradient
    associated with each input pixel value.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型进行反向传播。这一步骤帮助我们理解每个输入像素值相关的梯度。
- en: Update the input image pixel values based on the gradient corresponding to each
    input pixel value.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据每个输入像素值对应的梯度更新输入图像像素值。
- en: Repeat *steps 5, 6,* and *7* over multiple epochs.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多个epochs中重复*步骤5、6和7*。
- en: 'Let’s do this with code:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用代码来做这件事：
- en: 'The following code is available as `adversarial_attack.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from. We strongly recommend you execute
    the notebook in GitHub to reproduce the results and understand the steps to perform
    and the explanation of various code components in the text.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的GitHub存储库中的`Chapter11`文件夹中提供了`adversarial_attack.ipynb`代码文件：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。该代码包含用于下载数据的URL。我们强烈建议您在GitHub上执行该笔记本，以重现结果并理解执行步骤以及文本中各种代码组件的解释。
- en: 'Import the relevant packages, the image that we work on for this use case,
    and the pre-trained ResNet50 model. Also, specify that we want to freeze parameters
    as we are not updating the model (but updating the image so that the image fools
    the model):'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包、我们用于此用例的图像以及预训练的ResNet50模型。同时，指定我们要冻结参数，因为我们不会更新模型（但会更新图像以使其欺骗模型）：
- en: '[PRE39]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Import `image_net_classes` and assign IDs to each class:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`image_net_classes`并为每个类分配ID：
- en: '[PRE40]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Specify a function to normalize (`image2tensor`) and denormalize (`tensor2image`)
    the image:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个函数来对图像进行归一化（`image2tensor`）和反归一化（`tensor2image`）：
- en: '[PRE41]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define a function to predict the class of a given image (`predict_on_image`):'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来预测给定图像的类别（`predict_on_image`）：
- en: '[PRE42]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the preceding code, we are converting an input image into a tensor (which
    is a function to normalize using the `image2tensor` method defined earlier) and
    passing through a `model` to fetch the class (`clss`) of the object in the image
    and the probability (`prob`) of prediction.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们正在将输入图像转换为张量（使用先前定义的`image2tensor`方法进行归一化）并通过`model`获取图像中对象的类别（`clss`）和预测的概率（`prob`）。
- en: 'Define the `attack` function as follows:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`attack`函数如下：
- en: 'The `attack` function takes `image`, `model`, and `target` as input:'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`attack`函数接受`image`、`model`和`target`作为输入：'
- en: '[PRE43]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Convert the image into a tensor and specify that the input requires gradients
    to be calculated:'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为张量，并指定需要计算梯度的输入：
- en: '[PRE44]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Calculate the prediction by passing the normalized input (`input`) through
    the model, and then calculate the loss value corresponding to the specified target
    class:'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将规范化的输入(`input`)通过模型计算预测，然后计算相应目标类别的损失值：
- en: '[PRE45]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Perform backpropagation to reduce the loss:'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行反向传播以减少损失：
- en: '[PRE46]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Update the image very slightly based on the gradient direction:'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于梯度方向微调图像：
- en: '[PRE47]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, we are updating input values by a very small amount (multiplying
    by `epsilon`). Also, we are not updating the image by the magnitude of the gradient,
    but the direction of the gradient only (`input.grad.sign()`) after multiplying
    it by a very small value (`epsilon`).
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前述代码中，我们通过一个非常小的量（乘以`epsilon`）更新输入值。此外，我们仅仅通过梯度的方向（`input.grad.sign()`）进行更新图像，而不是梯度的大小，而且在此之前乘以了一个非常小的值（`epsilon`）。
- en: 'Return the output after converting the tensor into an image (`tensor2image`),
    which denormalizes the image:'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将张量转换为图像（`tensor2image`）并返回输出，这会使图像反标准化：
- en: '[PRE48]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Modify the image to belong to a different class:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改图像以属于不同的类别：
- en: 'Specify the targets (`desired_targets`) that we want to convert the image to:'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定我们想要将图像转换为的目标（`desired_targets`）：
- en: '[PRE49]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Loop through the targets and specify the target class in each iteration:'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环遍历目标，并在每次迭代中指定目标类别：
- en: '[PRE50]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Modify the image to attack over increasing epochs and collect them in a list:'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改图像以攻击逐步增加的时期，并将它们收集到一个列表中：
- en: '[PRE51]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following code results in modified images and the corresponding classes:'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码会导致修改后的图像和相应的类别：
- en: '[PRE52]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The preceding code generates the following:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成如下内容：
- en: '![](img/B18457_11_12.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_12.png)'
- en: 'Figure 11.12: Modified image and its corresponding class'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12：修改后的图像及其对应的类别
- en: We can see that as we modify the image very slightly, the prediction class is
    completely different but with very high confidence.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当我们轻微地修改图像时，预测类别完全不同，但置信度非常高。
- en: Now that we understand how to modify images so that they are classed as we wish,
    in the next section, we will learn about modifying an image (a content image)
    in the style of our choice.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何修改图像，使其成为我们希望的类别，接下来的部分中，我们将学习如何修改图像（内容图像）以我们选择的风格。
- en: Understanding neural style transfer
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经风格转移
- en: Imagine a scenario where you want to draw an image in the style of Van Gogh.
    Neural style transfer comes in handy in such a scenario. In neural style transfer,
    we use a content image and a style image, and we combine these two images in such
    a way that the combined image preserves the content of the content image while
    maintaining the style of the style image.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，你想以梵高的风格绘制一幅图像。在这种情况下，神经风格转移非常有用。在神经风格转移中，我们使用一个内容图像和一个风格图像，将这两个图像以一种方式结合起来，使得合成图像保留内容图像的内容，同时保持风格图像的风格。
- en: How neural style transfer works
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解神经风格转移的工作原理
- en: 'An example style image and content image are as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 示例风格图像和内容图像如下：
- en: '![](img/B18457_11_13.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_13.png)'
- en: 'Figure 11.13: (Left) Style image. (Right) Content image'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13：（左）风格图像。 （右）内容图像
- en: We want to retain the content in the picture on the right (the content image),
    but overlay it with the color and texture in the picture on the left (the style
    image).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望保留右侧图片（内容图像）中的内容，但同时叠加左侧图片（风格图像）中的颜色和纹理。
- en: 'The process of performing neural style transfer is as follows (we’ll go through
    the technique outlined in this paper: [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)).
    We try to modify the original image in a way that the loss value is split into
    **content loss** and **style loss**. Content loss refers to how **different**
    the generated image is from the content image. Style loss refers to how **correlated**
    the style image is to the generated image.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 执行神经风格转移的过程如下（我们将遵循此论文中概述的技术：[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)）。我们试图修改原始图像，使得损失值分解为**内容损失**和**风格损失**。内容损失指生成图像与内容图像有多**不同**。风格损失指风格图像与生成图像有多**相关**。
- en: While we mentioned that the loss is calculated based on the difference in images,
    in practice, we modify it slightly by ensuring that the loss is calculated using
    the feature layer activations of images and not the original images. For example,
    the content loss at layer 2 will be the squared difference between the *activations
    of the content image and the generated image* when passed through the second layer.
    This is because the feature layers capture certain attributes of the original
    image (for example, the outline of the foreground corresponding to the original
    image in the higher layers and the details of fine-grained objects in the lower
    layers).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们提到损失是基于图像差异计算的，但在实践中，我们稍微修改了它，确保使用图像的特征层激活而不是原始图像计算损失。例如，在第二层的内容损失将是通过第二层传递时的*内容图像和生成图像的激活*之间的平方差。这是因为特征层捕获原始图像的某些属性（例如，高层中对应于原始图像的前景轮廓以及低层中细粒度对象的细节）。
- en: 'While calculating the content loss seems straightforward, let’s try to understand
    how to calculate the similarity between the generated image and the style image.
    A technique called **gram matrix** comes in handy. The gram matrix calculates
    the similarity between a generated image and a style image, and is calculated
    as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算内容损失似乎很简单，让我们尝试理解如何计算生成图像与样式图像之间的相似性。一个称为**格拉姆矩阵**的技术非常方便。格拉姆矩阵计算生成图像和样式图像之间的相似性，计算方法如下：
- en: '![](img/B18457_11_005.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_005.png)'
- en: '*GM**l* is the gram matrix value at layer *l* for the style image, *S*, and
    the generated image, *G*. *N*[l]stands for the number of feature maps, where *M*[l]
    is the height times the width of the feature map.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '*GM**l* 是层*l*的样式图像*S*和生成图像*G*的格拉姆矩阵值。*N*[l]代表特征图的数量，其中*M*[l]是特征图的高度乘以宽度。'
- en: A gram matrix results from multiplying a matrix by the transpose of itself.
    Let’s discuss how this operation is used. Imagine that you are working on a layer
    that has a feature output of 32 x 32 x 256\. The gram matrix is calculated as
    the correlation of each of the 32 x 32 values in a channel with respect to the
    values across all channels. Thus, the gram matrix calculation results in a matrix
    that is 256 x 256 in shape. We now compare the 256 x 256 values of the style image
    and the generated image to calculate the style loss.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将矩阵乘以其自身的转置得到了格拉姆矩阵。让我们讨论这个操作是如何使用的。假设您正在处理一个具有32 x 32 x 256的特征输出层。格拉姆矩阵被计算为通道内每个32
    x 32值与所有通道中值的相关性。因此，格拉姆矩阵计算结果为256 x 256的矩阵形状。现在我们比较样式图像和生成图像的256 x 256值，以计算样式损失。
- en: Let’s understand why the gram matrix is important for style transfer. In a successful
    scenario, say we transferred Picasso’s style to the Mona Lisa. Let’s call the
    Picasso style *St* (for style), the original Mona Lisa *So* (for source), and
    the final image *Ta* (for target). Note that in an ideal scenario, the local features
    in image *Ta* are the same as the local features in *St*. Even though the content
    might not be the same, getting similar colors, shapes, and textures as the style
    image into the target image is what is important in style transfer.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解为什么格拉姆矩阵对于样式转移如此重要。在成功的情况下，假设我们将毕加索的风格转移到了蒙娜丽莎上。让我们称毕加索的风格为*St*（代表样式），原始蒙娜丽莎为*So*（代表源），最终图像为*Ta*（代表目标）。请注意，在理想情况下，图像*Ta*中的局部特征与*St*中的局部特征相同。即使内容可能不同，将样式图像的类似颜色、形状和纹理带入目标图像中是样式转移中的重要部分。
- en: By extension, if we were to send *So* and extract its features from an intermediate
    layer of VGG19, they would vary from the features obtained by sending *Ta*. However,
    within each feature set, the corresponding vectors will vary relative to each
    other in a similar fashion. Say, for example, the ratio of the mean of the first
    channel to the mean of the second channel if both the feature sets will be similar.
    This is why we are trying to compute using the gram loss.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展开来，如果我们发送*So*并从VGG19的中间层提取其特征，它们将与通过发送*Ta*获得的特征不同。然而，在每个特征集内，相应向量将以类似的方式相对于彼此变化。例如，如果两个特征集的第一个通道均值与第二个通道均值的比率将是相似的。这就是我们尝试使用格拉姆损失计算的原因。
- en: Content loss is calculated by comparing the difference in feature activations
    of the content image with respect to the generated image. Style loss is calculated
    by first calculating the gram matrix in the pre-defined layers and then comparing
    the gram matrices of the generated image and the style image.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较内容图像的特征激活的差异来计算内容损失。通过首先在预定义层中计算格拉姆矩阵，然后比较生成图像和样式图像的格拉姆矩阵来计算样式损失。
- en: Now that we are able to calculate the style loss and the content loss, the final
    modified input image is the image that minimizes the overall loss, that is, a
    weighted average of the style and content loss.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们能够计算样式损失和内容损失，最终修改的输入图像是最小化总损失的图像，即样式损失和内容损失的加权平均。
- en: Performing neural style transfer
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行神经风格迁移
- en: 'The high-level strategy we adopt to implement neural style transfer is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用的实施神经风格迁移的高层策略如下：
- en: Pass the input image through a pre-trained model.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入图像通过预训练模型传递。
- en: Extract the layer values at pre-defined layers.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取预定义层的层值。
- en: Pass the generated image through the model and extract its values at the same
    pre-defined layers.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像通过模型并在同一预定义层中提取其值。
- en: Calculate the content loss at each layer corresponding to the content image
    and generated image.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算与内容图像和生成图像相对应的每个层的内容损失。
- en: Pass the style image through multiple layers of the model and calculate the
    gram matrix values of the style image.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将样式图像通过模型的多层并计算样式图像的格拉姆矩阵值。
- en: Pass the generated image through the same layers that the style image is passed
    through and calculate its corresponding gram matrix values.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生成的图像通过样式图像传递的相同层，并计算其相应的格拉姆矩阵值。
- en: Extract the squared difference of the gram matrix values of the two images.
    This will be the style loss.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取两个图像的格拉姆矩阵值的平方差。这将是样式损失。
- en: The overall loss will be the weighted average of the style loss and content
    loss.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总损失将是样式损失和内容损失的加权平均。
- en: The generated image that minimizes the overall loss will be the final image
    of interest.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使总损失最小化的生成图像将是感兴趣的最终图像。
- en: 'Let’s now code up the preceding strategy:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写前面的策略代码：
- en: 'The following code is available as `neural_style_transfer.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    understand the steps to perform and the explanation of various code components
    in the text.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本书GitHub存储库的`Chapter11`文件夹中提供了名为`neural_style_transfer.ipynb`的以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。代码包含从中下载数据的URL，并且代码长度适中。我们强烈建议您在GitHub上执行笔记本以重现结果，同时理解执行步骤和文本中各种代码组件的解释。
- en: 'Import the relevant packages:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包：
- en: '[PRE53]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define the functions to preprocess and postprocess the data:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于预处理和后处理数据的函数：
- en: '[PRE54]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define the `GramMatrix` module:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`GramMatrix`模块：
- en: '[PRE55]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In the preceding code, we are computing all the possible inner products of the
    features with themselves, which is basically asking how all the vectors relate
    to each other.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们正在计算所有可能的特征与自身的内积，这基本上是在询问所有向量如何相互关联。
- en: 'Define the gram matrix’s corresponding MSE loss, `GramMSELoss`:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义格拉姆矩阵的相应均方误差损失，`GramMSELoss`：
- en: '[PRE56]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Once we have the gram vectors for both feature sets, it is important that they
    match as closely as possible, hence the `mse_loss`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对两个特征集都有了格拉姆向量，它们尽可能地匹配，因此`mse_loss`至关重要。
- en: 'Define the model class, `vgg19_modified`:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型类`vgg19_modified`：
- en: 'Initialize the class:'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化类：
- en: '[PRE57]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Extract the features:'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取特征：
- en: '[PRE58]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define the `forward` method, which takes the list of layers and returns the
    features corresponding to each layer:'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`forward`方法，该方法接受层列表并返回与每个层对应的特征：
- en: '[PRE59]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Define the model object:'
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型对象：
- en: '[PRE60]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Import the content and style images:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入内容和样式图像：
- en: '[PRE61]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Make sure that the images are resized to be of the same shape, 512 x 512 x
    3:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保图像调整大小为相同形状，512 x 512 x 3：
- en: '[PRE62]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Specify that the content image is to be modified with `requires_grad = True`:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定要使用`requires_grad = True`修改内容图像：
- en: '[PRE63]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Specify the layers that define content loss and style loss, that is, which
    intermediate VGG layers we are using, to compare gram matrices for style and raw
    feature vectors for content (note, the chosen layers below are purely experimental):'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定定义内容损失和样式损失的层次，即我们使用的中间VGG层，用于比较风格的Gram矩阵和内容的原始特征向量（注意，下面选择的层次纯粹是实验性的）：
- en: '[PRE64]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Define the loss function for content and style loss values:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 定义内容和样式损失值的损失函数：
- en: '[PRE65]'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define the weightage associated with content and style loss:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义与内容和样式损失相关联的权重：
- en: '[PRE66]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We need to manipulate our image such that the style of the target image resembles
    `style_image` as much as possible. Hence, we compute the `style_targets` values
    of `style_image` by computing `GramMatrix` of features obtained from a few chosen
    layers of VGG. Since the overall content should be preserved, we choose the `content_layer`
    variable with which we compute the raw features from VGG:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要操作我们的图像，使得目标图像的风格尽可能地与`style_image`相似。因此，我们通过计算从VGG的几个选择层次获得的特征的Gram矩阵来计算`style_image`的`style_targets`值。由于整体内容应该保持不变，我们选择`content_layer`变量来计算来自VGG的原始特征：
- en: '[PRE67]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Define the `optimizer` and the number of iterations (`max_iters`). Even though
    we could have used Adam or any other optimizer, LBFGS is an optimizer that has
    been observed to work best in deterministic scenarios. Additionally, since we
    are dealing with exactly one image, there is nothing random. Many experiments
    have revealed that LBFGS converges faster and with lower losses in neural transfer
    settings, so we will use this optimizer:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`optimizer`和迭代次数(`max_iters`)。尽管我们可以使用Adam或任何其他优化器，但LBFGS是一种观察到在确定性场景中工作最佳的优化器。此外，由于我们处理的是一张图像，没有任何随机性。许多实验表明，在神经传递设置中，LBFGS收敛更快，损失更低，因此我们将使用这个优化器：
- en: '[PRE68]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Perform the optimization. In deterministic scenarios where we are iterating
    on the same tensor again and again, we can wrap the optimizer step as a function
    with zero arguments and repeatedly call it, as shown here:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行优化。在确定性场景中，我们在同一个张量上进行迭代时，可以将优化器步骤包装为一个零参数函数，并重复调用它，如下所示：
- en: '[PRE69]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Plot the variation in the loss:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制损失变化情况：
- en: '[PRE70]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This results in the following output:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![](img/B18457_11_14.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14](img/B18457_11_14.png)'
- en: 'Figure 11.14: Loss variation over increasing epochs'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14：随着epoch增加的损失变化
- en: 'Plot the image with the combination of content and style images:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制具有内容和风格图像组合的图像：
- en: '[PRE71]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/B18457_11_15.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15](img/B18457_11_15.png)'
- en: 'Figure 11.15: Style transferred image'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：风格转移后的图像
- en: From the preceding picture, we can see that the image is a combination of the
    content and style images.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述图片可以看出，图像是内容和风格图像的组合。
- en: 'With this, we have seen two ways of manipulating an image: an adversarial attack
    to modify the class of an image and a style transfer to combine the style of one
    image with the content of another image. In the next section, we will learn about
    generating deepfakes, which transfer an expression from one face to another.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经看到了两种操作图像的方法：对图像进行对抗性攻击以修改图像的类别，以及风格转移来将一个图像的风格与另一个图像的内容结合。在接下来的部分，我们将学习生成Deepfakes，这将表情从一个脸部转移到另一个脸部。
- en: Understanding deepfakes
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Deepfakes
- en: 'We have learned about two different image-to-image tasks so far: semantic segmentation
    with UNet and image reconstruction with autoencoders. Deepfakery is an image-to-image
    task that has a very similar underlying theory.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了两种不同的图像到图像的任务：使用UNet进行语义分割和使用自动编码器进行图像重构。Deepfakery是一个具有非常相似基础理论的图像到图像任务。
- en: How deepfakes work
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度伪造的工作原理
- en: Imagine a scenario where you want to create an application that takes an image
    of a face and changes the expression in a way that you want. Deepfakes come in
    handy in this scenario. While we made a conscious choice to not discuss the very
    latest in deepfakes in this book, techniques such as few-shot adversarial learning
    are developed to generate realistic images with the facial expression of interest.
    Knowledge of how deepfakes work and GANs (which you will learn about in the next
    chapters) will help you identify videos that are fake.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情况，你想创建一个应用程序，它可以拍摄一张脸部图片，并以你期望的方式改变表情。在这种情况下，Deepfakes非常有用。尽管我们有意选择在本书中不讨论最新的Deepfakes技术，但一些技术如少样本对抗学习已经发展出来，用于生成具有感兴趣面部表情的逼真图片。了解Deepfakes的工作原理和GANs（你将在下一章节学习到）将帮助你识别假视频。
- en: In the task of deepfakery, we have a few hundred pictures of person A and a
    few hundred pictures of person B (or, possibly a video of people A and B). The
    objective is to reconstruct person B’s face with the facial expression of person
    A and vice versa.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在深伪造任务中，我们有几百张个人 A 的图片和几百张个人 B 的图片（或者可能是个人 A 和 B 的视频）。目标是重建具有个人 A 表情的个人 B 的面部，反之亦然。
- en: 'The following diagrams explain how the deepfake image generation process works:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示解释了深度伪造图像生成过程的工作原理：
- en: '![](img/B18457_11_16.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_16.png)'
- en: 'Figure 11.16: AutoEncoder workflow, where there is a single encoder and separate
    decoders for the two classes/set of faces'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16：自动编码器工作流程，其中有一个编码器和两个类/集合的面部的独立解码器
- en: 'In the preceding picture, we are passing images of person A and person B through
    an encoder (**Encoder**). Once we get the latent vectors corresponding to person
    A (**Latent Face A**) and person B (**Latent Face B**), we pass the latent vectors
    through their corresponding decoders (**Decoder A** and **Decoder B**) to fetch
    the corresponding original images (**Reconstructed Face A** and **Reconstructed
    Face B**). So far, the concepts of the encoder and decoder are very similar to
    what we saw in the *Understanding autoencoders* section. However, in this scenario,
    *we have only one encoder, but two decoders* (each decoder corresponding to a
    different person). The expectation is that the latent vectors obtained from the
    encoder represent the information about the facial expression present within the
    image, while the decoder fetches the image corresponding to the person. Once the
    encoder and the two decoders are trained, while performing deepfake image generation,
    we switch the connection within our architecture as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图片中，我们通过编码器（**编码器**）传递个人 A 和个人 B 的图像。一旦我们得到与个人 A（**潜在面部 A**）和个人 B（**潜在面部
    B**）对应的潜在向量，我们通过它们对应的解码器（**解码器 A** 和 **解码器 B**）传递潜在向量以获取相应的原始图像（**重构面部 A** 和 **重构面部
    B**）。到目前为止，编码器和两个解码器的概念与我们在*理解自动编码器*部分看到的非常相似。然而，在这种情况下，*我们只有一个编码器，但有两个解码器*（每个解码器对应不同的人）。期望从编码器获取的潜在向量表示图像中存在的面部表情的信息，而解码器获取相应的图像。一旦编码器和两个解码器训练好了，在执行深度伪造图像生成时，我们在我们的架构中切换连接如下：
- en: '![](img/B18457_11_17.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_17.png)'
- en: 'Figure 11.17: Image reconstruction from latent representation once the decoders
    are swapped'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17：从潜在表示图像重构一旦解码器被交换
- en: When the latent vector of person A is passed through decoder B, the reconstructed
    face of person B will have the characteristics of person A (a smiling face) and
    vice versa for person B when passed through decoder A (a sad face).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 当个人 A 的潜在向量通过解码器 B 传递时，个人 B 的重构面部将具有个人 A 的特征（一个微笑的面孔），反之亦然，当个人 B 通过解码器 A 传递时（一个悲伤的面孔）。
- en: One additional trick that helps in generating a realistic image is warping face
    images and feeding them to the network in such a way that the warped face is the
    input and the original image is expected as the output.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有助于生成逼真图像的技巧是对脸部图像进行扭曲，并以这样的方式将它们馈送到网络中，扭曲的脸部作为输入，期望原始图像作为输出。
- en: Now that we understand how it works, let’s implement the generation of fake
    images of one person with the expression of another person using autoencoders.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了它的工作原理，让我们使用自动编码器实现生成具有另一个人表情的假图像。
- en: Generating a deepfake
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成深度伪造
- en: 'Let’s now take a practical look:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一个实际例子：
- en: 'The following code is available as `Generating_Deep_Fakes.ipynb` in the `Chapter11`
    folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    understand the steps to perform and the explanation of various code components
    in the text.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可在本书的 GitHub 存储库的 `Chapter11` 文件夹中的 `Generating_Deep_Fakes.ipynb` 中找到：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。代码包含用于下载数据的URL，并且长度适中。我们强烈建议您在
    GitHub 中执行该笔记本，以重现结果，同时理解文本中执行步骤和各种代码组件的解释。
- en: 'Let’s download the data (which we have synthetically created) and the source
    code as follows:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们下载数据（我们已经合成创建）和源代码如下：
- en: '[PRE72]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Fetch face crops from the images as follows:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从图像中获取面部截图如下：
- en: 'Define the face cascade, which draws a bounding box around the face in an image.
    There’s more on cascades in the *OpenCV Utilities for Image Analysis* PDF in the
    GitHub repository. However, for now, it suffices to say that the face cascade
    draws a tight bounding box around the face present in the image:'
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义面部级联，它在图像中绘制一个围绕面部的边界框。在GitHub存储库的*OpenCV图像分析实用程序*PDF中有关级联的更多信息。但是，目前仅需说明面部级联在图像中绘制紧密的面部边界框：
- en: '[PRE73]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define a function (`crop_face`) for cropping faces from an image:'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为从图像中裁剪面部定义一个名为`crop_face`的函数：
- en: '[PRE74]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In the preceding function, we are passing the grayscale image (`gray`) through
    the face cascade and cropping the rectangle that contains the face. Next, we are
    returning a re-sized image (`img2`). Further, to account for a scenario where
    there is no face detected in the image, we are passing a flag to show whether
    a face is detected.
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前述函数中，我们通过面部级联将灰度图像（`gray`）传递，并裁剪包含面部的矩形。接下来，我们返回一个重新调整大小的图像（`img2`）。此外，为了考虑到在图像中未检测到面部的情况，我们传递一个标志以显示是否检测到面部。
- en: 'Crop the images of `personA` and `personB` and place them in separate folders:'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪`personA`和`personB`的图像，并将它们放在不同的文件夹中：
- en: '[PRE75]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Create a dataloader and inspect the data:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据加载器并检查数据：
- en: '[PRE76]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The dataloader is returning four tensors, `imsA`, `imsB`, `targetA`, and `targetB`.
    The first tensor (`imsA`) is a distorted (warped) version of the third tensor
    (`targetA`) and the second (`imsB`) is a distorted (warped) version of the fourth
    tensor (`targetB`).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器返回四个张量，`imsA`，`imsB`，`targetA`和`targetB`。第一个张量（`imsA`）是第三个张量（`targetA`）的扭曲（变形）版本，第二个（`imsB`）是第四个张量（`targetB`）的扭曲（变形）版本。
- en: Also, as you can see in the line `a =ImageDataset(Glob('cropped_faces_personA'),
    Glob('cropped_faces_personB'))`, we have two folders of images, one for each person.
    There is no relation between any of the faces, and in the `__iteritems__` dataset,
    we are randomly fetching two faces every time.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如您在`a =ImageDataset(Glob('cropped_faces_personA'), Glob('cropped_faces_personB'))`行中看到的那样，我们有两个图像文件夹，每个人一个。这些面部之间没有任何关系，并且在`__iteritems__`数据集中，我们每次随机获取两张面部。
- en: The key function in this step is `get_training_data`, present in `collate_fn`.
    This is an augmentation function for warping (distorting) faces. We are giving
    distorted faces as input to the autoencoder and trying to predict regular faces.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤中的关键函数是`get_training_data`，出现在`collate_fn`中。这是一个用于扭曲（变形）面部的增强函数。我们将扭曲的面部作为自编码器的输入，并尝试预测正常的面部。
- en: The advantage of warping is that not only does it increase our training data
    size but it also acts as a regularizer to the network, which is forced to understand
    key facial features despite being given a distorted face.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 扭曲的优势不仅在于它增加了我们的训练数据量，而且还作为网络的正则化器，强制它理解关键面部特征，尽管提供的是扭曲的面部。
- en: 'Let’s inspect a few images:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查一些图像：
- en: '[PRE77]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The preceding code results in the following output:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/B18457_11_18.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_18.png)'
- en: 'Figure 11.18: Input and output combination of a batch of images'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18：一批图像的输入和输出组合
- en: Note that the input images are warped, while the output images are not, and
    the input-to-output images now have a one-to-one correspondence.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入图像是扭曲的，而输出图像则不是，输入到输出图像现在具有一对一的对应关系。
- en: 'Build the model as follows:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型如下：
- en: 'Define the convolution (`_ConvLayer`) and upscaling (`_UpScale`) functions
    as well as the `Reshape` class that will be leveraged while building the model:'
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义卷积（`_ConvLayer`）和上采样（`_UpScale`）函数以及在构建模型时将利用的`Reshape`类：
- en: '[PRE78]'
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Define the `Autoencoder` model class, which has a single `encoder` and two
    decoders (`decoder_A` and `decoder_B`):'
  id: totrans-375
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Autoencoder`模型类，它有一个单独的`encoder`和两个解码器（`decoder_A`和`decoder_B`）：
- en: '[PRE79]'
  id: totrans-376
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Generate a summary of the model:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成模型的摘要：
- en: '[PRE80]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The preceding code generates the following output:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![](img/B18457_11_19.png)Figure 11.19: Summary of model architecture'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18457_11_19.png)图 11.19：模型架构总结'
- en: 'Define the `train_batch` logic:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`train_batch`逻辑：
- en: '[PRE81]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: What we are interested in is running `model(imgA, 'B')` (which would return
    an image of class B using an input image from class A), but we do not have a ground
    truth to compare it against. So instead, what we are doing is predicting `_imgA`
    from `imgA` (where `imgA` is a distorted version of `targetA`) and comparing `_imgA`
    with `targetA` using `nn.L1Loss`.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的是运行`model(imgA, 'B')`（它将使用类A的输入图像返回类B的图像），但我们没有地面真相来进行比较。因此，我们正在预测`imgA`（其中`imgA`是`targetA`的扭曲版本）的`_imgA`，并使用`nn.L1Loss`将`_imgA`与`targetA`进行比较。
- en: We do not need `validate_batch` as there is no validation dataset. We will predict
    new images during training and qualitatively see the progress.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要`validate_batch`，因为没有验证数据集。在训练期间，我们将预测新图像，并在质量上看到进展。
- en: 'Create all the required components to train the model:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练模型所需的所有组件：
- en: '[PRE82]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Train the model:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE83]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The preceding code results in reconstructed images, as follows:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码导致重构的图像如下：
- en: '![](img/B18457_11_20.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_20.png)'
- en: 'Figure 11.20: Original and reconstructed images'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.20：原始和重构图像
- en: 'The variation in loss values is as follows (you can refer to the digital version
    of the book for the colored image):'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值的变化如下（您可以参考书籍的数字版获取彩色图像）：
- en: '![](img/B18457_11_21.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_11_21.png)'
- en: 'Figure 11.21: Variation in loss across increasing epochs'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.21：随着 epochs 增加，损失的变化
- en: As you can see, we can swap expressions from one face to another by tweaking
    an autoencoder to have two decoders instead of one. Furthermore, with more epochs,
    the reconstructed image gets more realistic.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们可以通过调整自编码器以使用两个解码器而不是一个，从而在一个面孔和另一个面孔之间交换表情。此外，随着更多的 epochs，重构的图像变得更加逼真。
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have learned about the different variants of autoencoders:
    vanilla, convolutional, and variational. We also learned about how the number
    of units in the bottleneck layer influences the reconstructed image. Next, we
    learned about identifying images that are similar to a given image using the t-SNE
    technique. We learned that when we sample vectors, we cannot get realistic images,
    and by using VAEs, we learned about generating new images by using a combination
    of reconstruction loss and KL divergence loss. Next, we learned how to perform
    an adversarial attack on images to modify the class of an image while not changing
    the perceptive content of the image. We then learned about leveraging the combination
    of content loss and gram matrix-based style loss to optimize for content and style
    loss of images to come up with an image that is a combination of two input images.
    Finally, we learned about tweaking an autoencoder to swap two faces without any
    supervision.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了不同变体的自编码器：香草、卷积和变分。我们还学习了瓶颈层单位数量如何影响重构图像。接下来，我们学习了使用 t-SNE 技术识别与给定图像相似的图像。我们了解到当我们对向量进行采样时，无法获得逼真的图像，通过使用变分自编码器，我们学习了如何通过重构损失和
    KL 散度损失生成新图像。接着，我们学习了如何对图像进行对抗攻击，以修改图像的类别而不改变图像的感知内容。然后，我们学习了如何利用内容损失和基于 Gram
    矩阵的风格损失的组合来优化图像的内容和风格损失，从而生成两个输入图像的组合图像。最后，我们学习了如何调整自编码器以在没有任何监督的情况下交换两个面孔。
- en: Now that we have learned about generating novel images from a given set of images,
    in the next chapter, we will build upon this topic to generate completely new
    images using variants of a network called the generative adversarial network.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何从给定的图像集生成新图像，下一章中，我们将进一步讨论这个主题，使用生成对抗网络的变体生成全新的图像。
- en: Questions
  id: totrans-399
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the “encoder” in an autoencoder?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器中的“编码器”是什么？
- en: What loss function does an autoencoder optimize for?
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器优化哪种损失函数？
- en: How do autoencoders help in grouping similar images?
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器如何帮助分组类似的图像？
- en: When is a convolutional autoencoder useful?
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积自编码器在何时有用？
- en: Why do we get non-intuitive images if we randomly sample from the vector space
    of embeddings obtained from a vanilla/convolutional autoencoder?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们从香草/卷积自编码器获得的嵌入向量空间随机采样，为什么会得到非直观的图像？
- en: What are the loss functions that VAEs optimize for?
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分自编码器优化的是哪些损失函数？
- en: How do VAEs overcome the limitation of vanilla/convolutional autoencoders to
    generate new images?
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分自编码器如何克服香草/卷积自编码器生成新图像的限制？
- en: During an adversarial attack, why do we modify the input image pixels and not
    the weight values?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对抗攻击期间，为什么我们修改输入图像的像素而不是权重值？
- en: In a neural style transfer, what are the losses that we optimize for?
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经风格转移中，我们优化哪些损失以获得最佳结果？
- en: Why do we consider the activation of different layers and not the original image
    when calculating style and content loss?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当计算风格和内容损失时，为什么要考虑不同层的激活而不是原始图像？
- en: Why do we consider the gram matrix loss and not the difference between images
    when calculating the style loss?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当计算风格损失时，为什么要考虑 Gram 矩阵损失而不是图像之间的差异？
- en: Why do we warp images while building a model to generate deepfakes?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建用于生成深度伪造的模型时，为什么我们会扭曲图像？
- en: Learn more on Discord
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多信息
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
