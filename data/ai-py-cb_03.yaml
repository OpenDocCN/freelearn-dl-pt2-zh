- en: Patterns, Outliers, and Recommendations
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 模式、异常值和推荐
- en: In order to gain knowledge from data, it's important to understand the structure
    behind a dataset. The way we represent a dataset can make it more intuitive to
    work with in a certain way, and consequently, easier to draw insights from it.
    The law of the instrument states that when holding a hammer, everything seems
    like a nail (based on Andrew Maslow's *The Psychology of Science*, 1966) and is
    about the tendency to adapt jobs to the available tools. There is no silver bullet,
    however, as all methods have their drawbacks given the problem at hand. It's therefore
    important to know the basic methods in the arsenal of available tools in order
    to recognize the situations where we should use a hammer as supposed to a screwdriver.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从数据中获取知识，理解数据集背后的结构非常重要。我们对数据集的表示方式可以使其更直观地在某种方式下工作，并因此更容易从中获得洞察力。工具的法则指出，当手持锤子时，一切似乎都像是钉子（基于安德鲁·马斯洛的《科学心理学》，1966年），这是关于适应工具的趋势。然而，并没有银弹，因为所有方法都有它们在特定问题下的缺点。因此，了解可用工具库中的基本方法对于识别应该使用锤子而不是螺丝刀的情况至关重要。
- en: In this chapter, we'll look at different ways of representing data, be it visualizing
    customer groups for marketing purposes and finding unusual patterns, or projecting
    data to emphasize differences, recommending products to customers based on their
    own previous choices, along with those of other customers, and identifying fraudsters
    by their similarities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨不同的数据表示方法，无论是为了可视化客户群体以及找到异常模式，还是投射数据以强调差异，根据客户自己以及其他客户的先前选择向客户推荐产品，并通过相似性识别欺诈者社区。
- en: 'Specifically, we''ll present the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将提供以下配方：
- en: Clustering market segments
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类市场段
- en: Discovering anomalies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现异常
- en: Representing for similarity search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示相似性搜索
- en: Recommending products
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐产品
- en: Spotting fraudster communities
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现欺诈者社区
- en: Clustering market segments
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类市场段
- en: In this recipe, we'll apply clustering methods in order to find groups of customers
    for marketing purposes. We'll look at the German Credit Risk dataset, and we'll
    try to identify different segments of customers; ideally, we'd want to find the
    groups that are most profitable and different at the same time, so we can target
    them with advertising.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将应用聚类方法以便为营销目的找到客户群体。我们将查看德国信用风险数据集，并尝试识别不同的客户段。理想情况下，我们希望找到既有利可图又不同的群体，以便进行定向广告。
- en: Getting ready
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, we'll be using a dataset of credit risk, usually referred to
    in full as the German Credit Risk dataset. Each row describes a person who took
    a loan, gives us a few attributes about the person, and tells us whether the person
    paid the loan back (that is, whether the credit was a good or bad risk).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将使用信用风险数据集，通常被完整称为德国信用风险数据集。每行描述了一位借款人的信息，给出了关于这个人的几个属性，并告诉我们这个人是否还了贷款（即信用是否良好或不良风险）。
- en: 'We''ll need to download and load up the German credit data as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要按如下步骤下载和加载德国信用数据：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For visualizations, we''ll use the `dython` library. The `dython` library works
    directly on categorical and numeric variables, and makes adjustments for numeric-categorical
    or categorical-categorical comparisons. Please see the documentation for details,
    at [http://shakedzy.xyz/dython/](http://shakedzy.xyz/dython/). Let''s install
    the library as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可视化，我们将使用`dython`库。`dython`库直接处理分类和数值变量，并针对数值-分类或分类-分类比较进行调整。请参阅详细文档，网址为 [http://shakedzy.xyz/dython/](http://shakedzy.xyz/dython/)。让我们按以下步骤安装该库：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can now play with the German credit dataset, visualize it with `dython`,
    and see how the people represented inside can be clustered together in different
    groups.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`dython`库玩弄德国信用数据集，将其可视化，并看看如何将内部的人群聚合到不同的群组中。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: We'll first visualize the dataset, do some preprocessing, and apply a clustering
    algorithm. We'll try to make sense out of the clusters, and – with the new insights
    – cluster again.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将可视化数据集，进行一些预处理，并应用聚类算法。我们将试图从这些聚类中获得见解，并在新的见解基础上重新进行聚类。
- en: 'We''ll start by visualizing the features:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从可视化特征开始：
- en: '**Visualizing correlations**: In this recipe, we''ll use the `dython` library.
    We can calculate the correlations with dython''s `associations` function, which
    calls categorical, numerical (Pearson correlation), and mixed categorical-numerical
    correlation functions depending on the variable types:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可视化相关性**：在这个示例中，我们将使用`dython`库。我们可以使用dython的`associations`函数计算分类变量、数值变量（Pearson相关系数）和混合分类-数值变量的相关性函数：'
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This call not only calculates correlations, but also cleans up the correlation
    matrix by clustering variables together that are correlated. The data is visualized
    as shown in the following screenshot:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此调用不仅计算相关性，还通过将相关的变量聚类在一起来清理相关矩阵。数据可视化如下截图所示：
- en: '![](img/cf1b680e-510c-471b-a063-c0036295dbde.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf1b680e-510c-471b-a063-c0036295dbde.png)'
- en: We can't really see clear cluster demarcations; however, there seem to be a
    few groups if you look along the diagonal.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们无法真正看到清晰的聚类界限；然而，如果你沿对角线观察，似乎有几个群体。
- en: Also, a few variables such as telephone and job stand out a bit from the rest.
    In the notebook on GitHub, we've tried dimensionality reduction methods to see
    if this would help our clustering. However, dimensionality reduction didn't work
    that well, while clustering directly worked better: [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Clustering%20market%20segments.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Clustering%20market%20segments.ipynb).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些变量如电话和职位与其他变量略有不同。在GitHub的笔记本中，我们尝试了降维方法，看看是否能改善我们的聚类。然而，降维并没有效果那么好，而直接聚类效果更佳：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Clustering%20market%20segments.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Clustering%20market%20segments.ipynb)。
- en: As the first step for clustering, we'll convert some variables into dummy variables;
    this means we will do a one-hot-encoding of the categorical variables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为聚类的第一步，我们将一些变量转换为虚拟变量；这意味着我们将对分类变量进行独热编码。
- en: '**Preprocessing variables**: We one-hot encode (also called *dummy-transform*)
    the categorical variables as follows:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理变量**：我们对分类变量进行了独热编码（也称为*哑变换*），如下所示：'
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Unfortunately, when we visualize this dataset to highlight customer differences,
    the result is not appealing. You can see the notebook online for some attempts
    at this.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当我们将数据集可视化以突出客户差异时，结果并不理想。你可以在线查看笔记本以了解一些尝试。
- en: '**First attempt at clustering**: A typical method for clustering is `kmeans`.
    Let''s try it out:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚类的第一次尝试**：聚类的典型方法是`kmeans`。让我们来试一试：'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The inertia is the sum of distances to the closest cluster center over all the
    data points. A visual criterion for choosing the best number of clusters (the
    hyperparameter `k` in the *k*-means clustering algorithm) is called the **elbow
    criterion**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性是所有数据点到最近聚类中心的距离总和。在*k*-means聚类算法中选择最佳聚类数（超参数`k`）的一种视觉标准称为**肘部准则**。
- en: 'Let''s visualize the inertia over a number of clusters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化不同聚类数下的惯性：
- en: '![](img/02754259-7e11-4d76-8cfc-6a19784573c5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02754259-7e11-4d76-8cfc-6a19784573c5.png)'
- en: 'The basic idea of the elbow criterion is to choose the number of clusters where
    the error or inertia flattens off. According to the elbow criterion, we could
    be taking `4` clusters. Let''s get the clusters again:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: “肘部准则”的基本思想是选择误差或惯性变平的聚类数。根据肘部准则，我们可能会选择`4`个聚类。让我们重新获取这些聚类：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Summarizing clusters**: Now we can summarize the clusters:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**总结聚类**：现在我们可以总结这些聚类：'
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here''s the summary table for the clusters. We''ve included marketing characteristics,
    such as age, and others that give us an idea about how much money the customers
    make us. We are showing standard deviations over some of these in order to understand
    how consistent the groups are:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是聚类的汇总表。我们包括了营销特征，如年龄，以及其他让我们了解客户带来的收益的因素。我们展示了一些标准偏差，以了解这些群体的一致性程度：
- en: '![](img/1f6dde32-3fb8-4b7c-9f1d-c7060cc6d7bf.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f6dde32-3fb8-4b7c-9f1d-c7060cc6d7bf.png)'
- en: We see in this little excerpt that differences are largely due to differences
    in credit amount. This brings us back to where we started out, namely that we
    largely get out of the clustering what we put in. There's no trivial way of resolving
    this problem, but we can select the variables we want to focus on in our clusters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个小节中看到，差异主要是由信用金额的差异造成的。这使我们回到了最初的起点，即我们从聚类中获得的主要是我们输入的内容。解决这个问题没有简单的方法，但我们可以选择在我们的聚类中要关注的变量。
- en: '**New attempt at clustering**: We can go back to the drawing board, simplify
    our aims, and start again with the question of what we actually want to find:
    groups of customers that fulfill two characteristics:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**新的聚类尝试**：我们可以重新审视我们的目标，简化我们的目标，并从实际上我们想要找到的内容开始：满足两个特征的客户群体，即：'
- en: 'The clusters should distinguish customers by who makes us money: this leads
    us to variables such as credit amount, duration of their loan, and whether they
    paid it back.'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些聚类应该通过谁为我们赚钱来区分客户：这使我们关注如信用金额、贷款期限以及是否已经还清等变量。
- en: The clusters should highlight different characteristics with respect to marketing,
    such as age, gender, or some other characteristic.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些聚类应该突出与营销相关的不同特征，如年龄、性别或其他一些特征。
- en: 'With this in mind, we''ll make a new attempt:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，我们将进行新的尝试：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now produce the overview table again in order to view the cluster stats:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以再次生成概览表，以查看群体统计信息：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And here comes the new summary:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是新的摘要：
- en: '![](img/7097097c-6dea-412c-a491-5a535dfc88a5.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7097097c-6dea-412c-a491-5a535dfc88a5.png)'
- en: I would argue this is more useful than the previous clustering, because it clearly
    shows us which customers can make us money, and highlights other differences between
    them that are relevant to marketing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我会认为这比之前的聚类更有用，因为它清楚地显示了哪些客户能给我们带来利润，并突出了其他在营销中相关的差异。
- en: How it works...
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: Clustering is a very common visualization technique in business intelligence.
    In marketing, you'll target people differently, say teens, versus pensioners,
    and some groups are more valuable than others. Often, as a first step, dimensionality
    is reduced using a dimensionality reduction method or by feature selection, then
    groups are separated by applying a clustering algorithm. For example, you could
    first apply **Principal Component Analysis** (**PCA**) to reduce dimensionality
    (the number of features), and then *k*-means for finding groups of data points.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在商业智能中，聚类是一种非常常见的可视化技术。在营销中，你会针对不同的人群进行定位，比如青少年与退休人员，某些群体比其他群体更有价值。通常作为第一步，会通过降维方法或特征选择减少维度，然后通过应用聚类算法将群体分离。例如，你可以首先应用**主成分分析**（**PCA**）来降低维度（特征数量），然后用*k*-均值找到数据点的群体。
- en: 'Since visualizations are difficult to judge objectively, in the previous section,
    what we did was to take a step back and look at the actual purpose, the business
    goal, that we want to achieve. We took the following steps to achieve this goal:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于视觉化很难客观评判，在前一节中，我们所做的是退后一步，看看实际目的，即我们想要达成的业务目标。我们采取以下步骤来实现这个目标：
- en: We concentrated on the variables that are important for our goal.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们专注于对我们的目标重要的变量。
- en: We created a function that would help us determine the quality of the clusters.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了一个函数，帮助我们确定聚类的质量。
- en: From this premise, we then tried different methods and evaluated them against
    our business goal.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个前提，我们尝试了不同的方法，并根据我们的业务目标对它们进行了评估。
- en: 'If you''ve paid attention when looking at the recipe, you might have noticed
    that we don''t standardize our output (z-scores). In standardization with the
    z-score, a raw score *x* is converted into a standard score by subtracting the
    mean and dividing by the standard deviation, so every standardized variable has
    a mean of 0 and a standard deviation of 1:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在查看配方时留意到，你可能已经注意到我们不对输出（z-分数）进行标准化。在使用 z-分数进行标准化时，原始分数 *x* 通过减去均值并除以标准偏差被转换为标准分数，因此每个标准化变量的平均值为
    0，标准偏差为 1：
- en: '![](img/a2ad8f8e-a93b-4c82-b215-08c42d9df455.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2ad8f8e-a93b-4c82-b215-08c42d9df455.png)'
- en: We don't apply standardization because variables that have been dummy-transformed
    would have higher importance proportional to their number of factors. To put it
    simply, z-scores mean that every variable would have the same importance. One-hot
    encoding gives us a separate variable for each value that it can take. If we calculate
    and use z-scores after dummy-transforming, a variable that was converted to many
    new (dummy) variables, because it has many values, would be less important than
    another variable that has fewer values and consequently fewer dummy columns. This
    situation is something we want to avoid, so we don't apply z-scores.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应用标准化，因为已经进行虚拟转换的变量在因素数量上具有更高的重要性。简单来说，z分数意味着每个变量具有相同的重要性。独热编码为每个可以采用的值提供了一个单独的变量。如果我们在进行虚拟转换后计算和使用z分数，一个被转换为许多新（虚拟）变量的变量，因为它有许多值，会比另一个具有较少值和因此较少虚拟列的变量更不重要。我们希望避免这种情况，所以我们不应用z分数。
- en: The important thing to take away, however, is that we have to focus on differences
    that we can understand and describe. Otherwise, we might end up with clusters
    that are of limited use.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要记住的重要一点是，我们必须专注于我们能够理解和描述的差异。否则，我们可能会得到用途有限的聚类。
- en: In the next section, we'll go more into detail with the *k*-means algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更详细地讨论*k*-均值算法。
- en: There's more...
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: PCA was proposed in 1901 (by Karl Pearson, in *On Lines and Planes of Closest
    Fit to Systems of Points in Space*) and *k*-means in 1967 (by James MacQueen,
    in *Some Methods for Classification and Analysis of Multivariate Observations*).
    While both methods had their place when data and computing resources were hard
    to come by, today many alternatives exist that can work with more complex relationships
    between data points and features. On a personal note, as the authors of this book,
    we often find it frustrating to see methods that rely on normality or a very limited
    kind of relationship between variables, such as classic methods like PCA or *K*-means,
    especially when there are so many better methods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: PCA提出于1901年（由卡尔·皮尔逊在《关于空间中一组点的最佳适合线和平面》中提出），*k*-均值提出于1967年（由詹姆斯·麦克昆在《关于多元观测分类和分析的一些方法》中提出）。虽然这两种方法在数据和计算资源稀缺时有其用武之地，但今天存在许多替代方法可以处理数据点和特征之间更复杂的关系。作为本书的作者，我们个人经常感到沮丧，看到依赖正态性或变量之间一种非常有限关系的方法，例如经典的PCA或*k*-均值方法，尤其是在存在更多更好的方法时。
- en: Both PCA and *k*-means have serious shortcomings that affect their usefulness
    in practice. Since PCA operates over the correlation matrix, it can only find
    linear correlations between data points. This means that if variables were related,
    but not linearly (as you would see in a scatter plot), then PCA would fail. Furthermore,
    PCA is based on mean and variance, which are parameters for Gaussian distribution.
    *K*-means, being a centroid-based clustering algorithm, can only find spherical
    groups in Euclidean space – that is, it fails to uncover any more complicated
    structures. More information on this can be found at [https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和*k*-均值都存在严重缺陷，影响它们在实践中的有效性。由于PCA基于相关矩阵运作，它只能找到数据点之间的线性相关性。这意味着如果变量相关但不是线性相关（如在散点图中所见），PCA将失败。此外，PCA基于均值和方差，这些是高斯分布的参数。作为基于质心的聚类算法，*K*-均值只能在欧几里得空间中找到球形群体，因此它无法揭示任何更复杂的结构。有关更多信息，请参阅[https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages](https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages)。
- en: 'Other robust, nonlinear methods are available, for example, affinity propagation,
    fuzzy *c*-means, agglomerative clustering, and others. However, it''s important
    to remember that, although these methods separate data points into groups, the
    following statements are also true:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他强健的非线性方法可用，例如亲和传播、模糊*c*-均值、凝聚聚类等。然而，重要的是要记住，尽管这些方法将数据点分组，以下陈述也是正确的：
- en: This is done according to a heuristic.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是根据一种启发式方法完成的。
- en: It's based on the differences apparent in the dataset and based on the distance
    metric applied.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它基于数据集中显而易见的差异以及应用的距离度量。
- en: The purpose of clustering is to visualize and simplify the output for human
    observers.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类的目的是为了使人类观察者能够可视化和简化输出。
- en: 'Let''s look at the *k*-means algorithm in more detail. It''s actually really
    simple and can be written down from scratch in `numpy` or `jax`. This implementation
    is based on the one in NapkinML ([https://github.com/eriklindernoren/NapkinML](https://github.com/eriklindernoren/NapkinML)):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下*k*-means算法。实际上，这很简单，并且可以从头开始用`numpy`或`jax`编写。该实现基于NapkinML中的一个实现（[https://github.com/eriklindernoren/NapkinML](https://github.com/eriklindernoren/NapkinML)）：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The main logic – as should be expected – is in the `fit()` method. It comes
    in three steps that are iterated as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 主要逻辑 - 如预期的那样 - 在`fit()`方法中。它分为三步，如下迭代：
- en: Calculate the distances between each point and the centers of the clusters.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个点与簇中心之间的距离。
- en: Each point gets assigned to the cluster of its closest cluster center.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个点被分配到其最近簇中心的簇。
- en: The cluster centers are recalculated as the arithmetic mean.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 簇中心被重新计算为算术平均值。
- en: 'It''s surprising that such a simple idea can result in something that looks
    meaningful to human observers. Here''s an example of it being used. Let''s try
    it out with the Iris dataset that we already know from the *Classifying in scikit-learn,
    Keras, and PyTorch* recipe in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting
    Started with Artificial Intelligence in Python*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这样一个简单的想法竟然可以得出对人类观察者看起来有意义的结果。这里有一个使用它的例子。让我们试试用我们已经在[第1章](87098651-b37f-4b05-b0ee-878193f28b95.xhtml)中*Python中的人工智能入门*中知道的鸢尾花数据集来尝试它：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We end up with clusters that we can visualize or inspect, similar to before.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们得到可以可视化或检查的簇，类似于之前。
- en: See also
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: In order to get an overview of different clustering methods, please refer to
    a survey or review paper. Saxena et al. cover most of the important terminology
    in their article, *Review of Clustering Techniques and Developments* (2017).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得不同聚类方法的概述，请参考一篇调查或评论文章。Saxena等人在他们的文章*聚类技术与发展综述*（2017）中涵盖了大部分重要的术语。
- en: 'We would recommend looking at the following methods relevant to clustering
    and dimensionality reduction (we link to implementations):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议查看与聚类和降维相关的以下方法（我们链接到实现）：
- en: 'Affinity propagation ([https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)):
    A clustering method that finds a number of clusters as part of its heuristics'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亲和传播（[https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)）：一种聚类方法，作为其启发式算法的一部分找到多个簇。
- en: 'Fuzzy c-means ([https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html](https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html)):
    A fuzzy version of the *k*-means algorithm'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊c均值（[https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html](https://pythonhosted.org/scikit-fuzzy/auto_examples/plot_cmeans.html)）：*k*-means算法的模糊版本
- en: 'Local linear embedding ([https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html)):
    A lower-dimensional embedding, where local similarities are maintained'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地线性嵌入（[https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html)）：一个低维嵌入，保持局部相似性
- en: 'T-distributed stochastic neighbor embedding ([https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)):
    A nonlinear dimensionality reduction method suited for visualization in two- or
    three-dimensional space.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T分布随机邻居嵌入（[https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)）：一种非线性降维方法，适合在二维或三维空间中进行可视化。
- en: Mixture-modeling ([https://mixem.readthedocs.io/en/latest/examples/old_faithful.html](https://mixem.readthedocs.io/en/latest/examples/old_faithful.html))
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合建模（[https://mixem.readthedocs.io/en/latest/examples/old_faithful.html](https://mixem.readthedocs.io/en/latest/examples/old_faithful.html)）
- en: The idea of taking advantage of a pre-trained random forest in order to provide
    a custom-tailored kernel is discussed in *The Random Forest Kernel and other kernels
    for big data from random partitions* (2014) by Alex Davies and Zoubin Ghahramani,
    available at [https://arxiv.org/abs/1402.4293](https://arxiv.org/abs/1402.4293).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 利用预先训练的随机森林来提供定制的内核的想法在*The Random Forest Kernel and other kernels for big data
    from random partitions* (2014)由Alex Davies和Zoubin Ghahramani讨论，可在[https://arxiv.org/abs/1402.4293](https://arxiv.org/abs/1402.4293)找到。
- en: Discovering anomalies
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现异常
- en: An anomaly is anything that deviates from the expected or normal outcomes. Detecting
    anomalies can be important in **I****ndustrial Process Monitoring** (**IPM**),
    where data-driven fault detection and diagnosis can help achieve achieve higher
    levels of safety, efficiency, and quality.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 异常是任何偏离预期或正常结果的情况。在工业过程监控（IPM）中，检测异常可以帮助实现更高水平的安全性、效率和质量。
- en: In this recipe, we'll look at methods for outlier detection. We'll go through
    an example of outlier detection in a time series with **Python Outlier Detection** (**pyOD**),
    a toolbox for outlier detection that implements many state-of-the-art methods
    and visualizations. PyOD's documentation can be found at [https://pyod.readthedocs.io/en/latest/](https://pyod.readthedocs.io/en/latest/).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将探讨异常检测的方法。我们将通过使用Python异常检测（pyOD），这是一个实现许多最先进方法和可视化的异常检测工具箱，来演示时间序列中的异常检测示例。pyOD的文档可以在[https://pyod.readthedocs.io/en/latest/](https://pyod.readthedocs.io/en/latest/)找到。
- en: We'll apply an autoencoder for a similarity-based approach, and then an online
    learning approach suitable for finding events in streams of data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用基于相似性的自动编码器方法，并使用适用于查找数据流事件的在线学习方法。
- en: Getting ready
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe will focus on finding outliers. We'll demonstrate how to do this
    with the pyOD library including an autoencoder approach. We'll also outline the
    upsides and downsides to the different methods.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将专注于寻找异常值。我们将演示如何使用pyOD库包括自动编码器方法来做到这一点。我们还将概述不同方法的优缺点。
- en: The streams of data are time series of **key performance indicators** (**KPIs**) of
    website performance. This dataset is provided in the DONUT outlier detector repository,
    available at [https://github.com/haowen-xu/donut](https://github.com/haowen-xu/donut).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流是网站性能的KPI的时间序列。这个数据集提供在DONUT异常检测器库中，可以在[https://github.com/haowen-xu/donut](https://github.com/haowen-xu/donut)找到。
- en: 'Let''s download and load it as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤下载并加载它：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll use methods from pyOD, so let''s install that as well:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自pyOD的方法，因此也要安装它：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Please note that some pyOD methods have dependencies such as TensorFlow and
    Keras, so you might have to make sure that these are also installed. If you get
    a message reading `No Module named Keras` you can install Keras separately as
    follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些pyOD方法有依赖关系，如TensorFlow和Keras，因此您可能需要确保这些也已安装。如果您收到`No Module named Keras`的消息，您可以单独安装Keras如下：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Please note that it's usually better to use the Keras version that ships with
    TensorFlow.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通常最好使用与TensorFlow一起提供的Keras版本。
- en: Let's have a look at our dataset, and then apply different outlier detection
    methods.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们的数据集，然后应用不同的异常检测方法。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We''ll cover different steps and methods in this section. They are as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中涵盖不同的步骤和方法。它们如下：
- en: Visualizing
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化
- en: Benchmarking
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基准测试
- en: Running an isolation forest
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行孤立森林
- en: Running an autoencoder
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行自动编码器
- en: 'Let''s start by exploring and visualizing our dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从探索和可视化我们的数据集开始：
- en: 'We can visualize our dataset over time as a time series:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以将我们的数据集随时间作为时间序列进行可视化：
- en: 'Let''s have a look at our dataset with the following command:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令查看我们的数据集：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This is what it looks like:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来是这样的：
- en: '![](img/3651c14d-4b9b-4ac8-9293-a2b8101f54cc.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3651c14d-4b9b-4ac8-9293-a2b8101f54cc.png)'
- en: 'This time series of KPIs is geared toward monitoring the operation and maintenance
    of web services. They come with a label that indicates an abnormality – in other
    words, an outlier – if a problem has occurred with the service:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关键性能指标（KPIs）的时间序列旨在监控Web服务的运营和维护。如果服务出现问题，它们会带有一个标签，表示异常，即异常值：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is the resulting plot, where the dots represent outliers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果图，其中点代表异常值：
- en: '![](img/7c45a89a-1996-4c4f-a869-4dc7369bf9a2.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c45a89a-1996-4c4f-a869-4dc7369bf9a2.png)'
- en: 'Alternatively, we can see where outliers are located in the spectrum of the
    KPIs, and how clearly they are distinguishable from normal data, with the following
    code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以看到异常值在关键绩效指标谱中的位置，以及它们与正常数据的区别有多明显，使用以下代码：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With the preceding code, we plot two histograms against each other using line
    plots. Alternatively, we could be using `hist()` with opacity.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们绘制两个直方图的线图对比。或者，我们可以使用`hist()`函数并设置透明度。
- en: 'The following plot is the outlier distribution density, where the values of
    the time series are on the *x* axis, and the two lines show what''s recognized
    as normal and what''s recognized as an outlier, respectively – 0 indicates normal
    data points, and 1 indicates outliers:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了异常值分布密度，时间序列值位于*x*轴上，两条线分别表示被识别为正常和异常的值，0表示正常数据点，1表示异常值：
- en: '![](img/e49581a8-fb11-4b4a-902d-e2f23e88ac35.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e49581a8-fb11-4b4a-902d-e2f23e88ac35.png)'
- en: We'll be using the same visualization for all subsequent methods so we can compare
    them graphically.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为所有后续方法使用相同的可视化效果，以便进行图形比较。
- en: Outliers (shown with the dotted line) are hardly distinguishable from normal
    data points (the squares), so we won't be expecting perfect performance.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值（用虚线表示）与正常数据点（方块）几乎无法区分，因此我们不会期望完美的表现。
- en: We'll now implement benchmarking.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将实施基准测试。
- en: Before we go on and test methods for outlier detection, let's set down a process
    for comparing them, so we'll have a benchmark of the relative performances of
    the tested methods.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续测试异常值检测方法之前，让我们制定一个比较它们的过程，这样我们就有了测试方法相对性能的基准。
- en: 'We split our data into training and test sets as usual:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样将数据分为训练集和测试集：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s write a testing function that we can use with different outlier
    detection methods:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写一个测试函数，可以用不同的异常值检测方法进行测试：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This function tests an outlier detection method on the dataset. It trains a
    model, gets performance metrics from the model, and plots a visualization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数在数据集上测试异常值检测方法。它训练一个模型，从模型中获取性能指标，并绘制可视化结果。
- en: 'It takes these parameters:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受这些参数：
- en: '`X_train`: Training features'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_train`: 训练特征'
- en: '`y_train`: Training labels'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_train`: 训练标签'
- en: '`X_test`: Test features'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_test`: 测试特征'
- en: '`y_test`: Test labels'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_test`: 测试标签'
- en: '`only_neg`: Whether to use only normal points (that is, not outliers) for training'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`only_neg`: 是否仅使用正常点进行训练'
- en: '`basemethod`: The model to test'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`basemethod`: 要测试的模型'
- en: We can choose to only train on normal points (all points excluding outliers)
    in order to learn the distribution or general characteristics of these points,
    and the outlier detection method can then decide whether the new points do or
    don't fit these characteristics.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择仅在正常点上进行训练（即不包括异常值的所有点），以便学习这些点的分布或一般特征，然后异常值检测方法可以决定新点是否符合这些特征。
- en: 'Now that this is done, let''s test two methods for outlier detection: the isolation
    forest and an autoencoder.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这一步完成后，让我们测试两种异常值检测方法：孤立森林和自编码器。
- en: The isolation forest comes first.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 孤立森林排名第一。
- en: Here we'll look at the isolation forest.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将讨论孤立森林。
- en: 'We run the benchmarking method and hand over an isolation forest detection
    method:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行基准测试方法，并使用孤立森林检测方法：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The **Receiver Operating Characteristic** (**ROC**) performance of the isolation
    forest predictions against the test data is about 0.86, so it does reasonably
    well.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 孤立森林预测的接收器操作特征曲线（ROC）性能相对于测试数据约为0.86，因此表现相当不错。
- en: 'We can see from the following graph, however, that there are no 1s (predicted
    outliers) in the lower range of the KPI spectrum. The model misses out on outliers
    in the lower range:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从下图可以看出，在关键绩效指标谱的较低范围内没有1（预测的异常值）。该模型错过了较低范围的异常值：
- en: '![](img/9af65a20-b46d-4b87-9a99-b3f01a2a5474.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9af65a20-b46d-4b87-9a99-b3f01a2a5474.png)'
- en: It only recognizes points as outliers that have higher values (*>=1.5)*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 它仅识别具有更高值（*>=1.5*）的点作为异常值。
- en: 'Next, let''s try running an autoencoder:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试运行一个自编码器：
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see the Keras network structure and the output from the test function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到Keras网络结构以及测试函数的输出：
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The performance of the autoencoder is very similar to the isolation forest;
    however, the autoencoder finds outliers both in the lower and upper ranges of
    the KPI spectrum.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的性能与孤立森林非常相似；然而，自编码器在关键绩效指标谱的较低和较高范围都能找到异常值。
- en: 'Furthermore, we don''t get an appreciable difference when providing either
    only normal data or both normal data and outliers. We can see how the autoencoder
    works in the following graph:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在只提供正常数据或正常数据和异常值时并未获得明显差异。我们可以通过下图了解自动编码器的工作方式：
- en: '![](img/274696f4-58a3-40a2-952e-959b46161e5a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/274696f4-58a3-40a2-952e-959b46161e5a.png)'
- en: This doesn't look too bad, actually – values in the mid-range are classified
    as normal, while values on the outside of the spectrum are classified as outliers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上这看起来并不太糟糕 —— 中间范围内的值被分类为正常，而在谱外的值则被分类为异常值。
- en: Please remember that these methods are unsupervised; of course, we could get
    better results if we used a supervised method. As a practical consideration, if
    we use supervised methods with our own datasets, this would require us to do additional
    work by annotating anomalies, which we don't have to do with unsupervised methods.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些方法是无监督的；当然，如果我们使用自己的数据集使用监督方法，这将需要我们额外的工作来注释异常，而无监督方法则无需如此。
- en: How it works...
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: Outliers are extreme values that deviate from other observations on the data.
    Outlier detection is important in many domains, including network security, finance, traffic, social
    media, machine learning, the monitoring of machine model performance, and surveillance. A
    host of algorithms have been proposed for outlier detection in these domains.
    The most prominent algorithms include ***k*-Nearest Neighbors** (**kNN**), **L****ocal
    Outlier Factors** (**LOF**), and the isolation forest, and more recently, autoencoders, **L****ong
    Short-Term Memory** (**LSTM**), and **G****enerative Adversarial Networks** (**GANs**).
    We'll discover some of these methods in later recipes. In this recipe, we've used
    kNN, an autoencoder, and the isolation forest algorithm. Let's talk about these
    three methods briefly.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是偏离数据中其他观测值的极端值。在许多领域中，包括网络安全、金融、交通、社交媒体、机器学习、机器模型性能监控和监视中，异常值检测都是重要的。已提出了许多领域中的异常检测算法。最突出的算法包括***k*-最近邻**（**kNN**）、**局部异常因子**（**LOF**）和隔离森林，以及最近的自动编码器、**长短期记忆网络**（**LSTM**）和**生成对抗网络**（**GANs**）。我们将在后续的实例中探索其中一些方法。在这个实例中，我们使用了
    kNN、自动编码器和隔离森林算法。让我们简要地谈谈这三种方法。
- en: k-nearest neighbors
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-最近邻
- en: The kNN classifier is a non-parametric classifier introduced by Thomas Cover
    and Peter Hart (in *Nearest neighbor pattern classification*, 1967). The main
    idea is that a new point is likely to belong to the same class as its neighbors.
    The hyperparameter `k` is the number of neighbors to compare. There are weighted
    versions based on the relative distance of a new point from its neighbors.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 分类器是由 Thomas Cover 和 Peter Hart 提出的非参数分类器（见*最近邻模式分类*，1967）。其主要思想是一个新点很可能属于与其邻居相同的类。超参数
    `k` 是要比较的邻居数。还有基于新点与其邻居相对距离的加权版本。
- en: Isolation forest
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隔离森林
- en: 'The idea of the isolation forest is relatively simple: create random decision
    trees (this means each leaf uses a randomly chosen feature and a randomly chosen
    split value) until only one point is left. The length of the path across the trees
    to get to a terminal node indicates whether a point is an outlier.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离森林的思想相对简单：创建随机决策树（这意味着每个叶子节点使用随机选择的特征和随机选择的分割值），直到只剩下一个点。穿过树获取到终端节点的路径长度指示出点是否是异常值。
- en: 'You can find out more details about the isolation forest in its original publication
    by Liu et al., *Isolation Forest*. ICDM 2008: 413–422: [https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '您可以在原创文献中详细了解隔离森林，作者是 Liu 等人，*隔离森林*，ICDM 2008: 413–422: [https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf)。'
- en: Autoencoder
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动编码器
- en: An autoencoder is a neural network architecture, where we learn a representation
    based on a set of data. This is usually done by a smaller hidden layer (**bottleneck**)
    from which the original is to be restored. In that way, it's similar to many other
    dimensionality reduction methods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种神经网络架构，通过学习数据集的表示来实现。通常通过一个较小的隐藏层（**瓶颈**）来实现，从而可以恢复原始数据。这与许多其他降维方法类似。
- en: 'An autoencoder consists of two parts: the encoder and the decoder. What we
    are really trying to learn is the transformation of the encoder, which gives us
    a code or the representation of the data that we look for.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器由两部分组成：编码器和解码器。我们真正想学习的是编码器的转换，它给我们一个我们寻找的数据的代码或表示。
- en: 'More formally, we can define the encoder as the function ![](img/cceac08e-9c97-44f0-bc14-82340bf370d3.png),
    and the decoder as the function ![](img/f2ab91f4-652d-49b7-880a-58a35110e5e8.png).
    We try to find ![](img/4ff556c5-447c-4eeb-a8dd-ef0e76f6265a.png) and ![](img/339c4a30-f3ad-4d7e-a334-66ffee521b9f.png) so
    that the reconstruction error is minimized:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地，我们可以将编码器定义为函数 ![](img/cceac08e-9c97-44f0-bc14-82340bf370d3.png)，将解码器定义为函数 ![](img/f2ab91f4-652d-49b7-880a-58a35110e5e8.png)。我们试图找到 ![](img/4ff556c5-447c-4eeb-a8dd-ef0e76f6265a.png) 和 ![](img/339c4a30-f3ad-4d7e-a334-66ffee521b9f.png)，以使重建误差最小化：
- en: '![](img/f012cf6c-7842-446d-9f3a-46c9acb63767.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f012cf6c-7842-446d-9f3a-46c9acb63767.png)'
- en: The autoencoder represents data in an intermediate network layer, and the closer
    they can be reconstructed based on the intermediate representation, the less of
    an outlier they are.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器在中间网络层中表示数据，它们在基于中间表示的重建越接近时，异常程度越低。
- en: See also
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'Many implementations of outlier detection are publicly available for Python:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Python中有许多异常检测的公开实现：
- en: As part of Numenta's Cortical Learning Algorithm: [http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html#temporalanomaly-model](http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html#temporalanomaly-model)
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为Numenta皮层学习算法的一部分：[http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html#temporalanomaly-model](http://nupic.docs.numenta.org/stable/guides/anomaly-detection.html#temporalanomaly-model)
- en: Banpei, for singular spectrum transformation: [https://github.com/tsurubee/banpei](https://github.com/tsurubee/banpei)
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banpei，用于奇异谱变换：[https://github.com/tsurubee/banpei](https://github.com/tsurubee/banpei)
- en: Deep anomaly detection methods for time series: [https://github.com/KDD-OpenSource/DeepADoTS](https://github.com/KDD-OpenSource/DeepADoTS)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于时间序列的深度异常检测方法：[https://github.com/KDD-OpenSource/DeepADoTS](https://github.com/KDD-OpenSource/DeepADoTS)
- en: Telemanom – LSTMs for multivariate time-series data: [https://github.com/khundman/telemanom](https://github.com/khundman/telemanom)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telemanom – 用于多变量时间序列数据的LSTM：[https://github.com/khundman/telemanom](https://github.com/khundman/telemanom)
- en: DONUT – a variational auto-encoder for seasonal KPIs: [https://github.com/haowen-xu/donut](https://github.com/haowen-xu/donut)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DONUT – 用于季节性KPI的变分自动编码器：[https://github.com/haowen-xu/donut](https://github.com/haowen-xu/donut)
- en: A fantastic resource for material about outlier detection is the PyOD author's
    dedicated repository, available at [https://github.com/yzhao062/anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 关于异常检测的材料的一个极好资源是PyOD作者的专用存储库，位于[https://github.com/yzhao062/anomaly-detection-resources](https://github.com/yzhao062/anomaly-detection-resources)。
- en: Representing for similarity search
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于相似性搜索
- en: In this recipe, we want to find a way to decide whether two strings are similar
    given a representation of those two strings. We'll try to improve the way strings
    are represented in order to make more meaningful comparisons between strings. But
    first, we'll get a baseline using more traditional string comparison algorithms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们希望找到一种方法来确定两个字符串是否相似，给定这两个字符串的表示。我们将尝试改进字符串的表示方式，以便在字符串之间进行更有意义的比较。但首先，我们将使用更传统的字符串比较算法来建立基准。
- en: 'We''ll do the following: given a dataset of paired string matches, we''ll try
    out different functions for measuring string similarity, then a bag-of-characters
    representation, and finally a **Siamese neural network** (also called a **twin
    neural network**) dimensionality reduction of the string representation. We''ll
    set up a twin network approach for learning a latent similarity space of strings
    based on character n-gram frequencies.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行以下操作：给定一组成对字符串匹配的数据集，我们将尝试不同的函数来测量字符串的相似性，然后是基于字符n-gram频率的字符袋表示，最后是**共孪神经网络**（也称为**双胞胎神经网络**）的维度减少字符串表示。我们将设置一个双网络方法，通过字符n-gram频率学习字符串的潜在相似空间。
- en: A **Siamese neural network**, also sometimes called **twin neural network**,
    is named as such using the analogy of conjoined twins. It is a way to train a
    projection or a metric space. Two models are trained at the same time, and the
    output of the two models is compared. The training takes the comparison output
    rather than the models' outputs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**暹罗神经网络**，有时也被称为**双胞胎神经网络**，是以联合双胞胎的类比来命名的。这是一种训练投影或度量空间的方法。两个模型同时训练，比较的是两个模型的输出而不是模型本身的输出。'
- en: Getting ready
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As always, we need to download or load a dataset and install the necessary dependencies.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们需要下载或加载数据集并安装必要的依赖项。
- en: 'We''ll use a dataset of paired strings, where they are either matched or not
    based on whether they are similar:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一组配对字符串的数据集，根据它们的相似性来确定它们是否匹配：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can then read it in as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式读取它：
- en: '[PRE23]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The dataset includes pairs of strings that either correspond to each other
    or don''t correspond. It starts like this:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含一对相匹配或不匹配的字符串。它的开始如下：
- en: '![](img/b428a0db-d2f2-437f-9d15-0b9fb09da16e.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b428a0db-d2f2-437f-9d15-0b9fb09da16e.png)'
- en: 'There''s also a test dataset available from the same GitHub repo:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 同一 GitHub 仓库还提供了一个测试数据集：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can read it into a pandas DataFrame the same way as before:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像以前一样将它读入 pandas DataFrame 中：
- en: '[PRE25]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we''ll use a few libraries in this recipe that we can install like
    this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在本示例中使用一些库，可以像这样安装：
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The **Levenshtein distance** (also sometimes referred to as **edit distance**)
    measures the number of insertions, deletions, and substitutions that are necessary
    to transform one string into another string. It performs a search in order to
    come up with the shortest way to do this transformation. The library used here
    is a very fast implementation of this algorithm. You can find more about the `python-Levenshtein`
    library at [https://github.com/ztane/python-Levenshtein](https://github.com/ztane/python-Levenshtein).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**Levenshtein 距离**（有时也称为**编辑距离**）测量将一个字符串转换为另一个字符串所需的插入、删除和替换次数。它执行搜索以找出执行此转换的最短路径。这里使用的库是该算法的非常快速的实现。你可以在[https://github.com/ztane/python-Levenshtein](https://github.com/ztane/python-Levenshtein)了解更多关于`python-Levenshtein`库的信息。'
- en: The `annoy` library provides a highly optimized implementation of the nearest-neighbor
    search. Given a set of points and a distance, we can index all the points using
    a tree representation, and then for any point, we can traverse the tree to find
    similar data points. You can find out more about annoy at [https://github.com/spotify/annoy](https://github.com/spotify/annoy).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`annoy`库提供了高度优化的最近邻搜索实现。给定一组点和一个距离，我们可以使用树表示索引所有点，然后对于任何点，我们可以遍历树以找到类似的数据点。你可以在[https://github.com/spotify/annoy](https://github.com/spotify/annoy)了解更多关于annoy的信息。'
- en: Let's get to it.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: How to do it...
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: As mentioned before, we'll first calculate the baseline using standard string
    comparison functions, then we'll use a bag-of-characters approach, and then we'll
    learn a projection using a Siamese neural network approach. You can find the corresponding
    notebook on the book's GitHub repo, at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Representing%20for%20similarity%20search.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Representing%20for%20similarity%20search.ipynb).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将首先使用标准字符串比较函数计算基线，然后使用字符包法，最后采用暹罗神经网络方法学习投影。你可以在书的 GitHub 仓库中找到相应的笔记本，网址为[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Representing%20for%20similarity%20search.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Representing%20for%20similarity%20search.ipynb)。
- en: Baseline – string comparison functions
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Baseline – 字符串比较函数
- en: Let's first implement a few standard string comparison functions.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先实现几个标准字符串比较函数。
- en: 'We first need to make sure we clean up our strings:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要确保清理我们的字符串：
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We'll use this cleaning function in each of the string comparison functions
    that we'll see in the following code. We will use this function to remove special
    characters before any string distance calculation.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的代码中的每个字符串比较函数中使用这个清理函数。我们将使用这个函数在进行任何字符串距离计算之前去除特殊字符。
- en: 'Now we can implement simple string comparison functions. Let''s do the `Levenshtein`
    distance first:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现简单的字符串比较函数。首先做`Levenshtein`距离：
- en: '[PRE28]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now let''s do the `Jaro-Winkler` distance, which is the minimum number of single-character
    transpositions:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来计算`Jaro-Winkler`距离，它是最小单字符转置次数：
- en: '[PRE29]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We''ll also use the longest common substring between the compared pair. We
    can use `SequenceMatcher` for this, which is part of the Python standard library:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用被比较对之间的最长公共子串。我们可以使用`SequenceMatcher`来完成这一点，它是Python标准库的一部分：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can run over all string pairs and calculate the string distances based
    on each of the three methods. For each of the three algorithms, we can calculate
    the **area under the curve** (**AUC**) score to see how well it does at separating
    matching strings from non-matching strings:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以对所有字符串对进行遍历，并基于每种方法计算字符串距离。对于这三种算法，我们可以计算**曲线下面积**（**AUC**）分数，以查看它们在分离匹配字符串和非匹配字符串方面的表现如何：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The AUC scores for all algorithms are around 95%, which seems good. All three
    distances perform quite well already. Let's try to beat this.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所有算法的AUC分数约为95%，看起来很不错。这三种距离方法表现已经相当不错了。让我们尝试超过这个水平。
- en: Bag-of-characters approach
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符包方法
- en: We'll now implement a bag-of-characters method for string similarity.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现一种基于字符包的字符串相似性方法。
- en: 'A bag of characters means that we will create a histogram of characters, or
    in other words, we will count the occurrences of the characters in each word:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 字符包表示意味着我们将创建一个字符的直方图，或者换句话说，我们将计算每个单词中字符的出现次数：
- en: '[PRE32]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We've set the range of `ngrams` to just `1`, which means we want only single
    characters. This parameter can be interesting, however, if you want to include
    longer-range dependencies between characters, rather than just the character frequencies.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将`ngrams`的范围设置为`1`，这意味着我们只希望单个字符。然而，如果你希望包括字符之间更长范围的依赖关系而不仅仅是字符频率，这个参数可能会有趣。
- en: 'Let''s see what performance we can get out of this:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们可以通过这个方法获得什么样的性能：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you can see in the AUC score of about 93%, this approach doesn't yet perform
    quite as well overall, although the performance is not completely bad. So let's
    try to tweak this.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在大约93%的AUC分数中所见，这种方法整体表现还不如上面的方法好，尽管表现并非完全糟糕。因此，让我们尝试调整一下。
- en: Siamese neural network approach
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Siamese神经网络方法
- en: Now we'll implement a Siamese network to learn a projection that represents
    the similarities (or differences) between strings.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现一个Siamese网络，学习表示字符串之间相似性（或差异）的投影。
- en: The Siamese network approach may seem a little daunting if you are not familiar
    with it. We'll discuss it further in the *How it works...* section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对Siamese网络方法感到有些陌生，可能会觉得有点令人畏惧。我们将在*工作原理...*部分进一步讨论它。
- en: 'Let''s start with the string featurization function:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从字符串特征化函数开始：
- en: '[PRE34]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The `create_string_featurization_model` function returns a model for string
    featurization. The featurization model is a non-linear projection of the bag-of-characters
    output.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_string_featurization_model`函数返回一个字符串特征化模型。特征化模型是字符包输出的非线性投影。'
- en: 'The function has the following parameters:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 函数有以下参数：
- en: '`feature_dimensionality`: The number of features coming from the vectorizer
    (that is, the dimensionality of the bag-of-characters output)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_dimensionality`：来自向量化器的特征数（即字符包输出的维度）'
- en: '`output_dim`: The dimensions of the embedding/projection that we are trying
    to create'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dim`：我们试图创建的嵌入/投影的维度'
- en: Next, we need to create the **conjoined twins** of the two models. For this,
    we need a comparison function. We take the normalized Euclidean distance. This
    is the Euclidean distance between the two *L2-*normalized projected vectors.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建这两个模型的**联合孪生体**。为此，我们需要一个比较函数。我们使用归一化的欧氏距离。这是两个*L2*归一化投影向量之间的欧氏距离。
- en: 'The *L2* norm of a vector *x* is defined as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 向量*x*的*L2*范数定义如下：
- en: '![](img/91661c56-aaae-4e42-8cc6-1c08744738cf.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91661c56-aaae-4e42-8cc6-1c08744738cf.png)'
- en: The *L2* normalization is dividing the vector *x* by its norm.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '*L2*归一化是将向量*x*除以其范数。'
- en: 'We can define the distance function as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下定义距离函数：
- en: '[PRE35]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now the Siamese network can use the function by wrapping it as a Lambda layer.
    Let''s define how to conjoin the twins or, in other words, how we can wrap it
    into a bigger model so we can train with pairs of strings and the label (that
    is, similar and dissimilar):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Siamese网络可以通过将其包装为Lambda层来使用该函数。让我们定义如何联接孪生体，或者换句话说，我们如何将其包装成一个更大的模型，以便我们可以训练字符串对及其标签（即相似和不相似）。
- en: '[PRE36]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This is a verbose way of saying: take the two networks, calculate the normalized
    Euclidean distance, and take the distance as the output.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种冗长的说法：取两个网络，计算归一化欧氏距离，并将距离作为输出。
- en: 'Let''s create the twin network and train:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建双网络并进行训练：
- en: '[PRE37]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This creates a model with an output of 10 dimensions; given 41 dimensions from
    the n-gram featurizer, this means we have a total of 420 parameters (*41 * 10
    + 10*).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个输出为 10 维的模型；从 n-gram 特征提取器中获得了 41 维，这意味着我们总共有 420 个参数 (*41 * 10 + 10*)。
- en: As we've mentioned before, the output of our combined network is the Euclidean
    distance between the two outputs. This means we have to invert our target (matched)
    column in order to change the meaning from similar to distant, so that 1 corresponds
    to different and 0 to the same. We can do this easily by subtracting from 1.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，我们组合网络的输出是两个输出之间的欧氏距离。这意味着我们必须反转我们的目标（匹配）列，以便从相似变为不同，这样 1 对应不同，0 对应相同。我们可以通过简单地从
    1 中减去来轻松实现这一点。
- en: 'We can now get the performance of this new projection:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以获得这种新投影的性能：
- en: '[PRE38]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We've managed to beat the other methods. And that's before we've even tried
    to tune any hyperparameters. Our projection clearly works in highlighting differences
    between strings that are important for string similarity comparisons.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功地击败了其他方法。甚至在调整任何超参数之前，我们的投影显然在突出显示对字符串相似性比较重要的差异方面起作用。
- en: How it works...
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The scikit-learn `CountVectorizer` counts the occurrences of features in strings.
    A common use case is to count words in sentences – this representation is called
    a **bag of words**, and in that case, the features would be words. In our case,
    we are interested in character-based features, so we just count how many times
    an **a **occurred, a **b **occurred, and so on. We could make this representation
    cleverer by representing tuples of successive characters such as **ab** or **ba**;
    however, that's beyond our scope here.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 的 `CountVectorizer` 计算字符串中特征的出现次数。一个常见的用例是计算句子中的单词数 —— 这种表示称为**词袋**，在这种情况下，特征将是单词。在我们的情况下，我们对基于字符的特征感兴趣，因此我们只计算
    **a ** 出现的次数、**b ** 出现的次数，依此类推。我们可以通过表示连续字符元组如 **ab** 或 **ba** 来使这种表示更加智能化；然而，这超出了我们当前的范围。
- en: A Siamese network training is the situation where two (or more) neural networks
    are trained against each other by comparing the output of the networks given a
    pair (or tuple) of inputs and the knowledge of the difference between these inputs.
    Often the Siamese network consists of the same network (that is, the same weights).
    The comparison function between the two network outputs can be metrics such as
    the Euclidean distance or the cosine similarity. Since we know whether the two
    inputs are similar, and even how similar they are, we can train against this knowledge
    as the target.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Siamese 网络训练是指两个（或更多）神经网络相互训练，通过比较给定一对（或元组）输入的网络输出以及这些输入之间的差异的知识。通常，Siamese
    网络由相同的网络组成（即相同的权重）。两个网络输出之间的比较函数可以是诸如欧氏距离或余弦相似度之类的度量。由于我们知道两个输入是否相似，甚至知道它们有多相似，我们可以根据这些知识训练目标。
- en: 'The following diagram illustrates the information flow and the different building
    blocks that we''ll be using:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了信息流和我们将使用的不同构建块：
- en: '![](img/782734e6-0391-439c-bd33-4ba62c6f599a.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/782734e6-0391-439c-bd33-4ba62c6f599a.png)'
- en: Given the two strings that we want to compare, we'll use the same model to create
    features from each one, resulting in two representations. We can then compare
    these representations, and we hope that the comparison correlates with the outcome,
    so that if our comparison shows a big difference, the strings will be dissimilar,
    and if the comparison shows little difference, then the strings will be similar.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们想要比较的两个字符串，我们将使用同一模型从每个字符串创建特征，从而得到两个表示。然后我们可以比较这些表示，希望比较结果与结果相关联，因此如果我们的比较显示很大的差异，那么字符串将是不相似的，如果比较显示很小的差异，那么字符串将是相似的。
- en: We can actually directly train this complete model, given a string comparison
    model and a dataset consisting of a pair of strings and a target. This training
    will tune the string featurization model so the representation will be more useful.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以直接训练这个完整的模型，给定一个字符串比较模型和一个由字符串对和目标组成的数据集。这种训练将调整字符串特征化模型，使其表示更加有用。
- en: Recommending products
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐产品
- en: In this recipe, we'll be building a recommendation system. A recommender is
    an information-filtering system that predicts rankings or similarities by bringing
    content and social connections together.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将构建一个推荐系统。推荐系统是一个信息过滤系统，通过将内容和社交连接在一起预测排名或相似性。
- en: We'll download a dataset of book ratings that have been collected from the Goodreads
    website, where users rank and review books that they've read. We'll build different
    recommender models, and we'll suggest new books based on known ratings.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载从Goodreads网站收集的图书评分数据集，用户在该网站上对阅读过的书籍进行排名和评论。我们将构建不同的推荐模型，并基于已知的评分推荐新书。
- en: Getting ready
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To prepare for our recipe, we'll download the dataset and install the required
    dependencies.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备我们的步骤，我们将下载数据集并安装所需的依赖项。
- en: 'Let''s get the dataset and install the two libraries we''ll use here – `spotlight`
    and `lightfm` are recommender libraries:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们获取数据集，并在这里安装我们将使用的两个库 – `spotlight` 和 `lightfm` 是推荐系统库：
- en: '[PRE39]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then we need to get the dataset of book ratings:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要获取图书评分的数据集：
- en: '[PRE40]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The dataset comes in the shape of an interaction object. According to spotlight''s
    documentation, an interaction object can be defined as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以交互对象的形式呈现。根据spotlight的文档，交互对象可以定义如下：
- en: '[It] contains (at a minimum) a pair of user-item interactions, but can also
    be enriched with ratings, timestamps, and interaction weights.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[它]至少包含一对用户-项目交互，但也可以丰富评分、时间戳和交互权重。'
- en: For **implicit feedback** scenarios, user IDs and item IDs should only be provided
    for user-item pairs where an interaction was observed. All pairs that are not
    provided are treated as missing observations, and often interpreted as (implicit)
    negative signals.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**隐性反馈**场景，只应提供已观察到交互的用户-项目对的用户ID和项目ID。未提供的所有对都被视为缺失观察，并且通常被解释为（隐性）负信号。
- en: For **explicit feedback **scenarios, user IDs, item IDs, and ratings should
    be provided for all user-item-rating triplets that were observed in the dataset.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**显性反馈**场景，应提供数据集中观察到的所有用户-项目-评分三元组的用户ID、项目ID和评分。
- en: 'We have the following training and test datasets:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下训练和测试数据集：
- en: '[PRE41]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In order to know which books the item numbers refer to, we''ll download the
    following CSV file:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 要知道项目编号指的是哪些书籍，我们将下载以下CSV文件：
- en: '[PRE42]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we''ll implement a function to get the book titles by `id`. This will
    be useful for showing our recommendations later:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个根据`id`获取书名的函数。这对于后续展示我们的推荐非常有用：
- en: '[PRE43]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now we can use this function as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按以下方式使用这个函数：
- en: '[PRE44]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now that we've got the dataset and the libraries installed, we can start our
    recipe.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获取了数据集并安装了所需的库，我们可以开始我们的步骤了。
- en: How to do it...
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We'll first use a matrix factorization model, then a deep learning model. You
    can find more examples in the Jupyter notebook available at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Recommending_products.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Recommending_products.ipynb).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用矩阵分解模型，然后使用深度学习模型。您可以在Jupyter笔记本中找到更多示例，链接如下：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Recommending_products.ipynb](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/blob/master/chapter03/Recommending_products.ipynb)。
- en: 'We have to set a lot of parameters, including the number of latent dimensions
    and the number of epochs:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须设置许多参数，包括潜在维度的数量和周期数：
- en: '[PRE45]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The result is shown in the following screenshot:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在以下截图中：
- en: '![](img/b57e6718-d537-4cae-b6ce-138017220b1c.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b57e6718-d537-4cae-b6ce-138017220b1c.png)'
- en: 'We get the following recommendations:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下的推荐：
- en: '![](img/8257d8a3-4713-4b5e-b469-dc9b3aa82a38.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8257d8a3-4713-4b5e-b469-dc9b3aa82a38.png)'
- en: 'Now we''ll use the `lightfm` recommendation algorithm:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用`lightfm`推荐算法：
- en: '[PRE46]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can also look at the recommendations, as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以查看推荐内容，如下所示：
- en: '![](img/53b50f3c-aad6-453e-95c9-3d9e07787b1f.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53b50f3c-aad6-453e-95c9-3d9e07787b1f.png)'
- en: Both recommenders have their applications. On the basis of the precision at
    *k (k=5)* for both recommenders, we can conclude that the second recommender,
    `lightfm`, performs better.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 两个推荐系统都有它们的应用。基于精度在*k（k=5）*的基础上，我们可以得出结论，第二个推荐系统`lightfm`表现更好。
- en: How it works...
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: Recommenders recommend products to users.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统向用户推荐产品。
- en: 'They can produce recommendations based on different principles, such as the
    following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 他们可以根据不同的原则提出建议，例如以下内容：
- en: They can predict based on the assumption that customers who have shown similar
    tastes in previous purchases will buy similar items in the future (**collaborative
    filtering**).
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们可以基于这样一个假设进行预测：在以前的购买中表现出类似品味的顾客将来会购买类似的物品（**协同过滤**）。
- en: Predictions based on the idea that customers will have an interest in items
    similar to the ones they've bought in the past (**content-based filtering**).
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于顾客可能会对与他们过去购买的物品相似的物品感兴趣的理念进行预测（**基于内容的过滤**）。
- en: Predictions based on a combination of collaborative filtering, content-based
    filtering, or other approaches (a **hybrid recommender**).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于协同过滤、基于内容的过滤或其他方法的组合预测（**混合推荐**）。
- en: Hybrid models can combine approaches in different ways, such as making content-based
    and collaborative-based predictions separately and then adding up the scores,
    or by unifying the approaches into a single model.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型可以以不同方式组合方法，例如分别进行基于内容和基于协同的预测，然后将分数相加，或者将这些方法统一到一个单一模型中。
- en: 'Both models we''ve tried are based on the idea that we can separate the influences
    of users and items. We''ll explain each model in turn, and how they combine approaches,
    but first let''s explain the metric we are using: precision at *k*.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的两个模型都基于这样一种想法，即我们可以分离用户和物品的影响。我们将依次解释每个模型及其如何结合方法，但首先让我们解释一下我们正在使用的度量标准：*k*的精度。
- en: Precision at k
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k处的精度
- en: The metric we are extracting here is **precision at *k***. For example, precision
    at 10 calculates the number of relevant results among the top *k* documents, with
    typically *k=5* or *k=10*.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里提取的度量标准是**k处的精度**。例如，精度在10处计算前*k*个文档中的相关结果数量，通常*k*=5或*k*=10。
- en: 'Precision at *k* doesn''t take into account the ordering within the top *k*
    results, nor does it include how many of the really good results that we absolutely
    should have captured are actually returned: that would be recall. That said, precision
    at *k* is a sensible metric, and it''s intuitive.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 精度在*k*上不考虑在前*k*个结果中的排序，也不包括我们绝对应该捕捉到的真正好的结果的数量：这将是召回率。尽管如此，精度在*k*上是一个合理的度量标准，而且很直观。
- en: Matrix factorization
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: 'The explicit model in `spotlight` is based on the matrix factorization technique
    presented by Yehuda Koren and others (in *Matrix Factorization Techniques for
    Recommender Systems*, 2009). The basic idea is that a user-item (interaction)
    matrix can be decomposed into two components, ![](img/30cd346a-e2c6-4fab-b244-3ae3d317cb46.png) and ![](img/d3edb87b-cb64-441d-8da4-7d503d3d42f0.png),
    representing user latent factors and item latent factors respectively, so that
    recommendations given an item ![](img/b8cafe93-d461-4120-8df1-26c77ed61a27.png)
    and a user ![](img/6c7fa830-ca0a-42fe-8c15-35445af4e31c.png) can be calculated
    as follows:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '`spotlight`中的显式模型基于Yehuda Koren等人在《推荐系统的矩阵分解技术》（2009）中提出的矩阵分解技术。基本思想是将用户-物品（交互）矩阵分解为表示用户潜在因素和物品潜在因素的两个组成部分，以便根据给定物品和用户进行推荐，计算如下：'
- en: '![](img/4a4d3d97-fbf1-454d-a708-f387edd25f54.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a4d3d97-fbf1-454d-a708-f387edd25f54.png)'
- en: '**Matrix decomposition** or **matrix factorization** is the factorization of
    a matrix into a product of matrices. Many different such decompositions exist,
    serving a variety of purposes.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵分解**或**矩阵因子分解**是将矩阵分解为矩阵乘积的过程。存在许多不同的这类分解方法，用途各不相同。'
- en: A relatively simple decomposition is the **singular value decomposition** (**SVD**)
    however, modern recommenders use other decompositions. Both the `spotlight` matrix
    factorization and the `lightfm` model use linear integrations.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对简单的分解是**奇异值分解**（**SVD**），但现代推荐系统使用其他分解方法。`spotlight`矩阵分解和`lightfm`模型都使用线性整合。
- en: The lightfm model
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`lightfm`模型'
- en: 'The `lightfm` model was introduced in a paper by Kula (*Metadata Embeddings
    for User and Item Cold-start Recommendations,* 2015). More specifically, we use
    the WARP loss, which is explained in the paper *WSABIE: Scaling Up To Large Vocabulary
    Image Annotation* by Jason Weston et al., from 2011.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`lightfm`模型是由Kula在《用户和物品冷启动推荐的元数据嵌入》（2015）中介绍的。更具体地说，我们使用的是WARP损失，该损失在Jason
    Weston等人于2011年的《WSABIE：大规模词汇图像注释的扩展》中有详细解释。'
- en: 'In the `lightfm` algorithm, the following function is used for predictions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `lightfm` 算法中，用于预测的函数如下：
- en: '![](img/5434beaa-96ea-4332-8ea8-fed1f65ab755.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5434beaa-96ea-4332-8ea8-fed1f65ab755.png)'
- en: In the preceding function, we have bias terms for users and items and ![](img/748792ad-b5cf-4b86-99e4-a2d9a7885a5a.png)
    is the sigmoid function.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述函数中，我们为用户和项目有偏差项，![](img/748792ad-b5cf-4b86-99e4-a2d9a7885a5a.png) 是 sigmoid
    函数。
- en: 'The model training maximizes the likelihood of the data conditional on the
    parameters expressed as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练最大化数据在给定参数条件下的似然性表达如下：
- en: '![](img/715a6d4a-09ac-4fb0-9ebd-ecbc4ef2a389.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/715a6d4a-09ac-4fb0-9ebd-ecbc4ef2a389.png)'
- en: There are different ways to measure how well recommenders are performing and,
    as always, which one we choose to use depends on the goal we are trying to achieve.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以衡量推荐系统的表现，而我们选择使用哪种方法取决于我们试图实现的目标。
- en: See also
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Again, there are a lot of libraries around that make it easy to get up and
    running. First of all, I''d like to highlight these two, which we''ve already
    used in this recipe:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，有很多库可以轻松启动和运行。首先，我想强调这两个已经在本配方中使用过的库：
- en: lightfm: [https://github.com/lyst/lightfm](https://github.com/lyst/lightfm)
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lightfm：[https://github.com/lyst/lightfm](https://github.com/lyst/lightfm)
- en: Spotlight: [https://maciejkula.github.io/spotlight](https://maciejkula.github.io/spotlight)
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spotlight：[https://maciejkula.github.io/spotlight](https://maciejkula.github.io/spotlight)
- en: 'But some others are very promising, too:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 但还有一些其他很有前景的方法：
- en: Polara, which includes an algorithm called HybridSVD that seems to be very strong: [https://github.com/evfro/polara](https://github.com/evfro/polara)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polara 包含一个名为 HybridSVD 的算法，似乎非常强大：[https://github.com/evfro/polara](https://github.com/evfro/polara)
- en: DeepRec, which provides deep learning models for recommendations (based on TensorFlow): [https://github.com/cheungdaven/DeepRec](https://github.com/cheungdaven/DeepRec)
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepRec 提供基于 TensorFlow 的推荐深度学习模型：[https://github.com/cheungdaven/DeepRec](https://github.com/cheungdaven/DeepRec)
- en: You can find a demonstration of library functionality for item ranking with
    a dataset at the following repo: [https://github.com/cheungdaven/DeepRec/blob/master/test/test_item_ranking.py](https://github.com/cheungdaven/DeepRec/blob/master/test/test_item_ranking.py).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下存储库中找到有关使用数据集进行项目排名的库功能演示：[https://github.com/cheungdaven/DeepRec/blob/master/test/test_item_ranking.py](https://github.com/cheungdaven/DeepRec/blob/master/test/test_item_ranking.py)
- en: Microsoft has been writing about recommender best practices: [https://github.com/Microsoft/Recommenders](https://github.com/Microsoft/Recommenders).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft 已经撰写了有关推荐最佳实践的文章：[https://github.com/Microsoft/Recommenders](https://github.com/Microsoft/Recommenders)
- en: Last, but not least, you might find the following reading list about recommender
    systems useful: [https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/readingList.md](https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/readingList.md).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，您可能会发现以下有关推荐系统的阅读列表有用：[https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/readingList.md](https://github.com/DeepGraphLearning/RecommenderSystems/blob/master/readingList.md)
- en: Spotting fraudster communities
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现欺诈者社区
- en: In this recipe, we'll try to detect fraud communities using methods from network
    analysis. This is a use case that often seems to come up in graph analyses and
    intuitively appeals because, when carrying out fraud detection, we are interested
    in relationships between people, such as whether they live close together, are
    connected over social media, or have the same job.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将尝试使用网络分析方法来检测欺诈社区。这是一个在图分析中经常出现的用例，因为在进行欺诈检测时，我们关注的是人们之间的关系，比如他们是否住在附近，是否通过社交媒体连接，或者是否从事相同的工作。
- en: Getting ready
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to get everything in place for the recipe, we'll install the required
    libraries and we'll download a dataset.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备好制作这个配方，我们将安装所需的库并下载数据集。
- en: 'We will use the following libraries:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下库：
- en: '`networkx` - is a graph analysis library: [https://networkx.github.io/documentation](https://networkx.github.io/documentation).'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`networkx` - 是一个图分析库：[https://networkx.github.io/documentation](https://networkx.github.io/documentation)'
- en: '`annoy` - is a very efficient nearest-neighbors implementation: [https://github.com/spotify/annoy](https://github.com/spotify/annoy).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`annoy` - 是一个非常高效的最近邻实现：[https://github.com/spotify/annoy](https://github.com/spotify/annoy)'
- en: '`tqdm` - to provide us with progress bars: [https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm).'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tqdm` - 提供进度条：[https://github.com/tqdm/tqdm](https://github.com/tqdm/tqdm)'
- en: 'Furthermore, we''ll use SciPy, but this comes with the Anaconda distribution:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将使用 SciPy，但这是在 Anaconda 分发中包含的：
- en: '[PRE47]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We'll use the following dataset of fraudulent credit card transactions: [https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下欺诈信用卡交易数据集：[https://www.kaggle.com/mlg-ulb/creditcardfraud](https://www.kaggle.com/mlg-ulb/creditcardfraud)。
- en: 'The Credit Card Fraud dataset contains transactions made by credit cards in September 2013 by
    European  cardholders. This dataset presents transactions that occurred over two days, with
    492 fraudulent transactions out of 284,807 transactions. The dataset is highly unbalanced:
    the positive class (fraud) accounts for 0.172% of all transactions.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 信用卡欺诈数据集包含了2013年9月欧洲持卡人使用信用卡的交易记录。该数据集包含了284,807笔交易，其中492笔是欺诈交易。该数据集非常不平衡：正类（欺诈）占所有交易的0.172%。
- en: 'Let''s import the dataset, and then split it into training and test sets:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入数据集，然后将其分割为训练集和测试集：
- en: '[PRE48]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We are ready! Let's do the recipe!
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好了！让我们来做这道菜吧！
- en: How to do it...
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: We'll first create an adjacency matrix, then we can apply the community detection
    methods to it, and lastly, we'll evaluate the quality of the generated communities.
    The whole process has the added difficulty associated with a large dataset, which
    means we can only apply certain algorithms.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们会创建一个邻接矩阵，然后我们可以对其应用社区检测方法，最后我们将评估生成社区的质量。整个过程由于大数据集而增加了难度，这意味着我们只能应用某些算法。
- en: Creating an adjacency matrix
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建邻接矩阵
- en: First, we need to calculate the distances of all points. This is a real problem
    with a large dataset such as this. You can find several approaches online.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要计算所有点的距离。对于这样一个大数据集，这是一个真正的问题。你可以在网上找到几种方法。
- en: 'We use the `annoy` library from Spotify for this purpose, which is very fast
    and memory-efficient:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Spotify的`annoy`库进行此目的，这是非常快速和内存高效的：
- en: '[PRE49]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can then initialize our adjacency matrix with the distances as given by
    our index:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以根据索引给出的距离初始化我们的邻接矩阵：
- en: '[PRE50]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can now apply some community detection algorithms.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以应用一些社区检测算法。
- en: Community detection algorithms
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 社区检测算法
- en: 'The size of our matrix leaves us with limited choice. We''ll apply the two
    following algorithms:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们矩阵的大小限制了我们的选择。我们将应用以下两个算法：
- en: '**Strongly Connected Components** (**SCC**)'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强连通分量** (**SCC**)'
- en: The Louvain algorithm
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Louvain算法
- en: 'We can apply the SCC algorithm directly onto the adjacency matrix as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在邻接矩阵上应用SCC算法，如下所示：
- en: '[PRE51]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For the second algorithm, we first need to convert the adjacency matrix to
    a graph; this means that we treat each point in the matrix as an edge between
    nodes. In order to save space, we use a simplified graph class for this:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个算法，我们首先需要将邻接矩阵转换为图形；这意味着我们将矩阵中的每个点视为节点之间的边。为了节省空间，我们使用了一个简化的图形类：
- en: '[PRE52]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then we can apply the Louvain algorithm as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以按以下方式应用Louvain算法：
- en: '[PRE53]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Now we have two different partitions of our dataset. Let's find out if they
    are worth anything!
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有数据集的两个不同分区。让我们看看它们是否有价值！
- en: Evaluating the communities
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估社区
- en: In the ideal case, we'd expect that some communities have only fraudsters in
    them, while others (most) have none at all. This purity is what we would be looking
    for in a perfect community. However, since we also possibly want some suggestions
    of who else might be a fraudster, we would anticipate some points to be labeled
    as fraudsters in a majority-nonfraudster group and vice versa.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，我们期望一些社区只有欺诈者，而其他（大多数）社区则完全没有。这种纯度是我们在完美社区中寻找的。然而，由于我们可能也希望得到一些其他可能是欺诈者的建议，我们预计一些点会在大多数非欺诈组中被标记为欺诈者，反之亦然。
- en: 'We can start by looking at the histograms of fraud frequency per community.
    The Louvain fraudster distribution looks like this:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从观察每个社区的欺诈频率直方图开始。Louvain欺诈者分布如下：
- en: '![](img/8c6a7b40-e85a-4ab3-950c-a58c34b5ac75.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c6a7b40-e85a-4ab3-950c-a58c34b5ac75.png)'
- en: This shows that communities have a very high frequency of people who are not
    fraudsters, and very few other values. But can we quantify how good this is?
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明社区中有很多非欺诈者，而其他数值很少。但我们能量化这有多好吗？
- en: We can describe the fraudster distribution by calculating the **class entropy**
    in each cluster. We'll explain entropy in the *How it works...* section.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算每个群集中的**类熵**来描述欺诈者分布。我们将在*它的工作原理...*部分解释熵。
- en: We can then create appropriately chosen random experiments to see if any other
    community assignments would have resulted in a better class entropy. If we randomly
    shuffle the fraudsters and then calculate the entropy across communities, we get
    an entropy distribution. This will give us a **p-value**, the **statistical significance**,
    for the entropy of the Louvain communities.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建适当选择的随机实验，以查看任何其他社区分配是否会导致更好的类熵。如果我们随机重新排序欺诈者，然后计算跨社区的熵，我们将得到一个熵分布。这将为我们提供Louvain社区熵的**p值**，**统计显著性**。
- en: The **p-value** is the probability that we get a distribution like this (or
    better) purely by chance.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '**p值**是我们仅凭偶然性就能获得这种（或更好）分布的概率。'
- en: You can find the implementation for the sampling in the notebook on GitHub.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上的笔记本中找到采样的实现。
- en: We get a very low significance, meaning that it is highly unlikely to have gotten
    anything like this by chance, which leads us to conclude that we have found meaningful
    clusters in terms of identifying fraudsters.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了非常低的显著性，这意味着几乎不可能通过偶然得到类似的结果，这使我们得出结论，我们在识别欺诈者方面找到了有意义的聚类。
- en: How it works...
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: The hardest part of network analysis with a big dataset is constructing the
    adjacency matrix. You can find different approaches in the notebook available
    online. The two problems are runtime and memory. Both can increase exponentially
    with the number of data points.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 处理大数据集进行网络分析的最困难部分是构建邻接矩阵。您可以在在线笔记本中找到不同的方法。两个问题是运行时间和内存。这两者都可以随着数据点的数量呈指数增长。
- en: Our dataset contains 284,807 points. This means a full connectivity matrix between
    all points would take a few hundred gigabytes (at 4 bytes per point), ![](img/c2ffa319-8c64-46e9-922a-6b272cff3262.png).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含284,807个点。这意味着在所有点之间建立完全连接的矩阵将占用数百GB空间（每个点4字节），![](img/c2ffa319-8c64-46e9-922a-6b272cff3262.png)。
- en: We are using a sparse matrix where most adjacencies are 0s if they don't exceed
    the given threshold. We represent each connection between the points as a Boolean
    (1 bit) and we take a sample of 33%, 93,986 points, rather than the full dataset.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用稀疏矩阵，其中大多数邻接都是0，如果它们不超过给定的阈值。我们将每个点之间的每个连接表示为布尔值（1位），并且我们仅采用33%的样本，即93,986个点，而不是全部数据集。
- en: Graph community algorithms
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图形社区算法
- en: Let's go through two graph community algorithms to get an idea of how they work.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解两个图形社区算法的工作原理。
- en: Louvain algorithm
  id: totrans-378
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Louvain算法
- en: We've used the Louvain algorithm in this recipe. The algorithm was published
    in 2008 by Blondel *et al.* ([https://arxiv.org/abs/0803.0476](https://arxiv.org/abs/0803.0476)). Since
    its time complexity is ![](img/8f50fad5-5b38-4055-b54e-b0494c083389.png), the
    Louvain algorithm can and has been used with big datasets, including data from
    Twitter containing 2.4 million nodes and 38 million links.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个方案中使用了Louvain算法。该算法由Blondel等人于2008年发布（[https://arxiv.org/abs/0803.0476](https://arxiv.org/abs/0803.0476)）。由于其时间复杂度为![](img/8f50fad5-5b38-4055-b54e-b0494c083389.png)，Louvain算法已被用于包括Twitter在内的大数据集中，该数据集包含2.4百万节点和3800万链接。
- en: 'The main idea of the Louvain algorithm is to proceed in an agglomerative manner
    by successively merging communities together so as to increase their connectedness. The
    connectedness is measured by edge modularity, ![](img/cd651623-1eea-4d47-a0c3-6510d7c7d524.png),
    which is the density of edges within a community connected to other vertices of
    the same community versus the vertices of other communities. Any switch of community
    for a vertex ![](img/aae57f62-676c-4db5-905a-7e4064c902e3.png) has an associated ![](img/766320a0-c764-47a3-9ef4-0a65818930db.png). After
    the initial assignment of each of the vertices to their own communities, the heuristic
    operates in two steps: a greedy assignment of vertices to communities, and a coarse
    graining.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: Louvain算法的主要思想是通过逐步合并社区来增加它们的连接性。连接性通过边模块度![](img/cd651623-1eea-4d47-a0c3-6510d7c7d524.png)来衡量，这是社区内边缘与其他社区顶点相连的密度与其他社区顶点的比例。每个顶点的社区切换![](img/aae57f62-676c-4db5-905a-7e4064c902e3.png)都有一个相关的![](img/766320a0-c764-47a3-9ef4-0a65818930db.png)。在将每个顶点分配到它们自己的社区后，启发式操作分为两步：将顶点贪婪地分配到社区，以及粗粒度化。
- en: For all vertices ![](img/60211e6a-f320-4dc8-9bb5-16b631454862.png), assign them
    to the community so that ![](img/e7248b3a-8ff8-4182-bd06-6cccdf1cf6ca.png) is
    the highest it can be. This step can be repeated a few times until no improvement
    in modularity occurs.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有顶点 ![](img/60211e6a-f320-4dc8-9bb5-16b631454862.png)，分配它们到社区，使得 ![](img/e7248b3a-8ff8-4182-bd06-6cccdf1cf6ca.png)
    尽可能高。这一步骤可以重复几次，直到模块度不再改善。
- en: All communities are treated as vertices. This means that edges are also grouped
    together so that all edges that are part of the vertices that were grouped together
    are now edges of the newly created vertex.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有社区都被视为顶点。这意味着边也被分组在一起，以便所有属于被分组顶点的边现在成为新创建顶点的边。
- en: These two steps are iterated until no further improvement in modularity occurs.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 直到模块度再无改善为止，这两个步骤将被重复进行。
- en: Girvan–Newman algorithm
  id: totrans-384
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Girvan-Newman 算法
- en: As an example of another algorithm, let's look at the Girvan–Newman algorithm.
    The Girvan–Newman algorithm (by Girvan and Newman, 2002, with the paper available
    at [https://www.pnas.org/content/99/12/7821](https://www.pnas.org/content/99/12/7821))
    is based on the concept of the shortest path between nodes. The **edge betweenness **of
    an edge is the number of shortest paths between nodes that run along the edge.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一种算法的例子，让我们看看 Girvan-Newman 算法。Girvan-Newman 算法（由 Girvan 和 Newman，2002 年提出，相关论文请参见
    [https://www.pnas.org/content/99/12/7821](https://www.pnas.org/content/99/12/7821)）基于节点间的最短路径概念。**边介数**是边上运行的节点之间最短路径数。
- en: 'The algorithm works as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 算法工作如下：
- en: Calculate the edge betweenness of all edges.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有边的边介数。
- en: Remove the edge of the highest edge betweenness.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除具有最高边介数的边。
- en: Recalculate the edge betweenness.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算边的边介数。
- en: Iterate *steps 2* and *3* until no edges remain.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 和 *步骤 3* 直到没有边剩余。
- en: The result is a dendrogram that shows the arrangement of clusters by the steps
    of the algorithm.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个树状图，显示了算法步骤中聚类的排列方式。
- en: The whole algorithm has a time complexity of ![](img/83121cdb-be4a-4a35-8e79-a30d97d7b1f0.png),
    with edges *m* and vertices *n*.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 整个算法的时间复杂度为 ![](img/83121cdb-be4a-4a35-8e79-a30d97d7b1f0.png)，具有边数 *m* 和顶点数
    *n*。
- en: Information entropy
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息熵
- en: 'Given a discrete random variable ![](img/de6d5b7f-f53f-4b22-87f4-dc2b89398466.png) with
    possible values (or outcomes) ![](img/d7ba7d07-5891-4791-bafa-23b331a70c2b.png) that
    occur with probability ![](img/bb4b527e-1e5e-4877-875b-03259d6a76e0.png), the
    entropy of ![](img/491e6796-8028-4b71-8efb-1f7ce3b9881a.png) is formally defined
    as follows:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个离散随机变量 ![](img/de6d5b7f-f53f-4b22-87f4-dc2b89398466.png)，其可能值（或结果）为 ![](img/d7ba7d07-5891-4791-bafa-23b331a70c2b.png)，以概率
    ![](img/bb4b527e-1e5e-4877-875b-03259d6a76e0.png) 发生，![](img/491e6796-8028-4b71-8efb-1f7ce3b9881a.png)
    的熵正式定义如下：
- en: '![](img/4362bfae-04db-43bf-9c60-ed3134dcc2ce.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4362bfae-04db-43bf-9c60-ed3134dcc2ce.png)'
- en: This is generally taken as the level of surprise, uncertainty, or chaos in a
    random variable.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被视为随机变量中的意外性、不确定性或混乱程度。
- en: If a variable is not discrete, we can apply binning (for example, via a histogram)
    or use non-discrete versions of the formula.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果变量不是离散的，我们可以应用分箱（例如，通过直方图）或者使用非离散版本的公式。
- en: There's more...
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We could have applied other algorithms, such as SCC, published by David Pearce
    in 2005 (in *An Improved Algorithm for Finding the Strongly Connected Components
    of a Directed Graph*).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以应用其他算法，如 2005 年由 David Pearce 发布的 SCC 算法（在《找到有向图的强连通分量的改进算法》中）。
- en: 'We can try this method out as well:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以尝试这种方法：
- en: '[PRE54]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The SCC community fraudster distribution looks like this:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: SCC 社区欺诈分布如下：
- en: '![](img/7663ef5e-972f-430e-b909-efc2c2175b99.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7663ef5e-972f-430e-b909-efc2c2175b99.png)'
- en: And again we get a p-value that shows very high statistical significance. This
    means that this is unlikely to have occurred by pure chance, and indicates that
    our method is indeed a good classifier for fraud.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 再次得到一个 p 值，显示出非常高的统计显著性。这意味着这不太可能仅仅是偶然发生的，表明我们的方法确实是欺诈的良好分类器。
- en: 'We could have also applied more traditional clustering algorithms. For example,
    the affinity propagation algorithm takes an adjacency matrix, as follows:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以应用更传统的聚类算法。例如，亲和传播算法接受一个邻接矩阵，如下所示：
- en: '[PRE55]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: There are a host of other methods that we could apply. For some of them, we'd
    have to convert the adjacency matrix to a distance matrix.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他方法可供我们应用。对于其中一些方法，我们需要将邻接矩阵转换为距离矩阵。
- en: See also
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: You can find reading materials about graph classification and graph algorithms
    on GitHub, collected by Benedek Rozemberczki, at [https://github.com/benedekrozemberczki/awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上找到有关图分类和图算法的阅读材料，由 Benedek Rozemberczki 收集，网址为 [https://github.com/benedekrozemberczki/awesome-graph-classification](https://github.com/benedekrozemberczki/awesome-graph-classification)。
- en: If you are interested in graph convolution networks or graph attention networks,
    there's also a useful list for you at [https://github.com/Jiakui/awesome-gcn](https://github.com/Jiakui/awesome-gcn).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对图卷积网络或图注意力网络感兴趣，还有一个对您有用的列表在 [https://github.com/Jiakui/awesome-gcn](https://github.com/Jiakui/awesome-gcn)。
- en: 'There are some very nice graph libraries around for Python with many implementations
    for community detection or graph analysis:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: Python 有一些非常好的图库，具有许多用于社区检测或图分析的实现：
- en: Cdlib: [https://cdlib.readthedocs.io/en/latest/](https://cdlib.readthedocs.io/en/latest/)
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cdlib: [https://cdlib.readthedocs.io/en/latest/](https://cdlib.readthedocs.io/en/latest/)
- en: Karateclub: [https://karateclub.readthedocs.io/en/latest/](https://karateclub.readthedocs.io/en/latest/)
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karateclub: [https://karateclub.readthedocs.io/en/latest/](https://karateclub.readthedocs.io/en/latest/)
- en: Networkx: [https://networkx.github.io/](https://networkx.github.io/)
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Networkx：[https://networkx.github.io/](https://networkx.github.io/)
- en: Label propagation: [https://github.com/yamaguchiyuto/label_propagation](https://github.com/yamaguchiyuto/label_propagation)
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签传播：[https://github.com/yamaguchiyuto/label_propagation](https://github.com/yamaguchiyuto/label_propagation)
- en: 'Most Python libraries work with small- to medium-sized adjacency matrices (perhaps
    up to around 1,000 edges). Libraries suited for bigger data sizes include the
    following:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Python 库适用于小到中型邻接矩阵（大约高达 1,000 条边）。适用于更大数据规模的库包括以下内容：
- en: Snap.py: [https://snap.stanford.edu/snappy/index.html](https://snap.stanford.edu/snappy/index.html)
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snap.py：[https://snap.stanford.edu/snappy/index.html](https://snap.stanford.edu/snappy/index.html)
- en: Python-louvain[:](https://github.com/taynaud/python-louvain) [https://github.com/taynaud/python-louvain](https://github.com/taynaud/python-louvain)
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python-louvain：[https://github.com/taynaud/python-louvain](https://github.com/taynaud/python-louvain)
- en: Graph-tool: [https://graph-tool.skewed.de/](https://graph-tool.skewed.de/)
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graph-tool：[https://graph-tool.skewed.de/](https://graph-tool.skewed.de/)
- en: Cdlib also contains the BigClam algorithm, which works with big graphs.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: Cdlib 还包含 BigClam 算法，适用于大图。
- en: Some graph databases such as neo4j, which comes with a Python interface, implement
    community detection algorithms: [https://neo4j.com/docs/graph-algorithms/current/](https://neo4j.com/docs/graph-algorithms/current/).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 一些图数据库如 neo4j，带有 Python 接口，实现社区检测算法：[https://neo4j.com/docs/graph-algorithms/current/](https://neo4j.com/docs/graph-algorithms/current/)。
