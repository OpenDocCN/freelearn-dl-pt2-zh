- en: Other NN Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他神经网络架构
- en: Recurrent networks are essentially feedforward networks that retain state. All
    the networks we have looked at so far require an input of a fixed size, such as
    an image, and give a fixed size output, such as the probabilities of a particular
    class. Recurrent networks are different in that they accept a sequence, of arbitrary
    size, as the input and produce a sequence as output. Moreover, the internal state
    of the network's hidden layers is updated as a result of a learned function and
    the input. In this way, a recurrent network remembers its state. Subsequent states
    are a function of previous states.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络本质上是保持状态的前馈网络。到目前为止，我们看到的所有网络都需要固定大小的输入，比如图像，并给出固定大小的输出，比如特定类别的概率。循环网络不同之处在于，它们接受任意大小的序列作为输入，并产生序列作为输出。此外，网络的隐藏层的内部状态会随着学习的函数和输入的结果而更新。通过这种方式，循环网络记住了它的状态。后续状态是前几个状态的函数。
- en: 'In this chapter, we will cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Introduction to recurrent networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环网络简介
- en: Long short-term memory networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: Introduction to recurrent networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环网络简介
- en: Recurrent networks have been shown to be very powerful in predicting time series
    data. This is something fundamental to biological brains that enables us to do
    things such as safely drive a car, play a musical instrument, evade predators,
    understand language, and interact with a dynamic world. This sense of the flow
    of time and the understanding of how things change over time is fundamental to
    intelligent life, so it is no surprise that in artificial systems this ability
    is important.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 循环网络在预测时间序列数据方面显示出极强的能力。这对于生物大脑而言是一项基本技能，使我们能够安全驾驶汽车、演奏乐器、躲避捕食者、理解语言以及与动态世界互动。对时间流逝的感知以及事物随时间变化的理解对智能生命至关重要，因此在人工系统中，这种能力的重要性毋庸置疑。
- en: The ability to understand time series data is also important in creative endeavors,
    and recurrent networks have shown some ability in things such as composing a melody,
    constructing grammatically correct sentences, and creating visually pleasing images.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 理解时间序列数据的能力在创意工作中也非常重要，循环网络已显示出在作曲旋律、构造语法正确的句子和创建视觉上令人愉悦的图像等方面具有一定的能力。
- en: Feedforward and convolutional networks achieve very good results, as we have
    seen, in tasks such as the classification of static images. However, working with
    continuous data, as is required for tasks such as speech or handwriting recognition,
    predicting stock market prices, or forecasting the weather requires a different
    approach. In these types of tasks, both the input and the output are no longer
    a fixed size of data, but a sequence of arbitrary length.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，前馈和卷积网络在诸如静态图像分类等任务中表现出色。然而，处理连续数据，如语音或手写识别、预测股票市场价格或天气预报等任务，需要不同的方法。在这些类型的任务中，输入和输出不再是固定长度的数据，而是任意长度的序列。
- en: Recurrent artificial neurons
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环人工神经元
- en: 'For artificial neurons in feedforward networks, the flow of activation is simply
    from the input to the output. **Recurrent artificial neurons** (**RANs**) have
    a connection from the output of the activation layer to its linear input, essentially
    summing the output back into the input. A RAN can be *unrolled* in time: each
    subsequent state is a function of previous states. In this way, a RAN can be said
    to have a memory of its previous states:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前馈网络中的人工神经元，激活的流动只是从输入到输出。**循环人工神经元**（**RANs**）在激活层的输出到其线性输入之间建立了连接，实质上将输出再次加到输入中。RAN可以在时间上*展开*：每个后续状态都是前几个状态的函数。因此，可以说RAN具有其前几个状态的记忆：
- en: '![](img/a4845bc8-64be-4387-b11c-4fb06a3bbe93.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4845bc8-64be-4387-b11c-4fb06a3bbe93.png)'
- en: 'In the preceding diagram, the diagram on the left illustrates a single recurrent
    neuron. It sums its input, `x`, with the output, `y`, to produce a new output.
    The diagram on the right shows this same unit unrolled over three time steps.
    We can write an equation for the output with respect to the input for any given
    time step as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示意图中，左侧的图示了一个单个循环神经元。它将其输入`x`与输出`y`相加，产生一个新的输出。右侧的图示了相同单元在三个时间步长上的展开。我们可以写出任意给定时间步长输出关于输入的方程如下：
- en: '![](img/287b4383-2d94-4118-885a-e8a9f9ee6b03.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/287b4383-2d94-4118-885a-e8a9f9ee6b03.png)'
- en: Here, *y(t)* is the output vector at time *t*, *x[(t)]* is the input at time *t*, *y[(t-1)]*
    is the output of the previous time step, *b* is the bias term, and *Φ* is the
    activation, usually tanh or RelU. Notice that each unit has two sets of weights, *w[x]*
    and *w[y]*, for the inputs and the outputs respectively. This is, essentially,
    the formula we used for our linear networks with an added term to represent the
    output, fed back into the input at time *t-1*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y(t)* 是时间 *t* 的输出向量，*x[(t)]* 是时间 *t* 的输入，*y[(t-1)]* 是前一个时间步的输出，*b* 是偏置项，*Φ*
    是激活函数，通常是 tanh 或者 RelU。注意，每个单元都有两组权重，*w[x]* 和 *w[y]*，分别用于输入和输出。这本质上是我们在线性网络中使用的公式，其中添加了一个项来表示输出，反馈到时间
    *t-1* 的输入中。
- en: 'In the same way that with CNNs (Convolutional Neural Networks) we could compute
    the outputs of an entire layer over a batch, using a vectorized form of the previous
    equation, this is also possible with recurrent networks. The following is the
    vectorized form for a recurrent layer:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 CNNs（卷积神经网络）中可以计算整个层的输出一样，使用前述方程的向量化形式在递归网络中也是可能的。以下是递归层的向量化形式：
- en: '![](img/fd96e382-4f68-43c9-aab9-5ab570a90f2a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd96e382-4f68-43c9-aab9-5ab570a90f2a.png)'
- en: Here, *Y[(t)]* is the output at time *t*. This is a matrix of size (*m*, *n*),
    where *m* is the number of instances in the batch and *n* is the number of units
    in the layer. *X[(t)]* is a matrix of size (*m, i)* where *i* is the number of
    input features. *W[x]* is a matrix of size (*i, n),* containing the input weights
    of the current time step. *W[y]* is a matrix of size (*n*, *n*), containing the
    weights of the outputs for the previous time step.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Y[(t)]* 是时间 *t* 的输出。这是一个大小为 (*m*, *n*) 的矩阵，其中 *m* 是批次中实例的数量，*n* 是层中单元的数量。*X[(t)]*
    是一个大小为 (*m, i)* 的矩阵，其中 *i* 是输入特征的数量。*W[x]* 是一个大小为 (*i, n)* 的矩阵，包含当前时间步的输入权重。*W[y]*
    是一个大小为 (*n*, *n*) 的矩阵，包含前一个时间步的输出权重。
- en: Implementing a recurrent network
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现循环网络
- en: 'So we can concentrate on the models, we will use the same dataset we are familiar
    with. Even though we are working with static images, we can treat these as a time
    series by unrolling each 28 pixel input size over 28 time steps, enabling the
    network to make a computation on the complete image:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以专注于模型，使用我们熟悉的相同数据集。尽管我们处理的是静态图像，但我们可以将每个 28 像素输入大小在 28 个时间步骤上展开，使网络能够对完整图像进行计算：
- en: '![](img/695e96a7-25f6-4472-b4fa-30e2dde92cd9.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/695e96a7-25f6-4472-b4fa-30e2dde92cd9.png)'
- en: 'In the preceding model, we use the `nn.RNN` class to create a model with two
    recurrent layers. The `nn.RNN` class has the following default signature:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述模型中，我们使用 `nn.RNN` 类创建了一个具有两个循环层的模型。`nn.RNN` 类的默认签名如下：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The input is our 28 x 28 MNIST images. This model takes 28 pixels of each image,
    unrolling them over 28 time steps to make a computation over the entirety of all
    images in the batch. The `hidden_size` parameter is the dimension of the hidden
    layers, and this is something we choose. Here, we use a size of `100`. The `batch_first`
    parameter specifies the expected shape of the input and output tensors. We want
    this to have the shape in the form of (batch, sequence, features). In this example,
    the expected input/output tensor shape we want is (`100, 28, 28`). That is the
    batch size, the length of the sequence, and the number of features at each step;
    however, by default the `nn.RNN` class uses input/output tensors of the form (sequence,
    batch, features). Setting `batch_first = True` ensures the input/output tensors
    are of the shape (batch, sequence, features).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是我们的 28 x 28 MNIST 图像。此模型取每个图像的 28 个像素，将它们展开成 28 个时间步骤，以计算整个批次中所有图像的结果。`hidden_size`
    参数是隐藏层的维度，这是我们选择的。在这里，我们使用大小为 `100`。`batch_first` 参数指定输入和输出张量的预期形状。我们希望其形状为（批次大小，序列长度，特征数）。在本例中，我们期望的输入/输出张量形状是
    (`100, 28, 28`)。即批次大小、序列长度和每步的特征数；然而，默认情况下，`nn.RNN` 类使用的是形状为 (序列长度，批次大小，特征数) 的输入/输出张量。设置
    `batch_first = True` 确保输入/输出张量的形状为 (批次大小，序列长度，特征数)。
- en: In the `forward` method, we initialize a tensor for the hidden layer, `h0`,
    that is updated on every iteration of the model. The shape of this hidden tensor,
    representing the hidden state, is of the form (layers, batch, hidden). In this
    example, we have two layers. The second dimension of the hidden state is the batch
    size. Remember, we are using batch first so this is the first dimension of the
    input tensor, `x`, indexed using `x[0]`. The final dimension is the hidden size,
    which in this example we have set to `100`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward`方法中，我们初始化了一个用于隐藏层的张量`h0`，在模型的每次迭代中更新。这个隐藏张量的形状，表示隐藏状态，是（layers, batch,
    hidden）的形式。在这个例子中，我们有两层。隐藏状态的第二个维度是批处理大小。请记住，我们使用批处理优先，因此这是输入张量`x`的第一个维度，使用`x[0]`进行索引。最后一个维度是隐藏大小，在这个例子中，我们设置为`100`。
- en: The `nn.RNN` class requires an input consisting of the input tensor, `x`, and
    the `h0` hidden state. This is why in the `forward` method, we pass in these two
    variables. The `forward` method is called once every iteration, updating the hidden
    state and giving an output. Remember, number of iterations is the number of epochs
    multiplied by the data size divided by the batch size.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.RNN`类要求输入包括输入张量`x`和`h0`隐藏状态。这就是为什么在`forward`方法中，我们传入这两个变量。`forward`方法在每次迭代时被调用，更新隐藏状态并给出输出。请记住，迭代次数是每个批次的数据大小除以批次大小乘以数据集大小的结果。'
- en: 'Importantly, as you can see, we need to index the output using the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，正如你所看到的，我们需要使用以下方法对输出进行索引：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We are only interested in the output of the last time step, since this is the
    accumulated knowledge of all the images in the batch. If you remember, the output
    shape is of the form (batch, sequence, features) and in our model this is `(100,
    28, 100)`. The number of features is simply the number of hidden dimensions or
    number of units in the hidden layer. Now, we require all batches: this is why
    we use the colon as the first index. Here, `-1` indicates we only want the last
    element of the sequence. The last index, the colon, indicates we want all of the
    features. Hence, our output is all the features of the last time step in the sequence,
    for one entire batch.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关注最后一个时间步的输出，因为这是批处理中所有图像的累积知识。如果你记得，输出形状是（batch, sequence, features），在我们的模型中是`(100,
    28, 100)`。特征的数量简单地等于隐藏维度或隐藏层中的单元数。现在，我们需要所有批次：这就是为什么我们使用冒号作为第一个索引。在这里，`-1`表示我们只想要序列的最后一个元素。最后的索引，冒号，表示我们想要所有的特征。因此，我们的输出是序列中最后一个时间步的所有特征，针对整个批次。
- en: 'We can use almost identical training code; however, we do need to reshape the
    output when we call the model. Remember that for linear models, we reshaped the
    output using the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用几乎相同的训练代码；但是，在调用模型时，我们确实需要重塑输出。请记住，对于线性模型，我们使用以下方法重塑输出：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For convolution networks, by using `nn.CNN` we could pass in the unflattened
    image and for recurrent networks, when using `nn.RNN` we need the output to be
    of the form (batch, sequence, features). Therefore, we can use the following to
    reshape the output:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积网络，通过使用`nn.CNN`，我们可以传入未展平的图像；而对于循环网络，在使用`nn.RNN`时，我们需要输出的形式为（batch, sequence,
    features）。因此，我们可以使用以下方法重塑输出：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Remember, we need to change this line in both our training code and testing
    code. The following printout is the result of running three recurrent models using
    different layer and hidden size configurations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们需要在我们的训练代码和测试代码中修改此行。以下是运行三个不同层和隐藏尺寸配置的循环模型的结果打印输出：
- en: '![](img/1a0d6d67-9bb8-4cea-b1f0-b6cc9def140e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a0d6d67-9bb8-4cea-b1f0-b6cc9def140e.png)'
- en: 'To get a better understanding of how this model works, consider the following
    diagram, representing our two-layer model with a hidden size of `100`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解这个模型的工作原理，请考虑以下图表，表示我们的具有隐藏大小为`100`的两层模型：
- en: '![](img/fb219ae4-87ab-4346-b7fb-8005c36ad8fc.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb219ae4-87ab-4346-b7fb-8005c36ad8fc.png)'
- en: At each of the **28** time steps the network takes an input, consisting of **28**
    pixels—the features—from each of the images in the **100** image batch. Each of
    the time steps are basically a two-layer feedforward network. The only difference
    is that there is an extra input to each of the hidden layers. This input consists
    of the output from the equivalent layer in the previous time step. At each time
    step, another **28** pixels are sampled from each of the **100** images in the
    batch. Finally, when the entirety of all the images in the batch have been processed,
    the weights of the model are updated and the next iteration begins. Once all iterations
    are complete, we read the output to obtain a prediction.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一个**28**个时间步骤中，网络接收一个输入，包含**100**张图像中每张图像的**28**个像素（特征）。每个时间步骤基本上是一个两层前馈网络。唯一的区别在于每个隐藏层都有额外的输入。这个输入包括前一个时间步骤中相应层的输出。在每个时间步骤，从每个批次中的**100**张图像中抽取另外**28**个像素。最后，当批次中的所有图像都被处理完毕时，模型的权重会更新，下一个迭代开始。完成所有迭代后，我们读取输出以获取预测结果。
- en: 'To get a better understanding of what happens when we run the code, consider
    the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解运行代码时发生的情况，请考虑以下内容：
- en: '![](img/967a651a-c2c3-460e-8342-2e4fe6d4566f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/967a651a-c2c3-460e-8342-2e4fe6d4566f.png)'
- en: Here, we print out the size of the weight vectors for a two-layer RNN model
    with a hidden layer size of `100`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打印出了一个具有`100`隐藏层大小的双层RNN模型的权重向量大小。
- en: We retrieve the weights as a list containing `10` tensors. The first tensor
    of size `[100, 28]` consists of the inputs to the hidden layer, `100` units, and
    the `28` features, or pixels, of the input images. This is the *W*[*x* ]term in
    the vectorized form equation of the recurrent network. The next group of parameters,
    size `[100, 100]`, represented by the *W[y]* term in the preceding equation, is
    the output weights of the hidden layer, consisting of the `100` units each of
    size `100`. The next two single-dimension tensors, each of size `100`, are the
    bias units of the input and the hidden layer respectively. Next, we have the input
    weights, output weights, and biases of the second layer. Finally, we have the
    read out layer weights, a tensor of size `[10, 100]`, for `10` possible predictions
    using `100` features. The final single-dimension tensor of size `[10]` includes
    the bias units for the read out layer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将权重检索为包含`10`个张量的列表。第一个大小为`[100, 28]`的张量包含隐藏层的输入，`100`个单元和输入图像的`28`个特征或像素。这是递归网络向量化形式方程中的*W*[*x* ]项。接下来的参数组，大小为`[100,
    100]`，由前述方程中的*W[y]*项表示，是隐藏层的输出权重，包含每个大小为`100`的单元`100`。接下来的两个大小为`100`的单维张量分别是输入和隐藏层的偏置单元。接着我们有第二层的输入权重、输出权重和偏置。最后，我们有读取层权重，大小为`[10,
    100]`的张量，用于使用`100`个特征进行`10`个可能的预测。大小为`[10]`的最终单维张量包含读取层的偏置单元。
- en: 'In the following code, we have replicated what is happening in the recurrent
    layers of our model over a single batch of images:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们复制了模型中递归层在单个图像批次上的操作：
- en: '![](img/31b1e0d3-e2f6-4389-a44a-2d7a6b1776d4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31b1e0d3-e2f6-4389-a44a-2d7a6b1776d4.png)'
- en: You can see that we have simply created an iterator out of the `trainLoader`
    dataset object and assigned an `images` variable to a batch of images, as we did
    for our training code. The hidden layer, `h0`, needs to contain two tensors, one
    for each layer. In each of these tensors, for each image in the batch, the weights
    of the `100` hidden units are stored. This explains why we need a three-dimensional
    tensor. The first dimension of size `2` for the number of layers, the second dimension
    is of size `100` for the batch size, obtained from `images.size(0)`, and the third
    dimension is of size `100` for the number of hidden units. We then pass a reshaped
    image tensor and our hidden tensor to the model. This calls the model's `forward()`
    function, making the necessary computations, and returning two tensors an output
    tensor, and an updated hidden state tensor.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们简单地将`trainLoader`数据集对象创建为一个迭代器，并将一个`images`变量分配给一个图像批次，就像我们在训练代码中所做的那样。隐藏层`h0`需要包含两个张量，每个层次一个。在这些张量中，对于批次中的每个图像，`100`个隐藏单元的权重被存储。这解释了为什么我们需要一个三维张量。第一个大小为`2`，表示层数，第二个维度大小为`100`，来自`images.size(0)`，表示批次大小，第三个维度大小为`100`，表示隐藏单元数量。然后，我们将重塑后的图像张量和隐藏张量传递给模型。这调用了模型的`forward()`函数，进行必要的计算，并返回一个输出张量和一个更新后的隐藏状态张量。
- en: 'The following confirms these output sizes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下确认了这些输出的大小：
- en: '![](img/5b42a599-054b-4f56-9760-219f9ce3a197.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b42a599-054b-4f56-9760-219f9ce3a197.png)'
- en: 'This should help you understand why we need to resize the `images` tensor.
    Note that the features for the input are the `28` pixels for each of the images
    in the batch, which are unrolled over the sequence of `28` time steps. Next, let''s
    pass the output of the recurrent layer to our fully connected linear layer:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该帮助您理解为什么我们需要调整`images`张量的大小。请注意，输入的特征是每个图像中的`28`像素，这些像素在`28`个时间步上展开。接下来，让我们将递归层的输出传递给我们的全连接线性层：
- en: '![](img/51843d49-3dd9-46f3-8ad6-5f898419c92e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51843d49-3dd9-46f3-8ad6-5f898419c92e.png)'
- en: 'You can see that this will give us `10` predictions for each of the `100` features
    present in the output. This is why we need to index only the last element in the
    sequence. Remember the output from `nn.RNN` is of size (`100, 28, 100`). Note
    what happens to the size of this tensor when we index it using `-1`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，这将为输出中的`100`个特征的每个特征给出`10`个预测。这就是为什么我们只需索引序列中的最后一个元素。请记住，从`nn.RNN`输出的大小为（`100,
    28, 100`）。观察使用`-1`索引时此张量的大小会发生什么变化：
- en: '![](img/c4eb2a84-ea32-4a76-9483-38538e42e7ad.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4eb2a84-ea32-4a76-9483-38538e42e7ad.png)'
- en: This is the tensor containing the `100` features, the outputs of the hidden
    units, for each of the `100` images in the batch. This is passed to our linear
    layer to give the required `10` predictions for each image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这是包含`100`个特征的张量，即隐藏单元的输出，每个批次中有`100`张图片。这会传递给我们的线性层，以生成每个图像所需的`10`个预测。
- en: Long short-term memory networks
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: '**Long short-term memory networks (****LSTMS**), are a special type of RNN
    capable of learning long-term dependencies. While standard RNNs can remember previous
    states to some extent, they did this on a fairly basic level by updating a hidden
    state on each time step. This enabled the network to remember short-term dependencies.
    The hidden state, being a function of previous states, retains information about
    these previous states. However, the more time steps there are between the current
    state and a previous state, it diminishes the effect that this earlier state will
    have on the current state. Far less information is retained on a state that is
    say `10` time steps before the time step immediately preceding the current step.
    This is despite that fact that earlier time steps may contain important information
    with direct relevance to a particular problem or task we are trying to solve.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆网络（LSTMS）** 是一种特殊类型的RNN，能够学习长期依赖关系。虽然标准RNN在某种程度上可以记住先前的状态，但是它们通过在每个时间步更新隐藏状态的方式来完成这一点。这使得网络能够记住短期依赖关系。隐藏状态作为先前状态的函数，保留了关于这些先前状态的信息。然而，当前状态与先前状态之间的时间步长越多，这个早期状态对当前状态的影响就越小。在当前时间步之前的10个时间步的状态上保留的信息要少得多。这样做是尽管较早的时间步可能包含对特定问题或任务具有直接相关性的重要信息。'
- en: Biological brains have a remarkable ability to remember long-term dependencies,
    forming meaning and understanding using these dependencies. Consider how we follow
    the plot of a movie. We recall events that occurred at the beginning of the movie
    and immediately understand their relevance as the plot develops. Not only that,
    but we can apply context to the movie by recalling events in our own lives that
    give relevance and meaning to a story line. This ability to selectively apply
    memories to current context, yet at the same time filter out irrelevant details,
    is the strategy behind the design of LSTMs.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 生物大脑具有出色的记忆长期依赖能力，利用这些依赖形成意义和理解。考虑我们如何跟随电影的情节。我们回忆起电影开始时发生的事件，并且随着情节的发展立即理解它们的相关性。不仅如此，我们还可以通过回忆自己生活中的事件，为故事线赋予相关性和意义。这种有选择地将记忆应用于当前背景的能力，同时过滤掉无关细节，是设计LSTM的策略背后的原理。
- en: An LSTM network is an attempt to incorporate these long-term dependencies into
    an artificial network. It is considerably more complex than a standard RNN; however,
    it is still based on recurrent feedforward networks and understanding this theory
    should enable you to understand LSTMs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络试图将这些长期依赖关系整合到人工网络中。它比标准RNN复杂得多；然而，它仍基于递归前馈网络，理解这一理论应使您能够理解LSTM。
- en: 'The following diagram shows an LSTM over one single time step:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了LSTM在单个时间步上的示意图：
- en: '![](img/3fd63014-2c20-4b17-b7a2-ea6d998a8054.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3fd63014-2c20-4b17-b7a2-ea6d998a8054.png)'
- en: As with normal RNNs, each subsequent time step takes the hidden state of the
    previous time step, **h[t-1]**, along with the data, **x**[**t**][,] as its input.
    An LSTM also passes on a cell state that is calculated on each time step. You
    can see that **h[t-1]** and **x[t]** are each passed to four separate linear functions.
    Each pair of these linear functions is summed. Central to an LSTM are the four
    gates that these summations are passed in to. First, we have the **Forget Gate**.
    This uses a **sigmoid** for activation and is element-wise multiplied by the **Old
    Cell State**. Remember the **sigmoid** is effectively squashing the **linear**
    output values to values between zero and one. Multiplying by zero will effectively
    eliminate that particular value in the cell state and multiplying by one will
    keep this value. The **Forget Gate** essentially decides what information is passed
    to the next time step. This is achieved by element-wise multiplication with the
    **Old Cell State**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的RNN一样，每个后续时间步骤都将前一个时间步的隐藏状态 **h[t-1]** 和数据 **x**[**t**][,] 作为其输入。LSTM还传递在每个时间步骤上计算的细胞状态。您可以看到
    **h[t-1]** 和 **x[t]** 各自传递给四个单独的线性函数。这些线性函数的每对被求和。LSTM的核心是这四个门，这些总和被传递到这些门中。首先，我们有
    **遗忘门**。这使用 **sigmoid** 作为激活函数，并且与 **旧细胞状态** 进行逐元素乘法。请记住，**sigmoid** 有效地将 **线性**
    输出值压缩到零到一之间的值。乘以零将有效地消除细胞状态中的特定值，而乘以一将保留该值。**遗忘门** 实际上决定了传递到下一个时间步的信息。这是通过与 **旧细胞状态**
    的逐元素乘法来实现的。
- en: The **Input Gate** and the **Scaled new candidate** gate together determine
    what information is retained. The **Input Gate** also uses a **sigmoid** function
    and this is multiplied by the output of a **New Candidate** gate, creating a temporary
    tensor, the **Scaled new candidate**, **c[2]**. Note that the **New Candidate**
    gate uses **tanh** activation. Remember the **tanh** function outputs a value
    between `-1` and `1`. Using **tanh** and **sigmoid** activation in such a way—that
    is, by element-wise multiplication of their outputs—helps prevent the vanishing
    gradient problem, where outputs become saturated and their gradients repeatedly
    become close to zero, making them unable to perform meaningful computations. A
    **New Cell State** is calculated by summing the **Scaled new candidate** with
    the **Scaled Old Cell State**, and in this way is able to amplify important components
    of the input data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入门** 和 **缩放后的新候选门** 共同决定了保留哪些信息。**输入门** 还使用了 **sigmoid** 函数，并且乘以 **新候选门**
    的输出，生成一个临时张量，即 **缩放后的新候选门** **c[2]**。请注意，**新候选门** 使用 **tanh** 激活函数。记住 **tanh**
    函数的输出值在 `-1` 和 `1` 之间。通过这种方式使用 **tanh** 和 **sigmoid** 激活函数，即它们的输出进行逐元素乘法，有助于避免梯度消失问题，其中输出变得饱和并且它们的梯度重复接近零，使它们无法执行有意义的计算。通过将
    **缩放后的新候选门** 和 **缩放后的旧细胞状态** 相加来计算 **新细胞状态**，从而能够放大输入数据的重要组成部分。'
- en: The final gate, the output gate, **O***,* is another **sigmoid**. The new cell
    state is passed through a **tanh** function and this is element-wise multiplied
    by the output gate to calculate the **Hidden State**. This **Hidden State**, as
    with standard RNNs, is passed through a final non-linearity, a **sigmoid**, and
    a **Softmax** function to give the outputs. This has the overall effect of reinforcing
    high energy components, eliminating the lower energy components, as well as reducing
    the opportunities for vanishing gradients and reducing overfitting of the training
    set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个门，输出门 **O***,*，是另一个 **sigmoid**。新的细胞状态通过 **tanh** 函数传递，然后与输出门进行逐元素乘法，以计算
    **隐藏状态**。与标准RNN一样，这个 **隐藏状态** 通过最终的非线性传递，即 **sigmoid** 和 **Softmax** 函数，以产生输出。这总体上的效果是增强高能量组分，消除低能量组分，同时减少梯度消失的机会和减少训练集的过拟合。
- en: 'We can write the equations for each of the LSTM gates as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将每个LSTM门的方程式写成如下形式：
- en: '![](img/7b00d02f-bda6-4d4b-a621-f57869adb2ae.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b00d02f-bda6-4d4b-a621-f57869adb2ae.png)'
- en: '![](img/50eec000-b45e-40c4-8af2-855a00f47712.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50eec000-b45e-40c4-8af2-855a00f47712.png)'
- en: '![](img/d99dc7e7-9c88-42dd-99c3-bc0ebd847d75.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d99dc7e7-9c88-42dd-99c3-bc0ebd847d75.png)'
- en: '![](img/5a23b88d-addc-4eb6-a049-9870c24edeab.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a23b88d-addc-4eb6-a049-9870c24edeab.png)'
- en: 'Notice that these equations have an identical form to those of the RNN. The
    only difference is that we require eight separate weight tensors and eight bias
    tensors. It is these extra weight dimensions that give LSTMs their extra ability
    to learn and retain important features of the input data, as well as discard less
    important features. We can write the output of the linear output layer, of a particular
    time step, *t*, as the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这些方程式与RNN的形式完全相同。唯一的区别在于我们需要八个单独的权重张量和八个偏置张量。正是这些额外的权重维度赋予了LSTM额外的能力，可以学习和保留输入数据的重要特征，并丢弃不重要的特征。我们可以将某个时间步骤*t*的线性输出层输出写为以下形式：
- en: '![](img/16677b56-d59e-4b67-be36-f1740851bf1c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16677b56-d59e-4b67-be36-f1740851bf1c.png)'
- en: Implementing an LSTM
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个LSTM
- en: 'The following is the LSTM model class we will use for MNIST:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将用于MNIST的LSTM模型类：
- en: '![](img/0b1e1030-3e8b-47f9-a58c-fffa6bc4cfa9.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b1e1030-3e8b-47f9-a58c-fffa6bc4cfa9.png)'
- en: 'Notice that `nn.LSTM` is passed the same arguments as the previous RNN. This
    is not surprising, since LSTM is a recurrent network that works on sequences of
    data. Remember the input tensor has an axis of the form `(batch, sequence, feature)`,
    so we set `batch_first = True`. We initialize a fully connected linear layer for
    the output layer. Notice in the `forward` method that, as well as initializing
    a hidden state tensor, `h0`, we also initialize a tensor to hold the cell state,
    `c0`. Remember also the `out` tensor contains all `28` time steps. For our prediction,
    we are only interested in the last index in the sequence. This is why we apply
    the `[:, -1, :]` indexing to the `out` tensor before passing it to the linear
    output layer. We can print out the parameters for this model in the same way as
    for the RNN previously:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`nn.LSTM`传递的参数与之前的RNN相同。这并不奇怪，因为LSTM是一个适用于数据序列的循环网络。请记住输入张量具有形式`(batch, sequence,
    feature)`的轴，因此我们设置`batch_first=True`。我们为输出层初始化一个全连接线性层。请注意，在`forward`方法中，除了初始化隐藏状态张量`h0`，我们还初始化一个用于保存细胞状态的张量`c0`。还要记住`out`张量包含所有`28`个时间步长。对于我们的预测，我们只对序列中的最后一个索引感兴趣。这就是为什么在将其传递给线性输出层之前，我们对`out`张量应用了`[:,
    -1, :]`索引。我们可以像之前为RNN打印出这个模型的参数一样打印出来：
- en: '![](img/8b260399-17b5-49ae-a0d7-b743ce335ff5.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b260399-17b5-49ae-a0d7-b743ce335ff5.png)'
- en: These are the parameters for a single-layer LSTM with `100` hidden layers. There
    are six groups of parameters for this single-layer LSTM. Notice that instead of
    the input and hidden weight tensors having a size of `100` in the first dimension,
    as was the case for the RNN, for an LSTM, this is a size of `400`, representing `100`
    hidden units for each of the four LSTM gates.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是单层LSTM的参数，有`100`个隐藏层。这个单层LSTM有六组参数。请注意，与RNN的情况不同，对于LSTM，输入和隐藏权重张量的第一个维度大小为`400`，表示每个四个LSTM门的`100`隐藏单元。
- en: The first parameter tensor is for the input layer and is of size `[400,28]`.
    The first index, `400`, corresponds to the weights *w[1]*, *w[3]*, *w[5]*, and *w[7]*,
    each of which is of size `100`, for the inputs into the `100` hidden units specified.
    The `28` is the number of features, or pixels, present at the input. The next
    tensor, of size `[400,100]`, are the weights *w[2]*, *w[4]*, *w[6]*, and *w[8]* for
    each of `100` hidden units. The following two single-dimension tensors of size
    `[400]` are the two sets of bias units, *b[1]*, *b[3]*, *b[5]*, *b[7]* and *b[2]*,
    *b[4]*, *b[6]*, *b[8]*, for each of the LSTM gates. Finally, we have the output
    tensor of size `[10, 100]`. This is our output size, `10`, and the weight tensor
    *w[9. ]*The last single-dimension tensor of size `[10]` is the bias, *b9.*
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数张量用于输入层，大小为`[400,28]`。第一个索引`400`对应权重*w[1]*、*w[3]*、*w[5]*和*w[7]*，每个大小为`100`，用于输入到指定的`100`隐藏单元中。`28`是输入中存在的特征或像素数量。接下来的大小为`[400,100]`的张量是权重*w[2]*、*w[4]*、*w[6]*和*w[8]*，每个对应`100`隐藏单元。接下来的两个大小为`[400]`的一维张量是两组偏置单元，*b[1]*、*b[3]*、*b[5]*、*b[7]*
    和 *b[2]*、*b[4]*、*b[6]*、*b[8]*，分别用于每个LSTM门。最后，我们有大小为`[10, 100]`的输出张量。这是我们的输出大小`10`，权重张量*w[9]*。最后一个大小为`[10]`的单维张量是偏置，*b9*。
- en: Building a language model with a gated recurrent unit
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建带门控递归单元的语言模型
- en: To demonstrate the flexibility of recurrent networks, we are going to do something
    different in the final section of this chapter. Up until now, we have been working
    with probably the most-used testing data set, MNIST. This dataset has characteristics
    that are well known and it is extremely useful for comparing different types of
    models and testing different architectures and parameter sets. However, there
    are some tasks, such as natural language processing, that quite obviously require
    an entirely different type of dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示循环网络的灵活性，我们将在本章的最后一节做一些不同的事情。到目前为止，我们一直在使用可能是最常用的测试数据集之一，MNIST。这个数据集具有众所周知的特性，非常适合比较不同类型的模型、测试不同的架构和参数集。然而，有些任务，比如自然语言处理，显然需要完全不同类型的数据集。
- en: 'Also, the models we have built so far have been focused on one simple task:
    classification. This is the most straightforward machine learning task. To give
    you a flavor of other machine learning tasks, and to demonstrate the potential
    of recurrent networks, the model we are going to build is a character-based prediction
    model that attempts to predict each subsequent character based on the previous
    character, forming a learned body of text. The model first learns to create correct
    vowel—consonant sequences, words, and eventually sentences and paragraphs that
    mimic the form (but not the meaning) of those constructed by human authors.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们构建的模型都集中在一个简单的任务上：分类。这是最直接的机器学习任务。为了让你了解其他机器学习任务的风味，并展示循环网络的潜力，我们将构建一个基于字符的预测模型，该模型试图根据前一个字符预测每个后续字符，形成一个学习过的文本体。模型首先学习创建正确的元音—辅音序列、单词，最终生成模仿人类作者构建的形式（但不是意义）的句子和段落。
- en: 'The following is an adaption of code written by Sean Robertson and Pratheek
    that can be found here: [https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb).
    Here is the model definition:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由Sean Robertson和Pratheek编写的代码的改编版本，可以在这里找到：[https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb](https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb)。以下是模型定义：
- en: '![](img/ab215786-2550-43f4-9ab7-7d2818b425d9.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab215786-2550-43f4-9ab7-7d2818b425d9.png)'
- en: The purpose of this model is to take an input character at each time step, and
    output the most likely next character. Over subsequent training, it begins to
    build up sequences of characters that mimic text from a training sample. Our input
    and output sizes are simply the number of characters in the input text, and this
    is calculated and passed in as a parameter to the model. We initialize an encoder
    tensor using the `nn.Embedding` class. In a similar way to how we used one hot
    encoding to define a unique index for each word, the `nn.Embedding` module stores
    each word in a vocabulary as a multidimensional tensor. This enables us to encode
    semantic information in the word embedding. We need to pass the `nn.Embedding`
    module a vocabulary size—here, this is the number of characters in the input text—and
    a dimensionality in which to encode each character—here, this is the hidden size
    of the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的目的是在每个时间步骤获取一个输入字符，并输出最可能的下一个字符。在随后的训练中，它开始积累字符序列，模仿训练样本中的文本。我们的输入和输出大小仅仅是输入文本中字符的数量，这作为参数传递给模型进行计算。我们使用`nn.Embedding`类初始化一个编码器张量。与我们使用独热编码为每个单词定义唯一索引类似，`nn.Embedding`模块将词汇表中的每个单词存储为多维张量。这使我们能够在单词嵌入中编码语义信息。我们需要向`nn.Embedding`模块传递一个词汇量大小——这里是输入文本中字符的数量——以及用于编码每个字符的维度——这里是模型的隐藏大小。
- en: The word embedding model we are using is based on the `nn.GRU` module, or GRU.
    This is very similar to the LSTM module we used in the previous section. The difference
    is that GRU is a slightly simplified version of LSTM. It combines the input and
    forget gates into a single update gate, and combines the hidden state with the
    cell state. The result is that the GRU is more efficient than LSTM for many tasks.
    Finally, a linear output layer is initialized to decode the output from the GRU.
    In the `forward` method, we resize the input and pass it through the linear embedding
    layer, the GRU, and the final linear output layer, returning the hidden state
    and the output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的词嵌入模型基于`nn.GRU`模块，或者GRU。这与我们在前一节使用的LSTM模块非常相似。不同之处在于，GRU是LSTM的稍微简化版本。它将输入门和遗忘门合并为单个更新门，并将隐藏状态与单元状态结合起来。结果是，对于许多任务来说，GRU比LSTM更高效。最后，初始化线性输出层以解码GRU的输出。在`forward`方法中，我们调整输入大小，并通过线性嵌入层、GRU和最终的线性输出层传递它，返回隐藏状态和输出。
- en: 'Next, we need to import the data, and initialize variables containing the printable
    characters of our input text and the number of characters in the input text. Note
    the use of `unidecode` to remove non-unicode characters. You will need to import
    this module and possibly install it on your system if it is not installed already.
    We also define two convenience functions: a function to convert a character string
    into the integer equivalent of each Unicode character, and another function to
    sample random chunks of training text. The `random_training_set` function returns
    two tensors. The `inp` tensor contains all characters in the chunk, excluding
    the last character. The `target` tensor contains all elements of the chunk offset
    by one and so includes the last character. For example, if we were using a chunk
    size of `4`, and this chunk consisted of the Unicode character representations
    of `[41, 27, 29, 51]`, then the `inp` tensor would be `[41, 27, 29]` and the `target`
    tensor `[27, 29, 51]`. In this way, the target can train a model to make a prediction
    on the next character using target data:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要导入数据，并初始化包含输入文本的可打印字符和输入文本中字符数的变量。请注意使用`unidecode`来删除非Unicode字符。如果系统尚未安装此模块，您需要导入并可能在系统上安装它。我们还定义了两个方便的函数：一个函数用于将字符字符串转换为每个Unicode字符的整数等效值，另一个函数用于从训练文本中随机采样随机块。`random_training_set`函数返回两个张量。`inp`张量包含块中的所有字符，不包括最后一个字符。`target`张量包含所有偏移一个字符的块中的所有元素，因此包括最后一个字符。例如，如果我们使用块大小为`4`，并且该块由Unicode字符表示为`[41,
    27, 29, 51]`，那么`inp`张量将是`[41, 27, 29]`，`target`张量将是`[27, 29, 51]`。通过这种方式，目标可以训练模型使用目标数据进行下一个字符的预测：
- en: '![](img/96c4d2ad-bf56-4b88-9bd2-f491fb53ce27.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96c4d2ad-bf56-4b88-9bd2-f491fb53ce27.png)'
- en: 'Next, we write a method to evaluate the model. This is done by passing it one
    character at a time: the model outputs a multinomial probability distribution
    for the next most likely character. This is repeated to build up a sequence of
    characters, storing them in the `predicted` variable:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个评估模型的方法。这是通过逐个字符传递它来完成的：模型输出下一个最有可能字符的多项式概率分布。这样重复操作以构建一个字符序列，并将它们存储在`predicted`变量中：
- en: '![](img/9f7deb61-d682-40cc-985f-cd504b962315.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f7deb61-d682-40cc-985f-cd504b962315.png)'
- en: The `evaluate` function takes a `temperature` argument that divides the output
    and finds the exponent to create a probability distribution. The `temperature`
    argument has the effect of determining the level of probability required for each
    prediction. For temperature values above `1`, characters with lower probabilities
    are generated, the resulting text being more random. For lower temperature values
    below `1`, higher probability characters are generated. With temperature values
    close to `0`, only the most likely characters will be generated. For each iteration,
    a character is added to the `predicted` string until the required length, determined
    by the `predict_len` variable, is reached and the `predicted` string is returned.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate`函数接受一个`temperature`参数，该参数除以输出并找到指数以创建概率分布。`temperature`参数的作用是确定每个预测所需的概率水平。对于大于`1`的温度值，生成较低概率的字符，生成的文本更随机。对于小于`1`的较低温度值，生成较高概率的字符。对于接近`0`的温度值，只生成最可能的字符。对于每次迭代，将字符添加到`predicted`字符串中，直到达到由`predict_len`变量确定的所需长度，并返回`predicted`字符串。'
- en: 'The following function trains the model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的函数训练模型：
- en: '![](img/647f4ef9-1734-4c6b-963a-66b3076408af.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/647f4ef9-1734-4c6b-963a-66b3076408af.png)'
- en: We pass it the input chunk and the target chunk. The `for` loop runs the model
    through one iteration for each character in the chunk, updating the `hidden` state
    and returning the average loss for each character.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入块和目标块传递给它。`for` 循环在每个块中的每个字符上运行模型一次迭代，更新`hidden`状态，并返回每个字符的平均损失。
- en: 'Now, we are ready to instantiate and run the model. This is done with the following
    code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备实例化并运行模型。这是通过以下代码完成的：
- en: '![](img/8f027928-48d8-4ee1-be2c-31400e1cabd1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f027928-48d8-4ee1-be2c-31400e1cabd1.png)'
- en: Here, the usual variables are initialized. Notice that we are not using stochastic
    gradient descent for our optimizer, but rather use the Adam optimizer. The term
    Adam stands for *adaptive moment estimator*. Gradient descent uses a single fixed
    learning rate for all learnable parameters. The Adam optimizer uses an adaptive
    learning rate that maintains a per parameter learning rate. It can improve learning
    efficiency, particularly in sparse representations such as those used for natural
    language processing. Sparse representations are those where most of the values
    in a tensor are zero, for example in one-hot encoding or word embeddings.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通常变量被初始化。请注意，我们的优化器没有使用随机梯度下降，而是使用Adam优化器。术语Adam代表*自适应矩估计器*。梯度下降使用单一固定学习率来处理所有可学习参数。Adam优化器使用自适应学习率来维护每个参数的学习率。它可以提高学习效率，特别是在稀疏表示中，例如用于自然语言处理的表示。稀疏表示是那些张量中大部分数值为零的表示，例如单热编码或词嵌入。
- en: Once we run the model, it will print out the predicted text. At first, the text
    appears as almost random sequences of characters; however, after a few cycles
    of training, the model learns to format the text into English-like sentences and
    phrases. Generative models are powerful tools, enabling us to uncover probability
    distributions in input data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行模型，它将打印出预测的文本。起初，文本看起来几乎像是随机的字符序列；然而，在几个训练周期后，模型学会了将文本格式化为类似英语的句子和短语。生成模型是强大的工具，能够帮助我们揭示输入数据中的概率分布。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced recurrent neural networks and demonstrated how
    to use an RNN on the MNIST dataset. RNNs are particularly useful for working with
    time series data, since they are essentially feedforward networks that are unrolled
    over time. This makes them very suitable for tasks such as handwriting and speech
    recognition, as they operate on sequences of data. We also looked at a more powerful
    variant of the RNN, the LSTM. The LSTM uses four gates to decide what information
    to pass on to the next time step, enabling it to uncover long-term dependencies
    in data. Finally, in this chapter we built a simple language model, enabling us
    to generate text from sample input text. We used a model based on the GRU. The
    GRU is a slightly simplified version of the LSTM, containing three gates and combining
    the input and forget gates of the LSTM. This model used probability distributions
    to generate text from a sample input.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了循环神经网络，并演示了如何在MNIST数据集上使用RNN。RNN特别适用于处理时间序列数据，因为它们本质上是展开在时间上的前馈网络。这使它们非常适合处理手写和语音识别等数据序列任务。我们还看到了RNN的更强大变体，即LSTM。LSTM使用四个门来决定传递到下一个时间步的信息，从而使其能够发现数据中的长期依赖关系。最后，在本章中，我们构建了一个简单的语言模型，能够从样本输入文本中生成文本。我们使用的模型基于GRU。GRU是LSTM的稍简化版本，包含三个门，将LSTM的输入门和遗忘门结合在一起。该模型使用概率分布从样本输入生成文本。
- en: In the final chapter, we will examine some advanced features of PyTorch, such
    as using PyTorch in multiprocessor and distributed environments. We also see how
    to fine-tune PyTorch models and use pre-trained models for flexible image classification.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将探讨一些PyTorch的高级特性，例如在多处理器和分布式环境中使用PyTorch。我们还看到如何微调PyTorch模型，并使用预训练模型进行灵活的图像分类。
