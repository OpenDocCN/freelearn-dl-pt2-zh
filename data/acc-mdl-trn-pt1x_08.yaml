- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Simplifying the Model
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简化模型
- en: Have you heard about parsimony? **Parsimony**, in the context of model estimation,
    concerns keeping a model as simple as possible. Such a principle comes from the
    assumption that complex models (models with a higher number of parameters) overfit
    the training data, thus reducing the capacity to generalize and make good predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您听说过“简约原则”吗？**简约原则**在模型估计的背景下，意味着尽可能保持模型简单。这一原则来自这样的假设：复杂模型（参数数量较多的模型）会过度拟合训练数据，从而降低泛化能力和良好预测的能力。
- en: 'In addition, simplifying neural networks has two main benefits: reducing the
    model training time and making the model feasible to run in resource-constrained
    environments. One of the approaches to simplifying a model relies on reducing
    the number of parameters of the neural network by employing pruning and compression
    techniques.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，简化神经网络有两个主要好处：减少模型训练时间和使模型能够在资源受限的环境中运行。简化模型的一种方法是通过使用修剪和压缩技术来减少神经网络参数的数量。
- en: In this chapter, we show how to simplify a model by reducing the number of parameters
    of the neural network without sacrificing its quality.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示如何通过减少神经网络参数的数量来简化模型，而不牺牲其质量。
- en: 'Here is what you will learn as part of this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章节将学到的内容：
- en: The key benefits of simplifying a model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化模型的关键好处
- en: The concept and techniques of model pruning and compression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型修剪和压缩的概念与技术
- en: How to use the Microsoft NNI toolkit to simplify a model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 Microsoft NNI 工具包简化模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the complete code of examples mentioned in this chapter in the
    book’s GitHub repository at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在书的 GitHub 代码库中找到本章节提到的所有示例的完整代码，网址为 [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main)。
- en: You can access your favorite environments to execute this notebook, such as
    Google Colab or Kaggle.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以访问您喜爱的环境来执行这个笔记本，比如 Google Colab 或者 Kaggle。
- en: Knowing the model simplifying process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解模型简化的过程
- en: In simpler words, simplifying a model concerns removing connections, neurons,
    or entire layers of the neural network to get a lighter model, i.e., a model with
    a reduced number of parameters. Naturally, the efficiency of the simplified version
    must be very close to the one achieved by the original model. Otherwise, simplifying
    the model does not make any sense.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，简化模型涉及移除神经网络的连接、神经元或整个层，以获得一个更轻的模型，即具有减少参数数量的模型。当然，简化版本的效率必须非常接近原始模型的效果。否则，简化模型就没有任何意义。
- en: 'To understand this topic, we must answer the following questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个主题，我们必须回答以下问题：
- en: Why simplify a model? (reason)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要简化模型？（原因）
- en: How do we simplify a model? (process)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何简化模型？（过程）
- en: When do we simplify a model? (moment)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么时候简化模型？（时机）
- en: We will go through each of these questions in the following sections to get
    an overall understanding of model simplification.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中逐一回答这些问题，以便全面理解模型简化。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before moving on in this chapter, it is essential to say that model simplification
    is still an open research area. Consequently, some concepts and terms cited in
    this book may differ a little bit from other materials or how they are employed
    on frameworks and toolkits.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续本章之前，必须指出模型简化仍然是一个开放的研究领域。因此，本书中提到的一些概念和术语可能与其他资料或框架工具包中的使用稍有不同。
- en: Why simplify a model? (reason)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要简化模型？（原因）
- en: To get an insight into the reasons why a model should be simplified let’s make
    use of a simple yet nice analogy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解为什么需要简化模型，让我们利用一个简单而又生动的类比。
- en: 'Consider the hypothetical situation where we must build a bridge to connect
    two sides of a river. For safety purposes, we decided to put a column in every
    two meters of the bridge, as shown in *Figure 6**.1*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个假设情境，我们必须建造一座桥来连接河的两岸。为了安全起见，我们决定在桥每两米放置一个支柱，如*图 6**.1*所示：
- en: '![Figure 6.1 – Bridge analogy](img/B20959_06_1.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 桥梁类比](img/B20959_06_1.jpg)'
- en: Figure 6.1 – Bridge analogy
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 桥梁类比
- en: The bridge seems pretty safe, being sustained by its 16 columns. However, someone
    could look at the project and say we do not need all 16 columns to maintain the
    bridge. As the bridge’s designers, we could argue that safety comes first; thus,
    there is no problem with having additional columns to undoubtedly guarantee the
    bridge’s integrity.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这座桥似乎非常安全，由其16根柱子支撑。然而，有人可能会看到这个项目并说我们不需要所有16根柱子来维持桥梁。作为桥梁设计者，我们可以主张安全第一；因此，增加柱子以确保桥梁完整性是没有问题的。
- en: 'Even so, what if we could spare the columns a little bit without affecting
    the bridge’s structure? To put it differently, perhaps using 16 columns to support
    the bridge is too much in terms of safety. As we can see in *Figure 6**.2*, maybe
    nine columns could work fine for this scenario:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 即使如此，如果我们可以稍微减少柱子数量而不影响桥梁的结构呢？换句话说，也许用16根柱子支撑桥梁在安全性上有些过剩。正如我们在图**6.2**中看到的那样，也许只需要九根柱子在这种情况下就足够了：
- en: '![Figure 6.2 – The bridge remains up after removing some columns](img/B20959_06_2.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 - 移除一些柱子后桥梁仍然保持竖立状态](img/B20959_06_2.jpg)'
- en: Figure 6.2 – The bridge remains up after removing some columns
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图**6.2** - 移除一些柱子后桥梁仍然保持竖立状态
- en: If we could put fewer columns in the bridge and keep it as safe as before, we
    would reduce the budget and building time, as well as simplify the maintenance
    process in the future. There will not be a reasonable argument to refute this
    approach.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在桥上减少柱子数量并保持其安全性，我们将减少预算和建造时间，同时简化未来的维护过程。没有合理的理由反驳这种方法。
- en: This naïve analogy is helpful to heat our discussion about the reasons to simplify
    a model. As for the columns in the bridge, does a neural network model need all
    of its parameters to achieve good accuracy? The answer is not straightforward
    and depends on issues such as the problem type and model itself. However, considering
    that a simplified version of the model performs precisely as the original, why
    not try the former?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个天真的类比有助于加热我们关于简化模型原因的讨论。至于桥梁上的柱子，一个神经网络模型是否需要所有的参数来实现良好的准确性？答案并不简单，而是取决于问题类型和模型本身。然而，考虑到简化模型的表现与原始模型完全相同，为什么不尝试前者呢？
- en: 'After all, it is undeniable that simplifying a model has clear benefits:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，简化模型具有明显的好处：
- en: '**Accelerating the training process**: A neural network composed of a lower
    number of parameters is usually faster to train. As discussed in [*Chapter 1*](B20959_01.xhtml#_idTextAnchor016)*,
    Deconstructing the Training Process,* the number of parameters directly impacts
    the computational burden of neural networks.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加速训练过程**：通常，由较少参数组成的神经网络训练速度更快。正如在[*第1章*](B20959_01.xhtml#_idTextAnchor016)*，拆解训练过程*中讨论的那样，参数数量直接影响神经网络的计算负担。'
- en: '**Running inference on resource-constrained environments**: Some models are
    too large to store and so complex to execute that they do not fit in environments
    comprised of reduced memory and computing capacity. In this case, the only way
    to run the model in these environments relies on simplifying it as much as possible.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在资源受限环境中运行推理**：有些模型太大无法存储，执行起来太复杂，无法适应内存和计算能力有限的环境。在这种情况下，唯一的办法是尽可能简化模型来在这些环境中运行。'
- en: Now that the benefits of simplifying the model are crystal clear let’s jump
    to the next section to learn how to execute this process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，简化模型的好处已经十分明显，让我们跳到下一节来学习如何执行这个过程。
- en: How to simplify a model? (process)
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何简化模型？（过程）
- en: 'We simplify a model by applying a workflow comprised of two steps: **pruning**
    and **compression**:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过应用包括两个步骤的工作流程来简化模型：**修剪**和**压缩**：
- en: '![Figure 6.3 – Simplifying the workflow](img/B20959_06_3.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 - 简化工作流程](img/B20959_06_3.jpg)'
- en: Figure 6.3 – Simplifying the workflow
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 - 简化工作流程
- en: As pictorially described in *Figure 6**.3*, the simplifying workflow receives
    a dense neural network as input, where all neurons are fully connected to themselves,
    and this outputs a simplified version of the original model. In other words, the
    workflow transforms a dense neural network into a sparse neural network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如图**6.3**所示，简化工作流程接收密集神经网络作为输入，其中所有神经元都与自身完全连接，输出原始模型的简化版本。换句话说，该工作流程将密集神经网络转换为稀疏神经网络。
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: The terms **dense** and **sparse** come from math and are used to describe matrices.
    A dense matrix is filled with useful numbers, whereas a sparse matrix possesses
    a relevant quantity of null values (zeros). As the parameters of neural networks
    can be expressed in *n*-dimensional matrices, a non-fully connected neural network
    is also known as a sparse neural network because of the high number of null connections
    between neurons.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**密集**和**稀疏**来自数学，并用于描述矩阵。密集矩阵充满有用的数字，而稀疏矩阵则具有大量空值（零）。由于神经网络的参数可以用n维矩阵表示，非全连接神经网络也被称为稀疏神经网络，因为神经元之间的空连接数量很高。
- en: Let’s zoom in on the workflow to understand the role played by each step, starting
    with the pruning phase.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细查看工作流程，以理解每个步骤的作用，从修剪阶段开始。
- en: The pruning phase
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修剪阶段
- en: 'The **pruning phase** is responsible for receiving the original model and cutting
    off the parameters present in the connections (weights), neurons (bias), and filters
    (kernel values), resulting in a pruned model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**修剪阶段**负责接收原始模型并剪除连接（权重）、神经元（偏置）和滤波器（核值）中存在的参数，从而得到一个修剪过的模型：'
- en: '![Figure 6.4 – Pruning phase](img/B20959_06_4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 修剪阶段](img/B20959_06_4.jpg)'
- en: Figure 6.4 – Pruning phase
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 修剪阶段
- en: As shown in *Figure 6**.4*, many connections from the original model have been
    disabled (represented as opaque lines in the pruned model). The pruning phase
    decides which parameters should be removed depending on the **technique** applied
    during the process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图6.4所示，原始模型中的许多连接已被禁用（在修剪模型中表示为不透明的线条）。修剪阶段根据过程中应用的**技术**决定应移除哪些参数。
- en: 'A pruning technique has three dimensions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪技术具有三个维度：
- en: '**Criterion**: Defines which parameters to cut off'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准则**：定义要切断的参数'
- en: '**Scope**: Determines whether to drop an entire structure (neuron, layer, or
    filter) or isolated parameters'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**范围**：确定是否丢弃整个结构（神经元、层或滤波器）或孤立的参数'
- en: '**Method**: Defines whether to prune the network at once or iteratively prune
    the model until it reaches some stop criteria'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方法**：定义是一次性修剪网络还是迭代修剪模型，直到达到某个停止准则'
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Model pruning is a brave new world. Therefore, you can easily find many scientific
    papers proposing new methods and solutions to this area. One exciting paper is
    entitled *A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis,
    and Recommendations*, which summarizes the recent advances in this field and briefly
    presents other simplifying techniques such as quantization and knowledge distillation.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 模型修剪是一个崭新的世界。因此，您可以轻松找到许多提出新方法和解决方案的科学论文。一篇有趣的论文名为*深度神经网络修剪调查：分类、比较、分析和建议*，概述了这一领域的最新进展，并简要介绍了诸如量化和知识蒸馏等其他简化技术。
- en: In practice, the pruned model occupies the same amount of memory and requires
    the same computational capacity as the original model. This happens because the
    null parameters, although not having a practical effect on the forward and backward
    computations and results, were *not definitively removed from* *the network*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，修剪后的模型占用与原始模型相同的内存量，并且需要相同的计算能力。这是因为null参数虽然在前向和反向计算及结果上没有实际影响，但并未从网络中完全移除。
- en: 'For example, suppose two fully connected layers comprised of three neurons
    each. The weights of the connections can be represented as a matrix, as shown
    in *Figure 6**.5*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设由三个神经元组成的两个全连接层。连接的权重可以表示为一个矩阵，如图6.5所示：
- en: '![Figure 6.5 – Weights represented as a matrix](img/B20959_06_5.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 权重表示为矩阵](img/B20959_06_5.jpg)'
- en: Figure 6.5 – Weights represented as a matrix
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 权重表示为矩阵
- en: 'After applying the pruning process, the neural network had three connections
    disabled, as we can see in *Figure 6**.6*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用修剪过程后，神经网络有三个连接被禁用，正如我们在图6.6中所见：
- en: '![Figure 6.6 – Pruned weights changed to null](img/B20959_06_6.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 修剪后的权重更改为null](img/B20959_06_6.jpg)'
- en: Figure 6.6 – Pruned weights changed to null
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 修剪后的权重更改为null
- en: Remark that the weights were changed to null (0.00), leaving the connections
    (represented by those weights) out of the calculations carried out by the network.
    Therefore, these connections merely do not exist in terms of the meaning of neural
    network results.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，权重已更改为null（0.00），这些权重所代表的连接在网络计算中已被移除。因此，这些连接在神经网络结果的意义上实际上并不存在。
- en: However, the data structure is exactly the same as the original model. We still
    have nine float numbers, and all of them are still being multiplied (although
    with no practical effect) by the output of their respective neurons. From the
    point of view of memory consumption and computational burden, nothing has changed
    so far.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据结构与原始模型完全相同。我们仍然有九个浮点数，所有这些数仍然乘以它们各自神经元的输出（虽然没有实际效果）。从内存消耗和计算负担的角度来看，到目前为止什么都没有改变。
- en: 'Well, if the purpose of simplifying a model is to reduce the number of parameters,
    why do we continue having the same structure as before? Keep calm, and let’s execute
    the second phase of the simplifying workflow: the compression phase.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果简化模型的目的是减少参数数量，为什么我们继续使用与之前相同的结构呢？保持冷静，让我们继续执行简化工作流的第二阶段：压缩阶段。
- en: Compression phase
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 压缩阶段
- en: 'As illustrated in *Figure 6**.7*, the **compression phase** receives a pruned
    model as input and generates a new brain model comprised only of the non-pruned
    parameters, i.e., the parameters that the pruning process left untouched:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6**.7*所示，**压缩阶段**接收修剪模型作为输入，并生成一个仅由未修剪参数组成的新脑模型，即修剪过程未触及的参数：
- en: '![Figure 6.7 – Compression phase](img/B20959_06_7.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 压缩阶段](img/B20959_06_7.jpg)'
- en: Figure 6.7 – Compression phase
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 压缩阶段
- en: The new network can have a shape utterly different from the original model,
    with a distinct disposal of neurons and layers. Beyond everything, the compression
    process is free to generate a new model since it respects the parameters preserved
    by the pruning step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 新网络的形状可以与原始模型完全不同，神经元和层次的布置也各异。总之，压缩过程可以自由生成一个新模型，因为它遵循修剪步骤保留的参数。
- en: 'Therefore, the compression phase effectively removes the parameters of the
    pruned model, resulting in a truly simplified model. Let’s take the example presented
    in *Figure 6**.8* to understand what happens after model compression:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，压缩阶段有效地去除了修剪模型的参数，从而得到一个真正简化的模型。让我们以*图 6**.8*中的例子来理解模型压缩后发生了什么：
- en: '![Figure 6.8 – Model compression applied to a pruned network](img/B20959_06_8.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 应用于修剪网络的模型压缩](img/B20959_06_8.jpg)'
- en: Figure 6.8 – Model compression applied to a pruned network
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 应用于修剪网络的模型压缩
- en: The set of disabled parameters—connections, in this example—were removed from
    the model, reducing the weights’ matrix by one-third. Consequently, the weights’
    matrix now occupies less memory and requires fewer operations to complete the
    forward and backward phases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，一组禁用的参数——连接——已从模型中删除，将权重矩阵减少了三分之一。因此，权重矩阵现在占用更少的内存，并且在完成前向和后向阶段时需要更少的操作。
- en: Note
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can think about the relationship between pruning and compression phases as
    the process of deleting a file from a disk. When we ask the operating system to
    delete a file, it just marks the block as free where the file is allocated. In
    other words, the file content is still there and will be erased only when overwritten
    by a new file.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以把修剪和压缩阶段之间的关系想象成从磁盘中删除文件的过程。当我们要求操作系统删除文件时，它只是将分配给文件的块标记为自由。换句话说，文件内容仍然存在，只有当被新文件覆盖时才会被擦除。
- en: When do we simplify a model? (moment)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们何时简化模型？（时刻）
- en: 'We can simplify a model before or after training the neural network. In other
    words, the model simplification process can be applied to non-trained, pre-trained,
    or trained models, as explained in the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练神经网络之前或之后简化模型。换句话说，模型简化过程可以应用于未训练、预训练或已训练的模型，如下所述：
- en: '**Non-trained model**: Our goal here is to speed up the training process. As
    the model has not been trained yet, the neural network is filled with random parameters,
    preventing most pruning techniques from doing a useful job. To work around this
    issue, we usually run a **warmup phase**, which consists of training the network
    over a single epoch before simplifying the model.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未训练模型**：我们的目标是加快训练过程。由于模型尚未训练，神经网络填充有随机参数，大多数修剪技术无法有效地工作。为了解决这个问题，通常在简化模型之前会运行一个**预热阶段**，即在简化模型之前对网络进行单个时期的训练。'
- en: '**Pre-trained model**: We use pre-trained networks, which have a general efficiency
    on a general domain, to tackle a particular problem of that domain. In this case,
    we do not need to execute the warmup phase because the model has already been
    trained.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练模型**：我们使用预训练网络来解决特定问题域的问题，因为这些网络在一般领域上具有通用的效率。在这种情况下，我们不需要执行预热阶段，因为模型已经训练好了。'
- en: '**Trained model**: Simplifying a trained model is usually done to deploy the
    trained network in a resource-constrained environment.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练好的模型**：通常简化训练好的模型是为了在资源受限的环境中部署训练好的网络。'
- en: Now that we have the answers to the questions about model simplification, shall
    we use PyTorch, alongside a toolkit, to simplify a model? Follow me to the next
    section to learn how to do it!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回答了关于模型简化的问题，我们应该使用PyTorch及其工具包来简化模型吗？请跟随我到下一节来学习如何做到这一点！
- en: Using Microsoft NNI to simplify a model
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Microsoft NNI简化模型
- en: '**Neural Network Intelligence** (**NNI**) is an open-source project created
    by Microsoft to help deep learning practitioners automate tasks such as hyperparameter
    automatization and neural architecture searches.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络智能**（**NNI**）是微软创建的开源项目，旨在帮助深度学习从业者自动化任务，如超参数自动化和神经架构搜索。'
- en: NNI also has a set of tools to deal with model simplification in a simpler and
    straightforward manner. So, we can easily simplify a model by adding a couple
    of lines to our original code. NNI supports PyTorch and other well-known deep
    learning frameworks such as TensorFlow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: NNI还有一套工具集，用于更简单直接地处理模型简化。因此，我们可以通过在原始代码中添加几行代码来轻松简化模型。NNI支持PyTorch和其他知名的深度学习框架。
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: PyTorch has its own API to prune models, namely `torch.prune`. Unfortunately,
    at the time of writing this book, this API does not provide a mechanism to compress
    a model. Therefore, we have decided to introduce NNI as the solution to accomplish
    this task. More information about NNI can be found at [https://github.com/microsoft/nni](https://github.com/microsoft/nni).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch有自己的API来修剪模型，即`torch.prune`。不幸的是，在编写本书时，这个API不提供压缩模型的机制。因此，我们决定引入NNI作为完成此任务的解决方案。有关NNI的更多信息，请访问[https://github.com/microsoft/nni](https://github.com/microsoft/nni)。
- en: Let’s start by getting an overview of NNI in the next section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从下一节开始对NNI进行概述。
- en: Overview of NNI
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NNI概述
- en: 'Because NNI is not a native component of PyTorch, we need to install it via
    pip by executing the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于NNI不是PyTorch的本地组件，我们需要通过执行以下命令使用pip安装它：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'NNI has many modules, but for the purpose of model simplification, we are going
    to use only two of them, namely `pruning` and `speedup`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: NNI有很多模块，但为了简化模型，我们只会使用其中的两个，即`pruning`和`speedup`：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Pruning module
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修剪模块
- en: 'The `pruning` module provides a set of pruning techniques, also known as **pruners**.
    Each pruner applies a specific method to prune the model and requires a particular
    set of parameters. Among the parameters needed by the pruner, two of them are
    mandatory: the model and a **configuration list**.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`pruning`模块提供一组修剪技术，也称为**修剪器**。每个修剪器应用特定的方法来修剪模型，并需要一组特定的参数。在修剪器所需的参数中，有两个是强制性的：模型和**配置列表**。'
- en: The configuration list is a dictionary-based structure used to control pruner
    behavior. From the configuration list, we can indicate which structures (layers,
    operations, or filters) the pruner must work on and which ones it should leave
    untouched.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 配置列表是一个基于字典结构的结构，用于控制修剪器的行为。从配置列表中，我们可以指示修剪器必须处理哪些结构（层、操作或过滤器），以及哪些结构它应该保持不变。
- en: 'For example, the following configuration list tells the pruner to work on all
    layers implementing the `Linear` operator (layers created with the class `torch.nn.Linear`),
    except the one named `layer4`. In addition, the pruner will try to nullify 30%
    of the parameters, as indicated on the `sparse_ratio` key:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下配置列表告诉修剪器处理所有实施`Linear`运算符的层（使用类`torch.nn.Linear`创建的层），除了名为`layer4`的层。此外，修剪器将尝试使30%的参数归零，如`sparse_ratio`键中所示：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the complete list of key-value pairs accepted by the config list
    at [https://nni.readthedocs.io/en/stable/compression/config_list.html](https://nni.readthedocs.io/en/stable/compression/config_list.html).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://nni.readthedocs.io/en/stable/compression/config_list.html](https://nni.readthedocs.io/en/stable/compression/config_list.html)找到配置列表接受的键值对的完整列表。
- en: 'After setting the configuration list, we have everything to instantiate the
    pruner, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 设置了配置列表后，我们就可以实例化修剪器，如下所示：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The most crucial method provided by the pruner is called `compress`. Despite
    what the name suggests, it executes the pruning process by applying the corresponding
    pruning algorithm.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由修剪器提供的最关键方法称为`compress`。尽管名称暗示的是另一回事，它通过应用相应的剪枝算法执行剪枝过程。
- en: The `compress` method returns a data structure called **masks**, which denotes
    what parameters were dropped by the pruning algorithm. This information is used
    further to remove the pruned parameters from the network effectively.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`compress`方法返回一个名为**masks**的数据结构，该结构表示剪枝算法丢弃的参数。进一步使用此信息有效地从网络中移除被修剪的参数。'
- en: Note
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As stated before, the simplifying process is still ongoing. Therefore, we can
    face some tricks, such as the incoherent usage of terms. This is the reason why
    NNI calls the pruning phase `compress`, though the compressing step is accomplished
    by another method called `speedup`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，简化过程仍在进行中。因此，我们可能会遇到一些技巧，比如术语的不一致使用。这就是为什么NNI将剪枝阶段称为`compress`，尽管压缩步骤是由另一种称为`speedup`的方法完成的。
- en: Note that, until this point, nothing has indeed changed to the original model;
    not yet. To remove the pruned parameters effectively, we must rely on the `speedup`
    module.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，到目前为止，原始模型确实没有任何变化；还没有。要有效地移除被修剪的参数，我们必须依赖于`speedup`模块。
- en: The speedup module
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: speedup模块
- en: The `speedup` module provides a class named `ModelSpeedup`, which is used to
    create a **speeder**. A speeder executes the compression phase on the pruned model,
    i.e., it effectively removes the parameters dropped by the pruner.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`speedup`模块提供了一个名为`ModelSpeedup`的类，用于创建**速度器**。速度器在修剪模型上执行压缩阶段，即有效地移除修剪器丢弃的参数。'
- en: 'Along the lines of pruners, we also must instantiate an object from the `ModelSpeedup`
    class. This class requires three obligatory parameters: the pruned model, an input
    sample, and the masks generated by the pruner:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在修剪器方面，我们还必须从`ModelSpeedup`类实例化一个对象。此类需要三个必填参数：修剪模型、一个输入样本和由修剪器生成的掩码：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After that, we just need to call the `speedup_model` method so the speeder
    can compress the model and return a simplified version of the original model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们只需调用`speedup_model`方法，使速度器可以压缩模型并返回原始模型的简化版本：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that you have an overview of the fundamental steps to simplify a model through
    NNI let’s jump to the next section to learn how to use this toolkit in a practical
    example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经概述了通过NNI简化模型的基本步骤，让我们跳到下一节，学习如何在实际示例中使用此工具包。
- en: NNI in action!
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NNI的实际应用！
- en: To see NNI working in practice let’s simplify our well-known CNN model. In this
    example, we are going to simplify this model by training it over the CIFAR-10
    dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到NNI在实践中的工作，让我们简化我们众所周知的CNN模型。在此示例中，我们将通过使用CIFAR-10数据集来简化此模型。
- en: Note
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The complete code shown in this section is available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter06/nni-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter06/nni-cnn_cifar10.ipynb).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分显示的完整代码可在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter06/nni-cnn_cifar10.ipynb](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/code/chapter06/nni-cnn_cifar10.ipynb)找到。
- en: 'Let’s start by counting the original number of parameters of the CNN model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从计算CNN模型的原始参数数目开始：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The CNN model has `2,122,186` parameters distributed among the biases, weights,
    and filters of the neural network. We trained this model using the CIFAR-10 dataset
    only during 10 epochs since we are interested in comparing the training time and
    corresponding accuracy between distinct pruning configurations. So, the original
    model took 122 seconds to train in a CPU-based machine, reaching an accuracy of
    47.10%.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CNN模型在偏差、权重和神经网络的滤波器之间有`2,122,186`个参数。我们仅在CPU机器上使用CIFAR-10数据集进行了10个时期的训练，因为我们有兴趣比较不同修剪配置之间的训练时间和相应的准确性。因此，原始模型在122秒内训练，达到47.10%的准确性。
- en: 'Okay, so let’s remove some pillars of the bridge to see whether it still stands
    up. We are going to simplify the CNN model by considering the following strategy:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们移除桥的一些支柱，看看它是否仍然站立。我们将通过考虑以下策略简化CNN模型：
- en: 'Operation types: Conv2d'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作类型：Conv2d
- en: 'Sparsity per layer: 0.50'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层稀疏度：0.50
- en: 'Pruner algorithm: L1 Norm'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修剪器算法：L1 Norm
- en: This strategy tells the simplification process to look at only the convolutional
    layers of the neural network, and, for each layer, the pruning algorithm must
    throw away 50% of the parameters. As we are simplifying a fresh model, we need
    to execute a warmup phase to populate the network with some valuable parameters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略告诉简化过程只关注神经网络的卷积层，并且对于每一层，裁剪算法必须丢弃50%的参数。由于我们正在简化一个全新的模型，我们需要执行预热阶段来为网络引入一些有价值的参数。
- en: For this experiment, we have chosen the L1 Norm pruner, which removes parameters
    according to the magnitude measured by L1 normalization. In simpler words, the
    pruner will drop off the parameters, which will have a minor influence on neural
    network results.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们选择了L1范数裁剪器，它根据L1范数测量的大小来移除参数。简单来说，裁剪器会丢弃对神经网络结果影响较小的参数。
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can find more information about the L1 Norm pruner at [https://nni.readthedocs.io/en/stable/reference/compression/pruner.html#l1-norm-pruner](https://nni.readthedocs.io/en/stable/reference/compression/pruner.html#l1-norm-pruner).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[L1 Norm pruner](https://nni.readthedocs.io/en/stable/reference/compression/pruner.html#l1-norm-pruner)找到更多关于L1范数裁剪器的信息。
- en: 'The following excerpt of code shows a couple of lines needed to simplify the
    CNN model by applying the aforementioned strategy:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码摘录显示了应用上述策略简化CNN模型所需的几行代码：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'During the simplification process, NNI will output a bunch of lines, such as
    these:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在简化过程中，NNI将输出一堆如下的行：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After the process has been carried out, we can verify that the number of parameters
    of the original neural network has decreased by around 50%, as expected:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成这个过程后，我们可以验证原始神经网络的参数数量如预期般减少了约50%：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Well, the model is smaller and simpler. But how about the training time and
    efficiency? Let’s find out!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，模型变得更小更简单了。但是训练时间和效率呢？让我们看看！
- en: We trained the simplified model against CIFAR-10 by using the same hyperparameters
    through the same number of epochs. The training process of the simplified model
    took 89 seconds to complete, representing a performance improvement of 37%! Although
    the model’s efficiency decreased a little bit (from 47.10% to 42.71%), it remains
    very close to the original version.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的超参数通过相同数量的epochs对CIFAR-10进行了简化模型的训练。简化模型的训练过程只需89秒完成，表现提升了37%! 虽然模型的效率略有下降（从47.10%降至42.71%），但仍接近原始版本。
- en: 'It is interesting to note the trade-off between training time, accuracy, and
    sparsity ratio. As shown in *Table 6.1*, the model’s efficiency falls to 38.87%
    when 80% of parameters are removed from the network. On the other hand, the training
    process took only 76 seconds to finish, which is 61% faster than training the
    original network:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是要注意训练时间、准确性和稀疏比之间的权衡。如*表6.1*所示，当从网络中移除80%的参数时，模型的效率降至38.87%。另一方面，训练过程仅需76秒完成，比训练原始网络快61%：
- en: '| **Sparsity** **per layer** | **Training time** | **Accuracy** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **每层稀疏度** | **训练时间** | **准确性** |'
- en: '| 10% | 118 | 47.26% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 10% | 118 | 47.26% |'
- en: '| 20% | 113 | 45.84% |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 20% | 113 | 45.84% |'
- en: '| 30% | 107 | 44.66% |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 30% | 107 | 44.66% |'
- en: '| 40% | 100 | 45.18% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 40% | 100 | 45.18% |'
- en: '| 50% | 89 | 42.71% |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 89 | 42.71% |'
- en: '| 60% | 84 | 41.90% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 60% | 84 | 41.90% |'
- en: '| 70% | 81 | 40.84% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 70% | 81 | 40.84% |'
- en: '| 80% | 76 | 38.87% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 80% | 76 | 38.87% |'
- en: Table 6.1 – Relation between model accuracy, training time, and sparsity level
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – 模型准确性、训练时间和稀疏度水平之间的关系
- en: As the saying goes, there is no free lunch So, accuracy is expected to deteriorate
    slightly when simplifying the model. The goal here is to find a balance between
    a tolerable decrease in the model’s quality in the face of a reasonable performance
    improvement.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 俗话说，天下没有免费的午餐。因此，当简化模型时，准确性有望略微下降。这里的目标是在模型质量略微降低的情况下找到性能改进的平衡点。
- en: In this section, we have learned how to use NNI to simplify our model. By changing
    a couple of lines on our original code, we can simplify the model by cutting off
    a certain number of connections, thus contributing to reducing the training time,
    yet retaining the model’s quality.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用NNI来简化我们的模型。通过在我们原始代码中改变几行代码，我们可以通过减少一定数量的连接来简化模型，从而减少训练时间，同时保持模型的质量。
- en: The next section brings a couple of questions to help you retain what you have
    learned in this chapter.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将提出一些问题，帮助您记住本章学到的内容。
- en: Quiz time!
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测验时间！
- en: Let’s review what we have learned in this chapter by answering a few questions.
    At first, try to answer these questions without consulting the material.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答一些问题来回顾本章所学内容。首先，请尝试不查阅资料回答这些问题。
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The answers to all these questions are available at [https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter06-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter06-answers.md).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题的答案都可以在[https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter06-answers.md](https://github.com/PacktPublishing/Accelerate-Model-Training-with-PyTorch-2.X/blob/main/quiz/chapter06-answers.md)找到。
- en: Before starting the quiz, remember that it is not a test at all! This section
    aims to complement your learning process by revising and consolidating the content
    covered in this chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始测验之前，请记住这根本不是一个测试！本节旨在通过回顾和巩固本章节涵盖的内容来补充您的学习过程。
- en: Choose the correct option for the following questions.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为以下问题选择正确的选项。
- en: What are the two steps to take when simplifying a workflow?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在简化工作流程时需要执行哪两个步骤？
- en: Reduction and compression.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少和压缩。
- en: Pruning and reduction.
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝和减少。
- en: Pruning and compression.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝和压缩。
- en: Reduction and zipping.
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少和压缩。
- en: 'A pruning technique usually has the following dimensions:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝技术通常具有以下维度：
- en: Criterion, scope, and method.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准、范围和方法。
- en: Algorithm, scope, and magnitude.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法、范围和大小。
- en: Criterion, constraints, and targets.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准、约束和目标。
- en: Algorithm, constraints, and targets.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法、约束和目标。
- en: Concerning the compression phase, we can assert which of the following?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于压缩阶段，我们可以断言以下哪一个？
- en: It receives a compressed model as input and verifies the model’s integrity.
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接收一个压缩过的模型作为输入，并验证模型的完整性。
- en: It receives a compressed model as input and generates a model partially comprised
    only of the non-pruned parameters.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接收一个压缩过的模型作为输入，并生成一个仅由未剪枝参数部分组成的模型。
- en: It receives a pruned model as input and generates a new brain model comprised
    only of the non-pruned parameters.
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接收一个被剪枝的模型作为输入，并生成一个仅由未剪枝参数组成的新脑模型。
- en: It receives a pruned model as input and evaluates the pruning degree applied
    to that model.
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接收一个被剪枝的模型作为输入，并评估应用于该模型的剪枝程度。
- en: We can execute the model simplifying process on which of the following?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在以下哪些情况下执行模型简化过程？
- en: Pre-trained models only.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅预训练模型。
- en: Pre-trained and non-trained models only.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅预训练和非训练模型。
- en: Non-trained models only.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅非训练模型。
- en: Non-trained, pre-trained, and trained models.
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非训练、预训练和已训练模型。
- en: What is one of the main goals of simplifying a trained model?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简化训练模型的主要目标之一是什么？
- en: Accelerate the training process.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加速训练过程。
- en: Deploy it on resource-constrained environments.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其部署在资源受限的环境中。
- en: Improve the model’s accuracy.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提高模型的准确性。
- en: There is no reason to simplify a trained model.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有理由简化已训练的模型。
- en: 'Consider the following configuration list passed to the pruner:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑以下配置列表传递给剪枝者：
- en: '[PRE10]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Which of the following actions would the pruner take?
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 剪枝者会执行以下哪些操作？
- en: The pruner will try to nullify 75% of all network parameters.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝者将尝试使所有网络参数的75%无效化。
- en: The pruner will try to nullify 25% of the parameters of all fully connected
    layers.
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝者将尝试使所有全连接层参数的25%无效化。
- en: The pruner will try to nullify 25% of the parameters of convolutional layers,
    except the one labeled as “layer2”.
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝者将尝试使卷积层参数的25%无效化，除了标记为“layer2”的那个。
- en: The pruner will try to nullify 75% of the parameters of the convolutional layers,
    except the one labeled as “layer2.”
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝者将尝试使卷积层参数的75%无效化，除了标记为“layer2”的那个。
- en: What is more likely to happen to the model’s accuracy after executing the simplified
    workflow?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行简化工作流程后，模型的准确性更可能发生什么变化？
- en: The model’s accuracy tends to increase.
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的准确性倾向于增加。
- en: The model’s accuracy surely increases.
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的准确性肯定会增加。
- en: The model’s accuracy tends to reduce.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的准确性倾向于降低。
- en: The model’s accuracy stays the same.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型的准确性保持不变。
- en: It is necessary to execute a warmup phase before simplifying which of the following?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在简化以下哪些内容之前，有必要执行热身阶段？
- en: Non-trained models.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非训练模型。
- en: Trained models.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 已训练的模型。
- en: Pre-trained models.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练模型。
- en: None of the above.
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以上都不是。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned that simplifying a model by reducing the number
    of parameters can accelerate the network training process, besides making the
    model feasible to run on resource-constrained platforms.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解到通过减少参数数量来简化模型可以加速网络训练过程，使模型能够在资源受限的平台上运行。
- en: 'Then, we saw that the simplification process consists of two phases: pruning
    and compression. The former is responsible for determining which parameters must
    be dropped off from the network, whereas the latter effectively removes the parameters
    from the model.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们看到简化过程包括两个阶段：剪枝和压缩。前者负责确定网络中必须删除的参数，而后者则有效地从模型中移除这些参数。
- en: Although PyTorch provides an API to prune the model, it is not fully useful
    to simplify a model. Thus, you were introduced to Microsoft NNI, a powerful toolkit
    to automate tasks related to deep learning modes. Among the features provided
    by NNI, this tool offers a complete workflow to simplify a model. All of this
    is achieved with a couple of new lines added to the original code.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PyTorch提供了一个API来剪枝模型，但它并不完全有助于简化模型。因此，介绍了Microsoft NNI，一个强大的工具包，用于自动化与深度学习模型相关的任务。在NNI提供的功能中，这个工具提供了一个完整的工作流程来简化模型。所有这些都是通过向原始代码添加几行新代码来实现的。
- en: In the next chapter, you will learn how to reduce the numeric precision adopted
    by the neural network to accelerate the training process and decrease the amount
    of memory needed to store the model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，您将学习如何减少神经网络采用的数值精度，以加快训练过程并减少存储模型所需的内存量。
