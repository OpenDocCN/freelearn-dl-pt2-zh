- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Building Good Training Datasets – Data Preprocessing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建良好的训练数据集 - 数据预处理
- en: The quality of the data and the amount of useful information that it contains
    are key factors that determine how well a machine learning algorithm can learn.
    Therefore, it is absolutely critical to ensure that we examine and preprocess
    a dataset before we feed it to a machine learning algorithm. In this chapter,
    we will discuss the essential data preprocessing techniques that will help us
    to build good machine learning models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的质量和包含的有用信息量是决定机器学习算法学习效果的关键因素。因此，在将数据提供给机器学习算法之前，确保我们对数据进行必要的检查和预处理是非常关键的。在本章中，我们将讨论关键的数据预处理技术，这些技术将帮助我们构建良好的机器学习模型。
- en: 'The topics that we will cover in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将讨论的主题包括：
- en: Removing and imputing missing values from the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据集中删除和填补缺失值
- en: Getting categorical data into shape for machine learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类数据准备好供机器学习算法使用
- en: Selecting relevant features for the model construction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择用于模型构建的相关特征
- en: Dealing with missing data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: It is not uncommon in real-world applications for our training examples to be
    missing one or more values for various reasons. There could have been an error
    in the data collection process, certain measurements may not be applicable, or
    particular fields could have been simply left blank in a survey, for example.
    We typically see missing values as blank spaces in our data table or as placeholder
    strings such as `NaN`, which stands for “not a number,” or `NULL` (a commonly
    used indicator of unknown values in relational databases). Unfortunately, most
    computational tools are unable to handle such missing values or will produce unpredictable
    results if we simply ignore them. Therefore, it is crucial that we take care of
    those missing values before we proceed with further analyses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实应用中，由于各种原因，我们的训练样本可能缺少一个或多个值。数据收集过程中可能出现错误，某些测量可能不适用，或者在调查中可能简单地留空某些字段。我们通常在数据表中看到缺失值作为空格或占位符字符串，例如`NaN`（代表“不是一个数字”）或`NULL`（在关系数据库中常用于未知值的指示符）。不幸的是，大多数计算工具无法处理这些缺失值，或者如果我们简单地忽略它们，则会产生不可预测的结果。因此，在进一步分析之前，处理这些缺失值至关重要。
- en: In this section, we will work through several practical techniques for dealing
    with missing values by removing entries from our dataset or imputing missing values
    from other training examples and features.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过从数据集中删除条目或从其他训练样本和特征填补缺失值来解决缺失值的几种实用技术。
- en: Identifying missing values in tabular data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在表格数据中识别缺失值
- en: 'Before we discuss several techniques for dealing with missing values, let’s
    create a simple example `DataFrame` from a **comma-separated values** (**CSV**)
    file to get a better grasp of the problem:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论处理缺失值的几种技术之前，让我们从一个**逗号分隔值**（**CSV**）文件创建一个简单的示例`DataFrame`，以更好地理解问题：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the preceding code, we read CSV-formatted data into a pandas `DataFrame`
    via the `read_csv` function and noticed that the two missing cells were replaced
    by `NaN`. The `StringIO` function in the preceding code example was simply used
    for the purposes of illustration. It allowed us to read the string assigned to
    `csv_data` into a pandas `DataFrame` as if it was a regular CSV file on our hard
    drive.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，通过`read_csv`函数将CSV格式的数据读入pandas的`DataFrame`，注意到两个缺失的单元格被替换为`NaN`。在上面的代码示例中，`StringIO`函数仅用于说明目的。它允许我们将分配给`csv_data`的字符串读入pandas的`DataFrame`，就像它是硬盘上常规CSV文件一样。
- en: 'For a larger `DataFrame`, it can be tedious to look for missing values manually;
    in this case, we can use the `isnull` method to return a `DataFrame` with Boolean
    values that indicate whether a cell contains a numeric value (`False`) or if data
    is missing (`True`). Using the `sum` method, we can then return the number of
    missing values per column as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的`DataFrame`，手动查找缺失值可能会很繁琐；在这种情况下，我们可以使用`isnull`方法返回一个带有布尔值的`DataFrame`，指示单元格是否包含数值（`False`）或数据是否缺失（`True`）。然后，我们可以使用`sum`方法返回每列缺失值的数量如下：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This way, we can count the number of missing values per column; in the following
    subsections, we will take a look at different strategies for how to deal with
    this missing data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以统计每列缺失值的数量；在接下来的小节中，我们将介绍不同的策略来处理这些缺失数据。
- en: '**Convenient data handling with pandas’ DataFrame**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用pandas的DataFrame方便地处理数据**'
- en: 'Although scikit-learn was originally developed for working with NumPy arrays
    only, it can sometimes be more convenient to preprocess data using pandas’ `DataFrame`.
    Nowadays, most scikit-learn functions support `DataFrame` objects as inputs, but
    since NumPy array handling is more mature in the scikit-learn API, it is recommended
    to use NumPy arrays when possible. Note that you can always access the underlying
    NumPy array of a `DataFrame` via the `values` attribute before you feed it into
    a scikit-learn estimator:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 scikit-learn 最初只用于处理 NumPy 数组，但有时使用 pandas 的 `DataFrame` 来预处理数据可能更方便。现在，大多数
    scikit-learn 函数支持 `DataFrame` 对象作为输入，但由于 scikit-learn API 中 NumPy 数组处理更为成熟，建议在可能的情况下使用
    NumPy 数组。请注意，在将其馈送到 scikit-learn 估算器之前，您可以通过 `values` 属性随时访问 `DataFrame` 的底层 NumPy
    数组：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Eliminating training examples or features with missing values
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消除具有缺失值的训练样本或特征
- en: 'One of the easiest ways to deal with missing data is simply to remove the corresponding
    features (columns) or training examples (rows) from the dataset entirely; rows
    with missing values can easily be dropped via the `dropna` method:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据的最简单方法之一是完全删除数据集中对应的特征（列）或训练样本（行）；可以通过 `dropna` 方法轻松删除具有缺失值的行：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, we can drop columns that have at least one `NaN` in any row by setting
    the `axis` argument to `1`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以通过将 `axis` 参数设置为 `1` 来删除任何行中至少有一个 `NaN` 的列：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `dropna` method supports several additional parameters that can come in
    handy:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`dropna` 方法支持几个额外参数，这些参数可能非常方便：'
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Although the removal of missing data seems to be a convenient approach, it
    also comes with certain disadvantages; for example, we may end up removing too
    many samples, which will make a reliable analysis impossible. Or, if we remove
    too many feature columns, we will run the risk of losing valuable information
    that our classifier needs to discriminate between classes. In the next section,
    we will look at one of the most commonly used alternatives for dealing with missing
    values: interpolation techniques.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管删除缺失数据似乎是一个方便的方法，但它也有一定的缺点；例如，我们可能会删除太多样本，从而使得可靠的分析变得不可能。或者，如果我们删除了太多特征列，那么我们将面临失去分类器需要用来区分类别的宝贵信息的风险。在下一节中，我们将看一下处理缺失值的最常用替代方法之一：插值技术。
- en: Imputing missing values
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填补缺失值
- en: 'Often, the removal of training examples or dropping of entire feature columns
    is simply not feasible, because we might lose too much valuable data. In this
    case, we can use different interpolation techniques to estimate the missing values
    from the other training examples in our dataset. One of the most common interpolation
    techniques is **mean imputation**, where we simply replace the missing value with
    the mean value of the entire feature column. A convenient way to achieve this
    is by using the `SimpleImputer` class from scikit-learn, as shown in the following
    code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，删除训练样本或整个特征列根本不可行，因为我们可能会损失太多宝贵的数据。在这种情况下，我们可以使用不同的插值技术来估算数据集中其他训练样本的缺失值。其中最常见的插值技术之一是**均值插补**，我们只需用整个特征列的均值替换缺失值即可。通过使用
    scikit-learn 中的 `SimpleImputer` 类，我们可以方便地实现这一点，如下所示的代码：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we replaced each `NaN` value with the corresponding mean, which is separately
    calculated for each feature column. Other options for the `strategy` parameter
    are `median` or `most_frequent`, where the latter replaces the missing values
    with the most frequent values. This is useful for imputing categorical feature
    values, for example, a feature column that stores an encoding of color names,
    such as red, green, and blue. We will encounter examples of such data later in
    this chapter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们用对应的均值替换了每个 `NaN` 值，这些均值是单独计算得到的，针对每个特征列。`strategy` 参数的其他选项包括 `median`
    或 `most_frequent`，后者用最常见的值替换缺失值。例如，这对于填充分类特征值非常有用，比如存储颜色名称编码的特征列，如红色、绿色和蓝色。我们将在本章后面遇到此类数据的示例。
- en: 'Alternatively, an even more convenient way to impute missing values is by using
    pandas’ `fillna` method and providing an imputation method as an argument. For
    example, using pandas, we could achieve the same mean imputation directly in the
    `DataFrame` object via the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更方便的填补缺失值的方法是使用 pandas 的 `fillna` 方法，并提供一个填补方法作为参数。例如，使用 pandas，我们可以直接在 `DataFrame`
    对象中实现相同的均值插补，如下命令所示：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B17582_04_01.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_01.png)'
- en: 'Figure 4.1: Replacing missing values in data with the mean'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：用均值替换数据中的缺失值
- en: '**Additional imputation methods for missing data**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**用于缺失数据的其他填补方法**'
- en: For additional imputation techniques, including the `KNNImputer` based on a
    k-nearest neighbors approach to impute missing features by nearest neighbors,
    we recommend the scikit-learn imputation documentation at [https://scikit-learn.org/stable/modules/impute.html](https://scikit-learn.org/stable/modules/impute.html).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包括基于 k 最近邻方法的`KNNImputer`在内的其他填补技术，以通过最近邻来填补缺失特征，我们建议查阅 scikit-learn 填补文档
    [https://scikit-learn.org/stable/modules/impute.html](https://scikit-learn.org/stable/modules/impute.html)。
- en: Understanding the scikit-learn estimator API
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 scikit-learn 估计器 API
- en: In the previous section, we used the `SimpleImputer` class from scikit-learn
    to impute missing values in our dataset. The `SimpleImputer` class is part of
    the so-called **transformer** API in scikit-learn, which is used for implementing
    Python classes related to data transformation. (Please note that the scikit-learn
    transformer API is not to be confused with the transformer architecture that is
    used in natural language processing, which we will cover in more detail in *Chapter
    16*, *Transformers – Improving Natural Language Processing with Attention Mechanisms*.)
    The two essential methods of those estimators are `fit` and `transform`. The `fit`
    method is used to learn the parameters from the training data, and the `transform`
    method uses those parameters to transform the data. Any data array that is to
    be transformed needs to have the same number of features as the data array that
    was used to fit the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们使用了 scikit-learn 中的`SimpleImputer`类来填补数据集中的缺失值。`SimpleImputer`类是 scikit-learn
    中所谓的**转换器**API 的一部分，用于实现与数据转换相关的 Python 类。请注意，scikit-learn 转换器 API 与用于自然语言处理的
    transformer 架构不要混淆，我们将在*第 16 章*，*使用注意力机制改进自然语言处理的 Transformers*中更详细地讨论后者。这些估计器的两个关键方法是`fit`和`transform`。`fit`方法用于从训练数据中学习参数，`transform`方法使用这些参数来转换数据。任何要转换的数据数组都需要与用于拟合模型的数据数组具有相同数量的特征。
- en: '*Figure 4.2* illustrates how a scikit-learn transformer instance, fitted on
    the training data, is used to transform a training dataset as well as a new test
    dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.2*展示了一个在训练数据上拟合的 scikit-learn 转换器实例如何用于转换训练数据集以及新的测试数据集：'
- en: '![Diagram  Description automatically generated](img/B17582_04_02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_04_02.png)'
- en: 'Figure 4.2: Using the scikit-learn API for data transformation'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：使用 scikit-learn API 进行数据转换
- en: 'The classifiers that we used in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using Scikit-Learn*, belong to the so-called **estimators** in scikit-learn, with
    an API that is conceptually very similar to the scikit-learn transformer API.
    Estimators have a `predict` method but can also have a `transform` method, as
    you will see later in this chapter. As you may recall, we also used the `fit`
    method to learn the parameters of a model when we trained those estimators for
    classification. However, in supervised learning tasks, we additionally provide
    the class labels for fitting the model, which can then be used to make predictions
    about new, unlabeled data examples via the `predict` method, as illustrated in
    *Figure 4.3*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第 3 章*，*使用 Scikit-Learn 进行机器学习分类器导览*中使用的分类器属于 scikit-learn 中所谓的**估计器**，其
    API 在概念上与 scikit-learn 转换器 API 非常相似。估计器具有一个`predict`方法，但也可以有一个`transform`方法，正如你将在本章后面看到的。正如你可能记得的那样，我们还使用`fit`方法来学习这些估计器进行分类时的模型参数。然而，在监督学习任务中，我们额外提供类标签来拟合模型，然后可以通过`predict`方法对新的未标记数据示例进行预测，如*图
    4.3*所示：
- en: '![Diagram  Description automatically generated](img/B17582_04_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_04_03.png)'
- en: 'Figure 4.3: Using the scikit-learn API for predictive models such as classifiers'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：使用 scikit-learn API 进行分类器等预测模型的使用
- en: Handling categorical data
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分类数据
- en: So far, we have only been working with numerical values. However, it is not
    uncommon for real-world datasets to contain one or more categorical feature columns.
    In this section, we will make use of simple yet effective examples to see how
    to deal with this type of data in numerical computing libraries.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了数值数据。然而，现实世界的数据集通常包含一个或多个分类特征列。在本节中，我们将利用简单而有效的示例来看如何在数值计算库中处理这种类型的数据。
- en: 'When we are talking about categorical data, we have to further distinguish
    between **ordinal** and **nominal** features. Ordinal features can be understood
    as categorical values that can be sorted or ordered. For example, t-shirt size
    would be an ordinal feature, because we can define an order: *XL* > *L* > *M*.
    In contrast, nominal features don’t imply any order; to continue with the previous
    example, we could think of t-shirt color as a nominal feature since it typically
    doesn’t make sense to say that, for example, red is larger than blue.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论分类数据时，我们必须进一步区分**序数**和**名义**特征。序数特征可以理解为可以排序或有序的分类值。例如，T恤尺码就是一个序数特征，因为我们可以定义一个顺序：*XL* > *L* > *M*。相反，名义特征则不涉及任何顺序；继续上面的例子，我们可以认为T恤颜色是一个名义特征，因为通常没有意义说，例如，红色比蓝色大。
- en: Categorical data encoding with pandas
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用pandas进行分类数据编码
- en: 'Before we explore different techniques for handling such categorical data,
    let’s create a new `DataFrame` to illustrate the problem:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索处理这种分类数据的不同技术之前，让我们创建一个新的`DataFrame`来说明问题：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see in the preceding output, the newly created `DataFrame` contains
    a nominal feature (`color`), an ordinal feature (`size`), and a numerical feature
    (`price`) column. The class labels (assuming that we created a dataset for a supervised
    learning task) are stored in the last column. The learning algorithms for classification
    that we discuss in this book do not use ordinal information in class labels.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的输出中所看到的，新创建的`DataFrame`包含一个名义特征（`color`）、一个序数特征（`size`）和一个数值特征（`price`）列。类标签（假设我们为监督学习任务创建了一个数据集）存储在最后一列。我们在本书中讨论的分类学习算法不使用类标签中的序数信息。
- en: Mapping ordinal features
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 映射序数特征
- en: 'To make sure that the learning algorithm interprets the ordinal features correctly,
    we need to convert the categorical string values into integers. Unfortunately,
    there is no convenient function that can automatically derive the correct order
    of the labels of our `size` feature, so we have to define the mapping manually.
    In the following simple example, let’s assume that we know the numerical difference
    between features, for example, *XL* = *L* + 1 = *M* + 2:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保学习算法正确解释序数特征，我们需要将分类字符串值转换为整数。不幸的是，没有方便的函数可以自动推导我们的`size`特征标签的正确顺序，因此我们必须手动定义映射。在下面的简单示例中，假设我们知道特征之间的数值差异，例如，*XL* = *L* + 1 = *M* + 2：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we want to transform the integer values back to the original string representation
    at a later stage, we can simply define a reverse-mapping dictionary, `inv_size_mapping
    = {v: k for k, v in size_mapping.items()}`, which can then be used via the pandas
    `map` method on the transformed feature column and is similar to the `size_mapping`
    dictionary that we used previously. We can use it as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '如果我们希望在以后的阶段将整数值转换回原始字符串表示，我们可以简单地定义一个反向映射字典，`inv_size_mapping = {v: k for
    k, v in size_mapping.items()}`，然后可以通过pandas的`map`方法在转换后的特征列上使用它，类似于我们之前使用的`size_mapping`字典。我们可以这样使用它：'
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Encoding class labels
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类标签的编码
- en: 'Many machine learning libraries require that class labels are encoded as integer
    values. Although most estimators for classification in scikit-learn convert class
    labels to integers internally, it is considered good practice to provide class
    labels as integer arrays to avoid technical glitches. To encode the class labels,
    we can use an approach similar to the mapping of ordinal features discussed previously.
    We need to remember that class labels are *not* ordinal, and it doesn’t matter
    which integer number we assign to a particular string label. Thus, we can simply
    enumerate the class labels, starting at `0`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习库要求类标签被编码为整数值。尽管scikit-learn中大多数分类器的内部会将类标签转换为整数，但通常最好将类标签提供为整数数组以避免技术性故障。为了编码类标签，我们可以使用类似于前面讨论的序数特征映射的方法。我们需要记住类标签**不是**序数，并且分配给特定字符串标签的整数数值无关紧要。因此，我们可以简单地枚举类标签，从`0`开始：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can use the mapping dictionary to transform the class labels into
    integers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用映射字典将类标签转换为整数：
- en: '[PRE12]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can reverse the key-value pairs in the mapping dictionary as follows to
    map the converted class labels back to the original string representation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将映射字典中的键值对反转，以便将转换后的类标签映射回原始字符串表示如下：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Alternatively, there is a convenient `LabelEncoder` class directly implemented
    in scikit-learn to achieve this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，scikit-learn 中直接实现的便捷 `LabelEncoder` 类也可以达到这个目的：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that the `fit_transform` method is just a shortcut for calling `fit` and
    `transform` separately, and we can use the `inverse_transform` method to transform
    the integer class labels back into their original string representation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`fit_transform` 方法只是调用 `fit` 和 `transform` 的捷径，我们可以使用 `inverse_transform`
    方法将整数类标签转换回它们原始的字符串表示：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Performing one-hot encoding on nominal features
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对名义特征执行独热编码
- en: 'In the previous *Mapping ordinal features* section, we used a simple dictionary
    mapping approach to convert the ordinal `size` feature into integers. Since scikit-learn’s
    estimators for classification treat class labels as categorical data that does
    not imply any order (nominal), we used the convenient `LabelEncoder` to encode
    the string labels into integers. We could use a similar approach to transform
    the nominal `color` column of our dataset, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述 *映射序数特征* 部分，我们使用了一个简单的字典映射方法来将序数 `size` 特征转换为整数。由于 scikit-learn 的分类估计器将类标签视为不含任何顺序的分类数据（名义数据），我们使用了便捷的
    `LabelEncoder` 来将字符串标签编码为整数。我们可以使用类似的方法来转换数据集的名义 `color` 列，如下所示：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After executing the preceding code, the first column of the NumPy array, `X`,
    now holds the new `color` values, which are encoded as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后，NumPy 数组 `X` 的第一列现在包含新的 `color` 值，其编码如下：
- en: '`blue = 0`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blue = 0`'
- en: '`green = 1`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`green = 1`'
- en: '`red = 2`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`red = 2`'
- en: If we stop at this point and feed the array to our classifier, we will make
    one of the most common mistakes in dealing with categorical data. Can you spot
    the problem? Although the color values don’t come in any particular order, common
    classification models, such as the ones covered in the previous chapters, will
    now assume that `green` is larger than `blue`, and `red` is larger than `green`.
    Although this assumption is incorrect, a classifier could still produce useful
    results. However, those results would not be optimal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在此时停止并将数组馈送给分类器，我们将犯处理分类数据时最常见的错误之一。你能发现问题吗？尽管颜色值没有特定的顺序，但常见的分类模型（如前几章介绍的模型）现在会假设
    `green` 大于 `blue`，`red` 大于 `green`。虽然这种假设是不正确的，分类器仍然可能产生有用的结果。然而，这些结果将不会是最优的。
- en: 'A common workaround for this problem is to use a technique called **one-hot
    encoding**. The idea behind this approach is to create a new dummy feature for
    each unique value in the nominal feature column. Here, we would convert the `color`
    feature into three new features: `blue`, `green`, and `red`. Binary values can
    then be used to indicate the particular `color` of an example; for example, a
    `blue` example can be encoded as `blue=1`, `green=0`, `red=0`. To perform this
    transformation, we can use the `OneHotEncoder` that is implemented in scikit-learn’s
    `preprocessing` module:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的一个常见解决方案是使用一种称为 **独热编码** 的技术。这种方法的理念是为名义特征列中的每个唯一值创建一个新的虚拟特征。在这里，我们将把
    `color` 特征转换为三个新特征：`blue`、`green` 和 `red`。二进制值可以用来表示示例的特定 `color`；例如，一个 `blue`
    示例可以被编码为 `blue=1`、`green=0`、`red=0`。要执行这种转换，我们可以使用 scikit-learn 的 `preprocessing`
    模块中实现的 `OneHotEncoder`：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that we applied the `OneHotEncoder` to only a single column, `(X[:, 0].reshape(-1,
    1))`, to avoid modifying the other two columns in the array as well. If we want
    to selectively transform columns in a multi-feature array, we can use the `ColumnTransformer`,
    which accepts a list of `(name, transformer, column(s))` tuples as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们仅对单列 `(X[:, 0].reshape(-1, 1))` 应用了 `OneHotEncoder`，以避免修改数组中的其他两列。如果我们想要选择性地转换多特征数组中的列，我们可以使用
    `ColumnTransformer`，它接受以下形式的 `(name, transformer, column(s))` 列表：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code example, we specified that we want to modify only the
    first column and leave the other two columns untouched via the `'passthrough'`
    argument.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码示例中，我们指定只想修改第一列，并通过 `'passthrough'` 参数保持其他两列不变。
- en: 'An even more convenient way to create those dummy features via one-hot encoding
    is to use the `get_dummies` method implemented in pandas. Applied to a `DataFrame`,
    the `get_dummies` method will only convert string columns and leave all other
    columns unchanged:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 pandas 中实现的 `get_dummies` 方法更方便地创建这些虚拟特征的方法是应用于 `DataFrame`，`get_dummies`
    方法将仅转换字符串列，而保持所有其他列不变：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When we are using one-hot encoding datasets, we have to keep in mind that this
    introduces multicollinearity, which can be an issue for certain methods (for instance,
    methods that require matrix inversion). If features are highly correlated, matrices
    are computationally difficult to invert, which can lead to numerically unstable
    estimates. To reduce the correlation among variables, we can simply remove one
    feature column from the one-hot encoded array. Note that we do not lose any important
    information by removing a feature column, though; for example, if we remove the
    column `color_blue`, the feature information is still preserved since if we observe
    `color_green=0` and `color_red=0`, it implies that the observation must be `blue`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用独热编码数据集时，我们必须记住这会引入多重共线性，这对某些方法（例如需要矩阵求逆的方法）可能会有问题。如果特征高度相关，矩阵计算求逆将变得计算困难，这可能会导致数值不稳定的估计。为了减少变量之间的相关性，我们可以简单地从独热编码数组中删除一个特征列。注意，通过删除特征列，我们不会丢失任何重要信息；例如，如果我们删除列`color_blue`，仍然保留了特征信息，因为如果我们观察到`color_green=0`和`color_red=0`，则意味着观察必须是`blue`。
- en: 'If we use the `get_dummies` function, we can drop the first column by passing
    a `True` argument to the `drop_first` parameter, as shown in the following code
    example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`get_dummies`函数，可以通过将`drop_first`参数设置为`True`来删除第一列，如以下代码示例所示：
- en: '[PRE20]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In order to drop a redundant column via the `OneHotEncoder`, we need to set
    `drop=''first''` and set `categories=''auto''` as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过`OneHotEncoder`删除冗余列，我们需要设置`drop='first'`并将`categories='auto'`设置如下：
- en: '[PRE21]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Additional encoding schemes for nominal data**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**名义数据的附加编码方案**'
- en: 'While one-hot encoding is the most common way to encode unordered categorical
    variables, several alternative methods exist. Some of these techniques can be
    useful when working with categorical features that have high cardinality (a large
    number of unique category labels). Examples include:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然独热编码是编码无序分类变量的最常见方式，但也存在几种替代方法。在处理具有高基数（大量唯一类别标签）的分类特征时，某些技术可能会很有用。例如：
- en: Binary encoding, which produces multiple binary features similar to one-hot
    encoding but requires fewer feature columns, i.e., log[2](*K*) instead of *K* – 1,
    where *K* is the number of unique categories. In binary encoding, numbers are
    first converted into binary representations, and then each binary number position
    will form a new feature column.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制编码，产生多个类似于独热编码的二进制特征，但需要较少的特征列，即*log[2](*K*)*而不是*K* – 1，其中*K*是唯一类别的数量。在二进制编码中，数字首先转换为二进制表示，然后每个二进制数位置将形成一个新的特征列。
- en: Count or frequency encoding, which replaces the label of each category by the
    number of times or frequency it occurs in the training set.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数或频率编码，用训练集中每个类别出现的次数或频率替换每个类别的标签。
- en: 'These methods, as well as additional categorical encoding schemes, are available
    via the scikit-learn-compatible `category_encoders` library: [https://contrib.scikit-learn.org/category_encoders/](https://contrib.scikit-learn.org/category_encoders/).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法以及额外的分类编码方案都可以通过与scikit-learn兼容的`category_encoders`库来实现：[https://contrib.scikit-learn.org/category_encoders/](https://contrib.scikit-learn.org/category_encoders/)。
- en: While these methods are not guaranteed to perform better than one-hot encoding
    in terms of model performance, we can consider the choice of a categorical encoding
    scheme as an additional “hyperparameter” for improving model performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法在模型性能方面并不能保证优于独热编码，但我们可以考虑选择分类编码方案作为改进模型性能的额外“超参数”。
- en: 'Optional: encoding ordinal features'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可选：编码有序特征
- en: 'If we are unsure about the numerical differences between the categories of
    ordinal features, or the difference between two ordinal values is not defined,
    we can also encode them using a threshold encoding with 0/1 values. For example,
    we can split the feature `size` with values `M`, `L`, and `XL` into two new features,
    `x > M` and `x > L`. Let’s consider the original `DataFrame`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不确定有序特征类别之间的数值差异，或者两个有序值之间的差异未定义，我们也可以使用阈值编码将其编码为0/1值。例如，我们可以将具有`M`、`L`和`XL`值的特征`size`拆分为两个新特征，`x
    > M`和`x > L`。让我们考虑原始`DataFrame`：
- en: '[PRE22]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can use the `apply` method of pandas’ `DataFrame` to write custom lambda
    expressions in order to encode these variables using the value-threshold approach:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用pandas的`DataFrame`的`apply`方法，通过写入自定义lambda表达式来使用值阈值方法对这些变量进行编码：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Partitioning a dataset into separate training and test datasets
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据集分成单独的训练集和测试集。
- en: We briefly introduced the concept of partitioning a dataset into separate datasets
    for training and testing in *Chapter 1*, *Giving Computers the Ability to Learn
    from Data*, and *Chapter 3*, *A Tour of Machine Learning Classifiers Using Scikit-Learn*.
    Remember that comparing predictions to true labels in the test set can be understood
    as the unbiased performance evaluation of our model before we let it loose in
    the real world. In this section, we will prepare a new dataset, the **Wine** dataset.
    After we have preprocessed the dataset, we will explore different techniques for
    feature selection to reduce the dimensionality of a dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第1章*“使计算机能够从数据中学习”和*第3章*“使用Scikit-Learn进行机器学习分类器之旅”中简要介绍了将数据集划分为用于训练和测试的单独数据集的概念。请记住，在测试集中将预测与真实标签进行比较，可以理解为在我们将模型放入真实世界之前对其进行无偏差的性能评估。在本节中，我们将准备一个新的数据集，即**Wine**数据集。在我们预处理数据集之后，我们将探讨不同的特征选择技术以减少数据集的维度。
- en: The Wine dataset is another open-source dataset that is available from the UCI
    machine learning repository ([https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine));
    it consists of 178 wine examples with 13 features describing their different chemical
    properties.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Wine数据集是另一个开源数据集，可以从UCI机器学习库获取（[https://archive.ics.uci.edu/ml/datasets/Wine](https://archive.ics.uci.edu/ml/datasets/Wine)）；它包含了178个葡萄酒示例，其中13个特征描述了它们不同的化学特性。
- en: '**Obtaining the Wine dataset**'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取Wine数据集**'
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the dataset at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable on the UCI server. For instance, to load the Wine dataset
    from a local directory, you can replace this line:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的代码包中找到Wine数据集的副本（以及本书中使用的所有其他数据集），如果您在离线工作或者UCI服务器上的数据集临时不可用时，您可以使用该数据集。例如，要从本地目录加载Wine数据集，可以将此行替换为
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'with the following one:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与以下一个：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Using the pandas library, we will directly read in the open-source Wine dataset
    from the UCI machine learning repository:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas库，我们将直接从UCI机器学习库中读取开源的Wine数据集：
- en: '[PRE26]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The 13 different features in the Wine dataset, describing the chemical properties
    of the 178 wine examples, are listed in the following table:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Wine数据集中的13个不同特征描述了178个葡萄酒示例的化学特性，详见以下表：
- en: '![](img/B17582_04_04.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_04.png)'
- en: 'Figure 4.4: A sample of the Wine dataset'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：Wine数据集的样本
- en: The examples belong to one of three different classes, `1`, `2`, and `3`, which
    refer to the three different types of grape grown in the same region in Italy
    but derived from different wine cultivars, as described in the dataset summary
    ([https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例属于三个不同的类别之一，`1`，`2`和`3`，这些类别指的是在同一意大利地区种植的三种不同葡萄类型，但来自不同的葡萄酒品种，如数据集摘要所述（[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names))。
- en: 'A convenient way to randomly partition this dataset into separate test and
    training datasets is to use the `train_test_split` function from scikit-learn’s
    `model_selection` submodule:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个数据集随机划分为独立的测试和训练数据集的便捷方法是使用scikit-learn的`model_selection`子模块中的`train_test_split`函数：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: First, we assigned the NumPy array representation of the feature columns 1-13
    to the variable `X` and we assigned the class labels from the first column to
    the variable `y`. Then, we used the `train_test_split` function to randomly split
    `X` and `y` into separate training and test datasets.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将特征列1-13的NumPy数组表示分配给变量`X`，并将第一列的类标签分配给变量`y`。然后，我们使用`train_test_split`函数将`X`和`y`随机分割成独立的训练和测试数据集。
- en: By setting `test_size=0.3`, we assigned 30 percent of the wine examples to `X_test`
    and `y_test`, and the remaining 70 percent of the examples were assigned to `X_train`
    and `y_train`, respectively. Providing the class label array `y` as an argument
    to `stratify` ensures that both training and test datasets have the same class
    proportions as the original dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`test_size=0.3`，我们将30%的葡萄酒样本分配给`X_test`和`y_test`，剩余的70%样本分别分配给`X_train`和`y_train`。将类标签数组`y`作为参数传递给`stratify`，确保训练和测试数据集具有与原始数据集相同的类比例。
- en: '**Choosing an appropriate ratio for partitioning a dataset into training and
    test datasets**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **选择合适的比例将数据集划分为训练集和测试集**'
- en: If we are dividing a dataset into training and test datasets, we have to keep
    in mind that we are withholding valuable information that the learning algorithm
    could benefit from. Thus, we don’t want to allocate too much information to the
    test set. However, the smaller the test set, the more inaccurate the estimation
    of the generalization error. Dividing a dataset into training and test datasets
    is all about balancing this tradeoff. In practice, the most commonly used splits
    are 60:40, 70:30, or 80:20, depending on the size of the initial dataset. However,
    for large datasets, 90:10 or 99:1 splits are also common and appropriate. For
    example, if the dataset contains more than 100,000 training examples, it might
    be fine to withhold only 10,000 examples for testing in order to get a good estimate
    of the generalization performance. More information and illustrations can be found
    in section one of my article *Model evaluation, model selection, and algorithm
    selection in machine learning*, which is freely available at [https://arxiv.org/pdf/1811.12808.pdf](https://arxiv.org/pdf/1811.12808.pdf).
    Also, we will revisit the topic of model evaluation and discuss it in more detail
    in *Chapter 6*, *Learning Best Practices for Model Evaluation and Hyperparameter
    Tuning*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据集划分为训练集和测试集，必须记住我们正在保留学习算法可能从中受益的宝贵信息。因此，我们不希望将太多信息分配给测试集。然而，测试集越小，对泛化误差的估计就越不准确。将数据集划分为训练集和测试集就是要在这种权衡中找到平衡。在实践中，最常用的划分比例是60:40、70:30或80:20，这取决于初始数据集的大小。然而，对于大型数据集，90:10或99:1的划分也是常见且合适的。例如，如果数据集包含超过10万个训练样本，则仅保留1万个样本进行测试可能足以得到泛化性能的良好估计。更多信息和插图可以在我的文章《机器学习中的模型评估、模型选择和算法选择》第一章找到，该文章可以在[https://arxiv.org/pdf/1811.12808.pdf](https://arxiv.org/pdf/1811.12808.pdf)免费获取。此外，我们将在*第6章*
    *学习模型评估和超参数调优的最佳实践*中重新讨论模型评估的主题并进行更详细的讨论。
- en: Moreover, instead of discarding the allocated test data after model training
    and evaluation, it is a common practice to retrain a classifier on the entire
    dataset, as it can improve the predictive performance of the model. While this
    approach is generally recommended, it could lead to worse generalization performance
    if the dataset is small and the test dataset contains outliers, for example. Also,
    after refitting the model on the whole dataset, we don’t have any independent
    data left to evaluate its performance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与其在模型训练和评估后丢弃分配的测试数据，重新在整个数据集上训练分类器是一种常见的做法，因为这可以提高模型的预测性能。虽然这种方法通常是推荐的，但如果数据集很小且测试数据集包含异常值，例如，它可能导致更差的泛化性能。此外，在整个数据集上重新拟合模型之后，我们将没有任何独立的数据来评估其性能。
- en: Bringing features onto the same scale
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将特征调整到相同的尺度
- en: '**Feature scaling** is a crucial step in our preprocessing pipeline that can
    easily be forgotten. **Decision trees** and **random forests** are two of the
    very few machine learning algorithms where we don’t need to worry about feature
    scaling. Those algorithms are scale-invariant. However, the majority of machine
    learning and optimization algorithms behave much better if features are on the
    same scale, as we saw in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*, when we implemented the **gradient descent optimization**
    algorithm.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '-   **特征缩放** 是我们预处理流程中一个关键的步骤，容易被忽视。**决策树** 和 **随机森林** 是为数不多的两种机器学习算法，我们不需要担心特征缩放。这些算法是尺度不变的。然而，大多数机器学习和优化算法如果特征处于相同的尺度上表现更好，正如我们在*第2章*
    *用于分类的简单机器学习算法的训练*中实现 **梯度下降优化** 算法时所看到的那样。'
- en: The importance of feature scaling can be illustrated by a simple example. Let’s
    assume that we have two features where one feature is measured on a scale from
    1 to 10 and the second feature is measured on a scale from 1 to 100,000, respectively.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放的重要性可以通过一个简单的例子来说明。假设我们有两个特征，其中一个特征在1到10的范围内测量，而第二个特征在1到100,000的范围内测量。
- en: 'When we think of the squared error function in Adaline from *Chapter 2*, it
    makes sense to say that the algorithm will mostly be busy optimizing the weights
    according to the larger errors in the second feature. Another example is the **k-nearest
    neighbors** (**KNN**) algorithm with a Euclidean distance measure: the computed
    distances between examples will be dominated by the second feature axis.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑 Adaline 中的平方误差函数（来自*第二章*）时，可以说该算法主要忙于根据第二特征中较大的错误来优化权重。另一个例子是使用欧氏距离的**k最近邻**（**KNN**）算法：计算的示例间距离将由第二特征轴主导。
- en: 'Now, there are two common approaches to bringing different features onto the
    same scale: **normalization** and **standardization**. Those terms are often used
    quite loosely in different fields, and the meaning has to be derived from the
    context. Most often, normalization refers to the rescaling of the features to
    a range of [0, 1], which is a special case of **min-max scaling**. To normalize
    our data, we can simply apply the min-max scaling to each feature column, where
    the new value, ![](img/B17582_04_001.png), of an example, *x*^(^i^), can be calculated
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有两种常见方法将不同的特征调整到相同的比例：**归一化**和**标准化**。这些术语在不同领域中通常使用得相当松散，其含义必须从上下文中推断出来。最常见的情况是，归一化是指将特征重新缩放到[0, 1]的范围，这是**最小-最大缩放**的一种特殊情况。要将我们的数据归一化，我们可以简单地对每个特征列应用最小-最大缩放，其中示例的新值，![](img/B17582_04_001.png)，可以计算如下：
- en: '![](img/B17582_04_002.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_002.png)'
- en: Here, *x*^(^i^) is a particular example, *x*[min] is the smallest value in a
    feature column, and *x*[max] is the largest value.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*^(^i^) 是一个特定的示例，*x*[min] 是特征列中的最小值，*x*[max] 是最大值。
- en: 'The min-max scaling procedure is implemented in scikit-learn and can be used
    as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放过程在 scikit-learn 中实现，可以如下使用：
- en: '[PRE28]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Although normalization via min-max scaling is a commonly used technique that
    is useful when we need values in a bounded interval, standardization can be more
    practical for many machine learning algorithms, especially for optimization algorithms
    such as gradient descent. The reason is that many linear models, such as the logistic
    regression and SVM from *Chapter 3*, initialize the weights to 0 or small random
    values close to 0\. Using standardization, we center the feature columns at mean
    0 with standard deviation 1 so that the feature columns have the same parameters
    as a standard normal distribution (zero mean and unit variance), which makes it
    easier to learn the weights. However, we shall emphasize that standardization
    does not change the shape of the distribution, and it does not transform non-normally
    distributed data into normally distributed data. In addition to scaling data such
    that it has zero mean and unit variance, standardization maintains useful information
    about outliers and makes the algorithm less sensitive to them in contrast to min-max
    scaling, which scales the data to a limited range of values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过最小-最大缩放进行标准化是一种常用的技术，当我们需要在有界区间内的值时很有用，但对于许多机器学习算法，特别是像梯度下降这样的优化算法，标准化可能更为实用。原因是许多线性模型，例如*第三章*中的逻辑回归和SVM，将权重初始化为0或接近0的小随机值。使用标准化，我们将特征列居中于均值0且标准差为1，使得特征列具有与标准正态分布（零均值和单位方差）相同的参数，这样更容易学习权重。但是，我们应强调，标准化不会改变分布的形状，也不会将非正态分布的数据转换为正态分布的数据。除了缩放数据以使其具有零均值和单位方差外，标准化还保留有关异常值的有用信息，并使算法对其不敏感，而最小-最大缩放将数据缩放到一定范围的值。
- en: 'The procedure for standardization can be expressed by the following equation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化过程可以用以下方程表示：
- en: '![](img/B17582_04_003.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_003.png)'
- en: Here, ![](img/B17582_04_004.png) is the sample mean of a particular feature
    column, and ![](img/B17582_04_005.png) is the corresponding standard deviation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B17582_04_004.png) 是特定特征列的样本均值，![](img/B17582_04_005.png) 是相应的标准差。
- en: 'The following table illustrates the difference between the two commonly used
    feature scaling techniques, standardization and normalization, on a simple example
    dataset consisting of numbers 0 to 5:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格说明了两种常用的特征缩放技术——标准化和归一化——在一个由数字0到5组成的简单示例数据集上的差异：
- en: '| **Input** | **Standardized** | **Min-max normalized** |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| **输入** | **标准化** | **最小-最大归一化** |'
- en: '| 0.0 | -1.46385 | 0.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 0.0 | -1.46385 | 0.0 |'
- en: '| 1.0 | -0.87831 | 0.2 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | -0.87831 | 0.2 |'
- en: '| 2.0 | -0.29277 | 0.4 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 2.0 | -0.29277 | 0.4 |'
- en: '| 3.0 | 0.29277 | 0.6 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 3.0 | 0.29277 | 0.6 |'
- en: '| 4.0 | 0.87831 | 0.8 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 4.0 | 0.87831 | 0.8 |'
- en: '| 5.0 | 1.46385 | 1.0 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 5.0 | 1.46385 | 1.0 |'
- en: 'Table 4.1: A comparison between standardization and min-max normalization'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1：标准化和最小-最大归一化的比较
- en: 'You can perform the standardization and normalization shown in the table manually
    by executing the following code examples:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过执行以下代码示例手动执行表中显示的标准化和归一化：
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Similar to the `MinMaxScaler` class, scikit-learn also implements a class for
    standardization:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与`MinMaxScaler`类似，scikit-learn还实现了一个用于标准化的类：
- en: '[PRE30]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Again, it is also important to highlight that we fit the `StandardScaler` class
    only once—on the training data—and use those parameters to transform the test
    dataset or any new data point.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们只需在训练数据上一次性拟合`StandardScaler`类，然后使用这些参数来转换测试数据集或任何新的数据点。
- en: Other, more advanced methods for feature scaling are available from scikit-learn,
    such as `RobustScaler`. `RobustScaler` is especially helpful and recommended if
    we are working with small datasets that contain many outliers. Similarly, if the
    machine learning algorithm applied to this dataset is prone to **overfitting**,
    `RobustScaler` can be a good choice. Operating on each feature column independently,
    `RobustScaler` removes the median value and scales the dataset according to the
    1st and 3rd quartile of the dataset (that is, the 25th and 75th quantile, respectively)
    such that more extreme values and outliers become less pronounced. The interested
    reader can find more information about `RobustScaler` in the official scikit-learn
    documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特征缩放的其他更高级的方法可从scikit-learn中获取，例如`RobustScaler`。如果我们处理的数据集很小且包含许多异常值，`RobustScaler`尤为有用和推荐。同样，如果应用于该数据集的机器学习算法容易**过拟合**，`RobustScaler`是一个不错的选择。`RobustScaler`独立于每个特征列操作，去除中位数并根据数据集的第1和第3四分位数（即25th和75th分位数）来缩放数据集，使得更极端的值和异常值变得不那么显著。有兴趣的读者可以在官方scikit-learn文档中找到关于`RobustScaler`的更多信息，网址为[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)。
- en: Selecting meaningful features
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择有意义的特征
- en: 'If we notice that a model performs much better on a training dataset than on
    the test dataset, this observation is a strong indicator of overfitting. As we
    discussed in *Chapter 3*, *A Tour of Machine Learning Classifiers Using Scikit-Learn*,
    overfitting means the model fits the parameters too closely with regard to the
    particular observations in the training dataset but does not generalize well to
    new data; we say that the model has a **high variance**. The reason for the overfitting
    is that our model is too complex for the given training data. Common solutions
    to reduce the generalization error are as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们注意到一个模型在训练数据集上的表现远远优于在测试数据集上的表现，这一观察结果是过拟合的一个强烈指标。正如我们在*第三章*中讨论的那样，使用Scikit-Learn进行机器学习分类器的巡回时，过拟合意味着模型过于密切地拟合了训练数据集中的特定观测值，但在新数据上泛化能力不强；我们称这种模型具有**高方差**。过拟合的原因是我们的模型对给定的训练数据过于复杂。减少泛化误差的常见解决方案如下：
- en: Collect more training data
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集更多的训练数据
- en: Introduce a penalty for complexity via regularization
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入正则化通过复杂性来惩罚
- en: Choose a simpler model with fewer parameters
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个具有较少参数的简单模型
- en: Reduce the dimensionality of the data
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少数据的维度
- en: Collecting more training data is often not applicable. In *Chapter 6*, *Learning
    Best Practices for Model Evaluation and Hyperparameter Tuning*, we will learn
    about a useful technique to check whether more training data is helpful. In the
    following sections, we will look at common ways to reduce overfitting by regularization
    and dimensionality reduction via feature selection, which leads to simpler models
    by requiring fewer parameters to be fitted to the data. Then, in *Chapter 5*,
    *Compressing Data via Dimensionality Reduction*, we will take a look at additional
    feature extraction techniques.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 增加更多的训练数据通常是不适用的。在*第6章*，*学习模型评估和超参数调优的最佳实践*中，我们将学习一种有用的技术来检查是否增加更多的训练数据是有益的。在接下来的几节中，我们将探讨通过正则化和特征选择来减少过拟合的常见方法，从而通过需要较少参数来拟合数据的简化模型。然后，在*第5章*，*通过降维压缩数据*，我们将查看其他的特征提取技术。
- en: L1 and L2 regularization as penalties against model complexity
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1和L2正则化作为抵抗模型复杂性的惩罚项
- en: 'You will recall from *Chapter 3* that **L2 regularization** is one approach
    to reduce the complexity of a model by penalizing large individual weights. We
    defined the squared L2 norm of our weight vector, **w**, as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得*第三章*讲到的**L2 正则化**是通过对大的个体权重进行惩罚来减少模型复杂度的一种方法。我们定义了权重向量**w**的平方L2范数如下：
- en: '![](img/B17582_04_006.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_006.png)'
- en: 'Another approach to reduce the model complexity is the related **L1 regularization**:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少模型复杂性的方法是相关的**L1正则化**：
- en: '![](img/B17582_04_007.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_007.png)'
- en: Here, we simply replaced the square of the weights with the sum of the absolute
    values of the weights. In contrast to L2 regularization, L1 regularization usually
    yields sparse feature vectors, and most feature weights will be zero. Sparsity
    can be useful in practice if we have a high-dimensional dataset with many features
    that are irrelevant, especially in cases where we have more irrelevant dimensions
    than training examples. In this sense, L1 regularization can be understood as
    a technique for feature selection.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简单地用权重的绝对值之和替换了权重的平方。与L2正则化相比，L1正则化通常会产生稀疏的特征向量，大多数特征权重将为零。如果我们有一个高维数据集，有许多无关的特征，尤其是在训练样本比无关维度更多的情况下，稀疏性在实践中可能会很有用。从这个意义上讲，L1正则化可以被理解为一种特征选择技术。
- en: A geometric interpretation of L2 regularization
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2正则化的几何解释
- en: As mentioned in the previous section, L2 regularization adds a penalty term
    to the loss function that effectively results in less extreme weight values compared
    to a model trained with an unregularized loss function.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节提到的，L2正则化向损失函数添加一个惩罚项，使得相比使用非正则化损失函数训练的模型具有较少极端的权重值。
- en: To better understand how L1 regularization encourages sparsity, let’s take a
    step back and take a look at a geometric interpretation of regularization. Let’s
    plot the contours of a convex loss function for two weight coefficients, *w*[1]
    and *w*[2].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解L1正则化如何促进稀疏性，让我们退一步，从正则化的几何解释开始。我们来绘制两个权重系数*w*[1]和*w*[2]的凸损失函数等高线。
- en: 'Here, we will consider the **mean squared error** (**MSE**) loss function that
    we used for Adaline in *Chapter 2*, which computes the squared distances between
    the true and predicted class labels, *y* and ![](img/B17582_04_008.png), averaged
    over all *N* examples in the training set. Since the MSE is spherical, it is easier
    to draw than the loss function of logistic regression; however, the same concepts
    apply. Remember that our goal is to find the combination of weight coefficients
    that minimize the loss function for the training data, as shown in *Figure 4.5*
    (the point in the center of the ellipses):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将考虑**均方误差**（**MSE**）损失函数，我们在*第2章*中用于Adaline的，它计算真实和预测类标签*y*和![](img/B17582_04_008.png)之间的平方距离，平均值为所有*N*个训练集示例。由于MSE是球形的，比逻辑回归的损失函数更容易绘制；然而，相同的概念适用。记住，我们的目标是找到最小化训练数据损失函数的权重系数组合，如*图4.5*所示（椭圆中心的点）：
- en: '![Diagram, engineering drawing  Description automatically generated](img/B17582_04_05.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图表，工程图绘制 自动生成描述](img/B17582_04_05.png)'
- en: 'Figure 4.5: Minimizing the mean squared error loss function'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：最小化均方误差损失函数
- en: 'We can think of regularization as adding a penalty term to the loss function
    to encourage smaller weights; in other words, we penalize large weights. Thus,
    by increasing the regularization strength via the regularization parameter, ![](img/B17582_03_040.png),
    we shrink the weights toward zero and decrease the dependence of our model on
    the training data. Let’s illustrate this concept in the following figure for the
    L2 penalty term:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将正则化视为向损失函数添加惩罚项以鼓励较小的权重；换句话说，我们惩罚较大的权重。因此，通过增加正则化参数来增强正则化强度，![](https://example.org/B17582_03_040.png)，我们将权重收缩到零附近，并减少模型对训练数据的依赖。让我们在以下图中以L2惩罚项说明这个概念：
- en: '![Diagram, engineering drawing  Description automatically generated](img/B17582_04_06.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图解释自动生成](https://example.org/B17582_04_06.png)'
- en: 'Figure 4.6: Applying L2 regularization to the loss function'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6：将L2正则化应用于损失函数
- en: The quadratic L2 regularization term is represented by the shaded ball. Here,
    our weight coefficients cannot exceed our regularization budget—the combination
    of the weight coefficients cannot fall outside the shaded area. On the other hand,
    we still want to minimize the loss function. Under the penalty constraint, our
    best effort is to choose the point where the L2 ball intersects with the contours
    of the unpenalized loss function. The larger the value of the regularization parameter,
    ![](img/B17582_04_010.png), gets, the faster the penalized loss grows, which leads
    to a narrower L2 ball. For example, if we increase the regularization parameter
    toward infinity, the weight coefficients will become effectively zero, denoted
    by the center of the L2 ball. To summarize the main message of the example, our
    goal is to minimize the sum of the unpenalized loss plus the penalty term, which
    can be understood as adding bias and preferring a simpler model to reduce the
    variance in the absence of sufficient training data to fit the model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 二次L2正则化项由阴影球表示。在这里，我们的权重系数不能超出正则化预算——权重系数的组合不能超出阴影区域。另一方面，我们仍然希望最小化损失函数。在惩罚约束下，我们最好的选择是选择L2球与未惩罚损失函数轮廓相交的点。正则化参数![](https://example.org/B17582_04_010.png)值越大，惩罚损失增长速度越快，导致L2球越窄。例如，如果我们将正则化参数增加至无穷大，权重系数将有效变为零，即L2球的中心。总结这个示例的主要信息，我们的目标是最小化未惩罚损失加上惩罚项的总和，这可以理解为添加偏差并偏好简化模型以减少在缺乏足够训练数据来拟合模型时的方差。
- en: Sparse solutions with L1 regularization
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1正则化下的稀疏解决方案
- en: 'Now, let’s discuss L1 regularization and sparsity. The main concept behind
    L1 regularization is similar to what we discussed in the previous section. However,
    since the L1 penalty is the sum of the absolute weight coefficients (remember
    that the L2 term is quadratic), we can represent it as a diamond-shape budget,
    as shown in *Figure 4.7*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论L1正则化和稀疏性。L1正则化背后的主要概念与我们在前一节中讨论的相似。然而，由于L1惩罚是绝对权重系数的总和（请记住L2项是二次的），我们可以将其表示为钻石形状的预算，如*图4.7*所示：
- en: '![Diagram  Description automatically generated](img/B17582_04_07.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图解释自动生成](https://example.org/B17582_04_07.png)'
- en: 'Figure 4.7: Applying L1 regularization to the loss function'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：将L1正则化应用于损失函数
- en: In the preceding figure, we can see that the contour of the loss function touches
    the L1 diamond at *w*[1] = 0\. Since the contours of an L1 regularized system
    are sharp, it is more likely that the optimum—that is, the intersection between
    the ellipses of the loss function and the boundary of the L1 diamond—is located
    on the axes, which encourages sparsity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，我们可以看到损失函数的轮廓与L1钻石在*w*[1] = 0处接触。由于L1正则化系统的轮廓尖锐，最优解——即损失函数的椭圆和L1钻石边界的交点——更可能位于轴上，这鼓励稀疏性。
- en: '**L1 regularization and sparsity**'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**L1正则化和稀疏性**'
- en: The mathematical details of why L1 regularization can lead to sparse solutions
    are beyond the scope of this book. If you are interested, an excellent explanation
    of L2 versus L1 regularization can be found in *Section 3.4*, *The Elements of
    Statistical Learning* by *Trevor Hastie, Robert Tibshirani*, and *Jerome Friedman,
    Springer Science+Business Media,* 2009.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化如何导致稀疏解的数学细节超出了本书的范围。如果您有兴趣，可以在*Trevor Hastie, Robert Tibshirani*和*Jerome
    Friedman, Springer Science+Business Media, 2009*的*第3.4节*中找到关于L2与L1正则化的优秀解释。
- en: 'For regularized models in scikit-learn that support L1 regularization, we can
    simply set the `penalty` parameter to `''l1''` to obtain a sparse solution:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于scikit-learn中支持L1正则化的正则化模型，我们只需将`penalty`参数设置为`'l1'`即可获得稀疏解：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Note that we also need to select a different optimization algorithm (for example,
    `solver=''liblinear''`), since `''lbfgs''` currently does not support L1-regularized
    loss optimization. Applied to the standardized Wine data, the L1 regularized logistic
    regression would yield the following sparse solution:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于'lbfgs'当前不支持L1正则化损失优化，我们还需要选择不同的优化算法（例如，`solver='liblinear'`）。应用于标准化的Wine数据，L1正则化逻辑回归将产生以下稀疏解：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Both training and test accuracies (both 100 percent) indicate that our model
    does a perfect job on both datasets. When we access the intercept terms via the
    `lr.intercept_` attribute, we can see that the array returns three values:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试的准确率（均为100%）表明我们的模型在两个数据集上表现完美。当我们通过`lr.intercept_`属性访问截距项时，可以看到数组返回了三个值：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Since we fit the `LogisticRegression` object on a multiclass dataset via the
    **one-versus-rest** (**OvR**) approach, the first intercept belongs to the model
    that fits class 1 versus classes 2 and 3, the second value is the intercept of
    the model that fits class 2 versus classes 1 and 3, and the third value is the
    intercept of the model that fits class 3 versus classes 1 and 2:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通过**一对多**（**OvR**）方法在多类数据集上拟合了`LogisticRegression`对象，第一个截距属于拟合类别1与类别2和3的模型，第二个值是拟合类别2与类别1和3的模型的截距，第三个值是拟合类别3与类别1和2的模型的截距：
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The weight array that we accessed via the `lr.coef_` attribute contains three
    rows of weight coefficients, one weight vector for each class. Each row consists
    of 13 weights, where each weight is multiplied by the respective feature in the
    13-dimensional Wine dataset to calculate the net input:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`lr.coef_`属性访问的权重数组包含三行权重系数，即每个类别的一个权重向量。每行包含13个权重，其中每个权重都与13维Wine数据集中的相应特征相乘，以计算净输入：
- en: '![](img/B17582_04_011.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_011.png)'
- en: '**Accessing the bias unit and weight parameters of scikit-learn estimators**'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**访问scikit-learn估计器的偏置单元和权重参数**'
- en: In scikit-learn, `intercept_` corresponds to the bias unit and `coef_` corresponds
    to the values *w*[j].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`intercept_`对应于偏置单元，`coef_`对应于值*w*[j]。
- en: As a result of L1 regularization, which, as mentioned, serves as a method for
    feature selection, we just trained a model that is robust to the potentially irrelevant
    features in this dataset. Strictly speaking, though, the weight vectors from the
    previous example are not necessarily sparse because they contain more non-zero
    than zero entries. However, we could enforce sparsity (more zero entries) by further
    increasing the regularization strength—that is, choosing lower values for the
    `C` parameter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于L1正则化的结果，正如前面提到的，它作为特征选择的一种方法，我们刚刚训练了一个在该数据集中对潜在的不相关特征具有鲁棒性的模型。严格来说，尽管如此，前面例子中的权重向量未必是稀疏的，因为它们包含的非零条目比零条目多。然而，我们可以通过进一步增加正则化强度（即选择较低的`C`参数值）来强制稀疏化（增加零条目）。
- en: 'In the last example on regularization in this chapter, we will vary the regularization
    strength and plot the regularization path—the weight coefficients of the different
    features for different regularization strengths:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章关于正则化的最后一个例子中，我们将改变正则化强度并绘制正则化路径，即不同正则化强度下不同特征的权重系数：
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The resulting plot provides us with further insights into the behavior of L1
    regularization. As we can see, all feature weights will be zero if we penalize
    the model with a strong regularization parameter (*C* < 0.01); *C* is the inverse
    of the regularization parameter, ![](img/B17582_03_040.png):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的结果图为我们提供了关于L1正则化行为的进一步见解。正如我们所见，如果我们使用强正则化参数（*C* < 0.01），所有特征权重将变为零；其中*C*是正则化参数的倒数，![](img/B17582_03_040.png)：
- en: '![A picture containing chart  Description automatically generated](img/B17582_04_08.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含图表描述的图片](img/B17582_04_08.png)'
- en: 'Figure 4.8: The impact of the value of the regularization strength hyperparameter
    C'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8：正则化强度超参数C值的影响
- en: Sequential feature selection algorithms
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序特征选择算法
- en: 'An alternative way to reduce the complexity of the model and avoid overfitting
    is **dimensionality reduction** via feature selection, which is especially useful
    for unregularized models. There are two main categories of dimensionality reduction
    techniques: **feature selection** and **feature extraction**. Via feature selection,
    we *select* a subset of the original features, whereas in feature extraction,
    we *derive* information from the feature set to construct a new feature subspace.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 减少模型复杂性和避免过拟合的另一种方法是通过特征选择进行**降维**，特别适用于未正则化的模型。主要有两类降维技术：**特征选择**和**特征提取**。通过特征选择，我们*选择*原始特征的一个子集，而在特征提取中，我们*从*特征集中*提取*信息以构建一个新的特征子空间。
- en: In this section, we will take a look at a classic family of feature selection
    algorithms. In the next chapter, *Chapter 5*, *Compressing Data via Dimensionality
    Reduction*, we will learn about different feature extraction techniques to compress
    a dataset onto a lower-dimensional feature subspace.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一类经典的特征选择算法。在下一章，即*第5章*，*通过降维来压缩数据*，我们将学习不同的特征提取技术，以将数据集压缩到一个更低维度的特征子空间上。
- en: Sequential feature selection algorithms are a family of greedy search algorithms
    that are used to reduce an initial *d*-dimensional feature space to a *k*-dimensional
    feature subspace where *k*<*d*. The motivation behind feature selection algorithms
    is to automatically select a subset of features that are most relevant to the
    problem, to improve computational efficiency, or to reduce the generalization
    error of the model by removing irrelevant features or noise, which can be useful
    for algorithms that don’t support regularization.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序特征选择算法是一类贪婪搜索算法，用于将初始*d*维特征空间减少到一个*k*维特征子空间，其中*k*<*d*。特征选择算法的动机是自动选择与问题最相关的特征子集，以提高计算效率，或通过移除无关特征或噪声来减少模型的泛化误差，这对于不支持正则化的算法可能非常有用。
- en: A classic sequential feature selection algorithm is **sequential backward selection**
    (**SBS**), which aims to reduce the dimensionality of the initial feature subspace
    with a minimum decay in the performance of the classifier to improve upon computational
    efficiency. In certain cases, SBS can even improve the predictive power of the
    model if a model suffers from overfitting.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的顺序特征选择算法是**顺序向后选择**（**SBS**），其旨在减少初始特征子空间的维数，同时最小化分类器性能下降，以提高计算效率。在某些情况下，如果模型存在过拟合问题，SBS甚至可以改善模型的预测能力。
- en: '**Greedy search algorithms**'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**贪婪搜索算法**'
- en: Greedy algorithms make locally optimal choices at each stage of a combinatorial
    search problem and generally yield a suboptimal solution to the problem, in contrast
    to exhaustive search algorithms, which evaluate all possible combinations and
    are guaranteed to find the optimal solution. However, in practice, an exhaustive
    search is often computationally not feasible, whereas greedy algorithms allow
    for a less complex, computationally more efficient solution.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪算法在组合搜索问题的每个阶段都会做出局部最优选择，通常会得到问题的次优解，与穷举搜索算法相比，后者会评估所有可能的组合并保证找到最优解。然而，在实践中，穷举搜索通常计算量过大，而贪婪算法可以提供更简单、计算更高效的解决方案。
- en: 'The idea behind the SBS algorithm is quite simple: SBS sequentially removes
    features from the full feature subset until the new feature subspace contains
    the desired number of features. To determine which feature is to be removed at
    each stage, we need to define the criterion function, *J*, that we want to minimize.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: SBS算法的思想非常简单：顺序地从完整特征子集中移除特征，直到新的特征子空间包含所需数量的特征为止。为了确定每个阶段要移除哪个特征，我们需要定义要最小化的准则函数*J*。
- en: 'The criterion calculated by the criterion function can simply be the difference
    in the performance of the classifier before and after the removal of a particular
    feature. Then, the feature to be removed at each stage can simply be defined as
    the feature that maximizes this criterion; or in more simple terms, at each stage
    we eliminate the feature that causes the least performance loss after removal.
    Based on the preceding definition of SBS, we can outline the algorithm in four
    simple steps:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 由准则函数计算的准则可以简单地是分类器在移除特定特征之前和之后性能差异。然后，在每个阶段，我们可以简单地定义要移除的特征为最大化此准则的特征；或者更简单地说，在每个阶段，我们消除导致去除后性能损失最小的特征。基于前述对SBS的定义，我们可以用四个简单步骤概述算法：
- en: Initialize the algorithm with *k* = *d*, where *d* is the dimensionality of
    the full feature space, **X**[d].
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*k* = *d*初始化算法，其中*d*是完整特征空间**X**[d]的维数。
- en: 'Determine the feature, **x**^–, that maximizes the criterion: **x**^– = argmax
    *J*(**X**[k] – **x**), where ![](img/B17582_04_013.png).'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定最大化准则的特征**x**^–，其中**x**^– = argmax *J*(**X**[k] – **x**)，其中 ![](img/B17582_04_013.png)。
- en: 'Remove the feature, **x**^–, from the feature set: **X**[k][–1] = **X**[k] – **x**^–; *k* = *k* – 1.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征集中移除特征**x**^–：**X**[k][–1] = **X**[k] – **x**^–；*k* = *k* – 1。
- en: Terminate if *k* equals the number of desired features; otherwise, go to *step
    2*.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*k*等于所需特征的数量，则终止；否则，转到*步骤2*。
- en: '**A resource on sequential feature algorithms**'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**关于顺序特征算法的资源**'
- en: You can find a detailed evaluation of several sequential feature algorithms
    in *Comparative Study of Techniques for Large-Scale Feature Selection* by *F.
    Ferri*, *P. Pudil*, *M. Hatef*, and *J. Kittler*, pages 403-413, 1994.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*大规模特征选择技术比较研究*中，你可以找到对几种顺序特征算法的详细评估，作者是*F. Ferri*、*P. Pudil*、*M. Hatef*和*J.
    Kittler*，页面403-413，1994年。
- en: 'To practice our coding skills and ability to implement our own algorithms,
    let’s go ahead and implement it in Python from scratch:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了练习我们的编码技能和实现我们自己的算法的能力，让我们从头开始用Python实现它：
- en: '[PRE36]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding implementation, we defined the `k_features` parameter to specify
    the desired number of features we want to return. By default, we use `accuracy_score`
    from scikit-learn to evaluate the performance of a model (an estimator for classification)
    on the feature subsets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述实现中，我们定义了`k_features`参数，以指定我们希望返回的特征数。默认情况下，我们使用scikit-learn中的`accuracy_score`来评估模型（分类器的估计器）在特征子集上的性能。
- en: Inside the `while` loop of the `fit` method, the feature subsets created by
    the `itertools.combination` function are evaluated and reduced until the feature
    subset has the desired dimensionality. In each iteration, the accuracy score of
    the best subset is collected in a list, `self.scores_`, based on the internally
    created test dataset, `X_test`. We will use those scores later to evaluate the
    results. The column indices of the final feature subset are assigned to `self.indices_`,
    which we can use via the `transform` method to return a new data array with the
    selected feature columns. Note that, instead of calculating the criterion explicitly
    inside the `fit` method, we simply removed the feature that is not contained in
    the best performing feature subset.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在`fit`方法的`while`循环内，通过`itertools.combination`函数创建的特征子集将被评估和减少，直到特征子集具有所需的维数。在每次迭代中，基于内部创建的测试数据集`X_test`收集最佳子集的准确度分数到列表`self.scores_`中。我们稍后将使用这些分数来评估结果。最终特征子集的列索引被赋值给`self.indices_`，我们可以通过`transform`方法使用它们返回带有选定特征列的新数据数组。请注意，在`fit`方法内部，我们没有显式计算准则，而是简单地删除了未包含在性能最佳特征子集中的特征。
- en: 'Now, let’s see our SBS implementation in action using the KNN classifier from
    scikit-learn:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们使用scikit-learn中的KNN分类器实现的SBS算法的实际效果：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Although our SBS implementation already splits the dataset into a test and training
    dataset inside the `fit` function, we still fed the training dataset, `X_train`,
    to the algorithm. The SBS `fit` method will then create new training subsets for
    testing (validation) and training, which is why this test set is also called the
    **validation dataset**. This approach is necessary to prevent our *original* test
    set from becoming part of the training data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的SBS实现已经在`fit`函数内部将数据集分割成测试和训练数据集，我们仍将训练数据集`X_train`提供给算法。然后，SBS的`fit`方法将为测试（验证）和训练创建新的训练子集，这也是为什么这个测试集也称为**验证数据集**。这种方法是为了防止我们的*原始*测试集成为训练数据的一部分。
- en: 'Remember that our SBS algorithm collects the scores of the best feature subset
    at each stage, so let’s move on to the more exciting part of our implementation
    and plot the classification accuracy of the KNN classifier that was calculated
    on the validation dataset. The code is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的SBS算法收集了每个阶段最佳特征子集的分数，所以让我们继续进行我们实现更令人兴奋的部分，并绘制在验证数据集上计算的KNN分类器的分类准确率。代码如下：
- en: '[PRE38]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As we can see in *Figure 4.9*, the accuracy of the KNN classifier improved
    on the validation dataset as we reduced the number of features, which is likely
    due to a decrease in the **curse of dimensionality** that we discussed in the
    context of the KNN algorithm in *Chapter 3*. Also, we can see in the following
    plot that the classifier achieved 100 percent accuracy for *k* = {3, 7, 8, 9, 10, 11, 12}:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图4.9*中看到的，随着特征数量的减少，KNN分类器在验证数据集上的准确率有所提高，这很可能是由于我们在第3章中讨论的KNN算法背景下维度诅咒的减少。此外，在下图中我们可以看到，对于*k*
    = {3, 7, 8, 9, 10, 11, 12}，分类器在验证数据集上实现了100%的准确率：
- en: '![](img/B17582_04_09.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_09.png)'
- en: 'Figure 4.9: Impact of number of features on model accuracy'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：特征数量对模型准确率的影响
- en: 'To satisfy our own curiosity, let’s see what the smallest feature subset (*k*=3),
    which yielded such a good performance on the validation dataset, looks like:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 出于我们自己的好奇心，让我们看看最小的特征子集（*k*=3），它在验证数据集上表现出色：
- en: '[PRE39]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Using the preceding code, we obtained the column indices of the three-feature
    subset from the 11th position in the `sbs.subsets_` attribute and returned the
    corresponding feature names from the column index of the pandas Wine `DataFrame`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们从`sbs.subsets_`属性的第11个位置获得了三特征子集的列索引，并从pandas Wine `DataFrame`中返回了相应的特征名称。
- en: 'Next, let’s evaluate the performance of the KNN classifier on the original
    test dataset:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们评估KNN分类器在原始测试数据集上的性能：
- en: '[PRE40]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the preceding code section, we used the complete feature set and obtained
    approximately 97 percent accuracy on the training dataset and approximately 96
    percent accuracy on the test dataset, which indicates that our model already generalizes
    well to new data. Now, let’s use the selected three-feature subset and see how
    well KNN performs:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码段中，我们使用完整的特征集合，在训练数据集上获得了约97%的准确率，在测试数据集上获得了约96%的准确率，这表明我们的模型已经很好地推广到了新数据。现在，让我们使用选定的三特征子集，看看KNN的表现如何：
- en: '[PRE41]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: When using less than a quarter of the original features in the Wine dataset,
    the prediction accuracy on the test dataset declined slightly. This may indicate
    that those three features do not provide less discriminatory information than
    the original dataset. However, we also have to keep in mind that the Wine dataset
    is a small dataset and is very susceptible to randomness—that is, the way we split
    the dataset into training and test subsets, and how we split the training dataset
    further into a training and validation subset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Wine数据集中使用少于原始特征四分之一时，对测试数据集的预测准确率略有下降。这可能表明这三个特征提供的信息并不比原始数据集中的差异信息少。然而，我们也必须记住Wine数据集是一个小数据集，并且非常容易受到随机性的影响——也就是说，我们如何将数据集分割为训练和测试子集，以及如何将训练数据集进一步分割为训练和验证子集。
- en: While we did not increase the performance of the KNN model by reducing the number
    of features, we shrank the size of the dataset, which can be useful in real-world
    applications that may involve expensive data collection steps. Also, by substantially
    reducing the number of features, we obtain simpler models, which are easier to
    interpret.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们通过减少特征数量并没有提高KNN模型的性能，但我们缩小了数据集的大小，这在可能涉及昂贵数据收集步骤的真实应用中可能是有用的。此外，通过大幅减少特征数量，我们获得了更简单的模型，更易于解释。
- en: '**Feature selection algorithms in scikit-learn**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**scikit-learn中的特征选择算法**'
- en: You can find implementations of several different flavors of sequential feature
    selection related to the simple SBS that we implemented previously in the Python
    package `mlxtend` at [http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/).
    While our `mlxtend` implementation comes with many bells and whistles, we collaborated
    with the scikit-learn team to implement a simplified, user-friendly version, which
    has been part of the recent v0.24 release. The usage and behavior are very similar
    to the `SBS` code we implemented in this chapter. If you would like to learn more,
    please see the documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Python包`mlxtend`的[http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/)找到与我们之前实现的简单SBS相关的几种不同顺序特征选择的实现。虽然我们的`mlxtend`实现带有许多功能，但我们与scikit-learn团队合作实现了一个简化的、用户友好的版本，这已经成为最近v0.24版本的一部分。使用和行为与我们在本章实现的`SBS`代码非常相似。如果您想了解更多，请参阅[https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)的文档。
- en: There are many more feature selection algorithms available via scikit-learn.
    These include recursive backward elimination based on feature weights, tree-based
    methods to select features by importance, and univariate statistical tests. A
    comprehensive discussion of the different feature selection methods is beyond
    the scope of this book, but a good summary with illustrative examples can be found
    at [http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过scikit-learn提供的特征选择算法有很多选择。这些包括基于特征权重的递归向后消除、基于重要性选择特征的基于树的方法以及单变量统计测试。本书不涵盖所有特征选择方法的详细讨论，但可以在[http://scikit-learn.org/stable/modules/feature_selection.html](http://scikit-learn.org/stable/modules/feature_selection.html)找到一个具有说明性示例的良好总结。
- en: Assessing feature importance with random forests
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林评估特征重要性
- en: 'In previous sections, you learned how to use L1 regularization to zero out
    irrelevant features via logistic regression and how to use the SBS algorithm for
    feature selection and apply it to a KNN algorithm. Another useful approach for
    selecting relevant features from a dataset is using a **random forest**, an ensemble
    technique that was introduced in *Chapter 3*. Using a random forest, we can measure
    the feature importance as the averaged impurity decrease computed from all decision
    trees in the forest, without making any assumptions about whether our data is
    linearly separable or not. Conveniently, the random forest implementation in scikit-learn
    already collects the feature importance values for us so that we can access them
    via the `feature_importances_` attribute after fitting a `RandomForestClassifier`.
    By executing the following code, we will now train a forest of 500 trees on the
    Wine dataset and rank the 13 features by their respective importance measures—remember
    from our discussion in *Chapter 3*that we don’t need to use standardized or normalized
    features in tree-based models:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，您学习了如何使用L1正则化通过逻辑回归将无关特征置零，以及如何使用特征选择的SBS算法并将其应用于KNN算法。从数据集中选择相关特征的另一种有用方法是使用**随机森林**，这是一种在*第3章*中介绍的集成技术。使用随机森林，我们可以将特征重要性量化为从森林中所有决策树计算的平均不纯度减少，而不需要假设我们的数据是否可线性分离。方便的是，scikit-learn中的随机森林实现已经为我们收集了特征重要性值，因此我们可以在拟合`RandomForestClassifier`后通过`feature_importances_`属性访问它们。通过执行以下代码，我们现在将在Wine数据集上训练500棵树的森林，并根据它们各自的重要性测量排名13个特征——请记住，在*第3章*的讨论中，我们不需要在基于树的模型中使用标准化或归一化特征：
- en: '[PRE42]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After executing the code, we created a plot that ranks the different features
    in the Wine dataset by their relative importance; note that the feature importance
    values are normalized so that they sum up to 1.0:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，我们创建了一个图表，根据它们的相对重要性对Wine数据集中的不同特征进行了排序；请注意，特征重要性值已经标准化，使它们总和为1.0。
- en: '![](img/B17582_04_10.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_04_10.png)'
- en: 'Figure 4.10: Random forest-based feature importance of the Wine dataset'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：基于Wine数据集的基于随机森林的特征重要性
- en: We can conclude that the proline and flavonoid levels, the color intensity,
    the OD280/OD315 diffraction, and the alcohol concentration of wine are the most
    discriminative features in the dataset based on the average impurity decrease
    in the 500 decision trees. Interestingly, two of the top-ranked features in the
    plot are also in the three-feature subset selection from the SBS algorithm that
    we implemented in the previous section (alcohol concentration and OD280/OD315
    of diluted wines).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 根据500棵决策树中平均不纯度减少，我们可以得出，葡萄酒的脯氨酸和黄酮类水平、颜色强度、OD280/OD315波谱和酒精浓度是数据集中最具区分性的特征。有趣的是，绘图中排名前两位的特征也出现在我们在前一节实施的SBS算法的三特征子集选择中（酒精浓度和稀释葡萄酒的OD280/OD315）。
- en: However, as far as interpretability is concerned, the random forest technique
    comes with an important *gotcha* that is worth mentioning. If two or more features
    are highly correlated, one feature may be ranked very highly while the information
    on the other feature(s) may not be fully captured. On the other hand, we don’t
    need to be concerned about this problem if we are merely interested in the predictive
    performance of a model rather than the interpretation of feature importance values.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就可解释性而言，随机森林技术有一个值得注意的重要*坑*。如果两个或更多特征高度相关，一个特征可能排名很高，而另一个特征的信息可能未能完全捕获。另一方面，如果我们只关心模型的预测性能而不是特征重要性值的解释，那么我们就不需要担心这个问题。
- en: 'To conclude this section about feature importance values and random forests,
    it is worth mentioning that scikit-learn also implements a `SelectFromModel` object
    that selects features based on a user-specified threshold after model fitting,
    which is useful if we want to use the `RandomForestClassifier` as a feature selector
    and intermediate step in a scikit-learn `Pipeline` object, which allows us to
    connect different preprocessing steps with an estimator, as you will see in *Chapter
    6*, *Learning Best Practices for Model Evaluation and Hyperparameter Tuning*.
    For example, we could set the `threshold` to `0.1` to reduce the dataset to the
    five most important features using the following code:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 结束对特征重要性值和随机森林的讨论，值得一提的是，scikit-learn还实现了一个`SelectFromModel`对象，该对象在模型拟合后基于用户指定的阈值选择特征。如果我们希望将`RandomForestClassifier`作为特征选择器和scikit-learn
    `Pipeline`对象中的中间步骤，这将非常有用，您将在*第6章*中学到有关模型评估和超参数调整的最佳实践。例如，我们可以将`threshold`设置为`0.1`，使用以下代码将数据集减少到最重要的五个特征：
- en: '[PRE43]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by looking at useful techniques to make sure that we
    handle missing data correctly. Before we feed data to a machine learning algorithm,
    we also have to make sure that we encode categorical variables correctly, and
    in this chapter, we saw how we can map ordinal and nominal feature values to integer
    representations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过查看确保正确处理缺失数据的有用技术开始了本章。在将数据馈送到机器学习算法之前，我们还必须确保正确编码分类变量，本章中我们看到如何将有序和名义特征值映射为整数表示。
- en: Moreover, we briefly discussed L1 regularization, which can help us to avoid
    overfitting by reducing the complexity of a model. As an alternative approach
    to removing irrelevant features, we used a sequential feature selection algorithm
    to select meaningful features from a dataset.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们简要讨论了L1正则化，它可以通过减少模型的复杂性来帮助我们避免过拟合。作为移除不相关特征的替代方法，我们使用了顺序特征选择算法从数据集中选择有意义的特征。
- en: 'In the next chapter, you will learn about yet another useful approach to dimensionality
    reduction: feature extraction. It allows us to compress features onto a lower-dimensional
    subspace, rather than removing features entirely as in feature selection.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将了解到另一种有用的降维方法：特征提取。它允许我们将特征压缩到一个较低维度的子空间，而不是像特征选择那样完全删除特征。
- en: Join our book’s Discord space
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的Discord工作空间，与作者进行每月的*问我任何事*会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
