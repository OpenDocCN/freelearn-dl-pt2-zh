- en: 11 Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 深度强化学习
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在我们的书籍社区 Discord 上加入我们
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file90.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file90.png)'
- en: 'Machine learning is usually classified into three different paradigms: **supervised
    learning**, **unsupervised learning**, and **reinforcement learning** (**RL**).
    Supervised learning requires labeled data and has been the most popularly used
    machine learning paradigm so far. However, applications based on unsupervised
    learning, which does not require labels, have been steadily on the rise, especially
    in the form of generative models.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习通常分为三种不同的范式：**监督学习**、**无监督学习**和**强化学习**（**RL**）。监督学习需要标记数据，迄今为止一直是最受欢迎的机器学习范式。然而，基于无监督学习的应用，即不需要标签的学习，近年来稳步增长，尤其是生成模型的形式。
- en: An RL, on the other hand, is a different branch of machine learning that is
    considered to be the closest we have reached in terms of emulating how humans
    learn. It is an area of active research and development and is in its early stages,
    with some promising results. A prominent example is the famous AlphaGo model,
    built by Google's DeepMind, that defeated the world's best Go player.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，RL 是机器学习的一个不同分支，被认为是我们迄今为止模拟人类学习方式最接近的分支。这是一个积极研究和开发的领域，处于早期阶段，已取得一些有 promising
    的结果。一个著名的例子是由 Google 的 DeepMind 构建的 AlphaGo 模型，它击败了世界顶级围棋选手。
- en: In supervised learning, we usually feed the model with atomic input-output data
    pairs and hope for the model to learn the output as a function of the input. In
    RL, we are not keen on learning such individual input to individual output functions.
    Instead, we are interested in learning a strategy (or policy) that enables us
    to take a sequence of steps (or actions), starting from the input (state), in
    order to obtain the final output or achieve the final goal.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们通常向模型提供原子输入输出数据对，并希望模型学习将输出作为输入的函数。而在 RL 中，我们不关心学习这种单个输入到单个输出的函数。相反，我们感兴趣的是学习一种策略（或政策），使我们能够从输入（状态）开始采取一系列步骤（或行动），以获取最终输出或实现最终目标。
- en: Looking at a photo and deciding whether it's a cat or a dog is an atomic input-output
    learning task that can be solved through supervised learning. However, looking
    at a chess board and deciding the next move with the aim of winning the game requires
    strategy, and we need RL for complex tasks like these.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 查看照片并决定它是猫还是狗是一个原子输入输出学习任务，可以通过监督学习解决。然而，查看棋盘并决定下一步如何走以达到赢得比赛的目标则需要策略，我们需要 RL
    来处理这类复杂任务。
- en: In the previous chapters, we came across examples of supervised learning such
    as building a classifier to classify handwritten digits using the MNIST dataset.
    We also explored unsupervised learning while building a text generation model
    using an unlabeled text corpus.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们遇到了监督学习的例子，比如使用 MNIST 数据集构建分类器对手写数字进行分类。我们还在构建文本生成模型时探索了无监督学习，使用了一个无标签的文本语料库。
- en: In this chapter, we will uncover some of the basic concepts of RL and **deep
    reinforcement learning (DRL)**. We will then focus on a specific and popular type
    of DRL model – the **deep Q-learning** **Network** (**DQN**) model. Using PyTorch,
    we will build a DRL application. We will train a DQN model to learn how to play
    the game of Pong against a computer opponent (bot).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将揭示 RL 和**深度强化学习（DRL）**的一些基本概念。然后，我们将专注于一种特定且流行的 DRL 模型 - **深度 Q-learning
    网络（DQN）**模型。使用 PyTorch，我们将构建一个 DRL 应用程序。我们将训练一个 DQN 模型来学习如何与计算机对手（bot）玩乒乓球游戏。
- en: By the end of this chapter, you will have all the necessary context to start
    working on your own DRL project in PyTorch. Additionally, you will have hands-on
    experience of building a DQN model for a real-life problem. The skills you'll
    have gained in this chapter will be useful for working on other such RL problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将具备开始在 PyTorch 上进行自己的 DRL 项目所需的所有背景知识。此外，您还将亲自体验在真实问题上构建 DQN 模型的经验。本章中您将获得的技能对处理其他
    RL 问题也将非常有用。
- en: 'This chapter is broken down into the following topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容分为以下几个主题：
- en: Reviewing reinforcement learning concepts
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾强化学习概念
- en: Discussing Q-learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论 Q-learning
- en: Understanding deep Q-learning
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度 Q-learning
- en: Building a DQN model in PyTorch
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中构建 DQN 模型
- en: Reviewing reinforcement learning concepts
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾强化学习概念
- en: 'In a way, RL can be defined as learning from mistakes. Instead of getting the
    feedback for every data instance, as is the case with supervised learning, the
    feedback is received after a sequence of actions. The following diagram shows
    the high-level schematic of an RL system:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，RL可以被定义为从错误中学习。与监督学习中每个数据实例都得到反馈的情况不同，RL在一系列行动之后接收反馈。以下图表显示了RL系统的高级示意图：
- en: '![Figure 11\. 1 – Reinforcement learning schematic](img/file91.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 1 – 强化学习示意图](img/file91.jpg)'
- en: Figure 11\. 1 – Reinforcement learning schematic
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 1 – 强化学习示意图
- en: In an RL setting, we usually have an **agent**, which does the learning. The
    agent learns to make decisions and take **actions** according to these decisions.
    The agent operates within a provided **environment**. This environment can be
    thought of as a confined world where the agent lives, takes actions, and learns
    from its actions. An action here is simply the implementation of the decision
    the agent makes based on what it has learned.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL设置中，通常有一个**代理**进行学习。代理学习如何根据这些决策做出决定并采取**行动**。代理操作在一个提供的**环境**中。这个环境可以被视为一个有限的世界，在这个世界中，代理生活、采取行动并从其行动中学习。在这里，行动就是代理基于其所学内容做出决策的实施。
- en: We mentioned earlier that unlike supervised learning, RL does not have an output
    for each and every input; that is, the agent does not necessarily receive a feedback
    for each and every action. Instead, the agent works in **states**. Suppose it
    starts at an initial state, *S*0\. It then takes an action, say *a*0\. This action
    transitions the state of the agent from *S*0 to *S*1, after which the agent takes
    another action, *a*1, and the cycle goes on.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们前面提到，与监督学习不同，RL对于每个输入并不都有一个输出；也就是说，代理不一定会为每个动作都接收到反馈。相反，代理在**状态**中工作。假设它从初始状态*S*0开始。然后它执行一个动作，比如*a*0。这个动作将代理的状态从*S*0转换到*S*1，之后代理执行另一个动作*a*1，循环进行。
- en: Occasionally, the agent receives **rewards** based on its state. The sequence
    of states and actions that the agent traverses is also known as a **trajectory**.
    Let's say the agent received a reward at state *S*2\. In that case, the trajectory
    that resulted in this reward would be *S*0, *a*0, *S*1, *a*1, *S*2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔，代理根据其状态接收**奖励**。代理遍历的状态和行动序列也称为**轨迹**。假设代理在状态*S*2收到奖励。在这种情况下，导致该奖励的轨迹将是*S*0,
    *a*0, *S*1, *a*1, *S*2。
- en: Note
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注
- en: ''
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The rewards could either be positive or negative.
  id: totrans-25
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 奖励可以是正的也可以是负的。
- en: Based on the rewards, the agent learns to adjust its behavior so that it takes
    actions in a way that maximizes the long-term rewards. This is the essence of
    RL. The agent learns a strategy regarding how to act optimally (that is, to maximize
    the reward) based on the given state and reward.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于奖励，代理学习调整其行为，以便以最大化长期奖励的方式采取行动。这就是RL的本质。代理根据给定的状态和奖励学习一种如何行动最优化的策略。
- en: This learned strategy, which is basically actions expressed as a function of
    states and rewards, is called the **policy** of the agent. The ultimate goal of
    RL is to compute a policy that enables the agent to always receive the maximum
    reward from the given situation the agent is placed in.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习到的策略，基本上是行动作为状态和奖励的函数表达，被称为代理的**策略**。RL的最终目标是计算一个策略，使代理能够始终从其所处的情况中获得最大奖励。
- en: 'Video games are one of the best examples to demonstrate RL. Let''s use the
    video game Pong as an example, which is a virtual version of table tennis. The
    following is a snapshot of this game:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 视频游戏是展示RL的最佳例子之一。让我们以视频游戏《乒乓球》的虚拟版本——Pong为例。以下是该游戏的快照：
- en: '![Figure 11\. 2 – Pong video game](img/file92.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 2 – 乒乓视频游戏](img/file92.jpg)'
- en: Figure 11\. 2 – Pong video game
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 2 – 乒乓视频游戏
- en: Consider that the player to the right is the agent, which is represented by
    a short vertical line. Notice that there is a well-defined environment here. The
    environment consists of the playing area, which is denoted by the brown pixels.
    The environment also consists of a ball, which is denoted by a white pixel. As
    well as this, the environment consists of the boundaries of the playing area,
    denoted by the gray stripes and edges that the ball may bounce off. Finally, and
    most importantly, the environment includes an opponent, which looks like the agent
    but is placed on the left-hand side, opposite the agent.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑右侧的玩家是代理，用一个短竖线表示。请注意，这里有一个明确定义的环境。环境包括玩区，用棕色像素表示。环境还包括一个球，用白色像素表示。除此之外，环境还包括玩区边界，用灰色条纹和球可能反弹的边缘表示。最后，而且最重要的是，环境包括一个对手，看起来像代理，但位于左侧，与代理相对。
- en: Usually, in an RL setting, the agent at any given state has a finite set of
    possible actions, referred to as a discrete action space (as opposed to a continuous
    action space). In this example, the agent has two possible actions at all states
    – move up or move down, but with two exceptions. First, it can only move down
    when it is at the top-most position (state), and second, it can only move up when
    it is at the bottom-most position (state).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在强化学习设置中，代理在任何给定状态下有一组有限的可能动作，称为离散动作空间（与连续动作空间相对）。在这个例子中，代理在所有状态下有两种可能的动作
    - 向上移动或向下移动，但有两个例外。首先，它只能在处于最上位置（状态）时向下移动，其次，它只能在处于最下位置（状态）时向上移动。
- en: The concept of reward in this case can be directly mapped to what happens in
    an actual table tennis game. If you miss the ball, your opponent gains a point.
    Whoever scores 21 points first wins the game and receives a positive reward. Losing
    a game means negative rewards. Scoring a point or losing a point also results
    in smaller intermediate positive and negative rewards, respectively. A sequence
    of play starting from score 0-0 and leading to either of the players scoring 21
    points is called an **episode**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，奖励的概念可以直接映射到实际乒乓球比赛中发生的情况。如果你未能击中球，你的对手会得分。首先得到21分的人赢得比赛并获得正奖励。输掉比赛意味着负奖励。得分或失分也会导致较小的中间正奖励和负奖励。从0-0的得分到任一玩家得分21分的玩法序列被称为一个**episode**。
- en: Training our agent for a Pong game using RL is equivalent to training someone
    to play table tennis from scratch. Training results in a policy that the agent
    follows while playing the game. In any given situation – which includes the position
    of the ball, the position of the opponent, the scoreboard, as well as the previous
    reward – a successfully trained agent moves up or down to maximize its chances
    of winning the game.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强化学习训练我们的代理玩乒乓球游戏相当于从头开始训练某人打乒乓球。训练会产生一个政策，代理在游戏中遵循这个政策。在任何给定的情况下 - 包括球的位置、对手的位置、记分牌以及先前的奖励
    - 训练良好的代理会向上或向下移动，以最大化其赢得比赛的机会。
- en: So far, we have discussed the basic concepts behind RL by providing an example.
    In doing so, we have repeatedly mentioned terms such as strategy, policy, and
    learning. But how does the agent actually learn the policy? The answer is through
    an RL model, which works based on a pre-defined algorithm. Next, we will explore
    the different kinds of RL algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已通过提供一个例子来讨论强化学习背后的基本概念。在这样做的过程中，我们反复提到了策略、政策和学习等术语。但是代理实际上是如何学习策略的呢？答案是通过一个基于预定义算法的强化学习模型。接下来，我们将探讨不同类型的强化学习算法。
- en: Types of reinforcement learning algorithms
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习算法的类型
- en: 'In this section, we will look at the types of RL algorithms, as per the literature.
    We will then explore some of the subtypes within these types. Broadly speaking,
    RL algorithms can be categorized as either of the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将按照文献中的分类来查看强化学习算法的类型。然后我们将探索这些类型中的一些子类型。广义上讲，强化学习算法可以分为以下两种类型之一：
- en: '**Model-based**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型**'
- en: '**Model-free**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无模型**'
- en: Let's look at these one by one.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一看看这些。
- en: Model-based
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于模型
- en: As the name suggests, in model-based algorithms, the agent knows about the model
    of the environment. The model here refers to the mathematical formulation of a
    function that can be used to estimate rewards and how the states transition within
    the environment. Because the agent has some idea about the environment, it helps
    reduce the sample space to choose the next action from. This helps with the efficiency
    of the learning process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名称所示，在基于模型的算法中，智能体了解环境的模型。这里的模型指的是一个数学公式，可用于估计奖励以及环境中状态的转移方式。因为智能体对环境有一定的了解，这有助于减少选择下一步行动的样本空间。这有助于学习过程的效率。
- en: 'However, in reality, a modeled environment is not directly available most of
    the time. If we, nonetheless, want to use the model-based approach, we need to
    have the agent learn the environment model with its own experience. In such cases,
    the agent is highly likely to learn a biased representation of the model and perform
    poorly in the real environment. For this reason, model-based approaches are less
    frequently used for implementing RL systems. We will not be discussing models
    based on this approach in detail in this book, but here are some examples:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在现实中，建模环境大多数情况下并不直接可用。尽管如此，如果我们想使用基于模型的方法，我们需要让智能体通过自身经验学习环境模型。在这种情况下，智能体很可能会学习到模型的偏见表达，并在真实环境中表现不佳。因此，基于模型的方法在实施强化学习系统时使用较少。在本书中，我们将不详细讨论基于这种方法的模型，但这里有一些示例：
- en: '**Model-Based DRL with Model-Free Fine-Tuning** (**MBMF**).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的深度强化学习与无模型微调**（**MBMF**）。'
- en: '**Model-Based Value Estimation** (**MBVE**) for efficient Model-Free RL.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的价值估计**（**MBVE**）用于高效的无模型强化学习。'
- en: '**Imagination-Augmented Agents** (**I2A**) for DRL.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**想象增强智能体**（**I2A**）用于深度强化学习。'
- en: '**AlphaZero**, the famous AI bot that defeated Chess and Go champions.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaZero**，这位著名的AI机器人击败了国际象棋和围棋冠军。'
- en: Now, let's look at the other set of RL algorithms that work with a different
    philosophy.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一组采用不同哲学的强化学习算法。
- en: Model-free
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无模型
- en: 'The model-free approach works without any model of the environment and is currently
    more popularly used for RL research and development. There are primarily two ways
    of training the agent in a model-free RL setting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 无模型方法在没有环境模型的情况下运作，目前在强化学习研究与开发中更为流行。在无模型强化学习设置中，主要有两种训练智能体的方法：
- en: '**Policy optimization**'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**政策优化**'
- en: '**Q-learning**'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q学习**'
- en: Policy optimization
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 政策优化
- en: 'In this method, we formulate the policy in the form of a function of an action,
    given the current state, as demonstrated in the following equation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们将政策制定为一个关于行动的函数形式，给定当前状态，如以下方程所示：
- en: '![– Equation 11.1](img/file93.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![– 方程式 11.1](img/file93.png)'
- en: – Equation 11.1
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: – 方程式 11.1
- en: Here, *β* represents the internal parameters of this function, which is updated
    to optimize the policy function via gradient ascent. The objective function is
    defined using the policy function and the rewards. An approximation of the objective
    function may also be used in some cases for the optimization process. Furthermore,
    in some cases, an approximation of the policy function could be used instead of
    the actual policy function for the optimization process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*β*代表这个函数的内部参数，通过梯度上升更新以优化政策函数。目标函数使用政策函数和奖励定义。在某些情况下，可能会使用目标函数的近似来进行优化过程。此外，在某些情况下，可能会使用政策函数的近似来代替实际的政策函数进行优化过程。
- en: 'Usually, the optimizations that are performed under this approach are **on-policy**,
    which means that the parameters are updated based on the data gathered using the
    latest policy version. Some examples of policy optimization-based RL algorithms
    are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在这种方法下进行的优化是**在政策内**的，这意味着参数是基于使用最新政策版本收集的数据进行更新的。一些基于政策优化的强化学习算法的示例如下：
- en: '**Policy gradient**: This is the most basic policy optimization method where
    we directly optimize the policy function using gradient ascent. The policy function
    outputs the probabilities of different actions to be taken next, at each time
    step.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**政策梯度**：这是最基本的政策优化方法，我们直接使用梯度上升优化政策函数。政策函数输出在每个时间步骤下采取不同行动的概率。'
- en: '**Actor-critic**: Because of the on-policy nature of optimization under the
    policy gradient algorithm, every iteration of the algorithm needs the policy to
    be updated. This takes a lot of time. The actor-critic method introduces the use
    of a value function, as well as a policy function. The actor models the policy
    function and the critic models the value function.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员-批评家**：由于在政策梯度算法下的优化是基于政策的，算法的每次迭代都需要更新政策。这需要很长时间。演员-批评家方法引入了值函数和政策函数的使用。演员模拟政策函数，批评家模拟值函数。'
- en: By using a critic, the policy update process becomes faster. We will discuss
    the value function in more detail in the next section. However, we will not go
    into the mathematical details of the actor-critic method in this book.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用批评者，策略更新过程变得更快。我们将在下一节更详细地讨论价值函数。然而，本书不会深入讨论演员-批评家方法的数学细节。
- en: '**Trust region policy optimization** (**TRPO**): Like the policy gradient method,
    TRPO consists of an on-policy optimization approach. In the policy-gradient approach,
    we use the gradient for updating the policy function parameters, *β*. Since the
    gradient is a first-order derivative, it can be noisy for sharp curvatures in
    the function. This may lead us to making large policy changes that may destabilize
    the learning trajectory of the agent.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任区域策略优化** (**TRPO**): 类似于策略梯度方法，TRPO包含一个基于政策的优化方法。在策略梯度方法中，我们使用梯度来更新政策函数参数
    *β*。由于梯度是一阶导数，对于函数中的尖锐曲率可能会产生噪声。这可能导致我们进行大幅度的政策更改，从而可能不稳定代理的学习轨迹。'
- en: To avoid that, TRPO proposes a trust region. It defines an upper limit on how
    much the policy may change in a given update step. This ensures the stability
    of the optimization process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，TRPO提出了信任区域。它定义了政策在给定更新步骤中可以改变的上限。这确保了优化过程的稳定性。
- en: '**Proximal policy optimization** (**PPO**): Similar to TRPO, PPO aims to stabilize
    the optimization process. During gradient ascent, an update is performed per data
    sample in the policy gradient approach. PPO, however, uses a surrogate objective
    function, which facilitates updates over batches of data samples. This results
    in estimating gradients more conservatively, thereby improving the chances of
    the gradient ascent algorithm converging.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**近端策略优化** (**PPO**): 类似于TRPO，PPO旨在稳定优化过程。在策略梯度方法中，每个数据样本都会进行梯度上升更新。然而，PPO使用了一个替代的目标函数，可以在数据样本批次上进行更新。这导致更加保守地估计梯度，从而提高了梯度上升算法收敛的可能性。'
- en: Policy optimization functions directly work on optimizing the policy and hence
    are extremely intuitive algorithms. However, due to the on-policy nature of most
    of these algorithms, data needs to be resampled at each step after the policy
    is updated. This can be a limiting factor in solving RL problems. Next, we will
    discuss the other kind of model-free algorithm that is more sample-efficient,
    known as Q-learning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 策略优化函数直接工作于优化策略，因此这些算法非常直观。然而，由于这些算法大多是基于政策的，每次更新政策后都需要重新对数据进行采样。这可能成为解决RL问题的限制因素。接下来，我们将讨论另一种更加样本高效的无模型算法，称为Q学习。
- en: Q-learning
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q学习
- en: Contrary to policy optimization algorithms, **Q-learning** relies on a value
    function instead of a policy function. From here on, this chapter will focus on
    Q-learning. We will explore the fundamentals of Q-learning in detail in the next
    section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与策略优化算法相反，**Q学习**依赖于值函数而不是策略函数。从这一点开始，本章将重点讨论Q学习。我们将在下一节详细探讨Q学习的基础知识。
- en: Discussing Q-learning
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论Q学习
- en: The key difference between policy optimization and Q-learning is the fact that
    in the latter, we are not directly optimizing the policy. Instead, we optimize
    a value function. What is a **value function**? We have already learned that RL
    is all about an agent learning to gain the maximum overall rewards while traversing
    a trajectory of states and actions. A value function is a function of a given
    state the agent is currently at, and this function outputs the expected sum of
    rewards the agent will receive by the end of the current episode.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 策略优化和Q学习之间的关键区别在于，后者并没有直接优化策略。相反，我们优化一个值函数。什么是**值函数**？我们已经学到RL的关键在于代理学习如何在经过一系列状态和动作的轨迹时获得最大的总奖励。值函数是一个关于当前代理所处状态的函数，其输出为代理在当前回合结束时将获得的预期奖励总和。
- en: In Q-learning, we optimize a specific type of value function, known as the **action-value
    function**, which depends on both the current state and the action. At a given
    state, *S*, the action-value function determines the long-term rewards (rewards
    until the end of the episode) the agent will receive for taking action *a*. This
    function is usually expressed as *Q(S, a)*, and hence is also called the Q-function.
    The action-value is also referred to as the **Q-value**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q-learning 中，我们优化一种特定类型的值函数，称为**动作值函数**，它取决于当前状态和动作。在给定状态 *S*，动作值函数确定代理程序将为采取动作
    *a* 而获得的长期奖励（直到结束的奖励）。此函数通常表示为 *Q(S, a)*，因此也称为 Q 函数。动作值也称为**Q 值**。
- en: 'The Q-values for every (state, action) pair can be stored in a table where
    the two dimensions are state and action. For example, if there are four possible
    states, *S*1, *S*2, *S*3, and *S*4, and two possible actions, *a*1 and *a*2, then
    the eight Q-values will be stored in a 4x2 table. The goal of Q-learning, therefore,
    is to create this table of Q-values. Once the table is available, the agent can
    look up the Q-values for all possible actions from the given state and take the
    action with the maximum Q-value. However, the question is, where do we get the
    Q-values from? The answer lies in the **Bellman equation**, which is mathematically
    expressed as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每个（状态，动作）对的 Q 值可以存储在一个表中，其中两个维度分别是状态和动作。例如，如果有四个可能的状态 *S*1、*S*2、*S*3 和 *S*4，并且有两种可能的动作
    *a*1 和 *a*2，那么这八个 Q 值将存储在一个 4x2 的表中。因此，Q-learning 的目标是创建这个 Q 值表。一旦表格可用，代理程序可以查找给定状态的所有可能动作的
    Q 值，并采取具有最大 Q 值的动作。但问题是，我们从哪里获取 Q 值？答案在于**贝尔曼方程**，其数学表达如下：
- en: '![– Equation 11.2](img/file94.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![– 方程 11.2](img/file94.png)'
- en: – Equation 11.2
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: – 方程 11.2
- en: The Bellman equation is a recursive way of calculating Q-values. *R* in this
    equation is the reward received by taking action *a*t at state *S*t, while γ (gamma)
    is the **discount factor**, which is a scalar value between *0* and *1*. Basically,
    this equation states that the Q-value for the current state, *S*t, and action,
    *a*t, is equal to the reward, *R*, received by taking action *a*t at state *S*t,
    plus the Q-value resulting from the most optimal action, *a*t*+1*, taken from
    the next state, *S*t*+1*, multiplied by a discount factor. The discount factor
    defines how much weightage is to be given to the immediate reward versus the long-term
    future rewards.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是计算 Q 值的递归方式。在此方程中，*R* 是在状态 *S*t 采取动作 *a*t 后获得的奖励，而 γ（gamma）是**折扣因子**，它是一个介于
    *0* 和 *1* 之间的标量值。基本上，这个方程表明当前状态 *S*t 和动作 *a*t 的 Q 值等于在状态 *S*t 采取动作 *a*t 后获得的奖励
    *R*，加上从下一个状态 *S*t*+1 采取的最优动作 *a*t*+1 的 Q 值，乘以折扣因子。折扣因子定义了在即时奖励与长期未来奖励之间给予多大的权重。
- en: 'Now that we have defined most of the underlying concepts of Q-learning, let''s
    walk through an example to demonstrate how Q-learning exactly works. The following
    diagram shows an environment that consists of five possible states:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了 Q-learning 的大部分基础概念，让我们通过一个示例来演示 Q-learning 的工作原理。以下图示展示了一个包含五个可能状态的环境：
- en: '![Figure 11\. 3 – Q-learning example environment](img/file95.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 3 – Q-learning 示例环境](img/file95.jpg)'
- en: Figure 11\. 3 – Q-learning example environment
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 3 – Q-learning 示例环境
- en: 'There are two different possible actions – moving up (**a**1) or down (**a**2).
    There are different rewards at different states ranging from **+2** at state **S**4
    to **-1** at state **S**0\. Every episode in this environment starts from state
    **S**2 and ends at either **S**0 or **S**4\. Because there are five states and
    two possible actions, the Q-values can be stored in a 5x2 table. The following
    code snippet shows how rewards and Q-values can be written in Python:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种不同的可能动作 – 向上移动（**a**1）或向下移动（**a**2）。在不同状态下有不同的奖励，从状态 **S**4 的 **+2** 到状态
    **S**0 的 **-1**。每个环境中的一轮从状态 **S**2 开始，并以 **S**0 或 **S**4 结束。由于有五个状态和两种可能的动作，Q
    值可以存储在一个 5x2 的表中。以下代码片段展示了如何在 Python 中编写奖励和 Q 值：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We initialize all the Q-values to zero. Also, because there are two specific
    end states, we need to specify those in the form of a list, as shown here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有的 Q 值初始化为零。此外，由于有两个特定的结束状态，我们需要以列表的形式指定这些状态，如下所示：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This basically indicates that states **S**0 and **S**4 are end states. There
    is one final piece we need to look at before we can run the complete Q-learning
    loop. At each step of Q-learning, the agent has two options with regards to taking
    the next action:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上表明状态 **S**0 和 **S**4 是终止状态。在运行完整的 Q 学习循环之前，我们还需要查看一个最后的部分。在 Q 学习的每一步，代理有两种选择下一步行动的选项：
- en: Take the action that has the highest Q-value.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有最高 Q 值的动作。
- en: Randomly choose the next action.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择下一个动作。
- en: Why would the agent choose an action randomly?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么代理程序会随机选择一个动作？
- en: Remember that in *Chapter 7* *, Music and Text Generation with PyTorch*, in
    the *Text generation* section, we discussed how greedy search or beam search results
    in repetitive results, and hence introducing randomness helps in producing better
    results. Similarly , if the agent always chooses the next action based on Q-values,
    then it might get stuck choosing an action repeatedly that gives an immediate
    high reward in the short term. Hence, taking actions randomly once in a while
    will help the agent get out of such sub-optimal conditions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在*第 7 章*，使用 PyTorch 进行音乐和文本生成中，*文本生成*部分，我们讨论了贪婪搜索或波束搜索导致重复结果的问题，因此引入随机性有助于产生更好的结果。同样地，如果代理程序总是基于
    Q 值选择下一步动作，那么它可能会陷入重复选择立即高奖励的动作的子优化条件。因此，偶尔随机采取行动将有助于代理程序摆脱这种次优条件。
- en: 'Now that we''ve established that the agent has two possible ways of taking
    an action at each step, we need to decide which way the agent goes. This is where
    the **epsilon-greedy-action** mechanism comes into play. The following diagram
    shows how it works:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定代理在每一步都有两种可能的行动方式，我们需要决定代理选择哪种方式。这就是**epsilon-greedy-action**机制发挥作用的地方。下图展示了它的工作原理：
- en: '![Figure 11\. 4 – Epsilon-greedy-action mechanism](img/file96.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 4 – Epsilon-greedy-action 机制](img/file96.jpg)'
- en: Figure 11\. 4 – Epsilon-greedy-action mechanism
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 4 – Epsilon-greedy-action 机制
- en: 'Under this mechanism, at each episode, an epsilon value is pre-decided, which
    is a scalar value between `0` and `1`. In a given episode, for taking each next
    action, the agent generates a random number between `0` to `1`. If the generated
    number is less than the pre-defined epsilon value, the agent chooses the next
    action randomly from the available set of next actions. Otherwise, the Q-values
    for each of the next possible actions are retrieved from the Q-value table, and
    the action with the highest Q-value is chosen. The Python code for the epsilon-greedy-action
    mechanism is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在此机制下，每个周期中预先决定一个 epsilon 值，它是一个介于 `0` 和 `1` 之间的标量值。在给定的周期内，对于每次选择下一个动作，代理生成一个介于
    `0` 到 `1` 之间的随机数。如果生成的数字小于预定义的 epsilon 值，则代理随机从可用的下一个动作集中选择下一个动作。否则，从 Q 值表中检索每个可能的下一个动作的
    Q 值，并选择具有最高 Q 值的动作。epsilon-greedy-action 机制的 Python 代码如下：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Typically, we start with an epsilon value of `1` at the first episode and then
    linearly decrease it as the episodes progress. The idea here is that we want the
    agent to explore different options initially. However, as the learning process
    progresses, the agent is less susceptible to getting stuck collecting short-term
    rewards and hence it can better exploit the Q-values table.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们在第一个周期以 `1` 的 epsilon 值开始，然后随着周期的进展线性减少它。这里的想法是，我们希望代理程序最初探索不同的选项。然而，随着学习过程的进行，代理程序对收集短期奖励不那么敏感，因此它可以更好地利用
    Q 值表。
- en: 'We are now in a position to write the Python code for the main Q-learning loop,
    which will look as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写主要的 Q 学习循环的 Python 代码，如下所示：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: First, we define that the agent shall be trained for `100` episodes. We begin
    with an epsilon value of `1` and we define the discounting factor (gamma) as `0.9`.
    Next, we run the Q-learning loop, which loops over the number of episodes. In
    each iteration of this loop, we run through an entire episode. Within the episode,
    we first initialize the state of the agent to `S2`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们确定代理程序将被训练 `100` 个周期。我们从 epsilon 值为 `1` 开始，并定义折扣因子（gamma）为 `0.9`。接下来，我们运行
    Q 学习循环，该循环遍历周期数。在此循环的每次迭代中，我们通过整个周期运行。在周期内，我们首先将代理的状态初始化为 `S2`。
- en: 'Thereon, we run another internal loop, which only breaks if the agent reaches
    an end state. Within this internal loop, we decide on the next action for the
    agent using the epsilon-greedy-action mechanism. The agent then takes the action,
    which transitions the agent to a new state and may possibly yield a reward. The
    implementation for the `take_action` function is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们运行另一个内部循环，仅在代理达到结束状态时中断。在这个内部循环中，我们使用ε-贪婪动作机制为代理决定下一步动作。代理然后执行该动作，转移代理到一个新状态，并可能获得奖励。`take_action`函数的实现如下：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once we obtain the reward and the next state, we update the Q-value for the
    current state-action pair using equation 11.2 . The next state now becomes the
    current state and the process repeats. At the end of each episode, the epsilon
    value is reduced linearly. Once the entire Q-learning loop is over, we obtain
    a Q-values table. This table is essentially all that the agent needs to operate
    in this environment in order to gain the maximum long-term rewards.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得奖励和下一个状态，我们使用方程11.2更新当前状态-动作对的Q值。下一个状态现在成为当前状态，过程重复进行。在每个episode结束时，ε值线性减小。一旦整个Q学习循环结束，我们获得一个Q值表。这个表基本上是代理在这个环境中操作所需的一切，以获得最大的长期奖励。
- en: Ideally, a well-trained agent for this example would always move downward to
    receive the maximum reward of *+2* at *S*4, and would avoid going toward *S*0,
    which contains a negative reward of *-1*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，针对这个示例训练良好的代理总是向下移动，以获得*S*4处的最大奖励*+2*，并避免向*S*0移动，该位置含有*-1*的负奖励。
- en: This completes our discussion on Q-learning. The preceding code should help
    you get started with Q-learning in simple environments such as the one provided
    here. For more complex and realistic environments, such as video games, this approach
    will not work. Why?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们关于Q学习的讨论。前面的代码应该帮助您在提供的简单环境中开始使用Q学习。对于视频游戏等更复杂和现实的环境，这种方法将不起作用。为什么呢？
- en: We have noticed that the essence of Q-learning lies in creating the Q-values
    table. In our example, we only had 5 states and 2 actions, and therefore the table
    was of size 10, which is manageable. But in video games such as Pong, there are
    far too many possible states. This explodes the Q-values table's size, which makes
    our Q-learning algorithm extremely memory intensive and impractical to run.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，Q学习的本质在于创建Q值表。在我们的示例中，只有5个状态和2个动作，因此表的大小为10，这是可以管理的。但是在如Pong等视频游戏中，可能的状态太多了。这导致Q值表的大小爆炸增长，使得我们的Q学习算法极其占用内存且不可实际运行。
- en: Thankfully, there is a solution where we can still use the concept of Q-learning
    without having our machines run out of memory. This solution combines the worlds
    of Q-learning and deep neural networks and provides the extremely popular RL algorithm
    known as **DQN**. In the next section, we will discuss the basics of DQN and some
    of its novel characteristics.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一个解决方案，可以在不使我们的机器内存不足的情况下仍然使用Q学习的概念。这个解决方案将Q学习和深度神经网络的世界结合起来，提供了极其流行的RL算法，被称为**DQN**。在下一节中，我们将讨论DQN的基础知识和一些其新颖的特性。
- en: Understanding deep Q-learning
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解深度Q学习
- en: Instead of creating a Q-values table, **DQN** uses a **deep neural network**
    (**DNN**) that outputs a Q-value for a given state-action pair. DQN is used with
    complex environments such as video games, where there are far too many states
    for them to be managed in a Q-values table. The current image frame of the video
    game is used to represent the current state and is fed as input to the underlying
    DNN model, together with the current action.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**DQN**不创建一个Q值表，而是使用一个**深度神经网络**（**DNN**），该网络为给定的状态-动作对输出一个Q值。DQN在诸如视频游戏之类的复杂环境中使用，这些环境中的状态太多，无法在Q值表中管理。视频游戏的当前图像帧用来表示当前状态，并与当前动作一起作为输入传递给底层DNN模型。'
- en: The DNN outputs a scalar Q-value for each such input. In practice, instead of
    just passing the current image frame, *N* number of neighboring image frames in
    a given time window are passed as input to the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: DNN为每个这样的输入输出一个标量Q值。在实践中，与其仅传递当前图像帧不如将给定时间窗口内的*N*个相邻图像帧作为输入传递给模型。
- en: We are using a DNN to solve an RL problem. This has an inherent concern. While
    working with DNNs, we have always worked with **independent and identically distributed**
    (**iid**) data samples. However, in RL, every current output impacts the next
    input. For example, in the case of Q-learning, the Bellman equation itself suggests
    that the Q-value is dependent on another Q-value; that is, the Q-value of the
    next state-action pair impacts the Q-value of the current-state pair.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用一个深度神经网络来解决强化学习（RL）问题。这引发了一个固有的问题。在使用深度神经网络时，我们始终使用**独立同分布**（iid）的数据样本。然而，在强化学习中，每一个当前输出都会影响到下一个输入。例如，在Q-learning中，贝尔曼方程本身表明，Q值依赖于另一个Q值；也就是说，下一个状态-动作对的Q值影响了当前状态-动作对的Q值。
- en: 'This implies that we are working with a constantly moving target and there
    is a high correlation between the target and the input. DQN addresses these issues
    with two novel features:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在处理一个不断移动的目标，并且目标与输入之间有很高的相关性。DQN通过两个新特性来解决这些问题：
- en: Using two separate DNNs
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个单独的深度神经网络（DNNs）
- en: Experience replay buffer
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验重放缓冲区
- en: Let's look at these in more detail.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下这些内容。
- en: Using two separate DNNs
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用两个单独的深度神经网络（DNNs）
- en: 'Let''s rewrite the Bellman equation for DQNs:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新写DQNs的贝尔曼方程：
- en: '![– Equation 11.3](img/file97.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![– 方程 11.3](img/file97.jpg)'
- en: – Equation 11.3
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: – 方程 11.3
- en: This equation is mostly the same as for Q-learning except for the introduction
    of a new term,![img](img/file98.png)(theta).![img](img/file99.png)represents the
    weights of the DNN that the DQN model uses to get Q-values. But something is odd
    with this equation. Notice that![img](img/file100.png)is placed on both the left
    hand-side and the right-hand side of the equation. This means that at every step,
    we are using the same neural network for getting the Q-values of the current state-action,
    pair as well as the next state-action pair. This means that we are chasing a non-stationary
    target because every step,![img](img/file101.png), will be updated, which will
    change both the left-hand side as well as the right-hand side of the equation
    for the next step, causing instability in the learning process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程大部分与Q-learning的方程相同，只是引入了一个新术语，![img](img/file98.png)(θ)。![img](img/file99.png)代表了DQN模型用于获取Q值的DNN的权重。但是这个方程有些奇怪。注意到![img](img/file100.png)被放在了方程的左边和右边。这意味着在每一步中，我们使用同一个神经网络来获取当前状态-动作对和下一个状态-动作对的Q值。这意味着我们在追踪一个非静态目标，因为每一步，![img](img/file101.png)都会被更新，这将改变下一步的方程的左边和右边，导致学习过程中的不稳定性。
- en: 'This can be more clearly seen by looking at the loss function, which the DNN
    will be trying to minimize using gradient descent. The loss function is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看损失函数，可以更清楚地看到这一点。神经网络将试图使用梯度下降来最小化损失函数。损失函数如下：
- en: '![– Equation 11.4](img/file102.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![– 方程 11.4](img/file102.png)'
- en: – Equation 11.4
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: – 方程 11.4
- en: Keeping *R* (reward) aside for a moment, having the exact same network producing
    Q-values for current and next state-action pairs will lead to volatility in the
    loss function as both terms will be constantly changing. To address this issue,
    DQN uses two separate networks – a main DNN and a target DNN. Both DNNs have the
    exact same architecture.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 暂且将*R*（奖励）放在一边，对于同一个网络生成当前和下一个状态-动作对的Q值将导致损失函数的波动性增加。为了解决这个问题，DQN使用两个独立的网络——一个主DNN和一个目标DNN。两个DNN具有完全相同的架构。
- en: The main DNN is used for computing the Q-values of the current state-action
    pair, while the target DNN is used for computing the Q-values of the next (or
    target) state-action pair. However, although the weights of the main DNN are updated
    at every learning step, the weights of the target DNN are frozen. After every
    *K* gradient descent iterations, the weights of the main network are copied to
    the target network. This mechanism keeps the training procedure relatively stable.
    The weights-copying mechanism ensures accurate predictions from the target network.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的DNN用于计算当前状态-动作对的Q值，而目标DNN用于计算下一个（或目标）状态-动作对的Q值。然而，虽然主网络的权重在每一次学习步骤中都会更新，目标网络的权重却是冻结的。每经过*K*次梯度下降迭代，主网络的权重被复制到目标网络。这种机制保持了训练过程的相对稳定性。权重复制机制确保了来自目标网络的准确预测。
- en: Experience replay buffer
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 经验重放缓冲区
- en: Because the DNN expects iid data as input, we simply cache the last *X* number
    of steps (frames of the video game) into a buffer memory and then randomly sample
    batches of data from the buffer. These batches are then fed as inputs to the DNN.
    Because the batches consist of randomly sampled data, the distribution looks similar
    to that of iid data samples. This helps stabilize the DNN training process.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因为DNN期望的输入是iid数据，我们只需将视频游戏的最后*X*个步骤（帧）缓存在一个缓冲区内，然后从缓冲区中随机抽样数据批次。这些批次然后作为DNN的输入。因为批次由随机抽样的数据组成，其分布看起来类似于iid数据样本的分布。这有助于稳定DNN训练过程。
- en: Note
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Without the buffer trick, the DNN would receive correlated data, which would
    result in poor optimization results.
  id: totrans-125
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果没有缓冲区技巧，DNN将接收到相关的数据，这将导致优化结果不佳。
- en: These two tricks have proven significant in contributing to the success of DQNs.
    Now that we have a basic understanding of how DQN models work and their novel
    characteristics, let's move on to the final section of this chapter, where we
    will implement our own DQN model. Using PyTorch, we will build a CNN-based DQN
    model that will learn to play the Atari video game known as Pong and potentially
    learn to win the game against the computer opponent.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个技巧在贡献DQN成功方面已被证明非常重要。现在我们对DQN模型的工作原理和其新颖特性有了基本了解，让我们继续本章的最后一节，我们将实现自己的DQN模型。使用PyTorch，我们将构建一个基于CNN的DQN模型，该模型将学习玩名为Pong的Atari视频游戏，并可能学会击败电脑对手。
- en: Building a DQN model in PyTorch
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中构建DQN模型
- en: We discussed the theory behind DQNs in the previous section. In this section,
    we will take a hands-on approach. Using PyTorch, we will build a CNN-based DQN
    model that will train an agent to play the video game known as Pong. The goal
    of this exercise is to demonstrate how to develop DRL applications using PyTorch.
    Let's get straight into the exercise.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节讨论了DQN背后的理论。在这一节中，我们将采取实际操作的方式。使用PyTorch，我们将构建一个基于CNN的DQN模型，该模型将训练一个代理人玩称为Pong的视频游戏。这个练习的目标是展示如何使用PyTorch开发强化学习应用程序。让我们直接进入练习。
- en: Initializing the main and target CNN models
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化主要和目标CNN模型
- en: 'In this exercise, we will only show the important parts of the code for demonstration
    purposes. In order to access the full code, visit our github repository [11.1]
    . Follow these steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们仅展示代码的重要部分以演示目的。要访问完整代码，请访问我们的github仓库 [11.1]。请按照以下步骤进行：
- en: 'First, we need to import the necessary libraries:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必要的库：
- en: '[PRE5]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this exercise, besides the usual Python- and PyTorch-related imports, we
    are also using a Python library called `gym`. It is a Python library produced
    by OpenAI [11.2] that provides a set of tools for building DRL applications. Essentially,
    importing `gym` does away with the need of writing all the scaffolding code for
    the internals of an RL system. It also consists of built-in environments, including
    one for the video game Pong, which we will use in this exercise.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，除了常规的与Python和PyTorch相关的导入之外，我们还使用了一个名为`gym`的Python库。这是OpenAI [11.2]开发的一个Python库，提供了一套用于构建强化学习应用的工具。基本上，导入`gym`库消除了为RL系统的内部编写所有支撑代码的需要。它还包括一些内置的环境，包括一个用于视频游戏Pong的环境，在这个练习中我们将使用它。
- en: After importing the libraries, we must define the CNN architecture for the DQN
    model. This CNN model essentially takes in the current state input and outputs
    the probability distribution over all possible actions. The action with the highest
    probability gets chosen as the next action by the agent. Instead of using a regression
    model to predict the Q-values for each state-action pair, we cleverly turn this
    into a classification problem.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库后，我们必须为DQN模型定义CNN架构。这个CNN模型主要接受当前状态输入，并输出所有可能动作的概率分布。代理人选择具有最高概率的动作作为下一个动作。与使用回归模型预测每个状态-动作对的Q值不同，我们巧妙地将其转换为分类问题。
- en: 'The Q-value regression model will have to be run separately for all possible
    actions, and we will choose the action with the highest predicted Q-value. But
    using this classification model combines the task of calculating Q-values and
    predicting the best next action into one:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Q值回归模型将必须单独运行所有可能的动作，并且我们将选择预测Q值最高的动作。但是使用这个分类模型将计算Q值和预测最佳下一个动作的任务合并为一个：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As we can see, the model consists of three convolutional layers – `cnv1`, `cnv2`,
    and `cnv3` – with ReLU activations in-between them, followed by two fully connected
    layers. Now, let''s look at what a forward pass through this model entails:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，模型由三个卷积层`cnv1`、`cnv2`和`cnv3`组成，它们之间有 ReLU 激活函数，并跟随两个全连接层。现在，让我们看看通过该模型的前向传播包含哪些内容：
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `forward` method simply demonstrates a forward pass by the model, where
    the input is passed through the convolutional layers, flattened, and finally fed
    to the fully connected layers. Finally, let''s look at the other model methods:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 方法简单地演示了模型的前向传播，其中输入通过卷积层，展平，最后馈送到全连接层。最后，让我们看看其他模型方法：'
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code snippet, the `feat_size` method is simply meant to calculate
    the size of the feature vector after flattening the last convolutional layer output.
    Finally, the `perf_action` method is the same as the `take_action` method we discussed
    previously in the *Discussing Q-learning* section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，`feat_size` 方法只是用于计算将最后一个卷积层输出展平后的特征向量大小。最后，`perf_action` 方法与我们之前在*讨论
    Q 学习*部分讨论的 `take_action` 方法相同。
- en: 'In this step, we define a function that instantiates the main neural network
    and the target neural network:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们定义一个函数，实例化主神经网络和目标神经网络：
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These two models are instances of the same class and hence share the same architecture.
    However, they are two separate instances and hence will evolve differently with
    different sets of weights.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型是同一类的实例，因此共享相同的架构。然而，它们是两个独立的实例，因此将随不同的权重集合而有所不同。
- en: Defining the experience replay buffer
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义经验重播缓冲区
- en: 'As we discussed in the *Understanding deep Q-learning* section, the experience
    replay buffer is a significant feature of DQNs. With the help of this buffer,
    we can store several thousand transitions (frames) of a game and then randomly
    sample those video frames to train the CNN model. The following is the code for
    defining the replay buffer:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*理解深度 Q 学习*部分讨论的那样，经验重播缓冲区是 DQN 的一个重要特性。借助该缓冲区，我们可以存储几千个游戏转换（帧），然后随机采样这些视频帧来训练
    CNN 模型。以下是定义重播缓冲区的代码：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, `cap_max` is the defined buffer size; that is, the number of video game
    state transitions that shall be stored in the buffer. The `smpl` method is used
    during the CNN training loop to sample the stored transitions and generate batches
    of training data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`cap_max` 是定义的缓冲区大小；即，将存储在缓冲区中的视频游戏状态转换数量。`smpl` 方法在 CNN 训练循环中用于采样存储的转换并生成训练数据批次。
- en: Setting up the environment
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'So far, we have mostly focused on the neural network side of DQNs. In this
    section, we will focus on building one of the foundational aspects in an RL problem
    – the environment. Follow these steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注于 DQN 的神经网络方面。在本节中，我们将专注于构建 RL 问题的基础方面之一 - 环境。请按照以下步骤进行操作：
- en: 'First, we must define some video game environment initialization-related functions:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须定义一些与视频游戏环境初始化相关的函数：
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the `gym` library, we have access to a pre-built Pong video game environment.
    But here, we will augment the environment in a series of steps, which will include
    downsampling the video game image frames, pushing image frames to the experience
    replay buffer, converting images into PyTorch tensors, and so on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `gym` 库，我们可以访问预先构建的 Pong 视频游戏环境。但在这里，我们将通过一系列步骤增强环境，包括降低视频游戏图像帧率，将图像帧推送到经验重播缓冲区，将图像转换为
    PyTorch 张量等。
- en: 'The following are the defined classes that implement each of the environment
    control steps:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是实现每个环境控制步骤的定义类：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These classes will now be used for initializing and augmenting the video game
    environment.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类现在将用于初始化和增强视频游戏环境。
- en: 'Once the environment-related classes have been defined, we must define a final
    method that takes in the raw Pong video game environment as input and augments
    the environment, as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦定义了与环境相关的类，我们必须定义一个最终方法，该方法将原始 Pong 视频游戏环境作为输入，并增强环境，如下所示：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Some of the code in this step have been omitted as our focus is on the PyTorch
    aspect of this exercise. Please refer to this book's GitHub repository [11.3]
    for the full code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中的部分代码已经省略，因为我们的重点是这个练习中的 PyTorch 方面。请参考本书的 GitHub 仓库 [11.3] 获取完整的代码。
- en: Defining the CNN optimization function
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义 CNN 优化函数
- en: 'In this section, we will define the loss function for training our DRL model,
    as well as define what needs to be done at the end of each model training iteration.
    Follow these steps:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义用于训练我们的深度强化学习模型的损失函数，并定义每个模型训练迭代结束时需要执行的操作。按照以下步骤进行：
- en: 'We initialized our main and target CNN models in *step 2* of the *Initializing
    the main and target CNN models* section. Now that we have defined the model architecture,
    we shall define the `loss` function, which the model will be trained to minimize:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在“初始化主神经网络和目标神经网络”部分的“步骤 2”中初始化了我们的主要和目标 CNN 模型。现在我们已经定义了模型架构，我们将定义损失函数，该函数将被训练以最小化：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The loss function defined here is derived from equation 11.4\. . This loss is
    known as the **time/temporal difference loss** and is one of the foundational
    concepts of DQNs.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 此处定义的损失函数源自方程 11.4。此损失称为**时间差异损失**，是 DQN 的基础概念之一。
- en: 'Now that the neural network architecture and loss function are in place, we
    shall define the model `updation` function, which is called at every iteration
    of neural network training:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在神经网络架构和损失函数已经就位，我们将定义模型“更新”函数，该函数在神经网络训练的每次迭代时调用：
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function samples a batch of data from the experience replay buffer, computes
    the time difference loss on this batch of data, and also copies the weights of
    the main neural network to the target neural network once every `TGT_UPD_FRQ`
    iterations. `TGT_UPD_FRQ` will be assigned a value later.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数从经验重播缓冲区中抽取一批数据，计算这批数据的时间差损失，并在每 `TGT_UPD_FRQ` 次迭代时将主神经网络的权重复制到目标神经网络中。`TGT_UPD_FRQ`
    将在稍后分配一个值。
- en: Managing and running episodes
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 管理和运行剧集
- en: 'Now, let''s learn how to define the epsilon value:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何定义 epsilon 值：
- en: 'First, we will define a function that will update the epsilon value after each
    episode:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个函数，该函数将在每个剧集后更新 epsilon 值：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function is the same as the epsilon update step in our Q-learning loop,
    as discussed in the *Discussing Q-learning* section. The goal of this function
    is to linearly reduce the epsilon value per episode.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数与我们在“讨论 Q-learning”部分中讨论的 Q-learning 循环中的 epsilon 更新步骤相同。该函数的目标是按剧集线性减少 epsilon
    值。
- en: 'The next function is to define what happens at the end of an episode. If the
    overall reward that''s scored in the current episode is the best we''ve achieved
    so far, we save the CNN model weights and print the reward value:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个函数是定义剧集结束时发生的情况。如果当前剧集中得分的总奖励是迄今为止我们取得的最佳成绩，我们会保存 CNN 模型的权重并打印奖励值：
- en: '[PRE17]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: At the end of each episode, we also log the episode number, the reward at the
    end of the current episode, a running average of reward values across the past
    few episodes, and finally, the current epsilon value.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 每个剧集结束时，我们还会记录剧集编号、剧集结束时的奖励、过去几个剧集奖励值的滚动平均值，以及当前的 epsilon 值。
- en: 'We have finally reached one of the most crucial function definitions of this
    exercise. Here, we must specify the DQN loop. This is where we define the steps
    that shall be executed in an episode:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们终于到达了本练习中最关键的函数定义之一。在这里，我们必须指定 DQN 循环。这是我们定义在一个剧集中执行的步骤：
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The rewards and states are reset at the beginning of the episode. Then, we
    run an endless loop that only breaks if the agent reaches one of the end states.
    Within this loop, in each iteration, the following steps are executed:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励和状态会在每个剧集开始时重置。然后，我们运行一个无限循环，只有当代理达到其中一个终止状态时才会退出。在这个循环中，每次迭代执行以下步骤：
- en: i) First, the epsilon value is modified as per the *linear depreciation scheme*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: i) 首先，按线性折旧方案修改 epsilon 值。
- en: ii) The next action is predicted by the main CNN model. This action is executed,
    resulting in the next state and a reward. This state transition is recorded in
    the experience replay buffer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 下一个动作由主 CNN 模型预测。执行此动作会导致下一个状态和一个奖励。这个状态转换被记录在经验重播缓冲区中。
- en: iii) The next state now becomes the current state and we calculate the time
    difference loss, which is used to update the main CNN model while keeping the
    target CNN model frozen.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 接下来的状态现在成为当前状态，并计算时间差异损失，用于更新主 CNN 模型，同时保持目标 CNN 模型冻结。
- en: iv) If the new current state is an end state, then we break the loop (that is,
    end the episode) and log the results for this episode.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: iv) 如果新的当前状态是一个终止状态，那么我们中断循环（即结束剧集），并记录本剧集的结果。
- en: 'We have mentioned logging results throughout the training process. In order
    to store the various metrics around rewards and model performance, we must define
    a training metadata class, which will consist of various metrics as attributes:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在整个训练过程中提到了记录结果。为了存储围绕奖励和模型性能的各种指标，我们必须定义一个训练元数据类，其中将包含各种指标作为属性：
- en: '[PRE19]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will use these metrics to visualize model performance later in this exercise,
    once we've trained the model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些指标稍后在这个练习中可视化模型性能，一旦我们训练完模型。
- en: 'We store the model metric attributes in the previous step as private members
    and publicly expose their corresponding getter functions instead:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在上一步中将模型度量属性存储为私有成员，并公开它们相应的获取函数：
- en: '[PRE20]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `idx` attribute is critical for deciding when to copy the weights from the
    main CNN to the target CNN, while the `avg` attribute is useful for computing
    the running average of rewards that have been received in the past few episodes.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`idx` 属性对于决定何时从主 CNN 复制权重到目标 CNN 非常关键，而 `avg` 属性对于计算过去几集收到的奖励的运行平均值非常有用。'
- en: Training the DQN model to learn Pong
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练 DQN 模型以学习 Pong
- en: 'Now, we have all the necessary ingredients to start training the DQN model.
    Let''s get started:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们拥有开始训练 DQN 模型所需的所有必要组件。让我们开始吧：
- en: 'The following is a training wrapper function that will do everything we need
    it to do:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面是一个训练包装函数，它将做我们需要做的一切：
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Essentially, we initialize a logger and just run the DQN training system for
    a predefined number of episodes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们初始化了一个记录器，只需运行预定义数量的情节的 DQN 训练系统。
- en: 'Before we actually run the training loop, we need to define the hyperparameter
    values, which are as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们实际运行训练循环之前，我们需要定义以下超参数值：
- en: i) The batch size for each iteration of gradient descent to tune the CNN model
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: i) 每次梯度下降迭代的批量大小，用于调整 CNN 模型
- en: ii) The environment, which in this case is the Pong video game
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 环境，本例中是 Pong 游戏
- en: iii) The epsilon value for the first episode
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 第一集的 epsilon 值
- en: iv) The epsilon value for the last episode
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: iv) 最后一集的 epsilon 值
- en: v) The rate of depreciation for the epsilon value
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: v) epsilon 值的折旧率
- en: vi) Gamma; that is, the discounting factor
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: vi) Gamma；即折现因子
- en: vii) The initial number of iterations that are reserved just for pushing data
    to the replay buffer
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: vii) 最初仅用于向回放缓冲区推送数据的迭代次数
- en: viii) The learning rate
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: viii) 学习率
- en: ix) The size or capacity of the experience replay buffer
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ix) 经验回放缓冲区的大小或容量
- en: x) The total number of episodes to train the agent for
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: x) 训练代理程序的总集数
- en: xi) The number of iterations after which we copy the weights from the main CNN
    to the target CNN
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: xi) 多少次迭代后，我们从主 CNN 复制权重到目标 CNN
- en: 'We can instantiate all of these hyperparameters in the following piece of code:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的代码中实例化所有这些超参数：
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These values are experimental, and I encourage you to try changing them and
    observe the impact they have on the results.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值是实验性的，我鼓励您尝试更改它们并观察对结果的影响。
- en: 'This is the last step of the exercise and is where we actually execute the
    DQN training routine, as follows:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是练习的最后一步，也是我们实际执行 DQN 训练例程的地方，如下所示：
- en: i) First, we instantiate the game environment.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: i) 首先，我们实例化游戏环境。
- en: ii) Then, we define the device that the training will happen on – either CPU
    or GPU, based on availability.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 然后，我们定义训练将在其上进行的设备 – 根据可用性为 CPU 或 GPU。
- en: iii) Next, we instantiate the main and target CNN models. We also define *Adam*
    as the optimizer for the CNN models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 接下来，我们实例化主 CNN 模型和目标 CNN 模型。我们还将 *Adam* 定义为 CNN 模型的优化器。
- en: iv) We then instantiate an experience replay buffer.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: iv) 然后，我们实例化经验回放缓冲区。
- en: v) Finally, we begin training the main CNN model. Once the training routine
    finishes, we close the instantiated environment.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: v) 最后，我们开始训练主 CNN 模型。一旦训练例程完成，我们关闭实例化的环境。
- en: 'The code for this is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下所示：
- en: '[PRE23]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This should give us the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给我们以下输出：
- en: '![Figure 11\. 5 – DQN training logs](img/file103.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 5 – DQN 训练日志](img/file103.jpg)'
- en: Figure 11\. 5 – DQN training logs
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 5 – DQN 训练日志
- en: 'Furthermore, the following graph shows the progression of the current rewards,
    best rewards, and average rewards, as well as the epsilon values against the progression
    of the episodes:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，下图显示了当前奖励、最佳奖励和平均奖励的进展，以及 epsilon 值与情节进展的关系：
- en: '![Figure 11\. 6 – DQN training curves](img/file104.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 6 – DQN 训练曲线](img/file104.jpg)'
- en: Figure 11\. 6 – DQN training curves
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 6 – DQN训练曲线
- en: 'The following graph shows how the epsilon value decreases over episodes during
    the training process:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在训练过程中epsilon值随着回合数的减少情况：
- en: '![Figure 11\. 7 – Epsilon variation over episodes](img/file105.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 11\. 7 – 回合内的 epsilon 变化](img/file105.jpg)'
- en: Figure 11\. 7 – Epsilon variation over episodes
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 7 – 回合内的 epsilon 变化
- en: Notice that in *Figure 11.* *6*, the running average value of rewards in an
    episode (red curve) starts at **-20**, which is the scenario where the agent scores
    **0** points in a game and the opponent scores all **20** points. As the episodes
    progress, the average rewards keep increasing and by episode number **1500**,
    it crosses the zero mark. This means that after **1500** episodes of training,
    the agent has leveled up against the opponent.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在*图 11.* *6*中，一个回合内奖励的运行平均值（红色曲线）从**-20**开始，这是代理在游戏中得**0**分而对手得**20**分的情况。随着回合的进行，平均奖励不断增加，到第**1500**回合时，越过了零点标记。这意味着在经过**1500**回合的训练后，代理已经超越了对手。
- en: From here onward, the average rewards are positive, which indicates that the
    agent is winning against the opponent on average. We have only trained until **2000**
    episodes, which already results in the agent winning by a margin of over **7**
    average points against the opponent. I encourage you to train it for longer and
    see if the agent can absolutely crush the opponent by always scoring all the points
    and winning by a margin of **20** points.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，平均奖励变为正值，这表明代理平均上在对手手上占有优势。我们仅仅训练了**2000**回合，代理已经以超过**7**分的平均分数优势击败了对手。我鼓励你延长训练时间，看看代理是否能够始终得分，并以**20**分的优势击败对手。
- en: This concludes our deep dive into the implementation of a DQN model. DQN has
    been vastly successful and popular in the field of RL and is definitely a great
    starting point for those interested in exploring the field further. PyTorch, together
    with the gym library, is a great resource that enables us to work in various RL
    environments and work with different kinds of DRL models.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对DQN模型实现的深入挖掘。DQN在强化学习领域取得了巨大成功并广受欢迎，绝对是探索该领域的一个很好的起点。PyTorch与gym库一起，为我们在各种RL环境中工作和处理不同类型的DRL模型提供了极大的帮助。
- en: In this chapter, we have only focused on DQNs, but the lessons we've learned
    can be transferred to working with other variants of Q-learning models and other
    DRL algorithms.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只关注了DQNs，但我们所学到的经验可以应用到其他变体的Q学习模型和其他深度强化学习算法中。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: RL is one of the fundamental branches of machine learning and is currently one
    of the hottest, if not the hottest, areas of research and development. RL-based
    AI breakthroughs such as AlphaGo from Google's DeepMind have further increased
    enthusiasm and interest in the field. This chapter provided an overview of RL
    and DRL and walked us through a hands-on exercise of building a DQN model using
    PyTorch.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的一个基础分支，目前是研究和开发中最热门的领域之一。像谷歌DeepMind的AlphaGo这样基于RL的AI突破进一步增加了人们对这一领域的热情和兴趣。本章概述了强化学习和深度强化学习，并通过使用PyTorch构建DQN模型的实际练习带领我们深入探讨。
- en: RL is a vast field and one chapter is not enough to cover everything. I encourage
    you to use the high-level discussions from this chapter to explore the details
    around those discussions. From the next chapter onward, we will focus on the practical
    aspects of working with PyTorch, such as model deployment, parallelized training,
    automated machine learning, and so on. In the next chapter, we will start by discussing
    how to effectively use PyTorch to put trained models into production systems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个广泛的领域，一章篇幅远远不够覆盖所有内容。我鼓励你利用本章的高层次讨论去探索这些细节。从下一章开始，我们将专注于使用PyTorch处理实际工作中的各个方面，比如模型部署、并行化训练、自动化机器学习等等。在下一章中，我们将讨论如何有效地使用PyTorch将训练好的模型投入生产系统。
