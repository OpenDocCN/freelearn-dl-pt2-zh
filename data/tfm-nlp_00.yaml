- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Transformers are a game-changer for **Natural Language Understanding** (**NLU**),
    a subset of **Natural Language Processing** (**NLP**), which has become one of
    the pillars of artificial intelligence in a global digital economy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器对于**自然语言理解**（**NLU**）的重要性不言而喻，NLU是**自然语言处理**（**NLP**）的一个子集，在全球数字经济中已经成为人工智能的支柱之一。
- en: Transformer models mark the beginning of a new era in artificial intelligence.
    Language understanding has become the pillar of language modeling, chatbots, personal
    assistants, question answering, text summarizing, speech-to-text, sentiment analysis,
    machine translation, and more. We are witnessing the expansion of social networks
    versus physical encounters, e-commerce versus physical shopping, digital newspapers,
    streaming versus physical theaters, remote doctor consultations versus physical
    visits, remote work instead of on-site tasks, and similar trends in hundreds of
    more domains. It would be incredibly difficult for society to use web browsers,
    streaming services, and any digital activity involving language without AI language
    understanding. The paradigm shift of our societies from physical to massive digital
    information forced artificial intelligence into a new era. Artificial intelligence
    has evolved to billion-parameter models to face the challenge of trillion-word
    datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型标志着人工智能的新时代的开始。语言理解已成为语言建模、聊天机器人、个人助手、问答、文本摘要、语音转文本、情感分析、机器翻译等领域的支柱。我们正在见证社交网络与面对面的社交相比的扩张，电子商务与实体购物的竞争，数字新闻与传统媒体的竞争，流媒体与实体剧院的竞争，远程医生咨询与实体就诊的竞争，远程工作与现场任务的竞争以及数百个领域中类似的趋势。如果没有AI语言理解，社会将难以使用网络浏览器、流媒体服务以及任何涉及语言的数字活动。我们社会从物理信息到大规模数字信息的范式转变迫使人工智能进入了一个新时代。人工智能已经演化到了亿级参数模型来应对万亿字数据集的挑战。
- en: The Transformer architecture is both revolutionary and disruptive. It breaks
    with the past, leaving the dominance of RNNs and CNNs behind. BERT and GPT models
    abandoned recurrent network layers and replaced them with self-attention. Transformer
    models outperform RNNs and CNNs. The 2020s are experiencing a major change in
    AI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构既具有革命性又具有颠覆性。它打破了过去，摆脱了RNN和CNN的主导地位。 BERT和GPT模型放弃了循环网络层，并用自注意力替换它们。变压器模型胜过了RNN和CNN。
    2020年代正在经历人工智能的重大变革。
- en: Transformer encoders and decoders contain attention heads that train separately,
    parallelizing cutting-edge hardware. Attention heads can run on separate GPUs
    opening the door to billion-parameter models and soon-to-come trillion-parameter
    models. OpenAI trained a 175 billion parameter GPT-3 Transformer model on a supercomputer
    with 10,000 GPUs and 285,000 CPU cores.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器编码器和解码器包含可以单独训练的注意力头，可以并行化最先进的硬件。注意力头可以在单独的GPU上运行，为亿级参数模型和即将推出的万亿级参数模型敞开大门。OpenAI在拥有10000个GPU和285000个CPU核心的超级计算机上训练了一个有1750亿参数的GPT-3变压器模型。
- en: The increasing amount of data requires training AI models at scale. As such,
    transformers pave the way to a new era of parameter-driven AI. Learning to understand
    how hundreds of millions of words fit together in sentences requires a tremendous
    amount of parameters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量的增加要求以规模训练AI模型。因此，变压器为参数驱动AI开辟了新的时代。学会理解数亿个单词在句子中如何组合需要大量参数。
- en: Transformer models such as Google BERT and OpenAI GPT-3 have taken emergence
    to another level. Transformers can perform hundreds of NLP tasks they were not
    trained for.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌BERT和OpenAI GPT-3等变压器模型已经将新兴技术推上了新的高度。变压器可以执行它们未经过训练的数百种NLP任务。
- en: Transformers can also learn image classification and reconstruction by embedding
    images as sequences of words. This book will introduce you to cutting-edge computer
    vision transformers such as **Vision Transformers** (**ViT**), CLIP, and DALL-E.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器还可以通过将图像嵌入为单词序列来学习图像分类和重建。本书将向您介绍最前沿的计算机视觉变压器，如**视觉变压器**（**ViT**），CLIP和DALL-E。
- en: Foundation models are fully trained transformer models that can carry out hundreds
    of tasks without fine-tuning. Foundation models at this scale offer the tools
    we need in this massive information era.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型是完全训练的变压器模型，可以在不进行微调的情况下执行数百种任务。在这个大规模信息时代，基础模型提供了我们所需要的工具。
- en: Think of how many humans it would take to control the content of the billions
    of messages posted on social networks per day to decide if they are legal and
    ethical before extracting the information they contain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想想需要多少人来控制每天在社交网络上发布的数十亿条消息的内容，以在提取其中的信息之前判断它们是否合法和道德。
- en: Think of how many humans would be required to translate the millions of pages
    published each day on the web. Or imagine how many people it would take to manually
    control the millions of messages made per minute!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想想需要多少人来翻译每天在网络上发布的数百万页。或者想象一下手动控制每分钟生成的数百万条消息需要多少人！
- en: Finally, think of how many humans it would take to write the transcripts of
    all of the vast amount of hours of streaming published per day on the web. Finally,
    think about the human resources required to replace AI image captioning for the
    billions of images that continuously appear online.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，想想需要多少人来记录每天在网络上发布的大量流媒体的转录。最后，想想替代AI图像字幕对那些持续出现在网上的数十亿张图片所需的人力资源。
- en: This book will take you from developing code to prompt design, a new “programming”
    skill that controls the behavior of a transformer model. Each chapter will take
    you through the key aspects of language understanding from scratch in Python,
    PyTorch, and TensorFlow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书将带你从代码开发到提示设计，掌握控制Transformer模型行为的新 "编程" 技能。每一章都将带你从Python、PyTorch和TensorFlow的零基础开始，了解语言理解的关键方面。
- en: You will learn the architecture of the original Transformer, Google BERT, OpenAI
    GPT-3, T5, and several other models. You will fine-tune transformers, train models
    from scratch, and learn to use powerful APIs. Facebook, Google, Microsoft, and
    other big tech corporations share large datasets for us to explore.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习到原始Transformer、Google BERT、OpenAI GPT-3、T5以及其他几个模型的架构。你将微调transformer，从头开始训练模型，并学会使用强大的API。Facebook、Google、Microsoft和其他大型科技公司分享了大量数据集供我们探索。
- en: You will keep close to the market and its demand for language understanding
    in many fields such as media, social media, and research papers, for example.
    Among hundreds of AI tasks, we need to summarize the vast amounts of data for
    research, translate documents for every area of our economy, and scan all social
    media posts for ethical and legal reasons.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你将与市场及其对语言理解的需求保持紧密联系，例如媒体、社交媒体和研究论文等领域。在数百个AI任务中，我们需要总结大量的研究数据，为经济的各个领域翻译文件，并为伦理和法律原因扫描所有的社交媒体帖子。
- en: Throughout the book, you will work hands-on with Python, PyTorch, and TensorFlow.
    You will be introduced to the key AI language understanding neural network models.
    You will then learn how to explore and implement transformers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，你将尝试使用Python、PyTorch和TensorFlow进行实践。你将了解关键的AI语言理解神经网络模型。然后，你将学习如何探索和实现transformer。
- en: You will learn the new skills required to become an Industry 4.0 AI Specialist
    in this disruptive AI era. The book aims to give readers the knowledge and tools
    for Python deep learning needed for effectively developing the key aspects of
    language understanding.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个颠覆性的AI时代，你将学到成为工业4.0 AI专家所需的新技能。本书旨在为读者提供Python深度学习方面的知识和工具，以有效开发语言理解的核心内容。
- en: Who this book is for
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合谁阅读
- en: This book is not an introduction to Python programming or machine learning concepts.
    Instead, it focuses on deep learning for machine translations, speech-to-text,
    text-to-speech, language modeling, question answering, and many more NLP domains.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不是Python编程或机器学习概念的入门。相反，它专注于机器翻译、语音转文字、文字转语音、语言建模、问答等多个NLP领域的深度学习。
- en: 'Readers who can benefit the most from this book are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最能从本书受益的读者有：
- en: Deep learning and NLP practitioners who are familiar with Python programming.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉Python编程的深度学习和NLP从业者。
- en: Data analysts and data scientists who want an introduction to AI language understanding
    to process the increasing amounts of language-driven functions.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析师和数据科学家想要对AI语言理解有个初步了解，以处理越来越多的以语言为驱动的功能。
- en: What this book covers
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书涵盖的内容
- en: '**Part I: Introduction to Transformer Architectures**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一部分：Transformer架构简介**'
- en: '*Chapter 1*, *What are Transformers?*, explains, at a high level, what transformers
    are. We’ll look at the transformer ecosystem and the properties of foundation
    models. The chapter highlights many of the platforms available and the evolution
    of Industry 4.0 AI specialists.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 1 章*，*什么是 Transformer？*，在高层次上解释了 Transformer 是什么。我们将看看 Transformer 生态系统以及基础模型的特性。本章突出了许多可用的平台以及工业
    4.0 AI 专家的演变。'
- en: '*Chapter 2*, *Getting Started with the Architecture of the Transformer Model*,
    goes through the background of NLP to understand how RNN, LSTM, and CNN deep learning
    architectures evolved into the Transformer architecture that opened a new era.
    We will go through the Transformer’s architecture through the unique *Attention
    Is All You Need* approach invented by the Google Research and Google Brain authors.
    We will describe the theory of transformers. We will get our hands dirty in Python
    to see how the multi-attention head sub-layers work. By the end of this chapter,
    you will have understood the original architecture of the Transformer. You will
    be ready to explore the multiple variants and usages of the Transformer in the
    following chapters.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 2 章*，*了解 Transformer 模型的架构初步*，通过 NLP 的背景来理解 RNN、LSTM 和 CNN 深度学习架构是如何演变成打开新时代的
    Transformer 架构的。我们将通过 Google Research 和 Google Brain 的作者们提出的独特 *注意力就是一切* 方法来详细了解
    Transformer 的架构。我们将描述变压器的理论。我们将用 Python 亲自动手来看看多头注意力子层是如何工作的。在本章结束时，你将了解到 Transformer
    的原始架构。你将准备好在接下来的章节里探索 Transformer 的多种变体和用法。'
- en: '*Chapter 3*, *Fine-Tuning BERT Models*, builds on the architecture of the original
    Transformer. **Bidirectional Encoder Representations from Transformers** (**BERT**)
    shows you a new way of perceiving the world of NLP. Instead of analyzing a past
    sequence to predict a future sequence, BERT attends to the whole sequence! We
    will first go through the key innovations of BERT’s architecture and then fine-tune
    a BERT model by going through each step in a Google Colaboratory notebook. Like
    humans, BERT can learn tasks and perform other new ones without having to learn
    the topic from scratch.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 3 章*，*微调 BERT 模型*，在原始 Transformer 的架构基础上构建。**来自变压器的双向编码表示**（**BERT**）向我们展示了一个感知
    NLP 世界的新方式。BERT 不是分析过去的序列以预测未来的序列，而是关注整个序列！我们将首先了解 BERT 架构的关键创新，然后通过在 Google Colaboratory
    笔记本中逐步微调 BERT 模型。像人类一样，BERT 可以学习任务，并且执行其他新任务而无需从头学习话题。'
- en: '*Chapter 4*, *Pretraining a RoBERTa Model from Scratch*, builds a RoBERTa transformer
    model from scratch using the Hugging Face PyTorch modules. The transformer will
    be both BERT-like and DistilBERT-like. First, we will train a tokenizer from scratch
    on a customized dataset. The trained transformer will then run on a downstream
    masked language modeling task.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 4 章*，*从头开始预训练 RoBERTa 模型*，使用 Hugging Face PyTorch 模块从头构建一个 RoBERTa 变压器模型。
    这个变压器模型将类似于 BERT 和 DistilBERT。首先，我们将在一个定制的数据集上从头开始训练一个分词器。然后，训练好的变压器模型将在一个下游掩码语言建模任务中运行。'
- en: '**Part II: Applying Transformers for Natural Language Understanding and Generation**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 II 部分：将 Transformer 应用于自然语言理解和生成**'
- en: '*Chapter 5*, *Downstream NLP Tasks with Transformers*, reveals the magic of
    transformer models with downstream NLP tasks. A pretrained transformer model can
    be fine-tuned to solve a range of NLP tasks such as BoolQ, CB, MultiRC, RTE, WiC,
    and more, dominating the GLUE and SuperGLUE leaderboards. We will go through the
    evaluation process of transformers, the tasks, datasets, and metrics. We will
    then run some of the downstream tasks with Hugging Face’s pipeline of transformers.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 5 章*，*使用 Transformer 进行下游 NLP 任务*，展示了使用下游 NLP 任务的魔力。一个预训练的变压器模型可以被微调以解决一系列
    NLP 任务，如 BoolQ、CB、MultiRC、RTE、WiC 等，在 GLUE 和 SuperGLUE 排行榜上占据主导地位。我们将介绍变压器的评估过程、任务、数据集和指标。然后我们将使用
    Hugging Face 的变压器管道运行一些下游任务。'
- en: '*Chapter 6*, *Machine Translation with the Transformer*, defines machine translation
    to understand how to go from human baselines to machine transduction methods.
    We will then preprocess a WMT French-English dataset from the European Parliament.
    Machine translation requires precise evaluation methods, and in this chapter,
    we explore the BLEU scoring method. Finally, we will implement a Transformer machine
    translation model with Trax.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 6 章*，*使用 Transformer 进行机器翻译*，定义机器翻译以了解如何从人类标准到机器传导方法。然后，我们将预处理来自欧洲议会的 WMT
    法英数据集。机器翻译需要精确的评估方法，在本章中，我们将探讨 BLEU 评分方法。最后，我们将使用 Trax 实现一个 Transformer 机器翻译模型。'
- en: '*Chapter 7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*, explores
    many aspects of OpenAI’s GPT-2 and GPT-3 transformers. We will first examine the
    architecture of OpenAI’s GPT models before explaining the different GPT-3 engines.
    Then we will run a GPT-2 345M parameter model and interact with it to generate
    text. Next, we’ll see the GPT-3 playground in action before coding a GPT-3 model
    for NLP tasks and comparing the results to GPT-2.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 7 章*，*超人类 Transformers 的崛起与 GPT-3 引擎*，探讨了 OpenAI 的 GPT-2 和 GPT-3 transformers
    的许多方面。我们将首先检查 OpenAI 的 GPT 模型的架构，然后解释不同的 GPT-3 引擎。接着我们将运行一个 GPT-2 345M 参数模型，并与其交互生成文本。接下来，我们将看到
    GPT-3 游乐场的运作方式，然后编写一个用于 NLP 任务的 GPT-3 模型，并将结果与 GPT-2 进行比较。'
- en: '*Chapter 8*, *Applying Transformers to Legal and Financial Documents for AI
    Text Summarization*, goes through the concepts and architecture of the T5 transformer
    model. We will initialize a T5 model from Hugging Face to summarize documents.
    We will task the T5 model to summarize various documents, including a sample from
    the *Bill of Rights*, exploring the successes and limitations of transfer learning
    approaches applied to transformers. Finally, we will use GPT-3 to summarize some
    corporation law text to a second-grader.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 8 章*，*将 Transformer 应用于法律和金融文件以进行 AI 文本摘要*，介绍了 T5 Transformer 模型的概念和架构。我们将使用
    Hugging Face 的 T5 模型来进行文档摘要。我们将让 T5 模型摘要各种文档，包括 *权利法案* 中的样本，探讨将迁移学习方法应用于 Transformers
    的成功和局限性。最后，我们将使用 GPT-3 将一些公司法文本摘要成二年级水平的语言。'
- en: '*Chapter 9*, *Matching Tokenizers and Datasets*, analyzes the limits of tokenizers
    and looks at some of the methods applied to improve the data encoding process’s
    quality. We will first build a Python program to investigate why some words are
    omitted or misinterpreted by word2vector tokenizers. Following this, we find the
    limits of pretrained tokenizers with a tokenizer-agonistic method.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 9 章*，*匹配 Tokenizers 和数据集*，分析了分词器的限制，并研究了一些用于改进数据编码过程质量的方法。我们将首先构建一个 Python
    程序来研究为什么一些单词会被 word2vector 分词器省略或误解。在此之后，我们将使用一个与分词器无关的方法来找出预训练分词器的限制。'
- en: We will improve a T5 summary by applying some of the ideas that show that there
    is still much room left to improve the methodology of the tokenization process.
    Finally, we will test the limits of GPT-3’s language understanding.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过应用一些显示仍有很大改进空间的想法来改进 T5 摘要方法论。最后，我们将测试 GPT-3 的语言理解能力的限制。
- en: '*Chapter 10*, *Semantic Role Labeling with BERT-Based Transformers*, explores
    how transformers learn to understand a text’s content. **Semantic Role Labeling**
    (**SRL**) is a challenging exercise for a human. Transformers can produce surprising
    results. We will implement a BERT-based transformer model designed by the Allen
    Institute for AI in a Google Colab notebook. We will also use their online resources
    to visualize SRL outputs. Finally, we will question the scope of SRL and understand
    the reasons behind its limitations.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 10 章*，*使用基于 BERT 的 Transformers 进行语义角色标注*，探讨了 Transformers 如何学习理解文本内容。**语义角色标注**（**SRL**）对人类来说是一个具有挑战性的练习。Transformers
    可以产生令人惊讶的结果。我们将在 Google Colab 笔记本中实现由 AI 艾伦研究所设计的基于 BERT 的 Transformer 模型。我们还将使用他们的在线资源来可视化
    SRL 输出。最后，我们将质疑 SRL 的范围，并理解其局限性背后的原因。'
- en: '**Part III: Advanced Language Understanding Techniques**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三部分：高级语言理解技术**'
- en: '*Chapter 11*, *Let Your Data Do the Talking: Story, Questions, and Answers*,
    shows how a transformer can learn how to reason. A transformer must be able to
    understand a text, a story, and also display reasoning skills. We will see how
    question answering can be enhanced by adding NER and SRL to the process. We will
    build the blueprint for a question generator that can be used to train transformers
    or as a stand-alone solution.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 11 章*，*让你的数据说话：故事、问题和答案*，展示了 Transformer 如何学会推理。Transformer 必须能够理解文本、故事，并展现出推理技能。我们将看到如何通过添加
    NER 和 SRL 来增强问答过程。我们将为一个问题生成器建立蓝图，该生成器可以用于训练 Transformers 或作为独立解决方案。'
- en: '*Chapter 12*, *Detecting Customer Emotions to Make Predictions*, shows how
    transformers have improved sentiment analysis. We will analyze complex sentences
    using the Stanford Sentiment Treebank, challenging several transformer models
    to understand not only the structure of a sequence but also its logical form.
    We will see how to use transformers to make predictions that trigger different
    actions depending on the sentiment analysis output. The chapter finishes with
    some edge cases using GPT-3.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 12 章*，*检测客户情绪以进行预测*，展示了变压器如何改进情感分析。我们将使用斯坦福情感树库分析复杂句子，挑战几种变压器模型，以理解一个序列的结构和逻辑形式。我们将看到如何使用变压器进行预测，根据情感分析输出触发不同的动作。该章节最后通过使用
    GPT-3 来解决一些边缘案例。'
- en: '*Chapter 13*, *Analyzing Fake News with Transformers*, delves into the hot
    topic of fake news and how transformers can help us understand the different perspectives
    of the online content we see each day. Every day, billions of messages, posts,
    and articles are published on the web through social media, websites, and every
    form of real-time communication available. Using several techniques from the previous
    chapters, we will analyze debates on climate change and gun control and the Tweets
    from a former president. We will go through the moral and ethical problem of determining
    what can be considered fake news beyond reasonable doubt and what news remains
    subjective.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 13 章*，*使用变压器分析假新闻*，深入探讨了假新闻的热门话题以及变压器如何帮助我们理解我们每天看到的在线内容的不同视角。每天，在线社交媒体、网站以及各种实时通讯方式发布数十亿条消息、帖子和文章。使用前几章的几种技术，我们将分析气候变化和枪支管制的辩论，以及一位前总统的推文。我们将讨论在合理怀疑的基础上确定什么可以被认为是假新闻，以及哪些新闻是主观的道德和伦理问题。'
- en: '*Chapter 14*, *Interpreting Black Box Transformer Models*, lifts the lid on
    the black box that is transformer models by visualizing their activity. We will
    use BertViz to visualize attention heads and **Language Interpretability Tool**
    (**LIT**) to carry out a **principal component analysis** (**PCA**). Finally,
    we will use LIME to visualize transformers via dictionary learning.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 14 章*，*解释黑匣子变压器模型*，通过可视化它们的活动揭开了变压器模型的黑匣子。我们将使用 BertViz 来可视化注意力头，使用**语言可解释性工具**（**LIT**）来进行**主成分分析**（**PCA**）。最后，我们将使用
    LIME 通过字典学习来可视化变压器。'
- en: '*Chapter 15*, *From NLP to Task-Agnostic Transformer Models*, delves into the
    advanced models, Reformer and DeBERTa, running examples using Hugging Face. Transformers
    can process images as sequences of words. We will also look at different vision
    transformers such as ViT, CLIP, and DALL-E. We will test them on computer vision
    tasks, including generating computer images.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 15 章*，*从自然语言处理到任务无关的变压器模型*，深入研究了先进模型 Reformer 和 DeBERTa，运用 Hugging Face
    运行示例。变压器可以将图像处理为单词序列。我们还将看到不同的视觉变压器，如 ViT、CLIP 和 DALL-E。我们将对它们进行计算机视觉任务的测试，包括生成计算机图像。'
- en: '*Chapter 16*, *The Emergence of Transformer-Driven Copilots*, explores the
    maturity of Industry 4.0\. The chapter begins with prompt engineering examples
    using informal/casual English. Next, we will use GitHub Copilot and OpenAI Codex
    to create code from a few lines of instructions. We will see that vision transformers
    can help NLP transformers visualize the world around them. We will create a transformer-based
    recommendation system, which can be used by digital humans in whatever metaverse
    you may end up in!'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 16 章*，*变压器驱动联合驾驶员的出现*，探讨了工业 4.0 的成熟度。本章从使用非正式/随意英语的提示工程示例开始。接下来，我们将使用 GitHub
    Copilot 和 OpenAI Codex 从几行指令中创建代码。我们将看到视觉变压器如何帮助自然语言处理变压器可视化周围的世界。我们将创建一个基于变压器的推荐系统，该系统可以由数字人类在您可能会进入的任何元宇宙中使用！'
- en: '*Appendix I*, *Terminology of Transformer Models*, examines the high-level
    structure of a transformer, from stacks and sublayers to attention heads.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录 I*，*变压器模型术语*，检查了一个变压器的高层结构，从堆栈和子层到注意力头。'
- en: '*Appendix II*, *Hardware Constraints for Transformer Models*, looks at CPU
    and GPU performance running transformers. We will see why transformers and GPUs
    and transformers are a perfect match, concluding with a test using Google Colab
    CPU, Google Colab Free GPU, and Google Colab Pro GPU.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录 II*，*变压器模型的硬件约束*，着眼于运行变压器的 CPU 和 GPU 性能。我们将看到为什么变压器和 GPU 以及变压器是完美的匹配，最后进行一个测试，使用
    Google Colab CPU、Google Colab 免费 GPU 和 Google Colab Pro GPU。'
- en: '*Appendix III*, *Generic Text Completion with GPT-2*, provides a detailed explanation
    of generic text completion using GPT-2 from *Chapter 7*, *The Rise of Suprahuman
    Transformers with GPT-3 Engines*.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录 III*，*使用 GPT-2 进行通用文本完成*，详细解释了使用 GPT-2 进行通用文本完成的内容，从*第 7 章*，*GPT-3 引擎的超人类转变*。'
- en: '*Appendix IV*, *Custom Text Completion with GPT-2*, supplements *Chapter 7*,
    *The Rise of Suprahuman Transformers with GPT-3 Engines* by building and training
    a GPT-2 model and making it interact with custom text.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录 IV*，*使用 GPT-2 进行自定义文本完成*，通过构建和训练 GPT-2 模型并使其与自定义文本交互，补充了*第 7 章*，*GPT-3
    引擎的超人类转变*。'
- en: '*Appendix V*, *Answers to the Questions*, provides answers to the questions
    at the end of each chapter.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*附录 V*，*问题答案*，提供了每章末尾问题的答案。'
- en: To get the most out of this book
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 充分利用本书
- en: Most of the programs in the book are Colaboratory notebooks. All you will need
    is a free Google Gmail account, and you will be able to run the notebooks on Google
    Colaboratory’s free VM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数程序都是 Colaboratory 笔记本。您只需要一个免费的 Google Gmail 帐户，就可以在 Google Colaboratory
    的免费虚拟机上运行这些笔记本。
- en: You will need Python installed on your machine for some of the educational programs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在计算机上安装 Python 才能运行一些教育性程序。
- en: Take the necessary time to read *Chapter 2*, *Getting Started with the Architecture
    of the Transformer Model* and *Appendix I*, *Terminology of Transformer Models*.
    *Chapter 2* contains the description of the original Transformer, which is built
    from building blocks explained in *Appendix I*, *Terminology of Transformer Models*,
    that will be implemented throughout the book. If you find it difficult, then pick
    up the general intuitive ideas out of the chapter. You can then go back to these
    chapters when you feel more comfortable with transformers after a few chapters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 花费必要的时间阅读*第 2 章*，*开始理解变压器模型的架构* 和 *附录 I*，*变压器模型术语*。*第 2 章* 包含了原始变压器的描述，该描述是由
    *附录 I*，*变压器模型术语* 中解释的构建块构成的，这些构建块将在本书中实现。如果您觉得困难，可以从章节中提取一般的直观想法。当您在几章后对变压器感到更加舒适时，您可以再回到这些章节。
- en: After reading each chapter, consider how you could implement transformers for
    your customers or use them to move up in your career with novel ideas.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完每章后，考虑您如何为您的客户实施变压器或使用它们提出新颖的想法来提升您的职业生涯。
- en: Please note that we use OpenAI Codex later on in the book, which currently has
    a waiting list. Sign up now to avoid a long wait time at [https://openai.com/blog/openai-codex/](https://openai.com/blog/openai-codex/).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在书的后面使用 OpenAI Codex，目前有等待名单。现在注册以避免长时间等待，请访问[https://openai.com/blog/openai-codex/](https://openai.com/blog/openai-codex/)。
- en: Download the example code files
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: The code bundle for the book is hosted on GitHub at [https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包托管在 GitHub 上，地址为[https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition](https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition)。我们还提供了来自我们丰富书目和视频目录的其他代码包，请查看[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)。
- en: Download the color images
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that contains color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个 PDF 文件，其中包含本书中使用的截图/图表的彩色图像。您可以在此处下载：[https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781803247335_ColorImages.pdf)。
- en: Conventions used
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用约定
- en: There are several text conventions used throughout this book.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了几种文本约定。
- en: '`CodeInText`: Indicates sentences and words run through the models in the book,
    code words in text, database table names, folder names, filenames, file extensions,
    pathnames, dummy URLs, user input, and Twitter handles. For example, “However,
    if you wish to explore the code, you will find it in the Google Colaboratory `positional_encoding.ipynb`
    notebook and the `text.txt` file in this chapter’s GitHub repository.”'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`CodeInText`：指示在书中运行的句子和单词，文本中的代码单词，数据库表名，文件夹名，文件名，文件扩展名，路径名，虚拟 URL，用户输入和 Twitter
    句柄。例如，“但是，如果您想要探索代码，您可以在本章的 GitHub 代码库中找到 Google Colaboratory 的 `positional_encoding.ipynb`
    笔记本和 `text.txt` 文件。”'
- en: 'A block of code is set as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块设置如下：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起您对代码块的特定部分的注意时，相关行或项会以粗体显示：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都按如下方式编写：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: “In our case, we are looking for **t5-large**, a t5-large model
    we can smoothly run in Google Colaboratory.”'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要词语或在屏幕上看到的文字，例如在菜单或对话框中，也会在文本中以此形式出现。例如：“在我们的情况下，我们正在寻找**t5-large**，一个我们可以在
    Google Colaboratory 中顺利运行的 t5-large 模型。”'
- en: Warnings or important notes appear like this.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要提示以这种方式出现。
- en: Tips and tricks appear like this.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧以这种方式出现。
- en: Get in touch
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的读者的反馈总是受欢迎的。
- en: '**General feedback**: Email `feedback@packtpub.com` and mention the book’s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：发送电子邮件至`feedback@packtpub.com`，在邮件主题中提及书籍的标题。如果您对本书的任何方面有疑问，请通过`questions@packtpub.com`与我们联系。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit, [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：尽管我们已经尽一切努力确保我们内容的准确性，但错误是不可避免的。如果您在本书中发现错误，请将此错误报告给我们。请访问[http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata)，选择您的书籍，点击错误提交表单链接，并输入详细信息。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果您在互联网上任何形式的地方发现我们作品的任何非法复制，我们将不胜感激您能提供给我们位置地址或网站名称。请通过`copyright@packtpub.com`与我们联系，并附上链接到该材料的链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果您对成为作者感兴趣**：如果您对某个您擅长的主题感兴趣，并且您有兴趣编写或为一本书做出贡献，请访问[http://authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share your thoughts
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享您的想法
- en: Once you’ve read *Transformers for Natural Language Processing - Second Edition*,
    we’d love to hear your thoughts! Please [click here to go straight to the Amazon
    review page](https://packt.link/r/1803247339) for this book and share your feedback.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您阅读完*自然语言处理的变压器-第二版*，我们将很高兴听到您的想法！请[点击此处直达亚马逊评论页面](https://packt.link/r/1803247339)给这本书留下您的反馈。
- en: Your review is important to us and the tech community and will help us make
    sure we’re delivering excellent quality content.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 您的审阅对我们和技术社区都很重要，并将帮助我们确保我们提供的内容质量卓越。
