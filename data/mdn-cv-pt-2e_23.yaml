- en: Appendix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: Chapter 1, Artificial Neural Network Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章，人工神经网络基础
- en: What are the various layers in a neural network?
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中的各种层是什么？
- en: Input, hidden, and output.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 输入、隐藏和输出。
- en: What is the output of feedforward propagation?
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前向传播的输出是什么？
- en: Predictions that help in calculating the loss value.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助计算损失值的预测。
- en: How is the loss function of a continuous dependent variable different from that
    of a binary dependent variable or a categorical dependent variable?
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续因变量的损失函数与二元或分类因变量的损失函数有何不同？
- en: '**Mean squared error** (**MSE**) is the generally used loss function for continuous
    dependent variables, and binary cross-entropy is generally used for binary dependent
    variables. Categorical cross-entropy is used for categorical dependent variables.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差** (**MSE**)通常用于连续因变量的损失函数，而二元交叉熵通常用于二元因变量。分类交叉熵用于分类因变量。'
- en: What is stochastic gradient descent?
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是随机梯度下降？
- en: It is the process of reducing loss by adjusting weights in the direction of
    decreasing gradient.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过调整权重以减少梯度来降低损失的过程。
- en: What does a backpropagation exercise do?
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个反向传播练习的目的是什么？
- en: It computes the gradients of all weights with respect to loss using the chain
    rule.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用链式法则计算所有权重相对于损失的梯度。
- en: How does the update of all the weights across layers happen during backpropagation?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播期间如何更新所有层中的权重？
- en: It happens using the formula **W_new = W – alpha*(dL/dW)**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用公式 **W_new = W – alpha*(dL/dW)** 进行更新。
- en: What functions are performed within each epoch of training a neural network?
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练神经网络的每个epoch中执行了哪些功能？
- en: For each batch in an epoch, you perform forward-propagation, compute the loss,
    compute the weight gradients with backpropagation on the loss, and update the
    weights. You repeat with the next batch until all epochs are finished.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个epoch中的每个批次，您执行前向传播，计算损失，使用损失进行反向传播计算权重梯度，并更新权重。然后继续下一个批次，直到所有epoch完成。
- en: Why is training a network on a GPU faster when compared to training it on a
    CPU?
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GPU上训练网络比在CPU上训练更快的原因是什么？
- en: More matrix operations can be performed in parallel on GPU hardware.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU硬件上可以并行执行更多的矩阵操作。
- en: What is the impact of the learning rate when training a neural network?
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练神经网络时学习率的影响是什么？
- en: Too high a learning rate will explode the weights, and too small a learning
    rate will not change the weights at all.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率过高会导致权重爆炸，而学习率过小则根本不会改变权重。
- en: What is the typical value of the learning rate parameter?
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率参数的典型值是多少？
- en: It’s usually between **1e-2** and **1e-5**, but it is dependent on many factors,
    such as the architecture, dataset, and optimizers being used.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在**1e-2**到**1e-5**之间，但这取决于许多因素，例如正在使用的架构、数据集和优化器。
- en: Chapter 2, PyTorch Fundamentals
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章，PyTorch基础
- en: Why should we convert integer inputs into float values during training?
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在训练过程中应将整数输入转换为浮点值？
- en: '`nn.Linear` (and almost all torch layers) only accepts floats as inputs.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Linear`（以及几乎所有torch层）只接受浮点数作为输入。'
- en: What are the methods used to reshape a tensor object?
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于重塑张量对象的方法是什么？
- en: '`tensor.view(shape)`, `permute`, `flatten`, `squeeze`, and `unsqueeze`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensor.view(shape)`、`permute`、`flatten`、`squeeze`和`unsqueeze`。'
- en: Why is computation faster with tensor objects than with NumPy arrays?
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么使用张量对象计算速度比使用NumPy数组快？
- en: The capability to run on GPUs in parallel is only available on tensor objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 只有张量对象才能并行运行在GPU上的能力。
- en: What constitutes the `init` magic function in a neural network class?
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经网络类中，`init`魔术函数包含什么？
- en: Calling `super().__init__()` and specifying the neural network layers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`super().__init__()`并指定神经网络层。
- en: Why do we perform zero gradients before performing backpropagation?
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在执行反向传播之前要进行零梯度操作？
- en: To ensure that gradients from previous calculations are flushed out.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 确保前一次计算的梯度被清除的原因是什么？
- en: What magic functions constitute the dataset class?
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些魔术函数构成数据集类？
- en: '`__len__` and `__getitem__`.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`__len__`和`__getitem__`。'
- en: How do we make predictions on new data points?
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在新数据点上进行预测？
- en: By calling the model on the tensor as if it is a function – `model(x)`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在张量上调用模型，如同调用函数一样 – `model(x)`。
- en: How do we fetch the intermediate layer values of a neural network?
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何获取神经网络的中间层值？
- en: By creating a custom method that can perform `forward` only up to intermediate
    layers, or by returning the intermediate layer values as an additional output
    in the forward method itself.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建一个自定义方法，该方法可以仅执行到中间层的 `forward`，或者通过在 `forward` 方法本身中将中间层的值作为额外输出返回。
- en: How does the `Sequential` method help simplify the definition of the architecture
    of a neural network?
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Sequential` 方法如何帮助简化神经网络架构的定义？'
- en: We can avoid creating `__init__` and the `forward` method by connecting a sequence
    of layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过连接一系列的层来避免创建 `__init__` 和 `forward` 方法。
- en: While updating `loss_history`, we append `loss.item()` instead of `loss`. What
    does this accomplish, and why is it useful to append `loss.item()` instead of
    just `loss`?
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新 `loss_history` 时，我们附加 `loss.item()` 而不是 `loss`。这样做有什么作用，为什么附加 `loss.item()`
    而不只是 `loss` 是有用的？
- en: '`loss.item()` converts a 0-dimension torch tensor into a Python float that
    takes up less memory, is stored on the CPU, and can be used by other libraries,
    such as plotting, stats, etc.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss.item()` 将一个零维的 torch 张量转换成 Python 浮点数，占用更少的内存，存储在 CPU 上，并可被其他库（如绘图、统计等）使用。'
- en: What are the advantages of using `torch.save(model.state_dict())`?
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `torch.save(model.state_dict())` 的优势是什么？
- en: By saving only the model’s state dictionary rather than the entire model object,
    you can significantly reduce the storage space required. Also, this approach makes
    it easier to transfer models between different devices and environments, facilitating
    model deployment and sharing. Because a state dict is a dictionary, the model
    (and its intermediate layers) can be copied, updated, altered, and stored independently.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅保存模型的状态字典而不是整个模型对象，可以显著减少所需的存储空间。此外，这种方法使得在不同设备和环境之间传输模型变得更加容易，促进了模型的部署和共享。由于状态字典是一个字典，模型（及其中间层）可以独立复制、更新、修改和存储。
- en: Chapter 3, Building a Deep Neural Network with PyTorch
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章，使用 PyTorch 构建深度神经网络
- en: What happens if input values are not scaled in the input dataset?
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果输入数据集中的输入值没有进行缩放会发生什么？
- en: It takes longer to adjust weights to the optimal value because input values
    vary so widely when they are unscaled. There is a chance that the model might
    not learn at all.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入值未缩放时，调整权重到最佳值需要更长时间，因为输入值变化范围很广。有可能模型根本无法学习。
- en: What could be the issue if a background has a white pixel color while the content
    has a black pixel color when training a neural network?
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练神经网络时，背景为白色像素而内容为黑色像素时可能会出现什么问题？
- en: The mean of the data is close to 1 (1 being white and 0 being black). This may
    cause the neural network to take longer to train as a lot of neurons get activated
    initially. The network needs to learn – in the beginning stages – to ignore a
    majority of the not-so-useful content that is white in color.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的均值接近 1（1 表示白色，0 表示黑色）。这可能导致神经网络训练时间较长，因为初始阶段许多神经元被激活。网络需要在开始阶段学会忽略大部分白色内容。
- en: What is the impact of batch size on the model’s training time and memory?
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量大小对模型的训练时间和内存有什么影响？
- en: The larger the batch size, the greater the time and memory required to train
    the batch.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小越大，训练该批次所需的时间和内存就越多。
- en: What is the impact of the input value range on weight distribution at the end
    of training?
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入值范围对训练结束时权重分布的影响是什么？
- en: If the input value is not scaled to a certain range, certain weights can result
    in overfitting or cause vanishing/exploding gradients.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入值没有缩放到某个范围，某些权重可能导致过拟合或导致梯度消失/爆炸。
- en: How does batch normalization help improve accuracy?
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批量归一化如何帮助提高准确性？
- en: Just as it is important that we scale inputs for better convergence of the ANN,
    batch normalization scales activations for better convergence of its next layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们为了 ANN 更好的收敛而需要对输入进行缩放一样，批量归一化为其下一层的收敛性而缩放激活值。
- en: Why do weights behave differently during training and evaluation in the dropout
    layer?
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在 dropout 层中，权重在训练和评估时表现不同？
- en: During training, weights are scaled down by dropout, which randomly sets a fraction
    of input units to zero, aiding in preventing overfitting. During evaluation, dropout
    is typically turned off, causing the weights to behave differently as the full
    network capacity is utilized without scaling down.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，通过 dropout 缩放权重，它随机将输入单元的一部分设置为零，有助于防止过拟合。在评估期间，通常关闭 dropout，这导致权重的行为与全网络容量被利用而不是缩放下的行为不同。
- en: How do we know if a model has overfitted on training data?
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何知道模型是否在训练数据上过拟合？
- en: The validation loss will be much higher than the training loss.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 验证损失将远高于训练损失。
- en: How does regularization help in avoiding overfitting?
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化如何帮助避免过拟合？
- en: Regularization techniques help the model to train in a constrained environment,
    thereby forcing the ANN to adjust its weights in a less biased fashion.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化技术帮助模型在受限环境中训练，从而迫使ANN以较少偏向的方式调整其权重。
- en: How do L1 and L2 regularization differ from each other?
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L1和L2正则化在哪些方面有所不同？
- en: L1 is the sum of the absolute value of weights, while L2 is the sum of the square
    of weights added to the loss value and the typical loss.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: L1是权重绝对值的总和，而L2是权重平方的总和，加到损失值和典型损失中。
- en: How does dropout help in reducing overfitting?
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何通过**dropout**减少过拟合？
- en: By dropping some connections in the ANN, we force individual neurons to learn
    from less data as well as from different subsets of inputs at every iteration.
    This forces the model to learn without relying on specific neurons or a fixed
    set of connections.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在ANN中丢弃一些连接，我们迫使单个神经元在每次迭代时从更少的数据和不同的输入子集学习。这迫使模型学习而不依赖于特定的神经元或固定的连接。
- en: Chapter 4, Introducing Convolutional Neural Networks
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章，介绍卷积神经网络
- en: Why was the prediction on the translated image in the first section of the chapter
    low when using traditional neural networks?
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用传统神经网络时，在第一章节中使用翻译图像的预测为何较低？
- en: All images were centered in the original dataset, so the ANN only learned the
    task for centered images.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像都居中在原始数据集中，因此ANN只学习了对居中图像的任务。
- en: How is convolution done?
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积是如何进行的？
- en: Convolution is performed by sliding a small filter or kernel over the input
    signal, multiplying the values element-wise, and summing them up to produce an
    output value at each position. This process is repeated for every position in
    the input signal, resulting in a new output signal that represents the presence
    of patterns or features within the original input signal
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在输入信号上滑动一个小的过滤器或内核进行卷积，逐元素相乘并求和以产生每个位置的输出值。对输入信号的每个位置重复此过程，从而产生一个新的输出信号，表示原始输入信号中的模式或特征的存在。
- en: How are optimal weight values in a filter identified?
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何确定滤波器中的最佳权重值？
- en: Through backpropagation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向传播。
- en: How does the combination of convolution and pooling help in addressing the issue
    of image translation?
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积和池化的组合如何帮助解决图像翻译问题？
- en: While convolution gives important image features, pooling takes the most prominent
    features in a patch of the image. This makes pooling a robust operation; even
    if something is translated using a few pixels, pooling will still return the expected
    output.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然卷积提供重要的图像特征，但池化提取图像块中最显著的特征。这使得池化成为一种强大的操作；即使通过少量像素进行平移，池化仍会返回预期的输出。
- en: What do the convolution filters in layers close to the input learn?
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 靠近输入的卷积滤波器学习什么？
- en: Low-level features like edges.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 像边缘这样的低级特征。
- en: What functionality does pooling do that helps in building a model?
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 池化的功能有助于模型构建的哪些方面？
- en: It reduces the input size by reducing feature map size and makes model translation
    invariant.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少特征图大小来减少输入大小，并使模型具有平移不变性。
- en: Why can’t we take an input image, flatten it just like we did on the Fashion-MNIST
    dataset, and train a model for real-world images?
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们不能像对Fashion-MNIST数据集那样将输入图像展平，然后为真实世界图像训练模型？
- en: If the image size is even modestly large, the number of parameters connecting
    two layers will be in the millions, resulting in considerable compute and potentially,
    unstable training.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像尺寸稍大，连接两个层的参数数量将达到数百万，导致大量计算和潜在的不稳定训练。
- en: How does data augmentation help in improving image translation?
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据增强如何帮助提高图像翻译？
- en: Data augmentation creates copies of images that are translated by a few pixels.
    Therefore, the model is forced to learn the right classes even if the object in
    the image is off-center.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强创建了通过少量像素平移的图像副本。因此，即使图像中的对象偏离中心，模型也被迫学习正确的类别。
- en: In what scenario do we leverage `collate_fn` for dataloaders?
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在哪种情况下我们会利用`collate_fn`来处理数据加载器？
- en: When we need to perform batch-level transformations, which are difficult/slow
    to perform using `__getitem__`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要执行批级转换时，使用`__getitem__`执行起来会很困难/慢。
- en: What impact does varying the number of training data points have on the classification
    accuracy of the validation dataset?
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变化训练数据点的数量对验证数据集的分类准确性有什么影响？
- en: In general, the larger the dataset size, the better the model accuracy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，数据集大小越大，模型的准确性越高。
- en: Chapter 5, Transfer Learning for Image Classification
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章，图像分类的迁移学习
- en: What are VGG and ResNet pre-trained architectures trained on?
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VGG和ResNet预训练架构是在什么上训练的？
- en: The images in the ImageNet dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet数据集中的图像。
- en: Why does VGG11 have inferior accuracy to VGG16?
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么VGG11的准确率不如VGG16？
- en: VGG11 has fewer layers/blocks/parameters than VGG16.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: VGG11比VGG16拥有更少的层/块/参数。
- en: What does the number 11 in VGG11 represent?
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VGG11中的数字11代表什么？
- en: The 11 layer groups. Each group has two convolutional layers followed by ReLU
    and maxpool2d.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 11层组。每个组都有两个卷积层，后面是ReLU和maxpool2d。
- en: What does the term “residual” mean in “residual network”?
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “残差网络”中的“残差”一词是什么意思？
- en: The term “residual” refers to the concept of residual learning, where there
    is a shortcut connection that skips one or more layers and directly adds the input
    to the output of a subsequent layer, helping in training very deep networks.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: “残差”一词指的是残差学习的概念，其中有一条快捷连接跳过了一个或多个层，并直接将输入添加到后续层的输出中，有助于训练非常深的网络。
- en: What is the advantage of a residual network?
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差网络的优势是什么？
- en: It helps to prevent vanishing gradients and also helps to increase model depth
    without losing accuracy, by allowing gradients to directly pass through to the
    initial layers via shortcut connections.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 它有助于防止梯度消失，并且通过允许梯度直接通过快捷连接传递到初始层来增加模型深度而不丢失准确性。
- en: What are the various popular pre-trained models discussed in the book and what
    is the speciality of each network?
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 书中讨论的各种流行的预训练模型及其各自的特点是什么？
- en: AlexNet was the first successful convolutional neural network. VGG improved
    on AlexNet by making it deeper. Inception introduced an inception layer, which
    has multiple convolutional filters of different sizes and pooling operations.
    ResNet introduced skip connections, helping to create even deeper networks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet是第一个成功的卷积神经网络。VGG通过使其更深而改进了AlexNet。Inception引入了一个Inception层，其中包含多个不同大小的卷积滤波器和池化操作。ResNet引入了跳跃连接，有助于创建更深的网络。
- en: During transfer learning, why should images be normalized with the same mean
    and standard deviation as those that were used during the training of the pre-trained
    model?
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在迁移学习期间，为什么应该使用与预训练模型训练中使用的相同均值和标准差来规范化图像？
- en: Models are trained such that they expect input images to be normalized with
    a specific mean and standard deviation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是这样训练的，以期望输入图像被规范化为特定的均值和标准差。
- en: When and why do we freeze certain parameters in a model?
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型中何时以及为什么要冻结某些参数？
- en: We freeze certain parameters (typically called the backbone of the model) during
    retraining activities so that those parameters will not be updated during backpropagation.
    They need not be updated as they are already well trained, and this will also
    speed up the training time.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在重新训练活动中冻结某些参数（通常称为模型的骨干），以使这些参数在反向传播期间不会更新。它们不需要更新，因为它们已经训练得很好，这也将加快训练时间。
- en: How do we know the various modules that are present in a pre-trained model?
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何知道预训练模型中存在哪些模块？
- en: '`print(model)`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`print(model)`'
- en: How do we train a model that predicts categorical and numerical values together?
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何训练一个能够同时预测分类和数值的模型？
- en: By having multiple prediction heads and training with a separate loss for each
    head.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用多个预测头，并针对每个头部单独训练。
- en: Why might age and gender prediction code not always work for an image of your
    own if we were to execute the same code as that which we wrote in the *Implementing
    age estimation* *and* *gender classification* section?
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们执行与“实现年龄估计”和“性别分类”章节中编写的代码相同的代码，为什么年龄和性别预测代码对你自己的图像不一定总是有效？
- en: An image that does not have a similar distribution to the training data can
    give unexpected results. The image might come from a different demography/location.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图像如果与训练数据的分布不相似可能会产生意想不到的结果。图像可能来自不同的人口统计学/地理位置。
- en: How can we further improve the accuracy of the facial keypoint recognition model
    that we discussed in the *Implementing facial keypoint prediction* section?
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何进一步提高我们在“实现面部关键点预测”章节中讨论的面部关键点识别模型的准确性？
- en: We can add noise, color, and geometric augmentations to the training process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练过程中添加噪声、颜色和几何增强。
- en: Chapter 6, Practical Aspects of Image Classification
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章，图像分类的实际方面
- en: How are class activation maps obtained?
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何获取类激活映射？
- en: Refer to the eight steps provided in the *Generating CAMs* section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参考“生成CAMs”部分中提供的八个步骤。
- en: How do batch normalization and data augmentation help when training a model?
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练模型时，批量归一化和数据增强如何帮助？
- en: They help reduce overfitting. Batch normalization helps stabilize the learning
    process by normalizing the incoming data at each layer, while data augmentation
    increases the diversity of the training set.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 它们有助于减少过拟合。批量归一化通过在每一层归一化传入的数据来稳定学习过程，而数据增强则增加了训练集的多样性。
- en: What are the common reasons why a CNN model overfits?
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN模型过拟合的常见原因是什么？
- en: Too many CNN layers, no batch normalization, a lack of data augmentation, and
    a lack of dropout.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 过多的CNN层，没有批量归一化，缺乏数据增强和缺乏丢弃。
- en: What are the various scenarios where a CNN model works with training and validation
    data at the data scientists’ end but not in the real world?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据科学家端使用训练和验证数据但在真实世界中不适用的各种情况是什么？
- en: Apart from obvious scenarios such as using different model/library versions
    than were used during training, real-world data can have a different distribution
    from the data used to train and validate a model due to several factors, such
    as environment shift, sensor shift, and so on. Additionally, the model might have
    overfitted on training data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了明显的情况如使用与训练期间不同的模型/库版本外，真实世界数据由于环境变化、传感器变化等多种因素可能与用于训练和验证模型的数据分布不同。此外，模型可能已经在训练数据上过拟合。
- en: What are the various scenarios where we leverage OpenCV packages and when it
    is advantageous to use OpenCV over deep learning?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们何时利用OpenCV包，以及在何时使用OpenCV胜过深度学习的优势是什么？
- en: When working in constrained environments, where we know that the behavior of
    images has a limited scope, we prefer to use OpenCV as the time to code solutions
    is much faster. It is also preferred when speed to infer is more important in
    constrained environments.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在受限环境中工作时，我们知道图像的行为范围有限，因此我们更喜欢使用OpenCV，因为编码解决方案的时间更快。在受限环境中，速度更重要时，也更喜欢使用它。
- en: Chapter 7, Basics of Object Detection
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章，目标检测的基础知识
- en: How does the region proposal technique generate proposals?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区域提议技术如何生成提议？
- en: It identifies regions that are similar in color, texture, size, and shape.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 它识别颜色、纹理、大小和形状相似的区域。
- en: How is Intersection Over Union (IoU) calculated if there are multiple objects
    in an image?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果图像中有多个对象，如何计算交并比（IoU）？
- en: IoU is independently calculated for each object with the ground truth, using
    the IoU metric, which is a quotient where the numerator is the intersection area
    between an object and the ground truth, and the denominator is the union area
    between an object and the ground truth.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个对象与真实情况的IoU是独立计算的，使用IoU指标，其中分子是对象与真实情况之间的交集面积，分母是对象与真实情况之间的并集面积。
- en: Why is Fast R-CNN faster than R-CNN?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么Fast R-CNN比R-CNN更快？
- en: For all proposals, extracting the feature map from the VGG backbone is common.
    Reusing this feature map reduces almost 90% of the computations in Fast R-CNN
    as compared to R-CNN, which computes these features again and again for all proposals.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有提议，从VGG主干获取特征图是常见的。重复使用这个特征图减少了Fast R-CNN中的计算量，几乎比R-CNN再次为所有提议计算这些特征少了90%。
- en: How does RoI pooling work?
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoI池化如何工作？
- en: All the cropped images coming from `selectivesearch` are passed through an adaptive
    max-pooling kernel so that the final output is of the same size.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所有来自`selectivesearch`的裁剪图像都通过自适应最大池化核传递，以使最终输出的大小相同。
- en: What is the impact of not having multiple layers after obtaining a feature map
    when predicting bounding box corrections?
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预测边界框修正后从特征图获取多层次有什么影响？
- en: The model will struggle to capture complex relationships between features and
    fail to produce accurate bounding box corrections.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将难以捕捉特征之间的复杂关系，并且无法产生准确的边界框修正。
- en: Why do we have to assign a higher weightage to regression loss when calculating
    overall loss?
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当计算整体损失时，为什么我们必须为回归损失分配更高的权重？
- en: Classification loss is cross-entropy, which is generally of the order `log(n)`,
    resulting in outputs that can have a high range. However, bounding box regression
    losses are between 0 and 1\. As such, regression losses have to be scaled up.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 分类损失是交叉熵，通常为`log(n)`阶，导致输出可以有很高的范围。然而，边界框回归损失在0到1之间。因此，必须将回归损失进行放大。
- en: How does non-max suppression work?
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非最大抑制（non-max suppression）如何工作？
- en: By combining boxes of the same classes and with high IoUs, we eliminate redundant
    bounding box predictions.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合相同类别且具有高IoU的框，我们消除了冗余的边界框预测。
- en: Chapter 8, Advanced Object Detection
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章，高级物体检测
- en: Why is Faster R-CNN faster when compared to Fast R-CNN?
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Faster R-CNN相对于Fast R-CNN为何更快？
- en: We do not need to feed a lot of unnecessary proposals every time using the `selectivesearch`
    technique. Instead, Faster R-CNN automatically finds them using the region proposal
    network.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要每次使用`selectivesearch`技术喂入大量不必要的提议。相反，Faster R-CNN使用区域建议网络自动找到它们。
- en: How are YOLO and SSD faster when compared to Faster R-CNN?
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: YOLO和SSD与Faster R-CNN相比为何更快？
- en: We don’t need to rely on a new proposal network. The network directly finds
    the proposals in a single go.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要依赖新的提议网络。网络直接在一次运行中找到提议。
- en: What makes YOLO and SSD single-shot detector algorithms?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么YOLO和SSD是单次检测器算法？
- en: Networks predict all the proposals and predictions in one shot.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 网络一次性预测所有提议和预测。
- en: What is the difference between the objectness score and class score?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 物体性得分和类别得分之间有什么区别？
- en: The objectness score identifies if an object exists or not, but the class score
    predicts the class for an anchor box whose objectness is non-zero.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 物体性得分标识物体是否存在，而类别得分预测具有非零物体性的锚框的类别。
- en: Chapter 9, Image Segmentation
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章，图像分割
- en: How does upscaling help in the U-Net architecture?
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在U-Net架构中，放大（upscaling）如何帮助？
- en: Upscaling helps the feature map to increase in size so that the final output
    is the same size as the input size.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 放大（upscaling）帮助特征图增加尺寸，使得最终输出与输入尺寸相同。
- en: Why do we need to have a fully convolutional network in U-Net?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要在U-Net中使用完全卷积网络？
- en: Because both the inputs and outputs are images, it is difficult to predict an
    image-shaped tensor using the linear layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因为输入和输出都是图像，使用线性层预测图像形状的张量是困难的。
- en: How does RoI Align improve upon RoI pooling in Mask R-CNN?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoI Align如何改进Mask R-CNN中的RoI池化？
- en: RoI Align takes offsets of predicted proposals to fine-align the feature map.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: RoI Align使用预测提议的偏移量来精细对齐特征图。
- en: What is the major difference between U-Net and Mask R-CNN for segmentation?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: U-Net和Mask R-CNN在分割上的主要区别是什么？
- en: U-Net is fully convolutional and has a single end2end network, whereas Mask
    R-CNN uses mini networks, such as Backbone, RPN, etc, to do different tasks. Mask
    R-CNN is capable of identifying and separating several objects of the same type,
    but U-Net can only identify (not separate them into individual instances).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net是完全卷积的，具有单一的端到端网络，而Mask R-CNN使用Backbone、RPN等小网络来执行不同的任务。Mask R-CNN能够识别和分离多个相同类型的对象，但U-Net只能识别它们（不能将它们分隔为单独的实例）。
- en: What is instance segmentation?
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是实例分割？
- en: If there are different objects of the same class in the same image, then each
    object is called an instance. Applying image segmentation to predict, at the pixel
    level, all the instances separately is called instance segmentation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果同一图像中有不同类别的多个对象，则每个对象称为实例。将图像分割应用于分别预测所有实例的像素级别称为实例分割。
- en: Chapter 10, Applications of Object Detection and Segmentation
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章，物体检测和分割的应用
- en: Why is it important to convert datasets into a specific format for Detectron2?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集转换为Detectron2特定格式的重要性是什么？
- en: Detectron2 is a framework that can train/evaluate multiple architectures at
    the same time. To achieve such flexibility with the same code, it is important
    to create certain restrictions on the datasets so that the framework can manipulate
    the datasets. That is why it is recommended that all Detectron2 datasets be in
    the COCO format.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Detectron2是一个可以同时训练/评估多种架构的框架。为了在同一代码中实现这种灵活性，重要的是对数据集施加特定的限制，以便框架可以操作数据集。因此，建议所有Detectron2数据集都采用COCO格式。
- en: It is hard to directly perform a regression of the number of people in an image.
    What is the key insight that allowed the VGG architecture to perform crowd counting?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接对图像中的人数进行回归是困难的。VGG架构成功进行人群计数的关键见解是什么？
- en: We converted the target image into a heat map where the sum of intensity of
    all the pixels is equal to the number of people in the image.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将目标图像转换为热图，其中所有像素的强度之和等于图像中的人数。
- en: Explain self-supervision in the case of image-colorization.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释图像着色的自监督学习。
- en: We were able to create input output pairs from the given images by simply converting
    images into black and white (BW). We treated the BW images as input and the color
    images as the intended output.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够通过简单地将图像转换为黑白（BW）来创建给定图像的输入输出对。我们将BW图像视为输入，将彩色图像视为预期输出。
- en: How did we convert a 3D point cloud into an image that is compatible with YOLO?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何将3D点云转换为与YOLO兼容的图像？
- en: At every point in time, we viewed the 3D point cloud from top-down view and
    used the red channel to encode the highest point, green channel for intensity
    of the highest point, and blue channel for the density of points.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间点，我们从顶部视角查看3D点云，并使用红色通道编码最高点，绿色通道编码最高点的强度，蓝色通道编码点的密度。
- en: What is a simple way to handle videos using architectures that work only with
    images?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用仅适用于图像的架构处理视频的简单方法是什么？
- en: A simple way of doing this is to treat the frames’ dimensions as a batch dimension
    and pool all the feature vectors of all the frames to get a single feature vector.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的简单方法是将帧的维度视为批次维度，并汇总所有帧的特征向量以获得单个特征向量。
- en: Chapter 11, Autoencoders and Image Manipulation
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章，自编码器和图像操作
- en: What is the “encoder” in “autoencoder”?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “自编码器”中的“编码器”是什么？
- en: A neural network that converts an image into a vector representation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像转换为向量表示的神经网络。
- en: What loss function does an autoencoder optimize for?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器优化的损失函数是什么？
- en: The pixel-level MSE, directly comparing the prediction with the input.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级均方误差直接将预测与输入进行比较。
- en: How do autoencoders help in grouping similar images?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自编码器如何帮助将相似图像分组？
- en: Similar images will return similar encodings, which are easier to cluster.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 相似图像将返回相似的编码，更容易进行聚类。
- en: When is a convolutional autoencoder useful?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积自编码器何时有用？
- en: When the inputs are images, using a convolutional autoencoder helps in tasks
    such as image denoising, image reconstruction, anomaly detection, image data drift,
    quality control, and so on.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入为图像时，使用卷积自编码器有助于图像去噪、图像重建、异常检测、图像数据漂移、质量控制等任务。
- en: Why do we get non-intuitive images if we randomly sample from the vector space
    of embeddings obtained from a vanilla/convolutional autoencoder?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果从普通/卷积自编码器获得的嵌入向量空间中随机抽样，为什么会得到非直观的图像？
- en: The range of values in encodings is unconstrained, so proper outputs are highly
    dependent on the right range of values. Random sampling, in general, assumes a
    0 mean and 1 standard deviation, which may not be the range of good encodings.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 编码中值的范围不受限制，因此适当的输出高度依赖于正确的数值范围。一般而言，随机抽样假设平均值为0，标准偏差为1，这可能不是良好编码范围。
- en: What are the loss functions that a variational autoencoder optimizes for?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分自编码器优化的损失函数是什么？
- en: The pixel-level MSE and the KL divergence of the distribution of the mean and
    standard deviation from the encoder.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级均方误差和编码器均值和标准偏差的KL散度。
- en: How does the variational autoencoder overcome the limitation of vanilla/convolutional
    auto-encoders to generate new images?
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变分自编码器如何克服普通/卷积自编码器生成新图像的限制？
- en: By constraining predicted encodings to have a normal distribution, all encodings
    fall in the region of mean 0 and standard deviation 1, which is easy to sample
    from.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将预测的编码约束为正态分布，所有编码落入均值为0，标准偏差为1的区域，易于抽样。
- en: During an adversarial attack, why do we modify the input image pixels and not
    the weight values?
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对抗攻击期间，为什么修改输入图像像素而不是权重值？
- en: We do not have control over the neural network in adversarial attacks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗攻击中，我们无法控制神经网络。
- en: In a neural style transfer, what are the losses that we optimize for?
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经风格迁移中，我们优化哪些损失？
- en: The perceptual (VGG) loss of the generated image with the original image and
    the style loss coming from the gram matrices of the generated and style images.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像的感知（VGG）损失与原始图像的损失以及生成图像和样式图像的Gram矩阵之间的风格损失有关。
- en: Why do we consider the activation of different layers and not the original image
    when calculating style and content loss?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算风格和内容损失时，为什么考虑不同层的激活而不是原始图像？
- en: Using more intermediate layers ensures that the generated image preserves finer
    details about the image. Also, using more losses makes the gradient ascent more
    stable.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多中间层确保生成图像保留有关图像的更细节部分。此外，使用更多损失使梯度上升更稳定。
- en: Why do we consider the gram matrix loss and not the difference between images
    when calculating the style loss?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算风格损失时，为什么要考虑 gram 矩阵损失而不是图像之间的差异？
- en: The gram matrix loss gives an indication of the style of the image, i.e., how
    the textures shapes, and colors are arranged, and will ignore the actual content.
    That is why it is more convenient for style loss.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: gram 矩阵损失提供了图像风格的指示，即纹理、形状和颜色的排列方式，并忽略实际内容。这就是为什么它更适合风格损失的原因。
- en: Why do we warp images when building a model to generate deepfakes?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建用于生成深度伪造模型时为什么要扭曲图像？
- en: Warping images helps act as a regularizer. Further, it helps in generating as
    many images as required, helping in augmenting the dataset.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 扭曲图像有助于充当正则化器。此外，它有助于生成所需数量的图像，有助于增加数据集。
- en: Chapter 12, Image Generation Using GANs
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章，使用 GAN 生成图像
- en: What happens if the learning rate of generator and discriminator models is high?
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果生成器和判别器模型的学习率很高会发生什么？
- en: The model stability will be low.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的稳定性将会很低。
- en: In a scenario where the generator and discriminator are very well trained, what
    is the probability of a given image being real?
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成器和判别器训练得非常好的情况下，给定图像是真实的概率是多少？
- en: '0.5'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '0.5'
- en: Why do we use ConvTranspose2d to generate images?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们使用 ConvTranspose2d 来生成图像？
- en: We cannot upscale/generate images using a linear layer. ConvTranspose2d is a
    parametrized/neural-network-enabled way of upsampling images to a larger resolution.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能使用线性层来放大/生成图像。ConvTranspose2d 是一种通过参数化/神经网络方式将图像上采样到更大分辨率的方法。
- en: Why do we have embeddings with a higher embedding size than the number of classes
    in conditional GANs?
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在条件 GAN 中，为什么嵌入的嵌入大小比类数更高？
- en: Using more parameters gives the model more degrees of freedom to learn the important
    features of each class.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多参数使模型具有更多学习每个类别重要特征的自由度。
- en: How can we generate images of men with beards?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何生成有胡须的男性图像？
- en: By using a conditional GAN. Just as we had male and female images, we can have
    images of bearded males and other such classes while training our model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用条件 GAN。就像我们有男性和女性图像一样，我们可以在训练模型时有胡须的男性等类别的图像。
- en: Why do we have Tanh activation in the last layer in the generator and not ReLU
    or sigmoid?
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们在生成器的最后一层使用 Tanh 激活函数而不是 ReLU 或 sigmoid？
- en: The pixel range of normalized images is `[-1,1]`, and hence we use Tanh.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化图像的像素范围是 `[-1,1]`，因此我们使用 Tanh。
- en: Why did we get realistic images even though we did not denormalize the generated
    data?
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 即使我们没有对生成的数据进行反标准化，我们仍然获得了逼真的图像，为什么？
- en: Even though the pixel values were not between `[0,255]`, the relative values
    were sufficient for the `make_grid` utility to de-normalize input.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 即使像素值不在 `[0,255]` 范围内，相对值对于 `make_grid` 实用程序来说也足够进行反标准化输入。
- en: What happens if we do not crop faces corresponding to images before training
    a GAN?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在训练 GAN 前不对应于图像裁剪面部会发生什么？
- en: If there is too much background, the GAN can get wrong signals as to what is
    a face and what is not, so it might focus on generating more realistic backgrounds.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果背景太多，GAN 可能会错误地生成背景和面部的信号，因此它可能会集中于生成更逼真的背景。
- en: Why do the weights of the discriminator not get updated when the training generator
    is updated (as the `generator_train_step` function involves the discriminator
    network)?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练生成器时，为什么判别器的权重不会得到更新（因为 `generator_train_step` 函数涉及判别器网络）？
- en: It is a step-by-step process. When updating the generator, we assume the discriminator
    is able to do its best.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个逐步过程。在更新生成器时，我们假设判别器能够做到最好。
- en: Why do we fetch losses on both real and fake images while training the discriminator
    but only the loss on fake images while training the generator?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练判别器时为什么要获取真实和假图像的损失，而在训练生成器时只获取假图像的损失？
- en: Because whatever the generator creates are only fake images.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因为生成器创建的都是虚假图像。
- en: Chapter 13, Advanced GANs to Manipulate Images
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章，用于操作图像的高级 GAN
- en: Why do we need a Pix2Pix GAN if a supervised learning algorithm such as U-Net
    could have worked to generate images from contours?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果像 U-Net 这样的监督学习算法可以生成由轮廓图生成图像，为什么我们还需要 Pix2Pix GAN？
- en: U-Net only uses pixel-level loss during training. We needed Pix2Pix since there
    is no loss of realism when U-Net generates images.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net在训练期间仅使用像素级损失。在U-Net生成图像时，由于没有现实感的损失，我们需要Pix2Pix。
- en: Why do we need to optimize for three different loss functions in CycleGAN?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要优化CycleGAN中三种不同的损失函数？
- en: In CycleGAN, we optimize adversarial loss, cycle consistency loss, and identity
    loss in order to learn a more accurate mapping between two domains while preserving
    the original image’s structure and appearance. A detailed answer is provided in
    the seven points of the *How CycleGAN works* section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在CycleGAN中，我们优化对抗损失、循环一致性损失和身份损失，以便在保留原始图像结构和外观的同时学习两个域之间更准确的映射。在*CycleGAN工作原理*部分提供了详细答案。
- en: How do the tricks used by ProgressiveGAN help in building a StyleGAN model?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ProgressiveGAN使用的技巧如何帮助构建StyleGAN模型？
- en: ProgressiveGAN helps the network to learn a few upsampling layers at a time
    so that when the image has to be increased in size, the networks responsible for
    generating current-sized images are optimal.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ProgressiveGAN每次帮助网络学习几个上采样层，这样当需要增加图像尺寸时，负责生成当前尺寸图像的网络就会更加优化。
- en: How do we identify the latent vectors that correspond to a given custom image?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何识别与给定自定义图像对应的潜在向量？
- en: By adjusting the randomly generated noise in such a way that the MSE loss between
    the generated image and the image of interest is as small as possible.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整随机生成的噪声，使得生成图像与目标图像之间的均方误差尽可能小。
- en: Chapter 14, Combining Computer Vision and Reinforcement Learning
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章，结合计算机视觉和强化学习
- en: How does an agent calculate the value of a given state?
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理如何计算给定状态的值？
- en: By computing the expected reward at that state.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算在该状态下的预期奖励。
- en: How is a Q-table populated?
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q表如何填充？
- en: By computing the expected reward for each state-action pair, which is the sum
    of the immediate reward and the expected future reward. This calculation is refined
    with each episode, thereby improving the estimates every time.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算每个状态-动作对的预期奖励，即即时奖励和预期未来奖励的总和。这种计算在每个episode中都会得到改进，从而每次估计都会更加准确。
- en: Why do we have a discount factor in a state action value calculation?
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在状态动作值计算中为什么有折扣因子？
- en: Due to uncertainty, we are unsure of how the future might work. Hence, we reduce
    future rewards’ weightage, which is done by way of discounting.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不确定性，我们不确定未来可能会如何运作。因此，我们通过折扣的方式减少未来奖励的权重。
- en: Why do we need the exploration-exploitation strategy?
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要探索-利用策略？
- en: Only exploitation will make the model stagnant and predictable, and hence the
    model should be able to explore and find unseen steps that might be even more
    rewarding than what the model has already learned.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 仅利用会使模型停滞和可预测，因此模型应能够探索并找到未曾见过的步骤，这可能比模型已学到的更有回报。
- en: Why do we need to use Deep Q-Learning?
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要使用深度Q学习？
- en: We let the neural network learn the likely reward system without the need for
    costly algorithms that may take too much time or demand visibility of the entire
    environment.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让神经网络学习可能的奖励系统，而无需使用可能耗时或需要完全环境可见性的昂贵算法。
- en: How is the value of a given state-action combination calculated using Deep Q-Learning?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用深度Q学习如何计算给定状态-动作组合的值？
- en: It is simply the output of the neural network. The input is the state and the
    network predicts one expected reward for every action in the given state.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是神经网络的输出。输入是状态，网络预测给定状态中每个动作的预期奖励。
- en: Once an agent has maximized a reward in the CartPole environment, is there a
    chance that it can learn a suboptimal policy later?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦代理在CartPole环境中最大化了奖励，是否有可能以后学习到次优策略？
- en: There is a small, non-zero chance of it learning something sub-optimal if the
    neural network experiences forgetting due to bad episodes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经网络由于糟糕的episode而忘记了，有一小部分非零机会学习到一些次优内容。
- en: Chapter 15, Combining Computer Vision and NLP Techniques
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章，结合计算机视觉和自然语言处理技术
- en: What are the inputs, steps for calculation, and outputs of self-attention?
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自注意力的输入、计算步骤和输出是什么？
- en: The inputs to self-attention are a sequence of vectors. The steps involve computing
    attention scores between each pair of vectors by first converting the vectors
    into key vectors and query vectors. These two are matrix-multiplied, followed
    by a softmax function to obtain attention weights. The attention weights are then
    used to compute a weighted sum of the value vectors. The resulting context vectors
    are combined and typically passed through additional layers, such as feed-forward
    neural networks, to produce the final output.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的输入是一系列向量。步骤包括计算每对向量之间的注意力分数，首先将向量转换为键向量和查询向量。这两个向量进行矩阵乘法，然后通过 softmax 函数获得注意力权重。然后使用注意力权重计算值向量的加权和。生成的上下文向量被结合并通常通过额外的层，如前馈神经网络，产生最终的输出。
- en: How is an image transformed into a sequence input in a vision transformer?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在视觉变压器中将图像转换为序列输入？
- en: In a vision transformer, an image is first cut into a fixed-size grid of patches,
    which are sent through a Conv2D layer to turn into sequences of vectors.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉变压器中，首先将图像切割成固定大小的网格补丁，然后通过 Conv2D 层将其转换为向量序列。
- en: What are the inputs to the BERT transformer in a LayoutLM model?
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 LayoutLM 模型中，BERT 变压器的输入是什么？
- en: The text contained in the box and the 2D position of the box are the inputs
    to the model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 箱子中包含的文本和箱子的二维位置是模型的输入。
- en: What are the three objectives of BLIP?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BLIP 的三个目标是什么？
- en: Image-text matching, image-grounded text generation, and image-text contrastive
    learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图像-文本匹配、图像驱动文本生成和图像-文本对比学习。
- en: Chapter 16, Foundation Models in Computer Vision
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 16 章，计算机视觉中的基础模型
- en: How are text and images represented in the same domain using CLIP?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用 CLIP 将文本和图像表示在相同的领域中？
- en: By using contrastive loss, we force the embeddings from images and text from
    the same source to be as similar as possible while simultaneously ensuring that
    the embeddings of disparate sources are as different as possible.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用对比损失，我们强制来自相同来源的图像和文本的嵌入尽可能相似，同时确保不同来源的嵌入尽可能不同。
- en: How are different types of tokens, such as point tokens, bounding box tokens,
    and text tokens, calculated in the Segment Anything architecture?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Segment Anything 架构中，如何计算不同类型的令牌，如点令牌、边界框令牌和文本令牌？
- en: By using a prompt-encoder model that accepts points, boxes, or text, and by
    using a pre-training task of segmenting a meaningful object underneath the point,
    within the box, or as described by the text.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用一个接受点、框或文本的提示编码器模型，并使用分割意义对象的预训练任务来描述点下面的有意义对象、在框内部或由文本描述。
- en: How do diffusion models work?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩散模型如何工作？
- en: They work by gradually denoising a noisy image from full noise to partial noise
    to no noise. At every step, the model ensures that the image generated is only
    slightly better than the current image.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通过逐步去噪声图像，从完全噪声到部分噪声到无噪声来工作。在每个步骤中，模型确保生成的图像仅比当前图像稍好。
- en: What makes Stable Diffusion different from normal diffusion?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么使稳定扩散与普通扩散不同？
- en: Unlike in normal diffusion, where the model works directly on images, in Stable
    Diffusion, the whole of the denoising training occurs in a latent space that is
    encoded/decodable by a variational autoencoder, making the training significantly
    faster.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通扩散不同，在稳定扩散中，整个去噪训练发生在由变分自编码器编码/解码的潜空间中，使训练速度显著加快。
- en: What is the difference between Stable Diffusion and the SDXL model?
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 稳定扩散和 SDXL 模型之间有什么区别？
- en: SDXL has been trained on images of size 1,024, whereas the standard model works
    in the 512-pixel space.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL 已经在大小为 1,024 的图像上进行了训练，而标准模型则在 512 像素空间中工作。
- en: Chapter 17, Applications of Stable Diffusion
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 17 章，稳定扩散的应用
- en: What is the key concept behind image in-painting using Stable Diffusion?
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用稳定扩散进行图像修复的关键概念是什么？
- en: By generating latents only in the masked area and by preserving latents in the
    unmasked area, we ensure that we keep spatial consistency in the background while
    modifying the foreground to our desire.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅在掩码区域生成潜变量并在未掩码区域保留潜变量，我们确保在背景中保持空间一致性，同时根据需要修改前景。
- en: What are the key concepts behind ControlNet?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ControlNet 的关键概念是什么？
- en: There are two key concepts. First, we make a ControlNet branch by copying the
    downsampling path of the Stable Diffusion UNet2D model. All the modules in this
    branch have a zero-convolutional layer as a final layer and each module is added
    as a skip connection to the corresponding upsampling branch. Second, by training
    the new branch exclusively to accept special images such as canny, the training
    is faster.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个关键概念。首先，我们通过复制稳定扩散 UNet2D 模型的下采样路径来创建一个 ControlNet 分支。该分支中所有模块的最后一层都是零卷积层，并且每个模块都作为跳跃连接添加到相应的上采样分支。其次，通过专门训练新分支以接受诸如
    canny 之类的特殊图像，训练速度更快。
- en: What makes SDXL-Turbo faster than SDXL?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SDXL-Turbo 比 SDXL 更快的关键因素是什么？
- en: SDXL-Turbo follows a teacher-student training paradigm where the student learns
    to denoise several steps at once, making the student predict the same output as
    the teacher in fewer steps, i.e., faster.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL-Turbo 遵循一种师生训练范式，其中学生在一次去噪中学习几个步骤，使学生能够在更少的步骤中预测与老师相同的输出，即更快。
- en: What is the key concept behind DepthNet?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DepthNet 背后的关键概念是什么？
- en: Modifying the Stable Diffusion model UNet2D to accept five channels instead
    of the usual four, with the fifth channel being a depth map. This allows the prediction
    of an image that is depth-map accurate.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 修改稳定扩散模型 UNet2D，使其能接受五个通道而不是通常的四个，第五个通道是深度图。这允许准确预测深度图像。
- en: Chapter 18, Moving a Model to Production
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章，将模型移至生产环境
- en: What is the REST API and what does it do?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: REST API 是什么以及它的作用是什么？
- en: It is an interface for programs to communicate over the internet using HTTP
    methods.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个程序通过 HTTP 方法在互联网上进行通信的接口。
- en: What is Docker and why is it important for deploying deep learning applications?
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Docker 是什么，为什么它对部署深度学习应用程序如此重要？
- en: Docker is a containerization platform that allows developers to package, ship,
    and run applications in closed environments called containers. These play a handy
    role in deep learning deployments as the developer need not worry about downloading/installing
    libraries over multiple machines and can scale containers up/down depending on
    the load.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个容器化平台，允许开发者将应用程序打包、发布和在封闭环境（称为容器）中运行。在深度学习部署中，这些容器非常方便，开发者不必担心在多台机器上下载/安装库，并且可以根据负载扩展容器的规模。
- en: What is a simple and common technique for detecting unusual or novel images
    in a production setting that differ substantially from those used during training?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产环境中，检测与训练过程中明显不同的异常或新颖图像的简单常见技术是什么？
- en: We first measure the distance between the new image’s feature vectors and the
    feature vectors of training data. If this distance is too large compared to the
    distance of a sample image from validation data, we can tell if the image is unusual
    or not.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们测量新图像特征向量与训练数据特征向量之间的距离。如果这个距离与验证数据中样本图像的距离相比过大，我们可以判断图像是否异常。
- en: How can we speed up the similarity search of an image a large volume of vectors
    (millions)?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何加速图像大量向量（数百万个）的相似性搜索？
- en: By using existing libraries such as FAISS. These methods typically pre-index
    large volumes of vectors, based on techniques such as clustering, for faster retrieval
    of likely vector candidates during the similarity search.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用诸如 FAISS 等现有库。这些方法通常根据聚类等技术预先索引大量向量，以便在相似性搜索期间更快地检索可能的向量候选项。
- en: Learn more on Discord
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多信息
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
