- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Applying Machine Learning to Sentiment Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将机器学习应用于情感分析
- en: 'In the modern internet and social media age, people’s opinions, reviews, and
    recommendations have become a valuable resource for political science and businesses.
    Thanks to modern technologies, we are now able to collect and analyze such data
    most efficiently. In this chapter, we will delve into a subfield of **natural
    language processing** (**NLP**) called **sentiment analysis** and learn how to
    use machine learning algorithms to classify documents based on their sentiment:
    the attitude of the writer. In particular, we are going to work with a dataset
    of 50,000 movie reviews from the **Internet Movie Database** (**IMDb**) and build
    a predictor that can distinguish between positive and negative reviews.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代互联网和社交媒体时代，人们的意见、评论和推荐已成为政治科学和商业的宝贵资源。多亏了现代技术，我们现在能够最高效地收集和分析这些数据。在本章中，我们将深入探讨自然语言处理的一个子领域**情感分析**，并学习如何使用机器学习算法根据其情感（作者的态度）对文档进行分类。具体来说，我们将使用来自**互联网电影数据库**（**IMDb**）的50,000条电影评论数据集，并构建一个可以区分正面和负面评论的预测器。
- en: 'The topics that we will cover in this chapter include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题包括以下内容：
- en: Cleaning and preparing text data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理和准备文本数据
- en: Building feature vectors from text documents
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本文档中构建特征向量
- en: Training a machine learning model to classify positive and negative movie reviews
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个机器学习模型来分类正面和负面电影评论
- en: Working with large text datasets using out-of-core learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外存学习处理大文本数据集
- en: Inferring topics from document collections for categorization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档集合中推断主题以进行分类
- en: Preparing the IMDb movie review data for text processing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备IMDb电影评论数据进行文本处理
- en: As mentioned, sentiment analysis, sometimes also called **opinion mining**,
    is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing
    the sentiment of documents. A popular task in sentiment analysis is the classification
    of documents based on the expressed opinions or emotions of the authors with regard
    to a particular topic.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，情感分析，有时也称为**意见挖掘**，是自然语言处理的一个流行子学科；它涉及分析文档的情感。情感分析中的一个流行任务是根据作者对特定主题表达的观点或情绪对文档进行分类。
- en: 'In this chapter, we will be working with a large dataset of movie reviews from
    IMDb that has been collected by Andrew Maas and others (*Learning Word Vectors
    for Sentiment Analysis* by *A. L. Maas*, *R. E. Daly*, *P. T. Pham*, *D. Huang*,
    *A. Y. Ng*, and *C. Potts*, *Proceedings of the 49th Annual Meeting of the Association
    for Computational Linguistics: Human Language Technologies*, pages 142–150, Portland,
    Oregon, USA, Association for Computational Linguistics, June 2011). The movie
    review dataset consists of 50,000 polar movie reviews that are labeled as either
    positive or negative; here, positive means that a movie was rated with more than
    six stars on IMDb, and negative means that a movie was rated with fewer than five
    stars on IMDb. In the following sections, we will download the dataset, preprocess
    it into a useable format for machine learning tools, and extract meaningful information
    from a subset of these movie reviews to build a machine learning model that can
    predict whether a certain reviewer liked or disliked a movie.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用由Andrew Maas等人收集的IMDb电影评论大数据集（*学习情感分析的单词向量*，作者：*A. L. Maas*，*R. E.
    Daly*，*P. T. Pham*，*D. Huang*，*A. Y. Ng*和*C. Potts*，*第49届年度人类语言技术协会会议论文集：人类语言技术*，页码142-150，俄勒冈州波特兰市，美国人类语言技术协会，2011年6月）。电影评论数据集包含50,000条极性电影评论，标记为正面或负面；在此，正面意味着电影在IMDb上评分超过六星，负面意味着电影在IMDb上评分低于五星。在接下来的几节中，我们将下载数据集，预处理成适合机器学习工具使用的格式，并从这些电影评论的子集中提取有意义的信息，以构建一个可以预测某个评论者是否喜欢或不喜欢某部电影的机器学习模型。
- en: Obtaining the movie review dataset
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获得电影评论数据集
- en: 'A compressed archive of the movie review dataset (84.1 MB) can be downloaded
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    as a gzip-compressed tarball archive:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)下载电影评论数据集的压缩存档（84.1
    MB），这是一个gzip压缩的tarball存档：
- en: If you are working with Linux or macOS, you can open a new terminal window,
    `cd` into the download directory, and execute `tar -zxf aclImdb_v1.tar.gz` to
    decompress the dataset.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用 Linux 或 macOS，可以打开一个新的终端窗口，`cd` 到下载目录，并执行 `tar -zxf aclImdb_v1.tar.gz`
    解压缩数据集。
- en: If you are working with Windows, you can download a free archiver, such as 7-Zip
    ([http://www.7-zip.org](http://www.7-zip.org)), to extract the files from the
    download archive.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用 Windows，可以下载一个免费的文件解压缩程序，比如 7-Zip ([http://www.7-zip.org](http://www.7-zip.org))，以从下载存档中提取文件。
- en: 'Alternatively, you can unpack the gzip-compressed tarball archive directly
    in Python as follows:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，您可以直接在 Python 中解压缩 gzip 压缩的 tarball 存档，如下所示：
- en: '[PRE0]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preprocessing the movie dataset into a more convenient format
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将电影数据集预处理为更方便的格式
- en: Having successfully extracted the dataset, we will now assemble the individual
    text documents from the decompressed download archive into a single CSV file.
    In the following code section, we will be reading the movie reviews into a pandas
    `DataFrame` object, which can take up to 10 minutes on a standard desktop computer.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功提取数据集之后，我们现在将从解压缩的下载存档中组装单个文本文档到一个单独的 CSV 文件中。在以下代码部分中，我们将电影评论读入 pandas 的
    `DataFrame` 对象中，这可能在标准台式计算机上需要长达 10 分钟。
- en: 'To visualize the progress and estimated time until completion, we will use
    the **Python Progress Indicator** (**PyPrind**, [https://pypi.python.org/pypi/PyPrind/](https://pypi.python.org/pypi/PyPrind/))
    package, which was developed several years ago for such purposes. PyPrind can
    be installed by executing the `pip install pyprind` command:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化进度和预计完成时间，我们将使用**Python 进度指示器**（**PyPrind**，[https://pypi.python.org/pypi/PyPrind/](https://pypi.python.org/pypi/PyPrind/)）包，该包多年前为此类目的开发。PyPrind
    可通过执行 `pip install pyprind` 命令安装：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we first initialized a new progress bar object, `pbar`,
    with 50,000 iterations, which was the number of documents we were going to read
    in. Using the nested `for` loops, we iterated over the `train` and `test` subdirectories
    in the main `aclImdb` directory and read the individual text files from the `pos`
    and `neg` subdirectories that we eventually appended to the `df` pandas `DataFrame`,
    together with an integer class label (1 = positive and 0 = negative).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们首先使用 50,000 次迭代初始化了一个新的进度条对象 `pbar`，这是我们将要读取的文档数量。使用嵌套的 `for` 循环，我们迭代主
    `aclImdb` 目录中的 `train` 和 `test` 子目录，并从 `pos` 和 `neg` 子目录中读取单独的文本文件，最终将其与整数类标签（1
    = 正面，0 = 负面）一起追加到 `df` pandas 的 `DataFrame` 中。
- en: Since the class labels in the assembled dataset are sorted, we will now shuffle
    the `DataFrame` using the `permutation` function from the `np.random` submodule—this
    will be useful for splitting the dataset into training and test datasets in later
    sections, when we will stream the data from our local drive directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于组装数据集中的类标签已排序，我们现在将使用 `np.random` 子模块中的 `permutation` 函数来对 `DataFrame` 进行洗牌——这对于在后面的部分中将数据集拆分为训练集和测试集时将数据从本地驱动器直接流出非常有用。
- en: 'For our own convenience, we will also store the assembled and shuffled movie
    review dataset as a CSV file:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们还将组装和打乱的电影评论数据集存储为 CSV 文件：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since we are going to use this dataset later in this chapter, let’s quickly
    confirm that we have successfully saved the data in the right format by reading
    in the CSV and printing an excerpt of the first three examples:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在本章后面使用这个数据集，让我们快速确认我们已成功以正确格式保存数据，方法是读取 CSV 并打印前三个示例的摘录：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you are running the code examples in a Jupyter notebook, you should now
    see the first three examples of the dataset, as shown in *Figure 8.1*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Jupyter 笔记本中运行代码示例，现在应该看到数据集的前三个示例，如 *图 8.1* 所示：
- en: '![](img/B17582_08_01.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_01.png)'
- en: 'Figure 8.1: The first three rows of the movie review dataset'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：电影评论数据集的前三行
- en: 'As a sanity check, before we proceed to the next section, let’s make sure that
    the `DataFrame` contains all 50,000 rows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为健全性检查，在我们进入下一部分之前，请确保`DataFrame`包含所有 50,000 行：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Introducing the bag-of-words model
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍词袋模型
- en: 'You may remember from *Chapter 4*, *Building Good Training Datasets – Data
    Preprocessing*, that we have to convert categorical data, such as text or words,
    into a numerical form before we can pass it on to a machine learning algorithm.
    In this section, we will introduce the **bag-of-words** model, which allows us
    to represent text as numerical feature vectors. The idea behind bag-of-words is
    quite simple and can be summarized as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得*第4章*，*构建良好的训练数据集 - 数据预处理*中，我们必须将分类数据（如文本或单词）转换为数值形式，然后才能传递给机器学习算法。在本节中，我们将介绍**词袋模型**，它允许我们将文本表示为数值特征向量。词袋模型背后的思想非常简单，可以总结如下：
- en: We create a vocabulary of unique tokens—for example, words—from the entire set
    of documents.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从整个文档集中创建一个唯一标记的词汇表，例如单词。
- en: We construct a feature vector from each document that contains the counts of
    how often each word occurs in the particular document.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从每个文档中构建一个特征向量，其中包含特定文档中每个单词出现的次数。
- en: Since the unique words in each document represent only a small subset of all
    the words in the bag-of-words vocabulary, the feature vectors will mostly consist
    of zeros, which is why we call them **sparse**. Do not worry if this sounds too
    abstract; in the following subsections, we will walk through the process of creating
    a simple bag-of-words model step by step.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个文档中的唯一单词仅代表词袋词汇表中所有单词的一小部分，特征向量将主要由零组成，这就是我们称之为**稀疏**的原因。如果这听起来太抽象，请不要担心；在接下来的小节中，我们将逐步介绍创建简单词袋模型的过程。
- en: Transforming words into feature vectors
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将单词转换为特征向量
- en: 'To construct a bag-of-words model based on the word counts in the respective
    documents, we can use the `CountVectorizer` class implemented in scikit-learn.
    As you will see in the following code section, `CountVectorizer` takes an array
    of text data, which can be documents or sentences, and constructs the bag-of-words
    model for us:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要基于各个文档中的单词计数构建词袋模型，我们可以使用scikit-learn中实现的`CountVectorizer`类。如您将在以下代码部分中看到的那样，`CountVectorizer`接受一个文本数据数组，可以是文档或句子，并为我们构建词袋模型：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'By calling the `fit_transform` method on `CountVectorizer`, we constructed
    the vocabulary of the bag-of-words model and transformed the following three sentences
    into sparse feature vectors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`CountVectorizer`上调用`fit_transform`方法，我们构建了词袋模型的词汇表，并将以下三个句子转换为稀疏特征向量：
- en: '`''The sun is shining''`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''The sun is shining''`'
- en: '`''The weather is sweet''`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''The weather is sweet''`'
- en: '`''The sun is shining, the weather is sweet, and one and one is two''`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''The sun is shining, the weather is sweet, and one and one is two''`'
- en: 'Now, let’s print the contents of the vocabulary to get a better understanding
    of the underlying concepts:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印词汇表的内容，以更好地理解其中的概念：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see from executing the preceding command, the vocabulary is stored
    in a Python dictionary that maps the unique words to integer indices. Next, let’s
    print the feature vectors that we just created:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从执行前述命令中看到的那样，词汇表存储在Python字典中，将唯一单词映射到整数索引。接下来，让我们打印刚刚创建的特征向量：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Each index position in the feature vectors shown here corresponds to the integer
    values that are stored as dictionary items in the `CountVectorizer` vocabulary.
    For example, the first feature at index position `0` resembles the count of the
    word `''and''`, which only occurs in the last document, and the word `''is''`,
    at index position `1` (the second feature in the document vectors), occurs in
    all three sentences. These values in the feature vectors are also called the **raw
    term frequencies**: *tf*(*t*, *d*)—the number of times a term, *t*, occurs in
    a document, *d*. It should be noted that, in the bag-of-words model, the word
    or term order in a sentence or document does not matter. The order in which the
    term frequencies appear in the feature vector is derived from the vocabulary indices,
    which are usually assigned alphabetically.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里特征向量中的每个索引位置对应于存储在`CountVectorizer`词汇表中的整数值，这些值也称为**原始词项频率**：*tf*(*t*, *d*)
    — 术语*t*在文档*d*中出现的次数。应注意，在词袋模型中，句子或文档中术语的顺序并不重要。特征向量中术语频率出现的顺序是从词汇索引派生的，通常按字母顺序分配。
- en: '**N-gram models**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**N-gram模型**'
- en: 'The sequence of items in the bag-of-words model that we just created is also
    called the 1-gram or unigram model—each item or token in the vocabulary represents
    a single word. More generally, the contiguous sequences of items in NLP—words,
    letters, or symbols—are also called *n-grams*. The choice of the number, *n*,
    in the n-gram model depends on the particular application; for example, a study
    by Ioannis Kanaris and others revealed that n-grams of size 3 and 4 yield good
    performances in the anti-spam filtering of email messages (*Words versus character
    n-grams for anti-spam filtering* by *Ioannis Kanaris*, *Konstantinos Kanaris*,
    *Ioannis Houvardas*, and *Efstathios Stamatatos*, *International Journal on Artificial
    Intelligence Tools*, *World Scientific Publishing Company*, 16(06): 1047-1067,
    2007).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '我们刚刚创建的词袋模型中的项目序列也被称为1-gram或unigram模型，词汇表中的每个项目或标记代表一个单词。更一般地，在自然语言处理中，单词、字母或符号的连续序列也被称为*n-grams*。在n-gram模型中，数量n的选择取决于特定的应用程序；例如，Ioannis
    Kanaris等人的研究显示，大小为3和4的n-grams在电子邮件的反垃圾邮件过滤中表现良好（*Words versus character n-grams
    for anti-spam filtering*作者是*Ioannis Kanaris*, *Konstantinos Kanaris*, *Ioannis
    Houvardas*, 和 *Efstathios Stamatatos*，*International Journal on Artificial Intelligence
    Tools*, *World Scientific Publishing Company*，16(06): 1047-1067，2007年）。'
- en: 'To summarize the concept of the n-gram representation, the 1-gram and 2-gram
    representations of our first document, “the sun is shining”, would be constructed
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下n-gram表示法的概念，我们第一个文档“the sun is shining”的1-gram和2-gram表示法将如下构建：
- en: '1-gram: “the”, “sun”, “is”, “shining”'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1-gram: “the”, “sun”, “is”, “shining”'
- en: '2-gram: “the sun”, “sun is”, “is shining”'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2-gram: “the sun”, “sun is”, “is shining”'
- en: The `CountVectorizer` class in scikit-learn allows us to use different n-gram
    models via its `ngram_range` parameter. While a 1-gram representation is used
    by default, we could switch to a 2-gram representation by initializing a new `CountVectorizer`
    instance with `ngram_range=(2,2)`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，`CountVectorizer`类允许我们通过其`ngram_range`参数使用不同的n-gram模型。默认情况下使用1-gram表示，我们可以通过初始化新的`CountVectorizer`实例，并设置`ngram_range=(2,2)`切换到2-gram表示。
- en: Assessing word relevancy via term frequency-inverse document frequency
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过词项频率-逆文档频率评估词语相关性
- en: 'When we are analyzing text data, we often encounter words that occur across
    multiple documents from both classes. These frequently occurring words typically
    don’t contain useful or discriminatory information. In this subsection, you will
    learn about a useful technique called the **term frequency-inverse document frequency**
    (**tf-idf**), which can be used to downweight these frequently occurring words
    in the feature vectors. The tf-idf can be defined as the product of the term frequency
    and the inverse document frequency:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们分析文本数据时，我们经常会遇到跨越两类文档中的多个文档出现的词语。这些频繁出现的词语通常不含有有用或歧视性信息。在本小节中，您将了解一个称为**词项频率-逆文档频率**（**tf-idf**）的有用技术，可用于降低特征向量中这些频繁出现的词语的权重。tf-idf可以定义为术语频率和逆文档频率的乘积：
- en: '*tf-idf*(*t*, *d*) = *tf*(*t*, *d*) × *idf*(*t*, *d*)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*tf-idf*(*t*, *d*) = *tf*(*t*, *d*) × *idf*(*t*, *d*)'
- en: 'Here, *tf*(*t*, *d*) is the term frequency that we introduced in the previous
    section, and *idf*(*t*, *d*) is the inverse document frequency, which can be calculated
    as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*tf*(*t*, *d*)是我们在上一节介绍的词频，而*idf*(*t*, *d*)是逆文档频率，可以计算如下：
- en: '![](img/B17582_08_001.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_001.png)'
- en: Here, *n*[d] is the total number of documents, and *df*(*d*, *t*) is the number
    of documents, *d*, that contain the term *t*. Note that adding the constant 1
    to the denominator is optional and serves the purpose of assigning a non-zero
    value to terms that occur in none of the training examples; the *log* is used
    to ensure that low document frequencies are not given too much weight.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n*[d]是文档的总数，而*df*(*d*, *t*)是包含术语*t*的文档*d*的数量。请注意，将常数1添加到分母是可选的，其目的是为未出现在任何训练示例中的术语分配一个非零值；*log*用于确保低文档频率不会被赋予太大的权重。
- en: 'The scikit-learn library implements yet another transformer, the `TfidfTransformer`
    class, which takes the raw term frequencies from the `CountVectorizer` class as
    input and transforms them into tf-idfs:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库实现了另一个转换器，即`TfidfTransformer`类，它将来自`CountVectorizer`类的原始词项频率作为输入，并将其转换为tf-idfs：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you saw in the previous subsection, the word `'is'` had the largest term
    frequency in the third document, being the most frequently occurring word. However,
    after transforming the same feature vector into tf-idfs, the word `'``is'` is
    now associated with a relatively small tf-idf (0.45) in the third document, since
    it is also present in the first and second document and thus is unlikely to contain
    any useful discriminatory information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前一小节中看到的，单词 `'is'` 在第三篇文档中具有最高的词频，是出现频率最高的单词。然而，在将相同的特征向量转换为 tf-idf 后，单词
    `'is'` 现在与第三篇文档中的相对较小的 tf-idf（0.45）相关联，因为它还出现在第一篇和第二篇文档中，因此不太可能包含任何有用的区分信息。
- en: 'However, if we’d manually calculated the tf-idfs of the individual terms in
    our feature vectors, we would have noticed that `TfidfTransformer` calculates
    the tf-idfs slightly differently compared to the standard textbook equations that
    we defined previously. The equation for the inverse document frequency implemented
    in scikit-learn is computed as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们手动计算特征向量中各个术语的 tf-idf，我们会注意到 `TfidfTransformer` 计算 tf-idf 与我们之前定义的标准教科书方程式略有不同。在
    scikit-learn 中实现的逆文档频率方程式如下计算：
- en: '![](img/B17582_08_002.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_002.png)'
- en: 'Similarly, the tf-idf computed in scikit-learn deviates slightly from the default
    equation we defined earlier:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，scikit-learn 中计算的 tf-idf 稍微偏离了我们之前定义的默认方程式：
- en: '*tf-idf*(*t*, *d*) = *tf*(*t*, *d*) × (*idf*(*t*, *d*) + 1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*tf-idf*(*t*, *d*) = *tf*(*t*, *d*) × (*idf*(*t*, *d*) + 1)'
- en: Note that the “+1” in the previous idf equation is due to setting `smooth_idf=True`
    in the previous code example, which is helpful for assigning zero weight (that
    is, *idf*(*t*, *d*) = log(1) = 0) to terms that occur in all documents.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，先前 idf 方程中的“+1”是由于在前一个代码示例中设置了 `smooth_idf=True`，这对于将所有文档中出现的术语分配为零权重（即 *idf*(*t*,
    *d*) = log(1) = 0）非常有帮助。
- en: 'While it is also more typical to normalize the raw term frequencies before
    calculating the tf-idfs, the `TfidfTransformer` class normalizes the tf-idfs directly.
    By default (`norm=''l2''`), scikit-learn’s `TfidfTransformer` applies the L2-normalization,
    which returns a vector of length 1 by dividing an unnormalized feature vector,
    *v*, by its L2-norm:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在计算 tf-idf 前，通常会先对原始词频进行归一化，但 `TfidfTransformer` 类直接对 tf-idf 进行归一化处理。默认情况下（`norm='l2'`），scikit-learn
    的 `TfidfTransformer` 应用 L2 归一化，通过将未归一化的特征向量 *v* 除以其 L2 范数得到长度为 1 的向量：
- en: '![](img/B17582_08_003.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_003.png)'
- en: 'To make sure that we understand how `TfidfTransformer` works, let’s walk through
    an example and calculate the tf-idf of the word `''is''` in the third document.
    The word `''is''` has a term frequency of 3 (*tf* = 3) in the third document,
    and the document frequency of this term is 3 since the term `''is''` occurs in
    all three documents (*df* = 3). Thus, we can calculate the inverse document frequency
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们理解 `TfidfTransformer` 的工作原理，让我们通过一个示例来计算第三篇文档中单词 `'is'` 的 tf-idf。单词 `'is'`
    在第三篇文档中的词频为 3（*tf* = 3），而此单词的文档频率为 3，因为单词 `'is'` 出现在所有三篇文档中（*df* = 3）。因此，我们可以计算逆文档频率如下：
- en: '![](img/B17582_08_004.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_004.png)'
- en: 'Now, in order to calculate the tf-idf, we simply need to add 1 to the inverse
    document frequency and multiply it by the term frequency:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算 tf-idf，我们只需在逆文档频率上加 1 并乘以词频：
- en: '![](img/B17582_08_005.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_005.png)'
- en: 'If we repeated this calculation for all terms in the third document, we’d obtain
    the following tf-idf vectors: `[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69,
    1.29]`. However, notice that the values in this feature vector are different from
    the values that we obtained from `TfidfTransformer` that we used previously. The
    final step that we are missing in this tf-idf calculation is the L2-normalization,
    which can be applied as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对第三篇文档中的所有术语重复此计算，我们将获得以下 tf-idf 向量：`[3.39, 3.0, 3.39, 1.29, 1.29, 1.29,
    2.0, 1.69, 1.29]`。然而，请注意，此特征向量中的值与我们之前使用的 `TfidfTransformer` 获得的值不同。在此 tf-idf
    计算中我们缺少的最后一步是 L2 归一化，可以如下应用：
- en: '![](img/B17582_08_006.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_08_006.png)'
- en: As you can see, the results now match the results returned by scikit-learn’s
    `TfidfTransformer`, and since you now understand how tf-idfs are calculated, let’s
    proceed to the next section and apply those concepts to the movie review dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，现在的结果与 scikit-learn 的 `TfidfTransformer` 返回的结果相匹配，既然您现在理解了如何计算 tf-idf，让我们继续下一节并将这些概念应用到电影评论数据集中。
- en: Cleaning text data
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本数据清洗
- en: In the previous subsections, we learned about the bag-of-words model, term frequencies,
    and tf-idfs. However, the first important step—before we build our bag-of-words
    model—is to clean the text data by stripping it of all unwanted characters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的小节中，我们学习了词袋模型、词频和tf-idf。然而，在构建词袋模型之前的第一个重要步骤是通过去除所有不需要的字符来清理文本数据。
- en: 'To illustrate why this is important, let’s display the last 50 characters from
    the first document in the reshuffled movie review dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点的重要性，让我们显示重新排列的电影评论数据集中第一个文档的最后50个字符：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see here, the text contains HTML markup as well as punctuation and
    other non-letter characters. While HTML markup does not contain many useful semantics,
    punctuation marks can represent useful, additional information in certain NLP
    contexts. However, for simplicity, we will now remove all punctuation marks except
    for emoticon characters, such as :), since those are certainly useful for sentiment
    analysis.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在这里看到的那样，文本包含HTML标记以及标点符号和其他非字母字符。虽然HTML标记不包含许多有用的语义，但标点符号可以在某些NLP上下文中表示有用的额外信息。然而，为了简单起见，我们现在将删除除表情符号（如
    :)）之外的所有标点符号。
- en: 'To accomplish this task, we will use Python’s **regular expression** (**regex**)
    library, `re`, as shown here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这项任务，我们将使用Python的 **正则表达式** (**regex**) 库，即 `re`，如下所示：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Via the first regex, `<[^>]*>`, in the preceding code section, we tried to remove
    all of the HTML markup from the movie reviews. Although many programmers generally
    advise against the use of regex to parse HTML, this regex should be sufficient
    to *clean* this particular dataset. Since we are only interested in removing HTML
    markup and do not plan to use the HTML markup further, using regex to do the job
    should be acceptable. However, if you prefer to use sophisticated tools for removing
    HTML markup from text, you can take a look at Python’s HTML parser module, which
    is described at [https://docs.python.org/3/library/html.parser.html](https://docs.python.org/3/library/html.parser.html).
    After we removed the HTML markup, we used a slightly more complex regex to find
    emoticons, which we temporarily stored as emoticons. Next, we removed all non-word
    characters from the text via the regex `[\W]+` and converted the text into lowercase
    characters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前面代码部分的第一个正则表达式 `<[^>]*>`，我们尝试去除电影评论中的所有HTML标记。尽管许多程序员一般建议不要使用正则表达式来解析HTML，但这个正则表达式应该足以
    *清理* 这个特定的数据集。由于我们只关心去除HTML标记而不打算进一步使用HTML标记，使用正则表达式来完成工作应该是可接受的。但是，如果您更喜欢使用更复杂的工具来从文本中去除HTML标记，您可以查看Python的HTML解析器模块，该模块在
    [https://docs.python.org/3/library/html.parser.html](https://docs.python.org/3/library/html.parser.html)
    中有描述。在我们去除了HTML标记之后，我们使用稍微复杂一些的正则表达式来查找表情符号，然后将其临时存储为表情符号。接下来，我们通过正则表达式 `[\W]+`
    去除了文本中的所有非单词字符，并将文本转换为小写字符。
- en: '**Dealing with word capitalization**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理单词的大写**'
- en: In the context of this analysis, we assume that the capitalization of a word—for
    example, whether it appears at the beginning of a sentence—does not contain semantically
    relevant information. However, note that there are exceptions; for instance, we
    remove the notation of proper names. But again, in the context of this analysis,
    it is a simplifying assumption that the letter case does not contain information
    that is relevant for sentiment analysis.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个分析的上下文中，我们假设一个词的大小写——例如，它是否出现在句子的开头——不包含语义相关的信息。然而，请注意也有例外；例如，我们去除了专有名词的标注。但同样地，在这个分析的上下文中，我们做出的简化假设是字母大小写不包含对情感分析有相关性的信息。
- en: Eventually, we added the temporarily stored emoticons to the end of the processed
    document string. Additionally, we removed the *nose* character (- in :-)) from
    the emoticons for consistency.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将临时存储的表情符号添加到处理后的文档字符串的末尾。此外，我们还为了一致性从表情符号（- 在 :-)) 中删除了 *鼻子* 字符。
- en: '**Regular expressions**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则表达式**'
- en: Although regular expressions offer an efficient and convenient approach to searching
    for characters in a string, they also come with a steep learning curve. Unfortunately,
    an in-depth discussion of regular expressions is beyond the scope of this book.
    However, you can find a great tutorial on the Google Developers portal at [https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions)
    or you can check out the official documentation of Python’s `re` module at [https://docs.python.org/3.9/library/re.html](https://docs.python.org/3.9/library/re.html).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正则表达式提供了一种在字符串中搜索字符的高效和方便的方法，但它们也伴随着陡峭的学习曲线。不幸的是，深入讨论正则表达式超出了本书的范围。然而，你可以在Google开发者门户上找到一个很好的教程，网址为[https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions)，或者你可以查看Python
    `re` 模块的官方文档，网址为[https://docs.python.org/3.9/library/re.html](https://docs.python.org/3.9/library/re.html)。
- en: 'Although the addition of the emoticon characters to the end of the cleaned
    document strings may not look like the most elegant approach, we must note that
    the order of the words doesn’t matter in our bag-of-words model if our vocabulary
    consists of only one-word tokens. But before we talk more about the splitting
    of documents into individual terms, words, or tokens, let’s confirm that our `preprocessor`
    function works correctly:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在清理后的文档字符串的末尾添加表情符号字符可能看起来不是最优雅的方法，但我们必须注意，如果我们的词汇表只包含单词令牌，那么单词的顺序在我们的词袋模型中并不重要。但在我们更多地谈论如何将文档分割成单个术语、单词或令牌之前，让我们确认我们的`preprocessor`函数是否工作正常：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, since we will make use of the *cleaned* text data over and over again
    during the next sections, let’s now apply our `preprocessor` function to all the
    movie reviews in our `DataFrame`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在接下来的几节中，由于我们将反复使用*清理过的*文本数据，现在让我们将我们的`preprocessor`函数应用到我们`DataFrame`中的所有电影评论上：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Processing documents into tokens
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理文档为标记
- en: 'After successfully preparing the movie review dataset, we now need to think
    about how to split the text corpora into individual elements. One way to *tokenize*
    documents is to split them into individual words by splitting the cleaned documents
    at their whitespace characters:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功准备好电影评论数据集之后，现在我们需要考虑如何将文本语料库拆分为单独的元素。一种将文档*标记化*为单独单词的方法是通过在它们的空白字符处分割已清理的文档：
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the context of tokenization, another useful technique is **word stemming**,
    which is the process of transforming a word into its root form. It allows us to
    map related words to the same stem. The original stemming algorithm was developed
    by Martin F. Porter in 1979 and is hence known as the **Porter stemmer** algorithm
    (*An algorithm for suffix stripping* by *Martin F. Porter*, *Program: Electronic
    Library and Information Systems*, 14(3): 130–137, 1980). The **Natural Language
    Toolkit** (**NLTK**, [http://www.nltk.org](http://www.nltk.org)) for Python implements
    the Porter stemming algorithm, which we will use in the following code section.
    To install the NLTK, you can simply execute `conda install nltk` or `pip install
    nltk`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化的上下文中，另一种有用的技术是**词干化**，即将一个单词转换为其词根形式。它允许我们将相关的单词映射到相同的词干。最初的词干算法由Martin
    F. Porter在1979年开发，并因此被称为**Porter词干算法**（*由Martin F. Porter撰写的*“后缀剥离算法”*，*“程序：电子图书馆和信息系统”*，14(3)：130–137，1980）。**自然语言工具包**（**NLTK**，[http://www.nltk.org](http://www.nltk.org)）为Python实现了Porter词干算法，我们将在以下代码部分中使用它。要安装NLTK，你可以简单地执行`conda
    install nltk`或`pip install nltk`。
- en: '**NLTK online book**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**NLTK在线书籍**'
- en: Although the NLTK is not the focus of this chapter, I highly recommend that
    you visit the NLTK website as well as read the official NLTK book, which is freely
    available at [http://www.nltk.org/book/](http://www.nltk.org/book/), if you are
    interested in more advanced applications in NLP.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章的重点不在NLTK上，我强烈建议你访问NLTK网站并阅读官方的NLTK书籍，该书可以免费获取，网址为[http://www.nltk.org/book/](http://www.nltk.org/book/)，如果你对NLP的高级应用感兴趣的话。
- en: 'The following code shows how to use the Porter stemming algorithm:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何使用Porter词干算法：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Using the `PorterStemmer` from the `nltk` package, we modified our `tokenizer`
    function to reduce words to their root form, which was illustrated by the simple
    preceding example where the word `'running'` was *stemmed* to its root form `'run'`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`nltk`包中的`PorterStemmer`，我们修改了我们的`tokenizer`函数，将单词减少到它们的词根形式，这可以通过简单的前面的例子来说明，其中单词`'running'`被*词干化*为它的词根形式`'run'`。
- en: '**Stemming algorithms**'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干算法**'
- en: The Porter stemming algorithm is probably the oldest and simplest stemming algorithm.
    Other popular stemming algorithms include the newer Snowball stemmer (Porter2
    or English stemmer) and the Lancaster stemmer (Paice/Husk stemmer). While both
    the Snowball and Lancaster stemmers are faster than the original Porter stemmer,
    the Lancaster stemmer is also notorious for being more aggressive than the Porter
    stemmer, which means that it will produce shorter and more obscure words. These
    alternative stemming algorithms are also available through the NLTK package ([http://www.nltk.org/api/nltk.stem.html](http://www.nltk.org/api/nltk.stem.html)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Porter词干算法可能是最古老和最简单的词干算法。其他流行的词干算法包括更新的Snowball词干器（Porter2或英文词干器）和Lancaster词干器（Paice/Husk词干器）。虽然Snowball和Lancaster词干器比原始的Porter词干器更快，但Lancaster词干器因更为激进而出名，这意味着它将产生更短和更晦涩的单词。这些备选的词干算法也可通过NLTK包提供（[http://www.nltk.org/api/nltk.stem.html](http://www.nltk.org/api/nltk.stem.html)）。
- en: While stemming can create non-real words, such as `'thu'` (from `'thus'`), as
    shown in the previous example, a technique called *lemmatization* aims to obtain
    the canonical (grammatically correct) forms of individual words—the so-called
    *lemmas*. However, lemmatization is computationally more difficult and expensive
    compared to stemming and, in practice, it has been observed that stemming and
    lemmatization have little impact on the performance of text classification (*Influence
    of Word Normalization on Text Classification*, by *Michal Toman*, *Roman Tesar*,
    and *Karel Jezek*, *Proceedings of InSciT*, pages 354–358, 2006).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词干化可以创建非真实单词，例如从`'thus'`变成`'thu'`，如前面的例子所示，一种称为*词形还原*的技术旨在获取单词的规范（语法正确）形式——所谓的*词形*。然而，词形还原在计算上更为复杂且昂贵，与词干化相比，在实践中观察到，词干化和词形还原对文本分类的性能影响不大（*词归一化对文本分类的影响*，由*米哈尔·托曼*、*罗曼·泰萨尔*和*卡雷尔·耶日克*撰写，*InSciT会议论文集*，2006年，354–358页）。
- en: Before we jump into the next section, where we will train a machine learning
    model using the bag-of-words model, let’s briefly talk about another useful topic
    called **stop word removal**. Stop words are simply those words that are extremely
    common in all sorts of texts and probably bear no (or only a little) useful information
    that can be used to distinguish between different classes of documents. Examples
    of stop words are *is*, *and*, *has*, and *like*. Removing stop words can be useful
    if we are working with raw or normalized term frequencies rather than tf-idfs,
    which already downweight the frequently occurring words.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳转到下一节，在那里我们将使用词袋模型训练机器学习模型之前，让我们简要讨论另一个有用的主题，称为**停用词去除**。停用词简单地指那些在各种文本中极为常见且可能不包含（或只包含很少）有用信息的单词，这些单词用于区分不同类别的文档可能没有（或只有很少）有用信息。停用词的例子包括*is*、*and*、*has*和*like*。如果我们处理原始或标准化的词频而不是tf-idf时，去除停用词可能是有用的。
- en: 'To remove stop words from the movie reviews, we will use the set of 127 English
    stop words that is available from the NLTK library, which can be obtained by calling
    the `nltk.download` function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要从电影评论中删除停用词，我们将使用NLTK库中提供的127个英语停用词集合，可以通过调用`nltk.download`函数获取：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After we download the stop words set, we can load and apply the English stop
    word set as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载停用词集合后，我们可以加载并应用英语停用词集合如下：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Training a logistic regression model for document classification
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个用于文档分类的逻辑回归模型
- en: 'In this section, we will train a logistic regression model to classify the
    movie reviews into *positive* and *negative* reviews based on the bag-of-words
    model. First, we will divide the `DataFrame` of cleaned text documents into 25,000
    documents for training and 25,000 documents for testing:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练一个逻辑回归模型来根据词袋模型将电影评论分类为*正面*和*负面*评论。首先，我们将清理后的文本文档的`DataFrame`分为25,000个文档用于训练和25,000个文档用于测试：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we will use a `GridSearchCV` object to find the optimal set of parameters
    for our logistic regression model using 5-fold stratified cross-validation:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用一个`GridSearchCV`对象使用5折分层交叉验证来找到我们的逻辑回归模型的最佳参数集：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that for the logistic regression classifier, we are using the LIBLINEAR
    solver as it can perform better than the default choice (`'lbfgs'`) for relatively
    large datasets.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于逻辑回归分类器，我们使用LIBLINEAR求解器，因为它在相对较大的数据集上可能比默认选择(`'lbfgs'`)表现更好。
- en: '**Multiprocessing via the n_jobs parameter**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过n_jobs参数进行多进程处理**'
- en: Please note that we highly recommend setting `n_jobs=-1` (instead of `n_jobs=1`,
    as in the previous code example) to utilize all available cores on your machine
    and speed up the grid search. However, some Windows users reported issues when
    running the previous code with the `n_jobs=-1` setting related to pickling the
    `tokenizer` and `tokenizer_porter` functions for multiprocessing on Windows. Another
    workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`,
    with `[str.split]`. However, note that replacement by the simple `str.split` would
    not support stemming.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们强烈建议将`n_jobs=-1`（而不是`n_jobs=1`，如前面的代码示例中所示）设置为利用计算机上所有可用的核心并加快网格搜索速度。然而，一些Windows用户报告了在带有`n_jobs=-1`设置时运行先前代码时与在Windows上的多进程处理中`tokenizer`和`tokenizer_porter`函数序列化相关的问题。另一个解决方法是用`[str.split]`替换这两个函数`[tokenizer,
    tokenizer_porter]`。但是，请注意，简单的`str.split`替换不支持词干处理。
- en: When we initialized the `GridSearchCV` object and its parameter grid using the
    preceding code, we restricted ourselves to a limited number of parameter combinations,
    since the number of feature vectors, as well as the large vocabulary, can make
    the grid search computationally quite expensive. Using a standard desktop computer,
    our grid search may take 5-10 minutes to complete.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用上述代码初始化`GridSearchCV`对象及其参数网格时，由于特征向量数量以及庞大的词汇量，我们限制了参数组合的数量，这使得网格搜索在计算上相当昂贵。使用标准台式计算机，我们的网格搜索可能需要5-10分钟才能完成。
- en: 'In the previous code example, we replaced `CountVectorizer` and `TfidfTransformer`
    from the previous subsection with `TfidfVectorizer`, which combines `CountVectorizer`
    with the `TfidfTransformer`. Our `param_grid` consisted of two parameter dictionaries.
    In the first dictionary, we used `TfidfVectorizer` with its default settings (`use_idf=True`,
    `smooth_idf=True`, and `norm=''l2''`) to calculate the tf-idfs; in the second
    dictionary, we set those parameters to `use_idf=False`, `smooth_idf=False`, and
    `norm=None` in order to train a model based on raw term frequencies. Furthermore,
    for the logistic regression classifier itself, we trained models using L2 regularization
    via the penalty parameter and compared different regularization strengths by defining
    a range of values for the inverse-regularization parameter `C`. As an optional
    exercise, you are also encouraged to add L1 regularization to the parameter grid
    by changing `''clf__penalty'': [''l2'']` to `''clf__penalty'': [''l2'', ''l1'']`.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '在前一个代码示例中，我们用`TfidfVectorizer`替换了上一小节中的`CountVectorizer`和`TfidfTransformer`，`TfidfVectorizer`结合了`CountVectorizer`和`TfidfTransformer`的功能。我们的`param_grid`包含两个参数字典。在第一个字典中，我们使用了`TfidfVectorizer`的默认设置（`use_idf=True`，`smooth_idf=True`，`norm=''l2''`）来计算tf-idf；在第二个字典中，我们将这些参数设置为`use_idf=False`，`smooth_idf=False`，`norm=None`，以便基于原始词频训练模型。此外，对于逻辑回归分类器本身，我们通过惩罚参数进行了L2正则化训练模型，并通过定义逆正则化参数`C`的值范围比较不同的正则化强度。作为可选练习，您也可以通过将`''clf__penalty'':
    [''l2'']`更改为`''clf__penalty'': [''l2'', ''l1'']`，将L1正则化添加到参数网格中。'
- en: 'After the grid search has finished, we can print the best parameter set:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索完成后，我们可以打印出最佳参数集：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see in the preceding output, we obtained the best grid search results
    using the regular `tokenizer` without Porter stemming, no stop word library, and
    tf-idfs in combination with a logistic regression classifier that uses L2-regularization
    with the regularization strength `C` of `10.0`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的输出中看到的，我们使用了常规的`tokenizer`而没有使用Porter词干处理，也没有使用停用词库，而是将tf-idf与使用L2正则化和正则化强度为`10.0`的逻辑回归分类器组合起来获得了最佳的网格搜索结果。
- en: 'Using the best model from this grid search, let’s print the average 5-fold
    cross-validation accuracy scores on the training dataset and the classification
    accuracy on the test dataset:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个网格搜索中的最佳模型，让我们打印在训练数据集上的平均5折交叉验证准确率分数以及在测试数据集上的分类准确率：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The results reveal that our machine learning model can predict whether a movie
    review is positive or negative with 90 percent accuracy.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，我们的机器学习模型能够以90%的准确率预测电影评论是正面还是负面。
- en: '**The naïve Bayes classifier**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯分类器**'
- en: 'A still very popular classifier for text classification is the naïve Bayes
    classifier, which gained popularity in applications of email spam filtering. Naïve
    Bayes classifiers are easy to implement, computationally efficient, and tend to
    perform particularly well on relatively small datasets compared to other algorithms.
    Although we don’t discuss naïve Bayes classifiers in this book, the interested
    reader can find an article about naïve Bayes text classification that is freely
    available on arXiv (*Naive Bayes and Text Classification I – Introduction and
    Theory* by *S. Raschka*, *Computing Research Repository* (*CoRR*), abs/1410.5329,
    2014, [http://arxiv.org/pdf/1410.5329v3.pdf](http://arxiv.org/pdf/1410.5329v3.pdf)).
    Different versions of na**ï**ve Bayes classifiers referenced in this article are
    implemented in scikit-learn. You can find an overview page with links to the respective
    code classes here: [https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分类中仍然非常流行的分类器是朴素贝叶斯分类器，它在电子邮件垃圾邮件过滤的应用中广受欢迎。朴素贝叶斯分类器易于实现、计算效率高，并且在相对较小的数据集上表现特别好，与其他算法相比。虽然我们在本书中没有讨论朴素贝叶斯分类器，但感兴趣的读者可以在arXiv找到一篇关于朴素贝叶斯文本分类的文章（*Naive
    Bayes and Text Classification I – Introduction and Theory* by *S. Raschka*, *Computing
    Research Repository* (*CoRR*), abs/1410.5329, 2014, [http://arxiv.org/pdf/1410.5329v3.pdf](http://arxiv.org/pdf/1410.5329v3.pdf)）。本文提到的不同版本朴素贝叶斯分类器在scikit-learn中有实现。你可以在这里找到一个概述页面，其中包含到相应代码类的链接：[https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html)。
- en: Working with bigger data – online algorithms and out-of-core learning
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理更大的数据——在线算法和离线学习
- en: If you executed the code examples in the previous section, you may have noticed
    that it could be computationally quite expensive to construct the feature vectors
    for the 50,000-movie review dataset during a grid search. In many real-world applications,
    it is not uncommon to work with even larger datasets that can exceed our computer’s
    memory.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在前一节中执行了代码示例，可能会注意到在进行网格搜索时，为这个5万电影评论数据集构建特征向量可能会非常昂贵。在许多现实应用中，与超过计算机内存的更大数据集一起工作并不罕见。
- en: Since not everyone has access to supercomputer facilities, we will now apply
    a technique called **out-of-core learning**, which allows us to work with such
    large datasets by fitting the classifier incrementally on smaller batches of a
    dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于并非每个人都能使用超级计算机设施，我们现在将应用一种称为**离线学习**的技术，它允许我们通过在数据集的较小批次上逐步拟合分类器来处理这样大的数据集。
- en: '**Text classification with recurrent neural networks**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用循环神经网络进行文本分类**'
- en: In *Chapter 15*, *Modeling Sequential Data Using Recurrent Neural Networks*,
    we will revisit this dataset and train a deep learning-based classifier (a recurrent
    neural network) to classify the reviews in the IMDb movie review dataset. This
    neural network-based classifier follows the same out-of-core principle using the
    stochastic gradient descent optimization algorithm, but does not require the construction
    of a bag-of-words model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第15章*，*使用循环神经网络建模序列数据*，我们将重新访问这个数据集，并训练一个基于深度学习的分类器（循环神经网络），以对 IMDb 电影评论数据集中的评论进行分类。这个基于神经网络的分类器遵循相同的离线原则，使用随机梯度下降优化算法，但不需要构建词袋模型。
- en: Back in *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*,
    the concept of **stochastic gradient descent** was introduced; it is an optimization
    algorithm that updates the model’s weights using one example at a time. In this
    section, we will make use of the `partial_fit` function of `SGDClassifier` in
    scikit-learn to stream the documents directly from our local drive and train a
    logistic regression model using small mini-batches of documents.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾 *第2章*，*训练简单的机器学习算法进行分类*，介绍了**随机梯度下降**的概念；它是一种优化算法，通过逐个示例更新模型的权重。在本节中，我们将利用scikit-learn中`SGDClassifier`的`partial_fit`函数，直接从本地驱动器流式传输文档，并使用小批量文档训练一个逻辑回归模型。
- en: 'First, we will define a `tokenizer` function that cleans the unprocessed text
    data from the `movie_data.csv` file that we constructed at the beginning of this
    chapter and separates it into word tokens while removing stop words:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个`tokenizer`函数，清理本章开头构建的`movie_data.csv`文件中的未加工文本数据，并将其分割成单词标记，同时去除停用词：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we will define a generator function, `stream_docs`, that reads in and
    returns one document at a time:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个生成器函数，`stream_docs`，逐个读取并返回文档：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To verify that our `stream_docs` function works correctly, let’s read in the
    first document from the `movie_data.csv` file, which should return a tuple consisting
    of the review text as well as the corresponding class label:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的 `stream_docs` 函数是否正常工作，让我们从 `movie_data.csv` 文件中读取第一个文档，这应该返回一个由评论文本和相应类标签组成的元组：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will now define a function, `get_minibatch`, that will take a document stream
    from the `stream_docs` function and return a particular number of documents specified
    by the `size` parameter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个名为 `get_minibatch` 的函数，该函数将从 `stream_docs` 函数中获取文档流，并返回由 `size` 参数指定的特定数量的文档：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Unfortunately, we can’t use `CountVectorizer` for out-of-core learning since
    it requires holding the complete vocabulary in memory. Also, `TfidfVectorizer`
    needs to keep all the feature vectors of the training dataset in memory to calculate
    the inverse document frequencies. However, another useful vectorizer for text
    processing implemented in scikit-learn is `HashingVectorizer`. `HashingVectorizer`
    is data-independent and makes use of the hashing trick via the 32-bit `MurmurHash3`
    function by Austin Appleby (you can find more information about MurmurHash at
    [https://en.wikipedia.org/wiki/MurmurHash](https://en.wikipedia.org/wiki/MurmurHash)):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们不能对离线学习使用 `CountVectorizer`，因为它要求将完整的词汇表保存在内存中。此外，`TfidfVectorizer` 需要在内存中保存训练数据集的所有特征向量以计算逆文档频率。然而，在
    scikit-learn 中实现的另一个用于文本处理的有用的向量化器是 `HashingVectorizer`。`HashingVectorizer` 是数据独立的，并通过
    Austin Appleby 的 32 位 `MurmurHash3` 函数使用哈希技巧（有关 MurmurHash 的更多信息，请参阅 [https://en.wikipedia.org/wiki/MurmurHash](https://en.wikipedia.org/wiki/MurmurHash)）：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Using the preceding code, we initialized `HashingVectorizer` with our `tokenizer`
    function and set the number of features to `2**21`. Furthermore, we reinitialized
    a logistic regression classifier by setting the `loss` parameter of `SGDClassifier`
    to `'log'`. Note that by choosing a large number of features in `HashingVectorizer`,
    we reduce the chance of causing hash collisions, but we also increase the number
    of coefficients in our logistic regression model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们使用我们的 `tokenizer` 函数初始化了 `HashingVectorizer`，并将特征数设置为 `2**21`。此外，我们通过将
    `SGDClassifier` 的 `loss` 参数设置为 `'log'` 重新初始化了逻辑回归分类器。请注意，通过选择大量特征数在 `HashingVectorizer`
    中，我们减少了发生哈希碰撞的机会，但也增加了逻辑回归模型中的系数数量。
- en: 'Now comes the really interesting part—having set up all the complementary functions,
    we can start the out-of-core learning using the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是真正有趣的部分——在设置好所有补充函数之后，我们可以使用以下代码开始离线学习：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Again, we made use of the PyPrind package to estimate the progress of our learning
    algorithm. We initialized the progress bar object with 45 iterations and, in the
    following `for` loop, we iterated over 45 mini-batches of documents where each
    mini-batch consists of 1,000 documents. Having completed the incremental learning
    process, we will use the last 5,000 documents to evaluate the performance of our
    model:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们使用了 PyPrind 包来估算我们学习算法的进展。我们初始化了一个进度条对象，包含 45 次迭代，在接下来的 `for` 循环中，我们迭代了
    45 个文档的小批次，每个小批次包含 1,000 份文档。完成增量学习过程后，我们将使用最后的 5,000 份文档来评估模型的性能：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**NoneType error**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**NoneType 错误**'
- en: 'Please note that if you encounter a `NoneType` error, you may have executed
    the `X_test, y_test = get_minibatch(...)` code twice. Via the previous loop, we
    have 45 iterations where we fetch 1,000 documents each. Hence, there are exactly
    5,000 documents left for testing, which we assign via:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果遇到 `NoneType` 错误，可能是因为执行了两次 `X_test, y_test = get_minibatch(...)` 代码。通过前面的循环，我们有
    45 次迭代，每次获取 1,000 份文档。因此，还剩下确切的 5,000 份文档用于测试，我们通过以下方式分配：
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If we execute this code twice, then there are not enough documents left in the
    generator, and `X_test` returns `None`. Hence, if you encounter the `NoneType`
    error, you have to start at the previous `stream_docs(...)` code again.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们执行此代码两次，则生成器中将没有足够的文档，`X_test` 返回 `None`。因此，如果遇到 `NoneType` 错误，则必须重新开始前面的
    `stream_docs(...)` 代码。
- en: As you can see, the accuracy of the model is approximately 87 percent, slightly
    below the accuracy that we achieved in the previous section using the grid search
    for hyperparameter tuning. However, out-of-core learning is very memory efficient,
    and it took less than a minute to complete.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，模型的准确率约为 87%，略低于我们在前一节使用网格搜索进行超参数调整时达到的准确率。然而，离线学习非常节省内存，完成时间不到一分钟。
- en: 'Finally, we can use the last 5,000 documents to update our model:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用最后的 5,000 份文档来更新我们的模型：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**The word2vec model**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**word2vec模型**'
- en: A more modern alternative to the bag-of-words model is word2vec, an algorithm
    that Google released in 2013 (*Efficient Estimation of Word Representations in
    Vector Space* by *T. Mikolov*, *K. Chen*, *G. Corrado*, and *J. Dean*, [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现代化的替代词袋模型的选择是word2vec，这是谷歌于2013年发布的一种算法（*在向量空间中高效估计单词表示*，由*T. Mikolov*，*K.
    Chen*，*G. Corrado*和*J. Dean*撰写，[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)）。
- en: The word2vec algorithm is an unsupervised learning algorithm based on neural
    networks that attempts to automatically learn the relationship between words.
    The idea behind word2vec is to put words that have similar meanings into similar
    clusters, and via clever vector spacing, the model can reproduce certain words
    using simple vector math, for example, *king* – *man* + *woman* = *queen*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec算法是基于神经网络的无监督学习算法，旨在自动学习单词之间的关系。word2vec背后的想法是将意思相似的单词放入相似的集群中，并通过巧妙的向量间距，模型可以使用简单的向量数学重现某些单词，例如*king* – *man* + *woman* = *queen*。
- en: The original C-implementation with useful links to the relevant papers and alternative
    implementations can be found at [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在[https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)找到具有相关论文和替代实现的原始C实现。
- en: Topic modeling with latent Dirichlet allocation
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用潜在狄利克雷分配进行主题建模
- en: '**Topic modeling** describes the broad task of assigning topics to unlabeled
    text documents. For example, a typical application is the categorization of documents
    in a large text corpus of newspaper articles. In applications of topic modeling,
    we then aim to assign category labels to those articles, for example, sports,
    finance, world news, politics, and local news. Thus, in the context of the broad
    categories of machine learning that we discussed in *Chapter 1*, *Giving Computers
    the Ability to Learn from Data*, we can consider topic modeling as a clustering
    task, a subcategory of unsupervised learning.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题建模**描述了为未标记的文本文档分配主题的广泛任务。例如，在大量报纸文章的文本语料库中，典型的应用是对文档进行分类。在主题建模的应用中，我们的目标是为这些文章分配类别标签，例如体育、财经、世界新闻、政治和地方新闻。因此，在我们讨论的机器学习的广泛类别的背景下（*第1章*，*使计算机能够从数据中学习*），我们可以将主题建模视为一项聚类任务，无监督学习的一个子类别。'
- en: In this section, we will discuss a popular technique for topic modeling called
    **latent Dirichlet allocation** (**LDA**). However, note that while latent Dirichlet
    allocation is often abbreviated as LDA, it is not to be confused with *linear
    discriminant analysis*, a supervised dimensionality reduction technique that was
    introduced in *Chapter 5*, *Compressing Data via Dimensionality Reduction*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一种称为**潜在狄利克雷分配**（**LDA**）的流行主题建模技术。然而，请注意，虽然潜在狄利克雷分配经常缩写为LDA，但不要与*线性判别分析*混淆，后者是一种监督的降维技术，介绍在*第5章*，*通过降维压缩数据*中。
- en: Decomposing text documents with LDA
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LDA分解文本文档
- en: 'Since the mathematics behind LDA is quite involved and requires knowledge of
    Bayesian inference, we will approach this topic from a practitioner’s perspective
    and interpret LDA using layman’s terms. However, the interested reader can read
    more about LDA in the following research paper: *Latent Dirichlet Allocation*,
    by *David M. Blei*, *Andrew Y. Ng*, and *Michael I. Jordan*, *Journal of Machine
    Learning Research 3*, pages: 993-1022, Jan 2003, [https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LDA背后的数学内容相当复杂，并且需要贝叶斯推断的知识，我们将从实践者的角度来讨论这个话题，并用通俗的术语解释LDA。然而，有兴趣的读者可以在以下研究论文中进一步阅读关于LDA的信息：*潜在狄利克雷分配*，由*David
    M. Blei*，*Andrew Y. Ng*和*Michael I. Jordan*撰写，*机器学习研究杂志第3卷*，页码：993-1022，2003年1月，[https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)。
- en: LDA is a generative probabilistic model that tries to find groups of words that
    appear frequently together across different documents. These frequently appearing
    words represent our topics, assuming that each document is a mixture of different
    words. The input to an LDA is the bag-of-words model that we discussed earlier
    in this chapter.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种生成概率模型，试图找到在不同文档中经常一起出现的单词组。这些经常出现的单词代表我们的主题，假设每个文档是不同单词的混合物。LDA的输入是我们前面在本章讨论的词袋模型。
- en: 'Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个词袋矩阵作为输入，LDA将其分解为两个新矩阵：
- en: A document-to-topic matrix
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档-主题矩阵
- en: A word-to-topic matrix
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词-主题矩阵
- en: LDA decomposes the bag-of-words matrix in such a way that if we multiply those
    two matrices together, we will be able to reproduce the input, the bag-of-words
    matrix, with the lowest possible error. In practice, we are interested in those
    topics that LDA found in the bag-of-words matrix. The only downside may be that
    we must define the number of topics beforehand—the number of topics is a hyperparameter
    of LDA that has to be specified manually.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: LDA以这样的方式分解词袋矩阵，即如果我们将这两个矩阵相乘，我们将能够以最低可能的误差重新生成输入，即词袋矩阵。实际上，我们对LDA在词袋矩阵中找到的主题感兴趣。唯一的缺点可能是我们必须事先定义主题的数量
    - 主题的数量是LDA的超参数，必须手动指定。
- en: LDA with scikit-learn
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn的LDA
- en: In this subsection, we will use the `LatentDirichletAllocation` class implemented
    in scikit-learn to decompose the movie review dataset and categorize it into different
    topics. In the following example, we will restrict the analysis to 10 different
    topics, but readers are encouraged to experiment with the hyperparameters of the
    algorithm to further explore the topics that can be found in this dataset.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将使用在scikit-learn中实现的`LatentDirichletAllocation`类来分解电影评论数据集，并将其分类到不同的主题中。在下面的示例中，我们将分析限制在10个不同的主题，但鼓励读者调整算法的超参数以进一步探索此数据集中可以找到的主题。
- en: 'First, we are going to load the dataset into a pandas `DataFrame` using the
    local `movie_data.csv` file of the movie reviews that we created at the beginning
    of this chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用本章开头创建的电影评论数据集中的本地`movie_data.csv`文件将数据加载到pandas的`DataFrame`中：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we are going to use the already familiar `CountVectorizer` to create the
    bag-of-words matrix as input to the LDA.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用已经熟悉的`CountVectorizer`创建词袋矩阵作为LDA的输入。
- en: 'For convenience, we will use scikit-learn’s built-in English stop word library
    via `stop_words=''english''`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们将使用scikit-learn的内置英语停用词库，通过`stop_words='english'`：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice that we set the maximum document frequency of words to be considered
    to 10 percent (`max_df=.1`) to exclude words that occur too frequently across
    documents. The rationale behind the removal of frequently occurring words is that
    these might be common words appearing across all documents that are, therefore,
    less likely to be associated with a specific topic category of a given document.
    Also, we limited the number of words to be considered to the most frequently occurring
    5,000 words (`max_features=5000`), to limit the dimensionality of this dataset
    to improve the inference performed by LDA. However, both `max_df=.1` and `max_features=5000`
    are hyperparameter values chosen arbitrarily, and readers are encouraged to tune
    them while comparing the results.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将要考虑的单词的最大文档频率设置为10%（`max_df=.1`），以排除在文档中出现太频繁的单词。移除频繁出现的单词的背后理由是这些单词可能是所有文档中常见的单词，因此不太可能与给定文档的特定主题类别相关联。此外，我们将要考虑的单词数量限制为最常出现的5,000个单词（`max_features=5000`），以限制此数据集的维度，以改善LDA执行的推断。但是，`max_df=.1`和`max_features=5000`都是任意选择的超参数值，鼓励读者在比较结果时进行调整。
- en: 'The following code example demonstrates how to fit a `LatentDirichletAllocation`
    estimator to the bag-of-words matrix and infer the 10 different topics from the
    documents (note that the model fitting can take up to 5 minutes or more on a laptop
    or standard desktop computer):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例演示了如何将`LatentDirichletAllocation`估计器拟合到词袋矩阵，并从文档中推断出10个不同的主题（请注意，模型拟合可能需要长达5分钟或更长时间，在笔记本电脑或标准台式计算机上）：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: By setting `learning_method='batch'`, we let the `lda` estimator do its estimation
    based on all available training data (the bag-of-words matrix) in one iteration,
    which is slower than the alternative `'online'` learning method, but can lead
    to more accurate results (setting `learning_method='online'` is analogous to online
    or mini-batch learning, which we discussed in *Chapter 2*, *Training Simple Machine
    Learning Algorithms for Classification*, and previously in this chapter).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`learning_method='batch'`，我们让`lda`估计器在一次迭代中基于所有可用的训练数据（词袋矩阵）进行估计，这比替代的`'online'`学习方法慢，但可以导致更精确的结果（设置`learning_method='online'`类似于在线或小批量学习，在*第2章*，“用于分类的简单机器学习算法的训练”以及本章前面我们讨论过）。
- en: '**Expectation-maximization**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望最大化**'
- en: 'The scikit-learn library’s implementation of LDA uses the **expectation-maximization**
    (**EM**) algorithm to update its parameter estimates iteratively. We haven’t discussed
    the EM algorithm in this chapter, but if you are curious to learn more, please
    see the excellent overview on Wikipedia ([https://en.wikipedia.org/wiki/Expectation–maximization_algorithm](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm))
    and the detailed tutorial on how it is used in LDA in Colorado Reed’s tutorial,
    *Latent Dirichlet Allocation: Towards a Deeper Understanding*, which is freely
    available at [http://obphio.us/pdfs/lda_tutorial.pdf](http://obphio.us/pdfs/lda_tutorial.pdf).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 库对 LDA 的实现使用**期望最大化**（**EM**）算法来迭代更新其参数估计。我们在本章中没有讨论 EM 算法，但如果你想了解更多，请查看维基百科上关于期望最大化算法的优秀概述（[https://en.wikipedia.org/wiki/Expectation–maximization_algorithm](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm)）以及
    Colorado Reed 的教程 *潜在狄利克雷分配：迈向更深层理解* 的详细教程，该教程可以在 [http://obphio.us/pdfs/lda_tutorial.pdf](http://obphio.us/pdfs/lda_tutorial.pdf)
    免费获取。
- en: 'After fitting the LDA, we now have access to the `components_` attribute of
    the `lda` instance, which stores a matrix containing the word importance (here,
    `5000`) for each of the 10 topics in increasing order:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合 LDA 后，我们现在可以访问 `lda` 实例的 `components_` 属性，该属性存储了一个矩阵，按增加顺序包含了 10 个主题的单词重要性（此处为
    `5000`）：
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To analyze the results, let’s print the five most important words for each
    of the 10 topics. Note that the word importance values are ranked in increasing
    order. Thus, to print the top five words, we need to sort the topic array in reverse
    order:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要分析结果，让我们打印出每个 10 个主题中最重要的五个单词。请注意，单词重要性值是按增加顺序排名的。因此，要打印出前五个单词，我们需要将主题数组按相反顺序排序：
- en: '[PRE34]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Based on reading the five most important words for each topic, you may guess
    that the LDA identified the following topics:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每个主题的前五个最重要单词的阅读，您可以猜测 LDA 识别了以下主题：
- en: Generally bad movies (not really a topic category)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常的糟糕电影（不真正是一个主题类别）
- en: Movies about families
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 家庭题材电影
- en: War movies
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 战争电影
- en: Art movies
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 艺术电影
- en: Crime movies
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 犯罪电影
- en: Horror movies
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 恐怖电影
- en: Comedy movie reviews
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 喜剧电影评论
- en: Movies somehow related to TV shows
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与电视节目有关的电影
- en: Movies based on books
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于书籍改编的电影
- en: Action movies
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作电影
- en: 'To confirm that the categories make sense based on the reviews, let’s plot
    three movies from the horror movie category (horror movies belong to category
    6 at index position `5`):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认基于评论的分类是否合理，请绘制恐怖电影类别（恐怖电影属于索引位置 `5` 的第 6 类别）中的三部电影：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Using the preceding code example, we printed the first 300 characters from
    the top three horror movies. The reviews—even though we don’t know which exact
    movie they belong to—sound like reviews of horror movies (however, one might argue
    that `Horror movie #2` could also be a good fit for topic category 1: *Generally
    bad movies*).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '使用上述代码示例，我们打印了前三部恐怖电影的前 300 个字符。尽管我们不知道它们确切属于哪部电影，但听起来像是恐怖电影的评论（但是，有人可能会认为
    `恐怖电影 #2` 也可以很好地适应主题类别 1：*通常的糟糕电影*）。'
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to use machine learning algorithms to classify
    text documents based on their polarity, which is a basic task in sentiment analysis
    in the field of NLP. Not only did you learn how to encode a document as a feature
    vector using the bag-of-words model, but you also learned how to weight the term
    frequency by relevance using tf-idf.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用机器学习算法根据其极性对文本文档进行分类，这是自然语言处理领域情感分析的基本任务。你不仅学会了如何使用词袋模型将文档编码为特征向量，还学会了如何使用
    tf-idf 权重术语频率。
- en: Working with text data can be computationally quite expensive due to the large
    feature vectors that are created during this process; in the last section, we
    covered how to utilize out-of-core or incremental learning to train a machine
    learning algorithm without loading the whole dataset into a computer’s memory.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 处理文本数据可能会因为创建的大型特征向量而在计算上非常昂贵；在最后一节中，我们讨论了如何利用离线或增量学习来训练机器学习算法，而无需将整个数据集加载到计算机内存中。
- en: Lastly, you were introduced to the concept of topic modeling using LDA to categorize
    the movie reviews into different categories in an unsupervised fashion.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你被介绍了使用 LDA 进行主题建模的概念，以无监督方式将电影评论分类到不同的类别中。
- en: So far, in this book, we have covered many machine learning concepts, best practices,
    and supervised models for classification. In the next chapter, we will look at
    another subcategory of supervised learning, *regression analysis*, which lets
    us predict outcome variables on a continuous scale, in contrast to the categorical
    class labels of the classification models that we have been working with so far.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们已经涵盖了许多机器学习概念、最佳实践以及用于分类的监督模型。在下一章中，我们将研究另一类监督学习的子类别，*回归分析*，它让我们能够预测连续尺度上的结果变量，与我们目前所使用的分类模型的分类类别标签形成对比。
- en: Join our book’s Discord space
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的Discord工作空间，与作者进行每月的*问我任何事*会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
