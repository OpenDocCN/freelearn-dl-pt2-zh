- en: '*Chaper 4*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第四章*'
- en: Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够：
- en: Explain the training process of convolutional neural networks (CNNs)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释卷积神经网络（CNN）的训练过程
- en: Perform data augmentation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行数据增强
- en: Apply batch normalization to a CNN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对CNN应用批归一化
- en: Solve an image classification problem using a CNN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN解决图像分类问题
- en: In this chapter, you'll be introduced to CNN. You'll learn concepts such as
    convolutions, pooling, padding, and stride.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将被介绍CNN。您将学习卷积、池化、填充和步幅等概念。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Even though all neural network domains are popular nowadays, CNNs are probably
    the most popular of all neural network architectures. This is mainly because,
    although they work in many domains, they are particularly good at dealing with
    images, and advances in technology have allowed the collection of large amounts
    of images to be possible in order to tackle a great variety of today's challenges.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管当前神经网络领域都很受欢迎，但CNN可能是所有神经网络架构中最受欢迎的。这主要是因为尽管它们在许多领域都适用，但它们在处理图像时特别出色，技术的进步使得收集大量图像来解决当今各种挑战成为可能。
- en: From image classification to object detection, CNNs are being used to diagnose
    cancer patients and detect fraud in systems, as well as to construct well thought-out
    self-driving vehicles that will revolutionize the future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像分类到对象检测，CNN被用于诊断癌症患者、检测系统中的欺诈行为，以及构建深思熟虑的自动驾驶车辆，将彻底改变未来。
- en: This chapter will focus on explaining the reasons why CNNs outperform other
    architectures when dealing with images, as well as explaining the building blocks
    of their architecture in greater detail. It will cover the main coding structure
    for building a CNN to solve an image classification data problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点解释卷积神经网络（CNN）在处理图像时优于其他架构的原因，以及更详细地解释它们的架构构建模块。它将涵盖构建CNN解决图像分类数据问题的主要编码结构。
- en: Moreover, it will explore the concepts of data augmentation and batch normalization,
    which will be used to improve the performance of the model. The ultimate goal
    of this chapter will be to compare the results of three different approaches to
    tackling an image classification problem using CNNs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章将探讨数据增强和批归一化的概念，这些将用于改善模型的性能。本章的最终目标是比较使用CNN解决图像分类问题的三种不同方法的结果。
- en: Note
  id: totrans-14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As a reminder, the GitHub repository containing all the code used in this chapter
    can be found at [https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，本章中使用的所有代码的GitHub仓库可以在[https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch)找到。
- en: Building a CNN
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建CNN
- en: It is widely known that CNNs are the way to go when dealing with an image data
    problem. However, they are often underused, as they are typically known for image
    classification alone, when the reality is that their abilities extend to further
    domains in regard to images. This chapter will not only explain the reasons why
    CNNs are so good at understanding images, but will also identify the different
    tasks that can be tackled, as well as give some examples of real-life applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，处理图像数据问题时CNN是首选。然而，它们通常被低估，因为它们通常只被认为适用于图像分类，而事实上它们在处理图像方面的能力扩展到了更多领域。本章不仅将解释CNN在理解图像方面的优势，还将识别可以处理的不同任务，并给出一些现实生活中应用的示例。
- en: Moreover, this chapter will explore the different building blocks of CNNs and
    their application using PyTorch to ultimately build a model that solves a data
    problem using one of PyTorch's datasets for image classification.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章将探索CNN的不同构建模块及其在PyTorch中的应用，最终构建一个使用PyTorch图像分类数据集解决数据问题的模型。
- en: Why CNNs?
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么选择CNN？
- en: 'An image is a matrix of pixels, so you might ask why we don''t just flatten
    the matrix into a vector and process it using a traditional neural network architecture?
    The answer is that, even with the simplest image, there are some pixel dependencies
    that alter the meaning of the image. For instance, the representation of a cat''s
    eye, a car tire, or even the edge of an object is constructed of several pixels
    laid out in a certain way. By flattening the image, these dependencies are lost
    and so is the accuracy of a traditional model:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图像是像素矩阵，因此你可能会问，为什么我们不将矩阵展平成向量，然后使用传统的神经网络架构进行处理呢？答案是，即使是最简单的图像，也存在一些像素依赖关系会改变图像的含义。例如，猫眼的表现、车轮的轮胎，甚至是物体的边缘都是由几个以特定方式布置的像素构成的。展平图像会导致这些依赖关系丢失，传统模型的准确性也会因此丧失：
- en: '![](img/C11865_04_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C11865_04_01.jpg)'
- en: 'Figure 4.1: Representation of a flattened matrix'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.1：展平矩阵的表示
- en: On the other hand, a CNN is capable of capturing the spatial dependencies of
    images, as it processes them as matrices and analyzes entire chunks of an image
    at a time, depending on the size of the filter. For example, a convolutional layer
    using a filter of size 3x3, will analyze 9 pixels at a time until it has covered
    the entire image.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，CNN 能够捕捉图像的空间依赖关系，因为它将图像处理为矩阵并一次性分析整个图像的区块，这取决于滤波器的大小。例如，使用大小为 3x3 的卷积层将一次性分析9个像素，直到覆盖整个图像。
- en: Each chunk of the image is given a set of parameters (weight and bias) that
    will refer to the relevance of that set of pixels to the entire image, depending
    on the filter at hand. This means that a vertical edge filter will assign greater
    weights to the chunks of the image that contain a vertical edge. According to
    this, by reducing the number of parameters and by analyzing the image in chunks,
    CNNs are capable of rendering a better representation of the image.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的每个区块都被赋予一组参数（权重和偏差），这些参数将根据手头的滤波器确定该像素区块与整个图像的相关性。这意味着垂直边缘滤波器将赋予包含垂直边缘的图像区块更大的权重。因此，通过减少参数数量并分析图像的区块，CNN
    能够更好地呈现图像的表现。
- en: The Inputs
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入
- en: As mentioned before, the typical inputs of a CNN are images in the form of matrices.
    Each value of the matrix represents a pixel in the image, where the number is
    determined by the intensity of the color, with values ranging from 0 to 255.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，CNN 的典型输入是以矩阵形式表示的图像。矩阵中的每个值代表图像中的一个像素，其数值由颜色的强度确定，取值范围从 0 到 255。
- en: In gray-scaled images, white pixels are represented by the number 255 and black
    pixels by the number 0\. Gray pixels are any number in between, depending on the
    intensity of the color; the lighter the gray, the closer the number is to 255.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在灰度图像中，白色像素由数字255表示，黑色像素由数字0表示。灰色像素是介于两者之间的任意数字，取决于颜色的强度；灰色越浅，数字越接近255。
- en: Colored images are usually represented using the RGB system, which represents
    each color as the combination of red, green, and blue. Here, each pixel will have
    three dimensions, one for each color. The values in each dimension will range
    from 0 to 255\. Here, the more intense the color, the closer the number to 255.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像通常使用RGB系统表示，其中每种颜色表示为红、绿和蓝的组合。每个像素将具有三个维度，每个颜色一个维度。每个维度的值范围从0到255。颜色越浓，数字越接近255。
- en: According to the preceding paragraph, the matrix of a given image is three-dimensional,
    where the first dimension refers to the height of the image (in the number of
    pixels), the second dimension refers to the width of the image (in the number
    of pixels), and the third dimension is known as the channel and refers to the
    color scheme of the image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的段落，给定图像的矩阵是三维的，其中第一维度表示图像的高度（以像素数表示），第二维度表示图像的宽度（以像素数表示），第三维度称为通道，表示图像的颜色方案。
- en: 'The channel for colored images is three (one channel for each color in the
    RGB system). On the other hand, grey-scaled images only have one channel:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像的通道数为三（RGB 系统中每种颜色一个通道）。而灰度图像只有一个通道：
- en: '![Figure 4.2: Matrix representation of an image. To the left, a colored image.
    To the right, a grey-scaled image.](img/C11865_04_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2：图像的矩阵表示。左边是彩色图像，右边是灰度图像。](img/C11865_04_02.jpg)'
- en: 'Figure 4.2: Matrix representation of an image. To the left, a colored image.
    To the right, a grey-scaled image.'
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.2：图像的矩阵表示。左边是彩色图像，右边是灰度图像。
- en: Different to text data, images fed into CNNs do not require much preprocessing.
    Images are usually fed as they are, with the only changes being that values are
    normalized to speed up the learning process and improve performance, and that
    images can be downsized as a good practice, considering that CNN models are usually
    built using smaller images, which also helps to speed up the learning process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本数据不同，输入到CNN中的图像不需要太多预处理。 图像通常按原样输入，唯一的变化是将值标准化以加快学习过程并提高性能，并且作为良好实践，可以将图像缩小，考虑到CNN模型通常是使用较小的图像构建的，这也有助于加快学习过程。
- en: The simplest way to normalize inputs is to take the value of each pixel and
    divide it by 255, ending up with values ranging between 0 and 1\. Nevertheless,
    there are different methodologies to normalize an image, such as the mean-centering
    technique. The decision to choose one or the other is, most of the time, a matter
    of preference; however, when using pre-trained models, it is highly recommended
    that you use the same technique used to train the model the first time based on
    information that is always available in the documentation of the pre-trained model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化输入的最简单方法是取每个像素的值并将其除以255，最终得到在0到1之间的值范围。 然而，有不同的规范化图像的方法，例如均值中心化技术。 在选择使用其中一种方法时，通常是个人偏好的问题；
    但是，当使用预训练模型时，强烈建议使用第一次训练模型时使用的相同技术，这些信息始终包含在预训练模型的文档中。
- en: Applications of CNNs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN的应用
- en: Although CNNs are mainly used for computer vision problems, it is important
    to mention their capability to solve other learning problems, mainly in regard
    to analyzing sequences of data. For instance, CNNs have been known to perform
    well on sequences of text, audio, and video, sometimes in combination with other
    network architectures, or by converting the sequences into images that can be
    processed by CNNs. Some of the specific data problems that can be tackled using
    CNNs with sequences of data are machine translations of text, natural language
    processing, and video frame tagging, among many others.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CNN主要用于计算机视觉问题，但重要的是提到它们解决其他学习问题的能力，主要是关于分析数据序列。 例如，CNN已知在文本、音频和视频序列上表现良好，有时结合其他网络架构使用，或通过将序列转换为图像以供CNN处理。
    可以使用CNN处理数据序列的一些特定问题包括文本的机器翻译、自然语言处理和视频帧标记等。
- en: 'Moreover, there are different tasks that CNNs can perform that apply to all
    supervised learning problems. However, from now on, this chapter will focus on
    computer vision. The following is a brief explanation of each of these tasks,
    along with a real-life example of each of them:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CNN可以执行适用于所有监督学习问题的不同任务。 然而，从现在开始，本章将集中在计算机视觉上。 以下是每个任务的简要解释，以及每个任务的一个现实示例：
- en: '**Classification**: This is the most commonly known task in computer vision.
    The main idea is to classify the general contents of an image into a set of categories,
    known as labels.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类**：这是计算机视觉中最常见的任务。 主要思想是将图像的一般内容分类为一组标签。'
- en: 'For instance, classification can determine whether an image is of a dog, a
    cat, or any other animal. This classification is done by outputting the probability
    of the image belonging to each of the classes, as seen in the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，分类可以确定图像是狗、猫还是任何其他动物。 此分类通过输出图像属于每个类的概率来完成，如下图所示：
- en: '![Figure 4.3: Classification task](img/C11865_04_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3: 分类任务](img/C11865_04_03.jpg)'
- en: 'Figure 4.3: Classification task'
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.3: 分类任务'
- en: '**Localization**: The main purpose is to generate a bounding box that describes
    the object''s location in the image. The output consists of a class label and
    a bounding box.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**定位**：主要目的是生成描述图像中物体位置的边界框。 输出包括一个类标签和一个边界框。'
- en: 'It can be used in sensors to determine whether an object is to the left or
    right of the screen:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在传感器中使用，以确定对象是在屏幕的左侧还是右侧：
- en: '![Figure 4.4: Localization task](img/C11865_04_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4: 定位任务](img/C11865_04_04.jpg)'
- en: 'Figure 4.4: Localization task'
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.4: 定位任务'
- en: '**Detection**: This task consists of performing object localization on all
    objects in the image. The output consists of multiple bounding boxes, as well
    as multiple class labels (one for each box).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**检测**：此任务包括在图像中对所有对象执行对象定位。 输出包括多个边界框以及多个类标签（每个框一个）。'
- en: 'It is used in the construction of self-driving cars, with the objective of
    being able to locate the traffic signs, the road, other cars, pedestrians, and
    any other object that may be relevant to ensure a safe drive:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它被用于自动驾驶汽车的建设中，旨在能够定位交通标志、道路、其他车辆、行人和任何可能影响安全驾驶的对象：
- en: '![Figure 4.5: Detection task](img/C11865_04_05.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5：检测任务](img/C11865_04_05.jpg)'
- en: 'Figure 4.5: Detection task'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.5：检测任务
- en: '**Segmentation**: The task here is to output both a class label and an outline
    of each object present in the image. This is mainly used to mark important objects
    of an image for further analysis.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**分割**：这里的任务是输出每个对象的类别标签和轮廓。主要用于标记图像中的重要对象，以进行进一步分析。'
- en: 'For instance, it can be used to strictly delimit the area corresponding to
    a tumor in an image of the entire lung of a patient:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，它可以严格地限定在患者整个肺部图像中对应肿瘤的区域：
- en: '![Figure 4.6: Segmentation task](img/C11865_04_06.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：分割任务](img/C11865_04_06.jpg)'
- en: 'Figure 4.6: Segmentation task'
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.6：分割任务
- en: From this section onward, this chapter will focus on training a model to perform
    image classification, using one of PyTorch's image datasets.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一节开始，本章将重点讲述如何训练模型来执行图像分类，使用 PyTorch 的图像数据集之一。
- en: Building Blocks of CNNs
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN 的基本组成部分
- en: As mentioned before, a deep convolutional network is one that takes an image
    as an input, passes it through a series of convolutional layers with filters,
    pooling layers, and fully connected layers, to finally apply a `softmax` activation
    function that classifies the image into a class label. The classification, as
    with ANNs, is performed by calculating the probability of the image belonging
    to each of the class labels, giving each class label a value between zero and
    one. The class label with the higher probability is the one selected as the final
    prediction for that image.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度卷积网络将图像作为输入，经过一系列卷积层、池化层和全连接层，最终应用 `softmax` 激活函数对图像进行分类。与人工神经网络一样，分类通过计算图像属于每个类别的概率来进行，给每个类别标签赋予介于零和一之间的值。概率较高的类别标签被选择为该图像的最终预测。
- en: 'The following is a detailed explanation of each of these layers found, along
    with coding examples of how to define such layers in PyTorch:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面详细解释了每个发现的层，以及如何在 PyTorch 中定义这些层的编码示例：
- en: '**Convolutional layers**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积层**'
- en: This is the first step to extract features from an image. The objective is to
    maintain the relation between nearby pixels by learning the features over small
    sections of the image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从图像中提取特征的第一步。其目标是通过学习图像的小部分来保持附近像素之间的关系。
- en: A mathematical operation occurs in this layer, where two inputs are given (the
    image and the filter) and an output is obtained. As explained before, the operation
    consists of convolving the filter and a section of the image of the same size
    of the filter. This operation is repeated for all subsections of the image.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一层进行数学运算，输入两个（图像和滤波器），得到一个输出。如前所述，该操作包括对滤波器和与滤波器大小相同的图像部分进行卷积。这个操作对图像的所有子部分都会重复进行。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: Revisit *Chapter 2*, *Building Blocks of Neural Networks*, the section titled
    Introduction to CNNs, for a reminder of the exact calculation performed between
    the input and the filter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾*第二章*，*神经网络的基本组成部分*，标题为卷积神经网络介绍，以便回忆输入与滤波器之间的确切计算。
- en: 'The resulting matrix will have a shape depending on the shapes of the inputs,
    where an image matrix of size (h x w x c) and a filter of size (fh x fw x c) will
    output a matrix according to the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵的形状取决于输入的形状，其中图像矩阵的大小为 (h x w x c)，滤波器的大小为 (fh x fw x c)，将根据以下方程输出矩阵：
- en: '![Equation 4.7: Output height, width, and depth from a convolutional layer](img/C11865_04_07.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.7：卷积层的输出高度、宽度和深度](img/C11865_04_07.jpg)'
- en: 'Equation 4.7: Output height, width, and depth from a convolutional layer'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 方程式 4.7：卷积层的输出高度、宽度和深度
- en: Here, h refers to the height of the input image, w is the width, c refers to
    the depth (also known as channels), and fh and fw are values set by the user concerning
    the size of the filter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，h 是输入图像的高度，w 是宽度，c 是深度（也称为通道数），fh 和 fw 是用户根据滤波器大小设定的值。
- en: '![Figure 4.8: Dimensions of input, filter, and output](img/C11865_04_08.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8：输入、滤波器和输出的维度](img/C11865_04_08.jpg)'
- en: 'Figure 4.8: Dimensions of input, filter, and output'
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.8：输入、滤波器和输出的尺寸
- en: 'It is important to mention that, in a single convolutional layer, several filters
    can be applied to the same image, all of the same shape. Considering this, the
    output shape of a convolutional layer that applies two filters to its input, in
    terms of its depth, is equal to two, as seen in the following figure:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的是，在单个卷积层中，可以对同一图像应用多个相同形状的滤波器。考虑到这一点，对于将两个滤波器应用于其输入的卷积层而言，其输出形状的深度是两个，正如以下图所示：
- en: '![Figure 4.9: Convolutional layer with two filters](img/C11865_04_09.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9：具有两个滤波器的卷积层](img/C11865_04_09.jpg)'
- en: 'Figure 4.9: Convolutional layer with two filters'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.9：具有两个滤波器的卷积层
- en: Each of these filters will perform a different operation, in order to discover
    different features from an image. For instance, in a single convolutional layer
    with two filters, the operations could be vertical edge detection and horizontal
    edge detection. Moreover, as the network grows in terms of the number of layers,
    the filters will perform more complex operations that make use of previously detected
    features, for example, the detection of the outline of a person by using the inputs
    from the edge detectors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些滤波器中的每一个将执行不同的操作，以便从图像中发现不同的特征。例如，在具有两个滤波器的单个卷积层中，操作可能包括垂直边缘检测和水平边缘检测。此外，随着网络在层数上的增长，滤波器将执行更复杂的操作，利用先前检测到的特征，例如使用边缘检测器的输入来检测人物轮廓的操作。
- en: Furthermore, filters typically increase in each layer. This means that, while
    the first convolutional layer has eight filters, it is common to create the second
    convolutional layer to have twice this number (16), and so on.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，滤波器通常会在每一层中增加。这意味着，虽然第一个卷积层有八个滤波器，但通常会创建第二个卷积层，其滤波器数量是前者的两倍（16），依此类推。
- en: However, it is important to mention that, in PyTorch, as in many other frameworks,
    you should only define the number of filters to be used and not the type of filters
    (for instance, a vertical edge detector). Each filter configuration (the numbers
    that it contains to detect a specific feature) is part of the variables of the
    system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要提到的是，在 PyTorch 中，如同许多其他框架一样，你只需定义要使用的滤波器数量，而无需指定滤波器的类型（例如，垂直边缘检测器）。每个滤波器配置（用于检测特定特征的数字）是系统变量的一部分。
- en: 'There are two additional concepts to be introduced to the subject of convolutional
    layers, which will be explained as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 关于卷积层，还有两个额外的概念需要介绍，将在接下来进行解释：
- en: '**Padding**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**填充**：'
- en: The padding feature, as the name indicates, pads the image with zeros. This
    means that it adds additional pixels to each side of the image, which are filled
    with zeros.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 填充功能正如其名称所示，用零填充图像。这意味着在图像的每一侧添加额外的像素，并用零填充。
- en: 'The following figure shows an example of an image that has been padded by one
    to each side:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个图像示例，其每一侧都添加了一个填充像素：
- en: '![Figure 4.10: Graphical representation of an input image padded by one](img/C11865_04_10.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10：图像输入在一侧填充一个图形表示](img/C11865_04_10.jpg)'
- en: 'Figure 4.10: Graphical representation of an input image padded by one'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.10：图像输入在一侧填充一个图形表示
- en: This is used to maintain the shape of the input matrix once it has passed through
    the filter. This is because, especially in the first couple of layers, the objective
    should be to preserve as much information from the original input as possible,
    in order to extract the most features out of it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这被用来在通过滤波器后保持输入矩阵的形状。这是因为，特别是在前几层，目标应该是尽可能保留原始输入中的信息，以便从中提取最多的特征。
- en: 'To better understand the concept of padding, consider the following scenario:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解填充的概念，请考虑以下情景：
- en: Applying a three by three filter to a colored image of shape 32 x 32 x 3 would
    result in a matrix of shape 30 x 30 x 1\. This means that the input for the following
    layer has shrunk. On the other hand, by adding padding of one to the input image,
    the shape of the input is changed to 34 x 34 x 3, which results in an output of
    32 x 32 x 1, using the same filter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于形状为 32 x 32 x 3 的彩色图像应用一个 3 x 3 的滤波器将得到一个形状为 30 x 30 x 1 的矩阵。这意味着输入到下一层的图像尺寸已经减小。另一方面，通过在输入图像上添加填充值为
    1，输入的形状则变为 34 x 34 x 3，使用同样的滤波器将得到一个输出尺寸为 32 x 32 x 1 的矩阵。
- en: 'The following equation can be used to calculate the output width when using
    padding:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用填充时，可以使用以下方程式计算输出宽度：
- en: '![Equation 4.11: Output width after a convolutional layer using padding](img/C11865_04_11.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![方程 4.11：使用填充后卷积层的输出宽度](img/C11865_04_11.jpg)'
- en: 'Figure 4.11: Output width after a convolutional layer using padding'
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.11：使用填充后卷积层的输出宽度
- en: Here, W refers to the width of the input matrix, F refers to the width of the
    filter, and P refers to the padding. The same equation can be adapted to calculate
    the height of the output.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，W 是输入矩阵的宽度，F 是滤波器的宽度，P 是填充。同样的方程可以适应计算输出的高度。
- en: 'To obtain an output matrix of equal shape as the input, use the following equation
    to calculate the value for the padding (considering that the stride is equal to
    one):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得与输入矩阵相同形状的输出矩阵，请使用以下方程来计算填充值（考虑步幅等于一）：
- en: '![Equation 4.12: Padding number to get an output matrix of an equal size to
    the input](img/C11865_04_12.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![方程 4.12：获得与输入矩阵相同大小的输出矩阵所需的填充数](img/C11865_04_12.jpg)'
- en: 'Figure 4.12: Padding number to get an output matrix of an equal size to the
    input'
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.12：获得与输入矩阵相同大小的输出矩阵所需的填充数
- en: Keep in mind that the number of output channels (depth) will always be equal
    to the number of filters applied over the input.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，输出通道（深度）的数量始终等于应用于输入的滤波器数。
- en: '**Stride**:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**步幅**：'
- en: This parameter refers to the number of pixels that the filter will shift over
    the input matrix, both horizontally and vertically. As we have seen so far, the
    filter is passed through the top-left corner of the image, then it shifts over
    to the right by one pixel, and so on until it has gone through all sections of
    the image vertically and horizontally. This example is one of a convolutional
    layer, with stride equal to one, which is the default configuration for this parameter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数是指滤波器在输入矩阵上水平和垂直移动的像素数。正如我们迄今所见，滤波器从图像的左上角通过，然后向右移动一个像素，依此类推，直到垂直和水平地通过图像的所有部分。这个例子是步幅为一的卷积层，默认配置为此参数。
- en: 'When stride equals two, the shift would be of two pixels instead, as seen in
    the following figure:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当步幅为二时，移动将为两个像素，如下图所示：
- en: '![Figure 4.13: Graphical representation of a convolutional layer with stride
    of two](img/C11865_04_13.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13：步幅为二的卷积层的图形表示](img/C11865_04_13.jpg)'
- en: 'Figure 4.13: Graphical representation of a convolutional layer with stride
    of two'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.13：步幅为二的卷积层的图形表示
- en: As can be seen, the initial operation occurs in the top-left corner, then, by
    shifting two pixels to the right, the second calculation occurs in the top-right
    corner. Next, the calculation shifts two pixels downward to perform the calculations
    on the bottom-left corner, and finally, by shifting again two pixels to the right,
    the final calculation occurs in the bottom-right corner.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，初始操作发生在左上角，然后向右移动两个像素，第二次计算发生在右上角。接下来，计算向下移动两个像素，以在左下角执行计算，最后再次向右移动两个像素，最终计算发生在右下角。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The numbers in *Figure 4.13* are made up, and not actual calculations. The focus
    should be on the boxes that explain the shifting process when the stride is equal
    to two.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.13* 中的数字是虚构的，不是实际计算。重点应放在解释步幅为二时的移动过程的方框上。'
- en: 'The following equation can be used to calculate the output width when using
    stride:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用步幅时，可以使用以下方程来计算输出宽度：
- en: '![Equation 4.14: Output width after a convolutional layer using stride](img/C11865_04_14.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![方程 4.14：使用步幅进行卷积层后的输出宽度](img/C11865_04_14.jpg)'
- en: 'Equation 4.14: Output width after a convolutional layer using stride'
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 方程 4.14：使用步幅进行卷积层后的输出宽度
- en: Here, W refers to the width of the input matrix, F refers to the width of the
    filter and S refers to the stride. The same equation can be adapted to calculate
    the height of the output.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，W 是输入矩阵的宽度，F 是滤波器的宽度，S 是步幅。同样的方程可以适应计算输出的高度。
- en: 'Once these parameters are introduced, the final equation to calculate the output
    shape (width and height) of the matrix derived from a convolutional layer is as
    follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦引入这些参数，计算来自卷积层的矩阵输出形状（宽度和高度）的最终方程如下：
- en: '![Equation 4.15: Output width after a convolutional layer using padding and
    stride](img/C11865_04_15.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![方程 4.15：使用填充和步幅后卷积层的输出宽度](img/C11865_04_15.jpg)'
- en: 'Equation 4.15: Output width after a convolutional layer using padding and stride'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 方程 4.15：使用填充和步幅后的卷积层的输出宽度
- en: Whenever the value is a float, it should be rounded down. This basically means
    that some areas of the input are being ignored and no features are extracted from
    them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 每当数值为浮点数时，应向下取整。这基本上意味着输入的某些区域被忽略，并且不会从中提取任何特征。
- en: Finally, once the input has been passed through all the filters, the output
    is fed to an activation function in order to break linearity, similar to the process
    of traditional neural networks. Although there are several activation functions
    to be used in this step, the preferred one is the ReLU function since it has shown
    outstanding results in CNNs. The output obtained here becomes the input of the
    subsequent layer, which is usually a pooling layer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，一旦输入通过所有的过滤器，输出将被送入激活函数中，以打破线性，类似于传统神经网络的过程。虽然在这一步骤中有几种激活函数可供使用，但首选的是ReLU函数，因为它在CNN中表现出色。在此处获得的输出成为下一层的输入，通常是一个池化层。
- en: 'Exercise 8: Calculating the Output Shape of a Convolutional Layer'
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8：计算卷积层的输出形状
- en: Considering the equations given, consider the following scenarios and calculate
    the shape of the output matrix.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑给定的方程式，考虑以下情景并计算输出矩阵的形状。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注释
- en: This exercise does not require coding, but rather consists of a practice exercise
    on the concepts mentioned previously.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习不需要编码，而是由先前提到的概念练习组成。
- en: 'An input of shape 64 x 64 x 3\. A filter of shape 3 x 3 x 3:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入形状为64 x 64 x 3。形状为3 x 3 x 3的过滤器：
- en: '[PRE0]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'An input of shape 32 x 32 x 3\. 10 filters of shape 5 x 5 x 3\. Padding of
    2:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入形状为32 x 32 x 3。5个形状为5 x 5 x 3的过滤器。填充为2：
- en: '[PRE1]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'An input of shape 128 x 128 x 1\. 5 filters of shape 5 x 5 x 1\. Stride of
    3:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入形状为128 x 128 x 1。5个形状为5 x 5 x 1的过滤器。步长为3：
- en: '[PRE2]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'An input of shape 64 x 64 x 1\. A filter of shape 8 x 8 x 1\. Padding of 3
    and stride of 3:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入形状为64 x 64 x 1。形状为8 x 8 x 1的过滤器。填充为3，步长为3：
- en: '[PRE3]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Congratulations! You have successfully calculated the output shape of the matrix
    derived from a convolutional layer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您成功地计算出源自卷积层的矩阵输出形状。
- en: 'Coding a convolutional layer in PyTorch is very simple. Using custom modules,
    it would only require the creation of the `network` class, which would have an
    `__init__` function containing the layers of the network, and a `forward` function
    that defines the step to pass the information through the different layers previously
    defined, as shown in the following code snippet:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中编写卷积层非常简单。使用自定义模块，只需创建`network`类，其中包含网络的各层，以及一个`forward`函数，定义通过先前定义的不同层传递信息的步骤，如下面的代码片段所示：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When defining the convolutional layer, the arguments that are passed through
    from left to right refer to the input channels, output channels (number of filters),
    kernel size (filter size), stride, and padding.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义卷积层时，从左到右传递的参数是指输入通道数，输出通道数（过滤器数量），核大小（滤波器大小），步长和填充。
- en: Considering this, the preceding example consists of a convolutional layer with
    three input channel, 18 filters, each of size 3, and stride and padding equal
    to 1.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，前述的例子由具有三个输入通道，18个大小为3的过滤器组成，步长和填充均为1的卷积层组成。
- en: 'Another valid approach, equivalent to the previous example, consists of the
    combination of the syntax from custom modules and the use of `Sequential` containers,
    as can be seen in the following code snippet:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有效的方法，相当于前面的例子，包括来自自定义模块的语法组合和使用`Sequential`容器，如下面的代码片段所示：
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, the definition of layers occurs inside the `Sequential` container. Typically,
    one container would include a convolutional layer, an activation function, and
    a pooling layer. A new set of layers would be included in a different container
    below.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，层的定义发生在`Sequential`容器内。通常，一个容器会包含一个卷积层，一个激活函数和一个池化层。在下面的不同容器中将包含一组新的层。
- en: '**Pooling layers**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**池化层**'
- en: Conventionally, pooling layers are the last part of the feature selection step,
    which is why a pooling layer can mostly be found after a convolutional layer.
    As explained in previous chapters, the idea is to extract the most relevant information
    out of subsections of the image. The size of the pooling layer is typically two,
    and the stride is equal to its size.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，池化层是特征选择步骤的最后部分，这就是为什么池化层大多数情况下可以在卷积层之后找到的原因。正如在前面的章节中解释的那样，其思想是从图像的子部分中提取最相关的信息。池化层的大小通常为2，步长等于其大小。
- en: According to the preceding paragraph, pooling layers commonly reduce the input's
    height and weight by half. This is important considering that, in order for convolutional
    layers to find all features in an image, several filters need to be used, and
    the output from this operation can become too large, which means there are many
    parameters to consider. Pooling layers aim to reduce the number of parameters
    in the network by keeping the most relevant features. The selection of relevant
    features out of subsections of the image occurs either by grabbing the maximum
    number or by averaging the numbers in that region.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前文，池化层通常通过减半输入的高度和宽度来实现。这很重要，考虑到为了让卷积层在图像中找到所有特征，需要使用多个滤波器，并且该操作的输出可能会变得过大，这意味着有很多参数需要考虑。池化层旨在通过保留最相关的特征来减少网络中的参数数量。在图像的子区域中选择相关特征的方法可以是获取最大数量或对该区域中的数字进行平均。
- en: For image classification tasks, it is most common to use max pooling layers,
    over average pooling layers. This is because the former has shown better results
    in tasks where preserving the most relevant features is key, while the latter
    has been proven to work better in tasks such as smoothing images.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类任务，最常用的是最大池化层，而不是平均池化层。这是因为前者在保留最相关特征的任务中表现更好，而后者在平滑图像等任务中被证明更有效。
- en: 'To calculate the shape of the output matrix, the following equation can be
    used:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下方程式来计算输出矩阵的形状：
- en: '![Equation 4.16: Output matrix width after a pooling layer](img/C11865_04_16.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4.16：池化层后的输出矩阵宽度](img/C11865_04_16.jpg)'
- en: 'Equation 4.16: Output matrix width after a pooling layer'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 方程式 4.16：池化层后的输出矩阵宽度
- en: Here, W refers to the width of the input, F refers to the size of the filter,
    and S refers to the stride. The same equation can be adapted to calculate the
    output height
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，W指的是输入的宽度，F指的是滤波器的大小，S指的是步长。可以将相同的方程式用于计算输出的高度
- en: The channels or depth of the input remains unchanged as the pooling layer will
    perform the same operation over all channels of the image. This means that the
    result from a pooling layer only affects the input in terms of width and length.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的通道或深度保持不变，因为池化层将在图像的所有通道上执行相同的操作。这意味着池化层的结果仅影响输入的宽度和长度。
- en: 'Exercise 9: Calculating the Output Shape of a set of Convolutional and Pooling
    Layers'
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 9：计算一组卷积和池化层的输出形状
- en: The following exercise will combine both convolutional and pooling layers. The
    objective is to determine the size of the output matrix after going through a
    set of layers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的练习将结合卷积层和池化层。目标是确定经过一组层后的输出矩阵的大小。
- en: Note
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This activity does not require coding, but rather consists of a practice exercise
    on the concepts mentioned previously.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动不需要编码，而是基于之前提到的概念进行的实践练习。
- en: 'Consider the following sets of layers and specify the shape of the output layer
    at the end of all the transformations:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下一组层，并指定在所有变换结束时输出层的形状：
- en: An input image of size 256 x 256 x 3.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入图像大小为256 x 256 x 3。
- en: A convolutional layer with 16 filters of size three, and stride and padding
    of one.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有16个大小为三的滤波器、步长和填充均为一的卷积层。
- en: A pooling layer with a filter of size two and stride of size two as well.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个池化层，滤波器大小为二，步长大小也为二。
- en: A convolutional layer with eight filters of size seven, stride of one, and padding
    of three.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有八个大小为七、步长为一、填充为三的滤波器的卷积层。
- en: A pooling layer with a filter of size two and a stride of two as well.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个池化层，滤波器大小为二，步长也为二。
- en: 'Below, the output size of the matrix after going through each of this layers
    is shown:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面展示了通过每个层后经过的矩阵的输出大小：
- en: '[PRE6]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Congratulations! You have successfully calculated the output shapes of the matrix
    derived from a series of convolutional and pooling layers.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 恭喜！您已成功计算出通过一系列卷积和池化层导出的矩阵的输出形状。
- en: 'Using the same coding examples as before, the PyTorch way to define pooling
    layers is shown in the following code snippet:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用与之前相同的编码示例，定义池化层的PyTorch方式如下代码片段所示：
- en: '[PRE7]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, the arguments that go into the max pooling layers, from left to right,
    are the size of the filter and the stride.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，进入最大池化层的参数从左到右依次是滤波器的大小和步长。
- en: 'Again, an equally valid approach is shown here, with the use of custom modules
    and `Sequential` containers:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次展示了一个同样有效的方法，使用自定义模块和`Sequential`容器：
- en: '[PRE8]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As mentioned before, the pooling layer is also included in the same container
    as the convolutional layer and the activation function. A subsequent set of layers
    (convolutional, activation, and pooling) would be defined below, in a new `Sequential`
    container.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，池化层也包含在与卷积层和激活函数相同的容器中。后续的一组层（卷积、激活和池化）将在新的`Sequential`容器中定义。
- en: '**Fully connected layers**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**全连接层**'
- en: The fully connected (FC) layer or layers are defined at the end of the network
    architecture, after the input has gone through a set of convolutional and pooling
    layers. The output data from the layer preceding the first fully-connected layer
    is flattened from a matrix to a vector, which can be fed to the fully connected
    layer (the same as a hidden layer from traditional neural networks).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接（FC）层或层在网络架构末端定义，在输入通过一组卷积和池化层之后。从前一个全连接层之前的输出数据被从矩阵扁平化为向量，可以被馈送到全连接层（与传统神经网络中的隐藏层相同）。
- en: The main purpose of these FC layers is to consider all the features detected
    by the previous layers, in order to classify the image.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这些FC层的主要目的是考虑前面层次检测到的所有特征，以便对图像进行分类。
- en: The different FC layers are passed through an activation function, which is
    typically the ReLU, unless it is the final layer, which will use a softmax function
    to output the probability of the input belonging to each of the class labels.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的FC层通过激活函数传递，通常是ReLU，除非是最后一层，该层将使用softmax函数输出输入属于每个类标签的概率。
- en: The input size of the first fully connected layer corresponds to the size of
    the flattened output matrix from the previous layer. The output size is defined
    by the user, and, again, as with ANNs, there is not an exact science to setting
    this number. The last FC layer should have an output size equal to the number
    of class labels.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个全连接层的输入大小对应于前一层的扁平化输出矩阵的大小。输出大小由用户定义，与ANNs一样，设置这个数字并没有确切的科学依据。最后一个FC层的输出大小应等于类标签的数量。
- en: 'To define a set of FC layers in PyTorch, consider the following code snippet:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PyTorch中定义一组FC层，请考虑以下代码片段：
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, two fully connected layers are added to the network. Next, inside the
    forward function, the output from the pooling layer is flattened using the `view()`
    function. Then, it is passed through the first FC layer, which applies an activation
    function. Finally, the data is passed through a final FC layer, along with its
    activation function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，向网络添加了两个全连接层。接下来，在前向函数内部，使用`view()`函数将池化层的输出扁平化。然后，它通过第一个FC层，该层应用激活函数。最后，数据通过最后一个FC层及其激活函数传递。
- en: 'The code for defining fully connected layers using both custom modules and
    the `Sequential` container can be seen as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义模块和`Sequential`容器定义全连接层的代码如下所示：
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once the architecture of the network has been defined, the following steps of
    defining the different parameters (including the loss function and optimization
    algorithm), as well as the training process, can be handled in the same way as
    ANNs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络架构被定义，定义不同参数（包括损失函数和优化算法）以及训练过程的后续步骤可以像ANNs一样处理。
- en: Side Note – Downloading Datasets from PyTorch
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 旁注 - 从PyTorch下载数据集
- en: 'To load a dataset from PyTorch, use the following code. Besides downloading
    the dataset, it shows how to use data loaders to save resources by loading the
    images by batches, rather than all at once:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要从PyTorch加载数据集，请使用以下代码。除了下载数据集外，它还展示了如何使用数据加载器按批次加载图像以节省资源：
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, the dataset to be downloaded is the MNIST. This is a
    popular dataset that contains images of hand-written gray-scaled numbers going
    from zero to nine. The `transform` variable defined before downloading the dataset
    is in charge of performing some transformations on the dataset. In this case,
    the dataset will be both converted into tensors and normalized in all its dimensions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，要下载的数据集是MNIST。这是一个流行的数据集，包含从零到九的手写灰度数字的图像。在下载数据集之前定义的`transform`变量负责对数据集执行一些转换。在这种情况下，数据集将被转换为张量并在所有维度上进行归一化。
- en: The `SubsetRandomSampler()` function from PyTorch is used to divide the original
    training set into training and validations by randomly sampling indexes. Moreover,
    the `DataLoader()` functions are the ones in charge of loading the images by batches.
    The resulting variables (`train_loader`, `dev_loader`, and `test_loader`) of this
    function will contain the values for the features and the target separately.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyTorch中的`SubsetRandomSampler()`函数，通过随机采样索引将原始训练集分为训练集和验证集。此外，`DataLoader()`函数负责按批次加载图像。该函数的结果变量（`train_loader`、`dev_loader`和`test_loader`）将分别包含特征和目标的值。
- en: 'Activity 7: Building a CNN for an Image Classification Problem'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动7：为图像分类问题构建CNN
- en: Note
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The more complex the problem and the deeper the network, the longer it takes
    for the model to train. Considering this, the activities in this chapter may take
    longer than the ones in previous chapters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 问题越复杂，网络越深，模型训练时间越长。考虑到这一点，本章的活动可能比之前的章节需要更长的时间。
- en: 'In the following activity, a CNN will be trained on an image dataset from PyTorch.
    Let''s look at the following scenario:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，将在PyTorch的图像数据集上训练CNN。让我们看一下以下情景：
- en: You work at an artificial intelligence company that develops custom-made models
    for the needs of its customers. Your team is currently creating a model that can
    differentiate between vehicles and animals, and, more specifically, a model able
    to differentiate between different animals and different types of vehicles. They
    have provided you with a dataset containing 60,000 images and want you to build
    such a model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您在一家人工智能公司工作，该公司为客户的需求开发定制模型。您的团队目前正在创建一个能够区分车辆和动物的模型，更具体地说，是一个能够区分不同动物和不同类型车辆的模型。他们为您提供了包含60,000张图像的数据集，并希望您构建这样一个模型。
- en: Note
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For the activities within this chapter, you will need to have Python 3.6, Jupyter,
    NumPy, and Matplotlib.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中的活动，您需要准备Python 3.6、Jupyter、NumPy和Matplotlib。
- en: 'Import the following libraries:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入以下库：
- en: '[PRE12]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset to be used is CIFAR10 from PyTorch, which contains a total of 60,000
    images of vehicles and animals. There are 10 different class labels. The training
    set contains 50,000 images, while the testing set contains the remaining 10,000.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将使用的数据集是PyTorch中的CIFAR10，其中包含总共60,000张车辆和动物的图像。有10个不同的类标签。训练集包含50,000张图像，测试集包含剩余的10,000张。
- en: Set the transformations to be performed on the data, which will be the conversion
    of the data to tensors and the normalization of the pixel values.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置对数据执行的转换，即将数据转换为张量并对像素值进行归一化。
- en: Set a batch size of 100 images and download both the training and testing data
    from the CIFAR10 dataset.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置批量大小为100张图像，并从CIFAR10数据集下载训练和测试数据。
- en: Using a validation size of 20%, define the training and validation sampler that
    will be used to divide the dataset into those two sets.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用20%的验证大小，定义用于将数据集分为这两组的训练和验证采样器。
- en: Use the `DataLoader()` function to define the batches to be used for each set
    of data.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DataLoader()`函数定义每组数据集使用的批次。
- en: 'Define the architecture of your network. Use the following information to do
    so:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您的网络架构。使用以下信息来完成：
- en: 'Conv1: A convolutional layer that takes as input the colored image and passes
    it through 10 filters of size 3\. Both the padding and the stride should be set
    to 1.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conv1：一个卷积层，以彩色图像作为输入，并通过10个大小为3的滤波器进行处理。填充和步幅均设置为1。
- en: 'Conv2: A convolutional layer that passes the input data through 20 filters
    of size 3\. Both the padding and the stride should be set to 1.'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conv2：一个卷积层，通过20个大小为3的滤波器处理输入数据。填充和步幅均设置为1。
- en: 'Conv3: A convolutional layer that passes the input data through 40 filters
    of size three. Both the padding and the stride should be set to 1.'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conv3：一个卷积层，通过40个大小为三的滤波器处理输入数据。填充和步幅均设置为1。
- en: Use the ReLU activation function after each convolutional layer.
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个卷积层后使用ReLU激活函数。
- en: A pooling layer after each convolutional layer, with a filter size and stride
    of 2.
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层后使用池化层，滤波器大小和步幅均为2。
- en: A dropout term set to 20% after flattening the image.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在展平图像后设置20%的丢失率。
- en: 'Linear1: A fully-connected layer that receives as input the flattened matrix
    from the previous layer and generates an output of 100 units. Use the ReLU activation
    function for this layer. A dropout term here is set to 20%.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linear1：接收来自上一层的展平矩阵作为输入，并生成100个单元的输出。对于这一层，使用 ReLU 激活函数。此处的丢弃项设置为20%。
- en: 'Linear2: A fully-connected layer that generates 10 outputs, one for each class
    label. Use the `log_softmax` activation function for the output layer.'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linear2：生成10个输出的全连接层，每个类标签一个。对输出层使用 `log_softmax` 激活函数。
- en: Define all parameters required to train your model. Train for 50 epochs.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练模型所需的所有参数。训练50个 epoch。
- en: Train your network and be sure to save the values for the loss and accuracy
    of both the training and validation sets.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练您的网络，并确保保存训练和验证集的损失和准确性值。
- en: Plot the loss and accuracy of both sets.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制两个集的损失和准确性。
- en: Note
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Due to the shuffling of the data in each epoch, results will not be exactly
    reproducible. However, you should be able to arrive at similar results.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于每个 epoch 中数据的重新排序，结果将不完全可再现。但是，您应该能够得到类似的结果。
- en: Check the model's accuracy on the testing set.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查测试集上的模型准确性。
- en: Note
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 204.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可在第204页找到此活动的解决方案。
- en: Data Augmentation
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'Learning how to effectively code a neural network is one of the steps involved
    in developing state-of-the-art solutions. Additionally, to develop great deep
    learning solutions, it is also crucial to find an area of interest in which we
    can provide a solution to a current challenge (not an easy task, by the way).
    But, once all of that is done, we are typically faced with the same issue: getting
    a dataset of a decent size to get a good performance from our models, either by
    self-gathering or from the internet and other available sources.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何有效编写神经网络是开发最先进解决方案的步骤之一。此外，要开发出优秀的深度学习解决方案，还必须找到一个可以提供解决当前挑战的领域（顺便说一句，这并不是一件容易的事情）。但是，一旦所有这些都完成了，我们通常会面临同样的问题：获取一个足够大的数据集以使我们的模型性能良好，无论是通过自我收集还是来自互联网和其他可用来源。
- en: As you might imagine, and even though it is now possible to gather and store
    vast amounts of data, this is not an easy task due to the costs associated to
    it. And so, most of the time, we are stuck working with a dataset containing tens
    of thousands of entries, and even fewer when referring to images.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能想象的那样，即使现在可以收集和存储大量数据，由于相关的成本，这并不是一件容易的任务。因此，大多数情况下，我们都在处理包含数万条记录的数据集，而在涉及图像时，这个数字甚至更少。
- en: 'This becomes a relevant issue when developing a solution for a computer vision
    problem, mainly due to two reasons:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发计算机视觉问题的解决方案时，这变成一个相关问题，主要有两个原因：
- en: The larger the dataset, the better the results, and larger datasets are crucial
    to arrive at decent enough models. This is true considering that training a model
    is a matter of tuning a bunch of parameters such that it is capable of mapping
    a relationship between an input and an output, while minimizing the loss function
    by making the predicted value come as close to the ground truth as possible. Here,
    the more complex the model, the more parameters it requires.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集越大，结果越好，较大的数据集对于获得足够好的模型至关重要。这是真实的，考虑到训练模型是调整一堆参数的过程，使其能够尽可能接近地图输入和输出之间的关系，并通过最小化损失函数来预测值。在这里，模型越复杂，需要的参数就越多。
- en: Considering this, it is necessary to feed to the model a fair number of examples
    so that it is capable of finding such patterns, where the number of training examples
    should be proportional to the number of parameters to be tuned.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到这一点，有必要向模型提供足够数量的示例，以便它能够找到这些模式，其中训练示例的数量应与要调整的参数数量成比例。
- en: Moreover, one of the biggest challenges in computer vision problems is getting
    your model to perform well over several variations of an image. This means that
    images do not need to be fed following a specific alignment or have a set quality,
    but can instead be fed in their original formats, including different positions,
    angles, lighting, and other distortions. Because of this, it is necessary to find
    a way to feed the model with such variations.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，计算机视觉问题中最大的挑战之一是使您的模型在图像的多种变化上表现良好。这意味着图像不需要按特定对齐方式或具有固定质量进行馈送，而是可以以其原始格式进行馈送，包括不同的位置、角度、光照和其他失真。因此，有必要找到一种方法来将这些变化输入模型。
- en: So, the data augmentation technique was designed. In simple words, it is a measure
    to increase the number of training examples by slightly modifying the existing
    examples. For example, you could duplicate the instances currently available and
    add some noise to those duplicates to make sure they are not exactly the same.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，设计了数据增强技术。简单来说，它是通过轻微修改现有示例来增加训练示例的数量。例如，可以复制当前可用的实例，并对这些副本添加一些噪声，以确保它们并非完全相同。
- en: In computer vision problems, this means incrementing the number of images in
    the training dataset by altering the existing images, which can be done by slightly
    altering the current images to create duplicated versions that are slightly different.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉问题中，这意味着通过改变现有图像来增加训练数据集中的图像数量，可以通过微调当前图像来创建略有不同的重复版本。
- en: These minor adjustments to the images can be in the form of slight rotations,
    changes in the position of the object in the frame, horizontal or vertical flips,
    different color schemes, and distortions, among others. This technique works considering
    that CNNs will consider each of these images a different image.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些图像的微小调整可以是轻微的旋转、物体在帧内位置的变化、水平或垂直翻转、不同的色彩方案和扭曲等形式。这种技术有效，因为卷积神经网络将认为每个这样的图像是不同的图像。
- en: 'For instance, the following figure shows three images of a dog that, while
    to the human eye are the same image with certain variations, to the neural network
    are completely different:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下图示展示了一只狗的三张图像，对于人眼来说它们是带有某些变化的同一图像，但对于神经网络来说它们是完全不同的：
- en: '![Figure 4.17: Augmented images](img/C11865_04_17.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.17: 增强图像](img/C11865_04_17.jpg)'
- en: 'Figure 4.17: Augmented images'
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.17: 增强图像'
- en: A CNN capable of recognizing an object in an image independently of any sort
    of variation is considered to have the property of invariance. In fact, a CNN
    can be invariant to each type of variation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 能够独立于任何形式的变化识别图像中对象的 CNN 被认为具有不变性属性。事实上，CNN 可以对每种类型的变化都具有不变性。
- en: Data Augmentation with PyTorch
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 PyTorch 进行数据增强
- en: Performing data augmentation in PyTorch using the `torchvision` package is very
    easy. The package, in addition to containing popular datasets and model architectures,
    also has common image transformation functions to perform on datasets.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torchvision` 包在 PyTorch 中执行数据增强非常简单。该包除了包含流行的数据集和模型架构外，还包含常用的图像转换函数来处理数据集。
- en: Note
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In this section, a few of these image transformations will be mentioned. To
    get the entire list of possible transformations, visit [https://pytorch.org/docs/stable/torchvision/transforms.html](https://pytorch.org/docs/stable/torchvision/transforms.html).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，将提及一些这些图像转换。要获取所有可能的转换列表，请访问 [https://pytorch.org/docs/stable/torchvision/transforms.html](https://pytorch.org/docs/stable/torchvision/transforms.html).
- en: 'As with the process used in the previous activity to normalize and convert
    the dataset to tensors, performing data augmentation requires us to first define
    the desired transformations, then to apply them to the dataset, as shown in the
    following code snippet:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个活动中用于归一化和将数据集转换为张量的过程相似，执行数据增强需要我们首先定义所需的转换，然后将其应用于数据集，如下面的代码片段所示：
- en: '[PRE13]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, the data to be downloaded will undergo a horizontal flip (considering
    the probability value, which defines whether the image will be flipped), and will
    be converted to grayscale (also considering the probability). Then, the data is
    converted to tensors and normalized.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，将要下载的数据将经历水平翻转（考虑到定义图像是否将被翻转的概率值），并将被转换为灰度图像（同样考虑到概率）。然后，数据被转换为张量并进行归一化。
- en: Considering that a model is trained in an iterative process, in which the training
    data is fed multiple times, these transformations ensure that a second run through
    the dataset does not feed the exact same images to the model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到模型是通过迭代过程进行训练的，其中训练数据被多次输入，这些转换确保第二次通过数据集时不会向模型提供完全相同的图像。
- en: Moreover, it is important to mention that different transformations can be set
    for different sets. This is useful because the purpose of data augmentation is
    to increment the number of training examples, but the images that will be used
    for testing the model should be left mostly unaltered. Nevertheless, the testing
    set should be resized in order to feed equally sized images to the model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，重要的是提到可以为不同的数据集设置不同的转换。这是有用的，因为数据增强的目的是增加训练示例的数量，但用于测试模型的图像应该保持大部分不变。尽管如此，测试集应调整大小，以便将等尺寸的图像输入模型。
- en: 'This can be accomplished as shown in the code snippet:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 可以如下所示完成：
- en: '[PRE14]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As can be seen, a dictionary containing a set of transformations for the training
    and testing sets is defined. Then, they are called to apply the transformations
    to each of the sets, accordingly.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，定义了一个包含训练和测试集各自转换的字典。然后，根据需要调用它们来应用转换。
- en: 'Activity 8: Implementing Data Augmentation'
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动8：实施数据增强
- en: 'In the following activity, data augmentation will be introduced to the model
    created in the previous activity in order to test whether its accuracy can be
    improved. Let''s look at the following scenario:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个活动中，将引入数据增强到先前活动创建的模型中，以测试是否可以提高其准确性。让我们看看以下情景：
- en: The model that you have created is good, but the accuracy does not impress anyone
    yet. They have asked you to think of a methodology that could improve the performance
    of the model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 您创建的模型很好，但准确率尚未令任何人印象深刻。他们要求您考虑一种能够改进模型性能的方法论。
- en: Duplicate the notebook from the previous activity.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制上一个活动中的笔记本。
- en: 'Change the definition of the `transform` variable to include, in addition to
    normalizing and converting the data into tensors, the following transformations:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`transform`变量的定义修改，除了将数据标准化并转换为张量外，还包括以下转换：
- en: For the training/validation sets, a `RandomHorizontalFlip` function with a probability
    of 50% (0.5) and a `RandomGrayscale` function with a probability of 10% (0.1).
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练/验证集，使用概率为50%（0.5）的`RandomHorizontalFlip`函数和概率为10%（0.1）的`RandomGrayscale`函数。
- en: For the testing set, do not add any other transformation.
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试集，不添加任何其他转换。
- en: Train the model for 100 epochs.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型训练100个epochs。
- en: Note
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Due to the shuffling of the data in each epoch, results will not be exactly
    reproducible. However, you should be able to arrive at similar results.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于每个epoch中数据的洗牌，结果将不会完全可复制。然而，您应该能够得到类似的结果。
- en: Calculate the accuracy of the resulting model on the testing set.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在测试集上得到的模型准确性。
- en: Note
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 209.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第209页找到。
- en: Batch Normalization
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量标准化
- en: It is typical to normalize the input layer in an attempt to speed up learning,
    as well as to improve performance by rescaling all features to the same scale.
    So, the question is, if the model benefits from the normalization of the input
    layer, why not normalize the output of all hidden layers in an attempt to improve
    the training speed even more?
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在尝试加快学习速度的同时，对输入层进行标准化以及通过重新缩放所有特征到相同尺度来提高性能是很常见的。因此，问题在于，如果模型从输入层的标准化中获益，为什么不试图通过标准化所有隐藏层的输出来进一步提高训练速度呢？
- en: '**Batch normalization**, as its name suggests, normalizes the outputs from
    the hidden layers so that it reduces the variance from each layer, which is also
    known as covariance shift. This reduction of the covariance shift is useful as
    it allows the model to also work well over images that follow a different distribution
    than the images used to train it.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量标准化**，顾名思义，标准化隐藏层的输出，以减少每层的方差，这也被称为协方差偏移。减少协方差偏移对于使模型能够在遵循与训练图像不同分布的图像上表现良好也是有用的。'
- en: 'Take for instance a network that has the purpose of detecting whether an animal
    is a cat. When the network is trained only using images of black cats, batch normalization
    can help the network also classify new images of cats of different colors by normalizing
    the data so that both the black and colored cat images follow a similar distribution.
    Such problem is represented in the following figure:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以检测动物是否为猫为目的的网络为例。当网络仅使用黑猫图像进行训练时，批量标准化可以帮助网络通过标准化数据，使黑色和彩色猫图像都遵循相似的分布，从而对不同颜色的猫的新图像进行分类。这类问题如下图所示：
- en: '![Figure 4.18: Cat classifier. Model able to recognize colored cats, even after
    being trained using only black cats](img/C11865_04_18.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图4.18：猫分类器。即使仅使用黑猫进行训练，该模型也能识别有色猫](img/C11865_04_18.jpg)'
- en: 'Figure 4.18: Cat classifier. Model able to recognize colored cats, even after
    being trained using only black cats'
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.18：猫分类器。即使仅使用黑猫进行训练，该模型也能识别有色猫。
- en: 'Moreover, in addition to the above, batch normalization introduces the following
    benefits to the process of training the model, which ultimately helps you to arrive
    at a better performing model:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了上述内容，批量归一化还为训练模型的过程引入以下好处，最终帮助您得到表现更好的模型：
- en: It allows a higher learning rate to be set, as batch normalization helps to
    ensure that none of the outputs go too high or too low. A higher learning rate
    is equivalent to faster learning times.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许设置更高的学习率，因为批量归一化有助于确保输出不会过高或过低。更高的学习率等同于更快的学习速度。
- en: It helps to reduce overfitting because it has a regularization effect. This
    makes it possible to set the dropout probability at a lower value, which means
    that less information is ignored in each forward pass.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有助于减少过拟合，因为它具有正则化效果。这使得可以将丢弃概率设置为较低的值，这意味着每次前向传递中会忽略较少的信息。
- en: Note
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to mention that we should not rely mainly on batch normalization
    to deal with overfitting.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提醒一下，我们不应过于依赖批量归一化来处理过拟合问题。
- en: As explained in previous layers, the normalization of the output of a hidden
    layer is done by subtracting the batch mean and dividing by the batch standard
    deviation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面各层所述，对隐藏层输出进行归一化是通过减去批量均值并除以批量标准差完成的。
- en: Furthermore, it is important to mention that batch normalization is typically
    performed on the convolutional layers, as well as the fully connected layers (excluding
    the output layer).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，重要的是提到，批量归一化通常应用于卷积层以及全连接层（不包括输出层）。
- en: Batch Normalization with PyTorch
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch进行批量归一化
- en: 'In PyTorch, adding batch normalization is as simple as adding a new layer to
    the network architecture, considering that there are two different types, as explained
    here:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，添加批量归一化就像是向网络架构添加新层一样简单，考虑到这里所解释的两种不同类型：
- en: '**BatchNorm1d**: This layer is used to implement batch normalization on a two-dimensional
    or three-dimensional input. It receives the number of output nodes from the previous
    layer as an argument. This is commonly used on fully connected layers.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**BatchNorm1d**：此层用于对二维或三维输入实施批量归一化。它接收前一层的输出节点数作为参数。这在全连接层上通常使用。'
- en: '**BatchNorm2d**: This applies batch normalization on four-dimensional inputs.
    Again, the argument that it takes in is the number of output nodes from the previous
    layer. It is commonly used on convolutional layers, meaning that the argument
    that it takes in should be equal to the number of channels from the previous layer.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**BatchNorm2d**：这对四维输入应用批量归一化。同样，它接收前一层的输出节点数作为参数。它通常用于卷积层，因此它接收的参数应等于前一层的通道数。'
- en: 'According to this, the implementation of batch normalization in a CNN is as
    follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个，CNN中批量归一化的实现如下：
- en: '[PRE15]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As can be seen, batch normalization layers are initially defined in a similar
    way to any other layer. Next, each is applied to the output of its corresponding
    layer after the activation function.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 可见，批量归一化层的初始化方式与任何其他层类似。接下来，在激活函数后应用于其相应层的输出。
- en: 'Activity 9: Implementing Batch Normalization'
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动9：实现批量归一化
- en: 'For the following activity, we will implement batch normalization on the architecture
    of the previous activity in order to see if it is possible to further improve
    the performance of the model on the testing set. Let''s look at the following
    scenario:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的活动，我们将在上一个活动的架构上实现批量归一化，以查看是否可能进一步改善模型在测试集上的性能。让我们看看以下情景：
- en: 'Wow! You have impressed your teammates with the last improvement in performance,
    and now they are expecting more from you. They have asked you to give improving
    the model one last try so that the accuracy goes up to 80%:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！您以最后的性能改进给队友留下了深刻印象，现在他们期待您更多。他们要求您最后再试一次改进模型，以便将准确率提高到80%：
- en: Duplicate the notebook from the previous activity.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制来自上一个活动的笔记本。
- en: Add batch normalization to each convolutional layer, as well as to the first
    FC layer.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将批归一化添加到每个卷积层以及第一个全连接层。
- en: Train the model for 100 epochs.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型进行 100 个 epochs。
- en: Note
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Due to the shuffling of the data in each epoch, results will not be exactly
    reproducible. However, you should be able to arrive at similar results.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于每个 epoch 中数据的洗牌，结果将无法完全复现。不过，你应该能够得到相似的结果。
- en: Calculate the accuracy of the resulting model on the testing set.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的准确率。
- en: Note
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 211.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可在第211页找到。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: The previous chapter focused on CNNs, which consist of a kind of neural network
    architecture that performs outstandingly well on computer vision problems. It
    started by explaining the main reasons why CNNs are widely used for dealing with
    image datasets, as well as providing an introduction to the different tasks that
    can be solved through their use.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章集中讨论了 CNN（卷积神经网络），这是一种在计算机视觉问题上表现出色的神经网络架构。它首先解释了为什么 CNN 在处理图像数据集时被广泛使用的主要原因，并介绍了可以通过它们解决的不同任务的概述。
- en: Moreover, the chapter explained the different building blocks of the network's
    architecture, starting by explaining the nature of convolutional layers, then
    moving on to pooling layers, and finally explaining the fully connected layers.
    In each section, an explanation of the purpose of each layer was included, as
    well as the code snippets to effectively code the architecture in PyTorch.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章解释了网络架构的不同构建模块，从解释卷积层的性质开始，然后转向池化层，最后解释全连接层。在每个部分中，都包括了每个层的目的解释，以及有效编写
    PyTorch 架构所需的代码片段。
- en: This led to the introduction of an image classification problem to be solved
    using the building blocks previously explained.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致引入一个图像分类问题，使用之前解释过的构建模块来解决。
- en: Next, data augmentation was introduced as a tool to improve a network's performance
    by incrementing the number of training examples, without the need to gather more
    images. This technique focuses on performing some variations on the existing images
    to create "new" images to be fed to the model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，数据增强被引入为一个工具，通过增加训练示例的数量来提高网络性能，而无需收集更多图像。该技术专注于对现有图像进行一些变化，以创建“新”图像供模型使用。
- en: By implementing data augmentation, the second activity of the chapter aimed
    to solve the same image classification problem, with the objective of comparing
    results.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施数据增强，本章的第二个活动旨在解决同一图像分类问题，并旨在比较结果。
- en: Finally, this chapter explained the concept of batch normalization. It consists
    of normalizing the output from each hidden layer in order to speed up learning.
    After explaining the process of applying batch normalization in PyTorch, the last
    activity of this chapter, once again, aimed to solve the same image classification
    problem using batch normalization.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本章解释了批归一化的概念。它包括对每个隐藏层的输出进行归一化，以加快学习速度。在解释如何在 PyTorch 中应用批归一化的过程后，本章的最后一个活动再次旨在使用批归一化解决同一图像分类问题。
