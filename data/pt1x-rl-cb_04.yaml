- en: Temporal Difference and Q-Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差分和 Q-learning
- en: In the previous chapter, we solved MDPs by means of the Monte Carlo method,
    which is a model-free approach that requires no prior knowledge of the environment.
    However, in MC learning, the value function and Q function are usually updated
    until the end of an episode. This could be problematic, as some processes are
    very long or even fail to terminate. We will employ the **temporal difference**
    (**TD**) method in this chapter to solve this issue. In the TD method, we update
    the action values in every time step in an episode, which increases learning efficiency
    significantly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过蒙特卡洛方法解决了马尔可夫决策过程（MDP），这是一种无模型方法，不需要环境的先验知识。然而，在 MC 学习中，价值函数和 Q 函数通常在情节结束之前更新。这可能存在问题，因为有些过程非常长，甚至无法正常结束。在本章中，我们将采用**时间差分**（**TD**）方法来解决这个问题。在
    TD 方法中，我们在每个时间步更新动作值，显著提高了学习效率。
- en: The chapter will start with setting up the Cliff Walking and Windy Gridworld
    environment playgrounds, which will be used in TD control methods as the main
    talking point in this chapter. Through our step-by-step guides, readers will gain
    practical experience of Q-learning for off-policy control, and SARSA for on-policy
    control. We will also work on an interesting project, the Taxi problem, and demonstrate
    how to solve it using Q-learning and the SARSA algorithm, respectively. Finally,
    we will cover the double Q-learning algorithm by way of a bonus.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从设置 Cliff Walking 和 Windy Gridworld 环境的游乐场开始，这些将作为本章 TD 控制方法的主要讨论点。通过我们的逐步指南，读者将获得
    Q-learning 用于离策略控制和 SARSA 用于在策略控制的实际经验。我们还将处理一个有趣的项目——出租车问题，并展示如何分别使用 Q-learning
    和 SARSA 算法来解决它。最后，我们将额外介绍双 Q-learning 算法。
- en: 'We will cover of the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将包括以下的步骤：
- en: Setting up the Cliff Walking environment playground
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Cliff Walking 环境的游乐场
- en: Developing the Q-learning algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发 Q-learning 算法
- en: Setting up the Windy Gridworld environment playground
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Windy Gridworld 环境的游乐场
- en: Developing the SARSA algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发 SARSA 算法
- en: Solving the Taxi problem with Q-learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 Q-learning 解决出租车问题
- en: Solving the Taxi problem with SARSA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 SARSA 解决出租车问题
- en: Developing the Double Q-learning algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发 Double Q-learning 算法
- en: Setting up the Cliff Walking environment playground
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 Cliff Walking 环境的游乐场
- en: In the first recipe, we will start by getting familiar with the Cliff Walking
    environment, which we will solve with TD methods in upcoming recipes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个步骤中，我们将开始熟悉 Cliff Walking 环境，我们将在后续步骤中使用 TD 方法来解决它。
- en: Cliff Walking is a typical `gym` environment, with long episodes without a guarantee
    of termination. It is a grid problem with a 4 * 12 board. An agent makes a move
    up, right, down, and left at a step. The bottom-left tile is the starting point
    for the agent, and the bottom-right is the winning point where an episode will
    end if it is reached. The remaining tiles in the last row are cliffs where the
    agent will be reset to the starting position after stepping on any of them, but
    the episode continues. Each step the agent takes incurs a -1 reward, with the
    exception of stepping on the cliffs, where a -100 reward is incurred.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Cliff Walking 是一个典型的 `gym` 环境，具有长时间的不确定结束的情节。这是一个 4 * 12 的网格问题。一个代理在每一步可以向上、向右、向下和向左移动。左下角的方块是代理的起点，右下角是获胜的点，如果到达则会结束一个情节。最后一行剩余的方块是悬崖，代理踩上其中任何一个后会被重置到起始位置，但情节仍继续。每一步代理走的时候会产生一个
    -1 的奖励，除非踩到悬崖，那么会产生 -100 的奖励。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To run the Cliff Walking environment, let's first search for its name in the
    table of environments at [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    We get `CliffWalking-v0` and also know that the observation space is represented
    by an integer ranging from 0 (top-left tile) to 47 (bottom-right goal tile), and
    that there are four possible actions (up = 0, right = 1, down = 2, and left =
    3).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Cliff Walking 环境，首先在[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)表格中搜索它的名称。我们得到
    `CliffWalking-v0`，并且知道观察空间由整数表示，范围从 0（左上角方块）到 47（右下角目标方块），有四个可能的动作（上 = 0，右 = 1，下
    = 2，左 = 3）。
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做...
- en: 'Let''s simulate the Cliff Walking environment by performing the following steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤模拟 Cliff Walking 环境：
- en: 'We import the Gym library and create an instance of the Cliff Walking environment
    :'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入 Gym 库，并创建一个 Cliff Walking 环境的实例：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we reset the environment:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们重置环境：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The agent starts with state 36 as the bottom-left tile.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 代理从状态 36 开始，作为左下角的瓷砖。
- en: 'Then, we render the environment:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们渲染环境：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s now make a down movement regardless, even though it is not walkable:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，无论是否可行走，让我们进行一个向下的移动：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The agent stays still. Now, print out what we have just obtained:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 代理保持不动。现在，打印出我们刚刚获得的内容：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Again, each movement incurs a -1 reward:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，每个移动都会导致 -1 的奖励：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The episode is not done as the agent has not yet reached their goal:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该 episode 还没有完成，因为代理人还没有达到目标：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This means that the movement is deterministic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着移动是确定性的。
- en: 'Now, let''s perform an up movement since it is walkable:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行一个向上的移动，因为它是可行走的：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Print out what we have just obtained:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出我们刚刚获得的内容：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The agent moves up:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 代理人向上移动：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This incurs a -1 reward.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致 -1 的奖励。
- en: 'Now let''s try and make a right and a down movement:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们尝试向右和向下移动：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The agent stepped on the cliff, so was reset to the starting point and received
    a reward of -100:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 代理人踩到了悬崖，因此被重置到起点并获得了 -100 的奖励：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, let''s try to take the shortest path to reach the goal:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们尝试以最短路径达到目标：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How it works...
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In *Step 1*, we import the Gym library and create an instance of the Cliff Walking
    environment. Then, we reset the environment in *Step 2*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们导入 Gym 库并创建 Cliff Walking 环境的实例。然后，在*步骤2*中重置环境。
- en: 'In *Step 3*, we render the environment and you will see a 4 * 12 matrix as
    follows, representing a grid with the starting tile (x) where the agent is standing,
    the goal tile (T), 10 cliff tiles (C), and regular tiles (o):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们渲染环境，你会看到一个 4 * 12 的矩阵如下，表示一个网格，其中包括起始瓷砖（x）代表代理人所站的位置，目标瓷砖（T），10 个悬崖瓷砖（C），以及常规瓷砖（o）：
- en: '![](img/53ae5a1c-7019-4758-ae5f-be6045b9ffe7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53ae5a1c-7019-4758-ae5f-be6045b9ffe7.png)'
- en: In *Steps 4*, *5*, and *6*, we made various moves and saw the various outcomes
    of these movements and the rewards received.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*、*5*和*6*中，我们进行了各种移动，并看到了这些移动的各种结果和收到的奖励。
- en: As you could imagine, a Cliff Walking episode can be very long or even endless,
    since stepping on a cliff will reset the game. And the earlier the goal is reached
    the better, because each step will result in a reward of -1 or -100\. In the next
    recipe, we will solve the Cliff Walking problem with the help of a temporal difference
    method.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想象的，Cliff Walking 的一个场景可能会非常长，甚至是无限的，因为一旦踩到悬崖就会重置游戏。尽早达到目标是更好的，因为每走一步都会导致奖励为
    -1 或者 -100。在下一个实例中，我们将通过时间差分方法解决 Cliff Walking 问题。
- en: Developing the Q-learning algorithm
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发 Q-learning 算法
- en: Temporal difference (TD) learning is also a model-free learning algorithm, just
    like MC learning. You will recall that Q-function is updated at the end of the
    entire episode in MC learning (either in first - visit or every - visit mode).
    The main advantage of TD learning is that it updates the Q-function for every
    step in an episode.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 时间差分（TD）学习也是一种无模型学习算法，就像 MC 学习一样。你会记得，在 MC 学习中，Q 函数在整个 episode 结束时更新（无论是首次访问还是每次访问模式）。TD
    学习的主要优势在于它在 episode 中的每一步都更新 Q 函数。
- en: 'In this recipe, we will look into a popular TD method called **Q-****learning**.
    Q-learning is an off-policy learning algorithm. It updates the Q-function based
    on the following equation:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将介绍一种名为**Q-learning**的流行时间差分方法。Q-learning 是一种离策略学习算法。它根据以下方程更新 Q 函数：
- en: '![](img/ffb84f3a-a0f1-4f6d-a6ca-93c073b15b3e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffb84f3a-a0f1-4f6d-a6ca-93c073b15b3e.png)'
- en: Here, s' is the resulting state after taking action, a, in state s; r is the
    associated reward; α is the learning rate; and γ is the discount factor. Also,
    [![](img/99eb366b-52d8-48ec-ac69-981d19f676d5.png)] means that the behavior policy
    is greedy, where the highest Q-value among those in state s' is selected to generate
    learning data. In Q-learning, actions are taken according to the epsilon-greedy
    policy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，s' 是采取动作 a 后的结果状态 s；r 是相关的奖励；α 是学习率；γ 是折扣因子。此外，[![](img/99eb366b-52d8-48ec-ac69-981d19f676d5.png)]
    意味着行为策略是贪婪的，选择状态 s' 中最高的 Q 值来生成学习数据。在 Q-learning 中，动作是根据 epsilon-greedy 策略执行的。
- en: How to do it...
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We perform Q-learning to solve the Cliff Walking environment as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 Q-learning 解决 Cliff Walking 环境如下：
- en: 'Import the PyTorch and Gym libraries and create an instance of the Cliff Walking
    environment:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 和 Gym 库，并创建 Cliff Walking 环境的实例：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s start by defining the epsilon-greedy policy:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从定义 epsilon-greedy 策略开始：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, define the function that performs Q-learning:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在定义执行 Q-learning 的函数：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We specify the discount rate as `1`, the learning rate as `0.4`, and epsilon
    as `0.1`; and we simulate 500 episodes:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折扣率设为`1`，学习率设为`0.4`，ε设为`0.1`；然后模拟500个回合：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we create an instance of the epsilon-greedy policy:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建ε-贪心策略的一个实例：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we perform Q-learning with input parameters defined previously and
    print out the optimal policy:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用之前定义的输入参数执行Q学习，并打印出最优策略：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In *Step 2*, the epsilon-greedy policy takes in a parameter, ε, with a value
    from 0 to 1, and |A|, the number of possible actions. Each action is taken with
    a probability of ε/|A|, and the action with the highest state-action value is
    chosen with a probability of 1-ε+ε/|A|.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤2*中，ε-贪心策略接受一个参数ε，其值从0到1，|A|是可能动作的数量。每个动作的概率为ε/|A|，并且以1-ε+ε/|A|的概率选择具有最高状态-动作值的动作。
- en: 'In *Step 3*, we perform Q-learning in the following tasks:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤3*中，我们在以下任务中执行Q学习：
- en: We initialize the Q-table with all zeros.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用全零初始化Q表。
- en: In each episode, we let the agent follow the epsilon-greedy policy to choose
    what action to take. And we update the Q function for each step.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个回合中，我们让代理根据ε-贪心策略选择动作。然后，我们针对每个步骤更新Q函数。
- en: We run `n_episode` episodes.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们运行`n_episode`个回合。
- en: We obtain the optimal policy based on the optimal Q function.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们基于最优的Q函数获得了最优策略。
- en: 'In *Step 6*, again, up = 0, right = 1, down = 2, and left = 3; thus, following
    the optimal policy, the agent starts in state 36, then moves up to state 24, and
    then all the way right to state 35, and finally reaches the goal by moving down:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤6*中，再次，up = 0，right = 1，down = 2，left = 3；因此，根据最优策略，代理从状态36开始，然后向上移动到状态24，然后向右一直移动到状态35，最后向下到达目标：
- en: '![](img/1203edde-ed7a-4487-bbad-d57a34717a32.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1203edde-ed7a-4487-bbad-d57a34717a32.png)'
- en: As you can see in Q-learning , it optimizes the Q function by learning from
    the experience generated by another policy. This is quite similar to the off-policy
    MC control method. The difference is that it updates the Q function on the fly,
    instead of after the entire episode. It is considered advantageous for environments
    with long episodes where it is inefficient to delay learning until the end of
    an episode. In every single step in Q-learning (or any other TD method), we gain
    more information about the environment and use this information to update values
    right away. In our case, we obtained the optimal policy by running only 500 learning
    episodes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中可以看到，它通过学习由另一个策略生成的经验来优化Q函数。这与离策略MC控制方法非常相似。不同之处在于，它实时更新Q函数，而不是在整个回合结束后。这被认为是有利的，特别是对于回合时间较长的环境，延迟学习直到回合结束是低效的。在Q学习（或任何其他TD方法）的每一个步骤中，我们都会获取更多关于环境的信息，并立即使用此信息来更新值。在我们的案例中，通过仅运行500个学习回合，我们获得了最优策略。
- en: There's more...
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In fact, the optimal policy was obtained after around 50 episodes. We can plot
    the length of each episode over time to verify this. The total reward obtained
    in each episode over time is also an option.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在大约50个回合后获得了最优策略。我们可以绘制每个回合的长度随时间变化的图表来验证这一点。还可以选择随时间获得的每个回合的总奖励。
- en: 'We define two lists to store the length and total reward for each episode,
    respectively:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义两个列表分别存储每个回合的长度和总奖励：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We keep track of the length and total reward for each episode during learning.
    The following is the updated version of `q_learning`:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在学习过程中跟踪每个回合的长度和总奖励。以下是更新版本的`q_learning`：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, display the plot of episode lengths over time:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，展示随时间变化的回合长度的图表：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will result in the following plot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下绘图：
- en: '![](img/fbc47255-bf5d-48f9-a38c-d48fc25d7d53.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbc47255-bf5d-48f9-a38c-d48fc25d7d53.png)'
- en: 'Display the plot of episode rewards over time:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展示随时间变化的回合奖励的图表：
- en: '[PRE22]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will result in the following plot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下绘图：
- en: '![](img/9c6ecfbc-6646-495d-8522-36907e5ad4b2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c6ecfbc-6646-495d-8522-36907e5ad4b2.png)'
- en: Again, if you reduce the value of epsilon, you will see smaller fluctuations,
    which are the effects of random exploration in the epsilon-greedy policy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，如果减小ε的值，您将看到较小的波动，这是ε-贪心策略中随机探索的效果。
- en: Setting up the Windy Gridworld environment playground
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置多风格格子世界环境的游乐场
- en: In the previous recipe, we solved a relatively simple environment where we can
    easily obtain the optimal policy. In this recipe, let's simulate a more complex
    grid environment, Windy Gridworld, where an external force moves the agent from
    certain tiles. This will prepare us to search for the optimal policy using the
    TD method in the next recipe.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们解决了一个相对简单的环境，在那里我们可以很容易地获取最优策略。在这个示例中，让我们模拟一个更复杂的网格环境，风格网格世界，在这个环境中，外部力会将代理从某些瓦片移开。这将为我们在下一个示例中使用TD方法搜索最优策略做准备。
- en: 'Windy Gridworld is a grid problem with a 7 * 10 board, which is displayed as
    follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 风格网格世界是一个7 * 10的棋盘问题，显示如下：
- en: '![](img/7288ee40-bc04-447a-8be8-a68e272a40bf.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7288ee40-bc04-447a-8be8-a68e272a40bf.png)'
- en: An agent makes a move up, right, down, and left at a step. Tile 30 is the starting
    point for the agent, and tile 37 is the winning point where an episode will end
    if it is reached. Each step the agent takes incurs a -1 reward.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在每一步可以向上、向右、向下和向左移动。第30块瓦片是代理的起始点，第37块瓦片是获胜点，如果达到则一个episode结束。每一步代理走动会产生-1的奖励。
- en: The complexity in this environment is that there is extra wind force in columns
    4 to 9\. Moving from tiles on those columns, the agent will experience an extra
    push upward. The wind force in the seventh and eighth columns is 1, and the wind
    force in the fourth, fifth, sixth, and ninth columns is 2\. For example, if the
    agent tries to move right from state 43, they will land in state 34; if the agent
    tries to move left from state 48, they will land in state 37; if the agent tries
    to move up from state 67, they will land in state 37 as the agent receives an
    additional 2-unit force upward; if the agent tries to move down from state 27,
    they will land in state 17, as the 2 extra force upward offsets 1 downward.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中的复杂性在于，第4至9列有额外的风力。从这些列的瓦片移动时，代理会额外受到向上的推力。第7和第8列的风力为1，第4、5、6和9列的风力为2。例如，如果代理试图从状态43向右移动，它们将会落在状态34；如果代理试图从状态48向左移动，它们将会落在状态37；如果代理试图从状态67向上移动，它们将会落在状态37，因为代理会受到额外的2单位向上的力；如果代理试图从状态27向下移动，它们将会落在状态17，因为额外的2单位向上力抵消了1单位向下力。
- en: 'Currently, Windy Gridworld is not included in the Gym environment. We will
    implement it by taking the Cliff Walking environment as a reference: [https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，风格网格世界还没有包含在Gym环境中。我们将通过参考Cliff Walking环境来实现它：[https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py)。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s develop the Windy Gridworld environment:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开发风格网格世界环境：
- en: 'Import the necessary modules, NumPy, and the `discrete` class, from Gym:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Gym中导入必要的模块，NumPy和`discrete`类：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define four actions:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义四个动作：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s start by defining the `__init__` method in the `WindyGridworldEnv` class:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从在 `WindyGridworldEnv` 类中定义 `__init__` 方法开始：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This defines the observation space, the wind areas and forces, the transition
    and reward matrices, and the initial state.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了观察空间、风区域和风力、转移和奖励矩阵，以及初始状态。
- en: 'Next, we define the `_calculate_transition_prob` method to determines the outcome
    for an action, including the probability (which is 1), the new state, the reward
    (which is always -1), and whether it is complete:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义 `_calculate_transition_prob` 方法来确定动作的结果，包括概率（为1），新状态，奖励（始终为-1），以及是否完成：
- en: '[PRE26]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This computes the state based on the current state, movement, and wind effect,
    and ensures that the new position is within the grid. Finally, it checks whether
    the agent has reached the goal state.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这计算基于当前状态、移动和风效应的状态，并确保新位置在网格内。最后，它检查代理是否达到目标状态。
- en: 'Next, we define the `_limit_coordinates` method, which prevents the agent from
    falling out of the grid world:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义 `_limit_coordinates` 方法，用于防止代理掉出网格世界：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we add the `render` method in order to display the agent and the grid
    environment:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们添加 `render` 方法以显示代理和网格环境：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`X` represents the agent''s current position, `T` is the goal tile, and the
    remaining tiles are denoted as `o`.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`X` 表示代理当前的位置，`T` 是目标瓦片，其余瓦片表示为 `o`。'
- en: 'Now, let''s simulate the Windy Gridworld environment in the following steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们按以下步骤模拟风格网格世界环境：
- en: 'Create an instance of the Windy Gridworld environment:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个风格网格世界环境的实例：
- en: '[PRE29]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reset and render the environment:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置并渲染环境：
- en: '[PRE30]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The agent starts with state 30.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 代理器从状态 30 开始。
- en: 'Make a right movement:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向右移动一步：
- en: '[PRE31]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The agent lands in state 31, with a reward of -1.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 代理器降落在状态 31，奖励为 -1。
- en: 'Make two right moves:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 右移两步：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, make another right move:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，再向右移动一步：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With a 1-unit wind upward, the agent lands in state 24.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 风向上 1 单位，代理器降落在状态 24。
- en: Feel free to play around with the environment until the goal is reached.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试环境，直到达到目标。
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理……
- en: We just developed a grid environment similar to Cliff Walking. The difference
    between Windy Gridworld and Cliff Walking is the extra upward push. Each action
    in a Windy Gridworld episode will result in a reward of -1\. Hence, it is better
    to reach the goal earlier. In the next recipe, we will solve the Windy Gridworld
    problem with another TD control method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开发了一个类似于 Cliff Walking 的网格环境。Windy Gridworld 和 Cliff Walking 的区别在于额外的向上推力。每个动作在
    Windy Gridworld 剧集中将导致奖励 -1。因此，尽快达到目标更为有效。在下一个步骤中，我们将使用另一种 TD 控制方法解决 Windy Gridworld
    问题。
- en: Developing the SARSA algorithm
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发 SARSA 算法
- en: You will recall that Q-learning is an off-policy TD learning algorithm. In this
    recipe, we will solve an MDP with an on-policy TD learning algorithm, called **State-Action-Reward-State-Action**
    (**SARSA**).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得 Q-learning 是一种离策略 TD 学习算法。在本配方中，我们将使用一种在线策略 TD 学习算法解决 MDP，称为 **状态-行动-奖励-状态-行动**（**SARSA**）。
- en: 'Similar to Q-learning, SARSA focuses on state-action values. It updates the
    Q-function based on the following equation:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Q-learning，SARSA 关注状态-动作值。它根据以下方程更新 Q 函数：
- en: '![](img/81d5cfe8-f773-4d98-91fd-fc31a6e64ef9.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81d5cfe8-f773-4d98-91fd-fc31a6e64ef9.png)'
- en: Here, `s'` is the resulting state after taking the action, a, in state s; r
    is the associated reward; α is the learning rate; and γ is the discount factor.
    You will recall that in Q-learning, a behavior-greedy policy, [![](img/0d67589d-944b-4f62-bc0c-3ec728e5bbc7.png)],
    is used to update the Q value. In SARSA, we simply pick up the next action, `a'`,
    by also following an epsilon-greedy policy to update the Q value. And the action
    a' is taken in the next step. Hence, SARSA is an on-policy algorithm.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`s'` 是在状态 s 中采取动作 a 后的结果状态；r 是相关的奖励；α 是学习率；γ 是折扣因子。你会记得，在 Q-learning 中，一种行为贪婪策略
    [![](img/0d67589d-944b-4f62-bc0c-3ec728e5bbc7.png)] 用于更新 Q 值。在 SARSA 中，我们简单地通过遵循
    epsilon-greedy 策略来选择下一个动作 `a'` 来更新 Q 值。然后动作 `a'` 在下一步中被执行。因此，SARSA 是一个在线策略算法。
- en: How to do it...
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'We perform SARSA to solve the Windy Gridworld environment as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行 SARSA 解决 Windy Gridworld 环境，步骤如下：
- en: 'Import PyTorch and `WindyGridworldEnvmodule` (assuming that it is in a file
    called `windy_gridworld.py`), and create an instance of the Windy Gridworld environment:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 和 `WindyGridworldEnvmodule`（假设它在名为 `windy_gridworld.py` 的文件中），并创建
    Windy Gridworld 环境的实例：
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s start by defining the epsilon-greedy behavior policy:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从定义 epsilon-greedy 行为策略开始：
- en: '[PRE35]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We specify the number of episodes and initialize two variables used to track
    the length and total reward for each episode:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定了要运行的剧集数，并初始化了用于跟踪每一剧集的长度和总奖励的两个变量：
- en: '[PRE36]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we define the function that performs SARSA:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义执行 SARSA 的函数：
- en: '[PRE37]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We specify the discount rate as 1, the learning rate as 0.4, and epsilon as
    0.1:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折扣率指定为 1，学习率为 0.4，epsilon 为 0.1：
- en: '[PRE38]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we create an instance of the epsilon-greedy policy:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建 epsilon-greedy 策略的实例：
- en: '[PRE39]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, we perform SARSA with input parameters defined in the previous steps
    and print out the optimal policy:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用之前步骤中定义的输入参数执行 SARSA，并打印出最优策略：
- en: '[PRE40]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理……
- en: 'In *Step 4*, the SARSA function does the following tasks:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 4* 中，SARSA 函数执行以下任务：
- en: It initializes the Q-table with all zeros.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用全零初始化 Q 表。
- en: In each episode, it lets the agent follow the epsilon-greedy policy to choose
    what action to take. And for each step, it updates the Q function based on the
    equation [![](img/bb3e7ae0-26fd-4ac9-9ec4-35300e156091.png)], where `a'` is selected
    on the basis of the epsilon-greedy policy. The new action, a', is then taken in
    the new state, `s'`.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一剧集中，它让代理器遵循 epsilon-greedy 策略来选择采取的行动。对于每一步，它根据方程 [![](img/bb3e7ae0-26fd-4ac9-9ec4-35300e156091.png)]
    更新 Q 函数，其中 `a'` 是根据 epsilon-greedy 策略选择的。然后，在新状态 `s'` 中采取新的动作 `a'`。
- en: We run `n_episode` episodes.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们运行 `n_episode` 个剧集。
- en: We obtain the optimal policy based on the optimal Q function.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们基于最优 Q 函数获取最优策略。
- en: As you can see in the SARSA method, it optimizes the Q function by taking the
    action chosen under the same policy, the epsilon-greedy policy. This is quite
    similar to the on-policy MC control method. The difference is that it updates
    the Q function by small derivatives in individual steps, rather than after the
    entire episode. It is considered advantageous for environments with long episodes
    where it is inefficient to delay learning until the end of an episode. In every
    single step in SARSA, we gain more information about the environment and use this
    information to update values right away. In our case, we obtained the optimal
    policy by running only 500 learning episodes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 SARSA 方法中所见，它通过采取在相同策略下选择的动作来优化 Q 函数，即 epsilon 贪婪策略。这与 on-policy MC 控制方法非常相似。不同之处在于，它通过个别步骤中的小导数来更新
    Q 函数，而不是在整个集结束后。在集合长度较长的环境中，此方法被认为是优势，因为将学习延迟到集的结束是低效的。在 SARSA 的每一个单步中，我们获得更多关于环境的信息，并利用这些信息立即更新值。在我们的案例中，仅通过运行
    500 个学习集，我们获得了最优策略。
- en: There's more...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In fact, the optimal policy was obtained after around 200 episodes. We can
    plot the length and total reward for each episode over time to verify this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在大约 200 集后获得了最优策略。我们可以绘制每一集的长度和总奖励随时间变化的图表来验证这一点：
- en: 'Display a plot of episode lengths over time:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示随时间变化的剧集长度图：
- en: '[PRE41]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This will result in the following plot:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下图表：
- en: '![](img/48463089-864f-44cd-b295-53c5d038d2b7.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48463089-864f-44cd-b295-53c5d038d2b7.png)'
- en: You can see that the episode length starts to saturate after 200 episodes. Note
    that those small fluctuations are due to random exploration in the epsilon-greedy
    policy.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，剧集长度在 200 集后开始饱和。请注意，这些小波动是由 epsilon 贪婪策略中的随机探索造成的。
- en: 'Display a plot of episode rewards over time:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示随时间变化的剧集奖励图：
- en: '[PRE42]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will result in the following plot:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致以下图表：
- en: '![](img/3e027de5-e74f-4446-bac7-88045c5dda85.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e027de5-e74f-4446-bac7-88045c5dda85.png)'
- en: Again, if you reduce the value of epsilon, you will see smaller fluctuations,
    which are the effects of random exploration in the epsilon-greedy policy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你减小 epsilon 的值，你将看到更小的波动，这是在 epsilon 贪婪策略中随机探索的影响。
- en: In the upcoming two recipes, we will use the two TD methods we just learned
    to solve a more complex environment with more possible states and actions. Let's
    start with Q-learning.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个示例中，我们将使用我们刚学到的两种 TD 方法来解决一个更复杂的环境，该环境具有更多的可能状态和动作。让我们从 Q 学习开始。
- en: Solving the Taxi problem with Q-learning
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q 学习解决出租车问题
- en: 'The Taxi problem ([https://gym.openai.com/envs/Taxi-v2/](https://gym.openai.com/envs/Taxi-v2/))
    is another popular grid world problem. In a 5 * 5 grid, the agent acts as a taxi
    driver to pick up a passenger at one location and then drop the passenger off
    at their destination. Take a look at the following example:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车问题 ([https://gym.openai.com/envs/Taxi-v2/](https://gym.openai.com/envs/Taxi-v2/))
    是另一个流行的网格世界问题。在一个 5 * 5 的网格中，代理作为出租车司机，在一个位置接载乘客，然后将乘客送达目的地。看下面的例子：
- en: '![](img/d021faec-188e-42a1-b5a9-df0d237fcef6.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d021faec-188e-42a1-b5a9-df0d237fcef6.png)'
- en: 'Colored tiles have the following meanings:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色方块有以下含义：
- en: '**Yellow**: The starting position of the taxi. The starting location is random
    in each episode.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**黄色**：出租车的起始位置。起始位置在每一集中是随机的。'
- en: '**Blue**: The position of the passenger. It is also randomly selected in each
    episode.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝色**：乘客的位置。在每一集中也是随机选择的。'
- en: '**Purple**: The destination of the passenger. Again, it is randomly selected
    in each episode.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**紫色**：乘客的目的地。同样，在每一集中随机选择。'
- en: '**Green**: The position of the taxi with the passenger.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绿色**：带有乘客的出租车位置。'
- en: The four letters R, Y, B, and G indicate the only tiles that allow picking up
    and dropping off the passenger. One of them is the destination, and one is where
    the passenger is located.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: R、Y、B 和 G 这四个字母指示唯一允许接载和送达乘客的方块。其中一个是目的地，一个是乘客的位置。
- en: 'The taxi can take the following six deterministic actions:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车可以采取以下六个确定性动作：
- en: '**0**: Moving south'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0**：向南移动'
- en: '**1**: Moving north'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1**：向北移动'
- en: '**2**: Moving east'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2**：向东移动'
- en: '**3**: Moving west'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3**：向西移动'
- en: '**4**: Picking up the passenger'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4**：接载乘客'
- en: '**5**: Dropping of the passenger'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5**：送达乘客'
- en: There is a pillar | between two tiles, which prevents the taxi from moving from
    one tile to another.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 两个方块之间有一根柱子 |，防止出租车从一个方块移动到另一个方块。
- en: 'The reward for each step is generally -1, with the following exceptions:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步的奖励通常是 -1，以下是例外情况：
- en: '**+20**: The passenger is delivered to their destination. And an episode will
    end.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**+20**：乘客被送达目的地。一个回合将结束。'
- en: '**-10**: Attempted illegal pick-up or drop-off (not on any of R, Y, B, or G).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**-10**：尝试非法接乘或下车（不在 R、Y、B 或 G 中）。'
- en: One more thing to note is that the observation space is a lot larger than 25
    (5*5) since we should also consider the location of the passenger and the destination,
    and whether the taxi is empty or full. Hence, the observation space should be
    25 * 5 (4 possible locations for the passenger or already in the taxi) * 4 (destinations)
    = 500 dimensions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件需要注意的事情是，观察空间远远大于 25（5*5），因为我们还应考虑乘客和目的地的位置，以及出租车是否为空或已满。因此，观察空间应为 25 *
    5（乘客或已在出租车的 4 个可能位置） * 4（目的地）= 500 维度。
- en: Getting ready
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: To run the Taxi environment, let's first search for its name in the table of
    environments, [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    We get Taxi-v2 and also know that the observation space is represented by an integer
    ranging from 0 to 499, and that there are four possible actions (up = 0, right
    = 1, down = 2, and left = 3).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行出租车环境，让我们首先在环境表中搜索其名称，[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)。我们得到
    Taxi-v2，并且知道观察空间由一个从 0 到 499 的整数表示，并且有四种可能的动作（向上 = 0，向右 = 1，向下 = 2，向左 = 3）。
- en: How to do it...
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Let''s start by simulating the Taxi environment in the following steps:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从以下步骤开始模拟出租车环境：
- en: 'We import the Gym library and create an instance of the Taxi environment:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入 Gym 库并创建出租车环境的实例：
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, we reset the environment:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们重置环境：
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we render the environment:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们渲染环境：
- en: '[PRE45]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'You will see a similar 5 * 5 matrix as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个类似的 5 * 5 矩阵如下：
- en: '![](img/5775055b-af41-40d0-b307-3c76e94ceafd.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5775055b-af41-40d0-b307-3c76e94ceafd.png)'
- en: The passenger is in the R location, and the destination is in Y. You will see
    something different as the initial state is randomly generated.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 乘客位于 R 位置，目的地位于 Y。由于初始状态是随机生成的，您将看到不同的结果。
- en: 'Let''s now go and pick up the passenger by heading west for three tiles and
    north for two tiles (you can adjust this according to your initial state) and
    then executing the pick-up. Then, we render the environment again:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们向西移动三个瓷砖，向北移动两个瓷砖去接乘客（您可以根据初始状态进行调整），然后执行接乘客。接着，我们再次渲染环境：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You will see the latest matrix updated (again, you may get different output
    depending on your initial state):'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将看到更新的最新矩阵（根据您的初始状态可能会得到不同的输出）：
- en: '![](img/0964a8a6-b6e1-4b20-8659-ff3359e93e8e.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0964a8a6-b6e1-4b20-8659-ff3359e93e8e.png)'
- en: The taxi turns green.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车变成了绿色。
- en: 'Now, we go to the destination by heading south for four tiles (you can adjust
    this to your initial state) and then executing the drop-off:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们向南移动四个瓷砖去到达目的地（您可以根据初始状态进行调整），然后执行下车：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It finally receives a +20 reward and the episode ends.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后它获得 +20 的奖励，并且回合结束。
- en: 'Now, we render the environment:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们渲染环境：
- en: '[PRE48]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You will see the following updated matrix:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下更新的矩阵：
- en: '![](img/1fd1bd64-18eb-4eb0-8604-470dc0f1a587.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fd1bd64-18eb-4eb0-8604-470dc0f1a587.png)'
- en: 'We will now perform Q-learning to solve the Taxi environment as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将执行 Q 学习来解决出租车环境，如下所示：
- en: 'Import the PyTorch library:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 库：
- en: '[PRE49]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Then, start defining the epsilon-greedy policy. We will reuse the `gen_epsilon_greedy_policy`
    function defined in the *Developing the Q-learning algorithm* recipe.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始定义 epsilon-greedy 策略。我们将重用“开发 Q 学习算法”食谱中定义的 `gen_epsilon_greedy_policy`
    函数。
- en: 'Now, we specify the number of episodes and initialize two variables used to
    track the length and total reward for each episode:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们指定回合的数量，并初始化用于跟踪每个回合长度和总奖励的两个变量：
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Next, we define the function that performs Q-learning. We will reuse the `q_learning`
    function defined in the *Developing Q-learning algorithm* recipe.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义执行 Q 学习的函数。我们将重用“开发 Q 学习算法”食谱中定义的 `q_learning` 函数。
- en: 'Now, we specify the rest of the parameters, including the discount rate, learning
    rate, and epsilon, and create an instance of the epsilon-greedy-policy:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们指定其余的参数，包括折扣率、学习率和 epsilon，并创建一个 epsilon-greedy 策略的实例：
- en: '[PRE51]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Finally, we perform Q-learning to obtain the optimal policy for the taxi problem:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们进行 Q 学习来获得出租车问题的最优策略：
- en: '[PRE52]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: How it works...
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理…
- en: In this recipe, we solve the Taxi problem via off-policy Q-learning.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们通过离线 Q 学习解决了出租车问题。
- en: 'After *Step 6*, you can plot the length and total reward for each episode over
    time to verify whether the model converges. The plot of episode lengths over time
    is displayed as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤6*之后，您可以绘制每个周期的长度和总奖励，以验证模型是否收敛。时间序列的奖励图如下所示：
- en: '![](img/290869c8-ea60-4eb4-b33f-f4879ea04597.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/290869c8-ea60-4eb4-b33f-f4879ea04597.png)'
- en: 'The plot of episode rewards over time is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的奖励图如下所示：
- en: '![](img/c337e895-8933-4081-9245-e602be5f8336.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c337e895-8933-4081-9245-e602be5f8336.png)'
- en: You can see that the optimization starts to saturate after 400 episodes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，优化在400个周期后开始饱和。
- en: The Taxi environment is a relatively complex grid problem with 500 discrete
    states and 6 possible actions. Q-learning optimizes the Q function in every single
    step in an episode by learning from the experience generated by a greedy policy.
    We gain information about the environment during the learning process and use
    this information to update the values right away by following the epsilon-greedy
    policy.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车环境是一个相对复杂的网格问题，有500个离散状态和6种可能的动作。Q-learning 通过学习贪婪策略生成的经验来优化每个步骤中的 Q 函数。我们在学习过程中获取环境信息，并使用这些信息按照ε-贪婪策略立即更新值。
- en: Solving the Taxi problem with SARSA
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SARSA 解决出租车问题
- en: In this recipe, we will solve the Taxi environment with the SARSA algorithm
    and fine-tune the hyperparameters with the grid search algorithm.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 SARSA 算法解决出租车环境，并使用网格搜索算法微调超参数。
- en: We will start with our default set of hyperparameter values under the SARSA
    model. These are selected based on intuition and a number of trials. Moving on,
    we will come up with the best set of values.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 SARSA 模型的默认超参数值开始。这些值是基于直觉和一些试验选择的。接下来，我们将提出最佳值的一组值。
- en: How to do it...
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We perform SARSA to solve the Taxi environment as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下方式执行 SARSA 来解决出租车环境：
- en: 'Import PyTorch and the `gym` module, and create an instance of the Taxi environment:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 PyTorch 和`gym`模块，并创建出租车环境的一个实例：
- en: '[PRE53]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Then, start defining the epsilon-greedy behavior policy. We will reuse the `gen_epsilon_greedy_policy`
    function defined in the *Developing the SARSA algorithm* recipe.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始定义 ε-贪婪行为策略。我们将重用*开发 SARSA 算法*配方中定义的`gen_epsilon_greedy_policy`函数。
- en: 'We then specify the number of episodes and initialize two variables used to
    track the length and total reward for each episode:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定要追踪每个周期的长度和总奖励的两个变量的数量：
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Now, we define the function that performs SARSA. We will reuse the `sarsa` function
    defined in the *Developing the SARSA algorithm* recipe.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义执行 SARSA 的函数。我们将重用*开发 SARSA 算法*配方中定义的`sarsa`函数。
- en: 'We specify the discount rate as `1`, the default learning rate as `0.4`, and
    the default epsilon as `0.1`:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将折现率设定为`1`，默认学习率设定为`0.4`，默认 ε 设定为`0.1`：
- en: '[PRE55]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we create an instance of the epsilon-greedy-policy:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建 ε-贪婪策略的一个实例：
- en: '[PRE56]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Finally, we perform SARSA with input parameters defined in the previous steps:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用前面步骤中定义的输入参数执行 SARSA：
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: How it works...
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'After *Step 7*, you can plot the length and total reward for each episode over
    time to verify whether the model converges. A plot of episode lengths over time
    is displayed as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤7*之后，您可以绘制每个周期的长度和总奖励，以验证模型是否收敛。时间序列的奖励图如下所示：
- en: '![](img/a1b128c6-714e-43b5-ba65-2c85592c397c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1b128c6-714e-43b5-ba65-2c85592c397c.png)'
- en: 'The plot of episode rewards over time is as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列的奖励图如下所示：
- en: '![](img/daeaa720-0ccd-4356-8198-a0e2495505ca.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daeaa720-0ccd-4356-8198-a0e2495505ca.png)'
- en: This SARSA model works fine, but is not necessarily the best. We will later
    use grid search to search for the best set of hyperparameters under the SARSA
    model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 SARSA 模型工作得很好，但不一定是最好的。稍后，我们将使用网格搜索来寻找 SARSA 模型下最佳的一组超参数。
- en: The Taxi environment is a relatively complex grid problem with 500 discrete
    states and 6 possible actions. The SARSA algorithm optimizes the Q function in
    every single step in an episode by learning and optimizing the target policy.
    We gain information about the environment during the learning process and use
    this information to update values right away by following the epsilon-greedy policy.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 出租车环境是一个相对复杂的网格问题，有500个离散状态和6种可能的动作。SARSA 算法通过学习和优化目标策略来优化每个步骤中的 Q 函数。我们在学习过程中获取环境信息，并使用这些信息按照ε-贪婪策略立即更新值。
- en: There's more...
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Grid search is a programmatic way to find the best set of values for hyperparameters
    in reinforcement learning. The performance of each set of hyperparameters is measured
    by the following three metrics:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索是一种程序化的方法，用于在强化学习中找到超参数的最佳值集合。每组超参数的性能由以下三个指标来衡量：
- en: 'Average total reward over the first few episodes: We want to get the largest
    reward as early as possible.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前几个episode的平均总奖励：我们希望尽早获得最大的奖励。
- en: 'Average episode length over the first few episodes: We want the taxi to reach
    the destination as quickly as possible.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前几个episode的平均episode长度：我们希望出租车尽快到达目的地。
- en: 'Average reward for each time step over the first few episodes: We want to get
    the maximum reward as quickly as possible.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个时间步的前几个episode的平均奖励：我们希望尽快获得最大的奖励。
- en: 'Let''s go ahead and implement it:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续实施它：
- en: 'We herein use three alpha candidates [0.4, 0.5, and 0.6] and three epsilon
    candidates [0.1, 0.03, and 0.01], and only consider the first 500 episodes:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里使用了三个alpha候选值[0.4, 0.5和0.6]和三个epsilon候选值[0.1, 0.03和0.01]，并且仅考虑了前500个episode：
- en: '[PRE58]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We perform a grid search by training the SARSA model with each set of hyperparameters
    and evaluating the corresponding performance:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过训练每组超参数的SARSA模型并评估相应的性能来进行网格搜索：
- en: '[PRE59]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Running the preceding code generates the following results:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码会生成以下结果：
- en: '[PRE60]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We can see that the best hyperparameter set in this case is alpha: 0.6, epsilon:
    0.01, which achieves the largest reward per step and a large average reward and
    a short average episode length.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到，在这种情况下，最佳的超参数集合是alpha: 0.6，epsilon: 0.01，它实现了每步最大的奖励和较大的平均奖励以及较短的平均episode长度。'
- en: Developing the Double Q-learning algorithm
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发双Q-learning算法
- en: In this is a bonus recipe, in this chapter where we will develop the double
    Q-learning algorithm.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这是一个额外的步骤，在本章中我们将开发双Q-learning算法。
- en: 'Q-learning is a powerful and popular TD control reinforcement learning algorithm.
    However, it may perform poorly in some cases, mainly because of the greedy component,
    *maxa''Q(s'', a'')*. It can overestimate action values and result in poor performance.
    Double Q-learning was invented to overcome this by utilizing two Q functions.
    We denote two Q functions as *Q1* and *Q2*. In each step, one Q function is randomly
    selected to be updated. If *Q1* is selected, *Q1* is updated as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning是一种强大且流行的TD控制强化学习算法。然而，在某些情况下可能表现不佳，主要是因为贪婪组件*maxa'Q(s', a')*。它可能会高估动作值并导致性能不佳。双Q-learning通过利用两个Q函数来克服这一问题。我们将两个Q函数表示为*Q1*和*Q2*。在每一步中，随机选择一个Q函数进行更新。如果选择*Q1*，则更新如下：
- en: '![](img/f50e9925-36dc-4705-aef2-714aa1c8141b.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f50e9925-36dc-4705-aef2-714aa1c8141b.png)'
- en: 'If Q2 is selected, it is updated as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果选择Q2，则更新如下：
- en: '![](img/0256798e-e381-4404-bf79-1849916cd0ec.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0256798e-e381-4404-bf79-1849916cd0ec.png)'
- en: This means that each Q function is updated from another one following the greedy
    search, which reduces the overestimation of action values using a single Q function.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每个Q函数都从另一个Q函数更新，遵循贪婪搜索，这通过使用单个Q函数减少了动作值的高估。
- en: How to do it...
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We now develop double Q-learning to solve the Taxi environment as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开发双Q-learning来解决出租车环境，如下所示：
- en: 'Import the required libraries and create an instance of the Taxi environment:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库并创建Taxi环境的实例：
- en: '[PRE61]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Then, start defining the epsilon-greedy policy. We will reuse the `gen_epsilon_greedy_policy`
    function defined in the *Developing Q-Learning algorithm* recipe.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，开始定义epsilon-greedy策略。我们将重用在*开发Q-learning算法*步骤中定义的`gen_epsilon_greedy_policy`函数。
- en: 'We then specify the number of episodes and initialize two variables used to
    track the length and total reward for each episode:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定了episode的数量，并初始化了两个变量来跟踪每个episode的长度和总奖励：
- en: '[PRE62]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here, we simulate 3,000 episodes as double Q-learning takes more episodes to
    converge.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们模拟了3,000个episode，因为双Q-learning需要更多的episode才能收敛。
- en: 'Next, we define the function that performs double Q-learning:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义执行双Q-learning的函数：
- en: '[PRE63]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We then specify the rest of the parameters, including the discount rate, learning
    rate, and epsilon, and create an instance of the epsilon-greedy-policy:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们指定了剩余的参数，包括折扣率、学习率和epsilon，并创建了epsilon-greedy-policy的实例：
- en: '[PRE64]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we perform double Q-learning to obtain the optimal policy for the
    Taxi problem:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行双Q-learning以获得出租车问题的最优策略：
- en: '[PRE65]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: How it works...
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: We have solved the Taxi problem using the double Q-learning algorithm in this
    recipe.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本示例中使用双Q学习算法解决了出租车问题。
- en: 'In *Step 4*, we conduct double Q-learning with the following tasks:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4步*中，我们执行双Q学习，完成以下任务：
- en: Initialize the two Q-tables with all zeros.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将两个Q表初始化为全零。
- en: In each step of an episode, we randomly choose one Q function to update. Let
    the agent follow the epsilon-greedy policy to choose what action to take and update
    the selected Q function using another Q function.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个周期的每个步骤中，我们随机选择一个Q函数来更新。让代理根据epsilon-greedy策略选择动作并使用另一个Q函数更新所选的Q函数。
- en: Run `n_episode` episodes.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`n_episode`个周期。
- en: Obtain the optimal policy based on the optimal Q function by summing up (or
    averaging) two Q functions.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于最优Q函数获得最优策略，通过求和（或平均）两个Q函数来实现。
- en: 'After *Step 6*, you can plot the length and total reward for each episode over
    time to verify whether the model converges. The plot of episode length over time
    is displayed as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6步*之后，您可以绘制每个周期的长度和总奖励，以验证模型是否收敛。周期长度随时间的变化图如下所示：
- en: '![](img/3cf31fe2-49e0-4362-bdab-73859546d72b.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cf31fe2-49e0-4362-bdab-73859546d72b.png)'
- en: 'The plot of episode rewards over time is as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励随时间变化的图表如下所示：
- en: '![](img/b3e60071-6b77-4f61-833f-992801b9e010.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3e60071-6b77-4f61-833f-992801b9e010.png)'
- en: Double Q-learning overcomes the potential drawback of single Q-learning in complicated
    environments. It randomly rotates over two Q functions and updates them, which
    prevents action values from one Q function from being overestimated. At the same
    time, it might underestimate the Q function, since it doesn't update the same
    Q function over time steps. Hence, we can see that optimal action values take
    more episodes to converge.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 双Q学习克服了单Q学习在复杂环境中的潜在缺点。它随机地在两个Q函数之间切换并更新它们，这可以防止一个Q函数的动作值被高估。同时，它可能会低估Q函数，因为它不会在时间步长内更新相同的Q函数。因此，我们可以看到最优动作值需要更多的周期来收敛。
- en: See also
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For the theory behind double Q-learning, check out the original paper, [https://papers.nips.cc/paper/3964-double-q-learning](https://papers.nips.cc/paper/3964-double-q-learning),
    by Hado van Hasselt, published in *Advances in Neural Information Processing Systems
    23* (NIPS 2010), 2613-2621, 2010.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 了解双Q学习背后的理论，请参阅Hado van Hasselt的原始论文，[https://papers.nips.cc/paper/3964-double-q-learning](https://papers.nips.cc/paper/3964-double-q-learning)，发表于*神经信息处理系统进展
    23*（NIPS 2010），2613-2621，2010年。
