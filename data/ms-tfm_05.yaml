- en: '*Chapter 3*: Autoencoding Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：自动编码语言模型'
- en: In the previous chapter, we looked at and studied how a typical Transformer
    model can be used by HuggingFace's Transformers. So far, all the topics have included
    how to use pre-defined or pre-built models and less information has been given
    about specific models and their training.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们查看并研究了如何使用HuggingFace的Transformers的典型Transformer模型。到目前为止，所有主题都包括如何使用预定义或预构建模型，而对于特定模型及其训练的信息较少。
- en: In this chapter, we will gain knowledge of how we can train autoencoding language
    models on any given language from scratch. This training will include pre-training
    and task-specific training of the models. First, we will start with basic knowledge
    about the BERT model and how it works. Then we will train the language model using
    a simple and small corpus. Afterward, we will look at how the model can be used
    inside any Keras model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解如何从头开始在任何给定语言上训练自动编码语言模型。这种训练将包括模型的预训练和任务特定训练。首先，我们将从BERT模型的基本知识和其工作原理开始。然后，我们将使用一个简单且小型的语料库来训练语言模型。之后，我们将看看如何将该模型用于任何Keras模型内。
- en: 'For an overview of what will be learned in this chapter, we will discuss the
    following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解本章将学到的内容，我们将讨论以下主题：
- en: BERT – one of the autoencoding language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT——其中之一自动编码语言模型
- en: Autoencoding language model training for any language
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何语言的自动编码语言模型训练
- en: Sharing models with the community
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与社区共享模型
- en: Understanding other autoencoding models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解其他自动编码模型
- en: Working with tokenization algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记化算法工作
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The technical requirements for this chapter are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术要求如下：
- en: Anaconda
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Transformers >= 4.0.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers >= 4.0.0
- en: PyTorch >= 1.0.2
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch >= 1.0.2
- en: TensorFlow >= 2.4.0
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow >= 2.4.0
- en: Datasets >= 1.4.1
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集 >= 1.4.1
- en: Tokenizers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记器
- en: 'Please also check the corresponding GitHub code of `chapter 03`:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请还要检查`第03章`对应的GitHub代码：
- en: '[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH03).'
- en: 'Check out the following link to see Code in Action Video: [https://bit.ly/3i1ycdY](https://bit.ly/3i1ycdY)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看代码实战视频：[https://bit.ly/3i1ycdY](https://bit.ly/3i1ycdY)
- en: BERT – one of the autoencoding language models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT——其中之一自动编码语言模型
- en: '**Bidirectional Encoder Representations from Transformers**, also known as
    **BERT**, was one of the first autoencoding language models to utilize the encoder
    Transformer stack with slight modifications for language modeling.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自变换器的双向编码器表示**，也被称为**BERT**，是最早使用编码器Transformer堆栈的自动编码语言模型之一，稍作修改用于语言建模。'
- en: The BERT architecture is a multilayer Transformer encoder based on the Transformer
    original implementation. The Transformer model itself was originally for machine
    translation tasks, but the main improvement made by BERT is the utilization of
    this part of the architecture to provide better language modeling. This language
    model, after pretraining, is able to provide a global understanding of the language
    it is trained on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BERT架构是基于Transformer原始实现的多层Transformer编码器。Transformer模型本身最初用于机器翻译任务，但BERT所做的主要改进是利用该体系结构的这一部分来提供更好的语言建模。这种语言模型在预训练之后，能够提供对其训练语言的全局理解。
- en: BERT language model pretraining tasks
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT语言模型预训练任务
- en: 'To have a clear understanding of the masked language modeling used by BERT,
    let''s define it with more details. **Masked language modeling** is the task of
    training a model on input (a sentence with some masked tokens) and obtaining the
    output as the whole sentence with the masked tokens filled. But how and why does
    it help a model to obtain better results on downstream tasks such as classification?
    The answer is simple: if the model can do a cloze test (a linguistic test for
    evaluating language understanding by filling in blanks), then it has a general
    understanding of the language itself. For other tasks, it has been pretrained
    (by language modeling) and will perform better.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要清楚了解 BERT 所使用的遮罩语言建模，让我们更详细地定义它。**遮罩语言建模**是训练模型的任务，输入是一句话，其中有一些遮罩标记，输出是填满遮罩标记的完整句子。但是这样做为什么能帮助模型在分类等下游任务中获得更好的结果呢？答案很简单：如果模型能够完成完形填空测试（一种通过填写空白来评估语言理解能力的语言测试），那么它就对语言本身有了一般的理解。对于其他任务，它已经进行了预训练（通过语言建模），并且将表现更好。
- en: 'Here''s an example of a cloze test:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一道完形填空的例子：
- en: George Washington was the first President of the ___ States.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治·华盛顿是___州的第一任总统。
- en: It is expected that *United* should fill in the blank. For a masked language
    model, the same task is applied, and it is required to fill in the masked tokens.
    However, masked tokens are selected randomly from a sentence.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 预期 *United* 应该填入空白处。对于遮罩语言模型，应用了同样的任务，需要填补遮罩标记。不过，遮罩标记是从一句话中随机选择的。
- en: Another task that BERT is trained on is **Next Sentence Prediction** (**NSP**).
    This pretraining task ensures that BERT learns not only the relations of all tokens
    to each other in predicting masked ones but also helps it understand the relation
    between two sentences. A pair of sentences is selected and given to BERT with
    a *[SEP]* splitter token in between. It is also known from the dataset whether
    the second sentence comes after the first one or not.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 受训的另一个任务是**下一句预测**（**NSP**）。这个预训练任务确保 BERT 不仅学习了预测遮罩标记中所有令牌之间的关系，还帮助其理解两个句子之间的关系。会选择一对句子，并在它们之间放上一个*[SEP]*
    分隔符令牌。数据集中还知道第二个句子是在第一个句子之后还是之前。
- en: 'The following is an example of NSP:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 NSP 的示例：
- en: '*It is required from reader to fill the blank. Bitcoin price is way over too
    high compared to other altcoins.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*读者需要填写空白。比特币价格相比其他替代币高得太多了。*'
- en: In this example, the model is required to predict it as negative (the sentences
    are not related to each other).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，模型需要预测为否定（这两个句子之间没有关联）。
- en: 'These two pretraining tasks enable BERT to have an understanding of the language
    itself. BERT token embeddings provide a contextual embedding for each token. **Contextual
    embedding** means each token has an embedding that is completely related to the
    surrounding tokens. Unlike Word2Vec and such models, BERT provides better information
    for each token embedding. NSP tasks, on the other hand, enable BERT to have better
    embeddings for *[CLS]* tokens. This token, as was discussed in the first chapter,
    provides information about the whole input. *[CLS]* is used for classification
    tasks and in the pretraining part learns the overall embedding of the whole input.
    The following figure shows an overall look at the BERT model. *Figure 3.1* shows
    the respective input and output of the BERT model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种预训练任务使 BERT 能够对语言本身有所了解。BERT 令牌嵌入为每个令牌提供上下文嵌入。**上下文嵌入**意味着每个令牌的嵌入与周围令牌完全相关。与
    Word2Vec 和其他模型不同，BERT 为每个令牌嵌入提供更好的信息。另一方面，NSP 任务使 BERT 能够为*[CLS]* 令牌提供更好的嵌入。正如在第一章中讨论的那样，此令牌提供关于整个输入的信息。*[CLS]*
    用于分类任务，并且在预训练部分学习整个输入的总体嵌入。下图显示了 BERT 模型的整体外观。*图3.1* 显示了 BERT 模型的相应输入和输出：
- en: '![Figure 3.1 – The BERT model ](img/B17123_03_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – BERT 模型](img/B17123_03_01.jpg)'
- en: Figure 3.1 – The BERT model
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – BERT 模型
- en: Let's move on to the next section!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一部分！
- en: A deeper look into the BERT language model
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解 BERT 语言模型
- en: Tokenizers are one of the most important parts of many NLP applications in their
    respective pipelines. For BERT, WordPiece tokenization is used. Generally, **WordPiece**,
    **SentencePiece**, and **BytePairEncoding** (**BPE**) are the three most widely
    known tokenizers, used by different Transformer-based architectures, which are
    also covered in the next sections. The main reason that BERT or any other Transformer-based
    architecture uses subword tokenization is the ability of such tokenizers to deal
    with unknown tokens.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 标记器是许多NLP应用程序中各自流水线中最重要的部分之一。 对于BERT，使用的是WordPiece标记。 通常，**WordPiece**，**SentencePiece**和**BytePairEncoding**（**BPE**）是最广为人知的三种标记器，由不同的基于Transformer的架构使用，也将在接下来的部分中介绍。
    BERT或任何其他基于Transformer的架构使用子词标记化的主要原因是这些标记器处理未知标记的能力。
- en: BERT also uses positional encoding to ensure the position of the tokens is given
    to the model. If you recall from [*Chapter 1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016),
    *From Bag-of-Words to the Transformers*, BERT and similar models use non-sequential
    operations such as dense neural layers. Conventional models such as LSTM- and
    RNN-based models get the position by the order of the tokens in the sequence.
    In order to provide this extra information to BERT, positional encoding comes
    in handy.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: BERT还使用位置编码来确保将标记的位置提供给模型。如果您还记得[*章节1*](B17123_01_Epub_AM.xhtml#_idTextAnchor016)，*从词袋模型到Transformer*，BERT和类似的模型使用非顺序操作，如密集神经层。
    传统模型，如基于LSTM和RNN的模型，通过序列中标记的顺序获得位置。 为了为BERT提供这些额外信息，位置编码非常有用。
- en: Pretraining of BERT such as autoencoding models provides language-wise information
    for the model, but in practice, when dealing with different problems such as sequence
    classification, token classification, or question answering, different parts of
    the model output are used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的预训练（如自动编码模型）为模型提供了语言信息，但在实践中，当处理不同的问题，如序列分类，标记分类或问题回答时，会使用模型输出的不同部分。
- en: For example, in the case of a sequence classification task, such as sentiment
    analysis or sentence classification, it is proposed by the original BERT article
    that *[CLS]* embedding from the last layer must be used. However, there is other
    research that performs classification using different techniques using BERT (using
    average token embedding from all tokens, deploying an LSTM over the last layer,
    or even using a CNN on top of the last layer). The last *[CLS]* embedding for
    sequence classification can be used by any classifier, but the proposed, and the
    most common one, is a dense layer with an input size equal to the final token
    embedding size and an output size equal to the number of classes with a softmax
    activation function. Using sigmoid is also another alternative when the output
    could be multilabel and the problem itself is a multilabel classification problem.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在序列分类任务（如情感分析或句子分类）的情况下，原始BERT文章提出了必须使用最后一层的*[CLS]*嵌入。然而，还有其他研究使用BERT进行分类，使用不同的技术（使用所有标记的平均标记嵌入，在最后一层部署LSTM，甚至在最后一层之上使用CNN）。
    序列分类的最后一个*[CLS]*嵌入可以被任何分类器使用，但提出的，也是最常见的方法是使用具有输入大小等于最终标记嵌入大小和输出大小等于类数量的softmax激活函数的密集层。
    当输出可能是多标签并且问题本身是多标签分类问题时，使用sigmoid也是另一种选择。
- en: 'To give you more detailed information about how BERT actually works, the following
    illustration shows an example of an NSP task. Note that the tokenization is simplified
    here for better understanding:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给您更详细的关于BERT如何实际工作的信息，以下说明显示了一个NSP任务的示例。请注意，这里对标记化进行了简化，以便更好地理解：
- en: '![Figure 3.2 – BERT example for an NSP task ](img/B17123_03_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 - 用于NSP任务的BERT示例](img/B17123_03_02.jpg)'
- en: Figure 3.2 – BERT example for an NSP task
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 - 用于NSP任务的BERT示例
- en: The BERT model has different variations, with different settings. For example,
    the size of the input is variable. In the preceding example, it is set to *512*
    and the maximum sequence size that model can get as input is *512*. However, this
    size includes special tokens, *[CLS]* and *[SEP]*, so it will be reduced to *510*.
    On the other hand, using WordPiece as a tokenizer yields subword tokens, and the
    sequence size before we can have fewer words, and after tokenization, the size
    will increase because the tokenizer breaks words into subwords if they are not
    commonly seen in the pretrained corpus.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型有不同的变体，具有不同的设置。例如，输入大小是可变的。在前面的示例中，它被设置为*512*，而模型可以接受的最大序列大小是*512*。但是，这个大小包括特殊标记*[CLS]*和*[SEP]*，因此它会被缩减为*510*。另一方面，使用WordPiece作为标记器会产生子词标记，作为序列输入之前可以有较少的词，标记化之后，大小会增加，因为标记器会将词分解为子词，如果在预训练语料库中没有看到它们常见。
- en: 'The following figure shows an illustration of BERT for different tasks. For
    an NER task, the output of each token is used instead of *[CLS]*. In the case
    of question answering, the question and the answer are concatenated using the
    *[SEP]* delimiter token and the answer is annotated using *Start/End* and the
    *Span* output from the last layer. In this case, the *Paragraph* is the context
    that the *Question* is asked about it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了BERT用于不同任务的示例。对于NER任务，使用每个令牌的输出，而不是*[CLS]*。在问答情景中，使用*[SEP]*分隔符令牌将问题和答案连接起来，然后使用最后一层的*Start/End*和*Span*输出标记答案。在这种情况下，*Paragraph*是*Question*所询问的*Context*：
- en: '![Figure 3.3 – BERT model for various NLP tasks ](img/B17123_03_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 用于各种NLP任务的BERT模型](img/B17123_03_03.jpg)'
- en: Figure 3.3 – BERT model for various NLP tasks
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 用于各种NLP任务的BERT模型
- en: Regardless of all of these tasks, the most important ability of BERT is its
    contextual representation of text. The reason it is successful in various tasks
    is because of the Transformer encoder architecture that represents input in the
    form of dense vectors. These vectors can be easily transformed into output by
    very simple classifiers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不管这些任务如何，BERT最重要的能力是对文本的上下文表示。它成功的原因在于Transformer编码器架构，它以密集向量的形式表示输入。这些向量可以通过非常简单的分类器轻松转换为输出。
- en: Up to this point, you have learned about BERT and how it works. You have learned
    detailed information on various tasks that BERT can be used for and the important
    points of this architecture.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经了解了BERT以及它的工作原理。您已经详细了解了BERT可以用于的各种任务的重要信息以及这种架构的重要要点。
- en: In the next section, you will learn how you can pre-train BERT and use it after
    training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何预先训练BERT，并在训练后使用它。
- en: Autoencoding language model training for any language
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任何语言的自编码语言模型训练
- en: We have discussed how BERT works and that it is possible to use the pretrained
    version of it provided by the HuggingFace repository. In this section, you will
    learn how to use the HuggingFace library to train your own BERT.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了BERT的工作原理以及可以使用HuggingFace库提供的预训练版本。在本节中，您将学习如何使用HuggingFace库来训练您自己的BERT。
- en: Before we start, it is essential to have good training data, which will be used
    for the language modeling. This data is called the **corpus**, which is normally
    a huge pile of data (sometimes it is preprocessed and cleaned). This unlabeled
    corpus must be appropriate for the use case you wish to have your language model
    trained on; for example, if you are trying to have a special BERT for, let's say,
    the English language. Although there are tons of huge, good datasets, such as
    Common Crawl ([https://commoncrawl.org/](https://commoncrawl.org/)), we would
    prefer a small one for faster training.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，有一个很重要的问题，那就是需要有良好的训练数据，这将用于语言建模。这些数据称为**语料库**，通常是一大堆数据（有时经过预处理和清理）。这些无标签的语料库必须适合您希望训练语言模型的用例；例如，如果您尝试为英语单独创建一个特殊的BERT。尽管有成千上万的巨大优秀数据集，比如Common
    Crawl（[https://commoncrawl.org/](https://commoncrawl.org/)），我们更倾向于一个小一点的数据集，以便更快地训练。
- en: 'The IMDB dataset of 50K movie reviews (available at [https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews))
    is a large dataset for sentiment analysis, but small if you use it as a corpus
    for training your language model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 50K电影评论的IMDB数据集（可在[https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)找到）是一个用于情感分析的大型数据集，但如果您将其用作语料库来训练语言模型，则算是小型的：
- en: 'You can easily download and save it in `.txt` format for language model and
    tokenizer training by using the following code:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用以下代码轻松下载并保存为`.txt`格式，用于语言模型和分词器训练：
- en: '[PRE0]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After preparing the corpus, the tokenizer must be trained. The `tokenizers`
    library provides fast and easy training for the WordPiece tokenizer. In order
    to train it on your corpus, it is required to run the following code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在准备语料库之后，必须训练分词器。`tokenizers`库提供了快速简单的WordPiece分词器训练。为了在你的语料库上训练它，需要运行以下代码：
- en: '[PRE1]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will train the tokenizer. You can access the trained vocabulary by using
    the `get_vocab()` function of the trained `tokenizer` object. You can get the
    vocabulary by using the following code:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将训练分词器。你可以通过使用训练好的`tokenizer`对象的`get_vocab()`函数来访问训练好的词汇表。你可以通过以下代码获取词汇表：
- en: '[PRE2]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It is essential to save the tokenizer to be used afterwards. Using the `save_model()`
    function of the object and providing the directory will save the tokenizer vocabulary
    for further usage:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存分词器以供以后使用是必不可少的。使用对象的`save_model()`函数并提供目录将保存分词器词汇表供以后使用：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And you can reload it by using the `from_file()` function:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`from_file()`函数重新加载它：
- en: '[PRE5]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can use the tokenizer by following this example:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以按照以下示例使用分词器：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The special tokens `[CLS]` and `[SEP]` will be automatically added to the list
    of tokens because BERT needs them for processing input.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特殊的标记`[CLS]`和`[SEP]`将自动添加到标记列表中，因为BERT需要它们来处理输入。
- en: 'Let''s try another sentence using our tokenizer:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试使用我们的分词器来另一个句子：
- en: '[PRE7]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Seems like a good tokenizer for noisy and misspelled text. Now that you have
    your tokenizer ready and saved, you can train your own BERT. The first step is
    to use `BertTokenizerFast` from the `Transformers` library. You are required to
    load the trained tokenizer from the previous step by using the following command:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于嘈杂和拼写错误的文本，似乎是一个很好的分词器。现在你已经准备好并保存了你的分词器，你可以训练你自己的BERT。第一步是使用`Transformers`库中的`BertTokenizerFast`。你需要使用以下命令加载上一步训练好的分词器：
- en: '[PRE8]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have used `BertTokenizerFast` because it is suggested by the HuggingFace
    documentation. There is also `BertTokenizer`, which, according to the definition
    from the library documentation, is not implemented as fast as the fast version.
    In most of the pretrained models' documentations and cards, it is highly recommended
    to use the `BertTokenizerFast` version.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用了`BertTokenizerFast`，因为它是由HuggingFace文档建议使用的。还有`BertTokenizer`，根据库文档中的定义，它没有实现快速版本那么快。在大多数预训练模型的文档和卡片中，强烈建议使用`BertTokenizerFast`版本。
- en: 'The next step is preparing the corpus for faster training by using the following
    command:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是通过以下命令准备语料库以加快训练速度：
- en: '[PRE9]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And it is required to provide a data collator for masked language modeling:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且需要为掩码语言建模提供数据收集器：
- en: '[PRE10]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The data collator gets the data and prepares it for the training. For example,
    the data collator above takes data and prepares it for masked language modeling
    with a probability of `0.15`. The purpose of using such a mechanism is to do the
    preprocessing on the fly, which makes it possible to use fewer resources. On the
    other hand, it slows down the training process because each sample has to be preprocessed
    on the fly at training time.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据收集器获取数据并为训练准备好。例如，上面的数据收集器获取数据并准备好使用概率为`0.15`的掩码语言建模。使用这种机制的目的是在运行时进行预处理，这样可以使用更少的资源。另一方面，它会减慢训练过程，因为每个样本都必须在训练时动态进行预处理。
- en: 'The training arguments also provide information for the trainer in the training
    phase, which can be set by using the following command:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练参数还为训练器在训练阶段提供信息，可以使用以下命令设置：
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''ll now make the BERT model itself, which we are going to use with the default
    configuration (the number of attention heads, Transformer encoder layers, and
    so on):'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建BERT模型本身，我们将使用默认配置（注意力头数、Transformer编码器层数等）：
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And the final step is to make a trainer object:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是创建一个训练器对象：
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, you can train your language model using the following command:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以使用以下命令训练你的语言模型：
- en: '[PRE14]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It will show you a progress bar indicating the progress made in training:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它会显示一个进度条，指示训练的进度：
- en: '![Figure 3.4 – BERT model training progress ](img/B17123_03_04.jpg)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.4 - BERT模型训练进度](img/B17123_03_04.jpg)'
- en: Figure 3.4 – BERT model training progress
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.4 - BERT模型训练进度
- en: 'During the model training, a log directory called `runs` will be used to store
    the checkpoint in steps:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在模型训练过程中，将使用名为`runs`的日志目录存储步骤检查点：
- en: '![Figure 3.5 – BERT model checkpoints ](img/B17123_03_05.jpg)'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.5 – BERT 模型检查点](img/B17123_03_05.jpg)'
- en: Figure 3.5 – BERT model checkpoints
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.5 – BERT 模型检查点
- en: 'After the training is finished, you can easily save the model using the following
    command:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练结束后，您可以使用以下命令轻松保存模型：
- en: '[PRE15]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Up to this point, you have learned how you can train BERT from scratch for any
    specific language that you desire. You've learned how to train the tokenizer and
    BERT model using the corpus you have prepared.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直到目前为止，您已经学会了如何训练您希望的任何特定语言的 BERT。您已经学会了如何训练标记器和 BERT 模型，使用您准备的语料库。
- en: 'The default configuration that you provided for BERT is the most essential
    part of this training process, which defines the architecture of BERT and its
    hyperparameters. You can take a peek at these parameters by using the following
    code:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您提供的 BERT 默认配置是此训练过程中最关键的部分，它定义了 BERT 的架构和超参数。您可以使用以下代码查看这些参数：
- en: '[PRE16]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.6 – BERT model configuration ](img/B17123_03_06.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.6 – BERT 模型配置](img/B17123_03_06.jpg)'
- en: Figure 3.6 – BERT model configuration
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.6 – BERT 模型配置
- en: If you wish to replicate `max_position_embedding`, `num_attention_heads`, `num_hidden_layers`,
    `intermediate_size`, and `hidden_size`, directly affects the training time. Increasing
    them dramatically increases the training time for a large corpus.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您希望复制`max_position_embedding`、`num_attention_heads`、`num_hidden_layers`、`intermediate_size`和`hidden_size`，直接影响训练时间。将它们增加会显著增加大型语料库的训练时间。
- en: 'For example, you can easily make a new configuration for a tiny version of
    BERT for faster training using the following code:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，您可以使用以下代码轻松为小型 BERT 制作新配置以加快训练速度：
- en: '[PRE17]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is the result of the code:'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的结果：
- en: '![Figure 3.8 – Tiny BERT model configuration ](img/B17123_03_08.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.8 – 小型 BERT 模型配置](img/B17123_03_08.jpg)'
- en: Figure 3.8 – Tiny BERT model configuration
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.8 – 小型 BERT 模型配置
- en: 'By using the same method, we can make a tiny BERT model using this configuration:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的方法，我们可以使用这个配置制作一个微小的 BERT 模型：
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And using the same parameters for training, you can train this tiny new BERT:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且使用相同的参数进行训练，您可以训练这个微小的新 BERT：
- en: '[PRE19]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following is the output:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.9 – Tiny BERT model configuration ](img/B17123_03_09.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.9 – 小型 BERT 模型配置](img/B17123_03_09.jpg)'
- en: Figure 3.9 – Tiny BERT model configuration
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.9 – 小型 BERT 模型配置
- en: It is clear that the training time is dramatically decreased, but you should
    be aware that this is a tiny version of BERT with fewer layers and parameters,
    which is not as good as BERT Base.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 显然，训练时间显著减少，但您应该意识到这是一个具有更少层和参数的微小版 BERT，不如 BERT Base 好。
- en: Up to this point, you have learned how to train your own model from scratch,
    but it is essential to note that using the `datasets` library is a better choice
    when dealing with datasets for training language models or leveraging it to perform
    task-specific training.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何从头开始训练自己的模型，但需要注意的是在处理用于训练语言模型的数据集或利用它执行特定任务的数据集时，使用`datasets`库是更好的选择。
- en: 'The BERT language model can also be used as an embedding layer combined with
    any deep learning model. For example, you can load any pretrained BERT model or
    your own version that has been trained in the previous step. The following code
    shows how you must load it to be used in a Keras model:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 语言模型也可以作为嵌入层与任何深度学习模型结合使用。例如，您可以加载任何预训练的 BERT 模型或您在上一步中训练过的自己的版本。以下代码显示了如何加载它以在
    Keras 模型中使用：
- en: '[PRE20]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'But you do not need the whole model; instead, you can access the layers by
    using the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但您不需要整个模型；相反，您可以使用以下代码访问层：
- en: '[PRE21]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see, there is just a single layer from `TFBertMainLayer`, which
    you can access within your Keras model. But before using it, it is nice to test
    it and see what kind of output it provides:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，只有一个来自`TFBertMainLayer`的单层，您可以在 Keras 模型中访问它。但在使用之前，最好先测试它，看看它提供了什么样的输出：
- en: '[PRE22]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is as follows:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.10 – BERT model output ](img/B17123_03_10.jpg)'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.10 – BERT 模型输出](img/B17123_03_10.jpg)'
- en: Figure 3.10 – BERT model output
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.10 – BERT 模型输出
- en: 'As can be seen from the result, there are two outputs: one for the last hidden
    state and one for the pooler output. The last hidden state provides all token
    embeddings from BERT with additional *[CLS]* and *[SEP]* tokens at the start and
    end, respectively.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如结果所示，有两个输出：一个是最后的隐藏状态，一个是pooler输出。最后的隐藏状态提供了来自BERT的所有标记嵌入，同时在开头和结尾分别加上了*[CLS]*和*[SEP]*标记。
- en: 'Now that you have learned more about the TensorFlow version of BERT, you can
    make a Keras model using this new embedding:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经了解了TensorFlow版本的BERT更多信息，你可以使用这个新的嵌入创建一个keras模型：
- en: '[PRE23]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model object, which is a Keras model, has two inputs: one for tokens and
    one for masks. Tokens has `token_ids` from the tokenizer output and the masks
    will have `attention_mask`. Let''s try it and see what happens:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型对象，即一个Keras模型，有两个输入：一个用于标记，一个用于掩码。标记具有来自分词器输出的`token_ids`，掩码将具有`attention_mask`。让我们试一试，看会发生什么：
- en: '[PRE24]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is important to use `max_length`, `truncation`, and `pad_to_max_length`
    when using `tokenizer`. These parameters make sure you have the output in a usable
    shape by padding it to the maximum length of 256 that was defined before. Now
    you can run the model using this sample:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tokenizer`时重要的是使用`max_length`、`truncation`和`pad_to_max_length`这些参数。这些参数确保你通过将其填充到之前定义的256的最大长度来获得可用形状的输出。现在你可以使用这个样本运行模型了：
- en: '[PRE25]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following is the output:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![Figure 3.11 – BERT model classification output ](img/B17123_03_11.jpg)'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.11 – BERT模型分类输出](img/B17123_03_11.jpg)'
- en: Figure 3.11 – BERT model classification output
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.11 – BERT模型分类输出
- en: 'When training the model, you need to compile it using the `compile` function:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练模型时，你需要使用`compile`函数进行编译：
- en: '[PRE26]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 3.12 – BERT model summary ](img/B17123_03_12.jpg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.12 – BERT模型摘要](img/B17123_03_12.jpg)'
- en: Figure 3.12 – BERT model summary
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.12 – BERT模型摘要
- en: 'From the model summary, you can see that the model has 109,483,778 trainable
    parameters including BERT. But if you have your BERT model pretrained and you
    want to freeze it in a task-specific training, you can do so with the following
    command:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型摘要中，你可以看到模型有109,483,778个可训练参数，包括BERT。但如果你有预训练好的BERT模型，并且想要在特定任务的训练中冻结它，你可以使用以下命令：
- en: '[PRE27]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As far as we know, the layer index of the embedding layer is 2, so we can simply
    freeze it. If you rerun the summary function, you will see the trainable parameters
    are reduced to 1,538, which is the number of parameters of the last layer:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，嵌入层的层索引为2，因此我们可以简单地冻结它。如果你重新运行summary函数，你会看到可训练参数减少到了1,538，这是最后一层的参数个数：
- en: '![Figure 3.13 – BERT model summary with fewer trainable parameters ](img/B17123_03_13.jpg)'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图3.13 – 包括较少可训练参数的BERT模型摘要](img/B17123_03_13.jpg)'
- en: Figure 3.13 – BERT model summary with fewer trainable parameters
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.13 – BERT模型摘要，包括较少的可训练参数
- en: 'As you recall, we used the IMDB sentiment analysis dataset for training the
    language model. Now you can use it for training the Keras-based model for sentiment
    analysis. But first, you need to prepare the input and output:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所记得的，我们使用了IMDB情感分析数据集来训练语言模型。现在你可以用它来训练基于Keras的情感分析模型。但首先，你需要准备输入和输出：
- en: '[PRE28]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And finally, your data is ready, and you can fit your model:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你的数据准备好了，你可以拟合你的模型：
- en: '[PRE29]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: And after fitting the model, your model is ready to be used. Up to this point,
    you have learned how to perform model training for a classification task. You
    have learned how to save it, and in the next section, you will learn how it is
    possible to share the trained model with the community.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型之后，你的模型就准备好使用了。到目前为止，你已经学会了如何对分类任务进行模型训练。你已经学会了如何保存它，在下一节中，你将学会如何与社区分享训练好的模型。
- en: Sharing models with the community
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与社区共享模型
- en: 'HuggingFace provides a very easy-to-use model-sharing mechanism:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace提供了一个非常方便的模型共享机制：
- en: 'You can simply use the following `cli` tool to log in:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以简单地使用以下`cli`工具进行登录：
- en: '[PRE30]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After you''ve logged in using your own credentials, you can create a repository:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你自己的凭据登录后，你可以创建一个仓库：
- en: '[PRE31]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can put any model name for the `a-fancy-model-name` parameter and then
    it is essential to make sure you have git-lfs:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以为`a-fancy-model-name`参数随意取任何模型名称，然后确保你安装了git-lfs是非常重要的：
- en: '[PRE32]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Git LFS is a Git extension used for handling large files. HuggingFace pretrained
    models are usually large files that require extra libraries such as LFS to be
    handled by Git.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Git LFS是一个用于处理大文件的Git扩展。HuggingFace预训练模型通常是大文件，需要额外的库（如LFS）来处理Git。
- en: 'Then you can clone the repository you have just created:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以克隆你刚刚创建的仓库：
- en: '[PRE33]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Afterward, you can add and remove from the repository as you like, and then,
    just like Git usage, you have to run the following command:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，您可以随意向仓库中添加和删除，然后，就像使用 Git 一样，您必须运行以下命令：
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Autoencoding models rely on the left encoder side of the original Transformer
    and are highly efficient at solving classification problems. Even though BERT
    is a typical example of autoencoding models, there are many alternatives discussed
    in the literature. Let's take a look at these important alternatives.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码模型依赖于原始 Transformer 左编码器一侧，非常有效地解决分类问题。尽管 BERT 是自编码模型的典型示例，但文献中讨论了许多替代方案。让我们看一下这些重要的替代方案。
- en: Understanding other autoencoding models
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解其他自编码模型
- en: 'In this part, we will review autoencoding model alternatives that slightly
    modify the original BERT. These alternative re-implementations have led to better
    downstream tasks by exploiting many sources: optimizing the pre-training process
    and the number of layers or heads, improving data quality, designing better objective
    functions, and so forth. The source of improvements roughly falls into two parts:
    *better architectural design choice* and *pre-training control*.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分中，我们将回顾略微修改原始 BERT 的自编码模型替代方案。这些替代方案的重新实现通过利用许多来源（优化预训练过程和层或头的数量、改进数据质量、设计更好的目标函数等）导致了更好的下游任务。改进的来源大致分为两部分：*更好的架构设计选择*
    和 *预训练控制*。
- en: Many effective alternatives have been shared lately, so it is impossible to
    understand and explain them all here. We can take a look at some of the most cited
    models in the literature and the most used ones on NLP benchmarks. Let's start
    with **Albert** as a re-implementation of BERT that focuses especially on architectural
    design choice.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最近共享了许多有效的替代方案，因此不可能在这里理解和解释它们全部。我们可以看一些文献中引用最多的模型和 NLP 基准测试中使用最多的模型。让我们从 **Albert**
    开始，作为对架构设计选择特别关注的 BERT 的重新实现。
- en: Introducing ALBERT
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入 ALBERT
- en: The performance of language models is considered to improve as their size gets
    bigger. However, training such models is getting more challenging due to both
    memory limitations and longer training times. To address these issues, the Google
    team proposed the **Albert model** (**A Lite BERT** for Self-Supervised Learning
    of Language Representations), which is indeed a reimplementation of the BERT architecture
    by utilizing several new techniques that reduce memory consumption and increase
    the training speed. The new design led to the language models scaling much better
    than the original BERT. Along with 18 times fewer parameters, Albert trains 1.7
    times faster than the original BERT-large model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的性能被认为随着其规模的增大而提高。然而，由于内存限制和较长的训练时间，训练这些模型变得更加具有挑战性。为了解决这些问题，Google 团队提出了
    **Albert 模型**（**A Lite BERT** 用于语言表示的自监督学习），这实际上是通过利用几种新技术对 BERT 架构进行重新实现，从而减少了内存消耗并增加了训练速度。新设计导致语言模型比原始
    BERT 更好地扩展。与原始 BERT-large 模型相比，Albert 参数减少了 18 倍，训练速度提高了 1.7 倍。
- en: 'The Albert model mainly consists of three modifications of the original BERT:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Albert 模型主要由对原始 BERT 的三种修改组成：
- en: Factorized embedding parameterization
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子化嵌入参数化
- en: Cross-layer parameter sharing
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨层参数共享
- en: Inter-sentence coherence loss
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句间连贯性损失
- en: 'The first two modifications are parameter-reduction methods that are related
    to the issue of model size and memory consumption in the original BERT. The third
    corresponds to a new objective function: **Sentence-Order Prediction** (**SOP**),
    replacing the **Next Sentence Prediction** (**NSP**) task of the original BERT,
    which led to a much thinner model and improved performance.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种修改是与原始 BERT 中模型大小和内存消耗问题相关的参数减少方法。第三种对应于一个新的目标函数：**句子顺序预测**（**SOP**），取代了原始
    BERT 的 **下一句预测**（**NSP**）任务，从而导致了一个更薄的模型和更好的性能。
- en: Factorized embedding parameterization is used to decompose the large vocabulary-embedding
    matrix into two small matrices, which separate the size of the hidden layers from
    the size of the vocabulary. This decomposition reduces the embedding parameters
    from *O(V × H) to O(V × E + E × H)* where *V* is *Vocabulary*, *H* is *Hidden
    Layer Size*, *E* is *Embedings*, which leads to more efficient usage of the total
    model parameters *if H >> E* is satisfied.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分解嵌入参数化将大词汇嵌入矩阵分解为两个小矩阵，这两个矩阵将隐藏层的大小与词汇表的大小分开。这种分解将嵌入参数从 *O(V × H)* 减少到 *O(V
    × E + E × H)*，其中 *V* 是 *词汇表*，*H* 是 *隐藏层大小*，*E* 是 *嵌入*，如果满足 *H >> E*，则可以更有效地利用总模型参数。
- en: Cross-layer parameter sharing prevents the total number of parameters from increasing
    as the network gets deeper. The technique is considered another way to improve
    parameter efficiency since we can keep the parameter size smaller by sharing or
    copying. In the original paper, they experimented with many ways to share parameters,
    such as either sharing FF-only parameters across layers or sharing attention-only
    parameters or entire parameters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 跨层参数共享可以防止网络加深时总参数数量的增加。这一技术被认为是提高参数效率的另一种方式，因为我们可以通过共享或复制来保持参数大小较小。在原始论文中，他们尝试了许多共享参数的方法，例如跨层仅共享
    FF 参数、仅共享注意力参数或整个参数。
- en: The other modification of Albert is inter-sentence coherence loss. As we already
    discussed, the BERT architecture takes advantage of two loss calculations, the
    **Masked Language Modeling** (**MLM**) loss and NSP. NSP comes with binary cross-entropy
    loss for predicting whether or not two segments appear in a row in the original
    text. The negative examples are obtained by selecting two segments from different
    documents. However, the Albert team criticized NSP for being a topic detection
    problem, which is considered a relatively easy problem. Therefore, the team proposed
    a loss based primarily on coherence rather than topic prediction. They utilized
    SOP loss, which focuses on modeling inter-sentence coherence instead of topic
    prediction. SOP loss uses the same positive examples technique as BERT, (which
    is two consecutive segments from the same document), and as negative examples,
    the same two consecutive segments but with their order swapped. The model is then
    forced to learn finer-grained distinctions between coherence properties at the
    discourse level.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Albert 的另一个修改是跨句连贯性损失。正如我们已经讨论过的，BERT 架构利用了两种损失计算，即 **Masked Language Modeling**
    (**MLM**) 损失和 NSP。NSP 使用二进制交叉熵损失来预测原始文本中是否连续出现两个段落。负例通过从不同文档中选择两个段落获得。然而，Albert
    团队批评 NSP 是一个主题检测问题，被认为是一个相对容易的问题。因此，团队提出了一个基于连贯性而不是主题预测的损失。他们利用了 SOP 损失，它主要关注建模句子间的连贯性而不是主题预测。SOP
    损失使用与 BERT 相同的正例技术（即来自同一文档的两个连续段落），并将相同的两个连续段落作为负例，但交换它们的顺序。然后，模型被迫学习更细粒度的话语层面的连贯性特性之间的区别。
- en: 'Let''s compare the original BERT and Albert configuration with the `Transformers`
    library. The following piece of code shows how to configure a BERT-Base initial
    model. As you see in the output, the number of parameters is around 110 M:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `Transformers` 库来比较原始的 BERT 和 Albert 配置。以下代码片段展示了如何配置一个 BERT-Base 初始模型。如您在输出中所见，参数数量约为
    110 M：
- en: '[PRE35]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And the following piece of code shows how to define the Albert model with two
    classes, `AlbertConfig` and `AlbertModel`, from the `Transformers` library:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码片段显示了如何使用 `Transformers` 库定义具有两个类 `AlbertConfig` 和 `AlbertModel` 的 Albert
    模型：
- en: '[PRE36]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Due to that, the default Albert configuration points to Albert-xxlarge. We
    need to set the hidden size, the number of attention heads, and the intermediate
    size to fit Albert-base. And the code shows the Albert-base mode as 11M, 10 times
    smaller than the BERT-base model. The original paper on ALBERT reported benchmarking
    as in the following table:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由此，**默认 Albert 配置指向 Albert-xxlarge**。我们需要设置隐藏大小、注意力头数和中间大小以适应 Albert-base。代码显示
    Albert-base 模式为 11M，比 BERT-base 模型小 10 倍。ALBERT 的原始论文报告了以下表格中的基准性能：
- en: '![Figure 3.14 – Albert model benchmarking ](img/B17123_03_14.jpg)'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.14 – Albert 模型基准测试](img/B17123_03_14.jpg)'
- en: Figure 3.14 – Albert model benchmarking
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.14 – Albert 模型基准测试
- en: 'From this point on, in order to train an Albert language model from scratch,
    we need to go through similar phases to those we already illustrated in BERT training
    in the previous sections by using the uniform Transformers API. There''s no need
    to explain the same steps here! Instead, let''s load an already trained Albert
    language model as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这一点开始，为了从头开始训练一个 Albert 语言模型，我们需要通过使用统一的 Transformers API 在前面的章节中已经说明过的类似阶段。在这里没有必要解释相同的步骤！相反，让我们加载一个已经训练好的
    Albert 语言模型，如下所示：
- en: '[PRE37]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The preceding pieces of code download the Albert model weights and its configuration
    from the HuggingFace hub or from our local cache directory if already cached,
    which means you''ve already called the `AlbertTokenizer.from_pretrained()` function
    before. Since that the model object is a pre-trained language model, the things
    we can do with this model are limited for now. We need to train it on a downstream
    task to able to use it for inference, which will be the main subject of further
    chapters. Instead, we can take advantage of its masked language model objective
    as follows:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码片段从 HuggingFace hub 或我们的本地缓存目录下载 Albert 模型权重及其配置，如果已经缓存，则表示您已经调用了 `AlbertTokenizer.from_pretrained()`
    函数。由于该模型对象是一个预训练语言模型，目前我们可以对该模型做的事情是有限的。我们需要在下游任务上对其进行训练，以便将其用于推理，这将是后续章节的主要主题。相反，我们可以利用其掩码语言模型目标，如下所示：
- en: '[PRE38]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following is the output:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 3.15 – The fill-mask output results for albert-base-v2 ](img/B17123_03_15.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – albert-base-v2 的填充遮罩输出结果](img/B17123_03_15.jpg)'
- en: Figure 3.15 – The fill-mask output results for albert-base-v2
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – albert-base-v2 的填充遮罩输出结果
- en: The `fill-mask` pipeline computes the scores for each vocabulary token with
    the `SoftMax()` function and sorts the most probable tokens where `cute` is the
    winner with a probability score of 0.281\. You may notice that entries in the
    *token_str* column start with the `_` character, which is due to the metaspace
    component of the tokenizer of Albert.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`fill-mask` 管道使用 `SoftMax()` 函数计算每个词汇标记的分数，并对概率最高的标记进行排序，其中 `cute` 是获胜者，概率得分为
    0.281。您可能注意到 *token_str* 列中的条目以 `_` 字符开头，这是由于 Albert 的分词器的 metaspace 组件造成的。'
- en: Let's take a look at the next alternative, *RoBERTa*, which mostly focuses on
    the pre-training phase.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看下一个备选项，*RoBERTa*，它主要侧重于预训练阶段。
- en: RoBERTa
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RoBERTa
- en: '**Robustly Optimized BERT pre-training Approach** (**RoBERTa**) is another
    popular BERT reimplementation. It has provided many more improvements in training
    strategy than architectural design. It outperformed BERT in almost all individual
    tasks on GLUE. Dynamic masking is one of its original design choices. Although
    static masking is better for some tasks, the RoBERTa team showed that dynamic
    masking can perform well for overall performances. Let''s compare the changes
    from BERT and summarize all the features as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**鲁棒优化的 BERT 预训练方法**（**RoBERTa**）是另一种流行的 BERT 重新实现。它在训练策略上提供了更多的改进，而不是架构设计。它在
    GLUE 上的几乎所有单个任务中都优于 BERT。动态遮罩是其原始设计选择之一。虽然对于某些任务来说，静态遮罩更好，但 RoBERTa 团队表明，动态遮罩可以在整体性能方面表现良好。让我们比较与
    BERT 的变化并总结所有功能如下：'
- en: 'The changes in architecture are as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的变化如下：
- en: Removing the next sentence prediction training objective
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除下一句预测训练目标
- en: Dynamically changing the masking patterns instead of static masking, which is
    done by generating masking patterns whenever they feed a sequence to the model
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态地改变遮罩模式，而不是静态的遮罩，这是通过在将序列馈送到模型时生成遮罩模式来完成的
- en: '**BPE** sub-word tokenizer'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BPE** 子词分词器'
- en: 'The changes in training are as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的变化如下：
- en: 'Controlling the training data: More data is used, such as 160 GB instead of
    the 16 GB originally used in BERT. Not only the size of the data but the quality
    and diversity were taken into consideration in the study.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制训练数据：使用更多的数据，如 160 GB，而不是最初在 BERT 中使用的 16 GB。在研究中考虑到的不仅是数据的大小，还有质量和多样性。
- en: Longer iterations of up to 500K pretraining steps.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较长的迭代次数，最多达到 500K 的预训练步骤。
- en: A longer batch size.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更长的批量大小。
- en: Longer sequences, which leads to less padding.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更长的序列，这导致更少的填充。
- en: A large 50K BPE vocabulary instead of a 30K BPE vocabulary.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大的 50K BPE 词汇表，而不是 30K BPE 词汇表。
- en: 'Thanks to the Transformers uniform API, as in the Albert model pipeline above,
    we initialize the RoBERTa model as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了变压器统一的 API，就像上面的 Albert 模型管道一样，我们将 RoBERTa 模型初始化如下：
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In order to load the pre-trained model, we execute the following pieces of
    code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载预训练模型，我们执行以下代码片段：
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'These lines illustrate how the model processes a given text. The output representation
    at the last layer is not useful at the moment. As we''ve mentioned several times,
    we need to fine-tune the main language models. The following execution applies
    the `fill-mask` function using the `roberta-base` model:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行说明了模型如何处理给定的文本。目前，最后一层的输出表示并不有用。正如我们已经多次提到的那样，我们需要对主要语言模型进行微调。以下执行使用 `roberta-base`
    模型应用 `fill-mask` 函数：
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the output:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![Figure 3.16 – The fill-mask task results for roberta-base ](img/B17123_03_16.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.16 – roberta-base 的填充掩码任务结果](img/B17123_03_16.jpg)'
- en: Figure 3.16 – The fill-mask task results for roberta-base
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16 – roberta-base 的填充掩码任务结果
- en: 'Like the previous ALBERT `fill-mask` model, this pipeline ranks the suitable
    candidate words. Please ignore the prefix `Ġ` in the tokens – that is an encoded
    space character produced by the byte-level BPE tokenizer, which we will discuss
    later. You should have noticed that we used the `[MASK]` and `<mask>` tokens in
    ALBERT and RoBERTa pipeline in order to hold place for masked token. This is because
    of the configuration of `tokenizer`. To learn which token expression will be used,
    you can check `tokenizer.mask_token`. Please see the following execution:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于之前的 ALBERT `fill-mask` 模型，这个流水线对适当的候选词进行排名。请忽略令牌中的前缀 `Ġ` – 这是由字节级 BPE 分词器生成的编码空格字符，我们稍后会讨论。您应该已经注意到我们在
    ALBERT 和 RoBERTa 流水线中使用了 `[MASK]` 和 `<mask>` 令牌，以便为掩码令牌留出位置。这是由于 `tokenizer` 的配置。要了解将使用哪个令牌表达式，您可以检查
    `tokenizer.mask_token`。请参阅以下执行：
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To ensure proper mask token use, we can add the `fillmask.tokenizer.mask_token`
    expression in the pipeline as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保正确使用掩码令牌，我们可以将 `fillmask.tokenizer.mask_token` 表达式添加到管道中，如下所示：
- en: '[PRE43]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ELECTRA
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ELECTRA
- en: The **ELECTRA** model (*proposed by Kevin Clark et al. in 2020*) focuses on
    a new masked language model utilizing the replaced token detection training objective.
    During pre-training, the model is forced to learn to distinguish real input tokens
    from synthetically generated replacements where the synthetic negative example
    is sampled from plausible tokens rather than randomly sampled tokens. The Albert
    model criticized the NSP objective of BERT for being a topic detection problem
    and using low-quality negative examples. ELECTRA trains two neural networks, a
    generator and a discriminator, so that the former produces high-quality negative
    examples, whereas the latter distinguishes the original token from the replaced
    token. We know GAN networks from the field of computer vision, in which the generator
    *G* produces fake images and tries to fool the discriminator *D*, and the discriminator
    network tries to avoid being fooled. The ELECTRA model applies almost the same
    generator-discriminator approach to replace original tokens with high-quality
    negative examples that are plausible replacements but synthetically generated.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**ELECTRA** 模型（*由 Kevin Clark 等人于 2020 年提出*）专注于利用被替换的令牌检测训练目标的新掩码语言模型。在预训练期间，模型被迫学习区分真实输入令牌和合成生成的替换令牌，其中合成的负例是从可信令牌而不是随机抽样的令牌中抽样的。Albert
    模型批评了 BERT 的 NSP 目标，认为它是一个主题检测问题，并且使用了低质量的负例。ELECTRA 训练两个神经网络，一个生成器和一个鉴别器，前者产生高质量的负例，而后者区分原始令牌和替换令牌。我们从计算机视觉领域知道了
    GAN 网络，在这个网络中，生成器*G*产生假图像，并试图欺骗鉴别器*D*，而鉴别器网络试图避免被欺骗。ELECTRA 模型几乎应用了相同的生成器-鉴别器方法来用高质量的合成生成的可信替换替换原始令牌。'
- en: 'In order not to repeat the same code with other examples, we only provide a
    simple `fill-mask` example for the Electra generator as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不重复与其他示例相同的代码，我们只提供了一个简单的 `fill-mask` 示例作为 Electra 生成器，如下所示：
- en: '[PRE44]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You can see the entire list of models at the following link: [https://huggingface.co/Transformers/model_summary.html](https://huggingface.co/transformers/model_summary.html).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接中看到完整的模型列表：[https://huggingface.co/Transformers/model_summary.html](https://huggingface.co/transformers/model_summary.html)。
- en: The model checkpoints can be found at [https://huggingface.co/models](https://huggingface.co/models).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 模型检查点可以在 [https://huggingface.co/models](https://huggingface.co/models) 找到。
- en: Well done! We've finally completed the autoencoding model part. Now we'll move
    on to tokenization algorithms, which have an important effect on the success of
    Transformers.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们终于完成了自动编码模型部分。现在我们将转向标记化算法，这对于 Transformers 的成功具有重要影响。
- en: Working with tokenization algorithms
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标记化算法工作
- en: In the opening part of the chapter, we trained the BERT model using a specific
    tokenizer, namely `BertWordPieceTokenizer`. Now it is worth discussing the tokenization
    process in detail here. Tokenization is a way of splitting textual input into
    tokens and assigning an identifier to each token before feeding the neural network
    architecture. The most intuitive way is to split the sequence into smaller chunks
    in terms of space. However, such approaches do not meet the requirement of some
    languages, such as Japanese, and also may lead to huge vocabulary problems. Almost
    all Transformer models leverage subword tokenization not only for reducing dimensionality
    but also for encoding rare (or unknown) words not seen in training. The tokenization
    relies on the idea that every word, including rare words or unknown words, can
    be decomposed into meaningful smaller chunks that are widely seen symbols in the
    training corpus.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头部分，我们使用了特定的分词器，即`BertWordPieceTokenizer`来训练BERT模型。现在值得在这里详细讨论标记化过程。标记化是将文本输入分割成标记并在将其馈送到神经网络架构之前为每个标记分配一个标识符的一种方式。最直观的方法是根据空格将序列分割成较小的块。然而，这种方法不符合一些语言的要求，例如日语，并且也可能导致巨大的词汇问题。几乎所有的Transformer模型都利用子词标记化来降低维度，不仅编码训练中未见过的罕见（或未知）单词，而且还为每个单词分配一个唯一的标识符。标记化依赖于这样一个思想，即包括罕见单词或未知单词在内的每个单词都可以分解为在训练语料库中广泛出现的有意义的较小块。
- en: Some traditional tokenizers developed within Moses and the `nltk` library apply
    advanced rule-based techniques. But the tokenization algorithms that are used
    with Transformers are based on self-supervised learning and extract the rules
    from the corpus. Simple intuitive solutions for rule-based tokenization are based
    on using characters, punctuation, or whitespace. Character-based tokenization
    causes language models to lose the input meaning. Even though it can reduce the
    vocabulary size, which is good, it makes it hard for the model to capture the
    meaning of `cat` by means of the encodings of the characters `c`, `a`, and `t`.
    Moreover, the dimension of the input sequence becomes very large. Likewise, punctuation-based
    models cannot treat some expressions, such as *haven't* or *ain't*, properly.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一些传统的标记器是在Moses和`nltk`库中开发的，应用了先进的基于规则的技术。但是与Transformers一起使用的标记化算法基于自监督学习，并从语料库中提取规则。基于规则的标记化的简单直观解决方案是使用字符、标点符号或空格。基于字符的标记化会导致语言模型丢失输入的含义。尽管它可以减少词汇量，这是好的，但它使模型难以通过字符`c`、`a`和`t`的编码来捕获`cat`的含义。此外，输入序列的维度变得非常大。同样，基于标点符号的模型不能正确处理一些表达，例如*haven't*或*ain't*。
- en: 'Recently, several advanced subword tokenization algorithms, such as BPE, have
    become an integral part of Transformer architectures. These modern tokenization
    procedures consist of two phases: The pre-tokenization phase simply splits the
    input into tokens either using space as or language-dependent rules. Second, the
    tokenization training phase is to train the tokenizer and build a base vocabulary
    of a reasonable size based on tokens. Before training our own tokenizers, let''s
    load a pre-trained tokenizer. The following code loads a Turkish tokenizer, which
    is of type `BertTokenizerFast`, from the `Transformers` library with a vocabulary
    size of 32K:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些先进的子词标记化算法，如BPE，已成为Transformer架构的一个组成部分。这些现代标记化过程由两个阶段组成：预标记化阶段只是使用空格或语言相关的规则将输入分割为标记。其次，标记化训练阶段是为了训练分词器并基于标记构建一个合理大小的基本词汇表。在训练我们自己的分词器之前，让我们加载一个预训练的分词器。以下代码加载了一个土耳其分词器，类型为`BertTokenizerFast`，词汇量为32K：
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following code loads an English BERT tokenizer for the `bert-base-uncased`
    model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载了一个英语BERT分词器，用于`bert-base-uncased`模型：
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s see how they work! We tokenize the word `telecommunication` with these
    two tokenizers:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它们是如何工作的！我们使用这两个标记器对单词`telecommunication`进行标记化：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `word_en` token is already in the vocabulary of the English tokenizer but
    not in that of the Turkish one. So let''s see what happens with the Turkish tokenizer:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_en`标记已经在英语分词器的词汇表中，但不在土耳其分词器的词汇表中。所以让我们看看土耳其分词器会发生什么：'
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Since the Turkish tokenizer model has no such a word in its vocabulary, it
    needs to break the word into parts that make sense to it. All these split tokens
    are already stored in the model vocabulary. Please notice the output of the following
    execution:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 由于土耳其分词器模型的词汇表中没有这样一个词，它需要将单词分解成对它有意义的部分。所有这些分割的标记已经存储在模型词汇表中。请注意以下执行的输出：
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s tokenize the same word with the English tokenizer that we already loaded:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们已经加载的英语分词器对相同的单词进行分词：
- en: '[PRE50]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Since the English model has the word `telecommunication` in the base vocabulary,
    it does not need to break it into parts but rather takes it as a whole. By learning
    from the corpus, the tokenizers are capable of transforming a word into mostly
    grammatically logical subcomponents. Let''s take a difficult example from Turkish.
    As an agglutinative language, Turkish allows us to add many suffixes to a word
    stem to construct very long words. Here is one of the longest words in the Turkish
    language used in a text ([https://en.wikipedia.org/wiki/Longest_word_in_Turkish](https://en.wikipedia.org/wiki/Longest_word_in_Turkish)):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于英语模型在基础词汇表中有单词`telecommunication`，它不需要将其分割成部分，而是将其作为一个整体。通过从语料库中学习，分词器能够将一个单词转换为大部分语法逻辑的子部分。让我们以土耳其语的一个难例为例。作为一种聚合语言，土耳其语允许我们在一个词干上加入许多后缀，构成非常长的单词。以下是土耳其语中使用的最长单词之一（来源于[https://en.wikipedia.org/wiki/Longest_word_in_Turkish](https://en.wikipedia.org/wiki/Longest_word_in_Turkish)）：
- en: '*Muvaffakiyetsizleştiricileştiriveremeyebileceklerimizdenmişsinizcesine*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*Muvaffakiyetsizleştiricileştiriveremeyebileceklerimizdenmişsinizcesine*'
- en: 'It means that *As though you happen to have been from among those whom we will
    not be able to easily/quickly make a maker of unsuccessful ones*. The Turkish
    BERT tokenizer may not have seen this word in training, but it has seen its pieces;
    *muvaffak (succesful) as the stem, ##iyet(successfulness), ##siz (unsuccessfulness),
    ##leş (become unsuccessful)*, and so forth. The Turkish tokenizer extracts components
    that seem to be grammatically logical for the Turkish language when comparing
    the results with a Wikipedia article:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 它的意思是*仿佛你是我们中那些我们不能轻易快速地使成为失败者的人之一*。土耳其BERT分词器可能在训练中没有见过这个单词，但它已经看到了它的部分；*muvaffak（成功）作为词干，##iyet（成功性），##siz（不成功），##leş（变得不成功）*，等等。当将结果与维基百科文章进行比较时，土耳其分词器提取出了在土耳其语中看起来是语法合乎逻辑的组件。
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The Turkish tokenizer is an example of the WordPiece algorithm since it works
    with a BERT model. Almost all language models including BERT, DistilBERT, and
    ELECTRA require a WordPiece tokenizer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 土耳其分词器是WordPiece算法的一个例子，因为它与BERT模型协同工作。几乎所有的语言模型，包括BERT、DistilBERT和ELECTRA，都需要一个WordPiece分词器。
- en: Now we are ready to take a look at the tokenization approaches used with Transformers.
    First, we'll discuss the widely used tokenizations of BPE, WordPiece, and SentencePiece
    a bit and then train them with HuggingFace's fast `tokenizers` library.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备研究用于Transformers的分词方法。首先，我们将简要讨论一下BPE、WordPiece和SentencePiece的广泛使用的分词，然后用HuggingFace的快速`分词器`库进行训练。
- en: Byte pair encoding
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字节对编码
- en: '**BPE** is a data compression technique. It scans the data sequence and iteratively
    replaces the most frequent pair of bytes with a single symbol. It was first adapted
    and proposed in *Neural Machine Translation of Rare Words with Subword Units,
    Sennrich et al. 2015,* to solve the problem of unknown words and rare words for
    machine translation. Currently, it is successfully being used within GPT-2 and
    many other state-of-the-art models. Many modern tokenization algorithms are based
    on such compression techniques.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**BPE** 是一种数据压缩技术。它会扫描数据序列，并迭代地用一个单一的符号替换最常见的字节对。它最初是在*Neural Machine Translation
    of Rare Words with Subword Units, Sennrich et al. 2015*中提出来解决机器翻译中未知单词和稀有单词的问题。目前，它成功应用于GPT-2和许多其他领先的模型中。许多现代分词算法都是基于这样的压缩技术。'
- en: It represents text as a sequence of character n-grams, which are also called
    character-level subwords. The training starts initially with a vocabulary of all
    Unicode characters (or symbols) seen in the corpus. This can be small for English
    but can be large for character-rich languages such as Japanese. Then, it iteratively
    computes character bigrams and replaces the most frequent ones with special new
    symbols. For example, *t* and *h* are frequently occurring symbols. We replace
    consecutive symbols with the *th* symbol. This process is kept iteratively running
    until the vocabulary has attained the desired vocabulary size. The most common
    vocabulary size is around 30K.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 它将文本表示为字符 n-gram 的序列，也称为字符级子词。训练首先从语料库中看到的所有 Unicode 字符（或符号）的词汇表开始。对于英语来说，这可能很小，但对于日语等字符丰富的语言来说可能很大。然后，它迭代计算字符二元组，并将最常见的字符替换为特殊的新符号。例如，*t*
    和 *h* 是频繁出现的符号。我们用 *th* 符号替换连续的符号。该过程一直迭代运行，直到词汇表达到所需的词汇量为止。最常见的词汇量约为 30K。
- en: BPE is particularly effective at representing unknown words. However, it may
    not guarantee the handling of rare words and/or words including rare subwords.
    In such cases, it associates rare characters with a special symbol, *<UNK>*, which
    may lead to losing meaning in words a bit. As a potential solution, **Byte-Level
    BPE** (**BBPE**) has been proposed, which uses a 256-byte set of vocabulary instead
    of Unicode characters to ensure that every base character is included in the vocabulary.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 在表示未知词时特别有效。然而，它可能无法保证处理包含罕见子词或罕见词的情况。在这种情况下，它将罕见字符与特殊符号 *<UNK>* 关联起来，这可能会导致词义稍微丢失。作为潜在解决方案，**Byte-Level
    BPE** (**BBPE**) 已被提出，它使用 256 字节的词汇表，而不是 Unicode 字符，以确保每个基本字符都包含在词汇表中。
- en: WordPiece tokenization
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: WordPiece 分词
- en: '**WordPiece** is another popular word segmentation algorithm widely used with
    BERT, DistilBERT, and Electra. It was proposed by Schuster and Nakajima to solve
    the Japanese and Korean voice problem in 2012\. The motivation behind this work
    was that, although not a big issue for the English language, word segmentation
    is important preprocessing for many Asian languages, because in these languages
    spaces are rarely used. Therefore, we come across word segmentation approaches
    in NLP studies in Asian languages more often. Similar to BPE, WordPiece uses a
    large corpus to learn vocabulary and merging rules. While BPE and BBPE learn the
    merging rules based on co-occurrence statistics, the WordPiece algorithm uses
    maximum likelihood estimation to extract the merging rules from a corpus. It first
    initializes the vocabulary with Unicode characters, which are also called vocabulary
    symbols. It treats each word in the training corpus as a list of symbols (initially
    Unicode characters), and then it iteratively produces a new symbol merging two
    symbols out of all the possible candidate symbol pairs based on the likelihood
    maximization rather than frequency. This production pipeline continues until the
    desired vocabulary size is reached.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**WordPiece** 是另一种广泛与 BERT、DistilBERT 和 Electra 配合使用的流行分词算法。它由 Schuster 和 Nakajima
    在 2012 年提出，旨在解决日语和韩语语音问题。该工作的动机是，虽然对于英语来说不是一个大问题，但对于许多亚洲语言来说，分词是重要的预处理，因为在这些语言中，空格很少使用。因此，在亚洲语言的自然语言处理研究中，我们经常会遇到分词方法。与
    BPE 类似，WordPiece 使用大型语料库来学习词汇和合并规则。虽然 BPE 和 BBPE 基于共现统计学习合并规则，但 WordPiece 算法使用最大似然估计从语料库中提取合并规则。它首先用
    Unicode 字符（也称为词汇符号）初始化词汇表。它将训练语料库中的每个词视为符号列表（最初是 Unicode 字符），然后根据最大似然估计从所有可能的候选符号对中选择两个符号进行合并，而不是根据频率。该生产管道持续进行，直到达到所需的词汇量为止。'
- en: Sentence piece tokenization
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sentence piece 分词
- en: Previous tokenization algorithms treat text as a space-separated word list.
    This space-based splitting does not work in some languages. In the German language,
    compound nouns are written without spaces, for example, menschenrechte (human
    rights). The solution is to use language-specific pre-tokenizers. In German, an
    NLP pipeline leverages a compound-splitter module to check whether a word can
    be subdivided into smaller words. However, East Asian languages (for example,
    Chinese, Japanese, Korean, and Thai) do not use spaces between words. The `_`
    character, which is also why we saw `_` in the output of the Albert model example
    earlier. Other popular language models that use SentencePiece are XLNet, Marian,
    and T5\.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的标记化算法将文本视为以空格分隔的单词列表。这种基于空格的分割在一些语言中不起作用。在德语中，复合名词是没有空格的，例如menschenrechte（人权）。解决方案是使用特定于语言的预标记器。在德语中，NLP流水线利用复合分割模块来检查一个词是否可以分解为较小的词。然而，东亚语言（例如中文、日文、韩文和泰文）之间不使用空格。下划线`_`字符，这也是我们之前在Albert模型示例的输出中看到`_`的原因。其他使用SentencePiece的流行语言模型有XLNet、Marian和T5。
- en: So far, we have discussed subword tokenization approaches. It is time to start
    conducting experiments for training with the `tokenizers` library.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了子词标记化方法。现在是时候开始使用`tokenizers`库进行训练实验了。
- en: The tokenizers library
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记化器库
- en: You may have noticed that the already-trained tokenizers for Turkish and English
    are part of the `Transformers` library in the previous code examples. On the other
    hand, the HuggingFace team provided the `tokenizers` library independently from
    the `Transformers` library to be fast and give us more freedom. The library was
    originally written in Rust, which makes multi-core parallel computations possible
    and is wrapped with Python ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，以前的代码示例中土耳其语和英语的已经训练好的标记化器是`Transformers`库的一部分。另一方面，HuggingFace团队独立于`Transformers`库提供了`tokenizers`库，以便更快地给我们更多的自由。该库最初是用Rust编写的，这使得多核并行计算成为可能，并且用Python进行了包装（[https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)）。
- en: 'To install the `tokenizers` library, we use this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`tokenizers`库，我们使用这个：
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `tokenizers` library provides several components so that we can build an
    end-to-end tokenizer from preprocessing the raw text to decoding tokenized unit
    IDs:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenizers`库提供了几个组件，以便我们从预处理原始文本到解码标记化单元ID构建端到端的分词器：'
- en: '*Normalizer→ PreTokenizer → Modeling → Post-Processor → Decoding*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*Normalizer→ PreTokenizer → 建模 → 后处理 → 解码*'
- en: 'The following diagram depicts the tokenization pipeline:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描述了标记化流水线：
- en: '![Figure 3.17 – Tokenization pipeline ](img/B17123_03_17.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图3.17 – 标记化流水线](img/B17123_03_17.jpg)'
- en: Figure 3.17 – Tokenization pipeline
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 – 标记化流水线
- en: '**Normalizer** allows us to apply primitive text processing such as lowercasing,
    stripping, Unicode normalization, and removing accents.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规范化器**允许我们应用原始文本处理，例如小写处理、剥离、Unicode规范化和去除重音。'
- en: '**PreTokenizer** prepares the corpus for the next training phase. It splits
    the input into tokens depending on the rules, such as whitespace.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预标记器**为下一个训练阶段准备语料库。它根据规则（例如空格）将输入拆分为标记。'
- en: '**Model Training** is a subword tokenization algorithm such as *BPE*, *BBPE*,
    and *WordPiece*, which we''ve discussed already. It discovers subwords/vocabulary
    and learns generation rules.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**是一个子词标记化算法，例如*BPE*、*BBPE*和*WordPiece*，我们已经讨论过了。它发现子词/词汇并学习生成规则。'
- en: '**Post-processing** provides advanced class construction that is compatible
    with Transformers models such as BertProcessors. We mostly add special tokens
    such as *[CLS]* and *[SEP]* to the tokenized input just before feeding the architecture.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**提供了与Transformer模型（如BertProcessors）兼容的高级类构造。我们主要是在馈送给架构之前，向标记化的输入中添加特殊标记，例如*[CLS]*和*[SEP]*。'
- en: '**Decoder** is in charge of converting token IDs back to the original string.
    It is just for inspecting what is going on.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**负责将标记ID转换回原始字符串。它只是用来检查发生了什么。'
- en: Training BPE
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练BPE
- en: 'Let''s train a BPE tokenizer using Shakespeare''s plays:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用莎士比亚的戏剧来训练一个BPE分词器：
- en: 'It is loaded as follows:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载如下：
- en: '[PRE53]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We will need a post-processor (`TemplateProcessing`) for all the tokenization
    algorithms ahead. We need to customize the post-processor to make the input convenient
    for a particular language model. For example, the following template will be suitable
    for the BERT model since it needs the *[CLS]* token at the beginning of the input
    and *[SEP]* tokens both at the end and in the middle.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于所有的分词算法，我们需要一个后处理器（`TemplateProcessing`）。我们需要自定义后处理器以便为特定的语言模型提供方便的输入。例如，以下模板适用于BERT模型，因为它需要在输入开头有*[CLS]*标记，在末尾和中间都有*[SEP]*标记。
- en: 'We define the template as follows:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如下定义模板：
- en: '[PRE54]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We import the necessary components to build an end-to-end tokenization pipeline:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的组件来构建一个端到端的分词流程：
- en: '[PRE55]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We start by instantiating **BPE** as follows:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先实例化**BPE**如下：
- en: '[PRE56]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The preprocessing part has two components: *normalizer* and *pre-tokenizer*.
    We may have more than one normalizer. So, we compose a `Sequence` of normalizer
    components that includes multiple normalizers where `NFD()` is a Unicode normalizer
    and `StripAccents()` removes accents. For pre-tokenization, `Whitespace()` gently
    breaks the text based on space. Since the decoder component must be compatible
    with the model, `BPEDecoder` is selected for the `BPE` model:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理部分有两个组件：*normalizer*和*pre-tokenizer*。我们可能有多个normalizer。因此，我们组成一个包含多个normalizer的`Sequence`，其中`NFD()`是一个Unicode正规化组件，而`StripAccents()`会移除重音符号。对于pre-tokenization，`Whitespace()`会根据空格来分隔文本。由于解码器组件必须与模型兼容，因此选择了`BPEDecoder`作为`BPE`模型的解码器：
- en: '[PRE57]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Well! We are ready to train the tokenizer on the data. The following execution
    instantiates `BpeTrainer()`, which helps us to organize the entire training process
    by setting hyperparameters. We set the vocabulary size parameter to 5K since our
    Shakespeare corpus is relatively small. For a large-scale project, we use a bigger
    corpus and normally set the vocabulary size to around 30K:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 好了！我们已经准备好对数据进行分词器训练。以下代码实例化`BpeTrainer()`，它帮助我们通过设置超参数来组织整个训练过程。由于我们的莎士比亚语料库相对较小，我们将词汇表大小参数设置为5K。对于大规模项目，我们使用更大的语料库，通常将词汇表大小设置在30K左右：
- en: '[PRE58]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We have completed the training!
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已经完成了训练！
- en: Important note
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Training from the filesystem: To start the training process, we passed an in-memory
    Shakespeare object as a list of strings to `tokenizer.train_from_iterator()`.
    For a large-scale project with a large corpus, we need to design a Python generator
    that yields string lines mostly by consuming the files from the filesystem rather
    than in-memory storage. You should also check `tokenizer.train()` to train from
    the filesystem storage as applied in the BERT training section above.'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从文件系统训练：为了开始训练过程，我们将一个内存中的莎士比亚对象作为字符串列表传递给`tokenizer.train_from_iterator()`。对于一个具有大语料库的大规模项目，我们需要设计一个Python生成器，主要通过消耗文件系统的文件而不是内存存储来产生字符串行。您还应该在上面的BERT训练部分检查`tokenizer.train()`，以便从文件系统存储中进行训练。
- en: 'Let''s grab a random sentence from the play Macbeth, name it `sen`, and tokenize
    it with our fresh tokenizer:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们随机选取一句来自Macbeth剧本的句子，将其命名为`sen`，并使用我们的新分词器对其进行分词：
- en: '[PRE59]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Thanks to the post-processor function above, we see additional *[CLS]* and
    *[SEP]* tokens in the proper position. There is only one split word, *handle*
    (*hand*, *le*), since we passed to the model a sentence from the play Macbeth
    that the model already knew. Besides, we used a small corpus, and the tokenizer
    is not forced to use compression. Let''s pass a challenging phrase, `Hugging Face`,
    that the tokenizer might not know:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多亏了上面的后处理器函数，我们在正确的位置看到了额外的*[CLS]*和*[SEP]*标记。由于我们传递给模型的是Macbeth剧本中的一句话，模型已经知道这个句子，所以只有一个分割单词*handle*（*hand*，*le*）。此外，我们使用了一个小语料库，分词器没有强制使用压缩。让我们传递一个有挑战性的短语`Hugging
    Face`，分词器可能不认识：
- en: '[PRE60]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The term `Hugging` is lowercased and split into three pieces `hu`, `gg`, `ing`,
    since the model vocabulary contains all other tokens but `Hugging`. Let''s pass
    two sentences now:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 术语`Hugging`被转换为小写，并拆分为三段`hu`，`gg`，`ing`，因为模型的词汇表包含了除`Hugging`之外的所有其他标记。现在让我们传递两个句子：
- en: '[PRE61]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Notice that the post-processor injected the `[SEP]` token as an indicator.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，后处理器将`[SEP]`标记作为指示器插入。
- en: 'It is time to save the model. We can either save the sub-word tokenization
    model or the entire tokenization pipeline. First, let''s save the BPE model only:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是时候保存模型了。我们可以保存子词分词模型，也可以保存整个分词流程。首先，让我们仅保存BPE模型：
- en: '[PRE62]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The model saved two files regarding vocabulary and merging rules. The `merge.txt`
    file is composed of 4,948 merging rules:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有关词汇和合并规则，模型保存了两个文件。`merge.txt`文件由4948个合并规则组成：
- en: '[PRE63]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The top five rules ranked are as shown in the following where we see that (`t`,
    `h`) is the first ranked rule due to that being the most frequent pair. For testing,
    the model scans the textual input and tries to merge these two symbols first if
    applicable:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前五条规则按如下所示排名，我们会发现 (`t`, `h`) 是由于成为最常见的一对而排名第一。对于测试，模型会扫描文本输入，如果适用，会首先尝试合并这两个符号：
- en: '[PRE64]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The BPE algorithm ranks the rules based on frequency. When you manually calculate
    character bigrams in the Shakespeare corpus, you will find (`t`, `h`) the most
    frequent pair.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BPE算法根据频率对规则进行排序。当您在莎士比亚语料库中手动计算字符二元组时，您会发现 (`t`, `h`) 是最常见的一对。
- en: 'Let''s now save and load the entire tokenization pipeline:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们保存和加载整个分词流程：
- en: '[PRE65]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We successfully reloaded the tokenizer!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功重新加载了分词器！
- en: Training the WordPiece model
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练WordPiece模型
- en: 'In this section, we will train the WordPiece model:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将训练WordPiece模型：
- en: 'We start by importing the necessary modules:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的模块：
- en: '[PRE66]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The following lines instantiate an empty WordPiece tokenizer and prepare it
    for training. `BertNormalizer` is a pre-defined normalizer sequence that includes
    the processes of cleaning the text, transforming accents, handling Chinese characters,
    and lowercasing:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码示例创建一个空的WordPiece分词器，并准备好进行训练。`BertNormalizer`是一个预定义的规范化序列，包括文本清理、变换重音、处理中文字符和小写化的过程：
- en: '[PRE67]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now, we instantiate a proper trainer, `WordPieceTrainer()` for `WordPiece()`,
    to organize the training process:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建一个适当的训练器，`WordPieceTrainer()`用于`WordPiece()`，以组织训练过程：
- en: '[PRE68]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s use `WordPieceDecoder()` to treat the sentences properly:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`WordPieceDecoder()`正确处理句子：
- en: '[PRE69]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We have not come across any `[UNK]` tokens in the output since the tokenizer
    somehow knows or splits the input for encoding. Let''s force the model to produce
    `[UNK]`tokens as in the following code. Let''s pass a Turkish sentence to our
    tokenizer:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在输出中没有遇到任何`[UNK]`标记，因为分词器以某种方式知道或分割输入进行编码。让我们强制模型生成`[UNK]`标记，就像下面的代码中所示。让我们向分词器传递一句土耳其语的句子：
- en: '[PRE70]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Well done! We have a couple of unknown tokens since the tokenizer does not find
    a way to decompose the given word from the merging rules and the base vocabulary.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 做得好！我们有一些未知标记，因为分词器无法从合并规则和基础词汇中找到给定单词的拆分方式。
- en: 'So far, we have designed our tokenization pipeline all the way from the normalizer
    component to the decoder component. On the other hand, the `tokenizers` library
    provides us with an already made (not trained) empty tokenization pipeline with
    proper components to build quick prototypes for production. Here are some pre-made
    tokenizers:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经设计好了从规范化组件到解码器组件的分词流程。另一方面，`tokenizers`库为我们提供了一个已经准备好的（未经训练的）空的分词流程，其中包括适当的组件，用于快速创建生产原型。以下是一些预先制作的分词器：
- en: '`CharBPETokenizer`: The original BPE'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CharBPETokenizer`：原始的BPE'
- en: '`ByteLevelBPETokenizer`: The byte-level version of the BPE'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ByteLevelBPETokenizer`：BPE的字节级版本'
- en: '`SentencePieceBPETokenizer`: A BPE implementation compatible with the one used
    by *SentencePiece*'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SentencePieceBPETokenizer`：与*SentencePiece*使用的BPE兼容的BPE实现'
- en: '`BertWordPieceTokenizer`: The famous BERT tokenizer, using WordPiece'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertWordPieceTokenizer`：著名的BERT分词器，使用WordPiece'
- en: 'The following code imports these pipelines:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码导入了这些分词流程：
- en: '[PRE71]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: All these pipelines are already designed for us. The rest of the process (such
    as training, saving the model, and using the tokenizer) is the same as our previous
    BPE and WordPiece training procedure.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些分词流程都已经为我们设计好。其余的流程（如训练、保存模型和使用分词器）与之前的BPE和WordPiece训练过程相同。
- en: Well done! We have made great progress and trained our first Transformer model
    as well as its tokenizer.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！我们取得了巨大的进展，并且已经训练了我们的第一个Transformer模型以及其分词器。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have experienced autoencoding models both theoretically
    and practically. Starting with basic knowledge about BERT, we trained it as well
    as a corresponding tokenizer from scratch. We also discussed how to work inside
    other frameworks, such as Keras. Besides BERT, we also reviewed other autoencoding
    models. To avoid excessive code repetition, we did not provide the full implementation
    for training other models. During the BERT training, we trained the WordPiece
    tokenization algorithm. In the last part, we examined other tokenization algorithms
    since it is worth discussing and understanding all of them.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从理论和实践两方面体验了自动编码模型。从对 BERT 的基本知识开始，我们从零开始训练了它以及相应的分词器。我们还讨论了如何在其他框架内工作，例如
    Keras。除了 BERT，我们还回顾了其他自动编码模型。为了避免过多的代码重复，我们没有提供训练其他模型的完整实现。在 BERT 训练期间，我们训练了 WordPiece
    分词算法。在最后一部分，我们检查了其他分词算法，因为讨论和理解它们都是值得的。
- en: Autoencoding models use the left decoder side of the original Transformer and
    are mostly fine-tuned for classification problems. In the next chapter, we will
    discuss and learn about the right decoder part of Transformers to implement language
    generation models.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码模型使用原始 Transformer 的左解码器侧，主要用于分类问题的微调。在下一章中，我们将讨论并学习 Transformer 的右解码器部分，以实现语言生成模型。
