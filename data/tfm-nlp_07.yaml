- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: The Rise of Suprahuman Transformers with GPT-3 Engines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超人类变压器与 GPT-3 引擎的崛起
- en: In 2020, *Brown* et al. (2020) described the training of an OpenAI GPT-3 model
    containing 175 billion parameters that learned using huge datasets such as the
    400 billion byte-pair-encoded tokens extracted from Common Crawl data. OpenAI
    ran the training on a Microsoft Azure supercomputer with 285,00 CPUs and 10,000
    GPUs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2020 年，*Brown* 等人（2020）描述了一个包含 1750 亿个参数的 OpenAI GPT-3 模型的训练，该模型使用了诸如从 Common
    Crawl 数据中提取的 4000 亿个字节对编码的令牌等庞大的数据集进行了学习。OpenAI 在一个拥有 285,000 个 CPU 和 10,000 个
    GPU 的 Microsoft Azure 超级计算机上进行了训练。
- en: The machine intelligence of OpenAI’s GPT-3 engines and their supercomputer led
    *Brown* et al. (2020) to zero-shot experiments. The idea was to use a trained
    model for downstream tasks without further training the parameters. The goal would
    be for a trained model to go directly into multi-task production with an API that
    could even perform tasks it wasn’t trained for.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 GPT-3 引擎的机器智能和超级计算机使 *Brown* 等人（2020）进行了零次实验。这个想法是使用一个训练好的模型进行下游任务，而不需要进一步训练参数。目标是让一个训练好的模型直接进入多任务生产，使用甚至可以执行它没有经过训练的任务的
    API。
- en: The era of suprahuman cloud AI engines was born. OpenAI’s API requires no high-level
    software skills or AI knowledge. You might wonder why I used the term “suprahuman.”
    You will discover that a GPT-3 engine can perform many tasks as well as a human
    in many cases. For the moment, *it is essential to understand how GPT models are
    built and run to appreciate the magic*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 超人类云 AI 引擎的时代已经到来。OpenAI 的 API 不需要高级软件技能或 AI 知识。你可能想知道为什么我使用了“超人类”这个词。你会发现，在许多情况下，GPT-3
    引擎可以执行许多任务，就像人类一样。目前，*理解 GPT 模型是如何构建和运行的是至关重要的*。
- en: This chapter will first examine the architecture and the evolution of the size
    of the transformer model. We will investigate the zero-shot challenge of using
    trained transformer models with little to no fine-tuning of the model’s parameters
    for downstream tasks. We will explore the innovative architecture of GPT transformer
    models. OpenAI provides specially trained versions of their models named engines.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先研究变压器模型的架构和规模的演变。我们将探讨使用经过训练的变压器模型进行零次调优的挑战，对模型的参数进行几乎没有或没有任何调优的下游任务。我们将探讨
    GPT 变压器模型的创新架构。OpenAI 提供了他们模型的特别训练版本，称为引擎。
- en: We will use a 345M parameter GPT-2 transformer in TensorFlow from OpenAI’s repository.
    We must get our hands dirty to understand GPT models. We will interact with the
    model to produce text completion with general conditioning sentences.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 OpenAI 仓库中的 TensorFlow 中的 345M 参数的 GPT-2 变压器。我们必须亲自动手去理解 GPT 模型。我们将与模型进行交互，以生成通用调节句子的文本补全。
- en: We will continue by using a 117M parameter customized GPT-2 model. We will tokenize
    the high-level conceptual `Kant` dataset we used to train the RoBERTa model in
    *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用一个 117M 参数的定制 GPT-2 模型。我们将对我们在*第四章*，*从零开始预训练 RoBERTa 模型*中使用的高级概念 `Kant`
    数据集进行标记化。
- en: The chapter will then explore using a GPT-3 engine that does not require a data
    scientist, an artificial specialist, or even an experienced developer to *get
    started*. However, that does not mean that a data scientist or an AI specialist
    will not be required down the line.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节将探讨使用一个不需要数据科学家、人工专家甚至是经验丰富的开发人员就可以*开始使用*的 GPT-3 引擎。然而，这并不意味着在今后不会需要数据科学家或人工智能专家。
- en: We will see that GPT-3 engines do sometimes require fine-tuning. We will run
    a Google Colab notebook to fine-tune a GPT-3 Ada engine.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到 GPT-3 引擎有时确实需要微调。我们将运行一个 Google Colab 笔记本来微调一个 GPT-3 Ada 引擎。
- en: The chapter will end with the new mindset and skillset of an Industry 4.0 AI
    specialist.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节将以工业 4.0 AI 专家的新思维方式和技能集结束。
- en: By the end of the chapter, you will know how a GPT model is built and how to
    use a seamless GPT-3 API. You will understand the gratifying tasks an Industry
    4.0 AI specialist can accomplish in the 2020s!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将了解到一个 GPT 模型是如何构建的，以及如何使用无缝的 GPT-3 API。你将理解到在 2020 年代，一个工业 4.0 的 AI
    专家可以完成令人满意的任务！
- en: 'This chapter covers the following topics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Getting started with a GPT-3 model
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 GPT-3 模型
- en: The architecture of OpenAI GPT models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI GPT 模型的架构
- en: Defining zero-shot transformer models
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义零次训练的变压器模型
- en: The path from few-shots to one-shot
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从少次到一次的路径
- en: Building a near-human GPT-2 text completion model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个接近人类的 GPT-2 文本补全模型
- en: Implementing a 345M parameter model and running it
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个 345M 参数的模型并运行它
- en: Interacting with GPT-2 with a standard model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与一个标准模型交互使用 GPT-2
- en: Training a language modeling GPT-2 117M parameter model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个具有 117M 参数模型的语言建模 GPT-2
- en: Importing a customized and specific dataset
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入一个定制的特定数据集
- en: Encoding a customized dataset
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对一个定制数据集进行编码
- en: Conditioning the model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行条件调节
- en: Conditioning a GPT-2 model for specific text completion tasks
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件转换一个 GPT-2 模型以执行特定的文本完成任务
- en: Fine-tuning a GPT-3 model
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对一个 GPT-3 模型进行微调
- en: The role of an Industry 4.0 AI specialist
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工业 4.0 人工智能专家的角色
- en: Let’s begin our journey by exploring GPT-3 transformer models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索 GPT-3 转换器模型开始我们的旅程。
- en: Suprahuman NLP with GPT-3 transformer models
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-3 转换器模型进行超人类 NLP
- en: GPT-3 is built on the GPT-2 architecture. However, a fully trained GPT-3 transformer
    is a foundation model. A foundation model can do many tasks it wasn’t trained
    for. GPT-3 completion applies all NLP tasks and even programming tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 建立在 GPT-2 架构之上。然而，一个完全训练的 GPT-3 转换器是一个基础模型。基础模型可以执行许多它没有训练过的任务。GPT-3 完成应用于所有
    NLP 任务甚至编程任务。
- en: GPT-3 is one of the few fully trained transformer models that qualify as a foundation
    models. GPT-3 will no doubt lead to more powerful OpenAI models. Google will produce
    foundation models beyond the Google BERT version they trained on their supercomputers.
    Foundation models represent a new way of thinking about AI.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 是少数几个符合基础模型资格的完全训练的转换器模型之一。GPT-3 毫无疑问将产生更强大的 OpenAI 模型。Google 将为其在超级计算机上训练的
    Google BERT 版本之外的基础模型提供更多。基础模型代表了对人工智能的一种新思考方式。
- en: It will not take long for companies to realize they do not need a data scientist
    or an AI specialist to start an NLP project with an API like the one that OpenAI
    provides.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 公司很快就会意识到，他们不需要数据科学家或人工智能专家才能开始一个具有像 OpenAI 提供的 API 那样的 NLP 项目。
- en: Why bother with any other tool? An OpenAI API is available with access to one
    of the most efficient transformer models trained on one of the most powerful supercomputers
    in the world.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么还要麻烦使用任何其他工具？拥有访问世界上最强大的超级计算机之一上训练过的最有效的转换器模型之一的 OpenAI API。
- en: Why develop tools, download libraries, or use any other tool if an API exists
    that only deep pockets and the best research teams in the world can design, such
    as Google or OpenAI?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在只有深腰包和世界上最好的研究团队才能设计的 API，比如 Google 或 OpenAI，为什么要开发工具、下载库或使用任何其他工具？
- en: The answer to these questions is quite simple. It’s easy to start a GPT-3 engine,
    just as it is to start a Formula 1 or Indy 500 race car. No problem. But then,
    trying to drive such a car is nearly impossible without months of training! GPT-3
    engines are powerful AI race cars. You can get them to run in a few clicks. However,
    mastering their incredible horsepower requires the knowledge you have acquired
    from the beginning of this book up to now and what you will discover in the following
    chapters!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些问题的答案非常简单。启动 GPT-3 引擎就像启动一辆一级方程式赛车或印第500赛车一样简单。没问题。但是，要驾驶这样的车几乎是不可能的，如果没有几个月的培训！GPT-3
    引擎是强大的人工智能赛车。你可以通过几次点击让它们运行。然而，要掌握它们令人难以置信的动力需要你从本书的开始到现在所学到的知识，以及你将在接下来的章节中发现的东西！
- en: We first need to understand the architecture of GPT models to see where developers,
    AI specialists, and data scientists fit in the era of suprahuman NLP models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要了解 GPT 模型的架构，以了解开发者、人工智能专家和数据科学家在超人类 NLP 模型时代的定位。
- en: The architecture of OpenAI GPT transformer models
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI GPT 转换器模型的架构
- en: Transformers went from training, to fine-tuning, and finally to zero-shot models
    in less than three years between the end of 2017 and the first part of 2020\.
    A zero-shot GPT-3 transformer model requires no fine-tuning. The trained model
    parameters are not updated for downstream multi-tasks, which opens a new era for
    NLP/NLU tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2017 年底至 2020 年初的不到三年时间里，转换器模型从训练、微调，最终到零冲击模型。零冲击 GPT-3 转换器模型不需要微调。训练的模型参数不会为下游多任务更新，这为
    NLP/NLU 任务开启了一个新时代。
- en: In this section, we will first learn about the motivation of the OpenAI team
    that designed GPT models. We will begin by going through the fine-tuning of zero-shot
    models. Then we will see how to condition a transformer model to generate mind-blowing
    text completion. Finally, we will explore the architecture of GPT models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将首先了解设计 GPT 模型的 OpenAI 团队的动机。我们将从零冲击模型的微调开始。然后我们将看到如何使一个转换器模型调节以生成令人惊叹的文本完成。最后，我们将探索
    GPT 模型的架构。
- en: We will first go through the creation process of the OpenAI team.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将了解 OpenAI 团队的创建过程。
- en: The rise of billion-parameter transformer models
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 十亿参数变压器模型的崛起
- en: The speed at which transformers went from small models trained for NLP tasks
    to models that require little to no fine-tuning is staggering.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器从用于自然语言处理任务的小型模型迅速发展到几乎不需要微调的模型的速度令人震惊。
- en: '*Vaswani* et al. (2017) introduced the Transformer, which surpassed CNNs and
    RNNs on BLEU tasks. *Radford* et al. (2018) introduced the **Generative Pre-Training**
    (**GPT**) model, which could perform downstream tasks with fine-tuning. *Devlin*
    et al. (2019) perfected fine-tuning with the BERT model. *Radford* et al. (2019)
    went further with GPT-2 models.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*瓦斯瓦尼*等人（2017年）介绍了变压器，它在BLEU任务中超越了CNN和RNN。 *拉德福德*等人（2018年）引入了**生成式预训练**（**GPT**）模型，可以通过微调执行下游任务。
    *德夫林*等人（2019年）通过BERT模型完善了微调。 *拉德福德*等人（2019年）进一步推出了GPT-2模型。'
- en: '*Brown* et al. (2020) defined a GPT-3 zero-shot approach to transformers that
    does not require fine-tuning!'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*布朗*等人（2020年）定义了一个GPT-3零-shot方法来处理变压器，不需要微调！'
- en: At the same time, *Wang* et al. (2019) created GLUE to benchmark NLP models.
    But transformer models evolved so quickly that they surpassed human baselines!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，*王*等人（2019年）创建了GLUE来评估自然语言处理模型。但是变压器模型发展如此迅速，以至于超过了人类基线！
- en: '*Wang* et al. (2019, 2020) rapidly created SuperGLUE, set the human baselines
    much higher, and made the NLU/NLP tasks more challenging. Transformers are rapidly
    progressing, and some have already surpassed Human Baselines on the SuperGLUE
    leaderboards at the time of writing.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*王*等人（2019年，2020年）迅速创建了SuperGLUE，将人类基线设定得更高，并使得NLU/NLP任务更具挑战性。变压器发展迅速，一些模型在写作时已经超越了SuperGLUE排行榜上的人类基线。'
- en: How did this happen so quickly?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何如此迅速地发生的？
- en: We will look at one aspect, the models’ sizes, to understand how this evolution
    happened.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究一个方面，即模型的大小，以了解这种演变是如何发生的。
- en: The increasing size of transformer models
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器模型尺寸的增加
- en: 'From 2017 to 2020 alone, the number of parameters increased from 65M parameters
    in the original Transformer model to 175B parameters in the GPT-3 model, as shown
    in *Table 7.1*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在2017年至2020年，参数数量就从原始变压器模型的65M个增加到了GPT-3模型的175B个，如*表7.1*所示：
- en: '| **Transformer Model** | **Paper** | **Parameters** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **变压器模型** | **论文** | **参数** |'
- en: '| Transformer Base | *Vaswani* et al. (2017) | 65M |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 变压器基础 | *瓦斯瓦尼*等人（2017年） | 65M |'
- en: '| Transformer Big | *Vaswani* et al. (2017) | 213M |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 变压器大型 | *瓦斯瓦尼*等人（2017年） | 213M |'
- en: '| BERT-Base | *Devlin* et al. (2019) | 110M |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| BERT-Base | *德夫林*等人（2019年） | 110M |'
- en: '| BERT-Large | *Devlin* et al. (2019) | 340M |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| BERT-Large | *德夫林*等人（2019年） | 340M |'
- en: '| GPT-2 | *Radford* et al. (2019) | 117M |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | *拉德福德*等人（2019年） | 117M |'
- en: '| GPT-2 | *Radford* et al. (2019) | 345M |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | *拉德福德*等人（2019年） | 345M |'
- en: '| GPT-2 | *Radford* et al. (2019) | 1.5B |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | *拉德福德*等人（2019年） | 1.5B |'
- en: '| GPT-3 | *Brown* et al. (2020) | 175B |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | *布朗*等人（2020年） | 175B |'
- en: 'Table 7.1: The evolution of the number of transformer parameters'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1：变压器参数数量的演变
- en: '*Table 7.1* only contains the main models designed during that short time.
    The dates of the publications come after the date the models were actually designed.
    Also, the authors updated the papers. For example, once the original Transformer
    set the market in motion, transformers emerged from Google Brain and Research,
    OpenAI, and Facebook AI, which all produced new models in parallel.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.1*只包含了那个短时间内设计的主要模型。出版日期在实际设计模型的日期之后。此外，作者还更新了论文。例如，一旦原始变压器启动了市场，谷歌Brain、OpenAI和Facebook
    AI等机构都推出了新模型。'
- en: Furthermore, some GPT-2 models are larger than the smaller GPT-3 models. For
    example, the GPT-3 Small model contains 125M parameters, which is smaller than
    the 345M parameter GPT-2 model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些GPT-2模型比较较小的GPT-3模型还要大。例如，GPT-3 Small模型包含125M个参数，比345M参数的GPT-2模型小。
- en: 'The size of the architecture evolved at the same time:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 架构大小同时也在演变：
- en: The number of layers of a model went from 6 layers in the original Transformer
    to 96 layers in the GPT-3 model
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型的层数从原始变压器模型的6层增加到了GPT-3模型的96层
- en: The number of heads of a layer went from 8 in the original Transformer model
    to 96 in the GPT-3 model
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个层的头数从原始变压器模型的8个增加到了GPT-3模型的96个
- en: The context size went from 512 tokens in the original Transformer model to 12,288
    in the GPT-3 model
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始变压器模型的上下文大小从512个令牌增加到了GPT-3模型的12,288个令牌
- en: The architecture’s size explains why GPT-3 175B, with its 96 layers, produces
    more impressive results than GPT-2 1,542M, with only 40 layers. The parameters
    of both models are comparable, but the number of layers has doubled.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构的规模解释了为什么具有 96 层的 GPT-3 175B 模型比只有 40 层的 GPT-2 1,542M 模型产生了更令人印象深刻的结果。这两种模型的参数可比拟，但层数倍增了。
- en: Let’s focus on the context size to understand another aspect of the rapid evolution
    of transformers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注一下上下文大小，以了解 transformer 的快速演变的另一个方面。
- en: Context size and maximum path length
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文大小和最大路径长度
- en: The cornerstone of transformer models resides in the attention sub-layers. In
    turn, the key property of attention sub-layers is the method used to process context
    size.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 模型的基础在于注意力子层。而注意力子层的关键属性则是处理上下文大小的方法。
- en: The context size is one of the main ways humans and machines can learn languages.
    The larger the context size, the more we can understand a sequence presented to
    us.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文大小是人类和机器学习语言的主要方式之一。上下文大小越大，我们就能越好地理解呈现给我们的序列。
- en: However, the drawback of context size is the distance it takes to understand
    what a word refers to. The path taken to analyze long-term dependencies requires
    changing from recurrent to attention layers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，上下文大小的缺点是理解一个词所指内容所需的距离。分析长期依赖性所需的路径需要从循环转换为注意力层。
- en: 'The following sentence requires a long path to find what the pronoun “it” refers
    to:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子需要经过很长的路径才能找到“它”所指的内容：
- en: “Our *house* was too small to fit a big couch, a large table, and other furniture
    we would have liked in such a tiny space. We thought about staying for some time,
    but finally, we decided to sell *it*.”
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: “我们的*房子*太小了，无法容纳一张大沙发，一张大桌子和我们想在如此小的空间中放置的其他家具。我们考虑过留下一段时间，但最终决定把*它*卖掉。”
- en: The meaning of “it” can only be explained if we take a long path back to the
    word “house” at the beginning of the sentence. That’s quite a path for a machine!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要解释“它”的意思，只有在回到句子开头的“房子”一词时才能理解。这对于机器来说是一段相当长的路径！
- en: 'The order of function that defines the maximum path length can be summed up
    as shown in *Table 7.2* in Big *O* notation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 定义最大路径长度的函数顺序如 Big *O* 表 7.2 中所示：
- en: '| **Layer Type** | **Maximum Path Length** | **Context Size** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **层类型** | **最大路径长度** | **上下文大小** |'
- en: '| Self-Attention | 0(1) | 1 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 自注意力 | 0(1) | 1 |'
- en: '| Recurrent | 0(n) | 100 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 循环 | 0(n) | 100 |'
- en: 'Table 7.2: Maximum path length'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.2：最大路径长度
- en: '*Vaswani* et al. (2017) optimized the design of context analysis in the original
    Transformer model. Attention brings the operations down to a one-to-one token
    operation. The fact that all of the layers are identical makes it much easier
    to scale up the size of transformer models. A GPT-3 model with a size 100 context
    window has the same maximum length path as a size 10 context window.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*Vaswani* 等人（2017）对原始 Transformer 模型中的上下文分析设计进行了优化。自注意力将操作缩减为一对一的标记操作。所有层都相同，这使得扩大
    transformer 模型的规模变得更加容易。具有 100 个上下文窗口大小的 GPT-3 模型具有与大小为 10 的上下文窗口大小相同的最大长度路径。'
- en: For example, a recurrent layer in an RNN has to store the total length of the
    context step by step. The maximum path length is the context size. The maximum
    length size for an RNN that would process the context size of a GPT-3 model would
    be 0(n) times longer. Furthermore, an RNN cannot split the context into 96 heads
    running on a parallelized machine architecture, distributing the operations over
    96 GPUs, for example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，RNN 中的一个循环层必须逐步存储整个上下文的长度。最大路径长度就是上下文大小。处理 GPT-3 模型的上下文大小的 RNN 的最大长度大小将是
    0(n) 倍长。此外，RNN 无法将上下文分割成 96 个在并行化机器架构上运行的头，例如将操作分布在 96 个 GPU 上。
- en: 'The flexible and optimized architecture of transformers has led to an impact
    on several other factors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 的灵活和优化的架构对其他几个因素产生了影响：
- en: '*Vaswani* et al. (2017) trained a state-of-the-art transformer model with 36M
    sentences. *Brown* et al. (2020) trained a GPT-3 model with 400 billion byte-pair-encoded
    tokens extracted from Common Crawl data.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vaswani* 等人（2017）训练了一个包含 36M 个句子的最新 transformer 模型。*Brown* 等人（2020）则从 Common
    Crawl 数据中提取了 4000 亿字节对编码的标记来训练 GPT-3 模型。'
- en: Training large transformer models requires machine power that is only available
    to a small number of teams in the world. It took a total of 2.14*10^(23) FLOPS
    for *Brown* et al. (2020) to train GPT-3 175B.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练大型 transformer 模型需要世界上一小部分团队才拥有的计算机实力。*Brown* 等人（2020）训练 GPT-3 175B 需要总共 2.14*10^(23)
    FLOPS。
- en: Designing the architecture of transformers requires highly qualified teams that
    can only be funded by a small number of organizations in the world.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计转换器的架构需要高素质的团队，这些团队只能由世界上少数机构资助。
- en: The size and architecture will continue to evolve and probably increase to trillion-parameter
    models in the near future. Supercomputers will continue to provide the necessary
    resources to train transformers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 大小和架构将继续发展，可能在不久的将来增加到万亿参数模型。超级计算机将继续提供必要的资源来训练转换器。
- en: We will now see how zero-shot models were achieved.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到零-shot模型是如何实现的。
- en: From fine-tuning to zero-shot models
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从微调到零-shot模型
- en: From the start, OpenAI’s research teams, led by *Radford* et al. (2018), wanted
    to take transformers from trained models to GPT models. The goal was to train
    transformers on unlabeled data. Letting attention layers learn a language from
    unsupervised data was a smart move. Instead of teaching transformers to do specific
    NLP tasks, OpenAI decided to train transformers to learn a language.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始，由*Radford*等人(2018年)带领的OpenAI研究团队就希望把转换器从经过训练的模型转变为GPT模型。其目标是在未标记数据上训练转换器。让注意层从无监督数据中学习语言是一个聪明的举措。OpenAI决定训练转换器学习语言，而不是教会它们执行特定的NLP任务。
- en: OpenAI wanted to create a task-agnostic model. So they began to train transformer
    models on raw data instead of relying on labeled data by specialists. Labeling
    data is time-consuming and considerably slows down the transformer’s training
    process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI希望创建一个任务无关的模型。因此，他们开始对原始数据进行转换器模型的训练，而不是依赖专家标记的数据。标记数据耗时，并且显著减慢了转换器的训练过程。
- en: The first step was to start with unsupervised training in a transformer model.
    Then, they would only fine-tune the model’s supervised learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从一个转换器模型的无监督训练开始。然后，他们只会对模型进行监督学习的微调。
- en: OpenAI opted for a decoder-only transformer described in the Stacking decoder
    layers section. The metrics of the results were convincing and quickly reached
    the level of the best NLP models of fellow NLP research labs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI选择了在“叠加解码层”部分描述的只解码转换器。结果的度量是令人信服的，并很快达到了同行NLP研究实验室最佳NLP模型水平。
- en: 'The promising results of the first version of GPT transformer models soon led
    *Radford* et al. (2019) to come up with zero-shot transfer models. The core of
    their philosophy was to continue training GPT models to learn from raw text. They
    then took their research a step further, focusing on language modeling through
    examples of unsupervised distributions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个版本的GPT转换器模型的有希望的结果很快就促使*Radford*等人(2019年)提出了零-shot转移模型。他们的核心哲学是继续训练GPT模型从原始文本中学习。然后，他们进一步深入研究，聚焦于通过无监督分布的语言建模示例：
- en: Examples=(*x*[1], *x*[2], *x*[3], ,*x*[n])
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 示例=(*x*[1], *x*[2], *x*[3], ,*x*[n])
- en: 'The examples are composed of sequences of symbols:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 示例由符号序列组成：
- en: Sequences=(*s*[1], *s*[2], *s*[3], ,*s*[n])
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 序列=(*s*[1], *s*[2], *s*[3], ,*s*[n])
- en: 'This led to a metamodel that can be expressed as a probability distribution
    for any type of input:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个可以表示为任何类型输入的概率分布的元模型：
- en: '*p* (*output*/*input*)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* (*输出*/*输入*)'
- en: The goal was to generalize this concept to any type of downstream task once
    the trained GPT model understands a language through intensive training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是一旦训练的GPT模型通过深入训练理解了语言，就将这个概念推广到任何类型的下游任务。
- en: The GPT models rapidly evolved from 117M parameters to 345M parameters, to other
    sizes, and then to 1,542M parameters. 1,000,000,000+ parameter transformers were
    born. The amount of fine-tuning was sharply reduced. The results reached state-of-the-art
    metrics again.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型从117M参数迅速发展到345M参数，然后到其他大小，然后到1,542M参数。10亿+参数的转换器诞生了。微调量急剧减少。结果再次达到了最先进的指标。
- en: 'This encouraged OpenAI to go further, much further. *Brown* et al. (2020) went
    on the assumption that conditional probability transformer models could be trained
    in-depth and were able to produce excellent results with little to no fine-tuning
    for downstream tasks:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励OpenAI更进一步，更远。*Brown*等人(2020年)假设有条件概率转换器模型可以进行深度训练，并且能够在几乎不进行进一步微调的情况下产生出色的结果：
- en: '*p* (*output*/*multi-tasks*)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* (*输出*/*多任务*)'
- en: 'OpenAI was reaching its goal of training a model and then running downstream
    tasks directly without further fine-tuning. This phenomenal progress can be described
    in four phases:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI正在实现其目标，即训练模型，然后直接运行下游任务，而无需进一步微调。这一巨大的进步可以分为四个阶段：
- en: '**Fine-Tuning** (**FT**) is meant to be performed in the sense we have been
    exploring in previous chapters. A transformer model is trained and then fine-tuned
    on downstream tasks. *Radford* et al. (2018) designed many fine-tuning tasks.
    The OpenAI team then reduced the number of tasks progressively to `0` in the following
    steps.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**（**FT**）是指按照我们在之前章节探讨过的方式进行。一个变压器模型被训练，然后在下游任务上进行微调。*Radford*等人（2018年）设计了许多微调任务。然后，OpenAI团队在接下来的步骤中逐渐将任务数减少到`0`。'
- en: '**Few-Shot** (**FS**) represents a huge step forward. The GPT is trained. When
    the model needs to make inferences, it is presented with demonstrations of the
    task to perform as conditioning. Conditioning replaces weight updating, which
    the GPT team excluded from the process. We will be applying conditioning to our
    model through the context we provide to obtain text completion in the notebooks
    we will go through in this chapter.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本**（**FS**）代表了一个巨大的进步。GPT已经训练好了。当模型需要进行推理时，它会以任务的演示为条件。条件替代了权重更新，GPT团队在过程中排除了这一步骤。我们将通过我们在本章中将要进行的笔记本中提供的上下文来对我们的模型进行条件设定，以获得文本完成。'
- en: '**One-Shot** (**1S**) takes the process further. The trained GPT model is presented
    with only one demonstration of the downstream task to perform. No weight updating
    is permitted either.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单样本**（**1S**）将进程推至更远。训练后的GPT模型仅需要一个下游任务的演示来表现。同时也不允许进行权重更新。'
- en: '**Zero-Shot** (**ZS**) is the ultimate goal. The trained GPT model is presented
    with no demonstration of the downstream task to perform.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零样本**（**ZS**）是最终目标。训练后的GPT模型在没有演示所需下游任务的情况下进行表现。'
- en: Each of these approaches has various levels of efficiency. The OpenAI GPT team
    has worked hard to produce these state-of-the-art transformer models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法各自具有不同的效率水平。OpenAI GPT团队已经努力开发出这些最先进的变压器模型。
- en: 'We can now explain the motivations that led to the architecture of the GPT
    models:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以解释导致GPT模型架构的动机：
- en: Teaching transformer models how to learn a language through extensive training.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教授变压器模型如何通过广泛的培训学习语言。
- en: Focusing on language modeling through context conditioning.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过上下文条件对语言建模进行重点关注。
- en: The transformer takes the context and generates text completion in a novel way.
    Instead of consuming resources on learning downstream tasks, it works on understanding
    the input and making inferences no matter what the task is.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器以一种新颖的方式接受上下文并生成文本完成。它不会浪费资源学习下游任务，而是致力于理解输入并进行推理，无论任务是什么。
- en: Finding efficient ways to train models by masking portions of the input sequences
    forces the transformer to think with machine intelligence. Thus, machine intelligence,
    though not human, is efficient.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到训练模型的高效方法，通过掩盖输入序列的部分来迫使变压器以机器智能的方式进行思考。因此，尽管不是人类，但机器智能是高效的。
- en: We understand the motivations that led to the architecture of GPT models. Let’s
    now have a look at the decoder-layer-only GPT model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到了导致GPT模型架构的动机。现在让我们来看看仅解码器层的GPT模型。
- en: Stacking decoder layers
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠解码器层
- en: We now understand that the OpenAI team focused on language modeling. Therefore,
    it makes sense to keep the masked attention sublayer. Hence, the choice to retain
    the decoder stacks and exclude the encoder stacks. *Brown* et al. (2020) dramatically
    increased the size of the decoder-only transformer models to get excellent results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们明白了OpenAI团队专注于语言建模。因此，保留掩盖注意力子层是有意义的。因此，选择保留解码器堆叠并排除编码器堆叠的选择。*Brown*等人（2020年）显著增加了仅解码器变压器模型的规模，取得了出色的结果。
- en: GPT models have the same structure as the decoder stacks of the original Transformer
    designed by *Vaswani* et al. (2017). We described the decoder stacks in *Chapter
    2*, *Getting Started with the Architecture of the Transformer Model*. If necessary,
    take a few minutes to go back through the architecture of the original Transformer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型的结构与*Vaswani*等人（2017年）设计的原始变压器的解码器堆栈相同。我们在*第 2 章*中描述了解码器的堆栈，*开始使用变压器模型的架构*。如果需要的话，请花几分钟回顾一下原始变压器的架构。
- en: 'The GPT model has a decoder-only architecture, as shown in *Figure 7.1*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型具有仅解码器的架构，如*图 7.1*所示：
- en: '![](img/B17948_07_01.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_07_01.png)'
- en: 'Figure 7.1: GPT decoder-only architecture'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：GPT 仅解码器架构
- en: We can recognize the text and position embedding sub-layer, the masked multi-head
    self-attention layer, the normalization sub-layers, the feedforward sub-layer,
    and the outputs. In addition, there is a version of GPT-2 with both text prediction
    and task classification.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以识别文本和位置嵌入子层、蒙版多头自注意层、规范化子层、前馈子层和输出。此外，还有一种同时进行文本预测和任务分类的 GPT-2 版本。
- en: The OpenAI team customized and tweaked the decoder model by model. *Radford*
    et al. (2019) presented no fewer than four GPT models, and *Brown* et al. (2020)
    described no fewer than eight models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 团队通过模型定制和调整了解码模型。*Radford* 等人（2019）给出了至少四种 GPT 模型，*Brown* 等人（2020）描述了至少八种模型。
- en: 'The GPT-3 175B model has reached a unique size that requires computer resources
    that few teams in the world can access:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 175B 模型已经达到了很少团队可以访问的独特规模，需要大量的计算资源：
- en: '*n*[params] = 175.0B, *n*[layers] = 96, *d*[model] = 12288, *n*[heads] = 96'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[参数] = 175.0B，*n*[层数] = 96，*d*[模型] = 12288，*n*[头] = 96'
- en: Let’s look into the growing number of GPT-3 engines.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看越来越多的 GPT-3 引擎。
- en: GPT-3 engines
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3 引擎
- en: 'A GPT-3 model can be trained to accomplish specific tasks of different sizes.
    The list of engines available at this time is documented by OpenAI: [https://beta.openai.com/docs/engines](https://beta.openai.com/docs/engines)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型可以被训练来完成不同规模的特定任务。目前 OpenAI 记录了可用的引擎列表：[https://beta.openai.com/docs/engines](https://beta.openai.com/docs/engines)
- en: 'The base series of engines have different functions – for example:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基本系列的引擎具有不同的功能 - 例如：
- en: The Davinci engine can analyze complex intent
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达芬奇引擎能够分析复杂意图
- en: The Curie engine is fast and has good summarization
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 居里引擎速度快，具有很好的摘要能力
- en: The Babbage engine is good at semantic search
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴比奇引擎擅长语义检索
- en: The Ada engine is good at parsing text
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿达引擎擅长解析文本
- en: 'OpenAI is producing more engines to put on the market:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 正在生产更多的引擎投放市场：
- en: The Instruct series provides instructions based on a description. An example
    is available in the *More GPT-3 examples* section of this chapter.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Instruct 系列根据描述提供指令。此章节的 *更多 GPT-3 示例* 中有一个例子可供参考。
- en: The Codex series can translate language to code. We will explore this series
    in *Chapter 16*, *The Emergence of Transformer-Driven Copilots*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codex 系列可以将语言翻译成代码。我们将在 *第 16 章* *Transformer 驱动合作伙伴的出现* 中探索这个系列。
- en: The Content filter series filters unsafe or sensitive text. We will explore
    this series in *Chapter 16*, *The Emergence of Transformer-Driven Copilots*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容过滤系列可以过滤不安全或敏感的文本。我们将在 *第 16 章* *Transformer 驱动合作伙伴的出现* 中探索这个系列。
- en: We have explored the process that led us from fine-tuning to zero-shot GPT-3
    models. We have seen that GPT-3 can produce a wide range of engines.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了从微调到零样本 GPT-3 模型的过程。我们已经看到 GPT-3 可以生成各种引擎。
- en: It is now time to see how the source code of GPT models is built. Although the
    GPT-3 transformer model source code is not publicly available at this time, GPT-2
    models are sufficiently powerful to understand the inner workings of GPT models.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看 GPT 模型的源代码是如何构建的。尽管目前 GPT-3 变压器模型的源代码并不公开，但 GPT-2 模型已经足够强大，可以理解 GPT
    模型的内部工作原理。
- en: We are ready to interact with a GPT-2 model and train it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好与 GPT-2 模型进行交互和训练。
- en: We will first use a trained GPT-2 345M model for text completion with 24 decoder
    layers with self-attention sublayers of 16 heads.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将使用训练好的 GPT-2 345M 模型进行文本完成，它包括 24 个解码层和 16 个自注意子层的自注意。
- en: We will then train a GPT-2 117M model for customized text completion with 12
    decoder layers with self-attention layers of 12 heads.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对 12 个解码层和 12 个自注意层的 GPT-2 117M 模型进行定制文本完成训练。
- en: Let’s start by interacting with a pretrained 345M parameter GPT-2 model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先与一个预训练的 345M 参数 GPT-2 模型进行交互。
- en: Generic text completion with GPT-2
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-2 进行通用文本完成
- en: We will explore an example with a GPT-2 generic model from top to bottom. *The
    goal of the example we will run is to determine the level of abstract reasoning
    a GPT model can attain*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从上到下探讨使用 GPT-2 通用模型的例子。*我们要运行的例子的目标是确定 GPT 模型能达到的抽象推理水平*。
- en: This section describes the interaction with a GPT-2 model for text completion.
    We will focus on *Step 9* of the `OpenAI_GPT_2.ipynb` notebook described in detail
    in *Appendix III*, *Generic Text Completion with GPT-2*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了与 GPT-2 模型进行文本完成的交互。我们将重点关注 *OpenAI_GPT_2.ipynb* 中详细描述的 *附录 III* 中的 *使用
    GPT-2 进行通用文本完成的第 9 步*。
- en: You can first read this section to see how the generic pretrained GPT-2 model
    will react to a specific example. Then read *Appendix III*, *Generic Text Completion
    with GPT-2*, to go into the details of how a generic GPT-2 model is implemented
    in a Google Colab notebook.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以先阅读本节，看看通用预训练的 GPT-2 模型对特定示例的反应如何。然后阅读 *附录 III*，*使用 GPT-2 进行通用文本补全*，以深入了解通用
    GPT-2 模型如何在 Google Colab 笔记本中实现。
- en: You can also read *Appendix III* directly, which contains the interaction of
    *Step 9* described below.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以直接阅读 *附录 III*，其中包含下面描述的 *Step 9* 的交互。
- en: First, let’s understand the specific example of the pretrained GPT-2 being applied.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解应用预训练 GPT-2 的具体示例。
- en: 'Step 9: Interacting with GPT-2'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9步：与 GPT-2 交互
- en: In this section, we will interact with the GPT-2 345M model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将与 GPT-2 345M 模型进行交互。
- en: 'To interact with the model, run the `interact_model` cell:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要与模型交互，请运行 `interact_model` 单元格：
- en: '[PRE0]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will be prompted to enter some context:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你将被提示输入一些上下文：
- en: '![](img/B17948_07_02.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_07_02.png)'
- en: 'Figure 7.2: Context input for text completion'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：文本补全的上下文输入
- en: You can try any type of context you wish since this is a standard GPT-2 model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试任何类型的上下文，因为这是一个标准的 GPT-2 模型。
- en: 'We can try a sentence written by Immanuel Kant:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试一句由康德写的句子：
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Press *Enter* to generate text. The output will be relatively random since the
    GPT-2 model was not trained on our dataset, and we are running a stochastic model
    anyway.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 按下 *Enter* 生成文本。由于 GPT-2 模型未在我们的数据集上进行训练，并且我们无论如何都在运行随机模型，因此输出将相对随机。
- en: 'Let’s have a look at the first few lines the GPT model generated at the time
    I ran it:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我运行时 GPT 模型生成的前几行：
- en: '[PRE2]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To stop the cell, double-click on the run button of the cell.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止单元格，请双击单元格的运行按钮。
- en: You can also press *Ctrl* + *M* to stop generating text, but it may transform
    the code into text, and you will have to copy it back into a program cell.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以按下 *Ctrl* + *M* 停止生成文本，但这可能会将代码转换为文本，你将不得不将其复制回程序单元格。
- en: 'The output is rich. We can observe several facts:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出丰富。我们可以观察到几个事实：
- en: The context we entered *conditioned* the output generated by the model.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们输入的上下文 *条件了* 模型生成的输出。
- en: The context was a demonstration of the model. It learned what to say from the
    context without modifying its parameters.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文是模型的演示。它从上下文中学习要说什么，而不修改其参数。
- en: Text completion is conditioned by context. This opens the door to transformer
    models that do not require fine-tuning.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本补全受上下文条件限制。这为不需要微调的转换器模型打开了大门。
- en: From a semantic perspective, the output could be more interesting.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语义角度来看，输出可能更有趣。
- en: From a grammatical perspective, the output is convincing.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语法角度来看，输出是令人信服的。
- en: Can we do better? The following section presents the interaction of custom text
    completion.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？下一节介绍了自定义文本补全的交互。
- en: Training a custom GPT-2 language model
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练自定义 GPT-2 语言模型
- en: We will continue our top-to-bottom approach in this section by exploring an
    example with a GPT-2 custom model that we will train on a specific dataset. *The
    goal remains to determine the level of abstract reasoning a GPT model can attain*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将继续采用自上而下的方法，探讨一个在特定数据集上训练的 GPT-2 自定义模型的示例。*目标仍然是确定 GPT 模型可以达到的抽象推理水平*。
- en: This section describes the interaction with a GPT-2 model for text completion
    trained on a specific dataset. We will focus on *Step 12* of the `Training_OpenAI_GPT_2.ipynb`
    notebook described in detail in *Appendix IV*, *Custom Text Completion with GPT-2*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了与特定数据集训练的 GPT-2 模型进行文本补全的交互。我们将重点放在详细描述在 *附录 IV* 中的 *Training_OpenAI_GPT_2.ipynb*
    笔记本中的 *Step 12*，即 *自定义文本补全与 GPT-2*。
- en: You can read this section first to see how an example with a custom GPT-2 model
    will improve responses. Then read *Appendix IV*, *Custom Text Completion with
    GPT-2*, to understand how to train a GPT-2 to obtain specific responses.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以先阅读本节，看看带有自定义 GPT-2 模型的示例将如何改进响应。然后阅读 *附录 IV*，*自定义文本补全与 GPT-2*，以了解如何训练 GPT-2
    来获得特定的响应。
- en: You can also decide to read *Appendix IV* directly, which also contains the
    interaction of *Step 12* described below.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以决定直接阅读 *附录 IV*，其中也包含下面描述的 *Step 12* 的交互。
- en: First, let’s understand how the interaction with GPT-2 improved by training
    it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解通过训练 GPT-2 改进的交互方式。
- en: 'Step 12: Interactive context and completion examples'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12步：交互式上下文和补全示例
- en: We will now run a conditional sample. The context we enter will condition the
    model to think as we want it to, to complete the text by generating tailor-made
    paragraphs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将运行一个有条件的样本。我们输入的上下文将导致模型按照我们想要的方式思考，通过生成定制的段落来完成文本。
- en: 'Run the cell and explore the magic:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 运行单元格并探索魔法：
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If necessary, take a few minutes to go back to *Step 9*, *Interacting with
    GPT-2* of *Appendix III*, *Generic Text Completion with GPT-2*, to see the differences
    in the responses. The program prompts us to enter the context:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以花几分钟回到*附录III*的*第9步*，*与GPT-2交互*，以查看响应的差异。该程序提示我们输入上下文：
- en: '![](img/B17948_07_03.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_07_03.png)'
- en: 'Figure 7.3: Context input for text completion'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：文本补全的上下文输入
- en: 'Let’s enter the same paragraph written by Immanuel Kant as we did in *Step
    9* of the *Generic* *text completion with GPT-2* section of this chapter:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们输入与易卢泊儿·康德所写的相同段落，就像我们在本章的*通用* *GPT-2的文本补全*部分的*第9步*中所做的那样：
- en: '[PRE4]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Press *Enter* to generate text as we did previously. Though structured and logical,
    the outputs might change from one run to another, making transformers attractive.
    This time, the result is not random and is impressive.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 按*Enter*生成文本，就像我们之前所做的那样。尽管结构化和逻辑，但输出可能会因为每次运行而有所不同，这使得变压器很有吸引力。这一次，结果并不是随机的，而是令人印象深刻的。
- en: 'Let’s look at the first few lines the GPT-2 model produced:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看GPT-2模型生成的前几行：
- en: '[PRE5]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To stop the cell, double-click on the run button of the cell or press *Ctrl*
    + *M*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止运行单元格，请双击单元格的运行按钮或按下*Ctrl* + *M*。
- en: Wow! I doubt anybody could see the difference between the text completion produced
    by our trained GPT-2 model and a human. It might also generate different outputs
    at each run.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！我怀疑有人能看出我们训练过的GPT-2模型生成的文本补全和人类的区别。它也可能在每次运行时生成不同的输出。
- en: In fact, I think our model could outperform many humans in this abstract exercise
    in philosophy, reason, and logic!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我认为我们的模型在哲学、推理和逻辑的抽象练习中，可能会超越许多人类！
- en: 'We can draw some conclusions from our experiment:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从我们的实验中得出一些结论：
- en: A well-trained transformer model can produce text completion at a human level
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过良好训练的变压器模型能以人类水平产生文本补全
- en: A GPT-2 model can almost reach human level in text generation on complex and
    abstract reasoning
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在复杂和抽象推理中，GPT-2模型几乎可以达到人类水平的文本生成能力
- en: Text context is an efficient way of conditioning a model by demonstrating what
    is expected
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本背景是通过演示预期结果的一种有效的方式来给模型制定条件
- en: Text completion is text generation based on text conditioning if context sentences
    are provided
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本补全是基于文本条件的文本生成，如果提供了上下文的句子。
- en: You can enter conditioning text context examples to experiment with text completion.
    You can also train the model on your own data. Just replace the content of the
    `dset.txt` file with your own data and see what happens!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以输入条件文本背景示例来尝试文本补全。您还可以使用自己的数据来训练模型。只需用自己的数据替换`dset.txt`文件的内容，看看会发生什么！
- en: Remember that our trained GPT-2 model will react like a human. If you enter
    a short, incomplete, uninteresting, or tricky context, you will obtain puzzled
    or bad results. This is because GPT-2 expects the best out of us, as in real life!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住我们训练过的GPT-2模型会像人类一样反应。如果您输入一个简短、不完整、无趣或棘手的上下文，您将得到困惑或糟糕的结果。这是因为GPT-2希望我们做到最好，正如现实生活中一样！
- en: Let’s go to the GPT-3 playground to see how a trained GPT-3 reacts to the example
    tested with GPT-2.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去GPT-3 playground看看训练过的GPT-3对使用GPT-2测试的示例做出何种反应。
- en: Running OpenAI GPT-3 tasks
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行OpenAI GPT-3任务
- en: 'In this section, we will run GPT-3 in two different ways:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以两种不同的方式运行GPT-3：
- en: We will first run the GPT-3 tasks online with no code
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将首先在线运行GPT-3任务而不需要任何代码
- en: We will then implement GPT-3 in Google Colab notebook
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将在Google Colab笔记本中实现GPT-3
- en: We will be using GPT-3 engines in this book. When you sign up for the GPT-3
    API, OpenAI gives you a free budget to get started. This free budget should cover
    most of the cost, if not all of the cost, of running the examples in this book
    once or twice.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中我们将使用GPT-3引擎。当您注册GPT-3 API时，OpenAI会提供您一个免费的预算来开始使用。这个免费预算应该足以覆盖这本书中示例的大部分成本，如果不是全部。
- en: Let’s begin by running NLP tasks online.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从在线运行NLP任务开始。
- en: Running NLP tasks online
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线运行NLP任务
- en: We will now go through some Industry 4.0 examples without an API, directly asking
    GPT-3 to do something for us.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将不使用API直接向GPT-3提出要求，让我们来看看一些工业4.0的例子。
- en: 'Let us define a standard structure of a prompt and response as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们定义一种提示和回应的标准结构，如下:'
- en: '*N* = name of the NLP task (INPUT).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* = NLP 任务的名称（输入）。'
- en: '*E* = explanation for the GPT-3 engine. *E* precedes *T* (INPUT).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E* = GPT-3 引擎的解释。*E* 在 *T* 之前（输入）。'
- en: '*T* = the text or content we wish GPT-3 to look into (INPUT).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T* = 我们希望 GPT-3 查看的文本或内容（输入）。'
- en: '*S* = showing GPT-3 what is expected. *S* follows *T* and is added when necessary
    (INPUT).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* = 显示给 GPT-3 期望的内容。*S* 在必要时跟随 *T* 并添加（输入）。'
- en: '*R* = GPT-3’s response (OUTPUT).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R* = GPT-3 的回应（输出）。'
- en: The structure of the prompt described above is a guideline. However, GPT-3 is
    very flexible, and many variations are possible.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提示的结构是一个指南。然而，GPT-3 非常灵活，有许多变体可行。
- en: 'We are now ready to run some educational examples online with no API:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备在线运行一些教育示例，无需 API：
- en: 'Questions and answers (**Q&A**) on existing knowledge:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有知识的问题与答案（**Q&A**）：
- en: '*E* = `Q`'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E* = `Q`'
- en: '*T* = `Who was the president of the United States in 1965?`'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*T* = `1965年美国总统是谁？`'
- en: '*S* = None'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = 无'
- en: '*R* = `A`'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R* = `A`'
- en: 'Prompts and responses:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示和回答：
- en: '`Q: Who was the president of the United States in 1965?`'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Q: 1965年美国总统是谁？`'
- en: '`A: Lyndon B. Johnson was president of the United States in 1965.`'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`A: 1965年林登·约翰逊是美国总统。`'
- en: '`Q: Who was the first human on the moon?`'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Q: 谁是第一个登上月球的人类？`'
- en: '`A: Neil Armstrong was the first human on the moon.`'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`A: 尼尔·阿姆斯特朗是第一个登上月球的人类。`'
- en: '**Movie to Emoji**:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电影到表情符号**：'
- en: '*E* = Some examples of movie titles'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E* = 一些电影标题的示例'
- en: '*T* = None'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*T* = 无'
- en: '*S* = Implicit through examples'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = 通过例子隐含'
- en: '*R* = Some examples of emojis'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R* = 一些表情符号的示例'
- en: 'Prompts and responses:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示和回答：
- en: '![A picture containing text  Description automatically generated](img/B17948_07_08.png)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![包含文本的图片的图片描述 自动生成](img/B17948_07_08.png)'
- en: 'A new prompt and response:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新的提示和回答：
- en: '![](img/B17948_07_09.png)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17948_07_09.png)'
- en: 'Summarizing for a second grader (**Summarize for a 2nd grader**):'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给二年级学生总结（**给二年级学生总结**）：
- en: '*E* = `My second grader asked me what this passage means:`'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E* = `我的二年级孩子问我这段话是什么意思：`'
- en: '*T* = `"""The initial conclusions…."""`'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*T* = `"""初始结论……。"""`'
- en: '*S* = `I rephrased it for him, in plain language a second grader can understand:
    """`'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = `我用通俗易懂的语言给他解释了一下："""`'
- en: '*R* = The summary'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R* = 摘要'
- en: 'Prompt and response:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示和回答：
- en: '`My second grader asked me what this passage means:`'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我的二年级孩子问我这段话是什么意思：`'
- en: '`""" The initial conclusions can be divided into two categories: facts and
    fiction. The facts are that OpenAI has one of the most powerful NLP services in
    the world. The main facts are: OpenAI engines are powerful zero-shot that require
    no hunting for all kinds of transformer models, no pre-training, and no fine-tuning.
    The supercomputers used to train the models are unique. If the prompt is well-designed,
    we obtain surprisingly accurate responses. Implementing the NLP tasks in this
    section required a copy and paste action that any software beginner can perform.
    Fiction begins with dystopian and hype assertions AI will replace data scientists
    and AI specialists. Is that true? Before answering that question, first ask yourself
    the following questions about the example we just ran: How do we know the sentence
    was incorrect in the first place? How do we know the answer is correct without
    us humans reading and confirming this? How did the engine know it was a grammar
    correction task? If the response is incorrect, how can we understand what happened
    to help improve the prompt or revert to manual mode in a well-designed human interface?
    The truth is that humans will need to intervene to answers these questions manually,
    with rule-bases, quality control automated pipelines, and many other tools. The
    facts are convincing. It is true that running an NLP task requires little development.
    The fiction is not convincing. Humans are still required. OpenAI engines are not
    there to replace humans but to help them perform more high-level gratifying tasks.
    You can now fly a jet without having to build it! """`'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`"""初始结论可以分为两类：事实和虚构。事实是 OpenAI 拥有世界上最强大的自然语言处理服务之一。主要事实是：OpenAI 引擎是强大的零-shot，无需寻找各种变压器模型，无需预训练和微调。用于训练模型的超级计算机是独一无二的。如果提示设计得当，我们会得到令人惊讶的准确响应。在本节中实施
    NLP 任务需要执行复制粘贴操作，任何软件初学者都可以执行。虚构从反乌托邦和炒作的断言开始，AI 将取代数据科学家和 AI 专家。这是真的吗？在回答这个问题之前，首先问问自己关于我们刚刚运行的示例的以下问题：我们如何知道句子起初是错误的？如果没有我们人类读取和确认，我们如何知道答案是正确的？引擎是如何知道这是一个语法校正任务的？如果响应不正确，我们如何理解发生了什么以帮助改进提示或在设计良好的人机界面中恢复手动模式？事实是人类将需要手动干预来回答这些问题，使用基于规则的方法、质量控制自动流水线以及许多其他工具。事实是令人信服的。运行
    NLP 任务确实需要很少的开发。虚构不令人信服。人类仍然是必需的。OpenAI 引擎并不是为了取代人类，而是为了帮助他们执行更多高水平的令人满足的任务。现在你可以驾驶喷气机而不必自己建造它！"""`'
- en: '`I rephrased it for him, in plain language a second grader can understand:
    """`'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我为他重新表述了一下，用小学二年级孩子都能理解的简单语言："""`'
- en: '`The first conclusion is that OpenAI has one of the most powerful NLP services
    in the world. The most important facts are: OpenAI engines are very powerful zero-shot
    that require no hunting for all kinds of transformer models, no pre-training,
    and no fine-tuning. The supercomputers used to train the models are unique. If
    the prompt is well-designed, we get surprisingly accurate responses.`'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`OpenAI 的第一个结论是，它拥有世界上最强大的自然语言处理服务之一。最重要的事实是：OpenAI 引擎非常强大，零-shot，无需寻找各种变压器模型，无需预训练和微调。用于训练模型的超级计算机是独一无二的。如果提示设计得当，我们会得到令人惊讶的准确响应。`'
- en: The results are impressive and a huge step forward in the history of NLP.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人印象深刻，是自然语言处理历史上的一大进步。
- en: The number of NLP tasks we can implement with the zero-shot GPT-3 engines provided
    by OpenAI is endless since we control how we show what we expect of the transformer
    models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用由 OpenAI 提供的零-shot GPT-3 引擎实现的 NLP 任务的数量是无穷无尽的，因为我们控制着如何展示我们对变压器模型的期望。
- en: However, a true Industry 4.0 AI guru must get their hands dirty before implementing
    ready-to-use APIs. We will now explore the architecture of OpenAI GPT models and
    then build GPT-2 models to see how these engines work.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，真正的行业 4.0 人工智能专家必须在使用即用型 API 之前先动手。我们现在将探索 OpenAI GPT 模型的架构，然后构建 GPT-2 模型，看看这些引擎是如何工作的。
- en: The more we know about GPT models, the better an Industry 4.0 NLP expert can
    implement them in real-life projects.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 GPT 模型了解得越多，行业 4.0 的自然语言处理专家就越能将它们应用到现实项目中。
- en: Let’s continue our top-to-bottom approach and drill down into the architecture
    of OpenAI GPT transformer models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续我们的自上而下的方法，深入研究 OpenAI GPT 变压器模型的架构。
- en: Getting started with GPT-3 engines
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GPT-3 引擎入门
- en: OpenAI has some of the most powerful transformer engines in the world. One GPT-3
    model can perform hundreds of tasks. GPT-3 can do many tasks it wasn’t trained
    for.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 拥有世界上一些最强大的变压器引擎。一个 GPT-3 模型可以执行数百个任务。GPT-3 可以完成许多它没有接受训练的任务。
- en: This section will use the API in `Getting_Started_GPT_3.ipynb`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将在`Getting_Started_GPT_3.ipynb`中使用API。
- en: To use a GPT-3, you must first go to OpenAI’s website, [https://openai.com/](https://openai.com/),
    and sign up.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用GPT-3，您必须首先转到OpenAI的网站[https://openai.com/](https://openai.com/)并注册。
- en: OpenAI has a playground for everybody to try, just like Google Translate or
    any user-friendly online service. So, let’s try some tasks.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI为每个人提供了一个试验场，就像Google翻译或任何用户友好型在线服务一样。因此，让我们尝试一些任务。
- en: Running our first NLP task with GPT-3
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行我们的第一个NLP任务与GPT-3
- en: Let’s start using GPT-3 in a few steps.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从几个步骤开始使用GPT-3。
- en: Go to Google Colab and open `Getting_Started_GPT_3.ipynb`, which is the chapter
    directory of the book on GitHub.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 转到Google Colab并打开GitHub上书籍的章节目录`Getting_Started_GPT_3.ipynb`。
- en: You do not need to change the settings of the notebook. We are using an API,
    so we will not need much local computing power for the tasks in this section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 您无需更改笔记本的设置。我们正在使用一个API，因此在本节中不需要太多的本地计算能力。
- en: The steps of this section are the same ones as in the notebook.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的步骤与笔记本中的步骤相同。
- en: 'Running an NLP is done in three simple steps:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 运行NLP分为三个简单的步骤：
- en: 'Step 1: Installing OpenAI'
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤1：安装OpenAI
- en: 'Install `openai` using the following command:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令安装`openai`：
- en: '[PRE6]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If `openai` is not installed, you must restart the runtime. A message will
    indicate when to do this, as shown in the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未安装`openai`，则必须重新启动运行时。当需要这样做时，会显示一条消息，如下所示：
- en: '![Text  Description automatically generated](img/B17948_07_10.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本描述](img/B17948_07_10.png)'
- en: Restart the runtime and then run this cell again to make sure `openai` is imported.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动运行时，然后再次运行此单元格，以确保已导入`openai`。
- en: 'Step 2: Entering the API key'
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤2：输入API密钥
- en: 'An API key is given that can be used with Python, C#, Java, and many other
    options. We will be using Python in this section:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个可以在Python、C#、Java和许多其他选项中使用的API密钥。在本节中，我们将使用Python：
- en: '[PRE7]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can now update the next cell with your API key:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以使用您的API密钥更新下一个单元格：
- en: '[PRE8]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s now run an NLP task.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行一个NLP任务。
- en: 'Step 3: Running an NLP task with the default parameters'
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤3：使用默认参数运行NLP任务
- en: 'We copy and paste an OpenAI example for a **grammar correction** task:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们复制并粘贴了一个OpenAI的**语法校正**任务示例：
- en: '[PRE9]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The task is to correct this grammar mistake: `She no went to the market`.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是纠正这个语法错误：`She no went to the market`。
- en: 'We can process the response as we wish by parsing it. OpenAI’s response is
    a dictionary object. The OpenAI object contains detailed information on the task.
    We can ask the object to be displayed:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过解析来按需处理响应。OpenAI的响应是一个字典对象。OpenAI对象包含了有关任务的详细信息。我们可以要求该对象显示：
- en: '[PRE10]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can explore the object:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以探索对象：
- en: '[PRE11]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The “created” number and “id”, and “model” name can vary with each run.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: “创建”的编号和“id”，以及“model”名称在每次运行时都可能会有所不同。
- en: 'We can then ask the object dictionary to display `"text"` and to print the
    processed output:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以要求对象字典显示“`text`”，并打印处理后的输出：
- en: '[PRE12]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of “`text`" in the dictionary is the grammatically correct sentence:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 字典中的“`text`”的输出是语法正确的句子：
- en: '[PRE13]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: NLP tasks and examples
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NLP任务和示例
- en: Now we will cover an industrial approach to GPT-3 engine usage. For example,
    OpenAI provides an interactive educational interface that does not require an
    API. So a school teacher, a consultant, a linguist, a philosopher, or anybody
    that wishes to use a GPT-3 engine for educational purposes can do so with no experience
    at all in AI.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍一种工业化的GPT-3引擎使用方法。例如，OpenAI提供了一个交互式教育界面，不需要API。因此，一个学校教师、顾问、语言学家、哲学家或任何希望将GPT-3引擎用于教育目的的人都可以这样做，而且完全不需要AI方面的经验。
- en: We will first begin by using an API in a notebook.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将在笔记本中使用API。
- en: Grammar correction
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语法校正
- en: If we go back to `Getting_Started_GPT_3.ipynb`, which we began to explore in
    the *Getting started with GPT-3 engines* section of this chapter, we can experiment
    with grammar correction with different prompts.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到`Getting_Started_GPT_3.ipynb`，我们可以在本章的*开始使用GPT-3引擎*部分开始探索的内容中，用不同的提示语试验语法校正。
- en: 'Open the notebook and go to *Step 4: Example 1: Grammar correction*:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 打开笔记本，转到*步骤4：示例1：语法校正*：
- en: '[PRE14]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The request body is not limited to the prompt. The body contains several key
    parameters:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请求体不限于提示。请求体包含几个关键参数：
- en: '`engine="davinci"`. The choice of the OpenAI GPT-3 engine to use and possibly
    other models in the future.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`engine="davinci"`。选择要使用的OpenAI GPT-3引擎，以及将来可能的其他模型。'
- en: '`temperature=0`. A higher value such as `0.9` will force the model to take
    more risks. Do not modify the temperature and `top_p` at the same time.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature=0`。像`0.9`这样的较高值会促使模型更冒险。不要同时修改temperature和`top_p`。'
- en: '`max_tokens=60`. The maximum number of tokens of the response.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_tokens=60`。响应的最大token数。'
- en: '`top_p=1.0`. Another way to control sampling like `temperature`. In this case,
    the `top_p` percentage of tokens of the probability mass will be considered. `0.2`
    would make the system only take 20% of the top probability mass.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p=1.0`。另一种控制抽样的方法，类似于`temperature`。在这种情况下，`top_p`的百分比的token概率质量将被考虑。`0.2`会使系统仅考虑20%的最高概率质量。'
- en: '`frequency_penalty=0.0`. A value between `0` and `1` limits the frequency of
    tokens in a given response.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frequency_penalty=0.0`。值介于`0`和`1`之间，限制了给定响应中token的频率。'
- en: '`presence_penalty=0.0`. A value between `0` and `1` forces the system to use
    new tokens and produce new ideas.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`presence_penalty=0.0`。值介于`0`和`1`之间，强制系统使用新token并产生新想法。'
- en: '`stop=["\n"]`. A signal to the model to stop producing new tokens.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop=["\n"]`。向模型发出停止生成新token的信号。'
- en: 'Some of these parameters are described at the source code level in the *Steps
    7b-8: Importing and defining the model* section of *Appendix III*, *Generic Text
    Completion with GPT-2*.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数中的一些在*附录III*，*使用GPT-2进行通用文本完成*中的*步骤7b-8：导入和定义模型*部分以源代码级别进行描述。
- en: You can play around with these parameters in the GPT-3 model if you gain access
    or in the GPT-2 model in *Appendix III*, *Generic Text Completion with GPT-2*.
    The concepts are the same in both cases.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你获得访问权限，你可以在GPT-3模型中或在*附录III*，*使用GPT-2进行通用文本完成*中的GPT-2模型中玩弄这些参数。在这两种情况下概念是相同的。
- en: 'This section will focus on the prompt:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将专注于提示：
- en: '[PRE15]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The prompt can be divided into three parts:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以分为三个部分：
- en: '`Original`: This signals to the model that what follows is the original text,
    which the model will do something with'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`原始：`这表明模型接下来将处理的是原始文本，模型将对其进行某种处理'
- en: '`She no went to the market.\n`: This part of the prompt shows the model that
    this is the original text'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`她没去市场。\n`：提示的这部分向模型表明这是原始文本'
- en: '`Standard American English`: This shows the model what task is expected'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`标准美式英语`：这向模型显示了期望的任务是什么'
- en: 'Let’s see how far we can get by changing the task:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看通过改变任务我们能达到什么程度：
- en: 'Standard American English produces:'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准美式英语产生：
- en: '`prompt="Original: She no went to the market.\n Standard American English:"`'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`prompt="原始：她没去市场。\n 标准美式英语："`'
- en: 'The text in response is:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 响应中的文本是：
- en: '`"text": " She didn''t go to the market."`'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`"text": " 她没去市场。"`'
- en: That is fine, but what if we do not want a contraction in the sentence?
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那很好，但如果我们不想要句子中的缩写呢？
- en: 'English with no contractions produces:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不带缩写的英语产生：
- en: '`prompt="Original: She no went to the market.\n English with no contractions:"`'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`prompt="原始：她没去市场。\n 不带缩写的英语："`'
- en: 'The text in response is:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 响应中的文本是：
- en: '`"text": " She did not go to the market."`'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`"text": " 她没有去市场。"`'
- en: Wow! This is impressive. Let’s try another language.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哇！这很令人印象深刻。让我们尝试另一种语言。
- en: 'French with no contractions produces:'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有缩写的法语产生：
- en: '`"text": " Elle n''est pas all\u00e9e au march\u00e9."`'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`"text": " 她没有去市场。"`'
- en: This is impressive. `\u00e9` simply needs to be post-processed into `é`.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这很令人印象深刻。`\u00e9`只需进行后处理成`é`。
- en: Many more options are possible. Your Industry 4.0 cross-disciplinary imagination
    is the limit!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多选项可供选择。您的工业4.0跨学科想象力是极限！
- en: More GPT-3 examples
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更多的GPT-3示例
- en: 'OpenAI contains many examples. OpenAI provides an online playground to explore
    tasks. OpenAI also provides source code for each example: [https://beta.openai.com/examples](https://beta.openai.com/examples)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI包含许多示例。OpenAI提供了一个在线平台来探索任务。OpenAI还为每个示例提供了源代码：[https://beta.openai.com/examples](https://beta.openai.com/examples)
- en: 'Just click on an example such as the grammar example we explored in the *Grammar
    correction* section:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 点击一个示例，比如我们在*语法纠错*部分探讨过的语法示例：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_07_04.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 自动生成描述](img/B17948_07_04.png)'
- en: 'Figure 7.4: The Grammar correction section of OpenAI'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：OpenAI的语法纠错部分
- en: OpenAI will describe the prompt and the sample response for each task.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI将为每个任务描述提示和样本响应。
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_07_05.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序 自动生成描述](img/B17948_07_05.png)'
- en: 'Figure 7.5: The sample response corrects the prompt'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：样本响应纠正了提示
- en: 'You can choose to go to the playground and run it online as we did in this
    chapter’s *Running NLP tasks online* section. To do so, click on the **Open in
    Playground** button:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择去游乐场在线运行，就像我们在本章的 *在线运行 NLP 任务* 部分中所做的那样。要这样做，请单击 **在游乐场中打开** 按钮：
- en: '![Graphical user interface, application, chat or text message  Description
    automatically generated](img/B17948_07_06.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、应用程序、聊天或文本消息 自动生成的描述](img/B17948_07_06.png)'
- en: 'Figure 7.6: The Open in Playground button'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：游乐场按钮中的 Open
- en: 'You can choose to copy and paste the code to run the API as we are doing in
    the Google Colab notebook of this chapter:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择复制并粘贴代码来运行 API，就像我们在本章的 Google Colab 笔记本中所做的那样：
- en: '![Text  Description automatically generated](img/B17948_07_07.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![文本 自动生成的描述](img/B17948_07_07.png)'
- en: 'Figure 7.7: Running code using the Davinci engine'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：使用 Davinci 引擎运行代码
- en: '`Getting_Started_GPT_3.ipynb` contains ten examples that you can run to practice
    implementing the OpenAI GPT-3.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '`Getting_Started_GPT_3.ipynb` 包含了十个示例，你可以运行以练习实现 OpenAI GPT-3。'
- en: 'For each example:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个示例：
- en: You can first read the link to the explanation provided by OpenAI. A link to
    the documentation is provided above each cell.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，你可以阅读由 OpenAI 提供的解释链接。每个单元格上方都提供了文档链接。
- en: You can then run the cell to observe GPT-3’s behavior.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以运行单元格以观察 GPT-3 的行为。
- en: 'Run these ten examples in the notebook:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中运行这十个示例：
- en: 'Example 1: Grammar correction'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 1：语法纠正
- en: 'Example 2: English-to-French translation'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 2：英语到法语的翻译
- en: 'Example 3: Instruct series that provides instructions'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 3：提供说明的系列
- en: 'Example 4: Movie to emoji'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 4：电影到表情符号
- en: 'Example 5: Programming language to another language. For example, Python to
    JavaScript. Warning: you may need to obtain special permission from OpenAI to
    run this example, which uses the Davinci Codex engine, the code generator. If
    this example does not run in your notebook, please contact OpenAI to request access
    to Codex.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 5：将编程语言转换为另一种语言。例如，从 Python 到 JavaScript。警告：你可能需要获得 OpenAI 的特别许可才能运行这个示例，该示例使用了
    Davinci Codex 引擎，即代码生成器。如果此示例在你的笔记本中无法运行，请联系 OpenAI 请求 Codex 的访问权限。
- en: 'Example 6: Advanced tweet classifier'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 6：高级推文分类器
- en: 'Example 7: Q&A'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 7：问答
- en: Example 8 Summarize a text
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 8：总结一段文本
- en: 'Example 9: Parse unstructured data'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 9：解析非结构化数据
- en: 'Example 10: Calculate time complexity'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例 10：计算时间复杂度
- en: 'You can run many other tasks on the Examples page: [https://beta.openai.com/examples](https://beta.openai.com/examples)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在示例页面上运行许多其他任务：[https://beta.openai.com/examples](https://beta.openai.com/examples)
- en: Let’s now compare the output of GPT-2 and GPT-3.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们比较一下 GPT-2 和 GPT-3 的输出。
- en: Comparing the output of GPT-2 and GPT-3
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 GPT-2 和 GPT-3 的输出
- en: Our curiosity must be satisfied before we move on. What can the powerful GPT-3
    model produce with the example we submitted to a pretrained GPT-2 model and then
    our custom-trained GPT-2 model?
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们的好奇心必须得到满足。强大的 GPT-3 模型可以使用我们提交给预训练 GPT-2 模型的示例，然后使用我们自定义训练的 GPT-2
    模型产生什么？
- en: 'Our example used for the GPT-2 model:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于 GPT-2 模型的示例：
- en: '[PRE16]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result is mind-blowing! It explains what the text means, including some
    deep philosophical reflections!
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人震惊！它解释了文本的含义，包括一些深刻的哲学思考！
- en: We have proven our point in this chapter. Transformer models can attain to abstract
    reasoning, which can help make micro-decisions in our fast-moving world.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章证明了我们的观点。Transformer 模型可以实现抽象推理，这有助于在我们飞速发展的世界中做出微决策。
- en: OpenAI GPT-3 is a fully trained model. However, GPT-3 can be fine-tuned. Let’s
    see how.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT-3 是一个完全训练好的模型。但是，GPT-3 可以进行微调。我们来看看如何进行微调。
- en: Fine-tuning GPT-3
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 GPT-3 进行微调
- en: This section shows how to fine-tune GPT-3 to learn logic. Transformers need
    to learn logic, inferences, and entailment to understand language at a human level.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了如何对 GPT-3 进行微调以学习逻辑。Transformer 需要学习逻辑、推理和蕴含，以便以人类水平理解语言。
- en: Fine-tuning is the key to making GPT-3 your own application, to customizing
    it to make it fit the needs of your project. It’s a ticket to AI freedom to rid
    your application of bias, teach it things you want it to know, and leave your
    footprint on AI.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是使 GPT-3 成为你自己应用的关键，是将其定制为适应项目需求的钥匙。这是一个赋予你的应用 AI 自由的机会，以消除偏见，教它你想让它知道的东西，并在
    AI 上留下你的足迹。
- en: In this section, GPT-3 will be trained on the works of Immanuel Kant using `kantgpt.csv`.
    We used a similar file to train the BERT-type model in *Chapter 4*, *Pretraining
    a RoBERTa Model from Scratch*.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，将使用 `kantgpt.csv` 对 GPT-3 进行 Immanuel Kant 的作品进行训练。我们使用了类似的文件来训练第四章 *从头开始预训练
    RoBERTa 模型* 中的 BERT 类型模型。
- en: Once you master fine-tuning GPT-3, you can use other types of data to teach
    it specific domains, knowledge graphs, and texts.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦掌握了对GPT-3进行微调的技巧，就可以使用其他类型的数据来教授它特定领域、知识图和文本。
- en: OpenAI provides an efficient, well-documented service to fine-tune GPT-3 engines.
    It has trained GPT-3 models to become different types of engines, as seen in the
    *The rise of billion-parameter transformer models* section of this chapter.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一个高效、文档完善的服务，可以对GPT-3引擎进行微调。它已经训练了GPT-3模型，使其成为不同类型的引擎，正如本章中*十亿参数变压器模型的崛起*部分所示。
- en: The Davinci engine is powerful but can be more expensive to use. The Ada engine
    is less expensive and produces sufficient results to explore GPT-3 in our experiment.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Davinci引擎功能强大，但使用起来可能更昂贵。Ada引擎成本较低，并且产生的结果足以在我们的实验中探索GPT-3。
- en: 'Fine-tuning GPT-3 involves two phases:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的微调涉及两个阶段：
- en: Preparing the data
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Fine-tuning a GPT-3 model
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对GPT-3模型进行微调
- en: Preparing the data
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: Open `Fine_Tuning_GPT_3.ipynb` in Google Colab in the GitHub chapter directory.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub章节目录中在Google Colab中打开`Fine_Tuning_GPT_3.ipynb`。
- en: 'OpenAI has documented the data preparation process in detail:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI详细记录了数据准备过程：
- en: '[https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data](https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data](https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data)'
- en: 'Step 1: Installing OpenAI'
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1步：安装OpenAI
- en: '*Step 1* is to install and import `openai`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1步* 是安装和导入`openai`：'
- en: '[PRE17]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Restart the runtime once the installation is complete and run the cell again
    to make sure import openai has been executed
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完成，请重新启动运行时，然后再次运行单元格，确保已执行`import openai`。
- en: '[PRE18]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can also install wand to visualize the logs:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以安装wand来可视化日志：
- en: '[PRE19]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will now enter the API key
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将输入API密钥
- en: 'Step 2: Entering the API key'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步：输入API密钥
- en: '*Step 2* is to enter your key:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '*第2步* 是输入您的密钥：'
- en: '[PRE20]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 3: Activating OpenAI’s data preparation module'
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步：激活OpenAI的数据准备模块
- en: First, load your file. In this section, load `kantgpt.csv`. Now, `kantgpt.csv`.
    is a raw unstructured file. OpenAI has an inbuilt data cleaner that will ask questions
    at each step.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，加载您的文件。在此部分加载`kantgpt.csv`。现在，`kantgpt.csv`.是一个原始的非结构化文件。OpenAI有一个内置数据清理器，会在每个步骤问问题。
- en: OpenAI detects that the file is a CSV file and will convert it to a `JSONL`
    file. `JSONL` contains lines in plain structured text.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI检测到文件是CSV文件，将其转换为`JSONL`文件。`JSONL`包含纯结构化文本中的行。
- en: 'OpenAI tracks all the changes we approve:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI跟踪我们批准的所有更改：
- en: '[PRE21]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: OpenAI saves the converted file to `kantgpt_prepared.jsonl`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI将转换文件保存为`kantgpt_prepared.jsonl`。
- en: We are ready to fine-tune GPT-3.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好对GPT-3进行微调。
- en: Fine-tuning GPT-3
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3的微调
- en: 'You can split the notebook into two separate notebooks: one for data preparation
    and one for fine-tuning.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将笔记本分成两个独立的笔记本：一个用于数据准备，另一个用于微调。
- en: 'Step 4: Creating an OS environment'
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4步：创建一个操作系统环境
- en: '*Step 4* in the fine-tuning process creates an `os` environment for the API
    key:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '*微调* 过程中的*第4步* 为API密钥创建了一个`os`环境：'
- en: '[PRE22]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Step 5: Fine-tuning OpenAI’s Ada engine'
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第5步：微调OpenAI的Ada引擎
- en: '*Step 5* triggers fine-tuning the OpenAI Ada engine with the JSONL file that
    was saved after data preparation:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '*第5步* 触发了使用数���准备后保存的JSONL文件来微调OpenAI Ada引擎：'
- en: '[PRE23]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: OpenAI has many requests.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI有很多请求。
- en: 'If your steam is interrupted, OpenAI will indicate the instruction to continue
    fine-tuning. `Execute fine_tunes.follow` instruction:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的流程中断，OpenAI将指示如何继续微调。执行fine_tunes.follow指令：
- en: '[PRE24]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Step 6: Interacting with the fine-tuned model'
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第6步：与微调模型互动
- en: '*Step 6* is interacting with the fine-tuned model. The prompt is a sequence
    that is close to what *Immanuel Kant* might say:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6步* 是与微调模型互动。提示是接近*伊曼纽尔·康德*可能会说的内容的序列：'
- en: '[PRE25]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The instruction to run a completion task with `[YOUR_MODEL INFO]` is often displayed
    by OpenAI at the end of your fine-tune task. You can copy and paste it in a cell(add
    `"!"` to run the command line) or insert your `[YOUR_MODEL INFO]` in the following
    cell.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的微调任务结束时，OpenAI经常显示运行完成任务与`[YOUR_MODEL INFO]`的指令。您可以复制并粘贴到一个单元格中（在命令行前加上`"!"`）或将您的`[YOUR_MODEL
    INFO]`插入到下一个单元格。
- en: 'The completion is quite convincing:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 完成结果相当令人信服：
- en: '[PRE26]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have fine-tuned GPT-3, which shows the importance of understanding transformers
    and designing AI pipelines with APIs. Let’s see how this changes the role of AI
    specialists.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经对GPT-3进行了微调，这显示了理解变压器和设计具有API的人工智能流程的重要性。让我们看看这如何改变AI专家的角色。
- en: The role of an Industry 4.0 AI specialist
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Industry 4.0人工智能专家的角色
- en: In a nutshell, the role of an Industry 4.0 developer is to become a cross-disciplinary
    AI guru. Developers, data scientists, and AI specialists will progressively learn
    more about linguistics, business goals, subject matter expertise, and more. An
    Industry 4.0 AI specialist will guide teams with practical cross-disciplinary
    knowledge and experience.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，工业4.0开发人员的角色是成为跨学科AI专家。开发人员、数据科学家和AI专家将逐步学习有关语言学、业务目标、专业专业知识等更多领域。工业4.0
    AI专家将引导团队具备实践的跨学科知识和经验。
- en: 'Human experts are mandatory in three domains when implementing transformers:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施变压器时，人类专家在三个领域中是必不可少的：
- en: '**Morals and ethics**'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**道德和伦理**'
- en: An Industry 4.0 AI guru ensures moral and ethical practices are enforced when
    implementing humanlike transformers. European regulations, for example, are strict
    and require that automated decisions be explained to the users when necessary.
    The US has anti-discrimination laws to protect citizens from automated bias.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个工业4.0的AI专家确保在实施类似于人类的变压器时执行道德和伦理实践。例如，欧洲法规严格要求必要时向用户解释自动决策。美国有反歧视法律保护公民免受自动偏见影响。
- en: '**Prompts and responses**'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示和响应**'
- en: Users and UI developers will need Industry 4.0 AI gurus to explain how to create
    the right prompts for NLP tasks, show a transformer model how to do a task, and
    verify the response.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户和UI开发人员将需要工业4.0 AI专家解释如何为NLP任务创建正确的提示，展示变压器模型如何执行任务，并验证响应。
- en: '**Quality control and understanding the model**'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制和理解模型**'
- en: What happens when the model does not behave as expected even after tweaking
    its hyperparameters? We will go deeper into such issues in *Chapter 14*, *Interpreting
    Black Box Transformer Models*.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当模型即使调整超参数后仍不如预期时会发生什么？我们将在*第14章*《解释黑匣子变压器模型》中更深入地探讨这类问题。
- en: Initial conclusions
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初步结论
- en: 'The initial conclusions can be divided into two categories: facts and fiction.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 初步结论可分为两类：事实和虚构。
- en: 'One fact is that OpenAI has one of the most powerful NLP services in the world.
    Other facts include:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 一个事实是，OpenAI拥有全球最强大的NLP服务之一。其他事实包括：
- en: OpenAI engines are powerful zero-shot engines that require no hunting for all
    kinds of transformer models, no pre-training, and no fine-tuning
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 引擎是强大的零样本引擎，不需要寻找各种变压器模型，也不需要预训练和微调。
- en: The supercomputers used to train the models are unique
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练模型的超级计算机是独特的
- en: If a prompt is well designed, we can get surprisingly accurate responses
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果提示设计良好，我们可以获得令人惊讶的准确答案
- en: Implementing the NLP tasks in this chapter only required a copy and paste action
    that any software beginner can perform
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中实施NLP任务只需要拷贝和粘贴操作，任何软件初学者都可以执行
- en: 'Many people believe AI will replace data scientists and AI specialists. Is
    that true? Before answering that question, first, ask yourself the following questions
    about the examples we ran in this chapter:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人相信AI将取代数据科学家和AI专家。这是真的吗？在回答这个问题之前，首先问问自己关于本章运行的例子的以下问题：
- en: How do we know if a sentence is incorrect?
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何知道一个句子是不正确的？
- en: How do we know an answer is correct without us humans reading and confirming
    this?
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何确定一个答案是正确的，而不需要我们人类阅读和确认？
- en: How did the engine know it was a grammar correction task?
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引擎如何知道这是一个语法校对任务？
- en: If a response is incorrect, how can we understand what happened to help improve
    the prompt or revert to manual mode in a well-designed human interface?
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果响应不正确，我们如何理解发生了什么，以帮助改进提示或在设计良好的人机界面中恢复手动模式？
- en: The truth is that humans will need to intervene to answers these questions manually,
    with rule bases, quality controlled automated pipelines, and many other tools.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 实际情况是人类需要手动干预回答这些问题，通过规则基础、质量受控的自动流水线和许多其他工具。
- en: The facts are convincing. Running an NLP task with a transformer requires little
    development in many cases.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 事实令人信服。在许多情况下，使用变压器运行NLP任务只需要很少的开发。
- en: Humans are still required. OpenAI engines are not there to replace humans but
    to help them perform more high-level gratifying tasks. You can now fly a jet without
    having to build it!
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 人类仍然是必需的。OpenAI引擎不是为了取代人类，而是帮助他们执行更多高水平令人满意的任务。现在你可以驾驶喷气式飞机而不必自己建造！
- en: We need to answer the exciting questions we brought up in this section. So let’s
    now explore your new fascinating Industry 4.0 role on a wonderful path into the
    future of AI!
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要回答本节提出的激动人心的问题。所以让我们现在探索你在迈向AI未来的精彩工业4.0角色中的新奇旅程！
- en: Let’s sum up the chapter and move on to the next exploration!
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结本章并继续下一个探索！
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discovered the new era of transformer models training billions
    of parameters on supercomputers. OpenAI’s GPT models are taking NLU beyond the
    reach of most NLP development teams.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们发现了超级计算机上训练数十亿参数的变压器模型的新时代。OpenAI 的 GPT 模型正在将 NLU 推动到大多数 NLP 开发团队的视野之外。
- en: We saw how a GPT-3 zero-shot model performs many NLP tasks through an API and
    even directly online without an API. The online version of Google Translate has
    already paved the way for mainstream online usage of AI.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了 GPT-3 零-shot 模型如何通过 API 进行许多 NLP 任务，甚至可以直接在线进行而无需 API。Google 翻译的在线版本已经为
    AI 的主流在线使用铺平了道路。
- en: We explored the design of GPT models, which are all built on the original transformer’s
    decoder stack. The masked attention sub-layer continues the philosophy of left-to-right
    training. However, the sheer power of the calculations and the subsequent self-attention
    sub-layer makes it highly efficient.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了 GPT 模型的设计，这些模型都是构建在原始变压器的解码器堆叠上的。掩码式注意力子层延续了从左到右训练的思想。然而，计算的绝对能力和随后的自注意力子层使其非常高效。
- en: We then implemented a 345M parameter GPT-2 model with TensorFlow. The goal was
    to interact with a trained model to see how far we could get with it. We saw that
    the context provided conditioned the outputs. However, it did not reach the results
    expected when entering a specific input from the `Kant` dataset.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 TensorFlow 实现了一个包含 345M 个参数的 GPT-2 模型。目标是与训练模型互动，看看我们能达到什么程度。我们看到所提供的上下文条件了输出。然而，当输入特定来自`Kant`
    数据集的输入时，结果并没有达到预期的结果。
- en: We trained a 117M parameter GPT-2 model on a customized dataset. The interactions
    with this relatively small trained model produced fascinating results.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对一个包含 117M 个参数的定制数据集进行了训练的 GPT-2 模型。与这个相对较小的训练模型的交互产生了令人着迷的结果。
- en: We ran NLP tasks online with OpenAI’s API and fine-tuned a GPT-3 model. This
    chapter showed that the fully pretrained transformers and their engines can automatically
    accomplish many tasks with little help from engineers.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 OpenAI 的 API 在线运行了 NLP 任务，并对 GPT-3 模型进行了微调。本章展示了完全预训练的变压器及其引擎可以在很少有工程师的帮助下自动完成许多任务。
- en: Does this mean that users will not need AI NLP developers, data scientists,
    and AI specialists anymore in the future? Instead, will users simply upload the
    task definition and input text to cloud transformer models and download the results?
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 这是否意味着用户将不再需要 AI NLP 开发人员、数据科学家和 AI 专家？而是用户只需将任务定义和输入文本上传到云变压器模型中，然后下载结果？
- en: No, it doesn’t mean that at all. Industry 4.0 data scientists and AI specialists
    will evolve into pilots of powerful AI systems. They will be increasingly necessary
    to ensure the inputs are ethical and secure. These modern-age AI pilots will also
    understand how transformers are built and adjust the hyperparameters of an AI
    ecosystem.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 不，一点也不意味着。工业 4.0 数据科学家和 AI 专家将演变为强大 AI 系统的领航员。他们的角色将越来越重要，以确保输入是合乎道德和安全的。这些现代化的
    AI 飞行员还将了解变压器是如何构建的，并调整 AI 生态系统的超参数。
- en: In the next chapter, *Applying Transformers to Legal and Financial Documents
    for AI Text Summarization*, we will take transformer models to their limits as
    multi-task models and explore new frontiers.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章《将变压器应用于法律和金融文件进行 AI 文本摘要》中，我们将将变压器模型推到极限，作为多任务模型并探索新的领域。
- en: Questions
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: A zero-shot method trains the parameters once. (True/False)
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种零-shot 方法只需训练参数一次。（True/False）
- en: Gradient updates are performed when running zero-shot models. (True/False)
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行零-shot 模型时进行梯度更新。（True/False）
- en: GPT models only have a decoder stack. (True/False)
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT 模型只有一个解码器堆叠。（True/False）
- en: It is impossible to train a 117M GPT model on a local machine. (True/False)
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地机器上训练包含 117M 个参数的 GPT 模型是不可能的。（True/False）
- en: It is impossible to train the GPT-2 model with a specific dataset. (True/False)
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不能使用特定数据集训练 GPT-2 模型。（True/False）
- en: A GPT-2 model cannot be conditioned to generate text. (True/False)
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不能将 GPT-2 模型调节为生成文本。（True/False）
- en: A GPT-2 model can analyze the context of an input and produce completion content.
    (True/False)
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个 GPT-2 模型可以分析输入的上下文并生成完成的内容。（True/False）
- en: We cannot interact with a 345M-parameter GPT model on a machine with less than
    8 GPUs. (True/False)
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在少于 8 个 GPU 的机器上不能与包含 345M 个参数的 GPT 模型互动。（True/False）
- en: Supercomputers with 285,000 CPUs do not exist. (True/False)
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有 285,000 个 CPUs 的超级计算机并不存在。（True/False）
- en: Supercomputers with thousands of GPUs are game-changers in AI. (True/False)
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有成千上万个 GPU 的超级计算机在 AI 中起着决定性的作用。（True/False）
- en: References
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'OpenAI and GPT-3 engines: [https://beta.openai.com/docs/engines/engines](https://beta.openai.com/docs/engines/engines)'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI 和 GPT-3 引擎: [https://beta.openai.com/docs/engines/engines](https://beta.openai.com/docs/engines/engines)'
- en: '`BertViz` GitHub Repository by *Jesse Vig*: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertViz` 由 *Jesse Vig* 创建的 GitHub 仓库: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)'
- en: 'OpenAI’s supercomputer: [https://blogs.microsoft.com/ai/openai-azure-supercomputer/](https://blogs.microsoft.com/ai/openai-azure-supercomputer/)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI 的超级计算机: [https://blogs.microsoft.com/ai/openai-azure-supercomputer/](https://blogs.microsoft.com/ai/openai-azure-supercomputer/)'
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
- en: '*Alec Radford*, *Karthik Narasimhan*, *Tim Salimans*, *Ilya Sutskever*, 2018,
    *Improving* *Language Understanding by Generative Pre-Training*: [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alec Radford*, *Karthik Narasimhan*, *Tim Salimans*, *Ilya Sutskever*, 2018,
    *Improving* *Language Understanding by Generative Pre-Training*: [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
- en: '*Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, and *Kristina Toutanova*, 2019,
    *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*:
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, 和 *Kristina Toutanova*, 2019,
    *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*:
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
- en: '*Alec Radford*, *Jeffrey Wu*, *Rewon Child*, *David Luan*, *Dario Amodei*,
    *Ilya Sutskever*, 2019, *Language Models are Unsupervised Multitask Learners*:
    [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alec Radford*, *Jeffrey Wu*, *Rewon Child*, *David Luan*, *Dario Amodei*,
    *Ilya Sutskever*, 2019, *Language Models are Unsupervised Multitask Learners*:
    [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)'
- en: '*Tom B. Brown*, *Benjamin Mann*, *Nick Ryder*, *Melanie Subbiah*, *Jared Kaplany*,
    *Prafulla Dhariwal*, *Arvind Neelakantan*, *Pranav Shyam*, *Girish Sastry*, *Amanda
    Askell*, *Sandhini Agarwal*, *Ariel Herbert-Voss*, *Gretchen Krueger*, *Tom Henighan*,
    *Rewon Child*, *Aditya Ramesh*, *Daniel M. Ziegler*, *Jeffrey Wu*, *Clemens Winter*,
    *Christopher Hesse*, *Mark Chen*, *Eric Sigler*, *Mateusz Litwin*, *Scott Gray*,
    *Benjamin Chess*, *Jack Clark*, *Christopher Berner*, *Sam McCandlish*, *Alec
    Radford*, *Ilya Sutskever*, *Dario Amodei*, 2020, *Language Models are Few-Shot
    Learners*: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tom B. Brown*, *Benjamin Mann*, *Nick Ryder*, *Melanie Subbiah*, *Jared Kaplany*,
    *Prafulla Dhariwal*, *Arvind Neelakantan*, *Pranav Shyam*, *Girish Sastry*, *Amanda
    Askell*, *Sandhini Agarwal*, *Ariel Herbert-Voss*, *Gretchen Krueger*, *Tom Henighan*,
    *Rewon Child*, *Aditya Ramesh*, *Daniel M. Ziegler*, *Jeffrey Wu*, *Clemens Winter*,
    *Christopher Hesse*, *Mark Chen*, *Eric Sigler*, *Mateusz Litwin*, *Scott Gray*,
    *Benjamin Chess*, *Jack Clark*, *Christopher Berner*, *Sam McCandlish*, *Alec
    Radford*, *Ilya Sutskever*, *Dario Amodei*, 2020, *Language Models are Few-Shot
    Learners*: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *SuperGLUE: A Stickier
    Benchmark for General-Purpose Language Understanding Systems*: [https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *SuperGLUE: A Stickier
    Benchmark for General-Purpose Language Understanding Systems*: [https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding*: [https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding*: [https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)'
- en: 'OpenAI GPT-2 GitHub Repository: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI GPT-2 GitHub 仓库: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
- en: 'N. Shepperd’s GitHub Repository: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N. Shepperd 的 GitHub 仓库：[https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)
- en: 'Common Crawl data: [https://commoncrawl.org/big-picture/](https://commoncrawl.org/big-picture/)'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Common Crawl 数据：[https://commoncrawl.org/big-picture/](https://commoncrawl.org/big-picture/)
- en: Join our book’s Discord space
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，与作者进行每月的 *问答环节*：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
