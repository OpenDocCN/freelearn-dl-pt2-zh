- en: Chapter 1. Machine Learning – An Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 章节1. 机器学习——简介
- en: '**"Machine Learning (CS229) is the most popular course at Stanford" –this is
    how a Forbes article by Laura Hamilton started, continuing- "Why? Because, increasingly,
    machine learning is eating the world".**'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“机器学习（CS229）是斯坦福最受欢迎的课程”——这是由劳拉·汉密尔顿在《福布斯》上的一篇文章的开头，然后她继续说“为什么？因为越来越多地，机器学习正在改变世界”。**'
- en: Machine learning techniques are, indeed, being applied in a variety of fields,
    and data scientists are being sought after in many different industries. With
    machine learning, we identify the processes through which we gain knowledge that
    is not readily apparent from data, in order to be able to make decisions. Applications
    of machine learning techniques may vary greatly and are applicable in disciplines
    as diverse as medicine, finance, and advertising.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，机器学习技术正在被应用于各种领域，并且数据科学家在许多不同的行业中备受追捧。有了机器学习，我们可以确定从数据中并不容易发现的知识，以便能够做出决策。机器学习技术的应用可能差异巨大，并且适用于医学、金融和广告等多种学科领域。
- en: In this chapter, we will present different Machine learning approaches and techniques,
    and some of their applications to real-world problems, and we will introduce one
    of the major open source packages available in Python for machine learning, `scikit-learn`.
    This will lay the background for later chapters in which we will focus on a particular
    type of machine learning approach using neural networks that aims at emulating
    brain functionality, and in particular deep learning. Deep learning makes use
    of more advanced neural networks than those used during the 80's, thanks not only
    to recent developments in the theory but also to advances in computer speed and
    the use of **GPUs** (**Graphical** **Processing Units**) versus the more traditional
    use of **CPUs** (**Computing Processing Units**). This chapter is meant mostly
    as a summary of what machine learning is and can do, and to prepare the reader
    to better understand how deep learning differentiates itself from popular traditional
    machine learning techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍不同的机器学习方法和技术，以及它们在实际问题中的一些应用，并且我们将介绍Python中一种主要的开源机器学习软件包`scikit-learn`。这将为随后的章节打下基础，我们将专注于一种特定类型的机器学习方法，使用神经网络来模拟大脑功能，特别是深度学习。深度学习利用比80年代使用的更先进的神经网络，不仅得益于理论的最新发展，还得益于计算机速度的提高以及使用**GPU**（图形处理单元）而不是更传统的**CPU**（计算处理单元）。本章主要是对机器学习是什么以及能做什么的一个总结，并准备读者更好地了解深度学习如何与流行的传统机器学习技术有所不同。
- en: 'In particular, in this chapter we will cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，本章我们将涵盖：
- en: What is machine learning?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Different machine learning approaches
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的机器学习方法
- en: Steps involved in machine learning systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习系统涉及的步骤
- en: Brief description of popular techniques/algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的技术/算法的简要描述
- en: Applications in real-life
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现实生活中的应用
- en: A popular open source package
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个流行的开源软件包
- en: What is machine learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Machine learning is often mentioned together with terms such as "big data" and
    "artificial intelligence", or A.I. for short, but it is quite different from both.
    In order to understand what machine learning is and why it is useful, it is important
    to understand what big data is and how machine learning applies to it. Big data
    is a term used to describe huge data sets that are created as the result of large
    increases in data gathered and stored, for example, through cameras, sensors,
    or Internet social sites. It is estimated that Google alone processes over 20
    petabytes of information per day and this number is only going to increase. IBM
    estimated ([http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html](http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html))
    that every day, 2.5 quintillion bytes are created and that 90% of all the data
    in the world has been created in the last two years.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习经常和术语“大数据”和“人工智能”，或者简称为A.I.一起提到，但它与两者都大不相同。要理解机器学习是什么，以及它为何有用，了解大数据是什么以及机器学习如何应用于其中是很重要的。大数据是一个用来描述通过摄像头、传感器或互联网社交网站等方式产生的大规模数据集的术语。据估计，仅谷歌每天处理超过20PB的信息，而且这个数字还将继续增加。IBM估计（[http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html](http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html)）每天都会产生25亿GB的数据，而且世界上90%的数据是在过去两年内创建的。
- en: Clearly, humans alone are unable to grasp, let alone analyze, such a huge amount
    of data, and machine learning techniques are used to make sense of these very
    large data sets. Machine learning is the tool used for large-scale data processing
    and is well suited for complex datasets with huge numbers of variables and features.
    One of the strengths of many machine learning techniques, and deep learning in
    particular, is that it performs best when it can be used on large data sets improving
    its analytic and predictive power. In other words, machine learning techniques,
    and especially deep learning neural networks, "learn" best when they can access
    large data sets in order to discover patterns and regularities hidden in the data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，人类自己无法理解，更别说分析如此庞大的数据量了，而且机器学习技术被用来理解这些非常庞大的数据集。机器学习是用于大规模数据处理的工具，非常适用于具有大量变量和特征的复杂数据集。许多机器学习技术，特别是深度学习，其优势之一是在处理大量数据集时表现最好，从而提高了其分析和预测能力。换句话说，机器学习技术，尤其是深度学习神经网络，在可以访问大量数据集时“学习”最好，以发现数据中隐藏的模式和规律。
- en: 'On the other hand, machine learning''s predictive ability can be well adapted
    to artificial intelligence systems. Machine learning can be thought of as "the
    brain" of an artificial intelligence system. Artificial intelligence can be defined
    (though this definition may not be unique) as a system that can interact with
    its environment: artificial intelligence machines are endowed with sensors that
    allow them to know about the environment they are in and tools with which they
    can relate back. Machine learning is therefore the brain that allows the machine
    to analyze the data ingested through its sensors to formulate an appropriate answer.
    A simple example is Siri on an iPhone. Siri hears the command through its microphone
    and outputs an answer through its speakers or through its display, but in order
    to do so it needs to "understand" what it is being said to formulate the correct
    answer. Similarly, driverless cars will be equipped with cameras, GPS systems,
    sonars and lidars, but all this information needs to be processed in order to
    provide a correct answer, that is, whether to accelerate, brake, turn, and so
    on. The information processing that leads to the answer represents what machine
    learning is.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，机器学习的预测能力可以很好地适应人工智能系统。机器学习可以被认为是人工智能系统的“大脑”。人工智能可以被定义为（尽管这个定义可能不是独一无二的）一个能够与环境进行交互的系统：人工智能机器被赋予传感器，使它们可以了解所处的环境，以及可以进行关联的工具。因此，机器学习是允许机器分析经过传感器摄入的数据并制定适当答案的大脑。一个简单的例子是iPhone上的Siri。Siri通过麦克风听到命令，并通过扬声器或显示屏输出答案，但为了这样做，它需要“理解”所说的话来制定正确的答案。同样，无人驾驶汽车将配备摄像头、GPS系统、声纳和激光雷达，但所有这些信息都需要被处理，以便提供正确的答案，即加速，刹车，转弯等。导致答案的信息处理代表了机器学习的内容。
- en: Different machine learning approaches
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的机器学习方法
- en: 'The term machine learning, as we have seen, is used in a very general way and
    it refers to general techniques to extrapolate patterns from large sets or to
    the ability to make predictions on new data based on what is learnt by analyzing
    available known data. This is a very general and broad definition and it encompasses
    many different techniques. Machine learning techniques can be roughly divided
    into two large classes: Supervised and Unsupervised learning, though one more
    class is often added, and is referred to as Reinforcement Learning.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习这个术语，正如我们所看到的，被以非常普遍的方式使用，它指的是从大型数据集中外推模式的一般技术，或者是基于分析已知数据进行学习，并根据所学的内容对新数据进行预测的能力。这是一个非常普遍和广泛的定义，它囊括了许多不同的技术。机器学习技术可以大致分为两类：监督学习和无监督学习，尽管通常还会添加一类，称为强化学习。
- en: Supervised learning
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: The first class of machine algorithms is named *supervised learning*. Supervised
    learning algorithms are a class of machine learning algorithms that use a set
    of labeled data in order to classify similar un-labeled data. Labeled data is
    data that has already been classified, while un-labeled data is data that has
    not yet been labeled. Labels, as we will see, can either be discrete or continuous.
    In order to better understand this concept, let's use an example.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '机器算法的第一类被称为*监督学习*。 监督学习算法是一类使用一组已标记数据以对相似未标记数据进行分类的机器学习算法。 已标记数据是已经被分类的数据，而未标记数据是还没有被分类的数据。
    正如我们将看到的那样，标签可以是离散的或连续的。 为了更好地理解这个概念，让我们举个例子。 '
- en: Assume that a user receives a large amount of e-mails every day, some of which
    are important business e-mails and some of which are un-solicited junk e-mails,
    or spam. A supervised machine algorithm will be presented with a large body of
    e-mails that have already been labeled by the user as spam or not spam. The algorithm
    will run over all the labeled data and make predictions on whether the e-mail
    is spam or not. This means that the algorithm will examine each example and make
    a prediction for each example on whether the e-mail is spam or not. Typically,
    the first time the algorithm runs over all the un-labeled data, it will mislabel
    many of the e-mails and it may perform quite poorly. However, after each run,
    the algorithm will compare its prediction to the desired outcome (the label).
    In doing so, the algorithm will learn to improve its performance and accuracy.
    As noted above, an approach of this kind will benefit from large amounts of data
    on which it can better learn what characteristics (or features) cause each e-mail
    to be classified as spam or not.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个用户每天收到大量电子邮件，其中一些是重要的商务邮件，一些是垃圾邮件或垃圾邮件。 一个监督式机器算法将拿到用户已标记为垃圾邮件或非垃邮件的大量邮件。
    该算法将运行在所有已标记的数据上，并预测这些邮件是否属于垃圾邮件。 这意味着算法将检查每个示例，并为每个示例预测这封邮件是否是垃圾邮件。 通常情况下，算法首次运行未标记的数据时，会错误标记许多邮件，并且表现可能相当糟糕。
    然而，每次运行之后，算法将比较其预测与期望结果（标记）。 这样一来，算法将学会提高其性能和准确性。 如上所述，这种方法将受益于大量数据，以便更好地学习每封邮件被分类为垃圾邮件或非垃圾邮件的特征（或特征）。
- en: After the algorithm has run for a while on the labeled data (often also called
    training data) and it stops improving its accuracy, it can then be used on new
    e-mails to test its accuracy on new un-labeled data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在标记数据上运行一段时间后（通常也称为训练数据），并且在其准确性停止改善后，它可以用于新的未标记数据来测试其在新的电子邮件上的准确性。
- en: 'In the example we have used, we have described a process in which an algorithm
    learns from labeled data (the e-mails that have been classified as spam or not
    spam) in order to make predictions on new unclassified e-mails. It is important
    to note, however, that we can generalize this process to more than simply two
    classes: for example, we could run the software and train it on a set of labeled
    e-mails where the labels are called **Personal**, **Business**/**Work**, **Social**,
    or **Spam**.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们所使用的例子中，我们描述了一个从已标记数据中学习的算法过程（已被分类为垃圾邮件或非垃圾邮件的邮件），以便对新的未分类邮件进行预测。 然而，重要的是要注意，我们可以将这个过程泛化到不仅仅是两个类别：例如，我们可以运行软件并在一组已标记的电子邮件上对其进行训练，其中标签称为**个人**，**商务/工作**，**社交**或**垃圾邮件**。
- en: 'In fact, Gmail, the free e-mail service by Google, allows the user to select
    up to five categories, labeled as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，谷歌提供的免费电子邮件服务Gmail允许用户选择最多五种类别，分别标注为：
- en: '**Primary**, which includes person-to-person conversations'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主要**，包括人与人之间的对话'
- en: '**Social**, which includes messages from social networks and media sharing
    sites'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交**，包括社交网络和媒体分享网站的消息'
- en: '**Promotions**, which includes marketing e-mails, offers, and discounts'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促销**，包括市场营销邮件、优惠和折扣'
- en: '**Updates**, which includes bills, bank statements, and receipts'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新**，包括账单、银行对账单和收据'
- en: '**Forums**, which includes messages from online groups and mailing lists'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**论坛**，包括在线群体和邮件列表的消息'
- en: In some cases, the outcome may not necessarily be discrete, and we may not have
    a finite number of classes to classify our data into. For example, we may be trying
    to predict the life expectancy of a group of people based on pre-determined health
    parameters. In this case, since the outcome is a continuous function (we can specify
    a life expectancy as a real number expressing the number of years that person
    is expected to live) we do not talk about a classification task but rather of
    a regression problem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，结果可能并不一定是离散的，我们可能没有一个有限的类别来对数据进行分类。例如，我们可能正在基于预先确定的健康参数来预测一群人的预期寿命。在这种情况下，由于结果是一个连续函数（我们可以指定寿命预期为表示人们预期活多少年的实数），我们不谈论分类任务，而是一个回归问题。
- en: 'One way to think of supervised learning is by imagining we are trying to build
    a function *f* defined on the dataset. Our dataset will comprise information organized
    by *features*. In the example of e-mail classification, those features may be
    specific words that may appear more frequently than others in spam e-mails. The
    use of explicit sex-related words will most likely identify a spam e-mail rather
    than a business/work e-mail. On the contrary, words, such as "meeting", "business",
    and "presentation" will more likely describe a work e-mail. If we have access
    to metadata, the sender information may also be used to better classify e-mails.
    Each e-mail will then have associated a set of features, and each feature will
    have a value (in this case, how many times the specific word is present in the
    e-mail body). The machine learning algorithm will then seek to map those values
    to a discrete range which represents the set of classes, or a real value in the
    case of regression. The algorithm will run over many examples until it is able
    to define the best function that will allow matching most labeled data correctly.
    It can then be run over unlabeled data to make predictions without human intervention.
    This defines a function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下监督学习的一种方法是，我们试图构建一个在数据集上定义的函数*f*。我们的数据集将包含由*特征*组织的信息。在电子邮件分类的例子中，这些特征可能是在垃圾邮件中出现频率较高的特定单词。使用显式的与性相关的词语很可能可以识别出垃圾邮件而不是商务/工作邮件。相反，诸如“会议”，“商务”和“演示”之类的词可能更有可能描述一个工作邮件。如果我们有访问元数据的权限，发送方信息也可以用来更好地分类电子邮件。然后，每封电子邮件将关联一组特征，并且每个特征将有一个值（在这种情况下，特定单词在电子邮件正文中出现的次数）。机器学习算法将寻求将这些值映射到表示一组类别的离散范围，或者在回归的情况下是一个实值。该算法将运行多个示例，直到能够定义出最佳函数，以便正确匹配大部分标记数据。然后它可以在未标记数据上运行，以做出预测而不需要人为干预。这定义了一个函数：
- en: '![Supervised learning](img/00002.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/00002.jpeg)'
- en: We can also think of classification as a process seeking to separate different
    groups of data points. Once we have defined our features, any example, for example,
    an e-mail, in our dataset can be thought of as a point in the space of features,
    where each point represents a different example (or e-mail). The machine algorithm
    task will be to draw a hyper-plane (that is a plane in a high dimensional space)
    to separate points with different characteristics, in the same way we want to
    separate spam from non-spam e-mails.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将分类视为一种寻求分离不同数据点组的过程。一旦我们定义了我们的特征，任何例子，比如说，是我们数据集中的一个电子邮件，可以被视为特征空间中的一个点，每一个点代表一个不同的例子（或电子邮件）。机器算法的任务是画出一个超平面（即高维空间中的平面），将具有不同特征的点分开，正如我们想要将垃圾邮件与非垃圾邮件分开一样。
- en: While, as in the picture below, this may look trivial in a two-dimensional case,
    this can turn out to be very complex in examples with hundreds or thousands of
    dimensions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在二维情况下可能看起来很简单，但在具有数百或数千维度的情况下可能非常复杂。
- en: '![Supervised learning](img/00003.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/00003.jpeg)'
- en: Classification can be thought of as a way of separating the input data
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 分类可以被看作是分隔输入数据的一种方式
- en: 'In later chapters, we will see several examples of either classification or
    regression problems. One such problem we will discuss is that of the classification
    of digits: given a set of images representing 0 to 9, the machine learning algorithm
    will try to classify each image assigning to it the digits it depicts. For such
    examples, we will make use of one of the most classic datasets, the MNIST dataset.
    In this example, each digit is represented by an image with 28 x 28 (=784) pixels,
    and we need to classify each of the ten digits, therefore we need to draw 9 separating
    hyper planes in a 784-dimensional space.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将看到几个分类或回归问题的例子。我们将讨论其中一个问题是数字的分类：给定一组代表0到9的图像，机器学习算法将尝试对每个图像进行分类，并分配给它所描绘的数字。对于这样的例子，我们将使用最经典的数据集之一，即MNIST数据集。在这个例子中，每个数字由一个28
    x 28（=784）像素的图像表示，我们需要对每个数字进行分类，因此我们需要在784维空间中画出9个分隔超平面。
- en: '![Supervised learning](img/00004.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/00004.jpeg)'
- en: Example of handwritten digits from the MNIST dataset
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 来自MNIST数据集的手写数字示例
- en: Unsupervised learning
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The second class of machine learning algorithms is named *unsupervised learning*.
    In this case, we do not label the data beforehand, rather we let the algorithm
    come to its conclusion. One of the most common and perhaps simplest examples of
    unsupervised learning is clustering. This is a technique that attempts to separate
    the data into subsets.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的第二类称为*无监督学习*。在这种情况下，我们不事先为数据贴上标签，而是让算法得出结论。无监督学习中最常见、也许是最简单的例子之一是聚类。这是一种试图将数据分成子集的技术。
- en: For example, in the previous case of spam/not spam e-mails, the algorithm may
    be able to find elements that are common to all spam e-mails (for example, the
    presence of misspelled words). While this may provide a better than random classification,
    it isn't clear that spam/not spam e-mails can be so easily separated. The subsets
    into which the algorithm separates the data are different classes for the dataset.
    For clustering to work, each element in each cluster should in principle have
    high intra-class similarity and low similarity with other classes. Clustering
    may work with any number of classes, and the idea behind clustering methods such
    as k-means is to find k-subsets of the original data whose elements are closer
    (more similar) to each other than they are to any other element outside their
    class. Of course, in order to do this, we need to define what *closer* or *more
    similar* means, that is, we need to define some kind of metric that defines the
    distance between points.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，在前述的垃圾邮件/非垃圾邮件的情况下，算法可能能够找到所有垃圾邮件共有的元素（例如，拼写错误的单词的存在）。尽管这可能提供比随机分类更好的结果，但不清楚垃圾邮件/非垃圾邮件是否能够如此轻松地分开。算法将数据分离成的子集是数据集的不同类别。为了使聚类有效，每个聚类中的每个元素在原则上应具有高的类内相似度和与其他类别的低相似度。聚类可以处理任意数量的类别，并且聚类方法（如k均值）背后的想法是找到原始数据的k个子集，这些子集的元素彼此之间比与其类外的任何其他元素更接近（更相似）。当然，为了做到这一点，我们需要定义*更接近*或*更相似*的含义，也就是说，我们需要定义某种度量，来定义点之间的距离。
- en: 'In the following graph, we show how a set of points can be classified to form
    three subsets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了一组点如何被分类成三个子集：
- en: '![Unsupervised learning](img/00005.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/00005.jpeg)'
- en: 'Elements of a given dataset need not necessarily cluster together to form a
    finite set, but clustering may also include unbounded subsets of the given dataset
    as in the following picture:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集的元素不一定需要聚集在一起形成一个有限集合，而聚类也可能包括给定数据集的无界子集，如下图所示：
- en: '![Unsupervised learning](img/00006.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/00006.jpeg)'
- en: Clustering is not the only unsupervised technique and we will see that deep
    learning's recent successes are related to it being so effective in unsupervised
    learning tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类并不是唯一的无监督技术，我们将看到深度学习最近的成功与其在无监督学习任务中如此有效有关。
- en: New data is created every day, very quickly, and labeling all the new data is
    quite a laborious and time-consuming activity. One advantage of unsupervised learning
    algorithms is that they do not need labeled data. Unsupervised deep learning techniques
    and methods, such as Restricted Boltzmann machines, work by abstracting features
    from the data. For example, using the MNIST dataset, Restricted Boltzmann machines
    will abstract characteristics that are unique to each digit, detecting the shape
    of the lines and curves for each digit. Unsupervised learning works by revealing
    hidden structures in the data that allow us to classify it accordingly, not by
    matching it to a label.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每天都会快速产生新数据，而对所有新数据进行标记是一项相当费力和耗时的活动。无监督学习算法的一个优点是它们不需要标记数据。无监督深度学习技术和方法，比如受限玻尔兹曼机，通过从数据中抽象特征来工作。例如，使用MNIST数据集，受限玻尔兹曼机将提取出对每个数字独特的特征，检测每个数字的线条和曲线的形状。无监督学习通过揭示数据中的隐藏结构来分类数据，而不是通过将其与标签进行匹配。
- en: In addition, for instance with deep belief nets, we can improve performance
    of an unsupervised method by refining it with supervised learning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，例如使用深度信念网络，我们可以通过用监督学习对其进行改进来改善无监督方法的性能。
- en: Reinforcement learning
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'The third class of machine learning techniques is called *reinforcement learning*.
    This works differently from supervised learning though it still uses a feedback
    element to improve its performance. A common application of reinforcement learning
    techniques is in teaching machines how to play games: in this case, we do not
    label each move as good or bad but the feedback comes from the game, either through
    the outcome of the game or through signals during the game, such as scoring or
    losing points. Winning a game will reflect a positive outcome, similar to recognizing
    the right digit or whether an e-mail is spam or not, while losing the game would
    require further "learning". Reinforcement learning algorithms tend to reuse actions
    tried in the past that led to successful results, like winning in a game. However,
    when in uncharted territory, the algorithm must try new actions from which, depending
    on the result, it will learn the structure of the game more deeply. Since usually,
    actions are inter-related, it is not the single action that can be valued as "good"
    or "bad", but rather it is the whole dynamics of actions together that is evaluated.
    Similar to how in playing chess sometimes sacrificing a pawn may be considered
    a positive action if it brings a better positioning on the chessboard, even though
    the loss of a piece is, in general, a negative outcome, in reinforcement learning
    it is the whole problem and its goal that is explored. For example, a moving cleaning
    robot may have to decide whether to continue cleaning a room or to start to move
    back to its recharging station, and such a decision could be made on the basis
    of whether in similar circumstances it was able to find the charging station before
    the battery ran out. In reinforcement learning, the basic idea is that of *reward,*
    where the algorithm will seek to maximize the total reward it receives.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术的第三类是*强化学习*。尽管它仍然使用反馈元素来提高性能，但它的工作方式与监督学习不同。强化学习技术的一个常见应用是教导机器如何玩游戏：在这种情况下，我们不会将每一步标记为好或坏，而是从游戏中获得反馈，要么通过游戏的结果，要么通过游戏过程中的信号，比如得分或失分。赢得游戏会反映为积极的结果，类似于识别正确的数字或电子邮件是垃圾邮件还是不是，而输掉游戏将需要进一步的“学习”。强化学习算法倾向于重复尝试过去导致成功结果的动作，就像在游戏中获胜一样。然而，在未知领域，算法必须尝试新的动作，根据结果，它将更深入地学习游戏的结构。因为通常，动作是相互关联的，因此不能将单个动作评价为“好”或“坏”，而是整体动作的动态评价。类似于在下棋时有时候牺牲一个兵可能被认为是积极的动作，如果它带来了更好的棋盘位置，尽管丢失一个棋子通常是一个负面的结果，在强化学习中，探索的是整个问题及其目标。例如，一个移动的清洁机器人可能必须决定是继续清洁房间还是开始回到充电站，并且这样的决定可以基于在类似情况下它是否能在电池耗尽之前找到充电站。在强化学习中，基本思想是*奖励*，算法将寻求最大化总奖励。
- en: A simple example of reinforcement learning can be used to play the classic game
    of tic-tac-toe. In this case, each position on the board has associated a probability
    (a value), which is the probability of winning the game from that state based
    on previous experience. At the beginning, each state is set at 50%, which is we
    assume that at the beginning we have an equal probability of winning or losing
    from any position. In general, the machine will try to move towards positions
    with higher values in order to win the game, and will re-evaluate them if, instead,
    it loses. At each position, the machine will then make a choice based on the probable
    outcome, rather than on a fixed determined rule. As it keeps playing, these probabilities
    will get refined and output a higher or lower chance of success depending on the
    position.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个简单示例可以用来玩经典的井字棋游戏。在这种情况下，棋盘上的每个位置都关联了一个概率（一个值），这是基于先前经验从该状态赢得游戏的概率。开始时，每个状态被设定为50%，这意味着在开始时我们假设从任何位置开始我们赢得或输掉的概率是相等的。一般来说，机器将尝试朝着价值更高的位置前进以赢得游戏，并且如果失败则重新评估它们。在每个位置，机器将基于可能的结果而做出选择，而不是基于固定的确定规则。随着继续进行游戏，这些概率将得到精炼，并根据位置输出更高或更低的成功机会。
- en: Steps Involved in machine learning systems
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习系统涉及的步骤
- en: 'So far, we have discussed different machine learning approaches, and we have
    roughly organized them in three different classes. Another important aspect of
    classical machine learning is understanding the data to better understand the
    problem at hand. The important aspects we need to define in order to apply machine
    learning can roughly be described as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了不同的机器学习方法，并且我们已经大致将它们组织在三种不同的类别中。另一个重要的方面是了解数据，以更好地理解手头的问题。我们需要定义的重要方面大致可以描述如下：
- en: '**Learner**: this represents the algorithm being used and its "learning philosophy".
    As we will see in the next paragraph, there are many different machine learning
    techniques that can be applied to different learning problems. The choice of learner
    is important, since different problems can be better suited to certain machine
    learning algorithms.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习者**：这代表着使用的算法及其"学习哲学"。正如我们将在下一段中看到的，有许多不同的机器学习技术可以应用于不同的学习问题。学习者的选择很重要，因为不同的问题可以更适合某些机器学习算法。'
- en: '**Training data**: This is the raw dataset that we are interested in. Such
    data may be unlabeled for unsupervised learning, or it may include labels for
    supervised learning. It is important to have enough sample data for the learner
    to understand the structure of the problem.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：这是我们感兴趣的原始数据集。这样的数据可能是未标记的，用于无监督学习，或者它可能包含标签，用于监督学习。确保学习者有足够的样本数据以理解问题的结构非常重要。'
- en: '**Representation**: This is how the data is expressed in terms of the features
    chosen so that it can be ingested by the learner. For example, if we are trying
    to classify digits using images, this will represent the array of values describing
    the pixels of the images. A good choice of representation of the data is important
    to achieve better results.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表示**：这是数据以所选特征的方式表达，以便学习者可以摄入的方式。例如，如果我们试图使用图像对数字进行分类，这将代表描述图像像素的值数组。良好的数据表示选择对于取得更好的结果是重要的。'
- en: '**Goal**: This represents the reason to learn from the data for the problem
    at hand. This is strictly related to the target, and helps define how and what
    learner should be used and what representation to use. For example, the goal may
    be to clean our mailbox of unwanted e-mails, and the goal defines what the target
    of our learner is, for example, detection of spam e-mails.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：这代表了从手头的问题中学习的原因。这与目标息息相关，并且有助于定义应该使用什么学习者和什么表示。例如，目标可能是清理我们的邮箱中不需要的邮件，目标定义了我们的学习者的目标，例如，检测垃圾邮件。'
- en: '**Target**: This represents what is being learned and the final output. It
    may be a classification of the unlabeled data, it may be a representation of the
    input data according to hidden patterns or characteristics, it may be a simulator
    for future predictions, it may be a response to an outside stimulus, or it can
    be a strategy in the case of reinforcement learning.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：这代表着正在学习和最终输出的内容。它可以是未标记数据的分类，可以是根据隐藏的模式或特征表示输入数据，可以是未来预测的模拟器，可以是对外部刺激的响应，也可以是强化学习中的策略。'
- en: It can never be emphasized enough, though, that any machine learning algorithm
    can only achieve an approximation of the target, not a perfect numerical description.
    Machine learning algorithms are not exact mathematical solutions to problems,
    rather they are just approximations. In the previous paragraph, we have defined
    learning as a function from the space of features (the input) into a range of
    classes; we will later see how certain machine learning algorithms, such as neural
    networks, can be proved to be able to approximate any function to any degree,
    in theory. This theorem is called the Universal Approximation Theorem, but it
    does not imply that we can get a precise solution to our problem. In addition,
    solutions to the problem can be better achieved by better understanding of the
    training data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何强调都不为过，任何机器学习算法只能达到目标的近似值，而不能得到完美的数值描述。机器学习算法不是问题的精确数学解，而只是近似值。在前面的段落中，我们已经将学习定义为从特征空间（输入）到类别范围的函数；我们将在后面看到，某些机器学习算法，如神经网络，理论上可以被证明能够近似地逼近任何函数到任何程度。这个定理被称为通用逼近定理，但这并不意味着我们可以得到问题的精确解。此外，通过更好地理解训练数据，可以更好地解决问题。
- en: 'Typically, a problem solvable with classic machine learning techniques may
    require a thorough understanding and cleaning of the training data before deployment.
    If we are to state some of the steps required in approaching a machine learning
    problem, we may summarize them as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用经典机器学习技术可解决的问题在部署之前可能需要对训练数据进行深入了解和清理。如果我们要陈述处理机器学习问题所需的一些步骤，我们可以总结如下：
- en: '**Data Collection**: This implies the gathering of as much data as possible
    and in the supervised learning problem also its correct labeling.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集**：这意味着尽可能收集尽可能多的数据，并且在监督学习问题中还要正确标记数据。'
- en: '**Data Processing**: This implies cleaning of the data (for example removing
    redundant or highly correlated features, or filling missing data) and understanding
    of the features defining the training data.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理**：这意味着清理数据（例如删除多余或高度相关的特征，或填充缺失数据），并且理解定义训练数据的特征。'
- en: '**Creation of the test case**: Typically data can be divided into two or three
    sets: a training dataset, on which we train the algorithm, and a testing dataset,
    on which we test, after having trained the algorithm, the accuracy of the approach.
    Often, we also create a validation dataset, on which, after the training-testing
    procedure has been repeated many times and we are finally satisfied with the result,
    we make the final testing (or validation).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试用例的创建**：通常数据可以分为两到三组：一个用于训练算法的训练数据集，一个用于在训练算法后测试方法准确性的测试数据集。通常情况下，我们还会创建一个用于最终测试（或验证）的验证数据集，经过多次训练-测试流程后，我们最终满意结果，进而做最终测试（或验证）。'
- en: There are valid reasons to create a testing and a validation dataset. As we
    mentioned, machine learning techniques can only produce an approximation of the
    desired result. This is due to the fact that often, we can only include a finite
    and limited number of variables, and there may be many variables that are outside
    our own control. If we only used a single dataset, our model may end up "memorizing"
    the data and produce an extremely high accuracy value on the data it has memorized,
    but this result may not be reproducible on other similar datasets. One of the
    key desired goals of machine learning techniques is their ability to generalize.
    That is why we create both a testing dataset, used for tuning our model selection
    after training, and a final validation dataset only used at the end of the process
    to confirm the validity of the selected algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有充分的理由创建一个测试集和一个验证集。正如我们所提到的，机器学习技术只能产生期望结果的近似值。这是因为通常情况下，我们只能包括有限数量的变量，而可能有许多变量是我们无法控制的。如果我们只使用一个数据集，我们的模型可能最终会“记住”数据，并在已记住的数据上产生极高的准确性值，但这个结果可能无法在其他类似的数据集上重现。机器学习技术的一个关键目标是其泛化能力。这就是为什么我们既创建了一个用于训练后模型选择调整的测试数据集，又创建了一个仅在流程结束时才用于确认所选算法有效性的最终验证数据集。
- en: 'To understand the importance of selecting valid features in the data and the
    importance of avoiding "memorizing" the data (in more technical terms, this is
    what is referred to as "overfitting" in the literature, and this is what we will
    be calling it from now on), let''s use a joke taken from an *xkcd* comic as an
    example ([http://xkcd.com/1122](http://xkcd.com/1122)): "Until 1996 no democratic
    US presidential candidate who was an incumbent and with no combat experience had
    ever beaten anyone whose first name was worth more in Scrabble". It is apparent
    in this example that such a "rule" is meaningless, however it underscores the
    importance of selecting valid features (does how much a name is worth in Scrabble
    bear any relevance to selecting a US president?) and that selecting random features
    as predictors, while it may predict the current data, cannot be used as a predictor
    for more general data, and that the fact that for 52 elections this had held true
    was simple coincidence. This is what is generally called overfitting, that is,
    making predictions that fit the data at hand perfectly, but do not generalize
    to larger datasets. Overfitting is the process of trying to make sense of what
    is generally called "noise" (that is, information that does not have any real
    meaning) and trying to fit the model to small perturbations.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解在数据中选择有效特征的重要性以及避免“记忆”数据的重要性（更技术性的术语是文献中所称的“过拟合”，从现在开始我们将使用这个术语），让我们用一个来自*xkcd*漫画的笑话来举例（[http://xkcd.com/1122](http://xkcd.com/1122)）："直到1996年，没有一个现任的民主党美国总统候选人，且没有军事经验，曾经打败过任何人名的首字母在Scrabble中价值更高的人"。很显然，这种“规则”在这个例子中是没有意义的，但它突出了选择有效特征的重要性（一个人的名字在Scrabble中价值多少与选择美国总统有关吗？），而选择随机特征作为预测因子，虽然可能会预测当前数据，但不能用作更一般数据的预测器，而52次选举中这一点成立只是一个简单的巧合。这通常被称为过拟合，也就是说，做出与手头数据完全吻合的预测，但不能推广到更大的数据集。过度拟合是试图理解通常所说的“噪音”（即，没有任何实际意义的信息）的过程，并尝试适应模型的小扰动。
- en: Another example may be given by attempting to use machine learning to predict
    the trajectory of a ball thrown from the ground up in the air (not perpendicularly)
    until it reaches the ground again. Physics teaches us that the trajectory is shaped
    like a parabola, and we expect that a good machine learning algorithm observing
    thousands of such throws would come up with a parabola as a solution. However,
    if we were to zoom in on the ball and observe the smallest fluctuations in the
    air due to turbulence, we might notice that the ball does not hold a steady trajectory
    but may be subject to small perturbations. This is what we call "noise". A machine
    learning algorithm that tried to model these small perturbations would fail to
    see the big picture, and produce a result that is not satisfactory. In other words,
    overfitting is the process that makes the machine learning algorithm see the trees
    but forget about the forest.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可以通过尝试使用机器学习来预测从地面抛出并上升到又落回地面的球的轨迹（不是垂直向上）。物理学告诉我们，轨迹呈抛物线状，我们期望一个好的机器学习算法观察到数千次这样的抛掷会得出一个抛物线作为解。然而，如果我们放大球并观察空气湍流中最小的波动，我们可能会注意到球不会保持稳定的轨迹，而可能受到小的扰动。这就是我们所称的“噪音”。试图模拟这些小扰动的机器学习算法将无法看到整体图景，并得出一个不令人满意的结果。换句话说，过拟合是使机器学习算法看到树木而忘记了整个森林的过程。
- en: '![Steps Involved in machine learning systems](img/00007.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习系统涉及的步骤](img/00007.jpeg)'
- en: A good prediction model vs. a bad (overfitted) prediction model of the trajectory
    of a ball thrown from the ground
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的预测模型与一个糟糕的（过拟合的）预测模型有着不同的轨迹，对于一个从地面上抛出的球的轨迹来说。
- en: 'This is why we separate the training data from the test data: if the accuracy
    of the test data were not similar to the result achieved on the training data,
    that would be a good indication that we have overfitted the model. Of course,
    we need to make sure we don''t make the opposite error either, that is, underfit
    the model. In practice, though, if we are aiming at making our prediction model
    as accurate as possible on our training data, underfitting is much less of a risk
    than overfitting, and most care is therefore taken in order to avoid overfitting
    the model.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们将训练数据与测试数据分开的原因：如果测试数据的准确率与在训练数据上获得的结果不相似，那么这将是一个很好的指示，表明我们已经过度拟合了模型。当然，我们也需要确保我们不犯相反的错误，也就是说，不要欠拟合模型。但是在实践中，如果我们的目标是尽可能地使我们的预测模型在我们的训练数据上尽可能准确，那么欠拟合比过拟合的风险要小得多，因此我们应尽量避免过度拟合模型。
- en: '![Steps Involved in machine learning systems](img/00008.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习系统中涉及的步骤](img/00008.jpeg)'
- en: Underfitting can be a problem as well
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合也可能是一个问题
- en: Brief description of popular techniques/algorithms
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流行技术/算法的简要描述
- en: Besides grouping algorithms based upon their "learning style", that is, the
    three classes discussed at the beginning of the book, supervised learning, unsupervised
    learning, and reinforcement learning, we can also group them by their implementation.
    Clearly, each class discussed above may be implemented using different machine
    learning algorithms, as, for example, there are many different supervised learning
    techniques, where each of them may be best suited to the specific classifying
    or regression task at hand. In fact, the distinction between classification and
    regression is one of the most critical to make, and it is important to understand
    what we are trying to accomplish.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了根据它们的"学习风格"将算法分组之外，即本书开头讨论的三类，监督学习、无监督学习和强化学习，我们还可以根据它们的实现将它们分组。显然，上面讨论的每个类别都可以使用不同的机器学习算法来实现，例如，有许多不同的监督学习技术，每种技术可能最适合于手头的特定分类或回归任务。事实上，分类和回归之间的区别是最关键的之一，并且理解我们试图完成的任务是很重要的。
- en: The following is by no means meant to be an exhaustive list or a thorough description
    of each machine learning method, for which we refer the reader to the book *Python
    Machine Learning*, Sebastian Raschka ([https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning)),
    rather it is meant as a simple review to provide the reader with a simple flavor
    of the different techniques and how deep learning differs from them. In the next
    chapters, we will see that deep learning is not just another machine learning
    algorithm, but it differs in substantial ways from classical machine learning
    techniques.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下绝不意味着是一个详尽的列表或对每种机器学习方法的彻底描述，对于这些，我们建议读者参考Sebastian Raschka的书籍*Python Machine
    Learning* ([https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning](https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning))，而是意味着作为一个简单的回顾，为读者提供不同技术的简单味道以及深度学习与它们的区别。在接下来的章节中，我们将看到深度学习不仅仅是另一种机器学习算法，而且在很多方面与传统的机器学习技术不同。
- en: We will introduce a regression algorithm, linear regression, classical classifiers
    such as decision trees, naïve Bayes, and support vector machine, and unsupervised
    clustering algorithms such as k-means, and reinforcement learning techniques,
    the cross-entropy method, to give only a small glimpse of the variety of machine
    learning techniques that exist, and we will end this list by introducing neural
    networks, that is the main focus of this book.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍一个回归算法，线性回归，经典的分类器，如决策树、朴素贝叶斯和支持向量机，以及无监督的聚类算法，如k均值，和强化学习技术，交叉熵方法，仅仅是对存在的各种机器学习技术的一个小小的了解，我们将通过介绍神经网络来结束这个列表，这是本书的主要焦点。
- en: Linear regression
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: Regression algorithms are a kind of supervised algorithm that use features of
    the input data to predict a value, for example the cost of a house given certain
    features such as size, age, number of bathrooms, number of floors, location, and
    so on. Regression analysis tries to find the value of the parameters for the function
    that best fits an input dataset. In a linear regression algorithm, the goal is
    to minimize a cost function by finding appropriate parameters for the function
    on the input data that best approximate the target values. A cost function is
    a function of the error, which is how far we are from getting a correct result.
    A typical cost function used is the mean squared error, where we take the square
    of the difference between the expected value and the predicted result. The sum
    over all the input examples gives the error of the algorithm and it represents
    the cost function.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 回归算法是一种监督算法，它使用输入数据的特征来预测一个值，例如给定某些特征（如大小、年龄、浴室数量、楼层数、位置等）可以预测出房屋的价格。回归分析试图找到最能拟合输入数据集的函数参数值。在线性回归算法中，目标是通过找到适当的函数参数来最小化成本函数，使其最好地逼近目标值。成本函数是一个错误的函数，它表示我们离正确结果有多远。通常使用的成本函数是均方误差，其中我们计算预期值与预测结果之间的差的平方。对所有输入示例的总和给出了算法的错误值，它代表了成本函数。
- en: Say we have a 100-square meter house, built 25 years ago, with three bathrooms,
    and two floors. Furthermore, assume that we divide the city, where the houses
    are in 10 different areas, that we denote with integers from 1 to 10, and say
    this house is located in the area denoted by a 7\. We can then parameterize this
    house with a 5-dimensional vector *x = (100, 25, 3, 2, 7)*. Say that we also know
    that this house has an estimated value of €10,0000\. What we want to achieve is
    to create a function *f* such that *f(x) = 100000*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一所100平方米的房子，建于25年前，有三个浴室，两层。此外，假设我们将房子所在的城市分成10个不同的区域，用从1到10的整数表示，假设这所房子位于编号为7的区域。我们可以用一个5维向量
    *x = (100, 25, 3, 2, 7)* 来对这所房子进行参数化。假设我们还知道这所房子的估计价值为€10,0000。我们想要实现的是创建一个函数
    *f*，使得 *f(x) = 100000*。
- en: In linear regression, this means finding a vector *w= (w*[1]*, w*[*2*]*, w*[*3*]*,
    w*[*4*]*, w*[*5*]*)* such that *100*w*[*1*] *+ 25*w*[*2*] *+ 3*w*[*3*] *+ 2*w*[*4*]
    *+ 7*w*[*5*] *= 100000*. If we had a thousand houses, we could repeat the same
    process for every house, and ideally we would like to find a vector *w* that can
    predict the correct value (or close enough) for every house. Let's say that we
    initially pick some random value of *w.* In that case, we won't expect *f(x) =
    100*w*[*1*] *+ 25*w*[*2*] *+ 3*w*[*3*] *+ 2*w*[*4*] *+ 7*w*[5] to be equal to
    1,00,000, so we can calculate the error ∆ = (100000 − *f(x))*[*2*]. This is the
    squared error for one example *x,* and the mean of all the squared errors for
    all the examples represents our cost, that is, how much our function differs from
    the real value. Our aim is therefore to minimize this error, and to do so we calculate
    the derivative δ of the cost function with respect to *w*. The derivative indicates
    the direction where the function increases (or decreases), therefore, moving *w*
    in the opposite direction to the derivative will improve our function's accuracy.
    This is the main point of linear regression, moving towards the minimum of the
    cost function, which represents the error. Of course, we need to decide how fast
    we want to move in the direction of the derivative, as our derivative only indicates
    a direction. The cost function is not linear, therefore we need to make sure we
    only take small steps in the direction indicated by the derivative. Taking too
    large a step may possibly make us overshoot our minimum, and therefore not be
    able to converge to it. The magnitude of this step is what is called the *learning
    rate*, and lets us indicate its magnitude with the symbol "*lr*".
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，这意味着找到一个向量*w =（w*[1]*，w*[*2*]*，w*[*3*]*，w*[*4*]*，w*[*5*]*）*，使*100*w*[*1*]*
    + 25*w*[*2*]* + 3*w*[*3*]* + 2*w*[*4*]* + 7*w*[*5*]* = 100000*。如果我们有一千幢房子，我们可以为每栋房子重复相同的过程，理想情况下，我们希望找到一个*
    w*向量，可以为每栋房子预测正确的值（或足够接近）。假设我们最初选择了一些* w*的随机值。在这种情况下，我们不希望* f(x)* = 100* w*[*1*]*
    + 25* w*[*2*]* + 3* w*[*3*]* + 2* w*[*4*]* + 7* w*[*5*]* 等于1,00,000，因此我们可以计算误差∆
    =（100000 − * f(x) *）*[*2*]*。这是一个示例* x*的均方误差，所有示例的均方误差的均值代表了我们的成本，也就是我们的函数与真实值的差异有多少。因此，我们的目标是最小化此误差，为此我们计算成本函数相对于*
    w *的导数δ。导数指示函数增加（或减少）的方向，因此，把* w *移动到导数的相反方向会提高我们函数的准确性。这是线性回归的要点，向着成本函数的最小值移动，这代表了错误。当然，我们需要决定我们想以多快的速度沿着导数指示的方向移动，因为我们的导数只指明了一个方向。成本函数不是线性的，因此我们需要确保只在导数指示的方向上迈出小步。迈出太大的一步可能会导致我们超过最小值，因此无法收敛到最小值。这一步的大小就是所谓的*学习率*，让我们用符号“*lr*”表示它的大小。
- en: By setting *w = w -* δ**lr,* we are therefore improving the choice of *w* towards
    a better solution. Repeating this process many times will produce a value of *w*
    that represents the best possible choice for the function *f.* We should emphasize,
    however, that this process will only work locally, and it may not find a global
    best value if the space is not convex. As the image suggests, if many local minima
    exist, the algorithm may end up being stuck in one of these local minima and not
    be able to escape it to reach the global minimum of the error function, similar
    to how a small ball, moving down from a mountain, may get stuck in a small valley
    and never reach the bottom of the mountain.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置*w = w -* δ**lr,* 我们因此改善了*w*向更好解决方案的选择。重复这个过程多次将产生表示* f*的最佳可能选择的*w*值。然而，我们应该强调，如果空间不是凸的，这个过程只在局部起作用，并且可能无法找到全局的最佳值。正如图像所暗示的，如果存在许多局部最小值，算法可能最终会被困在其中一个局部最小值中，无法逃脱到达误差函数的全局最小值，类似于一个小球从山上滚下来，可能会卡在一个小山谷中，无法到达山底。
- en: '![Linear regression](img/00009.jpeg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/00009.jpeg)'
- en: The top graph is convex, and therefore there exists just one minimum. In the
    bottom graph the function has two local minima, and therefore, depending on the
    initialization, the process may find the first local minimum that is not the global
    minimum.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部图是凸的，因此只存在一个最小值。在底部图中，函数有两个局部最小值，因此，取决于初始化，该过程可能会找到第一个不是全局最小值的局部最小值。
- en: Decision trees
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树
- en: Another widely used supervised algorithm is the decision tree algorithm. A decision
    tree algorithm creates a classifier in the form of a "tree". A decision tree is
    comprised of decision nodes where tests on specific attributes are performed,
    and leaf nodes that indicate the value of the target attribute. Decision trees
    are a type of classifier that works by starting at the root node and moving down
    through the decision nodes until a leaf is reached.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的监督算法是决策树算法。决策树算法创建一个“树”形式的分类器。决策树由决策节点组成，其中对特定属性进行测试，以及指示目标属性值的叶节点。决策树是一种分类器，通过从根节点开始并通过决策节点向下移动直到达到叶子节点来工作。
- en: A classic application of this algorithm is the Iris flower dataset ([http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris))
    that contains data from 50 samples of three types of Irises (Iris setosa, Iris
    virginica, and Iris versicolor). Ronald Fisher, who created the dataset, measured
    four different features of these flowers, the length and width of their sepals
    and the length and width of their petals. Based on the different combinations
    of these features, it is possible to create a decision tree to decide to which
    species each flower belongs. We will here describe a simple simplified decision
    tree that will classify correctly, almost all the flowers only using two of these
    features, the petal length and width.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的一个经典应用是鸢尾花数据集（[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)），其中包含来自三种鸢尾花（山鸢尾、维吉尼亚鸢尾和变色鸢尾）的50个样本的数据。创建数据集的罗纳德·费舍尔测量了这些花的四种不同特征，即它们的萼片的长度和宽度，以及它们的花瓣的长度和宽度。根据这些特征的不同组合，可以创建一个决策树来决定每朵花属于哪个物种。在这里，我们将描述一个简单的简化决策树，它将只使用这两个特征，花瓣的长度和宽度，就能正确分类几乎所有的花。
- en: 'We start with the first node and we create the first test on petal length:
    if the petal length is less than 2.5, then the flower belongs to the Iris setosa
    species. This, in fact, classifies correctly, all the setosa flowers, which all
    have a petal length less than 2.5 cm. Therefore, we reach a leaf node, which is
    labeled by the outcome Iris setosa. If the petal length is greater than 2.5, we
    take a different branch and we reach a new decision node, and we test whether
    the petal width is larger than 1.8\. If the petal width is larger or equal to
    1.8, we reach a leaf node and we classify our flower as an Iris virginica, otherwise
    we reach a new decision node, where again we test whether the petal length is
    longer than 4.9\. If it is, we reach a leaf node labeled by the Iris virginica
    flower, otherwise we reach another leaf node, labeled by the Iris versicolor flower.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从第一个节点开始，我们对花瓣长度进行第一个测试：如果花瓣长度小于2.5，则该花属于山鸢尾物种。事实上，这将正确地对所有花瓣长度小于2.5厘米的山鸢尾花进行分类。因此，我们到达了一个标记为山鸢尾的叶子节点。如果花瓣长度大于2.5，我们选择另一条分支，我们到达一个新的决策节点，并测试花瓣宽度是否大于1.8。如果花瓣宽度大于或等于1.8，则我们到达一个叶节点，并将我们的花分类为维吉尼亚鸢尾，否则我们到达一个新的决策节点，在这里我们再次测试花瓣长度是否大于4.9。如果是，则我们到达一个标记为维吉尼亚鸢尾花的叶子节点，否则我们到达另一个标记为变色鸢尾花的叶子节点。
- en: 'The decision tree discussed can be shown as follows, where the left branch
    reflects the positive answer to the test in the decision node, while the right
    branch represents the negative answer to the test in the decision node. The end
    nodes for each branch are the leaf nodes:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所讨论的决策树可以表示如下，其中左分支反映了决策节点中测试的积极答案，而右分支表示决策节点中测试的否定答案。每个分支的末端节点都是叶节点。
- en: '![Decision trees](img/00010.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/00010.jpeg)'
- en: This example shows how very different the decision tree algorithm is from linear
    regression. In addition, when we introduce neural nets, the reader will be able
    to see an example of how neural nets work by using this same dataset. In that
    example, we will also provide Python code and we will show a few pictures of how
    neural nets will try to separate the flowers based on their features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了决策树算法与线性回归有多么不同。此外，当我们介绍神经网络时，读者将能够通过使用相同的数据集来看到神经网络是如何工作的一个示例。在那个例子中，我们还将提供Python代码，并展示一些神经网络将如何基于它们的特征来尝试分离花朵的图片。
- en: K-means
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-means
- en: Clustering algorithms, as we have already discussed, are a type of unsupervised
    machine learning method. The most common clustering technique is called k-means
    clustering and is a clustering technique that groups every element in a dataset
    by grouping them into k distinct subsets (hence the *k* in the name). K-means
    is a relatively simple procedure, and consists of choosing random k points that
    represent the distinct centers of the k subsets, which are called centroids. We
    then select, for each centroid, all the points closest to it. This will create
    k different subsets. At this point, for each subset, we will re-calculate the
    center. We have again, *k* new centroids, and we repeat the steps above, selecting
    for each centroid, the new subsets of points closest to the centroids. We continue
    this process until the centroids stop moving.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们已经讨论过的，聚类算法是一种无监督的机器学习方法。最常见的聚类技术是称为 k-means 聚类的技术，它是一种将数据集中的每个元素分组到 k 个不同子集中的聚类技术（因此在名称中有
    *k*）。K-means 是一个相对简单的过程，包括选择代表 k 个不同子集的随机 k 点，称为中心。然后，我们为每个中心选择所有最接近它的点。这将创建 k
    个不同的子集。此时，对于每个子集，我们将重新计算中心。我们再次有 *k* 个新中心，然后重复上述步骤，为每个中心选择与中心最接近的新子集。我们继续这个过程，直到中心停止移动。
- en: 'It is clear that for this technique to work, we need to be able to identify
    a metric that allows us to calculate the distance between points. This procedure
    can be summarized as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，为了使这种技术起作用，我们需要能够识别一种允许我们计算点之间距离的度量标准。此过程可以总结如下：
- en: Choose initial k-points, called the centroids.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择初始 k 点，称为中心点。
- en: To each point in the dataset, associate the closest centroid.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集中的每个点与最近的中心关联起来。
- en: Calculate the new center for the sets of points associated to a particular centroid.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算与特定中心点关联的点集的新中心。
- en: Define the new centers to be the new centroids.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新中心定义为新的中心。
- en: Repeat steps 3 and 4 until the centroids stop moving.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 3 和 4，直到中心停止移动。
- en: It is important to notice that this method is sensitive to the initial choice
    of random centroids, and that it may be a good idea to repeat the process for
    different initial choices. Also, it is possible for some of the centroids not
    to be closest to any of the points in the dataset, reducing the number of subsets
    down from k. It is also worth mentioning that if we used k-means with *k=3* in
    the above example discussing decision trees, we may not be getting the same classification
    for the iris dataset that we found using decision trees, highlighting once more
    how important it is to carefully choose and use the correct machine learning method
    for each problem.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，此方法对随机中心的初始选择非常敏感，可能重复使用不同的初始选择是个好主意。此外，某些中心可能不是数据集中任何点最近的中心，将子集数目减少到
    k。值得一提的是，如果我们在上面讨论决策树的例子中使用 *k=3* 的 k-means，我们可能不会得到与使用决策树找到的鸢尾花数据集相同的分类，再次强调了如何重要地为每个问题选择和使用正确的机器学习方法。
- en: Now, let's discuss a practical example that uses k-means clustering. Let's say
    a pizza delivery place wants to open four new franchises in a new city, and they
    need to choose the location for the four new sites. This is a problem that can
    be solved easily using k-means clustering. The idea is to find the locations where
    pizza is ordered most often; these will be our data points. Next, we choose four
    random points where the site locations will be located. By using k-means clustering
    techniques, we can later identify the four best locations that minimize the distance
    to each delivery place. This is an example where k-means clustering can help solve
    a business problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一个使用 k-means 聚类的实际示例。假设一个披萨外卖店想要在一个新城市开设四家新的连锁店，他们需要选择四个新店的位置。这是一个可以通过
    k-means 聚类轻松解决的问题。其思想是找到最常订购披萨的地点；这些地点将成为我们的数据点。接下来，我们选择四个随机点作为站点位置。通过使用 k-means
    聚类技术，我们可以后来确定最佳位置，以最小化到每个送货地点的距离。这是一个 k-means 聚类可以帮助解决业务问题的例子。
- en: '![K-means](img/00011.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![K-means](img/00011.jpeg)'
- en: On the left the distribution of points where pizza is delivered most often.
    On the right, the round points indicate where the new franchises should be located
    and their corresponding delivery areas
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 左边是披萨最常送达的点的分布。右边是圆点表示新的连锁店应该位于的位置及其相应的送货区域
- en: Naïve Bayes
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naïve Bayes is different from many other machine learning algorithms. Probabilistically,
    what most machine learning techniques try to evaluate is the probability of a
    certain event *Y* given conditions *X*, which we denote by *p(Y|X)*. For example,
    given the picture representing a digit (that is, a picture with a certain distribution
    of pixels), what is the probability that that number is 5? If the pixels' distribution
    is such that it is close to the pixel distribution of other examples that were
    labeled as 5, the probability of that event will be high, otherwise the probability
    will be low.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯不同于许多其他机器学习算法。从概率上讲，大多数机器学习技术试图评估的是在给定条件*X*下某个事件*Y*的概率，我们用*p(Y|X)*表示。例如，给定描述数字的图片(即，具有特定像素分布的图片)，那么该数字为5的概率是多少？如果像素的分布接近于其他标记为5的示例的像素分布，那么该事件的概率将很高，否则概率将很低。
- en: 'Sometimes we have the opposite information, that is, given we know that we
    have an event Y, we know the probability that our sample is *X*. The Bayes theorem
    states: *p(X|Y) = p(Y|X)*p(X)/p(Y)*, where *p(X|Y)* means the probability of generating
    instance X given Y, which is also why naïve Bayes is called a generative approach.
    In simple terms, we may calculate the probability that a certain pixel configuration
    represents the number 5, knowing what is the probability, given that we have a
    5, that a random pixel configuration may match the given one.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们具有相反的信息，也就是说，我们知道我们有一个事件*Y*的情况下，我们知道样本为*X*的概率。贝叶斯定理说明：*p(X|Y) = p(Y|X)*p(X)/p(Y)*，其中*p(X|Y)*表示在知道*Y*的情况下生成实例*X*的概率，这也是为什么朴素贝叶斯被称为生成方法。简而言之，我们可以计算某个像素配置代表数字5的概率，知道在有5的情况下，即使随机像素配置可能与给定的一致，这是为什么。
- en: 'This is best understood in the realm of medical testing. Let''s say we test
    for a specific disease or cancer. We want to know what is the probability we may
    have a particular disease given that our test result was positive. Now, most tests
    have a reliability value, which is the percentage chance of the test being positive
    when administered on people with the particular disease. By reversing the expression
    *p(X|Y) = p(Y|X)*p(X)/p(Y)*, we have that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这在医学测试领域是最容易理解的。假设我们针对特定疾病或癌症进行测试。我们想知道在我们测试结果为阳性的情况下，我们可能患有特定疾病的概率是多少。现在，大多数测试都有可靠性值，这是在针对患有特定疾病的人群时测试呈阳性的概率。通过颠倒表达式*p(X|Y)
    = p(Y|X)*p(X)/p(Y)*，我们有：
- en: '*p(cancer | test=positive) = p(test=positive | cancer) * p(cancer)/p(test=positive)*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(癌症 | 测试=阳性) = p(测试=阳性 | 癌症) * p(癌症)/p(测试=阳性)*'
- en: Assume that the test is 98% reliable, which means that in 98% of the cases,
    if the person has cancer, the test is positive, and likewise, if the person does
    not have cancer, the test result is negative. Also assume that this particular
    kind of cancer only affects older people, and only 2% of people below 50 have
    this kind of cancer, and the test administered on people under 50 is positive
    only on 3.9% of the population (we could have derived this fact from the data,
    but for simplicity we provide this information).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 假设测试的可靠率为98%，这意味着在98%的情况下，如果一个人患有癌症，那么测试结果为阳性，同样地，如果一个人没有癌症，测试结果将为阴性。还假设这种特定的癌症只影响年龄较大的人，50岁以下的人中只有2%的人患有这种癌症，对于这部分人群进行测试，结果呈阳性的概率仅占到人群的3.9%（我们可以从数据中推导出这个事实，但为简单起见，我们提供这些信息）。
- en: 'We could ask this question: if a test is 98% accurate for cancer and a 45-year-old
    person took the test, and it turned out to be positive, what is the probability
    that he/she may have cancer? Using the formula above we can calculate:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会提出这样一个问题：如果一项癌症测试的准确率为98%，45岁的人接受了测试，并且测试结果呈阳性，那么他/她可能患癌症的概率是多少？使用上述公式，我们可以计算：
- en: '*p(cancer | test=positive) = 0.98 * 0.02/0.039 = 0.50*'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(癌症 | 测试=阳性) = 0.98 * 0.02/0.039 = 0.50*'
- en: So, despite the high accuracy of the test, naïve Bayes tells us we also need
    to take into account the fact that the cancer is quite rare under 50, therefore
    the positivity of the test alone does not give a 98% probability of cancer. The
    probability *p(cancer)*, or more in general the probability p for the outcome
    we are trying to estimate, is called the prior probability, because it represents
    the probability of the event without any additional information, therefore before
    we took the test.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管测试的准确性很高，朴素贝叶斯告诉我们我们还需要考虑这样一个事实：50岁以下罹患癌症的情况相当罕见，因此仅仅通过测试结果的阳性并不能给出98%的癌症患病概率。概率*p(癌症)*，或者更一般地说是我们试图估计的结果的概率p，被称为先验概率，因为它代表了没有任何额外信息的事件概率，也就是在我们进行测试之前的概率。
- en: 'At this point, we may wonder what would happen if we had more information,
    for example if we performed a different test with different reliability, or knew
    some information about the person such as recurrence of cancer in the family.
    In the preceding equation we used, as one of the factors in the computation, the
    probability *p(test=positive | cancer)*, and if we performed a second test, and
    it came positive, we would also have *p(test2=positive | cancer)*. The naïve Bayes
    technique makes the assumption that each piece of information is independent of
    each other (this means that the outcome of test 2 did not know about the outcome
    of test 1 and it was independent of it, that is, taking test 1 could not change
    the outcome of test 2, and therefore its result was not biased by the first test).
    naïve Bayes is a classification algorithm that assumes the independence of different
    events to calculate their probability. Hence:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可能会想知道如果我们有更多信息会发生什么，例如如果我们使用了可靠性更高的不同测试，或者了解了一些人的信息，例如家族中的癌症复发情况。在上述方程中，我们使用了作为计算因素之一的概率
    *p(test=positive | cancer)*，如果我们进行第二个测试，并且结果也为正，我们也会有 *p(test2=positive | cancer)*。朴素贝叶斯技术假设每个信息片段都是独立的（这意味着第二个测试的结果不知道第一个测试的结果，并且它与第一个测试无关，即进行第一个测试不能改变第二个测试的结果，因此它的结果不受第一个测试的影响）。朴素贝叶斯是一种分类算法，它假设不同事件之间是相互独立的，以计算它们的概率。因此：
- en: '*p(test1 and test2=pos | cancer) =p(test1=pos | cancer)*p(test2=pos | cancer)*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*= p(当第一个测试和第二个测试都为正时，癌症的条件概率) =p(当第一个测试为正时，癌症的条件概率)*p(当第二个测试为正时，癌症的条件概率)*'
- en: This equation is also called the likelihood *L(test1 and test2 = pos)* that
    *test1* and *test2* be positive given the fact that the person does have cancer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程也称为在人确实患有癌症的情况下，*test1*和*test2*为阳性的概率 *L(test1和test2 = pos)*。
- en: 'We can then rewrite the equation as:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将方程重写为：
- en: '*p(cancer | both tests=pos) =*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*= p(癌症在两个测试都为正时的条件概率) =*'
- en: '*= p(both test=pos | cancer)*p(cancer)/p(both tests=pos) =*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*= p(当两个测试为正时，癌症的条件概率)*p(癌症)/p(两个测试都为正)* =*'
- en: '*= p(test1=pos | cancer)*p(test2=pos | cancer) *p(cancer)/p(both tests=pos)*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*= p(当第一个测试为正时，癌症的条件概率)*p(当第二个测试为正时，癌症的条件概率)*p(癌症)/p(两个测试都为正)*'
- en: Support vector machines
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Support vector machines is a supervised machine learning algorithm mainly used
    for classification. The advantage of support vector machines over other machine
    learning algorithms is that not only does it separate the data into classes, but
    it does so finding a separating hyper-plane (the analog of a plane in a space
    with more than three dimensions) that maximizes the margin separating each point
    from the hyper-plane. In addition, support vector machines can also deal with
    the case when the data is not linearly separable. There are two ways to deal with
    non-linearly separable data, one is by introducing soft margins, and another is
    by introducing the so-called kernel trick.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机是一种用于分类的监督机器学习算法。支持向量机相对于其他机器学习算法的优势在于，它不仅将数据分成类别，而且还通过寻找最大化将每个点与超平面（多于三个维度的空间中的平面的类比）分离的分隔超平面来实现这一目标。此外，支持向量机还可以处理数据不是线性可分的情况。处理非线性可分数据的方法有两种，一种是引入软边界，另一种是引入所谓的核技巧。
- en: Soft margins work by allowing a few miss-classified elements while retaining
    most predictive ability of the algorithm. As we have discussed above, in practice
    it is always better not to overfit any machine learning model, and we could do
    so by relaxing some of the support vector machine hypotheses.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 松弛边界在保留算法大部分预测能力的同时允许存在少量错误分类的元素。正如我们上面所讨论的，在实践中，最好不要过度拟合任何机器学习模型，我们可以通过放宽一些支持向量机的假设来实现这一点。
- en: 'The kernel trick instead involves mapping the space of features into another
    space where we can define a hyper-plane that, when mapped back into the space
    of features, is not a linear hyper-plane anymore, allowing to separate elements
    in the dataset that do not appear to be separable. Since this book will be mainly
    concerned with deep learning, we will not discuss in detail how support vector
    machines are implemented, that may take too much time, but rather want to emphasize
    the concept that support vector machines have been quite popular and effective
    thanks to their ability to generalize to non-linear situations. As we have seen
    before, the task of a supervised machine learning algorithm is to find a function
    from the space of features to a set of classes. Each input *x= (x*[*1*]*, x*[2]*,
    …, x*[n]) represents an input example, and each *x[i]* represents the value of
    x for the *i*^(th) feature. Earlier on we gave, as an example, trying to estimate
    the resell value of a certain house depending on some features, like number of
    bathrooms or location. If the *i*th feature corresponds to the number of bathrooms,
    *x[i]* would correspond to the number of bathrooms present in house *x*. We can
    create a function *k* from the space of features to a different representation
    of this space, called a kernel: for example *k* may map *x*[i] into *(x*[i]*)*²,
    and in general map the space of features non-linearly into another space *W*.
    So, a separating hyper-plane in *W*, can be mapped back into the space of features,
    where it would not be a linear hyper-plane anymore. The exact conditions under
    which this is true are well defined but beyond the scope of this short introduction.
    However, this again highlights the importance of the choice of correct features
    in classical machine learning algorithms, a choice that can allow finding the
    solution to specific problems.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，核技巧涉及将特征空间映射到另一个空间，在该空间中我们可以定义一个超平面，将其映射回特征空间时，它不再是一个线性超平面，从而允许分离数据集中看似不可分离的元素。由于本书主要关注深度学习，我们不会详细讨论支持向量机的实现方式，这可能会花费太多时间，而是要强调支持向量机之所以非常流行和有效的概念，这要归功于它们能够推广到非线性情况。正如我们之前所见，监督式机器学习算法的任务是找到从特征空间到一组类别的函数。每个输入*x=
    (x*[*1*]*, x*[2]*, …, x*[n])表示一个输入示例，而每个*x[i]*表示第*i*个特征的值。早些时候我们举了一个例子，试图根据一些特征（如浴室数量或位置）来估计某个房屋的转售价值。如果第*i*个特征对应于浴室数量，则*x[i]*将对应于房屋*x*中存在的浴室数量。我们可以从特征空间创建一个函数*k*到这个空间的另一个表示，称为核：例如，*k*可以将*x*[i]映射到*(x*[i]*)*²，并且通常将特征空间非线性地映射到另一个空间*W*。因此，在*W*中的分离超平面可以映射回特征空间，其中它将不再是线性超平面。这个条件确切的定义在哪些情况下成立是明确定义的，但超出了这个简短介绍的范围。然而，这再次突显了在经典机器学习算法中选择正确特征的重要性，这样的选择可以帮助找到特定问题的解决方案。
- en: '![Support vector machines](img/00012.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/00012.jpeg)'
- en: On the left a non-linearly separable set before the kernel was applied. On the
    right the same dataset after the kernel has been applied and the data can be linearly
    separated
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是应用核之前的非线性可分数据集。右侧是应用了核之后的相同数据集，数据可以线性分离。
- en: The cross-entropy method
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉熵方法
- en: So far, we have introduced supervised and unsupervised learning algorithms.
    The cross-entropy method belongs, instead, to the reinforcement learning class
    of algorithms, which will be discussed in great detail in [Chapter 7](part0046_split_000.html#1BRPS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 7. Deep Learning for Board Games"), *Deep Learning for Board Games* and
    [Chapter 8](part0057_split_000.html#1MBG21-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 8. Deep
    Learning for Computer Games"), *Deep Learning for Computer Games* of this book.
    The cross-entropy method is a technique to solve optimization problems, that is,
    to find the best parameters to minimize or maximize a specific function.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们介绍了监督学习和无监督学习算法。相反，交叉熵方法属于强化学习类算法，将在本书的[第7章](part0046_split_000.html#1BRPS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第7章 棋类游戏的深度学习")和[第8章](part0057_split_000.html#1MBG21-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第8章 电子游戏的深度学习")中详细讨论，《棋类游戏的深度学习》和《电子游戏的深度学习》。交叉熵方法是解决优化问题的技术，即找到最佳参数以最小化或最大化特定函数。
- en: 'In general, the cross-entropy method consists of the following phases:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，交叉熵方法包括以下阶段：
- en: Generate a random sample of the variables we are trying to optimize. For deep
    learning these variables might be the weights of a neural network.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成我们试图优化的变量的随机样本。对于深度学习，这些变量可能是神经网络的权重。
- en: Run the task and store the performance.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行任务并存储性能。
- en: Identify the best runs and select the top performing variables.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别最佳运行并选择性能最佳的变量。
- en: Calculate new means and variances for each variable, based on the top performing
    runs, and generate a new sample of the variables.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据表现最佳的运行，计算每个变量的新均值和方差，并生成变量的新样本。
- en: Repeat steps until a stop condition is reached or the system stops improving.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤，直到达到停止条件或系统停止改进。
- en: Suppose we are trying to solve for a function that depends on many variables,
    for example we are trying to build a model plane that can fly the longest when
    launched from a specific altitude. The distance that the plane covers will be
    a function of the size of its wings, their angle, the weight, and so on. Each
    time, we can record each variable and then launch the plane and measure the distance
    it flies. However, rather than trying all possible combinations, we create statistics,
    we select the best and worst runs, and we note at what values the variables were
    set during the best runs and during the worst runs. For example, if we detect
    that for each of the best runs the plane had wings of a specific size, we can
    conclude that that particular size may be optimal for the plane to fly a long
    distance. Conversely, if for each of the worst runs, the plane's wings were at
    a certain angle, we would conclude that that particular angle would be a bad choice
    for our plane's wings. In general, we will produce a probability distribution
    for each value that should produce the optimal plane, probabilities that are not
    random anymore, but based on the feedback we have received.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在尝试解决一个依赖于许多变量的函数，例如我们正在尝试建立一个模型飞机，当从特定高度发射时，它能够飞得最远。飞机飞行的距离将取决于其机翼的大小、它们的角度、重量等。每次，我们都可以记录每个变量，然后发射飞机并测量它飞行的距离。但是，我们不是尝试所有可能的组合，而是创建统计数据，我们选择最佳和最差的运行，并注意在最佳运行和最差运行期间变量设置的值。例如，如果我们发现每次最佳运行飞机都具有特定大小的机翼，我们可以得出结论，该特定大小可能对于飞机飞行很远是最优的。相反，如果对于每次最差运行，飞机的机翼都处于某个角度，我们会得出结论，该特定角度对于我们飞机的机翼是一个不好的选择。总的来说，我们将为应该产生最佳飞机的每个值产生一个概率分布，这些概率不再是随机的，而是基于我们收到的反馈。
- en: This method, therefore, uses the feedback from the run (how far the plane has
    flown) to determine the best solution to the problem (the value for each variable)
    in a typical reinforcement learning process.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该方法利用运行的反馈（飞机飞了多远）来确定问题的最佳解决方案（每个变量的值），这是典型的强化学习过程。
- en: Neural networks
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络
- en: After having refreshed the reader with some of the popular classical machine
    learning algorithms, we will now introduce neural networks, and explain in deeper
    detail how they work and how they differ from the algorithms we have briefly summarized.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在给读者带来一些流行的经典机器学习算法之后，我们现在将介绍神经网络，并深入解释它们的工作原理以及它们与我们简要总结的算法的区别。
- en: Neural networks are another machine learning algorithm and they have known periods
    of high popularity and periods during which they were rarely used. Understanding
    neural networks, to which we will dedicate the next and following chapters, is
    indeed key for following the content of this book.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是另一种机器学习算法，它们有着高人气的时期和极少被使用的时期。理解神经网络，我们将在接下来的几章中专门介绍，确实是理解本书内容的关键。
- en: The first example of a neural network was called the perceptron, which was invented
    by Frank Rosenblatt in 1957\. The perceptron is a network comprised of only an
    input and an output layer. In case of binary classifications, the output layer
    has only one neuron or unit. The perceptron seemed to be very promising from the
    start, though it was quickly realized that it could only learn linearly separable
    patterns. For example, Marvin Minsky and Seymour Papert showed that it could not
    learn the XOR logical function. In its most basic representations, perceptrons
    are just simple representations of one neuron and its input, input that can be
    comprised of several neurons.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个神经网络的例子被称为感知机，它是由弗兰克·罗森布拉特于1957年发明的。感知机是一个由输入层和输出层组成的网络。在二元分类的情况下，输出层只有一个神经元或单元。一开始，感知机似乎非常有前途，但很快就发现它只能学习线性可分的模式。例如，马文·明斯基和西摩·帕普特证明它无法学习异或逻辑函数。在其最基本的表示中，感知机只是一个神经元及其输入的简单表示，输入可以由多个神经元组成。
- en: Given different inputs into a neuron, we define an activation value by the formula
    ![Neural networks](img/00013.jpeg), where *x* [*i*] is the value for the input
    neuron, while *w* [*i*] is the value of the connection between the neuron *i*
    and the output. We will learn this in much deeper detail in the next chapter,
    for now we should just notice that perceptrons share many similarities with logistic
    regression algorithms, and are constrained by linear classifiers as well. If the
    activation value, which should be thought of as the neuron internal state, is
    greater than a fixed threshold *b*, then the neuron will activate, that is, it
    will fire, otherwise it will not.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 给定不同的输入到一个神经元，我们通过公式![神经网络](img/00013.jpeg)定义激活值，其中*x* [*i*]是输入神经元的值，而*w* [*i*]是神经元
    *i* 和输出之间的连接值。我们将在下一章节中深入学习这个公式，现在我们只需要注意感知机与逻辑回归算法有许多相似之处，并且也受到线性分类器的限制。如果激活值（被视为神经元的内部状态）大于固定阈值
    *b*，则神经元将被激活，即会发生激活，否则不会。
- en: '![Neural networks](img/00014.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/00014.jpeg)'
- en: A simple perceptron with three input units (neurons) and one output unit (neuron)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有三个输入单元（神经元）和一个输出单元（神经元）的简单感知机。
- en: The simple activation defined above can be interpreted as the dot product between
    the vector **w** and the vector **x**. The vector **w** is fixed, and it defines
    how the perceptron works, while **x** represents the input. A vector **x** is
    perpendicular to the weight vector **w** if *<* **w,x** *> = 0*, therefore all
    vectors **x** such that *<* **w,x** *> = 0* define a hyper-plane in **R** ^(*3*)
    (where *3* is the dimension of **x**, but it could be any integer in general).
    Hence, any vector **x** satisfying *<* **w,x** *> > 0* is a vector on the side
    of the hyper-plane defined by **w**. This makes it clear how a perceptron just
    defines a hyper-plane and it works as a classifier. In general, instead of *0*
    we can set the threshold to be any real number *b*, this has the effect of translating
    the hyper-plane away from the origin. However, rather than keeping track of this
    value, generally we include a bias unit in our network, which is an always on
    (*value = 1)* special neuron with connecting weight *-b*. In this case, if the
    connecting weight has value *–b*, the activation value becomes ![Neural networks](img/00015.jpeg)
    and setting *a(x) > 0* is equivalent to setting ![Neural networks](img/00016.jpeg).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 上面定义的简单激活可以理解为向量**w**和向量**x**的点积。向量**w**是固定的，并且它定义了感知机的工作方式，而**x**表示输入。如果*<*
    **w,x** *> = 0*，则向量**x**垂直于权重向量**w**，因此所有满足*<* **w,x** *> = 0*的向量**x**定义了**R**
    ^(*3*) （其中*3*是**x**的维度，但通常可以是任何整数）中的一个超平面。因此，任何满足*<* **w,x** *> > 0*的向量**x**都是位于**w**定义的超平面的一侧的向量。这清楚地说明了感知机如何定义一个超平面，并且可以作为一个分类器工作。一般而言，我们可以将阈值设置为任何实数*b*，这使得超平面远离原点。然而，通常我们在网络中包含一个偏置单元，它是一个始终开启（*值为1*）的特殊神经元，并具有连接权重*-b*。在这种情况下，如果连接权重的值为*-b*，激活值变为![神经网络](img/00015.jpeg)，而设置*a(x)
    > 0*等同于设置![神经网络](img/00016.jpeg)。
- en: '![Neural networks](img/00017.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/00017.jpeg)'
- en: A perceptron with added a bias unit for the output vector. Bias units are always
    on
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一个感知机加上一个偏置单元作为输出向量。偏置单元始终为开启状态。
- en: Perceptrons, while limited in their performance, are very important historically
    as they are the first examples of a neural network.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机在性能上有限，但在历史上非常重要，因为它们是神经网络的第一个例子。
- en: Neural networks, of course, do not need to have one single output neuron, and,
    in fact, in general they do not. If the network has more than one output neuron,
    for each output neuron we can repeat the same process. Each weight will then be
    labeled by two indices, *i* and *j,* to indicate thatthe weight is connecting
    the neuron *i* on the input layer to the neuron *j o*n the output layer. There
    will also be a connection from the bias unit, with value 1, to each neuron on
    the output layer. It should also be noted that we can define different activity
    functions on the activation value. We have defined the activation value as ![Neural
    networks](img/00018.jpeg) (from now on we will assume that the bias is included
    in this formula) and we have said that the neuron activates if the activation
    is greater than *0*. As we will see, this already defines an activity function,
    that is, a function defined on the activation, that is, on the neuron's internal
    state, and this is called the threshold activity, because the neuron activates
    when the activation is greater than *0*. However, we will see that neural nets
    can have many different activity functions that can be defined on their activation
    value, and we will discuss them in greater detail in the next chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络当然不需要只有一个输出神经元，事实上，一般情况下不会只有一个。如果网络有多个输出神经元，那么对于每个输出神经元，我们可以重复相同的过程。然后，每个权重都将被标记为两个指标，*i*
    和 *j*，以指示权重连接了输入层的神经元 *i* 和输出层的神经元 *j*。还将从偏置单元连接到输出层的每个神经元，其值为1。还应该注意，我们可以在激活值上定义不同的活动函数。我们已经将激活值定义为
    ![神经网络](img/00018.jpeg)（从现在开始，我们将假设偏差已包含在此公式中），并且我们已经说过，如果激活大于 *0*，则神经元激活。正如我们将看到的，这已经定义了一个活动函数，即在激活上定义的函数，即在神经元的内部状态上定义的函数，并且这被称为阈值活动，因为当激活大于
    *0* 时，神经元激活。但是，我们将看到，神经网络可以具有许多不同的活动函数，这些函数可以在其激活值上定义，并且我们将在下一章中对其进行更详细的讨论。
- en: Deep learning
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'The previous paragraph introduced a very simple example of a neural network,
    a feed-forward 1-layer network. They are called feed-forward because the information
    proceeds from the input towards the output and it never loops back, and 1-layer
    because there is only 1-output layer besides the input layer. This is not the
    general case. We have already discussed the limitations of 1-layer feed-forward
    networks when we mentioned that they can only work on linearly separable data,
    and in particular we mentioned that they cannot approximate the logical XOR function.
    There are, however, networks that have extra layers between the input and the
    output layers: these layers are called the hidden layers. A feed-forward network
    with hidden layers will then move the information from the input through its hidden
    layer to the output layer, which defines a function that takes an input and it
    defines an output. There exists a theorem, called the Universal Theorem, which
    states that any function can be approximated by a neural network with at least
    one hidden layer, and we will give an intuition of why this is true in the next
    chapter.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的段落介绍了一个非常简单的神经网络示例，即前馈1层网络。它们被称为前馈，因为信息从输入向输出传递，永远不会循环返回，而且是1层，因为除了输入层之外只有1个输出层。这不是一般情况。当我们提到它们只能处理线性可分数据时，我们已经讨论了前馈1层网络的局限性，特别是我们提到它们无法逼近逻辑异或函数。然而，存在具有额外层的网络，这些层位于输入和输出层之间：这些层称为隐藏层。具有隐藏层的前馈网络将信息从输入通过其隐藏层传递到输出层，这定义了一个接受输入并定义输出的函数。存在一个定理，称为通用定理，它说明任何函数都可以由具有至少一个隐藏层的神经网络逼近，并且我们将在下一章中给出为什么这是正确的直觉。
- en: For a long time, given this theorem and also the difficulty in working with
    complex networks, people have worked with shallow networks with only one hidden
    layer. However, recently people have realized that more complex networks with
    many hidden layers can understand levels of abstraction that shallow layers cannot.
    In addition, recurrent networks have been introduced where neurons can also feed
    information back into themselves. Some neural networks' structures can also permit
    to define an energy function that allows for the creation of memories. All of
    this exciting functionality will be discussed in the next chapters as we will
    delve through the most recent development in deep learning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，考虑到这个定理以及与复杂网络合作的困难，人们一直使用只有一个隐藏层的浅层网络。然而，最近人们意识到，具有许多隐藏层的更复杂的网络可以理解比浅层网络更多的抽象层次。此外，还引入了循环网络，其中神经元也可以将信息反馈给自身。一些神经网络的结构也可以允许定义能量函数，从而可以创建记忆。所有这些令人兴奋的功能将在接下来的章节中讨论，我们将深入研究深度学习的最新发展。
- en: '![Deep learning](img/00019.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习](img/00019.jpeg)'
- en: A neural network with one hidden layer
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个隐藏层的神经网络
- en: Applications in real life
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在现实生活中的应用
- en: Machine learning in general, and deep learning in particular, are producing
    more and more astonishing results in terms of quality of predictions, feature
    detections, and classifications. Many of these recent results have made the news
    in recent years. Such is the pace of progress, that many experts are worrying
    that very soon machines will be more intelligent than humans. At a UN meeting,
    on October 14, 2015, artificial intelligence experts and researchers in many other
    fields warned about the need to define ethical guidelines to prevent the danger
    that a super-intelligent class of machines may pose to the human race. Such fears
    arise from some recent incredible results in which computers have been able to
    beat the best human champions in games where it was thought that intuition would
    give humans an advantage over machines.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，机器学习，尤其是深度学习，在预测质量、特征检测和分类方面正在产生越来越令人惊讶的结果。近年来，许多最近的结果都成为了新闻。进展如此之快，以至于许多专家担心很快机器将比人类更加智能。在
    2015 年 10 月 14 日的联合国会议上，人工智能专家和其他许多领域的研究人员警告说，需要定义道德准则来防止超级智能机器可能给人类社会带来的危险。这些担忧源于一些最近的令人难以置信的结果，其中计算机已经能够击败人类最好的冠军，在人们认为直觉会给人类在与机器的比赛中带来优势的游戏中。
- en: AlphaGo is an artificial intelligence machine based on deep learning which made
    news in 2016 by beating world Go champion Lee Sedol. In January 2016, AlphaGo
    had already made the news when it beat the European champion Fan Hui, though,
    at the time, it seemed unlikely that it could go on to beat the world champion.
    Fast forward a couple of months and AlphaGo was able to achieve this remarkable
    feat by sweeping its opponent in a 4-1 victory series. The reason for celebration
    was due to the fact that Go has many more possible game variations than other
    games, such as chess, and it is impossible to be able to consider all possible
    moves in advance. In addition, in Go it is also very difficult to even judge the
    current position or value of a single stone on the board, unlike chess.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo 是基于深度学习的人工智能机器，在 2016 年曾因击败世界围棋冠军李世石而成为新闻头条。在 2016 年 1 月，AlphaGo 已经因击败欧洲冠军范辉而成为新闻，尽管当时看起来不太可能击败世界冠军。几个月后，AlphaGo
    成功通过以 4-1 的胜利系列横扫对手来实现这一非凡壮举。庆祝的原因在于围棋比其他游戏（如国际象棋）拥有更多可能的游戏变化，因此不可能事先考虑所有可能的走法。此外，在围棋中，即使是判断当前棋局上的单个棋子的位置或价值也是非常困难的，与国际象棋不同。
- en: The strength of AlphaGo was that it had not been programmed to play, but it
    had *learned* to play by playing thousands of games against itself using reinforcement
    learning and deep learning techniques. The ability to learn is what renders machine
    learning, and deep learning especially, a completely different approach to solving
    problems. Deep learning is about creating programs that may learn by themselves
    with little or no help from humans.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo 的优势在于它不是被编程来玩游戏的，而是通过使用强化学习和深度学习技术与自身进行数千场比赛而*学会*玩游戏的。学习能力是使机器学习，尤其是深度学习，成为解决问题的完全不同方法的原因。深度学习是关于创建可以自己学习的程序，几乎不需要人类的帮助。
- en: However, the variety of fields in which deep learning has been applied with
    considerable success is not limited to games. Kaggle ([http://www.kaggle.com](http://www.kaggle.com))
    is a web site hosting many different machine learning competitions. These vary
    extensively in terms of the field they are used in and their applications. In
    2013, the University of Oregon sponsored a competition in which it was asked to
    use machine learning techniques to detect and identify birds by using standard
    recording of real-world audio data. In order to gain better understanding of bird
    population trends, costly human effort is often required. Machine learning helps
    solve this problem by automatically identifying what birds are present by simply
    listening on an audio recording.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，深度学习获得重大成功的领域的种类不仅限于游戏。Kaggle([http://www.kaggle.com](http://www.kaggle.com))是一个主办许多不同机器学习竞赛的网站。这些竞赛在使用领域和应用领域上有很大的差异。2013年，俄勒冈大学发起了一场竞赛，要求使用机器学习技术通过标准的实际音频数据录音来检测和识别鸟类。为了更好地了解鸟类种群趋势，通常需要付出昂贵的人力努力。机器学习通过仅仅通过听音频记录自动识别存在的鸟类来解决这个问题。
- en: Amazon recently launched another competition for the problem of granting employees
    access to internal computers and networks, hoping that a successful solution would
    cut the costly delays of human supervisory intervention.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊最近启动了另一项竞赛，用于解决员工访问内部计算机和网络的问题，希望成功的解决方案能够减少人工监督干预带来的昂贵延迟。
- en: The Chicago Department of Health held a competition in 2015 where it asked "given
    weather location testing and spraying data … when and where different species
    of mosquitoes will test positive for West Nile virus".
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，芝加哥卫生部举办了一场比赛，要求提供“根据天气、地点、检测和喷洒数据……在什么地方以及何时会检测到不同种类的蚊子携带西尼罗河病毒”的答案。
- en: In August 2015, a competition asked to predict rental prices across Western
    Australia, and in February 2016 BNP Paribas, a French bank, launched a competition
    to accelerate its claim management process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年8月，一项竞赛要求预测西澳大利亚的租金价格，2016年2月，法国巴黎银行BNP Paribas发起了一场竞赛，以加快其索赔管理流程。
- en: This provides some idea of the variety of problems that can be solved using
    machine learning, and it should be noted that all these competitions offered prizes
    for the best solution. In 2009, Netflix launched a one million dollar competition
    to improve the accuracy of its prediction system on what movies a user may enjoy
    based on his/her previously ranked movies, and data scientist jobs are routinely
    ranked among the highest paid and most sought after work occupations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一些用机器学习解决的问题的种类，值得注意的是，所有这些比赛都提供了最佳解决方案的奖励。2009年，Netflix推出了一项百万美元的竞赛，以提高其根据用户之前观看电影的排名预测用户可能喜欢的电影的准确性，而数据科学家的工作通常被排名为工资最高且最受欢迎的工作之一。
- en: Machine learning is routinely used in applications ranging from self-driving
    cars, military drones, and target reconnaissance systems, to medical applications,
    such as applications able to read doctors' notes to spot potential health problems,
    and surveillance systems that can provide facial recognition.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从自动驾驶汽车、军事无人机和目标侦查系统到医疗应用，例如能够读取医生笔记以发现潜在健康问题的应用以及能够提供面部识别的监控系统，机器学习在各种应用中得到了常规使用。
- en: Optical character recognition is widely used, for example by post offices, to
    read addresses on envelopes, and we will show how to apply neural networks to
    digit recognition using the MNIST dataset. Unsupervised deep learning has also
    found many applications and great results in **Natural Language Processing** (**NLP**),
    and almost each one of us has an NLP application of deep learning on his/her smartphone,
    as both Apple and Android use deep learning applied to NLP for their virtual assistants
    (for example, Siri). Machine learning also finds applications in biometrics, for
    example in recognizing someone's physical characteristics, such as fingerprints,
    DNA or retina recognition. In addition, automobile autonomous driving has improved
    in recent years to the point that it is now a reality.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 光学字符识别广泛应用于诸如邮局等机构，以读取信封上的地址，我们将展示如何使用MNIST数据集将神经网络应用于数字识别。自主无监督深度学习在**自然语言处理**
    (**NLP**)方面也有许多应用和出色的成果，我们每个人几乎在自己的智能手机上都有一个应用深度学习在NLP上的NLP应用，因为苹果和安卓都使用应用于NLP的深度学习来作为虚拟助手（例如，Siri）。机器学习还可以应用于生物特征识别，例如识别指纹、DNA或视网膜。此外，自动驾驶汽车在近年来取得的进展已经到达了一个现实的阶段。
- en: Machine learning can also be applied to catalog pictures in a photo album, or,
    more importantly, satellite images, allowing the description of different images
    according to whether they are an urban environment or not, whether they describe
    forests, ice regions, water extensions, and so on.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习也可以应用于目录中的图片或者更重要的卫星图像，根据它们是不是城市环境，是否描述了森林、冰区、水域等不同环境进行描述。
- en: To summarize, machine learning has recently found applications in almost every
    aspect of our lives, and its accuracy and performance has seen a continuous improvement
    thanks also to increasingly better and faster computers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，机器学习最近在我们生活的方方面面都找到了应用，并且其准确性和性能也得到了持续改善，这也要归因于越来越好和更快的计算机。
- en: A popular open source package
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个受欢迎的开源包
- en: Machine learning is a popular and competitive field, and there are many open
    source packages that implement most of the classic machine learning algorithms.
    One of the most popular is `scikit-learn` ([http://scikit-learn.org](http://scikit-learn.org)),
    a widely used open source library used in Python.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个受欢迎且竞争激烈的领域，有许多开源包实现了大多数经典的机器学习算法。其中最受欢迎的是`scikit-learn`（[http://scikit-learn.org](http://scikit-learn.org)），这是一个广泛使用的Python开源库。
- en: '`scikit-learn` offers libraries that implement most classical machine-learning
    classifiers, regressors and clustering algorithms such as **support vector machines**
    (**SVM**), nearest neighbors, random forests, linear regression, k-means, decision
    trees and neural networks, and many more machine learning algorithms'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`提供了实现大多数经典机器学习分类器、回归器和聚类算法的库，例如**支持向量机**（**SVM**）、最近邻、随机森林、线性回归、k均值、决策树和神经网络等许多机器学习算法。'
- en: The base class `sklearn` has several packages available, depending on the type
    of algorithm chosen, such as `sklearn.ensemble`, `sklearn.linear_model`, `sklearn.naive_bayes`,
    `sklearn.neural_network`, `sklearn.svm`, and `sklearn.tree`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 基类`sklearn`有几个可用的包，取决于选择的算法类型，比如`sklearn.ensemble`、`sklearn.linear_model`、`sklearn.naive_bayes`、`sklearn.neural_network`、`sklearn.svm`和`sklearn.tree`。
- en: There are also helpers to do cross-validation and for helping select the best
    features. Rather than spending time describing all the functionality abstractly,
    we will instead start with one simple example using a multi-layer neural network.
    The `scikit-learn` library uses methods with similar signatures for each machine
    learning algorithm, so classifiers share the same common functionality. In addition,
    we want the reader to be able to quickly start getting a flavor of what neural
    networks can do without the need to spend time creating a neural network from
    scratch. The following chapters will discuss other libraries and more complex
    implementations for many different types of deep learning neural nets, but for
    now, the user can start getting a quick idea of their functionality.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些辅助工具来进行交叉验证，以及帮助选择最佳特征。我们将不再花时间抽象地描述所有功能，而是从使用多层神经网络的一个简单示例开始。`scikit-learn`库使用类似签名的方法来实现每种机器学习算法，因此分类器共享相同的通用功能。此外，我们希望读者能够快速了解神经网络的功能，而无需花时间从头开始创建神经网络。下一章将讨论其他库以及更多复杂的深度学习神经网络类型的实现，但目前，用户可以快速了解它们的功能。
- en: 'For example, if one wanted to use a multi-layer neural network with scikit-learn,
    it would be enough to import it in our program by typing:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果想要在scikit-learn中使用多层神经网络，只需在程序中导入它即可：
- en: '[PRE0]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each algorithm needs to be called using pre-defined parameters, though in most
    instances default values may be used. In the case of the MLPClassifier, no parameter
    is needed and one can use the default values (all parameters are described on
    the scikit-learn website, and in particular for the MLPClassifier one can find
    them at: [http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html](http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html)).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 每个算法都需要使用预定义的参数进行调用，尽管在大多数情况下可以使用默认值。对于MLPClassifier来说，不需要参数，可以使用默认值（所有参数都在scikit-learn网站上描述，特别是对于MLPClassifier可以在这里找到它们：[http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html](http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html)）。
- en: 'The algorithm is then called on the training data, using the labels for tuning
    of the parameters, using the `fit` function:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在训练数据上调用该算法，使用标签来调整参数，使用`fit`函数：
- en: '[PRE1]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the algorithm has been fit on the training data, it can be used to make
    predictions on the test data, using the `predict_proba` function that will output
    probabilities for each class:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦算法在训练数据上拟合好，它就可以使用`predict_proba`函数在测试数据上进行预测，该函数将为每个类输出概率：
- en: '[PRE2]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's write a full example of how to use a `MLPClassifier` classifier on the
    `iris` dataset that we discussed briefly when we introduced decision trees.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写一个完整的例子，演示如何在我们简要介绍决策树时简要讨论的`iris`数据集上使用`MLPClassifier`分类器。
- en: 'Scikit-learn makes it easy to load important classic datasets. To do this we
    only need:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn使加载重要的经典数据集变得很容易。要做到这一点，我们只需要：
- en: '[PRE3]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will load the dataset. Now, to load the classifier we just need:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将加载数据集。现在，要加载分类器，我们只需要：
- en: '[PRE4]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we tune the parameters using the data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用数据来调整参数：
- en: '[PRE5]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, since the weights are initialized randomly, the `random_state` value is
    simply there to force the initialization to always use the same random values
    in order to get consistent results across different trials. It is completely irrelevant
    to understanding the process. The `fit` function is the important method to call,
    it is the method that will find the best weights by training the algorithm using
    the data and labels provided, in a supervised fashion.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于权重是随机初始化的，`random_state`值只是为了强制初始化始终使用相同的随机值，以便在不同试验中获得一致的结果。这与理解过程完全无关。`fit`函数是调用的重要方法，它是通过使用提供的数据和标签，在受监督的方式下训练算法以找到最佳权重的方法。
- en: 'Now we can check our predictions and compare them to the actual result. Since
    the function `predict_proba` outputs the probabilities, while `predict` outputs
    the class with the highest probability, we will use the latter to make the comparison,
    and we will use one of sikit-learn helper modules to give us the accuracy:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查我们的预测并将其与实际结果进行比较。由于函数`predict_proba`输出概率，而`predict`输出具有最高概率的类，因此我们将使用后者进行比较，并使用sikit-learn的一个辅助模块来给出准确度：
- en: '[PRE6]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And that''s it. Of course, as we mentioned, it is usually better to split our
    data between training data and test data, and we can also improve on this simple
    code by using some regularization of the data. Scikit-learn provides some helper
    functions for this as well:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。当然，正如我们提到的，通常最好将我们的数据分为训练数据和测试数据，并且我们还可以通过对数据进行一些正则化来改进这个简单的代码。Scikit-learn也提供了一些辅助函数：
- en: '[PRE7]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code is self-explanatory, we split the data and we normalize it, which
    means we subtract the mean and scale the data to unit variance. Then we fit our
    algorithm on the training data and we test on the test data:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码很容易理解，我们将数据拆分并对其进行归一化处理，这意味着我们减去均值并将数据缩放到单位方差。然后我们在训练数据上拟合我们的算法，然后在测试数据上进行测试：
- en: '[PRE8]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And we get the following output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到以下输出：
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can draw some pictures to show the data and how the neural net divides the
    space into three regions to separate the three types of flowers (since we can
    only draw two-dimensional pictures we will only draw two features at the time).
    The first graph shows how the algorithm tries to separate the flowers based on
    their petal width and length, without having normalized the data:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制一些图片来展示数据以及神经网络将空间分成三个区域以分离三种类型的花（由于我们只能绘制二维图片，我们一次只会绘制两个特征）。第一个图显示了算法如何根据花瓣的宽度和长度来分离花，而没有对数据进行归一化：
- en: '![A popular open source package](img/00020.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![一个流行的开源包](img/00020.jpeg)'
- en: 'The second graph shows the same based only on the petal width and the sepal
    width, instead:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个图显示了基于花瓣宽度和花萼宽度的分离情况：
- en: '![A popular open source package](img/00021.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![一个流行的开源包](img/00021.jpeg)'
- en: 'The third graph shows the same data as the first one, after normalizing the
    data:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个图显示了第一个图的相同数据，在归一化数据之后：
- en: '![A popular open source package](img/00022.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![一个流行的开源包](img/00022.jpeg)'
- en: 'And, finally, the fourth graph is the same as the second one with the data
    normalized:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第四个图与第二个图相同，只是数据已归一化：
- en: '![A popular open source package](img/00023.jpeg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![一个流行的开源包](img/00023.jpeg)'
- en: We also show the code used to create these graphs. Note that the code to draw
    these pictures was adapted from similar code by Sebastian Raschka in his *Python
    Machine Learning* book, published by Packt Publishing, and we refer the reader
    to it for further details.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了用于创建这些图形的代码。请注意，用于绘制这些图片的代码是根据Sebastian Raschka在他的*Python机器学习*书籍中的类似代码进行调整的，该书由Packt
    Publishing出版，我们建议读者参考该书获取更多详情。
- en: The code to make the preceding drawings is as follows. Note that before data
    must be set to only contain the data relative to two variables, for example `data
    = iris.data[:,[1,3]]` for sepal and petal length, since we can only draw two-dimensional
    images.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 制作上述图形的代码如下。请注意，在数据之前必须设置为仅包含与两个变量相关的数据，例如对于萼片和花瓣长度，`data = iris.data[:,[1,3]]`，因为我们只能绘制二维图像。
- en: '[PRE10]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `MLPClassifier`, as we mentioned, does have many parameters that we can
    use; we will cite only the activation function and the number of hidden layers
    and how many neurons each may have, but the documentation for all possible parameters
    is available at [http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html](http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所述，`MLPClassifier`具有许多参数可供我们使用；我们仅引用激活函数和隐藏层数以及每个隐藏层可能具有的神经元数量，但所有可能参数的文档都可在
    [http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html](http://scikit-learn.org/dev/modules/generated/sklearn.neural_network.MLPClassifier.html)
    上找到。
- en: The number of hidden layers and the number of neurons can be specified by adding
    `hidden_layer_sizes=(n`[1]`, n`[2]`, n`[3]`, …, n`[m]`)`, where `n`[i] is the
    number of neurons in the *i*^(th) layer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层数和神经元数量可以通过添加`hidden_layer_sizes=(n`[1]`, n`[2]`, n`[3]`, …, n`[m]`)`来指定，其中`n`[i]是第*i*层的神经元数量。
- en: 'For a neural net with two hidden layers with `200` and `100` neurons respectively,
    we would write:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个具有分别为`200`和`100`个神经元的两个隐藏层的神经网络，我们会写成：
- en: '[PRE11]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The other important parameter refers to the activation function, that we have
    called previously the activity function. This module supports three types defined
    below:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的参数是激活函数，我们之前称之为活动函数。该模块支持以下三种类型的定义：
- en: Note
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The **ReLU** is the easiest, but also one of the most popular (and the default
    activation function) and it is simply defined as ![A popular open source package](img/00024.jpeg)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU** 是最简单的，也是最受欢迎的之一（也是默认的激活函数），其定义简单地为 ![一个流行的开源软件包](img/00024.jpeg)'
- en: 'The **logistic** function is used when we are interested in calculating the
    probability of an event, in fact it has values between 0 and 1 and it is defined
    as: ![A popular open source package](img/00025.jpeg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑** 函数用于计算事件的概率时使用，实际上它的值介于0和1之间，定义如下： ![一个流行的开源软件包](img/00025.jpeg)'
- en: 'Finally, the **tanh** is simply defined as: ![A popular open source package](img/00026.jpeg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**tanh** 简单定义为： ![一个流行的开源软件包](img/00026.jpeg)
- en: 'For example, to use two hidden layers with 200 and 100 neurons respectively
    with a logistic activation function, the code would modify to be:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要使用分别具有200个和100个神经元的两个隐藏层，其中一个逻辑激活函数，代码将修改为：
- en: '[PRE12]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We invite the reader to play with some of these parameters, and also to use
    the `max_iter` parameter that will limit the number of iterations. The number
    of iterations refers to the number of passes over the training data. A small value,
    such as `max_iter=100`, will not produce such good results, as the algorithm will
    not have the time to converge. Note, however, that on such a small dataset, more
    hidden layers will not necessarily result in better predictions, and they may
    instead degrade prediction accuracy.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请读者尝试一些这些参数，并且使用`max_iter`参数，它将限制迭代次数。迭代次数是指对训练数据的传递次数。小值，例如`max_iter=100`，将不会产生很好的结果，因为算法没有时间收敛。但请注意，在这样一个小数据集上，更多的隐藏层不一定会产生更好的预测结果，反而可能会降低预测准确性。
- en: That concludes this chapter, where we have introduced the reader to the importance
    of machine learning and the many applications in the real world. We have briefly
    mentioned some of the issues and problems, and we have touched on the topic of
    neural networks that will be the focus of the next chapter. We have also touched
    on how to use standard open source libraries such as `scikit-learn` to start implementing
    some simple multi-layer feed-forward networks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了本章，我们向读者介绍了机器学习的重要性以及在现实世界中的许多应用。我们简要提到了一些问题和困难，并且涉及了下一章的重点——神经网络的主题。我们还介绍了如何使用标准的开源库，例如`scikit-learn`来开始实现一些简单的多层前馈网络。
- en: We now turn to discussing neural networks in depth and the motivations behind
    their use.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转而深入讨论神经网络及其使用背后的动机。
- en: Summary
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered what machine learning is and why it is so important.
    We have provided several examples where machine learning techniques find applications
    and what kind of problems can be solved using machine learning. We have also introduced
    a particular type of machine learning algorithm, called neural networks, which
    is at the basis of deep learning, and have provided a coding example in which,
    using a popular machine learning library, we have solved a particular classification
    problem. In the next chapter, we will cover neural networks in better detail and
    will provide their theoretical justifications based on biological considerations
    drawn from observations of how our own brain works.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经介绍了什么是机器学习以及为什么它如此重要。我们提供了几个机器学习技术应用的示例，以及使用机器学习可以解决哪些问题。我们还介绍了一种特定类型的机器学习算法，称为神经网络，它是深度学习的基础，并提供了一个编码示例，在其中，使用一种流行的机器学习库，我们解决了一个特定的分类问题。在下一章中，我们将更详细地介绍神经网络，并根据从观察我们自己的大脑工作方式中得出的生物学考虑，提供它们的理论解释。
