- en: Deep Learning in Audio and Speech
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 音频和语音中的深度学习
- en: In this chapter, we'll deal with sounds and speech. Sound data comes in the
    form of waves, and therefore requires different preprocessing than other types
    of data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理声音和语音。声音数据以波形的形式出现，因此需要与其他类型的数据进行不同的预处理。
- en: Machine learning on audio signals finds commercial applications in speech enhancement
    (for example, in hearing aids), speech-to-text and text-to-speech, noise cancellation
    (as in headphones), recommending music to users based on their preferences (such
    as Spotify), and generating audio. Many fun problems can be encountered in audio,
    including the classification of music genres, the transcription of music, generating
    music, and many more besides.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频信号的机器学习中，商业应用包括语音增强（例如助听器）、语音到文本和文本到语音、噪声取消（例如耳机）、根据用户喜好推荐音乐（如 Spotify）以及生成音频。在音频中可以遇到许多有趣的问题，包括音乐流派的分类、音乐的转录、生成音乐等等。
- en: We'll implement several applications with sound and speech in this chapter.
    We'll first do a simple example of a classification task, where we try to distinguish
    different words. This would be a typical application in a smart home device to
    distinguish different commands. We'll then look at a text-to-speech architecture.
    You could apply this to create your own audio books from text, or for the voice
    output of your home-grown smart home device. We'll close with a recipe for generating
    music. This is perhaps more of a niche application in the commercial sense, but
    you could build your own music for fun or to entertain users of your video game.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中实现几个与声音和语音相关的应用。首先，我们将做一个简单的分类任务的例子，尝试区分不同的词汇。这将是智能家居设备中区分不同命令的典型应用。然后，我们将研究一个文本到语音的架构。您可以应用这个架构从文本创建自己的音频书籍，或者为您自己的智能家居设备的语音输出。最后，我们将结束于生成音乐的配方。从商业角度来看，这可能更多是一个利基应用，但您可以为了乐趣或娱乐您的视频游戏用户而构建自己的音乐。
- en: 'In this chapter, we''ll look at the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看看以下的配方：
- en: Recognizing voice commands
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别语音命令
- en: Synthesizing speech from text
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本合成语音
- en: Generating melodies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成旋律
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the source code for the notebooks associated with the recipes in
    this chapter on GitHub at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter09](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter09).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上找到与本章配方相关的笔记本的源代码：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter09](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter09)。
- en: 'We''ll use the `librosa` audio processing library ([https://librosa.org/doc/latest/index.html](https://librosa.org/doc/latest/index.html))
    in this chapter, which you can install as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用音频处理库 `librosa`（[https://librosa.org/doc/latest/index.html](https://librosa.org/doc/latest/index.html)）。您可以按如下方式安装它：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Librosa comes installed by default in Colab.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Librosa 在 Colab 中默认安装。
- en: For the recipes in this chapter, please make sure you have a GPU available.
    On Google Colab, make sure you activate a GPU runtime.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章中的配方，请确保您有一个可用的 GPU。在 Google Colab 上，请确保您激活了 GPU 运行时。
- en: Recognizing voice commands
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别语音命令
- en: In this recipe, we will look at a simple sound recognition problem on Google's
    Speech Commands dataset. We'll classify sound commands into different classes.
    We'll then set up a deep learning model and train it.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将在谷歌的语音命令数据集上解决一个简单的声音识别问题。我们将把声音命令分类到不同的类别中。然后，我们将建立一个深度学习模型并进行训练。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we''ll need the `librosa` library as mentioned at the start
    of the chapter. We''ll also need to download the Speech Commands dataset, and
    for that we''ll need to install the `wget` library first:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本配方，我们需要在章节开头提到的 `librosa` 库。我们还需要下载语音命令数据集，为此我们首先需要安装 `wget` 库：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Alternatively, we could use the `!wget` system command in Linux and macOS.
    We''ll create a new directory, download the archive with the dataset, and extract
    the `tarfile`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以在 Linux 和 macOS 中使用 `!wget` 系统命令。我们将创建一个新目录，下载带有数据集的存档文件，并提取 `tarfile`：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This gives us a number of files and directories within the `data/train` directory:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们在 `data/train` 目录下获得了许多文件和目录：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Most of these refer to speech commands; for example, the `bed` directory contains
    examples of the `bed` command.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数指的是语音命令；例如，`bed` 目录包含了 `bed` 命令的示例。
- en: With all of this available, we are now ready to start.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们现在准备好开始了。
- en: How to do it...
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this recipe, we'll train a neural network to recognize voice commands. This
    recipe is inspired by the TensorFlow tutorial on speech commands at [https://www.tensorflow.org/tutorials/audio/simple_audio](https://www.tensorflow.org/tutorials/audio/simple_audio).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将训练一个神经网络来识别语音命令。这个教程的灵感来自于 TensorFlow 在语音命令上的教程，网址是 [https://www.tensorflow.org/tutorials/audio/simple_audio](https://www.tensorflow.org/tutorials/audio/simple_audio)。
- en: 'We''ll first perform data exploration, then we''ll import and preprocess our
    dataset for training, and then we will create a model, train it, and check its
    performance in validation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先进行数据探索，然后导入和预处理数据集进行训练，接着创建模型，训练并在验证中检查其性能：
- en: 'Let''s start with some data exploration: we''ll listen to a command, look at
    its waveform, and then at its spectrum. The `librosa` library provides functionality
    to load sound files into a vector:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从一些数据探索开始：我们将听一个命令，查看其波形，然后查看其频谱。`librosa` 库提供了将声音文件加载到向量中的功能：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also get a Jupyter widget for listening to sound files or to the loaded
    vector:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以获得一个 Jupyter 小部件来听声音文件或加载的向量：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The widget looks like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 小部件看起来像这样：
- en: '![](img/06c58925-04d1-4647-9a74-4f0976ea4f8d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06c58925-04d1-4647-9a74-4f0976ea4f8d.png)'
- en: Pressing play, we hear the sound. Note that this works even over a remote connection,
    for example, if we use Google Colab.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 按下播放，我们听到声音。请注意，即使在远程连接上（例如使用 Google Colab），这也可以工作。
- en: 'Let''s look at the sound waveform now:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下声音波形：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The waveform looks like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 波形看起来像这样：
- en: '![](img/6e74df18-4205-44ba-a3a2-30486e0fdc03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e74df18-4205-44ba-a3a2-30486e0fdc03.png)'
- en: This is also called the pressure-time plot, and shows the (signed) amplitude
    over time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这也称为压力-时间图，显示随时间变化的（有符号的）振幅。
- en: 'We can plot the spectrum as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下绘制频谱：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The spectrum looks like this:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 频谱看起来像这样：
- en: '![](img/593a7454-c705-4231-82c5-e2720f935923.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/593a7454-c705-4231-82c5-e2720f935923.png)'
- en: Please note that we've used a log scale on the *y*-axis.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在*y*轴上使用了对数尺度。
- en: 'Now, let''s get to the data importing and preprocessing. We have to iterate
    over files, and store them as a vector:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们进行数据导入和预处理。我们需要迭代文件，并将它们存储为向量：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For simplicity, we are only taking three commands here: `bed`, `bird`, and
    `tree`. This is enough to illustrate the problems and the application of a deep
    neural network to sound classification, and is simple enough that it won''t take
    very long. This process can, however, still take a while. It took about an hour
    on Google Colab.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为简单起见，我们这里只使用了三个命令：`bed`、`bird` 和 `tree`。这已足以说明深度神经网络在声音分类中的问题和应用，也足够简单，不会花费太多时间。然而，这个过程仍然可能需要一些时间。在
    Google Colab 上大约需要一个小时。
- en: 'Finally, we need to convert the Python list of features to a NumPy array, and
    we need to split the training and validation data:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将 Python 特征列表转换为 NumPy 数组，并且需要分割训练和验证数据：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we need to do something with our training data. We'll need a model that
    we can train.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要对我们的训练数据做一些处理。我们需要一个可以训练的模型。
- en: 'Let''s create a deep learning model and then train and test it. First we need
    to create our model and normalization. Let''s do the normalization first:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个深度学习模型，然后进行训练和测试。首先我们需要创建我们的模型和标准化。让我们先进行标准化操作：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is followed by the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是以下内容：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Please note the `conv_layer()` function, which provides the core of the network.
    Very similar convolutional modules can be used in vision, it is just that we use
    1D convolutions here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 `conv_layer()` 函数，它提供了网络的核心部分。在视觉中可以使用非常类似的一维卷积模块，这里只是我们在这里使用了一维卷积。
- en: 'This gives us a relatively small model of only about 75,000 parameters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个相对较小的模型，仅约有 75,000 个参数：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You'll note that the biggest layer (in terms of parameters) is the final dense
    layer. We could have further reduced the number of parameters by changing the
    convolutional or maxpooling operations before the dense layer.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最大的层（按参数计算）是最后的密集层。我们可以通过修改密集层之前的卷积或最大池化操作进一步减少参数数量。
- en: 'We can now perform training and validation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行训练和验证：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We should see something like 0.805 as the output for the model accuracy in the
    validation set.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证集中，模型的准确率输出应该大约为 0.805。
- en: How it works...
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Sound is not that different from other domains, except for the preprocessing.
    It's important to have at least a basic understanding about how sound is stored
    in a file. At their most basic level, sounds are stored as amplitude over time
    and frequency. Sounds are sampled at discrete intervals (this is the *sampling
    rate*). 48 kHz would be a typical recording quality for a DVD, and refers to a
    sampling frequency of 48,000 times per second. The *bit depth* (also known as
    the *dynamic range*) is the resolution for the amplitude of the signal (for example,
    16 bits means a range of 0-65,535).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 声音与其他领域并无太大不同，除了预处理。了解如何在文件中存储声音至关重要。在基本水平上，声音以振幅随时间和频率存储。声音以离散间隔采样（这是*采样率*）。48
    kHz 是 DVD 的典型录音质量，指的是每秒 48,000 次的采样频率。*比特深度*（也称为*动态范围*）是信号振幅的分辨率（例如，16 比特意味着振幅范围为
    0-65,535）。
- en: For machine learning, we can do feature extraction from the waveform, and use
    1D convolutions on the raw waveforms, or 2D convolutions on the spectrogram representation
    (for example, Mel spectrograms – Davis and Mermelstein, *Experiments in syllable-based
    recognition of continuous speech*, 1980). We've dealt with convolutions before,
    in [Chapter 7](f386de9e-b56d-4b39-bf36-803860def385.xhtml), *Advanced Image Applications*. Briefly,
    convolutions are feedforward filters that are applied to rectangular patches over
    the layer input. The resulting maps are usually followed by subsampling by pooling
    layers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习，我们可以从波形中进行特征提取，并在原始波形上使用1D卷积，或在声谱图表示（例如，Mel声谱图 – Davis 和 Mermelstein，*连续语音中基于音节的识别实验*，1980年）上使用2D卷积。我们之前在[第7章](f386de9e-b56d-4b39-bf36-803860def385.xhtml)中处理过卷积，*高级图像应用*。简而言之，卷积是对层输入上的矩形补丁应用的前向滤波器。生成的映射通常在池化层之后进行子采样。
- en: The convolutional layers can be stacked very deeply (for example, Dai and others,
    2016: [https://arxiv.org/abs/1610.00087](https://arxiv.org/abs/1610.00087)). We've
    made it easy for the reader to experiment with stacked layers. The number of layers,
    `nlayers`, is one of the parameters in `create_model()`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层可以非常深度堆叠（例如，Dai等人，2016：[https://arxiv.org/abs/1610.00087](https://arxiv.org/abs/1610.00087)）。我们已经为读者实验堆叠层提供了便利。层数`nlayers`是`create_model()`的参数之一。
- en: Interestingly, many speech recognition models use recurrent neural networks.
    However, some models, such as Facebook's wav2letter ([https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter)), for
    example, use a fully convolutional model instead, which is not too dissimilar
    to the approach taken in this recipe.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，许多语音识别模型使用递归神经网络。然而，一些模型，如Facebook的wav2letter ([https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter))，例如，使用完全卷积模型，这与本方案中采用的方法并无太大不同。
- en: See also
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: Apart from `librosa`, useful libraries for audio processing in Python include
    `pydub` ([https://github.com/jiaaro/pydub](https://github.com/jiaaro/pydub)) and
    `scipy`. The pyAudioProcessing library comes with feature extraction and classification functionality for
    audio: [https://github.com/jsingh811/pyAudioProcessing](https://github.com/jsingh811/pyAudioProcessing).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`librosa`，在 Python 中用于音频处理的有用库还包括`pydub` ([https://github.com/jiaaro/pydub](https://github.com/jiaaro/pydub))
    和 `scipy`。`pyAudioProcessing` 库提供了音频的特征提取和分类功能：[https://github.com/jsingh811/pyAudioProcessing](https://github.com/jsingh811/pyAudioProcessing)。
- en: 'There are a few more libraries and repositories that are interesting to explore:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些有趣的库和存储库值得探索：
- en: wav2letter++ is an open source speech processing toolkit from the speech team
    at Facebook AI Research with Python bindings: [https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: wav2letter++ 是 Facebook AI Research 的语音处理开源工具包，并带有 Python 绑定：[https://github.com/facebookresearch/wav2letter](https://github.com/facebookresearch/wav2letter)。
- en: A project from a master's thesis – *Structured Autoencoder with Application
    to Music Genre Recognition*: [https://github.com/mdeff/dlaudio](https://github.com/mdeff/dlaudio).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个硕士论文项目 – *结构化自编码器及其在音乐流派识别中的应用*：[https://github.com/mdeff/dlaudio](https://github.com/mdeff/dlaudio)。
- en: Erdene-Ochir Tuguldur maintains a GitHub repository for Mongolian speech recognition
    with PyTorch that includes training from scratch: [https://github.com/tugstugi/mongolian-speech-recognition](https://github.com/tugstugi/mongolian-speech-recognition).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erdene-Ochir Tuguldur 在 GitHub 维护一个用 PyTorch 实现的蒙古语音识别库，包括从零开始的训练：[https://github.com/tugstugi/mongolian-speech-recognition](https://github.com/tugstugi/mongolian-speech-recognition)。
- en: Synthesizing speech from text
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本转语音合成
- en: A text-to-speech program, easily intelligible by humans, can allow people with visual
    or reading impairments to listen to written words on a home computer, or can allow
    you to enjoy a book while driving a car. In this recipe, we'll work through loading
    a text-to-speech model, and having it read a text to us. In the *How it works...*
    section, we'll go through the model implementation and the model architecture.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一个文本到语音程序，对人类来说很容易理解，可以让有视觉或阅读障碍的人听到家用电脑上的书写文字，或者在驾车时让您享受阅读一本书。在这个示例中，我们将加载一个文本到语音模型，并让它朗读给我们听。在
    *它是如何工作的……* 部分，我们将介绍模型实现和模型架构。
- en: Getting ready
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作完成
- en: 'For this recipe, please make sure you have a GPU available. On Google Colab,
    make sure you activate a GPU runtime. We''ll also need the `wget` library, which
    we can install from the notebook as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，请确保您有一个可用的 GPU。在 Google Colab 上，请确保您激活了 GPU 运行时。我们还需要安装 `wget` 库，可以在笔记本中如下安装：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also need to clone the `pytorch-dc-tts` repository from GitHub and install
    its requirements. Please run this from the notebook (or run it from the terminal
    without the leading exclamation marks):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从 GitHub 克隆`pytorch-dc-tts`存储库并安装其要求。请从笔记本运行此命令（或在终端中运行，不带前导感叹号）：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Please note that you need to have Git installed in order for this to work. If
    you don't have Git installed, you can download the repository directly from within
    your web browser.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要安装 Git 才能使其正常工作。如果您没有安装 Git，您可以直接从您的 Web 浏览器中下载存储库。
- en: We are ready to tackle the main recipe.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好处理主要示例了。
- en: How to do it...
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We''ll download the Torch model files, load them up in Torch, and then we''ll
    synthesize speech from sentences:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载 Torch 模型文件，加载它们到 Torch 中，然后从句子中合成语音：
- en: '**Downloading the model files**: We''ll download the dataset from `dropbox`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载模型文件**：我们将从`dropbox`下载数据集：'
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now we can load the model in torch.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在 torch 中加载模型。
- en: '**Loading the model**: Let''s get the dependencies out of the way:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载模型**：让我们先处理依赖关系：'
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can load the model:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以加载模型：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Finally, we can read sentences out loud.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以大声朗读这些句子。
- en: '**Synthesizing speech**: We''ve chosen a few garden-path sentences. These are
    sentences that are grammatically correct, but mislead the reader regarding their
    initial understanding of it.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语音合成**：我们选择了一些花园路径句子。这些句子在语法上是正确的，但会误导读者关于它们最初的理解。'
- en: 'The following sentences are examples of garden-path sentences – sentences that
    mislead listeners about how words relate to one another. We chose them because
    they are short and fun. You can find these and more garden-path sentences in the
    academic literature, such as in *Up the Garden Path* (Tomáš Gráf; published in
    Acta Universitatis Carolinae Philologica, 2013):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子是花园路径句子的例子——这些句子会误导听众关于单词如何相互关联的理解。我们选择它们是因为它们既简短又有趣。您可以在学术文献中找到这些及更多花园路径句子，比如在《Up
    the Garden Path》（Tomáš Gráf；2013年发表于 Acta Universitatis Carolinae Philologica）中：
- en: '[PRE19]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can generate speech from these sentences as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下步骤从这些句子生成语音：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the *There's more...* section, we'll have a look at how to train a model
    for a different dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *还有更多...* 部分，我们将看一下如何为不同数据集训练模型。
- en: How it works...
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Speech synthesis is the production of human speech by a program, called a speech
    synthesizer. A synthesis from natural language to speech is called **text-to-speech** (**TTS**).
    Synthesized speech can be generated by concatenating audio from recorded pieces
    that come in units such as distinct sounds, phones, and pairs of phones (diphones).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 语音合成是通过程序产生人类语音的过程，称为语音合成器。从自然语言到语音的合成称为**文本到语音**（**TTS**）。合成的语音可以通过连接来自录制片段的音频生成，这些片段以单位如独特的声音、音素和音素对（双音素）出现。
- en: Let's look a bit into the details of two methods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微深入了解两种方法的细节。
- en: Deep Convolutional Networks with Guided Attention
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于引导注意力的深度卷积网络
- en: In this recipe, we've loaded the model published by Hideyuki Tachibana and others, *Efficiently
    Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided
    Attention* (2017; [https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969)).
    We used the implementation at [https://github.com/tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们加载了 Hideyuki Tachibana 和其他人发表的模型，《基于深度卷积网络和引导注意力的高效可训练文本到语音系统》（2017年；[https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969)）。我们使用了在
    [https://github.com/tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts)
    上的实现。
- en: Published in 2017, the novelty of this method is to work without recurrency
    in the network, instead relying on convolutions, a decision that results in much
    faster training and inference compared to other models. In fact, they claim that
    training their deep convolutional TTS networks only took about 15 hours on a gaming
    PC equipped with two off-the-shelf GPUs. Crowdsourced mean opinion scores didn't
    seem to increase after 15 hours of training on a dataset of readings from the
    **librivox** public domain audiobook project. The authors furnish a demo page
    to showcase audio samples at different stages of the training, where you can hear
    sentences spoken, such as *the two-player zero-sum game of wasserstein gan is
    derived by considering kantorovich-rubinstein duality*: [https://tachi-hi.github.io/tts_samples/](https://tachi-hi.github.io/tts_samples/).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 发表于2017年，这种方法的新颖之处在于在网络中不使用循环，而是依赖于卷积，这一决定导致训练和推断比其他模型快得多。事实上，他们声称在一台配备两个现成的GPU的游戏PC上训练他们的深度卷积TTS网络只需大约15小时。在**librivox**公共领域有声书项目的数据集上进行了15小时的训练后，众包的平均意见分数似乎没有增加。作者提供了一个演示页面，展示了训练不同阶段的音频样本，您可以听到说出的句子，例如*Wasserstein
    GAN的两人零和博弈是通过考虑Kantorovich-Rubinstein对偶导出的*：[https://tachi-hi.github.io/tts_samples/](https://tachi-hi.github.io/tts_samples/)。
- en: 'The architecture consists of two sub-networks, which can be trained separately,
    one to synthesize spectrograms from text, and another to create waveforms from
    spectrograms. The text-to-spectrogram part consists of these modules:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构由两个子网络组成，可以分别训练，一个用于从文本合成频谱图，另一个用于从频谱图创建波形。文本到频谱图部分包括以下模块：
- en: Text encoder
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本编码器
- en: Audio encoder
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频编码器
- en: Attention
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力
- en: Audio decoder
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频解码器
- en: 'The interesting part of this is the guided attention mentioned in the title
    of the paper, which is responsible for the alignment of characters with time.
    They constrain this attention matrix to be nearly linear with time, as opposed
    to reading characters in random order given a **guided attention loss**:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的有趣之处在于标题中提到的引导注意力，它负责将字符与时间对齐。他们约束这个注意力矩阵几乎是线性随时间的，而不是随机顺序阅读字符，给定一个**引导注意力损失**：
- en: '![](img/d73731e5-df40-4531-9499-ba2eb6aef7cc.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d73731e5-df40-4531-9499-ba2eb6aef7cc.png)'
- en: This favors values on the diagonal of the matrix rather than off it. They argue
    that this constraint helps to speed up the training time considerably.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这偏爱矩阵对角线上的值而不是矩阵外的值。他们认为这种约束有助于显著加快训练时间。
- en: WaveGAN
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WaveGAN
- en: 'In the *There''s more...* section, we''ll be loading up a different model,
    WaveGAN, published as *WaveGAN: Learn to synthesize raw audio with generative
    adversarial networks*, by Chris Donahue and others (2018; [https://arxiv.org/abs/1802.04208](https://arxiv.org/abs/1802.04208)).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '在*还有更多...*部分，我们将加载一个不同的模型，WaveGAN，由Chris Donahue等人发布，标题为*WaveGAN: 使用生成对抗网络学习合成原始音频*（2018年；[https://arxiv.org/abs/1802.04208](https://arxiv.org/abs/1802.04208)）。'
- en: 'Donahue and others train a GAN in an unsupervised setting for the synthesis
    of raw audio waveforms. They try two different strategies:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Donahue等人在无监督环境中训练了一个GAN以合成原始音频波形。他们尝试了两种不同的策略：
- en: A **spectrogram–strategy** (**SpecGAN**), where they use a DCGAN (please refer
    to the *Generating images* recipe in [Chapter 7](f386de9e-b56d-4b39-bf36-803860def385.xhtml), *Advanced
    Image Applications*), and apply it to spectrograms (frequency over time)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种名为**Spectrogram-Strategy**（**SpecGAN**）的方法，他们使用了DCGAN（请参阅[第7章](f386de9e-b56d-4b39-bf36-803860def385.xhtml)中的*生成图像*章节，*高级图像应用*），并将其应用于频谱图（频率随时间变化）
- en: A **waveform****–strategy** (**WaveGAN**), where they flatten the architecture
    (1D convolutions)
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种名为**Waveform-Strategy**（**WaveGAN**）的方法，他们将架构展平（1D卷积）
- en: For the first strategy, they had to develop a spectrogram that they could convert
    back to text.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一种策略，他们必须开发一个能够将频谱图转换回文本的方法。
- en: For the WaveGAN, they flattened the 2D convolutions into 1D while keeping the
    size (for example, a kernel of 5x5 became a 1D kernel of 25). Strides of 2x2 became
    4\. They removed the batch normalization layers. They trained using a Wasserstein
    GAN-GP strategy (Ishaan Gulrajani and others, 2017; *Im**proved training of Wasserstein
    GANs*; [https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于WaveGAN，他们将2D卷积展平为1D，同时保持大小（例如，5x5的核变为25的1D核）。步幅为2x2变为4。他们移除了批标准化层。他们使用了Wasserstein
    GAN-GP策略进行训练（Ishaan Gulrajani等人，2017年；*Wasserstein GANs的改进训练*；[https://arxiv.org/abs/1704.00028](https://arxiv.org/abs/1704.00028)）。
- en: Their WaveGAN performed notably worse in human judgments (mean opinion scores)
    than their SpecGAN. You can find a few examples of generated sounds at [https://chrisdonahue.com/wavegan_examples/](https://chrisdonahue.com/wavegan_examples/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的 WaveGAN 在人类评判（平均意见分数）方面表现明显不如他们的 SpecGAN。您可以在 [https://chrisdonahue.com/wavegan_examples/](https://chrisdonahue.com/wavegan_examples/)
    找到一些生成的声音示例。
- en: There's more...
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多......
- en: We can also use the WaveGAN model to synthesize speech from text.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 WaveGAN 模型从文本合成语音。
- en: 'We''ll download the checkpoints of a model trained on the speech commands we
    encountered in the previous recipe, *Recognizing voice commands*. Then we''ll
    run the model to generate speech:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载在前一教程中遇到的语音命令上训练的模型检查点。然后我们将运行模型生成语音：
- en: '**Downloading the TensorFlow model checkpoints**: We''ll download the model
    data as follows:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载 TensorFlow 模型检查点**：我们将按以下方式下载模型数据：'
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can load the computation graph into memory:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将计算图加载到内存中：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can now generate speech.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成语音。
- en: '**Generating speech**: The model architecture involves a latent representation
    of the letters. We can listen to what the model constructs based on random initializations
    of the latent representation:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成语音**：模型架构涉及字母的潜在表示。我们可以根据潜在表示的随机初始化来听模型构建的内容：'
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This should show us two examples of generated sounds, each with a Jupyter widget:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该展示了两个使用 Jupyter 小部件的生成声音示例：
- en: '![](img/49495435-bc36-4d82-824f-1741e83b9903.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49495435-bc36-4d82-824f-1741e83b9903.png)'
- en: If these don't sound particularly natural, don't be afraid. After all, we've
    used a random initialization of the latent space.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些听起来并不特别自然，不要担心。毕竟，我们使用了潜在空间的随机初始化。
- en: See also
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks
    with Guided Attention* ([https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969)).
    on Erdene-Ochir Tuguldur''s GitHub repository, you can find a PyTorch implementation
    of that paper. The Mongolian text-to-speech was trained on 5 hours of audio from
    the Mongolian Bible: [https://github.com/tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*基于深度卷积网络和引导注意力的高效可训练文本到语音系统* ([https://arxiv.org/abs/1710.08969](https://arxiv.org/abs/1710.08969))。在
    Erdene-Ochir Tuguldur 的 GitHub 仓库中，您可以找到该论文的 PyTorch 实现。蒙古文文本到语音是在《蒙古圣经》的 5 小时音频上训练的：[https://github.com/tugstugi/pytorch-dc-tts](https://github.com/tugstugi/pytorch-dc-tts)。'
- en: On Chris Donahue's GitHub repository of WaveGAN, you can see the WaveGAN implementation
    and examples for training from audio files in formats such as MP3, WAV, OGG, and
    others without preprocessing ([https://github.com/chrisdonahue/wavegan](https://github.com/chrisdonahue/wavegan)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Chris Donahue 的 WaveGAN GitHub 仓库中，您可以看到 WaveGAN 的实现以及从 MP3、WAV、OGG 等格式的音频文件中进行训练的示例，而无需预处理
    ([https://github.com/chrisdonahue/wavegan](https://github.com/chrisdonahue/wavegan))。
- en: Mozilla open sourced their TensorFlow implementation of Baidu's Deep Speech
    architecture (2014), which you can find here: [https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Mozilla 开源了他们的 TensorFlow 实现 Baidu 的 Deep Speech 架构（2014 年），您可以在这里找到：[https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech)。
- en: Generating melodies
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成旋律
- en: '**Artificial intelligence** (**AI**) in music is a fascinating topic. Wouldn''t
    it be cool if your favorite group from the 70s was bringing out new songs, but
    maybe more modern? Sony did this with the Beatles, and you can hear a song on
    YouTube, complete with automatically generated lyrics, called *Daddy''s car*: [https://www.youtube.com/watch?v=LSHZ_b05W7o](https://www.youtube.com/watch?v=LSHZ_b05W7o).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能** (**AI**) 在音乐中是一个迷人的话题。如果您最喜欢的 70 年代乐队正在推出新歌，但可能更现代化会很酷吧？索尼与披头士合作做到了这一点，您可以在
    YouTube 上听到一首歌，完整地包含了自动生成的歌词，名为 *Daddy''s car*：[https://www.youtube.com/watch?v=LSHZ_b05W7o](https://www.youtube.com/watch?v=LSHZ_b05W7o)。'
- en: In this recipe, we'll be generating a melody. More specifically, we'll be continuing
    a song using functionality in the Magenta Python library.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将生成一个旋律。更具体地说，我们将使用 Magenta Python 库中的功能继续一首歌曲。
- en: Getting ready
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We need to install the Magenta library, and a few system libraries as dependencies.
    Please note that you need admin privileges in order to install system dependencies. If
    you are not on Linux (or *nix), you'll have to find the ones corresponding to
    your system.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装 Magenta 库以及一些系统库作为依赖。请注意，为了安装系统依赖项，您需要管理员权限。如果您不是在 Linux（或 *nix）上，您将需要找到与您的系统对应的依赖项。
- en: 'On macOS, this should be relatively straightforward. Otherwise, it might be
    easier to run this in a Colab environment:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在macOS上，这应该相对简单。否则，在Colab环境中运行可能更容易：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you are on Colab, you need another tweak to allow Python to find your system
    libraries:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Colab上，您需要另一种调整以允许Python找到您的系统库：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This is a clever workaround for Python's foreign library import system, taken
    from the original Magenta tutorial, at [https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb](https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Python外部库导入系统的一个聪明的解决方法，取自原始的Magenta教程，位于[https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb](https://colab.research.google.com/notebooks/magenta/hello_magenta/hello_magenta.ipynb)。
- en: It's time to get creative!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候发挥创造力了！
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We''ll first put together the start of a melody, and then we will load the
    `MelodyRNN` model from Magenta and let it continue the melody:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先组合一段旋律的开头，然后从Magenta加载`MelodyRNN`模型让它继续旋律：
- en: 'Let''s put a melody together. We''ll take *Twinkle Twinkle Little Star*. The
    Magenta project works with a note sequence representation called `NoteSequence`,
    which comes with many utilities, including conversion to and from MIDI. We can
    add notes to a sequence like this:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们一起编曲。我们将使用*小星星*。Magenta项目使用一种名为`NoteSequence`的音符序列表示，附带许多实用工具，包括与MIDI之间的转换。我们可以像这样向序列添加音符：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can visualize the sequence using Bokeh, and then we can play the note sequence:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Bokeh可视化序列，然后播放音符序列：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This looks as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来如下：
- en: '![](img/0820a4e4-87cd-483f-8680-e38f3789ca89.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0820a4e4-87cd-483f-8680-e38f3789ca89.png)'
- en: We can listen to the first 9 seconds of the song.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以听歌的前9秒。
- en: 'Let''s load the `MelodyRNN` model from `magenta`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从`magenta`加载`MelodyRNN`模型：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This should only take a few seconds. The Magenta model is remarkably small compared
    to some other models we've encountered in this book.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该只需几秒钟。与本书中遇到的其他模型相比，Magenta模型非常小。
- en: 'We can now feed in our previous melody, along with a few parameters in order
    to continue the song:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将之前的旋律和一些参数一起输入，以继续这首歌曲：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now plot and play the new music:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制和播放新音乐：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once again, we get the Bokeh library plot and a play widget:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们得到了Bokeh库的绘图和播放小部件：
- en: '![](img/142f9df4-6d24-4be8-8268-20ea3b437323.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/142f9df4-6d24-4be8-8268-20ea3b437323.png)'
- en: 'We can create a MIDI file from our note sequence like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样从我们的音符序列创建一个MIDI文件：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This creates a new MIDI file on disk.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在磁盘上创建一个新的MIDI文件。
- en: 'On Google Colab, we can download the MIDI file like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在Google Colab上，我们可以这样下载MIDI文件：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We can feed different melodies via MIDI files into the model, or we can try
    with other parameters; we can increase or decrease the randomness (the `temperature` parameter),
    or let the sequence continue for longer periods (the `num_steps` parameter).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过MIDI文件将不同的旋律输入模型，或者我们可以尝试其他参数；我们可以增加或减少随机性（`temperature`参数），或让序列继续较长时间（`num_steps`参数）。
- en: How it works...
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: MelodyRNN is an LSTM-based language model for musical notes. In order to understand
    MelodyRNN, we first need to understand how **Long Short-Term Memory** (**LSTM**)
    works. Published in 1997 by Sepp Hochreiter and Jürgen Schmidhuber (*Long short-term
    memory*: [https://doi.org/10.1162%2Fneco.1997.9.8.1735](https://doi.org/10.1162%2Fneco.1997.9.8.1735)),
    and updated numerous times since, LSTM is the most well-known example of a **Recurrent
    Neural Network** (**RNN**) and represents a state-of-the-art model for image recognition
    and machine learning tasks with sequences such as speech recognition, natural
    language processing, and time series. LSTMs were, or have been, behind popular
    tools by Google, Amazon, Microsoft, and Facebook for voice recognition and language
    translation.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: MelodyRNN是一种基于LSTM的音符语言模型。为了理解MelodyRNN，我们首先需要了解**长短期记忆**（**LSTM**）的工作原理。1997年由Sepp
    Hochreiter和Jürgen Schmidhuber发布（*长短期记忆*：[https://doi.org/10.1162%2Fneco.1997.9.8.1735](https://doi.org/10.1162%2Fneco.1997.9.8.1735)），LSTM是**循环神经网络**（**RNN**）的最著名例子，代表了处理图像识别、语音识别、自然语言处理和时间序列等任务的最先进模型。LSTM曾或正在背后支持Google、Amazon、Microsoft和Facebook的流行工具，用于语音识别和语言翻译。
- en: 'The basic unit of an LSTM layer is an LSTM cell, which consists of several
    regulators, which we can see in the following schematic:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层的基本单元是LSTM单元，由几个调节器组成，我们可以在下面的示意图中看到：
- en: '![](img/487e29d1-87a7-44b2-b4b2-478e2eecd894.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/487e29d1-87a7-44b2-b4b2-478e2eecd894.png)'
- en: This diagram is based on Alex Graves and others, *Speech recognition with deep
    recurrent neural networks*, (2013), taken from the English language Wikipedia
    article on LSTMs at [https://en.wikipedia.org/wiki/Long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本图表基于Alex Graves及其它人的*深度递归神经网络语音识别*（2013），摘自维基百科关于LSTMs的英语条目，网址为[https://en.wikipedia.org/wiki/Long_short-term_memory](https://en.wikipedia.org/wiki/Long_short-term_memory)。
- en: 'The regulators include the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 监管机构包括以下内容：
- en: An input gate
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入门
- en: An output gate
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出门
- en: A forget gate
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个遗忘门
- en: We can explain the intuition behind these gates without getting lost in the
    equations. An input gate regulates how strongly the input influences the cell,
    an output gate dampens the outgoing cell activation, and the forget gate is a
    decay on the cell activity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以解释这些门背后的直觉，而不陷入方程的细节。一个输入门调节输入对单元的影响力，一个输出门减少传出单元的激活，而遗忘门则衰减单元的活动。
- en: LSTMs have the advantage of being able to handle sequences of different lengths.
    However, their performance deteriorates with longer sequences. In order to learn
    even longer sequences, the Magenta library provides a model that includes an attention
    mechanism (Dzmitry Bahdanau and others, 2014, *Neural Machine Translation by Jointly
    Learning to Align and Translate*; [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)). Bahdanau
    and others showed that their attention mechanism leads to a much improved performance
    on longer sequences.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs能够处理不同长度的序列，这是它们的优点。然而，随着序列长度的增加，它们的性能会下降。为了学习更长的序列，Magenta库提供了包含注意力机制的模型（Dzmitry
    Bahdanau及其它人，2014年，*通过联合学习对齐和翻译来进行神经机器翻译*；[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)）。Bahdanau及其它人表明，他们的注意力机制显著改善了处理长序列的性能。
- en: 'In MelodyRNN, an attention mask *a* is applied as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在MelodyRNN中，注意力掩码*a*的应用如下：
- en: '![](img/a2fa2dce-f0eb-485b-a755-072fafefcdba.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2fa2dce-f0eb-485b-a755-072fafefcdba.png)'
- en: You can find more details in the Magenta documentation at [https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/](https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Magenta文档中找到更多细节：[https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/](https://magenta.tensorflow.org/2016/07/15/lookback-rnn-attention-rnn/)。
- en: See also
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: Please note that Magenta has different variations of the MelodyRNN model available
    ([https://github.com/magenta/magenta/tree/master/magenta/models/melody_rnn](https://github.com/magenta/magenta/tree/master/magenta/models/melody_rnn)).
    Apart from MelodyRNN, Magenta provides further models, including a variational
    autoencoder for music generation, and many browser-based tools for exploring and
    generating music: [https://github.com/magenta/magenta](https://github.com/magenta/magenta).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Magenta有不同版本的MelodyRNN模型可用（[https://github.com/magenta/magenta/tree/master/magenta/models/melody_rnn](https://github.com/magenta/magenta/tree/master/magenta/models/melody_rnn)）。除了MelodyRNN外，Magenta还提供了其他模型，包括用于生成音乐的变分自编码器，以及许多基于浏览器的工具用于探索和生成音乐：[https://github.com/magenta/magenta](https://github.com/magenta/magenta)。
- en: DeepBeat is a project for hip-hop beat generation: [https://github.com/nicholaschiang/deepbeat](https://github.com/nicholaschiang/deepbeat).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DeepBeat是一个用于嘻哈节拍生成的项目：[https://github.com/nicholaschiang/deepbeat](https://github.com/nicholaschiang/deepbeat)。
- en: 'Jukebox is an open sourced project based on the paper *Jukebox: A Generative
    Model for Music*, by Dhariwal and others (2020; [https://arxiv.org/abs/2005.00341](https://arxiv.org/abs/2005.00341)).
    You can find many audio samples at [https://openai.com/blog/jukebox/](https://openai.com/blog/jukebox/).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jukebox是基于Dhariwal及其它人的论文*Jukebox: A Generative Model for Music*（2020年；[https://arxiv.org/abs/2005.00341](https://arxiv.org/abs/2005.00341)）的开源项目。您可以在[https://openai.com/blog/jukebox/](https://openai.com/blog/jukebox/)找到许多音频样本。'
- en: You can find the original implementation of Parag K. Mital's NIPS paper, *Time
    Domain Neural Audio Style Transfer* (2017; [https://arxiv.org/abs/1711.11160](https://arxiv.org/abs/1711.11160)),
    at [https://github.com/pkmital/time-domain-neural-audio-style-transfer](https://github.com/pkmital/time-domain-neural-audio-style-transfer).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/pkmital/time-domain-neural-audio-style-transfer](https://github.com/pkmital/time-domain-neural-audio-style-transfer)找到Parag
    K. Mital的NIPS论文*时域神经音频风格转换*（2017年）的原始实现。
