- en: '*Chapter 6*: Fine-Tuning Language Models for Token Classification'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第六章*：为标记分类对语言模型进行微调'
- en: In this chapter, we will learn about fine-tuning language models for token classification.
    Tasks such as **Named Entity Recognition** (**NER**), **Part-of-Speech** (**POS**)
    tagging, and **Question Answering** (**QA**) are explored in this chapter. We
    will learn how a specific language model can be fine-tuned on such tasks. We will
    focus on BERT more than other language models. You will learn how to apply POS,
    NER, and QA using BERT. You will get familiar with the theoretical details of
    these tasks such as their respective datasets and how to perform them. After finishing
    this chapter, you will be able to perform any token classification using Transformers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习为标记分类对语言模型进行微调。本章探讨了诸如**命名实体识别**（**NER**）、**词性**（**POS**）标注和**问答**（**QA**）等任务。我们将学习如何将特定语言模型微调用于此类任务。我们将更多地关注
    BERT，而不是其他语言模型。您将学习如何使用 BERT 应用 POS、NER 和 QA。您将熟悉这些任务的理论细节，如它们各自的数据集以及如何执行它们。完成本章后，您将能够使用
    Transformers 执行任何标记分类。
- en: 'In this chapter, we will fine-tune BERT for the following tasks: fine-tuning
    BERT for token classification problems such as NER and POS, fine-tuning a language
    model for an NER problem, and thinking of the QA problem as a start/stop token
    classification.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将为以下任务微调 BERT：为 NER 和 POS 等标记分类问题微调 BERT，为 NER 问题微调语言模型，并将 QA 问题视为起始/终止标记分类。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to token classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍标记分类
- en: Fine-tuning language models for NER
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 NER 进行语言模型微调
- en: Question answering using token classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记分类进行问答
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using Jupyter Notebook to run our coding exercises and Python 3.6+
    and the following packages need to be installed:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Jupyter Notebook 运行我们的编码练习，并且需要安装 Python 3.6+ 和以下软件包：
- en: '`sklearn`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`'
- en: '`transformers 4.0+`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers 4.0+`'
- en: '`Datasets`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`数据集`'
- en: '`seqeval`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seqeval`'
- en: 'All notebooks with coding exercises will be available at the following GitHub
    link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有带有编码练习的笔记本都将在以下 GitHub 链接中提供：[https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH06)。
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/2UGMQP2](https://bit.ly/2UGMQP2)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看实际代码视频：[https://bit.ly/2UGMQP2](https://bit.ly/2UGMQP2)
- en: Introduction to token classification
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍标记分类
- en: The task of classifying each token in a token sequence is called **token classification**.
    This task says that a specific model must be able to classify each token into
    a class. POS and NER are two of the most well-known tasks in this criterion. However,
    QA is also another major NLP task that fits in this category. We will discuss
    the basics of these three tasks in the following sections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将标记序列中的每个标记分类的任务称为**标记分类**。该任务要求特定模型能够将每个标记分类到一个类别中。POS 和 NER 是这一标准中最知名的两个任务。然而，QA
    也是另一个属于这一类别的重要 NLP 任务。我们将在以下章节讨论这三个任务的基础知识。
- en: Understanding NER
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 NER
- en: 'One of the well-known tasks in the category of token classification is NER
    – the recognition of each token as an entity or not and identifying the type of
    each detected entity. For example, a text can contain multiple entities at the
    same time – person names, locations, organizations, and other types of entities.
    The following text is a clear example of NER:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记分类类别中一个著名的任务是 NER - 将每个标记识别为实体或非实体，并识别每个检测到的实体的类型。例如，文本可以同时包含多个实体 - 人名、地名、组织名和其他类型的实体。以下文本是
    NER 的明显示例：
- en: '`George Washington is one the presidents of the United States of America.`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`乔治·华盛顿是美利坚合众国的总统之一。`'
- en: '*George Washington* is a person name while *the* *United States of America*
    is a location name. A sequence tagging model is expected to tag each word in the
    form of tags, each containing information about the tag. BIO''s tags are the ones
    that are universally used for standard NER tasks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*乔治·华盛顿*是一个人名，而*美利坚合众国*是一个地名。序列标注模型应该能够以标签的形式标记每个单词，每个标签都包含有关该标签的信息。BIO 的标签是标准
    NER 任务中通用的标签。'
- en: 'The following table is a list of tags and their descriptions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格是标签及其描述的列表：
- en: '![Table 1 – Table of BIOS tags and their descriptions ](img/B17123_06_Table_1.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![表 1 – BIOS 标签及其描述表](img/B17123_06_Table_1.jpg)'
- en: Table 1 – Table of BIOS tags and their descriptions
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1 – BIOS 标签及其描述表
- en: 'From this table, **B** indicates the beginning of a tag, and **I** denotes
    the inside of a tag, while **O** is the outside of the entity. This is the reason
    that this type of annotation is called **BIO**. For example, the sentence shown
    earlier can be annotated using BIO:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个表格可以看出，**B** 表示标记的开始，**I** 表示标记的内部，而 **O** 则表示实体的外部。这就是为什么这种类型的标注被称为 **BIO**。例如，前面显示的句子可以使用
    BIO 进行标注：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Accordingly, the sequence must be tagged in BIO format. A sample dataset can
    be in the format shown as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，序列必须以 BIO 格式进行标记。一个样本数据集可以使用如下格式：
- en: '![Figure 6.1 – CONLL2003 dataset ](img/B17123_06_002.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – CONLL2003 数据集](img/B17123_06_002.jpg)'
- en: Figure 6.1 – CONLL2003 dataset
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – CONLL2003 数据集
- en: In addition to the NER tags we have seen, there are POS tags available in this
    dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们见过的 NER 标签外，该数据集还包含了 POS 标签
- en: Understanding POS tagging
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 POS 标记
- en: POS tagging, or grammar tagging, is annotating a word in a given text according
    to its respective part of speech. As a simple example, in a given text, identification
    of each word's role in the categories of noun, adjective, adverb, and verb is
    considered to be POS. However, from a linguistic perspective, there are many roles
    other than these four.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: POS 标记，或语法标记，是根据给定文本中的各自词的词性对其进行标注。举个简单的例子，在给定文本中，识别每个词的角色，如名词、形容词、副词和动词都被认为是词性标注。然而，从语言学角度来看，除了这四种角色外还有很多其他角色。
- en: 'In the case of POS tags, there are variations, but the Penn Treebank POS tagset
    is one of the most well-known ones. The following screenshot shows a summary and
    respective description of these roles:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 POS 标签的情况下，有各种变化，但是宾州树库的 POS 标签集是最著名的之一。下面的截图显示了这些角色的摘要和相应的描述：
- en: '![Figure 6.2 – Penn Treebank POS tags ](img/B17123_06_003.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 宾州树库 POS 标签](img/B17123_06_003.jpg)'
- en: Figure 6.2 – Penn Treebank POS tags
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 宾州树库 POS 标签
- en: Datasets for POS tasks are annotated like the example shown in *Figure 6.1*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: POS 任务的数据集如 *图 6.1* 所示进行了标注。
- en: The annotation of these tags is very useful in specific NLP applications and
    is one of the building blocks of many other methods. Transformers and many advanced
    models can somehow understand the relation of words in their complex architecture.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签的标注在特定的 NLP 应用中非常有用，是许多其他方法的基石之一。变压器和许多先进模型在其复杂的结构中某种程度上能理解单词之间的关系。
- en: Understanding QA
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 QA
- en: A QA or reading comprehension task comprises a set of reading comprehension
    texts with respective questions on them. An exemplary dataset from this scope
    is **SQUAD** or **Stanford Question Answering Dataset**. This dataset consists
    of Wikipedia texts and respective questions asked about them. The answers are
    in the form of segments of the original Wikipedia text.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: QA 或阅读理解任务包括一组阅读理解文本，并相应地提出问题。这个范围内的示例数据集包括 **SQUAD** 或 **斯坦福问答数据集**。该数据集由维基百科文本和关于它们提出的问题组成。答案以原始维基百科文本的片段形式给出。
- en: 'The following screenshot shows an example of this dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了这个数据集的一个示例：
- en: '![Figure 6.3 – SQUAD dataset example ](img/B17123_06_004.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – SQUAD 数据集示例](img/B17123_06_004.jpg)'
- en: Figure 6.3 – SQUAD dataset example
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – SQUAD 数据集示例
- en: The highlighted red segments are the answers and important parts of each question
    are highlighted in blue. It is required for a good NLP model to segment text according
    to the question, and this segmentation can be done in the form of sequence labeling.
    The model labels the start and the end of the segment as answer start and end
    segments.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 突出显示的红色部分是答案，每个问题的重要部分用蓝色突出显示。要求一个良好的 NLP 模型按照问题对文本进行分割，这种分割可以通过序列标注的形式进行。模型会将答案的开始和结束部分标记为答案的起始和结束部分。
- en: Up to this point, you have learned the basics of modern NLP sequence tagging
    tasks such as QA, NER, and POS. In the next section, you will learn how it is
    possible to fine-tune BERT for these specific tasks and use the related datasets
    from the `datasets` library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学会了现代 NLP 序列标注任务的基础知识，如 QA、NER 和 POS。在接下来的部分，你将学习如何对这些特定任务进行 BERT 微调，并使用
    `datasets` 库中相关的数据集。
- en: Fine-tuning language models for NER
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 NER 微调语言模型
- en: In this section, we will learn how to fine-tune BERT for an NER task. We first
    start with the `datasets` library and by loading the `conll2003` dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何为 NER 任务微调 BERT。我们首先从 `datasets` 库开始，并加载 `conll2003` 数据集。
- en: 'The dataset card is accessible at [https://huggingface.co/datasets/conll2003](https://huggingface.co/datasets/conll2003).
    The following screenshot shows this model card from the HuggingFace website:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集卡片可在 [https://huggingface.co/datasets/conll2003](https://huggingface.co/datasets/conll2003)
    上访问。以下截图显示了来自 HuggingFace 网站的此模型卡片：
- en: '![Figure 6.4 – CONLL2003 dataset card from HuggingFace ](img/B17123_06_005.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 来自 HuggingFace 的 CONLL2003 数据集卡片](img/B17123_06_005.jpg)'
- en: Figure 6.4 – CONLL2003 dataset card from HuggingFace
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 来自 HuggingFace 的 CONLL2003 数据集卡片
- en: 'From this screenshot, it can be seen that the model is trained on this dataset
    and is currently available and listed in the right panel. However, there are also
    descriptions of the dataset such as its size and its characteristics:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从此截图中可以看出，模型是在此数据集上进行训练的，目前可用，并在右侧面板中列出。但是，还有关于数据集的描述，例如其大小和特征：
- en: 'To load the dataset, the following commands are used:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要加载数据集，使用以下命令：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A download progress bar will appear and after finishing the downloading and
    caching, the dataset will be ready to use. The following screenshot shows the
    progress bars:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将出现一个下载进度条，下载和缓存完成后，数据集将准备好供使用。以下截图显示了进度条：
- en: '![Figure 6.5 – Downloading and preparing the dataset ](img/B17123_06_006.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.5 – 下载和准备数据集](img/B17123_06_006.jpg)'
- en: Figure 6.5 – Downloading and preparing the dataset
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.5 – 下载和准备数据集
- en: 'You can easily double-check the dataset by accessing the train samples using
    the following command:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过使用以下命令访问训练样本轻松地检查数据集：
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following screenshot shows the result:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了结果：
- en: '![Figure 6.6 – CONLL2003 train samples from the datasets library ](img/B17123_06_007.jpg)'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.6 – 从 datasets 库获取的 CONLL2003 训练样本](img/B17123_06_007.jpg)'
- en: Figure 6.6 – CONLL2003 train samples from the datasets library
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.6 – 从 datasets 库获取的 CONLL2003 训练样本
- en: 'The respective tags for POS and NER are shown in the preceding screenshot.
    We will use only NER tags for this part. You can use the following command to
    get the NER tags available in this dataset:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述截图显示了 POS 和 NER 的相应标签。我们将仅使用此部分的 NER 标签。您可以使用以下命令获取此数据集中可用的 NER 标签：
- en: '[PRE3]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is also shown in *Figure 6.7*. All the BIO tags are shown and there
    are nine tags in total:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果也显示在 *图 6.7* 中。所有 BIO 标签都显示在此处，共有九个标签：
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to load the BERT tokenizer:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是加载 BERT 分词器：
- en: '[PRE5]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `tokenizer` class can work with white-space tokenized sentences also. We
    need to enable our tokenizer for working with white-space tokenized sentences,
    because the NER task has a token-based label for each token. Tokens in this task
    are usually the white-space tokenized words rather than BPE or any other tokenizer
    tokens. According to what is said, let''s see how `tokenizer` can be used with
    a white-space tokenized sentence:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenizer` 类也可以处理基于空格的分词句子。我们需要启用我们的分词器以处理基于空格的分词句子，因为命名实体识别任务为每个标记有一个基于标记的标签。在这个任务中，标记通常是基于空格分词的单词，而不是
    BPE 或任何其他分词器的标记。根据所说的内容，让我们看看 `tokenizer` 如何与基于空格的分词句子一起使用：'
- en: '[PRE6]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, by just setting `is_split_into_words` to `True`, the problem
    is solved.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您所见，仅需将 `is_split_into_words` 设置为 `True`，问题就解决了。
- en: 'It is required to preprocess the data before using it for training. To do so,
    we must use the following function and map into the entire dataset:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用数据进行训练之前，必须对数据进行预处理。为此，我们必须使用以下函数并将其映射到整个数据集中：
- en: '[PRE7]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This function will make sure that our tokens and labels are aligned properly.
    This alignment is required because the tokens are tokenized in pieces, but the
    words must be of one piece. To test and see how this function works, you can run
    it by giving a single sample to it:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此函数将确保我们的标记和标签正确对齐。此对齐是必需的，因为标记是分割成片段的，但单词必须是一个整体。要测试并查看此函数的工作原理，您可以将单个样本提供给它并运行它：
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And the result is shown as follows:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE9]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'But this result is not readable, so you can run the following code to have
    a readable version:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是这个结果是不可读的，所以您可以运行以下代码以获得可读版本：
- en: '[PRE10]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result is shown as follows:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![Figure 6.7 – Result of the tokenize and align functions](img/B17123_06_008.jpg)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.7 – tokenize 和 align 函数的结果](img/B17123_06_008.jpg)'
- en: Figure 6.7 – Result of the tokenize and align functions
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.7 – tokenize 和 align 函数的结果
- en: 'The mapping of this function to the dataset can be done by using the `map`
    function of the `datasets` library:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数与数据集的映射可以通过`datasets`库的`map`函数完成：
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the next step, it is required to load the BERT model with the respective
    number of labels:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '接下来，需要加载具有相应标签数量的 BERT 模型:'
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The model will be loaded and ready to be trained. In the next step, we must
    prepare the trainer and training parameters:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型将被加载并准备好进行训练。在接下来的步骤中，我们必须准备好训练器和训练参数：
- en: '[PRE13]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'It is required to prepare the data collator. It will apply batch operations
    on the training dataset to use less memory and perform faster. You can do so as
    follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '需要准备数据收集器。它将在训练数据集上应用批量操作，以使用更少的内存并执行更快。您可以像下面这样做:'
- en: '[PRE14]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To be able to evaluate model performance, there are many metrics available
    for many tasks in HuggingFace''s `datasets` library. We will be using the sequence
    evaluation metric for NER. seqeval is a good Python framework to evaluate sequence
    tagging algorithms and models. It is necessary to install the `seqeval` library:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '为了能够评估模型的性能，在 HuggingFace 的`datasets`库中有许多任务的许多指标可用。我们将使用用于 NER 的序列评估指标。seqeval
    是一个用于评估序列标记算法和模型的良好 Python 框架。需要安装`seqeval`库:'
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Afterward, you can load the metric:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，您可以加载指标：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It is easily possible to see how the metric works by using the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '通过以下代码，很容易看出指标是如何工作的:'
- en: '[PRE17]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result is as follows:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '结果如下:'
- en: '![Figure 6.8 – Output of the seqeval metric ](img/B17123_06_009.jpg)'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 6.8 – seqeval 指标的输出](img/B17123_06_009.jpg)'
- en: Figure 6.8 – Output of the seqeval metric
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.8 – seqeval 指标的输出
- en: Various metrics such as accuracy, F1-score, precision, and recall are computed
    for the sample input.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于样本输入，计算各种指标，如准确率、F1 分数、精确度和召回率。
- en: 'The following function is used to compute the metrics:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '以下函数用于计算指标:'
- en: '[PRE18]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The last steps are to make a trainer and train it accordingly:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最后一步是制作训练器并相应地对其进行训练:'
- en: '[PRE19]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After running the `train` function of `trainer`, the result will be as follows:![Figure
    6.9 – Trainer results after running train ](img/B17123_06_010.jpg)
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`trainer`的`train`函数后，结果如下所示:![图 6.9-运行 train 后的 Trainer 结果](img/B17123_06_010.jpg)
- en: Figure 6.9 – Trainer results after running train
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.9-运行 train 后的 Trainer 结果
- en: 'It is necessary to save the model and tokenizer after training:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在训练后，必须保存模型和分词器:'
- en: '[PRE20]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you wish to use the model with the pipeline, you must read the config file
    and assign `label2id` and `id2label` correctly according to the labels you have
    used in the `label_list` object:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '如果您希望使用管道（pipeline）使用模型，则必须读取配置文件，并根据`label_list`对象中使用的标签正确分配`label2id`和`id2label`:'
- en: '[PRE21]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Afterward, it is easy to use the model as in the following example:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '之后，您可以像以下示例一样轻松使用模型:'
- en: '[PRE22]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And the result will appear as seen here:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '然后结果将如下所示:'
- en: '[PRE23]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Up to this point, you have learned how to apply POS using BERT. You learned
    how to train your own POS tagging model using Transformers and you also tested
    the model. In the next section, we will focus on QA.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何使用 BERT 应用 POS。您了解了如何使用 Transformers 训练自己的 POS 标注模型，并对模型进行了测试。在接下来的部分，我们将专注于
    QA。
- en: Question answering using token classification
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用令牌分类进行问答
- en: A **QA** problem is generally defined as an NLP problem with a given text and
    a question for AI, and getting an answer back. Usually, this answer can be found
    in the original text but there are different approaches to this problem. In the
    case of **Visual Question Answering** (**VQA**), the question is about a visual
    entity or visual concept rather than text but the question itself is in the form
    of text.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**QA**问题通常被定义为一个 NLP 问题，给定一个文本和一个问题，需要 AI 返回一个答案。通常，这个答案可以在原始文本中找到，但对于这个问题存在不同的方法。在**视觉问答**（**VQA**）的情况下，问题涉及的是视觉实体或视觉概念，而不是文本，但问题本身是以文本形式呈现的。
- en: 'Some examples of VQA are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '一些 VQA 的示例如下:'
- en: '![Figure 6.10 – VQA examples ](img/B17123_06_011.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – VQA 示例](img/B17123_06_011.jpg)'
- en: Figure 6.10 – VQA examples
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – VQA 示例
- en: 'Most of the models that are intended to be used in VQA are multimodal models
    that can understand the visual context along with the question and generate the
    answer properly. However, unimodal fully textual QA or just QA is based on textual
    context and textual questions with respective textual answers:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: VQA 中大多数模型都是多模态模型，可以理解视觉上下文以及问题，并能正确生成答案。然而，单模全文本 QA 或者仅 QA 是基于文本上下文和文本问题以及相应的文本答案：
- en: 'SQUAD is one of the most well-known datasets in the field of QA. To see examples
    of SQUAD and examine them, you can use the following code:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SQUAD是问答领域中最知名的数据集之一。要查看SQUAD的示例并对其进行检查，您可以使用以下代码：
- en: '[PRE24]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following is the result:'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE25]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: However, there is version 2 of the SQUAD dataset, which has more training samples,
    and it is highly recommended to use it. To have an overall understanding of how
    it is possible to train a model for a QA problem, we will focus on the current
    part of this problem.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 但是，SQUAD数据集还有第2版，其中有更多的训练样本，并且强烈建议使用它。为了全面了解如何为QA问题训练模型的可能性，我们将重点放在解决这个问题的当前部分上。
- en: 'To start, load SQUAD version 2 using the following code:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始，使用以下代码加载SQUAD第2版：
- en: '[PRE26]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After loading the SQUAD dataset, you can see the details of this dataset by
    using the following code:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在加载SQUAD数据集之后，您可以通过使用以下代码查看此数据集的详细信息：
- en: '[PRE27]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result is as follows:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![Figure 6.11 – SQUAD dataset (version 2) details ](img/B17123_06_012.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.11 – SQUAD数据集（第2版）详细信息](img/B17123_06_012.jpg)'
- en: Figure 6.11 – SQUAD dataset (version 2) details
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.11 – SQUAD数据集（第2版）详细信息
- en: The details of the SQUAD dataset will be shown as seen in *Figure 6.11*. As
    you can see, there are more than 130,000 training samples with more than 11,000
    validation samples.
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SQUAD数据集的详细信息将显示在*图6.11*中。正如您所看到的，有超过130,000个训练样本和超过11,000个验证样本。
- en: 'As we did for NER, we must preprocess the data to have the right form to be
    used by the model. To do so, you must first load your tokenizer, which is a pretrained
    tokenizer as long as you are using a pretrained model and want to fine-tune it
    for a QA problem:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像我们对NER所做的那样，我们必须预处理数据，使其具有适合模型使用的正确形式。为此，您必须首先加载您的分词器，只要您使用预训练模型并希望为QA问题进行微调：
- en: '[PRE28]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you have seen, we are going to use the `distillBERT` model.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们将使用`distillBERT`模型。
- en: According to our SQUAD example, we need to give more than one text to the model,
    one for the question and one for the context. Accordingly, we need our tokenizer
    to put these two side by side and separate them with the special `[SEP]` token
    because `distillBERT` is a BERT-based model.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据我们的SQUAD示例，我们需要向模型提供不止一个文本，一个用于问题，一个用于上下文。因此，我们的分词器需要将这两个文本并排放在一起，并使用特殊的`[SEP]`标记将它们分开，因为`distillBERT`是基于BERT的模型。
- en: There is another problem in the scope of QA, and it is the size of the context.
    The context size can be longer than the model input size, but we cannot reduce
    it to the size the model accepts. With some problems, we can do so but in QA,
    it is possible that the answer could be in the truncated part. We will show you
    an example where we tackle this problem using document stride.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在QA范围内还有另一个问题，即上下文的大小。上下文的大小可以比模型输入大小长，但我们不能将其缩减到模型接受的大小。对于某些问题，我们可能可以这样做，但在QA中，答案可能在被截断的部分中。我们将向您展示一个示例，展示我们如何使用文档步幅来解决此问题。
- en: 'The following is an example to show how it works using `tokenizer`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一个示例，展示了如何使用`tokenizer`：
- en: '[PRE29]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The stride is the document stride used to return the stride for the second
    part, like a window, while the `return_overflowing_tokens` flag gives the model
    information on whether it should return the extra tokens. The result of `tokenized_example`
    is more than a single tokenized output, instead having two input IDs. In the following,
    you can see the result:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步幅是用于返回第二部分的窗口的文档步幅，而`return_overflowing_tokens`标志向模型提供有关是否应返回额外标记的信息。`tokenized_example`的结果不止一个标记化输出，而是有两个输入ID。在以下，您可以看到结果：
- en: '[PRE30]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Accordingly, you can see the full result by running the following `for` loop:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，您可以通过运行以下`for`循环看到完整的结果：
- en: '[PRE31]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The result is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE32]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see from the preceding output, with a window of 128 tokens, the rest
    of the context is replicated again in the second output of input IDs.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您可以从前面的输出中看到的那样，使用128个标记的窗口，剩余的上下文再次复制到了第二个输出的输入ID中。
- en: Another problem is the end span, which is not available in the dataset, but
    instead, the start span or the start character for the answer is given. It is
    easy to find the length of the answer and add it to the start span, which would
    automatically yield the end span.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个问题是结束跨度，在数据集中不可用，而是给出了答案的开始跨度或开始字符。很容易找到答案的长度并将其添加到起始跨度，这将自动产生结束跨度。
- en: 'Now that we know all the details of this dataset and how to deal with them,
    we can easily put them together to make a preprocessing function (link: [https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py)):'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了该数据集的所有细节以及如何处理它们，我们可以轻松地将它们组合在一起，制作一个预处理函数（链接：[https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py)）：
- en: '[PRE33]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Mapping this function to the dataset would apply all the required changes:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此函数映射到数据集将应用所有所需的更改：
- en: '[PRE34]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Just like other examples, you can now load a pretrained model to be fine-tuned:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像其他示例一样，您现在可以加载预训练的模型进行微调：
- en: '[PRE35]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The next step is to create training arguments:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是创建训练参数：
- en: '[PRE36]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we are not going to use a data collator, we will give a default data collator
    to the model trainer:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们不打算使用数据收集器，我们将为模型训练器提供一个默认的数据收集器：
- en: '[PRE37]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, everything is ready to make the trainer:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，一切准备就绪，可以制作训练器：
- en: '[PRE38]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And the trainer can be used with the `train` function:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练器可以与`train`函数一起使用：
- en: '[PRE39]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The result will be something like the following:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '![Figure 6.12 – Training results ](img/B17123_06_013.jpg)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.12 - 训练结果](img/B17123_06_013.jpg)'
- en: Figure 6.12 – Training results
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12 - 训练结果
- en: As you can see, the model is trained with three epochs and the outputs for loss
    in validation and training are reported.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您所见，该模型使用三个epochs进行训练，并报告验证和训练中的损失输出。
- en: 'Like any other model, you can easily save this model by using the following
    function:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像其他模型一样，您可以轻松地使用以下函数保存此模型：
- en: '[PRE40]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If you want to use your saved model or any other model that is trained on QA,
    the `transformers` library provides a pipeline that's easy to use and implement
    with no extra effort.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您想使用您保存的模型或任何在QA上训练的其他模型，则`transformers`库提供了一个易于使用和实施的管道，无需额外努力。
- en: 'By using this pipeline functionality, you can use any model. The following
    is an example given for using a model with the QA pipeline:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用这个管道功能，您可以使用任何模型。以下是使用QA管道的示例：
- en: '[PRE41]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The pipeline just requires two inputs to make the model ready for usage, the
    model and the tokenizer. Although, you are also required to give it a pipeline
    type, which is QA in the given example.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 管道只需要两个输入，即模型和tokenizer，即可使模型准备就绪。但是，在给定示例中，您还需要给它一个管道类型，即QA。
- en: 'The next step is to give it the inputs it requires, `context` and `question`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是给它提供所需的输入，即`context`和`question`：
- en: '[PRE42]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The model can be used by the following example:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型可以使用以下示例：
- en: '[PRE43]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'And the result can be seen as follows:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '[PRE44]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Up to this point, you have learned how you can train on the dataset you want.
    You have also learned how you can use the trained model using pipelines.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何在想要的数据集上进行训练。您还学会了如何使用管道使用训练好的模型。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we discussed how to fine-tune a pretrained model to any token
    classification task. Fine-tuning models on NER and QA problems were explored.
    Using the pretrained and fine-tuned models on specific tasks with pipelines was
    detailed with examples. We also learned about various preprocessing steps for
    these two tasks. Saving pretrained models that are fine-tuned on specific tasks
    was another major learning point of this chapter. We also saw how it is possible
    to train models with a limited input size on tasks such as QA that have longer
    sequence sizes than the model input. Using tokenizers more efficiently to have
    document splitting with document stride was another important item in this chapter
    too.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何对预训练模型进行微调以适用于任何令牌分类任务。我们探讨了在NER和QA问题上微调模型的方法。使用预训练和微调后的模型在特定任务中使用管道进行详细说明，并给出了示例。我们还了解了这两个任务的各种预处理步骤。保存在特定任务上微调的预训练模型是本章的另一个重点学习内容。我们还看到了如何将具有比模型输入更长序列大小的QA等任务的有限输入大小的模型进行训练的可能性。在本章中，更高效地使用标记器以具有文档间距和文档步幅的文档分割也是另一个重要内容。
- en: In the next chapter, we will discuss text representation methods using Transformers.
    By studying the chapter, you will learn how to perform zero-/few-shot learning
    and semantic text clustering.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论使用Transformer进行文本表示的方法。通过学习本章，您将学习如何执行零/少量样本学习和语义文本聚类。
