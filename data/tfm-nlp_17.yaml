- en: Appendix I — Terminology of Transformer Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录I — 变压器模型术语
- en: The past decades have produced **Convolutional Neural Networks** (**CNNs**),
    **Recurrent Neural Networks** (**RNNs**), and more types of **Artificial Neural
    Networks** (**ANNs**). They all have a certain amount of vocabulary in common.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几十年产生了**卷积神经网络**（**CNNs**）、**循环神经网络**（**RNNs**）和更多类型的**人工神经网络**（**ANNs**）。它们都有一定数量的共同词汇。
- en: Transformer models introduced some new words and used existing words slightly
    differently. This appendix briefly describes transformer models to clarify the
    usage of deep learning vocabulary when applied to transformers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于变压器时，变压器模型引入了一些新词汇，并略微不同地使用现有词汇。本附录简要描述了变压器模型，以澄清深度学习词汇在应用于变压器时的使用情况。
- en: The motivation of transformer model architecture relies upon an industrial approach
    to deep learning. The geometric nature of transformers boosts parallel processing.
    In addition, the architecture of transformers perfectly fits hardware optimization
    requirements. Google, for example, took advantage of the stack structure of transformers
    to design domain-specific optimized hardware that requires less floating-number
    precision.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型架构的动机取决于深度学习的工业化方法。变压器的几何特性增强了并行处理。此外，变压器的架构完全符合硬件优化要求。例如，谷歌利用变压器的堆栈结构设计了领域特定的优化硬件，需要较少的浮点数精度。
- en: Designing transformers models implies taking hardware into account. Therefore,
    the architecture of a transformer combines software and hardware optimization
    from the start.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 设计变压器模型意味着考虑硬件。因此，变压器的架构从一开始就结合了软件和硬件优化。
- en: This appendix defines some of the new usages of neural network language.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录定义了一些神经网络语言的新用法。
- en: Stack
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆栈
- en: A *stack* contains identically sized layers that differ from classical deep
    learning models, as shown in *Figure I.1*. A stack runs from *bottom to top*.
    A stack can be an encoder or a decoder.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*堆栈*包含大小相同的层，与经典的深度学习模型不同，如*图I.1*所示。一个堆栈从*底部到顶部*运行。堆栈可以是编码器或者解码器。
- en: '![](img/B17948_Appendix_I_01.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_I_01.png)'
- en: 'Figure I.1: Layers form a stack'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图I.1：层形成一个堆栈
- en: Transformer stacks learn and see more as they rise in the stacks. Each layer
    transmits what it learned to the next layer just as our memory does.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器堆栈随着升高而学习并看到更多。每一层将其学到的内容传递给下一层，就像我们的记忆一样。
- en: Imagine that a *stack* is the Empire State Building in New York City. At the
    bottom, you cannot see much. But you will see more and farther as you ascend throught
    the offices on higher floors and look out the windows. Finally, at the top, you
    have a fantastic view of Manhattan!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，*堆栈*就像纽约市的帝国大厦。在底部，你看不到太多。但是当你升上更高层的办公室并透过窗户望出去时，你将看到更多，更远的景色。最后，在顶部，你将拥有一个美妙的曼哈顿景色！
- en: Sublayer
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子层
- en: Each layer contains sublayers, as shown in *Figure I.2*. Each sublayer of different
    layers has an identical structure, which boosts hardware optimization.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个层包含子层，如*图I.2*所示。不同层的每个子层具有相同的结构，这有助于硬件优化。
- en: The original Transformer contains two sublayers that run from *bottom to top:*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 原始变压器包含从*底部到顶部*运行的两个子层：
- en: A self-attention sublayer, designed specifically for NLP and hardware optimization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力子层，专为自然语言处理和硬件优化而设计
- en: A classical feedforward network with some tweaking
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种经过一些调整的经典前馈网络
- en: '![](img/B17948_Appendix_I_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_Appendix_I_02.png)'
- en: 'Figure I.2: A layer contains two sublayers'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图I.2：一个层包含两个子层
- en: Attention heads
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意头部
- en: A self-attention sublayer is divided into n independent and identical layers
    called *heads*. For example, the original Transformer contains eight heads.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力子层被分为n个独立和相同的层，称为*头部*。例如，原始变压器包含八个头部。
- en: '*Figure I.3* represents heads as processors to show that transformers’ industrialized
    structure fits hardware design:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*图I.3*将头部表示为处理器，以显示变压器的工业化结构适合硬件设计：'
- en: '![A picture containing text, scoreboard  Description automatically generated](img/B17948_Appendix_I_03.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![包含文字的图片，自动生成描述](img/B17948_Appendix_I_03.png)'
- en: 'Figure I.3: A self-attention sublayer contains heads'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图I.3：自注意力子层包含头部
- en: Note that the attention heads are represented by microprocessors in *Figure
    I.3* to stress the parallel processing power of transformer architectures.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*图I.3*中的注意头部被表示为微处理器，以突出变压器架构的并行处理能力。
- en: Transformer architectures fit both NLP and hardware-optimization requirements.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: transformer 结构适用于自然语言处理和硬件优化需求。
- en: Join our book’s Discord space
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 加入该书的 Discord 工作区，与作者一起参加每月的*问我任何事*活动：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
