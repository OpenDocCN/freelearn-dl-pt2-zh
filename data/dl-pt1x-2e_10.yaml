- en: Working with Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成对抗网络进行工作
- en: All the examples that we saw in the previous chapters were focused on solving
    problems such as classification or regression. This chapter is very interesting
    and important for understanding how deep learning is evolving to solve problems
    in unsupervised learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中看到的所有示例都集中在解决分类或回归等问题上。对于理解深度学习如何发展以解决无监督学习问题，本章非常有趣且重要。
- en: 'In this chapter, we will train networks that learn how to create the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练网络学习如何创建以下内容：
- en: Images based on content and a particular artistic style, popularly called style
    transfer
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于内容和特定艺术风格的图像，通常称为风格转移
- en: Generating faces of new people using a particular type of **generative adversarial
    network** (**GAN**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定类型的**生成对抗网络**（**GAN**）生成新人脸。
- en: These techniques form the basis of most of the advanced research that is happening
    in the deep learning space. Going into the exact specifics of each of the subfields,
    such as GANs and language modeling is beyond the scope of this book, as they deserve
    a separate book for themselves. We will learn how they work in general and the
    process of building them in PyTorch.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术构成了深度学习领域正在进行的大部分先进研究的基础。深入研究每个子领域的具体细节，如GAN和语言建模，超出了本书的范围，它们值得单独的一本书来探讨。我们将学习它们的一般工作原理以及在PyTorch中构建它们的过程。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Neural style transfer
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经风格转移
- en: Introducing GANs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍生成对抗网络
- en: DCGANs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DCGANs
- en: Neural style transfer
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经风格转移
- en: We humans generate artwork with different levels of accuracy and complexity.
    Though the process of creating art can be a very complex process, it can be seen
    as a combination of the two most important factors, namely, what to draw and how
    to draw. What to draw is inspired by what we see around us, and how we draw will
    also take influences from certain things that are found around us. This could
    be an oversimplification from an artist's perspective, but for understanding how
    we can create artwork using deep learning algorithms, it is very useful.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类以不同精度和复杂度生成艺术作品。尽管创建艺术的过程可能非常复杂，但可以看作是两个最重要因素的结合，即要画什么和如何画。要画什么受到我们周围所见的启发，而如何画也会受到我们周围某些事物的影响。从艺术家的角度来看，这可能是一种过于简化的看法，但对于理解如何使用深度学习算法创建艺术作品非常有用。
- en: We will train a deep learning algorithm to take content from one image and then
    draw it according to a specific artistic style. If you are an artist or in the
    creative industry, you can directly use the amazing research that has gone on
    in recent years to improve this and create something cool within the domain you
    work in. Even if you are not, it still introduces you to the field of generative
    models, where networks generate new content.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练一个深度学习算法，从一幅图像中提取内容，然后根据特定的艺术风格进行绘制。如果你是艺术家或者从事创意行业，你可以直接利用最近几年来进行的令人惊叹的研究来改进这一过程，并在你所工作的领域内创造出有趣的东西。即使你不是，它仍然可以向你介绍生成模型的领域，其中网络生成新的内容。
- en: 'Let''s understand what is done in neural style transfer at a high-level, and
    then dive into details, along with the PyTorch code required to build it. The
    style transfer algorithm is provided with a content image (C) and a style image
    (S)—the algorithm has to generate a new image (O) that has the content from the
    content image and the style from the style image. This process of creating neural
    style transfer was introduced by Leon Gates and others in 2015 in their paper,
    *A Neural Algorithm of Artistic Style *([https://arxiv.org/pdf/1508.06576.pdf](https://arxiv.org/pdf/1508.06576.pdf)).
    The following is the content image (C) that we will be using:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从高层次理解神经风格转移的工作，并深入探讨相关细节，以及构建它所需的PyTorch代码。风格转移算法提供了一个内容图像（C）和一个风格图像（S）——算法必须生成一个新图像（O），其中包含来自内容图像的内容和来自风格图像的风格。这种神经风格转移的过程是由Leon
    Gates等人在2015年的论文*《艺术风格的神经算法》*中介绍的（[https://arxiv.org/pdf/1508.06576.pdf](https://arxiv.org/pdf/1508.06576.pdf)）。以下是我们将使用的内容图像（C）：
- en: '![](img/ae2eed1a-2f91-4555-8108-e93918763e13.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae2eed1a-2f91-4555-8108-e93918763e13.png)'
- en: 'And the following is the style image (S):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是风格图像（S）：
- en: '![](img/77e1e4d5-53f2-48c7-a99b-d5a757abaf2f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77e1e4d5-53f2-48c7-a99b-d5a757abaf2f.png)'
- en: 'Source of the preceding image: The Great Wave Off Kanagawa from by Katsushika
    Hokusai ([https://commons.wikimedia.org/wiki/File:The_Great_Wave_off_Kanagawa.jpg](https://commons.wikimedia.org/wiki/File:The_Great_Wave_off_Kanagawa.jpg)[)](https://commons.wikimedia.org/wiki/File:The_Great_Wave_off_Kanagawa.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图片来源：来自葛饰北斋的《神奈川冲浪里》（[https://commons.wikimedia.org/wiki/File:The_Great_Wave_off_Kanagawa.jpg](https://commons.wikimedia.org/wiki/File:The_Great_Wave_off_Kanagawa.jpg)）
- en: 'And this is the image that we will get as the result:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将得到的结果图片：
- en: '![](img/fe2c6633-93e1-407b-af23-90a1f80994ef.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe2c6633-93e1-407b-af23-90a1f80994ef.png)'
- en: The idea behind style transfer becomes clear when you understand how **convolutional
    neural networks** (**CNNs**) work. When CNNs are trained for object recognition,
    the early layers of a trained CNN learn very generic information like lines, curves,
    and shapes. The last layers in a CNN capture the higher-level concepts from an
    image, such as eyes, buildings, and trees. So the values of the last layers of
    similar images tend to be closer. We take the same concept and apply it for content
    loss. The last layer for the content image and the generated image should be similar,
    and we calculate the similarity using the mean square error (MSE). We use our
    optimization algorithms to bring down the loss value.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当你理解**卷积神经网络**（**CNNs**）的工作原理时，风格转移背后的思想变得清晰。 当CNNs被用于目标识别时，训练的早期层学习非常通用的信息，如线条，曲线和形状。
    CNN的最后几层捕捉图像的更高级概念，如眼睛，建筑物和树木。 因此，类似图像的最后几层的值往往更接近。 我们采用相同的概念并应用于内容损失。 内容图像和生成图像的最后一层应该类似，并且我们使用均方误差（MSE）来计算相似性。
    我们使用优化算法降低损失值。
- en: The style of the image is generally captured across multiple layers in a CNN
    by a technique called the gram matrix. The gram matrix calculates the correlation
    between the feature maps captured across multiple layers. The gram matrix gives
    a measure of calculating the style. Similarly styled images have similar values
    for the gram matrix. The style loss is also calculated using the MSE between the
    gram matrix of the style image and the generated image.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，通过称为Gram矩阵的技术通常捕获图像的风格。 Gram矩阵计算跨多个层捕获的特征图之间的相关性。 Gram矩阵提供了计算风格的一种方法。类似风格的图像具有Gram矩阵的类似值。风格损失还使用风格图像的Gram矩阵与生成图像之间的均方误差（MSE）来计算。
- en: 'We will use a pretrained VGG19 model, provided in the TorchVision models. The
    steps required for training a style transfer model are similar to any other deep
    learning models, except for the fact that calculating losses is more involved
    than for a classification or regression model. The training of the neural style
    algorithm can be broken down to the following steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用预训练的VGG19模型，该模型提供在TorchVision模型中。 训练样式转移模型所需的步骤与任何其他深度学习模型相似，唯一不同的是计算损失比分类或回归模型更复杂。
    神经风格算法的训练可以分解为以下步骤：
- en: Loading data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Creating a VGG19 model.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个VGG19模型。
- en: Defining content loss.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义内容损失。
- en: Defining style loss.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义风格损失。
- en: Extracting losses across layers from the VGG model.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从VGG模型中提取跨层的损失。
- en: Creating an optimizer.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建优化器。
- en: Training—generating an image similar to the content image, and a style similar
    to the style image.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 - 生成类似于内容图像的图像和类似于样式图像的样式。
- en: Loading the data
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: Loading data is similar to what we saw for solving image classification problems
    in Chapter 3, *Diving Deep into Neural Networks*. We will be using the pretrained
    VGG model, so we have to normalize the images using the same values on which the
    pretrained model is trained.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据类似于我们在第3章“深入神经网络”的解决图像分类问题中看到的。 我们将使用预训练的VGG模型，因此必须使用与预训练模型相同的值对图像进行归一化处理。
- en: 'The following code shows how we can do this. The code is mostly self-explanatory
    as we already discussed it in detail in the previous chapters:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了如何实现此目标。 代码大部分是不言自明的，因为我们在前几章中已经详细讨论过它：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, we defined three functionalities, preprocess does all the preprocessing
    required and uses the same values for normalization as those with which the VGG
    model was trained. The output of the model needs to be normalized back to its
    original values; the `processing` function does the processing required. The generated
    model may be out of the range of accepted values, and the `postprocess_b` function
    limits all the values greater than one to one, and values that are less than zero
    to zero.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们定义了三个功能：`preprocess` 执行所有必需的预处理，并使用与训练 VGG 模型时相同的标准化值。模型的输出需要被归一化回其原始值；`processing`
    函数执行所需的处理。生成的模型可能超出接受值的范围，`postprocess_b` 函数将所有大于一的值限制为一，并将小于零的值限制为零。
- en: 'Now we define the `loader` function, which loads the image, applies the `preprocessing`
    transformation, and converts it into a variable:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义 `loader` 函数，它加载图像，应用 `preprocessing` 转换，并将其转换为变量：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following function loads the style and content image:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数加载样式和内容图像：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can either create an image with noise (random numbers), or we can use the
    same content image. We will use the content image in this case. The following
    code creates the content image:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用噪声（随机数）创建图像，也可以使用相同的内容图像。在这种情况下，我们将使用内容图像。以下代码创建内容图像：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use an optimizer to tune the values of the `output_image` variable in
    order for the image to be closer to the content image and style image. For that
    reason, we are asking PyTorch to maintain the gradients by mentioning `requires_grad=True`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用优化器来调整 `output_image` 变量的值，以使图像更接近内容图像和样式图像。出于这个原因，我们要求 PyTorch 通过提及 `requires_grad=True`
    来保持梯度。
- en: Creating the VGG model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 VGG 模型
- en: 'We will load a pretrained model from `torchvisions.models`. We will be using
    this model only for extracting features, and the PyTorch VGG model is defined
    in such a way that all the convolutional blocks will be in the features module
    and the fully connected, or linear, layers are in the classifier module. Since
    we will not be training any of the weights or parameters in the VGG model, we
    will also freeze the model, as the following code demonstrates:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 `torchvisions.models` 中加载预训练模型。我们将仅使用此模型来提取特征，并且 PyTorch 的 VGG 模型是这样定义的：所有卷积块位于特征模块中，全连接或线性层位于分类器模块中。由于我们不会训练
    VGG 模型中的任何权重或参数，因此我们还将冻结该模型，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code, we created a VGG model, used only its convolution blocks, and
    froze all of the parameters of the model as we will be using it only for extracting
    features.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们创建了一个 VGG 模型，仅使用其卷积块，并冻结了模型的所有参数，因为我们将仅用它来提取特征。
- en: Content loss
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容损失
- en: The **content loss** is the distance between the input and the output images.
    The aim is to preserve the original content of the image. It is the MSE calculated
    on the output of a particular layer, extracted by passing two images through the
    network. We extract the outputs of the intermediate layers from the VGG by using
    the `register_forward_hook` functionality, passing in the content image and the
    image to be optimized.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容损失**是输入图像和输出图像之间的距离。其目的是保持图像的原始内容。它是在通过网络传递两个图像并提取特定层的输出后计算的 MSE。我们通过使用
    `register_forward_hook` 功能从 VGG 中提取中间层的输出来实现，传入内容图像和要优化的图像。'
- en: 'We calculate the MSE obtained from the outputs of these layers, as described
    in the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据这些层的输出计算得到的 MSE，如下面的代码所述：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will implement `dummy_fn` for this code in the coming sections. For now,
    all we know is that the `dummy_fn` function returns the outputs of particular
    layers by passing an image. We pass the outputs generated by passing the content
    image and noise image to the MSE loss function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的部分为此代码实现 `dummy_fn`。现在我们知道的是，`dummy_fn` 函数通过传递图像返回特定层的输出。我们通过将内容图像和噪声图像传递给
    MSE 损失函数来传递生成的输出。
- en: Style loss
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样式损失
- en: The style loss is calculated across multiple layers. Style loss is the MSE of
    the gram matrix generated for each feature map. The gram matrix represents the
    correlation value of its features. Let's understand how gram matrix works by using
    the following diagram and a code implementation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 样式损失是跨多个层计算的。样式损失是每个特征图生成的 Gram 矩阵的 MSE。Gram 矩阵表示其特征的相关值。让我们通过以下图表和代码实现来理解 Gram
    矩阵的工作原理。
- en: 'The following table shows the output of a feature map of dimension [2, 3, 3,
    3], having the column attributes **Batch_size**, **Channels**, and **Values**:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了维度为 [2, 3, 3, 3] 的特征映射的输出，具有列属性 **Batch_size**，**Channels** 和 **Values**：
- en: '![](img/f5f3c025-6bb8-48b4-aa51-44ac7c74c93a.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5f3c025-6bb8-48b4-aa51-44ac7c74c93a.png)'
- en: 'To calculate the gram matrix, we flatten all the values per channel and then
    find its correlation by multiplying with its transpose, as shown in the following
    table:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 Gram 矩阵，我们展平每个通道的所有值，然后通过与其转置相乘来找到其相关性，如下表所示：
- en: '![](img/8c6392dc-69c2-40eb-8111-3d6adf5cf0bd.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c6392dc-69c2-40eb-8111-3d6adf5cf0bd.png)'
- en: 'All we did is flatten all the values, with respect to each channel, to a single
    vector or tensor. The following code implements this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的所有工作就是将所有值按照每个通道展平为单个向量或张量。以下代码实现了这一点：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We implement the `GramMatrix` function as another PyTorch module with a `forward`
    function so that we can use it like a PyTorch layer. We are extracting the different
    dimensions from the input image in this line:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `GramMatrix` 函数实现为另一个 PyTorch 模块，具有 `forward` 函数，以便像 PyTorch 层一样使用它。在这一行中，我们从输入图像中提取不同的维度：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, `b` represents batch, `c` represents filters or channels, `h` represents
    height, and `w` represents width. In the next step, we will use the following
    code to keep the batch and channel dimensions intact and flatten all the values
    along the height and width dimension as shown in the preceding diagram:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`b` 表示批次，`c` 表示过滤器或通道，`h` 表示高度，`w` 表示宽度。在下一步中，我们将使用以下代码保持批次和通道维度不变，并在高度和宽度维度上展平所有值，如前面的图示所示：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The gram matrix is calculated by multiplying the flattening values along with
    its transposed vector. We can do it by using the PyTorch batch matrix multiplication
    function, provided as `torch.bmm()`, as shown in the following code:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将其转置向量与其展平值相乘来计算 Gram 矩阵。我们可以使用 PyTorch 提供的批次矩阵乘法函数 `torch.bmm()` 来实现，如下代码所示：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We finish normalizing the values of the gram matrix by dividing it by the number
    of elements. This prevents a particular feature map with a lot of values dominating
    the score. Once `GramMatrix` is calculated, it becomes simple to calculate the
    style loss, which is implemented in this code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完成了通过将其除以元素数量来规范 Gram 矩阵值的工作。这可以防止某个具有大量值的特征映射主导得分。一旦计算了 `GramMatrix`，就可以简单地计算风格损失，这在以下代码中实现：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `StyleLoss` class is implemented as another PyTorch layer. It calculates
    the MSE between the input `GramMatrix` values and the style image `GramMatrix`
    values.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`StyleLoss` 类被实现为另一个 PyTorch 层。它计算输入 `GramMatrix` 值与风格图像 `GramMatrix` 值之间的均方误差（MSE）。'
- en: Extracting the losses
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取损失
- en: 'Just like we extracted the activation of a convolution layer using the `register_forward_hook()`
    function, we can extract losses of different convolutional layers required to
    calculate style loss and content loss. The one difference in this case is that
    instead of extracting from one layer, we need to extract outputs of multiple layers.
    The following class integrates the required change:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们使用 `register_forward_hook()` 函数提取卷积层的激活一样，我们可以提取不同卷积层的损失，以计算风格损失和内容损失。在这种情况下的一个区别是，我们需要提取多个层的输出而不是一个层的输出。以下类集成了所需的更改：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `__init__` method takes the model on which we need to call the `register_forward_hook`
    method and the layer numbers for which we need to extract the outputs. The `for`
    loop in the `__init__` method iterates through the layer numbers and registers
    the forward hook required to pull the outputs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__` 方法接受我们需要调用 `register_forward_hook` 方法的模型和需要提取输出的层编号。`__init__` 方法中的
    `for` 循环遍历层编号并注册所需的前向钩子，用于提取输出。'
- en: The `hook_fn` function passed to the `register_forward_hook` method is called
    by PyTorch after that layer on which the `hook_fn` function is registered. Inside
    the function, we capture the output and store it in the features array.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给 `register_forward_hook` 方法的 `hook_fn` 函数在注册 `hook_fn` 函数的层之后由 PyTorch 调用。在函数内部，我们捕获输出并将其存储在特征数组中。
- en: We need to call the remove function once when we don't want to capture the outputs.
    Forgetting to invoke the remove methods can cause out-of-memory exceptions as
    all the outputs get accumulated.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不想捕获输出时，需要调用 remove 函数一次。忘记调用 remove 方法可能会导致内存不足异常，因为所有输出都会累积。
- en: 'Let''s write another utility function that can extract the outputs required
    for the style and content images. The following function does the same:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们写另一个实用函数，可以提取用于样式和内容图像的输出。以下函数执行相同操作：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Inside the `extract_layers` function, we create objects for the `LayerActivations`
    class by passing in the model and the layer numbers. The features list may contain
    outputs from previous runs, so we are reinitiating to an empty list. Then we pass
    in the image through the model, and we are not going to use the outputs. We are
    more interested in the outputs generated in the features array. We call the remove
    method to remove all the registered hooks from the model and return the features.
    The following code shows how we extract the targets required for style and content
    image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `extract_layers` 函数内部，我们通过向模型和层编号传递来创建 `LayerActivations` 类的对象。特征列表可能包含来自先前运行的输出，因此我们将其重新初始化为空列表。然后我们通过模型传递图像，并且我们不会使用输出。我们更关心的是特征数组中生成的输出。我们调用
    remove 方法来从模型中删除所有已注册的钩子，并返回特征。以下代码展示了我们提取样式和内容图像所需目标的方法：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once we extract the targets, we need to detach the outputs from the graphs
    that created them. Remember that all these outputs are PyTorch variables, which
    maintain information on how they are created. But, for our case, we are interested
    in only the output values and not the graph, as we are not going to update either
    the style image or the content image. The following code illustrates this technique:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提取了目标，我们需要将输出从创建它们的图中分离出来。请记住，所有这些输出都是 PyTorch 变量，它们保留了它们创建方式的信息。但是，对于我们的情况，我们只关注输出值，而不是图形，因为我们不会更新样式图像或内容图像。以下代码展示了这一技术：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once we have detached, let''s add all the targets into one list. The following
    code illustrates this technique:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们分离了，让我们把所有的目标添加到一个列表中。以下代码展示了这一技术：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When calculating the style loss and content loss, we passed on two lists called
    content layers and style layers. Different layer choices will have an impact on
    the quality of the image generated. Let''s pick the same layers as the authors
    of the paper mentioned. The following code shows the choice of layers that we
    are using here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算样式损失和内容损失时，我们传递了称为内容层和样式层的两个列表。不同的层选择将影响生成图像的质量。让我们选择与论文作者提到的相同层。以下代码显示了我们在这里使用的层的选择：
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The optimizer expects a single scalar quantity to minimize. To achieve a single
    scalar value, we sum up all the losses that have arrived at different layers.
    It is common practice to do a weighted sum of these losses, and again we pick
    the same weights as used in the paper''s implementation in the GitHub repository
    ([https://github.com/leongatys/PytorchNeuralStyleTransfer](https://github.com/leongatys/PytorchNeuralStyleTransfer)).
    Our implementation is a slightly modified version of the author''s implementation.
    The following code describes the weights being used, which are calculated by the
    number of filters in the selected layers:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器期望最小化一个单一的标量数量。为了获得单一标量值，我们将所有到达不同层的损失相加起来。习惯上，对这些损失进行加权和是常见做法，而我们选择与 GitHub
    仓库中论文实现中使用的相同权重。我们的实现是作者实现的一个稍微修改的版本。以下代码描述了使用的权重，这些权重是通过所选层中的过滤器数量计算得出的：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To visualize this, we can print the VGG layers. Take a minute to observe which
    layers we are picking, and you can experiment with different layer combinations.
    We will use the following code to print the VGG layers:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行可视化，我们可以打印 VGG 层。花一分钟观察我们选择了哪些层，并尝试不同的层组合。我们将使用以下代码来打印 VGG 层：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This results in the following output:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have to define the loss functions and the optimizer to generate artistic
    images. We will initialize both of them in the following section.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须定义损失函数和优化器来生成艺术图像。我们将在以下部分中初始化它们两个。
- en: Creating a loss function for each layer
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为每个层创建损失函数
- en: 'We have already defined loss functions as PyTorch layers. So, let''s create
    the loss layers for different style losses and content losses. The following code
    defines the function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将损失函数定义为 PyTorch 层。因此，让我们为不同的样式损失和内容损失创建损失层。以下代码定义了这个函数：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `loss_fns` function is a list containing a bunch of style loss objects and
    content loss objects based on the length of the arrays created.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss_fns` 函数是一个列表，包含一堆基于创建的数组长度的样式损失对象和内容损失对象。'
- en: Creating the optimizer
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建优化器
- en: 'In general, we pass in the parameters of a network like VGG to be trained.
    But, in this example, we are using VGG models as feature extractors, and so we
    cannot pass the VGG parameters. Here, we will only provide the parameters of the
    `opt_img` variable that we will optimize to make the image have the required content
    and style. The following code creates the optimizer that optimizes its values:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们传递网络的参数，如VGG的参数进行训练。但在本例中，我们将VGG模型用作特征提取器，因此不能传递VGG的参数。在这里，我们只提供将优化以使图像具有所需内容和风格的`opt_img`变量的参数。以下代码创建优化器以优化其值：
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now we have all the components for training.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好所有训练组件。
- en: Training the model
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'The training method is different compared to the other models that we have
    trained till now. Here, we need to calculate loss at multiple layers, and every
    time the optimizer is called, it will change the input image so that its content
    and style gets close to the target''s content and style. Let''s look at the code
    used for training, and then we will walk through the important steps in the training:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们到目前为止训练过的其他模型相比，训练方法有所不同。在这里，我们需要在多个层级计算损失，并且每次调用优化器时，都会改变输入图像，使其内容和风格接近目标的内容和风格。让我们看一下用于训练的代码，然后我们将逐步介绍训练的重要步骤：
- en: '[PRE22]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are running the training loop for 500 iterations. For every iteration, we
    calculate the output from different layers of the VGG model using our `extract_layers`
    function. In this case, the only thing that changes is the values of `output_image`,
    which will contain our style image. Once the outputs are calculated, we calculate
    the losses by iterating through the outputs and passing them to the corresponding
    loss functions, along with their respective targets. We sum up all the losses
    and call the backward function. At the end of the closure function, the loss is
    returned. The closure method is called along with the `optimizer.step` method
    for `max_iterations`. If you are running on a GPU, it could take a few minutes
    to run; if you are running on a CPU, try reducing the size of the image to make
    it run faster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在运行500次迭代的训练循环。对于每次迭代，我们使用我们的`extract_layers`函数计算来自VGG模型不同层的输出。在这种情况下，唯一改变的是`output_image`的值，它将包含我们的样式图像。一旦计算出输出，我们通过迭代输出并将它们传递给相应的损失函数，同时传递它们的相应目标来计算损失。我们总结所有的损失并调用反向传播函数。在闭包函数的末尾，返回损失。对于`max_iterations`，同时调用闭包方法和`optimizer.step`方法。如果您在GPU上运行，可能需要几分钟才能运行；如果您在CPU上运行，请尝试减小图像的大小以加快运行速度。
- en: 'After running for 500 epochs, the resulting image on my machine looks as shown
    here. Try different combinations of content and style to generate interesting
    images:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行了500个epochs之后，在我的设备上生成的图像如下所示。尝试不同的内容和风格的组合来生成有趣的图像：
- en: '![](img/437c55d3-b973-4645-8f0c-299c1950f01f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/437c55d3-b973-4645-8f0c-299c1950f01f.png)'
- en: In the next section, let's go ahead and generate human faces using **deep convolutional
    generative adversarial networks** (**DCGANs**).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，让我们使用深度卷积生成对抗网络（DCGANs）生成人脸。
- en: Introducing GANs
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入GANs
- en: 'GANs were introduced by Ian Goodfellow in 2014 and have become very popular.
    There have been many significant developments to GAN research in recent times,
    and the following timeline shows some of the most noteworthy advances and key
    developments in GAN research:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GANs是由Ian Goodfellow于2014年引入，并变得非常流行。最近GAN研究取得了许多重要进展，以下时间轴显示了GAN研究中一些最显著的进展和关键发展：
- en: '![](img/ba98fa2c-d37b-4b53-82b1-86e006287777.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba98fa2c-d37b-4b53-82b1-86e006287777.png)'
- en: 'In this chapter, we will focus on the PyTorch implementation of DCGAN. However,
    there is a very useful GitHub repository that provides a host of PyTorch implementation
    examples of the GANs shown in the timeline along with others. It can be accessed
    via the following link: [https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于DCGAN的PyTorch实现。然而，有一个非常有用的GitHub仓库提供了一堆PyTorch实现示例，包括时间轴上显示的GAN以及其他模型。可以通过以下链接访问：[https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)。
- en: The GAN addresses the problem of unsupervised learning by training two deep
    neural networks, called a generator and discriminator, which compete with each
    other. In the course of training, both eventually become better at the tasks that
    they perform.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: GAN通过训练两个深度神经网络——生成器和鉴别器来解决无监督学习问题，它们相互竞争。在训练过程中，两者最终都变得更擅长执行它们所执行的任务。
- en: GANs are intuitively understood using the case of a counterfeiter (generator)
    and the police (discriminator). Initially, the counterfeiter shows the police
    fake money. The police identifies it as fake and explains to the counterfeiter
    why it is fake. The counterfeiter makes new fake money based on the feedback it
    received. The police finds it's fake and informs the counterfeiter why it is fake.
    It repeats this a huge number of times until the counterfeiter is able to make
    fake money, which the police is unable to recognize. In the GAN scenario, we end
    up with a generator that generates fake images that are quite similar to the real
    ones, and a classifier becomes great at identifying a fake from the real thing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: GAN可以用一个造假者（生成器）和警察（鉴别器）的案例直观理解。最初，造假者向警察展示假钱。警察识别出它是假的，并解释给造假者为什么是假的。造假者根据收到的反馈制造新的假钱。警察发现它是假的，并告诉造假者为什么是假的。重复进行大量次数，直到造假者能够制造出警察无法识别的假钱。在GAN场景中，我们最终得到一个生成器，生成的假图像非常类似于真实图像，而分类器变得擅长识别真伪。
- en: GAN is a combination of a forger network and an expert network, each being trained
    to beat the other. The generator network takes a random vector as input and generates
    a synthetic image. The discriminator network takes an input image, and predicts
    whether the image is real or fake. We pass the discriminator network either a
    real image or a fake image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是伪造者网络和专家网络的结合体，每个网络都经过训练以击败另一个。生成器网络以随机向量作为输入，并生成合成图像。鉴别器网络接收输入图像，并预测图像是真实的还是伪造的。我们向鉴别器网络传递真实图像或伪造图像。
- en: The generator network is trained to produce images and fool the discriminator
    network into believing they are real. The discriminator network is also constantly
    improving at not getting fooled, as we pass the feedback while training it.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络被训练生成图像，并欺骗鉴别器网络认为它们是真实的。鉴别器网络也在不断改进，以免受骗，因为我们在训练时传递反馈。
- en: 'The following diagram depicts the architecture of a GAN model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表描述了GAN模型的架构：
- en: '![](img/05cb99b2-943e-45e6-8a0b-6b5ff08d566f.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05cb99b2-943e-45e6-8a0b-6b5ff08d566f.png)'
- en: Though the idea of GANs sounds simple in theory, training a GAN model that actually
    works is very difficult as there are two deep neural networks that need to be
    trained in parallel.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GAN的理念在理论上听起来很简单，但训练一个真正有效的GAN模型非常困难，因为需要并行训练两个深度神经网络。
- en: 'The DCGAN is one of the early models that demonstrated how to build a GAN that
    learns by itself and generates meaningful images. You can learn more about it
    here: [https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf).
    We will walk through each of the components of this architecture along with some
    of the reasoning behind it and how this can be implemented in PyTorch.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN是早期展示如何构建一个可以自我学习并生成有意义图像的GAN模型之一。您可以在这里了解更多信息：[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)。我们将逐步讲解这种架构的每个组成部分以及背后的一些推理，以及如何在PyTorch中实现它。
- en: DCGAN
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DCGAN
- en: 'In this section, we will implement different parts of training a GAN architecture,
    based on the DCGAN paper I mentioned in the preceding information box. Some of
    the important parts of training a DCGAN include the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将根据我在前面信息框中提到的DCGAN论文实现GAN架构的不同部分。训练DCGAN的一些重要部分包括以下内容：
- en: A generator network, which maps a latent vector (list of numbers) of some fixed
    dimension to images of some shape. In our implementation, the shape is (3, 64,
    64).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器网络，将固定维度的潜在向量（数字列表）映射到某些形状的图像。在我们的实现中，形状是（3, 64, 64）。
- en: A discriminator network, which takes as input an image generated by the generator
    or from the actual dataset, and maps to that a score estimating if the input image
    is real or fake.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器网络，以生成器生成的图像或来自实际数据集的图像作为输入，并映射到评估输入图像是否真实或伪造的分数。
- en: Defining loss functions for the generator and discriminator.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义生成器和鉴别器的损失函数。
- en: Defining an optimizer.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个优化器。
- en: Let's explore each of these sections in detail. The implementation provides
    a more detailed explanation of the code that is available in the PyTorch GitHub
    repository: [https://github.com/pytorch/examples/tree/master/dcgan.](https://github.com/pytorch/examples/tree/master/dcgan)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨每个部分。这一实现提供了更详细的解释，说明了在PyTorch GitHub存储库中提供的代码：[https://github.com/pytorch/examples/tree/master/dcgan.](https://github.com/pytorch/examples/tree/master/dcgan)
- en: Defining the generator network
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器网络
- en: The generator network takes a random vector of fixed dimension as input, and
    applies a set of transposed convolutions, batch normalization, and ReLU activation
    to it, and generates an image of the required size. Before looking into the generator
    implementation, let's look at defining transposed convolution and batch normalization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络将固定维度的随机向量作为输入，并对其应用一组转置卷积、批量归一化和ReLU激活函数，生成所需尺寸的图像。在深入研究生成器实现之前，让我们先来定义转置卷积和批量归一化。
- en: Transposed convolutions
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转置卷积
- en: Transposed convolutions are also called fractionally strided convolutions. They
    work in the opposite way to how convolution works. Intuitively, they try to calculate
    how the input vector can be mapped to higher dimensions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积也称为分数步幅卷积。它们的工作方式与卷积相反。直观地说，它们试图计算如何将输入向量映射到更高的维度。
- en: 'Let''s look at the following diagram to understand it better:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下面的图表以更好地理解它：
- en: '![](img/1a04a38c-5d6b-4228-a1f9-097c0a29349e.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a04a38c-5d6b-4228-a1f9-097c0a29349e.png)'
- en: This diagram is referenced in the Theano documentation (another popular deep
    learning framework—[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)).
    If you want to explore more about how strided convolutions work, I strongly recommend
    you read this article from the Theano documentation. What is important for us
    is that it helps to convert a vector to a tensor of the required dimensions, and
    we can train the values of the kernels by backpropagation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此图表被引用在Theano文档中（另一个流行的深度学习框架—[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)）。如果你想更深入地了解步幅卷积的工作原理，我强烈推荐你阅读这篇文章。对我们而言重要的是，它有助于将向量转换为所需维度的张量，并且我们可以通过反向传播训练核的值。
- en: Batch normalization
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化
- en: 'We have already observed a couple of times that all the features that are being
    passed to either machine learning or deep learning algorithms are normalized;
    that is, the values of the features are centered to zero by subtracting the mean
    from the data and giving the data a unit standard deviation by dividing the data
    by its standard deviation. We would generally do this by using the PyTorch `torchvision.Normalize`
    method. The following code shows an example:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次观察到，所有传递给机器学习或深度学习算法的特征都经过了归一化处理；即，通过从数据中减去均值来将特征的值居中到零，并通过将数据除以其标准差来给数据一个单位标准差。通常我们会使用PyTorch的`torchvision.Normalize`方法来实现这一点。以下代码展示了一个例子：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In all the examples we have seen, the data is normalized just before it enters
    a neural network; there is no guarantee that the intermediate layers get a normalized
    input. The following diagram shows how the intermediate layers in the neural network
    fail to get normalized data:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们所见的所有示例中，数据在进入神经网络之前都进行了归一化；不能保证中间层得到归一化的输入。下图展示了神经网络中间层未获得归一化数据的情况：
- en: '![](img/6aad96ae-05b1-4e43-afd9-3f35022daee2.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6aad96ae-05b1-4e43-afd9-3f35022daee2.png)'
- en: 'Batch normalization acts like an intermediate function or a layer, which normalizes
    the intermediate data when the mean and variance change over time during training.
    Batch normalization was introduced in 2015 by Ioffe and Szegedy ([https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)).
    Batch normalization behaves differently during training and validation or testing.
    During training, the mean and variance are calculated for the data in the batch.
    For validation and testing, the global values are used. All we need to understand
    in order to use it is that it normalizes the intermediate data. Some of the key
    advantages of using batch normalization are that it does the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化充当一个中间函数或层，当训练过程中的均值和方差随时间变化时，它会归一化中间数据。批量归一化是由Ioffe和Szegedy在2015年提出的（[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)）。批量归一化在训练和验证或测试期间表现不同。训练期间，会计算批次数据的均值和方差。验证和测试期间则使用全局值。我们只需理解它归一化了中间数据。使用批量归一化的一些关键优势包括以下几点：
- en: Improves gradient flow through the network, thus helping us build deeper networks
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善了网络中的梯度流，从而帮助我们构建更深的网络
- en: Allows higher learning rates
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许更高的学习率
- en: Reduces the strong dependency of initialization
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少了初始化的强依赖
- en: Acts as a form of regularization and reduces the dependency of dropout
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为正则化的一种形式，并减少了对丢弃的依赖
- en: 'Most of the modern architectures, such as ResNet and Inception, extensively
    use batch normalization in their architectures. We will be diving deeper into
    these architectures in the next chapter. Batch normalization layers are introduced
    after a convolution layer or linear/fully connected layers, as shown in the following
    diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代架构，如ResNet和Inception，在它们的架构中广泛使用批标准化。我们将在下一章节深入探讨这些架构。批标准化层是在卷积层或线性/全连接层之后引入的，如下图所示：
- en: '![](img/31c77afe-a498-4552-86ed-6aaea16c9890.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31c77afe-a498-4552-86ed-6aaea16c9890.png)'
- en: By now, we have an intuitive understanding of the key components of a generator
    network.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对生成器网络的关键组成部分有了直观的理解。
- en: Generator
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: 'Let''s quickly look at the following generator network code, and then discuss
    the key features of the generator network:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览以下生成器网络代码，然后讨论生成器网络的关键特性：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In most of the code examples we have seen, we use a bunch of different layers
    and then define the flow in the forward method. In the generator network, we define
    the layers and the flow of the data inside the `__init__` method using a sequential
    model. The model takes as input a tensor of size nz, and then passes it on to
    a transposed convolution to map the input to the image size that it needs to generate.
    The forward function passes on the input to the sequential module and returns
    the output.  The last layer of the generator network is a tanh layer, which limits
    the range of values the network can generate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到的大多数代码示例中，我们使用了一系列不同的层，然后在前向方法中定义数据的流动。在生成器网络中，我们在`__init__`方法中定义了层和数据的流动，使用了顺序模型。该模型接收大小为nz的张量作为输入，然后将其传递给转置卷积以映射输入到需要生成的图像大小。前向函数将输入传递给顺序模块并返回输出。生成器网络的最后一层是一个tanh层，限制了网络可以生成的值的范围。
- en: 'Instead of using the same random weights, we initialize the model with weights
    as defined in the paper. The following is the weight initialization code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再使用相同的随机权重初始化模型，而是根据论文中定义的权重初始化模型。以下是权重初始化代码：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We call the weight function by passing the function to the generator object,
    `net_generator`. Each layer is passed on to the function; if the layer is a convolution
    layer we initialize the weights differently, and if it is `BatchNorm`, then we
    initialize it a bit differently. We call the function on the network object using
    the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将函数传递给生成器对象`net_generator`来调用权重函数。每一层都会传递给该函数；如果该层是卷积层，我们会以不同的方式初始化权重，如果是`BatchNorm`层，则会稍有不同。我们使用以下代码在网络对象上调用该函数：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Defining the discriminator network
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义鉴别器网络
- en: 'Let''s quickly look at the following discriminator network code, and then discuss
    the key features of the discriminator network:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览一下以下鉴别器网络代码，然后讨论鉴别器网络的关键特性：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: There are two important things in the previous network, namely, the usage of
    leaky ReLU as an activation function, and the usage of sigmoid as the last activation
    layer. First, let's understand what leaky ReLU is.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述网络中有两个重要点，即使用Leaky ReLU作为激活函数，以及在最后使用sigmoid作为激活层。首先，让我们了解一下Leaky ReLU是什么。
- en: Leaky ReLU is an attempt to fix the dying ReLU problem. Instead of the function
    returning zero when the input is negative, leaky ReLU will output a very small
    number like 0.001\. In the paper, it is shown that using leaky ReLU improves the
    efficiency of the discriminator.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 是为了解决ReLU激活函数中的“神经元死亡”问题。当输入为负数时，Leaky ReLU不会返回零，而是输出一个非常小的数值，如0.001。论文中显示，使用Leaky
    ReLU可以提高鉴别器的效率。
- en: Another important difference is not using fully connected layers at the end
    of the discriminator. It is common to see the last fully connected layers being
    replaced by global average pooling. But using global average pooling reduces the
    rate of the convergence speed (number of iterations to build an accurate classifier).
    The last convolution layer is flattened and passed to a sigmoid layer.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的区别是在鉴别器末端不使用全连接层。通常会看到最后的全连接层被全局平均池化替换。但使用全局平均池化会降低收敛速度（构建准确分类器所需的迭代次数）。最后的卷积层被展平并传递给sigmoid层。
- en: Other than these two differences, the rest of the network is similar to the
    other image classifier networks we have seen in the book.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两个区别外，该网络的其余部分与我们在书中看到的其他图像分类器网络类似。
- en: Defining loss and optimizer
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义损失和优化器
- en: 'We will define a binary cross-entropy loss and two optimizers, one for the
    generator and another one for the discriminator, in the following code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下代码中定义二元交叉熵损失和两个优化器，一个用于生成器，另一个用于鉴别器。
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Up to this point, it is very similar to what we have seen in all our previous
    examples. Let's explore how we can train the generator and discriminator.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这与我们在所有先前示例中看到的非常相似。让我们探索如何训练生成器和鉴别器。
- en: Training the discriminator
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练鉴别器
- en: 'The loss of the discriminator network depends on how it performs on real images
    and how it performs on fake images generated by the generator network. The loss
    can be defined as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络的损失取决于其在真实图像上的表现以及其在生成器网络生成的假图像上的表现。损失可以定义如下：
- en: '![](img/67e56b88-415b-4aad-9ac2-fa5bd8efa0ba.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67e56b88-415b-4aad-9ac2-fa5bd8efa0ba.png)'
- en: So, we need to train the discriminator with real images and the fake images
    generated by the generator network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要使用真实图像和生成器网络生成的假图像来训练鉴别器。
- en: Training the discriminator with real images
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用真实图像训练鉴别器
- en: Let's pass some real images as direct information to train the discriminator.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将一些真实图像直接作为信息传递给训练鉴别器。
- en: 'First, we will take a look at the code for doing the same and then explore
    the important features:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看执行相同操作的代码，然后探索其重要特征：
- en: '[PRE29]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the previous code, we calculate the loss and the gradients required for the
    discriminator image. The `inputv` and `labelv` values represents the input image
    from the CIFAR10 dataset and labels, which is one for real images. It is pretty
    straightforward, as it is similar to what we do for other image classifier networks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们计算了鉴别器图像所需的损失和梯度。`inputv` 和 `labelv` 值表示 CIFAR10 数据集中的输入图像和标签，对于真实图像标签为
    1。这很简单明了，与我们对其他图像分类器网络所做的工作类似。
- en: Training the discriminator with fake images
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用假图像训练鉴别器
- en: Now pass some random images to train the discriminator.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在传递一些随机图像来训练鉴别器。
- en: 'Let''s look at the code for it and then explore the important features:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下相关代码，然后探索其重要特征：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The first line in this code passes a vector with a size of 100, and the generator
    network (`net_generator`) generates an image. We pass on the image to the discriminator
    for it to identify whether the image is real or fake. We do not want the generator
    to get trained, as the discriminator is getting trained. So, we remove the fake
    image from its graph by calling the detach method on its variable. Once all the
    gradients are calculated, we call the optimizer to train the discriminator.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码中的第一行传递了一个大小为 100 的向量，生成器网络（`net_generator`）生成一张图像。我们将图像传递给鉴别器，以便其识别图像是真实的还是假的。我们不希望生成器得到训练，因为鉴别器正在训练中。因此，我们通过在其变量上调用
    `detach` 方法来从其图中移除假图像。一旦计算出所有梯度，我们调用优化器来训练鉴别器。
- en: Training the generator network
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练生成器网络
- en: 'Let''s look at the following code for training the generator network and then
    explore the important features:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下用于训练生成器网络的以下代码，然后探索其重要特征：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It looks similar to what we did while we trained the discriminator on fake images,
    except for some key differences. We are passing the same fake images created by
    the generator, but this time we are not detaching it from the graph that produced
    it, because we want the generator to be trained. We calculate the loss (`err_generator`)
    and calculate the gradients. Then we call the generator optimizer, as we only
    want the generator to be trained, and we repeat this entire process for several
    iterations before we have the generator producing slightly realistic images.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来与我们在训练假图像上训练鉴别器时做的很相似，除了一些关键的不同之处。我们传递了生成器创建的相同假图像，但这次我们没有从生成它的图中分离它，因为我们希望训练生成器。我们计算损失（`err_generator`）并计算梯度。然后我们调用生成器优化器，因为我们只想训练生成器，并在生成器生成略微逼真图像之前重复整个过程多次。
- en: Training the complete network
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练完整网络
- en: 'We have looked at individual pieces of how a GAN is trained. Let''s summarize
    them as follows and look at the complete code that will be used to train the GAN
    network we created:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看了 GAN 训练的各个部分。让我们总结如下，并查看用于训练我们创建的 GAN 网络的完整代码：
- en: Train the discriminator network with real images
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用真实图像训练鉴别器网络
- en: Train the discriminator network with fake images
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用假图像训练鉴别器网络
- en: Optimize the discriminator
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化鉴别器
- en: Train the generator based on the discriminator feedback
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据鉴别器的反馈训练生成器
- en: Optimize the generator network alone
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅优化生成器网络
- en: 'We will use the following code to train the network:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来训练网络：
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `vutils.save_image` will take a tensor and save it as an image. If provided
    with a mini-batch of images, then it saves them as a grid of images.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `vutils.save_image` 将接收一个张量并保存为图像。如果提供了一个图像的小批量，则将它们保存为图像网格。
- en: In the following sections, we will take a look at what the generated images
    and real images look like.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看看生成的图像和真实图像的样子。
- en: Inspecting the generated images
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查生成的图像
- en: So, let's compare the generated images and real images.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们比较生成的图像和真实图像。
- en: 'The generated images will be as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像如下所示：
- en: '![](img/d3831026-e209-4ed4-a4ff-64ce32619d78.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3831026-e209-4ed4-a4ff-64ce32619d78.png)'
- en: 'The real images are as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 实际图像如下所示：
- en: '![](img/c7a8f6dc-3021-421b-ad1b-cd60fd6b60d6.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7a8f6dc-3021-421b-ad1b-cd60fd6b60d6.png)'
- en: Comparing both sets of images, we can see that our GAN was able to learn how
    to generate images.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这两组图像，我们可以看到我们的GAN能够学习如何生成图像。
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to train deep learning algorithms that can generate
    artistic style transfers using generative networks. We also learned how to generate
    new images using GAN and DCGAN. In DCGAN, we explored training the discriminator
    with real and fake images and inspected the generated images. Apart from training
    to generate new images, we also have a discriminator, which can be used for classification
    problems. The discriminator learns important features about the images that can
    be used for classification tasks when there is a limited amount of labeled data
    available. When there is limited labeled data, we can train a GAN that will give
    us a classifier, which can be used to extract features—and a classifier module
    can be built on top of it.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何训练能够使用生成网络生成艺术风格转换的深度学习算法。我们还学习了如何使用GAN和DCGAN生成新图像。在DCGAN中，我们探索了使用真实和虚假图像来训练鉴别器，并检查了生成的图像。除了训练生成新图像外，我们还有一个鉴别器，可以用于分类问题。当有限的标记数据可用时，鉴别器学习有关图像的重要特征，这些特征可以用于分类任务。当有限的标记数据时，我们可以训练一个GAN，它将给我们一个分类器，可以用来提取特征，然后可以在其上构建一个分类器模块。
- en: In the next chapter, we will cover some of the modern architectures, such as
    ResNet and Inception, for building better computer vision models and models such
    as sequence-to-sequence, which can be used for building language translation and
    image captioning.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一些现代架构，如ResNet和Inception，用于构建更好的计算机视觉模型，以及用于构建语言翻译和图像字幕的序列到序列模型。
