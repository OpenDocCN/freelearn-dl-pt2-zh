- en: Implementing Policy Gradients and Policy Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现策略梯度和策略优化
- en: In this chapter, we will focus on policy gradient methods as one of the most
    popular reinforcement learning techniques over recent years. We will start with
    implementing the fundamental REINFORCE algorithm and will proceed with an improvement
    algorithm baseline. We will also implement a more powerful algorithm, actor-critic,
    and its variations, and apply it to solve the CartPole and Cliff Walking problems.
    We will also experience an environment with continuous action space and resort
    to Gaussian distribution to solve it. By way of a fun section at the end, we will
    train an agent based on the cross-entropy method to play the CartPole game.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于策略梯度方法，这是近年来最流行的强化学习技术之一。我们将从实现基础的 REINFORCE 算法开始，并继续改进算法基线。我们还将实现更强大的算法，演员-评论家及其变体，并将其应用于解决
    CartPole 和 Cliff Walking 问题。我们还将体验一个具有连续动作空间的环境，并采用高斯分布来解决它。最后的有趣部分，我们将基于交叉熵方法训练一个代理来玩
    CartPole 游戏。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下实例：
- en: Implementing the REINFORCE algorithm
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 REINFORCE 算法
- en: Developing the REINFORCE algorithm with baseline
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发带基线的 REINFORCE 算法
- en: Implementing the actor-critic algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现演员-评论家算法
- en: Solving Cliff Walking with the actor-critic algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用演员-评论家算法解决 Cliff Walking
- en: Setting up the continuous Mountain Car environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置连续的 Mountain Car 环境
- en: Solving the continuous Mountain Car environment with the advantage actor-critic
    network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优势演员-评论家网络解决连续的 Mountain Car 环境
- en: Playing CartPole through the cross-entropy method
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过交叉熵方法玩 CartPole
- en: Implementing the REINFORCE algorithm
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 REINFORCE 算法
- en: A recent publication stipulated that policy gradient methods are becoming more
    and more popular. Their learning goal is to optimize the probability distribution
    of actions so that given a state, a more rewarding action will have a higher probability
    value. In the first recipe of the chapter, we will talk about the REINFORCE algorithm,
    which is foundational to advanced policy gradient methods.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一篇文章指出，策略梯度方法变得越来越流行。它们的学习目标是优化动作的概率分布，以便在给定状态下，更有益的动作将具有更高的概率值。在本章的第一个实例中，我们将讨论
    REINFORCE 算法，这是高级策略梯度方法的基础。
- en: '**The REINFORCE** algorithm is also known as the **Monte Carlo policy gradient**,
    as it optimizes the policy based on Monte Carlo methods. Specifically, it collects
    trajectory samples from one episode using its current policy and uses them to
    the policy parameters, θ . The learning objective function for policy gradients
    is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**REINFORCE** 算法也被称为 **蒙特卡罗策略梯度**，因为它基于蒙特卡罗方法优化策略。具体而言，它使用当前策略从一个回合中收集轨迹样本，并用它们来更新策略参数
    θ 。策略梯度的学习目标函数如下：'
- en: '![](img/be1f51e8-dae6-47fc-887a-dab7ec8b8735.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be1f51e8-dae6-47fc-887a-dab7ec8b8735.png)'
- en: 'Its gradient can be derived as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其梯度可以如下推导：
- en: '![](img/e464bcc7-6206-4b29-82e9-1d8e2399292a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e464bcc7-6206-4b29-82e9-1d8e2399292a.png)'
- en: Here, [![](img/882979d6-0eff-4d04-aed6-2b874834e4bf.png)] is the return, which
    is the cumulative discounted reward until time, t, [![](img/cf46b0c7-4df9-4378-ad71-978797d59fa4.png)]and
    is the stochastic policy, which determines the probabilities of taking certain
    actions at a given state. Since a policy update is conducted after the entire
    episode finishes and all samples are collected, REINFORCE is an off-policy algorithm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，[![](img/882979d6-0eff-4d04-aed6-2b874834e4bf.png)] 是返回值，即累积折扣奖励直到时间 t，[![](img/cf46b0c7-4df9-4378-ad71-978797d59fa4.png)]并且是随机策略，确定在给定状态下采取某些动作的概率。由于策略更新是在整个回合结束后和所有样本被收集后进行的，REINFORCE
    算法是一种离策略算法。
- en: After we compute the policy gradients, we use backpropagation to update the
    policy parameters. With the updated policy, we roll out an episode, collect a
    set of samples, and use them to repeatedly update the policy parameters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算策略梯度后，我们使用反向传播来更新策略参数。通过更新后的策略，我们展开一个回合，收集一组样本，并使用它们来重复更新策略参数。
- en: We will now develop the REINFORCE algorithm to solve the CartPole ([https://gym.openai.com/envs/CartPole-v0/](https://gym.openai.com/envs/CartPole-v0/))
    environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将开发 REINFORCE 算法来解决 CartPole ([https://gym.openai.com/envs/CartPole-v0/](https://gym.openai.com/envs/CartPole-v0/))
    环境。
- en: How to do it...
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We develop the REINFORCE algorithm to solve the CartPole environment as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发带基线的 REINFORCE 算法来解决 CartPole 环境如下：
- en: 'Import all the necessary packages and create a CartPole instance:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包并创建一个CartPole实例：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s start with the `__init__method` of the `PolicyNetwork` class, which
    approximates the policy using a neural network:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从`PolicyNetwork`类的`__init__`方法开始，该方法使用神经网络逼近策略：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, add the `predict` method, which computes the estimated policy:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，添加`predict`方法，计算估计的策略：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now develop the training method, which updates the neural network with samples
    collected in an episode:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们开发训练方法，使用一个情节中收集的样本更新神经网络：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The final method for the `PolicyNetwork` class is `get_action`, which samples
    an action given a state based on the predicted policy:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PolicyNetwork`类的最终方法是`get_action`，它基于预测的策略对给定状态采样一个动作：'
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It also returns the log probability of the selected action, which will be used
    as part of the training sample.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 它还返回所选动作的对数概率，这将作为训练样本的一部分使用。
- en: That's all for the `PolicyNetwork` class!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`PolicyNetwork`类的全部内容！
- en: 'Now, we can move on to developing the **REINFORCE** algorithm with a policy
    network model:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始开发**REINFORCE**算法，使用一个策略网络模型：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定策略网络的大小（输入、隐藏和输出层）、学习率，然后相应地创建`PolicyNetwork`实例：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We set the discount factor as `0.9`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将折扣因子设置为`0.9`：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We perform learning with the REINFORCE algorithm using the policy network we
    just developed for 500 episodes, and we also keep track of the total rewards for
    each episode:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用刚开发的策略网络执行REINFORCE算法的学习，共500个情节，并跟踪每个情节的总回报：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s now display the plot of episode reward over time:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们显示随时间变化的回报情节图：
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: In *Step 2*, we use a neural network with one hidden layer for simplicity. The
    input of the policy network is a state, followed by a hidden layer, while the
    output is the probability of taking possible individual actions. Therefore, we
    use the softmax function as the activation for the output layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Step 2*中，为了简单起见，我们使用一个隐藏层的神经网络。策略网络的输入是一个状态，接着是一个隐藏层，输出是可能采取的个别动作的概率。因此，我们使用softmax函数作为输出层的激活函数。
- en: '*Step 4* is for updating the network parameters: given all the data gathered
    in an episode, including the returns and the log probabilities of all steps, we
    compute the policy gradients, and then update the policy parameters accordingly
    via backpropagation.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 4* 用于更新网络参数：给定在一个情节中收集的所有数据，包括所有步骤的回报和对数概率，我们计算策略梯度，然后通过反向传播相应地更新策略参数。'
- en: 'In *Step 6*, the REINFORCE algorithm does the following tasks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Step 6* 中，REINFORCE 算法执行以下任务：
- en: 'It runs an episode: for each step in the episode, it samples an action based
    on the current estimated policy; it stores the reward and the log policy at each
    step.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它运行一个情节：对于情节中的每一步，根据当前估计的策略采样一个动作；它在每一步存储回报和对数策略。
- en: Once an episode finishes, it calculates the discounted cumulative rewards at
    each step; it normalizes the resulting returns by subtracting their mean and then
    dividing them by their standard deviation.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦一个情节结束，它计算每一步的折扣累积回报；通过减去它们的平均值然后除以它们的标准差来对结果进行归一化。
- en: It computes policy gradients using the returns and log probabilities, and then
    updates the policy parameters. We also display the total reward for each episode.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用回报和对数概率计算策略梯度，然后更新策略参数。我们还显示每个情节的总回报。
- en: It runs `n_episode` episodes by repeating the aforementioned steps.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过重复上述步骤运行`n_episode`个情节。
- en: '*Step 8* will generate the following training logs:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 8* 将生成以下训练日志：'
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will observe the following plot in *Step 9*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在*Step 9*中观察以下情节：
- en: '![](img/1a4cb660-d548-43f3-88f6-5a260d87dbff.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a4cb660-d548-43f3-88f6-5a260d87dbff.png)'
- en: You can see that most of the last 200 episodes have rewards with a maximum value
    of +200.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到最近200个情节中的大部分奖励最高值为+200。
- en: 'The REINFORCE algorithm is a family of policy gradient methods that updates
    the policy parameters directly through the following rule:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**REINFORCE** 算法是一系列策略梯度方法的家族，通过以下规则直接更新策略参数：'
- en: '![](img/854ef6cc-3c4a-4666-be4a-76a12b7337b8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/854ef6cc-3c4a-4666-be4a-76a12b7337b8.png)'
- en: 'Here, α is the learning rate, [![](img/f6974151-1450-44d0-8d7c-a73695e2bade.png)],
    as the probability mappings of actions, and [![](img/478630e2-8421-4b5f-bf2a-7499715d4dfd.png)],
    as the cumulative discounted rewards, are experiences collected in an episode.
    Since the set of training samples is constructed only after the full episode is
    completed, learning in REINFORCE is in an off-policy manner. The learning process
    can be summarized as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α是学习率，[![](img/f6974151-1450-44d0-8d7c-a73695e2bade.png)]，作为动作的概率映射，而[![](img/478630e2-8421-4b5f-bf2a-7499715d4dfd.png)]，作为累积折现奖励，是在一个episode中收集的经验。由于训练样本集仅在完成整个episode后构建，因此REINFORCE中的学习是以离线策略进行的。学习过程可以总结如下：
- en: Randomly initialize the policy parameter, *θ*.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化策略参数*θ*。
- en: Perform an episode by selecting actions based on the current policy.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前策略选择动作执行一个episode。
- en: At each step, store the log probability of the chosen action as well as the
    resulting reward.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个步骤中，存储所选动作的对数概率以及产生的奖励。
- en: Calculate the returns for individual steps.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算各步骤的回报。
- en: Compute policy gradients using log probabilities and returns, and update the
    policy parameter, θ, via backpropagation.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对数概率和回报计算策略梯度，并通过反向传播更新策略参数θ。
- en: Repeat *Steps 2* to *5*.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*至*5*。
- en: Again, since the REINFORCE algorithm relies on a full trajectory generated by
    a stochastic policy, it constitutes a Monte Carlo method.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，由于REINFORCE算法依赖于由随机策略生成的完整轨迹，因此它构成了一种蒙特卡洛方法。
- en: See also
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见：
- en: 'It is quite tricky to derive the policy gradient equation. It utilizes the
    log-derivative trick. In case you are wondering, here is a detailed explanation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 推导策略梯度方程相当棘手。它利用了对数导数技巧。如果你想知道，这里有一个详细的解释：
- en: '[http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf](http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf](http://www.1-4-5.net/~dmm/ml/log_derivative_trick.pdf)'
- en: Developing the REINFORCE algorithm with baseline
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发带基线的REINFORCE算法
- en: In the REINFORCE algorithm, Monte Carlo plays out the whole trajectory in an
    episode that is used to update the policy afterward. However, the stochastic policy
    may take different actions at the same state in different episodes. This can confuse
    the training, since one sampled experience wants to increase the probability of
    choosing one action while another sampled experience may want to decrease it.
    To reduce this high variance problem in vanilla REINFORCE, we will develop a variation
    algorithm, REINFORCE with baseline, in this recipe.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在REINFORCE算法中，蒙特卡洛模拟在一个episode中播放整个轨迹，然后用于更新策略。然而，随机策略可能在不同的episode中在相同状态下采取不同的行动。这可能会导致训练时的混淆，因为一个采样经验希望增加选择某个动作的概率，而另一个采样经验可能希望减少它。为了减少这种高方差问题，在传统REINFORCE中，我们将开发一种变体算法，即带基线的REINFORCE算法。
- en: 'In REINFORCE with baseline, we subtract the baseline state-value from the return,
    G. As a result, we use an advantage function A in the gradient update, which is
    described as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在带基线的REINFORCE中，我们从回报G中减去基线状态值。因此，我们在梯度更新中使用了优势函数A，描述如下：
- en: '![](img/6a840734-d2e4-437c-ae60-966118f045cd.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a840734-d2e4-437c-ae60-966118f045cd.png)'
- en: Here, V(s) is the value function that estimates the state-value given a state.
    Typically, we can use a linear function, or a neural network, to approximate the
    state-value. By introducing the baseline value, we can calibrate the rewards with
    respect to the average action given a state.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，V(s)是估计给定状态的状态值函数。通常，我们可以使用线性函数或神经网络来逼近状态值。通过引入基线值，我们可以根据状态给出的平均动作校准奖励。
- en: We develop the REINFORCE with baseline algorithm using two neural networks,
    one for policy, and another one for value estimation, in order to solve the CartPole
    environment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个神经网络开发了带基线的REINFORCE算法，一个用于策略，另一个用于值估计，以解决CartPole环境。
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We solve the CartPole environment using the REINFORCE with baseline algorithm
    as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用REINFORCE算法解决CartPole环境的方法如下：
- en: 'Import all the necessary packages and create a CartPole instance:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包并创建一个CartPole实例：
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For the policy network part, it is basically the same as the `PolicyNetwork`
    class we used in the *Implementing the REINFORCE algorithm* recipe. Keep in mind
    that the advantage values are used in the `update` method:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于策略网络部分，基本上与我们在*实现REINFORCE算法*配方中使用的`PolicyNetwork`类相同。请记住，在`update`方法中使用了优势值：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For the value network part, we use a regression neural network with one hidden
    layer:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于价值网络部分，我们使用了一个带有一个隐藏层的回归神经网络：
- en: '[PRE13]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Its learning goal is to approximate state-values; hence, we use the mean squared
    error as the loss function.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 它的学习目标是近似状态值；因此，我们使用均方误差作为损失函数。
- en: 'The `update` method trains the value regression model with a set of input states
    and target outputs, via backpropagation of course:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`update`方法通过反向传播训练值回归模型，使用一组输入状态和目标输出，当然是：'
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And the `predict` method estimates the state-value:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 而`predict`方法则是用来估计状态值：
- en: '[PRE15]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can move on to developing the REINFORCE with baseline algorithm with
    a policy and value network model:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以继续开发基准REINFORCE算法，其中包括一个策略和价值网络模型：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定策略网络的大小（输入、隐藏和输出层）、学习率，然后相应地创建一个`PolicyNetwork`实例：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As for the value network, we also set its size and create an instance:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 至于价值网络，我们也设置了其大小并创建了一个实例：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We set the discount factor as `0.9`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将折扣因子设置为`0.9`：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We perform learning using the REINFORCE with baseline algorithm for 2,000 episodes,
    and we also keep track of the total rewards for each episode:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用基准的REINFORCE算法进行2,000个episode的学习，并且我们还会追踪每个episode的总奖励：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we display the plot of episode reward over time:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们展示随时间变化的episode奖励的图表：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: REINFORCE relies heavily on Monte Carlo methods to generate a whole trajectory
    used to train the policy network. However, different actions may be taken in different
    episodes under the same stochastic policy. To reduce the variance for the sampled
    experience, we subtract the state-value from the return . The resulting advantage
    measures the reward relative to the average action, which will be used in the
    gradient update.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE高度依赖蒙特卡洛方法生成用于训练策略网络的整个轨迹。然而，在相同的随机策略下，不同的episode可能会采取不同的动作。为了减少采样经验的方差，我们从返回中减去状态值。由此产生的优势度量了相对于平均动作的奖励，这将在梯度更新中使用。
- en: 'In *Step 4*, REINFORCE with a baseline algorithm does the following tasks:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤4*中，使用基准的REINFORCE算法执行以下任务：
- en: It runs an episode—es the state, reward, and the log policy at each step.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它运行一个episode——处理状态、奖励，并且每一步记录策略的日志。
- en: Once an episode finishes, it calculates the discounted cumulative reward at
    each step; it estimates the baseline values using the value network; it computes
    the advantage values by subtracting the baseline values from the returns.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦一个episode完成，它会计算每一步的折扣累积奖励；它通过价值网络估计基准值；它通过从返回中减去基准值计算优势值。
- en: It computes policy gradients using the advantage values and log probabilities,
    and updates the policy and value networks. We also display the total reward for
    each episode.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用优势值和对数概率计算策略梯度，并更新策略和价值网络。我们还显示每个episode的总奖励。
- en: It runs `n_episode` episodes by repeating the aforementioned steps.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过重复上述步骤运行`n_episode`个episode。
- en: 'Executing the code in *Step 7* will result in the following plot:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*步骤7*中的代码将导致以下图表：
- en: '![](img/2f2957a0-aa31-4cce-9e87-a8e6d4ed9949.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f2957a0-aa31-4cce-9e87-a8e6d4ed9949.png)'
- en: You can see that the performance is very stable after around 1,200 episodes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在大约1,200个episode后，性能非常稳定。
- en: With the additional value baseline, we are able to recalibrate the rewards and
    reduce variance on the gradient estimates.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过额外的价值基准，我们能够重新校准奖励并减少梯度估计的方差。
- en: Implementing the actor-critic algorithm
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施演员-评论家算法
- en: In the REINFORCE with baseline algorithm, there are two separate components,
    the policy model and the value function. We can actually combine the learning
    of these two components, since the goal of learning the value function is to update
    the policy network. This is what the **actor-critic** algorithm does, and which
    we are going to develop in this recipe.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在基准REINFORCE算法中，有两个独立的组成部分，策略模型和价值函数。实际上，我们可以结合这两个组件的学习，因为学习价值函数的目标是更新策略网络。这就是**演员-评论家**算法所做的事情，这也是我们将在本文中开发的内容。
- en: 'The network for the actor-critic algorithm consists of the following two parts:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家算法的网络包括以下两个部分：
- en: '**Actor**: This takes in the input state and outputs the action probabilities.
    Essentially, it learns the optimal policy by updating the model using information
    provided by the critic.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员**：它接收输入状态并输出动作概率。本质上，通过使用评论家提供的信息来更新模型，它学习最优策略。'
- en: '**Critic**: This evaluates how good it is to be at the input state by computing
    the value function. The value guides the actor on how it should adjust.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论家**：这评估了在输入状态时表现良好的价值函数。价值指导演员如何调整。'
- en: These two components share parameters of input and hidden layers in the network,
    as learning is more efficient in this way than learning them separately. Accordingly,
    the loss function is a summation of two parts, specifically, the negative log
    likelihood of action measuring the actor, and the mean squared error between the
    estimated and computed return measuring the critic.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个组件在网络中共享输入和隐藏层的参数，这样学习效率比分开学习更高。因此，损失函数是两部分的总和，具体是测量演员的动作的负对数似然和估计和计算回报之间的均方误差测量评论家。
- en: A more popular version of the actor-critic algorithm is **Advantage Actor-Critic**
    (**A2C**). As its name implies, the critic part computes the advantage value,
    instead of the state-value, which is similar to REINFORCE with baseline. It evaluates
    how better an action is at a state compared to the other actions, and is known
    to reduce variance in policy networks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家算法的一个更受欢迎的版本是**优势演员-评论家**（**A2C**）。正如其名称所示，评论部分计算优势值，而不是状态值，这类似于带基线的REINFORCE。它评估了一个动作在一个状态下相对于其他动作的优越性，并且已知可以减少策略网络中的方差。
- en: How to do it...
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We develop the actor-critic algorithm in order to solve the CartPole environment
    as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发演员-评论家算法以解决CartPole环境，具体如下：
- en: 'Import all the necessary packages and create a CartPole instance:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包并创建一个CartPole实例：
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s start with the actor-critic neural network model:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从演员-评论家神经网络模型开始：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We continue with the `__init__` method of the `PolicyNetwork` class using the
    actor-critic neural network:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续使用演员-评论家神经网络开发`PolicyNetwork`类的`__init__`方法：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that we use herein a learning rate reducer that allows a dynamic learning
    rate according to learning progress.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在此处使用了一个学习率减少器，根据学习进展动态调整学习率。
- en: 'Next, we add the `predict` method, which computes the estimated action probabilities
    and state-value:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加`predict`方法，它计算估计的动作概率和状态值：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now develop the `training` method, which updates the neural network with
    samples collected in an episode:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开发`training`方法，用于使用在一个episode中收集的样本更新神经网络：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The final method for the `PolicyNetwork` class is get_action, which samples
    an action given a state based on the predicted policy:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PolicyNetwork`类的最终方法是`get_action`，根据预测的策略在给定状态下对动作进行抽样：'
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It also returns the log probability of the selected action, as well as the estimated
    state-value.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 它还返回所选动作的对数概率，以及估计的状态值。
- en: That's all for the `PolicyNetwork` class!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`PolicyNetwork`类的全部内容！
- en: 'Now, we can move on to developing the main function, training an actor-critic
    model:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以继续开发主函数，训练演员-评论家模型：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定策略网络的大小（输入、隐藏和输出层）、学习率，然后相应地创建一个`PolicyNetwork`实例：
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We set the discount factor as `0.9`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将折现因子设置为`0.9`：
- en: '[PRE30]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We perform learning with the actor-critic algorithm using the policy network
    we just developed for 1,000 episodes, and we also keep track of the total rewards
    for each episode:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用刚刚开发的策略网络进行演员-评论家算法的学习，进行了1,000个episode，并跟踪每个episode的总奖励：
- en: '[PRE31]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, we display the plot of episode reward over time:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们显示随时间变化的episode奖励的图表：
- en: '[PRE32]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works...
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理是...
- en: As you can see in *Step 2*, the actor and critic share parameters of the input
    and the hidden layers; the output of the actor consists of the probability of
    taking individual actions, and the output of the critic is the estimated value
    of the input state.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*步骤 2*中所看到的，演员和评论家共享输入和隐藏层的参数；演员的输出包括采取各个动作的概率，评论家的输出是输入状态的估计值。
- en: In *Step 5*, we compute the advantage value and its negative log likelihood.
    The loss function in actor-critic is the combination of the negative log likelihood
    of advantage and the mean squared error between the return and estimated state-value.
    Note that we use `smooth_l1_loss`, which is a squared term, if the absolute error
    falls below 1, and an absolute error otherwise.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们计算优势值及其负对数似然。演员-评论家中的损失函数是优势的负对数似然与回报与估计状态值之间均方误差的组合。请注意，我们使用`smooth_l1_loss`，当绝对误差低于1时，它是一个平方项，否则是一个绝对误差。
- en: 'In *Step 7*, the training function for the actor-critic model carries out the
    following tasks:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Step 7*中，演员-评论者模型的训练函数执行以下任务：
- en: 'It runs an episode: for each step in the episode, it samples an action based
    on the current estimated policy; it stores the reward, log policy, and estimated
    state-value at each step.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它运行一个episode：对于每个步骤，根据当前估计的策略采样一个动作；它在每个步骤存储奖励、对数策略和估计的状态值。
- en: Once an episode finishes, it calculates the discounted cumulative rewards at
    each step; it normalizes the resulting returns by subtracting their mean and then
    dividing them by their standard deviation.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦一个episode结束，它会计算每一步的折现累积奖励；然后通过减去它们的均值并除以它们的标准差来归一化返回结果。
- en: It updates policy parameters using the returns, log probabilities, and state-values.
    We also display the total reward for each episode.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用回报、对数概率和状态值更新策略参数。我们还会显示每个episode的总奖励。
- en: If the total reward for an episode is more than +195, we reduce the learning
    rate slightly.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个episode的总奖励超过+195，我们会稍微降低学习率。
- en: It runs `n_episode` episodes by repeating the aforementioned steps.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重复上述步骤，它运行`n_episode`个episode。
- en: 'You will see the following logs after executing the training in *Step 9*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行*Step 9*的训练后，您将看到以下日志：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following plot is the result of *Step 10*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表展示了*Step 10*的结果：
- en: '![](img/f41226b6-6f40-4a12-899d-0d2507546eb7.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f41226b6-6f40-4a12-899d-0d2507546eb7.png)'
- en: You can see that the rewards after around 400 episodes stay in the maximum value
    of +200.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到大约400个episode后的奖励保持在+200的最大值。
- en: In the advantage actor-critic algorithm, we decompose learning into two pieces
    – actor and critic. Critic in A2C evaluates how good an action is at a state,
    which guides the actor on how it should react. Again, the advantage value is computed
    as A(s,a) = Q(s,a) -V(s), which means subtracting state-values from Q values.
    Actor estimates the action probabilities based on critic's guidance. The introduction
    of advantage can reduce variance, and hence, A2C is considered a more stable model
    than the standard actor-critic. As we can see in the CartPole environment, the
    performance of A2C has been consistent after the training of several hundred episodes.
    It outperforms REINFORCE with baseline.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在优势演员-评论者算法中，我们将学习分解为两个部分 - 演员和评论者。A2C中的评论者评估在状态下动作的好坏，这指导演员如何反应。再次，优势值被计算为A(s,a)
    = Q(s,a) - V(s)，这意味着从Q值中减去状态值。演员根据评论者的指导估计动作的概率。优势的引入可以减少方差，因此A2C被认为比标准演员-评论者模型更稳定。正如我们在CartPole环境中看到的，经过数百个episode的训练后，A2C的表现一直很稳定。它优于带基准的REINFORCE算法。
- en: Solving Cliff Walking with the actor-critic algorithm
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用演员-评论者算法解决Cliff Walking
- en: In this recipe, let's solve a more complicated Cliff Walking environment using
    the A2C algorithm.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用A2C算法解决一个更复杂的Cliff Walking环境问题。
- en: Cliff Walking is a typical Gym environment with long episodes without a guarantee
    of termination. It is a grid problem with a 4 * 12 board. An agent makes a move
    of up, right, down and left at a step. The bottom-left tile is the starting point
    for the agent, and the bottom-right is the winning point where an episode will
    end if it is reached. The remaining tiles in the last row are cliffs where the
    agent will be reset to the starting position after stepping on any of them, but
    the episode continues. Each step the agent takes incurs a -1 reward, with the
    exception of stepping on the cliffs, where a -100 reward incurs.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Cliff Walking是一个典型的Gym环境，episode很长且没有终止的保证。这是一个4 * 12的网格问题。代理在每一步可以向上、向右、向下和向左移动。左下角的瓦片是代理的起点，右下角是获胜点，如果到达则结束episode。最后一行的其余瓦片是悬崖，代理在踩到它们后将被重置到起始位置，但episode继续。代理每走一步会产生-1的奖励，但踩到悬崖时会产生-100的奖励。
- en: 'The state is an integer from 0 to 47, indicating where the agent is located,
    as illustrated:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 状态是一个从0到47的整数，表示代理的位置，如图所示：
- en: '![](img/3f3f091a-97ef-4fee-8d51-45257ee50773.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f3f091a-97ef-4fee-8d51-45257ee50773.png)'
- en: Such value does not contain a numerical meaning. For example, being at state
    30 does not mean it is 3 times different from being in state 10\. Hence, we will
    first convert it into a one-hot encoded vector before feeding the state to the
    policy network.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的值并不包含数值意义。例如，处于状态30并不意味着它比处于状态10多3倍。因此，在将状态输入策略网络之前，我们将首先将其转换为一个one-hot编码向量。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'We solve Cliff Walking using the A2C algorithm as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用A2C算法解决Cliff Walking如下：
- en: 'Import all the necessary packages and create a CartPole instance:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包，并创建一个CartPole实例：
- en: '[PRE34]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As the state becomes 48-dimension, we use a more complicated actor-critic neural
    network with two hidden layers:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于状态变为48维，我们使用了一个更复杂的具有两个隐藏层的actor-critic神经网络：
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Again, the actor and critic share parameters of the input and hidden layers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，actor和critic共享输入和隐藏层的参数。
- en: We continue with the `PolicyNetwork` class using the actor-critic neural network
    we just developed in *Step 2*. It is the same as the `PolicyNetwork` class in
    the *Implementing the actor-critic algorithm* recipe.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续使用刚刚在*Step 2*中开发的actor-critic神经网络来使用`PolicyNetwork`类。它与*Implementing the
    actor-critic algorithm*案例中的`PolicyNetwork`类相同。
- en: 'Next, we develop the main function, training an actor-critic model. It is almost
    the same as the one in the *Implementing the actor-critic algorithm* recipe, with
    the additional transformation of state into a one-hot encoded vector:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开发主函数，训练一个actor-critic模型。它几乎与*Implementing the actor-critic algorithm*案例中的模型相同，只是额外将状态转换为one-hot编码向量：
- en: '[PRE36]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定策略网络的大小（输入、隐藏和输出层）、学习率，然后相应地创建一个`PolicyNetwork`实例：
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'And we set the discount factor as `0.9`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将折扣因子设为`0.9`：
- en: '[PRE38]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We perform learning with the actor-critic algorithm using the policy network
    we just developed for 1,000 episodes, and we also keep track of the total rewards
    for each episode:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用刚刚开发的策略网络进行1000个episode的actor-critic算法学习，并跟踪每个episode的总奖励：
- en: '[PRE39]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we display the plot of episode reward over time since the 100th episode:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们展示自第100个episode开始训练后的奖励变化曲线图：
- en: '[PRE40]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: You may notice in *Step 4* that we reduce the learning rate slightly if the
    total reward for an episode is more than -14\. A reward of -13 is the maximum
    value we are able to achieve, by taking the path 36-24-25-26-27-28-29-30-31-32-33-34-35-47\.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到在*Step 4*中，如果一个episode的总奖励超过-14，我们会略微降低学习率。-13的奖励是我们能够通过路径36-24-25-26-27-28-29-30-31-32-33-34-35-47获得的最大值。
- en: 'You will see the following logs after executing the training in *Step 6*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*Step 6*训练后，您将看到以下日志：
- en: '[PRE41]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following plot is the result of *Step 7*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了*Step 7*的结果：
- en: '![](img/9d2e58d4-093b-42d1-9f0f-202d1a669b37.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d2e58d4-093b-42d1-9f0f-202d1a669b37.png)'
- en: As we can observe, after around the 180th episode, rewards in most episodes
    achieve the optimal value, -13.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们可以观察到的那样，在大约第180个episode之后，大多数episode的奖励达到了最优值-13。
- en: In this recipe, we solved the Cliff Walking problem with the A2C algorithm.
    As the integer state from 0 to 47 represents the location of the agent in the
    4*12 board, which doesn't contain numerical meaning, we first converted it into
    a one-hot encoded vector of 48 dimensions. To deal with the input of 48 dimensions,
    we use a slightly more complicated neural network with two hidden layers. A2C
    has proved to be a stable policy method in our experiment.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们使用A2C算法解决了Cliff Walking问题。整数状态从0到47表示4*12棋盘中代理的位置，这些位置并没有数值意义，因此我们首先将其转换为48维的one-hot编码向量。为了处理48维输入，我们使用了一个稍微复杂的具有两个隐藏层的神经网络。在我们的实验中，A2C已被证明是一个稳定的策略方法。
- en: Setting up the continuous Mountain Car environment
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置连续的Mountain Car环境
- en: So far, the environments we have worked on have discrete action values, such
    as 0 or 1, representing up or down, left or right. In this recipe, we will experience
    a Mountain Car environment with continuous actions.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所处理的环境都具有离散的动作值，比如0或1，代表上下左右。在这个案例中，我们将体验一个具有连续动作的Mountain Car环境。
- en: 'Continuous Mountain Car ([https://github.com/openai/gym/wiki/MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0))
    is a Mountain Car environment with continuous actions whose value is from -1 to
    1\. As shown in the following screenshot, its goal is to get the car to the top
    of the hill on the right-hand side:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 连续的Mountain Car（[https://github.com/openai/gym/wiki/MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0)）是一个具有连续动作的Mountain
    Car环境，其值从-1到1。如下截图所示，其目标是将汽车驶到右侧山顶上：
- en: '![](img/2805ac57-39fd-4fd6-a56b-18e052a2926b.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2805ac57-39fd-4fd6-a56b-18e052a2926b.png)'
- en: In a one-dimensional track, the car is positioned between -1.2 (leftmost) and
    0.6 (rightmost), and the goal (yellow flag) is located at 0.5\. The engine of
    the car is not strong enough to drive it to the top in a single pass, so it has
    to drive back and forth to build up momentum. Hence, the action is a float that
    represents the force of pushing the car to the left if it is in a negative value
    from -1 to 0, or pushing the car to the right if it is in a positive value from
    0 to 1.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在一维赛道上，汽车位于-1.2（最左侧）到0.6（最右侧）之间，并且目标（黄旗）位于0.5处。汽车的引擎不足以在一次通过中将其推上山顶，因此必须来回驾驶以积累动量。因此，动作是一个浮点数，表示如果其值在-1到0之间则将汽车向左推，如果在0到1之间则将汽车向右推。
- en: 'There are two states of the environment:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 环境有两个状态：
- en: '**Position of the car**: This is a continuous variable from -1.2 to 0.6'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汽车的位置**：这是一个从-1.2到0.6的连续变量'
- en: '**Velocity of the car**: This is a continuous variable from -0.07 to 0.07'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汽车的速度**：这是一个从-0.07到0.07的连续变量'
- en: The starting state consists of a position between -0.6 to -0.4, and a velocity
    of 0.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 初始状态包括位置在-0.6到-0.4之间，速度为0。
- en: The reward associated with each step is *-a²*, where a is the action. And there
    is an additional reward of + 100 for reaching the goal. So, it penalizes the force
    taken in each step until the car reaches the goal. An episode ends when the car
    reaches the goal position (obviously), or after 1,000 steps.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步的奖励与动作a相关，为*-a²*。并且达到目标还会有额外的+100奖励。因此，它惩罚了每一步中所采取的力量，直到汽车到达目标位置。一个episode在汽车到达目标位置（显然是），或者经过1000步后结束。
- en: How to do it...
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s simulate the continuous Mountain Car environment by observing the following
    steps:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下步骤来模拟连续的山车环境：
- en: 'We import the Gym library and create an instance of the continuous Mountain
    Car environment:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入Gym库并创建一个连续山车环境的实例：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Take a look at the action space:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下动作空间：
- en: '[PRE43]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We then reset the environment:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们重置环境：
- en: '[PRE44]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The car starts with state [-0.56756635, 0\. ], which means that the initial
    position is around -0.56 and the velocity is 0\. You may see a different initial
    position as it is randomly generated from -0.6 to -0.4.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车的初始状态为[-0.56756635, 0\. ]，这意味着初始位置大约为-0.56，速度为0\. 由于初始位置是从-0.6到-0.4随机生成的，所以可能看到不同的初始位置。
- en: 'Let''s now take a naive approach: we just take a random action from -1 to 1:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们采取一个简单的方法：我们只是随机选择一个动作从-1到1：
- en: '[PRE45]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The state (position and velocity) keeps changing accordingly, and the reward
    is *-a²* for each step.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 状态（位置和速度）会相应地发生变化，每一步的奖励为*-a²*。
- en: You will also see in the video that the car is repeatedly moving right and back
    to the left.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会在视频中看到汽车反复向右移动和向左移动。
- en: How it works...
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理是这样的...
- en: As you can imagine, the continuous Mountain Car problem is a challenging environment,
    even more so than the original discrete one with only three different possible
    actions. We need to drive the car back and forth to build up momentum with the
    right amount of force and direction. Furthermore, the action space is continuous,
    which means that the value lookup / update method (such as the TD method, DQN)
    will not work. In the next recipe, we will solve the continuous Mountain Car problem
    with a continuous control version of the A2C algorithm.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所想象的那样，连续的山车问题是一个具有挑战性的环境，甚至比仅有三种不同可能动作的原始离散问题更加困难。我们需要来回驾驶汽车以积累正确的力量和方向。此外，动作空间是连续的，这意味着值查找/更新方法（如TD方法、DQN）将不起作用。在下一个示例中，我们将使用A2C算法的连续控制版本来解决连续的山车问题。
- en: Solving the continuous Mountain Car environment with the advantage actor-critic
    network
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用优势演员-评论者网络解决连续山车环境
- en: In this recipe, we are going to solve the continuous Mountain Car problem using
    the advantage actor-critic algorithm, a continuous version this time of course.
    You will see how it differs from the discrete version.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用优势演员-评论者算法来解决连续的山车问题，这当然是一个连续版本。你会看到它与离散版本有何不同。
- en: 'As we have seen in A2C for environments with discrete actions, we sample actions
    based on the estimated probabilities. How can we model a continuous control, since
    we can''t do such sampling for countless continuous actions? We can actually resort
    to Gaussian distribution. We can assume that the action values are under a Gaussian
    distribution:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在具有离散动作的环境中看到的那样，在连续控制中，由于我们无法对无数个连续动作进行采样，我们如何建模？实际上，我们可以借助高斯分布。我们可以假设动作值服从高斯分布：
- en: '![](img/1ceacbcf-3d76-4b84-80b6-2a9a8b84ab95.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ceacbcf-3d76-4b84-80b6-2a9a8b84ab95.png)'
- en: Here, the mean, [![](img/80063eb8-d3ac-4465-a2b3-bfbab4aa8e51.png)], and deviation,
    ![](img/2aed4bd3-11a4-42cc-8496-64b4c2a2fa2c.png), are computed from the policy
    network. With this tweak, we can sample actions from the constructed Gaussian
    distribution by current mean and deviation. The loss function in continuous A2C
    is similar to the one we used in discrete control, which is a combination of negative
    log likelihood computed with the action probabilities under the Gaussian distribution
    and the advantage values, and the regression error between the actual return values
    and estimated state-values.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，均值，[![](img/80063eb8-d3ac-4465-a2b3-bfbab4aa8e51.png)]，以及偏差，![](img/2aed4bd3-11a4-42cc-8496-64b4c2a2fa2c.png)，是从策略网络中计算出来的。通过这种调整，我们可以通过当前均值和偏差构建的高斯分布采样动作。连续A2C中的损失函数类似于我们在离散控制中使用的损失函数，它是在高斯分布下动作概率的负对数似然和优势值之间的组合，以及实际回报值与预估状态值之间的回归误差。
- en: Note that one Gaussian distribution is used to simulate action in one dimension,
    so, if the action space is in k dimensions, we need to use k Gaussian distributions.
    In the continuous Mountain Car environment, the action space is one-dimensional.
    The main difficulty of A2C as regards continuous control is how to construct the
    policy network, as it computes parameters for the Gaussian distribution.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一个高斯分布用于模拟一个维度的动作，因此，如果动作空间是k维的，我们需要使用k个高斯分布。在连续Mountain Car环境中，动作空间是一维的。就连续控制而言，A2C的主要困难在于如何构建策略网络，因为它计算了高斯分布的参数。
- en: How to do it...
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We solve the continuous Mountain Car problem using continuous A2C as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用连续A2C来解决连续Mountain Car问题，具体如下：
- en: 'Import all the necessary packages and create a continuous Mountain Car instance:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包并创建一个连续Mountain Car实例：
- en: '[PRE46]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s start with the actor-critic neural network model:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从演员-评论神经网络模型开始：
- en: '[PRE47]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We continue with the `__init__` method of the `PolicyNetwork` class using the
    actor-critic neural network we just developed:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续使用刚刚开发的演员-评论神经网络的`PolicyNetwork`类中的`__init__`方法：
- en: '[PRE48]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, we add the `predict` method, which computes the estimated action probabilities
    and state-value:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加`predict`方法，用于计算预估的动作概率和状态值：
- en: '[PRE49]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We now develop the training method, which updates the policy network with samples
    collected in an episode. We will reuse the update method developed in the *Implementing
    the actor-critic algorithm* recipe and will not repeat it here.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在开发训练方法，该方法使用一个episode中收集的样本来更新策略网络。我们将重用*实施演员-评论算法*食谱中开发的更新方法，这里不再重复。
- en: 'The final method for the `PolicyNetwork` class is `get_action`, which samples
    an action from the estimated Gaussian distribution given a state:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PolicyNetwork`类的最终方法是`get_action`，它从给定状态的预估高斯分布中采样一个动作：'
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: It also returns the log probability of the selected action, and the estimated
    state-value.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 它还返回所选动作的对数概率和预估状态值。
- en: That's all for the `PolicyNetwork` class for continuous control!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是用于连续控制的`PolicyNetwork`类的全部内容！
- en: 'Now, we can move on to developing the main function, training an actor-critic
    model:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以继续开发主函数，训练一个演员-评论模型：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `scale_state` function is used to normalize (standardize) the inputs for
    faster model convergence. We first randomly generate 10,000 observations and use
    them to train a scaler:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scale_state`函数用于对输入进行标准化（规范化），以加快模型的收敛速度。我们首先随机生成10,000个观测数据，并用它们来训练一个缩放器：'
- en: '[PRE52]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Once the scaler is trained, we use it in the `scale_state` function to transform
    new input data:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦缩放器被训练好，我们就在`scale_state`函数中使用它来转换新的输入数据：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We specify the size of the policy network (input, hidden, and output layers),
    the learning rate, and then create a `PolicyNetwork` instance accordingly:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定策略网络的大小（输入、隐藏和输出层），学习率，然后相应地创建一个`PolicyNetwork`实例：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We set the discount factor as `0.9`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将折现因子设为`0.9`：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We perform continuous control with the actor-critic algorithm using the policy
    network we just developed for 200 episodes, and we also keep track of the total
    rewards for each episode:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用刚刚开发的策略网络进行200个剧集的actor-critic算法进行连续控制，并且我们还跟踪每个剧集的总奖励：
- en: '[PRE56]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, let''s display the plot of episode reward over time:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们展示随时间变化的剧集奖励图：
- en: '[PRE57]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: How it works...
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何工作...
- en: In this recipe, we used Gaussian A2C to solve the continuous Mountain Car environment.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用高斯A2C来解决连续的Mountain Car环境。
- en: In *Step 2*, the network in our example has one hidden layer. There are three
    separate components in the output layer. They are the mean and deviation of the
    Gaussian distribution, and the state-value. The output of the distribution mean
    is scaled to the range of [-1, 1] (or [-2, 2] in this example), using a tanh activation
    function. As for the distribution deviation, we use softplus as the activation
    function to ensure a positive deviation. The network returns the current Gaussian
    distribution (actor) and estimated state-value (critic).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2步*中，我们示例中的网络有一个隐藏层。输出层有三个单独的组件。它们是高斯分布的均值和偏差，以及状态值。分布均值的输出通过tanh激活函数缩放到[-1,
    1]范围（或此示例中的[-2, 2]），而分布偏差使用softplus作为激活函数以确保正偏差。网络返回当前的高斯分布（actor）和估计的状态值（critic）。
- en: The training function for the actor-critic model in *Step 7* is quite similar
    to what we developed in the *Implementing the actor-critic algorithm* recipe.
    You may notice that we add a value clip to the sampled action in order to keep
    it within the [-1, 1] range . We will explain what the `scale_state` function
    does in an upcoming step.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7步*的actor-critic模型训练函数与我们在*实施actor-critic算法*配方中开发的内容非常相似。您可能会注意到我们在采样动作时添加了一个值剪辑，以使其保持在[-1,
    1]范围内。我们将在接下来的步骤中解释`scale_state`函数的作用。
- en: 'You will see the following logs after executing the training in *Step 10*:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行*第10步*的训练后，您将看到以下日志：
- en: '[PRE58]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following plot is the result of *Step 11*:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是*第11步*的结果：
- en: '![](img/51521f2d-b160-4dc5-a9e3-ba36cae41947.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51521f2d-b160-4dc5-a9e3-ba36cae41947.png)'
- en: According to the resolved requirements in [https://github.com/openai/gym/wiki/MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0),
    getting a reward above +90 is regarded as the environment being solved. We have
    multiple episodes where we solve the environment.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[https://github.com/openai/gym/wiki/MountainCarContinuous-v0](https://github.com/openai/gym/wiki/MountainCarContinuous-v0)中解决的要求，获得超过+90的奖励被视为环境已解决。我们有多个剧集解决了环境问题。
- en: In continuous A2C, we assume that each dimension of the action space is Gaussian
    distributed. The mean and deviation of a Gaussian distribution are parts of the
    output layer of the policy network. The remainder of the output layer is for the
    estimation of state-value. An action (or set of actions) is (are) sampled from
    the Gaussian distribution(s) parameterized by the current mean(s) and deviation(s).
    The loss function in continuous A2C is similar to its discrete version, which
    is the combination of negative log likelihood computed with the action probabilities
    under the Gaussian distributions and the advantage values, and the regression
    error between the actual return values and estimated state-values.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续的A2C中，我们假设动作空间的每个维度都服从高斯分布。高斯分布的均值和偏差是策略网络输出层的一部分。输出层的其余部分用于估计状态值。从当前均值和偏差参数化的高斯分布中采样一个或多个动作。连续A2C的损失函数类似于其离散版本，即负对数似然与高斯分布下动作概率以及优势值之间的组合，以及实际回报值与估计状态值之间的回归误差。
- en: There's more...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'So far, we have always modeled policy in a stochastic manner where we sample
    actions from distributions or computed probabilities. As a bonus section, we will
    briefly talk about the **Deterministic Policy Gradient** (**DPG**), where we model
    the policy as a deterministic decision. We simply treat the deterministic policy
    as a special case of stochastic policy by directly mapping input states to actions
    instead of probabilities of actions. The DPG algorithm generally uses the following
    two sets of neural networks:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直是以随机的方式建模策略，从分布或计算的概率中采样动作。作为一个额外部分，我们将简要讨论**确定性策略梯度**（**DPG**），在这里我们将策略建模为确定性决策。我们简单地将确定性策略视为随机策略的特例，直接将输入状态映射到动作而不是动作的概率。DPG算法通常使用以下两组神经网络：
- en: '**Actor-critic network**: This is very similar to the A2C we have experienced,
    but in a deterministic manner. It predicts state-values and actions to be taken.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Actor-critic 网络**：这与我们之前体验过的 A2C 非常相似，但是是以确定性方式进行。它预测状态值和需要执行的动作。'
- en: '**Target actor-critic network**: This is a periodical copy of the actor-critic
    network with the purpose of stabilizing learning. Obviously, you don''t want to
    have targets that keep on changing. This network provides time-delayed targets
    for training.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标 actor-critic 网络**：这是 actor-critic 网络的定期副本，其目的是稳定学习。显然，你不希望目标一直在变化。该网络为训练提供了延迟的目标。'
- en: As you can see, there is not much new in DPG, but a nice combination of A2C
    and a time-delayed target mechanism. Feel free to implement the algorithm yourself
    and use it to solve the continuous Mountain Car environment.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在 DPG 中并没有太多新东西，但它是 A2C 和延迟目标机制的良好结合。请随意自行实现该算法，并用它来解决连续的 Mountain Car
    环境。
- en: See also
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'If you are not familiar with softplus activation, or want to read more about
    DPG, please check out the following material:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 softplus 激活函数不熟悉，或者想要了解更多关于 DPG 的内容，请查看以下材料：
- en: 'Softplus: [https://en.wikipedia.org/wiki/Rectifier_(neural_networks)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Softplus: [https://en.wikipedia.org/wiki/Rectifier_(neural_networks)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))'
- en: 'Original paper of the DFP: [https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf](https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DFP 的原始论文：[https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf](https://hal.inria.fr/file/index/docid/938992/filename/dpg-icml2014.pdf)
- en: Playing CartPole through the cross-entropy method
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过交叉熵方法玩 CartPole
- en: In this last recipe, by way of a bonus (and fun) section, we will develop a
    simple, yet powerful, algorithm to solve CartPole. It is based on cross-entropy,
    and directly maps input states to an output action. In fact, it is more straightforward
    than all the other policy gradient algorithms in this chapter.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个最后的示例中，作为一个额外的（也很有趣的）部分，我们将开发一个简单而强大的算法来解决 CartPole 问题。它基于交叉熵，直接将输入状态映射到输出动作。事实上，它比本章中所有其他策略梯度算法更为直接。
- en: 'We have applied several policy gradient algorithms to solve the CartPole environment.
    They use complicated neural network architectures and a loss function, which may
    be overkill for simple environments such as CartPole. Why don''t we directly predict
    the actions for given states? The idea behind this is straightforward: we model
    the mapping from state to action, and train it ONLY with the most successful experiences
    from the past. We are only interested in what the correct actions should be. The
    objective function, in this case, is the cross-entropy between the actual and
    predicted actions. In CartPole, there are two possible actions: left and right.
    For simplicity, we can convert it into a binary classification problem with the
    following model diagram:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经应用了几种策略梯度算法来解决 CartPole 环境。它们使用复杂的神经网络架构和损失函数，这对于如 CartPole 这样的简单环境可能有些过度。为什么不直接预测给定状态下的动作呢？其背后的思想很简单：我们对过去最成功的经验进行建模，仅对正确的动作感兴趣。在这种情况下，目标函数是实际动作和预测动作之间的交叉熵。在
    CartPole 中，有两种可能的动作：左和右。为了简单起见，我们可以将其转换为二元分类问题，并使用以下模型图表述：
- en: '![](img/8a968090-e424-4f42-b82d-6d100b67aa41.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a968090-e424-4f42-b82d-6d100b67aa41.png)'
- en: How to do it...
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现它...
- en: 'We solve the CartPole problem using cross-entropy as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用交叉熵来解决 CartPole 问题如下：
- en: 'Import all the necessary packages and create a CartPole instance:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的包并创建一个 CartPole 实例：
- en: '[PRE59]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s start with the action estimator:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从动作估算器开始：
- en: '[PRE60]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We now develop the main training function for the cross-entropy algorithm:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们为交叉熵算法开发主要的训练函数：
- en: '[PRE61]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We then specify the input size of the action estimator and the learning rate:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们指定动作估算器的输入大小和学习率：
- en: '[PRE62]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then we create an `Estimator` instance accordingly:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们相应地创建一个 `Estimator` 实例：
- en: '[PRE63]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We will generate 5,000 random episodes and cherry-pick the best 10,000 (state,
    action) pairs for training the estimator:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将生成 5,000 个随机的情节，并精选出最佳的 10,000 个（状态，动作）对用于估算器的训练：
- en: '[PRE64]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After the model is trained, let''s test it out. We use it to play 100 episodes
    and record the total rewards:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练完成后，让我们来测试一下。我们将用它来玩 100 个情节，并记录总奖励：
- en: '[PRE65]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We then visualize the performance as follows:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将性能可视化如下：
- en: '[PRE66]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: How it works...
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: As you can see in *Step 2*, the action estimator has two layers – input and
    output layers, followed by a sigmoid activation function, and the loss function
    is binary cross-entropy.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在*Step 2*中所看到的，动作估计器有两层 - 输入层和输出层，接着是一个sigmoid激活函数，损失函数是二元交叉熵。
- en: '*Step 3* is for training the cross-entropy model. Specifically, for each training
    episode, we take random actions, accumulate rewards, and record states and actions.
    After experiencing `n_episode` episodes, we take the most successful episodes
    (with the highest total rewards) and extract `n_samples` of (state, action) pairs
    as training samples. We then train the estimator for 100 iterations on the training
    set we just constructed.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*Step 3*是为了训练交叉熵模型。具体而言，对于每个训练集，我们采取随机行动，累积奖励，并记录状态和行动。在体验了`n_episode`个集数后，我们提取最成功的集数（具有最高总奖励）并提取`n_samples`个（状态，行动）对作为训练样本。然后我们在刚构建的训练集上对估计器进行100次迭代的训练。'
- en: 'Executing the lines of code in *Step 7* will result in the following plot:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 执行*Step 7*中的代码行将产生以下绘图：
- en: '![](img/de8cf5c8-e443-43a1-b654-5816869e7d01.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de8cf5c8-e443-43a1-b654-5816869e7d01.png)'
- en: As you can see, there are +200 rewards for all testing episodes!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，所有测试集都有+200的奖励！
- en: Cross-entropy is very simple, yet useful, for simple environments. It directly
    models the relationship between input states and output actions. A control problem
    is framed into a classification problem where we try to predict the correct action
    among all the alternatives. The trick is that we only learn from the right experience,
    which guides the model in terms of what the most rewarding action should be, given
    a state.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵对于简单环境非常简单，但却很有用。它直接建模输入状态和输出行动之间的关系。一个控制问题被构建成一个分类问题，我们试图在所有备选行动中预测正确的行动。关键在于我们只从正确的经验中学习，这指导模型在给定状态时应该选择哪个最有益的行动。
