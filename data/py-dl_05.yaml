- en: Chapter 5. Image Recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。图像识别
- en: Vision is arguably the most important human sense. We rely on our vision to
    recognize our food, to run away from danger, to recognize our friends and family,
    and to find our way in familiar surroundings. We rely on our vision, in fact,
    to read this book and to recognize each and every letter and symbol printed in
    it. However, image recognition has (and in many ways still is) for the longest
    time been one of the most difficult problems in computer science. It is very hard
    to teach a computer programmatically how to recognize different objects, because
    it is difficult to explain to a machine what features make up a specified object.
    In deep learning, however, as we have seen, the neural network learns by itself,
    that is, it learns what features make up each object, and it is therefore well
    suited for a task such as image recognition.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉可以说是人类最重要的感官之一。我们依赖视觉来识别食物，逃离危险，认出朋友和家人，以及在熟悉的环境中找到方向。我们甚至依赖视觉来阅读这本书，并识别其中打印的每一个字母和符号。然而，图像识别一直以来一直是计算机科学中最困难的问题之一。因为要教会计算机如何识别不同的物体是非常困难的，因为很难向机器解释构成指定物体的特征。然而，正如我们所看到的，深度学习中的神经网络通过自身学习，也就是学会了构成每个物体的特征，因此非常适合图像识别这样的任务。
- en: 'In this chapter we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Similarities between artificial and biological models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人造模型和生物模型之间的相似之处
- en: Intuition and justification for CNN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 的直觉和理由
- en: Convolutional layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池层
- en: Dropout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃
- en: Convolutional layers in deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习中的卷积层
- en: Similarities between artificial and biological models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人造模型和生物模型之间的相似之处
- en: 'Human vision is a complex and heavily structured process. The visual system
    works by hierarchically understanding reality through the retina, the thalamus,
    the visual cortex, and the inferior temporal cortex. The input to the retina is
    a two-dimensional array of color intensities that is sent, through the optical
    nerve, to the thalamus. The thalamus receives sensory information from all of
    our senses with the exception of the olfactory system and then it forwards the
    visual information collected from the retina to the primary visual cortex, which
    is the striate cortex (called V1), which extracts basic information such as lines
    and movement directions. The information then moves to the V2 region that is responsible
    for color interpretation and color constancy under different lighting conditions,
    then to the V3 and V4 regions that improve color and form perception. Finally,
    the information goes down to the **Inferior Temporal** cortex (**IT**) for object
    and face recognition (in reality, the IT region is also further subdivided in
    three sub-regions, the posterior IT, central IT, and anterior IT). It is therefore
    clear that the brain processes visual information by hierarchically processing
    the information at different levels. Our brain then seemingly works by creating
    simple abstract representations of reality at different levels that can then be
    recombined together (see for reference: J. DiCarlo, D. Zoccolan, and N. Rust,
    *How does the brain solve visual object recognition?*, [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444)).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人类视觉是一个复杂且结构严谨的过程。视觉系统通过视网膜、丘脑、视觉皮层和颞下皮质等阶级性地理解现实。视网膜的输入是一个二维的颜色密度数组，通过视神经传递到丘脑。丘脑除了嗅觉系统的感官信息外，还接收从视网膜收集的视觉信息，然后将该信息传递到初级视觉皮层，也就是V1区，它提取基本信息，例如线条和运动方向。然后信息流向负责色彩解释和不同光照条件下的颜色恒定性的V2区，然后到达V3和V4区，改善色彩和形态感知。最后，信息传递到**颞下皮质**（**IT**），用于物体和面部识别（事实上，IT区域还进一步细分为三个亚区，即后部IT、中央IT和前部IT）。因此，大脑通过在不同层级处理信息来处理视觉信息。我们的大脑似乎通过在不同层级上创建简单的抽象现实表示，然后将它们重新组合在一起来解决这个问题（详细参考：J.
    DiCarlo, D. Zoccolan, and N. Rust, *大脑是如何处理视觉物体识别的？*，[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3306444)）。
- en: 'The Deep Learning neural networks we have seen so far work similarly by creating
    abstract representations, as we have seen in RBMs, for example, but there is another
    important piece to the puzzle for understanding sensory information: the information
    we extract from sensory inputs is often determined mostly by the information most
    closely related. Visually, we can assume that pixels that are close by are most
    closely related and their collective information is more relevant than what we
    can derive from pixels very far from each other. In understanding speech, as another
    example, we have discussed how the study of tri-phones is important, that is,
    the fact that the understanding of a sound is dependent on the sounds preceding
    and following it. To recognize letters or digits, we need to understand the dependency
    of pixels close by, since that is what determines the shape of the element to
    figure out the difference between, say, a 0 or a 1\. Pixels that are very far
    from those making up a 0 hold, in general, little or no relevance for our understanding
    of the digit "0". Convolutional networks are built exactly to address this issue:
    how to make information pertaining to neurons that are closer more relevant than
    information coming from neurons that are farther apart. In visual problems, this
    translates into making neurons process information coming from pixels that are
    near, and ignoring information related to pixels that are far apart.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前看到的深度学习神经网络通过创建抽象表示来工作，就像我们在RBM中看到的那样，但是理解感官信息的重要拼图中还有另一个重要部分：我们从感官输入中提取的信息通常主要由最相关的信息确定。从视觉上看，我们可以假设附近的像素是最相关的，它们的集体信息比我们从彼此非常遥远的像素中得出的信息更相关。在理解语音方面，我们已经讨论过研究三音素的重要性，也就是说，对音频的理解依赖于其前后的声音。要识别字母或数字，我们需要理解附近像素的依赖性，因为这决定了元素的形状，从而区分例如0和1等之间的差异。总的来说，远离0的像素通常对我们理解数字"0"没有或几乎没有影响。卷积网络的构建正是为了解决这个问题：如何使与更近的神经元相关的信息比来自更远的神经元更相关的信息。在视觉问题中，这意味着让神经元处理来自附近像素的信息，并忽略与远离像素相关的信息。
- en: Intuition and justification
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直觉和理解
- en: 'We have already mentioned in [Chapter 3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 3. Deep Learning Fundamentals"), *Deep Learning Fundamentals*, the paper
    published in 2012 by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton titled:
    *ImageNet Classification with Deep Convolutional Neural Networks*. Though the
    genesis of convolutional may be traced back to the ''80s, that was one of the
    first papers that highlighted the deep importance of convolutional networks in
    image processing and recognition, and currently almost no deep neural network
    used for image recognition can work without some convolutional layer.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 3. Deep
    Learning Fundamentals")中已经提到了Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年发表的论文：*使用深度卷积神经网络进行ImageNet分类*。尽管卷积的起源可以追溯到80年代，但那是第一篇突出卷积网络在图像处理和识别中深刻重要性的论文之一，当前几乎没有用于图像识别的深度神经网络可以在没有某些卷积层的情况下工作。
- en: 'An important problem that we have seen when working with classical feed-forward
    networks is that they may overfit, especially when working with medium to large
    images. This is often due to the fact that neural networks have a very large number
    of parameters, in fact in classical neural nets all neurons in a layer are connected
    to each and every neuron in the next. When the number of parameters is large,
    over-fitting is more likely. Let''s look at the following images: we can fit the
    data by drawing a line that goes exactly through all the points, or better, a
    line that will not match exactly the data but is more likely to predict future
    examples.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在使用传统前馈网络时遇到的一个重要问题是它们可能会过拟合，特别是在处理中等到大型图像时。这通常是因为神经网络具有非常多的参数，事实上，在经典神经网络中，一层中的所有神经元都连接到下一层中的每一个神经元。当参数数量很大时，过拟合的可能性更大。让我们看以下图片：我们可以通过画一条穿过所有点的线来拟合数据，或者更好的是，一条不完全匹配数据但更可能预测未来示例的线。
- en: '![Intuition and justification](img/00201.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Intuition and justification](img/00201.jpeg)'
- en: The points in the figure represent input data points. While they clearly follow
    the shape of a parabola, because of noise in the data, they may not be precisely
    plotted onto a parabola
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的点表示输入数据点。虽然它们明显遵循抛物线的形状，但由于数据中的噪声，它们可能不会被精确地绘制到抛物线上。
- en: 'In the first example of the two pictures represented, we overfit the data.
    In the second we have matched our prediction to the data in such a way that our
    prediction is more likely to better predict future data. In the first case, we
    just need three parameters to describe the curve: *y = ax² + bx + c*, while in
    the second case we would need many more than just three parameters to write the
    equation for that curve. This gives an intuitive explanation of why, sometimes,
    having too many parameters may not be a good thing and it may lead to over-fitting.
    A classical feed-forward network for an image as small as those in the `cifar10`
    examples (`cifar10` is an established computer-vision dataset consisting of 60000
    32 x 32 images divided in to 10 classes, and we will see a couple of examples
    from this dataset in this chapter) has inputs of size 3 x 32 x 32, which is already
    about four times as large as a simple `mnist` digit image. Larger images, say
    3 x 64 x 64, would have about as many as 16 times the number of input neurons
    multiplying the number of connection weights:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在两幅图中的第一个例子中，我们对数据进行了过拟合。在第二个例子中，我们已经将我们的预测与数据匹配得更好，这样我们的预测更有可能更好地预测未来的数据。在第一种情况下，我们只需要三个参数来描述曲线：*y
    = ax² + bx + c*，而在第二种情况下，我们需要比三个参数多得多的参数来编写该曲线的方程。这直观地解释了为什么有时候拥有太多参数可能不是一件好事，而且可能导致过拟合。对于像
    `cifar10` 示例中那样小的图像（`cifar10` 是一个经过验证的计算机视觉数据集，由 60000 张 32 x 32 图像组成，分为 10 类，在本章中我们将看到该数据集的几个示例），经典的前馈网络的输入大小为
    3 x 32 x 32，已经约为简单 `mnist` 数字图像的四倍。更大的图像，比如 3 x 64 x 64，将拥有大约 16 倍于输入神经元数量的连接权重：
- en: '![Intuition and justification](img/00202.jpeg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![直觉和正当理由](img/00202.jpeg)'
- en: In the left figure we draw a line that matches the data exactly. In the second
    figure we draw a line that approximates the shape of the line connecting the data
    points, but that does not match exactly the data points. The second curve, though
    less precise on the current input, is more likely to predict future data points
    than the curve in the first figure.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在左图中，我们画了一条与数据完全匹配的直线。在第二个图中，我们画了一条近似连接数据点形状的直线，但并不完全匹配数据点。尽管第二条曲线在当前输入上不够精确，但比第一张图中的曲线更有可能预测未来的数据点。
- en: Convolutional networks reduce the number of parameters needed, since they require
    neurons to only connect locally to neurons corresponding to neighboring pixels,
    and therefore help avoid overfitting. In addition, reducing the number of parameters
    also helps computationally. In the next section, will introduce some convolutional
    layer examples to help the intuition and then we will move to formally define
    them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络减少了所需的参数数量，因为它们要求神经元仅在本地与对应于相邻像素的神经元连接，因此有助于避免过拟合。此外，减少参数数量也有助于计算。在下一节中，我们将介绍一些卷积层的示例来帮助理解，然后我们将正式定义它们。
- en: Convolutional layers
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: A convolutional layer (sometimes referred to in the literature as "filter")
    is a particular type of neural network that manipulates the image to highlight
    certain features. Before we get into the details, let's introduce a convolutional
    filter using some code and some examples. This will make the intuition simpler
    and will make understanding the theory easier. To do this we can use the `keras`
    datasets, which makes it easy to load the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层（有时在文献中称为 "滤波器"）是一种特殊类型的神经网络，它操作图像以突出显示某些特征。在深入了解细节之前，让我们使用一些代码和一些示例介绍一个卷积滤波器。这将使直觉更简单，也将更容易理解理论。为此，我们可以使用
    `keras` 数据集，这使得加载数据变得容易。
- en: 'We will import `numpy`, then the `mnist` dataset, and `matplotlib` to show
    the data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将导入 `numpy`，然后是 `mnist` 数据集，以及 `matplotlib` 来展示数据：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s define our main function that takes in an integer, corresponding to
    the image in the `mnist` dataset, and a filter, in this case we will define the
    `blur` filter:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的主函数，该函数接受一个整数，对应于 `mnist` 数据集中的图像，以及一个滤波器，这种情况下我们将定义 `blur` 滤波器：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we define a new image `imC`, of size `(im.width-2, im.height-2)`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一个新的图像 `imC`，大小为 `(im.width-2, im.height-2)`：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'At this point we do the convolution, which we will explain soon (as we will
    see, there are in fact several types of convolutions depending on different parameters,
    for now we will just explain the basic concept and get into the details later):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此时我们进行卷积，我们将很快解释（正如我们将看到的，实际上有几种类型的卷积取决于不同的参数，现在我们只是解释基本概念，并稍后详细介绍）：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we are ready to display the original image and the new image:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备显示原始图像和新图像：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we are ready to load the `mnist` dataset using Keras as we did in [Chapter
    3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 3. Deep
    Learning Fundamentals"), *Deep Learning Fundamentals*. Also, let''s define a filter.
    A filter is a small region (in this case 3 x 3) with each entry defining a real
    value. In this case we define a filter with the same value all over:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用Keras加载`mnist`数据集，就像我们在[第3章](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第3章。深度学习基础")中所做的那样，*深度学习基础*。此外，让我们定义一个滤波器。滤波器是一个小区域（在本例中为3 x 3），每个条目定义一个实数值。在这种情况下，我们定义一个所有条目值都相同的滤波器：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Since we have nine entries, we set the value to be 1/9 to normalize the values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有九个条目，我们将值设置为1/9以归一化值。
- en: 'And we can call the `main` function on any image (expressed by an integer that
    indicates the position) in such a dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对任何图像（用一个表示位置的整数表示）调用`main`函数在这样一个数据集中：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's look at what we did. We multiplied each entry of the filter with an entry
    of the original image, and then we summed them all up to get a single value. Since
    the filter size is smaller than the image size, we moved the filter by 1 pixel
    and kept doing this process until we covered the whole image. Since the filter
    was composed by values that are all equal to 1/9, we have in fact averaged all
    input values with the values that are close to it, and this has the effect of
    blurring the image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们做了什么。我们将滤波器的每个条目与原始图像的一个条目相乘，然后将它们全部加起来得到一个单一的值。由于滤波器的大小小于图像的大小，我们将滤波器移动1像素，并继续执行此过程，直到覆盖整个图像。由于滤波器由所有等于1/9的值组成，实际上我们已经用接近它的值的值平均了所有输入值，这就有了模糊图像的效果。
- en: 'This is what we get:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们得到的：
- en: '![Convolutional layers](img/00203.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00203.jpeg)'
- en: On top is the original mnist image, on the bottom is the new image after we
    applied the filter
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部是原始mnist图像，底部是我们应用滤波器后的新图像
- en: 'In the choice of the filter we can use any value we want; in this case we have
    used values that are all the same. However, we can instead use different values,
    for example values that only look at the neighboring values of the input, add
    them up, and subtract the value of the center input. Let''s define a new filter,
    and let''s call it edges, in the following way:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择滤波器时，我们可以使用任何值；在这种情况下，我们使用的是全部相同的值。但是，我们可以使用不同的值，例如仅查看输入的相邻值，将它们相加，并减去中心输入的值。让我们定义一个新的滤波器，并将其称为边缘，如下所示：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we now apply this filter, instead of the filter `blur` defined earlier,
    we get the following images:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在应用此滤波器，而不是之前定义的`模糊`滤波器，则会得到以下图像：
- en: '![Convolutional layers](img/00204.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00204.jpeg)'
- en: On top is the original mnist image, on the bottom is the new image after we
    applied the filter
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部是原始mnist图像，底部是我们应用滤波器后的新图像
- en: It is clear, therefore, that filters can alter the images, and show "features"
    that can be useful to detect and classify images. For example, to classify digits,
    the color of the inside is not important, and a filter such as "edges" helps identify
    the general shape of the digit which is what is important for a correct classification.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此很明显，滤波器可以改变图像，并显示可以用于检测和分类图像的“特征”。例如，要对数字进行分类，内部的颜色并不重要，而诸如“边缘”之类的滤波器有助于识别数字的一般形状，这对于正确分类是重要的。
- en: 'We can think of filters in the same way we think about neural networks, and
    think that the filter we have defined is a set of weights, and that the final
    value represents the activation value of a neuron in the next layer (in fact,
    even though we chose particular weights to discuss these examples, we will see
    that the weights will be *learned* by the neural network using back-propagation):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将滤波器视为与神经网络相同，认为我们定义的滤波器是一组权重，并且最终值表示下一层中神经元的激活值（实际上，尽管我们选择了特定的权重来讨论这些示例，但我们将看到权重将通过反向传播由神经网络*学习*）：
- en: '![Convolutional layers](img/00205.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00205.jpeg)'
- en: The filter covers a fixed region, and for each neuron in that region, it defines
    a connection weight to a neuron in the next layer. The neuron in the next layer
    will then have an input value equal to the regular activation value calculated
    by summing the contributions of all input neurons mediated by the corresponding
    connection weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器覆盖了一个固定的区域，对于该区域中的每个神经元，它定义了与下一层中的神经元的连接权重。然后，下一层中的神经元将具有输入值，该输入值等于通过相应的连接权重中介的所有输入神经元的贡献总和计算得到的常规激活值。
- en: 'We then keep the same weights and we slide the filter across, generating a
    new set of neurons, which correspond to the filtered image:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们保持相同的权重，滑动滤波器，生成一个新的神经元集，这些神经元对应于过滤后的图像：
- en: '![Convolutional layers](img/00206.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00206.jpeg)'
- en: 'We can keep repeating the process until we have moved across the whole image,
    and we can repeat this process with as many filters as we like, creating a new
    set of images, each of which will have different features or characteristics highlighted.
    While we have not used a bias in our examples, it is also possible to add a bias
    to the filter, which will be added to the neural network, and we can also define
    different activity functions. In our code example you will notice that we have
    forced the value to be in the range (0, 255), which can be thought of as a simple
    threshold function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不断重复这个过程，直到我们移动到整个图像上，我们可以使用尽可能多的滤波器重复这个过程，创建一组新的图像，每个图像都会突出显示不同的特征或特性。虽然我们在示例中没有使用偏置，但也可以向滤波器添加偏置，这将添加到神经网络中，我们还可以定义不同的活动函数。在我们的代码示例中，您会注意到我们强制值保持在范围(0,
    255)内，这可以被认为是一个简单的阈值函数：
- en: '![Convolutional layers](img/00207.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00207.jpeg)'
- en: As the filter moves across the image, we define new activation values for the
    neurons in the output image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当滤波器在图像上移动时，我们为输出图像中的神经元定义新的激活值。
- en: 'Since one may define many filters, we should think of the output not as a single
    image, but as a set of images, one for each filter defined. If we used just the
    "edges" and the "blur" filter, the output layer would therefore have two images,
    one per filter chosen. The output will therefore have, besides a width and a height,
    also a depth equal to the number of filters chosen. In actuality, the input layer
    can also have a depth if we use color images as input; images are in fact usually
    comprised of three channels, which in computer graphics are represented by RGB,
    the red channel, the green channel, and the blue channel. In our example, the
    filter is represented by a two-dimensional matrix (for example the `blur` filter
    is a 3 x 3 matrix with all entries equal to 1/9\. However, if the input is a color
    image, the filter will also have a depth (in this case equal to three, the number
    of color channels), and it will therefore be represented by three (number of color
    channels) 3 x 3 matrices. In general, the filter will therefore be represented
    by a three-dimensional array, with a width, a height, and a depth, which are sometimes
    called "volumes". In the preceding example, since the `mnist` images are gray-scale
    only, the filter had depth 1\. A general filter of depth *d* is therefore comprised
    of *d* filters of the same width and height. Each of those *d* filters are called
    a "slice" or a "leaf":'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可以定义许多滤波器，因此我们应该将输出视为一组图像，每个滤波器定义一个图像。如果我们仅使用“边缘”和“模糊”滤波器，则输出层将有两个图像，每个选择的滤波器一个。因此，输出将除了宽度和高度外，还具有等于选择的滤波器数的深度。实际上，如果我们使用彩色图像作为输入，输入层也可以具有深度；图像实际上通常由三个通道组成，在计算机图形中用RGB表示，红色通道、绿色通道和蓝色通道。在我们的示例中，滤波器由二维矩阵表示（例如`模糊`滤波器是一个3
    x 3矩阵，所有条目都相等于1/9）。然而，如果输入是彩色图像，则滤波器也将具有深度（在这种情况下等于三，即颜色通道的数量），因此将由三个（颜色通道数）3
    x 3矩阵表示。一般来说，滤波器因此将由一个三维数组表示，具有宽度、高度和深度，有时被称为“体积”。在前面的示例中，由于`mnist`图像仅为灰度，因此滤波器的深度为1。因此，深度为*d*的通用滤波器由具有相同宽度和高度的*d*个滤波器组成。这些*d*个滤波器中的每一个称为“切片”或“叶子”：
- en: '![Convolutional layers](img/00208.jpeg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00208.jpeg)'
- en: 'Similarly, as before, for each "leaf" or "slice", we connect each neuron in
    the small sub-region, as well as a bias, to a neuron and we calculate its activation
    value defined by the connection weights set in the filter, and we slide the filter
    across the whole area. Such a procedure, as it is easy to calculate, requires
    a number of parameters that are equal to the number of weights defined by the
    filter (in our example above, this would be 3 x 3=9), multiplied by the number
    of "leaves", that is, the depth of the layer, plus one bias. This defines a feature
    map, because it highlights specific features of the input. In our code above we
    defined two feature maps, a "blur" and an "edges". Therefore, we need to multiply
    the number of parameters by the number of feature maps. Note that the weights
    for each filter are fixed; when we slide the filter across the region we do not
    change weights. Therefore, if we start with a layer with size (width, height,
    depth), and a filter of dimension `(filter_w, filter_h)`, the output layer after
    having applied the convolution is `(width - filter_w + 1, height – filter_h +
    1)`. The depth of the new layer depends on how many feature maps we want to create.
    In our `mnist` code example earlier, if we applied both the `blur` and `edges`
    filters, we would have an input layer of size (28 x 28 x 1), since there is only
    one channel because the digits are gray-scale images, and an output layer of dimension
    (26 x 26 x 2), since our filters had dimension (3 x 3) and we used two filters.
    The number of parameters is only 18 (3 x 3 x 2), or 20 (3 x 3 x 2+2) if we add
    a bias. This is way less than what we would need to have with classical feed-forward
    networks, whereas, since the input is 784 pixels, a simple hidden layer with just
    50 neurons would need 784 x 50 = 39200 parameters, or 39250 if we add the bias:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，和以前一样，对于每个“叶片”或“片段”，我们连接小的子区域中的每个神经元以及一个偏置到一个神经元，并计算其激活值，其由滤波器中设置的连接权重定义，并滑动滤波器跨整个区域。这样的过程，因为它容易计算，所以需要的参数数量等于滤波器定义的权重数（在我们上面的示例中，这将是3
    x 3 = 9），乘以“叶片”的数量，也就是层的深度，再加上一个偏置。这定义了一个特征图，因为它突出显示了输入的特定特征。在我们上面的代码中，我们定义了两个特征图，一个“模糊”和一个“边缘”。因此，我们需要将参数的数量乘以特征图的数量。请注意，每个滤波器的权重是固定的；当我们滑动滤波器跨区域时，我们不会改变权重。因此，如果我们从尺寸为（宽度，高度，深度）的层开始，以及一个维度为`(filter_w，filter_h)`的滤波器，那么应用卷积后的输出层是`(width
    - filter_w + 1，height - filter_h + 1)`。新层的深度取决于我们想要创建多少特征图。在我们之前的`mnist`代码示例中，如果我们同时应用了`模糊`和`边缘`滤波器，我们将拥有一个尺寸为（28
    x 28 x 1）的输入层，因为只有一个通道，因为数字是灰度图像，并且一个尺寸为（26 x 26 x 2）的输出层，因为我们的滤波器尺寸为（3 x 3），我们使用了两个滤波器。参数的数量仅为18（3
    x 3 x 2），如果我们添加一个偏置，则为20（3 x 3 x 2 + 2）。这比我们在传统的前馈网络中所需的要少得多，因为由于输入是784像素，一个只有50个神经元的简单隐藏层将需要784
    x 50 = 39200个参数，如果我们添加偏置，则为39250个：
- en: '![Convolutional layers](img/00209.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层](img/00209.jpeg)'
- en: We slide the filter across the image over all the "leaves" comprising the layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将滤波器沿着包含在层中的所有“叶片”滑过图像。
- en: Convolutional layers moreover can work better, since each neuron gets its input
    only from neighboring neurons, and does not care about collecting input from neurons
    that are distant from each other.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，卷积层可以更好地工作，因为每个神经元仅从相邻的神经元获得其输入，并且不关心从彼此相距较远的神经元收集输入的情况。
- en: Stride and padding in convolutional layers
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层中的步幅和填充
- en: 'The examples we have shown, aided by pictures, in fact only tell one particular
    application of filters (as we mentioned earlier, there are different types of
    convolutions, depending on the parameters chosen). In fact, the size of the filter
    may vary, as well as how it moves across the image and its behavior at the edges
    of the image. In our example, we moved the filter across the image 1 pixel at
    a time. How many pixels (neurons) we skip each time we move our filter is called
    the stride. In the above example, we used a stride of 1, but it is not unusual
    to use larger strides, of 2 or even more. In this case the output layer would
    have a smaller width and height:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所展示的示例，辅以图片，实际上只讲述了滤波器的一个特定应用（正如我们之前提到的，根据所选参数，有不同类型的卷积）。实际上，滤波器的大小可能会有所不同，以及它在图像上的移动方式以及在图像边缘的行为。在我们的示例中，我们每次将滤波器沿图像移动1个像素。我们每次移动滤波器时跳过多少像素（神经元）称为步幅。在上面的示例中，我们使用了步幅为1，但使用较大的步幅，如2甚至更大，也并不罕见。在这种情况下，输出层的宽度和高度将较小：
- en: '![Stride and padding in convolutional layers](img/00210.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层中的步幅和填充](img/00210.jpeg)'
- en: A filter applied with stride 2—the filter is moved by two pixels at a time.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用步长为2的滤波器应用——滤波器每次移动两个像素。
- en: 'In addition, we might also decide to apply the filter partially outside of
    the original picture. In that case, we would assume that the missing neurons would
    have value 0\. This is called padding; that is, we add 0 value neurons outside
    the original image. This can be useful if, for example, we want the output image
    to be the same size as the input image. Above, we wrote the formula for the size
    of the new output image in case of zero padding, and that was (`width - filter_w
    + 1, height – filter_h + 1`) for an input of size (`width, height`) and a filter
    of dimensions (`filter_w, filter_h`). If we use a padding `P` all around the image,
    the output size will be (`width + 2P - filter_w + 1, height + 2P – filter_h +
    1`). To summarize, in each dimension (either width or height), let the size of
    the input slice be called *I=(I[w]*, *I[h])*, the size of the filter *F=(F[w]*,*F[h])*,
    the size of the stride *S=(S[w]*,*S[h])*, and the size of the padding *P=(P[w]*,P[h],
    then the size *O=(O[w]*, O[h] for the output slice is given by:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可能也决定部分地在原始图片外应用滤镜。在这种情况下，我们会假设缺失的神经元值为0。这就是所谓的填充；也就是，在原始图像外部添加值为0的神经元。如果我们想要输出图像与输入图像大小相同的话，这可能会很有用。在上面，我们写出了零填充情况下新输出图像大小的公式，即(`width
    - filter_w + 1, height – filter_h + 1`)，对应输入大小为(`width, height`)和滤波器尺寸为(`filter_w,
    filter_h`)。如果我们在图像的四周使用填充`P`，输出大小将为(`width + 2P - filter_w + 1, height + 2P –
    filter_h + 1`)。总结一下，在每个维度上（无论是宽度还是高度），让输入切片的大小称为*I=(I[w]*(I[h])*, 滤波器的大小为*F=(F[w],F[h])*,
    步长的大小为*S=(S[w],S[h])*, 和填充的大小为*P=(P[w],P[h])，那么输出切片的大小*O=(O[w], O[h])就由下式给出：
- en: '![Stride and padding in convolutional layers](img/00211.jpeg)![Stride and padding
    in convolutional layers](img/00212.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层中的步长和填充](img/00211.jpeg)![卷积层中的步长和填充](img/00212.jpeg)'
- en: This of course identifies one of the constraints for *S*, that it must divide
    *(I + 2P – F)* both in the width direction and the height direction. The dimension
    for the final volume is obtained by multiplying for the number of desired feature
    maps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这也确定了*S*的约束之一，即它必须在宽度方向和高度方向上都能整除*(I + 2P – F)*。最终体积的尺寸通过乘以所需的特征映射数得到。
- en: 'The number of parameters *W* used, instead, is independent of the stride and
    padding, and it is just a function of the (square) size of the filter, the depth
    *D* (number of slices) of the input, and the number of feature maps *M* chosen:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，使用的参数数目*W*与步长和填充无关，仅仅是滤波器大小的函数，输入的深度*D*（切片数量），以及选定的特征映射数量*M*：
- en: '![Stride and padding in convolutional layers](img/00213.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![卷积层中的步长和填充](img/00213.jpeg)'
- en: The use of padding (also called zero-padding, as we are padding the image with
    zeros) is sometimes useful if we are seeking to make the output dimension the
    same as the input dimension. If we use a filter of dimension (2 x 2), it is in
    fact clear that by applying a padding of value 1 and a stride of 1, we have the
    dimension of the output slice the same as the size of the input slice.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用填充（也称为零填充，因为我们用零填充图像）有时很有用，如果我们希望输出维度与输入维度相同的话。如果我们使用一个大小为(2 x 2)的滤波器，实际上可以清楚地看到通过应用值为1的填充和步长为1，输出切片的尺寸与输入切片的大小相同。
- en: Pooling layers
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: In the previous section, we have derived the formula for the size for each slice
    in a convolutional layer. As we discussed, one of the advantages of convolutional
    layers is that they reduce the number of parameters needed, improving performance
    and reducing over-fitting. After a convolutional operation, another operation
    is often performed—pooling. The most classical example is called max-pooling,
    and this means creating (2 x 2) grids on each slice, and picking the neuron with
    the maximum activation value in each grid, discarding the rest. It is immediate
    that such an operation discards 75% of the neurons, keeping only the neurons that
    contribute the most in each cell.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们已经推导出了卷积层中每个切片大小的公式。正如我们讨论过的那样，卷积层的优势之一是它减少了所需的参数数量，提升了性能，减少了过拟合。在执行卷积操作后，通常会执行另一个操作——池化。最经典的例子就是最大池化，这意味着在每个切片上创建(2
    x 2)的网格，并在每个网格中选择具有最大激活值的神经元，丢弃其他的。很明显，这样的操作会丢弃75%的神经元，仅保留在每个单元格中贡献最多的神经元。
- en: 'There are two parameters for each pooling layer, similar to the stride and
    padding parameters found in convolutional layers, and they are the size of the
    cell and the stride. One typical choice is to choose a cell size of 2 and a stride
    of 2, though it is not uncommon to pick a cell size of 3 and a stride of 2, creating
    some overlap. It should be noted, however, that if the cell size is too large,
    the pooling layer may be discarding too much information and is not helpful. We
    can derive a formula for the output of a pooling layer, similar to the one we
    derived for convolutional layers. Let''s call, like before, *I* the size of the
    input slice, *F* the size of the cell (also called the receptive field), *S* the
    size of the stride, and *O* the size of the output. Pooling layers typically do
    not use any padding. Then we obtain in each dimension:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个汇集层来说有两个参数，类似于卷积层中的步幅和填充参数，它们是单元大小和步幅。一个典型的选择是选择单元大小为2，步幅为2，不过选择单元大小为3，步幅为2，创建一些重叠也不少见。然而需要注意的是，如果单元大小太大，汇集层可能会丢弃太多信息，这对于帮助并不利。我们可以推导出与我们推导卷积层的公式类似的汇集层输出的公式。\\
- en: '![Pooling layers](img/00214.jpeg)![Pooling layers](img/00215.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![汇集层](img/00214.jpeg)![汇集层](img/00215.jpeg)'
- en: Pooling layers do not change the depth of the volume of the layer, keeping the
    same number of slices, since the pooling operation is performed in each slice
    independently.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 汇集层不会改变层的体积深度，保持相同数量的片，因为汇集操作是在每个片中独立地进行。
- en: It should also be noted that, similar to how we can use different activation
    functions, we can also use different pooling operations. Taking the max is one
    of the most common operations, but it is not uncommon to take the average of all
    the values, or even an *L ²* measure, which is the square root of the sum of all
    the squares. In practice, max-pooling often performs better, since it retains
    the most relevant structures in the image.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，类似于我们可以使用不同的激活函数一样，我们也可以使用不同的汇集操作。取最大值是最常见的操作之一，不过取所有值的平均值或者*L ²*度量也并不少见，这是所有平方的平方根。在实践中，最大汇聚通常表现更好，因为它保留了图像中最相关的结构。
- en: 'It should be noted, however, that while pooling layers are still very much
    used, one can sometimes achieve similar or better results by simply using convolutional
    layers with larger strides instead of pooling layers (see, for example, J. Springerberg,
    A. Dosovitskiy, T. Brox, and M. Riedmiller, *Striving for Simplicity: The All
    Convolutional Net*, (2015), [https://arxiv.org/pdf/1412.6806.pdf](https://arxiv.org/pdf/1412.6806.pdf)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而需要注意的是，虽然汇集层仍然被广泛使用，有时候只需使用步幅较大的卷积层而不是汇集层，就能达到类似或更好的结果（例如，见J. Springerberg,
    A. Dosovitskiy, T. Brox, 和 M. Riedmiller，*追求简洁：全卷积网络*，(2015)，[https://arxiv.org/pdf/1412.6806.pdf](https://arxiv.org/pdf/1412.6806.pdf)）。
- en: However, if pooling layers are used, they are generally used in the middle of
    a sequence of a few convolutional layers, generally after every other convolutional
    operation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果使用汇集层，它们通常被用于在几个卷积层中间，通常是在每隔一个卷积操作之后。
- en: 'It is also important to note that pooling layers add no new parameters, since
    they are simply extracting values (like the max) without needing additional weights
    or biases:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，汇集层不会增加新的参数，因为它们只是提取值（如最大值）而不需要额外的权重或偏置：
- en: '![Pooling layers](img/00216.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![汇集层](img/00216.jpeg)'
- en: 'An example of a max-pool layer: the maximum from each 2x2 cell is calculated
    to generate a new layer.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最大汇聚层的例子：计算每个2x2单元的最大值以生成一个新层。
- en: Dropout
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 丢弃
- en: 'Another important technique that can be applied after a pooling layer, but
    can also generally be applied to a fully connected layer, is to "drop" some neurons
    and their corresponding input and output connections randomly and periodically.
    In a dropout layer we specify a probability *p* for neurons to "drop out" stochastically.
    During each training period, each neuron has probability *p* to be dropped out
    from the network, and a probability *(1-p)* to be kept. This is to ensure that
    no neuron ends up relying too much on other neurons, and each neuron "learns"
    something useful for the network. This has two advantages: it speeds up the training,
    since we train a smaller network each time, and also helps in preventing over-fitting
    (see N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    *Dropout: A Simple Way to Prevent Neural Networks from Overfitting,* in *Journal
    of Machine Learning Research* 15 (2014), 1929-1958, [http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个重要的技术是可以在池化层之后应用的，但也通常可以应用于全连接层的技术是随机定期“丢弃”一些神经元及其相应的输入和输出连接。在一个丢弃层中，我们为神经元指定了一个概率*p*以随机方式“丢弃”。在每个训练周期中，每个神经元都有概率*p*被从网络中丢弃，概率*(1-p)*被保留。这是为了确保没有神经元过多地依赖其他神经元，并且每个神经元都“学到”了对网络有用的东西。这有两个优点：它加快了训练，因为我们每次训练一个较小的网络，还有助于防止过拟合（参见N.
    Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov的*Dropout:
    A Simple Way to Prevent Neural Networks from Overfitting*，刊登于*机器学习研究杂志*15 (2014),
    1929-1958, [http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf)）。'
- en: It is however, important to note that dropout layers are not strictly restricted
    to convolutional layers; in fact dropout layers find applications in different
    neural network architectures. Dropout layers should be regarded as a regularization
    technique for reducing overfitting, and we mention them since they will be explicitly
    used in our code example.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，重要的是要注意，丢弃层不仅仅限于卷积层；事实上，丢弃层在不同的神经网络架构中都有应用。丢弃层应被视为减少过拟合的正则化技术，我们提到它们是因为它们将在我们的代码示例中被明确使用。
- en: Convolutional layers in deep learning
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习中的卷积层
- en: When we introduced the idea of deep learning, we discussed how the word "deep"
    refers not only to the fact that we use many layers in our neural net, but also
    to the fact that we have a "deeper" learning process. Part of this deeper learning
    process was the ability of the neural net to learn features autonomously. In the
    previous section, we defined specific filters to help the network learn specific
    characteristics. This is not necessarily what we want. As we discussed, the point
    of deep learning is that the system learns on its own, and if we had to teach
    the network what features or characteristics are important, or how to learn to
    recognize digits by applying layers such as the *edges* layer that highlights
    the general shape of a digit, we would be doing most of the work and possibly
    constraining the network to learn features that may be relevant to us but not
    to the network, degrading its performance. The point of Deep Learning is that
    the system needs to learn by itself.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们介绍深度学习的概念时，我们讨论了“深度”一词不仅指的是我们在神经网络中使用了许多层，还指的是我们有一个“更深入”的学习过程。这种更深入的学习过程的一部分是神经网络自主学习特征的能力。在前一节中，我们定义了特定的滤波器来帮助网络学习特定的特征。这并不一定是我们想要的。正如我们讨论过的，深度学习的重点在于系统能够自主学习，如果我们不得不教会网络哪些特征或特性是重要的，或者如何通过应用*边缘*层来学习识别数字的形状，我们将会做大部分的工作，并可能限制网络学习可能对我们有用但对网络本身并不重要的特征，从而降低其性能。深度学习的重点在于系统必须自行学习。
- en: In the [Chapter 2](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 2. Neural Networks"), *Neural Networks*, we showed how the hidden layers
    in a neural net learn the weights by using back-propagation; the weights were
    not set by the operator. Similarly, it makes no sense for the operator to set
    the weights in the filters, rather we want the neural net to learn the weights
    in the filters, again by using back-propagation. All the operator needs to do
    is to set the size of the layer, the stride, and the padding, and decide how many
    feature maps we are asking the network to learn. By using supervised learning
    and back-propagation, the neural net will set the weights (and biases) for each
    filter autonomously.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0019_split_000.html#I3QM1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 2. Neural
    Networks") *神经网络*中，我们展示了神经网络中的隐藏层如何通过使用反向传播学习权重； 操作员没有设置权重。 同样，操作员设置滤波器中的权重是毫无意义的，我们希望神经网络通过使用反向传播再次学习滤波器中的权重。
    操作员唯一需要做的是设置图层的大小、步长和填充，并决定我们要求网络学习多少个特征图。 通过使用监督学习和反向传播，神经网络将自主设置每个滤波器的权重（和偏差）。
- en: We should also mention that, while it may be simpler to use the description
    of convolutional layers we have provided, convolutional layers could still be
    thought of as the regular fully connected layers we introduced in [Chapter 3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 3. Deep Learning Fundamentals"), *Deep Learning Fundamentals*. In fact,
    the two main characteristics of convolutional layers are the fact that each neuron
    only connects to a small region of the input layer, and the fact that different
    slices corresponding to the same small region share the same weights. These two
    properties can be rendered with a regular layer by creating a matrix of weights
    which is sparse, that is, with many zeros (due to the local connectivity of the
    convolutional network) and which has many weights repeated (due to the parameter
    sharing properties across slices). Understanding this point makes it clear why
    convolutional layers have much fewer parameters than fully connected layers; in
    convolutional layers the matrix of weights is comprised mostly of zeros entries.
    In practice, however, it helps the intuition to think of convolutional layers
    as they have been described in this chapter, since one can better appreciate how
    convolutional layers can highlight features of the original image, as we have
    shown graphically by blurring the image or highlighting the contours of the digits
    in our examples.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要提及的是，虽然使用我们提供的卷积层描述可能更简单，但卷积层仍然可以被认为是我们在[第3章](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 3. Deep Learning Fundamentals") *深度学习基础*中介绍的普通全连接层。 实际上，卷积层的两个主要特征是每个神经元只连接到输入层的一个小区域，并且对应于相同小区域的不同切片共享相同的权重。
    这两个属性可以通过创建一个稀疏的权重矩阵来呈现在普通层中，即具有许多零（由于卷积网络的局部连接性）和许多重复权重（由于切片之间的参数共享特性）。 理解这一点清楚地说明了为什么卷积层的参数要比全连接层少得多；
    在卷积层中，权重矩阵主要由零条目组成。 然而，在实践中，将卷积层想象成本章节中描述的方式对直觉有所帮助，因为这样可以更好地欣赏卷积层如何突出显示原始图像的特征，正如我们通过模糊图像或突出我们示例中数字的轮廓来图形化展示的那样。
- en: One more important point to make is that convolutional networks should generally
    have a depth equal to a number which is iteratively divisible by 2, such as 32,
    64, 96, 128, and so on. This is important when using pooling layers, such as the
    max-pool layer, since the pooling layer (if it has size (2,2)) will divide the
    size of the input layer, similarly to how we should define "stride" and "padding"
    so that the output image will have integer dimensions. In addition, padding can
    be added to ensure that the output image size is the same as the input.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 再要明确的一点是，卷积网络的深度通常应该等于可以通过2进行迭代除法的数字，例如32，64，96，128等。 这在使用池化层时很重要，比如max-pool层，因为池化层（如果其大小为（2,2））将使输入层的大小除以2，类似于我们如何定义“步进”和“填充”，以使输出图像具有整数尺寸。
    另外，可以添加填充以确保输出图像大小与输入相同。
- en: Convolutional layers in Theano
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Theano中的卷积层
- en: Now that we have the intuition of how convolutional layers work, we are going
    to implement a simple example of a convolutional layer using Theano.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道卷积层是如何工作的，我们将使用Theano实现一个卷积层的简单示例。
- en: 'Let us start by importing the modules that are needed:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入所需的模块：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Theano works by first creating a symbolic representation of the operations we
    define. We will later have another example using Keras, that, while it provides
    a nice interface to make creating neural networks easier, it lacks some of the
    flexibility one can have by using Theano (or TensorFlow) directly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Theano首先创建我们定义的操作的符号表示。我们稍后将通过另一个使用 Keras 的例子，它提供了一个很好的接口来更轻松地创建神经网络，但是使用 Theano（或者
    TensorFlow）直接使用时可能缺少一些灵活性。
- en: 'We define the variables needed and the neural network operations, by defining
    the number of feature maps (the depth of the convolutional layer) and the size
    of the filter, then we symbolically define the input using the Theano tensor class.
    Theano treats the image channels as a separate dimension, so we define the input
    as a tensor4\. Next we initialize the weights using a random distribution between
    -0.2 and 0.2\. We are now ready to call the Theano convolution operation and then
    apply the logistic sigmoid function on the output. Finally, we define the function
    `f` that takes an input and defines an output using the operations used:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过定义所需的变量和神经网络操作来定义特征图的数量（卷积层的深度）和滤波器的大小，然后我们使用 Theano 张量类来符号化地定义输入。Theano把图像通道视为一个单独的维度，所以我们把输入定义为
    tensor4。接下来，我们使用-0.2和0.2之间的随机分布来初始化权重。我们现在可以调用 Theano 卷积操作，然后在输出上应用逻辑 sigmoid
    函数。最后，我们定义函数`f`，它接受一个输入，并使用所使用的操作来定义一个输出：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `skimage` module we imported can be used to load an image, we will import
    an image called `lena` *,* then after having reshaped the image to be passed in
    to the Theano function we defined, we can call the Theano function on it:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入的`skimage`模块可以用来加载一个名为`lena`的图像，然后在将图像重塑为可传递给我们定义的 Theano 函数后，我们就可以在该图像上调用
    Theano 函数：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is it. We can now print out the original image and the filtered images
    by using this simple code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们现在可以通过这段简单的代码打印出原始图片和经过滤波的图片。
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If the reader were interested in visualizing the weights used, in Theano, it
    is possible to print out the values by using `print W.get_value()`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果读者对可视化所使用的权重感兴趣，在 Theano 中，可以使用`print W.get_value()`来打印值。
- en: 'The output from this code is as follows: (since we have not fixed a random
    seed, and since the weights are initialized randomly, the reader may get slightly
    different images):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出如下：（由于我们还没有固定随机种子，并且权重是随机初始化的，读者可能会得到略有不同的图像）：
- en: '![Convolutional layers in Theano](img/00217.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![Theano中的卷积层](img/00217.jpeg)'
- en: The original and filtered images.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图片和滤波后的图片。
- en: A convolutional layer example with Keras to recognize digits
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个使用Keras识别数字的卷积层示例
- en: In the third chapter, we introduced a simple neural network to classify digits
    using Keras and we got 94%. In this chapter, we will work to improve that value
    above 99% using convolutional networks. Actual values may vary slightly due to
    variability in initialization.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们介绍了使用 Keras 对数字进行分类的简单神经网络，我们得到了94%的准确率。在本章中，我们将努力使用卷积网络将该准确率提高到99%以上。由于初始化的变化，实际值可能会略有不同。
- en: 'First of all, we can start by improving the neural network we had defined by
    using 400 hidden neurons and run it for 30 epochs; that should get us already
    up to around 96.5% accuracy:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过使用 400 个隐藏神经元来改进我们之前定义的神经网络，并将其运行 30 个周期；这样就应该已经将准确率提高到了大约 96.5%：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next we could try scaling the input. Images are comprised of pixels, and each
    pixel has an integer value between 0 and 255\. We could make that value a float
    and scale it between 0 and 1 by adding these four lines of code right after we
    define our input:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以尝试对输入进行缩放。图像由像素组成，每个像素的整数值在 0 到 255 之间。我们可以使该值成为浮点数，并将其在 0 到 1 之间缩放，只需在定义输入后添加这四行代码即可：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If we run our network now, we get a poorer accuracy, just above 92%, but we
    need not worry. By rescaling, we have in fact changed the values of the gradient
    of our function, which therefore will converge much more slowly, but there is
    an easy work-around. In our code, inside the `model.compile` function, we defined
    an optimizer equal to "sgd". That is the standard stochastic gradient descent,
    which uses the gradient to converge to a minimum. However, Keras allows other
    choices, in particular "adadelta", which automatically uses momentum and adjusts
    the learning rate depending on the gradient, making it larger or smaller in an
    inversely proportional way to the gradient, so that the network does not learn
    too slowly and it does not skip minima by taking too large a step. By using adadelta,
    we dynamically adjust the parameters with time (see also: Matthew D. Zeiler, *Adadelta:
    An Adaptive Learning Rate Method*, arXiv:1212.5701v1 ([https://arxiv.org/pdf/1212.5701v1.pdf](https://arxiv.org/pdf/1212.5701v1.pdf))).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在运行我们的网络，我们得到的准确率较低，略高于92%，但我们不需要担心。通过重新缩放，我们实际上改变了我们函数的梯度值，因此它将收敛得更慢，但有一个简单的解决方法。在我们的代码中，在`model.compile`函数内，我们定义了一个优化器等于"sgd"。这是标准的随机梯度下降，它使用梯度收敛到最小值。然而，Keras允许其他选择，特别是"adadelta"，它自动使用动量，并根据梯度调整学习率，使其与梯度成反比地变大或变小，以便网络不会学习得太慢，也不会通过采取太大的步骤跳过最小值。通过使用adadelta，我们动态调整参数随时间改变（也见：Matthew
    D. Zeiler，*Adadelta：一种自适应学习率方法*，arXiv:1212.5701v1 ([https://arxiv.org/pdf/1212.5701v1.pdf](https://arxiv.org/pdf/1212.5701v1.pdf))）。
- en: 'Inside the main function, we are now going to change our compile function and
    use:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在主函数内部，我们现在将改变我们的编译函数并使用：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If we run our algorithm again, we are now at about 98.25% accuracy. Finally,
    let''s modify our first dense (fully connected) layer and use the `relu` activation
    function instead of the `sigmoid`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次运行我们的算法，现在我们的准确率约为98.25%。最后，让我们修改我们的第一个密集（全连接）层，使用`relu`激活函数而不是`sigmoid`：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This will now give around 98.4% accuracy. The problem is that now it becomes
    increasingly difficult to improve our results using a classical feed-forward architecture,
    due to over-fitting, and increasing the number of epochs or modifying the number
    of hidden neurons will not bring any added benefit, as the network will simply
    learn to over-fit the data, rather than learn to generalize better. We are therefore
    now going to introduce convolutional networks in the example.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将带来大约98.4%的准确率。问题在于，现在使用传统的前馈架构变得越来越难以改善我们的结果，由于过拟合，增加迭代次数或修改隐藏神经元的数量将带来任何额外的好处，因为网络将简单地学会对数据进行过度拟合，而不是学会更好地泛化。因此，我们现在将在示例中引入卷积网络。
- en: 'To do this, we keep our input scaled between 0 and 1\. However, we reshape
    the data to a volume of size (28, 28, 1) = (width of image, height of image, number
    of channels) in order to be used by a convolutional layer, and we bring the number
    of hidden neurons down to 200, but we now add a simple convolutional layer at
    the beginning, with a 3 x 3 filter, no padding, and stride 1, followed by a max-pooling
    layer of stride 2 and size 2\. In order to be able to then pass the output to
    the dense layer, we need to flatten the volume (convolutional layers are volumes)
    to pass it to the regular dense layer with 100 hidden neurons by using the following
    code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们保持我们的输入值在0和1之间。然而，为了被卷积层使用，我们将数据重塑成大小为（28，28，1）的体积=（图像宽度，图像高度，通道数），并将隐藏神经元的数量减少到200个，但现在我们在开始处添加了一个简单的卷积层，使用3
    x 3的滤波器，不填充，步长为1，然后是一个步幅为2且大小为2的最大池化层。为了将输出传递给密集层，我们需要将体积（卷积层是体积）拉直以传递给具有100个隐藏神经元的常规密集层，使用以下代码：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also reduce the number of epochs down to just 8, and we will get an
    accuracy of around 98.55%. Often it is common to use pairs of convolutional layers,
    so we add a second one similar to the first one, (before the pooling layer):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将迭代次数减少到8次，然后我们将得到大约98.55%的准确率。通常情况下，常用成对的卷积层，所以我们添加了一个类似第一个卷积层的第二个卷积层（在池化层之前）：
- en: '[PRE17]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: And we will now be at 98.9%.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的准确率已经达到了98.9%。
- en: 'In order to get to 99%, we add a dropout layer as we have discussed. This does
    not add any new parameters, but helps prevent overfitting, and we add it right
    before the flatten layer:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到99%，我们按照我们所讨论的方法添加一个辍学层。这不会增加任何新的参数，但能帮助防止过拟合，并且我们将其添加在拉直层之前：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example we use a dropout rate of about 25%, so each neuron is randomly
    dropped once every four times.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了约25%的辍学率，因此每个神经元每四次就会被随机抛弃一次。
- en: 'This will take us above 99%. If we want to improve more (accuracy may vary
    due to differences in initializations), we can also add more dropout layers, for
    example, after the hidden layer and increase the number of epochs. This would
    force the neurons on the final dense layer, prone to overfit, to be dropped randomly.
    Our final code looks like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们的准确度达到99%以上。如果我们想进一步提高（准确度可能因初始化的差异而有所不同），我们还可以添加更多的dropout层，例如在隐藏层之后，并增加时期的数量。这将迫使最终密集层中容易过拟的神经元被随机丢弃。我们的最终代码如下：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It is possible to further optimize this network, but the point here is not to
    get an award-winning score, but to understand the process, and understand how
    each step we have taken has improved performance. It is also important to understand
    that by using the convolutional layer, we have in fact also avoided overfitting
    our network, by utilizing fewer parameters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络可以进一步优化，但这里的重点不是获得一个获奖得分，而是理解过程，并了解我们采取的每一步是如何提高性能的。还要注意，通过使用卷积层，我们实际上也避免了网络的过拟合问题，因为利用了更少的参数。
- en: A convolutional layer example with Keras for cifar10
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras进行cifar10的卷积层示例
- en: 'We can now try to use the same network on the `cifar10` dataset. In [Chapter
    3](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f "Chapter 3. Deep
    Learning Fundamentals"), *Deep Learning Fundamentals*, we were getting a low 50%
    accuracy on test data, and to test the new network we have just used for the `mnist`
    dataset, we need to just make a couple of small changes to our code: we need to
    load the `cifar10` dataset (without doing any re-shaping, those lines will be
    deleted):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试在`cifar10`数据集上使用相同的网络。在[第3章](part0022_split_000.html#KVCC2-c1ed1b54ca0b4e9fbb9fe2b2431d634f)中的*深度学习基础知识*中，我们在测试数据上得到了50%的低准确度，为了测试刚刚在`mnist`数据集上使用的新网络，我们只需要对代码进行一些小的修改：我们需要加载`cifar10`数据集（不进行任何重新调整，那些行将被删除）：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And then change the input values for the first convolutional layer:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 改变第一个卷积层的输入值：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Running this network for 5 epochs will give us around 60% accuracy (up from
    about 50%) and 66% accuracy after 10 epochs, but then the network starts to overfit
    and stops improving performance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个网络5个时期将给我们约60%的准确度（从约50%提高）和10个时期后的66%的准确度，但接着网络开始过拟合并停止改善性能。
- en: 'Of course the `cifar10` images have 32 x 32 x 3 = 3072 pixels, instead of 28
    x 28=784 pixels, so we may need to add a couple more convolutional layers, after
    the first two:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，`cifar10`的图像有32 x 32 x 3 = 3072个像素，而不是28 x 28 = 784个像素，所以在前两层之后，我们可能需要添加几个额外的卷积层：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In general, it is better to split large convolutional layers into smaller-sized
    convolutional layers. For example, if we have two consecutive (3 x 3) convolutional
    layers, the first layer will have a (3 x 3) view of the input image, and the second
    layer will have a (5 x 5) view of the input image for each pixel. However, each
    layer will have non-linear features that will stack up, creating more complex
    and interesting features of the input than we would get by simply creating a single
    (5 x 5) filter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，最好将大型卷积层划分为较小尺寸的卷积层。例如，如果我们有两个连续的（3 x 3）卷积层，第一层将具有对输入图像的（3 x 3）视图，第二层将为每个像素提供对输入图像的（5
    x 5）视图。然而，每层都会具有非线性特征，这些特征将堆叠起来，创建出比仅仅创建单个（5 x 5）滤波器时更复杂和有趣的输入特征。
- en: 'If we run this network for 3 epochs, we are also getting around 60%, but after
    20 epochs we are up to 75% accuracy by using a simple network. The state-of-the-art
    convolutional networks can get around 90% accuracy, but require longer training
    and are more complicated. We will graphically present the architecture of one
    important convolutional neural network, called VGG-16, in the next paragraph so
    that the user can try to implement it using Keras or any other language he or
    she is comfortable with, such as Theano or TensorFlow (the network was originally
    created using Caffe, an important deep learning framework developed at Berkeley,
    see: [http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org)).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个网络运行3个时期，我们的准确度也在60%左右，但是经过20个时期后，通过使用简单的网络，我们的准确度达到了75%。先进的卷积网络可以达到90%的准确度，但需要更长的训练时间，并且更加复杂。我们将以图形方式展示一个重要的卷积神经网络的架构，称为VGG-16，在下一段中，用户可以尝试使用Keras或其他他们熟悉的语言来实现它，例如Theano或TensorFlow（该网络最初是使用Caffe创建的，Caffe是在伯克利开发的一个重要的深度学习框架，详情请见：[http://caffe.berkeleyvision.org](http://caffe.berkeleyvision.org)）。
- en: 'When working with neural networks, it is important to be able to "see" the
    weights the network has learned. This allows the user to understand what features
    the network is learning and to allow for better tuning. This simple code will
    output all the weights for each layer:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经网络时，能够“看到”网络学习的权重是很重要的。这使用户能够了解网络正在学习什么特征，并且能够进行更好的调整。这个简单的代码将输出每个层的所有权重：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If, for example, we are interested in the weights for layer 0, the first convolutional
    layer, we can apply them to the image to see what features the network is highlighting.
    If we apply these filters to the image `lena`, we get:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们对第 0 层，即第一个卷积层的权重感兴趣，我们可以将它们应用于图像，以查看网络正在突出显示的特征。如果我们将这些滤波器应用于图像`lena`，我们会得到：
- en: '![A convolutional layer example with Keras for cifar10](img/00218.jpeg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![一个使用 Keras 对 cifar10 进行卷积层示例](img/00218.jpeg)'
- en: We can see how each filter is highlighting different features.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个滤波器如何突出显示不同的特征。
- en: Pre-training
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练
- en: As we have seen, neural networks, and convolutional networks in particular,
    work by tuning the weights of the network as if they were coefficients of a large
    equation in order to get the correct output given a specific input. The tuning
    happens through back-propagation to move the weights towards the best solution
    given the chosen neural net architecture. One of the problems is therefore finding
    the best initialization values for the weights in the neural network. Libraries
    such as Keras can automatically take care of that. However, this topic is important
    enough to be worth discussing this point.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，神经网络，特别是卷积网络，通过调整网络的权重，就像它们是一个大型方程的系数一样来获得给定特定输入的正确输出。调整通过反向传播来移动权重，以使它们朝着给定选择的神经网络架构的最佳解决方案移动。因此，其中一个问题是找到神经网络中权重的最佳初始化值。诸如
    Keras 的库可以自动处理这个问题。然而，这个话题足够重要，值得讨论这一点。
- en: Restricted Boltzmann machines have been used to pre-train the network by using
    the input as the desired output to make the network automatically learn representations
    of the input and tune its weights accordingly, and this topic has already been
    discussed in [Chapter 4](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning"), *Unsupervised Feature Learning*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用输入作为期望输出来预先训练网络来使用受限玻尔兹曼机，使网络自动学习输入的表示并相应地调整其权重，这个话题已经在[第 4 章](part0027_split_000.html#PNV62-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 4. Unsupervised Feature Learning")中讨论过，*无监督特征学习*。
- en: In addition, there exists many pre-trained networks that offer good results.
    As we have mentioned, many people have been working on convolutional neural networks
    and have been getting impressive results, and one can often save time by reutilizing
    the weights learnt by these networks and applying them to other projects.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，存在许多预训练网络提供良好的结果。正如我们所提到的，许多人一直在研究卷积神经网络，并取得了令人印象深刻的结果，通过重新利用这些网络学到的权重并将它们应用于其他项目，通常可以节省时间。
- en: 'The VGG-16 model used in K. Simonyan, A. Zisserman, *Very Deep Convolutional
    Networks for Large-Scale Image Recognition* arXiv:1409.1556, [http://arxiv.org/pdf/1409.1556v6.pdf](http://arxiv.org/pdf/1409.1556v6.pdf),
    is an important model for image recognition. In this model, the input is a fixed
    224 x 224 RGB-valued image where the only pre-processing is subtracting the mean
    RGB-value computed on the training set. We outline the architecture for this network
    in the attached diagram, and the user can try to implement by himself or herself
    such a network, but also keep in mind the computationally intensive nature of
    running such a network. In this network the architecture is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: K. Simonyan, A. Zisserman在*Very Deep Convolutional Networks for Large-Scale
    Image Recognition*中使用的 VGG-16 模型，[http://arxiv.org/pdf/1409.1556v6.pdf](http://arxiv.org/pdf/1409.1556v6.pdf)，是图像识别中的重要模型。在这个模型中，输入是一个固定的
    224 x 224 的 RGB 图像，唯一的预处理是减去在训练集上计算的平均 RGB 值。我们在附图中概述了这个网络的架构，用户可以尝试自己实现这样一个网络，但也要注意运行这样一个网络的计算密集性。在这个网络中，架构如下：
- en: '![Pre-training](img/00219.jpeg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![预训练](img/00219.jpeg)'
- en: VGG-16 convolutional neural network architecture by Simonyan and Zisserman.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman 的 VGG-16 卷积神经网络架构。
- en: We also refer the interested reader to another noteworthy example, the AlexNet
    network, contained in Alex Krizhevsky, Ilya Sutskeve, Geoffrey Hinton, *ImageNet
    Classification with Deep Convolutional Networks*, in Advances in Neural Information
    Processing Systems 25 (NIPS 2012), [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf),
    that we will not be discussing this here for the sake of brevity, but we invite
    the interested reader to look at it. We also invite the interested reader to look
    at [https://github.com/fchollet/deep-learning-models](https://github.com/fchollet/deep-learning-models)
    for code examples of the VGG-16 and other networks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将感兴趣的读者引荐到另一个值得注意的例子，即AlexNet网络，包含在Alex Krizhevsky, Ilya Sutskeve, Geoffrey
    Hinton的*使用深度卷积神经网络进行ImageNet分类*中，出自于 Advances in Neural Information Processing
    Systems 25 (NIPS 2012)，[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)，但我们出于简洁起见，在此不讨论它，但我们邀请感兴趣的读者去查看。我们还邀请感兴趣的读者查看
    [https://github.com/fchollet/deep-learning-models](https://github.com/fchollet/deep-learning-models)
    获取VGG-16和其他网络的代码示例。
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: It should be noted, as it may have become clear, that there is no general architecture
    for a convolutional neural network. However, there are general guidelines. Normally,
    pooling layers follow convolutional layers, and often it is customary to stack
    two or more successive convolutional layers to detect more complex features, as
    it is done in the VGG-16 neural net example shown earlier. Convolutional networks
    are very powerful. However, they can be quite resource-heavy (the VGG-16 example
    above, for example, is relatively complex), and usually require a long training
    time, which is why the use of GPU can help speed up performance. Their strength
    comes from the fact that they do not focus on the entire image, rather they focus
    on smaller sub-regions to find interesting features that make up the image in
    order to be able to find discriminating elements between different inputs. Since
    convolutional layers are very resource-heavy, we have introduced pooling layers
    that help reduce the number of parameters without adding complexity, while the
    use of dropout layers helps insure that no neuron will rely too heavily on other
    neurons, and therefore each element in the neural net will contribute to learning.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，正如可能已经清楚的，卷积神经网络没有通用的架构。但是，有一些一般性的指导原则。通常，池化层跟在卷积层后面，并且经常习惯于堆叠两个或更多连续的卷积层来检测更复杂的特征，就像在前面展示的VGG-16神经网络示例中所做的那样。卷积网络非常强大。然而，它们可能非常耗费资源（例如上面的VGG-16示例相对复杂），通常需要长时间的训练，这就是为什么使用GPU可以帮助加速性能的原因。它们的优势在于它们不专注于整个图像，而是专注于较小的子区域，以找到组成图像的有趣特征，从而能够找到不同输入之间的区别元素。由于卷积层非常耗费资源，我们引入了池化层来帮助减少参数数量而不增加复杂性，而使用丢弃层有助于确保没有神经元过于依赖其他神经元，因此神经网络中的每个元素都会有助于学习。
- en: In this chapter, starting from drawing an analogy with how our visual cortex
    works, we have introduced convolutional layers and followed up with a descriptive
    intuition of why they work. We have introduced filters, we have also covered how
    filters can be of different sizes and can have different padding, and we have
    looked at how setting zero-padding can ensure that the resulting image has the
    same size as the original image. As mentioned above, pooling layers can help reduce
    the complexity, while dropout layers can make the neural network much more effective
    at recognizing patterns and features, and in particular can be quite effective
    at reducing the risk of over-fitting.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从类比我们的视觉皮层如何工作开始，介绍了卷积层，并随后描述了它们为何有效的直观理解。我们介绍了滤波器，还涵盖了滤波器可以有不同的大小和不同的填充方式，我们还看到了如何设置零填充可以确保结果图像与原始图像具有相同的大小。如上所述，池化层可以帮助减少复杂性，而丢弃层可以使神经网络在识别模式和特征方面更加有效，并且特别适用于减少过拟合的风险。
- en: In general, in the examples given, and in the `mnist` example in particular,
    we have shown how convolutional layers in neural networks can achieve much better
    accuracy than regular deep neural networks when dealing with images, reaching
    over 99% accuracy in digit recognition, without overfitting the model, by limiting
    the use of parameters. In the next chapters, we will look at speech recognition
    and then start looking at examples of models that use reinforcement learning,
    rather than supervised or unsupervised learning, by introducing deep learning
    for board games and deep learning for video games.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在给定的例子中，特别是在`mnist`的例子中，我们已经展示了神经网络中的卷积层在处理图像时，可以比普通的深度神经网络取得更好的准确性，在数字识别方面达到了超过99%的准确度，而且通过限制参数的使用，避免了模型过拟合的问题。在接下来的章节中，我们将会研究语音识别，然后开始研究使用强化学习而不是监督或非监督学习的模型的例子，介绍在棋盘游戏和视频游戏中使用深度学习的例子。
