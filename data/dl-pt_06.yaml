- en: Deep Learning with Sequence Data and Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列数据和文本的深度学习
- en: 'In the last chapter, we covered how to handle spatial data using **Convolution
    Neural Networks** (**CNNs**) and also built image classifiers. In this chapter,
    we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了如何使用**卷积神经网络**（**CNNs**）处理空间数据，并构建了图像分类器。在本章中，我们将涵盖以下主题：
- en: Different representations of text data that are useful for building deep learning
    models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于构建深度学习模型有用的文本数据的不同表示形式
- en: Understanding **recurrent neural networks** (**RNNs**) and different implementations
    of RNNs, such as **Long Short-Term Memory** (**LSTM**) and **Gated Recurrent Unit**
    (**GRU**), which power most of the deep learning models for text and sequential
    data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解**循环神经网络**（**RNNs**）及其不同实现，如**长短期记忆**（**LSTM**）和**门控循环单元**（**GRU**），它们支持大多数文本和序列数据的深度学习模型
- en: Using one-dimensional convolutions for sequential data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一维卷积处理序列数据
- en: 'Some of the applications that can be built using RNNs are:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用RNN构建的一些应用包括：
- en: '**Document classifiers**: Identifying the sentiment of a tweet or review, classifying
    news articles'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档分类器**：识别推文或评论的情感，分类新闻文章'
- en: '**Sequence-to-sequence learning**: For tasks such as language translations,
    converting English to French'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到序列学习**：用于任务如语言翻译，将英语转换为法语'
- en: '**Time-series forecasting**: Predicting the sales of a store given details
    about previous days'' store details'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列预测**：根据前几天的商店销售详情预测商店的销售情况'
- en: Working with text data
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: 'Text is one of the commonly used sequential data types. Text data can be seen
    as either a sequence of characters or a sequence of words. It is common to see
    text as a sequence of words for most problems. Deep learning sequential models
    such as RNN and its variants are able to learn important patterns from text data
    that can solve problems in areas such as:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 文本是常用的序列数据类型之一。文本数据可以看作是字符序列或单词序列。对于大多数问题，将文本视为单词序列是很常见的。深度学习序列模型如RNN及其变体能够从文本数据中学习重要模式，可以解决以下领域的问题：
- en: Natural language understanding
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: Document classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类
- en: Sentiment classification
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分类
- en: These sequential models also act as important building blocks for various systems,
    such as **question and answering** (**QA**) systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些序列模型也是各种系统的重要构建块，如**问答系统**（**QA**）。
- en: Though these models are highly useful in building these applications, they do
    not have an understanding of human language, due to its inherent complexities.
    These sequential models are able to successfully find useful patterns that are
    then used for performing different tasks. Applying deep learning to text is a
    fast-growing field, and a lot of new techniques arrive every month. We will cover
    the fundamental components that power most of the modern-day deep learning applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型在构建这些应用中非常有用，但由于其固有的复杂性，它们并不理解人类语言。这些序列模型能够成功地找到有用的模式，然后用于执行不同的任务。将深度学习应用于文本是一个快速发展的领域，每个月都有许多新技术问世。我们将介绍支持大多数现代深度学习应用的基本组件。
- en: 'Deep learning models, like any other machine learning model, do not understand
    text, so we need to convert text into numerical representation. The process of
    converting text into numerical representation is called **vectorization** and
    can be done in different ways, as outlined here:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型和其他机器学习模型一样，不理解文本，因此我们需要将文本转换为数值表示。将文本转换为数值表示的过程称为**向量化**，可以用不同的方法进行，如下所述：
- en: Convert text into words and represent each word as a vector
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为单词，并将每个单词表示为一个向量
- en: Convert text into characters and represent each character as a vector
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为字符，并将每个字符表示为一个向量
- en: Create *n*-gram of words and represent them as vectors
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建*n*-gram单词并将它们表示为向量
- en: 'Text data can be broken down into one of these representations. Each smaller
    unit of text is called a **token**, and the process of breaking text into tokens
    is called **tokenization**. There are a lot of powerful libraries available in
    Python that can help us in tokenization. Once we convert the text data into tokens,
    we then need to map each token to a vector. One-hot encoding and word embedding are
    the two most popular approaches for mapping tokens to vectors. The following diagram
    summarizes the steps for converting text into their vector representations:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据可以分解为这些表示之一。每个文本的较小单元称为**标记**，将文本分解为标记的过程称为**标记化**。Python 中有很多强大的库可以帮助我们进行标记化。一旦我们将文本数据转换为标记，我们就需要将每个标记映射到一个向量上。一热编码和词嵌入是将标记映射到向量的两种最流行的方法。以下图表总结了将文本转换为其向量表示的步骤：
- en: '![](img/aff887f1-cdb3-42fc-8d15-62760bd50005.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aff887f1-cdb3-42fc-8d15-62760bd50005.png)'
- en: Let's look in more detail at tokenization, *n*-gram representation, and vectorization.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看一下标记化、*n*-gram 表示和向量化。
- en: Tokenization
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记化
- en: Given a sentence, splitting it into either characters or words is called **tokenization**.
    There are libraries, such as spaCy, that offer complex solutions to tokenization.
    Let's use simple Python functions such as `split` and `list` to convert the text
    into tokens.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个句子，将其拆分为字符或单词称为**标记化**。有一些库，比如 spaCy，提供了复杂的标记化解决方案。让我们使用简单的 Python 函数，比如
    `split` 和 `list`，将文本转换为标记。
- en: 'To demonstrate how tokenization works on characters and words, let''s consider
    a small review of the movie *Thor: Ragnarok*. We will work with the following
    text:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示标记化在字符和单词上的工作原理，让我们考虑电影《雷神3：诸神黄昏》的简短评论。我们将使用以下文本：
- en: The action scenes were top notch in this movie. Thor has never been this epic
    in the MCU. He does some pretty epic sh*t in this movie and he is definitely not
    under-powered anymore. Thor in unleashed in this, I love that.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这部电影的动作场面非常出色。在 MCU 中，Thor 从未如此史诗般。他在这部电影中表现得相当史诗，绝对不再是无能为力。Thor 在这部电影中释放了自我，我喜欢这一点。
- en: Converting text into characters
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转换为字符
- en: 'The Python `list` function takes a string and converts it into a list of individual
    characters. This does the job of converting the text into characters. The following
    code block shows the code used and the results:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的 `list` 函数接受一个字符串并将其转换为单个字符的列表。这完成了将文本转换为字符的任务。以下代码块展示了使用的代码和结果：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The result is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This result shows how our simple Python function has converted text into tokens.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果展示了我们简单的 Python 函数如何将文本转换为标记。
- en: Converting text into words
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本转换为单词
- en: 'We will use the `split` function available in the Python string object to break
    the text into words. The `split` function takes an argument, based on which it
    splits the text into tokens. For our example, we will use spaces as the delimiters.
    The following code block demonstrates how we can convert text into words using
    the Python `split` function:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python 字符串对象中的 `split` 函数将文本分割成单词。`split` 函数接受一个参数，根据此参数将文本分割为标记。对于我们的示例，我们将使用空格作为分隔符。以下代码块演示了如何使用
    Python 的 `split` 函数将文本转换为单词：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we did not use any separator; by default, the `split`
    function splits on white spaces.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们没有使用任何分隔符；`split` 函数默认在空格上分割。
- en: N-gram representation
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*N*-gram 表示'
- en: 'We have seen how text can be represented as characters and words. Sometimes
    it is useful to look at two, three, or more words together. *N*-grams are groups
    of words extracted from given text. In an *n*-gram, *n* represents the number
    of words that can be used together. Let''s look at an example of what a bigram (*n=2*)
    looks like. We used the Python `nltk` package to generate a bigram for `thor_review`.
    The following code block shows the result of the bigram and the code used to generate
    it:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到文本可以表示为字符和单词。有时候，查看两个、三个或更多单词在一起是很有用的。*N*-gram 是从给定文本中提取的一组单词。在 *n*-gram
    中，*n* 表示可以一起使用的单词数。让我们看一个 *bigram*（*n=2*）的示例。我们使用 Python 的 `nltk` 包为 `thor_review`
    生成了一个 *bigram*。以下代码块展示了 *bigram* 的结果以及生成它所使用的代码：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `ngrams` function accepts a sequence of words as its first argument and
    the number of words to be grouped as the second argument. The following code block
    shows how a trigram representation would look, and the code used for it:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`ngrams` 函数接受一系列单词作为第一个参数，并将要分组的单词数作为第二个参数。以下代码块展示了三元组表示法的样子，以及用于它的代码：'
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The only thing that changed in the preceding code is the *n*-value, the second
    argument to the function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中唯一改变的是函数的第二个参数*n*-值。
- en: Many supervised machine learning models, for example, Naive Bayes, use *n*-grams
    to improve their feature space. *n*-grams are also used for spelling correction
    and text-summarization tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 许多监督学习模型，例如朴素贝叶斯，使用*n*-gram来改善它们的特征空间。*n*-gram也用于拼写纠正和文本摘要任务。
- en: One challenge with *n*-gram representation is that it loses the sequential nature
    of text. It is often used with shallow machine learning models. This technique
    is rarely used in deep learning, as architectures such as RNN and Conv1D learn
    these representations automatically.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*-gram表示的一个挑战是它丢失了文本的顺序性质。它通常与浅层机器学习模型一起使用。这种技术在深度学习中很少使用，因为像RNN和Conv1D这样的架构可以自动学习这些表示。'
- en: Vectorization
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化
- en: There are two popular approaches to mapping the generated tokens to vectors
    of numbers, called **one-hot encoding** and **word embedding**. Let's understand
    how tokens can be converted to these vector representations by writing a simple
    Python program. We will also discuss the various pros and cons of each method.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种流行的方法可以将生成的标记映射到数字向量中，称为**单热编码**和**词嵌入**。让我们通过编写一个简单的Python程序来了解如何将标记转换为这些向量表示。我们还将讨论每种方法的各种优缺点。
- en: One-hot encoding
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单热编码
- en: 'In one-hot encoding, each token is represented by a vector of length N, where
    *N* is the size of the vocabulary. The vocabulary is the total number of unique
    words in the document. Let''s take a simple sentence and observe how each token
    would be represented as one-hot encoded vectors. The following is the sentence
    and its associated token representation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在单热编码中，每个标记由长度为N的向量表示，其中*N*是文档中唯一单词的数量。让我们看一个简单的句子，并观察每个标记如何表示为单热编码向量。以下是句子及其相关标记表示：
- en: '*An apple a day keeps doctor away said the doctor*.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*一天一个苹果，医生远离你*。'
- en: 'One-hot encoding for the preceding sentence can be represented into a tabular
    format as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前述句子的单热编码可以用表格格式表示如下：
- en: '| An | 100000000 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| An | 100000000 |'
- en: '| apple | 010000000 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| apple | 010000000 |'
- en: '| a | 001000000 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| a | 001000000 |'
- en: '| day | 000100000 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| day | 000100000 |'
- en: '| keeps | 000010000 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| keeps | 000010000 |'
- en: '| doctor | 000001000 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| doctor | 000001000 |'
- en: '| away | 000000100 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| away | 000000100 |'
- en: '| said | 000000010 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| said | 000000010 |'
- en: '| the | 000000001 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| the | 000000001 |'
- en: 'This table describes the tokens and their one-hot encoded representation. The
    vector length is 9, as there are nine unique words in the sentence. A lot of machine
    learning libraries have eased the process of creating one-hot encoding variables.
    We will write our own implementation to make it easier to understand, and we can
    use the same implementation to build other features required for later examples.
    The following code contains a `Dictionary` class, which contains functionality
    to create a dictionary of unique words along with a function to return a one-hot
    encoded vector for a particular word. Let''s take a look at the code and then
    walk through each functionality:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此表描述了标记及其单热编码表示。向量长度为9，因为句子中有九个唯一单词。许多机器学习库已经简化了创建单热编码变量的过程。我们将编写自己的实现以便更容易理解，并可以使用相同的实现来构建后续示例所需的其他特征。以下代码包含了一个`Dictionary`类，其中包含创建唯一单词字典以及返回特定单词的单热编码向量的功能。让我们看一下代码，然后逐个功能进行解释：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code provides three important functionalities:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码提供了三个重要功能：
- en: The initialization function, `__init__`, creates a `word2idx` dictionary, which
    will store all unique words along with the index. The `idx2word` list stores all
    the unique words, and the `length` variable contains the total number of unique
    words in our documents.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化函数`__init__`创建了一个`word2idx`字典，它将存储所有唯一单词及其索引。`idx2word`列表存储所有唯一单词，`length`变量包含文档中唯一单词的总数。
- en: The `add_word` function takes a word and adds it to `word2idx` and `idx2word`, and
    increases the length of the vocabulary, provided the word is unique.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_word`函数接受一个单词并将其添加到`word2idx`和`idx2word`中，并增加词汇表的长度（假设单词是唯一的）。'
- en: The `onehot_encoded` function takes a word and returns a vector of length N
    with zeros throughout, except at the index of the word. If the index of the passed
    word is two, then the value of the vector at index two will be one, and all the
    remaining values will be zeros.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`onehot_encoded`函数接受一个单词并返回一个长度为 N 的向量，其中除了单词的索引处为一之外，所有其他值都为零。如果传递的单词的索引是二，则向量在索引二处的值将为一，所有其他值将为零。'
- en: 'As we have defined our `Dictionary` class, let''s use it on our `thor_review`
    data. The following code demonstrates how the `word2idx` is built and how we can
    call our `onehot_encoded` function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经定义了`Dictionary`类，让我们在`thor_review`数据上使用它。以下代码演示了如何构建`word2idx`以及如何调用我们的`onehot_encoded`函数：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'One-hot encoding for the word `were` is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 单词`were`的一热编码如下：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: One of the challenges with one-hot representation is that the data is too sparse,
    and the size of the vector quickly grows as the number of unique words in the
    vocabulary increases, which is considered to be a limitation, and hence it is
    rarely used with deep learning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一热表示法的一个挑战是数据过于稀疏，且随着词汇表中独特单词数量的增加，向量的大小迅速增长，这被认为是一种限制，因此在深度学习中很少使用。
- en: Word embedding
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Word embedding is a very popular way of representing text data in problems that
    are solved by deep learning algorithms. Word embedding provides a dense representation
    of a word filled with floating numbers. The vector dimension varies according
    to the vocabulary size. It is common to use a word embedding of dimension size
    50, 100, 256, 300, and sometimes 1,000\. The dimension size is a hyper-parameter
    that we need to play with during the training phase.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是在深度学习算法解决的文本数据中表示问题的非常流行的方法。词嵌入提供了一个由浮点数填充的单词的密集表示。向量维度根据词汇表的大小而变化。通常使用的词嵌入维度大小有
    50、100、256、300，有时候甚至是 1,000。维度大小是我们在训练阶段需要调整的超参数之一。
- en: If we are trying to represent a vocabulary of size 20,000 in one-hot representation
    then we will end up with 20,000 x 20,000 numbers, most of which will be zero.
    The same vocabulary can be represented in word embedding as 20,000 x dimension
    size, where the dimension size could be 10, 50, 300, and so on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图用一热表示法表示一个大小为 20,000 的词汇表，那么我们将得到 20,000 x 20,000 个数字，其中大部分将是零。同样的词汇表可以用词嵌入表示为大小为
    20,000 x 维度大小的形式，其中维度大小可以是 10、50、300 等。
- en: 'One way to create word embeddings is to start with dense vectors for each token
    containing random numbers, and then train a model such as a document classifier
    or sentiment classifier. The floating point numbers, which represent the tokens,
    will get adjusted in a way such that semantically closer words will have similar
    representation. To understand it, let''s look at the following figure, where we
    plotted the word embedding vectors on a two-dimensional plot of five movies:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 创建词嵌入的一种方法是从每个标记的随机数密集向量开始，然后训练一个模型，如文档分类器或情感分类器。代表标记的浮点数将以一种使语义上更接近的单词具有类似表示的方式进行调整。为了理解它，让我们看看以下图例，其中我们在五部电影的二维图上绘制了词嵌入向量：
- en: '![](img/f2302e41-c762-4566-aa6e-6b176145c527.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2302e41-c762-4566-aa6e-6b176145c527.png)'
- en: The preceding image shows how the dense vectors are tuned in order to have smaller
    distances for words that are semantically similar. Since movie titles such as
    **Superman**, **Thor**, and **Batman** are action movies based on comics, the
    embedding for such words is closer, whereas the embedding for the movie **Titanic**
    is far from the action movies and closer to the movie title **Notebook**, since
    they are romantic movies.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图像展示了如何调整密集向量以使语义相似的单词之间具有较小的距离。由于像**Superman**、**Thor**和**Batman**这样的电影标题是基于漫画的动作电影，它们的嵌入更接近，而电影**Titanic**的嵌入则远离动作电影，更接近电影**Notebook**，因为它们是浪漫电影。
- en: Learning word embedding may not be feasible when you have too little data, and
    in such cases we can use word embeddings that are trained by some other machine
    learning algorithm. An embedding generated from another task is called a **pretrained**
    word embedding. We will learn how to build our own word embeddings and use pretrained
    word embeddings.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 学习词嵌入可能在数据量太少时不可行，在这种情况下，我们可以使用由其他机器学习算法训练的词嵌入。从另一个任务生成的嵌入称为**预训练**词嵌入。我们将学习如何构建自己的词嵌入并使用预训练词嵌入。
- en: Training word embedding by building a sentiment classifier
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过构建情感分类器训练词嵌入
- en: 'In the last section, we briefly learned about word embedding without implementing
    it. In this section, we will download a dataset called `IMDB`, which contains
    reviews, and build a sentiment classifier which calculates whether a review''s
    sentiment is positive, negative, or unknown. In the process of building, we will
    also train word embedding for the words present in the `IMDB` dataset. We will
    use a library called `torchtext` that makes a lot of processes such as downloading,
    text vectorization, and batching much easier. Training a sentiment classifier
    will involve the following steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要介绍了单词嵌入的概念，但没有实现它。在本节中，我们将下载一个名为`IMDB`的数据集，其中包含评论，并构建一个情感分类器，用于判断评论的情感是积极的、消极的还是未知的。在构建过程中，我们还将为`IMDB`数据集中的单词训练单词嵌入。我们将使用一个名为`torchtext`的库，它通过提供不同的数据加载器和文本抽象化简化了许多与**自然语言处理**（**NLP**）相关的活动。训练情感分类器将涉及以下步骤：
- en: Downloading IMDB data and performing text tokenization
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载IMDB数据并执行文本标记化
- en: Building a vocabulary
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立词汇表
- en: Generating batches of vectors
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成向量批次
- en: Creating a network model with embeddings
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建带有嵌入的网络模型
- en: Training the model
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Downloading IMDB data and performing text tokenization
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载IMDB数据并执行文本标记化
- en: 'For applications related to computer vision, we used the `torchvision` library,
    which provides us with a lot of utility functions, helping to building computer
    vision applications. In the same way, there is a library called `torchtext`, part
    of PyTorch, which is built to work with PyTorch and eases a lot of activities
    related to **natural language processing** (**NLP**) by providing different data
    loaders and abstractions for text. At the time of writing, `torchtext` does not
    come with PyTorch installation and requires a separate installation. You can run
    the following code in the command line of your machine to get `torchtext` installed:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与计算机视觉相关的应用程序，我们使用了`torchvision`库，该库为我们提供了许多实用函数，帮助构建计算机视觉应用程序。同样，还有一个名为`torchtext`的库，它是PyTorch的一部分，专门用于处理与PyTorch相关的许多文本活动，如下载、文本向量化和批处理。在撰写本文时，`torchtext`不随PyTorch安装而提供，需要单独安装。您可以在您的计算机命令行中运行以下代码来安装`torchtext`：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once it is installed, we will be able to use it. Torchtext provides two important
    modules called `torchtext.data` and `torchtext.datasets`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完成，我们将能够使用它。Torchtext提供了两个重要的模块，称为`torchtext.data`和`torchtext.datasets`。
- en: 'We can download the `IMDB Movies` dataset from the following link:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从以下链接下载`IMDB Movies`数据集：
- en: '[https://www.kaggle.com/orgesleka/imdbmovies](https://www.kaggle.com/orgesleka/imdbmovies)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/orgesleka/imdbmovies](https://www.kaggle.com/orgesleka/imdbmovies)'
- en: torchtext.data
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext.data
- en: 'The `torchtext.data` instance defines a class called `Field`, which helps us
    to define how the data has to be read and tokenized. Let''s look at the following
    example, which we will use for preparing our `IMDB` dataset:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext.data`实例定义了一个名为`Field`的类，它帮助我们定义数据的读取和标记化方式。让我们看看下面的示例，我们将用它来准备我们的`IMDB`数据集：'
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, we define two `Field` objects, one for actual text and
    another for the label data. For actual text, we expect `torchtext` to lowercase
    all the text, tokenize the text, and trim it to a maximum length of `20`. If we
    are building the application for a production environment, we may fix the length
    to a much larger number. But, for the toy example it works well. The `Field` constructor
    also accepts another argument called **tokenize**, which by default uses the `str.split`
    function. We can also specify spaCy as the argument, or any other tokenizer. For
    our example we will stick with `str.split`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们定义了两个`Field`对象，一个用于实际文本，另一个用于标签数据。对于实际文本，我们期望`torchtext`将所有文本转换为小写，标记化文本，并将其修剪为最大长度为`20`。如果我们为生产环境构建应用程序，可能会将长度固定为更大的数字。但是对于玩具示例，这个长度可以工作得很好。`Field`构造函数还接受另一个名为**tokenize**的参数，默认使用`str.split`函数。我们还可以指定spaCy作为参数，或任何其他的分词器。在我们的示例中，我们将坚持使用`str.split`。
- en: torchtext.datasets
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: torchtext.datasets
- en: 'The `torchtext.datasets` instance provide wrappers for using different datasets
    like IMDB, TREC (question classification), language modeling (WikiText-2), and
    a few other datasets. We will use `torch.datasets` to download the `IMDB` dataset
    and split it into `train` and `test` datasets. The following code does that, and
    when you run it for the first time it could take several minutes, depending on
    your broadband connection, as it downloads the `IMDB` datasets from the internet:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext.datasets`实例提供了使用不同数据集的包装器，如IMDB、TREC（问题分类）、语言建模（WikiText-2）和其他几个数据集。我们将使用`torch.datasets`下载`IMDB`数据集并将其分割为`train`和`test`数据集。以下代码执行此操作，当您第一次运行它时，根据您的宽带连接速度，可能需要几分钟时间从互联网上下载`IMDB`数据集：'
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The previous dataset''s `IMDB` class abstracts away all the complexity involved
    in downloading, tokenizing, and splitting the database into `train` and `test`
    datasets. `train.fields` contains a dictionary where `TEXT` is the key and the
    value `LABEL`. Let''s look at `train.fields` and each element of `train` contains:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 先前数据集的`IMDB`类将下载、标记和分割数据库到`train`和`test`数据集中所涉及的所有复杂性抽象化。`train.fields`包含一个字典，其中`TEXT`是键，值是`LABEL`。让我们来看看`train.fields`，而每个`train`元素包含：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We can see from these results that a single element contains a field, `text`,
    along with all the tokens representing the `text`, and a `label` field that contains
    the label of the text. Now we have the `IMDB` dataset ready for batching.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从这些结果看到，单个元素包含一个字段`text`，以及表示`text`的所有标记，还有一个包含文本标签的`label`字段。现在我们已经准备好对`IMDB`数据集进行批处理。
- en: Building vocabulary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建词汇表
- en: 'When we created one-hot encoding for `thor_review`, we created a `word2idx`
    dictionary, which is referred to as the vocabulary since it contains all the details
    of the unique words in the documents. The `torchtext` instance makes that easier
    for us. Once the data is loaded, we can call `build_vocab` and pass the necessary
    arguments that will take care of building the vocabulary for the data. The following
    code shows how the vocabulary is built:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为`thor_review`创建一位热编码时，我们创建了一个`word2idx`字典，它被称为词汇表，因为它包含文档中所有唯一单词的详细信息。`torchtext`实例使我们更容易。一旦数据加载完毕，我们可以调用`build_vocab`并传递必要的参数，这些参数将负责为数据构建词汇表。以下代码显示了如何构建词汇表：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we pass in the `train` object on which we need to build
    the vocabulary, and we also ask it to initialize vectors with pretrained embeddings
    of dimensions `300`. The `build_vocab` object just downloads and creates the dimension
    that will be used later, when we train the sentiment classifier using pretrained
    weights. The `max_size` instance limits the number of words in the vocabulary,
    and `min_freq` removes any word which has not occurred more than ten times, where
    `10` is configurable.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们传递了需要构建词汇表的`train`对象，并要求它使用维度为`300`的预训练嵌入来初始化向量。`build_vocab`对象只是下载并创建将在以后训练情感分类器时使用的维度。`max_size`实例限制了词汇表中单词的数量，而`min_freq`则移除了出现次数不到十次的任何单词，其中`10`是可配置的。
- en: 'Once the vocabulary is built, we can obtain different values such as frequency,
    word index, and the vector representation for each word. The following code demonstrates
    how to access these values:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦词汇表构建完成，我们可以获取不同的值，如频率、单词索引和每个单词的向量表示。以下代码演示了如何访问这些值：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following code demonstrates how to access the results:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码演示了如何访问结果：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `stoi` gives access to a dictionary containing words and their indexes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`stoi`提供了一个包含单词及其索引的字典。'
- en: Generate batches of vectors
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成向量的批次
- en: 'Torchtext provides `BucketIterator`, which helps in batching all the text and
    replacing the words with the index number of the words. The `BucketIterator` instance
    comes with a lot of useful parameters like `batch_size`, `device` (GPU or CPU),
    and `shuffle` (whether data has to be shuffled). The following code demonstrates
    how to create iterators that generate batches for the `train` and `test` datasets:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Torchtext提供了`BucketIterator`，它有助于对所有文本进行批处理，并用单词的索引号替换这些单词。`BucketIterator`实例带有许多有用的参数，如`batch_size`、`device`（GPU或CPU）和`shuffle`（数据是否需要洗牌）。以下代码演示了如何创建迭代器以为`train`和`test`数据集生成批次：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code gives a `BucketIterator` object for `train` and `test` datasets.
    The following code will show how to create a `batch` and display the results of
    the `batch`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码为`train`和`test`数据集提供了一个`BucketIterator`对象。以下代码将展示如何创建一个批次并显示批次的结果：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: From the results in the preceding code block, we can see how the text data is
    converted into a matrix of size (`batch_size` * `fix_len`), which is (`128x20`)
    .
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码块的结果中，我们可以看到文本数据如何转换为大小为(`batch_size` * `fix_len`)的矩阵，即(`128x20`)。
- en: Creating a network model with embedding
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建带嵌入的网络模型
- en: 'We discussed word embeddings briefly earlier. In this section, we create word
    embeddings as part of our network architecture and train the entire model to predict
    the sentiment of each review. At the end of the training, we will have a sentiment
    classifier model and also the word embeddings for the `IMDB` datasets. The following
    code demonstrates how to create a network architecture to predict the sentiment
    using word embeddings:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前简要讨论了词嵌入。在本节中，我们将词嵌入作为网络架构的一部分，并训练整个模型来预测每个评论的情感。训练结束时，我们将得到一个情感分类器模型，以及针对`IMDB`数据集的词嵌入。以下代码展示了如何创建一个使用词嵌入来预测情感的网络架构：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, `EmbNet` creates the model for sentiment classification.
    Inside the `__init__` function, we initialize an object of the `nn.Embedding`
    class, which takes two arguments, namely, the size of the vocabulary and the dimensions
    that we wish to create for each word. As we have limited the number of unique
    words, the vocabulary size will be 10,000 and we can start with a small embedding
    size of `10`. For running the program quickly, a small embedding size is useful,
    but when you are trying to build applications for production systems, use embeddings
    of a large size. We also have a linear layer that maps the word embeddings to
    the category (positive, negative, or unknown).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`EmbNet`创建了用于情感分类的模型。在`__init__`函数内部，我们初始化了`nn.Embedding`类的一个对象，它接受两个参数，即词汇表的大小和我们希望为每个单词创建的维度。由于我们限制了唯一单词的数量，词汇表大小将为10,000，并且我们可以从小的嵌入大小`10`开始。对于快速运行程序，小的嵌入大小是有用的，但当您尝试为生产系统构建应用程序时，请使用较大的嵌入。我们还有一个线性层，将词嵌入映射到类别（积极、消极或未知）。
- en: The `forward` function determines how the input data is processed. For a batch
    size of 32 and sentences of a maximum length of 20 words, we will have inputs
    of the shape 32 x 20\. The first embedding layer acts as a lookup table, replacing
    each word with the corresponding embedding vector. For an embedding dimension
    of 10, the output becomes 32 x 20 x 10 as each word is replaced with its corresponding
    embedding. The `view()` function will flatten the result from the embedding layer.
    The first argument passed to `view` will keep that dimension intact. In our case,
    we do not want to combine data from different batches, so we preserve the first
    dimension and flatten the rest of the values in the tensor. After the `view` function
    is applied, the tensor shape changes to 32 x 200\. A dense layer maps the flattened
    embeddings to the number of categories. Once the network is defined, we can train
    the network as usual.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`函数确定如何处理输入数据。对于批量大小为32且最大长度为20个单词的句子，我们将得到形状为32 x 20的输入。第一个嵌入层充当查找表，将每个单词替换为相应的嵌入向量。对于嵌入维度为10，输出将变为32
    x 20 x 10，因为每个单词都被其相应的嵌入替换。`view()`函数将扁平化嵌入层的结果。传递给`view`的第一个参数将保持该维度不变。在我们的情况下，我们不想组合来自不同批次的数据，因此保留第一个维度并扁平化张量中的其余值。应用`view`函数后，张量形状变为32
    x 200。密集层将扁平化的嵌入映射到类别数量。定义了网络架构后，我们可以像往常一样训练网络。'
- en: Remember that in this network, we lose the sequential nature of the text and
    we just use them as a bag of words.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个网络中，我们失去了文本的顺序性，只是把它们作为一个词袋来使用。
- en: Training the model
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Training the model is very similar to what we saw for building image classifiers,
    so we will be using the same functions. We pass batches of data through the model,
    calculate the outputs and losses, and then optimize the model weights, which includes
    the embedding weights. The following code does this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型与构建图像分类器非常相似，因此我们将使用相同的函数。我们通过模型传递数据批次，计算输出和损失，然后优化模型权重，包括嵌入权重。以下代码执行此操作：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code, we call the `fit` method by passing the `BucketIterator`
    object that we created for batching the data. The iterator, by default, does not
    stop generating batches, so we have to set the `repeat` variable of the `BucketIterator`
    object to `False`. If we don't set the `repeat` variable to `False` then the `fit`
    function will run indefinitely. Training the model for around 10 epochs gives
    a validation accuracy of approximately 70%.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们通过传递我们为批处理数据创建的 `BucketIterator` 对象调用 `fit` 方法。迭代器默认不会停止生成批次，因此我们必须将
    `BucketIterator` 对象的 `repeat` 变量设置为 `False`。如果我们不将 `repeat` 变量设置为 `False`，则 `fit`
    函数将无限运行。在大约 10 个 epochs 的训练后，模型的验证准确率约为 70%。
- en: Using pretrained word embeddings
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的词嵌入
- en: 'Pretrained word embeddings would be useful when we are working in specific
    domains, such as medicine and manufacturing, where we have lot of data to train
    the embeddings. When we have little data on which we cannot meaningfully train
    the embeddings, we can use embeddings, which are trained on different data corpuses
    such as Wikipedia, Google News and Twitter tweets. A lot of teams have open source
    word embeddings trained using different approaches. In this section, we will explore
    how `torchtext` makes it easier to use different word embeddings, and how to use
    them in our PyTorch models. It is similar to transfer learning, which we use in
    computer vision applications. Typically, using pretrained embedding would involve
    the following steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定领域（如医学和制造业）工作时，预训练的词嵌入非常有用，因为我们有大量数据可以训练嵌入。当我们有少量数据无法进行有意义的训练时，我们可以使用在不同数据集（如维基百科、Google
    新闻和 Twitter 推文）上训练的嵌入。许多团队都有使用不同方法训练的开源词嵌入。在本节中，我们将探讨 `torchtext` 如何简化使用不同词嵌入，并如何在我们的
    PyTorch 模型中使用它们。这类似于我们在计算机视觉应用中使用的迁移学习。通常，使用预训练嵌入会涉及以下步骤：
- en: Downloading the embeddings
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载嵌入
- en: Loading the embeddings in the model
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载模型中的嵌入
- en: Freezing the embedding layer weights
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冻结嵌入层权重
- en: Let's explore in detail how each step is implemented.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨每个步骤的实现方式。
- en: Downloading the embeddings
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载嵌入
- en: 'The `torchtext` library abstracts away a lot of complexity involved in downloading
    the embeddings and mapping them to the right word. Torchtext provides three classes,
    namely `GloVe`, `FastText`, `CharNGram`, in the `vocab` module, that ease the
    process of downloading embeddings, and mapping them to our vocabulary. Each of
    these classes provides different embeddings trained on different datasets and
    using different techniques. Let''s look at some of the different embeddings provided:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchtext` 库在下载嵌入并将其映射到正确单词中，抽象出了许多复杂性。Torchtext 在 `vocab` 模块中提供了三个类，分别是 `GloVe`、`FastText`、`CharNGram`，它们简化了下载嵌入和映射到我们词汇表的过程。每个类别提供了在不同数据集上训练的不同嵌入，使用了不同的技术。让我们看一些提供的不同嵌入：'
- en: '`charngram.100d`'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`charngram.100d`'
- en: '`fasttext.en.300d`'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fasttext.en.300d`'
- en: '`fasttext.simple.300d`'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fasttext.simple.300d`'
- en: '`glove.42B.300d`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.42B.300d`'
- en: '`glove.840B.300d`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.840B.300d`'
- en: '`glove.twitter.27B.25d`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.25d`'
- en: '`glove.twitter.27B.50d`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.50d`'
- en: '`glove.twitter.27B.100d`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.100d`'
- en: '`glove.twitter.27B.200d`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.twitter.27B.200d`'
- en: '`` `glove.6B.50d` ``'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`` `glove.6B.50d` ``'
- en: '`glove.6B.100d`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.100d`'
- en: '`glove.6B.200d`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.200d`'
- en: '`glove.6B.300d`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glove.6B.300d`'
- en: 'The `build_vocab` method of the `Field` object takes in an argument for the
    embeddings. The following code explains how we download the embeddings:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`Field` 对象的 `build_vocab` 方法接受一个用于嵌入的参数。以下代码解释了我们如何下载这些嵌入：'
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The value to the argument vector denotes what embedding class is to be used.
    The `name` and `dim` arguments determine on what embeddings can be used. We can
    easily access the embeddings from the `vocab` object. The following code demonstrates
    it, along with a view of how the results will look:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 参数向量的值表示使用的嵌入类别。`name` 和 `dim` 参数确定可以使用的嵌入。我们可以轻松地从 `vocab` 对象中访问嵌入。以下代码演示了它，同时展示了结果的样子：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now we have downloaded and mapped the embeddings to our vocabulary. Let's understand
    how we can use them with a PyTorch model.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载并将嵌入映射到我们的词汇表中。让我们了解如何在 PyTorch 模型中使用它们。
- en: Loading the embeddings in the model
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载模型中的嵌入
- en: The `vectors` variable returns a torch tensor of shape `vocab_size x dimensions`
    containing the pretrained embeddings. We have to store the embeddings to the weights
    of our embedding layer. We can assign the weights of the embeddings by accessing
    the weights of the embeddings layer as demonstrated by the following code.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`vectors`变量返回一个形状为`vocab_size x dimensions`的torch张量，其中包含预训练的嵌入。我们必须将这些嵌入的权重存储到我们的嵌入层权重中。我们可以通过访问嵌入层权重来分配嵌入的权重，如下面的代码所示。'
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`model` represents the object of our network, and `embedding` represents the
    embedding layer. As we are using the embedding layer with new dimensions, there
    will be a small change in the input to the linear layer that comes after the embedding
    layer. The following code has the new architecture, which is similar to the previously-used
    architecture where we trained our embeddings:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`代表我们网络的对象，`embedding`代表嵌入层。因为我们使用了新维度的嵌入层，线性层输入会有些变化。下面的代码展示了新的架构，与我们之前训练嵌入时使用的架构类似：'
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once the embeddings are loaded, we have to ensure that, during training, we
    do not change the embedding weights. Let's discuss how to achieve that.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 加载嵌入向量后，我们必须确保在训练过程中不改变嵌入层权重。让我们讨论如何实现这一点。
- en: Freeze the embedding layer weights
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 冻结嵌入层权重
- en: 'It is a two-step process to tell PyTorch not to change the weights of the embedding
    layer:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉PyTorch不要改变嵌入层权重是一个两步骤过程：
- en: Set the `requires_grad` attribute to `False`, which instructs PyTorch that it
    does not need gradients for these weights.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`requires_grad`属性设置为`False`，告诉PyTorch不需要这些权重的梯度。
- en: Remove the passing of the embedding layer parameters to the optimizer. If this
    step is not done, then the optimizer throws an error, as it expects all the parameters
    to have gradients.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除传递给优化器的嵌入层参数。如果不执行此步骤，则优化器会抛出错误，因为它期望所有参数都有梯度。
- en: 'The following code demonstrates how easy it is to freeze the embedding layer
    weights and instruct the optimizer not to use those parameters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码展示了如何轻松冻结嵌入层权重，并告知优化器不使用这些参数：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We generally pass all the model parameters to the optimizer, but in the previous
    code we pass parameters which have `requires_grad` to be `True`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们将所有模型参数传递给优化器，但在前面的代码中，我们传递了`requires_grad`为`True`的参数。
- en: We can train the model using this exact code and should achieve similar accuracy.
    All these model architectures fail to take advantage of the sequential nature
    of the text. In the next section, we explore two popular techniques, namely RNN
    and Conv1D, that take advantage of the sequential nature of the data.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这段代码训练模型，并应该获得类似的准确度。所有这些模型架构都没有利用文本的序列性质。在下一节中，我们将探讨两种流行的技术，即RNN和Conv1D，它们利用数据的序列性质。
- en: Recursive neural networks
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: RNNs are among the most powerful models that enable us to take on applications
    such as classification, labeling on sequential data, generating sequences of text
    (such as with the *SwiftKey* *Keyboard* app which predicts the next word), and
    converting one sequence to another such as translating a language, say, from French
    to English. Most of the model architectures such as feedforward neural networks
    do not take advantage of the sequential nature of data. For example, we need the
    data to present the features of each example in a vector, say all the tokens that represent
    a sentence, paragraph, or documents. Feedforward networks are designed just to
    look at all the features once and map them to output. Let's look at a text example
    which shows why the order, or sequential nature, is important of text. *I had
    cleaned my car* and *I had my car cleaned* are two English sentences with the
    same set of words, but they mean different things only when we consider the order
    of the words.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是最强大的模型之一，使我们能够处理分类、序列数据标签和文本生成等应用（例如*SwiftKey*键盘应用可以预测下一个词），以及将一种序列转换为另一种语言，例如从法语到英语。大多数模型架构如前馈神经网络没有利用数据的序列性质。例如，我们需要数据来表示每个示例的特征向量，比如代表句子、段落或文档的所有标记。前馈网络设计只是一次性查看所有特征并将其映射到输出。让我们看一个文本示例，展示为什么文本的顺序或序列性质很重要。*I
    had cleaned my car* 和 *I had my car cleaned* 是两个英语句子，包含相同的单词集合，但只有在考虑单词顺序时它们的含义不同。
- en: 'Humans make sense of text data by reading words from left to right and building
    a powerful model that kind of understands all the different things the text says.
    RNN works slightly similarly, by looking at one word in text at a time. RNN is
    also a neural network which has a special layer in it, which loops over the data
    instead of processing all at once. As RNNs can process data in sequence, we can
    use vectors of different lengths and generate outputs of different lengths. Some
    of the different representations are provided in the following image:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过从左到右阅读单词并构建一个强大的模型来理解文本数据。RNN的工作方式略有相似，它一次查看文本中的一个单词。RNN也是一种神经网络，其中有一个特殊的层，该层循环处理数据而不是一次性处理所有数据。由于RNN可以按顺序处理数据，我们可以使用不同长度的向量并生成不同长度的输出。以下图像提供了一些不同的表示形式：
- en: '![](img/163554cd-be34-4dd8-911e-465a50c652c5.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/163554cd-be34-4dd8-911e-465a50c652c5.jpeg)'
- en: Image source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- en: The previous image is from one of the famous blogs on RNN ([http://karpathy.github.io/2015/05/21/rnn-effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness))
    , in which the author, Andrej Karpathy, writes about how to build an RNN from
    scratch using Python and use it as sequence generator.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图片来自RNN的一个著名博客（[http://karpathy.github.io/2015/05/21/rnn-effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness))，作者Andrej
    Karpathy讲解如何使用Python从头构建RNN，并将其用作序列生成器。
- en: Understanding how RNN works with an example
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过示例理解RNN的工作原理
- en: Let's start with an assumption that we have an RNN model already built, and
    try to understand what functionality it provides. Once we understand what an RNN
    does, then let's explore what happens inside an RNN.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从假设我们已经构建了一个RNN模型开始，并尝试理解它提供了哪些功能。一旦我们了解了RNN的功能，然后让我们探索RNN内部发生了什么。
- en: 'Let''s consider the Thor review as input to the RNN model. The example text
    we are looking at is *the action scenes were top notch in this movie....* . We
    first start by passing the first word, **the**, to our model; the model generates
    two different things, a **State Vector** and an **Output** vector. The state vector
    is passed to the model when it processes the next word in the review, and a new
    state vector is generated. We just consider the **Output** of the model generated
    during the last sequence. The following figure summarizes it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将《雷神》影评作为RNN模型的输入。我们正在查看的示例文本是*这部电影的动作场面非常棒……*。我们首先将第一个词**the**传递给我们的模型；模型生成两种不同的东西，一个**状态向量**和一个**输出**向量。状态向量在处理评论中的下一个词时传递给模型，并生成一个新的状态向量。我们只考虑在最后一个序列中模型生成的**输出**。下图总结了这一过程：
- en: '![](img/428b553f-c7f7-4772-9354-f99487db93e1.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/428b553f-c7f7-4772-9354-f99487db93e1.png)'
- en: 'The preceding figure demonstrates the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示示了以下内容：
- en: How RNN works by unfolding it and the image
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何通过展开和图像来理解RNN的工作方式
- en: How the state is recursively passed to the same model
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何递归地将状态传递给相同的模型
- en: 'By now you will have an idea of what RNN does, but not how it works. Before
    we get into how it works, let''s look at a code snippet which showcases in more
    detail what we have learnt. We will still view RNN as a black box:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，您应该已经了解了RNN的作用，但还不清楚它是如何工作的。在我们深入了解其工作原理之前，让我们先看一个代码片段，详细展示我们所学的内容。我们仍然将RNN视为一个黑匣子：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the preceding code, the `hidden` variable represents the state vector, sometimes
    called **hidden state**. By now we should have an idea of how RNN is used. Now,
    let''s look at the code that implements RNN and understand what happens inside
    the RNN. The following code contains the `RNN` class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`hidden`变量表示状态向量，有时称为**隐藏状态**。到现在为止，我们应该已经了解了RNN的使用方式。现在，让我们看一下实现RNN并理解RNN内部发生了什么的代码。以下代码包含`RNN`类：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Except for the word `RNN` in the preceding code, everything else would sound
    pretty similar to what we have used in the previous chapters, as PyTorch hides
    a lot of complexity of backpropagation. Let's walk through the `init` function
    and the `forward` function to understand what is happening.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面代码中的词语`RNN`外，其他内容听起来与我们在前几章中使用的内容非常相似，因为PyTorch隐藏了很多反向传播的复杂性。让我们逐步讲解`init`函数和`forward`函数，以理解发生了什么。
- en: The `__init__` function initializes two linear layers, one for calculating the
    output and the other for calculating the state or hidden vector.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`函数初始化两个线性层，一个用于计算输出，另一个用于计算状态或隐藏向量。'
- en: The `forward` function combines the `input` vector and the `hidden` vector and
    passes it through the two linear layers, which generates an output vector and
    a hidden state. For the `output` layer, we apply a `log_softmax` function.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`函数将`input`向量和`hidden`向量结合起来，并通过两个线性层将其传递，生成一个输出向量和一个隐藏状态。对于`output`层，我们应用了`log_softmax`函数。'
- en: 'The `initHidden` function helps in creating hidden vectors with no state for
    calling RNN the very first time. Let''s take a visual look into what the `RNN`
    class does in the following figure:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`initHidden`函数帮助创建没有状态的隐藏向量，用于第一次调用RNN。让我们通过下图视觉地了解`RNN`类的作用：'
- en: '![](img/da427f73-3c11-40e1-8ac7-6b792031c8f2.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da427f73-3c11-40e1-8ac7-6b792031c8f2.png)'
- en: The preceding figure shows how an RNN works.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图展示了RNN的工作方式。
- en: The concepts of RNN are sometimes tricky to understand when you meet them for
    the first time, so I would strongly recommend some of the amazing blogs provided
    in the following links: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    and [http://colah.github.io/posts/2015-08-Understanding-LSTMs/.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次遇到RNN这些概念有时可能会感到棘手，因此我强烈建议阅读以下链接提供的一些令人惊叹的博客：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    和 [http://colah.github.io/posts/2015-08-Understanding-LSTMs/.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: In the next section, we will learn how to use a variant of RNN called **LSTM** to
    build a sentiment classifier on the `IMDB` dataset.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习如何使用一种称为**LSTM**的变种RNN来构建一个情感分类器，用于`IMDB`数据集。
- en: LSTM
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM
- en: RNNs are quite popular in building real-world applications like language translation,
    text classification and many more sequential problems, but in reality, we rarely
    would use a vanilla version of RNN which we saw in the previous section. The vanilla
    version of RNN has problems like vanishing gradients and gradient explosion when
    dealing with large sequences. In most of the real-world problems, variants of
    RNN such as LSTM or GRU are used, which solve the limitations of plain RNN and
    also have the ability to handle sequential data better. We will try to understand
    what happens in LSTM, and build a network based on LSTM to solve the text classification
    problem on the `IMDB` datasets.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在构建语言翻译、文本分类等许多实际应用中非常流行，但在现实中，我们很少会使用我们在前面部分看到的普通RNN版本。普通RNN存在梯度消失和梯度爆炸等问题，处理大序列时尤为突出。在大多数真实世界的问题中，会使用像LSTM或GRU这样的RNN变体，这些变体解决了普通RNN的局限性，并且能更好地处理序列数据。我们将尝试理解LSTM中发生的情况，并基于LSTM构建一个网络来解决`IMDB`数据集上的文本分类问题。
- en: Long-term dependency
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期依赖
- en: RNNs, in theory, should learn all the dependency required from the historical
    data to build a context of what happens next. Say, for example, we are trying
    to predict the last word in the sentence *the clouds are in the sky*. RNN would
    be able to predict it, as the information (clouds) is just a few words behind.
    Let's take another long paragraph where the dependency need not be that close,
    and we want to predict the last word in it. The sentence looks like *I am born
    in Chennai a city in Tamilnadu. Did schooling in different states of India and
    I speak... *. The vanilla version of RNN, in practice, finds it difficult to remember
    the contexts that happened in the earlier parts of sequences. LSTMs and other
    different variants of RNN solve this problem by adding different neural networks
    inside the LSTM which later decides how much, or what data can be remembered.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在理论上应该从历史数据中学习所有必要的依赖关系，以建立下文的上下文。例如，假设我们试图预测句子“the clouds are in the sky”的最后一个词。RNN可以预测，因为信息（clouds）仅仅落后几个词。再来看一个更长的段落，依赖关系不必那么紧密，我们想预测其中的最后一个词。句子看起来像“我出生在金奈，一个坐落在泰米尔纳德邦的城市。在印度的不同州接受教育，我说...”。在实践中，普通版本的RNN很难记住序列前部发生的上下文。LSTM及其他不同的RNN变体通过在LSTM内部添加不同的神经网络来解决这个问题，后者决定可以记住多少或者可以记住什么数据。
- en: LSTM networks
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络
- en: LSTMs are a special kind of RNN, capable of learning long-term dependency. They
    were introduced in 1997 and got popular in the last few years with advancements
    in available data and hardware. They work tremendously well on a large variety
    of problems and are widely used.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是一种特殊类型的RNN，能够学习长期依赖关系。它们于1997年引入，并随着可用数据和硬件的进展在过去几年中变得流行起来。它们在各种问题上表现出色，并被广泛使用。
- en: LSTMs are designed to avoid long-term dependency problems by having a design
    by which it is natural to remember information for a long period of time. In RNNs,
    we saw how they repeat themselves over each element of the sequence. In standard
    RNNs, the repeating module will have a simple structure like a single linear layer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的设计旨在通过一种设计自然地记住信息以解决长期依赖问题。在 RNN 中，我们看到它们如何在序列的每个元素上重复自身。在标准 RNN 中，重复模块将具有像单个线性层的简单结构。
- en: 'The following figure shows how a simple RNN repeats itself:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个简单的 RNN 如何重复自身：
- en: '![](img/d9bb542e-3479-4972-a3f0-c9ae6bb1ea82.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9bb542e-3479-4972-a3f0-c9ae6bb1ea82.png)'
- en: 'Inside LSTM, instead of using a simple linear layer we have smaller networks
    inside the LSTM which does an independent job. The following diagram showcases
    what happens inside an LSTM:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LSTM 内部，我们不是使用简单的线性层，而是在 LSTM 内部有较小的网络，这些网络完成独立的工作。以下图表展示了 LSTM 内部发生的情况：
- en: '![](img/9b1faf38-e33d-487a-9ea3-6d028543434b.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b1faf38-e33d-487a-9ea3-6d028543434b.png)'
- en: Image source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (diagram
    by Christopher Olah)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：http://colah.github.io/posts/2015-08-Understanding-LSTMs/（由 Christopher
    Olah 绘制的图表）
- en: 'Each of the small rectangular (yellow) boxes in the second box in the preceding
    diagram represents a PyTorch layer, the circles represent an element matrix or
    vector addition, and the merging lines represent that two vectors are being concatenated.
    The good part is, we need not implement all of this manually. Most of the modern
    deep learning frameworks provide an abstraction which will take care of what happens
    inside an LSTM. PyTorch provides abstraction of all the functionality inside `nn.LSTM`
    layer, which we can use like any other layer. The most important thing in the
    LSTM is the cell state that passes through all the iterations, represented by
    the horizontal line across the cells in the preceding diagram. Multiple networks
    inside LSTM control what information travels across the cell state. The first
    step in LSTM (a small network represented by the symbol **σ**) is to decide what
    information is going to be thrown away from the cell state. This network is called
    **forget gate** and has a sigmoid as an activation function, which outputs values
    between 0 and 1 for each element in the cell state.The network (PyTorch layer)
    is represented using the following formula:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图表中，第二个框中的每个小矩形（黄色）代表一个 PyTorch 层，圆圈表示元素矩阵或向量加法，合并线表示两个向量正在连接。好消息是，我们不需要手动实现所有这些。大多数现代深度学习框架提供了一个抽象层，可以处理
    LSTM 内部的所有功能。PyTorch 提供了 `nn.LSTM` 层的抽象，我们可以像使用任何其他层一样使用它。LSTM 中最重要的是通过所有迭代传递的细胞状态，如上述图表中横跨细胞的水平线所示。LSTM
    内部的多个网络控制信息如何在细胞状态之间传播。LSTM 中的第一步（由符号 **σ** 表示的小网络）是决定从细胞状态中丢弃哪些信息。这个网络称为 **遗忘门**，具有
    sigmoid 作为激活函数，为细胞状态中的每个元素输出介于 0 和 1 之间的值。该网络（PyTorch 层）由以下公式表示：
- en: '![](img/b1a8fd1b-94ec-407b-a9fa-b2ba2ab10a4e.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1a8fd1b-94ec-407b-a9fa-b2ba2ab10a4e.png)'
- en: 'The values from the network decide which values are to be held in the cell
    state and which are to be thrown away. The next step is to decide what information
    we are going to add to the cell state. This has two parts; a sigmoid layer, called
    **input gate**, which decides what values to be updated; and a tanh layer, which
    creates new values to be added to the cell state. The mathematical representation
    looks like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的值决定了哪些值将保存在细胞状态中，哪些将被丢弃。下一步是决定要添加到细胞状态的信息。这有两个部分；一个称为 **输入门** 的 sigmoid 层，决定要更新的值；一个
    tanh 层，用于创建要添加到细胞状态的新值。数学表示如下：
- en: '![](img/c8c503d6-8b2b-443c-922b-8fb0ed18aa49.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8c503d6-8b2b-443c-922b-8fb0ed18aa49.png)'
- en: '![](img/57590e15-f86c-48a0-99a3-9ef824cbe5d2.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57590e15-f86c-48a0-99a3-9ef824cbe5d2.png)'
- en: 'In the next step, we combine the two values generated by the input gate and
    tanh. Now we can update the cell state, by doing an element-wise multiplication
    between the forget gate and the sum of the product of *i*[*t* ]and C[t],as represented
    by the following formula:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将输入门生成的两个值与 tanh 结合。现在我们可以通过以下公式更新细胞状态，即在遗忘门和 *i*[*t* ]与 C[t] 乘积之和的逐元素乘法之间，如下所示：
- en: '![](img/3d42a0f3-2a83-4057-ac84-d385c413aa34.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d42a0f3-2a83-4057-ac84-d385c413aa34.png)'
- en: Finally, we need to decide on the output, which will be a filtered version of
    the cell state. There are different versions of LSTM available and most of them
    work on similar principles. As developers or data scientists, we rarely need to
    worry about what goes on inside LSTM. If you want to learn more about them, go
    through the following blog links, which cover a lot of theory in a very intuitive
    way.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要决定输出，这将是细胞状态的过滤版本。有不同版本的LSTM可用，它们大多数都基于类似的原理运作。作为开发人员或数据科学家，我们很少需要担心LSTM内部的运作。如果您想更多了解它们，请阅读以下博客链接，这些链接以非常直观的方式涵盖了大量理论。
- en: Look at Christopher Olah's amazing blog on LSTM ([http://colah.github.io/posts/2015-08-Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs)),
    and another blog from Brandon Rohrer ([https://brohrer.github.io/how_rnns_lstm_work.html](https://brohrer.github.io/how_rnns_lstm_work.html))
    where he explains LSTM in a nice video.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Christopher Olah关于LSTM的精彩博客（[http://colah.github.io/posts/2015-08-Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs)），以及Brandon
    Rohrer的另一篇博客（[https://brohrer.github.io/how_rnns_lstm_work.html](https://brohrer.github.io/how_rnns_lstm_work.html)），他在一个很棒的视频中解释了LSTM。
- en: 'Since we understand LSTM, let''s implement a PyTorch network which we can use
    to build a sentiment classifier. As usual, we will follow these steps for creating
    the classifier:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们理解了LSTM，让我们实现一个PyTorch网络，我们可以用来构建情感分类器。像往常一样，我们将按照以下步骤创建分类器：
- en: Preparing the data
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Creating the batches
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建批次
- en: Creating the network
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建网络
- en: Training the model
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Preparing the data
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'We use the same torchtext for downloading, tokenizing and building vocabulary
    for the `IMDB` dataset. When creating the `Field` object, we leave the `batch_first`
    argument at `False`. RNN networks expect the data to be in the form of `Sequence_length`,
    `batch_size` and features. The following is used for preparing the dataset:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用相同的torchtext来下载、分词和构建`IMDB`数据集的词汇表。在创建`Field`对象时，我们将`batch_first`参数保留为`False`。RNN网络期望数据的形式是`Sequence_length`、`batch_size`和特征。以下用于准备数据集：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Creating batches
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建批次
- en: We use the torchtext `BucketIterator` for creating batches, and the size of
    the batches will be sequence length and batches. For our case, the size will be
    [`200`, `32`], where *200* is the sequence length and *32* is the batch size.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用torchtext的`BucketIterator`来创建批次，批次的大小将是序列长度和批次大小。对于我们的情况，大小将为[`200`, `32`]，其中*200*是序列长度，*32*是批次大小。
- en: 'The following is the code used for batching:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于分批的代码：
- en: '[PRE28]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Creating the network
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网络
- en: 'Let''s look at the code and then walk through the code. You may be surprised
    at how similar the code looks:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看代码，然后逐步分析代码。您可能会对代码看起来多么相似感到惊讶：
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The `init` method creates an embedding layer of the size of the vocabulary and
    `hidden_size`. It also creates an LSTM and a linear layer. The last layer is a
    `LogSoftmax` layer for converting the results from the linear layer to probabilities.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`init`方法创建一个与词汇表大小和`hidden_size`相同的嵌入层。它还创建一个LSTM和一个线性层。最后一层是一个`LogSoftmax`层，用于将线性层的结果转换为概率。'
- en: In the `forward` function, we pass the input data of size [`200`, `32`], which
    gets passed through the embedding layer and each token in the batch gets replaced
    by embedding and the size turns to [200, 32, 100], where *100* is the embedding
    dimensions. The LSTM layer takes the output of the embedding layer along with
    two hidden variables. The hidden variables should be of the same type of the embeddings
    output, and their size should be [`num_layers`, `batch_size`, `hidden_size`].
    The LSTM processes the data in a sequence and generates the output of the shape
    [`Sequence_length`, `batch_size`, `hidden_size`], where each sequence index represents
    the output of that sequence. In this case, we just take the output of the last
    sequence, which is of shape [`batch_size`, `hidden_dim`], and pass it on to a
    linear layer to map it to the output categories. Since the model tends to overfit,
    add a dropout layer. You can play with the dropout probabilities.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward`函数中，我们传递大小为[`200`, `32`]的输入数据，它经过嵌入层处理，批次中的每个标记都被嵌入替换，大小变为[200, 32,
    100]，其中*100*是嵌入维度。LSTM层使用嵌入层的输出以及两个隐藏变量进行处理。隐藏变量应与嵌入输出相同类型，并且它们的大小应为[`num_layers`,
    `batch_size`, `hidden_size`]。LSTM按序列处理数据并生成形状为[`Sequence_length`, `batch_size`,
    `hidden_size`]的输出，其中每个序列索引表示该序列的输出。在本例中，我们只取最后一个序列的输出，其形状为[`batch_size`, `hidden_dim`]，然后将其传递给线性层以映射到输出类别。由于模型容易过拟合，添加一个dropout层。您可以调整dropout的概率。
- en: Training the model
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Once the network is created, we can train the model using the same code as
    seen in the previous examples. The following is the code for training the model:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 创建网络后，我们可以使用与前面示例中相同的代码来训练模型。以下是训练模型的代码：
- en: '[PRE30]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Following is the result of the training model:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练模型的结果：
- en: '[PRE31]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Training the model for four epochs gave an accuracy of 84%. Training for more
    epochs resulted in an overfitted model, as the loss started increasing. We can
    try some of the techniques that we tried such as decreasing the hidden dimensions,
    increasing sequence length, and training with smaller learning rates to further
    improve the accuracy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行四次纪元的训练得到了84%的准确率。训练更多的纪元导致了一个过拟合的模型，因为损失开始增加。我们可以尝试一些我们尝试过的技术，如减少隐藏维度、增加序列长度以及以更小的学习率进行训练以进一步提高准确性。
- en: We will also explore how we can use one-dimensional convolutions for training
    on sequence data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨如何使用一维卷积来对序列数据进行训练。
- en: Convolutional network on sequence data
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列数据上的卷积网络
- en: We learned how CNNs solve problems in computer vision by learning features from
    the images. In images, CNNs work by convolving across height and width. In the
    same way, time can be treated as a convolutional feature. One-dimensional convolutions
    sometimes perform better than RNNs and are computationally cheaper. In the last
    few years, companies like Facebook have shown success in audio generation and
    machine translation. In this section, we will learn how CNNs can be used to build
    a text classification solution.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了CNN如何通过从图像中学习特征解决计算机视觉问题。在图像中，CNN通过在高度和宽度上进行卷积来工作。同样，时间可以被视为一个卷积特征。一维卷积有时比RNN表现更好，并且计算成本更低。在过去几年中，像Facebook这样的公司已经展示出在音频生成和机器翻译方面的成功。在本节中，我们将学习如何使用CNN构建文本分类解决方案。
- en: Understanding one-dimensional convolution for sequence data
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解序列数据上的一维卷积
- en: 'In [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep Learning
    for Computer Vision*, we have seen how two-dimensional weights are learned from
    the training data. These weights move across the image to generate different activations.
    In the same way, one-dimensional convolution activations are learned during training
    of our text classifier, where these weights learn patterns by moving across the
    data. The following diagram explains how one-dimensional convolutions will work:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml) *计算机视觉的深度学习* 中，我们看到了如何从训练数据中学习二维权重。这些权重在图像上移动以生成不同的激活。类似地，一维卷积激活在我们的文本分类器训练中也是学习的，这些权重通过在数据上移动来学习模式。以下图解释了一维卷积是如何工作的：
- en: '![](img/1459e19d-8b6f-4b96-94ee-c1b561de8530.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1459e19d-8b6f-4b96-94ee-c1b561de8530.png)'
- en: For training a text classifier on the `IMDB` dataset, we will follow the same
    steps as we followed for building the classifier using LSTM. The only thing that
    changes is that we use `batch_first = True`, unlike in our LSTM network. So, let's
    look at the network, the training code, and its results.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在`IMDB`数据集上训练文本分类器，我们将按照构建使用LSTM分类器的步骤进行操作。唯一不同的是，我们使用`batch_first = True`，而不像我们的LSTM网络那样。因此，让我们看看网络，训练代码以及其结果。
- en: Creating the network
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建网络
- en: 'Let''s look at the network architecture and then walk through the code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看网络架构，然后逐步分析代码：
- en: '[PRE32]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code, instead of an LSTM layer we have a `Conv1d` layer and
    an `AdaptiveAvgPool1d` layer. The convolution layer accepts the sequence length
    as its input size, and the output size to the hidden size, as the kernel size
    three. Since we have to change the dimensions of the linear layer, every time
    we try to run it with different lengths we use an `AdaptiveAvgpool1d` which takes
    input of any size and generates an output of the given size. So, we can use a
    linear layer whose size is fixed. The rest of the code is similar to what we have
    seen in most of the network architectures.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们有一个`Conv1d`层和一个`AdaptiveAvgPool1d`层而不是一个LSTM层。卷积层接受序列长度作为其输入大小，输出大小为隐藏大小，内核大小为3。由于我们必须改变线性层的维度，每次尝试以不同的长度运行时，我们使用一个`AdaptiveAvgpool1d`，它接受任意大小的输入并生成给定大小的输出。因此，我们可以使用一个固定大小的线性层。其余代码与我们在大多数网络架构中看到的代码类似。
- en: Training the model
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'The training steps for the model are the same as the previous example. Let''s
    just look at the code to call the `fit` method and the results it generated:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练步骤与上一个例子相同。让我们看看调用`fit`方法的代码以及它生成的结果：
- en: '[PRE33]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We ran the model for four epochs, which gave approximately 83% accuracy. Here
    are the results of running the model:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行了四轮训练，达到了大约83%的准确率。以下是运行模型的结果：
- en: '[PRE34]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Since the `validation loss` started increasing after three epochs, I stopped
    running the model. A couple of things that we could try to improve the results
    are using pretrained weights, adding another convolution layer, and trying a `MaxPool1d`
    layer between the convolutions. I leave it to you to try this and see if that
    helps improve the accuracy.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`验证损失`在三轮后开始增加，我停止了模型的运行。我们可以尝试几件事情来改善结果，如使用预训练权重、添加另一个卷积层，并在卷积之间尝试`MaxPool1d`层。我把这些尝试留给你来测试是否能提高准确率。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we learned different techniques to represent text data in deep
    learning. We learned how to use pretrained word embeddings and our own trained
    embeddings when working on a different domain. We built a text classifier using
    LSTMs and one-dimensional convolutions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了不同的技术来表示深度学习中的文本数据。我们学习了如何在处理不同领域时使用预训练的词嵌入和我们自己训练的词嵌入。我们使用LSTM和一维卷积构建了文本分类器。
- en: In the next chapter, we will learn how to train deep learning algorithms to
    generate stylish images, and new images, and to generate text.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何训练深度学习算法来生成时尚图像和新图像，并生成文本。
