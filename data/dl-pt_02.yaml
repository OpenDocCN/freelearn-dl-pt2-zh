- en: Building Blocks of Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的构建模块
- en: 'Understanding the basic building blocks of a neural network, such as tensors,
    tensor operations, and gradient descents, is important for building complex neural
    networks. In this chapter, we will build our first `Hello world` program in neural
    networks by covering the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 理解神经网络的基本构建模块，如张量、张量操作和梯度下降，对于构建复杂的神经网络至关重要。在本章中，我们将通过以下主题构建我们的第一个`Hello world`神经网络程序：
- en: Installing PyTorch
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: Implementing our first neural network
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现我们的第一个神经网络
- en: Splitting the neural network into functional blocks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经网络拆分为功能块
- en: Walking through each fundamental block covering tensors, variables, autograds,
    gradients, and optimizers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐步了解每个基础模块，涵盖张量、变量、自动微分、梯度和优化器
- en: Loading data using PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 加载数据
- en: Installing PyTorch
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: PyTorch is available as a Python package and you can either use `pip`, or `conda`,
    to build it or you can build it from source. The recommended approach for this
    book is to use the Anaconda Python 3 distribution. To install Anaconda, please
    refer to the Anaconda official documentation at [https://conda.io/docs/user-guide/install/index.html](https://conda.io/docs/user-guide/install/index.html).
    All the examples will be available as Jupyter Notebooks in the book's GitHub repository.
    I would strongly recommend you use Jupyter Notebook, since it allows you to experiment
    interactively. If you already have Anaconda Python installed, then you can proceed
    with the following steps for PyTorch installation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 可作为 Python 包使用，您可以选择使用`pip`或`conda`来安装，或者您可以从源代码构建。本书推荐的方法是使用 Anaconda
    Python 3 发行版。要安装 Anaconda，请参考 Anaconda 官方文档 [https://conda.io/docs/user-guide/install/index.html](https://conda.io/docs/user-guide/install/index.html)。所有示例将作为
    Jupyter Notebook 提供在本书的 GitHub 仓库中。我强烈建议您使用 Jupyter Notebook，因为它允许您进行交互式实验。如果您已经安装了
    Anaconda Python，则可以按照以下步骤安装 PyTorch。
- en: 'For GPU-based installation with Cuda 8:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CUDA 8 的 GPU 安装
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For GPU-based installation with Cuda 7.5:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 CUDA 7.5 的 GPU 安装：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For non-GPU-based installation:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 非 GPU 安装：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At the time of writing, PyTorch does not work on a Windows machine, so you can
    try a **virtual machine** (**VM**) or Docker image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，PyTorch 不支持 Windows 操作系统，因此您可以尝试使用**虚拟机**（**VM**）或 Docker 镜像。
- en: Our first neural network
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个神经网络
- en: 'We present our first neural network, which learns how to map training examples
    (input array) to targets (output array). Let''s assume that we work for one of
    the largest online companies, **Wondermovies,** which serves videos on demand.
    Our training dataset contains a feature that represents the average hours spent
    by users watching movies on the platform and we would like to predict how much
    time each user would spend on the platform in the coming week. It''s just an imaginary
    use case, don''t think too much about it. Some of the high-level activities for
    building such a solution are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了我们的第一个神经网络，它学习如何将训练样本（输入数组）映射到目标（输出数组）。假设我们为最大的在线公司之一**奇妙电影**工作，该公司提供视频点播服务。我们的训练数据集包含一个特征，代表用户在平台上观看电影的平均时间，我们想预测每个用户在未来一周内在平台上的使用时间。这只是一个虚构的用例，不要过于深思。构建这样一个解决方案的一些高级活动如下：
- en: '**Data preparation**: The `get_data` function prepares the tensors (arrays)
    containing input and output data'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据准备**：`get_data` 函数准备包含输入和输出数据的张量（数组）。'
- en: '**Creating learnable** **parameters**: The `get_weights` function provides
    us with tensors containing random values that we will optimize to solve our problem'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建可学习参数**：`get_weights` 函数提供了包含随机值的张量，我们将优化以解决问题'
- en: '**Network model**: The `simple_network` function produces the output for the
    input data, applying a linear rule, multiplying weights with input data, and adding
    the bias term (*y = Wx+b*)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络模型**：`simple_network` 函数为输入数据生成输出，应用线性规则，将权重与输入数据相乘，并添加偏差项（*y = Wx+b*）'
- en: '**Loss**: The `loss_fn` function provides information about how good the model
    is'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失**：`loss_fn` 函数提供了关于模型性能的信息'
- en: '**Optimizer**: The `optimize` function helps us in adjusting random weights
    created initially to help the model calculate target values more accurately'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化器**：`optimize` 函数帮助我们调整最初创建的随机权重，以帮助模型更准确地计算目标值'
- en: If you are new to machine learning, do not worry, as we will understand exactly
    what each function does by the end of the chapter. The following functions abstract
    away PyTorch code to make it easier for us to understand. We will dive deep into
    each of these functionalities in detail. The aforementioned high level activities
    are common for most machine learning and deep learning problems. Later chapters
    in the book discuss techniques that can be used to improve each function to build
    useful applications.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是机器学习的新手，不用担心，因为我们将在本章结束时准确了解每个函数的功能。以下函数将PyTorch代码抽象化，以便更容易理解。我们将详细探讨每个功能的细节。上述高级活动对大多数机器学习和深度学习问题都很常见。本书后面的章节讨论了用于改进每个功能以构建有用应用程序的技术。
- en: 'Lets consider following linear regression equation for our neural network:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑我们神经网络的线性回归方程：
- en: '![](img/c152c855-4352-491c-8602-80be5cd3c4d3.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c152c855-4352-491c-8602-80be5cd3c4d3.png)'
- en: 'Let''s write our first neural network in PyTorch:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在PyTorch中编写我们的第一个神经网络：
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By the end of this chapter, you will have an idea of what is happening inside
    each function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，您将对每个函数内部发生的情况有所了解。
- en: Data preparation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'PyTorch provides two kinds of data abstractions called `tensors` and `variables`.
    Tensors are similar to `numpy` arrays and they can also be used on GPUs, which
    provide increased performance. They provide easy methods of switching between
    GPUs and CPUs. For certain operations, we can notice a boost in performance and
    machine learning algorithms can understand different forms of data, only when
    represented as tensors of numbers. Tensors are like Python arrays and can change
    in size. For example, images can be represented as three-dimensional arrays (height,
    weight, channel (RGB)). It is common in deep learning to use tensors of sizes
    up to five dimensions. Some of the commonly used tensors are as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了称为`张量`和`变量`的两种数据抽象。张量类似于`numpy`数组，可以在GPU上使用，提供了增强的性能。它们提供了在GPU和CPU之间轻松切换的方法。对于某些操作，我们可以注意到性能的提升，并且只有当表示为数字张量时，机器学习算法才能理解不同形式的数据。张量类似于Python数组，并且可以改变大小。例如，图像可以表示为三维数组（高度、宽度、通道（RGB））。在深度学习中使用大小高达五维的张量是很常见的。一些常用的张量如下：
- en: Scalar (0-D tensors)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标量（0-D张量）
- en: Vector (1-D tensors)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量（1-D张量）
- en: Matrix (2-D tensors)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵（2-D张量）
- en: 3-D tensors
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3-D张量
- en: Slicing tensors
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切片张量
- en: 4-D tensors
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4-D张量
- en: 5-D tensors
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5-D张量
- en: Tensors on GPU
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU上的张量
- en: Scalar (0-D tensors)
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标量（0-D张量）
- en: 'A tensor containing only one element is called a **scalar**. It will generally
    be of type `FloatTensor` or `LongTensor`. At the time of writing, PyTorch does
    not have a special tensor with zero dimensions. So, we use a one-dimension tensor
    with one element, as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 只包含一个元素的张量称为**标量**。通常会是`FloatTensor`或`LongTensor`类型。在撰写本文时，PyTorch没有零维特殊张量。因此，我们使用一个具有一个元素的一维张量，如下所示：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Vectors (1-D tensors)
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量（1-D张量）
- en: 'A `vector` is simply an array of elements. For example, we can use a vector
    to store the average temperature for the last week:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`向量`只是一个元素数组。例如，我们可以使用一个向量来存储上周的平均温度：'
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Matrix (2-D tensors)
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵（2-D张量）
- en: 'Most of the structured data is represented in the form of tables or matrices.
    We will use a dataset called `Boston House Prices`, which is readily available
    in the Python scikit-learn machine learning library. The dataset is a `numpy`
    array consisting of `506` samples or rows and `13` features representing each
    sample. Torch provides a utility function called `from_numpy()`, which converts
    a `numpy` array into a `torch` tensor. The shape of the resulting tensor is `506`
    rows x `13` columns:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数结构化数据以表格或矩阵形式表示。我们将使用名为`Boston House Prices`的数据集，它在Python scikit-learn机器学习库中已经准备好。数据集是一个`numpy`数组，包含`506`个样本或行和`13`个特征，每个样本表示一个。Torch提供了一个实用函数`from_numpy()`，它将`numpy`数组转换为`torch`张量。结果张量的形状是`506`行
    x `13`列：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 3-D tensors
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3-D张量
- en: 'When we add multiple matrices together, we get a *3-D tensor*. 3-D tensors
    are used to represent data-like images. Images can be represented as numbers in
    a matrix, which are stacked together. An example of an image shape is `224`, `224`,
    `3`, where the first index represents height, the second represents width, and
    the third represents a channel (RGB). Let''s see how a computer sees a panda,
    using the next code snippet:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将多个矩阵相加时，我们得到一个 *3-D 张量*。3-D 张量用于表示类似图像的数据。图像可以表示为矩阵中的数字，这些数字被堆叠在一起。图像形状的一个例子是
    `224`、`224`、`3`，其中第一个索引表示高度，第二个表示宽度，第三个表示通道（RGB）。让我们看看计算机如何使用下一个代码片段看到一只熊猫：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since displaying the tensor of size `224`, `224`, `3` would occupy a couple
    of pages in the book, we will display the image and learn to slice the image into
    smaller tensors to visualize it:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于显示大小为 `224`、`224`、`3` 的张量会占据书中的几页，我们将显示图像并学习如何将图像切成较小的张量以进行可视化：
- en: '![](img/b5de7e6a-4c8a-4aed-91da-5d4180b3f9f3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5de7e6a-4c8a-4aed-91da-5d4180b3f9f3.png)'
- en: Displaying the image
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 显示图像
- en: Slicing tensors
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切片张量
- en: 'A common thing to do with a tensor is to slice a portion of it. A simple example
    could be choosing the first five elements of a one-dimensional tensor; let''s
    call the tensor `sales`. We use a simple notation, `sales[:slice_index]` where
    `slice_index` represents the index where you want to slice the tensor:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对张量进行切片是常见的操作。一个简单的例子可能是选择一维张量 `sales` 的前五个元素；我们使用简单的表示法 `sales[:slice_index]`，其中
    `slice_index` 表示要切片张量的索引：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's do more interesting things with our panda image, such as see what the
    panda image looks like when only one channel is chosen and see how to select the
    face of the panda.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用熊猫图像做更有趣的事情，比如看看当只选择一个通道时熊猫图像是什么样子，以及如何选择熊猫的脸部。
- en: 'Here, we select only one channel from the panda image:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从熊猫图像中选择了一个通道：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/21adcb3c-80af-424f-9cc0-16258a64e071.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21adcb3c-80af-424f-9cc0-16258a64e071.png)'
- en: 'Now, lets crop the image. Say we want to build a face detector for pandas and
    we need just the face of a panda for that. We crop the tensor image such that
    it contains only the panda''s face:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们裁剪图像。假设我们要构建一个熊猫的面部检测器，我们只需要熊猫的面部。我们裁剪张量图像，使其仅包含熊猫的面部：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/47dea173-616d-4538-89fd-c376e409ff73.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47dea173-616d-4538-89fd-c376e409ff73.png)'
- en: 'Another common example would be where you need to pick a specific element of
    a tensor:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的例子是需要选择张量的特定元素：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will revisit image data in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep
    Learning for Computer Vision,* when we discuss using CNNs to build image classifiers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第 5 章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)，*深度学习用于计算机视觉*中重新讨论图像数据时，讨论使用
    CNN 构建图像分类器。
- en: Most of the PyTorch tensor operations are very similar to `NumPy` operations.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 PyTorch 张量操作与 `NumPy` 操作非常相似。
- en: 4-D tensors
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4-D 张量
- en: One common example for four-dimensional tensor types is a batch of images. Modern
    CPUs and GPUs are optimized to perform the same operations on multiple examples
    faster. So, they take a similar time to process one image or a batch of images.
    So, it is common to use a batch of examples rather than use a single image at
    a time. Choosing the batch size is not straightforward; it depends on several
    factors. One major restriction for using a bigger batch or the complete dataset
    is GPU memory limitations—*16*, *32*, and *64* are commonly used batch sizes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 四维张量类型的一个常见例子是图像批次。现代 CPU 和 GPU 都经过优化，可以更快地在多个示例上执行相同的操作。因此，它们处理一张图像或一批图像的时间相似。因此，常见的做法是使用一批示例而不是逐个使用单个图像。选择批次大小并不简单；它取决于多个因素。使用更大的批次或完整数据集的一个主要限制是
    GPU 内存限制—*16*、*32* 和 *64* 是常用的批次大小。
- en: 'Let''s look at an example where we load a batch of cat images of size `64`
    x `224` x `224` x `3` where *64* represents the batch size or the number of images,
    *244* represents height and width, and *3* represents channels:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，我们加载一个大小为 `64` x `224` x `224` x `3` 的猫图像批次，其中 *64* 表示批次大小或图像数量，*244*
    表示高度和宽度，*3* 表示通道：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 5-D tensors
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5-D 张量
- en: One common example where you may have to use a five-dimensional tensor is video
    data. Videos can be split into frames, for example, a 30-second video containing
    a panda playing with a ball may contain 30 frames, which could be represented
    as a tensor of shape (1 x 30 x 224 x 224 x 3). A batch of such videos can be represented
    as tensors of shape (32 x 30 x 224 x 224 x 3)—*30* in the example represents,
    number of frames in that single video clip, where *32* represents the number of
    such video clips.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的例子是，您可能需要使用五维张量来处理视频数据。视频可以分割成帧，例如，一个30秒的视频包含一个熊猫和一个球玩耍的视频可能包含30帧，可以表示为形状为（1
    x 30 x 224 x 224 x 3）的张量。一批这样的视频可以表示为形状为（32 x 30 x 224 x 224 x 3）的张量，*30*在这个例子中表示单个视频剪辑中的帧数，其中*32*表示这样的视频剪辑数量。
- en: Tensors on GPU
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU 上的张量
- en: We have learned how to represent different forms of data in tensor representation.
    Some of the common operations we perform once we have data in the form of tensors
    are addition, subtraction, multiplication, dot product, and matrix multiplication.
    All of these operations can be either performed on the CPU or the GPU. PyTorch
    provides a simple function called `cuda()` to copy a tensor on the CPU to the
    GPU. We will take a look at some of the operations and compare the performance
    between matrix multiplication operations on the CPU and GPU.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何在张量表示中表示不同形式的数据。一旦数据以张量形式存在，我们执行的一些常见操作包括加法、减法、乘法、点积和矩阵乘法。所有这些操作可以在
    CPU 或 GPU 上执行。PyTorch 提供了一个简单的函数叫做`cuda()`来将一个在 CPU 上的张量复制到 GPU 上。我们将看一些操作并比较在
    CPU 和 GPU 上矩阵乘法操作的性能。
- en: 'Tensor addition can be obtained by using the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 张量加法可以通过以下代码获得：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For tensor matrix multiplication, lets compare the code performance on CPU and
    GPU. Any tensor can be moved to the GPU by calling the `.cuda()` function.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于张量矩阵乘法，我们比较在 CPU 和 GPU 上的代码性能。任何张量可以通过调用`.cuda()`函数移动到 GPU 上。
- en: 'Multiplication on the GPU runs as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 上的乘法运行如下：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These fundamental operations of addition, subtraction, and matrix multiplication
    can be used to build complex operations, such as a **Convolution Neural Network**
    (**CNN**) and a **recurrent neural network** (**RNN**), which we will learn about
    in the later chapters of the book.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本操作包括加法、减法和矩阵乘法，可以用来构建复杂的操作，比如**卷积神经网络**（**CNN**）和**循环神经网络**（**RNN**），这些我们将在本书的后面章节学习。
- en: Variables
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量
- en: 'Deep learning algorithms are often represented as computation graphs. Here
    is a simple example of the variable computation graph that we built in our example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习算法通常表示为计算图。这里是我们在例子中构建的变量计算图的简单示例：
- en: '![](img/4bffe8e5-599a-424e-8217-abbdee0cc8b2.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bffe8e5-599a-424e-8217-abbdee0cc8b2.png)'
- en: Variable computation graph
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 变量计算图
- en: 'Each circle in the preceding computation graph represents a variable. A variable
    forms a thin wrapper around a tensor object, its gradients, and a reference to
    the function that created it. The following figure shows `Variable` class components:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述计算图中，每个圆圈表示一个变量。一个变量围绕一个张量对象、它的梯度和创建它的函数引用形成一个薄包装。下图展示了`Variable`类的组成部分：
- en: '![](img/c3cefc1f-2e5c-4d58-a488-f970ea340fa4.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3cefc1f-2e5c-4d58-a488-f970ea340fa4.png)'
- en: Variable class
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 变量类
- en: The gradients refer to the rate of the change of the `loss` function with respect
    to various parameters (**W**, **b**). For example, if the gradient of **a** is
    2, then any change in the value of **a** would modify the value of **Y** by two
    times. If that is not clear, do not worry—most of the deep learning frameworks
    take care of calculating gradients for us. In this chapter, we learn how to use
    these gradients to improve the performance of our model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度指的是`loss`函数相对于各个参数（**W**、**b**）的变化率。例如，如果**a**的梯度为2，那么**a**值的任何变化都将使**Y**值增加两倍。如果这不清楚，不要担心——大多数深度学习框架会帮我们计算梯度。在本章中，我们将学习如何利用这些梯度来提高模型的性能。
- en: Apart from gradients, a variable also has a reference to the function that created
    it, which in turn refers to how each variable was created. For example, the variable
    `a` has information that it is generated as a result of the product between `X`
    and `W`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了梯度外，变量还有一个指向创建它的函数的引用，该函数反过来指向如何创建每个变量。例如，变量`a`包含它是由`X`和`W`的乘积生成的信息。
- en: 'Let''s look at an example where we create variables and check the gradients
    and the function reference:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，我们在其中创建变量并检查梯度和函数引用：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding example, we called a `backward` operation on the variable to
    compute the gradients. By default, the gradients of the variables are none.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们对变量执行了`backward`操作以计算梯度。默认情况下，变量的梯度为none。
- en: The `grad_fn` of the variable points to the function it created. If the variable
    is created by a user, like the variable `x` in our case, then the function reference
    is `None`. In the case of variable `y,` it refers to its function reference, `MeanBackward`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的`grad_fn`指向它创建的函数。如果变量是用户创建的，例如我们的变量`x`，那么函数引用为`None`。对于变量`y`，它指向其函数引用，`MeanBackward`。
- en: The Data attribute accesses the tensor associated with the variable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据属性访问与变量相关联的张量。
- en: Creating data for our neural network
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为我们的神经网络创建数据
- en: 'The `get_data` function in our first neural network code creates two variables, `x`
    and `y`, of sizes (`17`, `1`) and (`17`). We will take a look at what happens
    inside the function:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一个神经网络代码中的`get_data`函数创建了两个变量`x`和`y`，大小分别为（`17`，`1`）和（`17`）。我们将看一下函数内部发生了什么：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Creating learnable parameters
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建可学习参数
- en: 'In our neural network example, we have two learnable parameters, `w` and `b`,
    and two fixed parameters, `x` and `y`. We have created variables `x` and `y` in
    our `get_data` function. Learnable parameters are created using random initialization
    and have the `require_grad` parameter set to `True`, unlike `x` and `y`, where
    it is set to `False`. There are different practices for initializing learnable
    parameters, which we will explore in the coming chapters. Let''s take a look at
    our `get_weights` function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的神经网络示例中，我们有两个可学习参数，`w`和`b`，以及两个固定参数，`x`和`y`。我们在`get_data`函数中创建了变量`x`和`y`。可学习参数是使用随机初始化创建的，并且`require_grad`参数设置为`True`，而`x`和`y`的设置为`False`。有不同的实践方法用于初始化可学习参数，我们将在接下来的章节中探讨。让我们看一下我们的`get_weights`函数：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Most of the preceding code is self-explanatory; `torch.randn` creates a random
    value of any given shape.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分前面的代码都是不言自明的；`torch.randn`创建给定形状的随机值。
- en: Neural network model
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络模型
- en: 'Once we have defined the inputs and outputs of the model using PyTorch variables,
    we have to build a model which learns how to map the outputs from the inputs.
    In traditional programming, we build a function by hand coding different logic
    to map the inputs to the outputs. However, in deep learning and machine learning,
    we learn the function by showing it the inputs and the associated outputs. In
    our example, we implement a simple neural network which tries to map the inputs
    to outputs, assuming a linear relationship. The linear relationship can be represented
    as *y = wx + b*, where *w* and *b* are learnable parameters. Our network has to
    learn the values of *w* and *b*, so that *wx + b* will be closer to the actual
    *y*. Let''s visualize our training dataset and the model that our neural network
    has to learn:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用PyTorch变量定义了模型的输入和输出，我们必须构建一个模型，该模型学习如何映射输出到输入。在传统编程中，我们通过手工编码不同的逻辑来构建函数，将输入映射到输出。然而，在深度学习和机器学习中，我们通过向其展示输入和关联输出来学习函数。在我们的例子中，我们实现了一个简单的神经网络，试图将输入映射到输出，假设是线性关系。线性关系可以表示为*y
    = wx + b*，其中*w*和*b*是可学习参数。我们的网络必须学习*w*和*b*的值，以便*wx + b*更接近实际*y*。让我们可视化我们的训练数据集和我们的神经网络必须学习的模型：
- en: '![](img/4780c540-f052-46b7-94eb-6580d4ae5814.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4780c540-f052-46b7-94eb-6580d4ae5814.png)'
- en: Input data points
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据点
- en: 'The following figure represents a linear model fitted on input data points:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表示在输入数据点上拟合的线性模型：
- en: '![](img/81c03405-d31a-45b7-9fbd-4de358111092.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81c03405-d31a-45b7-9fbd-4de358111092.png)'
- en: Linear model fitted on input data points
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入数据点上拟合的线性模型
- en: The dark-gray (blue) line in the image represents the model that our network
    learns.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的深灰色（蓝色）线代表我们的网络学到的模型。
- en: Network implementation
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络实现
- en: 'As we have all the parameters (`x`, `w`, `b`, and `y`) required to implement
    the network, we perform a matrix multiplication between `w` and `x`. Then, sum
    the result with `b`. That will give our predicted `y`. The function is implemented
    as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有所有参数（`x`，`w`，`b`和`y`）来实现网络，我们对`w`和`x`进行矩阵乘法。然后，将结果与`b`相加。这将给出我们预测的`y`。函数实现如下：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'PyTorch also provides a higher-level abstraction in `torch.nn` called **layers**,
    which will take care of most of these underlying initialization and operations
    associated with most of the common techniques available in the neural network.
    We are using the lower-level operations to understand what happens inside these
    functions. In later chapters, that is [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml),
    Deep Learning for Computer Vision and [Chapter 6](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml),
    Deep Learning with Sequence Data and Text, we will be relying on the PyTorch abstractions
    to build complex neural networks or functions. The previous model can be represented
    as a `torch.nn` layer, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 还提供了一个名为 `torch.nn` 的更高级抽象，称为**层**，它将处理大多数神经网络中可用的常见技术的初始化和操作。我们使用较低级别的操作来理解这些函数内部发生的情况。在以后的章节中，即
    [第 5 章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)，计算机视觉的深度学习和 [第 6 章](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml)，序列数据和文本的深度学习，我们将依赖于
    PyTorch 抽象来构建复杂的神经网络或函数。前面的模型可以表示为一个 `torch.nn` 层，如下所示：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now that we have calculated the `y` values, we need to know how good our model
    is, which is done in the `loss` function.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出了 `y` 值，我们需要知道我们的模型有多好，这是在 `loss` 函数中完成的。
- en: Loss function
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'As we start with random values, our learnable parameters, `w` and `b`, will
    result in `y_pred`, which will not be anywhere close to the actual `y`. So, we
    need to define a function which tells the model how close its predictions are
    to the actual values. Since this is a regression problem, we use a loss function
    called **sum of squared error** (**SSE**). We take the difference between the
    predicted `y` and the actual `y` and square it. SSE helps the model to understand
    how close the predicted values are to the actual values. The `torch.nn` library
    has different loss functions, such as MSELoss and cross-entropy loss. However,
    for this chapter, let''s implement the `loss` function ourselves:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从随机值开始，我们的可学习参数 `w` 和 `b` 会导致 `y_pred`，它与实际的 `y` 差距很大。因此，我们需要定义一个函数，告诉模型其预测与实际值的接近程度。由于这是一个回归问题，我们使用一个称为**平方误差和**（**SSE**）的损失函数。我们取预测的
    `y` 与实际 `y` 的差值并求平方。SSE 帮助模型理解预测值与实际值的接近程度。`torch.nn` 库提供了不同的损失函数，如 MSELoss 和交叉熵损失。然而，在本章中，让我们自己实现
    `loss` 函数：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Apart from calculating the loss, we also call the `backward` operation, which
    calculates the gradients of our learnable parameters, `w` and `b`. As we will
    use the `loss` function more than once, we remove any previously calculated gradients
    by calling the `grad.data.zero_()` operation. The first time we call the `backward`
    function, the gradients are empty, so we zero the gradients only when they are
    not `None`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算损失之外，我们还调用 `backward` 操作来计算我们可学习参数 `w` 和 `b` 的梯度。由于我们将多次使用 `loss` 函数，因此通过调用
    `grad.data.zero_()` 操作来删除先前计算的任何梯度。第一次调用 `backward` 函数时，梯度为空，因此只有在梯度不为 `None`
    时才将梯度清零。
- en: Optimize the neural network
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化神经网络
- en: 'We started with random weights to predict our targets and calculate loss for
    our algorithm. We calculate the gradients by calling the `backward` function on
    the final `loss` variable. This entire process repeats for one epoch, that is,
    for the entire set of examples. In most of the real-world examples, we will do
    the optimization step per iteration, which is a small subset of the total set.
    Once the loss is calculated, we optimize the values with the calculated gradients
    so that the loss reduces, which is implemented in the following function:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从随机权重开始预测我们的目标，并为我们的算法计算损失。通过在最终 `loss` 变量上调用 `backward` 函数来计算梯度。整个过程在一个 epoch
    中重复进行，即整个示例集。在大多数实际示例中，我们将在每次迭代中执行优化步骤，这是总集的一个小子集。一旦计算出损失，我们就用计算出的梯度优化值，使损失减少，这在下面的函数中实现：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The learning rate is a hyper-parameter, which allows us to adjust the values
    in the variables by a small amount of the gradients, where the gradients denote
    the direction in which each variable (`w` and `b`) needs to be adjusted.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率是一个超参数，它允许我们通过梯度的微小变化来调整变量的值，其中梯度表示每个变量（`w` 和 `b`）需要调整的方向。
- en: Different optimizers, such as Adam, RmsProp, and SGD are already implemented
    for use in the `torch.optim` package. We will be making use of these optimizers
    in later chapters to reduce the loss or improve the accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的优化器，如 Adam、RmsProp 和 SGD，已经在 `torch.optim` 包中实现供后续章节使用以减少损失或提高精度。
- en: Loading data
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: Preparing data for deep learning algorithms could be a complex pipeline by itself.
    PyTorch provides many utility classes that abstract a lot of complexity such as
    data-parallelization through multi-threading, data-augmenting, and batching. In
    this chapter, we will take a look at two of the important utility classes, namely
    the `Dataset` class and the `DataLoader` class. To understand how to use these
    classes, let's take the `Dogs vs. Cats` dataset from Kaggle ([https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data))
    and create a data pipeline that generates a batch of images in the form of PyTorch
    tensors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习算法准备数据本身可能是一个复杂的流水线。PyTorch提供许多实用类，通过多线程实现数据并行化、数据增强和批处理等复杂性抽象化。在本章中，我们将深入了解两个重要的实用类，即`Dataset`类和`DataLoader`类。要了解如何使用这些类，让我们从Kaggle的`Dogs
    vs. Cats`数据集（[https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)）入手，创建一个数据流水线，以生成PyTorch张量形式的图像批次。
- en: Dataset class
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集类
- en: 'Any custom dataset class, say for example, our `Dogs` dataset class, has to
    inherit from the PyTorch dataset class. The custom class has to implement two
    main functions, namely `__len__(self)` and `__getitem__(self, idx)`. Any custom
    class acting as a `Dataset` class should look like the following code snippet:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 任何自定义数据集类，例如我们的`Dogs`数据集类，都必须继承自PyTorch数据集类。自定义类必须实现两个主要函数，即`__len__(self)`和`__getitem__(self,
    idx)`。任何作为`Dataset`类的自定义类应如以下代码片段所示：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We do any initialization, if required, inside the `init` method—for example,
    reading the index of the table and reading the filenames of the images, in our
    case. The `__len__(self)` operation is responsible for returning the maximum number
    of elements in our dataset. The `__getitem__(self, idx)` operation returns an
    element based on the `idx` every time it is called. The following code implements
    our `DogsAndCatsDataset` class:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`init`方法内进行任何初始化（如果需要），例如读取表的索引和图像文件名，在我们的情况下。`__len__(self)`操作负责返回数据集中的最大元素数。`__getitem__(self,
    idx)`操作每次调用时根据索引返回一个元素。以下代码实现了我们的`DogsAndCatsDataset`类：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once the `DogsAndCatsDataset` class is created, we can create an object and
    iterate over it, which is shown in the following code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了`DogsAndCatsDataset`类，我们就可以创建一个对象并对其进行迭代，如下所示：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Applying a deep learning algorithm on a single instance of data is not optimal.
    We need a batch of data, as modern GPUs are optimized for better performance when
    executed on a batch of data. The `DataLoader` class helps to create batches by
    abstracting a lot of complexity.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个数据实例上应用深度学习算法并不理想。我们需要一批数据，因为现代GPU在批处理数据上执行时能够提供更好的性能优化。`DataLoader`类通过抽象化大量复杂性来帮助创建批次。
- en: DataLoader class
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据加载器类
- en: 'The `DataLoader` class present in PyTorch''s `utils` class combines a dataset
    object along with different samplers, such as `SequentialSampler` and `RandomSampler`,
    and provides us with a batch of images, either using a single or multi-process
    iterators. Samplers are different strategies for providing data to algorithms.
    The following is an example of a `DataLoader` for our `Dogs vs. Cats` dataset:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的`utils`类中的`DataLoader`类结合了数据集对象和不同的采样器，例如`SequentialSampler`和`RandomSampler`，并提供了一个图像批次，使用单进程或多进程迭代器。采样器是为算法提供数据的不同策略。以下是我们的`Dogs
    vs. Cats`数据集的`DataLoader`示例：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`imgs` will contain a tensor of shape (32, 224, 224, 3), where *32* represents
    the batch size.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`imgs`将包含形状为(32, 224, 224, 3)的张量，其中*32*表示批处理大小。'
- en: The PyTorch team also maintains two useful libraries, called `torchvision` and
    `torchtext`, which are built on top of the `Dataset` and `DataLoader` classes.
    We will use them in the relevant chapters.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch团队还维护了两个有用的库，称为`torchvision`和`torchtext`，它们构建在`Dataset`和`DataLoader`类之上。我们将在相关章节中使用它们。
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored various data structures and operations provided
    by PyTorch. We implemented several components, using the fundamental blocks of
    PyTorch. For our data preparation, we created the tensors used by our algorithm.
    Our network architecture was a model for learning to predict average hours spent
    by users on our Wondermovies platform. We used the loss function to check the
    standard of our model and used the `optimize` function to adjust the learnable
    parameters of our model to make it perform better.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了由PyTorch提供的各种数据结构和操作。我们使用PyTorch的基本组件实现了几个部分。对于我们的数据准备，我们创建了算法使用的张量。我们的网络架构是一个模型，用于学习预测用户在我们的Wondermovies平台上平均花费的小时数。我们使用损失函数来检查我们模型的标准，并使用`optimize`函数来调整模型的可学习参数，使其表现更好。
- en: We also looked at how PyTorch makes it easier to create data pipelines by abstracting
    away several complexities that would require us to parallelize and augment data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了PyTorch如何通过抽象化处理数据管道的多个复杂性，这些复杂性原本需要我们进行数据并行化和增强。
- en: In the next chapter, we will dive deep into how neural networks and deep learning
    algorithms work. We will explore various PyTorch built-in modules for building
    network architectures, loss functions, and optimizations. We will also show how
    to use them on real-world datasets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨神经网络和深度学习算法的工作原理。我们将探索用于构建网络架构、损失函数和优化的各种PyTorch内置模块。我们还将展示如何在真实世界数据集上使用它们。
