- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Combining Computer Vision and Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合计算机视觉和强化学习
- en: In the previous chapter, we learned how to generate images of interest. In this
    chapter, we will learn how to combine reinforcement learning-based techniques
    (primarily, deep Q-learning) with computer vision-based techniques. This is especially
    useful in scenarios where the learning environment is complex and we cannot gather
    data for all the cases. In such scenarios, we want the model to learn by itself
    in a simulated environment that resembles reality as closely as possible. Such
    models come in handy when used for self-driving cars, robotics, bots in games
    (real as well as digital), and the field of self-supervised learning, in general.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何生成感兴趣的图像。在本章中，我们将学习如何将基于强化学习的技术（主要是深度Q-learning）与基于计算机视觉的技术相结合。这在学习环境复杂且无法收集所有案例数据的场景中特别有用。在这种情况下，我们希望模型在模拟环境中尽可能接近现实地自学习。这种模型在用于自动驾驶汽车、机器人、游戏中的机器人（真实和数字）、以及自监督学习领域时非常有用。
- en: We will start by learning about the basics of reinforcement learning, and then
    about the terminology associated with identifying how to calculate the value (**Q-value**)
    associated with taking an action in a given state. Then, we will learn about filling
    a **Q-table**, which helps to identify the value associated with various actions
    in a given state. We will also learn about identifying the Q-values of various
    actions in scenarios where coming up with a Q-table is infeasible, due to a high
    number of possible states; we’ll do this using a **Deep Q-Network** (**DQN**).
    This is where we will understand how to leverage neural networks in combination
    with reinforcement learning. Then, we will learn about scenarios where the DQN
    model itself does not work, addressing this by using the DQN alongside the **fixed
    targets model**. Here, we will play a video game known as Pong by leveraging CNN
    in conjunction with reinforcement learning. Finally, we will leverage what we’ve
    learned to build an agent that can drive a car autonomously in a simulated environment
    – CARLA.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从了解强化学习的基本知识开始，然后了解与确定如何计算与在给定状态下采取行动相关的价值（**Q值**）相关的术语。然后，我们将学习填充**Q表**，这有助于确定在给定状态下与各种行动相关联的值。我们还将学习在由于可能状态数过高而无法创建Q表的情况下，如何利用**深度Q网络**（**DQN**）来确定各种行动的Q值。这是我们将了解如何将神经网络与强化学习相结合。接下来，我们将学习DQN模型本身无法工作的情况，通过使用**固定目标模型**来解决这个问题。在这里，我们将通过利用CNN与强化学习来玩一个名为Pong的视频游戏。最后，我们将利用我们所学到的知识来构建一个可以在模拟环境中自主驾驶汽车的代理人
    - CARLA。
- en: 'In summary, in this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在本章中，我们将涵盖以下主题：
- en: Learning the basics of reinforcement learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习强化学习的基础知识
- en: Implementing Q-learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Q-learning
- en: Implementing deep Q-learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现深度Q-learning
- en: Implementing deep Q-learning with fixed targets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现具有固定目标的深度Q-learning
- en: Implementing an agent to perform autonomous driving
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个执行自主驾驶的代理
- en: All code snippets within this chapter are available in the `Chapter14` folder
    of the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码片段都在GitHub存储库的`Chapter14`文件夹中可用，网址是[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: As the field evolves, we will periodically add valuable supplements to the GitHub
    repository. Do check the `supplementary_sections` folder within each chapter’s
    directory for new and useful content.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 随着领域的发展，我们将定期向GitHub存储库添加有价值的补充内容。请检查每个章节目录下的`supplementary_sections`文件夹获取新的有用内容。
- en: Learning the basics of reinforcement learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习强化学习的基础知识
- en: '**Reinforcement learning** (**RL**) is an area of machine learning concerned
    with how software **agents** ought to take **actions** in a given **state** of
    an **environment,** maximizing the notion of cumulative **reward**.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是机器学习的一个领域，关注软件**代理人**如何在给定**环境**的**状态**下采取**行动**，最大化累积**奖励**的概念。'
- en: 'To understand how RL helps, let’s consider a simple scenario. Imagine that
    you are playing chess against a computer. Let’s identify the different components
    involved:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解RL如何帮助，让我们考虑一个简单的场景。想象一下你正在与计算机下国际象棋。让我们确定涉及的不同组件：
- en: The computer is an **agent** that has learned/is learning how to play chess.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机是一个已经学会/正在学习如何下国际象棋的**代理人**。
- en: The setup (rules) of the game constitutes the **environment.**
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏设置（规则）构成了**环境**。
- en: As we make a move (take an **action**), the **state** of the board (the location
    of various pieces on the chessboard) changes.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们进行一步棋（采取**行动**）时，棋盘的**状态**（棋盘上各个棋子的位置）会发生变化。
- en: At the end of the game, depending on the result, the agent gets a **reward**.
    The objective of the agent is to maximize the reward.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏结束时，根据结果，代理获得**奖励**。代理的目标是最大化奖励。
- en: If the machine (*agent1*) is playing against a human, the number of games that
    it can play is finite (depending on the number of games the human can play). This
    might create a bottleneck for the agent to learn well. However, what if `agent1`
    (the agent that is learning the game) can play against *agent2* (`agent2` could
    be another agent that is learning chess, or it could be a piece of chess software
    that has been pre-programmed to play the game well)? Theoretically, the agents
    can play infinite games with each other, which results in maximizing the opportunity
    to learn to play the game well. This way, by playing multiple games, the learning
    agent is likely to learn how to address the different scenarios/states of the
    game well.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器（*agent1*）与人类对弈，它能够进行的游戏数量是有限的（取决于人类可以进行的游戏数量）。这可能对代理学习游戏造成瓶颈。但是，如果`agent1`（正在学习游戏的代理）能够与*agent2*对弈（`agent2`可以是另一个正在学习国际象棋的代理，或者是一个已经预先编程以玩游戏的国际象棋软件）呢？从理论上讲，这些代理可以无限对弈，这最大化了学习玩游戏的机会。通过进行多场游戏，学习代理很可能学会如何处理游戏的不同场景/状态。
- en: 'Let’s understand the process that the learning agent will follow to learn well:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解学习代理将要遵循的学习过程：
- en: Initially, the agent takes a random action in a given state.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初，代理在给定状态下采取随机动作。
- en: The agent stores the action it has taken in various states within a game in
    **memory**.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理将其在游戏中各个状态下采取的动作存储在**内存**中。
- en: Then, the agent associates the result of the action in various states with a
    **reward**.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，代理将在各个状态下动作的结果与**奖励**关联起来。
- en: After playing multiple games, the agent can correlate the action in a state
    to a potential reward by replaying its **experiences**.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进行多场游戏之后，代理可以通过重播其**经历**来关联状态中的动作和潜在奖励。
- en: Next comes the question of quantifying the **value** that corresponds to taking
    an action in a given state. We’ll learn how to calculate this in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是量化在给定状态下采取行动所对应的**价值**的问题。我们将在下一节学习如何计算这个价值。
- en: Calculating the state value
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算状态价值
- en: 'To understand how to quantify the value of a state, let’s use a simple scenario
    where we will define the environment and objective, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解如何量化一个状态的价值，让我们使用一个简单的场景来定义环境和目标，如下所示：
- en: '![A picture containing shoji  Description automatically generated](img/B18457_14_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![包含将自动生成描述的将棋图片](img/B18457_14_01.png)'
- en: 'Figure 14.1: Environment'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：环境
- en: The environment is a grid with two rows and three columns. The agent starts
    at the **Start** cell, and it achieves its objective (is rewarded with a score
    of +1) if it reaches the bottom-right grid cell. The agent does not get a reward
    if it goes to any other cell. The agent can take an action by going to the right,
    left, bottom, or up, depending on the feasibility of the action (the agent can
    go to the right or the bottom of the start grid cell, for example). The reward
    of reaching any of the remaining cells other than the bottom-right cell is 0.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是一个具有两行三列的网格。代理从**起始**单元格开始，并且如果到达右下角的网格单元格，则实现其目标（奖励得分+1）。如果它去到任何其他单元格，代理将不会获得奖励。代理可以通过向右、向左、向下或向上移动来采取行动，具体取决于行动的可行性（例如，代理可以从起始网格单元格向右或向下移动）。到达除右下角单元格以外的任何其他单元格的奖励为0。
- en: 'By using this information, let’s calculate the **value** of a cell (the state
    that the agent is in, in a given snapshot). Given that some energy is spent moving
    from one cell to another, we discount the value of reaching a cell by a discount
    factor of γ, where γ takes care of the energy that’s spent in moving from one
    cell to another. Furthermore, the introduction of γ results in the agent learning
    to play well sooner. With this, let’s formalize the widely used Bellman equation,
    which helps to calculate the value of a cell:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些信息，让我们计算一个单元格的**价值**（代理在给定快照中所处的状态）。鉴于从一个单元格移动到另一个单元格会消耗一些能量，我们通过一个折扣因子
    γ 来打折到达单元格的价值，其中 γ 考虑到从一个单元格移动到另一个单元格所花费的能量。此外，引入 γ 会导致代理更快地学会玩得好。因此，让我们形式化广泛使用的贝尔曼方程，帮助计算单元格的价值：
- en: '![](img/B18457_14_001.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_001.png)'
- en: 'With the preceding equation in place, let’s calculate the values of all cells
    (**once the optimal actions in a state have been identified**), with the value
    of γ being 0.9 (the typical value of γ is between 0.9 and 0.99):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有了上述方程式，让我们计算所有单元格的值（**一旦确定了状态中的最优动作**），其中γ的值为0.9（γ的典型值在0.9到0.99之间）：
- en: '![](img/B18457_14_002.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_002.png)'
- en: 'From the preceding calculations, we can understand how to calculate the values
    in a given state (cell), when given the optimal actions in that state. These values
    are as follows for our simplistic scenario of reaching the terminal state:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述计算中，我们可以理解如何计算给定状态（单元格）中的值，当给出该状态中的最优动作时。对于我们达到终端状态的简化情景，这些值如下：
- en: '![A picture containing shape  Description automatically generated](img/B18457_14_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含形状描述的图片自动生成](img/B18457_14_02.png)'
- en: 'Figure 14.2: Value of each cell'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2：每个单元格的值
- en: With the values in place, we expect the agent to follow a path of increasing
    value.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些值，我们期望代理会遵循增值路径。
- en: Now that we understand how to calculate the state value, in the next section,
    we will understand how to calculate the value associated with a state-action combination.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何计算状态值，接下来的部分中，我们将了解如何计算与状态-动作组合相关联的值。
- en: Calculating the state-action value
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算状态-动作值
- en: In the previous section, we provided a scenario where we already know that the
    agent is taking optimal actions (which is not realistic). In this section, we
    will look at a scenario where we can identify the value that corresponds to a
    state-action combination.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们提供了一个情景，其中我们已经知道代理正在采取最优动作（这并不现实）。在本节中，我们将看看一个情景，我们可以识别与状态-动作组合对应的值。
- en: 'In the following image, each sub-cell within a cell represents the value of
    taking an action in the cell. Initially, the cell values for various actions are
    as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，单元格内的每个子单元格代表在该单元格中采取动作的值。最初，给定状态中各种动作的单元格值如下：
- en: '![Polygon  Description automatically generated](img/B18457_14_03.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![多边形描述自动生成](img/B18457_14_03.png)'
- en: 'Figure 14.3: Initial values of different actions in a given state'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3：给定状态中不同动作的初始值
- en: Note that, in the preceding image, cell *b1* (the 1^(st) row and the 2^(nd)
    column) will have a value of 1 if the agent moves right from the cell (as it corresponds
    to the terminal cell); the other actions result in a value of 0\. X indicates
    that the action is not possible, and hence no value is associated with it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图像中，单元格*b1*（第1行和第2列）将具有值1，如果代理从该单元格向右移动（因为它对应终端单元格）；其他动作结果为0。X表示该动作不可能，因此与之相关联的值为零。
- en: 'Over four iterations (steps), the updated cell values for the actions in the
    given state are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在四次迭代（步骤）后，给定状态中各动作的更新单元格值如下：
- en: '![Diagram  Description automatically generated](img/B18457_14_04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图解描述自动生成](img/B18457_14_04.png)'
- en: 'Figure 14.4: Updated cell values after four iterations'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4：四次迭代后更新的单元格值
- en: This would then go through multiple iterations to provide the optimal action
    that maximizes value at each cell.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随后将通过多次迭代提供最大化每个单元格价值的最优动作。
- en: 'Let’s understand how to obtain the cell values in the second table (*Iteration
    2* in the preceding image). Let’s narrow this down to 0.3, which was obtained
    by taking the downward action when present in the 1^(st) row and the 2^(nd) column
    of the second table. When the agent takes the downward action, there is a 1/3
    chance of it taking the optimal action in the next state. Hence, the value of
    taking a downward action is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解如何获取第二个表中的单元格值（在前述图像中称为*迭代2*）。我们将其缩小到0.3，这是通过在第二个表的第1行和第2列中存在时采取向下动作获得的。当代理采取向下动作时，有1/3的机会采取下一个状态的最优动作。因此，采取向下动作的值如下：
- en: '![](img/B18457_14_003.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_003.png)'
- en: Similarly, we can obtain the values of taking different possible actions in
    different cells.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以获取不同单元格中采取不同可能动作的值。
- en: Now that we know how the values of various actions in a given state are calculated,
    in the next section, we will learn about Q-learning and how we can leverage it,
    along with the Gym environment, so that it can play various games.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何计算给定状态中各种动作的值，在接下来的部分中，我们将学习Q-learning以及如何利用它与Gym环境，使其能够玩各种游戏。
- en: Implementing Q-learning
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施Q-learning
- en: Technically, now that we have calculated the various state-action values we
    need, we can identify the action that will be taken in every state. However, in
    the case of a more complex scenario – for example, when playing video games –
    it gets tricky to fetch state information. OpenAI’s **Gym** environment comes
    in handy in this scenario. It contains a pre-defined environment for the game
    we’re playing. Here, it fetches the next state information, given an action that’s
    been taken in the current state. So far, we have considered the scenario of choosing
    the most optimal path. However, there can be scenarios where we are stuck at the
    local minima.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，现在我们已经计算出了所需的各种状态-动作值，我们可以确定在每个状态下将采取的动作。然而，在更复杂的情景中——例如玩视频游戏时——获取状态信息就变得棘手。OpenAI的**Gym**环境在这种情况下非常有用。它包含了我们正在玩的游戏的预定义环境。在这里，它获取下一个状态的信息，给定在当前状态下已经采取的动作。到目前为止，我们考虑了选择最优路径的情况。然而，可能会出现我们陷入局部最小值的情况。
- en: In this section, we will learn about Q-learning, which helps to calculate the
    value associated with the action in a state, as well as about leveraging the Gym
    environment so that we can play various games. For now, we’ll take a look at a
    simple game called Frozen Lake that is available within the Gym environment. We’ll
    also take a look at exploration-exploitation, which helps us avoid getting stuck
    at the local minima. However, before we do that, we will learn about the Q-value.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习Q-learning，它帮助计算状态中与动作相关联的值，以及利用Gym环境，使我们能够玩各种游戏。目前，我们将看看一个称为Frozen
    Lake的简单游戏，它在Gym环境中可用。我们还将学习探索-利用，这有助于避免陷入局部最小值。然而，在此之前，我们将学习Q值。
- en: Defining the Q-value
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义Q值
- en: 'The Q in Q-learning or Q-value represents the quality (value) of an action.
    Let’s recap how to calculate it:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning或者Q值中的Q代表动作的质量（值）。让我们回顾一下如何计算它：
- en: '![](img/B18457_14_001.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_001.png)'
- en: 'We already know that we must keep **updating** the state-action value of a
    given state until it is saturated. Hence, we’ll modify the preceding formula like
    so:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道，我们必须持续**更新**给定状态的状态-动作值，直到饱和为止。因此，我们将修改前述公式如下：
- en: '![](img/B18457_14_005.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_005.png)'
- en: 'In the preceding equation, we replace 1 with the learning rate so that we can
    update the value of the action that’s taken in a state more gradually:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，我们用学习率替换1，以便我们可以更渐进地更新在状态中采取的动作的值：
- en: '![](img/B18457_14_006.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_006.png)'
- en: With this formal definition of Q-value in place, in the next section, we’ll
    learn about the Gym environment and how it helps us fetch the Q-table (which stores
    information about the values of various actions that have been taken at various
    states) and, thus, come up with the optimal actions in a state.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Q值的正式定义中，接下来的部分中，我们将学习关于Gym环境以及它如何帮助我们获取Q表（其中存储了在各种状态下执行的各种动作的价值信息），从而在状态中提出最优动作。
- en: Understanding the Gym environment
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Gym环境
- en: 'In this section, we will explore the Gym environment and the various functionalities
    present in it while playing the Frozen Lake game:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索Gym环境以及其中的各种功能，同时玩冰湖游戏：
- en: The following code is available as `Understanding_the_Gym_environment.ipynb`
    in the `Chapter14` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码作为`Understanding_the_Gym_environment.ipynb`在本书GitHub存储库的`Chapter14`文件夹中提供，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Install and import the relevant packages:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并导入相关软件包：
- en: '[PRE0]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Print the various environments present in the Gym environment:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印Gym环境中的各种环境：
- en: '[PRE1]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding code prints a dictionary containing all the games available within
    Gym.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码打印了一个包含在Gym中所有可用游戏的字典。
- en: 'Create an environment for the chosen game:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为所选游戏创建一个环境：
- en: '[PRE2]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Inspect the created environment:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查已创建的环境：
- en: '[PRE3]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code results in the following output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![](img/B18457_14_05.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_05.png)'
- en: 'Figure 14.5: Environment state'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：环境状态
- en: In the preceding image, the agent starts at **the top left**. There are four
    holes amid the frozen lake. The agent gets a reward of 0 if they fall in the hole
    and the game is terminated. The objective of the game is for the agent to reach
    **the goal (bottom right)** by taking certain actions (mentioned in step 6).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图像中，代理从 **左上角** 开始。在冰冻湖中间有四个洞。如果代理掉入洞中，将获得 0 的奖励并终止游戏。游戏的目标是使代理通过采取特定的动作（在步骤
    6 中提到）达到 **目标（右下角）**。
- en: 'Print the size of the observation space (the number of states) in the game:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印游戏中观测空间的大小（即状态数）：
- en: '[PRE4]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code gives us an output of `16`. This represents the 16 cells
    that the game has.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码给出了 `16` 的输出。这代表游戏的 16 个单元格。
- en: 'Print the number of possible actions:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印可能动作的数量：
- en: '[PRE5]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding code results in a value of `4`, which represents the four possible
    actions that can be taken.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果是 `4`，表示可以执行的四种可能动作。
- en: 'Sample a random action at a given state:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在给定状态下随机抽取一个动作：
- en: '[PRE6]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, `.sample()` specifies that we fetch one of the possible four actions
    in a given state. The scalar corresponding to each action can be associated with
    the name of the action. We can do this by inspecting the source code in GitHub:
    [https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`.sample()` 指定我们在给定状态下随机选择四种可能的动作之一。每个动作对应的标量可以与动作的名称相关联。我们可以通过查看 GitHub
    上的源代码来做到这一点：[https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py)。
- en: 'Reset the environment to its original state:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将环境重置为其原始状态：
- en: '[PRE7]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Take (`step`) an action:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 (`step`) 一个动作：
- en: '[PRE8]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code fetches the next state, the reward, the flag that states
    whether the game was completed, and additional information. We can execute the
    game with `.step`, since the environment readily provides the next state when
    it’s given a step with an action.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码获取下一个状态、奖励、游戏是否完成的标志以及其他信息。我们可以使用 `.step` 执行游戏，因为环境在给定动作的情况下会提供下一个状态。
- en: These steps form the basis for us to build a Q-table that dictates the optimal
    action to be taken in each state. We’ll do this in the next section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤为我们建立一个指导在每个状态下采取最优动作的 Q 表奠定了基础。我们将在下一节中完成这一操作。
- en: Building a Q-table
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Q 表
- en: In the previous section, we learned how to calculate Q-values for various state-action
    pairs manually. In this section, we will leverage the Gym environment and the
    various modules associated with it to populate the Q-table – where rows represent
    the possible states of an agent and columns represent the actions the agent can
    take. The values of the Q-table represent the Q-values of taking an action in
    a given state.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了如何手动计算各种状态-动作对的 Q 值。在本节中，我们将利用 Gym 环境及其相关模块填充 Q 表，其中行表示代理的可能状态，列表示代理可以执行的动作。Q
    表的值表示在给定状态下执行动作的 Q 值。
- en: 'We can populate the values of the Q-table using the following strategy:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下策略填充 Q 表的数值：
- en: Initialize the game environment and the Q-table with zeros.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化游戏环境和 Q 表，以零填充。
- en: Take a random action and fetch the next state, the reward, the flag stating
    whether the game was completed, and additional information.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行一个随机动作，并获取下一个状态、奖励、游戏是否完成的标志以及其他信息。
- en: Update the Q-value using the Bellman equation we defined earlier.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前定义的贝尔曼方程更新 Q 值。
- en: Repeat *steps 2* and *3* so that there’s a maximum of 50 steps in an episode.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 和 *3*，使每个回合最多有 50 步。
- en: Repeat *steps 2*, *3*, and *4* over multiple episodes.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2*、*3* 和 *4* 多个回合。
- en: 'Let’s code up the preceding strategy:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写上述策略的代码：
- en: The following code is available as `Building_Q_table.ipynb` in the `Chapter14`
    folder in this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可在本书的 GitHub 代码库的 `Chapter14` 文件夹中的 `Building_Q_table.ipynb` 中找到：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Install and initialize the game environment:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装和初始化游戏环境：
- en: '[PRE9]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Initialize the Q-table with zeros:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用零初始化 Q 表：
- en: '[PRE10]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The preceding code checks the possible actions and states that can be used to
    build a Q-table. The Q-table’s dimension should be the number of states multiplied
    by the number of actions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码检查可以用来构建 Q 表的可能动作和状态。Q 表的维度应该是状态数乘以动作数。
- en: 'Play multiple episodes while taking a random action:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行多个回合，同时随机选择一个动作：
- en: 'Here, we first reset the environment at the end of every episode:'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们首先在每个episode结束时重置环境：
- en: '[PRE11]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Take a maximum of 50 steps per episode:'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个episode最多执行50步：
- en: '[PRE12]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We consider a maximum of 50 steps per episode, as it’s possible for the agent
    to keep oscillating between two states forever (think of left and right actions
    being performed consecutively forever). Thus, we need to specify the maximum number
    of steps an agent can take.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们考虑每个episode最多50步，因为代理可能会在两个状态之间永远循环（考虑连续执行左右动作）。因此，我们需要指定代理可以采取的最大步数。
- en: 'Sample a random action and take (`step`) the action:'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机采样一个动作并执行（`step`）该动作：
- en: '[PRE13]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Update the Q-value that corresponds to the state and the action:'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新对应于状态和动作的Q值：
- en: '[PRE14]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we specified that the learning rate is `0.1` and that
    we’re updating the Q-value of a state-action combination, by taking the maximum
    Q-value of the next state (`np.max(qtable[new_state,:])`) into consideration.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上述代码中，我们指定学习速率为`0.1`，并更新状态-动作组合的Q值，考虑到下一个状态的最大Q值（`np.max(qtable[new_state,:])`）。
- en: 'Update the `state` value to `new_state`, which we obtained previously, and
    accumulate `reward` into `total_rewards`:'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新`state`值为之前获得的`new_state`，并将`reward`累积到`total_rewards`中：
- en: '[PRE15]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Place the rewards in a list (`episode_rewards`), and print the Q-table (`qtable`):'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将奖励放入列表（`episode_rewards`），并打印Q表（`qtable`）：
- en: '[PRE16]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code fetches the Q-values of the various actions across states:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码获取了各种动作在不同状态下的Q值：
- en: '![Table  Description automatically generated with medium confidence](img/B18457_14_06.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated with medium confidence](img/B18457_14_06.png)'
- en: 'Figure 14.6: Q-values of the various actions across states'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：各种动作在状态间的Q值
- en: We will learn about how the obtained Q-table is leveraged in the next section.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何在下一节中利用获得的Q表。
- en: So far, we have kept taking a random action every time. However, in a realistic
    scenario, once we have learned that certain actions can’t be taken in certain
    states and vice versa, we don’t need to take a random action anymore. The concept
    of exploration-exploitation comes in handy in such a scenario.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在每次随机采取一个动作。然而，在现实场景中，一旦我们学到某些动作不能在某些状态下执行，反之亦然，我们就不需要再随机采取动作了。在这种情况下，探索-利用的概念非常有用。
- en: Leveraging exploration-exploitation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用探索-利用
- en: 'The concept of exploration-exploitation can be described as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 探索-利用的概念可以描述如下：
- en: '**Exploration** is a strategy where we learn what needs to be done (what action
    to take) in a given state.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索** 是一种策略，我们学习在给定状态下需要做什么（采取什么动作）。'
- en: '**Exploitation** is a strategy where we leverage what has already been learned
    – that is, which action to take in a given state.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用** 是一种策略，利用已经学到的知识 - 即在给定状态下采取哪个动作。'
- en: During the initial stages, it is ideal to have a high amount of exploration,
    as the agent won’t know what optimal actions to take initially. Through the episodes,
    as the agent learns the Q-values of various state-action combinations over time,
    we must leverage exploitation to perform the action that leads to a high reward.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始阶段，高量的探索是理想的，因为代理一开始不会知道哪些动作是最优的。随着episode的进行，随着代理逐渐学习各种状态-动作组合的Q值，我们必须利用利用，执行能够带来高奖励的动作。
- en: 'With this intuition in place, let’s modify the Q-value calculation that we
    built in the previous section so that it includes exploration and exploitation:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个直觉之后，让我们修改在前一节中构建的Q值计算方法，以便包含探索和利用：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The bold lines in the preceding code are what’s been added to the code that
    was shown in the previous section. Within this code, we specify that, over increasing
    episodes, we perform more exploitation than exploration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中的粗体行是在之前显示的代码基础上新增的部分。在这段代码中，我们指定随着更多的episode进行，我们执行更多的利用而非探索。
- en: 'Once we’ve obtained the Q-table, we can leverage it to identify the steps that
    the agent needs to take to reach its destination:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了Q表，我们可以利用它来确定代理需要执行的步骤以达到目的地：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we fetch the current `state` that the agent is in, identify
    the `action` that results in a maximum value in the given state-action combination,
    take the action (`step`) to fetch the `new_state` object that the agent would
    be in, and repeat these steps until the game is complete (terminated).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们获取代理所处的当前`state`，确定在给定状态-动作组合中产生最大值的`action`，执行该动作（`step`）以获取代理将会处于的`new_state`对象，并重复这些步骤直到游戏完成（终止）。
- en: 'The preceding code results in the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码的输出结果如下：
- en: '![](img/B18457_14_07.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_07.png)'
- en: 'Figure 14.7: Optimal actions that an agent takes'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：代理执行的最优动作
- en: As you can see from the preceding figure, the agent is able to take the optimal
    action to reach its goal. Note that this is a simplified example, since the state
    spaces are discrete, resulting in us building a Q-table.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从前面的图中看到的那样，代理能够采取最优动作来达到其目标。请注意，这只是一个简化的例子，因为状态空间是离散的，导致我们构建了一个 Q 表。
- en: But what if the state spaces are continuous (for example, the state space is
    a snapshot image of a game’s current state)? Building a Q-table becomes very difficult
    (as the number of possible states is very large). Deep Q-learning comes in handy
    in such a scenario. We’ll learn about this in the next section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果状态空间是连续的（例如，状态空间是游戏当前状态的快照图像）会怎么样？构建 Q 表变得非常困难（因为可能的状态数量非常大）。在这种情况下，深度
    Q 学习非常有用。我们将在下一节学习关于这个的内容。
- en: Implementing deep Q-learning
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施深度 Q 学习
- en: So far, we have learned how to build a Q-table, which provides values that correspond
    to a given state-action combination by replaying a game – in this case, the Frozen
    Lake game – over multiple episodes. However, when the state spaces are continuous
    (such as a snapshot of a game of Pong – which is an image), the number of possible
    state spaces becomes huge. We will address this in this section, as well as the
    ones to follow, using deep Q-learning. In this section, we will learn how to estimate
    the Q-value of a state-action combination without a Q-table by using a neural
    network – hence the term **deep** Q-learning. Compared to a Q-table, deep Q-learning
    leverages a neural network to map any given state-action (where the state can
    be continuous or discrete) combination to Q-values.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何构建 Q 表，通过多次回放游戏（在本例中是 Frozen Lake 游戏）来为给定的状态-动作组合提供对应的值。然而，当状态空间是连续的（比如
    Pong 游戏的快照 - 也就是图像），可能的状态空间数量就变得非常庞大。我们将在本节以及接下来的节中解决这个问题，使用深度 Q 学习。在本节中，我们将学习如何通过神经网络估计状态-动作组合的
    Q 值，因此称为**深度** Q 学习。与 Q 表相比，深度 Q 学习利用神经网络来映射任何给定的状态-动作（其中状态可以是连续的或离散的）组合到 Q 值。
- en: For this exercise, we will work on the CartPole environment in Gym. Let’s first
    understand what this is.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在 Gym 的 CartPole 环境中进行操作。让我们首先了解这是什么。
- en: Understanding the CartPole environment
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 CartPole 环境
- en: 'Our task is to balance a cart pole for as long as possible. The following image
    shows what the CartPole environment looks like:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务是尽可能长时间地平衡车杆。下面的图像展示了 CartPole 环境的样子：
- en: '![Chart, diagram  Description automatically generated](img/B18457_14_08.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图表、图示 由描述自动生成](img/B18457_14_08.png)'
- en: 'Figure 14.8: Possible actions in a CartPole environment'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：CartPole 环境中的可能动作
- en: 'Note that the pole shifts to the left when the cart moves to the right and
    vice versa. Each state within this environment is defined using four observations,
    whose names and minimum and maximum values are as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在购物车向右移动时，杆向左移动，反之亦然。此环境中的每个状态都使用四个观测定义，它们的名称及其最小和最大值如下：
- en: '| **Observation** | **Minimum Value** | **Maximum Value** |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| **观察** | **最小值** | **最大值** |'
- en: '| Cart position | -2.4 | 2.4 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 购物车位置 | -2.4 | 2.4 |'
- en: '| Cart velocity | -inf | inf |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 购物车速度 | -inf | inf |'
- en: '| Pole angle | -41.8° | 41.8° |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 杆角度 | -41.8° | 41.8° |'
- en: '| Pole velocity at the tip | -inf | inf |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 杆顶端的速度 | -inf | inf |'
- en: 'Table 14.1: Observations (states) in a CartPole environment'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14.1：CartPole 环境中的观察（状态）
- en: Note that all the observations that represent a state have continuous values.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表示状态的所有观察都具有连续的值。
- en: 'At a high level, deep Q-learning for the game of CartPole balancing works as
    follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，CartPole 平衡游戏的深度 Q 学习工作如下：
- en: Fetch the input values (the image of the game/metadata of the game).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取输入值（游戏图像/游戏的元数据）。
- en: Pass the input values through a network that has as many outputs as there are
    possible actions.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入值通过一个具有与可能动作数相同输出数量的网络。
- en: The output layers predict the action values that correspond to taking an action
    in a given state.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层预测与在给定状态下采取行动相对应的动作值。
- en: 'A high-level overview of the network architecture is as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构的高级概述如下：
- en: '![Diagram  Description automatically generated](img/B18457_14_09.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18457_14_09.png)'
- en: 'Figure 14.9: Network architecture to identify the right value of an action
    when given a state'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9：在给定状态时识别正确动作值的网络架构
- en: 'In the preceding image, the network architecture uses the state (four observations)
    as input and the Q-value of taking left and right actions in the current state
    as output. We train the neural network as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图像中，网络架构使用状态（四个观察）作为输入，当前状态下采取左右动作的 Q 值作为输出。我们按以下方式训练神经网络：
- en: During the exploration phase, we perform a random action that has the highest
    value in the output layer.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在探索阶段，我们执行具有输出层中最高值的随机动作。
- en: Then, we store the action, the next state, the reward, and the flag stating
    whether the game was complete in memory.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将动作、下一个状态、奖励以及标志游戏是否完成存储在内存中。
- en: 'In a given state, if the game is not complete, the Q-value of taking an action
    in a given state will be calculated as follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在给定状态下，如果游戏尚未完成，则在该状态下采取行动的 Q 值将如下计算：
- en: '![](img/B18457_14_007.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_14_007.png)'
- en: The Q-values of the current state-action combinations remain unchanged except
    for the action that is taken in *step 2*.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前状态-动作组合的 Q 值保持不变，除了在 *步骤 2* 中执行的动作。
- en: Perform *steps 1* to *4* multiple times and store the experiences.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多次执行步骤 1 到 4 并存储经验。
- en: Fit a model that takes the state as input and the action values as the expected
    outputs (from memory and replay experience), and minimize the **mean squared error**
    (**MSE**) loss between the target Q-value of the best action in the next state
    and the predicted Q-value of the action in the given state.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适合一个以状态作为输入和以行动值作为预期输出的模型（从记忆和重播经验中），并最小化**均方误差**（**MSE**）损失，该损失是最佳动作在下一个状态的目标
    Q 值与在给定状态下动作的预测 Q 值之间的差值。
- en: Repeat the preceding steps over multiple episodes while decreasing the exploration
    rate.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多个情节上重复上述步骤，同时减少探索率。
- en: With the preceding strategy in place, let’s code up deep Q-learning so that
    we can perform CartPole balancing.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有了上述策略，让我们编写深度 Q 学习，以便进行 CartPole 平衡。
- en: Performing CartPole balancing
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行 CartPole 平衡
- en: 'To perform CartPole balancing, you can use the following code:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行 CartPole 平衡，您可以使用以下代码：
- en: This code is available as `Deep_Q_Learning_Cart_Pole_balancing.ipynb` in the
    `Chapter14` folder in this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend that you execute the notebook in GitHub to reproduce the results to
    understand the steps you need to perform and the various code components.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码作为 `Deep_Q_Learning_Cart_Pole_balancing.ipynb` 存在于此书 GitHub 存储库的 `Chapter14`
    文件夹中，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。该代码包含用于下载数据的 URL，并且长度适中。我们强烈建议您在
    GitHub 上执行笔记本以重现结果，以便理解所需执行的步骤和各种代码组件。
- en: 'Install and import the relevant packages:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并导入相关包：
- en: '[PRE19]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the environment:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义环境：
- en: '[PRE20]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the network architecture:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络架构：
- en: '[PRE21]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that the architecture is fairly simple, since it only contains 24 units
    in the 2 hidden layers. The output layer contains as many units as there are possible
    actions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于仅在 2 个隐藏层中包含 24 个单元，因此体系结构相当简单。输出层包含与可能动作数相同数量的单元。
- en: 'Define the `Agent` class, as follows:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `Agent` 类，如下所示：
- en: 'Define the `__init__` method with the various parameters, network, and experience
    defined:'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用各种参数、网络和经验定义 `__init__` 方法：
- en: '[PRE22]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the `step` function, which fetches data from memory and fits it to the
    model by calling the `learn` function:'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `step` 函数，从内存中获取数据，并通过调用 `learn` 函数将其适应模型：
- en: '[PRE23]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that we are learning on a random sample of experiences (using `self.memory`)
    instead of a consecutive sequence of experiences, ensuring that the model learns
    what to do based on current inputs only. If we were to give experiences sequentially,
    there would be a risk of the model learning the correlations in the consecutive
    inputs.
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们正在学习随机样本的经验（使用 `self.memory`），而不是连续经验序列，确保模型仅基于当前输入学习要做什么。如果我们按顺序提供经验，模型可能会学习连续输入中的相关性。
- en: 'Define the `act` function, which predicts an action when given a state:'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `act` 函数，它在给定状态时预测动作：
- en: '[PRE24]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that, in the preceding code, we performing exploration-exploitation while
    determining the action to take.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在上述代码中，我们在确定要采取的动作时进行探索利用。
- en: 'Define the `learn` function, which fits the model so that it predicts action
    values when given a state:'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `learn` 函数，它适配模型以便在给定状态时预测动作值：
- en: '[PRE25]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code, we fetch the sampled experiences and predict the Q-value
    of the action we performed. Furthermore, given that we already know the next state,
    we can predict the best Q-value of the actions in the next state. This way, we
    now know the target value that corresponds to the action that was taken in a given
    state. Finally, we compute the loss between the expected value (`Q_targets`) and
    the predicted value (`Q_expected`) of the Q-value of the action that was taken
    in the current state.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上述代码中，我们获取了抽样的经验并预测了我们执行的动作的 Q 值。此外，考虑到我们已经知道下一个状态，我们可以预测下一个状态中动作的最佳 Q 值。这样，我们现在知道了与在给定状态下采取的动作对应的目标值。最后，我们计算了预期值
    (`Q_targets`) 和预测值 (`Q_expected`) 之间的损失，即当前状态中采取的动作的 Q 值的损失。
- en: 'Define the `sample_experiences` function in order to sample experiences from
    memory:'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `sample_experiences` 函数以从记忆中抽取经验：
- en: '[PRE26]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the `agent` object:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `agent` 对象：
- en: '[PRE27]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Perform deep Q-learning, as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行深度 Q 学习，如下所示：
- en: 'Initialize the list that will store the score information and also the hyperparameters:'
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化将存储得分信息和超参数的列表：
- en: '[PRE28]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Reset the environment in each episode and fetch the state’s shape (number of
    observations). Furthermore, reshape it so that we can pass it to a network:'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每一回合中重置环境并获取状态的形状（观察数量）。此外，重塑它以便我们可以将其传递给网络：
- en: '[PRE29]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Loop through `max_t` time steps, identify the action to be performed, and perform
    (`step`) it. Then, reshape it so that the reshaped state is passed to the neural
    network:'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `max_t` 时间步中循环，确定要执行的动作并执行 (`step`)。然后，重塑状态，以便将重塑后的状态传递给神经网络：
- en: '[PRE30]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Fit the model by specifying `agent.step` on top of the current state and resetting
    the state to the next state so that it can be useful in the next iteration:'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定 `agent.step` 在当前状态的基础上适配模型，并重置状态到下一个状态，以便在下一次迭代中有用：
- en: '[PRE31]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Store the score values, print periodically, and stop training if the mean of
    the scores in the previous 10 steps is greater than 450 (which in general is a
    good score and, hence, chosen):'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储得分值，定期打印，并在前 10 步得分的均值大于 450 时停止训练（这通常是一个不错的分数，因此被选择）：
- en: '[PRE32]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the variation in scores over increasing episodes:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制随着回合增加得分变化的图表：
- en: '[PRE33]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A plot showing the variation of scores over episodes is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是显示得分随回合变化的图表：
- en: '![Chart, histogram  Description automatically generated](img/B18457_14_10.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图 由系统自动生成的描述](img/B18457_14_10.png)'
- en: 'Figure 14.10: Scores over increasing episodes'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：随着回合增加的得分
- en: From the preceding image, we can see that, after episode 2,000, the model attained
    a high score when balancing the CartPole.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以看出，在第 2,000 个回合后，模型在平衡 CartPole 时取得了高分。
- en: Now that we have learned how to implement deep Q-learning, in the next section,
    we will learn how to work on a different state space – a video frame in Pong,
    instead of the four state spaces that define the state in the CartPole environment.
    We will also learn how to implement deep Q-learning with the fixed targets model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何实现深度 Q 学习，在接下来的部分，我们将学习如何处理不同的状态空间 - Pong 中的视频帧，而不是定义 CartPole 环境中的四个状态空间。我们还将学习如何使用固定目标模型实现深度
    Q 学习。
- en: Implementing deep Q-learning with the fixed targets model
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用固定目标模型实现深度 Q 学习
- en: In the previous section, we learned how to leverage deep Q-learning to solve
    the CartPole environment in Gym. In this section, we will work on a more complicated
    game of Pong and understand how deep Q-learning, alongside the fixed targets model,
    can solve the game. While working on this use case, you will also learn how to
    leverage a CNN-based model (in place of the vanilla neural network we used in
    the previous section) to solve the problem. The theory from the previous section
    remains largely the same, with one crucial change, a “fixed target model.” Essentially,
    we create a copy of the local model and use that as our guide for our local model
    at every 1,000 steps, along with the local model’s rewards for those 1,000 steps.
    This makes the local model more grounded and updates its weights more smoothly.
    After the 1,000 steps, we update the target model with the local model to update
    the overall learnings.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了如何利用深度Q学习解决Gym中的CartPole环境问题。在本节中，我们将解决一个更复杂的乒乓球游戏，并了解如何利用深度Q学习以及固定目标模型来解决游戏。在处理此用例时，您还将学习如何利用基于CNN的模型（代替我们在前一节中使用的普通神经网络）来解决问题。前一节的理论基本保持不变，但关键变化是“固定目标模型”。基本上，我们创建了本地模型的副本，并在每1,000步与本地模型一起使用本地模型的奖励。这使得本地模型更加稳定，并更平滑地更新其权重。在1,000步之后，我们使用本地模型更新目标模型，以更新整体学习。
- en: The reason why two models can be effective is that we reduce the burden on the
    local model to simultaneously select actions and generate the targets to train
    the network – such interdependence can lead to significant oscillations in the
    training process.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型有效的原因是，我们减轻了本地模型同时选择动作和生成训练网络目标的负担 - 这种相互依赖会导致训练过程中的显著振荡。
- en: Understanding the use case
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解用例
- en: The objective of this use case is to build an agent that can play against a
    computer (a pre-trained, non-learning agent) and beat it in a game of Pong, where
    the agent is expected to achieve a score of 21 points.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 该用例的目标是构建一个能够与计算机（预训练的非学习代理）对战并在乒乓球游戏中击败它的代理，期望代理能够获得21分。
- en: 'The strategy that we will adopt to solve the problem of creating a successful
    agent for the game of Pong is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略来解决创建成功的乒乓球游戏代理的问题如下：
- en: 'Crop the irrelevant portion of the image to fetch the current frame (state):'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪裁图像的不相关部分以获取当前帧（状态）：
- en: '![Chart  Description automatically generated](img/B18457_14_11.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18457_14_11.png)'
- en: 'Figure 14.11: Original image and processed image (frame) in the Pong game'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：乒乓球游戏中的原始图像和处理后图像（帧）
- en: Note that, in the preceding image, we have taken the original image and cropped
    the top and bottom pixels of the original image in the processed image.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述图像中，我们已经对原始图像进行了处理，剪裁了处理后图像的顶部和底部像素。
- en: Stack four consecutive frames – the agent needs the sequence of states to understand
    whether the ball approaches it or not.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 堆叠四个连续帧 - 代理需要状态序列来判断球是否接近它。
- en: Let the agent play by taking random actions initially, and keep collecting the
    current state, future state, action taken, and rewards in memory. Only keep information
    about the last 10,000 actions in memory and flush the historical ones beyond 10,000.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理通过最初采取随机动作来进行游戏，并在内存中收集当前状态、未来状态、采取的动作和奖励。只保留最近10,000个动作的信息，并在超过10,000个动作时清除历史信息。
- en: Build a network (local network) that takes a sample of states from memory and
    predicts the values of the possible actions.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个网络（本地网络），从内存中取样状态并预测可能动作的值。
- en: Define another network (target network) that is a replica of the local network.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义另一个网络（目标网络），它是本地网络的复制品。
- en: Update the target network every 1,000 times the local network is updated. The
    weights of the target network at the end of every 1,000 epochs are the same as
    the weights of the local network.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当更新本地网络1,000次时，更新目标网络。每1,000个周期结束时，目标网络的权重与本地网络的权重相同。
- en: Leverage the target network to calculate the Q-value of the best action in the
    next state.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用目标网络计算下一状态中最佳动作的Q值。
- en: For the action that the local network suggests, we expect it to predict the
    summation of the immediate reward and the Q-value of the best action in the next
    state.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本地网络建议的动作，我们期望它预测即时奖励和下一状态中最佳动作的Q值之和。
- en: Minimize the MSE loss of the local network.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化本地网络的均方误差损失。
- en: Let the agent keep playing until it maximizes its rewards.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理程序继续播放，直到最大化其奖励。
- en: With the preceding strategy in place, we can now code up the agent so that it
    maximizes its rewards when playing Pong.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有了上述策略，我们现在可以编写代理程序，使其在玩乒乓球时最大化奖励。
- en: Coding up an agent to play Pong
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写代理程序玩乒乓球
- en: 'Follow these steps to code up the agent so that it self-learns how to play
    Pong:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 编写代理程序以使其自学习如何玩乒乓球的步骤如下：
- en: The following code is available as `Pong_Deep_Q_Learning_with_Fixed_targets.ipynb`
    in the `Chapter14` folder in this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend that you execute the notebook in GitHub to reproduce the results to
    understand the steps to perform and the various code components.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书的GitHub存储库的`Chapter14`文件夹中的`Pong_Deep_Q_Learning_with_Fixed_targets.ipynb`文件中提供。代码包含从中下载数据的URL，并且代码长度适中。我们强烈建议您在GitHub上执行笔记本以重现结果，以理解执行步骤和各种代码组件。
- en: 'Import the relevant packages and set up the game environment:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并设置游戏环境：
- en: '[PRE34]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the state size and action size:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义状态大小和动作大小：
- en: '[PRE35]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define a function that will preprocess a frame so that it removes the bottom
    and top pixels that are irrelevant:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将预处理帧以去除不相关的底部和顶部像素：
- en: '[PRE36]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define a function that will stack four consecutive frames, as follows:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将堆叠四个连续的帧，如下所示：
- en: 'The function takes `stacked_frames`, the current `state`, and the flag of `is_new_episode`
    as input:'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数接受`stacked_frames`、当前`state`和`is_new_episode`标志作为输入：
- en: '[PRE37]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'If the episode is new (a restart of the game), we will start with a stack of
    initial frames:'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这是新的场次（游戏重新开始），我们将以初始帧的堆栈开始：
- en: '[PRE38]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If the episode is not new, we’ll remove the oldest frame from `stacked_frames`
    and append the latest frame:'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果该场次不是新的，我们将从`stacked_frames`中移除最旧的帧并附加最新的帧：
- en: '[PRE39]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the network architecture – that is, `DQNetwork`:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络架构——即`DQNetwork`：
- en: '[PRE40]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define the `Agent` class, as we did in the previous section, as follows:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前一节中所做的那样，定义`Agent`类：
- en: 'Define the `__init__` method:'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`__init__`方法：
- en: '[PRE41]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that the only addition we’ve made to the `__init__` method in the preceding
    code, compared to the code provided in the previous section, is the `target` network
    and the frequency with which it will be updated (these lines were shown in bold
    in the preceding code).
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在前述代码的`__init__`方法中，与前一节提供的代码相比，我们唯一增加的部分是`target`网络及其更新频率（这些行在前述代码中以粗体显示）。
- en: 'Define the method that will update the weights (`step`), just like we did in
    the previous section:'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将更新权重的方法（`step`），就像我们在前一节中所做的那样：
- en: '[PRE42]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the `act` method, which will fetch the action to be performed in a given
    state:'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`act`方法，该方法将获取给定状态下要执行的动作：
- en: '[PRE43]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the `learn` function, which will train the local model:'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`learn`函数，该函数将训练本地模型：
- en: '[PRE44]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that, in the preceding code, `Q_targets_next` is predicted using the target
    model instead of the local model that was used in the previous section (we’ve
    highlighted this line in the code). We also update the target network after every
    1,000 steps, where `learn_every_target_counter` is the counter that helps to identify
    whether we should update the target model.
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在前述代码中，`Q_targets_next`是使用目标模型而不是在前一节中使用的本地模型预测的（我们在代码中突出显示了这一行）。我们还会在每1,000步之后更新目标网络，其中`learn_every_target_counter`是帮助我们确定是否应更新目标模型的计数器。
- en: 'Define a function (`target_update`) that will update the target model:'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数(`target_update`)，用于更新目标模型：
- en: '[PRE45]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define a function that will sample experiences from memory:'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将从内存中采样经验：
- en: '[PRE46]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the `Agent` object:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Agent`对象：
- en: '[PRE47]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define the parameters that will be used to train the agent:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义将用于训练代理程序的参数：
- en: '[PRE48]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Train the agent over increasing episodes, as we did in the previous section:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练代理程序随着场次的增加，就像我们在前一节中所做的那样：
- en: '[PRE49]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following plot shows the variation of scores over increasing episodes:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了随着场次增加得分的变化：
- en: '![Chart  Description automatically generated](img/B18457_14_12.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图表 自动生成的描述](img/B18457_14_12.png)'
- en: 'Figure 14.12: Scores over increasing epochs'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：随着时代增长的得分
- en: From the preceding image, we can see that the agent gradually learned to play
    Pong and that, by the end of 800 episodes, it had learned how to play it while
    receiving a high reward.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像中，我们可以看到代理逐渐学会了玩 Pong 游戏，并且在 800 个周期结束时，它学会了在接受高奖励的同时玩这个游戏。
- en: Now that we’ve trained an agent to play Pong well, in the next section, we will
    train an agent so that it can drive a car autonomously in a simulated environment.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个能够很好地玩 Pong 游戏的代理，下一节我们将训练一个代理，使其能够在模拟环境中自动驾驶汽车。
- en: Implementing an agent to perform autonomous driving
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个代理程序执行自动驾驶
- en: 'Now that you have seen RL working in progressively challenging environments,
    we will conclude this chapter by demonstrating that the same concepts can be applied
    to a self-driving car. Since it is impractical to see this working on an actual
    car, we will resort to a simulated environment. This scenario has the following
    components:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到 RL 在逐步挑战的环境中运作，我们将通过演示相同的概念可以应用于自动驾驶汽车来结束本章。由于在实际汽车上看到这种工作是不切实际的，我们将转而使用模拟环境。这种情景包含以下组成部分：
- en: The environment is a full-fledged city of traffic, with cars and additional
    details within the image of a road. The actor (agent) is a car.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是一个充满交通的完整城市，道路上有汽车和其他细节。演员（代理）是一辆汽车。
- en: The inputs to the car are the various sensory inputs such as a dashcam, **Light
    Detection and Ranging** (**LIDAR**) sensors, and GPS coordinates.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汽车的输入包括各种感官输入，例如仪表板摄像头、**光学雷达**（**LIDAR**）传感器和 GPS 坐标。
- en: The outputs are going to be how fast/slow the car will move, along with the
    level of steering.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出将是汽车移动的快慢程度，以及转向的级别。
- en: This simulation will attempt to be an accurate representation of real-world
    physics. Thus, note that the fundamentals will remain the same, whether it is
    a car simulation or a real car.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 此仿真将尝试准确地反映现实世界的物理情况。因此，请注意，无论是汽车仿真还是真实汽车，基本原理都将保持不变。
- en: Note that the environment we are going to install needs a **graphical user interface**
    (**GUI**) to display the simulation. Also, the training will take at least a day,
    if not more. Because of the non-availability of a visual setup and the time usage
    limits of Google Colab, we will not use Google Colab notebooks as we have done
    so far. This is the only section of this book that requires an active Linux operating
    system and, preferably, a GPU to achieve acceptable results in a few days of training.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们要安装的环境需要**图形用户界面**（**GUI**）来显示仿真内容。此外，训练将至少需要一天时间，如果不是更长。由于 Google Colab
    缺乏可视化设置并且时间使用限制，我们将不再使用 Google Colab 笔记本，正如我们迄今所做的那样。这是本书唯一一个需要活跃的 Linux 操作系统，并且最好有
    GPU 以在几天的训练中获得可接受结果的部分。
- en: Setting up the CARLA environment
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 CARLA 环境
- en: 'As we mentioned previously, we need an environment that can simulate complex
    interactions to make us believe that we are, in fact, dealing with a realistic
    scenario. CARLA is one such environment. The environment author stated the following
    about CARLA:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们需要一个可以模拟复杂交互的环境，使我们相信我们实际上正在处理一个真实场景。CARLA 就是这样的一个环境。环境作者对 CARLA
    表示如下：
- en: “*CARLA has been developed from the ground up to support development, training,
    and validation of autonomous driving systems. In addition to open source code
    and protocols, CARLA provides open digital assets (urban layouts, buildings, and
    vehicles) that were created for this purpose and can be used freely. The simulation
    platform supports flexible specification of sensor suites, environmental conditions,
    full control of all static and dynamic actors, maps generation, and much more.*”
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: “*CARLA 是从头开始开发的，旨在支持自动驾驶系统的开发、训练和验证。除了开放源代码和协议外，CARLA 还提供了专为此目的创建并可以自由使用的开放数字资产（城市布局、建筑物和车辆）。仿真平台支持传感器套件的灵活规格、环境条件、对所有静态和动态参与者的全面控制、地图生成等等。*”
- en: 'There are two steps we need to follow to set up the environment:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要遵循两个步骤来设置环境：
- en: Install the CARLA binaries for the simulation environment.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 CARLA 仿真环境的二进制文件。
- en: Install the Gym version, which provides Python connectivity for the simulation
    environment.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Gym 版本，该版本提供了 Python 连接以进行仿真环境。
- en: 'The steps for this section have been presented as a video walkthrough here:
    [https://tinyurl.com/mcvp-self-driving-agent](https://tinyurl.com/mcvp-self-driving-agent).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分的步骤已作为视频教程呈现在此处：[https://tinyurl.com/mcvp-self-driving-agent](https://tinyurl.com/mcvp-self-driving-agent)。
- en: Let’s get started!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Installing the CARLA binaries
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装CARLA二进制文件
- en: 'In this section, we will learn how to install the necessary CARLA binaries:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何安装必要的CARLA二进制文件：
- en: Visit [https://github.com/carla-simulator/carla/releases/tag/0.9.6](https://github.com/carla-simulator/carla/releases/tag/0.9.6)
    and download the `CARLA_0.9.6.tar.gz` compiled version file.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[https://github.com/carla-simulator/carla/releases/tag/0.9.6](https://github.com/carla-simulator/carla/releases/tag/0.9.6)，并下载编译版本文件`CARLA_0.9.6.tar.gz`。
- en: 'Move it to a location where you want CARLA to live in your system and unzip
    it. Here, we will demonstrate this by downloading and unzipping CARLA into the
    `Documents` folder:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其移动到您希望CARLA在系统中居住的位置并解压缩它。在这里，我们将通过下载并解压缩CARLA到`Documents`文件夹来演示这一点：
- en: '[PRE50]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Add CARLA to `PYTHONPATH` so that any module on your machine can import it:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CARLA添加到`PYTHONPATH`中，以便您的机器上的任何模块都可以导入它：
- en: '[PRE51]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In the preceding code, we added the directory containing CARLA to a global
    variable called `PYTHONPATH`, which is an environment variable for accessing all
    Python modules. Adding it to `~/.bashrc` will ensure that every time a terminal
    is opened, it can access this new folder. After running the preceding code, restart
    the terminal and run `ipython -c "import carla; carla.__spec__"`. You should get
    the following output:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将包含CARLA的目录添加到名为`PYTHONPATH`的全局变量中，这是一个用于访问所有Python模块的环境变量。将其添加到`~/.bashrc`将确保每次打开终端时都可以访问此新文件夹。运行上述代码后，重新启动终端并运行`ipython
    -c "import carla; carla.__spec__"`。您应该会得到以下输出：
- en: '![Text  Description automatically generated](img/B18457_14_13.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本说明](img/B18457_14_13.png)'
- en: 'Figure 14.13: Location of CARLA on your machine'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.13：CARLA在您的机器上的位置
- en: 'Finally, provide the necessary permissions and execute CARLA, as follows:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，按以下方式提供必要的权限并执行CARLA：
- en: '[PRE52]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'After a minute or two, you should see a window similar to the following showing
    CARLA running as a simulation, ready to take inputs:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一两分钟后，您应该会看到一个类似于以下显示CARLA作为仿真运行的窗口，准备接受输入：
- en: '![A picture containing text  Description automatically generated](img/B18457_14_14.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本 说明](img/B18457_14_14.png)'
- en: 'Figure 14.14: Window showing CARLA running'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.14：显示CARLA运行的窗口
- en: In this section, we’ve verified that `CARLA` is a simulation environment whose
    binaries work as expected. Let’s move on to installing the Gym environment for
    it. Leave the terminal running as is, since we need the binary to be running in
    the background throughout this exercise.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已验证`CARLA`是一个仿真环境，其二进制文件按预期工作。让我们继续为其安装Gym环境。保持终端运行状态不变，因为我们需要二进制文件在整个练习过程中后台运行。
- en: Installing the CARLA Gym environment
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装CARLA的Gym环境
- en: 'Since there is no official Gym environment, we will take advantage of a user-implemented
    GitHub repository and install the Gym environment for CARLA from there. Follow
    these steps to install CARLA’s Gym environment:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有官方的Gym环境，我们将利用用户实施的GitHub存储库，并从那里为CARLA安装Gym环境。按照以下步骤安装CARLA的Gym环境：
- en: 'Clone the Gym repository to a location of your choice and install the library:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆Gym存储库到您选择的位置并安装该库：
- en: '[PRE53]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Test your setup by running the following command:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令来测试您的设置：
- en: '[PRE54]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'A window similar to the following should open, showing that we have added a
    fake car to the environment. From here, we can monitor the top view, the LIDAR
    sensor point cloud, and our dashcam:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 应该会打开一个类似以下的窗口，显示我们已将一个虚拟汽车添加到环境中。从这里，我们可以监视俯视图，激光雷达传感器点云和我们的车载摄像头：
- en: '![A picture containing text, light  Description automatically generated](img/B18457_14_15.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本，包含文本的图片 说明](img/B18457_14_15.png)'
- en: 'Figure 14.15: Overview of the current episode'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.15：当前情节的概述
- en: 'Here, we can observe the following:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以观察到以下内容：
- en: The first view contains a view that is very similar to what vehicle GPS systems
    show in a car – that is, our vehicle, the various waypoints, and the road lanes.
    However, we shall not use this input for training, as it also shows other cars
    in the view, which is unrealistic.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个视图包含一个与车辆GPS系统显示非常相似的视图 - 即我们的车辆，各种航点和道路车道。然而，我们不会使用此输入进行训练，因为它还显示了视图中的其他车辆，这是不现实的。
- en: The second view is more interesting. Some consider it as the eye of a self-driving
    car. LIDAR emits pulsed light into the surrounding environment (in all directions),
    multiple times every second. It captures the reflected light to determine how
    far the nearest obstacle is in that direction. The onboard computer collates all
    the nearest obstacle information to recreate a 3D point cloud that gives it a
    3D understanding of its environment.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二视图更为有趣。有些人认为它就像是自动驾驶汽车的眼睛。激光雷达每秒多次向周围环境发射脉冲光（在所有方向），捕获反射光以确定该方向上最近的障碍物距离。车载计算机汇总所有最近的障碍物信息，以重建一个三维点云，使其能够三维理解其环境。
- en: In both the first and second views, we can see that there is a strip ahead of
    the car. This is a waypoint indication of where the car is supposed to go.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一和第二视图中，我们可以看到汽车前方有一条带状物。这是一个航向指示，指示汽车应该去的方向。
- en: The third view is a simple dashboard camera.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三视图是一个简单的仪表盘摄像头。
- en: 'Apart from these three, CARLA provides additional sensor data, such as the
    following:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三个外，CARLA 还提供其他传感器数据，例如以下内容：
- en: '`lateral-distance` (a deviation from the lane it should be in)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lateral-distance`（车道偏离距离）'
- en: '`delta-yaw` (an angle with respect to the road ahead)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta-yaw`（相对于前方道路的角度）'
- en: '`speed`'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`速度`'
- en: Whether there’s a hazardous obstacle in front of the vehicle
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆前方是否有危险障碍物
- en: We are going to use the first four sensors mentioned previously, along with
    LIDAR and our dashcam, to train the model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前面提到的前四个传感器以及 LIDAR 和我们的仪表盘摄像头来训练模型。
- en: We are now ready to understand the components of CARLA and create a DQN model
    for a self-driving car.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备理解 CARLA 的组件并为自动驾驶汽车创建 DQN 模型。
- en: Training a self-driving agent
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练自动驾驶代理
- en: We will create two files before we start the training process in a notebook
    – that is, `model.py` and `actor.py`. These will contain the model architecture
    and the `Agent` class, respectively. The `Agent` class contains the various methods
    we’ll use to train an agent.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中开始训练过程之前，我们将创建两个文件 —— `model.py` 和 `actor.py`。它们分别包含模型架构和 `Agent` 类。`Agent`
    类包含我们将用于训练代理的各种方法。
- en: The code instructions for this section are present in the `Carla.md` file in
    the `Chapter14` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 此部分的代码说明位于本书 GitHub 存储库的 `Chapter14` 文件夹中的 `Carla.md` 文件中，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Creating model.py
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 `model.py`
- en: 'This is going to be a PyTorch model that will accept the image that’s provided
    to it, as well as other sensor inputs. It will be expected to return the most
    likely action:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个 PyTorch 模型，它将接受提供给它的图像以及其他传感器输入。预计它将返回最可能的行动：
- en: '[PRE55]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let’s break down this code. As you can see, there are more types of data being
    fed into the `forward` method than in the previous sections, where we were just
    accepting an image as input:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解析这段代码。正如您所看到的，与之前只接受图像输入的部分不同，这里的 `forward` 方法中输入的数据类型更多：
- en: '`self.image_branch` expects the image coming from the dashcam of the car.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.image_branch` 期望来自汽车仪表盘摄像头的图像。'
- en: '`self.lidar_branch` accepts the image that’s generated by the LIDAR sensor.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.lidar_branch` 接受由 LIDAR 传感器生成的图像。'
- en: '`self.sensor_branch` accepts four sensor inputs in the form of a NumPy array.
    These four items are:'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.sensor_branch` 接受四个传感器输入，形式为 NumPy 数组。这四个项目是：'
- en: The lateral distance (a deviation from the lane it is supposed to be in)
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 横向距离（车道偏离距离）
- en: '`delta-yaw` (an angle with respect to the road ahead)'
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`delta-yaw`（相对于前方道路的角度）'
- en: Speed
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度
- en: The presence of any hazardous obstacles in front of the vehicle
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆前方是否存在任何危险障碍物
- en: See line number 544 in `gym_carla/envs/carla_env.py` (the repository that has
    been Git-cloned) for the same outputs. Using a different branch in the neural
    network will let the module provide different levels of importance for each sensor,
    and the outputs are summed up as the final output. Note that there are nine outputs;
    we will look at these later.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `gym_carla/envs/carla_env.py` 中看到第 544 行（已经进行了 Git 克隆的存储库）得到相同的输出。在神经网络的不同分支上，该模块将提供不同级别的传感器重要性，并将输出汇总为最终输出。注意，共有九个输出；我们稍后将对其进行查看。
- en: Creating actor.py
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 `actor.py`
- en: 'Much like the previous sections, we will use some code to store replay information
    and play it back when training is necessary:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几节类似，我们将使用一些代码来存储回放信息，并在需要时进行播放：
- en: 'Let’s get the imports and hyperparameters in place:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们把导入和超参数放在正确的位置：
- en: '[PRE56]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, we’ll initialize the target and local networks. No changes have been
    made to the code from the previous section here, except for the module that is
    imported:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化目标网络和本地网络。在这里与上一节相比，代码没有做任何更改，只是导入的模块不同：
- en: '[PRE57]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Since there are more sensors to handle, we’ll transport them as a dictionary
    of state. The state contains the `''image''`, `''lidar''`, and `''sensor''` keys,
    which we introduced in the previous section. We perform preprocessing before sending
    them to the neural network, as shown in the following code:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于有更多传感器需要处理，我们将它们作为状态字典传输。状态包含了在前一节介绍的`'image'`、`'lidar'`和`'sensor'`键。在将它们发送给神经网络之前，我们进行预处理，如下面的代码所示：
- en: '[PRE58]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, we need to fetch items from replay memory. The following instructions
    are executed:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要从重放内存中提取项目。执行以下指令：
- en: Obtain a batch of current and next states.
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获得一批当前状态和下一个状态。
- en: Compute the expected reward, `Q_expected`, if a network performs actions in
    the current state.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算网络在当前状态下执行操作时的预期奖励，`Q_expected`。
- en: Compare it with the target reward, `Q_targets`, that will have been obtained
    when the next state was fed to the network.
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其与目标奖励`Q_targets`进行比较，当下一个状态被送入网络时将获得该奖励。
- en: Periodically update the target network with the local network.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定期使用本地网络更新目标网络。
- en: 'Here is the code that can be used to achieve this:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是用于实现此目的的代码：
- en: '[PRE59]'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The only major change in the `ReplayBuffer` class is going to be how the data
    is stored. Since we have multiple sensors, each memory state (both the current
    and the next state) is stored as a tuple of data – that is, `states = [images,
    lidars, sensors]`:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`ReplayBuffer`类中唯一的主要更改是数据存储方式。由于我们有多个传感器，每个内存状态（当前状态和下一个状态）都存储为数据元组 - 即`states
    = [images, lidars, sensors]`：
- en: '[PRE60]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Note that the lines of code in bold fetch the current states, actions, rewards,
    and next states’ information.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，粗体代码行获取当前状态、动作、奖励和下一个状态的信息。
- en: Now that the critical components are in place, let’s load the Gym environment
    into a Python notebook and start training.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 现在关键的组件都已就位，让我们将Gym环境加载到Python笔记本中并开始训练。
- en: Training a DQN with fixed targets
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用固定目标训练DQN
- en: 'There is no additional theory we need to learn here. The basics remain the
    same; we’ll only make changes to the Gym environment, the architecture of the
    neural network, and the actions our agent needs to take:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们没有额外的理论需要学习。基础知识保持不变；我们只会对Gym环境、神经网络的架构以及我们的代理需要采取的行动进行更改：
- en: 'First, load the hyperparameters associated with the environment. Refer to each
    comment beside every key-value pair presented in the `params` dictionary in the
    following code. Since we will simulate a complex environment, we need to choose
    the environment’s parameters, such as the number of cars in the city, the number
    of walkers, which town to simulate, the resolution of the dashcam image, and the
    LIDAR sensors:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，加载与环境相关的超参数。在以下代码中，每个键值对旁边的每个注释都很重要。因为我们将模拟一个复杂的环境，我们需要选择环境的参数，如城市中的汽车数量、行人数量、模拟哪个城镇、仪表摄像头图像的分辨率和LIDAR传感器：
- en: '[PRE61]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'In the preceding `params` dictionary, the following are important for our simulation
    in terms of the action space:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述`params`字典中，对于我们模拟中的动作空间，以下内容很重要：
- en: '`''discrete'': True`: Our actions lie in a discrete space.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''discrete'': True`：我们的动作位于一个离散空间中。'
- en: '`''discrete_acc'':[-1,0,1]`: All the possible accelerations that the self-driven
    car is allowed to make during the simulation.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''discrete_acc'':[-1,0,1]`：自驾车在模拟过程中可以进行的所有可能的加速度。'
- en: '`''discrete_steer'':[-0.3,0,0.3]`: All the possible steering magnitudes that
    the self-driven car is allowed to make during the simulation.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''discrete_steer'':[-0.3,0,0.3]`：自驾车在模拟过程中可以进行的所有可能的转向幅度。'
- en: As you can see, the `discrete_acc` and `discrete_steer` lists contain three
    items each. This means that there are 3 x 3 possible unique actions the car can
    take. So, the network in the `model.py` file has nine discrete states.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`discrete_acc`和`discrete_steer`列表各包含三个项目。这意味着汽车可以采取9种唯一的离散动作。因此，`model.py`文件中的网络有九个离散状态。
- en: Feel free to change the parameters once you’ve gone through the official documentation.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您浏览了官方文档，可以随意更改参数。
- en: 'With that, we have all the components we need to train the model. Load a pre-trained
    model, if one exists. If we are starting from scratch, keep it as `None`:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些，我们就有了训练模型所需的所有组件。如果存在预训练模型，请加载它。如果我们从头开始，将其保持为`None`：
- en: '[PRE62]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Fix the number of episodes, and define the `dqn` function to train the agent,
    as follows:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 固定episode数量，并定义`dqn`函数来训练代理，如下所示：
- en: 'Reset the state:'
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置状态：
- en: '[PRE63]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Wrap the state into a dictionary (as discussed in the `actor.py:Actor` class)
    and act on it:'
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将状态封装到字典中（如在`actor.py:Actor`类中讨论的），并对其进行操作：
- en: '[PRE64]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Store the next state that’s obtained from the environment, and then store the
    `state, next_state` pair (along with the rewards and other state information)
    to train the actor using a DQN:'
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储从环境中获得的下一个状态，然后存储`state, next_state`对（以及奖励和其他状态信息），以使用DQN来训练actor：
- en: '[PRE65]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We must repeat the loop until we get a done signal, after which we reset the
    environment and start storing actions once again. After every 100 episodes, store
    the model.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须重复这个循环，直到收到完成信号，然后重置环境并重新开始存储行动。每隔100个episode，存储模型。
- en: 'Call the `dqn` function to train the model:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`dqn`函数来训练模型：
- en: '[PRE66]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Since this is a more complex environment, training can take a few days, so
    be patient and continue training a few hours at a time using the `load_path` and
    `save_path` arguments. With enough training, the vehicle can maneuver and learn
    how to drive by itself. Here’s a video of the training result we were able to
    achieve after two days of training: [https://tinyurl.com/mcvp-self-driving-agent-result](https://tinyurl.com/mcvp-self-driving-agent-result).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个更复杂的环境，训练可能需要几天时间，因此请耐心，并继续使用`load_path`和`save_path`参数每次训练几个小时。通过足够的训练，车辆可以学会如何自主驾驶。这是我们在两天训练后能够达到的训练结果视频：[https://tinyurl.com/mcvp-self-driving-agent-result](https://tinyurl.com/mcvp-self-driving-agent-result)。
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we learned how the values of various actions in a given state
    are calculated. We then learned how the agent updates the Q-table, using the discounted
    value of taking an action in a given state. In the process of doing this, we learned
    how the Q-table is infeasible in a scenario where the number of states is high.
    We also learned how to leverage deep Q-networks to address the scenario where
    the number of possible states is high. Then, we moved on to leveraging CNN-based
    neural networks while building an agent that learned how to play Pong, using a
    DQN based on fixed targets. Finally, we learned how to leverage a DQN with fixed
    targets to perform self-driving, using the CARLA simulator.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何计算给定状态下各种行动的价值。然后，我们学习了代理如何更新Q表，使用折扣值来执行给定状态下的行动。在此过程中，我们了解了在状态数量较多的情况下，Q表是不可行的。我们还学习了如何利用深度Q网络来解决可能状态数量较高的情况。接着，我们利用基于固定目标的DQN，结合CNN-based神经网络来建立一个学习如何玩乒乓球的代理。最后，我们学习了如何利用具有固定目标的DQN来进行自动驾驶，使用CARLA模拟器。
- en: As we have seen repeatedly in this chapter, you can use deep Q-learning to learn
    very different tasks – such as CartPole balancing, playing Pong, and self-driving
    navigation – with almost the same code. While this is not the end of our journey
    into exploring RL, at this point, we should be able to appreciate how we can use
    CNN-based and reinforcement learning-based algorithms together to solve complex
    problems and build learning agents.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中多次看到的那样，您可以使用深度Q学习来学习非常不同的任务，比如CartPole平衡、玩乒乓球和自动驾驶导航，几乎使用相同的代码。虽然这不是我们探索RL旅程的终点，但在这一点上，我们应该能够理解如何将基于CNN和基于强化学习的算法结合起来解决复杂问题并构建学习代理。
- en: So far, we have learned how to combine computer vision-based techniques with
    techniques from other prominent areas of research, including meta-learning, natural
    language processing, and reinforcement learning. Apart from this, we’ve also learned
    how to perform object classification, detection, segmentation, and image generation
    using GANs. In the next chapter, we will switch gears and learn how to move a
    deep learning model into production.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们学习了如何将基于计算机视觉的技术与其他重要研究领域的技术结合起来，包括元学习、自然语言处理和强化学习。除此之外，我们还学习了如何使用GAN进行对象分类、检测、分割和图像生成。在下一章中，我们将转向学习如何将深度学习模型投入到生产中。
- en: Questions
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How does an agent calculate the value of a given state?
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理如何计算给定状态的价值？
- en: How is a Q-table populated?
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何填充Q表？
- en: Why do we have a discount factor in a state-action value calculation?
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在状态-行动价值计算中需要折扣因子？
- en: Why do we need the exploration-exploitation strategy?
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么需要探索-利用策略？
- en: Why do we need to use deep Q-learning?
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么需要使用深度Q学习？
- en: How is the value of a given state-action combination calculated using deep Q-learning?
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用深度 Q 学习计算给定状态-动作组合的值？
- en: Once an agent has maximized a reward in the CartPole environment, is there a
    chance that it can learn a suboptimal policy later?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦代理在 CartPole 环境中最大化了奖励，是否有可能后来学习到次优策略？
- en: Learn more on Discord
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
