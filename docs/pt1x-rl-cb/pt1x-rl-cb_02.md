# é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹å’ŒåŠ¨æ€è§„åˆ’

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡è§‚å¯Ÿ**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**ï¼ˆMDPsï¼‰å’ŒåŠ¨æ€è§„åˆ’æ¥ç»§ç»­æˆ‘ä»¬çš„å®žè·µå¼ºåŒ–å­¦ä¹ æ—…ç¨‹ã€‚æœ¬ç« å°†ä»Žåˆ›å»ºé©¬å°”å¯å¤«é“¾å’Œ MDP å¼€å§‹ï¼Œè¿™æ˜¯å¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ ¸å¿ƒã€‚æ‚¨è¿˜å°†é€šè¿‡å®žè·µç­–ç•¥è¯„ä¼°æ›´åŠ ç†Ÿæ‚‰è´å°”æ›¼æ–¹ç¨‹ã€‚ç„¶åŽæˆ‘ä»¬å°†ç»§ç»­å¹¶åº”ç”¨ä¸¤ç§æ–¹æ³•è§£å†³ MDP é—®é¢˜ï¼šå€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£ã€‚æˆ‘ä»¬å°†ä»¥ FrozenLake çŽ¯å¢ƒä½œä¸ºç¤ºä¾‹ã€‚åœ¨æœ¬ç« çš„æœ€åŽï¼Œæˆ‘ä»¬å°†é€æ­¥å±•ç¤ºå¦‚ä½•ä½¿ç”¨åŠ¨æ€è§„åˆ’è§£å†³æœ‰è¶£çš„ç¡¬å¸æŠ›æŽ·èµŒåšé—®é¢˜ã€‚

æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ç¤ºä¾‹ï¼š

+   åˆ›å»ºé©¬å°”å¯å¤«é“¾

+   åˆ›å»ºä¸€ä¸ª MDP

+   æ‰§è¡Œç­–ç•¥è¯„ä¼°

+   æ¨¡æ‹Ÿ FrozenLake çŽ¯å¢ƒ

+   ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ MDP

+   ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³ MDP

+   ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ MDP

# æŠ€æœ¯è¦æ±‚

è¦æˆåŠŸæ‰§è¡Œæœ¬ç« ä¸­çš„ç¤ºä¾‹ï¼Œè¯·ç¡®ä¿ç³»ç»Ÿä¸­å®‰è£…äº†ä»¥ä¸‹ç¨‹åºï¼š

+   Python 3.6, 3.7 æˆ–æ›´é«˜ç‰ˆæœ¬

+   Anaconda

+   PyTorch 1.0 æˆ–æ›´é«˜ç‰ˆæœ¬

+   OpenAI Gym

# åˆ›å»ºé©¬å°”å¯å¤«é“¾

è®©æˆ‘ä»¬ä»Žåˆ›å»ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾å¼€å§‹ï¼Œä»¥ä¾¿äºŽå¼€å‘ MDPã€‚

é©¬å°”å¯å¤«é“¾æè¿°äº†éµå®ˆ**é©¬å°”å¯å¤«æ€§è´¨**çš„äº‹ä»¶åºåˆ—ã€‚å®ƒç”±ä¸€ç»„å¯èƒ½çš„çŠ¶æ€ *S = {s0, s1, ... , sm}* å’Œè½¬ç§»çŸ©é˜µ *T(s, s')* å®šä¹‰ï¼Œå…¶ä¸­åŒ…å«çŠ¶æ€ *s* è½¬ç§»åˆ°çŠ¶æ€ *s'* çš„æ¦‚çŽ‡ã€‚æ ¹æ®é©¬å°”å¯å¤«æ€§è´¨ï¼Œè¿‡ç¨‹çš„æœªæ¥çŠ¶æ€ï¼Œåœ¨ç»™å®šå½“å‰çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œä¸Žè¿‡åŽ»çŠ¶æ€æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚æ¢å¥è¯è¯´ï¼Œè¿‡ç¨‹åœ¨ *t+1* æ—¶åˆ»çš„çŠ¶æ€ä»…ä¾èµ–äºŽ *t* æ—¶åˆ»çš„çŠ¶æ€ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥å­¦ä¹ å’Œç¡çœ è¿‡ç¨‹ä¸ºä¾‹ï¼ŒåŸºäºŽä¸¤ä¸ªçŠ¶æ€ *s0*ï¼ˆå­¦ä¹ ï¼‰å’Œ *s1*ï¼ˆç¡çœ ï¼‰ï¼Œåˆ›å»ºäº†ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹è½¬ç§»çŸ©é˜µï¼š

![](img/4bb4c9e3-4d16-402a-af8a-a586d8db69a1.png)

åœ¨æŽ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—ç»è¿‡ k æ­¥åŽçš„è½¬ç§»çŸ©é˜µï¼Œä»¥åŠåœ¨åˆå§‹çŠ¶æ€åˆ†å¸ƒï¼ˆå¦‚ *[0.7, 0.3]*ï¼Œè¡¨ç¤ºæœ‰ 70% çš„æ¦‚çŽ‡ä»Žå­¦ä¹ å¼€å§‹ï¼Œ30% çš„æ¦‚çŽ‡ä»Žç¡çœ å¼€å§‹ï¼‰ä¸‹å„ä¸ªçŠ¶æ€çš„æ¦‚çŽ‡ã€‚

# å¦‚ä½•åš...

è¦ä¸ºå­¦ä¹  - ç¡çœ è¿‡ç¨‹åˆ›å»ºä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œå¹¶å¯¹å…¶è¿›è¡Œä¸€äº›åˆ†æžï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1.  å¯¼å…¥åº“å¹¶å®šä¹‰è½¬ç§»çŸ©é˜µï¼š

```py
>>> import torch
>>> T = torch.tensor([[0.4, 0.6],
...                   [0.8, 0.2]])
```

1.  è®¡ç®—ç»è¿‡ k æ­¥åŽçš„è½¬ç§»æ¦‚çŽ‡ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥ k = `2`, `5`, `10`, `15`, å’Œ `20` ä¸ºä¾‹ï¼š

```py
>>> T_2 = torch.matrix_power(T, 2)
>>> T_5 = torch.matrix_power(T, 5)
>>> T_10 = torch.matrix_power(T, 10)
>>> T_15 = torch.matrix_power(T, 15)
>>> T_20 = torch.matrix_power(T, 20)
```

1.  å®šä¹‰ä¸¤ä¸ªçŠ¶æ€çš„åˆå§‹åˆ†å¸ƒï¼š

```py
>>> v = torch.tensor([[0.7, 0.3]])
```

1.  åœ¨*æ­¥éª¤ 2*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç»è¿‡ k = `1`, `2`, `5`, `10`, `15`, å’Œ `20` æ­¥åŽçš„è½¬ç§»æ¦‚çŽ‡ï¼Œç»“æžœå¦‚ä¸‹ï¼š

```py
>>> v_1 = torch.mm(v, T)
>>> v_2 = torch.mm(v, T_2)
>>> v_5 = torch.mm(v, T_5)
>>> v_10 = torch.mm(v, T_10)
>>> v_15 = torch.mm(v, T_15)
>>> v_20 = torch.mm(v, T_20)
```

# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨*æ­¥éª¤ 2*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº†ç»è¿‡ k æ­¥åŽçš„è½¬ç§»æ¦‚çŽ‡ï¼Œå³è½¬ç§»çŸ©é˜µçš„ k æ¬¡å¹‚ã€‚ç»“æžœå¦‚ä¸‹ï¼š

```py
>>> print("Transition probability after 2 steps:\n{}".format(T_2))
Transition probability after 2 steps:
tensor([[0.6400, 0.3600],
 [0.4800, 0.5200]])
>>> print("Transition probability after 5 steps:\n{}".format(T_5))
Transition probability after 5 steps:
tensor([[0.5670, 0.4330],
 [0.5773, 0.4227]])
>>> print(
"Transition probability after 10 steps:\n{}".format(T_10))
Transition probability after 10 steps:
tensor([[0.5715, 0.4285],
 [0.5714, 0.4286]])
>>> print(
"Transition probability after 15 steps:\n{}".format(T_15))
Transition probability after 15 steps:
tensor([[0.5714, 0.4286],
 [0.5714, 0.4286]])
>>> print(
"Transition probability after 20 steps:\n{}".format(T_20))
Transition probability after 20 steps:
tensor([[0.5714, 0.4286],
 [0.5714, 0.4286]])
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡ 10 åˆ° 15 æ­¥ï¼Œè¿‡æ¸¡æ¦‚çŽ‡ä¼šæ”¶æ•›ã€‚è¿™æ„å‘³ç€æ— è®ºè¿‡ç¨‹å¤„äºŽä»€ä¹ˆçŠ¶æ€ï¼Œè½¬ç§»åˆ° s0ï¼ˆ57.14%ï¼‰å’Œ s1ï¼ˆ42.86%ï¼‰çš„æ¦‚çŽ‡éƒ½ç›¸åŒã€‚

åœ¨*æ­¥éª¤ 4*ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—äº† k = `1`ï¼Œ`2`ï¼Œ`5`ï¼Œ`10`ï¼Œ`15`å’Œ`20`æ­¥åŽçš„çŠ¶æ€åˆ†å¸ƒï¼Œè¿™æ˜¯åˆå§‹çŠ¶æ€åˆ†å¸ƒå’Œè¿‡æ¸¡æ¦‚çŽ‡çš„ä¹˜ç§¯ã€‚æ‚¨å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°ç»“æžœï¼š

```py
>>> print("Distribution of states after 1 step:\n{}".format(v_1))
Distribution of states after 1 step:
tensor([[0.5200, 0.4800]])
>>> print("Distribution of states after 2 steps:\n{}".format(v_2))
Distribution of states after 2 steps:
tensor([[0.5920, 0.4080]])
>>> print("Distribution of states after 5 steps:\n{}".format(v_5))
Distribution of states after 5 steps:
tensor([[0.5701, 0.4299]])
>>> print(
 "Distribution of states after 10 steps:\n{}".format(v_10))
Distribution of states after 10 steps:
tensor([[0.5714, 0.4286]])
>>> print(
 "Distribution of states after 15 steps:\n{}".format(v_15))
Distribution of states after 15 steps:
tensor([[0.5714, 0.4286]])
>>> print(
 "Distribution of states after 20 steps:\n{}".format(v_20))
Distribution of states after 20 steps:
tensor([[0.5714, 0.4286]])
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡ 10 æ­¥åŽï¼ŒçŠ¶æ€åˆ†å¸ƒä¼šæ”¶æ•›ã€‚é•¿æœŸå†…å¤„äºŽ s0ï¼ˆ57.14%ï¼‰å’Œ s1ï¼ˆ42.86%ï¼‰çš„æ¦‚çŽ‡ä¿æŒä¸å˜ã€‚

ä»Ž[0.7, 0.3]å¼€å§‹ï¼Œç»è¿‡ä¸€æ¬¡è¿­ä»£åŽçš„çŠ¶æ€åˆ†å¸ƒå˜ä¸º[0.52, 0.48]ã€‚å…¶è¯¦ç»†è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](img/19ed17b5-c90e-42d9-92f8-adbd951bb10b.png)

ç»è¿‡å¦ä¸€æ¬¡è¿­ä»£ï¼ŒçŠ¶æ€åˆ†å¸ƒå¦‚ä¸‹[0.592, 0.408]ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºè®¡ç®—ï¼š

![](img/4e6a9d0f-ddda-4d73-b19b-2f3b56883a05.png)

éšç€æ—¶é—´çš„æŽ¨ç§»ï¼ŒçŠ¶æ€åˆ†å¸ƒè¾¾åˆ°å¹³è¡¡ã€‚

# è¿˜æœ‰æ›´å¤š...

äº‹å®žä¸Šï¼Œæ— è®ºåˆå§‹çŠ¶æ€å¦‚ä½•ï¼ŒçŠ¶æ€åˆ†å¸ƒéƒ½å°†å§‹ç»ˆæ”¶æ•›åˆ°[0.5714, 0.4286]ã€‚æ‚¨å¯ä»¥å°è¯•å…¶ä»–åˆå§‹åˆ†å¸ƒï¼Œä¾‹å¦‚[0.2, 0.8]å’Œ[1, 0]ã€‚åˆ†å¸ƒåœ¨ç»è¿‡ 10 æ­¥åŽä»å°†ä¿æŒä¸º[0.5714, 0.4286]ã€‚

é©¬å°”å¯å¤«é“¾ä¸ä¸€å®šä¼šæ”¶æ•›ï¼Œç‰¹åˆ«æ˜¯å½“åŒ…å«çž¬æ€æˆ–å½“å‰çŠ¶æ€æ—¶ã€‚ä½†å¦‚æžœå®ƒç¡®å®žæ”¶æ•›ï¼Œæ— è®ºèµ·å§‹åˆ†å¸ƒå¦‚ä½•ï¼Œå®ƒå°†è¾¾åˆ°ç›¸åŒçš„å¹³è¡¡ã€‚

# å¦è§

å¦‚æžœæ‚¨æƒ³é˜…è¯»æ›´å¤šå…³äºŽé©¬å°”å¯å¤«é“¾çš„å†…å®¹ï¼Œä»¥ä¸‹æ˜¯ä¸¤ç¯‡å…·æœ‰è‰¯å¥½å¯è§†åŒ–æ•ˆæžœçš„åšå®¢æ–‡ç« ï¼š

+   [`brilliant.org/wiki/markov-chains/`](https://brilliant.org/wiki/markov-chains/)

+   [`setosa.io/ev/markov-chains/`](http://setosa.io/ev/markov-chains/)

# åˆ›å»º MDP

åŸºäºŽé©¬å°”å¯å¤«é“¾çš„å‘å±•ï¼ŒMDP æ¶‰åŠä»£ç†å’Œå†³ç­–è¿‡ç¨‹ã€‚è®©æˆ‘ä»¬ç»§ç»­å‘å±•ä¸€ä¸ª MDPï¼Œå¹¶è®¡ç®—æœ€ä¼˜ç­–ç•¥ä¸‹çš„å€¼å‡½æ•°ã€‚

é™¤äº†ä¸€ç»„å¯èƒ½çš„çŠ¶æ€ï¼Œ*S = {s0, s1, ... , sm}*ï¼ŒMDP ç”±ä¸€ç»„åŠ¨ä½œï¼Œ*A = {a0, a1, ... , an}*ï¼›è¿‡æ¸¡æ¨¡åž‹ï¼Œ*T(s, a, s')*ï¼›å¥–åŠ±å‡½æ•°ï¼Œ*R(s)*ï¼›å’ŒæŠ˜çŽ°å› å­ð²å®šä¹‰ã€‚è¿‡æ¸¡çŸ©é˜µï¼Œ*T(s, a, s')*ï¼ŒåŒ…å«ä»ŽçŠ¶æ€ s é‡‡å–åŠ¨ä½œ a ç„¶åŽè½¬ç§»åˆ° s'çš„æ¦‚çŽ‡ã€‚æŠ˜çŽ°å› å­ð²æŽ§åˆ¶æœªæ¥å¥–åŠ±å’Œå³æ—¶å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ã€‚

ä¸ºäº†ä½¿æˆ‘ä»¬çš„ MDP ç¨å¾®å¤æ‚åŒ–ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å’Œç¡çœ è¿‡ç¨‹å»¶ä¼¸åˆ°å¦ä¸€ä¸ªçŠ¶æ€ï¼Œ`s2 play` æ¸¸æˆã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªåŠ¨ä½œï¼Œ`a0 work` å’Œ `a1 slack`ã€‚*3 * 2 * 3* è¿‡æ¸¡çŸ©é˜µ *T(s, a, s')* å¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/c142bd78-673a-4dc7-a222-014889c5cc5f.png)

è¿™æ„å‘³ç€ï¼Œä¾‹å¦‚ï¼Œå½“ä»ŽçŠ¶æ€ s0 study ä¸­é‡‡å– a1 slack è¡ŒåŠ¨æ—¶ï¼Œæœ‰ 60%çš„æœºä¼šå®ƒå°†å˜æˆ s1 sleepï¼ˆå¯èƒ½ä¼šç´¯ï¼‰ï¼Œæœ‰ 30%çš„æœºä¼šå®ƒå°†å˜æˆ s2 play gamesï¼ˆå¯èƒ½æƒ³æ”¾æ¾ï¼‰ï¼Œè¿˜æœ‰ 10%çš„æœºä¼šç»§ç»­å­¦ä¹ ï¼ˆå¯èƒ½æ˜¯çœŸæ­£çš„å·¥ä½œç‹‚ï¼‰ã€‚æˆ‘ä»¬ä¸ºä¸‰ä¸ªçŠ¶æ€å®šä¹‰å¥–åŠ±å‡½æ•°ä¸º[+1, 0, -1]ï¼Œä»¥è¡¥å¿è¾›å‹¤å·¥ä½œã€‚æ˜¾ç„¶ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ**æœ€ä¼˜ç­–ç•¥**æ˜¯åœ¨æ¯ä¸ªæ­¥éª¤é€‰æ‹© a0 å·¥ä½œï¼ˆç»§ç»­å­¦ä¹ â€”â€”ä¸åŠªåŠ›å°±æ²¡æœ‰æ”¶èŽ·ï¼Œå¯¹å§ï¼Ÿï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é€‰æ‹© 0.5 ä½œä¸ºèµ·å§‹æŠ˜æ‰£å› å­ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¡ç®—**çŠ¶æ€å€¼å‡½æ•°**ï¼ˆä¹Ÿç§°ä¸º**å€¼å‡½æ•°**ï¼Œç®€ç§°**å€¼**æˆ–**æœŸæœ›æ•ˆç”¨**ï¼‰åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹çš„å€¼ã€‚

# å¦‚ä½•åš...

åˆ›å»º MDP å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤å®Œæˆï¼š

1.  å¯¼å…¥ PyTorch å¹¶å®šä¹‰è½¬ç§»çŸ©é˜µï¼š

```py
 >>> import torch
 >>> T = torch.tensor([[[0.8, 0.1, 0.1],
 ...                    [0.1, 0.6, 0.3]],
 ...                   [[0.7, 0.2, 0.1],
 ...                    [0.1, 0.8, 0.1]],
 ...                   [[0.6, 0.2, 0.2],
 ...                    [0.1, 0.4, 0.5]]]
 ...                  )
```

1.  å®šä¹‰å¥–åŠ±å‡½æ•°å’ŒæŠ˜æ‰£å› å­ï¼š

```py
 >>> R = torch.tensor([1., 0, -1.])
 >>> gamma = 0.5
```

1.  åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€ä¼˜ç­–ç•¥æ˜¯åœ¨æ‰€æœ‰æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ`a0`ï¼š

```py
>>> action = 0
```

1.  æˆ‘ä»¬ä½¿ç”¨**çŸ©é˜µæ±‚é€†**æ–¹æ³•è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„å€¼`V`ï¼š

```py
 >>> def cal_value_matrix_inversion(gamma, trans_matrix, rewards):
 ...     inv = torch.inverse(torch.eye(rewards.shape[0]) 
 - gamma * trans_matrix)
 ...     V = torch.mm(inv, rewards.reshape(-1, 1))
 ...     return V
```

æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­å±•ç¤ºå¦‚ä½•æŽ¨å¯¼ä¸‹ä¸€ä¸ªéƒ¨åˆ†çš„å€¼ã€‚

1.  æˆ‘ä»¬å°†æ‰€æœ‰å˜é‡è¾“å…¥å‡½æ•°ä¸­ï¼ŒåŒ…æ‹¬ä¸ŽåŠ¨ä½œ`a0`ç›¸å…³çš„è½¬ç§»æ¦‚çŽ‡ï¼š

```py
 >>> trans_matrix = T[:, action]
 >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)
 >>> print("The value function under the optimal 
 policy is:\n{}".format(V))
 The value function under the optimal policy is:
 tensor([[ 1.6787],
 [ 0.6260],
 [-0.4820]])
```

# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„...

åœ¨è¿™ä¸ªè¿‡äºŽç®€åŒ–çš„å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œæœ€ä¼˜ç­–ç•¥ï¼Œå³èŽ·å¾—æœ€é«˜æ€»å¥–åŠ±çš„ç­–ç•¥ï¼Œæ˜¯åœ¨æ‰€æœ‰æ­¥éª¤ä¸­é€‰æ‹©åŠ¨ä½œ a0ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæƒ…å†µä¸ä¼šé‚£ä¹ˆç®€å•ã€‚æ­¤å¤–ï¼Œä¸ªåˆ«æ­¥éª¤ä¸­é‡‡å–çš„è¡ŒåŠ¨ä¸ä¸€å®šç›¸åŒã€‚å®ƒä»¬é€šå¸¸ä¾èµ–äºŽçŠ¶æ€ã€‚å› æ­¤ï¼Œåœ¨å®žé™…æƒ…å†µä¸­ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸é€šè¿‡æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥æ¥è§£å†³ä¸€ä¸ª MDP é—®é¢˜ã€‚

ç­–ç•¥çš„å€¼å‡½æ•°è¡¡é‡äº†åœ¨éµå¾ªç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œå¯¹äºŽä¸€ä¸ª agent è€Œè¨€å¤„äºŽæ¯ä¸ªçŠ¶æ€çš„å¥½å¤„ã€‚å€¼è¶Šå¤§ï¼ŒçŠ¶æ€è¶Šå¥½ã€‚

åœ¨*ç¬¬ 4 æ­¥*ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨**çŸ©é˜µæ±‚é€†æ³•**è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„å€¼`V`ã€‚æ ¹æ®**è´å°”æ›¼æ–¹ç¨‹**ï¼Œæ­¥éª¤*t+1*çš„å€¼ä¸Žæ­¥éª¤*t*çš„å€¼ä¹‹é—´çš„å…³ç³»å¯ä»¥è¡¨è¾¾å¦‚ä¸‹ï¼š

![](img/56fc727f-bb72-4413-8ebf-104b07f358b8.png)

å½“å€¼æ”¶æ•›æ—¶ï¼Œä¹Ÿå°±æ˜¯*Vt+1 = Vt*æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æŽ¨å¯¼å‡ºå€¼`V`ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![](img/7f19bce0-3a09-4649-87f8-f2e13badfd54.png)

è¿™é‡Œï¼Œ*I*æ˜¯å…·æœ‰ä¸»å¯¹è§’çº¿ä¸Šçš„ 1 çš„å•ä½çŸ©é˜µã€‚

ä½¿ç”¨çŸ©é˜µæ±‚é€†è§£å†³ MDP çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯ä½ æ€»æ˜¯å¾—åˆ°ä¸€ä¸ªç¡®åˆ‡çš„ç­”æ¡ˆã€‚ä½†å…¶å¯æ‰©å±•æ€§æœ‰é™ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦è®¡ç®—ä¸€ä¸ª m * m çŸ©é˜µçš„æ±‚é€†ï¼ˆå…¶ä¸­*m*æ˜¯å¯èƒ½çš„çŠ¶æ€æ•°é‡ï¼‰ï¼Œå¦‚æžœæœ‰å¤§é‡çŠ¶æ€ï¼Œè®¡ç®—æˆæœ¬ä¼šå˜å¾—å¾ˆé«˜æ˜‚ã€‚

# è¿˜æœ‰æ›´å¤š...

æˆ‘ä»¬å†³å®šå°è¯•ä¸åŒçš„æŠ˜æ‰£å› å­å€¼ã€‚è®©æˆ‘ä»¬ä»Ž 0 å¼€å§‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åªå…³å¿ƒå³æ—¶å¥–åŠ±ï¼š

```py
 >>> gamma = 0
 >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)
 >>> print("The value function under the optimal policy is:\n{}".format(V))
 The value function under the optimal policy is:
 tensor([[ 1.],
 [ 0.],
 [-1.]])
```

è¿™ä¸Žå¥–åŠ±å‡½æ•°ä¸€è‡´ï¼Œå› ä¸ºæˆ‘ä»¬åªçœ‹ä¸‹ä¸€æ­¥çš„å¥–åŠ±ã€‚

éšç€æŠ˜çŽ°å› å­å‘ 1 é æ‹¢ï¼Œæœªæ¥çš„å¥–åŠ±è¢«è€ƒè™‘ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ ð²=0.99ï¼š

```py
 >>> gamma = 0.99
 >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)
 >>> print("The value function under the optimal policy is:\n{}".format(V))
 The value function under the optimal policy is:
 tensor([[65.8293],
 [64.7194],
 [63.4876]])
```

# å¦è¯·å‚é˜…

è¿™ä¸ªé€ŸæŸ¥è¡¨ï¼Œ[`cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html`](https://cs-cheatsheet.readthedocs.io/en/latest/subjects/ai/mdp.html)ï¼Œä½œä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹çš„å¿«é€Ÿå‚è€ƒã€‚

# æ‰§è¡Œç­–ç•¥è¯„ä¼°

æˆ‘ä»¬åˆšåˆšå¼€å‘äº†ä¸€ä¸ªé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨çŸ©é˜µæ±‚é€†è®¡ç®—äº†æœ€ä¼˜ç­–ç•¥çš„å€¼å‡½æ•°ã€‚æˆ‘ä»¬è¿˜æåˆ°äº†é€šè¿‡æ±‚é€†å¤§åž‹ m * m çŸ©é˜µï¼ˆä¾‹å¦‚ 1,000ã€10,000 æˆ– 100,000ï¼‰çš„é™åˆ¶ã€‚åœ¨è¿™ä¸ªæ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•ï¼Œç§°ä¸º**ç­–ç•¥è¯„ä¼°**ã€‚

ç­–ç•¥è¯„ä¼°æ˜¯ä¸€ä¸ªè¿­ä»£ç®—æ³•ã€‚å®ƒä»Žä»»æ„çš„ç­–ç•¥å€¼å¼€å§‹ï¼Œç„¶åŽæ ¹æ®**è´å°”æ›¼æœŸæœ›æ–¹ç¨‹**è¿­ä»£æ›´æ–°å€¼ï¼Œç›´åˆ°æ”¶æ•›ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒçŠ¶æ€ *s* ä¸‹ç­–ç•¥ *Ï€* çš„å€¼æ›´æ–°å¦‚ä¸‹ï¼š

![](img/3f9f1117-0f84-4327-808c-1923adad27b8.png)

è¿™é‡Œï¼Œ*Ï€(s, a)* è¡¨ç¤ºåœ¨ç­–ç•¥ *Ï€* ä¸‹åœ¨çŠ¶æ€ *s* ä¸­é‡‡å–åŠ¨ä½œ *a* çš„æ¦‚çŽ‡ã€‚*T(s, a, s')* æ˜¯é€šè¿‡é‡‡å–åŠ¨ä½œ *a* ä»ŽçŠ¶æ€ *s* è½¬ç§»åˆ°çŠ¶æ€ *s'* çš„è½¬ç§»æ¦‚çŽ‡ï¼Œ*R(s, a)* æ˜¯åœ¨çŠ¶æ€ *s* ä¸­é‡‡å–åŠ¨ä½œ *a* åŽèŽ·å¾—çš„å¥–åŠ±ã€‚

æœ‰ä¸¤ç§æ–¹æ³•æ¥ç»ˆæ­¢è¿­ä»£æ›´æ–°è¿‡ç¨‹ã€‚ä¸€ç§æ˜¯è®¾ç½®ä¸€ä¸ªå›ºå®šçš„è¿­ä»£æ¬¡æ•°ï¼Œæ¯”å¦‚ 1,000 å’Œ 10,000ï¼Œæœ‰æ—¶å¯èƒ½éš¾ä»¥æŽ§åˆ¶ã€‚å¦ä¸€ç§æ˜¯æŒ‡å®šä¸€ä¸ªé˜ˆå€¼ï¼ˆé€šå¸¸æ˜¯ 0.0001ã€0.00001 æˆ–ç±»ä¼¼çš„å€¼ï¼‰ï¼Œä»…åœ¨æ‰€æœ‰çŠ¶æ€çš„å€¼å˜åŒ–ç¨‹åº¦ä½ŽäºŽæŒ‡å®šçš„é˜ˆå€¼æ—¶ç»ˆæ­¢è¿‡ç¨‹ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ ¹æ®æœ€ä¼˜ç­–ç•¥å’Œéšæœºç­–ç•¥å¯¹å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹æ‰§è¡Œç­–ç•¥è¯„ä¼°ã€‚

# å¦‚ä½•æ“ä½œ...

è®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªç­–ç•¥è¯„ä¼°ç®—æ³•ï¼Œå¹¶å°†å…¶åº”ç”¨äºŽæˆ‘ä»¬çš„å­¦ä¹ -ç¡çœ -æ¸¸æˆè¿‡ç¨‹å¦‚ä¸‹ï¼š

1.  å¯¼å…¥ PyTorch å¹¶å®šä¹‰è¿‡æ¸¡çŸ©é˜µï¼š

```py
 >>> import torch
 >>> T = torch.tensor([[[0.8, 0.1, 0.1],
 ...                    [0.1, 0.6, 0.3]],
 ...                   [[0.7, 0.2, 0.1],
 ...                    [0.1, 0.8, 0.1]],
 ...                   [[0.6, 0.2, 0.2],
 ...                    [0.1, 0.4, 0.5]]]
 ...                  )
```

1.  å®šä¹‰å¥–åŠ±å‡½æ•°å’ŒæŠ˜çŽ°å› å­ï¼ˆçŽ°åœ¨ä½¿ç”¨ `0.5`ï¼‰ï¼š

```py
 >>> R = torch.tensor([1., 0, -1.])
 >>> gamma = 0.5
```

1.  å®šä¹‰ç”¨äºŽç¡®å®šä½•æ—¶åœæ­¢è¯„ä¼°è¿‡ç¨‹çš„é˜ˆå€¼ï¼š

```py
 >>> threshold = 0.0001
```

1.  å®šä¹‰æœ€ä¼˜ç­–ç•¥ï¼Œå…¶ä¸­åœ¨æ‰€æœ‰æƒ…å†µä¸‹é€‰æ‹©åŠ¨ä½œ a0ï¼š

```py
 >>> policy_optimal = torch.tensor([[1.0, 0.0],
 ...                                [1.0, 0.0],
 ...                                [1.0, 0.0]])
```

1.  å¼€å‘ä¸€ä¸ªç­–ç•¥è¯„ä¼°å‡½æ•°ï¼ŒæŽ¥å—ä¸€ä¸ªç­–ç•¥ã€è¿‡æ¸¡çŸ©é˜µã€å¥–åŠ±ã€æŠ˜çŽ°å› å­å’Œé˜ˆå€¼ï¼Œå¹¶è®¡ç®— `value` å‡½æ•°ï¼š

```py
>>> def policy_evaluation(
 policy, trans_matrix, rewards, gamma, threshold):
...     """
...     Perform policy evaluation
...     @param policy: policy matrix containing actions and their 
 probability in each state
...     @param trans_matrix: transformation matrix
...     @param rewards: rewards for each state
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the given policy for all possible states
...     """
...     n_state = policy.shape[0]
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state, actions in enumerate(policy):
...             for action, action_prob in enumerate(actions):
...                 V_temp[state] += action_prob * (R[state] + 
 gamma * torch.dot(
 trans_matrix[state, action], V))
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

1.  çŽ°åœ¨è®©æˆ‘ä»¬æ’å…¥æœ€ä¼˜ç­–ç•¥å’Œæ‰€æœ‰å…¶ä»–å˜é‡ï¼š

```py
>>> V = policy_evaluation(policy_optimal, T, R, gamma, threshold)
>>> print(
 "The value function under the optimal policy is:\n{}".format(V)) The value function under the optimal policy is:
tensor([ 1.6786,  0.6260, -0.4821])
```

è¿™ä¸Žæˆ‘ä»¬ä½¿ç”¨çŸ©é˜µæ±‚é€†å¾—åˆ°çš„ç»“æžœå‡ ä¹Žç›¸åŒã€‚

1.  æˆ‘ä»¬çŽ°åœ¨å°è¯•å¦ä¸€ä¸ªç­–ç•¥ï¼Œä¸€ä¸ªéšæœºç­–ç•¥ï¼Œå…¶ä¸­åŠ¨ä½œä»¥ç›¸åŒçš„æ¦‚çŽ‡é€‰æ‹©ï¼š

```py
>>> policy_random = torch.tensor([[0.5, 0.5],
...                               [0.5, 0.5],
...                               [0.5, 0.5]])
```

1.  æ’å…¥éšæœºç­–ç•¥å’Œæ‰€æœ‰å…¶ä»–å˜é‡ï¼š

```py
>>> V = policy_evaluation(policy_random, T, R, gamma, threshold)
>>> print(
 "The value function under the random policy is:\n{}".format(V))
The value function under the random policy is:
tensor([ 1.2348,  0.2691, -0.9013])
```

# å·¥ä½œåŽŸç†...

æˆ‘ä»¬åˆšåˆšçœ‹åˆ°äº†ä½¿ç”¨ç­–ç•¥è¯„ä¼°è®¡ç®—ç­–ç•¥å€¼çš„æ•ˆæžœæœ‰å¤šä¹ˆæœ‰æ•ˆã€‚è¿™æ˜¯ä¸€ç§ç®€å•çš„æ”¶æ•›è¿­ä»£æ–¹æ³•ï¼Œåœ¨**åŠ¨æ€è§„åˆ’å®¶æ—**ä¸­ï¼Œæˆ–è€…æ›´å…·ä½“åœ°è¯´æ˜¯**è¿‘ä¼¼åŠ¨æ€è§„åˆ’**ã€‚å®ƒä»Žå¯¹å€¼çš„éšæœºçŒœæµ‹å¼€å§‹ï¼Œç„¶åŽæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è¿­ä»£æ›´æ–°ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ã€‚

åœ¨ç¬¬ 5 æ­¥ä¸­ï¼Œç­–ç•¥è¯„ä¼°å‡½æ•°æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

+   å°†ç­–ç•¥å€¼åˆå§‹åŒ–ä¸ºå…¨é›¶ã€‚

+   æ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹æ›´æ–°å€¼ã€‚

+   è®¡ç®—æ‰€æœ‰çŠ¶æ€ä¸­å€¼çš„æœ€å¤§å˜åŒ–ã€‚

+   å¦‚æžœæœ€å¤§å˜åŒ–å¤§äºŽé˜ˆå€¼ï¼Œåˆ™ç»§ç»­æ›´æ–°å€¼ã€‚å¦åˆ™ï¼Œç»ˆæ­¢è¯„ä¼°è¿‡ç¨‹å¹¶è¿”å›žæœ€æ–°çš„å€¼ã€‚

ç”±äºŽç­–ç•¥è¯„ä¼°ä½¿ç”¨è¿­ä»£é€¼è¿‘ï¼Œå…¶ç»“æžœå¯èƒ½ä¸Žä½¿ç”¨ç²¾ç¡®è®¡ç®—çš„çŸ©é˜µæ±‚é€†æ–¹æ³•çš„ç»“æžœä¸å®Œå…¨ç›¸åŒã€‚äº‹å®žä¸Šï¼Œæˆ‘ä»¬å¹¶ä¸çœŸçš„éœ€è¦ä»·å€¼å‡½æ•°é‚£ä¹ˆç²¾ç¡®ã€‚æ­¤å¤–ï¼Œå®ƒå¯ä»¥è§£å†³**ç»´åº¦è¯…å’’**é—®é¢˜ï¼Œè¿™å¯èƒ½å¯¼è‡´è®¡ç®—æ‰©å±•åˆ°æ•°ä»¥åƒè®¡çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸æ›´å–œæ¬¢ç­–ç•¥è¯„ä¼°è€Œä¸æ˜¯å…¶ä»–æ–¹æ³•ã€‚

è¿˜æœ‰ä¸€ä»¶äº‹è¦è®°ä½ï¼Œç­–ç•¥è¯„ä¼°ç”¨äºŽ**é¢„æµ‹**ç»™å®šç­–ç•¥çš„é¢„æœŸå›žæŠ¥æœ‰å¤šå¤§ï¼›å®ƒä¸ç”¨äºŽ**æŽ§åˆ¶**é—®é¢˜ã€‚

# è¿˜æœ‰æ›´å¤šå†…å®¹...

ä¸ºäº†æ›´ä»”ç»†åœ°è§‚å¯Ÿï¼Œæˆ‘ä»¬è¿˜ä¼šç»˜åˆ¶æ•´ä¸ªè¯„ä¼°è¿‡ç¨‹ä¸­çš„ç­–ç•¥å€¼ã€‚

åœ¨ `policy_evaluation` å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è®°å½•æ¯æ¬¡è¿­ä»£çš„å€¼ï¼š

```py
>>> def policy_evaluation_history(
 policy, trans_matrix, rewards, gamma, threshold):
...     n_state = policy.shape[0]
...     V = torch.zeros(n_state)
...     V_his = [V]
...     i = 0
...     while True:
...         V_temp = torch.zeros(n_state)
...         i += 1
...         for state, actions in enumerate(policy):
...             for action, action_prob in enumerate(actions):
...                 V_temp[state] += action_prob * (R[state] + gamma * 
 torch.dot(trans_matrix[state, action], V))
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         V_his.append(V)
...         if max_delta <= threshold:
...             break
...     return V, V_his
```

çŽ°åœ¨æˆ‘ä»¬å°† `policy_evaluation_history` å‡½æ•°åº”ç”¨äºŽæœ€ä¼˜ç­–ç•¥ï¼ŒæŠ˜çŽ°å› å­ä¸º `0.5`ï¼Œä»¥åŠå…¶ä»–å˜é‡ï¼š

```py
>>> V, V_history = policy_evaluation_history(
 policy_optimal, T, R, gamma, threshold)
```

ç„¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç ç»˜åˆ¶äº†å€¼çš„åŽ†å²ç»“æžœï¼š

```py
>>> import matplotlib.pyplot as plt
>>> s0, = plt.plot([v[0] for v in V_history])
>>> s1, = plt.plot([v[1] for v in V_history])
>>> s2, = plt.plot([v[2] for v in V_history])
>>> plt.title('Optimal policy with gamma = {}'.format(str(gamma)))
>>> plt.xlabel('Iteration')
>>> plt.ylabel('Policy values')
>>> plt.legend([s0, s1, s2],
...            ["State s0",
...             "State s1",
...             "State s2"], loc="upper left")
>>> plt.show()
```

æˆ‘ä»¬çœ‹åˆ°äº†ä»¥ä¸‹ç»“æžœï¼š

![](img/51417194-cf66-4db6-8e61-2d782b6981f6.png)

åœ¨æ”¶æ•›æœŸé—´ï¼Œä»Žç¬¬ 10 åˆ°ç¬¬ 14 æ¬¡è¿­ä»£ä¹‹é—´çš„ç¨³å®šæ€§æ˜¯éžå¸¸æœ‰è¶£çš„ã€‚

æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„æŠ˜çŽ°å› å­ï¼Œ0.2 å’Œ 0.99ï¼Œè¿è¡Œç›¸åŒçš„ä»£ç ã€‚æˆ‘ä»¬å¾—åˆ°äº†æŠ˜çŽ°å› å­ä¸º 0.2 æ—¶çš„ä»¥ä¸‹ç»˜å›¾ï¼š

![](img/f874a0f1-4024-4877-ad18-4240810a31ae.png)

å°†æŠ˜çŽ°å› å­ä¸º 0.5 çš„ç»˜å›¾ä¸Žè¿™ä¸ªè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å› å­è¶Šå°ï¼Œç­–ç•¥å€¼æ”¶æ•›å¾—è¶Šå¿«ã€‚

åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¾—åˆ°äº†æŠ˜çŽ°å› å­ä¸º 0.99 æ—¶çš„ä»¥ä¸‹ç»˜å›¾ï¼š

![](img/e9ddb167-e202-49ec-baa5-8f8e2d317e95.png)

é€šè¿‡å°†æŠ˜çŽ°å› å­ä¸º 0.5 çš„ç»˜å›¾ä¸ŽæŠ˜çŽ°å› å­ä¸º 0.99 çš„ç»˜å›¾è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å› å­è¶Šå¤§ï¼Œç­–ç•¥å€¼æ”¶æ•›æ‰€éœ€çš„æ—¶é—´è¶Šé•¿ã€‚æŠ˜çŽ°å› å­æ˜¯å³æ—¶å¥–åŠ±ä¸Žæœªæ¥å¥–åŠ±ä¹‹é—´çš„æƒè¡¡ã€‚

# æ¨¡æ‹Ÿ FrozenLake çŽ¯å¢ƒ

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¤„ç†è¿‡çš„ MDP çš„æœ€ä¼˜ç­–ç•¥éƒ½ç›¸å½“ç›´è§‚ã€‚ç„¶è€Œï¼Œåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¦‚ FrozenLake çŽ¯å¢ƒï¼Œæƒ…å†µå¹¶ä¸é‚£ä¹ˆç®€å•ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œè®©æˆ‘ä»¬çŽ©ä¸€ä¸‹ FrozenLake çŽ¯å¢ƒï¼Œå¹¶å‡†å¤‡å¥½æŽ¥ä¸‹æ¥çš„æ•™ç¨‹ï¼Œæˆ‘ä»¬å°†æ‰¾åˆ°å®ƒçš„æœ€ä¼˜ç­–ç•¥ã€‚

FrozenLake æ˜¯ä¸€ä¸ªå…¸åž‹çš„ Gym çŽ¯å¢ƒï¼Œå…·æœ‰**ç¦»æ•£**çŠ¶æ€ç©ºé—´ã€‚å®ƒæ˜¯å…³äºŽåœ¨ç½‘æ ¼ä¸–ç•Œä¸­å°†ä»£ç†ç¨‹åºä»Žèµ·å§‹ä½ç½®ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®ï¼Œå¹¶åŒæ—¶é¿å¼€é™·é˜±ã€‚ç½‘æ ¼å¯ä»¥æ˜¯å››ä¹˜å›› ([`gym.openai.com/envs/FrozenLake-v0/`](https://gym.openai.com/envs/FrozenLake-v0/)) æˆ–è€…å…«ä¹˜å…«ã€‚

t ([`gym.openai.com/envs/FrozenLake8x8-v0/`](https://gym.openai.com/envs/FrozenLake8x8-v0/))ã€‚ç½‘æ ¼ç”±ä»¥ä¸‹å››ç§ç±»åž‹çš„æ–¹å—ç»„æˆï¼š

+   **S**ï¼šä»£è¡¨èµ·å§‹ä½ç½®

+   **G**ï¼šä»£è¡¨ç›®æ ‡ä½ç½®ï¼Œè¿™ä¼šç»ˆæ­¢ä¸€ä¸ªå›žåˆ

+   **F**ï¼šä»£è¡¨å†°é¢æ–¹å—ï¼Œå¯ä»¥è¡Œèµ°çš„ä½ç½®

+   **H**ï¼šä»£è¡¨ä¸€ä¸ªåœ°æ´žä½ç½®ï¼Œè¿™ä¼šç»ˆæ­¢ä¸€ä¸ªå›žåˆ

æ˜¾ç„¶æœ‰å››ç§åŠ¨ä½œï¼šå‘å·¦ç§»åŠ¨ï¼ˆ0ï¼‰ã€å‘ä¸‹ç§»åŠ¨ï¼ˆ1ï¼‰ã€å‘å³ç§»åŠ¨ï¼ˆ2ï¼‰å’Œå‘ä¸Šç§»åŠ¨ï¼ˆ3ï¼‰ã€‚å¦‚æžœä»£ç†ç¨‹åºæˆåŠŸåˆ°è¾¾ç›®æ ‡ä½ç½®ï¼Œå¥–åŠ±ä¸º+1ï¼Œå¦åˆ™ä¸º 0ã€‚æ­¤å¤–ï¼Œè§‚å¯Ÿç©ºé—´ç”±ä¸€ä¸ª 16 ç»´æ•´æ•°æ•°ç»„è¡¨ç¤ºï¼Œæœ‰ 4 ç§å¯èƒ½çš„åŠ¨ä½œï¼ˆè¿™æ˜¯æœ‰é“ç†çš„ï¼‰ã€‚

è¿™ä¸ªçŽ¯å¢ƒçš„æ£˜æ‰‹ä¹‹å¤„åœ¨äºŽå†°é¢å¾ˆæ»‘ï¼Œä»£ç†ç¨‹åºå¹¶ä¸æ€»æ˜¯æŒ‰å…¶æ„å›¾ç§»åŠ¨ã€‚ä¾‹å¦‚ï¼Œå½“å®ƒæ‰“ç®—å‘ä¸‹ç§»åŠ¨æ—¶ï¼Œå¯èƒ½ä¼šå‘å·¦æˆ–å‘å³ç§»åŠ¨ã€‚

# å‡†å¤‡å·¥ä½œ

è¦è¿è¡Œ FrozenLake çŽ¯å¢ƒï¼Œè®©æˆ‘ä»¬é¦–å…ˆåœ¨è¿™é‡Œçš„çŽ¯å¢ƒè¡¨ä¸­æœç´¢å®ƒï¼š[`github.com/openai/gym/wiki/Table-of-environments`](https://github.com/openai/gym/wiki/Table-of-environments)ã€‚æœç´¢ç»“æžœç»™å‡ºäº†`FrozenLake-v0`ã€‚

# æ€Žä¹ˆåšâ€¦â€¦

è®©æˆ‘ä»¬æŒ‰ä»¥ä¸‹æ­¥éª¤æ¨¡æ‹Ÿå››ä¹˜å››çš„ FrozenLake çŽ¯å¢ƒï¼š

1.  æˆ‘ä»¬å¯¼å…¥`gym`åº“ï¼Œå¹¶åˆ›å»º FrozenLake çŽ¯å¢ƒçš„ä¸€ä¸ªå®žä¾‹ï¼š

```py
>>> import gym
>>> import torch
>>> env = gym.make("FrozenLake-v0")
>>> n_state = env.observation_space.n
>>> print(n_state)
16
>>> n_action = env.action_space.n
>>> print(n_action)
4
```

1.  é‡ç½®çŽ¯å¢ƒï¼š

```py
>>> env.reset()
0
```

ä»£ç†ç¨‹åºä»ŽçŠ¶æ€`0`å¼€å§‹ã€‚

1.  æ¸²æŸ“çŽ¯å¢ƒï¼š

```py
>>> env.render()
```

1.  è®©æˆ‘ä»¬åšä¸€ä¸ªå‘ä¸‹çš„åŠ¨ä½œï¼Œå› ä¸ºè¿™æ˜¯å¯è¡Œèµ°çš„ï¼š

```py
>>> new_state, reward, is_done, info = env.step(1)
>>> env.render()
```

1.  æ‰“å°å‡ºæ‰€æœ‰è¿”å›žçš„ä¿¡æ¯ï¼Œç¡®è®¤ä»£ç†ç¨‹åºä»¥ 33.33%çš„æ¦‚çŽ‡è½åœ¨çŠ¶æ€`4`ï¼š

```py
>>> print(new_state)
4
>>> print(reward)
0.0
>>> print(is_done)
False
>>> print(info)
{'prob': 0.3333333333333333}
```

ä½ å¾—åˆ°äº†`0`ä½œä¸ºå¥–åŠ±ï¼Œå› ä¸ºå®ƒå°šæœªåˆ°è¾¾ç›®æ ‡ï¼Œå¹¶ä¸”å›žåˆå°šæœªç»“æŸã€‚å†æ¬¡çœ‹åˆ°ä»£ç†ç¨‹åºå¯èƒ½ä¼šé™·å…¥çŠ¶æ€ 1ï¼Œæˆ–è€…å› ä¸ºè¡¨é¢å¤ªæ»‘è€Œåœç•™åœ¨çŠ¶æ€ 0ã€‚

1.  ä¸ºäº†å±•ç¤ºåœ¨å†°é¢ä¸Šè¡Œèµ°æœ‰å¤šå›°éš¾ï¼Œå®žçŽ°ä¸€ä¸ªéšæœºç­–ç•¥å¹¶è®¡ç®— 1,000 ä¸ªå›žåˆçš„å¹³å‡æ€»å¥–åŠ±ã€‚é¦–å…ˆï¼Œå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®ç»™å®šçš„ç­–ç•¥æ¨¡æ‹Ÿä¸€ä¸ª FrozenLake å›žåˆå¹¶è¿”å›žæ€»å¥–åŠ±ï¼ˆæˆ‘ä»¬çŸ¥é“è¿™è¦ä¹ˆæ˜¯ 0ï¼Œè¦ä¹ˆæ˜¯ 1ï¼‰ï¼š

```py
>>> def run_episode(env, policy):
...     state = env.reset()
...     total_reward = 0
...     is_done = False
...     while not is_done:
...         action = policy[state].item()
...         state, reward, is_done, info = env.step(action)
...         total_reward += reward
...         if is_done:
...             break
...     return total_reward
```

1.  çŽ°åœ¨è¿è¡Œ`1000`ä¸ªå›žåˆï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªå›žåˆä¸­éƒ½ä¼šéšæœºç”Ÿæˆå¹¶ä½¿ç”¨ä¸€ä¸ªç­–ç•¥ï¼š

```py
>>> n_episode = 1000
>>> total_rewards = []
>>> for episode in range(n_episode):
...     random_policy = torch.randint(
 high=n_action, size=(n_state,))
...     total_reward = run_episode(env, random_policy)
...     total_rewards.append(total_reward)
...
>>> print('Average total reward under random policy: {}'.format(
 sum(total_rewards) / n_episode))
Average total reward under random policy: 0.014
```

è¿™åŸºæœ¬ä¸Šæ„å‘³ç€ï¼Œå¦‚æžœæˆ‘ä»¬éšæœºæ‰§è¡ŒåŠ¨ä½œï¼Œå¹³å‡åªæœ‰ 1.4%çš„æœºä¼šä»£ç†ç¨‹åºèƒ½å¤Ÿåˆ°è¾¾ç›®æ ‡ä½ç½®ã€‚

1.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨éšæœºæœç´¢ç­–ç•¥è¿›è¡Œå®žéªŒã€‚åœ¨è®­ç»ƒé˜¶æ®µï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆä¸€å †ç­–ç•¥ï¼Œå¹¶è®°å½•ç¬¬ä¸€ä¸ªè¾¾åˆ°ç›®æ ‡çš„ç­–ç•¥ï¼š

```py
>>> while True:
...     random_policy = torch.randint(
 high=n_action, size=(n_state,))
...     total_reward = run_episode(env, random_policy)
...     if total_reward == 1:
...         best_policy = random_policy
...         break
```

1.  æŸ¥çœ‹æœ€ä½³ç­–ç•¥ï¼š

```py
>>> print(best_policy)
tensor([0, 3, 2, 2, 0, 2, 1, 1, 3, 1, 3, 0, 0, 1, 1, 1])
```

1.  çŽ°åœ¨è¿è¡Œ 1,000 ä¸ªå›žåˆï¼Œä½¿ç”¨æˆ‘ä»¬åˆšæŒ‘é€‰å‡ºçš„ç­–ç•¥ï¼š

```py
>>> total_rewards = []
>>> for episode in range(n_episode):
...     total_reward = run_episode(env, best_policy)
...     total_rewards.append(total_reward)
...
>>> print('Average total reward under random search 
     policy: {}'.format(sum(total_rewards) / n_episode))
Average total reward under random search policy: 0.208
```

ä½¿ç”¨éšæœºæœç´¢ç®—æ³•ï¼Œå¹³å‡æƒ…å†µä¸‹ä¼šæœ‰ 20.8% çš„æ¦‚çŽ‡è¾¾åˆ°ç›®æ ‡ã€‚

è¯·æ³¨æ„ï¼Œç”±äºŽæˆ‘ä»¬é€‰æ‹©çš„ç­–ç•¥å¯èƒ½ç”±äºŽå†°é¢æ»‘åŠ¨è€Œè¾¾åˆ°ç›®æ ‡ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´ç»“æžœå˜åŒ–å¾ˆå¤§ï¼Œå¯èƒ½ä¸æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚

# å·¥ä½œåŽŸç†â€¦â€¦

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬éšæœºç”Ÿæˆäº†ä¸€ä¸ªç”± 16 ä¸ªåŠ¨ä½œç»„æˆçš„ç­–ç•¥ï¼Œå¯¹åº” 16 ä¸ªçŠ¶æ€ã€‚è¯·è®°ä½ï¼Œåœ¨ FrozenLake ä¸­ï¼Œç§»åŠ¨æ–¹å‘ä»…éƒ¨åˆ†ä¾èµ–äºŽé€‰æ‹©çš„åŠ¨ä½œï¼Œè¿™å¢žåŠ äº†æŽ§åˆ¶çš„ä¸ç¡®å®šæ€§ã€‚

åœ¨è¿è¡Œ *Step 4* ä¸­çš„ä»£ç åŽï¼Œä½ å°†çœ‹åˆ°ä¸€ä¸ª 4 * 4 çš„çŸ©é˜µï¼Œä»£è¡¨å†°æ¹–å’Œä»£ç†ç«™ç«‹çš„ç“·ç –ï¼ˆçŠ¶æ€ 0ï¼‰ï¼š

![](img/aa71c6fa-1ea2-4769-9588-2e08637c775c.png)

åœ¨è¿è¡Œ *Step 5* ä¸­çš„ä»£ç è¡ŒåŽï¼Œä½ å°†çœ‹åˆ°å¦‚ä¸‹ç»“æžœç½‘æ ¼ï¼Œä»£ç†å‘ä¸‹ç§»åŠ¨åˆ°çŠ¶æ€ 4ï¼š

![](img/c1deaef9-3cbb-4ce1-9181-1ca34373ce78.png)

å¦‚æžœæ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ä¹‹ä¸€ï¼Œä¸€ä¸ªå›žåˆå°†ç»ˆæ­¢ï¼š

+   ç§»åŠ¨åˆ° H æ ¼ï¼ˆçŠ¶æ€ 5ã€7ã€11ã€12ï¼‰ã€‚è¿™å°†ç”Ÿæˆæ€»å¥–åŠ± 0ã€‚

+   ç§»åŠ¨åˆ° G æ ¼ï¼ˆçŠ¶æ€ 15ï¼‰ã€‚è¿™å°†äº§ç”Ÿæ€»å¥–åŠ± +1ã€‚

# è¿˜æœ‰æ›´å¤šå†…å®¹â€¦â€¦

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ P å±žæ€§æŸ¥çœ‹ FrozenLake çŽ¯å¢ƒçš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬è½¬ç§»çŸ©é˜µå’Œæ¯ä¸ªçŠ¶æ€åŠåŠ¨ä½œçš„å¥–åŠ±ã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽçŠ¶æ€ 6ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```py
>>> print(env.env.P[6])
{0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}
```

è¿™ä¼šè¿”å›žä¸€ä¸ªå­—å…¸ï¼Œå…¶é”®ä¸º 0ã€1ã€2 å’Œ 3ï¼Œåˆ†åˆ«ä»£è¡¨å››ç§å¯èƒ½çš„åŠ¨ä½œã€‚å€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«åœ¨æ‰§è¡ŒåŠ¨ä½œåŽçš„ç§»åŠ¨ã€‚ç§»åŠ¨åˆ—è¡¨çš„æ ¼å¼å¦‚ä¸‹ï¼šï¼ˆè½¬ç§»æ¦‚çŽ‡ï¼Œæ–°çŠ¶æ€ï¼ŒèŽ·å¾—çš„å¥–åŠ±ï¼Œæ˜¯å¦ç»“æŸï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æžœä»£ç†å¤„äºŽçŠ¶æ€ 6 å¹¶æ‰“ç®—æ‰§è¡ŒåŠ¨ä½œ 1ï¼ˆå‘ä¸‹ï¼‰ï¼Œæœ‰ 33.33% çš„æ¦‚çŽ‡å®ƒä¼šè¿›å…¥çŠ¶æ€ 5ï¼ŒèŽ·å¾—å¥–åŠ± 0 å¹¶ç»ˆæ­¢è¯¥å›žåˆï¼›æœ‰ 33.33% çš„æ¦‚çŽ‡å®ƒä¼šè¿›å…¥çŠ¶æ€ 10ï¼ŒèŽ·å¾—å¥–åŠ± 0ï¼›æœ‰ 33.33% çš„æ¦‚çŽ‡å®ƒä¼šè¿›å…¥çŠ¶æ€ 7ï¼ŒèŽ·å¾—å¥–åŠ± 0 å¹¶ç»ˆæ­¢è¯¥å›žåˆã€‚

å¯¹äºŽçŠ¶æ€ 11ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```py
>>> print(env.env.P[11])
{0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}
```

ç”±äºŽè¸©åˆ°æ´žä¼šç»ˆæ­¢ä¸€ä¸ªå›žåˆï¼Œæ‰€ä»¥ä¸ä¼šå†æœ‰ä»»ä½•ç§»åŠ¨ã€‚

éšæ„æŸ¥çœ‹å…¶ä»–çŠ¶æ€ã€‚

# ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ MDP

å¦‚æžœæ‰¾åˆ°å…¶æœ€ä¼˜ç­–ç•¥ï¼Œåˆ™è®¤ä¸º MDP å·²è§£å†³ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ **å€¼è¿­ä»£** ç®—æ³•æ‰¾å‡º FrozenLake çŽ¯å¢ƒçš„æœ€ä¼˜ç­–ç•¥ã€‚

å€¼è¿­ä»£çš„æ€æƒ³ä¸Žç­–ç•¥è¯„ä¼°éžå¸¸ç›¸ä¼¼ã€‚å®ƒä¹Ÿæ˜¯ä¸€ç§è¿­ä»£ç®—æ³•ã€‚å®ƒä»Žä»»æ„ç­–ç•¥å€¼å¼€å§‹ï¼Œç„¶åŽæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹è¿­ä»£æ›´æ–°å€¼ï¼Œç›´åˆ°æ”¶æ•›ã€‚å› æ­¤ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒä¸æ˜¯é‡‡ç”¨è·¨æ‰€æœ‰åŠ¨ä½œçš„å€¼çš„æœŸæœ›ï¼ˆå¹³å‡å€¼ï¼‰ï¼Œè€Œæ˜¯é€‰æ‹©å®žçŽ°æœ€å¤§ç­–ç•¥å€¼çš„åŠ¨ä½œï¼š

![](img/aa401157-f4e2-414b-9843-6b221c86fa9f.png)

è¿™é‡Œï¼ŒV*(s)è¡¨ç¤ºæœ€ä¼˜å€¼ï¼Œå³æœ€ä¼˜ç­–ç•¥çš„å€¼ï¼›T(s, a, s')æ˜¯é‡‡å–åŠ¨ä½œ a ä»ŽçŠ¶æ€ s è½¬ç§»åˆ°çŠ¶æ€ sâ€™çš„è½¬ç§»æ¦‚çŽ‡ï¼›è€Œ R(s, a)æ˜¯é‡‡å–åŠ¨ä½œ a æ—¶åœ¨çŠ¶æ€ s ä¸­æ”¶åˆ°çš„å¥–åŠ±ã€‚

è®¡ç®—å‡ºæœ€ä¼˜å€¼åŽï¼Œæˆ‘ä»¬å¯ä»¥ç›¸åº”åœ°èŽ·å¾—æœ€ä¼˜ç­–ç•¥ï¼š

![](img/94ddcae0-0acc-4516-a10b-c27bb79080ea.png)

# å¦‚ä½•åšâ€¦

è®©æˆ‘ä»¬ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³ FrozenLake çŽ¯å¢ƒå¦‚ä¸‹ï¼š

1.  å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆ›å»º FrozenLake çŽ¯å¢ƒçš„å®žä¾‹ï¼š

```py
>>> import torch
>>> import gym
>>> env = gym.make('FrozenLake-v0')
```

1.  å°†æŠ˜æ‰£å› å­è®¾ä¸º`0.99`ï¼Œæ”¶æ•›é˜ˆå€¼è®¾ä¸º`0.0001`ã€‚

```py
>>> gamma = 0.99
>>> threshold = 0.0001
```

1.  çŽ°åœ¨å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å€¼è¿­ä»£ç®—æ³•è®¡ç®—æœ€ä¼˜å€¼ï¼š

```py
>>> def value_iteration(env, gamma, threshold):
...     """
...     Solve a given environment with value iteration algorithm
...     @param env: OpenAI Gym environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values for 
 all states are less than the threshold
...     @return: values of the optimal policy for the given 
 environment
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.empty(n_state)
...         for state in range(n_state):
...             v_actions = torch.zeros(n_action)
...             for action in range(n_action):
...                 for trans_prob, new_state, reward, _ in 
 env.env.P[state][action]:
...                     v_actions[action] += trans_prob * (reward 
 + gamma * V[new_state])
...             V_temp[state] = torch.max(v_actions)
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

1.  æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼Œç„¶åŽæ‰“å°æœ€ä¼˜å€¼ï¼š

```py
>>> V_optimal = value_iteration(env, gamma, threshold)
>>> print('Optimal values:\n{}'.format(V_optimal))
Optimal values:
tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,
 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
```

1.  çŽ°åœ¨æˆ‘ä»¬æœ‰äº†æœ€ä¼˜å€¼ï¼Œæˆ‘ä»¬å¼€å‘æå–æœ€ä¼˜ç­–ç•¥çš„å‡½æ•°ï¼š

```py
>>> def extract_optimal_policy(env, V_optimal, gamma):
...     """
...     Obtain the optimal policy based on the optimal values
...     @param env: OpenAI Gym environment
...     @param V_optimal: optimal values
...     @param gamma: discount factor
...     @return: optimal policy
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     optimal_policy = torch.zeros(n_state)
...     for state in range(n_state):
...         v_actions = torch.zeros(n_action)
...         for action in range(n_action):
...             for trans_prob, new_state, reward, _ in 
                                   env.env.P[state][action]:
...                 v_actions[action] += trans_prob * (reward 
 + gamma * V_optimal[new_state])
...         optimal_policy[state] = torch.argmax(v_actions)
...     return optimal_policy
```

1.  æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæœ€ä¼˜å€¼ï¼Œç„¶åŽæ‰“å°æœ€ä¼˜ç­–ç•¥ï¼š

```py
>>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
>>> print('Optimal policy:\n{}'.format(optimal_policy))
Optimal policy:
tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])
```

1.  æˆ‘ä»¬æƒ³è¦è¯„ä¼°æœ€ä¼˜ç­–ç•¥çš„å¥½åç¨‹åº¦ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æœ€ä¼˜ç­–ç•¥è¿è¡Œ 1,000 æ¬¡æƒ…èŠ‚ï¼Œå¹¶æ£€æŸ¥å¹³å‡å¥–åŠ±ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é‡å¤ä½¿ç”¨æˆ‘ä»¬åœ¨å‰é¢çš„é…æ–¹ä¸­å®šä¹‰çš„`run_episode`å‡½æ•°ï¼š

```py
>>> n_episode = 1000
>>> total_rewards = []
>>> for episode in range(n_episode):
...     total_reward = run_episode(env, optimal_policy)
...     total_rewards.append(total_reward)
>>> print('Average total reward under the optimal 
 policy: {}'.format(sum(total_rewards) / n_episode))
Average total reward under the optimal policy: 0.75
```

åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹ï¼Œä»£ç†å°†å¹³å‡ 75%çš„æ—¶é—´åˆ°è¾¾ç›®æ ‡ã€‚è¿™æ˜¯æˆ‘ä»¬èƒ½å¤Ÿåšåˆ°çš„æœ€å¥½ç»“æžœï¼Œå› ä¸ºå†°å¾ˆæ»‘ã€‚

# å·¥ä½œåŽŸç†â€¦

åœ¨å€¼è¿­ä»£ç®—æ³•ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡è¿­ä»£åº”ç”¨è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ¥èŽ·å¾—æœ€ä¼˜å€¼å‡½æ•°ã€‚

ä¸‹é¢æ˜¯è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹çš„å¦ä¸€ç‰ˆæœ¬ï¼Œé€‚ç”¨äºŽå¥–åŠ±éƒ¨åˆ†ä¾èµ–äºŽæ–°çŠ¶æ€çš„çŽ¯å¢ƒï¼š

![](img/9f8162d0-dc85-4f29-9da6-927b2ca44e6c.png)

è¿™é‡Œï¼ŒR(s, a, s')è¡¨ç¤ºé€šè¿‡é‡‡å–åŠ¨ä½œ a ä»ŽçŠ¶æ€ s ç§»åŠ¨åˆ°çŠ¶æ€ s'è€Œæ”¶åˆ°çš„å¥–åŠ±ã€‚ç”±äºŽè¿™ä¸ªç‰ˆæœ¬æ›´å…¼å®¹ï¼Œæˆ‘ä»¬æ ¹æ®å®ƒå¼€å‘äº†æˆ‘ä»¬çš„`value_iteration`å‡½æ•°ã€‚æ­£å¦‚æ‚¨åœ¨*Step 3*ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

+   å°†ç­–ç•¥å€¼åˆå§‹åŒ–ä¸ºå…¨éƒ¨ä¸ºé›¶ã€‚

+   æ ¹æ®è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ›´æ–°å€¼ã€‚

+   è®¡ç®—æ‰€æœ‰çŠ¶æ€çš„å€¼çš„æœ€å¤§å˜åŒ–ã€‚

+   å¦‚æžœæœ€å¤§å˜åŒ–å¤§äºŽé˜ˆå€¼ï¼Œåˆ™ç»§ç»­æ›´æ–°å€¼ã€‚å¦åˆ™ï¼Œç»ˆæ­¢è¯„ä¼°è¿‡ç¨‹ï¼Œå¹¶è¿”å›žæœ€æ–°çš„å€¼ä½œä¸ºæœ€ä¼˜å€¼ã€‚

# è¿˜æœ‰æ›´å¤šâ€¦

æˆ‘ä»¬åœ¨æŠ˜æ‰£å› å­ä¸º 0.99 æ—¶èŽ·å¾—äº† 75%çš„æˆåŠŸçŽ‡ã€‚æŠ˜æ‰£å› å­å¦‚ä½•å½±å“æ€§èƒ½ï¼Ÿè®©æˆ‘ä»¬ç”¨ä¸åŒçš„å› å­è¿›è¡Œä¸€äº›å®žéªŒï¼ŒåŒ…æ‹¬`0`ã€`0.2`ã€`0.4`ã€`0.6`ã€`0.8`ã€`0.99`å’Œ`1.`ï¼š

```py
>>> gammas = [0, 0.2, 0.4, 0.6, 0.8, .99, 1.]
```

å¯¹äºŽæ¯ä¸ªæŠ˜æ‰£å› å­ï¼Œæˆ‘ä»¬è®¡ç®—äº† 10,000 ä¸ªå‘¨æœŸçš„å¹³å‡æˆåŠŸçŽ‡ï¼š

```py
>>> avg_reward_gamma = []
>>> for gamma in gammas:
...     V_optimal = value_iteration(env, gamma, threshold)
...     optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
...     total_rewards = []
...     for episode in range(n_episode):
...         total_reward = run_episode(env, optimal_policy)
...         total_rewards.append(total_reward)
...     avg_reward_gamma.append(sum(total_rewards) / n_episode)
```

æˆ‘ä»¬ç»˜åˆ¶äº†å¹³å‡æˆåŠŸçŽ‡ä¸ŽæŠ˜æ‰£å› å­çš„å›¾è¡¨ï¼š

```py
>>> import matplotlib.pyplot as plt
>>> plt.plot(gammas, avg_reward_gamma)
>>> plt.title('Success rate vs discount factor')
>>> plt.xlabel('Discount factor')
>>> plt.ylabel('Average success rate')
>>> plt.show()
```

æˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹çš„ç»˜å›¾ï¼š

![](img/650d388b-391a-4be4-8537-d49d31b08315.png)

ç»“æžœæ˜¾ç¤ºï¼Œå½“æŠ˜æ‰£å› å­å¢žåŠ æ—¶ï¼Œæ€§èƒ½æœ‰æ‰€æå‡ã€‚è¿™è¯å®žäº†ä¸€ä¸ªå°çš„æŠ˜æ‰£å› å­ç›®å‰ä»·å€¼å¥–åŠ±ï¼Œè€Œä¸€ä¸ªå¤§çš„æŠ˜æ‰£å› å­åˆ™æ›´çœ‹é‡æœªæ¥çš„æ›´å¥½å¥–åŠ±ã€‚

# ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³ MDP

è§£å†³ MDP çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨**ç­–ç•¥è¿­ä»£**ç®—æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬é…æ–¹ä¸­è®¨è®ºå®ƒã€‚

ç­–ç•¥è¿­ä»£ç®—æ³•å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ã€‚å®ƒä»Žä»»æ„ç­–ç•¥å¼€å§‹ã€‚æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒé¦–å…ˆæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è®¡ç®—ç»™å®šæœ€æ–°ç­–ç•¥çš„ç­–ç•¥å€¼ï¼›ç„¶åŽæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ä»Žç»“æžœç­–ç•¥å€¼ä¸­æå–ä¸€ä¸ªæ”¹è¿›çš„ç­–ç•¥ã€‚å®ƒåå¤è¯„ä¼°ç­–ç•¥å¹¶ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬ï¼Œç›´åˆ°ç­–ç•¥ä¸å†æ”¹å˜ä¸ºæ­¢ã€‚

è®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªç­–ç•¥è¿­ä»£ç®—æ³•ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è§£å†³ FrozenLake çŽ¯å¢ƒã€‚ä¹‹åŽï¼Œæˆ‘ä»¬å°†è§£é‡Šå®ƒçš„å·¥ä½œåŽŸç†ã€‚

# å¦‚ä½•åšâ€¦

è®©æˆ‘ä»¬ä½¿ç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³ FrozenLake çŽ¯å¢ƒï¼š

1.  æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„åº“å¹¶åˆ›å»º FrozenLake çŽ¯å¢ƒçš„å®žä¾‹ï¼š

```py
>>> import torch
>>> import gym
>>> env = gym.make('FrozenLake-v0')
```

1.  çŽ°åœ¨ï¼Œæš‚å°†æŠ˜æ‰£å› å­è®¾å®šä¸º`0.99`ï¼Œæ”¶æ•›é˜ˆå€¼è®¾å®šä¸º`0.0001`ï¼š

```py
>>> gamma = 0.99
>>> threshold = 0.0001
```

1.  çŽ°åœ¨æˆ‘ä»¬å®šä¹‰`policy_evaluation`å‡½æ•°ï¼Œå®ƒè®¡ç®—ç»™å®šç­–ç•¥çš„å€¼ï¼š

```py
>>> def policy_evaluation(env, policy, gamma, threshold):
...     """
...     Perform policy evaluation
...     @param env: OpenAI Gym environment
...     @param policy: policy matrix containing actions and 
 their probability in each state
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the given policy
...     """
...     n_state = policy.shape[0]
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state in range(n_state):
...             action = policy[state].item()
...             for trans_prob, new_state, reward, _ in 
 env.env.P[state][action]:
...                 V_temp[state] += trans_prob * (reward 
 + gamma * V[new_state])
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

è¿™ä¸Žæˆ‘ä»¬åœ¨*æ‰§è¡Œç­–ç•¥è¯„ä¼°*é…æ–¹ä¸­æ‰€åšçš„ç±»ä¼¼ï¼Œä½†è¾“å…¥æ˜¯ Gym çŽ¯å¢ƒã€‚

1.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„ç¬¬äºŒä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œå³ç­–ç•¥æ”¹è¿›éƒ¨åˆ†ï¼š

```py
>>> def policy_improvement(env, V, gamma):
...     """
...     Obtain an improved policy based on the values
...     @param env: OpenAI Gym environment
...     @param V: policy values
...     @param gamma: discount factor
...     @return: the policy
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     policy = torch.zeros(n_state)
...     for state in range(n_state):
...         v_actions = torch.zeros(n_action)
...         for action in range(n_action):
...             for trans_prob, new_state, reward, _ in 
 env.env.P[state][action]:
...                 v_actions[action] += trans_prob * (reward 
 + gamma * V[new_state])
...         policy[state] = torch.argmax(v_actions)
...     return policy
```

è¿™æ ¹æ®è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ä»Žç»™å®šçš„ç­–ç•¥å€¼ä¸­æå–äº†ä¸€ä¸ªæ”¹è¿›çš„ç­–ç•¥ã€‚

1.  çŽ°åœ¨æˆ‘ä»¬ä¸¤ä¸ªç»„ä»¶éƒ½å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬æŒ‰ä»¥ä¸‹æ–¹å¼å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•ï¼š

```py
>>> def policy_iteration(env, gamma, threshold):
...     """
...     Solve a given environment with policy iteration algorithm
...     @param env: OpenAI Gym environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: optimal values and the optimal policy for the given 
 environment
...     """
...     n_state = env.observation_space.n
...     n_action = env.action_space.n
...     policy = torch.randint(high=n_action, size=(n_state,)).float()
...     while True:
...         V = policy_evaluation(env, policy, gamma, threshold)
...         policy_improved = policy_improvement(env, V, gamma)
...         if torch.equal(policy_improved, policy):
...             return V, policy_improved
...         policy = policy_improved
```

1.  æ’å…¥çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼š

```py
>>> V_optimal, optimal_policy = 
 policy_iteration(env, gamma, threshold)
```

1.  æˆ‘ä»¬å·²ç»èŽ·å¾—äº†æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚è®©æˆ‘ä»¬çœ‹ä¸€çœ‹å®ƒä»¬ï¼š

```py
>>> print('Optimal values:\n{}'.format(V_optimal))
Optimal values:
tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,
 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
>>> print('Optimal policy:\n{}'.format(optimal_policy))
Optimal policy:
tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])
```

è¿™ä¸Žä½¿ç”¨å€¼è¿­ä»£ç®—æ³•å¾—åˆ°çš„ç»“æžœå®Œå…¨ä¸€æ ·ã€‚

# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„â€¦

ç­–ç•¥è¿­ä»£ç»“åˆäº†æ¯æ¬¡è¿­ä»£ä¸­çš„ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›ã€‚åœ¨ç­–ç•¥è¯„ä¼°ä¸­ï¼Œæ ¹æ®è´å°”æ›¼æœŸæœ›æ–¹ç¨‹è®¡ç®—ç»™å®šç­–ç•¥ï¼ˆè€Œéžæœ€ä¼˜ç­–ç•¥ï¼‰çš„å€¼ï¼Œç›´åˆ°å®ƒä»¬æ”¶æ•›ï¼š

![](img/6408dcd4-b641-4ef3-bacb-6ceb32d75026.png)

è¿™é‡Œï¼Œa = Ï€(s)ï¼Œå³åœ¨çŠ¶æ€ s ä¸‹æ ¹æ®ç­–ç•¥Ï€é‡‡å–çš„åŠ¨ä½œã€‚

åœ¨ç­–ç•¥æ”¹è¿›ä¸­ï¼Œæ ¹æ®è´å°”æ›¼æœ€ä¼˜æ€§æ–¹ç¨‹ä½¿ç”¨æ”¶æ•›çš„ç­–ç•¥å€¼ V(s)æ›´æ–°ç­–ç•¥ï¼š

![](img/f60962b4-570b-4954-8da6-79733626a594.png)

è¿™é‡å¤ç­–ç•¥è¯„ä¼°å’Œç­–ç•¥æ”¹è¿›æ­¥éª¤ï¼Œç›´åˆ°ç­–ç•¥æ”¶æ•›ã€‚åœ¨æ”¶æ•›æ—¶ï¼Œæœ€æ–°çš„ç­–ç•¥å’Œå…¶å€¼å‡½æ•°æ˜¯æœ€ä¼˜ç­–ç•¥å’Œæœ€ä¼˜å€¼å‡½æ•°ã€‚å› æ­¤ï¼Œåœ¨ç¬¬ 5 æ­¥ï¼Œ`policy_iteration`å‡½æ•°æ‰§è¡Œä»¥ä¸‹ä»»åŠ¡ï¼š

+   åˆå§‹åŒ–ä¸€ä¸ªéšæœºç­–ç•¥ã€‚

+   ä½¿ç”¨ç­–ç•¥è¯„ä¼°ç®—æ³•è®¡ç®—ç­–ç•¥çš„å€¼ã€‚

+   åŸºäºŽç­–ç•¥å€¼èŽ·å–æ”¹è¿›çš„ç­–ç•¥ã€‚

+   å¦‚æžœæ–°ç­–ç•¥ä¸Žæ—§ç­–ç•¥ä¸åŒï¼Œåˆ™æ›´æ–°ç­–ç•¥å¹¶è¿è¡Œå¦ä¸€æ¬¡è¿­ä»£ã€‚å¦åˆ™ï¼Œç»ˆæ­¢è¿­ä»£è¿‡ç¨‹å¹¶è¿”å›žç­–ç•¥å€¼å’Œç­–ç•¥ã€‚

# è¿˜æœ‰æ›´å¤š...

æˆ‘ä»¬åˆšåˆšç”¨ç­–ç•¥è¿­ä»£ç®—æ³•è§£å†³äº† FrozenLake çŽ¯å¢ƒã€‚å› æ­¤ï¼Œæ‚¨å¯èƒ½æƒ³çŸ¥é“ä½•æ—¶æœ€å¥½ä½¿ç”¨ç­–ç•¥è¿­ä»£è€Œä¸æ˜¯å€¼è¿­ä»£ï¼Œåä¹‹äº¦ç„¶ã€‚åŸºæœ¬ä¸Šæœ‰ä¸‰ç§æƒ…å†µå…¶ä¸­ä¸€ç§æ¯”å¦ä¸€ç§æ›´å ä¼˜åŠ¿ï¼š

+   å¦‚æžœæœ‰å¤§é‡çš„åŠ¨ä½œï¼Œè¯·ä½¿ç”¨ç­–ç•¥è¿­ä»£ï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´å¿«åœ°æ”¶æ•›ã€‚

+   å¦‚æžœåŠ¨ä½œæ•°é‡è¾ƒå°‘ï¼Œè¯·ä½¿ç”¨å€¼è¿­ä»£ã€‚

+   å¦‚æžœå·²ç»æœ‰ä¸€ä¸ªå¯è¡Œçš„ç­–ç•¥ï¼ˆé€šè¿‡ç›´è§‰æˆ–é¢†åŸŸçŸ¥è¯†èŽ·å¾—ï¼‰ï¼Œè¯·ä½¿ç”¨ç­–ç•¥è¿­ä»£ã€‚

åœ¨è¿™äº›æƒ…å†µä¹‹å¤–ï¼Œç­–ç•¥è¿­ä»£å’Œå€¼è¿­ä»£é€šå¸¸æ˜¯å¯æ¯”è¾ƒçš„ã€‚

åœ¨ä¸‹ä¸€ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åº”ç”¨æ¯ç§ç®—æ³•æ¥è§£å†³ç¡¬å¸æŠ›æŽ·èµŒåšé—®é¢˜ã€‚æˆ‘ä»¬å°†çœ‹åˆ°å“ªç§ç®—æ³•æ”¶æ•›å¾—æ›´å¿«ã€‚

# å‚è§

è¯·éšæ„ä½¿ç”¨æˆ‘ä»¬åœ¨è¿™ä¸¤ä¸ªæ¡ˆä¾‹ä¸­å­¦åˆ°çš„çŸ¥è¯†æ¥è§£å†³ä¸€ä¸ªæ›´å¤§çš„å†°æ ¼ï¼Œå³ `FrozenLake8x8-v0` çŽ¯å¢ƒ ([`gym.openai.com/envs/FrozenLake8x8-v0/`](https://gym.openai.com/envs/FrozenLake8x8-v0/))ã€‚

# è§£å†³ç¡¬å¸æŠ›æŽ·èµŒåšé—®é¢˜

å¯¹ç¡¬å¸æŠ›æŽ·èµŒåšåº”è¯¥å¯¹æ¯ä¸ªäººéƒ½å¾ˆç†Ÿæ‚‰ã€‚åœ¨æ¸¸æˆçš„æ¯ä¸€è½®ä¸­ï¼ŒèµŒå¾’å¯ä»¥æ‰“èµŒç¡¬å¸æ˜¯å¦ä¼šæ­£é¢æœä¸Šã€‚å¦‚æžœç»“æžœæ˜¯æ­£é¢ï¼ŒèµŒå¾’å°†èµ¢å¾—ä»–ä»¬ä¸‹æ³¨çš„ç›¸åŒé‡‘é¢ï¼›å¦åˆ™ï¼Œä»–ä»¬å°†å¤±åŽ»è¿™ç¬”é‡‘é¢ã€‚æ¸¸æˆå°†ç»§ç»­ï¼Œç›´åˆ°èµŒå¾’è¾“æŽ‰ï¼ˆæœ€ç»ˆä¸€æ— æ‰€æœ‰ï¼‰æˆ–èµ¢å¾—ï¼ˆèµ¢å¾—è¶…è¿‡ 100 ç¾Žå…ƒï¼Œå‡è®¾ï¼‰ã€‚å‡è®¾ç¡¬å¸ä¸å…¬å¹³ï¼Œå¹¶ä¸”æœ‰ 40%çš„æ¦‚çŽ‡æ­£é¢æœä¸Šã€‚ä¸ºäº†æœ€å¤§åŒ–èµ¢çš„æœºä¼šï¼ŒèµŒå¾’åº”è¯¥æ ¹æ®å½“å‰èµ„æœ¬åœ¨æ¯ä¸€è½®ä¸‹æ³¨å¤šå°‘ï¼Ÿè¿™ç»å¯¹æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜è¦è§£å†³ã€‚

å¦‚æžœç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚çŽ‡è¶…è¿‡ 50%ï¼Œå°±æ²¡ä»€ä¹ˆå¥½è®¨è®ºçš„ã€‚èµŒå¾’å¯ä»¥æ¯è½®ä¸‹æ³¨ä¸€ç¾Žå…ƒï¼Œå¹¶ä¸”å¤§å¤šæ•°æƒ…å†µä¸‹åº”è¯¥èƒ½èµ¢å¾—æ¸¸æˆã€‚å¦‚æžœæ˜¯å…¬å¹³ç¡¬å¸ï¼ŒèµŒå¾’æ¯è½®ä¸‹æ³¨ä¸€ç¾Žå…ƒæ—¶ï¼Œå¤§çº¦ä¸€åŠçš„æ—¶é—´ä¼šèµ¢ã€‚å½“æ­£é¢æœä¸Šçš„æ¦‚çŽ‡ä½ŽäºŽ 50% æ—¶ï¼Œä¿å®ˆçš„ç­–ç•¥å°±è¡Œä¸é€šäº†ã€‚éšæœºç­–ç•¥ä¹Ÿä¸è¡Œã€‚æˆ‘ä»¬éœ€è¦ä¾é æœ¬ç« å­¦åˆ°çš„å¼ºåŒ–å­¦ä¹ æŠ€æœ¯æ¥åšå‡ºæ˜Žæ™ºçš„æŠ•æ³¨ã€‚

è®©æˆ‘ä»¬å¼€å§‹å°†æŠ›ç¡¬å¸èµŒåšé—®é¢˜åˆ¶å®šä¸ºé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªæ— æŠ˜æ‰£ã€å‘¨æœŸæ€§çš„æœ‰é™ MDPï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹æ€§ï¼š

+   çŠ¶æ€æ˜¯èµŒå¾’çš„ç¾Žå…ƒèµ„æœ¬ã€‚æ€»å…±æœ‰ 101 ä¸ªçŠ¶æ€ï¼š0ã€1ã€2ã€â€¦ã€98ã€99 å’Œ 100+ã€‚

+   å¦‚æžœè¾¾åˆ°çŠ¶æ€ 100+ï¼Œåˆ™å¥–åŠ±ä¸º 1ï¼›å¦åˆ™ï¼Œå¥–åŠ±ä¸º 0ã€‚

+   è¡ŒåŠ¨æ˜¯èµŒå¾’åœ¨ä¸€è½®ä¸­å¯èƒ½ä¸‹æ³¨çš„é‡‘é¢ã€‚å¯¹äºŽçŠ¶æ€ sï¼Œå¯èƒ½çš„è¡ŒåŠ¨åŒ…æ‹¬ 1ã€2ã€â€¦ï¼Œä»¥åŠ min(s, 100 - s)ã€‚ä¾‹å¦‚ï¼Œå½“èµŒå¾’æœ‰ 60 ç¾Žå…ƒæ—¶ï¼Œä»–ä»¬å¯ä»¥ä¸‹æ³¨ä»Ž 1 åˆ° 40 çš„ä»»æ„é‡‘é¢ã€‚è¶…è¿‡ 40 çš„ä»»ä½•é‡‘é¢éƒ½æ²¡æœ‰æ„ä¹‰ï¼Œå› ä¸ºå®ƒå¢žåŠ äº†æŸå¤±å¹¶ä¸”ä¸å¢žåŠ èµ¢å¾—æ¸¸æˆçš„æœºä¼šã€‚

+   åœ¨é‡‡å–è¡ŒåŠ¨åŽï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€å–å†³äºŽç¡¬å¸æ­£é¢æœä¸Šçš„æ¦‚çŽ‡ã€‚å‡è®¾æ˜¯ 40%ã€‚å› æ­¤ï¼Œåœ¨é‡‡å–è¡ŒåŠ¨ *a* åŽï¼ŒçŠ¶æ€ s çš„ä¸‹ä¸€ä¸ªçŠ¶æ€å°†ä»¥ 40% çš„æ¦‚çŽ‡ä¸º *s+a*ï¼Œä»¥ 60% çš„æ¦‚çŽ‡ä¸º *s-a*ã€‚

+   è¿‡ç¨‹åœ¨çŠ¶æ€ 0 å’ŒçŠ¶æ€ 100+ å¤„ç»ˆæ­¢ã€‚

# å¦‚ä½•åšâ€¦

æˆ‘ä»¬é¦–å…ˆä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³æŠ›ç¡¬å¸èµŒåšé—®é¢˜ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1.  å¯¼å…¥ PyTorchï¼š

```py
>>> import torch
```

1.  æŒ‡å®šæŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼ï¼š

```py
>>> gamma = 1
>>> threshold = 1e-10
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æŠ˜æ‰£å› å­è®¾ä¸º 1ï¼Œå› ä¸ºè¿™ä¸ª MDP æ˜¯ä¸€ä¸ªæ— æŠ˜æ‰£çš„è¿‡ç¨‹ï¼›æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªå°é˜ˆå€¼ï¼Œå› ä¸ºæˆ‘ä»¬é¢„æœŸç­–ç•¥å€¼è¾ƒå°ï¼Œæ‰€æœ‰å¥–åŠ±éƒ½æ˜¯ 0ï¼Œé™¤äº†æœ€åŽä¸€ä¸ªçŠ¶æ€ã€‚

1.  å®šä¹‰ä»¥ä¸‹çŽ¯å¢ƒå˜é‡ã€‚

æ€»å…±æœ‰ 101 ä¸ªçŠ¶æ€ï¼š

```py
>>> capital_max = 100
>>> n_state = capital_max + 1
```

ç›¸åº”çš„å¥–åŠ±æ˜¾ç¤ºå¦‚ä¸‹ï¼š

```py
>>> rewards = torch.zeros(n_state)
>>> rewards[-1] = 1
>>> print(rewards)
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])
```

å‡è®¾æ­£é¢æœä¸Šçš„æ¦‚çŽ‡æ˜¯ 40%ï¼š

```py
>>> head_prob = 0.4
```

å°†è¿™äº›å˜é‡æ”¾å…¥å­—å…¸ä¸­ï¼š

```py
>>> env = {'capital_max': capital_max,
...        'head_prob': head_prob,
...        'rewards': rewards,
...        'n_state': n_state}
```

1.  çŽ°åœ¨æˆ‘ä»¬å¼€å‘ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å€¼è¿­ä»£ç®—æ³•è®¡ç®—æœ€ä¼˜å€¼ï¼š

```py
>>> def value_iteration(env, gamma, threshold):
...     """
...     Solve the coin flipping gamble problem with 
 value iteration algorithm
...     @param env: the coin flipping gamble environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the optimal policy for the given 
 environment
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state in range(1, capital_max):
...             v_actions = torch.zeros(
 min(state, capital_max - state) + 1)
...             for action in range(
 1, min(state, capital_max - state) + 1):
...                 v_actions[action] += head_prob * (
 rewards[state + action] +
 gamma * V[state + action])
...                 v_actions[action] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V[state - action])
...             V_temp[state] = torch.max(v_actions)
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

æˆ‘ä»¬åªéœ€è®¡ç®—çŠ¶æ€ 1 åˆ° 99 çš„å€¼ï¼Œå› ä¸ºçŠ¶æ€ 0 å’ŒçŠ¶æ€ 100+ çš„å€¼ä¸º 0ã€‚è€Œç»™å®šçŠ¶æ€ *s*ï¼Œå¯èƒ½çš„è¡ŒåŠ¨å¯ä»¥æ˜¯ä»Ž 1 åˆ° *min(s, 100 - s)*ã€‚åœ¨è®¡ç®—è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥ç‰¢è®°è¿™ä¸€ç‚¹ã€‚

1.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®æœ€ä¼˜å€¼æå–æœ€ä¼˜ç­–ç•¥ï¼š

```py
>>> def extract_optimal_policy(env, V_optimal, gamma):
...     """
...     Obtain the optimal policy based on the optimal values
...     @param env: the coin flipping gamble environment
...     @param V_optimal: optimal values
...     @param gamma: discount factor
...     @return: optimal policy
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     optimal_policy = torch.zeros(capital_max).int()
...     for state in range(1, capital_max):
...         v_actions = torch.zeros(n_state)
...         for action in range(1, 
 min(state, capital_max - state) + 1):
...             v_actions[action] += head_prob * (
 rewards[state + action] +
 gamma * V_optimal[state + action])
...             v_actions[action] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V_optimal[state - action])
...         optimal_policy[state] = torch.argmax(v_actions)
...     return optimal_policy
```

1.  æœ€åŽï¼Œæˆ‘ä»¬å¯ä»¥å°†çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼è¾“å…¥ï¼Œè®¡ç®—å‡ºæœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¡æ—¶äº†ä½¿ç”¨å€¼è¿­ä»£è§£å†³èµŒåš MDP æ‰€éœ€çš„æ—¶é—´ï¼›æˆ‘ä»¬å°†å…¶ä¸Žç­–ç•¥è¿­ä»£å®Œæˆæ‰€éœ€çš„æ—¶é—´è¿›è¡Œæ¯”è¾ƒï¼š

```py
>>> import time
>>> start_time = time.time()
>>> V_optimal = value_iteration(env, gamma, threshold)
>>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma)
>>> print("It takes {:.3f}s to solve with value 
 iteration".format(time.time() - start_time))
It takes 4.717s to solve with value iteration
```

æˆ‘ä»¬åœ¨ `4.717` ç§’å†…ä½¿ç”¨å€¼è¿­ä»£ç®—æ³•è§£å†³äº†èµŒåšé—®é¢˜ã€‚

1.  æŸ¥çœ‹æˆ‘ä»¬å¾—åˆ°çš„æœ€ä¼˜ç­–ç•¥å€¼å’Œæœ€ä¼˜ç­–ç•¥ï¼š

```py
>>> print('Optimal values:\n{}'.format(V_optimal))
>>> print('Optimal policy:\n{}'.format(optimal_policy))
```

1.  æˆ‘ä»¬å¯ä»¥ç»˜åˆ¶ç­–ç•¥å€¼ä¸ŽçŠ¶æ€çš„å›¾è¡¨å¦‚ä¸‹ï¼š

```py
>>> import matplotlib.pyplot as plt
>>> plt.plot(V_optimal[:100].numpy())
>>> plt.title('Optimal policy values')
>>> plt.xlabel('Capital')
>>> plt.ylabel('Policy value')
>>> plt.show()
```

çŽ°åœ¨æˆ‘ä»¬å·²ç»é€šè¿‡å€¼è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ï¼ŒæŽ¥ä¸‹æ¥æ˜¯ç­–ç•¥è¿­ä»£ï¼Ÿæˆ‘ä»¬æ¥çœ‹çœ‹ã€‚

1.  æˆ‘ä»¬é¦–å…ˆå¼€å‘`policy_evaluation`å‡½æ•°ï¼Œè¯¥å‡½æ•°æ ¹æ®ç­–ç•¥è®¡ç®—å€¼ï¼š

```py
>>> def policy_evaluation(env, policy, gamma, threshold):
...     """
...     Perform policy evaluation
...     @param env: the coin flipping gamble environment
...     @param policy: policy tensor containing actions taken 
 for individual state
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values 
 for all states are less than the threshold
...     @return: values of the given policy
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     V = torch.zeros(n_state)
...     while True:
...         V_temp = torch.zeros(n_state)
...         for state in range(1, capital_max):
...             action = policy[state].item()
...             V_temp[state] += head_prob * (
 rewards[state + action] +
 gamma * V[state + action])
...             V_temp[state] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V[state - action])
...         max_delta = torch.max(torch.abs(V - V_temp))
...         V = V_temp.clone()
...         if max_delta <= threshold:
...             break
...     return V
```

1.  æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„å¦ä¸€ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼Œå³ç­–ç•¥æ”¹è¿›éƒ¨åˆ†ï¼š

```py
>>> def policy_improvement(env, V, gamma):
...     """
...     Obtain an improved policy based on the values
...     @param env: the coin flipping gamble environment
...     @param V: policy values
...     @param gamma: discount factor
...     @return: the policy
...     """
...     head_prob = env['head_prob']
...     n_state = env['n_state']
...     capital_max = env['capital_max']
...     policy = torch.zeros(n_state).int()
...     for state in range(1, capital_max):
...         v_actions = torch.zeros(
 min(state, capital_max - state) + 1)
...         for action in range(
 1, min(state, capital_max - state) + 1):
...             v_actions[action] += head_prob * (
 rewards[state + action] + 
 gamma * V[state + action])
...             v_actions[action] += (1 - head_prob) * (
 rewards[state - action] +
 gamma * V[state - action])
...         policy[state] = torch.argmax(v_actions)
...     return policy
```

1.  æœ‰äº†è¿™ä¸¤ä¸ªç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å‘ç­–ç•¥è¿­ä»£ç®—æ³•çš„ä¸»è¦å…¥å£å¦‚ä¸‹ï¼š

```py
>>> def policy_iteration(env, gamma, threshold):
...     """
...     Solve the coin flipping gamble problem with policy 
 iteration algorithm
...     @param env: the coin flipping gamble environment
...     @param gamma: discount factor
...     @param threshold: the evaluation will stop once values
 for all states are less than the threshold
...     @return: optimal values and the optimal policy for the 
 given environment
...     """
...     n_state = env['n_state']
...     policy = torch.zeros(n_state).int()
...     while True:
...         V = policy_evaluation(env, policy, gamma, threshold)
...         policy_improved = policy_improvement(env, V, gamma)
...         if torch.equal(policy_improved, policy):
...             return V, policy_improved
...         policy = policy_improved
```

1.  æœ€åŽï¼Œæˆ‘ä»¬å°†çŽ¯å¢ƒã€æŠ˜æ‰£å› å­å’Œæ”¶æ•›é˜ˆå€¼æ’å…¥ä»¥è®¡ç®—æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ã€‚æˆ‘ä»¬è®°å½•è§£å†³ MDP æ‰€èŠ±è´¹çš„æ—¶é—´ï¼š

```py
>>> start_time = time.time()
>>> V_optimal, optimal_policy 
 = policy_iteration(env, gamma, threshold)
>>> print("It takes {:.3f}s to solve with policy 
 iteration".format(time.time() - start_time))
It takes 2.002s to solve with policy iteration
```

1.  æŸ¥çœ‹åˆšåˆšèŽ·å¾—çš„æœ€ä¼˜å€¼å’Œæœ€ä¼˜ç­–ç•¥ï¼š

```py
>>> print('Optimal values:\n{}'.format(V_optimal))
>>> print('Optimal policy:\n{}'.format(optimal_policy))
```

# å®ƒæ˜¯å¦‚ä½•è¿ä½œçš„â€¦â€¦

åœ¨æ‰§è¡Œ*ç¬¬ 7 æ­¥*ä¸­çš„ä»£ç è¡ŒåŽï¼Œæ‚¨å°†çœ‹åˆ°æœ€ä¼˜ç­–ç•¥å€¼ï¼š

```py
Optimal values:
tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,
 0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,
 0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,
 0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,
 0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,
 0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,
 0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,
 0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,
 0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,
 0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,
 0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,
 0.9643, 0.0000])
```

æ‚¨è¿˜å°†çœ‹åˆ°æœ€ä¼˜ç­–ç•¥ï¼š

```py
Optimal policy:
tensor([ 0,  1, 2, 3, 4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 22, 29, 30, 31, 32, 33,  9, 35,
 36, 37, 38, 11, 40,  9, 42, 43, 44, 5, 4,  3, 2, 1, 50, 1, 2, 47,
 4, 5, 44,  7, 8, 9, 10, 11, 38, 12, 36, 35, 34, 17, 32, 19, 30,  4,
 3, 2, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,
 10, 9, 8,  7, 6, 5, 4,  3, 2, 1], dtype=torch.int32)
```

*ç¬¬ 8 æ­¥* ç”Ÿæˆäº†ä»¥ä¸‹æœ€ä¼˜ç­–ç•¥å€¼çš„å›¾è¡¨ï¼š

![](img/030a3b7f-e8f7-4a1d-8536-44740a2774d3.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œéšç€èµ„æœ¬ï¼ˆçŠ¶æ€ï¼‰çš„å¢žåŠ ï¼Œä¼°è®¡çš„å¥–åŠ±ï¼ˆç­–ç•¥å€¼ï¼‰ä¹Ÿåœ¨å¢žåŠ ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ã€‚

åœ¨*ç¬¬ 9 æ­¥*ä¸­æˆ‘ä»¬æ‰€åšçš„äº‹æƒ…ä¸Ž*Solving an MDP with a policy iteration algorithm*é…æ–¹ä¸­çš„æ‰€åšçš„éžå¸¸ç›¸ä¼¼ï¼Œä½†è¿™æ¬¡æ˜¯é’ˆå¯¹æŠ›ç¡¬å¸èµŒåšçŽ¯å¢ƒã€‚

åœ¨*ç¬¬ 10 æ­¥*ä¸­ï¼Œç­–ç•¥æ”¹è¿›å‡½æ•°ä»Žç»™å®šçš„ç­–ç•¥å€¼ä¸­æå–å‡ºæ”¹è¿›çš„ç­–ç•¥ï¼ŒåŸºäºŽè´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ã€‚

æ­£å¦‚æ‚¨åœ¨*ç¬¬ 12 æ­¥*ä¸­æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬é€šè¿‡ç­–ç•¥è¿­ä»£åœ¨`2.002`ç§’å†…è§£å†³äº†èµŒåšé—®é¢˜ï¼Œæ¯”å€¼è¿­ä»£æ‰€èŠ±è´¹çš„æ—¶é—´å°‘äº†ä¸€åŠã€‚

æˆ‘ä»¬ä»Ž*ç¬¬ 13 æ­¥*å¾—åˆ°çš„ç»“æžœåŒ…æ‹¬ä»¥ä¸‹æœ€ä¼˜å€¼ï¼š

```py
Optimal values:
tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,
 0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,
 0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,
 0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,
 0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,
 0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,
 0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,
 0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,
 0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,
 0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,
 0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,
 0.9643, 0.0000])
```

å®ƒä»¬è¿˜åŒ…æ‹¬æœ€ä¼˜ç­–ç•¥ï¼š

```py
Optimal policy:
tensor([ 0,  1, 2, 3, 4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,
 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 22, 29, 30, 31, 32, 33,  9, 35,
 36, 37, 38, 11, 40,  9, 42, 43, 44, 5, 4,  3, 2, 1, 50, 1, 2, 47,
 4, 5, 44,  7, 8, 9, 10, 11, 38, 12, 36, 35, 34, 17, 32, 19, 30,  4,
 3, 2, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,
 10, 9, 8,  7, 6, 5, 4,  3, 2, 1, 0], dtype=torch.int32)
```

æ¥è‡ªå€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£çš„ä¸¤ç§æ–¹æ³•çš„ç»“æžœæ˜¯ä¸€è‡´çš„ã€‚

æˆ‘ä»¬é€šè¿‡å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£è§£å†³äº†èµŒåšé—®é¢˜ã€‚å¤„ç†å¼ºåŒ–å­¦ä¹ é—®é¢˜ä¸­æœ€æ£˜æ‰‹çš„ä»»åŠ¡ä¹‹ä¸€æ˜¯å°†è¿‡ç¨‹å½¢å¼åŒ–ä¸º MDPã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œé€šè¿‡ä¸‹æ³¨ä¸€å®šçš„èµŒæ³¨ï¼ˆåŠ¨ä½œï¼‰ï¼Œå°†å½“å‰èµ„æœ¬ï¼ˆçŠ¶æ€ï¼‰çš„ç­–ç•¥è½¬åŒ–ä¸ºæ–°èµ„æœ¬ï¼ˆæ–°çŠ¶æ€ï¼‰ã€‚æœ€ä¼˜ç­–ç•¥æœ€å¤§åŒ–äº†èµ¢å¾—æ¸¸æˆçš„æ¦‚çŽ‡ï¼ˆ+1 å¥–åŠ±ï¼‰ï¼Œå¹¶åœ¨æœ€ä¼˜ç­–ç•¥ä¸‹è¯„ä¼°äº†èµ¢å¾—æ¸¸æˆçš„æ¦‚çŽ‡ã€‚

å¦ä¸€ä¸ªæœ‰è¶£çš„äº‹æƒ…æ˜¯æ³¨æ„æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­å¦‚ä½•ç¡®å®šè´å°”æ›¼æ–¹ç¨‹ä¸­çš„è½¬æ¢æ¦‚çŽ‡å’Œæ–°çŠ¶æ€ã€‚åœ¨çŠ¶æ€ s ä¸­é‡‡å–åŠ¨ä½œ aï¼ˆæ‹¥æœ‰èµ„æœ¬ s å¹¶ä¸‹æ³¨ 1 ç¾Žå…ƒï¼‰ï¼Œå°†æœ‰ä¸¤ç§å¯èƒ½çš„ç»“æžœï¼š

+   å¦‚æžœç¡¬å¸æ­£é¢æœä¸Šï¼Œåˆ™ç§»åŠ¨åˆ°æ–°çŠ¶æ€ s+aã€‚å› æ­¤ï¼Œè½¬æ¢æ¦‚çŽ‡ç­‰äºŽæ­£é¢æœä¸Šçš„æ¦‚çŽ‡ã€‚

+   å¦‚æžœç¡¬å¸åé¢æœä¸Šï¼Œåˆ™ç§»åŠ¨åˆ°æ–°çŠ¶æ€ s-aã€‚å› æ­¤ï¼Œè½¬æ¢æ¦‚çŽ‡ç­‰äºŽåé¢æœä¸Šçš„æ¦‚çŽ‡ã€‚

è¿™ä¸Ž FrozenLake çŽ¯å¢ƒéžå¸¸ç›¸ä¼¼ï¼Œä»£ç†äººåªæœ‰ä»¥ä¸€å®šæ¦‚çŽ‡ç€é™†åœ¨é¢„æœŸçš„ç“¦ç‰‡ä¸Šã€‚

æˆ‘ä»¬è¿˜éªŒè¯äº†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç­–ç•¥è¿­ä»£æ¯”å€¼è¿­ä»£æ”¶æ•›æ›´å¿«ã€‚è¿™æ˜¯å› ä¸ºå¯èƒ½æœ‰å¤šè¾¾ 50 ä¸ªå¯èƒ½çš„è¡ŒåŠ¨ï¼Œè¿™æ¯” FrozenLake ä¸­çš„ 4 ä¸ªè¡ŒåŠ¨æ›´å¤šã€‚å¯¹äºŽå…·æœ‰å¤§é‡è¡ŒåŠ¨çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼Œç”¨ç­–ç•¥è¿­ä»£è§£å†³æ¯”å€¼è¿­ä»£æ›´æœ‰æ•ˆçŽ‡ã€‚

# è¿˜æœ‰æ›´å¤š...

ä½ å¯èƒ½æƒ³çŸ¥é“æœ€ä¼˜ç­–ç•¥æ˜¯å¦çœŸçš„æœ‰æ•ˆã€‚è®©æˆ‘ä»¬åƒèªæ˜Žçš„èµŒå¾’ä¸€æ ·çŽ© 10,000 ä¸ªå‰§é›†çš„æ¸¸æˆã€‚æˆ‘ä»¬å°†æ¯”è¾ƒæœ€ä¼˜ç­–ç•¥ä¸Žå¦å¤–ä¸¤ç§ç­–ç•¥ï¼šä¿å®ˆç­–ç•¥ï¼ˆæ¯è½®ä¸‹æ³¨ä¸€ç¾Žå…ƒï¼‰å’Œéšæœºç­–ç•¥ï¼ˆä¸‹æ³¨éšæœºé‡‘é¢ï¼‰ï¼š

1.  æˆ‘ä»¬é¦–å…ˆé€šè¿‡å®šä¹‰ä¸‰ç§ä¸Šè¿°çš„æŠ•æ³¨ç­–ç•¥å¼€å§‹ã€‚

æˆ‘ä»¬é¦–å…ˆå®šä¹‰æœ€ä¼˜ç­–ç•¥ï¼š

```py
>>> def optimal_strategy(capital):
...     return optimal_policy[capital].item()
```

ç„¶åŽæˆ‘ä»¬å®šä¹‰ä¿å®ˆç­–ç•¥ï¼š

```py
>>> def conservative_strategy(capital):
...     return 1
```

æœ€åŽï¼Œæˆ‘ä»¬å®šä¹‰éšæœºç­–ç•¥ï¼š

```py
>>> def random_strategy(capital):
...     return torch.randint(1, capital + 1, (1,)).item()
```

1.  å®šä¹‰ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œç”¨ä¸€ç§ç­–ç•¥è¿è¡Œä¸€ä¸ªå‰§é›†ï¼Œå¹¶è¿”å›žæ¸¸æˆæ˜¯å¦èŽ·èƒœï¼š

```py
>>> def run_episode(head_prob, capital, policy):
...     while capital > 0:
...         bet = policy(capital)
...         if torch.rand(1).item() < head_prob:
...             capital += bet
...             if capital >= 100:
...                 return 1
...         else:
...             capital -= bet
...     return 0
```

1.  æŒ‡å®šä¸€ä¸ªèµ·å§‹èµ„æœ¬ï¼ˆå‡è®¾æ˜¯`50`ç¾Žå…ƒï¼‰å’Œä¸€å®šæ•°é‡çš„å‰§é›†ï¼ˆ`10000`ï¼‰ï¼š

```py
>>> capital = 50
>>> n_episode = 10000
```

1.  è¿è¡Œ 10,000 ä¸ªå‰§é›†å¹¶è·Ÿè¸ªèŽ·èƒœæ¬¡æ•°ï¼š

```py
>>> n_win_random = 0
>>> n_win_conservative = 0
>>> n_win_optimal = 0
>>> for episode in range(n_episode):
...     n_win_random += run_episode(
 head_prob, capital, random_strategy)
...     n_win_conservative += run_episode(
 head_prob, capital, conservative_strategy)
...     n_win_optimal += run_episode(
 head_prob, capital, optimal_strategy)
```

1.  æ‰“å°å‡ºä¸‰ç§ç­–ç•¥çš„èŽ·èƒœæ¦‚çŽ‡ï¼š

```py
>>> print('Average winning probability under the random 
 policy: {}'.format(n_win_random/n_episode))
Average winning probability under the random policy: 0.2251
>>> print('Average winning probability under the conservative 
 policy: {}'.format(n_win_conservative/n_episode))
Average winning probability under the conservative policy: 0.0
>>> print('Average winning probability under the optimal 
 policy: {}'.format(n_win_optimal/n_episode))
Average winning probability under the optimal policy: 0.3947
```

æˆ‘ä»¬çš„æœ€ä¼˜ç­–ç•¥æ˜¾ç„¶æ˜¯èµ¢å®¶ï¼
