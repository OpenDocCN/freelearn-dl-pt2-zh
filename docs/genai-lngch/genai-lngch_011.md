# 10 生成模型的未来

## 加入我们的书籍社区 Discord

[`packt.link/EarlyAccessCommunity`](https://packt.link/EarlyAccessCommunity)

![自动生成的二维码描述](img/file65.png)

在这本书中，到目前为止，我们已经讨论了用于构建应用程序的生成模型。我们探讨了 LLM 和图像模型用于内容创建，工具使用，代理策略，检索增强生成的语义搜索以及使用提示和微调来条件化模型。此外，我们还实现了一些简单的应用程序，例如为开发人员和数据科学家。在本章中，我们将讨论这给我们留下了什么，以及未来将引领我们何方。过去一年人工智能的进展速度急剧加快，像 DALL-E，Midjourney 和 ChatGPT 这样的突破带来了惊人的成果。这些生成式人工智能模型可以创建逼真的图像，撰写文章和代码，并具有超过大多数人类的对话能力。2022 年，生成式人工智能初创公司的风险投资激增，几乎与过去五年的总投资额相匹配。最近，像 Salesforce 和 Accenture 这样的主要参与者已经做出了对生成式人工智能的数十亿美元的重大承诺。将基础模型根据特定用例进行独特定制被视为真正的价值创造机会。但尚不清楚哪些实体 - 大型科技公司，初创公司或基础模型开发者 - 将获得最大的上升空间。从技术上讲，像 ChatGPT 这样的生成模型通常作为黑匣子运作，对其决策过程的透明度有限。模型可解释性的缺乏使得完全理解模型行为或控制输出变得困难。还存在关于可能由于不完善的训练数据而产生偏见的担忧。在实践层面上，生成模型需要大量的计算资源进行训练和部署。对于许多组织来说，获得有效利用这些人工智能系统的基础设施仍然是一项障碍。积极的一面是，人工智能可以使技能民主化，使业余人员能够以专业品质进行设计，写作等。企业可以从更快，更便宜，按需的工作中受益。然而，存在着关于工作岗位流失的重大担忧，特别是对于专业化的中产阶级角色，如平面设计师，律师和医生。他们的工作正在被自动化，而低技能工人则学会利用人工智能作为超能力。更不祥的是，人工智能可能会被军事，恐怖分子，罪犯和政府用于宣传和影响。实时产生的深度伪造品将推动欺诈活动并侵蚀信任。前进的道路在热情和实用性之间取得平衡，优先考虑人类尊严。通过承认风险，促进开放讨论并制定周到的政策，我们可以建立一个由人工智能活力可能性推动的公平未来。本章的主要部分包括：

+   当前生成式 AI 的状态

+   可能的未来能力

+   社会影响

+   实际实施

+   未来之路

让我们从模型的当前状态和它们的能力开始。

## 当前生成式 AI 的状态

正如本书中所讨论的，在过去几年里，生成式 AI 模型已经在跨模态包括文本、图像、音频和视频的人类样式内容上取得了新的里程碑。领先的模型如 OpenAI 的 GPT-4 和 DALL-E 2，Google 的 Imagen 和 Parti，以及 Anthropic 的 Claude 在语言生成方面表现出惊人的流畅性以及创造性的视觉艺术性。在 2022 年至 2023 年期间，模型取得了长足的进步。如果生成模型以前只能产生勉强连贯的文本或颗粒状的图像，现在我们看到了高质量的 3D 模型、视频，并生成连贯和上下文相关的散文和对话，与人类的流畅水平相媲美甚至超越。这些 AI 模型利用巨大的数据集和计算规模，使它们能够捕捉复杂的语言模式，展现对世界知识的细致理解，翻译文本，总结内容，回答自然语言问题，创作吸引人的视觉艺术，并获得描述图像的能力。看似神奇的是，AI 生成的输出仿佛人类的智慧——绘制原创艺术品、写诗、产生人类水平的散文，甚至从各种来源复杂地聚合和合成信息。但让我们更加细致一些。生成模型除了优点外还有弱点。与人类认知相比，缺陷仍然存在，包括经常生成看似合理但错误或荒谬的陈述。幻觉显示出缺乏现实基础，因为它们基于数据中的模式而不是对真实世界的理解。此外，模型在进行数学、逻辑或因果推理方面存在困难。它们很容易被复杂的推理问题所困扰，这可能限制它们在某些工作领域的适用性。对于预测以及模型本身缺乏可解释性的黑匣子问题阻碍了故障排除工作，并且在所需参数内控制模型行为仍然具有挑战性。AI 模型可能表现出有害的意外偏见，这引发了重大的道德关切——这个问题主要是由训练数据本身存在的偏见所致。这个偏见问题不仅会扭曲输出，还会传播和放大社会的不平等。以下是一张表，将当前 LLM 相对于人类认知的主要优势和不足之处可视化：

| **LLM 的优势** | **LLM 的不足之处** |
| --- | --- |
| 语言流畅性 - 能够生成语法连贯、上下文相关的散文和对话。GPT-4 能够产生人类水平的散文。 | 事实准确性 - LLM 经常生成看似合理但错误或荒谬的陈述。缺乏现实基础。 |
| 知识综合 - 多元来源信息的复杂聚合和展示。 | 逻辑推理 - 无法进行数学、逻辑或因果推理。容易被复杂的推理问题所困扰。 |
| 创造性输出 - 反映人类创造力的想象力和原创文本、艺术、音乐。克劳德写诗，DALL-E 2 绘制原创艺术。 | 可控性 - 难以限制模型行为在期望的参数内。可能展现出有害的意外偏见。 |
|  | 偏见 - 有可能传播和放大训练数据中存在的社会偏见。引发了伦理关切。 |
|  | 透明度 - 对模型预测的可解释性缺乏。"黑盒子"问题限制了故障排除。 |

图 10.1：LLM 的优势和不足。

尽管生成式人工智能的能力已经取得了长足的进步，但它们存在的问题领域需要解决，以便这些技术能够在未来有效地发挥作用。尽管如此，它们的深远潜力预示着一个令人兴奋的未来，如果能够负责任地开发和监管。生成模型的弱点定义了一些技术挑战，我们现在将看到。 

### 技术挑战

尽管取得了快速进展，但要安全、负责任地实现生成式人工智能的全部潜力仍然存在重大技术障碍。正如前面提到的，尽管生成式人工智能模型取得了相当大的进步，但它们仍然面临着重大的技术挑战，需要克服这些挑战才能安全、负责任地发挥其全部潜力。我们已经在之前的章节中讨论了其中一些问题和潜在解决方案。这张表总结了其中一些挑战以及解决这些挑战的技术方法的概况：

| **挑战** | **描述** | **潜在解决方案** |
| --- | --- | --- |
| 真实多样的内容生成 | 现有模型在逻辑一致性和事实可信度方面存在困难。生成的样本重复、乏味，缺乏人类细微差别。 | 通过人类反馈进行强化学习数据增强和综合技术模块化领域知识 |
| 输出质量控制 | 缺乏可靠约束生成内容属性的机制。模型偶尔产生有害、偏见或荒谬的结果。 | 受限优化目标调节系统中断和校正技术 |
| 避免偏见 | 模型无意中放大了训练数据中存在的社会偏见。开发技术来削减偏见仍然很困难。 | 平衡和代表性的训练数据偏见缓解算法持续的测试和审计 |
| 事实准确性 | 无法推理客观真相限制了其在现实世界应用中的可靠性。基于常识和物理学的模型是一个未解之谜。 | 整合知识库混合神经符号架构检索增强生成 |
| 可解释性 | 大型神经网络的不透明行为对故障排除或偏见构成障碍，因此需要可解释的人工智能技术。 | 模型内省技术概念归因方法简化模型架构 |
| 数据隐私 | 收集和处理大规模数据集会带来关于同意、匿名化、访问控制和数据滥用的挑战。 | 差分隐私和安全多方计算合成数据生成联邦学习 |
| 延迟和计算 | 部署庞大的模型需要大量计算资源，延迟了许多应用所需的实时交互。 | 将模型压缩为更小的形式优化推断引擎专用的人工智能硬件加速器 |
| 数据许可证 | 组织可能需要获取商业许可证来使用现有数据集或构建定制数据集来训练生成模型。这可能是一个复杂和耗时的过程。 | 开源和合成数据 |

图 10.2：技术挑战和潜在解决方案。

主要的问题是，这些模型生成的内容通常受到缺乏现实感和多样性的限制。尽管它们展示了令人印象深刻的能力来模仿类似人类的语言和创造力，但在生成逻辑一致且事实可信的内容方面，它们仍然表现不佳。它们的输出往往缺乏人类的细微差别，结果变得相当重复和单调。潜在的解决方案包括从人类反馈中进行强化学习以提高连贯性和细微差别，受控数据增强和合成技术，以及结合模块化领域知识的架构。另一个关键障碍是控制输出质量。尽管经过严格的培训和开发，现有的 AI 机制在可靠地约束生成内容的属性方面仍然不足。这导致不时产生可能有害、带有偏见或完全荒谬的内容，这对它们的广泛接受和应用构成了风险。有希望的方法包括约束优化目标、人在循环调节系统，以及在生成过程中中断和更正模型输出的技术。偏见确实是这些 AI 模型的一个主要问题，因为它们经常无意中放大了埋藏在训练数据中的社会偏见。开发纠正这种偏见的技术仍然是一个复杂的问题。像平衡和代表性训练数据、偏见缓解算法，以及为公平性进行持续测试和审计的策略都旨在解决这个问题。这些 AI 模型无法推理客观事实的能力明显限制了它们在现实世界应用中的可靠性。将这些模型基于常识和物理学进行建模是 AI 社区仍在努力解决的一个悬而未决的问题。混合神经符号结构、知识库的整合，以及检索增强生成都提供了有希望的方向。AI 的黑箱性质提出了另一个复杂的挑战：可解释性。大型神经网络的不透明行为为故障排除或偏见设置了障碍，这强调了对更透明的 AI 技术的需求。模型内省、概念归因方法以及简化模型架构可能提供解决方案。此外，由于对大量数据集的收集和处理，数据隐私问题变得突出起来。这一方面引入了关于同意、匿名化、访问控制和数据滥用的挑战。差分隐私、安全多方计算、合成数据生成和联邦学习等技术可能有助于解决隐私风险。最后但同样重要的是，部署这些庞大模型需要大量的计算资源，导致显著的延迟和计算问题。这可能会延迟许多应用所需的实时交互性，这表明提高效率是关键。解决方案包括将模型压缩成更小的形式因子、优化推理引擎和专用的 AI 硬件加速器。展望未来，生成式 AI 系统有望变得更加强大和多样化。让我们拭目以待！

## 可能的未来能力

目前，非常大型模型的训练计算的加倍时间约为 8 个月，超过了莫尔定律（晶体管密度的成本每 18 个月增加一倍）或洛克定律（像 GPU 和 TPU 这样的硬件成本每 4 年减半）等缩放定律。这张图表明了大型模型训练计算的这一趋势（来源：Epoch，《机器学习中参数、计算和数据趋势》。在线发布在 epochai.org。检索自：https://epochai.org/mlinputs/visualization）：

![图 10.3：显著 AI 系统的训练 FLOPs。](img/file66.png)

图 10.3：显著 AI 系统的训练 FLOPs。

正如第一章所讨论的，大型系统的参数规模正在以与训练计算相似的速度增长，这意味着如果这种增长持续下去，我们可能会看到更大更昂贵的系统。经验推导的缩放定律预测了基于训练预算、数据集大小和参数数量的 LLM 性能。这意味着高度强大的系统可能会集中在大型科技公司手中。

> **KM 缩放**定律，由 Kaplan 和同事提出，通过对模型性能与不同数据大小、模型大小和训练计算的拟合进行经验分析，提出了幂律关系，表明了模型性能与模型大小、数据集大小和训练计算等因素之间的强烈依赖关系。
> 
> > **Chinchilla 缩放定律**，由 Google DeepMind 团队开发，涉及对更广泛范围的模型大小和数据大小进行实验，并建议将计算预算最优地分配到模型大小和数据大小，这可以通过在约束条件下优化特定损失函数来确定。

未来的进展可能更多地取决于数据效率和模型质量，而不是纯粹的规模大小。尽管庞大的模型抓住了头条，但计算能力和能源限制限制了模型的不受限制的增长。未来将会看到庞大、通用模型与更小、可访问的专业化细分模型共存，这些模型提供更快、更便宜的培训、维护和推理。已经有研究表明，更小的专业化模型可以表现出很高的性能。我们最近看到了像 phi-1（*只需教科书*，2023 年，Gunasekar 和同事）这样的模型，拥有 10 亿个参数数量级，尽管规模较小，但在评估基准上却能取得高精度。作者认为，提高数据质量可以极大地改变规模定律的形态。更多的工作表明，模型可以大大缩小，而精度只会略微降低（*只需一个宽度的前馈*，Pessoa Pires 等人，2023 年），这支持了对模型训练和访问的民主化的论点。此外，迁移学习、蒸馏和提示技术等技术可以使更小的模型利用大型基础的能力，而不需要复制它们的成本。为了弥补限制，像搜索引擎和计算器这样的工具已经被纳入到代理人和多步推理策略中，插件和扩展可能越来越多地用于扩展功能。由于不同因素的影响，AI 培训成本正在下降——根据 ARK 投资管理有限责任公司的数据，约为每年 70%。最近由 Mosaic ML 发布的 AI 培训工具可以将语言模型培训到与 GPT-3 相同水平的性能，成本大约只有两年前估计的 460 万美元的十分之一。这将促进实验，但进步将更多地来自培训制度、数据质量和新型架构，而不仅仅是模型的大小。在由资源丰富的大型科技公司主导的军备竞赛之后，负责任的、经济实惠的创新可能成为优先事项。在 3-5 年的时间范围内（2025-2027 年），围绕计算和人才可用性的限制可能会大大缓解，削弱了中心化壕沟。具体来说，如果云计算成本如预期般下降，而且 AI 技能通过教育和自动化工具变得更加普及，那么自我训练定制的 LLM 可能对许多公司来说变得可行。这可以更好地满足个性化和数据隐私的需求。然而，有些能力，如上下文学习，根据规模定律是不可预测的，只会在大型模型中出现。进一步推测，即使在更多数据的基础上训练出巨大的模型，也可能表现出更多的行为和技能，极端的规模化最终可能会产生**人工通用智能**（**AGI**）——与或超出人类智力相当的推理。然而，从神经科学的角度来看，在我们当前的技术阶段，AGI 接管世界的威胁似乎被夸大了（参见 Jaan Aru 等人，*通过神经科学的视角审视人工意识的可行性*；2023 年）：

+   **缺乏具象、嵌入信息**：当前的大型语言模型仅在文本数据上接受训练，而不是像人类那样通过丰富的多模态输入来发展对物理世界的常识推理。这种缺乏扎根的学习给发展人类级别的智能带来了重大障碍。

+   **与生物大脑的不同架构**：类似 GPT-4 这样的模型中使用的相对简单的堆叠变压器架构缺乏被认为能够启动人类意识和一般推理的复杂经常性和层次性结构。

+   **狭窄的能力**：现有模型仍然专门针对特定领域，如文本，并在灵活性、因果推理、规划、社交技能和一般问题解决智能方面表现不佳。这种情况可能会随着工具使用的增加或模型的根本性变化而改变。

+   **社会能力或意图有限**：当前的人工智能系统没有固有动机、社交智能或超出培训目标的意图。对恶意目标或对统治的欲望的担忧似乎没有根据。

+   **有限的现实世界知识**：尽管摄取了大量数据集，大型模型的事实知识和常识与人类相比仍然非常有限。这影响了在物理世界中的适用性。

+   **基于数据驱动的局限性**：依赖于训练数据中的模式识别，而非结构化知识，使得可靠地推广到新颖情况变得困难。

鉴于这些论点，如今人工智能迅速演化为恶意超级智能的风险似乎高度不太可能。也就是说，随着能力不断提升，深思熟虑地处理长期安全研究和伦理问题仍然是明智之举。但是，当前神经科学或模型能力的证据并不支持对即将到来的人工智能普遍性的不可避免和迫在眉睫的主张。然而，快速进步的速度引发了人类淘汰和失业的担忧，这可能进一步分化经济阶级。与过去的物理自动化不同，生成式人工智能威胁到先前被认为免于自动化的认知工作类别。道德和公平地管理这种劳动力转型将需要远见和规划。还有关于人工智能是否应该创作反映人类状况的艺术、文学或音乐的哲学辩论。让我们更广泛地考虑社会影响吧！

## 社会影响

高度完善的生成式人工智能的出现很可能会在未来几年改变社会的许多方面。随着生成模型的不断发展和为企业和创意项目增加价值，生成式人工智能将塑造技术和人类交互在各个领域的未来。尽管它们的广泛应用为企业和个人带来了许多好处和机遇，但对于日益依赖各个领域的 AI 模型所引发的伦理和社会问题，有必要加以重视和解决。如果能够慎重部署，生成式人工智能在个人、社会和工业领域都能带来巨大的潜在好处。在个人层面，这些模型可以增强创造力和生产力，增加对健康、教育和金融等服务的可及性。它们能够使知识资源的获取民主化，通过综合专业知识帮助学生学习或为专业人士做决策。作为虚拟助手，它们能够提供即时定制的信息，以便完成例行任务。通过自动化机械任务，它们可能会释放出人类的时间，从而提高更高价值工作的经济产出。从经济上讲，生产力的提高很可能会导致某些工作类别的大规模中断。新兴产业和工作可能会出现来支持 AI 系统。认真考虑和解决这些变化是至关重要的。随着模型的不断改进和运行成本的降低，这可能会触发生成式人工智能和 LLM 应用在新领域大规模扩展。除了硬件成本的降低外，根据赖特定律，随着每次累计生产倍增，成本可能会以 10-30%的速度下降。这种成本曲线反映了诸如代码、工具和技术的重复利用等效率。随着成本的降低扩大了采用率，进一步推动了成本的降低。这将导致一个反馈周期，更多的效率推动更多的使用驱动更多的效率。

> **赖特定律**，又称**经验曲线效应**，是经济学和商业领域的一项观察，它指出对于许多产品，成本在每次累计生产翻倍时都会以固定百分比下降。具体来说，它指出在每次累计生产翻倍时，成本 tend to decrease by a fixed percentage（通常在 10-30%范围内）。
> 
> > 该定律以西奥多·保罗·赖特命名，他是一位美国飞机工程师，于 1936 年在分析飞机生产成本趋势时首次观察到这一现象。赖特注意到，每当飞机机身的累计生产量翻倍时，生产所需的劳动力就会减少 10-15%。
> > 
> > 这种关系可以用数学方式表达为：
> > 
> > ![](img/file67.png)
> > 
> > 其中，*C*[1]代表生产第一单位的成本，*C*[*x*]代表生产第 x 单位的成本，b 代表进步比率，该比率在许多行业估计在 0.75 到 0.9 之间。
> > 
> > 沃特定律背后的逻辑是随着生产增加，工人通过实践、标准化工作流程和开发更好的工具和流程，在制造产品方面变得更加高效。公司还会找到优化供应链、物流和资源利用的方法，以降低成本。

在工业上，这些模型带来了广泛的机会，可以增强人类的能力并改变工作流程。在内容生产中，生成式人工智能可以比人类更快地为营销活动或新闻报道起草初稿，从而促进更大的创造力和定制。对于开发人员，自动生成的代码和快速迭代可以加快软件构建的速度。研究人员可以快速从论文中综合出新的发现，推动科学的进步。生成式人工智能还可以以大规模进行个性化定制。推荐内容可以精细到个人。市场推广可以针对不同的细分市场和地理位置进行定制。总的来说，这些模型可以提高从工业设计到供应链等各个领域的生产率。至于这项技术的推广，存在两种主要的情景。在第一种情况下，每家公司或个人都会使用自己的专有数据来训练定制模型。然而，这需要相当的人工智能/机器学习专业知识才能正确地开发、训练和部署这样的系统——这样的专业人才目前依然稀缺且昂贵。计算成本也极其高昂，专门的硬件如大量昂贵的 GPU 集群只适用于大型实体。在模型以敏感信息进行训练时也存在进一步的数据隐私合规风险。如果这些围绕专业知识、计算要求和数据隐私的障碍能够克服，那么精细调整的 LLM（大语言模型）可以显著提高组织的生产率和效率，通过自动化例行任务和提供适合特定业务的见解。然而，一个缺点是在小型私有数据集上训练的模型可能缺乏大规模公共语料库上训练的模型的泛化能力。集中式和自助式模型可以共存，为不同的用例提供服务。短期内，大型科技公司在提供行业特定的精细调整服务方面具有优势。但随着时间的推移，更多的内部训练可能会出现，这是由定制和隐私需求驱动的。降低成本、传播专业知识和解决健壮性挑战的进展速度将决定集中式优势的持续时间。在这些领域的快速创新有利于瓦解留下的障碍，但围绕主导性框架、数据集和模型的平台效应可能使当前领导者继续聚集。如果出现了简化和自动化人工智能开发的强大工具，定制的生成模型甚至可能对地方政府、社区组织和个人来应对超局部挑战具有可行性。尽管目前大型科技公司受益于规模经济，但由小型实体驱动的分布式创新可能会释放生成式人工智能在社会各个领域的全部潜力。最后，生成式人工智能的出现与我们如何生产和消费创意作品的更广泛变化相交。互联网已经培育出了一个混搭文化，其中衍生作品和协作内容的创建非常普遍。随着人工智能模型通过重新组合现有材料生成新的作品，它们符合混搭文化的迭代、集体生产原则。然而，生成模型可以合成和重新利用受版权保护的内容的规模提出了棘手的法律问题。利用广泛数据集训练的模型，包括书籍、文章、图像等，归因权和版税问题可能变得非常复杂。当前的检测机制无法以高于机会水平的准确率找到由生成式人工智能创作的内容。这反映出了围绕作者身份和版权法的更广泛争论。让我们来看看生成模型在不同方面将产生深远的近期影响，从创意努力开始。

### 创意产业和广告

游戏和娱乐产业正在利用生成式人工智能来打造独特沉浸式的用户体验。通过自动化创意任务可以获得主要的效率提升，提高在线休闲时间。生成式人工智能可以使机器通过学习模式和示例来生成新的原创内容，如艺术、音乐和文学。这对创意产业有着重要影响，它可以增强创意过程，可能创造新的收入来源。它还为媒体、电影和广告开启了新的个性化、动态内容创作规模。然而，生成内容在全面部署之前需要有关准确性和消除偏见的广泛质量控制。在新闻业中，使用大规模数据集进行自动生成文章可以使记者专注于更复杂的调查报道。**人工智能生成内容**（AIGC）在转变媒体制作和发布中发挥越来越重要的作用，增强了效率和多样性。在新闻业中，文本生成工具能够自动完成人类记者传统上完成的写作任务，显著提高生产效率，同时保持及时性。像美联社一样的媒体机构每年使用 AIGC 生成成千上万的报道。洛杉矶时报的 Quakebot 等机器记者可以快速生成有关突发新闻的文章。其他应用包括彭博新闻的 Butletin 服务，其中聊天机器人创建个性化的一句新闻摘要。AIGC 还使得可以使用 AI 虚拟主持人与真实主持人一起主持广播，通过模仿人类的外观和从文本输入获得语音，实现不同角度的广播效果。中国新闻机构新华社的虚拟主持人辛小薇就是一个例子，她可以以不同的角度播报新闻，产生沉浸式的效果。AIGC 正在从剧本创作转变到后期制作，AI 剧本写作工具分析数据以生成优化的剧本。视觉效果团队通过将 AI 增强的数字环境和年轻化与现场影像混合，实现沉浸式视觉效果。深度伪造技术可以逼真地重新制作或复活角色。AI 还可以完成自动生成字幕的任务，甚至通过对大量音频样本进行模型训练来预测无声电影中的对话。通过字幕来扩大可访问性，并且还可以在场景中同步重新配音。在后期制作中，使用像 Colourlab.Ai 和 Descript 这样的 AI 调色和编辑工具可以简化色彩校正等流程。在广告方面，AIGC 释放了高效、定制广告创意和个性化的新潜力。AI 生成内容使广告商能够以大规模为个人消费者创建个性化、引人入胜的广告。像创意广告系统（CAS）和个性化广告文案智能生成系统（SGS-PAC）这样的平台利用数据自动生成针对特定用户需求和兴趣的广告。AI 还协助广告创意和设计——像 Vinci 这样的工具可以根据用户的喜好从产品图像和口号生成定制的吸引人海报，而像 Brandmark.io 这样的公司则可以根据用户的偏好生成徽标变体。GAN 技术可以自动化生成带关键词的产品清单，用于有效的点对点营销。合成广告生产也在上升，实现高度个性化、可扩展的宣传活动，节省时间。在音乐方面，像 Google 的 Magenta、IBM 的 Watson Beat 或索尼 CSL 的 Flow Machine 等工具可以生成原创的旋律和作曲。AIVA 同样可以根据用户调试的参数创建独特的作曲。LANDR 的 AI 母带使用机器学习来处理和改善音频质量。在视觉艺术中，MidJourney 使用神经网络生成可以启动绘画项目的灵感图像。艺术家们已经使用其成果创作了多次获奖作品。DeepDream 的算法在图像上施加模式，创造出迷幻艺术作品。GAN 可以生成以期望风格为导

### 经济

部署生成式人工智能和其他技术可能有助于加速生产率增长，部分弥补就业增长下降，并促进整体经济增长。假设能源和计算可持续扩展，通过将生成式人工智能整合到业务流程中所带来的巨大生产率提升，似乎可能在未来十年推动许多任务的自动化。然而，这一过渡可能会扰乱劳动力市场，需要进行调整。麦肯锡公司的研究估计，到 2030-2060 年，30-50％的当前工作活动可能会被自动化。生成式人工智能到 2030 年每年可能会为全球生产力增加 6-8 万亿美元，为人工智能经济影响的先前估计增加 15-40％。根据 Tyna Eloundou 和同事的研究《GPTs 是 GPTs：对大型语言模型对劳动市场影响潜力的初步观察，2023 年》，大约 80％的美国工人至少有 10％的工作任务受到 LLMs 的影响，而 19％可能受到 50％以上任务的影响。影响跨越所有工资水平，高薪工作面临更大的暴露。仅有 15％的美国工人任务可以通过仅 LLMs 显着加快完成。但使用 LLM 技术，这一比例增加到所有任务的 47-56％，显示了互补技术的巨大影响。从地理角度看，生成式人工智能的外部私人投资主要来自科技巨头和风险投资公司，主要集中在北美，反映了该大陆目前对整体人工智能投资格局的主导地位。总部位于美国的与生成式人工智能相关的公司在 2020 年至 2022 年期间筹集了约 80 亿美元，占此类公司在该期间总投资额的 75％。自动化采用在发达经济体中可能会更快，由于高工资将使其更早经济上可行。工作自动化的规模并不直接等同于失业人数。与其他技术一样，生成式人工智能通常使职业中的个别活动自动化，而不是整个职业。然而，组织可能会决定通过减少某些职业类别的就业来实现提高生产率的好处。过去 30 年来 GDP 增长的主要引擎——生产率增长，在过去十年有所放缓。部署生成式人工智能和其他技术可能有助于加速生产率增长，部分弥补就业增长下降并促进整体经济增长。根据麦肯锡公司分析师的估算，从 2023 年到 2040 年，根据自动化采纳率的不同，个别工作活动的自动化可能会为全球经济每年提供 0.2 至 3.3％的生产率提升，然而，前提是受到这项技术影响的人员转移到其他工作活动，并至少与他们 2022 年的生产力水平相匹配。

### 教育

一个潜在的近期情景是，个性化人工智能导师和导师的崛起可能使得与人工智能驱动的经济相关的高需求技能的教育民主化。在教育领域，生成式人工智能已经改变了我们的教学和学习方式。像 ChatGPT 这样的工具可以用来自动生成个性化课程和定制内容，以满足个别学生的需求。这通过自动化重复的教学任务大大减轻了教师的工作负担。人工智能导师可以即时反馈学生写作作业，让教师有更多时间专注于更复杂的技能。由生成式人工智能驱动的虚拟模拟也可以创建吸引人的、量身定制的学习体验，适应不同学习者的需求和兴趣。然而，随着这些技术的发展，需要进一步研究围绕持续存在偏见和传播错误信息的风险。知识的加速增长和科学发现的过时意味着，培养孩子的好奇心驱动的学习应该集中于发展涉及启动和维持好奇心的认知机制，例如对知识盲区的意识和使用适当策略解决这些问题。尽管针对每个学生定制的人工智能导师可能会提高结果和参与度，但贫困学校可能会落后，加剧不平等。政府应该促进平等获取，以防止生成式人工智能成为富裕阶层的特权。民主化机会对所有学生来说仍然至关重要。如果经过深思熟虑实施，个性化的人工智能驱动的教育可以使任何有动力学习的人获得至关重要的技能。交互式人工智能助手可以根据学生的优势、需求和兴趣调整课程，使学习高效、吸引人且公平。但是，关于获取、偏见和社会化的挑战需要解决。

### 工作

假设能源和计算能够可持续扩展，那么将生成式人工智能集成到业务流程中所带来的巨大生产力提升似乎很可能在未来十年内实现自动化许多任务。这个过渡可能会扰乱劳动力市场，需要进行调整。人工智能的自动化可能会在短期内取代许多行政、客户服务、写作、法律和创意类工作，而农业和建筑等行业的职业可能几乎不受影响。然而，过去的工业革命最终导致了新型的工作和产业，尽管存在困难的劳动力过渡。人工智能自动化也可能出现类似的动态。这将在一定程度上影响几乎所有职业，尽管某些职业受到的影响更大。生成式人工智能分析和生成自然语言内容的能力可能显著增加像沟通、协作和报告等许多白领职业的自动化潜力。然而，整个职位被消除的程度仍然不确定。过去的技术创新最终创造了新的工作类型，即使过渡过程艰难。根据麦肯锡公司的研究，生成式人工智能在以下四个领域可以带来大约 75%的价值：客户运作、市场和销售、软件工程和研发。突出的例子包括生成式人工智能支持与客户的互动、为市场和销售生成创意内容以及根据自然语言提示起草计算机代码等任务。语言模型和生成式人工智能具有中断和自动化传统上由人类执行的各种行业任务的潜力。对于不同的职位，这些预测似乎是可信的：

+   初级软件工程师可能会被人工智能编码助手增强或取代。

+   分析师和顾问使用来自人工智能的数据洞察力，客服代理人被对话式人工智能取代。

+   技术撰稿人和新闻工作者受到人工智能内容生成的帮助。

+   教师利用人工智能进行课程准备和个性化辅导。

+   法律助理使用人工智能进行摘要和文件审查。

+   图形设计师将受到人工智能图像生成的赋能，然而，使图像的创建和操作对更多人开放，也可能对工资产生影响。

+   然而，对于高级软件工程师来开发专门的 AI 解决方案和系统的需求将继续保持强劲。

+   数据科学家可能会从构建预测模型转向更加专注于验证、调试和最大化 AI 系统的价值。

+   程序员将越来越多地编写工具来协助人工智能的开发。

+   类似提示工程的新职位开始出现。

人工智能能够执行一些任务，涵盖自然语言处理、内容创作，甚至是复杂的创造性工作，效率高，错误少于人类。技能较低的个体可能能够执行更高技能的工作，而技能较高的个体可能会面临较少的工作机会。例如，法律助理使用模板文件，并填写必要的信息以满足客户的需求。人工智能，配备了广泛的法律文件、立法、大学课程、期刊、新闻文章和法院案例的知识，可以比法律助理做得更好。结果是，对于起草目的，初级律师的需求可能会减少，法律助理使用人工智能软件来传达客户的具体要求。软件开发人员和数据科学家都可以从 LLM 的潜力中受益，但必须仔细考虑其能力和局限性，以实现最佳利用。对于初级开发人员和数据科学家来说，LLM 可以自动化例行任务，提供基本解决方案，并减少错误，通过释放更多时间进行更复杂的工作来加速学习。然而，仅依赖人工智能可能会阻碍更深层次的技术增长，因此 LLM 应被视为支持性工具，同时积极发展实践经验。高级开发人员和数据科学家具有超出当前人工智能能力的领域知识和问题解决能力。虽然自动化标准解决方案可能会节省一些时间，但他们的专业知识对于指导人工智能工具，确保可靠和可扩展的结果至关重要。人工智能生产力的激增意味着公司对人工智能人才的需求量很大，对于招聘和留住这些人才的竞争非常激烈。还将需要越来越多的网络安全专业人员来保护人工智能系统免受攻击。此外，随着人工智能系统变得更加普及，可能会有更多的工作涉及人工智能伦理、监管和公共政策等领域。因此，投资吸引和培养这样的人才对于公司在这个快速发展的领域保持相关性至关重要。所有行业的创作者都将受到影响。在音乐方面，人工智能正在帮助音乐家进行整个创作过程，从创作歌词和旋律到数字化母带和增强音频。生成艺术工具允许视觉艺术家尝试定制绘画，以迎合他们独特的风格。2023 年 3 月的高盛研究表明，行政和法律职位最容易受到影响。他们估计，约三分之二的现有工作将面临人工智能的自动化，并得出结论，生成性人工智能工具可能会影响全球 3 亿个全职工作岗位，占目前劳动力的 20%以上。采用的速度是一个关键的未知数。麦肯锡分析师估计，自动化可能吸收 60 到 70％的员工工时，因此在 2030 年至 2060 年之间，今天的工作活动中大约一半可能会被自动化。根据普华永道的数据，到 2030 年中期，高达 30%的工作可能是可以自动化的。但是，真实世界的采用取决于许多难以预测的因素，如监管、社会接受和再培训政策。软件和应用程序开发等知识工作领域已经开始看到这一转变的影响。生成性人工智能已被用于简化从初始代码生成到图像编辑和设计的任务。它减少了开发人员和设计师的重复手工工作，使他们能够将精力集中在更高价值的创新上。然而，对于自动生成的输出进行细致的监控和迭代纠正错误仍然至关重要。大规模自动化工作活动可能会导致劳动力需求的重大转变，从而导致职业的实质性变化，迫使员工获得新的技能。由于它们的能力基本上是为了进行认知任务，生成性人工智能很可能会对知识工作产生最大的影响，特别是涉及决策和协作的活动，而以前这些活动的自动化潜力较低。以前，自动化的影响主要集中在中低收入五分位数的工作中，而生成性人工智能可能会对高薪工作中的活动产生最大的影响。许多工人将需要在现有职业或新职业中实质性地改变他们的工作。他们还需要支持来进行过渡到新的活动。管理这种转变将需要政策远见，通过再培训计划、创造就业激励措施和

### 法律

生成模型如 LLMs 可以自动化常规法律任务，如合同审查、文件生成和案件准备。它们还可以实现更快速、全面的法律研究和分析。额外的应用包括用通俗的语言解释复杂的法律概念，并使用案例数据预测诉讼结果。然而，鉴于透明度、公平性和问责制等考虑，负责任和道德的使用仍然至关重要。总体而言，正确实施的 AI 工具承诺提高法律生产力和司法准入，同时需要持续关注可靠性和伦理问题。

### 制造业

在汽车行业，它们被用来为模拟生成 3D 环境，并帮助汽车的开发。此外，生成式人工智能也被用于使用合成数据对自动驾驶车辆进行道路测试。这些模型还可以处理对象信息以理解周围环境，通过对话理解人类意图，对人类输入生成自然语言响应，并创建操纵计划以协助人类完成各种任务。

### 医学

一个能够从基因序列准确预测物理性质的模型将代表医学的重大突破，并可能对社会产生深远影响。它可以进一步加速药物发现和精准医学，实现更早的疾病预测和预防，提供对复杂疾病的更深入理解，并改善基因治疗。然而，这也引发了围绕基因工程的主要道德关切，并可能加剧社会不平等。新技术已经采用神经网络降低了长读取 DNA 测序的错误率（Baid 和同事们；*DeepConsensus improves the accuracy of sequences with a gap-aware sequence transformer*，2022 年 9 月），根据 ARK 投资管理公司（2023 年）的报告，短期内，像这样的技术已经可以用不到 1000 美元的价格交付第一个高质量的完整长读取基因组。这意味着大规模的基因表达模型也许并不遥远。

### 军事

世界各国军队正在投资研究开发致命自主武器系统（LAWS）。机器人和无人机可以在没有任何人类监督的情况下识别目标并使用致命武力。机器可以比人类更快地处理信息并做出反应，从而从致命决策中消除情感。然而，这引发了重大的道德问题。允许机器确定是否应该夺取生命跨越了一个令人不安的界限。即使使用了复杂的人工智能，战争中的复杂因素，如比例和区分平民和战斗人员，仍需要人类判断。如果部署，完全自主的致命武器将代表放弃对生死决策的控制迈出令人震惊的一步。它们可能违反国际人道主义法，或被专制政权用来恐吓人民。一旦完全独立释放，自主杀手机器人的行动将是不可预测或控制的。

### 虚假信息和网络安全

AI 对抗虚假信息是一把双刃剑。虽然它能够进行大规模检测，但自动化使得传播复杂、个性化的宣传更容易。AI 的使用是否负责任取决于它是帮助还是损害安全。它增加了对利用生成式黑客和社会工程学进行网络攻击的误解的脆弱性。与微观目标和深度伪造等 AI 技术相关的威胁是相当显著的。强大的 AI 可以心理剖析用户，以提供能够促进隐蔽的操纵、避免广泛检查的个性化的虚假信息。大数据和 AI 可以利用心理脆弱性，并渗透到在线论坛中攻击扩散阴谋论。虚假信息已经成为一个多方面的现象，涉及偏见信息、操纵、宣传和旨在影响政治行为等。例如，在 COVID-19 大流行期间，虚假信息和信息瘟疫的传播一直是一个重要挑战。AI 在选举、战争或外部势力等话题上有影响公众舆论的潜力。它也可以生成虚假的音频/视频内容以损害声誉和造成混淆。国家和非国家行为者正在将这些能力用于宣传，以破坏声誉和造成混乱。政党、政府、犯罪团伙甚至司法系统都可以利用 AI 发起诉讼并提取资金。这可能会在各个领域产生深远的影响。大部分互联网用户可能获得所需的信息而无需访问外部网站。大型企业成为信息的守门人，控制公众舆论，实际上能够限制某些行动或观点，这是一个危险信号。建立认真的治理和数字素养非常重要，以建立韧性。虽然不存在单一解决方案，但促进负责任的 AI 开发的集体努力可以帮助民主社会应对新兴威胁。

## 实际实现挑战

在负责任的情况下实现生成式 AI 的潜力涉及解决一些实际的法律、伦理和监管问题：

+   **法律**: 版权法律在 AI 生成的内容方面仍然存在模糊不清的问题。谁拥有输出——模型创造者、培训数据贡献者或终端用户？在培训中复制受版权保护的数据也引发了需要澄清的合理使用争议。

+   **数据保护**: 收集、处理和存储训练高级模型所需的大量数据集会创建数据隐私和安全风险。确保同意、匿名和安全访问的治理模型至关重要。

+   **监管和法规**: 要求对先进的 AI 系统进行监管，以确保不歧视、准确和有责任感。但是需要灵活的政策平衡创新和风险，而不是繁琐的官僚主义。

+   **伦理学**：引导发展朝着有益结果的框架至关重要。通过专注于透明度、可解释性和人类监督的设计实践，将伦理学融入其中有助于建立信任。

总体而言，政策制定者、研究人员和公民社会之间的积极合作对于解决围绕权利、伦理和治理尚未解决的问题至关重要。在设置务实的防护措施的同时，生成模型可以实现其承诺，同时减轻伤害。但公共利益必须保持是引导人工智能进步的指南针。对算法透明度的需求日益增长。这意味着科技公司和开发人员应该揭示其系统的源代码和内部运作方式。然而，这些公司和开发人员主张披露专有信息将损害其竞争优势，因此存在抵制的情况。开源模型将继续蓬勃发展，欧盟和其他国家的地方立法将推动人工智能的透明使用。人工智能偏见的后果包括因人工智能系统偏见决策而给个人或群体带来潜在伤害。将伦理学培训纳入计算机科学课程可以帮助减少人工智能代码中的偏见。通过教导开发人员如何构建符合伦理要求的应用程序，可以将代码中嵌入偏见的概率降到最低。为了走上正确的道路，组织需要优先考虑透明度、问责制和防护措施，以防止其人工智能系统出现偏见。人工智能偏见预防是许多组织的长期重点，然而，如果没有立法推动，引入它可能需要时间。例如，欧盟国家的地方立法，如欧洲委员会关于人工智能监管的协调规则的提议，将推动更加道德化的语言和形象使用。德国关于虚假新闻的当前法律规定了一个 24 小时的时间框架，要求平台删除虚假新闻和仇恨言论，这对于大型和小型平台都是不切实际的。此外，较小平台的有限资源使得它们无法监管所有内容是不现实的。此外，在线平台不应该拥有唯一的权威来确定什么是真实的，因为这可能导致过度审查。需要更加细致的政策来平衡言论自由、问责制和各种技术平台的可行性。仅依赖私营公司来监管在线内容引发了对监督和正当程序不足的担忧。政府、公民社会、学术界和工业界之间更广泛的合作可以制定更有效的框架来对抗虚假信息，同时保护权利。

## 未来之路

即将到来的生成式 AI 模型时代提供了许多有趣的机会和空前的进展，然而也存在许多不确定性。正如本书所讨论的那样，近年来已经取得了许多突破，但仍然存在着一系列持续的挑战，主要涉及这些模型中的精度、推理能力、可控性和固有偏见。虽然未来可能会出现超级智能的 AI 看起来有些言过其实，但持续的趋势预示着几十年内将出现先进的能力。在个人层面上，生成式内容的普及引起了关于错误信息、学术抄袭和网络空间中的冒充等问题的担忧。随着这些模型变得越来越擅长模仿人类表达，人们可能难以分辨哪些是人类生成的，哪些是 AI 生成的，从而导致新形式的欺骗。还有人类过时和岗位流失的忧虑，这可能会进一步划分经济阶层。与过去的物理自动化不同，生成式 AI 威胁着曾被认为是免于自动化的认知工作类别。在管理这个工作转型的道德和公平方面，需要有远见和规划。此外，还存在关于 AI 是否应该创作反映人类状况的艺术、文学或音乐的哲学辩论。对于企业来说，尚未建立起有效的治理框架来适应可接受的使用情况。生成式模型放大了滥用风险，范围从创建错误信息，如深度伪造，到生成不安全的医疗建议。涉及内容许可和知识产权的法律问题也随之产生。虽然这些模型可以增强企业生产力，但质量控制和偏见缓解都带来了额外的成本。尽管大型科技公司目前主导着生成式 AI 的研究和开发，但规模较小的实体最终可能会从这些技术中受益最多。随着计算、数据存储和 AI 人才成本的降低，专门模型的定制化预训练可能会成为中小型公司的可行选择。与依赖于大型科技公司的通用模型不同，针对专业数据集调整过的定制化生成式 AI 可以更好地满足独特需求。初创公司和非营利组织通常擅长快速迭代，为专业领域构建前沿解决方案。成本降低后，通过民主化的访问和使用，这些集中的参与者可以训练优秀的模型，超过通用系统的能力。随着生成式 AI 工具相对容易地基于基础模型构建，已经出现了饱和的隐忧。模型和工具的定制化将会创造价值，但尚不清楚谁将获得最大的盈利。尽管目前市场炒作居高不下，但投资者仍在遵循 2021 年 AI 崛起/崩溃周期所带来的低估值和怀疑态度。长期市场影响和获胜的生成式

> **2021 年 AI 繁荣 / 萧条周期** 指的是投资和增长在 AI 创业公司领域的迅速加速，随后市场降温和稳定，因为预测未能实现而估值下降。
> 
> > 快速总结：
> > 
> > 繁荣阶段（2020-2021）：
> > 
> > AI 创业公司提供了创新能力，如计算机视觉、自然语言处理、机器人技术和机器学习平台，因此受到了极大关注和投资。根据 Pitchbook 的数据，2021 年 AI 创业公司的总投资额达到创纪录的 73 亿美元，全球成立和获得投资的 AI 创业公司数量超过几百家。
> > 
> > 破坏阶段（2022）：   
> > 
> > 在 2022 年，市场经历了一次修正，AI 创业公司的估值从 2021 年的最高点大幅下跌。许多知名的 AI 创业公司，如 Anthropic 和 Cohere，面临了估值的降价。许多投资者变得更加谨慎和选择性，对投资 AI 创业公司更加审慎。更广泛的科技行业市场调整也促成了萧条。   
> > 
> > 主要因素：
> > 
> > 过度炒作、不切实际的增长预测、2021 年的历史性高估值以及更广泛的经济条件都促成了繁荣和萧条周期。该周期遵循了此前在互联网和区块链等行业中看到的经典模式。

未来几十年，最深刻的挑战可能是伦理问题。随着 AI 被赋予了更加重要的决策权，使其与人类价值观保持一致变得至关重要。虽然准确性、推理能力、可控性和减少偏见仍然是技术上的优先任务，但其他重点应包括加强模型的韧性，促进透明度并确保与人类价值观的一致性。为了最大限度地发挥效益，公司需要确保开发中有人类监督、多样性和透明度。政策制定者可能需要实施防范滥用的保障措施，并为工人提供过渡支持。通过负责任的实施，生成式 AI 可以推动更繁荣的社会中的增长、创造和可访问性。及早解决潜在风险并确保公共福利设计的收益公正分配将在利益相关者中培养信任感，如：

+   进展的动态：微调变革的速度对于避免不希望的影响至关重要。此外，过度缓慢的发展可能会扼杀创新，因此确定一个理想步伐需要包容公众讨论是至关重要的。

+   人机共生：与其追求完全自动化，更有优势的系统会将人的创造力与 AI 的生产效率相融合和互补。这种混合模型将确保最佳监督。

+   **促进获取和包容**：对人工智能资源、相关教育和各种机会的平等获取对于消除差距的扩大至关重要。代表性和多样性应该是优先考虑的。

+   **预防措施和风险管理**：通过跨学科的见解不断评估新出现的能力是必要的，以避免未来的危险。然而，过度的担忧不应该妨碍潜在的进步。

+   **捍卫民主规范**：相比于单方面强加的单一规定，协作讨论、共同努力和达成妥协无疑将更有益于界定人工智能未来的发展方向。公共利益必须优先考虑。

虽然未来的能力仍然不确定，但积极的治理和对获取的民主化至关重要，以引导这些技术走向公平、善良的结果。研究人员、决策者和公民社会围绕透明度、问责制和伦理等问题的合作可以帮助将新兴创新与共同的人类价值观相一致。目标应该是赋予人类潜能，而不仅仅是技术进步。
