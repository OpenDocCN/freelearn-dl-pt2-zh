["```py\ngit clone https://github.com/openai/gym.git \ncd gym \npip install -e . \n```", "```py\ngit clone https://github.com/benelot/pybullet-gym.git \ncd pybullet-gym \npip install -e . \n```", "```py\nimport gym  \nimport pybulletgym  \nenv = gym.make('HopperMuJoCoEnv-v0')\nenv.render(\"human\")\nenv.reset() \n```", "```py\nenv.reset() \nfor t in range(1000):\n    action = env.action_space.sample()\n    _, _, done, _ = env.step(action)\n    env.render(\"human\")\n    if done:\n        break \n```", "```py\nimport numpy as np\nmujoco_hopper_np = np.load('deterministic.trpo.Hopper.0.00.npz')\nfor i_episode in range(20):\n    observation = env.reset()\n    episode = np.random.choice(mujoco_hopper_np['acs'].shape[0])\n    for t in range(1000):\n        env.render(\"human\")\n        action = mujoco_hopper_np = \\\n        np.load('deterministic.trpo.Hopper.0.00.npz')['acs'][episode][t]\n        observation, reward, done, info = env.step(action)\n        if done:\n            print(\"Episode finished after {} timesteps\".format(t+1))\n            break\nenv.close() \n```", "```py\nprint(mujoco_hopper_np.files) \n```", "```py\n['ep_rets', 'obs', 'rews', 'acs'] \n```", "```py\nprint(mujoco_hopper_np['obs'].shape) \n```", "```py\nprint(mujoco_hopper_np ['acs'].shape) \n```", "```py\nimport tensorflow_probability as tfp\nimport tensorflow as tf\ntfd = tfp.distributions\nclass ActorCritic(tf.keras.Model):\n    def __init__(self, name='actor_critic', dim_actions=3, num_layers=2, input_shape=(11), num_units=100, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._num_layers = num_layers\n        self._num_units = num_units\n        self._dim_actions = dim_actions\n        self._layers = list()\n        for n, l in enumerate(range(self._num_layers)):\n            self._layers.append(tf.keras.layers.Dense(\n                                          self._num_units,\n                                          activation=tf.nn.relu))\n            if n == 0:\n                self._layers[-1].build(input_shape)\n            else:\n                self._layers[-1].build((num_units))\n            self._layers.append(tf.keras.layers.BatchNormalization())\n        self._value_output = tf.keras.layers.Dense(1,activation=None)\n        self._value_output.build((num_units))\n        self._action_output = tf.keras.layers.Dense(\n                                          self._dim_actions,\n                                          activation=tf.nn.tanh)\n        self._action_output.build((num_units))\n        self._action_dist_std = tf.Variable([1., 1, 1], \n                                            trainable=False)\n        self._action_dist =  None\n    def get_params(self):\n        weights = []\n        for layer in self.layers:\n            weights += layer.trainable_weights\n        return weights+\\\n            self._action_output.trainable_weights + \\\n            self._value_output.trainable_weights + \\\n            [self._action_dist_std]\n    def call(self, inputs):\n        x = self._layers[0](inputs)\n        for layer in self._layers[1:self._num_layers]:\n            x = layer(x)\n        return self._value_output(x)\n    def log_prob(self, x):\n        return self._action_dist.log_prob(x)\n    def sample(self, inputs, output='action'):\n        x = self._layers[0](inputs)\n        for layer in self._layers[1:self._num_layers]:\n            x = layer(x)\n        self._action_dist = tfd.Normal(self._action_output(x),\n                                       [1,1,1])\n        if output == 'action':\n            return self._action_dist.sample()\n        elif output == 'entropy':\n            return tf.reduce_mean(self._action_dist.entropy())\n        else:\n            raise ValueError(\"unknown sample type: {}\".format(output)) \n```", "```py\n def get_params(self):\n        weights = []\n        for layer in self.layers:\n            weights += layer.trainable_weights\n        return weights+\\\n            self._action_output.trainable_weights + \\\n            self._value_output.trainable_weights + \\\n            [self._action_dist_std] \n```", "```py\n def call(self, inputs):\n        x = self._layers[0](inputs)\n        for layer in self._layers[1:self._num_layers]:\n            x = layer(x)\n        return self._value_output(x) \n```", "```py\ndef sample(self, inputs, output='action'):\n        x = self._layers[0](inputs)\n        for layer in self._layers[1:self._num_layers]:\n            x = layer(x)\n        self._action_dist = tfd.Normal(self._action_output(x), [1,1,1])\n\n        if output == 'action':\n            return self._action_dist.sample()\n        elif output == 'entropy':\n            return tf.reduce_mean(self._action_dist.entropy())\n        else:\n            raise ValueError(\"unknown sample type: {}\".format(output)) \n```", "```py\n def log_prob(self, x):\n        return self._action_dist.log_prob(x) \n```", "```py\nclass PPO(tf.keras.Model):\n\n    def __init__(self, name='ppo', dim_actions=3, num_layers=2, num_units=100, eps=0.2, v_coeff=0.5, ent_coeff=0.01, lr=3e-2, **kwargs):\n        super().__init__(name=name, *kwargs)\n        self._dim_actions = dim_actions\n        self._num_layers = num_layers\n        self._num_units = num_units\n        self._eps = eps\n        self._v_coeff = v_coeff\n        self._ent_coeff = ent_coeff\n        self._policy = ActorCritic(num_layers=self._num_layers, \n                                   num_units=self._num_units,\n                                   dim_actions=self._dim_actions)\n        self._new_policy = ActorCritic(num_layers=self._num_layers, \n                                       num_units=self._num_units,\n                                       dim_actions=self._dim_actions)\n        self._policy.compile(run_eagerly=True)\n        self._new_policy.compile(run_eagerly=True)\n        self._optimizer = tf.keras.optimizers.Adam(lr) \n```", "```py\ndef loss(self, actions, observations, advantages, returns):\n        ratio = tf.exp(self._new_policy.log_prob(actions) - \n                       self._policy.log_prob(actions))\n        surr = ratio * advantages\n        actor_loss = tf.reduce_mean(\n                tf.minimum(surr, tf.clip_by_value(ratio, 1 - self._eps,\n                                    1 + self._eps) * advantages))\n        critic_loss = tf.reduce_mean(tf.square(returns - self._new_policy.call(observations)))\n        return -1*actor_loss - self._ent_coeff * \\\n    tf.reduce_mean(self._new_policy.sample(observations, 'entropy'))\\\n                         + self._v_coeff * critic_loss \n```", "```py\ndef compute_gae(next_value, rewards, masks, values, gamma=0.99, tau=0.95):\n    values = values + [next_value]\n    gae = 0\n    returns = []\n    for step in reversed(range(len(rewards))):\n        delta = rewards[step] + gamma * values[step + 1] * \\ \n                    masks[step] - values[step]\n        gae = delta + gamma * tau * masks[step] * gae\n        returns.insert(0, gae + values[step])\n    return returns \n```", "```py\ndef train_policy(self, actions, observations, advantages, returns):\n    params = self._new_policy.get_params()\n    with tf.GradientTape(watch_accessed_variables=False) as g:\n        g.watch(params)\n        def loss(actions, observations, advantages, returns):\n            ...\n\n        cost = loss(actions, observations, advantages, returns)\n    grads = g.gradient(cost, params)\n    grads = [grad if grad is not None else tf.zeros_like(var)\n        for var, grad in zip(params, grads)]\n    self._optimizer.apply_gradients(zip(grads, params),\n        experimental_aggregate_gradients=False) \n```", "```py\ndef update_policy(self):\n    self._policy = copy.deepcopy(self._new_policy) \n```", "```py\ndef get_action(self, x):\n    return self._new_policy.sample(x, output='action')\ndef get_value(self, x):\n    return self._new_policy.call(x) \n```", "```py\nclass Discriminator(tf.keras.Model):\n\n    def __init__(self, name='discriminator', dim_actions=3, num_layers=2, num_units=100, lr=3e-2, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self._dim_actions = dim_actions\n        self._num_layers = num_layers\n        self._num_units = num_units\n        self._layers = list()\n        for l in range(self._num_layers):\n            self._layers.append(tf.keras.layers.Dense(\n                                          self._num_units,\n                                          activation=tf.nn.relu))\n            self._layers.append(tf.keras.layers.BatchNormalization())\n        self._layers.append(tf.keras.layers.Dense(1,activation=None))\n        self._optimizer = tf.keras.optimizers.Adam(lr) \n        return self._new_policy.call(x) \n```", "```py\ndef get_reward(self, x):\n    return -1 *tf.squeeze(tf.math.log(tf.sigmoid(self.call(x)))) \n```", "```py\n def call(self, x):\n        for layer in self._layers:\n            x = layer(x)\n        return x \n```", "```py\ndef loss(self, x):\n    expert_out, policy_out = tf.sigmoid(tf.split(self.call(x), \n                               num_or_size_splits=2, axis=0))\n    return (tf.nn.sigmoid_cross_entropy_with_logits(tf.ones_like(policy_out), policy_out) + tf.nn.sigmoid_cross_entropy_with_logits(tf.zeros_like(expert_out), expert_out)) \n```", "```py\n def get_params(self):\n        weights = []\n        for layer in self.layers:\n            weights += layer.trainable_weights\n        return weights \n```", "```py\ndef train_discriminator(self, x):\n    params = self.get_params()\n    with tf.GradientTape(watch_accessed_variables=False) as g:\n        g.watch(params)\n        cost = self.loss(x)\n    grads = g.gradient(cost, params)\n    grads = [grad if grad is not None else tf.zeros_like(var)\n          for var, grad in zip(params, grads)]\n    self._optimizer.apply_gradients(zip(grads, params),\n        experimental_aggregate_gradients=False) \n```", "```py\ndef ppo_iteration(mini_batch_size, observations, actions, returns, advantages):\n    batch_size = observations.shape[0]\n    for _ in range(batch_size // mini_batch_size):\n        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n        yield (observations[rand_ids, :], actions[rand_ids, :],\n               returns[rand_ids, :], advantages[rand_ids, :]) \n```", "```py\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\ndef plot(frame_idx, rewards, policy_ob_ac_rew, expert_ob_ac_rew):\n    clear_output(True)\n    plt.figure(figsize=(20,5))\n    plt.subplot(131)\n    plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n    plt.ylabel('Agent Reward')\n    plt.xlabel('Step in Training')\n    plt.plot(rewards)\n    plt.subplot(132)\n    plt.title('frame %s.' % (frame_idx))\n    plt.plot(policy_ob_ac_rew)\n    plt.plot(expert_ob_ac_rew)\n    plt.legend(['Agent','Expert'])\n    plt.xlabel('Steps in Test Simulation')\n    plt.ylabel('Discriminator Reward')\n    plt.show() \n```", "```py\ndef test_env(model, vis=False):\n    ob = env.reset()\n    ob = tf.reshape(tf.convert_to_tensor(ob), (1,11))\n    done = False\n    total_reward = 0\n    while not done:\n        if vis:\n            env.render()\n        ac = model.get_action(ob)[0]\n        ac = tf.reshape(tf.convert_to_tensor(ac), (3, 1))\n        next_ob, reward, done, _ = env.step(ac)\n        ob = next_ob\n        ob = tf.reshape(tf.convert_to_tensor(ob), (1,11))\n        total_reward += reward\n    return total_reward \n```", "```py\nppo_hidden_size           = 32\ndiscriminator_hidden_size = 32\nlr                        = 3e-4\nnum_steps                 = 1000\nmini_batch_size           = 50\nppo_epochs                = 5\nmax_frames = 100000000\nframe_idx  = 0\ntest_rewards = [] \n```", "```py\nob = env.reset()\nppo = PPO(lr=lr, num_units=ppo_hidden_size)\ndiscriminator = Discriminator(lr=lr, num_units=discriminator_hidden_size)\ni_update = 0 \n```", "```py\nwhile frame_idx < max_frames:\n    i_update += 1\n\n    values = []\n    obs = []\n    acs = []\n    rewards = []\n    masks = []\n    entropy = 0\n    for _ in range(num_steps):\n        ob = tf.reshape(tf.convert_to_tensor(ob), (1,11))\n        ac = ppo.get_action(ob)\n        ac = tf.reshape(tf.convert_to_tensor(ac), (3, 1))\n        next_ob, _, done, _ = env.step(ac)\n        reward = discriminator.get_reward(np.concatenate([ob, \n                     tf.transpose(ac)], axis=1))\n        value = ppo.get_value(ob)\n        values.append(value)\n        rewards.append(reward)\n        masks.append((1-done))\n        obs.append(ob)\n        acs.append(np.transpose(ac))\n        ob = next_ob\n        frame_idx += 1\n        if frame_idx % 1000 == 0 and i_update > 1:\n            test_reward = np.mean([test_env(ppo) for _ in range(10)])\n            test_rewards.append(test_reward)\n            plot(frame_idx, test_rewards, \n                 discriminator.get_reward(policy_ob_ac), \n                 discriminator.get_reward(expert_ob_ac))\n\n    next_ob = tf.reshape(tf.convert_to_tensor(next_ob), (1,11))\n\n    next_value = ppo.get_value(next_ob)\n    returns = compute_gae(next_value, rewards, masks, values)\n    returns = np.concatenate(returns)\n    values = np.concatenate(values)\n    obs = np.concatenate(obs)\n    acs = np.concatenate(acs)\n    advantages = returns - values \n```", "```py\n # Policy Update\n    if i_update % 3 == 0:\n        ppo.update_policy()\n        for _ in range(ppo_epochs):\n            for ob_batch, ac_batch, return_batch, adv_batch in ppo_iteration(mini_batch_size, obs, acs, returns, advantages):\n                ppo.train_policy(ac_batch, ob_batch, adv_batch,                     return_batch) \n```", "```py\n # Discriminator Update\n    expert_samples = np.random.randint(0, mujoco_hopper_np['acs'].\\\n                                       shape[0], 1)\n    expert_ob_ac = np.concatenate([\nmujoco_hopper_np['obs'][expert_samples,:num_steps,:].reshape(num_steps,11), \nmujoco_hopper_np['acs'][expert_samples,:num_steps,:].reshape(num_steps,3)],1)\n    policy_ob_ac = np.concatenate([obs, acs], 1)\n    discriminator.train_discriminator(np.concatenate([expert_ob_ac, policy_ob_ac], axis=0)) \n```"]