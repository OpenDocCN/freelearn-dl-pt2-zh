- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Semantic Role Labeling with BERT-Based Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 BERT 的变压器的语义角色标记
- en: Transformers have made more progress in the past few years than NLP in the past
    generation. Standard NLU approaches first learn syntactical and lexical features
    to explain the structure of a sentence. The former NLP models would be trained
    to understand a language’s basic syntax before running **Semantic Role Labeling**
    (**SRL**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器在过去几年中取得的进展比上一代 NLP 更大。标准的 NLU 方法首先学习句法和词汇特征来解释句子的结构。以前的 NLP 模型在运行**语义角色标注**（**SRL**）之前会被训练来理解语言的基本句法。
- en: '*Shi* and *Lin* (2019) started their paper by asking if preliminary syntactic
    and lexical training can be skipped. Can a BERT-based model perform SRL without
    going through those classical training phases? The answer is yes!'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shi* 和 *Lin*（2019）在他们的论文中提出了一个问题，是否可以跳过初步的句法和词汇训练。基于 BERT 的模型可以在不经历这些传统训练阶段的情况下执行
    SRL 吗？答案是肯定的！'
- en: '*Shi* and *Lin* (2019) suggested that SRL can be considered sequence labeling
    and provide a standardized input format. Their BERT-based model produced surprisingly
    good results.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shi* 和 *Lin*（2019）建议将 SRL 视为序列标记，并提供标准化的输入格式。他们基于 BERT 的模型产生了令人惊讶的好结果。'
- en: This chapter will use a pretrained BERT-based model provided by the Allen Institute
    for AI based on the *Shi* and *Lin* (2019) paper. *Shi* and *Lin* took SRL to
    the next level by dropping syntactic and lexical training. We will see how this
    was achieved.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用基于 *Shi* 和 *Lin*（2019）论文的艾伦人工智能研究所提供的预训练基于 BERT 的模型。*Shi* 和 *Lin* 通过放弃句法和词汇训练将
    SRL 提高到了一个新的水平。我们将看到他们是如何做到的。
- en: We will begin by defining SRL and the standardization of the sequence labeling
    input formats. We will then get started with the resources provided by the Allen
    Institute for AI. Next, we will run SRL tasks in a Google Colab notebook and use
    online resources to understand the results.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先定义 SRL 和序列标记输入格式的标准化。然后，我们将开始使用艾伦人工智能研究所提供的资源。接下来，我们将在 Google Colab 笔记本中运行
    SRL 任务，并使用在线资源来理解结果。
- en: Finally, we will challenge the BERT-based model by running SRL samples. The
    first samples will show how SRL works. Then, we will run some more difficult samples.
    We will progressively push the BERT-based model to the limits of SRL. Finding
    the limits of a model is the best way to ensure that real-life implementations
    of transformer models remain realistic and pragmatic.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过运行 SRL 样本来挑战基于 BERT 的模型。第一批样本将展示 SRL 的工作原理。然后，我们将运行一些更困难的样本。我们将逐步推动基于
    BERT 的模型达到 SRL 的极限。找到模型的极限是确保变压器模型的现实和实用之路。
- en: 'This chapter covers the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Defining semantic role labeling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义语义角色标记
- en: Defining the standardization of the input format for SRL
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义 SRL 输入格式的标准化
- en: The main aspects of the BERT-based model’s architecture
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 BERT 的模型架构的主要方面
- en: How an encoder-only stack can manage a masked SRL input format
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器堆栈如何管理屏蔽的 SRL 输入格式
- en: BERT-based model SRL attention process
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 BERT 的模型 SRL 注意力过程
- en: Getting started with the resources provided by the Allen Institute for AI
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用由艾伦人工智能研究所提供的资源
- en: Building a notebook to run a pretrained BERT-based model
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建笔记本来运行预训练的基于 BERT 的模型
- en: Testing sentence labeling on basic examples
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基本示例上测试句子标记
- en: Testing SRL on difficult examples and explaining the results
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在困难的例子上测试 SRL 并解释结果
- en: Taking the BERT-based model to the limit of SRL and explaining how this was
    done
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将基于 BERT 的模型推向 SRL 的极限并解释如何做到这一点
- en: Our first step will be to explore the SRL approach defined by *Shi* and *Lin*
    (2019).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是探索 *Shi* 和 *Lin*（2019）定义的 SRL 方法。
- en: Getting started with SRL
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始 SRL
- en: SRL is as difficult for humans as for machines. However, once again, transformers
    have taken a step closer to our human baselines.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类和机器来说，SRL 都是一样困难的。然而，一次又一次，变压器已经朝着我们人类基线迈出了一步。
- en: In this section, we will first define SRL and visualize an example. We will
    then run a pretrained BERT-based model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先定义 SRL 并举例说明。然后，我们将运行一个预训练的基于 BERT 的模型。
- en: Let’s begin by defining the problematic task of SRL.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义 SRL 这个棘手的任务。
- en: Defining semantic role labeling
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义语义角色标记
- en: '*Shi* and *Lin* (2019) advanced and proved the idea that we can find who did
    what, and where, without depending on lexical or syntactic features. This chapter
    is based on *Peng Shi* and *Jimmy Lin*’s research at the *University of Waterloo*,
    California. They showed how transformers learn language structures better with
    attention layers.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shi* 和 *Lin* (2019)推进并证明了一个想法，即我们可以找出谁做了什么以及在哪里，而不依赖于词汇或句法特征。本章基于 *Peng Shi*
    和 *Jimmy Lin* 在加利福尼亚滑铁卢大学展开的研究。他们展示了变压器如何通过注意力层更好地学习语言结构。'
- en: SRL labels the *semantic role* as the role a word or group of words plays in
    a sentence and the relationship established with the predicate.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SRL将*语义角色*定义为单词或一组单词在句子中所起的作用以及与谓语所建立的关系。
- en: A *semantic role* is the role a noun or noun phrase plays in relation to the
    main verb in a sentence. For example, in the sentence `Marvin walked in the park`,
    `Marvin` is the *agent* of the event occurring in the sentence. The *agent* is
    the *doer* of the event. The main verb, or *governing verb*, is `walked`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*语义角色*是一个名词或名词短语在句子中与主要动词的关系中所扮演的角色。例如，在句子`Marvin walked in the park`中，`Marvin`是句子中发生事件的*施事者*。*施事者*是事件的*执行者*。主动词，或*控制动词*，是`walked`。'
- en: The *predicate* describes something about the subject or agent. The predicate
    could be anything that provides information on the features or actions of a subject.
    In our approach, we will refer to the predicate as the main *verb*. For example,
    in the sentence `Marvin walked in the park`, the predicate is `walked` in its
    restricted form.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*谓语*描述了主语或施事者的一些信息。谓语可以是任何提供有关主题特征或行为的信息的东西。在我们的方法中，我们将谓语称为主要*动词*。例如，在句子`Marvin
    walked in the park`中，谓语是`walked`。'
- en: The words `in the park` *modify* the meaning of `walked` and are the *modifier*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 词组`在公园里` *修饰*了`走`的含义，并且是*修饰语*。
- en: The noun or noun phrases that revolve around the predicate are *arguments* or
    *argument terms*. `Marvin`, for example, is an *argument* of the *predicate* `walked`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 环绕谓词的名词或名词短语称为*论元*或*论元项*。例如，`Marvin`是`walked`的一个*论元*。
- en: We can see that SRL does not require a syntax tree or a lexical analysis.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，SRL不需要句法树或词汇分析。
- en: Let’s visualize the SRL of our example.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化我们的示例的SRL。
- en: Visualizing SRL
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化SRL
- en: This chapter will be using the Allen Institute’s visual and code resources (see
    the *References* section for more information). The Allen Institute for AI has
    excellent interactive online tools, such as the one we’ve used to represent SRL
    visually throughout this chapter. You can access these tools at [https://demo.allennlp.org/](https://demo.allennlp.org/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用Allen Institute的视觉和代码资源（有关更多信息，请参见*参考*部分）。 Allen Institute for AI拥有出色的交互式在线工具，例如我们在本章中用来可视化SRL的工具。您可以在[https://demo.allennlp.org/](https://demo.allennlp.org/)上访问这些工具。
- en: The Allen Institute for AI advocates *AI for the common good*. We will make
    good use of this approach. All of the figures in this chapter were created with
    the AllenNLP tools.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Allen Institute for AI倡导*AI造福人类*。我们将充分利用这种方法。本章中的所有图表都是用AllenNLP工具创建的。
- en: 'The Allen Institute provides transformer models that continuously evolve. Therefore,
    the examples in this chapter might produce different results when you run them.
    The best way to get the most out of this chapter is to:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Allen Institute提供了不断发展的变压器模型。因此，当您运行这些示例时，本章中的示例可能会产生不同的结果。最充分利用本章的最佳方法是：
- en: Read and understand the concepts explained beyond merely running a program
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读并理解解释的概念，而不只是运行程序
- en: Take the time to understand the examples provided
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花时间理解提供的示例
- en: 'Then run your own experiments with sentences of your choice with the tool used
    in this chapter: [https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用本章中使用的工具运行您自己选择的句子进行您自己的实验：[https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling)。
- en: 'We will now visualize our SRL example. *Figure 10.1* is an SRL representation
    of `Marvin walked in the park`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们可视化我们的SRL示例。*图10.1*是`Marvin walked in the park`的SRL表示：
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17948_10_01.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![包含图形用户界面的图片 描述自动生成](img/B17948_10_01.png)'
- en: 'Figure 10.1: The SRL representation of a sentence'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图10.1: 句子的SRL表示'
- en: 'We can observe the following labels in *Figure 10.1*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图10.1*中观察到以下标签：
- en: '**Verb**: The predicate of the sentence'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动词**：句子的谓语'
- en: '**Argument**: An argument of the sentence named **ARG0**'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**论据**：一个被命名为**ARG0**的句子论点'
- en: '**Modifier**: A modifier of the sentence. In this case, a location. It could
    have been an adverb, an adjective, or anything that modifies the predicate’s meaning'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修饰语**：句子的修饰成分。在这种情况下是一个地点。它也可以是副词，形容词，或任何修改谓词意义的成分'
- en: 'The text output is interesting as well, which contains shorter versions of
    the labels of the visual representation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 文本输出同样有趣，其中包含了可视表示标签的缩写形式：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We have defined SRL and gone through an example. It is time to look at the BERT-based
    model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了SRL并进行了一个例子。现在是时候看看基于BERT的模型了。
- en: Running a pretrained BERT-based model
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行一个预训练的基于BERT的模型
- en: This section will begin by describing the architecture of the BERT-based model
    used in this chapter.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将首先描述本章中使用的基于BERT的模型的架构。
- en: Then we will define the method used to experiment with SRL samples with a BERT
    model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将定义使用BERT模型实验SRL样本的方法。
- en: Let’s begin by looking at the architecture of the BERT-based model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始看一下基于BERT的模型的架构。
- en: The architecture of the BERT-based model
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于BERT的模型架构
- en: AllenNLP’s BERT-based model is a 12-layer encoder-only BERT model. The AllenNLP
    team implemented the BERT model described in *Shi* and *Lin* (2019) with an additional
    linear classification layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP的基于BERT的模型是一个12层只编码器的BERT模型。AllenNLP团队实现了*Shi*和*Lin*（2019）描述的BERT模型，并添加了一个额外的线性分类层。
- en: For more on the description of a BERT model, take a few minutes, if necessary,
    to go back to *Chapter 3*, *Fine-Tuning BERT Models*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于BERT模型的描述，如有必要，请花几分钟回顾*第3章*，*BERT模型微调*。
- en: The BERT-based model takes full advantage of bidirectional attention with a
    simple approach and architecture. The core potential of transformers resides in
    the attention layers. We have seen transformer models with both encoder and decoder
    stacks. We have seen other transformers with encoder layers only or decoder layers
    only. The main advantage of transformers remains in the near-human approach of
    attention layers.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BERT的模型充分利用了双向注意力以简单的方法和架构。变压器模型的核心潜力存在于注意力层中。我们已经看到了既有编码器和解码器堆栈的变压器模型。我们已经看到了其他只有编码器层或者解码器层的变压器。变压器的主要优势仍然在于近乎人类的注意力层方法。
- en: 'The input format of the predicate identification format defined by *Shi* and
    *Lin* (2019) shows how far transformers have come to understand a language in
    a standardized fashion:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shi*和*Lin*（2019）定义的谓词标识格式的输入格式显示了变压器在标准化语言理解方面取得的进展：'
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The training process has been standardized:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流程已被标准化：
- en: '`[CLS]` indicates that this is a classification exercise'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]`表示这是一个分类练习'
- en: '`[SEP]` is the first separator, indicating the end of the sentence'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`是第一个分隔符，表示句子的结束'
- en: '`[SEP]` is followed by the predicate identification designed by the authors'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`后面是作者设计的谓词标识'
- en: '`[SEP]` is the second separator, which indicates the end of the predicate identifier'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`是第二个分隔符，表示谓词标识符的结束'
- en: This format alone is enough to train a BERT model to identify and label the
    semantic roles in a sentence.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 光是这种格式就足够训练一个BERT模型来识别并标记句子中的语义角色。
- en: Let’s set up the environment to run SRL samples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置环境来运行SRL样本。
- en: Setting up the BERT SRL environment
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置BERT SRL环境
- en: We will be using a Google Colab notebook, the AllenNLP visual text representations
    of SRL available at [https://demo.allennlp.org/](https://demo.allennlp.org/) under
    the *Semantic Role Labeling* section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Google Colab记事本，在[https://demo.allennlp.org/](https://demo.allennlp.org/)上提供的AllenNLP
    SRL的文本可视化下进行
- en: 'We will apply the following method:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用以下方法：
- en: We will open `SRL.ipynb`, install `AllenNLP`, and run each sample
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将打开`SRL.ipynb`，安装`AllenNLP`，并运行每个样本
- en: We will display the raw output of the SRL run
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将显示SRL运行的原始输出
- en: We will visualize the output using AllenNLP’s online visualization tools
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用AllenNLP的在线可视化工具来可视化输出
- en: We will display the output using AllenNLP’s online text visualization tools
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用AllenNLP的在线文本可视化工具来显示输出
- en: This chapter is self-contained. You can read through it or run the samples as
    described.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是自包含的。您可以阅读它，或按照描述运行示例。
- en: The SRL model output may differ when AllenNLP changes the transformer model
    used. This is because AllenNLP models and transformers, in general, are continuously
    trained and updated. Also, the datasets used for training might change. Finally,
    these are not rule-based algorithms that produce the same result each time. The
    outputs might change from one run to another, as described and shown in the screenshots.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当AllenNLP更改了使用的transformer模型时，SRL模型的输出可能会有所不同。这是因为AllenNLP的模型和transformers通常都是持续训练和更新的。此外，用于训练的数据集可能会发生变化。最后，这些不是基于规则的算法，每次产生的结果可能会有所不同，正如屏幕截图中所描述和显示的那样。
- en: Let’s now run some SRL experiments.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行一些SRL实验。
- en: SRL experiments with the BERT-based model
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于BERT的模型进行SRL实验
- en: We will run our SRL experiments using the method described in the *Setting up
    the BERT SRL environment* section of this chapter. We will begin with basic samples
    with various sentence structures. We will then challenge the BERT-based model
    with some more difficult samples to explore the system’s capacity and limits.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用本章“设置BERT SRL环境”的方法来运行SRL实验。我们将从具有各种句子结构的基本样本开始。然后，我们将挑战基于BERT的模型，使用一些更难的样本来探索系统的能力和限制。
- en: 'Open `SRL.ipynb` and run the installation cell:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`SRL.ipynb`并运行安装单元格：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we import the tagging module and a trained BERT predictor:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们导入标记模块和训练好的BERT预测器：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We also add two functions to display the JSON object SRL BERT returns. The
    first one displays the verb of the predicate and the description:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了两个函数来显示SRL BERT返回的JSON对象。第一个显示谓语动词和描述：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The second one displays the full response, including the tags:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数显示完整的响应，包括标签：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At the time of this publication, the BERT model was specifically trained for
    semantic role labeling use. The name of the model is SRL BERT. SRL BERT was trained
    using the OntoNotes 5.0 dataset: [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本出版物发布之时，BERT模型专门用于语义角色标注。该模型的名称是SRL BERT。SRL BERT是使用OntoNotes 5.0数据集进行训练的：[https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)。
- en: This dataset contains sentences and annotations. The dataset was designed to
    identify predicates (a part of a sentence containing a verb) in a sentence and
    identify the words that provide more information on the verbs. Each verb comes
    with its “arguments” that tell us more about it. A “frame” contains the arguments
    of a verb.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含句子和注释。该数据集设计用于识别句子中的谓词（包含动词的部分），并确定提供有关动词的更多信息的单词。每个动词都带有其关于它的“参数”，告诉我们更多信息。一个“框架”包含一个动词的参数。
- en: SRL BERT is thus a specialized model trained to perform a specific task, and
    as such, it is not a foundation model like OpenAI GPT-3 as we saw in *Chapter
    7*, *The Rise of Suprahuman Transformers with GPT-3 Engines*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SRL BERT是一个专门训练来执行特定任务的专用模型，并且与我们在*第7章*看到的OpenAI GPT-3一样，并不是像OpenAI GPT-3这样的基础模型。
- en: SRL BERT will focus on semantic role labeling with acceptable accuracy as long
    as the sentence contains a predicate.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: SRL BERT将专注于语义角色标注，只要句子包含谓词，就可以以可以接受的准确度执行。
- en: We are now ready to warm up with some basic samples.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用一些基本样本进行热身了。
- en: Basic samples
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本样本
- en: Basic samples seem intuitively simple but can be tricky to analyze. Compound
    sentences, adjectives, adverbs, and modals are difficult to identify, even for
    non-expert humans.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基本样本在直观上看起来很简单，但分析起来可能有点棘手。复合句，形容词，副词和情态动词对于非专业人士来说甚至也很难识别。
- en: Let’s begin with an easy sample for the transformer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从transformer的一个简单样本开始。
- en: Sample 1
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本1
- en: 'The first sample is long but relatively easy for the transformer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个样本很长，但对transformer来说相对容易：
- en: '`Did Bob really think he could prepare a meal for 50 people in only a few hours?`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: “Bob真的以为自己能在几小时内为50个人准备一顿饭吗？”
- en: 'Run the *Sample 1* cell in `SRL.ipynb`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`SRL.ipynb`中的*样本1*单元格：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'BERT SRL identified the four predicates; the verb for each one labeled the
    result as shown in this excerpt using the `head(prediction)` function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: BERT SRL识别了四个谓词；每个谓词的动词都标记了结果，如在使用`head(prediction)`函数显示的这段摘录中：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can view the full response by running the `full(prediction)` cell.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行`full(prediction)`单元格来查看完整的响应。
- en: 'If we use the description of the arguments of the dataset based on the structure
    of the PropBank (Proposition Bank), the verb `think`, for example, in this excerpt
    can be interpreted as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据PropBank（命题银行）的结构使用数据集参数的描述，例如在这段摘录中，动词`think`可以被解释为：
- en: '`V` identifies the verb `think`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V`标识动词`think`'
- en: '`ARG0` identifies the agent; thus, `Bob` is the agent or “pro-agent”'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARG0`标识代理人; 因此，`Bob`是代理人或“主体代理人”'
- en: '`ARGM-ADV` considers `really` as an adverb (`ADV`), with `ARGM` meaning that
    the adverb provides an adjunct (not necessary) thus not numbered'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARGM-ADV`将`really`视为副词(`ADV`)，`ARGM`表示该副词提供一个修饰语（不是必要的），因此没有编号'
- en: 'If we run the sample in the AllenNLP online interface, we obtain a visual representation
    of the SRL task of one frame per verb. The first verb is `Did`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在AllenNLP在线界面上运行样本，则会获得每个动词一个框架的SRL任务的可视化表示。第一个动词是`Did`：
- en: '![Graphical user interface  Description automatically generated](img/B17948_10_02.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，自动生成描述](img/B17948_10_02.png)'
- en: 'Figure 10.2: Identifying the verb “Did”'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：识别动词“Did”
- en: 'The second verb identified is `think`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个确认的动词是`think`：
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_10_03.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 自动生成描述](img/B17948_10_03.png)'
- en: 'Figure 10.3: Identifying the verb “think”'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：识别动词“think”
- en: 'If we take a close look at this representation, we can detect some interesting
    properties of the SRL BERT that:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细观察这个表示，我们可以检测到SRL BERT的一些有趣属性：
- en: Detected the verb `think`
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测到动词`think`
- en: Avoided the `prepare` trap that could have been interpreted as the main verb.
    Instead, `prepare` remained part of the argument of `think`
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免了可能被解释为主要动词的`prepare`陷阱。 相反，`prepare`仍然是`think`的论点的一部分
- en: Detected an adverb and labeled it
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测到一个副词并进行标记
- en: 'The third verb is `could`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个动词是`could`：
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_10_04.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 自动生成描述](img/B17948_10_04.png)'
- en: 'Figure 10.4: Identifying the verb “could” and the argument'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：识别动词“could”和论点
- en: 'The transformer then moved to the verb `prepare`, labeled it, and analyzed
    its context:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器然后转向动词`prepare`，标记它，并分析其上下文：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_10_05.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，自动生成描述，置信度中等](img/B17948_10_05.png)'
- en: 'Figure 10.5: Identifying the verb “prepare”, the arguments, and the modifiers'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：识别动词“prepare”，论点和修饰语
- en: 'Again, the simple BERT-based transformer model detected a lot of information
    on the grammatical structure of the sentence and found:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，基于简单BERT的转换器模型检测到了大量关于句子语法结构的信息，并发现：
- en: The verb `prepare` and isolated it
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定动词`prepare`并将其孤立出来
- en: The noun `he` and labeled it as an argument and did the same for `a meal for
    50 people`, which is a “proto-patient,” which involves a modification by other
    participants
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定名词`he`并将其标记为论据，对于`a meal for 50 people`也做同样处理，这是一个“原始患者”，涉及由其他参与者修改
- en: That `in only a few hours` is a temporal modifier (**ARGM-TMP**)
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in only a few hours`是一个时间修饰语（**ARGM-TMP**）'
- en: That `could` was a modal modifier that indicates the modality of a verb, such
    as the likelihood of an event
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`could`是一个表示动词情态性的情态修饰语，例如事件发生的可能性'
- en: We will now analyze another relatively long sentence.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将分析另一个相对较长的句子。
- en: Sample 2
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本 2
- en: 'The following sentence seems easy but contains several verbs:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的句子看似简单，但包含多个动词：
- en: '`Mrs. and Mr. Tomaso went to Europe for vacation and visited Paris and first
    went to visit the Eiffel Tower.`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`Mrs.和Mr. Tomaso去欧洲度假并访问巴黎，首先去参观埃菲尔铁塔。`'
- en: 'Will this confusing sentence make the transformer hesitate? Let’s see by running
    the *Sample 2* cell of the `SRL.ipynb` notebook:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个令人困惑的句子会使转换器犹豫吗？让我们通过运行`SRL.ipynb`笔记本的*Sample 2*单元格来看一下：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The excerpt of the output proves that the transformer correctly identified
    the verbs in the sentence:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出摘录证明转换器正确识别了句子中的动词：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Running the sample on AllenNLP online identified four predicates, thus generating
    four frames.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在AllenNLP在线上运行样本，可识别四个谓词，从而生成四个框架。
- en: 'The first frame is for `went`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个框架是`went`：
- en: '![](img/B17948_10_06.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_06.png)'
- en: 'Figure 10.6: Identifying the verb “went,” the arguments, and the modifier'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：识别动词“went”，论点和修饰语
- en: 'We can interpret the arguments of the verb `went`. `Mrs. and Mr. Tomaso` are
    the agents. The transformer found that the main modifier of the verb was the purpose
    of the trip: `to Europe`. The result would not be surprising if we did not know
    that *Shi* and *Lin* (2019) had only built a simple BERT model to obtain this
    high-quality grammatical analysis.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以解释动词`went`的论点。`Mrs.和Mr. Tomaso`是代理。 转换器发现动词的主要修饰语是旅行目的：`to Europe`。 如果我们不知道*Shi*和*Lin*（2019）只是建立了一个简单的BERT模型来获取这种高质量的语法分析，则结果将不令人惊讶。
- en: 'We can also notice that `went` was correctly associated with `Europe`. The
    transformer correctly identified the verb `visited` as being related to `Paris`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以注意到`went`与`Europe`正确关联。变压器正确地识别了动词`visited`与`Paris`相关联：
- en: '![](img/B17948_10_07.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_07.png)'
- en: 'Figure 10.7: Identifying the verb “visited” and the arguments'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：识别动词“visited”和论点
- en: The transformer could have associated the verb `visited` directly with the `Eiffel
    Tower`. But it didn’t. It stood its ground and made the right decision.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器本可以将动词`visited`直接与`Eiffel Tower`关联起来。但是它没有。它坚持自己的立场并做出了正确的决定。
- en: 'The next task we asked the transformer to do was identify the context of the
    second use of the verb `went`. Again, it did not fall into the trap of merging
    all of the arguments related to the verb `went`, used twice in the sentence. Again,
    it correctly split the sequence and produced an excellent result:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求变压器做的下一个任务是确定动词`went`的第二次使用的上下文。同样，它没有陷入将与动词`went`相关的所有论点合并的陷阱。再次，它正确地分割了序列并产生了一个出色的结果：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_08.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文字描述的图像 自动产生的描述](img/B17948_10_08.png)'
- en: 'Figure 10.8: Identifying the verb “went,” the argument, and the modifiers'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：识别动词“went”，论点和修饰语
- en: The verb `went` was used twice, but the transformer did not fall into the trap.
    It even found that `first` was a temporal modifier of the verb `went`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 动词`went`被使用了两次，但变压器没有陷入陷阱。它甚至发现`first`是动词`went`的时间修饰语。
- en: 'Finally, the verb `visit` was used a second time, and SRL BERT correctly interpreted
    its use:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，动词`visit`第二次被使用，而 SRL BERT 正确解释了它的使用：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_09.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含文字描述的图像 自动产生的描述](img/B17948_10_09.png)'
- en: 'Figure 10.9: Identifying the verb “visit” and the arguments'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：识别动词“visit”和论点
- en: Let’s run a sentence that is a bit more confusing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个更加令人困惑的句子。
- en: Sample 3
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 3
- en: '*Sample 3* will make things more difficult for our transformer model. The following
    sample contains variations of the verb `drink` four times:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 3* 将使我们的变压器模型变得更加困难。以下示例包含动词`drink`的变体四次：'
- en: '`John wanted to drink tea, Mary likes to drink coffee but Karim drank some
    cool water and Faiza would like to drink tomato juice.`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`John 想喝茶，Mary 喜欢喝咖啡，但 Karim 喝了一些凉爽的水，Faiza 想喝番茄汁。`'
- en: 'Let’s run *Sample 3* in the `SRL.ipynb` notebook:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 `SRL.ipynb` 笔记本中运行 *示例 3*：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The transformer found its way around, as shown in the following excerpt of
    the output that contains the verbs:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器找到了它的方法，如下面输出的摘录所示，其中包含动词：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We obtain several visual representations when we run the sentence on the AllenNLP
    online interface. We will examine two of them.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 AllenNLP 在线界面上运行句子时，我们获得了几个视觉表示。我们将检查其中的两个。
- en: 'The first one is perfect. It identifies the verb `wanted` and makes the correct
    associations:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个完美。它识别了动词`wanted`并做出了正确的关联：
- en: '![](img/B17948_10_10.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_10.png)'
- en: 'Figure 10.10: Identifying the verb “wanted” and the arguments'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：识别动词“wanted”和论点
- en: When it identified the verb `drank`, it correctly excluded `Faiza` and only
    produced `some cool water` as the argument.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当它识别到动词`drank`时，它正确地排除了`Faiza`，并且只产生了`some cool water`作为论点。
- en: '![](img/B17948_10_11.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_11.png)'
- en: 'Figure 10.11: Identifying the verb “drank” and the arguments'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：识别动词“drank”和论点
- en: We’ve found that the BERT-based transformer produces relatively good results
    up to now on basic samples. So let’s try some more difficult ones.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们发现基于 BERT 的变压器在基本样本上产生了相对较好的结果。所以让我们尝试一些更困难的样本。
- en: Difficult samples
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 困难样本
- en: This section will run samples that contain problems that the BERT-based transformer
    will first solve. Finally, we will end with an intractable sample.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将运行包含 BERT-based 变压器首先解决的问题的示例。最后，我们将以一个棘手的样本结束。
- en: Let’s start with a complex sample that the BERT-based transformer can analyze.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个 BERT-based 变压器可以分析的复杂样本开始。
- en: Sample 4
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 4
- en: '*Sample 4* takes us into more tricky SRL territory. The sample separates `Alice`
    from the verb `liked`, creating a long-term dependency that has to jump over `whose
    husband went jogging every Sunday.`'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 4* 将我们带入更复杂的 SRL 领域。该样本将`Alice`与动词`liked`分开，创建了一个长期依赖，必须跳过`whose husband
    went jogging every Sunday.`'
- en: 'The sentence is:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 句子是：
- en: '`Alice, whose husband went jogging every Sunday, liked to go to a dancing class
    in the meantime.`'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`Alice，她的丈夫每个星期天都去慢跑，而她则喜欢在此期间去跳舞课。`'
- en: 'A human can isolate `Alice` and find the predicate:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人可以分离出`Alice`并找到谓词：
- en: '`Alice liked to go to a dancing class in the meantime.`'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`爱丽丝喜欢在此期间去上舞蹈课。`'
- en: Can the BERT model find the predicate like us?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型能像我们一样找到谓词吗？
- en: 'Let’s find out by first running the code in `SRL.ipynb`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先在`SRL.ipynb`中运行代码，看看结果如何：
- en: '[PRE12]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output identified the verbs for each predicate and labeled each frame:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 输出确定了每个谓词的动词并标记了每个框架：
- en: '[PRE13]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s focus on the part we are interested in and see if the model found the
    predicate. It did! It found the verb `liked` as shown in this excerpt of the output,
    although the verb `like` is separated from `Alice` by another predicate:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注我们感兴趣的部分，看看模型是否找到了谓词。它找到了！它在输出的节选中找到了动词`liked`，尽管动词`like`与`Alice`被另一个谓词隔开：
- en: '`Verb: liked [ARG0: Alice , whose husband went jogging every Sunday]`'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`动词：liked [ARG0: Alice，whose husband went jogging every Sunday]`'
- en: 'Let’s now look at the visual representation of the model’s analysis after running
    the sample on AllenNLP’s online UI. The transformer first finds Alice’s husband:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看在AllenNLP的在线UI上运行样本后模型分析的视觉表示。转换器首先找到了爱丽丝的丈夫：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_12.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_12.png)'
- en: 'Figure 10.12: The predicate “went” has been identified'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.12：谓语“went”已被识别
- en: 'The transformer explains that:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器解释道：
- en: The predicate or verb is `went`
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谓语或动词是`went`
- en: '`whose husband` is the argument'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`whose husband`是参数'
- en: '`jogging` is another argument related to `went`'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`慢跑`是另一个与`went`相关的参数'
- en: '`every Sunday` is a temporal modifier represented in the raw output as `[ARGM-TMP:
    every Sunday]`'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`每个星期天`是一个表示在原始输出中的时间修饰语 `[ARGM-TMP: 每个星期天]`'
- en: 'The transformer then found what Alice’s husband was *doing*:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后转换器发现了爱丽丝的丈夫在*做*什么：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_13.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_13.png)'
- en: 'Figure 10.13: SRL detection of the verb “jogging”'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：动词“慢跑”的SRL检测
- en: We can see that the verb `jogging` was identified and was related to `whose
    husband` with the temporal modifier `every Sunday`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到动词`jogging`被识别出来并与`她的丈夫`以及时态修饰语`每个星期天`相关联。
- en: 'The transformer doesn’t stop there. It now detects what Alice liked:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器到此为止。现在它发现了爱丽丝喜欢什么：
- en: '![](img/B17948_10_14.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_14.png)'
- en: 'Figure 10.14: Identifying the verb “liked”'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：识别动词“liked”
- en: 'The transformer also detects and analyzes the verb `go` correctly:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器还正确地检测和分析了动词`go`：
- en: '![](img/B17948_10_15.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_15.png)'
- en: 'Figure 10.15: Detecting the verb “go,” its arguments, and modifier'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：检测动词“go”，它的参数和修饰语
- en: We can see that the temporal modifier `in the meantime` was also identified.
    It is quite a performance considering the simple *sequence* + *verb* input SRL
    BERT was trained with.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，时态修饰语`与此同时`也被识别出来了。考虑到SRL BERT训练时的简单*序列* + *动词*输入，这是相当出色的表现。
- en: 'Finally, the transformer identifies the last verb, `dancing`, as being related
    to `class`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，转换器确定了最后一个动词`dancing`与`class`有关：
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_16.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant texte  Description générée automatiquement](img/B17948_10_16.png)'
- en: 'Figure 10.16: Relating the argument “class” to the verb “dancing”'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16：将参数“class”与动词“跳舞”相关联
- en: The results produced by *Sample 4* are quite convincing! Let’s try to find the
    limit of the transformer model.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例4*产生的结果相当令人信服！让我们尝试找到转换器模型的极限。'
- en: Sample 5
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例5
- en: '*Sample 5* does not repeat a verb several times. However, *Sample 5* contains
    a word with multiple functions and meanings. It goes beyond polysemy since the
    word `round` can have different meanings and grammatical functions. The word `round`
    can be a noun, an adjective, an adverb, a transitive verb, or an intransitive
    verb.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例5* 不会多次重复动词。但是，*示例5* 包含一个具有多种功能和含义的单词。它超越了一词多义，因为单词`round`可以有不同的含义和语法功能。单词`round`可以是名词、形容词、副词、及物动词或不及物动词。'
- en: As a transitive or intransitive verb, `round` can attain perfection or completion.
    In this sense, `round` can be used with `off`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 作为及物动词或不及物动词，`round`可以达到完美或完成。在这个意义上，`round`可以用于`off`。
- en: 'The following sentence uses `round` in the past tense:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下句子使用`round`的过去式：
- en: '`The bright sun, the blue sky, the warm sand, the palm trees, everything round
    off.`'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`明亮的太阳，蓝天，温暖的沙滩，棕榈树，一切都圆满结束。`'
- en: The verb `round` in this predicate is used in a sense of “to bring to perfection.”
    Of course, the most accessible grammatical form would have been “rounded.” but
    let’s see what happens with our sentence.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 此谓词中的动词`round`是“使完美”的意思。当然，最容易理解的语法形式应该是“rounded”。但我们来看看我们的句子会发生什么。
- en: 'Let’s run *Sample 5* in `SRL.ipynb`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`SRL.ipynb`中运行*Sample 5*：
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output shows no verbs. The transformer did not identify the predicate.
    In fact, it found no verbs at all even when we run the `full(prediction)` function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示没有动词。变换器未识别出谓词。实际上，即使我们运行`full(prediction)`函数时，它也找不到任何动词：
- en: '[PRE15]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'However, the online version seems to interpret the sentence better because
    it found the verb:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在线版本似乎更好地解释了句子，因为它找到了动词：
- en: '![](img/B17948_10_17.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_17.png)'
- en: 'Figure 10.17: Detecting the verb “round” and “everything” as the argument'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.17：检测动词“round”和“everything”作为参数
- en: 'Since we like our SRL transformer, we will be kind to it. *We will show it
    what to do with a verb form that is more frequently used*. Let’s change the sentence
    from the past tense to the present tense by adding an `s` to `round`:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们喜欢我们的SRL变换器，我们会对它友好。*我们将向它展示如何处理更常用的动词形式*。我们将句子从过去时改为现在时，通过在`round`后加上`s`：
- en: '`The bright sun, the blue sky, the warm sand, the palm trees, everything` `rounds`
    `off.`'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`明亮的阳光，蓝色的天空，温暖的沙滩，棕榈树，一切都` `rounds` `off。`'
- en: 'Let’s give `SRL.ipynb` another try with the present tense:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试用现在时运行`SRL.ipynb`：
- en: '[PRE16]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The raw output shows that the predicate was found, as shown in the following
    output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输出显示找到了谓词，如下输出所示：
- en: '[PRE17]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we run the sentence on AllenNLP, we obtain the visual explanation:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在AllenNLP上运行句子，我们将得到视觉解释：
- en: '![](img/B17948_10_18.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_18.png)'
- en: 'Figure 10.18: Detecting the word “rounds” as a verb'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18：检测单词“rounds”作为动词
- en: Our BERT-based transformer did well because the word `round` can be found as
    `rounds` in its present tense form.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于BERT的变换器表现良好，因为单词`round`在其现在时形式`rounds`中可以找到。
- en: The BERT model initially failed to produce the result we expected. But with
    a little help from its friends, all ended well for this sample.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型最初未能产生我们预期的结果。但在它的朋友的一点帮助下，这个样本最终结束得不错。
- en: 'We can see that:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到：
- en: Outputs may vary with the evolution of the versions of a model we have implemented
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出可能会随着我们实施的模型版本的演变而有所不同
- en: An Industry 4.0 pragmatic mindset requires more cognitive efforts to *show*
    a transformer what to do
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工业4.0的实用主义心态需要更多认知努力来*向变换器展示要做什么*
- en: Let’s try another sentence that’s difficult to label.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试另一个难以标记的句子。
- en: Sample 6
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本6
- en: '*Sample 6* takes a word we often think is just a noun. However, more words
    than we suspect can be both nouns and verbs. For example, *to ice* is a verb used
    in hockey to shoot a puck all the way across the rink and beyond the goal line
    of an opponent. A puck is the disk used in hockey.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sample 6*选取了一个我们经常认为只是名词的词。然而，我们怀疑的词比我们想象的更多，可以是名词也可以是动词。例如，*to ice*是曲棍球中用来将冰球射向整个溜冰场并超出对手球门线的动词。冰球是曲棍球中使用的圆盘。'
- en: 'A hockey coach can start the day by telling a team to train icing pucks. We
    then can obtain the *imperative* sentence when the coach yells:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 曲棍球教练可以通过告诉队伍训练冰球来开始一天。然后当教练喊道时，我们可以得到*祈使句*：
- en: '`Now, ice pucks guys!`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`现在，让冰球过去，伙计们！`'
- en: Note that `guys` can mean persons regardless of their sex.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`guys`可以表示无论性别的人。
- en: 'Let’s run the *Sample 6* cell to see what happens:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行*Sample 6*单元格，看看会发生什么：
- en: '[PRE18]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The transformer fails to find the verb: `"verbs": []`.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器无法找到动词：“verbs”：[]。
- en: Game over! We can see that transformers have made tremendous progress, but there
    is still a lot of room for developers to improve the models. Humans are still
    in the game to *show* transformers what to do.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏结束！我们可以看到变换器已经取得了巨大的进步，但开发人员仍然有很多改进模型的空间。人类仍然在*向变换器展示要做什么*的游戏中。
- en: 'The online interface confuses `pucks` with a verb:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在线界面将`pucks`与动词混淆：
- en: '![](img/B17948_10_19.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_10_19.png)'
- en: 'Figure 10.19: The model incorrectly labels “pucks” as the verb'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19：模型错误地将“pucks”标记为动词
- en: This problem may be solved with another model, but you will reach another limit.
    Even GPT-3 has limits you will have to cope with.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可能会通过另一个模型解决，但你会遇到另一个限制。即使是GPT-3也有你必须应对的限制。
- en: When you implement transformers for a customized application with specialized
    jargon or technical vocabulary, you will reach intractable limits at some point.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当你为一个定制的应用程序实现变换器时，使用专业术语或技术词汇，你会在某个时候达到难以解决的限制。
- en: These limits will require your expertise to make a project a success. Therefore,
    you will have to create specialized dictionaries to succeed in a project. This
    is good news for developers! You will develop new cross-disciplinary and cognitive
    skills that your team will appreciate.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Try some examples or samples of your own to see what SRL can do with the approach’s
    limits. Then explore how to develop preprocessing functions to show the transformer
    what to do for your customized applications.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Before we leave, let’s question the motivation of SRL.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Questioning the scope of SRL
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are alone when faced with a real-life project. We have a job to do, and the
    only people to satisfy are those who asked for that project.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '*Pragmatism must come first. Technical ideology after.*'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: In the 2020s, former AI ideology and new ideology coexist. By the end of the
    decade, there will be only one winner merging some of the former into the latter.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'This section questions the productivity of SRL and also its motivation through
    two aspects:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: The limit of predicate analysis
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questioning the use of the term “semantic”
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limit of predicate analysis
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SRL relies on predicates. SRL BERT only works as long as you provide a verb.
    But millions of sentences do not contain verbs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: If you provide SRL BERT in the **Semantic Role Labeling** section in the AllenNLP
    demo interface ([https://demo.allennlp.org/](https://demo.allennlp.org/)) with
    an assertion alone, it works.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'But what happens if your assertion is an answer to a question:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Person 1: `What would you like to drink, please?`'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Person 2: `A cup of coffee, please.`'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we enter Person 2’s answer, SRL BERT finds nothing:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B17948_10_20.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.20: No frame obtained'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The output is `0` total frames. SRL was unable to analyze this sentence because
    it contains an *ellipsis*. The predicate is implicit, not explicit.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: The definition of an ellipsis is the act of leaving one or several words out
    of a sentence that is not necessary for it to be understood.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Hundreds of millions of sentences containing an ellipsis are spoken and written
    each day.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Yet SRL BERT yields *0 Total Frames* for all of them.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'The following answers (`A`) to questions (`Q`) beginning with `what`, `where`,
    and `how` yield *0 Total Frames*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '`Q: What would you like to have for breakfast?`'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '`A: Pancakes with hot chocolate.`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '(The model deduces: `pancakes`=proper noun, `with`=preposition, `hot`= adjective,
    and `chocolate`=common noun.)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '`Q: Where do you want to go?`'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '`A: London, please.`'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '(The model deduces: `London`=proper noun and `please`=adverb.)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '`Q: How did you get to work today?`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '`A: Subway.`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '(The model deduces: `subway`=proper noun.)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: We could find millions more examples that SRL BERT fails to understand because
    the sentences do not contain predicates.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'We could apply this to questions in the middle of dialogue as well and still
    obtain no frames (outputs) from SRL BERT:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'Context: The middle of a conversation during which person 2 did not want coffee:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '`Q: So, tea?`'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '`A: No thanks.`'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '`Q: Ok, hot chocolate?`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '`A: Nope.`'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '`Q: A glass of water?`'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '`A: Yup!`'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: We just saw a conversation with no frames, no semantic labeling. Nothing.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'We can finish with some movie, concert, or exhibition reviews on social media
    that produce 0 frames:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '`Best movie ever!`'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Worst concert in my life!`'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Excellent exhibition!`'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This section showed the limits of SRL. Let’s now redefine SRL and show how to
    implement it.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Redefining SRL
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SRL BERT presupposes that sentences contain predicates, which is a false assumption
    in many cases. Analyzing a sentence cannot be based on a predicate analysis alone.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'A predicate contains a verb. The predicate tells us more about the subject.
    The following predicate contains a verb and additional information:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`ate...quickly` tells us more about the way the dog ate. However, a verb alone
    can be a predicate, as in:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '`Dogs eat.`'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: The problem here resides in the fact that “verbs” and “predicates” are part
    of syntax and grammar analysis, not semantics.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how words fit together from a grammatical, functional point of
    view is restrictive.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Take this sentence that means absolutely nothing:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'SRL BERT perfectly performs “semantic” analysis on a sentence that means nothing:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing timeline  Description automatically generated](img/B17948_10_21.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.21: Analyzing a meaningless sentence'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'We can draw some conclusions from these examples:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: SRL predicate analysis only works when there is a verb in the sentence
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SRL predicate analysis cannot identify an ellipsis
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicates and verbs are part of the structure of a language, of grammatical
    analysis
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicate analysis identifies structures but not the meaning of a sentence
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammatical analysis goes far beyond predicate analysis as the necessary center
    of a sentence
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantics focuses on the meaning of a phrase or sentence. Semantics focuses
    on context and the way words relate to each other.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Grammatical analysis includes syntax, inflection, and the functions of words
    in a phrase or sentence. The term semantic role labeling is misleading; it should
    be named “predicate role labeling.”
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: We perfectly understand sentences without predicates and beyond sequence structure.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment analysis can decode the meaning of a sentence and give us an output
    without predicate analysis. Sentiment analysis algorithms perfectly understand
    that “Best movie ever” is positive regardless of the presence of a predicate or
    not.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Using SRL alone to analyze language is restrictive. Using SRL in an AI pipeline
    or other AI tools can be very productive to add more intelligence to natural language
    understanding.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: My recommendation is to use SRL with other AI tools, as we will see in *Chapter
    13*, *Analyzing Fake News with Transformers*.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now conclude our exploration of the scope and limits of SRL.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored SRL. SRL tasks are difficult for both humans and
    machines. Transformer models have shown that human baselines can be reached for
    many NLP topics to a certain extent.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: We found that a simple BERT-based transformer can perform predicate sense disambiguation.
    We ran a simple transformer that could identify the meaning of a verb (predicate)
    without lexical or syntactic labeling. *Shi* and *Lin* (2019) used a standard
    *sentence* + *verb* input format to train their BERT-based transformer.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: We found that a transformer trained with a stripped-down *sentence* + *predicate*
    input could solve simple and complex problems. The limits were reached when we
    used relatively rare verb forms. However, these limits are not final. If difficult
    problems are added to the training dataset, the research team could improve the
    model.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: We also discovered that AI for the good of humanity exists. The *Allen Institute
    for AI* has made many free AI resources available. In addition, the research team
    has added visual representations to the raw output of NLP models to help users
    understand AI. We saw that explaining AI is as essential as running programs.
    The visual and text representations provided a clear view of the potential of
    the BERT-based model.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we explored the scope and limits of SRL to optimize how we will use
    this method with other AI tools.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Transformers will continue to improve the standardization of NLP through their
    distributed architecture and input formats.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, *Chapter 11*, *Let Your Data Do the Talking: Story, Questions,
    and Answers*, we will challenge transformers on tasks usually only humans perform
    well. We will explore the potential of transformers when faced with **Named Entity
    Recognition** (**NER**) and question-answering tasks.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Semantic Role Labeling** (**SRL**) is a text generation task. (True/False)'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A predicate is a noun. (True/False)
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A verb is a predicate. (True/False)
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arguments can describe who and what is doing something. (True/False)
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A modifier can be an adverb. (True/False)
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A modifier can be a location. (True/False)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A BERT-based model contains encoder and decoder stacks. (True/False)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A BERT-based SRL model has standard input formats. (True/False)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can solve any SRL task. (True/False)
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Peng Shi* and *Jimmy Lin*, 2019, *Simple BERT Models for Relation Extraction
    and Semantic Role Labeling*: [https://arxiv.org/abs/1904.05255](https://arxiv.org/abs/1904.05255)'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Allen Institute for AI: [https://allennlp.org/](https://allennlp.org/)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Allen Institute for AI semantic role labeling resources: [https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling)'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
