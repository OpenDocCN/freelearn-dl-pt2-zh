- en: CUDA - GPU-Accelerated Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA - GPU加速训练
- en: This chapter will look at the hardware side of deep learning. First, we will
    take a look at how CPUs and GPUs serve our computational needs for building **Deep
    Neural Networks **(**DNNs**), how they are different, and what their strengths
    are. The performance improvements offered by GPUs are central to the success of
    deep learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨深度学习的硬件方面。首先，我们将看看CPU和GPU在构建**深度神经网络**（**DNNs**）时如何满足我们的计算需求，它们之间的区别以及它们的优势在哪里。GPU提供的性能改进是深度学习成功的核心。
- en: We will learn about how to get Gorgonia working with our GPU and how to accelerate
    our Gorgonia models using **CUDA**: NVIDIA's software library for facilitating
    the easy construction and execution of GPU-accelerated deep learning models. We
    will also learn about how to build a model that uses GPU-accelerated operations
    in Gorgonia, and then benchmark the performance of these models versus their CPU
    counterparts to determine which is the best option for different tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何让 Gorgonia 与我们的GPU配合工作，以及如何利用**CUDA**来加速我们的 Gorgonia 模型：这是 NVIDIA 的软件库，用于简化构建和执行GPU加速深度学习模型。我们还将学习如何构建一个使用GPU加速操作的模型，并对比这些模型与CPU对应物的性能来确定不同任务的最佳选择。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: CPUs versus GPUs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU与GPU对比
- en: Understanding Gorgonia and CUDA
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Gorgonia 和 CUDA
- en: Building a model in Gorgonia with CUDA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CUDA 在 Gorgonia 中构建模型
- en: Performance benchmarking of CPU versus GPU models for training and inference
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练和推理的CPU与GPU模型的性能基准测试
- en: CPUs versus GPUs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU与GPU对比
- en: At this point, we've covered much of the basic theory and practice of neural
    networks, but we haven't given much consideration to the processors running them.
    So let's take a break from coding and go into more depth about the little slices
    of silicon that are actually doing the work.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了神经网络的基本理论和实践，但我们还没有多考虑运行它们的处理器。因此，让我们暂停编码，更深入地讨论实际执行工作的这些小小硅片。
- en: The 30,000-foot view is that CPUs were originally designed to favor scalar operations,
    which are performed sequentially, and GPUs are designed for vector operations,
    which are performed in parallel. Neural networks perform a large number of independent
    calculations within a layer (say, each neuron multiplied by its weight), and so
    they are a processing workload amenable to a chip design that favors massive parallelism.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从30,000英尺高空看，CPU最初是为了支持标量操作而设计的，这些操作是按顺序执行的，而GPU则设计用于支持向量操作，这些操作是并行执行的。神经网络在每个层内执行大量的独立计算（比如，每个神经元乘以它的权重），因此它们是适合于偏向大规模并行的芯片设计的处理工作负载。
- en: 'Let''s make this a little more concrete by walking through an example of the
    types of operations that take advantage of the performance characteristics of
    each. Take the two-row vectors of [1, 2, 3] and [4, 5, 6]. If we were to perform
    element-wise matrix multiplication on these, it would look like this:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例来具体说明一下，这种类型的操作如何利用每种性能特征。拿两个行向量 [1, 2, 3] 和 [4, 5, 6] 作为例子，如果我们对它们进行逐元素矩阵乘法，看起来会像这样：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, the CPU performed the computation sequentially, while the GPU
    performed it in parallel. This resulted in the GPU taking less time to complete
    the computation than the CPU. This is a fundamental difference between the two
    types of processors that we care about for the workloads associated with DNNs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，CPU是按顺序执行计算，而GPU是并行执行的。这导致GPU完成计算所需的时间比CPU少。这是我们在处理与DNN相关的工作负载时关心的两种处理器之间的基本差异。
- en: Computational workloads and chip design
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算工作负载和芯片设计
- en: 'How does this difference manifest in terms of the actual design of the processor
    itself? This diagram, taken from NVIDIA''s own CUDA documentation, illustrates
    these differences:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种差异如何在处理器的实际设计中体现？这张图表，摘自NVIDIA自己的CUDA文档，说明了这些差异：
- en: '![](img/0bdb4464-70b6-4df5-a217-e1d027a3a3c9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0bdb4464-70b6-4df5-a217-e1d027a3a3c9.png)'
- en: 'Control or cache units are reduced, while there is a significant increase in
    the number of cores or ALUs. This results in improvement of an order of magnitude
    (or more) in performance. The caveat to this is that GPU efficiency is far from
    perfect with respect to memory, compute, and power. This is why a number of companies
    are racing to design a processor for DNN workloads from the ground up, to optimize
    the ratio of cache units/ALUs, and to improve the way in which data is pulled
    into memory and then fed into the compute units. Currently, memory is a bottleneck
    in GPUs, as illustrated by the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 控制或缓存单元减少，而核心或 ALUs 数量显著增加。这导致性能提升一个数量级（或更多）。与此相关的警告是，相对于内存、计算和功耗，GPU 的效率远非完美。这就是为什么许多公司正在竞相设计一个从头开始为
    DNN 工作负载优化缓存单元/ALUs比例，并改善数据被拉入内存然后供给计算单元的方式的处理器。目前，内存在 GPU 中是一个瓶颈，如下图所示：
- en: '![](img/bfcece11-f4c4-4dc6-9a86-012ddebb635f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfcece11-f4c4-4dc6-9a86-012ddebb635f.png)'
- en: The ALUs can only work if they have something to work on. If we exhaust the
    on-die memory, we have to go to the L2 cache, which is faster in a GPU than in
    a CPU, but still takes far longer than on-die L1 memory to access. We will discuss
    these shortcomings in the context of new and competing chip designs in a later
    chapter. For now, the important thing to understand is that, ideally, we want
    to have as many ALUs and as much on-die cache as we can cram into a chip, in the
    right ratio, and with fast communication between the processors and their memory.
    For this process, CPUs do work, but GPUs are far better. And for now, they are
    the most suitable hardware for machine learning that is widely available to consumers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当 ALUs 有东西可以处理时，它们才能工作。如果我们用尽了芯片上的内存，我们必须去 L2 缓存，这在 GPU 中比在 CPU 中更快，但访问芯片内
    L1 内存仍然比访问芯片外 L2 缓存要慢得多。我们将在后面的章节中讨论这些缺陷，以及新的和竞争性的芯片设计的背景。目前，理解的重要事情是，理想情况下，我们希望在芯片中尽可能塞入尽可能多的
    ALUs 和尽可能多的芯片内缓存，以正确的比例，并且在处理器和它们的内存之间进行快速通信。对于这个过程，CPU 确实工作，但 GPU 更好得多。而且目前，它们是广泛面向消费者的最适合机器学习的硬件。
- en: Memory access in GPUs
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU 中的内存访问
- en: By now, it should hopefully be clear to you that fast and local memory is key
    to the performance of the kinds of workloads we are offloading to our processor
    when doing deep learning. It is, however, not just the quantity and proximity
    of memory that matters, but also how this memory is accessed. Think of sequential
    access versus random access performance on hard drives, as the principle is the
    same.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能已经清楚，当我们把深度学习的工作负载卸载到我们的处理器时，快速和本地的内存是性能的关键。然而，重要的不仅仅是内存的数量和接近程度，还有这些内存的访问方式。想象一下硬盘上的顺序访问与随机访问性能，原则是相同的。
- en: Why does this matter for DNNs? Put simply, they are high-dimensional structures
    that have to be embedded, ultimately, in a 1D space for the memory that feeds
    our ALUs. Modern (vector) GPUs, built for graphics workloads, assume that they
    will be accessing adjacent memory, which is where one part of a 3D scene will
    be stored next to a related part (adjacent pixels in a frame). Thus, they are
    optimized for this assumption. Our networks are not 3D scenes. The layout of their
    data is sparse and dependent on network (and, in turn, graph) structure and the
    information they hold.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么对 DNNs 这么重要？简单来说，它们是高维结构，最终需要嵌入到供给我们 ALUs 的内存的一维空间中。现代（向量）GPU，专为图形工作负载而建，假设它们将访问相邻的内存，即一个
    3D 场景的一部分将存储在相关部分旁边（帧中相邻像素）。因此，它们对这种假设进行了优化。我们的网络不是 3D 场景。它们的数据布局是稀疏的，依赖于网络（及其反过来的图）结构和它们所持有的信息。
- en: 'The following diagram represents the memory access motifs for these different
    workloads:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下图代表了这些不同工作负载的内存访问模式：
- en: '![](img/b7d762a5-eb4d-4f84-bb1b-6ea46e2e761e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7d762a5-eb4d-4f84-bb1b-6ea46e2e761e.png)'
- en: For DNNs, we are looking to get as close to **Strided** memory access patterns
    as possible when we write our operations. After all, matrix multiplication happens
    to be one of the more common operations in DNNs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度神经网络（DNNs），当我们编写操作时，我们希望尽可能接近**跨距（Strided）**内存访问模式。毕竟，在 DNNs 中，矩阵乘法是比较常见的操作之一。
- en: Real-world performance
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际性能
- en: To get a feel for the real-world performance differences, let's compare one
    of the CPUs that's best suited for neural network workloads, the Intel Xeon Phi,
    versus an NVIDIA Maxwell GPU from 2015.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真实体验实际性能差异，让我们比较适合神经网络工作负载之一的 CPU，即 Intel Xeon Phi，与 2015 年的 NVIDIA Maxwell
    GPU。
- en: Intel Xeon Phi CPU
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Intel Xeon Phi CPU
- en: 'Here are some hard performance numbers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些硬性能数字：
- en: This chip's compute units are capable of 2,400 Gflops/sec, and pulls 88 Gwords/sec
    from DRAM, with a ratio of 27/1
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该芯片的计算单元每秒可达2,400 Gflops，并从DRAM中提取88 Gwords/sec，比率为27/1
- en: This means that there are 27 floating-point operations per word fetched from
    memory
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这意味着每次从内存中提取的字，有27次浮点操作
- en: NVIDIA Maxwell GPU
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NVIDIA 的 Maxwell GPU
- en: 'Now, here are the numbers for a reference NVIDIA GPU. Pay specific attention
    to the change in ratio:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是参考 NVIDIA GPU 的数字。特别注意比率的变化：
- en: 6,100 Gflops/sec
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6,100 Gflops/sec
- en: 84 Gwords/sec
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 84 Gwords/sec
- en: Ratio is 72/1
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比率为72/1
- en: So, just in terms of raw operations per chunk of memory, GPUs have a clear advantage.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，就每块内存中的原始操作而言，GPU具有明显的优势。
- en: A full detour into microprocessor design is of course outside the scope of this
    book, but it is useful to think about the processor's distribution of memory and
    compute units. The design philosophy for modern chips can be summed up as *cram
    as many floating-point units onto the chip as possible to achieve the m**aximum
    computation relative to the power required/heat generated*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，深入微处理器设计的细节超出了本书的范围，但思考处理器内存和计算单元的分布是有用的。现代芯片的设计理念可以总结为*尽可能多地将浮点单位集成到芯片上，以实现最大的计算能力相对于所需的功耗/产生的热量*。
- en: The idea is to keep those ALUs as full as possible, thus minimizing the amount
    of time they sit idle while memory gets filled.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 思想是尽可能保持这些算术逻辑单元（ALUs）处于完整状态，从而最小化它们空闲时的时间。
- en: Understanding Gorgonia and CUDA
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 Gorgonia 和 CUDA
- en: Before we step into how Gorgonia works with CUDA, let's quickly introduce you
    to CUDA and what it is.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入介绍 Gorgonia 如何与 CUDA 协作之前，让我们快速介绍一下 CUDA 及其背景。
- en: CUDA
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**CUDA**'
- en: CUDA is NVIDIA's programming language for its GPUs. This means your AMD card
    does not support CUDA. In the growing landscape of deep learning libraries, languages,
    and tools, it is a de facto standard. The C implementation is freely available,
    but of course, it is only compatible with NVIDIA's own hardware.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 是 NVIDIA 的 GPU 编程语言。这意味着您的 AMD 卡不支持 CUDA。在不断发展的深度学习库、语言和工具的景观中，它是事实上的标准。C
    实现是免费提供的，但当然，它仅与 NVIDIA 自家的硬件兼容。
- en: Basic Linear Algebra Subprograms
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础线性代数子程序
- en: As we've seen in the networks we've built so far, tensor operations are fundamental
    to machine learning. GPUs are designed for these types of vector or matrix operations,
    but our software also needs to be designed to take advantage of these optimizations.
    Enter **BLAS**!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们迄今所建立的网络中所看到的，张量操作对机器学习至关重要。GPU专为这些类型的向量或矩阵操作设计，但我们的软件也需要设计以利用这些优化。这就是**BLAS**的作用！
- en: 'BLAS provide the building blocks for linear algebra operations, commonly used
    in graphics programming as well as machine learning. BLAS libraries are low level,
    originally written in Fortran, and group the functionality they offer into three
    *levels*, defined by the types of operations covered, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**BLAS** 提供了线性代数操作的基本组成部分，通常在图形编程和机器学习中广泛使用。**BLAS** 库是低级的，最初用Fortran编写，将其提供的功能分为三个*级别*，根据涵盖的操作类型定义如下：'
- en: '**Level 1**: Vector operations on strided arrays, dot products, vector norms,
    and generalized vector addition'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 1**：对步进数组的向量操作，点积，向量范数和广义向量加法'
- en: '**Level 2**: Generalized matrix-vector multiplication, solver for linear equations
    involving triangular matrices'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 2**：广义矩阵-向量乘法，解决包含上三角矩阵的线性方程'
- en: '**Level 3**: Matrix operations, including **General Matrix Multiplication**
    (**GEMM**)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Level 3**：矩阵操作，包括**广义矩阵乘法**（**GEMM**）'
- en: Level 3 operations are what we're really interested in for deep learning. Here's
    an example from the CUDA-fied convolution operation in Gorgonia.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Level 3** 操作是我们在深度学习中真正感兴趣的。以下是 Gorgonia 中 CUDA 优化卷积操作的示例。'
- en: CUDA in Gorgonia
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gorgonia 中的 CUDA
- en: Gorgonia has implemented support for NVIDIA's CUDA as part of its `cu` package.
    It abstracts out almost all the complexity, so all we have to do is simply specify
    the `--tags=cuda` flag at build time and ensure the operations we are calling
    are in fact present in the Gorgonia API.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Gorgonia 已经实现了对 NVIDIA 的 CUDA 的支持，作为其`cu`包的一部分。它几乎隐藏了所有的复杂性，因此我们在构建时只需简单地指定`--tags=cuda`标志，并确保我们调用的操作实际上存在于
    Gorgonia 的 API 中。
- en: Not every possible operation is implemented, of course. The emphasis is on operations
    that benefit from parallel execution, amenable to GPU acceleration. As we will
    cover in [Chapter 5](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml), *Next Word Prediction
    with Recurrent Neural Networks*, many of the operations involved in **Convolutional
    Neural Networks** (**CNNs**) meet this criterion.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，并非所有可能的操作都实现了。重点是那些从并行执行中获益并适合 GPU 加速的操作。正如我们将在[第5章](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml)中介绍的，*使用递归神经网络进行下一个词预测*，许多与**卷积神经网络**（**CNNs**）相关的操作符合这一标准。
- en: 'So, what''s available? The following list outlines the options:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，有哪些可用的呢？以下列表概述了选项：
- en: 1D or 2D convolutions (used in CNNs)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1D 或 2D 卷积（在 CNN 中使用）
- en: 2D max pooling (also used in CNNs!)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2D 最大池化（也用于 CNN 中！）
- en: Dropout (kill some neurons!)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout（杀死一些神经元！）
- en: ReLU (recall activation functions in [Chapter 2](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml),
    *What is a Neural Network and How Do I Train One?*)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU（回顾[第2章](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml)，*什么是神经网络及其训练方式？*中的激活函数）
- en: Batch normalization
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化
- en: We will now look at the implementation of each, in turn.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将依次查看每个的实现。
- en: 'Looking at `gorgonia/ops/nn/api_cuda.go`, we see the function for a 2D convolution
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 `gorgonia/ops/nn/api_cuda.go`，我们可以看到以下形式的 2D 卷积函数：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following 1D convolution function returns an instance of `Conv2d()`, which
    is a neat way of providing us with both options:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 1D 卷积函数返回一个 `Conv2d()` 实例，这是一种提供两种选项的简洁方法：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next is the `MaxPool2D()` function. In a CNN, the max pooling layer is part
    of the process of feature extraction. The dimensionality of the input is reduced,
    before being passed on to the subsequent convolutional layer.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `MaxPool2D()` 函数。在 CNN 中，最大池化层是特征提取过程的一部分。输入的维度被减少，然后传递给后续的卷积层。
- en: 'Here, we create an instance of `MaxPool` that carries our `XY` parameters,
    and we return the result of running `ApplyOp()` across our input node, as shown
    in the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个带有 `XY` 参数的 `MaxPool` 实例，并返回在我们的输入节点上运行 `ApplyOp()` 的结果，如以下代码所示：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Dropout()` is a regularization technique that is used to prevent our networks
    from overfitting. We want to learn the most general representation of our input
    data possible, and dropout helps us do that.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout()` 是一种正则化技术，用于防止网络过拟合。我们希望尽可能学习输入数据的最一般表示，而丢失功能可以帮助我们实现这一目标。'
- en: 'The structure of `Dropout()` should be familiar by now. It is another operation
    that can be parallelized within a layer, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout()` 的结构现在应该已经很熟悉了。它是另一种在层内可以并行化的操作，如下所示：'
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The standard ReLU function we covered in [Chapter 2](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml),
    *What is a Neural Network and How Do I Train One?*, is also available, as shown
    here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml)中介绍的标准 ReLU 函数也是可用的，如下所示：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`BatchNorm()` is slightly more complicated. Looking back at the original paper
    that described batch normalization, by Szegedy and Ioffe (2015), we see how, for
    a given batch, we normalize the output of the previous layer by subtracting the
    mean of the batch and dividing by the standard deviation. We can also observe
    the addition of two parameters that we will train with SGD.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchNorm()` 稍微复杂一些。回顾一下由 Szegedy 和 Ioffe（2015）描述批标准化的原始论文，我们看到对于给定的批次，我们通过减去批次的均值并除以标准差来对前一层的输出进行归一化。我们还可以观察到添加了两个参数，这些参数将通过
    SGD 进行训练。'
- en: 'And now, we can see the CUDA-fied Gorgonia implementation as follows. First,
    let''s perform the function definition and a data type check:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到 CUDA 化的 Gorgonia 实现如下。首先，让我们执行函数定义和数据类型检查：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, it needs to create some scratch variables to allow the VM to allocate
    spare memory:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，需要创建一些临时变量，以允许虚拟机分配额外的内存：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then create the equivalent variables in our computation graph:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在计算图中创建等效的变量：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then create our scale and bias variables in the graph, before applying our
    function and returning the results:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在应用函数并返回结果之前，我们在图中创建了我们的比例和偏差变量：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, let's take a look at how to build a model in Gorgonia that leverages CUDA.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何在 Gorgonia 中构建利用 CUDA 的模型。
- en: Building a model in Gorgonia with CUDA support
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Gorgonia 中构建支持 CUDA 的模型
- en: Building a model in Gorgonia with CUDA support that we do a few things first.
    We need to install Gorgonia's `cu` interface to CUDA, and then have a model ready
    to train!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持 CUDA 的 Gorgonia 中构建一个模型之前，我们需要先做几件事情。我们需要安装 Gorgonia 的 `cu` 接口到 CUDA，并且准备好一个可以训练的模型！
- en: Installing CUDA support for Gorgonia
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Gorgonia安装CUDA支持
- en: 'To make use of CUDA, you need a computer with a GPU made by NVIDIA. Unfortunately,
    setting up CUDA to work with Gorgonia is a slightly more involved process, as
    it involves setting up a C compiler environment to work with Go, as well as a
    C compiler environment that works with CUDA. NVIDIA has kindly ensured that its
    compiler works with the common toolchain for each platform: Visual Studio on Windows,
    Clang-LLVM on macOS, and GCC on Linux.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用CUDA，您需要一台配有NVIDIA GPU的计算机。不幸的是，将CUDA设置为与Gorgonia配合使用是一个稍微复杂的过程，因为它涉及设置能够与Go配合使用的C编译环境，以及能够与CUDA配合使用的C编译环境。NVIDIA已经确保其编译器与每个平台的常用工具链兼容：在Windows上是Visual
    Studio，在macOS上是Clang-LLVM，在Linux上是GCC。
- en: Installing CUDA and ensuring that everything works correctly requires a fair
    bit of work. We'll look at doing this for Windows and Linux. As Apple has not
    made a computer featuring an NVIDIA GPU for several years (as of writing this),
    we will not cover how to do this on macOS. You can still use CUDA by connecting
    an external GPU to your macOS, but this is a fairly involved process and Apple
    does not (as of writing this) have an officially supported setup with an NVIDIA
    GPU.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 安装CUDA并确保一切正常运行需要一些工作。我们将介绍如何在Windows和Linux上完成此操作。由于截至撰写本文时，Apple已经多年未推出配备NVIDIA
    GPU的计算机，因此我们不会介绍如何在macOS上执行此操作。您仍然可以通过将外部GPU连接到您的macOS上来使用CUDA，但这是一个相当复杂的过程，并且截至撰写本文时，Apple尚未正式支持使用NVIDIA
    GPU的设置。
- en: Linux
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux
- en: As we've discussed, once CUDA is set up nicely, running your Gorgonia code on
    your GPU is as simple as adding `-tags=cuda` when building it. But how do we get
    to a point where that is possible? Let's find out.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们讨论过的，一旦CUDA设置好了，只需在构建Gorgonia代码时添加`-tags=cuda`就可以简单地在GPU上运行它。但是如何达到这一点呢？让我们看看。
- en: 'This guide requires you to install standard Ubuntu 18.04\. NVIDIA provides
    distribution-independent instructions (and troubleshooting steps) at: [https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此指南要求您安装标准的Ubuntu 18.04。NVIDIA提供了独立于发行版的安装说明（以及故障排除步骤）：[https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html)。
- en: 'At a high level, you need to install the following packages:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，您需要安装以下软件包：
- en: NVIDIA driver
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA驱动
- en: CUDA
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA
- en: cuDNN
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuDNN
- en: libcupti-dev
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: libcupti-dev
- en: 'First, you need to ensure you have NVIDIA''s proprietary (not the open source
    default) driver installed. A quick way to check whether you are running it is
    to execute `nvidia-smi`. You should see output similar to the following, which
    indicates the driver version number and other details about your GPU:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要确保安装了NVIDIA的专有（而不是开源默认）驱动程序。快速检查是否运行了它的方法是执行`nvidia-smi`。您应该看到类似以下内容的输出，指示驱动程序版本号和关于您的GPU的其他详细信息：
- en: '![](img/34e90c45-3eef-4f47-9c8c-9ffed01c4adf.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34e90c45-3eef-4f47-9c8c-9ffed01c4adf.png)'
- en: 'If you get `command not found`, you have a couple of options, depending on
    the distribution of Linux you are running. The latest Ubuntu distribution allows
    you to install most of CUDA''s dependencies (including the proprietary NVIDIA
    driver) from the default repositories. This can be done by executing the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现`command not found`错误，则有几种选择，这取决于您所运行的Linux发行版。最新的Ubuntu发行版允许您从默认存储库安装大部分CUDA依赖项（包括专有的NVIDIA驱动程序）。可以通过执行以下命令完成此操作：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Alternatively, you can follow the steps in the official NVIDIA guide (linked
    previously) to manually install the various dependencies.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以按照官方NVIDIA指南中的步骤手动安装各种依赖项。
- en: 'Once the installation has completed and you have rebooted your system, confirm
    that the drivers are installed by running `nvidia-smi` again. You also need to
    verify that the CUDA C compiler (part of the `nvidia-cuda-toolkit` package) is
    installed by executing `nvcc --version`. The output should look similar to the
    following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成并重新启动系统后，请再次运行`nvidia-smi`确认驱动程序已安装。您还需要验证CUDA C编译器（`nvidia-cuda-toolkit`包的一部分）是否已安装，方法是执行`nvcc
    --version`。输出应该类似于以下内容：
- en: '![](img/f3df211d-9e0b-4cfd-aac2-f5b616ae25b5.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3df211d-9e0b-4cfd-aac2-f5b616ae25b5.png)'
- en: 'Once CUDA itself is installed, there are some additional steps you need to
    perform to ensure that Gorgonia has the necessary CUDA libraries compiled and
    available for use:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了CUDA之后，还需要执行一些额外的步骤，以确保Gorgonia已经编译并准备好使用必要的CUDA库：
- en: 'Ensure that the target directory for the modules you are building exists. If
    not, create it with the following command:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你正在构建的模块的目标目录存在。 如果不存在，请使用以下命令创建它：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Run `cudagen` to build the modules as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `cudagen` 来按如下方式构建模块：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After the program executes, verify that the `/target` directory is populated
    with files representing CUDA-fied operations that we will use when building our
    networks, as shown in the following screenshot:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序执行后，请验证 `/target` 目录是否填充了表示我们在构建网络时将使用的 CUDA 化操作的文件，如下截图所示：
- en: '![](img/3c52c837-75d7-4f5b-a77e-993d56bb3dcf.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c52c837-75d7-4f5b-a77e-993d56bb3dcf.png)'
- en: 'Now that the preliminaries are out of the way, let''s test that everything
    is working using the following commands:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在初步工作已完成，让我们使用以下命令测试一切是否正常：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should see output similar to the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似以下的输出：
- en: '![](img/0e17b690-3ae4-41f0-a06d-453389811d1b.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e17b690-3ae4-41f0-a06d-453389811d1b.png)'
- en: You're now ready to take advantage of all the computing capacity provided by
    your GPU!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好利用 GPU 提供的所有计算能力了！
- en: Windows
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Windows
- en: 'The setup for Windows is very similar, but you also need to provide the C compilers
    that are required for both Go and CUDA. This setup is outlined in the following
    steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 的设置非常类似，但你还需要提供适用于 Go 和 CUDA 的 C 编译器。 这个设置在以下步骤中详细说明：
- en: Install a GCC environment; the easiest way to do this on Windows is to install
    MSYS2\. You can download MSYS2 from [https://www.msys2.org/](https://www.msys2.org/).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 GCC 环境；在 Windows 上做到这一点的最简单方法是安装 MSYS2。 你可以从 [https://www.msys2.org/](https://www.msys2.org/)
    下载 MSYS2。
- en: 'After installing MSYS2, update your installation with the following commands:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装 MSYS2 后，使用以下命令更新你的安装：
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Restart MSYS2 and run the following again:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新启动 MSYS2 并再次运行以下命令：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Install the GCC package as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 GCC 包如下：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Install Visual Studio 2017 to get a compiler compatible with CUDA. At the time
    of writing, you can download this from [https://visualstudio.microsoft.com/downloads/](https://visualstudio.microsoft.com/downloads/).
    The Community Edition works fine; if you have a license for any of the other editions,
    they will do as well.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 Visual Studio 2017 以获取与 CUDA 兼容的编译器。 在撰写本文时，你可以从 [https://visualstudio.microsoft.com/downloads/](https://visualstudio.microsoft.com/downloads/)
    下载此软件。 社区版工作正常；如果你有其他版本的许可证，它们也可以使用。
- en: Install CUDA. Download this from the NVIDIA website at: [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
    In my experience, the network installer is less reliable than the local installer,
    so do try the local installer if you cannot get the network installer to work.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 CUDA。 你可以从 NVIDIA 网站下载此软件：[https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)。
    根据我的经验，如果无法使网络安装程序工作，请尝试本地安装程序。
- en: Following that, you should also install cuDNN from NVIDIA: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn).
    The installation process is literally a copy and paste operation and is fairly
    straightforward.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你还应该从 NVIDIA 安装 cuDNN：[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)。
    安装过程是简单的复制粘贴操作，非常简单。
- en: 'Set up the environment variables so that Go and the NVIDIA CUDA compiler driver
    (`nvcc`) know where to find the relevant compilers. You should replace paths,
    where appropriate, with the location where CUDA, MSYS2, and Visual Studio are
    installed. The items you need to add and the relevant variable names are as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置环境变量，以便 Go 和 NVIDIA CUDA 编译器驱动程序 (`nvcc`) 知道如何找到相关的编译器。 你应该根据需要替换 CUDA、MSYS2
    和 Visual Studio 安装的位置。 你需要添加的内容和相关变量名如下：
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Your environment should now be set up correctly to compile CUDA-enabled Go binaries.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境现在应该正确设置，以编译支持 CUDA 的 Go 二进制文件。
- en: 'Now, for Gorgonia, you need to do a few things first, as outlined in the following
    steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了 Gorgonia，你需要首先按以下步骤进行一些操作：
- en: 'Firstly, ensure the following `target` directory for the modules you will be
    building exists:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先确保为你将要构建的模块存在以下 `target` 目录：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, run `cudagen` to build the modules as follows:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后运行 `cudagen` 来按如下方式构建模块：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that you have everything in place, you should install `cudatest`, like
    so:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你已经安装好 `cudatest`，如下所示：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you run `cudatest` now and all is well, you will get something similar to
    the following output:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果现在运行 `cudatest`，并且一切正常，你将得到类似以下的输出：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Performance benchmarking of CPU versus GPU models for training and inference
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为训练和推理的 CPU 对 GPU 模型的性能基准测试
- en: Now that we've done all that work, let's explore some of the advantages of using
    a GPU for deep learning. First, let's go through how to actually get your application
    to use CUDA, and then we'll go through some of the CPU and GPU speeds.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了所有这些工作，让我们探索使用GPU进行深度学习的一些优势。首先，让我们详细了解如何使你的应用程序实际使用CUDA，然后我们将详细介绍一些CPU和GPU的速度。
- en: How to use CUDA
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用CUDA
- en: 'If you''ve completed all the previous steps to get CUDA working, then using
    CUDA is a fairly simple affair. You simply need to compile your application with
    the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经完成了所有前面的步骤来使CUDA工作，那么使用CUDA是一个相当简单的事情。你只需使用以下内容编译你的应用程序：
- en: '[PRE22]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This builds your executable with CUDA support and uses CUDA, rather than the
    CPU, to run your deep learning model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这样构建你的可执行文件就支持CUDA，并使用CUDA来运行你的深度学习模型，而不是CPU。
- en: 'To illustrate, let''s use an example we''re already familiar with – a neural
    network with weights:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，让我们使用一个我们已经熟悉的例子 – 带有权重的神经网络：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This is just our simple feedforward neural network that we built to use on the
    MNIST dataset.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们简单的前馈神经网络，我们构建它来在MNIST数据集上使用。
- en: CPU results
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU结果
- en: 'By running the code, we get output telling us when we started each epoch, and
    roughly what our cost function value was for the last execution. For this specific
    task, we''re only running it for 10 epochs and the results can be seen as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行代码，我们得到的输出告诉我们每个epoch开始的时间，以及上次执行时我们的成本函数值大约是多少。对于这个特定的任务，我们只运行了10个epochs，结果如下所示：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can see that every epoch takes around 26–27 seconds on this CPU, an Intel
    Core i7-2700K.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在这个CPU上，每个epoch大约需要26–27秒，这是一台Intel Core i7-2700K。
- en: GPU results
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU结果
- en: 'We can do the same for the GPU build of the executable. This allows us to compare
    how long an epoch takes to train through the model. As our model is not complex,
    we don''t expect to see that much of a difference:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对可执行文件的GPU构建执行相同的操作。这使我们能够比较每个epoch训练模型所需的时间。由于我们的模型并不复杂，我们不指望看到太大的差异：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: On this GPU (an NVIDIA Geforce GTX960), we can see that this is marginally faster
    for this simple task, at 23–24 seconds.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个GPU（一台NVIDIA Geforce GTX960）上，我们可以看到对于这个简单的任务，速度稍快一些，大约在23–24秒之间。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we had a look at the hardware side of deep learning. We also
    had a look at how CPUs and GPUs serve our computational needs. We also looked
    at how CUDA, NVIDIA's software,  facilitates GPU-accelerated deep learning that
    is implemented in Gorgonia, and finally, we looked at how to build a model that
    uses the features implemented by CUDA Gorgonia.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看了深度学习的硬件方面。我们还看了CPU和GPU如何满足我们的计算需求。我们还看了CUDA如何在Gorgonia中实现GPU加速的深度学习，最后，我们看了如何构建一个使用CUDA
    Gorgonia实现特性的模型。
- en: In the next chapter, we will look into vanilla RNNs and the issues involved
    with RNNs. We will also learn about how to build an LSTM in Gorgonia as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨基本的RNN和与RNN相关的问题。我们还将学习如何在Gorgonia中构建LSTM模型。
