["```py\nfrom pyspark.sql import SparkSession\nspark_session = SparkSession.builder\\\n        .appName(\"covid_analysis\")\\\n        .getOrCreate()\n```", "```py\n# convert to df without schema\ndf_ri_freq = rdd_ri_freq.toDF() \n```", "```py\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n# rdd for research interest frequency data\nrdd_ri_freq = ... \n# convert to df with schema\nschema = StructType(\n          [StructField(\"ri\", StringType(), False), \n           StructField(\"frequency\", IntegerType(), False)])\ndf = spark.createDataFrame(rdd_ri_freq, schema)\n```", "```py\n# datasets location\ngoogle_scholar_dataset_path = \"s3a://my-bucket/dataset/dataset_csv/dataset-google-scholar/output.csv\"\n# load google scholar dataset\ndf_gs = spark_session. \\\n        .read \\\n        .option(\"header\", True) \\\n        .csv(google_scholar_dataset_path)\n```", "```py\n# loada json file\njson_file_path=\"s3a://my-bucket/json/cities.json\"\ndf = spark_session.read.json(json_file_path)\n```", "```py\n# research_interest cannot be None\ndf_gs_clean = df_gs.filter(\"research_interest != 'None'\")\n```", "```py\n# we are only interested in research_interest column\nrdd_ri = df_gs_clean.rdd.map(lambda x: (x[\"research_interest\"]))\n```", "```py\n# raw research_interest data into pairs of research area and a frequency count\nrdd_flattened_ri = rdd_ri.flatMap(lambda x: [(w.lower(), 1) for w in x.split('##')])\n```", "```py\n# The pairs are grouped based on research area and the frequencies are summed up\nrdd_ri_freq = rdd_flattened_ri.reduceByKey(add)\n```", "```py\n# we are interested in the first 5 entries\nrdd_ri_freq_5 = rdd_ri_freq.take(5)\n```", "```py\n# calculate average number of 1st corona vaccine per jurisdiction (state)\ndf_avg_1 = df_covid.groupby(\"jurisdiction\")\\\n  .agg(F.avg(\"_1st_dose_allocations\")\n  .alias(\"avg\"))\\\n  .sort(F.col(\"avg\").desc())\\\n  .toDF(\"state\", \"avg\")\n```", "```py\n# At jurisdiction (state) level, calculate at average weekly 1st & 2nd dose vaccine distribution. Similarly calculate sum for 1st and 2nd dose\ndf_avg = df_covid.groupby(F.lower(\"jurisdiction\").alias(\"state\"))\\\n  .agg(F.avg(\"_1st_dose_allocations\").alias(\"avg_1\"), \\\n       F.avg(\"_2nd_dose_allocations\").alias(\"avg_2\"), \\\n       F.sum(\"_1st_dose_allocations\").alias(\"sum_1\"), \\\n       F.sum(\"_2nd_dose_allocations\").alias(\"sum_2\")\n       ) \\\n  .sort(F.col(\"avg_1\").desc())\n```", "```py\n# at jurisdiction (state) level, calculate total number of deaths and total number of cases\ndf_cases = df_covid2 \\\n          .groupby(F.lower(\"state\").alias(\"state\")) \\\n          .agg(F.sum(\"deaths\").alias(\"sum_deaths\"), \\\n               F.sum(\"cases\").alias(\"sum_cases\"))\n```", "```py\n# creating an alias for each DataFrame\ndf_moderna = df_avg.alias(\"df_moderna\")\ndf_ny = df_cases.alias(\"df_ny\")\ndf_inner = df_moderna.join(df_ny, F.col(\"df_moderna.state\") == F.col(\"df_ny.state\"), 'inner')\n```", "```py\n# join results in all rows from the left table. Missing entries from the right table will result in \"null\"\ndf_left = df_moderna.join(df_ny, F.col(\"df_m.state\") == F.col(\"df_ny.state\"), 'left')\n```", "```py\nimport pyspark.sql.functions as F\n```", "```py\n# list of research_interests that are under same domain\nlst_ai  = [\"data_science\", \"artificial_intelligence\",\n           \"machine_learning\"]\n@F.udf\ndef is_ai(research):\n    \"\"\" return 1 if research in AI domain else 0\"\"\"\n    try:\n      # split the research interest with delimiter \"##\"  \n      lst_research = [w.lower() for w in str(research).split(\"##\")]\n      for res in lst_research:\n        # if present in AI domain\n        if res in lst_ai:\n          return 1\n      # not present in AI domain\n      return 0\n    except:\n      return -1\n# create a new column \"is_artificial_intelligence\"\ndf_gs_new = df_gs.withColumn(\"is_artificial_intelligence\",\\ is_ai(F.col(\"research_interest\")))\n```", "```py\nS3_output_path = \"s3a:\\\\my-bucket\\output\\vaccine_state_avg.csv\"\n# writing a DataFrame as a CSVfile\nsample_data_frame.\\\n        .coalesce(1) \\\n        .write \\\n        .mode(\"overwrite\") \\\n        .option(\"header\", True) \\\n        .option(\"quoteAll\",True) \\\n        .csv(s3_output_path)\n```", "```py\nS3_output_path = \"s3a:\\\\my-bucket\\output\\vaccine_state_avg.json\"\n# Writing a DataFrame as a json file\nsample_data_frame \\\n        .write \\\n        .json(s3_output_path)\n```", "```py\nfrom awsglue.utils import getResolvedOptions\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n```", "```py\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n# glue_job_google_scholar.py\n# spark context\nspark_context = SparkContext()\n# glue context\nglueContext = GlueContext(spark_context)\n# spark\nspark_session = glueContext.spark_session\n# job\njob = Job(glueContext)\n# initialize job\njob.init(args['JOB_NAME'], args)\n```", "```py\nfrom awsglue.dynamicframe import DynamicFrame\n```", "```py\n# glue context\ngoogle_authors = glueContext.create_dynamic_frame.from_catalog(\n           database=\"google_scholar\",\n           table_name=\"google_authors\")\n```", "```py\n# convert the glue DynamicFrame to Spark DataFrame\ngoogle_authors_df = google_authors.toDF()\n```", "```py\nfrom awsglue.transforms import *\n```", "```py\n# path for output file\npath_s3_write= \"s3://google-scholar-csv/write/\"\n# write to s3 as a CSV file with separator |\nglueContext.write_dynamic_frame.from_options(\n    frame = dynamic_frame_write,\n    connection_type = \"s3\",\n    connection_options = {\n            \"path\": path_s3_write\n                         },\n    format = \"csv\",\n    format_options={\n            \"quoteChar\": -1,\n            \"separator\": \"|\"\n                   })\njob.commit()\n```", "```py\n# create a DynamicFrame from a Spark DataFrame\ndynamic_frame = DynamicFrame.fromDF(df_sort, glueContext, \"dynamic_frame\")\n```", "```py\npip install https://github.com/aws-samples/sagemaker-run-notebook/releases/download/v0.20.0/sagemaker_run_notebook-0.20.0.tar.gz\n```", "```py\nrun-notebook schedule --at \"cron(0 8 * * * 2021)\" --name nightly notebook.ipynb\n```", "```py\nFrom sagemaker.processing import PySparkProcessor, ProcessingInput\n# ecr image URI\necr_image_uri = '664544806723.dkr.ecr.eu-central-1.amazonaws.com/linear-learner:latest'\n# create PySparkProcessor instance with initial job setup\nspark_processor = PySparkProcessor(\n    base_job_name=\"my-sparkjob\", # job name\n    framework_version=\"2.4\", # tensorflow version\n    py_version=\"py37\", # python version\n    container_version=\"1\", # container version\n    role=\"myiamrole\", # IAM role\n    instance_count=2, # ec2 instance count\n    instance_type=\"ml.c5.xlarge\", # ec2 instance type\n    max_runtime_in_seconds=1200, # maximum run time\n    image_uri=ecr_image_uri # ECR image\n)\n```", "```py\n# input s3 path\npath_input = \"s3://mybucket/input/\"\n# output s3 path\npath_output = \"s3://mybucket/output/\"\n# run method to execute job\nspark_processor.run(\n    submit_app=\"process.py\", # processing python script\n    arguments=['input', path_input, # input argument for script\n               'output', path_output # input argument for script\n              ])\n```", "```py\n# ecr image URI\necr_image_uri = '664544806723.dkr.ecr.eu-central-1.amazonaws.com/linear-learner:latest'\n# input data path\npath_data = '/opt/ml/processing/input_data'\n# output data path\npath_data = '/opt/ml/processing/processed_data'\n# s3 path for source\npath_source = 's3://mybucket/input'\n# s3 path for destination\npath_dest = 's3://mybucket/output'\n# create Processor instance\nprocessor = Processor(image_uri=ecr_image_uri, # ECR image\n               role='myiamrole', # IAM role\n               instance_count=1, # instance count\n               instance_type=\"ml.m5.xlarge\" # instance type\n           )\n# calling \"run\" method of Processor instance\nprocessor.run(inputs=[ProcessingInput(\n                 source=path_source, # input source\n                 destination=path_data # input destination)],\n              outputs=[ProcessingOutput(\n                 source=path_data, # output source\n                 destination=path_dest # output destination)], ))\n```"]