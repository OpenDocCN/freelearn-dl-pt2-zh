["```py\n    !pip install -qU torch_snippets\n    import os\n    %%writefile kaggle.json\n     {\"username\":\"XXX\", \"key\":\"XXX\"}\n     !mkdir -p ~/.kaggle\n     !cp kaggle.json ~/.kaggle/\n     !chmod 600 /root/.kaggle/kaggle.json\n     !kaggle datasets download -d sixhky/open-images-bus-trucks/\n     !unzip -qq open-images-bus-trucks.zip\n     !rm open-images-bus-trucks.zip \n    ```", "```py\n    from torch_snippets import *\n    from PIL import Image\n    IMAGE_ROOT = 'images/images'\n    DF_RAW = df = pd.read_csv('df.csv') \n    ```", "```py\n    label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n    label2target['background'] = 0\n    target2label = {t:l for l,t in label2target.items()}\n    background_class = label2target['background']\n    num_classes = len(label2target) \n    ```", "```py\n    def preprocess_image(img):\n        img = torch.tensor(img).permute(2,0,1)\n        return img.to(device).float() \n    ```", "```py\n    class OpenDataset(torch.utils.data.Dataset):\n        w, h = 224, 224\n        def __init__(self, df, image_dir=IMAGE_ROOT):\n            self.image_dir = image_dir\n            self.files = glob.glob(self.image_dir+'/*')\n            self.df = df\n            self.image_infos = df.ImageID.unique() \n    ```", "```py\n     def __getitem__(self, ix):\n            # load images and masks\n            image_id = self.image_infos[ix]\n            img_path = find(image_id, self.files)\n            img = Image.open(img_path).convert(\"RGB\")\n            img = np.array(img.resize((self.w, self.h),\n                                  resample=Image.BILINEAR))/255.\n            data = df[df['ImageID'] == image_id]\n            labels = data['LabelName'].values.tolist()\n            data = data[['XMin','YMin','XMax','YMax']].values\n            # Convert to absolute coordinates\n            data[:,[0,2]] *= self.w\n            data[:,[1,3]] *= self.h\n            boxes = data.astype(np.uint32).tolist()\n            # torch FRCNN expects ground truths as\n            # a dictionary of tensors\n            target = {}\n            target[\"boxes\"] = torch.Tensor(boxes).float()\n            target[\"labels\"]= torch.Tensor([label2target[i] \\\n                                    for i in labels]).long()\n            img = preprocess_image(img)\n            return img, target \n    ```", "```py\n     def collate_fn(self, batch):\n            return tuple(zip(*batch))\n        def __len__(self):\n            return len(self.image_infos) \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    trn_ids, val_ids = train_test_split(df.ImageID.unique(),\n                        test_size=0.1, random_state=99)\n    trn_df, val_df = df[df['ImageID'].isin(trn_ids)], \\\n                        df[df['ImageID'].isin(val_ids)]\n    train_ds = OpenDataset(trn_df)\n    test_ds = OpenDataset(val_df)\n    train_loader = DataLoader(train_ds, batch_size=4,\n                              collate_fn=train_ds.collate_fn,\n                                              drop_last=True)\n    test_loader = DataLoader(test_ds, batch_size=4,\n                               collate_fn=test_ds.collate_fn,\n                                              drop_last=True) \n    ```", "```py\n    import torchvision\n    from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    def get_model():\n        model = torchvision.models.detection\\\n                    .fasterrcnn_resnet50_fpn(pretrained=True)\n        in_features = model.roi_heads.box_predictor.cls_score.in_features\n        model.roi_heads.box_predictor = \\\n                     FastRCNNPredictor(in_features, num_classes)\n        return model \n    ```", "```py\n    # Defining training and validation functions\n    def train_batch(inputs, model, optimizer):\n        model.train()\n        input, targets = inputs\n        input = list(image.to(device) for image in input)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n        optimizer.zero_grad()\n        losses = model(input, targets)\n        loss = sum(loss for loss in losses.values())\n        loss.backward()\n        optimizer.step()\n        return loss, losses\n    @torch.no_grad()\n    def validate_batch(inputs, model):\n        model.train()\n    #to obtain losses, model needs to be in train mode only\n    #Note that here we aren't defining the model's forward #method\n    #hence need to work per the way the model class is defined\n        input, targets = inputs\n        input = list(image.to(device) for image in input)\n        targets = [{k: v.to(device) for k, v \\\n                    in t.items()} for t in targets]\n        optimizer.zero_grad()\n        losses = model(input, targets)\n        loss = sum(loss for loss in losses.values())\n        return loss, losses \n    ```", "```py\n    model = get_model().to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.005,\n                               momentum=0.9,weight_decay=0.0005)\n    n_epochs = 5\n    log = Report(n_epochs) \n    ```", "```py\n    for epoch in range(n_epochs):\n        _n = len(train_loader)\n        for ix, inputs in enumerate(train_loader):\n            loss, losses = train_batch(inputs, model, optimizer)\n            loc_loss, regr_loss, loss_objectness, \\\n                loss_rpn_box_reg = \\\n                    [losses[k] for k in ['loss_classifier', \\\n                    'loss_box_reg', 'loss_objectness', \\\n                    'loss_rpn_box_reg']]\n            pos = (epoch + (ix+1)/_n)\n            log.record(pos, trn_loss=loss.item(),\n                     trn_loc_loss=loc_loss.item(),\n                     trn_regr_loss=regr_loss.item(),\n                     trn_objectness_loss=loss_objectness.item(),\n                   trn_rpn_box_reg_loss=loss_rpn_box_reg.item(),\n                     end='\\r')\n        _n = len(test_loader)\n        for ix,inputs in enumerate(test_loader):\n            loss, losses = validate_batch(inputs, model)\n            loc_loss, regr_loss, loss_objectness, \\\n                loss_rpn_box_reg = \\\n                    [losses[k] for k in ['loss_classifier', \\\n                    'loss_box_reg', 'loss_objectness', \\\n                    'loss_rpn_box_reg']]\n            pos = (epoch + (ix+1)/_n)\n            log.record(pos, val_loss=loss.item(),\n                     val_loc_loss=loc_loss.item(),\n                     val_regr_loss=regr_loss.item(),\n                     val_objectness_loss=loss_objectness.item(),\n                     val_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n        if (epoch+1)%(n_epochs//5)==0: log.report_avgs(epoch+1) \n    ```", "```py\n    log.plot_epochs(['trn_loss','val_loss']) \n    ```", "```py\n    from torchvision.ops import nms\n    def decode_output(output):\n        'convert tensors to numpy arrays'\n        bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n        labels = np.array([target2label[i] for i in \\\n                    output['labels'].cpu().detach().numpy()])\n        confs = output['scores'].cpu().detach().numpy()\n        ixs = nms(torch.tensor(bbs.astype(np.float32)),\n                                torch.tensor(confs), 0.05)\n        bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n        if len(ixs) == 1:\n            bbs,confs,labels = [np.array([tensor]) for tensor \\\n                                    in [bbs, confs, labels]]\n        return bbs.tolist(), confs.tolist(), labels.tolist() \n    ```", "```py\n    model.eval()\n    for ix, (images, targets) in enumerate(test_loader):\n        if ix==3: break\n        images = [im for im in images]\n        outputs = model(images)\n        for ix, output in enumerate(outputs):\n            bbs, confs, labels = decode_output(output)\n            info = [f'{l}@{c:.2f}' for l,c in zip(labels,confs)]\n            show(images[ix].cpu().permute(1,2,0), bbs=bbs,\n                                             texts=labels, sz=5) \n    ```", "```py\n    !git clone https://github.com/AlexeyAB/darknet\n    %cd darknet \n    ```", "```py\n    !sed -i 's/OPENCV=0/OPENCV=1/' Makefile\n    # In case you dont have a GPU, make sure to comment out the\n    # below 3 lines\n    !sed -i 's/GPU=0/GPU=1/' Makefile\n    !sed -i 's/CUDNN=0/CUDNN=1/' Makefile\n    !sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile \n    ```", "```py\n    !make \n    ```", "```py\n    !pip install -q torch_snippets \n    ```", "```py\n    !wget --quiet \\\n     https://www.dropbox.com/s/agmzwk95v96ihic/open-images-bus-trucks.tar.xz\n    !tar -xf open-images-bus-trucks.tar.xz\n    !rm open-images-bus-trucks.tar.xz \n    ```", "```py\n    !wget --quiet\\ https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights \n    ```", "```py\n    !./darknet detector test cfg/coco.data cfg/yolov4.cfg\\ yolov4.weights\n     data/person.jpg \n    ```", "```py\n    %%writefile data/obj.names\n    bus\n    truck \n    ```", "```py\n    %%writefile data/obj.data\n    classes = 2\n    train = data/train.txt\n    valid = data/val.txt\n    names = data/obj.names\n    backup = backup/ \n    ```", "```py\n    !mkdir -p data/obj\n    !cp -r open-images-bus-trucks/images/* data/obj/\n    !cp -r open-images-bus-trucks/yolo_labels/all/{train,val}.txt data/\n    !cp -r open-images-bus-trucks/yolo_labels/all/labels/*.txt data/obj/ \n    ```", "```py\n1 0.62 0.50 0.25 0.12\n0 0.12 0.67 0.38 0.08 \n```", "```py\n# create a copy of existing configuration and modify it in place\n!cp cfg/yolov4-tiny-custom.cfg cfg/yolov4-tiny-bus-trucks.cfg\n# max_batches to 4000 (since the dataset is small enough)\n!sed -i 's/max_batches = 500200/max_batches=4000/' cfg/yolov4-tiny-bus-trucks.cfg\n# number of sub-batches per batch\n!sed -i 's/subdivisions=1/subdivisions=16/' cfg/yolov4-tiny-bus-trucks.cfg\n# number of batches after which learning rate is decayed\n!sed -i 's/steps=400000,450000/steps=3200,3600/' cfg/yolov4-tiny-bus-trucks.cfg\n# number of classes is 2 as opposed to 80\n# (which is the number of COCO classes)\n!sed -i 's/classes=80/classes=2/g' cfg/yolov4-tiny-bus-trucks.cfg\n# in the classification and regression heads,\n# change number of output convolution filters\n# from 255 -> 21 and 57 -> 33, since we have fewer classes\n# we don't need as many filters\n!sed -i 's/filters=255/filters=21/g' cfg/yolov4-tiny-bus-trucks.cfg\n!sed -i 's/filters=57/filters=33/g' cfg/yolov4-tiny-bus-trucks.cfg \n```", "```py\n!wget --quiet https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29\n!cp yolov4-tiny.conv.29 build/darknet/x64/ \n```", "```py\n!./darknet detector train data/obj.data \\\ncfg/yolov4-tiny-bus-trucks.cfg yolov4-tiny.conv.29 -dont_show -mapLastAt \n```", "```py\n!pip install torch_snippets\nfrom torch_snippets import Glob, stem, show, read\n# upload your own images to a folder\nimage_paths = Glob('images-of-trucks-and-busses')\nfor f in image_paths:\n    !./darknet detector test \\\n    data/obj.data cfg/yolov4-tiny-bus-trucks.cfg\\\n    backup/yolov4-tiny-bus-trucks_4000.weights {f}\n    !mv predictions.jpg {stem(f)}_pred.jpg\nfor i in Glob('*_pred.jpg'):\n    show(read(i, 1), sz=20) \n```", "```py\nclass SSD300(nn.Module):\n    ...\n    def __init__(self, n_classes, device):\n        ...\n        self.base = VGGBase()\n        self.aux_convs = AuxiliaryConvolutions()\n        self.pred_convs = PredictionConvolutions(n_classes)\n        ... \n```", "```py\n    import os\n    if not os.path.exists('open-images-bus-trucks'):\n        !pip install -q torch_snippets\n        !wget --quiet https://www.dropbox.com/s/agmzwk95v96ihic/\\\n        open-images-bus-trucks.tar.xz\n        !tar -xf open-images-bus-trucks.tar.xz\n        !rm open-images-bus-trucks.tar.xz\n        !git clone https://github.com/sizhky/ssd-utils/\n    %cd ssd-utils \n    ```", "```py\n    from torch_snippets import *\n    DATA_ROOT = '../open-images-bus-trucks/'\n    IMAGE_ROOT = f'{DATA_ROOT}/images'\n    DF_RAW = pd.read_csv(f'{DATA_ROOT}/df.csv')\n    df = DF_RAW.copy()\n    df = df[df['ImageID'].isin(df['ImageID'].unique().tolist())]\n    label2target = {l:t+1 for t,l in enumerate(DF_RAW['LabelName'].unique())}\n    label2target['background'] = 0\n    target2label = {t:l for l,t in label2target.items()}\n    background_class = label2target['background']\n    num_classes = len(label2target)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    import collections, os, torch\n    from PIL import Image\n    from torchvision import transforms\n    normalize = transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406],\n                    std=[0.229, 0.224, 0.225]\n                )\n    denormalize = transforms.Normalize( \n                    mean=[-0.485/0.229,-0.456/0.224,-0.406/0.255],\n                    std=[1/0.229, 1/0.224, 1/0.255]\n                )\n    def preprocess_image(img):\n        img = torch.tensor(img).permute(2,0,1)\n        img = normalize(img)\n        return img.to(device).float()\n    class OpenDataset(torch.utils.data.Dataset):\n        w, h = 300, 300\n        def __init__(self, df, image_dir=IMAGE_ROOT):\n            self.image_dir = image_dir\n            self.files = glob.glob(self.image_dir+'/*')\n            self.df = df\n            self.image_infos = df.ImageID.unique()\n            logger.info(f'{len(self)} items loaded')\n\n        def __getitem__(self, ix):\n            # load images and masks\n            image_id = self.image_infos[ix]\n            img_path = find(image_id, self.files)\n            img = Image.open(img_path).convert(\"RGB\")\n            img = np.array(img.resize((self.w, self.h),\n                           resample=Image.BILINEAR))/255.\n            data = df[df['ImageID'] == image_id]\n            labels = data['LabelName'].values.tolist()\n            data = data[['XMin','YMin','XMax','YMax']].values\n            data[:,[0,2]] *= self.w\n            data[:,[1,3]] *= self.h\n            boxes = data.astype(np.uint32).tolist() # convert to\n            # absolute coordinates\n            return img, boxes, labels\n        def collate_fn(self, batch):\n            images, boxes, labels = [], [], []\n            for item in batch:\n                img, image_boxes, image_labels = item\n                img = preprocess_image(img)[None]\n                images.append(img)\n                boxes.append(torch.tensor( \\                       \n                           image_boxes).float().to(device)/300.)\n                labels.append(torch.tensor([label2target[c] \\\n                      for c in image_labels]).long().to(device))\n            images = torch.cat(images).to(device)\n            return images, boxes, labels\n        def __len__(self):\n            return len(self.image_infos) \n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    trn_ids, val_ids = train_test_split(df.ImageID.unique(),\n                                 test_size=0.1, random_state=99)\n    trn_df, val_df = df[df['ImageID'].isin(trn_ids)], \\\n                    df[df['ImageID'].isin(val_ids)]\n    train_ds = OpenDataset(trn_df)\n    test_ds = OpenDataset(val_df)\n    train_loader = DataLoader(train_ds, batch_size=4,\n                              collate_fn=train_ds.collate_fn,\n                              drop_last=True)\n    test_loader = DataLoader(test_ds, batch_size=4,\n                             collate_fn=test_ds.collate_fn,\n                             drop_last=True) \n    ```", "```py\n    def train_batch(inputs, model, criterion, optimizer):\n        model.train()\n        N = len(train_loader)\n        images, boxes, labels = inputs\n        _regr, _clss = model(images)\n        loss = criterion(_regr, _clss, boxes, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        return loss\n    @torch.no_grad()\n    def validate_batch(inputs, model, criterion):\n        model.eval()\n        images, boxes, labels = inputs\n        _regr, _clss = model(images)\n        loss = criterion(_regr, _clss, boxes, labels)\n        return loss \n    ```", "```py\n    from model import SSD300, MultiBoxLoss\n    from detect import * \n    ```", "```py\n    n_epochs = 5\n    model = SSD300(num_classes, device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy, device=device)\n    log = Report(n_epochs=n_epochs)\n    logs_to_print = 5 \n    ```", "```py\n    for epoch in range(n_epochs):\n        _n = len(train_loader)\n        for ix, inputs in enumerate(train_loader):\n            loss = train_batch(inputs, model, criterion, optimizer)\n            pos = (epoch + (ix+1)/_n)\n            log.record(pos, trn_loss=loss.item(), end='\\r')\n        _n = len(test_loader)\n        for ix,inputs in enumerate(test_loader):\n            loss = validate_batch(inputs, model, criterion)\n            pos = (epoch + (ix+1)/_n)\n            log.record(pos, val_loss=loss.item(), end='\\r') \n    ```", "```py\n    image_paths = Glob(f'{DATA_ROOT}/images/*')\n    image_id = choose(test_ds.image_infos)\n    img_path = find(image_id, test_ds.files)\n    original_image = Image.open(img_path, mode='r')\n    original_image = original_image.convert('RGB') \n    ```", "```py\n    bbs, labels, scores = detect(original_image, model,\n                                 min_score=0.9, max_overlap=0.5,\n                                 top_k=200, device=device) \n    ```", "```py\n    labels = [target2label[c.item()] for c in labels]\n    label_with_conf = [f'{l} @ {s:.2f}' for l,s in zip(labels,scores)]\n    print(bbs, label_with_conf)\n    show(original_image, bbs=bbs,\n         texts=label_with_conf, text_sz=10) \n    ```"]