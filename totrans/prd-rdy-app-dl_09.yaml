- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Revealing the Secret of Deep Learning Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示深度学习模型的秘密
- en: So far, we have described how to construct and efficiently train a **deep learning**
    (**DL**) model. However, model training often involves multiple iterations because
    only rough guidance on how to configure the training correctly for a given task
    exists.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经描述了如何构建并高效训练**深度学习**（**DL**）模型。然而，模型训练通常涉及多次迭代，因为针对特定任务正确配置训练的粗略指导仅存。
- en: 'In this chapter, we will introduce hyperparameter tuning, the most standard
    process of finding the right training configuration. As we guide you through the
    steps of hyperparameter tuning, we will introduce popular search algorithms adopted
    for the tuning process (grid search, random search, and Bayesian optimization).
    We will also look into the field of Explainable AI, which is the process of understanding
    what models do during prediction. We will describe the three most common techniques
    in this domain: **Permutation Feature Importance** (**PFI**), **SHapley Additive
    exPlanations** (**SHAP**), **Local Interpretable Model-agnostic Explanations**
    (**LIME**).'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍超参数调优，这是寻找正确训练配置的最标准过程。随着我们指导您完成超参数调优的步骤，我们将介绍用于调优过程的流行搜索算法（网格搜索、随机搜索和贝叶斯优化）。我们还将探讨可解释
    AI 领域，即在预测过程中理解模型操作的过程。我们将描述这一领域中的三种最常见技术：**置换特征重要性**（**PFI**）、**SHapley 加法解释**（**SHAP**）、**局部可解释模型无关解释**（**LIME**）。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将涵盖以下主要主题：
- en: Obtaining the best performing model using hyperparameter tuning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用超参数调优获取最佳性能模型
- en: Understanding the behavior of the model with Explainable AI
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过可解释 AI 理解模型行为
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can download the supplemental material for this chapter from this book’s
    GitHub repository at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_7](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_7).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本书的 GitHub 存储库下载本章的补充材料，链接为 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_7](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_7)。
- en: Obtaining the best performing model using hyperparameter tuning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用超参数调优获取最佳性能模型
- en: As described in [*Chapter 3*](B18522_03.xhtml#_idTextAnchor062), *Developing
    a Powerful Deep Learning Model*, obtaining a DL model that extracts the right
    pattern for the underlying task requires multiple components to be configured
    appropriately. While building the right model architecture often introduces many
    difficulties, setting up the proper model training is another challenge that most
    people struggle with.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [*第 3 章*](B18522_03.xhtml#_idTextAnchor062) 中描述的，*开发强大的深度学习模型*，获取能够为底层任务提取正确模式的
    DL 模型需要适当配置多个组件。尽管构建合适的模型架构通常会引入许多困难，但设置适当的模型训练是大多数人都在努力解决的另一个挑战。
- en: In **machine learning** (**ML**), *a* **hyperparameter** *refers to any parameter
    that controls the learning process*. In many cases, data scientists often focus
    on model-relevant hyperparameters such as the number of a particular type of layer,
    learning rate, or type of optimizer. However, hyperparameters also include data-relevant
    configurations such as types of augmentation to apply and a sampling strategy
    for model training. The iterative process of changing a set of hyperparameters,
    and understanding performance changes, to find the right set of hyperparameters
    for the target task is called *hyperparameter tuning*. To be precise, you will
    have a set of hyperparameters that you want to explore. For each iteration, one
    or more hyperparameters will be configured differently and a new model will be
    trained with the adjusted setting. After the iterative process, the hyperparameter
    configuration used for the best model will be the final output.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在**机器学习**（**ML**）中，*超参数* *是指控制学习过程的任何参数*。在许多情况下，数据科学家通常关注与模型相关的超参数，例如特定类型层的数量、学习率或优化器类型。然而，超参数还包括数据相关配置，如应用的增强类型和模型训练的抽样策略。将一组超参数进行更改并理解性能变化的迭代过程，以找到目标任务的正确超参数集合，称为*超参数调优*。确切地说，您将有一组要探索的超参数。对于每次迭代，将会以不同的设置配置一个或多个超参数，并使用调整后的设置训练新模型。经过迭代过程，用于最佳模型的超参数配置将是最终输出。
- en: In this chapter, we will learn various techniques and tools available for hyperparameter
    tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习用于超参数调整的各种技术和工具。
- en: Hyperparameter tuning techniques
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整技术
- en: '*Techniques for hyperparameter tuning can differ by how the values for the
    target hyperparameters are selected*. Out of the various techniques, we will be
    focusing on the most common ones: **grid search**, **random search**, and **Bayesian
    optimization**.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*超参数调整技术可以通过选择目标超参数的值的方式而异*。在各种技术中，我们将重点介绍最常见的几种：**网格搜索**、**随机搜索**和**贝叶斯优化**。'
- en: Grid search
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: The most basic approach is called grid search, where *every possible value is
    evaluated one by one*. For example, if you want to explore a learning rate from
    0 to 1 with an increase of 0.25, then grid search will train the model for every
    possible learning rate (0.25, 0.5, 0.75, and 1) and select the learning rate that
    generates the best model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的方法被称为网格搜索，其中*每个可能的值都逐个评估*。例如，如果你想探索从0到1的学习率，增加幅度为0.25，那么网格搜索将会为每个可能的学习率（0.25、0.5、0.75和1）训练模型，并选择生成最佳模型的学习率。
- en: Random search
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索
- en: On the other hand, *random search generates a random value for the hyperparameter
    and repeats the training until the maximum number of experiments is reached*.
    If we convert the example in the previous section for random search, we must define
    the maximum number of experiments and a boundary for the learning rate. In this
    example, we will set the maximum number as 5 and the boundary as 0 to 1\. Then,
    random search will select a random value between 0 and 1 and train a model with
    the selected learning rate. This process will be repeated 5 times and the learning
    rate that generates the best model will be selected as the output of hyperparameter
    tuning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*随机搜索生成超参数的随机值，并重复训练，直到达到最大实验次数为止*。如果我们将前面章节的例子转换为随机搜索，我们必须定义最大实验次数和学习率的边界。在这个例子中，我们将最大实验次数设置为5，边界设置为0到1。然后，随机搜索将在0到1之间选择一个随机值，并使用所选的学习率训练模型。这个过程将重复5次，选择生成最佳模型的学习率作为超参数调整的输出。
- en: 'To help your understanding, the following diagram summarizes the difference
    between grid search and random search:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助理解，以下图表总结了网格搜索和随机搜索之间的区别：
- en: '![Figure 7.1 – The difference between grid search and random search'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1 – 网格搜索和随机搜索的差异'
- en: '](img/B18522_07_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_07_01.jpg)'
- en: Figure 7.1 – The difference between grid search and random search
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 网格搜索和随机搜索的差异
- en: In the preceding diagram, *x* and *y* indicate two different hyperparameters.
    The purple and green graphs on each axis represent the model performance changes
    concerning each hyperparameter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，*x*和*y*表示两个不同的超参数。每个轴上的紫色和绿色图表显示了模型性能随每个超参数变化的情况。
- en: 'While grid search and random search are easy to implement, they both have a
    common limitation: they do not guarantee the best value for the target hyperparameter.
    This issue mainly comes from the fact that the previous results are not considered
    when selecting the next value to explore. To overcome this issue, a new search
    algorithm was introduced: Bayesian optimization.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网格搜索和随机搜索都很容易实现，但它们都有一个共同的限制：它们不能保证目标超参数的最佳值。这个问题主要源于在选择下一个要探索的值时没有考虑前面的结果。为了克服这个问题，引入了一种新的搜索算法：贝叶斯优化。
- en: Bayesian optimization
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: 'The idea of Bayesian optimization is straightforward: *a surrogate model that
    maps the relationship between the hyperparameters and the underlying model is
    constructed and adjusted throughout the hyperparameter tuning so that we can select
    a hyperparameter value that will most likely lead us to have a better understanding
    about the relationship from the following experiment*. Using the generated surrogate
    model, we can select the hyperparameter value that will likely give us a better
    model.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯优化的理念很简单：*构建一个代理模型，映射超参数与底层模型之间的关系，并在整个超参数调整过程中进行调整，以便我们可以选择一个超参数值，这个超参数值很可能会在后续实验中使我们对关系有更好的理解*。利用生成的代理模型，我们可以选择很可能会给我们带来更好模型的超参数值。
- en: There are many ways to build a surrogate model. If we assume that the relationship
    can be represented as a linear function, the surrogate model generation process
    will simply be linear regression. In reality, the relationship is much more complex,
    and the most successful technique is to use Gaussian process regression. Here,
    we assume that the relationship can be represented by a set of normal distributions.
    In other words, each value we select is randomly selected from a multivariate
    normal distribution. We would need to introduce multiple probability and mathematical
    terms if we wanted to go over every detail of Bayesian optimization. We believe
    that the high-level description in this section and the complete example in the
    following section will be sufficient for you to apply hyperparameter tuning using
    Bayesian optimization. If you would like to understand the theory behind Bayesian
    optimization, please go to [https://ieeexplore.ieee.org/abstract/document/7352306](https://ieeexplore.ieee.org/abstract/document/7352306).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多构建代理模型的方法。如果我们假设关系可以表示为线性函数，那么代理模型生成过程将简单地是线性回归。实际上，关系要复杂得多，最成功的技术是使用高斯过程回归。在这里，我们假设关系可以用一组正态分布来表示。换句话说，我们选择的每个值都是从多元正态分布中随机选择的。如果我们想详细讨论贝叶斯优化的每个细节，我们将需要引入多个概率和数学术语。我们相信，本节的高层描述和下一节的完整示例足以让您应用使用贝叶斯优化进行超参数调整。如果您想了解贝叶斯优化背后的理论，请访问[https://ieeexplore.ieee.org/abstract/document/7352306](https://ieeexplore.ieee.org/abstract/document/7352306)。
- en: Hyperparameter tuning tools
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整工具
- en: 'As hyperparameter tuning plays an important role in ML projects, many libraries
    exist that are designed to simplify the process. The popular ones are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于超参数调整在ML项目中起着重要作用，因此存在许多旨在简化此过程的库。以下是一些流行的库：
- en: '**Scikit-Optimize**: [https://scikit-optimize.github.io](https://scikit-optimize.github.io)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scikit-Optimize**: [https://scikit-optimize.github.io](https://scikit-optimize.github.io)'
- en: '**Optuna**: [https://optuna.org](https://optuna.org)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Optuna**: [https://optuna.org](https://optuna.org)'
- en: '**HyperOpt**: [http://hyperopt.github.io](http://hyperopt.github.io)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HyperOpt**: [http://hyperopt.github.io](http://hyperopt.github.io)'
- en: '**Ray Tune**: [https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ray Tune**: [https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html)'
- en: '**Bayesian Optimization**: [https://github.com/fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bayesian Optimization**: [https://github.com/fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization)'
- en: '**Metric Optimization Engine** (**MOE**): [https://github.com/Yelp/MOE](https://github.com/Yelp/MOE)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Metric Optimization Engine** (**MOE**): [https://github.com/Yelp/MOE](https://github.com/Yelp/MOE)'
- en: '**Spearmint**: [https://github.com/HIPS/Spearmint](https://github.com/HIPS/Spearmint)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spearmint**: [https://github.com/HIPS/Spearmint](https://github.com/HIPS/Spearmint)'
- en: '**GPyOpt**: [https://github.com/SheffieldML/GPyOpt](https://github.com/SheffieldML/GPyOpt)'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPyOpt**: [https://github.com/SheffieldML/GPyOpt](https://github.com/SheffieldML/GPyOpt)'
- en: '**SigOpt**: [https://sigopt.com](https://sigopt.com/)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SigOpt**: [https://sigopt.com](https://sigopt.com/)'
- en: '**FLAML**: [https://github.com/microsoft/FLAML](https://github.com/microsoft/FLAML)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FLAML**: [https://github.com/microsoft/FLAML](https://github.com/microsoft/FLAML)'
- en: '**Dragonfly**: [https://github.com/dragonfly/dragonfly](https://github.com/dragonfly/dragonfly)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dragonfly**: [https://github.com/dragonfly/dragonfly](https://github.com/dragonfly/dragonfly)'
- en: '**HpBandSter**: [https://github.com/automl/HpBandSter](https://github.com/automl/HpBandSter)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HpBandSter**: [https://github.com/automl/HpBandSter](https://github.com/automl/HpBandSter)'
- en: '**Nevergrad**: [https://github.com/facebookresearch/nevergrad](https://github.com/facebookresearch/nevergrad)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nevergrad**: [https://github.com/facebookresearch/nevergrad](https://github.com/facebookresearch/nevergrad)'
- en: '**ZOOpt**: [https://github.com/polixir/ZOOpt](https://github.com/polixir/ZOOpt)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZOOpt**: [https://github.com/polixir/ZOOpt](https://github.com/polixir/ZOOpt)'
- en: '**HEBO**: [https://github.com/huawei-noah/HEBO/tree/master/HEBO](https://github.com/huawei-noah/HEBO/tree/master/HEBO)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HEBO**: [https://github.com/huawei-noah/HEBO/tree/master/HEBO](https://github.com/huawei-noah/HEBO/tree/master/HEBO)'
- en: '**SageMaker**: [https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker**: [https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)'
- en: Among the various tools, we will look at Ray Tune as we covered how to use Ray
    for distributed training in [*Chapter 6*](B18522_06.xhtml#_idTextAnchor133), *Efficient
    Model Training*, in the *Training a model using Ray* section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种工具中，我们将关注 Ray Tune，因为我们在《第 6 章》[*Chapter 6*](B18522_06.xhtml#_idTextAnchor133)
    *Efficient Model Training* 的 *Training a model using Ray* 部分中介绍了如何使用 Ray 进行分布式训练。
- en: Hyperparameter tuning using Ray Tune
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Ray Tune 进行超参数调整
- en: As part of Ray, a framework developed for scaling Python workloads across machines,
    Ray Tune is designed for experiment execution and hyperparameter tuning at scale.
    In this section, we will walk you through how to configure and schedule hyperparameter
    tuning using **Ray Tune**. Even though the examples are designed for an abstract
    representation of model training functionality, setups and documentation of Ray
    Tune are clear enough that **PyTorch** and **TensorFlow** (**TF**) integration
    will come naturally at the end of the section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Ray 的一部分，一个旨在跨多台机器扩展 Python 工作负载的框架，Ray Tune 专为大规模实验执行和超参数调整而设计。在本节中，我们将为您介绍如何使用
    **Ray Tune** 进行超参数调整的配置和调度。尽管示例旨在抽象表示模型训练功能，但 Ray Tune 的设置和文档足够清晰，以至于 **PyTorch**
    和 **TensorFlow**（**TF**）的集成自然而然地在本节末尾完成。
- en: 'First, we will look at the basics of Ray Tune. The core functionality of Ray
    Tune comes from the `tune.run` function, which manages all the experiments, logs,
    and checkpoints. The basic usage of the `tune.run` function is demonstrated in
    the following code snippet:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍 Ray Tune 的基础知识。Ray Tune 的核心功能来自 `tune.run` 函数，该函数管理所有实验、日志和检查点。`tune.run`
    函数的基本用法如下代码片段所示：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `tune.run` function takes in `run_or_experiment`, which defines the training
    logic, and `conf`, which configures the hyperparameter tuning. The number of experiments
    depends on the type of search function provided for each hyperparameter in `conf`.
    In the preceding example, we have `tune.grid_search([10, 20, 30, 40])`, which
    will spin up four experiments, each running the function provided for `run_or_experiment`
    (`tr_function`) with a distinct value of `num_iterations`. Within `tr_function`,
    we can access the assigned hyperparameter through the `conf` argument. It is worth
    mentioning that Ray Tune provides a vast number of sampling methods ([https://docs.ray.io/en/latest/tune/api_docs/search_space.html#tune-sample-docs](https://docs.ray.io/en/latest/tune/api_docs/search_space.html#tune-sample-docs)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`tune.run` 函数接受 `run_or_experiment` 参数，该参数定义训练逻辑，以及 `conf` 参数，该参数配置超参数调整。每个超参数在
    `conf` 中提供的搜索函数类型决定了实验的数量。在上述示例中，我们使用了 `tune.grid_search([10, 20, 30, 40])`，这将启动四个实验，每个实验都会运行为
    `num_iterations` 分配的函数 (`tr_function`)，并使用不同的 `num_iterations` 值。在 `tr_function`
    中，我们可以通过 `conf` 参数访问分配的超参数。值得一提的是，Ray Tune 提供了大量的采样方法 ([https://docs.ray.io/en/latest/tune/api_docs/search_space.html#tune-sample-docs](https://docs.ray.io/en/latest/tune/api_docs/search_space.html#tune-sample-docs))。'
- en: 'Ray Tune has integrated many open source optimization libraries as part of
    `tune.suggest` providing various state-of-the-art search algorithms for hyperparameter
    tuning. Popular ones include HyperOpt, Bayesian Optimization, Scitkit-Optimize,
    and Optuna. The complete list can be found at [https://docs.ray.io/en/latest/tune/api_docs/suggestion.html](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html).
    In the following example, we will describe how to use `BayesOptSearch`, which,
    as its name suggests, implements Bayesian optimization:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Ray Tune 作为 `tune.suggest` 的一部分整合了许多开源优化库，为超参数调整提供各种先进的搜索算法。流行的算法包括 HyperOpt、Bayesian
    Optimization、Scitkit-Optimize 和 Optuna。完整列表可以在 [https://docs.ray.io/en/latest/tune/api_docs/suggestion.html](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html)
    找到。在下面的示例中，我们将描述如何使用 `BayesOptSearch`，正如其名称所示，它实现了贝叶斯优化：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code snippet, we provided an instance of `BayesOptSearch` to
    the `search_alg` parameter. This example will try to find `num_iterations`, which
    will provide us a model with the highest `mean_accuracy`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，我们向 `search_alg` 参数提供了 `BayesOptSearch` 的一个实例。该示例将尝试找到 `num_iterations`，以提供具有最高
    `mean_accuracy` 的模型。
- en: 'Another key parameter of `tune.run` is `stop`. This parameter can take in a
    dictionary, a function, or a `Stopper` object that defines a stopping criterion.
    If it is a dictionary, keys must be one of the fields in the returned results
    of the `run_or_experiment` function. If it’s a function, it should return a Boolean
    that becomes `True` once the stopping criteria are met. These two cases are described
    in the following examples:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个`tune.run`的关键参数是`stop`。该参数可以接受一个字典、一个函数或一个`Stopper`对象，用于定义停止标准。如果是一个字典，键必须是`run_or_experiment`函数返回结果中的字段之一。如果是一个函数，它应返回一个布尔值，一旦满足停止条件就为`True`。以下是这两种情况的示例：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the dictionary-based example, each trial will stop if it completes 10 iterations
    or `mean_accuracy` reaches the specified value, `0.96`. The function-based example
    implements the same logic but uses the `stp_function` function. For a `stopper`
    class use case, you can refer to [https://docs.ray.io/en/latest/tune/tutorials/tune-stopping.html#stopping-with-a-class](https://docs.ray.io/en/latest/tune/tutorials/tune-stopping.html#stopping-with-a-class).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于字典的示例中，每个试验将在完成10次迭代或`mean_accuracy`达到指定值`0.96`时停止。基于函数的示例实现了相同的逻辑，但使用了`stp_function`函数。对于`stopper`类用例，您可以参考[https://docs.ray.io/en/latest/tune/tutorials/tune-stopping.html#stopping-with-a-class](https://docs.ray.io/en/latest/tune/tutorials/tune-stopping.html#stopping-with-a-class)。
- en: '*A trial is an internal data structure of Ray Tune that contains metadata about
    each experiment* ([https://docs.ray.io/en/latest/tune/api_docs/internals.html#trial-objects](https://docs.ray.io/en/latest/tune/api_docs/internals.html#trial-objects)).
    Each trial gets a unique ID (`trial.trial_id`) and its hyperparameter settings
    can be checked through `trial.config`. Interestingly, different scales of machine
    resources can be allocated for each trial through the `resources_per_trial` parameter
    of `tune.run` and `trial.placement_group_factory`. Additionally, the `num_samples`
    parameter can be used to control the number of trials.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*试验是Ray Tune的内部数据结构，包含关于每个实验的元数据* ([https://docs.ray.io/en/latest/tune/api_docs/internals.html#trial-objects](https://docs.ray.io/en/latest/tune/api_docs/internals.html#trial-objects))。每个试验都有一个唯一的ID（`trial.trial_id`），其超参数设置可以通过`trial.config`进行检查。有趣的是，可以通过`tune.run`的`resources_per_trial`参数和`trial.placement_group_factory`为每个试验分配不同规模的机器资源。此外，`num_samples`参数可用于控制试验的数量。'
- en: 'The summary of your experiments can be obtained using the `Analysis` instance
    returned from `ray.tune`. The following code snippet describes a set of information
    you can retrieve from an `Analysis` instance:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ray.tune`返回的`Analysis`实例可以获得您实验的摘要。以下代码片段描述了您可以从`Analysis`实例中检索的一组信息：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can also retrieve other useful information from an `Analysis` instance.
    The complete details can be found at [https://docs.ray.io/en/latest/tune/api_docs/analysis.html](https://docs.ray.io/en/latest/tune/api_docs/analysis.html).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从`Analysis`实例中检索其他有用的信息。完整详情请参阅[https://docs.ray.io/en/latest/tune/api_docs/analysis.html](https://docs.ray.io/en/latest/tune/api_docs/analysis.html)。
- en: This completes the core components of Ray Tune. If you want to integrate Ray
    Tune for your PyTorch or TF model training, all you must do is adjust `tr_function`
    in the examples so that it trains your model as it logs relevant performance metrics.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了Ray Tune的核心组件。如果您希望为PyTorch或TF模型训练集成Ray Tune，您所需做的就是调整示例中的`tr_function`，使其在记录相关性能指标的同时训练您的模型。
- en: Overall, we have explored different options for hyperparameter tuning. The tools
    we have covered in this section should help us efficiently find the best configuration
    for our DL model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们已探讨了超参数调整的不同选项。本节介绍的工具应有助于我们有效地找到DL模型的最佳配置。
- en: Things to remember
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. Obtaining a working DL model for a particular task requires finding the right
    model architecture and using appropriate training configurations. The process
    of finding the best combination is called hyperparameter tuning.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: a. 获得特定任务的工作DL模型需要找到合适的模型架构并使用适当的训练配置。找到最佳组合的过程称为超参数调整。
- en: b. The three most popular hyperparameter tuning techniques are grid search,
    random search, and Bayesian optimization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: b. 三种最流行的超参数调整技术是网格搜索、随机搜索和贝叶斯优化。
- en: c. Popular hyperparameter tuning tools include Scikit-Optimize, Optuna, Hyperopt,
    Ray Tune, Bayesian Optimization, MOE, Spearmint, GpyOpt, and SigOpt.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: c. 流行的超参数调整工具包括Scikit-Optimize、Optuna、Hyperopt、Ray Tune、Bayesian Optimization、MOE、Spearmint、GpyOpt和SigOpt。
- en: So far, we have treated DL models as black boxes. Hyperparameter tuning involves
    searching an unknown space that does not explain how the model finds the underlying
    pattern. In the next section, we will look at what researchers have recently worked
    on to understand the flexibility of DL.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将DL模型视为黑匣子。超参数调优涉及搜索未知空间，无法解释模型如何找到潜在模式。在下一节中，我们将探讨研究人员最近致力于理解DL灵活性的工作。
- en: Understanding the behavior of the model with Explainable AI
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过可解释人工智能理解模型的行为
- en: '**Explainable AI** is a very active area of research. In business settings,
    understanding AI models can easily lead to a distinctive competitive advantage.
    The so-called *black-box models (complex algorithmic models)*, even though they
    bring exceptional results, are commonly criticized due to their hidden logic.
    It is hard for higher-level management to fully design the core business based
    on AI, as interpreting the model and predictions is not an easy task. How can
    you convince your business partners that an AI model will always deliver the expected
    results? How can you ensure that the model will still work on new data? How does
    the model generate the results? Explainable AI helps us address these questions.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释人工智能**是一个非常活跃的研究领域。在商业环境中，理解AI模型往往能够轻松带来显著的竞争优势。所谓的*黑匣子模型（复杂算法模型）*，尽管能够带来出色的结果，却因其隐含逻辑而常遭批评。高层管理人员很难完全基于AI设计核心业务，因为解释模型和预测并不是一件容易的事情。您如何说服您的业务合作伙伴，表明AI模型将始终产生预期结果？您如何确保模型在新数据上仍然有效？模型是如何生成结果的？可解释人工智能帮助我们解答这些问题。'
- en: 'Before we go any further, let’s look at two important concepts: **interpretability**
    and **explainability**. At first, they might sound similar. Interpretability tells
    us why a specific input produces the specific model’s output: the effects of specific
    variables on the result. Explainability goes beyond interpretability; it focuses
    not only on causality between inputs and outputs but helps us understand how a
    model works as a whole, including all its sub-elements. Explainability is also
    driven by three fundamental ideas: transparency, reproducibility, and transferability.
    This means that we should be able to fully understand what our models do, how
    data affects the model as it passes through, and be able to reproduce the results.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探讨之前，让我们先了解两个重要的概念：**可解释性**和**解释性**。乍一听可能很相似。可解释性告诉我们为什么特定的输入会产生特定模型的输出：特定变量对结果的影响。解释性超越了可解释性；它不仅关注输入和输出之间的因果关系，还帮助我们理解模型作为一个整体的工作方式，包括其所有子元素。解释性还由透明性、可复现性和可转移性三个基本理念驱动。这意味着我们应该能够完全理解我们的模型所做的事情，数据如何在经过模型时影响模型，并能够重现结果。
- en: Explainable AI plays a role in every step of an ML project – development (an
    explanation of model architecture and meaning of each hyperparameter), training
    (changes within the model throughout training), as well as inference (results
    interpretation). In the case of DL models, it is hard to achieve explainability
    due to the complexity of the network architecture, high algorithmic complexity,
    and use of random numbers while initializing weights, biases, regularization,
    and hyperparameters optimization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释人工智能在每个ML项目的各个步骤中发挥作用——开发（模型架构的解释和每个超参数的含义）、训练（训练过程中模型的变化）、以及推理（结果解释）。在DL模型的情况下，由于网络架构复杂、算法复杂性高，以及在初始化权重、偏置、正则化和超参数优化时使用随机数，因此实现可解释性是困难的。
- en: 'In this section, we will discuss a few methods that are commonly used to build
    additional trustworthiness behind DL models: **Permutation Feature Importance**
    (**PFI**), **Feature Importance** (**FI**), **SHapley Additive exPlanations**
    (**SHAP**), and **Local Interpretable Model-agnostic Explanations** (**LIME**).
    All these methods are model agnostic; they can be applied to DL models as well
    as other supporting ML models commonly used to set baseline evaluation metrics.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论几种常用于增强DL模型可信度的方法：**置换特征重要性**（**PFI**）、**特征重要性**（**FI**）、**SHapley加性解释**（**SHAP**）和**局部可解释模型无关解释**（**LIME**）。所有这些方法都是模型无关的；它们既可应用于DL模型，也可应用于其他支持ML模型，常用于设定基线评估指标。
- en: Permutation Feature Importance
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 置换特征重要性
- en: 'Neural networks lack intrinsic attributes needed to understand the impact of
    input features on the predictions (the model’s output). However, there is a model
    agnostic approach called **Permutation Feature Importance** (**PFI**) designed
    for this difficulty. *The idea of PFI comes from the relationship between input
    features and outputs: for an input feature that has a high correlation with an
    output variable, changing its value will increase the model’s prediction error*.
    If the relationship is weak, the model performance won’t be affected as much.
    If the relationship is strong, the performance will be degraded. PFI is often
    applied to test sets to obtain a broader understanding of the model’s interpretability
    on unseen data.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络缺乏理解输入特征对预测（模型输出）影响所需的内在属性。然而，有一种称为**置换特征重要性**（**PFI**）的模型无关方法可以解决这一困难。*PFI的思想来自于输入特征与输出之间的关系：对于与输出变量高度相关的输入特征，改变其值会增加模型的预测误差。*如果关系较弱，模型的性能不会受到太多影响。如果关系很强，性能将下降。PFI经常应用于测试集，以获取对未见数据上模型可解释性的更广泛理解。
- en: The key disadvantage of PFI relates to the fact that it will not work correctly
    when data has a group of correlated input features. In this case, even though
    you change one feature from the group, the model performance does not change much
    because other features will remain unchanged.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PFI的主要缺点与数据具有一组相关输入特征相关。在这种情况下，即使您改变该组的一个特征，模型的性能也不会改变太多，因为其他特征将保持不变。
- en: Going further with the idea, we can completely remove that feature and measure
    the model performance. This approach is called **Feature Importance** (**FI**),
    also known as **Permutation Importance** (**PI**) or **Mean Decrease Accuracy**
    (**MDA**). Let’s have a look at how we can implement FI for any black-box model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 深入探讨这个想法，我们可以完全删除该特征并测量模型的性能。这种方法称为**特征重要性**（**FI**），也称为**置换重要性**（**PI**）或**平均减少精度**（**MDA**）。让我们看看如何为任何黑盒模型实现FI。
- en: Feature Importance
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'In this section, we will use the *ELI5* Python package ([https://eli5.readthedocs.io](https://eli5.readthedocs.io))
    to perform FI analysis. It stands out in the field of FI because it’s very simple
    to use. Let’s look at a minimal code example for TF with a Keras-defined model
    (see [*Chapter 3*](B18522_03.xhtml#_idTextAnchor062), *Developing a Powerful Deep
    Learning Model*, for details on model definition):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用*ELI5* Python包（[https://eli5.readthedocs.io](https://eli5.readthedocs.io)）执行FI分析。它在FI领域突出的原因是非常简单易用。让我们看一下在Keras定义的TF模型中使用ELI5的最小代码示例（有关模型定义的详细信息，请参见[*第3章*](B18522_03.xhtml#_idTextAnchor062)，*开发一个强大的深度学习模型*）：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see, the code is almost self-explanatory. First, we need to create
    a wrapper for the score function that calculates the target evaluation metric.
    Then, the `tf.keras` model gets passed to the constructor of the `PermutationImportance`
    class. The `fit` function, which takes in features and labels, handles the FI
    calculation. After this calculation, we can access the mean FI for each feature
    (`fi_perm`) and the standard deviation of the permuted results (`fi_std`). The
    following code snippet shows how to visualize the results of permutation importance
    as a bar graph:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，代码几乎是自解释的。首先，我们需要为计算目标评估指标的评分函数创建一个包装器。然后，将`tf.keras`模型传递给`PermutationImportance`类的构造函数。`fit`函数负责处理特征和标签的特征重要性计算。计算完成后，我们可以访问每个特征的平均特征重要性（`fi_perm`）和置换结果的标准差（`fi_std`）。以下代码片段展示了如何将置换重要性的结果可视化为条形图：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the model is neither based on scikit-learn nor Keras, you need to use the
    `permutation_importance.get_score_importance` function. The following code snippet
    describes how to use this function with a PyTorch model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型既不基于scikit-learn也不基于Keras，则需要使用`permutation_importance.get_score_importance`函数。以下代码片段描述了如何在PyTorch模型中使用该函数：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Unlike the `PermutationImportance` class, the `get_score_importances` function
    takes in a scoring function, features, and labels all at the same time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与`PermutationImportance`类不同，`get_score_importances`函数同时接收评分函数、特征和标签。
- en: Next, we will have a look at **SHapley Additive exPlanations** (**SHAP**), which
    is also a model-agnostic approach.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看**SHapley Additive exPlanations**（**SHAP**），这也是一种与模型无关的方法。
- en: SHapley Additive exPlanations (SHAP)
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHapley Additive exPlanations（SHAP）
- en: 'SHAP is an interpretation method that leverages Shapley values to understand
    the given black-box model. We won’t cover the cooperative game theory that SHAP
    is based on but we will cover the process at a high level. First, let’s look at
    the definition of Shapley values: *the average of marginal contributions among
    all possible coalitions over different simulations*. What exactly does this mean?
    Let’s say that a group of four friends (*f1*, *f2*, *f3*, and *f4*) is working
    to get the highest score together for an online game. To calculate the Shapley
    value for a person, we need to calculate the marginal contribution, which is the
    difference in score when the person is playing versus not playing. This calculation
    must be done for all possible subgroups (**coalitions**).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP是一种解释方法，利用夏普利值来理解给定黑盒模型。我们不会涵盖SHAP基于的合作博弈理论，但我们将以高层次来讨论过程。首先，让我们看看Shapley值的定义：*在不同模拟中，所有可能联盟的平均边际贡献*。这究竟意味着什么？假设有四位朋友(*f1*、*f2*、*f3*和*f4*)共同努力为一个在线游戏获得最高分数。要计算一个人的夏普利值，我们需要计算边际贡献，即该人参与游戏与不参与游戏时分数的差异。这个计算必须针对所有可能的子组（**联盟**）进行。
- en: 'Let’s take a closer look. To calculate the marginal contribution of *f1* for
    the coalition of friends *f2*, *f3*, and *f4*, we need to do the following :'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看一看。为了计算*f1*对朋友*f2*、*f3*和*f4*联合体的边际贡献，我们需要执行以下操作：
- en: Calculate the score (*s1*) generated by all friends (*f1*, *f2*, *f3*, and *f4*).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有朋友(*f1*、*f2*、*f3*和*f4*)生成的分数(*s1*)。
- en: Calculate the score (*s2*) generated by friends *f2*, *f3*, and *f4*.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算朋友*f2*、*f3*和*f4*生成的分数(*s2*)。
- en: Finally, the marginal contribution of friend *f1* for the coalition of friends
    *f2*, *f3*, and *f4* *(v)* equals *s1-s2*.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，朋友*f1*对朋友*f2*、*f3*和*f4*联合体的边际贡献*(v)*等于*s1-s2*。
- en: 'Now, we need to calculate marginal contributions for *all subgroups* (not only
    for a coalition of friends; that is, *f2*, *f3*, and *f4*). Here is every possible
    combination:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算所有子组合的边际贡献（不仅仅是朋友的联合体；即*f2*、*f3*和*f4*）。这里是每一个可能的组合：
- en: '*f1* versus *no one* is contributing (*v1*)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*与*无人*正在贡献(*v1*)'
- en: '*f1* and *f2* versus *f2* (*v2*)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f2*与*f2*(*v2*)'
- en: '*f1* and *f3* versus *f3* (*v3*)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f3*与*f3*(*v3*)'
- en: '*f1* and *f4* versus *f4* (*v4*)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f4*与*f4*(*v4*)'
- en: '*f1* and *f2* and *f3* versus *f2* and *f3* (*v5*)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f2*和*f3*与*f2*和*f3*(*v5*)'
- en: '*f1* and *f2* and *f4* versus *f2* and *f4* (*v6*)'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f2*和*f4*与*f2*和*f4*(*v6*)'
- en: '*f1* and *f3* and *f4* versus *f3* and *f4* (*v7*)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f3*和*f4*与*f3*和*f4*(*v7*)'
- en: '*f1* and *f2* and *f3* and *f4* versus *f2* and *f3* and *f4* (*v8*)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*f1*和*f2*和*f3*和*f4*与*f2*和*f3*和*f4*(*v8*)'
- en: Overall, the Shapley value (*SV*) for *f1* is *(v1+v2+...+v8) / 8*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，*f1*的夏普利值(*SV*)为*(v1+v2+...+v8) / 8*。
- en: For have our results to be statistically sound, we need to run these calculations
    over multiple simulations. You can see that if we extend the number of friends,
    the calculations get extremely complex, resulting in high consumption of computational
    resources. Therefore, specific approximations are used, resulting in different
    types of so-called explainers (approximators of Shapley values) in the `shap`
    library ([https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html)).
    Comparing Shapley's values for all friends, we can find the individual’s contribution
    to the final score.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的结果具有统计学意义，我们需要在多次模拟中运行这些计算。您可以看到，如果我们扩展朋友的数量，计算将变得非常复杂，导致计算资源的高消耗。因此，我们使用具体的近似值，产生了不同类型的所谓解释器（夏普利值的近似器），这些解释器可以在`shap`库中找到([https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html))。通过比较所有朋友的夏普利值，我们可以找出每个个体对最终分数的贡献。
- en: 'If we go back to the explanation of DL models, we can see that the friends
    become a set of features and that the score is the model performance. With this
    in mind, let’s have a look at SHAP explainers, which can be used for DL models:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到DL模型的解释，我们可以看到朋友们变成了一组特征，而分数则是模型的性能。有了这个理念，让我们来看看SHAP解释器，它可以用于DL模型：
- en: '`KernelExplainer`: This is the most popular method and is model agnostic. It’s
    based on **Local Interpretable Model-agnostic Explanations** (**LIME**), which
    we will discuss in the next section.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KernelExplainer`：这是最流行的方法，也是与模型无关的。它基于**局部可解释模型无关解释**（**LIME**），我们将在下一节讨论。'
- en: '`DeepExplainer`: This method is based on the DeepList approach, which decomposes
    the output on a specific input ([https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DeepExplainer`：这种方法基于 DeepList 方法，它在特定输入上分解输出（[https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)）。'
- en: '`GradientExplainer`: This method is based on the extension of integrated gradients
    ([https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GradientExplainer`：这种方法基于集成梯度的扩展（[https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)）。'
- en: 'For example, we will present a minimalistic code example where SHAP is applied
    to a TF model. The complete details can be found in the official documentation
    at [https://shap-lrjball.readthedocs.io/en/latest/index.html](https://shap-lrjball.readthedocs.io/en/latest/index.html):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将展示一个应用于 TF 模型的极简代码示例。完整详情请参阅官方文档 [https://shap-lrjball.readthedocs.io/en/latest/index.html](https://shap-lrjball.readthedocs.io/en/latest/index.html)：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For PyTorch models, you will need to wrap your model in a wrapper to convert
    the input and output into the correct types (`f=lambda x: model(torch.autograd.Variable(torch.from_numpy(x))).detach().numpy()`).
    In the proceeding example, we have defined `KernelExplainer`, which takes in a
    DL model and `sampled_data` as inputs. Next, we calculate the SHAP values (approximations
    of Shapley values) using the `explainer.shap_values` function. In this example,
    we are using `300` perturbation samples to estimate the SHAP values for the given
    prediction. If our `sampled_data` contains `100` examples, we will be performing
    100*300 model evaluations. Similarly, you can use `GradientExplainer` (`shap.GradientExplainer(model,
    sampled_data)`) or `DeepExplainer` (`shap.DeepExplainer(model, sampled_data)`).
    The size of `sampled_data` needs to be big enough to represent the distribution
    correctly. In the last few lines, we visualize the SHAP values in an additive
    force layout using the `shap.force_plot` function and create a global model interpretation
    plot using the `shap.summary_plot` function.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 PyTorch 模型，您需要将模型包装在一个包装器中，以将输入和输出转换为正确的类型（`f=lambda x: model(torch.autograd.Variable(torch.from_numpy(x))).detach().numpy()`）。在下面的示例中，我们定义了
    `KernelExplainer`，它接受 DL 模型和 `sampled_data` 作为输入。接下来，我们使用 `explainer.shap_values`
    函数计算 SHAP 值（Shapley 值的近似值）。在本例中，我们使用 `300` 个扰动样本来估计给定预测的 SHAP 值。如果我们的 `sampled_data`
    包含 `100` 个示例，则将执行 100*300 次模型评估。类似地，您可以使用 `GradientExplainer`（`shap.GradientExplainer(model,
    sampled_data)`）或 `DeepExplainer`（`shap.DeepExplainer(model, sampled_data)`）。`sampled_data`
    的大小需要足够大，以正确表示分布。在最后几行中，我们使用 `shap.force_plot` 函数在加性力布局中可视化 SHAP 值，并使用 `shap.summary_plot`
    函数创建全局模型解释图。'
- en: Now, let’s look at the LIME approach.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 LIME 方法。
- en: Local Interpretable Model-agnostic Explanations (LIME)
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地可解释的模型无关解释（LIME）
- en: LIME is a method that trains a local surrogate model to explain the model predictions.
    First, you need to prepare a model you want to interpret and a sample. LIME uses
    your model to collect predictions from a set of perturbed data and compare them
    against the original sample to assign similarity weights (higher if predictions
    are closer to the prediction on the initial sample). LIME fits an intrinsically
    interpretable surrogate model on the sampled data using a specific number of features
    weighted by the similarity weights. Finally, LIME treats the surrogate model interpretation
    as an interpretation of the black-box model for your selected example. To perform
    LIME analysis, we can use the `lime` package ([https://lime-ml.readthedocs.io](https://lime-ml.readthedocs.io)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 是一种训练本地替代模型以解释模型预测的方法。首先，您需要准备一个要解释的模型和一个样本。LIME 使用您的模型从一组扰动数据中收集预测，并将它们与原始样本进行比较，以分配相似性权重（如果预测接近初始样本的预测，则权重较高）。LIME
    使用特定数量的特征通过相似性权重在采样数据上拟合一个内在可解释的替代模型。最后，LIME 将替代模型解释视为所选示例的黑盒模型的解释。要执行 LIME 分析，我们可以使用
    `lime` 包（[https://lime-ml.readthedocs.io](https://lime-ml.readthedocs.io)）。
- en: 'Let’s have a look at an example designed for a DL model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个为 DL 模型设计的示例：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding example, we are using the `LimeTabularExplainer` class. The
    constructor takes in a train set, feature, class names, and a mode type (`'classification'`).
    Similarly, you can set LIME for regression problems by providing the `'regression'`
    mode type. Then, by showing the five most important features and their influences,
    we explain the first prediction from the test set (`x[0]`). Lastly, we generate
    a plot from the computed LIME explanation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们正在使用 `LimeTabularExplainer` 类。构造函数接受一个训练集、特征、类名和模式类型 (`'classification'`)。同样，您可以通过提供
    `'regression'` 模式类型来设置用于回归问题的 LIME。然后，通过展示五个最重要的特征及其影响，我们解释了测试集的第一个预测 (`x[0]`)。最后，我们生成了一个基于计算的
    LIME 解释的图表。
- en: Things to remember
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事项
- en: a. Model interpretability and explainability are the two key concepts in Explainable
    AI.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: a. 在解释性人工智能中，模型可解释性和解释性是两个关键概念。
- en: b. Popular model-agnostic techniques in Explainable AI are PFI, FI, SHAP, and
    LIME.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: b. 解释性人工智能中流行的无关模型技术包括 PFI、FI、SHAP 和 LIME。
- en: c. PFI, FI, and SHAP are methods that allow you to interpret your model at both
    local (a single sample) and global (a set of samples) levels. On the other hand,
    LIME focuses on a single sample and the corresponding model prediction.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: c. PFI、FI 和 SHAP 是允许您在本地（单个样本）和全局（一组样本）级别解释您的模型的方法。另一方面，LIME 关注单个样本及其对应的模型预测。
- en: 'In this section, we have explained the idea of Explainable AI and the four
    most common techniques: PFI, FI, SHAP, and LIME.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经解释了解释性人工智能的概念以及四种最常见的技术：PFI、FI、SHAP 和 LIME。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter with hyperparameter tuning. We described the three basic
    search algorithms that are used for hyperparameter tuning (grid search, random
    search, and Bayesian optimization) and introduced many tools you can integrate
    into your project. Out of the tools we listed, we covered Ray Tune as it supports
    distributed hyperparameter tuning and implements many of the state-of-the-art
    search algorithms out of the box.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从调整超参数开始了本章。我们描述了用于超参数调整的三种基本搜索算法（网格搜索、随机搜索和贝叶斯优化），并介绍了许多可集成到项目中的工具。在列出的工具中，我们涵盖了
    Ray Tune，因为它支持分布式超参数调整，并实现了许多即用的最先进搜索算法。
- en: Then, we discussed Explainable AI. We explained the most standard techniques
    (PFI, FI, SHAP, and LIME) and how they can be used to find out how a model's behavior
    changes with respect to each feature in a dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了解释性人工智能。我们解释了最标准的技术（PFI、FI、SHAP 和 LIME），以及它们如何用来发现模型在数据集中每个特征变化时的行为变化。
- en: In the next chapter, we will shift our focus toward deployment. We will learn
    about ONNX, an open format for ML models, and look at how to convert a TF or PyTorch
    model into an ONNX model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把焦点转向部署。我们将学习 ONNX，这是一种用于机器学习模型的开放格式，并了解如何将 TF 或 PyTorch 模型转换为 ONNX
    模型。
