["```py\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.12/nvidia-device-plugin.yml\n```", "```py\nkubectl create namespace tf-inference\n```", "```py\nkind: Deployment\n  apiVersion: apps/v1\n  metadata:\n    name: tf-inference # name for the endpoint / deployment\n    labels:\n      app: demo\n      role: master\n  spec:\n    replicas: 1 # number of pods in the cluster\n    selector:\n      matchLabels:\n        app: demo\n        role: master\n```", "```py\n    template:      \n      metadata:\n        labels:\n          app: demo\n          role: master\n      spec:\n        containers:\n        - name: demo\n          image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-gpu-py36-cu100-ubuntu18.04 # ECR image for TensorFlow inference\n          command:\n          - /usr/bin/tensorflow_model_server # start inference endpoint\n          args: # arguments for the inference serving\n          - --port=9000\n          - --rest_api_port=8500\n          - --model_name=saved_model\n          - --model_base_path=s3://mybucket/models\n          ports:\n          - name: http\n            containerPort: 8500 # HTTP port\n          - name: gRPC\n            containerPort: 9000 # gRPC port\n```", "```py\nkind: Service\n  apiVersion: v1\n  metadata:\n    name: tf-inference # name for the service\n    labels:\n      app: demo\n  spec:\n    Ports:\n    - name: http-tf-serving\n      port: 8500 # HTTP port for the webserver inside the pods\n      targetPort: 8500 # HTTP port for access outside the pods\n    - name: grpc-tf-serving\n      port: 9000 # gRPC port for the webserver inside the pods\n      targetPort: 9000 # gRPC port for access outside the pods\n    selector:\n      app: demo\n      role: master\n    type: ClusterIP\n```", "```py\npip install torchserve torch-model-archiver\n```", "```py\ntorch-model-archiver --model-name archived_model --version 1.0 --serialized-file model.pth --handler run_inference\n```", "```py\nmxnet-model-server --start --mms-config /home/model-server/config.properties --models archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar\n```", "```py\n       containers:\n       - name: pytorch-service\n         image: \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.3.1-gpu-py36-cu101-ubuntu16.04\"\n         args:\n         - mxnet-model-server\n         - --start\n         - --mms-config /home/model-server/config.properties\n         - --models archived_model=https://dlc-samples.s3.amazonaws.com/pytorch/multi-model-server/archived_model.mar\n         ports:\n         - name: mms\n           containerPort: 8080\n```", "```py\nkubectl get services --all-namespaces -o wide\n```", "```py\nNAME         TYPE      CLUSTER-IP   EXTERNAL-IP    PORT(S)  AGE \ntf-inference ClusterIP 10.3.xxx.xxx 104.198.xxx.xx 8500/TCP 54s\n```", "```py\ncurl -d demo_input.json -X POST http://104.198.xxx.xx:8500/v1/models/demo:predict\n```", "```py\n{\n    \"instances\": [1.0, 2.0, 5.0]\n}\n```", "```py\n{\n    \"predictions\": [2.5, 3.0, 4.5]\n}\n```", "```py\nimport tensorflow as tf\ntf.neuron.saved_model.compile(\n    tf_model_dir, # input TF model dir \n    neuron_model_dir # output neuron compiled model dir\n)\n```", "```py\n       containers:\n       - name: neuron-demo\n         image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-neuron:1.15.4-neuron-py37-ubuntu18.04\n         command:\n         - /usr/local/bin/entrypoint.sh\n         args:\n         - --port=8500\n         - --rest_api_port=9000\n         - --model_name=neuron_model\n         - --model_base_path=s3://mybucket/neuron_model/\n         ports:\n         - name: http\n           containerPort: 8500 # HTTP port\n         - name: gRPC\n           containerPort: 9000 # gRPC port\n```", "```py\nkubectl autoscale deployment <application-name> --cpu-percent=60 --min=1 --max=10\n```", "```py\nfrom sagemaker import get_execution_role\nfrom sagemaker import Session\n# IAM role of the notebook\nrole = get_execution_role()\n# A Session object for SageMaker\nsess = Session()\n# default bucket object\nbucket = sess.default_bucket()\n```", "```py\nimport tarfile\nmodel_archive = \"model.tar.gz\"\nwith tarfile.open(model_archive, mode=\"w:gz\") as archive:\n   archive.add(\"export\", recursive=True) \n# model artifacts uploaded to S3 bucket\nmodel_s3_path = sess.upload_data(path=model_archive, key_prefix=\"model\")\n```", "```py\nfrom sagemaker.tensorflow.serving import Model\n# TF version\ntf_framework_version = \"2.8\"\n# Model instance for inference endpoint creation\nsm_model = Model(\n    model_data=model_s3_path, # S3 path for model\n    framework_version=tf_framework_version, # TF version\n    role=role) # IAM role of the notebook\npredictor = sm_model.deploy(\n    initial_instance_count=1, # number of instances used\n    instance_type=\"ml.c5.xlarge\")\n```", "```py\ninput = {\n    \"instances\": [1.0, 2.0, 5.0]\n}\nresults = predictor.predict(input)\n```", "```py\n{\n    \"predictions\": [2.5, 3.0, 4.5]\n}\n```", "```py\nimport boto3\nclient = boto3.client(\"runtime.sagemaker\")\n# SageMaker Inference endpoint name\nendpoint_name = \"run_model_prediction\"\n# Payload for inference which consists of the input data\npayload = \"...\"\n# SageMaker endpoint called to get HTTP response (inference)\nresponse = client.invoke_endpoint(\n   EndpointName=endpoint_name,\n   ContentType=\"text/csv\", # content type\n   Body=payload # input data to the endpoint)\n```", "```py\nfrom sagemaker.tensorflow import TensorFlowModel\n# Model instance\nsm_model = TensorFlowModel(\n   model_data=model_s3_path,\n   framework_version=tf_framework_version,\n   role=role) # IAM role of the notebook\n# Predictor\npredictor = sm_model.deploy(\n   initial_instance_count=1,\n   instance_type=\"ml.c5.xlarge\")\n```", "```py\nfrom sagemaker.tensorflow.estimator import TensorFlow\n# create an estimator\nestimator = TensorFlow(\n    entry_point=\"tf-train.py\",\n    ...,\n    instance_count=1,    \n    instance_type=\"ml.c4.xlarge\",\n    framework_version=\"2.2\",\n    py_version=\"py37\" )\n# train the model\nestimator.fit(inputs)\n# deploy the model and returns predictor instance for inference\npredictor = estimator.deploy(\n    initial_instance_count=1, \n    instance_type=\"ml.c5.xlarge\")\n```", "```py\nfrom sagemaker.pytorch import PyTorchModel\nmodel = PyTorchModel(\n    entry_point=\"inference.py\",\n    source_dir=\"s3://bucket/model\",\n    role=role, # IAM role for SageMaker\n    model_data=pt_model_data, # model file\n    framework_version=\"1.11.0\", # PyTorch version\n    py_version=\"py3\", # python version\n)\n```", "```py\nfrom sagemaker.pytorch.estimator import PyTorch\n# create an estimator\nestimator = PyTorch(\n    entry_point=\"pytorch-train.py\",\n    ...,\n    instance_count=1,\n    instance_type=\"ml.c4.xlarge\",\n    framework_version=\"1.11\",\n    py_version=\"py37\")\n# train the model\nestimator.fit(inputs)\n# deploy the model and returns predictor instance for inference\npredictor = estimator.deploy(\n   initial_instance_count=1, \n   instance_type=\"ml.c5.xlarge\")\n```", "```py\nfrom sagemaker.model import Model\n# Load an ONNX model file for endpoint creation\nsm_model= Model(    \n    model_data=model_data, # path for an ONNX .tar.gz file\n    entry_point=\"inference.py\", # an inference script\n    role=role,\n    py_version=\"py3\",\n    framework=\"onnx\",\n    framework_version=\"1.4.1\", # ONNX version\n)\n# deploy model\npredictor = sm_model.deploy(\n   initial_instance_count=1, # number of instances to use\n   instance_type=ml.c5.xlarge) # instance type for deploy\n```", "```py\nfrom sagemaker.tensorflow import TensorFlowModel\n# Load ONNX model file as a TensorFlowModel\ntf_model = TensorFlowModel(    \n    model_data=model_data, # path to the ONNX .tar.gz file\n    entry_point=\"tf_inference.py\", \n    role=role,\n    py_version=\"py3\", # Python version\n    framework_version=\"2.1.1\", # TensorFlow version\n)\nfrom sagemaker.pytorch import PyTorchModel\n# Load ONNX model file as a PyTorchModel\npytorch_model = PyTorchModel(\n    model_data=model_data, # path to the ONNX .tar.gz file\n    entry_point=\"pytorch_inference.py\",\n    role=role,\n    py_version=\"py3\", # Python version\n    framework_version=\"1.11.0\", # PyTorch version\n)\n```", "```py\nfrom sagemaker import transformer\nbucket_name = \"my-bucket\" # S3 bucket with data\n# location of the input data\ninput_location = \"s3://{}/{}\".format(bucket_name, \"input_data\")\n# location where the predictions will be stored\nbatch_output = \"s3://{}/{}\".format(bucket_name, \"batch-results\")\n# initialize the transformer object\ntransformer = transformer.Transformer(\n   base_transform_job_name=\"Batch-Transform\", # job name\n   model_name=model_name, # Name of the inference endpoint\n   max_payload= 5, # maximum payload\n   instance_count=1, # instance count to start with\n   instance_type=\"ml.c4.xlarge\", # ec2 instance type\n   output_path=batch_output # S3 for batch inference output)\n# triggers the prediction on the whole dataset\ntf_transformer = transformer.transformer(\n   input_location, # input S3 path for input data\n   content_type=\"text/csv\", # input content type as CSV\n   split_type=\"Line\" # split type for input as Line)\n```", "```py\n# sm_model created from Model\nsm_model = Model(...)\n# instance type of which the model will be optimized for\ninstance_family = \"ml_c5\"\n# DL framework\nframework = \"tensorflow\"\ncompilation_job_name = \"tf-compile\"\ncompiled_model_path = \"s3:...\"\n# shape of an input data\ndata_shape = {\"inputs\":[1, data.shape[0], data.shape[1]]}\nestimator = sm_model.compile(\n   target_instance_family=instance_family,\n   input_shape=data_shape,\n   ob_name=compilation_job_name,\n   role=role,\n   framework=framework,\n   framework_version=tf_framework_version,\n   output_path=compiled_model_path)\n# deploy the neo model on instances of the target type\npredictor = estimator.deploy(\n   initial_instance_count=1,\n   instance_type=instance_family)\n```", "```py\n# deploying a Tensorflow/PyTorch/other model files using EI\npredictor = sm_model.deploy(\n   initial_instance_count=1, # ec2 initial count\n   instance_type=\"ml.m4.xlarge\", # ec2 instance type\n   accelerator_type=\"ml.eia2.medium\" # accelerator type)\n```", "```py\nimport boto3\n# Sagemaker runtime client instance\nruntime_sagemaker_client = boto3.client(\"sagemaker-runtime\")\n# send a request to the endpoint targeting  specific model \nresponse = runtime_sagemaker_client.invoke_endpoint(\n   EndpointName=\"<ENDPOINT_NAME>\",\n   ContentType=\"text/csv\",\n   TargetModel=\"<MODEL_FILENAME>.tar.gz\",\n   Body=body)\n```"]