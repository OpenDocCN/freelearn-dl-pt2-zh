- en: Whats Next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've finally finished the book! Give yourself a pat on the back! Thanks for
    going through the book, and I sincerely hope that it helps you in your path ahead,
    be it as a data scientist, a machine learning engineer, or one of the many other
    evolving job titles in the AI space. By now, you should have a firm grasp of the
    PyTorch API and how to use it to perform several tasks across computer vision,
    natural language processing, and reinforcement learning. However, this is by no
    means an end to the journey you've started, but rather a beginning to a fantastic
    road ahead!
  prefs: []
  type: TYPE_NORMAL
- en: What's next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at the next sequence of logical steps to take
    in order to build on the progress you've made with this book. However, let's first
    take a step back and look at all the tools and techniques we've learned so far
    and understand how all of what you've learned so far fits into a deeper learning
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following bullet list will help you understand the essence of each chapter
    from the book and can act as a quick guide to put what we''ve learned throughout
    the book into context:'
  prefs: []
  type: TYPE_NORMAL
- en: History of AI, neural networks, and deep learning. The various deep learning
    frameworks in use. The importance and necessity of PyTorch. Improvements in PyTorch
    v1.0\. GPU versus CPU. Using CUDA to parallelize tensor computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The building blocks of neural networks: How do networks learn representations?
    We looked at PyTorch `tensors`, `tensor operations`, `nn.module`, `torch optim`,
    and how the PyTorch define by run dynamic DAGs work.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned about the different processes involved in training a neural network,
    such as the PyTorch dataset for data preparation, data loaders for batching tensors,
    the `torch.nn` package for creating network architectures, and using PyTorch loss
    functions and optimizers. We also went through different techniques to handle
    overfitting, such as dropout, l1 and l2 regularization, and using batch normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned the different building blocks of **convolution neural networks**
    (**CNNs**), and also learned about transfer learning, which helps us to use a
    pretrained model. We also saw techniques, such as using preconvoluted features,
    which help in reducing the time taken to train the models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We learned about word embedding and how to use it for text classification problems.
    We also explored how we can use pretrained word embedding. We explored **recurrent
    neural network** (**RNN**), its variants such as **long short-term memory** (**LSTM**),
    and how to use them for text classification problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we examined the idea of semi-supervised learning using autoencoders
    to denoise data, and variational autoencoders to generate new images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explored generative models and learned how PyTorch can be used for creating
    an artistic style transfer, and for creating new CIFAR images using a **generative
    adversarial network** (**GAN**). We also explored language modeling techniques
    that can be used to generate new text or to create domain-specific embedding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explored modern architectures, such as ResNet, Inception, DenseNet and encode-decoder
    architecture. We also saw how these models can be used for transfer learning.
    We also built an ensemble model by combining all these models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we looked at how reinforcement learning can be used to train models
    to make decisions in uncertain sequential environments. We looked at the various
    deep reinforcement learning strategies, such as deep-Q learning and policy based
    and actor-critic models. We used an OpenAI gym environment to solve the famous
    cart-pole problem using deep reinforcement learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading and implementing research papers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a constantly evolving field and keeping up to date with the
    latest developments in the field will definitely impact your ability to contribute
    to the team you are working in and also to the field in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Research papers at first might look like jargon-filled convoluted text that
    is hard to understand, but making a constant effort to read and implement these
    algorithms will significantly boost your capabilities. One of my favorite repositories
    of papers and their corresponding implementations in code is paperswithcode ([https://paperswithcode.com/sota](https://paperswithcode.com/sota)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c602442-caaa-43f8-bed5-627a121175cb.png)'
  prefs: []
  type: TYPE_IMG
- en: The paperswithcode website
  prefs: []
  type: TYPE_NORMAL
- en: You should find the latest research papers and code for various tasks in AI
    such as CV, NLP, reinforcement learning, speech, and so on. Going through at least
    one paper per week and implementing the code by downloading the source code will
    help you stay on track with the latest developments in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Interesting ideas to explore
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the concepts that we learned in the book form the foundation of modern
    applications that are powered by deep learning. In this section, we will look
    at the different interesting projects that we can do that are related to computer
    vision and **natural language processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the examples we have seen in this book help you in detecting whether a
    given image is this (cat) or that (dog). But, to solve some of the problems in
    the real world, you may need to identify different objects in an image, such as
    the ones shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaa25c1b-2c27-4dbc-8185-e0257e21505a.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of an object detection algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'This image shows the output of an object detection algorithm where the algorithm
    is detecting objects such as a beautiful dog and cat. Just as there are off-the-shelf
    algorithms for image classification, there are a bunch of amazing algorithms that
    can help in building object recognition systems. Here is a list of some of the
    important algorithms and the papers on object detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single shot multibox detector** (**SSD**): [https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster RCNN: [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO2: [https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume you are reading this book from the terrace of a building. What
    do you see around you? Can you draw an outline of what you see? If you are a good
    artist, unlike me, then you would have probably drawn a couple of buildings, trees,
    birds, and a few more interesting things surrounding you. Image segmentation algorithms
    try to capture something similar. Given an image, they generate a prediction for
    each pixel, identifying which class each pixel belongs to. The following image
    shows what image segmentation algorithms identify:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd2f326a-1ecb-4c66-a936-83b3b80248db.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of an image segmentation algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the important algorithms that you may want to explore for image segmentation
    are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: R-CNN: [https://arxiv.org/abs/1311.2524](https://arxiv.org/abs/1311.2524)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast R-CNN: [https://arxiv.org/abs/1504.08083](https://arxiv.org/abs/1504.08083)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster R-CNN: [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mask R-CNN: [https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenNMT in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Open Source Neural Machine Translation** (**OpenNMT**) ([https://github.com/OpenNMT/OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py))
    project helps in building a lot of applications that are powered by the encoder-decoder
    architecture. Some of the applications that you can build are translation systems,
    text summarization, and image-to-text.
  prefs: []
  type: TYPE_NORMAL
- en: Allen NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Allen NLP is an open source project built on PyTorch that enables us to do many
    NLP tasks much more easily. There is a demo page ([http://demo.allennlp.org/machinecomprehension](http://demo.allennlp.org/machinecomprehension))
    that you should look at to understand what you can build using Allen NLP.
  prefs: []
  type: TYPE_NORMAL
- en: fast.ai – making neural nets uncool again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of my favorite places to learn about deep learning, and a great place of
    inspiration, is a MOOC, with the sole motive of making deep learning accessible
    to all, organized by two amazing mentors from `fast.ai` ([http://www.fast.ai/](http://www.fast.ai/)),
    Jeremy Howard and Rachel Thomas. For a new version of their course, they built
    an incredible framework ([https://github.com/fastai/fastai](https://github.com/fastai/fastai))
    on top of PyTorch, making it much easier and quicker to build applications. If
    you have not already started their course, I would strongly recommend you start
    it. Exploring how the `fast.ai` framework is built will give you great insight
    into many powerful techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Open neural network exchange
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Open neural network exchange** (**ONNX**) ([http://onnx.ai/](http://onnx.ai/))
    is the first step towards an open ecosystem that empowers you to choose the right
    tools as the project evolves. ONNX provides an open source format for deep learning
    models. It defines an extensible computation graph model as well as definitions
    of built-in operators and standard data types. Caffe2, PyTorch, Microsoft Cognitive
    Toolkit, Apache MXNet, and other tools are developing ONNX support. This project
    can help in product ionizing PyTorch models.'
  prefs: []
  type: TYPE_NORMAL
- en: How to keep yourself updated
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social media platforms, particularly Twitter, help you to stay updated in the
    field. There are many people you can follow. If you are unsure of where to start,
    I would recommend following Jeremy Howard ([https://twitter.com/jeremyphoward](https://twitter.com/jeremyphoward))
    and any interesting people he may follow. By doing this, you will be forcing the
    Twitter recommendation system to work for you. Another important Twitter account
    you need to follow is PyTorch ([https://twitter.com/PyTorch](https://twitter.com/PyTorch)).
    The amazing people behind PyTorch have some great content being shared. If you
    are looking for research papers, then look at [http://www.arxiv-sanity.com/](http://www.arxiv-sanity.com/),
    where many smart researchers publish their papers. More great resources for learning
    about PyTorch are its tutorials ([http://pytorch.org/tutorials/](http://pytorch.org/tutorials/)),
    its source code ([https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)),
    and its documentation ([http://pytorch.org/docs/0.3.0/](http://pytorch.org/docs/0.3.0/)[)](http://pytorch.org/docs/0.3.0/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is much more to deep learning and PyTorch. PyTorch is a relatively new
    framework, which, at the time of writing this chapter, is 3 years old. There is
    much more to learn and explore, so happy learning. All the best.
  prefs: []
  type: TYPE_NORMAL
