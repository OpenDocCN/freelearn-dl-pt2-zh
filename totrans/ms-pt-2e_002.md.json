["```py\n<start> PyTorch is a deep learning library. <end>\n```", "```py\n<start> : 0\nPyTorch : 1\nis : 2\na : 3\ndeep : 4\nlearning : 5\nlibrary : 6\n. : 7\n<end> : 8\n```", "```py\n<start> PyTorch is a deep learning library. <end> -> [0, 1, 2, 3, 4, 5, 6, 7, 8]\n```", "```py\n# download images and annotations to the data directory\n!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data_dir/\n!wget http://images.cocodataset.org/zips/train2014.zip -P ./data_dir/\n!wget http://images.cocodataset.org/zips/val2014.zip -P ./data_dir/\n# extract zipped images and annotations and remove the zip files\n!unzip ./data_dir/captions_train-val2014.zip -d ./data_dir/\n!rm ./data_dir/captions_train-val2014.zip\n!unzip ./data_dir/train2014.zip -d ./data_dir/\n!rm ./data_dir/train2014.zip\n!unzip ./data_dir/val2014.zip -d ./data_dir/\n!rm ./data_dir/val2014.zip\n```", "```py\nimport nltk\nfrom pycocotools.coco import COCO\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.nn.utils.rnn import pack_padded_sequence\n```", "```py\nnltk.download('punkt')\n```", "```py\ndef build_vocabulary(json, threshold):\n    \"\"\"Build a vocab wrapper.\"\"\"\n    coco = COCO(json)\n    counter = Counter()\n    ids = coco.anns.keys()\n    for i, id in enumerate(ids):\n        caption = str(coco.anns[id]['caption'])\n        tokens = nltk.tokenize.word_tokenize(caption.lower())\n        counter.update(tokens)\n        if (i+1) % 1000 == 0:\n            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n```", "```py\n # If word freq < 'thres', then word is discarded.\n    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n    # Create vocab wrapper + add special tokens.\n    vocab = Vocab()\n    vocab.add_token('<pad>')\n    vocab.add_token('<start>')\n    vocab.add_token('<end>')\n    vocab.add_token('<unk>')\n    # Add words to vocab.\n    for i, token in enumerate(tokens):\n        vocab.add_token(token)\n    return vocab\n```", "```py\nvocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\nvocab_path = './data_dir/vocabulary.pkl'\nwith open(vocab_path, 'wb') as f:\n    pickle.dump(vocab, f)\nprint(\"Total vocabulary size: {}\".format(len(vocab)))\nprint(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n```", "```py\ndef reshape_images(image_path, output_path, shape):\n    images = os.listdir(image_path)\n    num_im = len(images)\n    for i, im in enumerate(images):\n        with open(os.path.join(image_path, im), 'r+b') as f:\n            with Image.open(f) as image:\n                image = reshape_image(image, shape)\n                image.save(os.path.join(output_path, im), image.format)\n        if (i+1) % 100 == 0:\n            print (\"[{}/{}] Resized the images and saved into '{}'.\".format(i+1, num_im, output_path))\nreshape_images(image_path, output_path, image_shape)\n```", "```py\nclass CustomCocoDataset(data.Dataset):\n    \"\"\"COCO Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n        \"\"\"Set path for images, texts and vocab wrapper.\n\n        Args:\n            data_path: image directory.\n            coco_json_path: coco annotation file path.\n            vocabulary: vocabulary wrapper.\n            transform: image transformer.\n        \"\"\"\n        ...\n    def __getitem__(self, idx):\n        \"\"\"Returns one data sample (X, y).\"\"\"\n        ...\n        return image, ground_truth\n    def __len__(self):\n        return len(self.indices)\n```", "```py\ndef collate_function(data_batch):\n    \"\"\"Creates mini-batches of data\n    We build custom collate function rather than using standard collate function,\n    because padding is not supported in the standard version.\n    Args:\n        data: list of (image, caption)tuples.\n            - image: tensor of shape (3, 256, 256).\n            - caption: tensor of shape (:); variable length.\n    Returns:\n        images: tensor of size (batch_size, 3, 256, 256).\n        targets: tensor of size (batch_size, padded_length).\n        lengths: list.\n    \"\"\"\n    ...       \n    return imgs, tgts, cap_lens\n```", "```py\ndef get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle):\n    # COCO dataset\n    coco_dataset = CustomCocoDataset(data_path=data_path,\n                       coco_json_path=coco_json_path,\n                       vocabulary=vocabulary,\n                       transform=transform)\n    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataset, batch_size=batch_size, shuffle=shuffle,  collate_fn=collate_function)\n    return custom_data_loader\n```", "```py\nclass CNNModel(nn.Module):\n    def __init__(self, embedding_size):\n        \"\"\"Load pretrained ResNet-152 & replace last fully connected layer.\"\"\"\n        super(CNNModel, self).__init__()\n        resnet = models.resnet152(pretrained=True)\n        module_list = list(resnet.children())[:-1]\n      # delete last fully connected layer.\n        self.resnet_module = nn.Sequential(*module_list)\n        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n            def forward(self, input_images):\n        \"\"\"Extract feats from images.\"\"\"\n        with torch.no_grad():\n            resnet_features = self.resnet_module(input_images)\n        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n        final_features = self.batch_norm(self.linear_layer(resnet_features))\n        return final_features\n```", "```py\nclass LSTMModel(nn.Module):\n    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n        ...\n        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n        ...\n\n    def forward(self, input_features, capts, lens):\n        ...\n        hidden_variables, _ = self.lstm_layer(lstm_input)\n        model_outputs = self.linear_layer(hidden_variables[0])\n        return model_outputs\n```", "```py\n def sample(self, input_features, lstm_states=None):\n        \"\"\"Generate caps for feats with greedy search.\"\"\"\n        sampled_indices = []\n        ...\n        for i in range(self.max_seq_len):\n...\n            sampled_indices.append(predicted_outputs)\n            ...\n        sampled_indices = torch.stack(sampled_indices, 1)\n        return sampled_indices\n```", "```py\n# Device configuration device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n```", "```py\n# Image pre-processing, normalization for pretrained resnet\ntransform = transforms.Compose([\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))])\n```", "```py\n# Load vocab wrapper\nwith open('data_dir/vocabulary.pkl', 'rb') as f:\n    vocabulary = pickle.load(f)\n\n# Instantiate data loader\ncustom_data_loader = get_loader('data_dir/resized_images', 'data_dir/annotations/captions_train2014.json', vocabulary,\n                         transform, 128,\n                         shuffle=True)\n```", "```py\n# Build models\nencoder_model = CNNModel(256).to(device)\ndecoder_model = LSTMModel(256, 512, len(vocabulary), 1).to(device)\n\n# Loss & optimizer\nloss_criterion = nn.CrossEntropyLoss()\nparameters = list(decoder_model.parameters()) + list(encoder_model.linear_layer.parameters()) + list(encoder_model.batch_norm.parameters())\noptimizer = torch.optim.Adam(parameters, lr=0.001)\n```", "```py\nfor epoch in range(5):\n    for i, (imgs, caps, lens) in enumerate(custom_data_loader):\n        tgts = pack_padded_sequence(caps, lens, batch_first=True)[0]\n        # Forward pass, backward propagation\n        feats = encoder_model(imgs)\n        outputs = decoder_model(feats, caps, lens)\n        loss = loss_criterion(outputs, tgts)\n        decoder_model.zero_grad()\n        encoder_model.zero_grad()\n        loss.backward()\n        optimizer.step()\n```", "```py\n # Log training steps\n        if i % 10 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n                  .format(epoch, 5, i, total_num_steps, loss.item(), np.exp(loss.item())))\n        # Save model checkpoints\n        if (i+1) % 1000 == 0:\n            torch.save(decoder_model.state_dict(), os.path.join(\n                'models_dir/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n            torch.save(encoder_model.state_dict(), os.path.join(\n                'models_dir/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n```", "```py\nimage_file_path = 'sample.jpg'\n# Device config\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef load_image(image_file_path, transform=None):\n    img = Image.open(image_file_path).convert('RGB')\n    img = img.resize([224, 224], Image.LANCZOS)\n    if transform is not None:\n        img = transform(img).unsqueeze(0)\n    return img\n# Image pre-processing\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))])\n```", "```py\n# Load vocab wrapper\nwith open('data_dir/vocabulary.pkl', 'rb') as f:\n    vocabulary = pickle.load(f)\n# Build models\nencoder_model = CNNModel(256).eval()  # eval mode (batchnorm uses moving mean/variance)\ndecoder_model = LSTMModel(256, 512, len(vocabulary), 1)\nencoder_model = encoder_model.to(device)\ndecoder_model = decoder_model.to(device)\n```", "```py\n# Load trained model params\nencoder_model.load_state_dict(torch.load('models_dir/encoder-2-3000.ckpt'))\ndecoder_model.load_state_dict(torch.load('models_dir/decoder-2-3000.ckpt'))\n```", "```py\n# Prepare image\nimg = load_image(image_file_path, transform)\nimg_tensor = img.to(device)\n# Generate caption text from image\nfeat = encoder_model(img_tensor)\nsampled_indices = decoder_model.sample(feat)\nsampled_indices = sampled_indices[0].cpu().numpy()\n          # (1, max_seq_length) -> (max_seq_length)\n```", "```py\n# Convert numeric tokens to text tokens\npredicted_caption = []\nfor token_index in sampled_indices:\n    word = vocabulary.i2w[token_index]\n    predicted_caption.append(word)\n    if word == '<end>':\n        break\npredicted_sentence = ' '.join(predicted_caption)\n```", "```py\n# Print image & generated caption text\nprint (predicted_sentence)\nimg = Image.open(image_file_path)\nplt.imshow(np.asarray(img))\n```"]