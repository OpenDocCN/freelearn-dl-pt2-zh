- en: '*Chapter 10*: Going Live with OpenAI-Powered Apps'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before going live with apps that use the OpenAI API, they must be approved for
    publishing by OpenAI. The approval process helps prevent the OpenAI API from being
    misused either intentionally or accidentally. It also helps app providers, and
    OpenAI, plan for resource requirements to ensure the app performs well at launch,
    and as usage grows.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll discuss OpenAI application use case guidelines along
    with the review and approval process. Then we’ll discuss changes to our GPT Answers
    app based on OpenAI guidelines. Finally, we’ll implement the suggested updates
    and discuss the process for submitting our app for review, and hopefully, approval!
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics we’ll cover are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Going live
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding use case guidelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing potential approval issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing the pre-launch review request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires that you have access to the **OpenAI API**. You can request
    access by visiting [https://openai.com](https://openai.com).
  prefs: []
  type: TYPE_NORMAL
- en: Going live
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI defines a live application as any application that is serving API outputs
    to more than five people. This includes people in your company or organization.
    So, even a private beta app is considered live if it is using the OpenAI API and
    has more than five users. To move beyond this limit, your app needs to be reviewed
    and approved by OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Going live without approval from OpenAI could result in your API key being revoked
    immediately, and without warning. Further, going live without approval could possibly
    cause your account to be permanently blocked from further API access. So, it’s
    a good idea to understand the OpenAI use cases guidelines and review process.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding use case guidelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a wide range of applications that could use the OpenAI API. However,
    not all use cases are permitted, so every application must be reviewed and approved
    before going live.
  prefs: []
  type: TYPE_NORMAL
- en: Every app is evaluated on a case-by-case basis, so the only way to know whether
    your application will be allowed is to go through the review process. However,
    OpenAI publishes guidelines that you can review and follow to give your app the
    best chances of approval. You can find the guidelines located at [https://beta.openai.com/docs/use-case-guidelines](https://beta.openai.com/docs/use-case-guidelines).
    Before investing a lot of time in an app, you should first read the guidelines
    carefully.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not going to cover all the app guidelines in this section. But mostly,
    the guidelines relate to safety and security. Safety, as defined by OpenAI, is
    *Freedom from conditions that can cause physical, psychological, or social harm
    to people, including but not limited to death, injury, illness, distress, misinformation,
    or radicalization, damage to or loss of property or opportunity, or damage to
    the environment*.
  prefs: []
  type: TYPE_NORMAL
- en: So, apps that cheat, deceive, exploit, harass, hurt, intimidate, manipulate,
    mislead, steal, trick, or that could potentially cause harm or damage in any way,
    whether intentional or not, are not allowed. Most of the guidelines should seem
    pretty obvious. But some guidelines aren’t so obvious. For example, in most cases,
    you can’t build apps that generate content for Twitter tweets. This is because
    using AI-generated content violates Twitter’s acceptable use policies. So, again,
    the point of this section is not to cover the specific guidelines; the point is
    to emphasize the importance of reviewing and understanding the guidelines before
    building an app. By reviewing the guidelines before you start building, you’ll
    be able to focus on all the acceptable use cases and avoid potential approval
    issues. We’ll look at some of the potential issues that we can address before
    the review process next.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing potential approval issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading the OpenAI use case guidelines, we can consider how they apply
    to our GPT Answers app. Our application is limited to answering questions with
    answers that we provide in our answers file. So, it has a very limited scope and
    does not generate open-ended responses. Based on that, the guidelines suggest
    our app is *Almost-always approvable*. However, again, every app is approved on
    a case-by-case basis, so that’s not a guarantee.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we want to do everything we can as a developer to consider safety best
    practices. OpenAI publishes safety best practices at [https://beta.openai.com/docs/safety-best-practices](https://https://beta.openai.com/docs/safety-best-practices)
    that will help ensure our app is safe and can’t be exploited. This will also help
    increase the chances of our app being approved for publishing. Based on those
    guidelines, we’re going to consider a few modifications to our GPT Answers app.
    Specifically, we are going to consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Content filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Input and output lengths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at each of these considerations individually and discuss how they
    apply to our app.
  prefs: []
  type: TYPE_NORMAL
- en: Content filtering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Content filtering probably isn’t necessary for our GPT Answers app because the
    completions are being generated from an answers file that we’re providing – which
    is kind of one way to filter the output. However, we might not want to send inappropriate
    questions to the OpenAI API because even though the response will be safe in our
    case, we’ll still be using tokens. So, we’ll implement content filtering for the
    questions to check for inappropriate words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The content filtering process flow that we’ll be implementing is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The user asks a question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We check the question for profane language.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If profane language is detected, we display: **That’s not a question we can
    answer**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If profane language is not detected, we pass the question to the OpenAI API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’ll use a `Node.js` library called `bad-words` to check for profanity in the
    question text before sending it to the OpenAI API. If profanity is found in the
    text, we’ll politely respond with a message saying, **That’s not a question we
    can answer**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement content filtering on the question text, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Require the `bad-words` library on the first line of `routes/answer.js` with
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the `routes/answer.js` file, add the following code above the line that
    begins with `const data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Click the **Stop** and then the **Run** button and test it by entering a question
    that includes profanity. You should see a result like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Filtering profanity in questions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_10_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Filtering profanity in questions
  prefs: []
  type: TYPE_NORMAL
- en: Now we have content filtering in place for the question. If we were generating
    answers using the completions endpoint, we’d also want to look at using the content
    filter engine that we discussed in [*Chapter 6*](B16854_06_ePub_AM.xhtml#_idTextAnchor126),
    *Content Filtering*, to apply content filtering to the answer. But again, since
    we are generating answers from a file we’re providing, that’s probably not necessary
    for the GPT Answers app. So, let’s move on and consider input and output lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output lengths
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI recommends limiting both input and output lengths. Outputs can be easily
    limited with the `max_tokens` parameter. We’ve set the `max_tokens` parameter
    for the GPT Answers app to `150`. This is the recommended length for scoped output
    – like answers to questions from our answers file. This will support ~6-8 sentences
    for our answer text. If you have shorter answers, you can reduce the `max_tokens`
    length. Less is better provided you’re allowing enough to fully answer the questions.
  prefs: []
  type: TYPE_NORMAL
- en: An injection attack is an attack that exploits web applications that allow untrusted
    or unintended input to be executed. For example, in the GPT Answers app – what
    if the user submits something other than a question and our backend code were
    to pass it on to the OpenAI API? Remember text in/text out? Although our application
    is tightly scoped and something other than a relevant question isn’t going to
    return anything, it’s still worth adding in a bit of code to prevent very large
    text inputs because the input will still use tokens. So, we’ll add some code to
    limit the input length. The average sentence is 75-100 characters, so we’ll limit
    the input to 150 characters to allow for longer sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'To limit the input length in our GPT Answers app, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `routes/answer.js` and add the following code on a new line after the
    line that begins with `router.post`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Stop and run the Express service by clicking the **Stop** button and then the
    **Run** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a text input over 150 characters long into the question input and click
    the **GET ANSWER** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the form now tells the user the text they entered was too long,
    as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Form output with long text'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_10_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Form output with long text
  prefs: []
  type: TYPE_NORMAL
- en: Again, while our app shouldn’t generate unexpected completions, limiting the
    input, along with request rate limiting, will help prevent malicious attempts
    to exploit your app. Let’s talk about rate limiting next.
  prefs: []
  type: TYPE_NORMAL
- en: Request rate limiting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rate limiting prevents users from making more than a predefined number of requests
    in a given timeframe. This prevents malicious scripts from potentially making
    a large number of requests to your app. We will add rate-limiting functionality
    to our GPT Answers app using a library available for `Node.js` called `Express
    Rate Limit` and we’ll set the limit to a maximum of six requests per minute –
    per OpenAI's suggested guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement rate limiting, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `app.js` and after *line 9* (or after `var app = express();`), add the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open `routes/answer.js` and add the following code after the line that begins
    with `router.post`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous changes added a rate limit of six requests per minute. When the
    rate limit is hit before a request is made to the OpenAI API, we respond with
    a message to ask again in a minute, as in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Message when request rate is exceeded'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_10_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Message when request rate is exceeded
  prefs: []
  type: TYPE_NORMAL
- en: Because we’re sending the message back in the same JSON format as an answer,
    the message is displayed on the form page.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about the rate limiter library used by visiting [https://www.npmjs.com/package/limiter](https://https://www.npmjs.com/package/limiter).
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now that we have reviewed the use case guidelines and implemented some
    safety best practices, we’re ready to discuss the OpenAI pre-launch review and
    approval process.
  prefs: []
  type: TYPE_NORMAL
- en: Completing the pre-launch review request
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When your app is ready to go live, you begin the approval process by completing
    the **Pre-Launch Review Request form** located at [https://beta.openai.com/forms/pre-launch-review](https://beta.openai.com/forms/pre-launch-review).
  prefs: []
  type: TYPE_NORMAL
- en: The form collects your contact information, along with a link to your LinkedIn
    profile, a video demo of your app, and answers to a number of specific questions
    about the app use case and your growth plans. In the following sections, we’ll
    list the current questions and example answers that might apply to the GPT Answers
    app.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of questions on the Pre-Launch Review Request form, so the recommendation
    is to complete the questions first in a Google doc (or some other app) and then
    copy and paste the answers into the form when you’re ready.
  prefs: []
  type: TYPE_NORMAL
- en: The form begins by collecting your contact details. After providing your contact
    information, the first set of questions ask about the use case at a high level.
  prefs: []
  type: TYPE_NORMAL
- en: High-level use case questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The high-level use case questions are pretty straightforward. However, one
    of the questions asks for a video demo. So, you’ll need to provide a video walk-through
    and post it someplace like YouTube so you can provide a link. Here are the questions
    and some example answers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'QUESTION: Have you reviewed OpenAI’s use case guidelines?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: Yes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Please describe what your company does.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: My company provides technical learning resources.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Please describe what your application does.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: It lets users get answers to questions about me.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Has your application been reviewed by OpenAI previously? What was
    the outcome of this review? How does this submission relate to the prior review?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: No'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Please link to a short video demonstration of your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: A link to a video demo goes here.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Please share a login credential the OpenAI team can use to demo/test
    your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: No login credentials are required.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next set of questions relates to security and risk mitigation. As you might
    guess, there are a lot of questions about security and risk mitigation. Let’s
    take a look.
  prefs: []
  type: TYPE_NORMAL
- en: Security and risk mitigation questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are 14 security and risk mitigation questions at the time this book is
    being written. Some of the questions you’ll see are questions about content filtering,
    setting input and output lengths, and request rate limiting. These are important
    and required for approval, which is why we implemented them in our GPT Answers
    app:'
  prefs: []
  type: TYPE_NORMAL
- en: 'QUESTION: What is the maximum number of characters that a user can insert into
    your application’s input textboxes?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: 150'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: What are the maximum output tokens for a run of your application?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: 150'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Who are the typical users of your application (for example, enterprise
    businesses, research labs, entrepreneurs, academics, and so on)? Do you verify
    or authenticate users in some way? If so, how?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: The most likely users are recruiters who are interested in my professional
    background. Users are not verified but rate limiting is in place using the user’s
    IP address.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Do users need to pay to access your application? If so, how much?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: No'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Do you implement rate-limiting for your application? If so, what
    are the rate limits and how are they enforced?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: Yes, rate limiting is enforced by IP address and requests are limited
    to six requests per minute.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Do you implement a form of content filtration for your application?
    If so, what is being filtered, by what means, and how is this enforced?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: All answers are generated from an answers file that is pre-uploaded
    for use with the answers endpoint. So, content filtering is not used.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Do you capture user feedback on the quality of your outputs or on
    other details (for instance, returning unpleasant content)? If so, how is this
    data monitored and acted upon?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: A link is provided to a Google form that lets users report any issues
    they might encounter.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Will you monitor the usage of particular users of your application
    (for example, investigating spikes in volume, flagging certain keywords, et cetera)?
    If so, in what ways and with what enforcement mechanisms?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: No, because the scope is limited to just the data in the answers file
    that is provided by me.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Is it clearly disclosed in your application that the content is generated
    by an AI? In what way?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: Yes, the text on the question input pages lets the user know the answers
    are generated by GPT-3.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Is there a **human in the loop** in your application in some form?
    If so, please describe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: Yes, all of the answers to questions are from an answers file that
    is originally created and updated by humans.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Are there any other security or risk-mitigation factors you have
    implemented for this project? Please describe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: OpenAI token usage will be closely monitored for unusual usage patterns.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: What, if any, is the relationship between your application and social
    media?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: None.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: What, if any, is the relationship between your application and political
    content?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: None.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: If your team has particular credentials or background that may help
    to mitigate any risks described above, please elaborate here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: We have no specific credentials.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After the security and risk mitigation questions, you’ll be asked about your
    growth plans.
  prefs: []
  type: TYPE_NORMAL
- en: Growth plan questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To manage resource requirements and limit the potential for abuse, new applications
    are granted a maximum spend limit. This puts a cap on the maximum number of tokens
    that can be processed and therefore limits the scalability. However, the maximum
    spend limit can be increased over time as you build a track record with your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your initial spend limit will need to be approved to go live and an additional
    form needs to be submitted to request a spend limit increase after your application
    is launched. The spend limit increase form can be located at [https://beta.openai.com/forms/quota-increase](https://beta.openai.com/forms/quota-increase).
    To calculate your spend limit, enter a typical prompt into the Playground and
    set the engine and response length. Then hover over the number just below the
    prompt input and you’ll see an estimated cost, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Estimated cost'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16854_10_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Estimated cost
  prefs: []
  type: TYPE_NORMAL
- en: With the cost from the Playground, you can multiply by the estimated number
    of users and requests you’ll get on a monthly basis. You will need to provide
    an estimate for the questions in the growth plans section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following questions are asked about your growth plans:'
  prefs: []
  type: TYPE_NORMAL
- en: 'QUESTION: What $-value monthly quota would you like to request?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: $18'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: What amount of token consumption do you expect per month? For which
    engine(s)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: ~ 1 Mn ada tokens and ~1 Mn curie tokens.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: To how many users (approximately) will you initially roll out your
    application? How do you know these users? / How will you find these users?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: 5,000 users who subscribe to our SaaS service'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Please describe your growth plans following the initial rollout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: We plan to introduce the app to all new users of our service – ~500
    / month'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: If approved, on what date would you intend to launch your application?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: 2021-11-05'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: You may elaborate here on the launch date above if useful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: We want to launch as soon as possible.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Following the growth planning questions, there are just a few miscellaneous
    questions to wrap up, and you’re done.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping-up questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The wrapping-up questions request feedback on the app development process and
    your experience building the app. This is an opportunity to provide OpenAI with
    information that can help them improve the development experience for other developers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'QUESTION: We love feedback! Is there anything you’d like to share with the
    OpenAI team (for example, the hardest part of building your application or the
    features you would like to see)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: The hardest part was figuring out the best way to do request rate limiting.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Are there any collaborators you would like added to API access if
    we approve your application? If so, please list their emails separated by commas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: No, just me at this point.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: We are especially interested in feedback about this process. How
    long did this form take you to complete? What did you find most difficult about
    it?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: It took me about 5 days. I’m new to coding so the learning curve was
    challenging.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Anything else you would like to share?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: I’m really enjoying working with the API!'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'QUESTION: Date of form submission'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ANSWER: 05/11/2021'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After you complete and submit the Pre-Launch Review Request form, you should
    hear back within a few days. The response back will be an approval or a rejection
    with a reason for the rejection. Depending on the rejection reason, you might
    be able to address any noted issues and resubmit for another review. However,
    hopefully, your application is approved, and you’re cleared to go live!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations, you’ve completed *Exploring GPT-3* and your first OpenAI-powered
    app! At this point, your application should be ready for the review process. Keep
    in mind that all apps are approved on a case-by-case basis. So, just completing
    the steps in this book doesn’t guarantee approval. But you now understand the
    use case guidelines and the application review and approval process. Further,
    you have the knowledge and skills to address any changes that OpenAI might require
    to complete the review.
  prefs: []
  type: TYPE_NORMAL
