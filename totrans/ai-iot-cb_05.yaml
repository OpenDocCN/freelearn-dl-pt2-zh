- en: Anomaly Detection
  prefs: []
  type: TYPE_NORMAL
- en: diarizationThe predictive/prescriptive AI life cycle of a device starts with
    data collection design. Data is analyzed for factors such as correlation and variance.
    Then the devices start being manufactured. Other than a small number of sample
    devices, there is usually no device failures, that produce machine learning models.
    To compensate for this, most manufacturers use duty cycle thresholds to determine
    whether a device is in a good state or a bad state. These duty cycle standards
    may be that that the device is running too hot or an arbitrary value is put on
    a sensor for an alert. But the data quickly needs more advanced analysis. The
    sheer volume of data can be daunting for an individual. The analyst needs to look
    through millions of records to find the proverbial needle in a haystack. Using
    an analyst-in-the-middle approach using anomaly detection can efficiently help
    to find issues with devices. Anomaly detection is done through statistical, unsupervised,
    or supervised machine learning techniques. In other words, typically an analyst
    starts out looking at a single data point that is being examined for things such
    as spikes and dips. Then, multiple data points are pulled into an unsupervised
    learning model that clusters the data, allowing the data scientist to see when
    a set of values or patterns is not like the other sets of values. Finally, after
    enough device failures happen, the analyst can use the same type of machine learning
    we would use for predictive maintenance. Some machine learning algorithms, such
    as isolated forest, are better suited for anomaly detection, but the principles
    are the same.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection can be carried out before enough data is collected for supervised
    learning, or it can be part of a continuous monitoring solution. Anomaly detection
    can, for example, alert you to production issues with different factories. When
    an electrical engineer hands over a physical design to a factory, they perform
    a **bill of material** (**BOM**) optimization. In short, they alter the design
    to be one that can be put together more easily, or one that is more cost effective.
    Most physical devices are produced for a decade. In that time, parts that existed
    when the devices were first made may not be available, which means changes are
    needed to the BOM. Moving to a new manufacturer will also change the design as
    they produce their own BOM optimization. Anomaly detection can help pinpoint new
    issues popping up within your fleet.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways of looking at anomaly detection. In [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*, in *Analyzing chemical sensors with anomaly detection*
    recipe, we used K-means, a popular anomaly detection algorithm, to determine whether
    the chemical signature of a food item was different from the air. That is just
    one type of anomaly detection. There are many different types of anomaly detection.
    Some are specific to a particular machine and look at something that is abnormal
    over a period of time. Other anomaly detection algorithms look at a device acting
    normally and abnormally using supervised learning. Some devices are affected by
    their local environments or seasonality. Finally, in this chapter, we are going
    to talk about deploying anomaly detection to the edge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes are included in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Z-Spikes on a Raspberry Pi and Sense HAT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using autoencoders to detect anomalies in labeled data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using isolated forest for unlabeled datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting time series anomalies with Luminol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting seasonality-adjusted anomalies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting spikes with streaming analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting anomalies on the edge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Z-Spikes on a Raspberry Pi and Sense HAT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spikes or sudden changes to an individual device can warrant an alert. IoT devices
    are often subject to movement and weather. They can be affected by times of day
    or seasons of the year. The fleet of devices could be spread out throughout the
    world. Trying to get clear insights across the entire fleet can be challenging.
    Using a machine learning algorithm that incorporates the entire fleet enables
    us to treat each device separately.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases for Z-Spikes can be a sudden discharge of batteries or a sudden temperature
    increase. People use Z-Spikes to tell whether something has been jostled or is
    suddenly vibrating. Z-Spikes can be used on pumps to see whether there is a blockage.
    Because Z-Spikes do so well across non-homologous environments, they are often
    a great candidate for edge deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are going to deploy Z-Spikes on a Raspberry Pi with a Sense
    HAT. The hardware itself is a fairly common development board and sensor setup
    for people learning about IoT. In fact, students can send their code to the International
    Space Station to be run on their Raspberry Pi and Sense HAT. If you do not have
    the equipment, there is an alternative code in the GitHub repository that simulates
    the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have powered on your Raspberry Pi and attached your Sense HAT, you
    will need to install SciPy. In Python, you can usually install everything you
    need with `pip`, but in this case, you will need to install it through the Linux
    operating system. To do this, run the following commands in a terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You will then need to `pip` install `numpy`, `kafka`, and `sense_hat`. You will
    also need to set up Kafka on a PC. There are instructions in [Chapter 1](a6e87d27-4456-40a7-a006-5fdb54960858.xhtml),
    *Setting up the IoT and AI Environment*, in the *Setting up Kafka* recipe. Do
    not try to set up Kafka on the Raspberry Pi as it requires too much memory. Instead,
    set it up on a PC.
  prefs: []
  type: TYPE_NORMAL
- en: For the Raspberry Pi, you will need to connect a monitor, keyboard, and mouse.
    There is a Python editor in the developer tools menu. You will also need to know
    the IP address of the Kafka service.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait for Sense HAT to register with the OS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Z-score helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `sendAlert` helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `combined_value` helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `main` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm is checking whether the last record is more than 4 standard deviations
    (*σ*) from the preceding 1,000 values. *4σ* should have an anomaly 1 in every
    15,787 readings or once every 4 hours. If we were to change that to 4.5 it would
    be once every 40 hours.
  prefs: []
  type: TYPE_NORMAL
- en: We import `scipy` for our Z-score evaluation and `numpy` for data manipulation.
    We then add the script to the Raspberry Pi startup so that the script will start
    automatically whenever there is a power reset. The machine needs to wait for peripherals,
    such as the Sense HAT initialization. The 60-second delay allows the OS to be
    aware of the Sense HAT before trying to initialize it. Then we initialize our
    variables. These variables are the device name, the IP address of the Kafka server,
    and the Sense HAT. Then we enable the Sense HAT's **internal measuring units**
    (**IMUs**). We disable the compass and enable the gyroscope and accelerometer.
    Finally, we create two arrays to put the data in. Next, we create a Z-score helper
    function into which we can input an array of values to return the Z-scores. Next,
    we need a function that we can use to send the alerts. The `sense.gyro_raw` function
    gets the most recent gyroscope and accelerometer reading and puts them into a
    Python object. It then converts it to JSON. We then create a key value that is
    UTF-8 byte encoded. Similarly, we encode the message payload. Next, we create
    a Kafka topic name. Then, we send the key and message to the topic. We then check
    under `__main__` to see whether we are running the current file from a command
    shell. If we are, then we set a counter called `x` to `0`. Then we start an infinite
    loop. Then we start putting in the gyroscope and accelerometer data. We then check
    whether the array has 1,000 elements in it. If so, we remove the last value in
    the array so that we keep the array small. We then increment our counter to accumulate
    2 minutes of data. Finally, we check whether we are over 4 standard deviations
    away from the 1,000 values from our array; if so, we send our alert.
  prefs: []
  type: TYPE_NORMAL
- en: While this is a great way of looking at a device, we may want to do anomaly
    detection across our entire fleet. In the next recipes, we are going to create
    a message sender and receiver. If we were to do this in this recipe, we would
    make a Kafka producer message to send data on every iteration of our loop.
  prefs: []
  type: TYPE_NORMAL
- en: Using autoencoders to detect anomalies in labeled data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have labeled data, you can train a model to detect whether the data is
    normal or abnormal. For example, reading the current of an electric motor can
    show when extra drag is put on the motor by such things as failing ball bearings
    or other failing hardware. In IoT, anomalies can be a previously known phenomenon
    or a new event that has not been seen before. As the name suggests, autoencoders
    take in data and encode it to an output. With anomaly detection, we see whether
    a model can determine whether data is non-anomalous. In this recipe, we are going
    to use a Python object detection library called `pyod`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are going to use data gathered from the motion sensors on
    our Sense HAT. The final recipe in this chapter shows how to generate this dataset.
    We have also put this labeled dataset in the GitHub repository for this book.
    We are going to use a Python outlier detection framework called `pyod` or **Python
    Outlier Detection**. It wraps TensorFlow and performs various machine learning
    algorithms, such as autoencoders and isolated forests.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Load text files into our notebooks using NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the autoencoder algorithm to fix the model to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the prediction scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we import `pyod`, our Python object detection library. Then we import
    `numpy` for data manipulation and `pickle` for saving our model. Next, we use
    `numpy` to load our data. Then we train our model and get the prediction scores.
    Finally, we save our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'An autoencoder takes data as input and reduces the number of nodes through
    a smaller hidden layer that forces it to reduce the dimensionality. The target
    output for an autoencoder is the input. This allows us to use machine learning
    to train a model on what is non-anomalous. We can then determine how far a value
    falls away from the trained model. These values would be anomalous. The following
    diagram shows conceptually how data is coded into a set of inputs. Then, its dimensionality
    is reduced in the hidden layer and, finally, is outputted into a larger set of
    outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd3fbd34-c76d-40de-8003-e797a77e5cf7.png)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training our model, we need to know at what level to send the alert.
    When training, setting the contamination (see the following code) determines the
    proportion of outliers in the data that are needed to trigger the alerting function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also change the regularizer, as in the following example. The regularizer is
    used to balance the bias and variance to prevent over and underfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We could also change the number of neurons, our loss function, or the optimizer.
    This is often referred to as changing or tuning the hyperparameters in data science.
    Tuning the hyperparameters allows us to affect our success metrics, thereby improving
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Using isolated forest for unlabeled datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Isolated forest is a popular machine learning algorithm for anomaly detection.
    Isolated forests can assist in complex data models that have overlapping values.
    An isolated forest is an ensemble regression. Rather than using a clustering or
    distance-based algorithm like other machine learning algorithms, it separates
    outlying data points from normal data points. It does this by building a decision
    tree and calculates a score based on node count traversal in its path decision
    tree of where the data lies. In other words, it counts the number of nodes it
    traverses to determine an outcome. The more data that has been trained on a model,
    the more nodes an isolated forest would need to traverse.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the previous recipe, we are going to use `pyod` to easily train a
    model. We are going to use the Sense HAT dataset that is in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have completed the previous recipe on autoencoders, then you have everything
    you need. In this recipe, we are using `pyod` for our object detection library.
    The training dataset and the test dataset are in the GitHub repository for this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Upload the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Evaluate against the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we import `pyod`. We then import `numpy` for data processing and `pickle`
    for saving our model. Next, we perform the isolated forest training. Then we evaluate
    our results. We get two different types of results: one is a `1` or `0` to determine
    whether it is normal or anomalous, and the second gives us a score of the test.
    Finally, we save our model.'
  prefs: []
  type: TYPE_NORMAL
- en: The isolated forest algorithm segments the data using a tree-based approach.
    The more clustered the data is, the more segmented it is. The isolated forest
    algorithm looks for the data that is not part of the dense segmented area by counting
    the amounts of segments it would need to traverse to get there.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Anomaly detection is one of those analysis techniques where visualization can
    help us determine which hyperparameters and algorithms to use. scikit-learn has
    an example of how to do this on their website ([https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html](https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_anomaly_comparison.html)).
    A reference to this is in the GitHub repository of this book. The diagram that
    follows is an example of anomaly detection using multiple algorithms and settings
    on a toy dataset. There is no right answer in anomaly detection, but only what
    works best for the problem at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ff5953f-c353-4f1a-8624-56c659225c79.png)'
  prefs: []
  type: TYPE_IMG
- en: Detecting time series anomalies with Luminol
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Luminol is a time series anomaly detection algorithm released by LinkedIn. It
    uses a bitmap to check how many detection strategies, that are robust in datasets,
    tend to drift. It is also very lightweight and can handle large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we are going to use a publicly accessible IoT dataset from
    the city of Chicago. The city of Chicago has IoT sensors measuring the water quality
    of their lakes. Because the dataset needs some massaging before we get it into
    the right format for anomaly detection, we will use the `prepdata.py` file to
    extract one data point from one lake.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get ready for this recipe, you will need to download the CSV file from the
    GitHub repository for this book. Next, you will need to install `luminol`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps involved in this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prep the data with `prepdata.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Import libraries in `Luminol.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform anomaly detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the `dataprep` Python library, you will only import `pandas` so that we can
    take the CSV file and turn it into a `pandas` DataFrame. Once we have a `pandas`
    DataFrame we will filter out on `Rainbow Beach` (in our case, we are only looking
    at `Rainbow Beach`). Then we will take out anomalous data such as data where the
    water temperature is below -100 degrees. Then we will convert the `time` string
    into a string that pandas can read. We do this so that when it outputs, it outputs
    to a standard time series format. Then we select only the two columns we need
    to analyze, `Measurement Timestamp` and `Turbidity`. Finally, we save the file
    in CSV format.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a Luminol file. From here, we use `pip` to install `luminol`
    and `time`. We then use the anomaly detector on the CSV file and return all of
    the scores. Finally, we return scores if the value of our score item is greater
    than 0\. In other words, we only return scores if there is an anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to anomaly detection, Luminol can also form correlation analysis.
    This can help the analyst determine whether two time series datasets are correlated
    to each other. So, for example, our dataset from the city of Chicago measured
    various aspects of water purity in their lakes. We could compare lakes against
    each other to see whether there was a common effect in two different lakes at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting seasonality-adjusted anomalies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data from a temperature sensor might trend upward throughout the day if the
    device is outdoors. Similarly, the internal temperature of an exterior device
    may be lower in the winter. Not all devices are affected by seasonality but for
    the ones that are, choosing an algorithm that handles seasonality and trends is
    important. According to a research paper (*Automatic Anomaly Detection in the
    Cloud Via Statistical Learning*) from data scientists at Twitter, **Seasonal ESD**
    is a machine learning algorithm that takes seasonality and trends to find anomalies
    regardless of the seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we are going to use the city of Chicago lake water purity dataset.
    We are going to pull in the data file we prepared in the *Detecting time series
    anomalies with Luminol* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get ready, you will need the Seasonal ESD library. This can be installed
    simply with the following `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The dataset can be found in the GitHub repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps to execute this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Import and manipulate the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform anomaly detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Output the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we first imported `numpy` and `pandas` for data manipulation.
    We then imported `sesd`, our anomaly detection package. Next, we got the raw data
    ready for machine learning. We did this by removing the data that clearly had
    an issue, such as sensors that were not working properly. We then filtered the
    data into one column. We then put that column through the seasonal ESD algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the Z-score algorithm in the first recipe, this recipe uses an online
    approach. It uses **Seasonal and Trend decomposition using Loess** (**STL**) decomposition
    as a preprocessing step before doing anomaly detection. A data source may have
    a trend and a season, as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0889f98d-799d-472f-88af-134fa7386937.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What decomposition allows you to do is look at the trend and the seasonality
    independently (as shown in the following trend graph). This helps to ensure the
    data is not affected by seasonality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1453da0-afc3-4f11-9675-c25ffa0376ee.png)'
  prefs: []
  type: TYPE_IMG
- en: The Seasonal ESD algorithm is more complicated than the Z-score algorithm. For
    example, Z-score algorithms would show false positives in devices that were stationed
    outdoors.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting spikes with streaming analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stream Analytics is a tool that connects IoT Hub to other resources within Azure
    using a SQL interface. Stream Analytics moves data from IoT Hub to Cosmos DB,
    storage blobs, serverless functions, or a number of other scalable options. Streaming
    analytics has a few functions built-in, and you can create more functions yourself
    using JavaScript; anomaly detection is one of those functions. In this example,
    we are going to use Raspberry Pi to stream gyroscope and acceleration data to
    IoT Hub. Then we'll connect streaming analytics and, using its SQL interface,
    we will output only the anomalous results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this experiment, you will need IoT Hub. Next, you'll need to create a streaming
    analytics job. To do this, you will go into the Azure portal and create a new
    streaming analytics job through the **Create new resource** wizard. After you
    create a new streaming analytics job, you will see that there are three main components
    on the **Overview** page. These are inputs, outputs, and queries. Inputs, as the
    name suggests, are the streams you want to input; in our case, we are inputting
    IoT Hub. To connect to IoT Hub you need to click on **Inputs**, then select the
    input type of IoT Hub, and then select the IoT Hub instance you created for this
    recipe. Next, you can create an output. This could be a database such as Cosmos
    DB or a function app so that you can send alerts through any number of messaging
    systems. For the sake of simplicity, we are not going to specify output for this
    recipe. For testing purposes, you can review the output on the Stream Analytics
    query editor.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a joined device value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Get and send the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a SQL query that uses the `AnomalyDetection_SpikeAndDip` algorithm to
    detect anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To import the libraries on the Raspberry Pi you will need to log in to the Raspberry
    Pi and use `pip` to install `azure-iot-device` and `SenseHat`. Next, you'll need
    to go onto that machine and create a file called `device.py`. Then you will import
    the `time`, Azure IoT Hub, Sense HAT, and `json` libraries. Next, you'll need
    to go into IoT Hub and create a device through the portal, get your connection
    string, and enter it in the spot where it says Your device key here. You then
    initialize `SenseHat` and set the internal measuring units to `True`, initializing
    our sensors. Then create a helper function that combines our `x`, `y`, and `z`
    data. Next, get the data from sensors and send that to IoT Hub. Finally, wait
    for a second before sending that data again.
  prefs: []
  type: TYPE_NORMAL
- en: Next, go into the Stream Analytics job that you had set up and click on Edit
    query. From here, create a common table expression. A common table expression
    allows you to make a complex query more simple. Then use the built-in anomaly
    detection spikes and dips algorithm over a 120-second window. The quick editor
    allows you to test live data in the stream and view that the anomaly detectors
    gave the resulting score of anomalous or non-anomalous.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies on the edge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final recipe, we are going to use `SenseHat` on the Raspberry Pi to
    collect data, train that data on our local computer, then deploy a machine learning
    model on the device. To avoid redundancy after recording your data you will need
    to run either of the recipes on autoencoders or isolated forest from earlier in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: People use motion sensors in IoT to ensure shipping containers are safely transported aboard
    ships. For example, proving that a shipping container was dropped in a particular
    harbor would help with insurance claims. They are also used for worker safety
    to detect falls or workers acting unsafely. They are also used on devices that
    are prone to vibration when malfunctioning. Some examples of this are washing
    machines, wind turbines, and cement mixers.
  prefs: []
  type: TYPE_NORMAL
- en: During the data collection phase, you will need to safely simulate falling or
    working unsafely. You could also put a sensor on a washing machine that is unbalanced.
    The data in the GitHub repository has data from normal work and data that came
    from dancing, which in our case we are calling **anomalous**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get ready for this you will need a Raspberry Pi with a Sense HAT. You will
    need a way of getting data from the Raspberry Pi. You can do this by enabling
    `SSH` or using a USB drive. On the Raspberry Pi, you will need to use `pip` to
    install `sense_hat` and `numpy`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait for the user input to start:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Gather the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Output the files to disk for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Copy files from the Raspberry Pi to your local computer by using a thumb drive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an isolated forest using the isolated forest recipe and output the `pickle`
    file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the `iforrest.p` file to the Raspberry Pi and create a file called `AnomalyDetection.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the machine learning file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the output for the LEDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the anomaly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We create two files – one that gathers information (called `Gather.py`) and
    another that detects the anomalies on the device (called `AnomalyDetection.py`).
    In the `Gather.py` file, we import the classes, initialize `SenseHat`, set a variable
    for the number of readings we will be collecting, get both the gyroscopic and
    accelerometer readings, create an array of normal anonymous strings, and set the
    initial gyroscope and sensor ratings. Then we loop through our actions and tell
    the user to press *Enter* when they want to record normal greetings, and then
    tell them to press *Enter* when they want to record anomalous readings. From there,
    we gather data and give feedback to the user to let them know how many more data
    points they will be gathering. At this point, you should be using the device in
    a way that is normal for its use, such as fall detection by holding it close to
    your body. Then, for the next loop of anomalous readings, you drop the device.
    Finally, we create the training and test sets that we will use for the machine
    learning model. We then need to copy the data files into a local computer, and
    then we perform the analysis in the same way as we did the isolated forest earlier
    in this chapter. We would then get a `pickle` file that we will be using in the
    `AnomalyDetection.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: From here, we need to create the `AnomalyDetection.py` file that we will be
    using on our Raspberry Pi. Then we load our `pickle` file, which is our machine
    learning model. From here, we are going to create `alert` and not-alert (`clear`)
    variables that we can toggle for the LED display on the sense set. Finally, we
    run the loop, and if it predicts that the device is acting anomalously we display
    an `alert` signal on the sense set; otherwise, we display a `clear` signal.
  prefs: []
  type: TYPE_NORMAL
