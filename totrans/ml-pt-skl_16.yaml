- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Transformers – Improving Natural Language Processing with Attention Mechanisms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器 - 使用注意机制改进自然语言处理
- en: In the previous chapter, we learned about **recurrent neural networks** (**RNNs**)
    and their applications in **natural language processing** (**NLP**) through a
    sentiment analysis project. However, a new architecture has recently emerged that
    has been shown to outperform the RNN-based **sequence-to-sequence** (**seq2seq**)
    models in several NLP tasks. This is the so-called **transformer** architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了关于**循环神经网络**（**RNNs**）及其在**自然语言处理**（**NLP**）中的应用，通过一个情感分析项目。然而，最近出现了一种新的架构，已被证明在几个NLP任务中优于基于RNN的**序列到序列**（**seq2seq**）模型。这就是所谓的**变压器**架构。
- en: Transformers have revolutionized natural language processing and have been at
    the forefront of many impressive applications ranging from automated language
    translation ([https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html](https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html))
    and modeling fundamental properties of protein sequences ([https://www.pnas.org/content/118/15/e2016239118.short](https://www.pnas.org/content/118/15/e2016239118.short))
    to creating an AI that helps people write code ([https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer](https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器已经彻底改变了自然语言处理，并在许多引人注目的应用中处于前沿，从自动语言翻译（[https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html](https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html)）到对蛋白质序列的基本属性建模（[https://www.pnas.org/content/118/15/e2016239118.short](https://www.pnas.org/content/118/15/e2016239118.short)）以及创建帮助人们编写代码的AI（[https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer](https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer)）。
- en: In this chapter, you will learn about the basic mechanisms of *attention* and
    *self-attention* and see how they are used in the original transformer architecture.
    Then, equipped with an understanding of how transformers work, we will explore
    some of the most influential NLP models that emerged from this architecture and
    learn how to use a large-scale language model, the so-called BERT model, in PyTorch.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解*注意力*和*自注意力*的基本机制，并看到它们如何在原始变压器架构中使用。然后，掌握了变压器的工作原理后，我们将探索从这种架构中出现的一些最有影响力的NLP模型，并学习如何在PyTorch中使用大规模语言模型，即所谓的BERT模型。
- en: 'We will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Improving RNNs with an attention mechanism
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用注意机制改进RNN
- en: Introducing the stand-alone self-attention mechanism
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入独立的自注意力机制
- en: Understanding the original transformer architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解原始变压器架构
- en: Comparing transformer-based large-scale language models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较基于变压器的大规模语言模型
- en: Fine-tuning BERT for sentiment classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为情感分类微调BERT
- en: Adding an attention mechanism to RNNs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将注意机制添加到RNN中
- en: In this section, we discuss the motivation behind developing an **attention
    mechanism**, which helps predictive models to focus on certain parts of the input
    sequence more than others, and how it was originally used in the context of RNNs.
    Note that this section provides a historical perspective explaining why the attention
    mechanism was developed. If individual mathematical details appear complicated,
    you can feel free to skip over them as they are not needed for the next section,
    explaining the self-attention mechanism for transformers, which is the focus of
    this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论开发**注意机制**背后的动机，这有助于预测模型更专注于输入序列的某些部分，以及它最初是如何在RNN的背景下使用的。请注意，本节提供了一个历史视角，解释了为什么开发注意机制。如果个别数学细节显得复杂，可以放心跳过，因为这些对于接下来的章节并不需要，而后者将重点介绍变压器中的自注意力机制解释。
- en: Attention helps RNNs with accessing information
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意帮助RNN访问信息
- en: 'To understand the development of an attention mechanism, consider the traditional
    RNN model for a **seq2seq task** like language translation, which parses the entire
    input sequence (for instance, one or more sentences) before producing the translation,
    as shown in *Figure 16.1*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解注意机制的发展，请考虑传统的RNN模型，例如用于语言翻译等**seq2seq任务**，它在生成翻译之前会解析整个输入序列（例如一个或多个句子），如*图16.1*所示：
- en: '![Diagram  Description automatically generated](img/B17582_16_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_16_01.png)'
- en: 'Figure 16.1: A traditional RNN encoder-decoder architecture for a seq2seq modeling
    task'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：用于seq2seq建模任务的传统RNN编码器-解码器架构
- en: 'Why is the RNN parsing the whole input sentence before producing the first
    output? This is motivated by the fact that translating a sentence word by word
    would likely result in grammatical errors, as illustrated in *Figure 16.2*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么RNN在生成第一个输出之前要解析整个输入句子？这是因为逐字翻译句子很可能导致语法错误，如图16.2所示：
- en: '![A picture containing text  Description automatically generated](img/B17582_16_02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成带有文字的图片](img/B17582_16_02.png)'
- en: 'Figure 16.2: Translating a sentence word by word can lead to grammatical errors'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：逐字翻译句子可能导致语法错误
- en: However, as illustrated in *Figure 16.2*, one limitation of this seq2seq approach
    is that the RNN is trying to remember the entire input sequence via one single
    hidden unit before translating it. Compressing all the information into one hidden
    unit may cause loss of information, especially for long sequences. Thus, similar
    to how humans translate sentences, it may be beneficial to have access to the
    whole input sequence at each time step.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如图16.2所示，这种seq2seq方法的一个局限性在于RNN试图通过一个单一的隐藏单元记住整个输入序列再进行翻译。将所有信息压缩到一个隐藏单元中可能会导致信息丢失，特别是对于长序列。因此，类似于人类翻译句子的方式，每个时间步骤都可以访问整个输入序列可能是有益的。
- en: In contrast to a regular RNN, an attention mechanism lets the RNN access all
    input elements at each given time step. However, having access to all input sequence
    elements at each time step can be overwhelming. So, to help the RNN focus on the
    most relevant elements of the input sequence, the attention mechanism assigns
    different attention weights to each input element. These attention weights designate
    how important or relevant a given input sequence element is at a given time step.
    For example, revisiting *Figure 16.2*, the words “mir, helfen, zu” may be more
    relevant for producing the output word “help” than the words “kannst, du, Satz.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通的循环神经网络相比，注意力机制允许循环神经网络在每个时间步访问所有输入元素。然而，每个时间步访问所有输入序列元素可能会很复杂。因此，为了帮助循环神经网络集中精力处理输入序列中最相关的元素，注意力机制为每个输入元素分配不同的注意力权重。这些注意力权重指定了在特定时间步骤上给定输入序列元素的重要性或相关性。例如，重新审视*图16.2*，单词“mir,
    helfen, zu”可能比“kannst, du, Satz”对生成输出词“help”更相关。
- en: The next subsection introduces an RNN architecture that was outfitted with an
    attention mechanism to help process long sequences for language translation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下一小节介绍了一种带有注意力机制的RNN架构，以帮助处理用于语言翻译的长序列。
- en: The original attention mechanism for RNNs
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于RNN的原始注意力机制
- en: 'In this subsection, we will summarize the mechanics of the attention mechanism
    that was originally developed for language translation and first appeared in the
    following paper: *Neural Machine Translation by Jointly Learning to Align and
    Translate* by *Bahdanau, D., Cho, K., and Bengio, Y.,* 2014, [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将总结最初用于语言翻译的注意力机制的机制，并首次出现在以下论文中：*Neural Machine Translation by Jointly
    Learning to Align and Translate*，作者为*Bahdanau, D., Cho, K., and Bengio, Y.,* 2014，[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)。
- en: 'Given an input sequence ![](img/B17582_16_001.png), the attention mechanism
    assigns a weight to each element ![](img/B17582_16_002.png) (or, to be more specific,
    its hidden representation) and helps the model identify which part of the input
    it should focus on. For example, suppose our input is a sentence, and a word with
    a larger weight contributes more to our understanding of the whole sentence. The
    RNN with the attention mechanism shown in *Figure 16.3* (modeled after the previously
    mentioned paper) illustrates the overall concept of generating the second output
    word:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个输入序列 ![](img/B17582_16_001.png)，注意力机制为每个元素 ![](img/B17582_16_002.png)（或者更具体地说，它的隐藏表示）分配一个权重，并帮助模型确定它应该专注于输入的哪一部分。例如，假设我们的输入是一个句子，具有较大权重的单词对我们理解整个句子更有贡献。图16.3中带有注意力机制的RNN（模仿前述论文）说明了生成第二个输出词的整体概念：
- en: '![Diagram  Description automatically generated](img/B17582_16_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成描述的图解](img/B17582_16_03.png)'
- en: 'Figure 16.3: RNN with attention mechanism'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：带有注意力机制的RNN
- en: The attention-based architecture depicted in the figure consists of two RNN
    models, which we will explain in the next subsections.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图中描述的基于注意力的架构由两个RNN模型组成，我们将在下一小节中解释。
- en: Processing the inputs using a bidirectional RNN
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用双向RNN处理输入
- en: 'The first RNN (RNN #1) of the attention-based RNN in *Figure 16.3* is a bidirectional
    RNN that generates context vectors, ![](img/B17582_16_003.png). You can think
    of a context vector as an augmented version of the input vector, ![](img/B17582_16_002.png).
    In other words, the ![](img/B17582_16_005.png) input vector also incorporates
    information from all other input elements via an attention mechanism. As we can
    see in *Figure 16.3*, RNN #2 then uses this context vector, prepared by RNN #1,
    to generate the outputs. In the remainder of this subsection, we will discuss
    how RNN #1 works, and we will revisit RNN #2 in the next subsection.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.3* 中基于注意力的 RNN 中的第一个 RNN（RNN #1）是一个双向 RNN，生成上下文向量 ![](img/B17582_16_003.png)。可以将上下文向量视为输入向量
    ![](img/B17582_16_002.png) 的增强版本。换句话说，输入向量 ![](img/B17582_16_002.png) 还通过注意力机制从所有其他输入元素获取信息。如
    *图 16.3* 所示，RNN #2 然后使用由 RNN #1 准备的这个上下文向量生成输出。在本小节的其余部分，我们将讨论 RNN #1 的工作原理，并在下一小节重新审视
    RNN #2。'
- en: 'The bidirectional RNN #1 processes the input sequence *x* in the regular forward
    direction (![](img/B17582_16_006.png)) as well as backward (![](img/B17582_16_007.png)).
    Parsing a sequence in the backward direction has the same effect as reversing
    the original input sequence—think of reading a sentence in reverse order. The
    rationale behind this is to capture additional information since current inputs
    may have a dependence on sequence elements that came either before or after it
    in a sentence, or both.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '双向 RNN #1 处理输入序列 *x* 的正向（![](img/B17582_16_006.png)）和反向（![](img/B17582_16_007.png)）两个方向。以反向顺序解析序列与反转原始输入序列效果相同，可以将其理解为逆序阅读句子。这样做的原理是为了捕捉额外的信息，因为当前的输入可能依赖于句子中之前或之后的序列元素，或者两者都有。'
- en: Consequently, from reading the input sequence twice (that is, forward and backward),
    we have two hidden states for each input sequence element. For instance, for the
    second input sequence element ![](img/B17582_16_008.png), we obtain the hidden
    state ![](img/B17582_16_009.png)from the forward pass and the hidden state ![](img/B17582_16_010.png)
    from the backward pass. These two hidden states are then concatenated to form
    the hidden state ![](img/B17582_16_011.png). For example, if both ![](img/B17582_16_009.png)
    and ![](img/B17582_16_010.png) are 128-dimensional vectors, the concatenated hidden
    state ![](img/B17582_16_011.png) will consist of 256 elements. We can consider
    this concatenated hidden state as the “annotation” of the source word since it
    contains the information of the *j*th word in both directions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过两次读取输入序列（即正向和反向），我们为每个输入序列元素得到两个隐藏状态。例如，对于第二个输入序列元素 ![](img/B17582_16_008.png)，我们从正向传递得到隐藏状态
    ![](img/B17582_16_009.png)，从反向传递得到隐藏状态 ![](img/B17582_16_010.png)。然后，这两个隐藏状态被拼接成隐藏状态
    ![](img/B17582_16_011.png)。例如，如果 ![](img/B17582_16_009.png) 和 ![](img/B17582_16_010.png)
    都是 128 维向量，则拼接后的隐藏状态 ![](img/B17582_16_011.png) 将包含 256 个元素。我们可以将这个拼接的隐藏状态视为源词的“注释”，因为它包含了双向阅读中第
    *j* 个词的信息。
- en: In the next section, we will see how these concatenated hidden states are further
    processed and used by the second RNN to generate the outputs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将看到如何进一步处理和使用第二个 RNN 生成输出的这些拼接隐藏状态。
- en: Generating outputs from context vectors
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从上下文向量生成输出
- en: 'In *Figure 16.3*, we can consider RNN #2 as the main RNN that is generating
    the outputs. In addition to the hidden states, it receives so-called context vectors
    as input. A context vector ![](img/B17582_16_003.png) is a weighted version of
    the concatenated hidden states, ![](img/B17582_16_016.png), which we obtained
    from RNN #1 in the previous subsection. We can compute the context vector of the
    *i*th input as a weighted sum:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在 *图 16.3* 中，我们可以将 RNN #2 视为生成输出的主要 RNN。除了隐藏状态外，它还接收所谓的上下文向量作为输入。上下文向量 ![](img/B17582_16_003.png)
    是拼接隐藏状态 ![](img/B17582_16_016.png) 的加权版本，这些隐藏状态是我们在前一小节从 RNN #1 获取的。我们可以计算第 *i*
    个输入的上下文向量为加权求和：'
- en: '![](img/B17582_16_017.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_017.png)'
- en: Here, ![](img/B17582_16_018.png) represents the attention weights over the input
    sequence ![](img/B17582_16_019.png) in the context of the *i*th input sequence
    element. Note that each *i*th input sequence element has a unique set of attention
    weights. We will discuss the computation of the attention weights ![](img/B17582_16_018.png)
    in the next subsection.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_16_018.png) 表示对输入序列 ![](img/B17582_16_019.png) 的注意力权重，用于第
    *i* 个输入序列元素的上下文。注意，每个第 *i* 个输入序列元素都有一组唯一的注意力权重。我们将在下一小节讨论注意力权重 ![](img/B17582_16_018.png)
    的计算方法。
- en: 'For the remainder of this subsection, let us discuss how the context vectors
    are used via the second RNN in the preceding figure (RNN #2). Just like a vanilla
    (regular) RNN, RNN #2 also uses hidden states. Considering the hidden layer between
    the aforementioned “annotation” and final output, let us denote the hidden state
    at time ![](img/B17582_16_021.png) as ![](img/B17582_16_022.png). Now, RNN #2
    receives the aforementioned context vector ![](img/B17582_16_003.png) at each
    time step *i* as input.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节的其余部分，让我们讨论通过前述图中的第二个RNN（RNN #2）如何使用上下文向量。就像普通的RNN一样，RNN #2也使用隐藏状态。考虑到前述的“注释”和最终输出之间的隐藏层，让我们将时间
    ![](img/B17582_16_021.png) 处的隐藏状态表示为 ![](img/B17582_16_022.png)。现在，RNN #2在每个时间步
    *i* 接收上述的上下文向量 ![](img/B17582_16_003.png) 作为输入。'
- en: In *Figure 16.3*, we saw that the hidden state ![](img/B17582_16_022.png) depends
    on the previous hidden state ![](img/B17582_16_025.png), the previous target word
    ![](img/B17582_16_026.png), and the context vector ![](img/B17582_16_027.png),
    which are used to generate the predicted output ![](img/B17582_16_028.png) for
    target word ![](img/B17852_09_037.png) at time *i*. Note that the sequence vector
    ![](img/B17582_16_030.png) refers to the sequence vector representing the correct
    translation of input sequence ![](img/B17582_16_031.png) that is available during
    training. During training, the true label (word) ![](img/B17852_09_037.png) is
    fed into the next state ![](img/B17582_16_033.png); since this true label information
    is not available for prediction (inference), we feed the predicted output ![](img/B17582_16_028.png)
    instead, as depicted in the previous figure.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图16.3*中，我们看到隐藏状态 ![](img/B17582_16_022.png) 取决于前一个隐藏状态 ![](img/B17582_16_025.png)、前一个目标词
    ![](img/B17582_16_026.png) 和上下文向量 ![](img/B17582_16_027.png)，这些用于生成目标词 ![](img/B17582_16_028.png)
    在时间 *i* 处的预测输出。请注意，序列向量 ![](img/B17582_16_030.png) 指的是代表输入序列 ![](img/B17582_16_031.png)
    的正确翻译的序列向量，在训练期间可用。在训练期间，真实标签（单词） ![](img/B17582_16_037.png) 被馈送到下一个状态 ![](img/B17582_16_033.png)；由于这个真实标签信息在预测（推断）时不可用，我们改为馈送预测输出
    ![](img/B17582_16_028.png)，如前图所示。
- en: 'To summarize what we have just discussed above, the attention-based RNN consists
    of two RNNs. RNN #1 prepares context vectors from the input sequence elements,
    and RNN #2 receives the context vectors as input. The context vectors are computed
    via a weighted sum over the inputs, where the weights are the attention weights
    ![](img/B17582_16_035.png)The next subsection discusses how we compute these attention
    weights.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '总结我们刚刚讨论的内容，基于注意力的RNN由两个RNN组成。RNN #1从输入序列元素准备上下文向量，而RNN #2将上下文向量作为输入接收。上下文向量通过对输入进行加权求和来计算，其中权重是注意力权重
    ![](img/B17582_16_035.png)。下一小节讨论如何计算这些注意力权重。'
- en: Computing the attention weights
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算注意力权重
- en: 'Finally, let us visit the last missing piece in our puzzle—attention weights.
    Because these weights pairwise connect the inputs (annotations) and the outputs
    (contexts), each attention weight ![](img/B17582_16_036.png) has two subscripts:
    *j* refers to the index position of the input and *i* corresponds to the output
    index position. The attention weight ![](img/B17582_16_036.png) is a normalized
    version of the alignment score ![](img/B17582_16_038.png), where the alignment
    score evaluates how well the input around position *j* matches with the output
    at position *i*. To be more specific, the attention weight is computed by normalizing
    the alignment scores as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们访问我们谜题中的最后一块遗失的部分——注意力权重。因为这些权重成对连接输入（注释）和输出（上下文），每个注意力权重 ![](img/B17582_16_036.png)
    都有两个下标：*j* 指的是输入的索引位置，*i* 对应输出的索引位置。注意力权重 ![](img/B17582_16_036.png) 是对齐分数 ![](img/B17582_16_038.png)
    的归一化版本，其中对齐分数评估了位置 *j* 周围的输入与位置 *i* 处的输出匹配的程度。更具体地说，注意力权重通过以下方式计算归一化的对齐分数：
- en: '![](img/B17582_16_039.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_039.png)'
- en: Note that this equation is similar to the softmax function, which we discussed
    in *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*, in the
    section *Estimating class probabilities in multiclass classification via the softmax
    function*. Consequently, the attention weights ![](img/B17582_16_040.png)...![](img/B17582_16_041.png)
    sum up to 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个方程式类似于我们在*第12章*的*PyTorch中的神经网络训练并行化*中讨论过的softmax函数，在*通过softmax函数估计多类分类中的类概率*小节。因此，注意力权重
    ![](img/B17582_16_040.png)...![](img/B17582_16_041.png) 总和为1。
- en: Now, to summarize, we can structure the attention-based RNN model into three
    parts. The first part computes bidirectional annotations of the input. The second
    part consists of the recurrent block, which is very much like the original RNN,
    except that it uses context vectors instead of the original input. The last part
    concerns the computation of the attention weights and context vectors, which describe
    the relationship between each pair of input and output elements.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，总结一下，我们可以将基于注意力的RNN模型结构化为三个部分。第一部分计算输入的双向注释。第二部分包括循环块，与原始RNN非常相似，只是使用上下文向量代替原始输入。最后一部分涉及注意力权重和上下文向量的计算，描述每对输入和输出元素之间的关系。
- en: The transformer architecture also utilizes an attention mechanism, but unlike
    the attention-based RNN, it solely relies on the **self-attention** mechanism
    and does not include the recurrent process found in the RNN. In other words, a
    transformer model processes the whole input sequence all at once instead of reading
    and processing the sequence one element at a time. In the next section, we will
    introduce a basic form of the self-attention mechanism before we discuss the transformer
    architecture in more detail in the following section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构也利用注意力机制，但与基于注意力的RNN不同，它仅依赖于自注意力机制，并且不包括RNN中发现的循环过程。换句话说，变压器模型一次处理整个输入序列，而不是逐个元素地读取和处理序列。在接下来的小节中，我们将介绍自注意力机制的基本形式，然后在下一小节中更详细地讨论变压器架构。
- en: Introducing the self-attention mechanism
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入自注意力机制
- en: In the previous section, we saw that attention mechanisms can help RNNs with
    remembering context when working with long sequences. As we will see in the next
    section, we can have an architecture entirely based on attention, without the
    recurrent parts of an RNN. This attention-based architecture is known as **transformer**,
    and we will discuss it in more detail later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，我们看到注意力机制可以帮助RNN在处理长序列时记住上下文。正如我们将在下一小节中看到的那样，我们可以有一个完全基于注意力而非循环部分的架构。这种基于注意力的架构被称为**变压器**，我们将在后面更详细地讨论它。
- en: In fact, transformers can appear a bit complicated at first glance. So, before
    we discuss transformers in the next section, let us dive into the **self-attention**
    mechanism used in transformers. In fact, as we will see, this self-attention mechanism
    is just a different flavor of the attention mechanism that we discussed in the
    previous section. We can think of the previously discussed attention mechanism
    as an operation that connects two different modules, that is, the encoder and
    decoder of the RNN. As we will see, self-attention focuses only on the input and
    captures only dependencies between the input elements. without connecting two
    modules.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，变压器一开始看起来可能有点复杂。因此，在我们在下一小节讨论变压器之前，让我们先深入探讨变压器中使用的**自注意力**机制。实际上，正如我们将看到的，这种自注意力机制只是我们在前一小节讨论的注意力机制的另一种形式。我们可以将前面讨论的注意力机制视为连接两个不同模块的操作，即RNN的编码器和解码器。正如我们将看到的，自注意力仅关注输入，并且仅捕捉输入元素之间的依赖关系，而不连接两个模块。
- en: In the first subsection, we will introduce a basic form of self-attention without
    any learning parameters, which is very much like a pre-processing step to the
    input. Then in the second subsection, we will introduce the common version of
    self-attention that is used in the transformer architecture and involves learnable
    parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一小节中，我们将介绍一种没有任何学习参数的基本自注意力形式，这非常类似于输入的预处理步骤。然后在第二小节中，我们将介绍变压器架构中使用的常见自注意力版本，涉及可学习参数。
- en: Starting with a basic form of self-attention
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从基本的自注意力开始
- en: To introduce self-attention, let’s assume we have an input sequence of length *T*,
    ![](img/B17582_16_042.png) as well as an output sequence, ![](img/B17582_16_043.png).
    To avoid confusion, we will use ![](img/B17582_16_044.png) as the final output
    of the whole transformer model and ![](img/B17582_16_045.png) as the output of
    the self-attention layer because it is an intermediate step in the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍自注意力，让我们假设我们有一个长度为*T*的输入序列，![](img/B17582_16_042.png)，以及一个输出序列，![](img/B17582_16_043.png)。为避免混淆，我们将使用![](img/B17582_16_044.png)作为整个变压器模型的最终输出，![](img/B17582_16_045.png)作为自注意力层的输出，因为它是模型中的中间步骤。
- en: Each *i*th element in these sequences, ![](img/B17582_16_046.png) and ![](img/B17582_16_047.png),
    are vectors of size *d* (that is, ![](img/B17582_16_048.png)) representing the
    feature information for the input at position *i*, which is similar to RNNs. Then,
    for a seq2seq task, the goal of self-attention is to model the dependencies of
    the current input element to all other input elements. To achieve this, self-attention
    mechanisms are composed of three stages. First, we derive importance weights based
    on the similarity between the current element and all other elements in the sequence.
    Second, we normalize the weights, which usually involves the use of the already
    familiar softmax function. Third, we use these weights in combination with the
    corresponding sequence elements to compute the attention value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些序列中的每个第 *i* 个元素，![](img/B17582_16_046.png) 和 ![](img/B17582_16_047.png)，都是大小为
    *d* 的向量（即 ![](img/B17582_16_048.png)），表示位置 *i* 处输入的特征信息，类似于循环神经网络。然后，对于 seq2seq
    任务，自注意力的目标是建模当前输入元素与序列中所有其他输入元素之间的依赖关系。为了实现这一目标，自注意机制由三个阶段组成。首先，我们根据当前元素与序列中所有其他元素之间的相似性导出重要性权重。其次，我们对权重进行归一化，通常涉及使用已熟悉的
    softmax 函数。第三，我们使用这些权重与相应的序列元素结合计算注意力值。
- en: 'More formally, the output of self-attention, ![](img/B17582_16_049.png) is
    the weighted sum of all *T* input sequences, ![](img/B17582_16_050.png) (where
    ![](img/B17582_16_051.png)). For instance, for the *i*th input element, the corresponding
    output value is computed as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，自注意力的输出，![](img/B17582_16_049.png)，是所有输入序列 *T* 的加权和，![](img/B17582_16_050.png)（其中
    ![](img/B17582_16_051.png)）。例如，对于第 *i* 个输入元素，相应的输出值计算如下：
- en: '![](img/B17582_16_052.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_052.png)'
- en: Hence, we can think of ![](img/B17582_16_047.png) as a context-aware embedding
    vector in input vector ![](img/B17582_16_046.png) that involves all other input
    sequence elements weighted by their respective attention weights. Here, the attention
    weights, ![](img/B17582_16_018.png), are computed based on the similarity between
    the current input element, ![](img/B17582_16_046.png), and all other elements
    in the input sequence, ![](img/B17582_16_057.png). More concretely, this similarity
    is computed in two steps explained in the next paragraphs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将 ![](img/B17582_16_047.png) 视为上下文感知的嵌入向量，在输入向量 ![](img/B17582_16_046.png)
    中涉及所有其他输入序列元素，这些元素根据它们各自的注意力权重 ![](img/B17582_16_018.png) 计算。更具体地说，这种相似性是通过下文中解释的两个步骤来计算的。
- en: 'First, we compute the dot product between the current input element, ![](img/B17582_16_046.png),
    and another element in the input sequence, ![](img/B17582_16_050.png):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算当前输入元素 ![](img/B17582_16_046.png) 与输入序列中另一个元素 ![](img/B17582_16_050.png)
    的点积：
- en: '![](img/B17582_16_060.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_060.png)'
- en: 'Before we normalize the ![](img/B17582_16_061.png) values to obtain the attention
    weights, ![](img/B17582_16_062.png), let’s illustrate how we compute the ![](img/B17582_16_061.png)
    values with a code example. Here, let’s assume we have an input sentence “can
    you help me to translate this sentence” that has already been mapped to an integer
    representation via a dictionary as explained in *Chapter 15*, *Modeling Sequential
    Data Using Recurrent Neural Networks*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们归一化 ![](img/B17582_16_061.png) 值以获得注意力权重 ![](img/B17582_16_062.png) 之前，让我们通过代码示例说明如何计算
    ![](img/B17582_16_061.png) 值。在这里，假设我们有一个输入句子“can you help me to translate this
    sentence”，该句子已经通过字典映射到整数表示，如 *第 15 章，使用循环神经网络建模顺序数据* 中所述：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s also assume that we already encoded this sentence into a real-number
    vector representation via an embedding layer. Here, our embedding size is 16,
    and we assume that the dictionary size is 10\. The following code will produce
    the word embeddings of our eight words:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经通过嵌入层将这句话编码成实数向量表示。在这里，我们的嵌入大小是16，并假设词典大小是10。以下代码将产生我们八个单词的词嵌入：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we can compute ![](img/B17582_16_064.png) as the dot product between the
    *i*th and *j*th word embeddings. We can do this for all ![](img/B17582_16_064.png)
    values as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算 ![](img/B17582_16_064.png) 作为第 *i* 和第 *j* 个词嵌入之间的点积。我们可以对所有 ![](img/B17582_16_064.png)
    值进行如下计算：
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'While the preceding code is easy to read and understand, `for` loops can be
    very inefficient, so let’s compute this using matrix multiplication instead:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述代码易于阅读和理解，`for`循环可能非常低效，因此让我们改用矩阵乘法来计算：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can use the `torch.allclose` function to check that this matrix multiplication
    produces the expected results. If two tensors contain the same values, `torch.allclose`
    returns `True`, as we can see here:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`torch.allclose`函数检查该矩阵乘法是否产生预期结果。如果两个张量包含相同的值，`torch.allclose`将返回`True`，如我们可以看到的那样：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We have learned how to compute the similarity-based weights for the *i*th input
    and all inputs in the sequence (![](img/B17582_16_066.png) to ![](img/B17582_16_067.png)),
    the “raw” weights (![](img/B17582_16_068.png) to ![](img/B17582_16_069.png)).
    We can obtain the attention weights, ![](img/B17582_16_036.png), by normalizing
    the ![](img/B17582_16_071.png) values via the familiar softmax function, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了如何计算基于相似性的第*i*个输入及其序列中所有输入的权重（从![](img/B17582_16_066.png)到![](img/B17582_16_067.png)），“原始”权重（从![](img/B17582_16_068.png)到![](img/B17582_16_069.png)）。我们可以通过常见的softmax函数对![](img/B17582_16_071.png)值进行标准化来获取注意力权重，如下所示：
- en: '![](img/B17582_16_072.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_072.png)'
- en: Notice that the denominator involves a sum over all input elements (![](img/B17582_16_006.png)).
    Hence, due to applying this softmax function, the weights will sum to 1 after
    this normalization, that is,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意分母涉及对所有输入元素的求和（![](img/B17582_16_006.png)）。因此，应用此softmax函数后，权重在标准化后将总和为1，即，
- en: '![](img/B17582_16_074.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_074.png)'
- en: 'We can compute the attention weights using PyTorch’s softmax function as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch的softmax函数计算注意力权重如下：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that `attention_weights` is an ![](img/B17582_16_075.png) matrix, where
    each element represents an attention weight, ![](img/B17582_16_018.png). For instance,
    if we are processing the *i*th input word, the *i*th row of this matrix contains
    the corresponding attention weights for all words in the sentence. These attention
    weights indicate how relevant each word is to the *i*th word. Hence, the columns
    in this attention matrix should sum to 1, which we can confirm via the following
    code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`attention_weights`是一个![](img/B17582_16_075.png)矩阵，其中每个元素表示一个注意力权重，![](img/B17582_16_018.png)。例如，如果我们正在处理第*i*个输入单词，则此矩阵的第*i*行包含句子中所有单词的对应注意力权重。这些注意力权重指示每个单词与第*i*个单词的相关性。因此，此注意力矩阵中的列应该总和为1，我们可以通过以下代码确认：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now that we have seen how to compute the attention weights, let us recap and
    summarize the three main steps behind the self-attention operation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何计算注意力权重，让我们回顾和总结自注意操作的三个主要步骤：
- en: For a given input element, ![](img/B17582_16_046.png), and each *j*th element
    in the set {1, ..., *T*}, compute the dot product, ![](img/B17582_16_078.png)
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的输入元素，![](img/B17582_16_046.png)，以及集合{1, ..., *T*}中的每个第*j*个元素，计算点积，![](img/B17582_16_078.png)
- en: Obtain the attention weight, ![](img/B17582_16_018.png), by normalizing the
    dot products using the softmax function
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用softmax函数对点积进行标准化来获取注意力权重，![](img/B17582_16_018.png)
- en: Compute the output, ![](img/B17582_16_047.png), as the weighted sum over the
    entire input sequence: ![](img/B17582_16_081.png)
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出，![](img/B17582_16_047.png)，作为整个输入序列的加权和：![](img/B17582_16_081.png)
- en: 'These steps are further illustrated in *Figure 16.4*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤在图16.4中进一步说明：
- en: '![Chart, diagram, box and whisker chart  Description automatically generated](img/B17582_16_04.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图表、图示、箱须图 描述自动生成](img/B17582_16_04.png)'
- en: 'Figure 16.4: A basic self-attention process for illustration purposes'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：用于说明目的的基本自注意过程
- en: 'Lastly, let us see a code example for computing the context vectors, ![](img/B17582_16_047.png),
    as the attention-weighted sum of the inputs (step 3 in *Figure 16.4*). In particular,
    let’s assume we are computing the context vector for the second input word, that
    is, ![](img/B17582_16_083.png):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看一个用于计算上下文向量![](img/B17582_16_047.png)的代码示例，作为输入的注意力加权和（图16.4中的步骤3）。特别是，让我们假设我们正在计算第二个输入单词的上下文向量，即![](img/B17582_16_083.png)：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Again, we can achieve this more efficiently by using matrix multiplication.
    Using the following code, we are computing the context vectors for all eight input
    words:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以通过矩阵乘法更高效地实现这一点。使用以下代码，我们正在计算所有八个输入单词的上下文向量：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similar to the input word embeddings stored in `embedded_sentence`, the `context_vectors`
    matrix has dimensionality ![](img/B17582_16_084.png). The second row in this matrix
    contains the context vector for the second input word, and we can check the implementation
    using `torch.allclose()` again:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与存储在`embedded_sentence`中的输入单词嵌入类似，`context_vectors`矩阵具有维度![](img/B17582_16_084.png)。此矩阵中的第二行包含第二个输入单词的上下文向量，并且我们可以再次使用`torch.allclose()`检查其实现：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we can see, the manual `for` loop and matrix computations of the second context
    vector yielded the same results.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，第二个上下文向量的手动 `for` 循环和矩阵计算产生了相同的结果。
- en: This section implemented a basic form of self-attention, and in the next section,
    we will modify this implementation using learnable parameter matrices that can
    be optimized during neural network training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本节实现了自注意力的基本形式，而在下一节中，我们将修改这一实现，使用可在神经网络训练期间优化的可学习参数矩阵。
- en: 'Parameterizing the self-attention mechanism: scaled dot-product attention'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数化自注意力机制：缩放点积注意力
- en: 'Now that you have been introduced to the basic concept behind self-attention,
    this subsection summarizes the more advanced self-attention mechanism called **scaled
    dot-product attention** that is used in the transformer architecture. Note that
    in the previous subsection, we did not involve any learnable parameters when computing
    the outputs. In other words, using the previously introduced basic self-attention
    mechanism, the transformer model is rather limited regarding how it can update
    or change the attention values during model optimization for a given sequence.
    To make the self-attention mechanism more flexible and amenable to model optimization,
    we will introduce three additional weight matrices that can be fit as model parameters
    during model training. We denote these three weight matrices as ![](img/B17582_16_085.png),
    ![](img/B17582_16_086.png), and ![](img/B17582_16_087.png). They are used to project
    the inputs into query, key, and value sequence elements, as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了自注意力背后的基本概念，本小节总结了更先进的自注意力机制，称为**缩放点积注意力**，这在变压器架构中被使用。请注意，在前一小节中，在计算输出时我们没有涉及任何可学习的参数。换句话说，使用先前介绍的基本自注意力机制时，变压器模型在如何在给定序列的模型优化过程中更新或更改注意力值方面是相当受限的。为了使自注意力机制更加灵活且有利于模型优化，我们将引入三个额外的权重矩阵，在模型训练过程中可以作为模型参数拟合。我们将这三个权重矩阵表示为 ![](img/B17582_16_085.png)，![](img/B17582_16_086.png)，和
    ![](img/B17582_16_087.png)。它们用于将输入投影到查询、键和值序列元素，如下所示：
- en: '**Query sequence**: ![](img/B17582_16_088.png) for ![](img/B17582_16_089.png)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询序列**：![](img/B17582_16_088.png) 用于 ![](img/B17582_16_089.png)'
- en: '**Key sequence**:![](img/B17582_16_090.png) for ![](img/B17582_16_089.png)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键序列**：![](img/B17582_16_090.png) 用于 ![](img/B17582_16_089.png)'
- en: '**Value sequence**: ![](img/B17582_16_092.png) for ![](img/B17582_16_093.png)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值序列**：![](img/B17582_16_092.png) 用于 ![](img/B17582_16_093.png)'
- en: '*Figure 16.5* illustrates how these individual components are used to compute
    the context-aware embedding vector corresponding to the second input element:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.5* 展示了这些单独组件如何用于计算与第二输入元素对应的上下文感知嵌入向量：'
- en: '![Diagram  Description automatically generated](img/B17582_16_05.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_16_05.png)'
- en: 'Figure 16.5: Computing the context-aware embedding vector of the second sequence
    element'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5：计算第二序列元素的上下文感知嵌入向量
- en: '**Query, key, and value terminology**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**查询、键和值术语**'
- en: The terms query, key, and value that were used in the original transformer paper
    are inspired by information retrieval systems and databases. For example, if we
    enter a query, it is matched against the key values for which certain values are
    retrieved.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始变压器论文中使用的查询、键和值术语灵感来自信息检索系统和数据库。例如，如果我们输入一个查询，它将与键值匹配，其中某些值将被检索出来。
- en: 'Here, both ![](img/B17582_16_094.png) and ![](img/B17582_16_095.png) are vectors
    of size ![](img/B17582_16_096.png). Therefore, the projection matrices ![](img/B17582_16_085.png) and ![](img/B17582_16_098.png) have
    the shape ![](img/B17582_16_099.png), while ![](img/B17582_16_100.png) has the
    shape ![](img/B17582_16_101.png). (Note that ![](img/B17582_16_102.png) is the
    dimensionality of each word vector, ![](img/B17582_16_046.png).) For simplicity,
    we can design these vectors to have the same shape, for example, using ![](img/B17582_16_104.png).
    To provide additional intuition via code, we can initialize these projection matrices
    as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_16_094.png) 和 ![](img/B17582_16_095.png) 都是大小为 ![](img/B17582_16_096.png)
    的向量。因此，投影矩阵 ![](img/B17582_16_085.png) 和 ![](img/B17582_16_098.png) 的形状为 ![](img/B17582_16_099.png)，而
    ![](img/B17582_16_100.png) 的形状为 ![](img/B17582_16_101.png)。（注意，![](img/B17582_16_102.png)
    是每个单词向量的维度，![](img/B17582_16_046.png)。）为简单起见，我们可以设计这些向量具有相同的形状，例如使用 ![](img/B17582_16_104.png)。为了通过代码提供额外的直觉，我们可以初始化这些投影矩阵如下：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using the query projection matrix, we can then compute the query sequence.
    For this example, consider the second input element, ![](img/B17582_16_046.png),
    as our query, as illustrated in *Figure 16.5*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用查询投影矩阵，我们可以计算查询序列。对于此示例，将第二个输入元素，![](img/B17582_16_046.png)，作为我们的查询，如*图 16.5*所示：
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In a similar fashion, we can compute the key and value sequences, ![](img/B17582_16_095.png)and
    ![](img/B17582_16_107.png):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们可以计算关键序列和值序列，![](img/B17582_16_095.png)和![](img/B17582_16_107.png)：
- en: '[PRE12]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'However, as we can see from *Figure 16.5*, we also need the key and value sequences
    for all other input elements, which we can compute as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们从*图 16.5*中看到的那样，我们还需要计算所有其他输入元素的关键序列和值序列，计算方法如下：
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the key matrix, the *i*th row corresponds to the key sequence of the *i*th
    input element, and the same applies to the value matrix. We can confirm this by
    using `torch.allclose()` again, which should return `True`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在关键矩阵中，第 *i* 行对应于第 *i* 个输入元素的关键序列，值矩阵也是如此。我们可以再次使用 `torch.allclose()` 来确认这一点，它应该返回
    `True`：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the previous section, we computed the unnormalized weights, ![](img/B17582_16_108.png),
    as the pairwise dot product between the given input sequence element, ![](img/B17582_16_046.png),
    and the *j*th sequence element, ![](img/B17582_16_050.png). Now, in this parameterized
    version of self-attention, we compute ![](img/B17582_16_108.png) as the dot product
    between the query and key:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们计算了未归一化权重，![](img/B17582_16_108.png)，作为给定输入序列元素，![](img/B17582_16_046.png)，和第
    *j* 个序列元素，![](img/B17582_16_050.png)，之间的成对点积。现在，在这个参数化的自注意力版本中，我们将 ![](img/B17582_16_108.png)
    计算为查询和关键之间的点积：
- en: '![](img/B17582_16_112.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_112.png)'
- en: 'For example, the following code computes the unnormalized attention weight,
    ![](img/B17582_16_113.png), that is, the dot product between our query and the
    third input sequence element:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码计算了未归一化的注意力权重，![](img/B17582_16_113.png)，即我们的查询与第三个输入序列元素之间的点积：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Since we will be needing these later, we can scale up this computation to all
    keys:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们稍后将需要这些，我们可以将此计算扩展到所有关键序列：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The next step in self-attention is to go from the unnormalized attention weights,
    ![](img/B17582_16_071.png), to the normalized attention weights, ![](img/B17582_16_036.png),
    using the softmax function. We can then further use ![](img/B17582_16_116.png)
    to scale ![](img/B17582_16_071.png) before normalizing it via the softmax function,
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力的下一步是从未归一化的注意力权重，![](img/B17582_16_071.png)，转换为归一化的注意力权重，![](img/B17582_16_036.png)，使用softmax函数。然后我们可以进一步使用 ![](img/B17582_16_116.png) 来缩放 ![](img/B17582_16_071.png)，然后通过softmax函数进行归一化，如下所示：
- en: '![](img/B17582_16_118.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_118.png)'
- en: Note that scaling ![](img/B17582_16_071.png) by ![](img/B17582_16_116.png),
    where typically ![](img/B17582_16_121.png), ensures that the Euclidean length
    of the weight vectors will be approximately in the same range.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过缩放 ![](img/B17582_16_071.png) 乘以 ![](img/B17582_16_116.png)，其中通常 ![](img/B17582_16_121.png)，确保权重向量的欧几里得长度大致处于相同的范围内。
- en: 'The following code is for implementing this normalization to compute the attention
    weights for the entire input sequence with respect to the second input element
    as the query:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于实现此归一化，以计算关于第二个输入元素作为查询的整个输入序列的注意力权重：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, the output is a weighted average of value sequences: ![](img/B17582_16_122.png),
    which can be implemented as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出是值序列的加权平均值：![](img/B17582_16_122.png)，可以按以下方式实现：
- en: '[PRE18]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this section, we introduced a self-attention mechanism with trainable parameters
    that lets us compute context-aware embedding vectors by involving all input elements,
    which are weighted by their respective attention scores. In the next section,
    we will learn about the transformer architecture, a neural network architecture
    centered around the self-attention mechanism introduced in this section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一个带有可训练参数的自注意力机制，它让我们能够通过涉及所有输入元素的加权注意力分数来计算上下文感知嵌入向量。在接下来的一节中，我们将学习变压器架构，这是围绕本节介绍的自注意力机制的神经网络架构。
- en: 'Attention is all we need: introducing the original transformer architecture'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力是我们所需的一切：介绍原始变压器架构
- en: Interestingly, the original transformer architecture is based on an attention
    mechanism that was first used in an RNN. Originally, the intention behind using
    an attention mechanism was to improve the text generation capabilities of RNNs
    when working with long sentences. However, only a few years after experimenting
    with attention mechanisms for RNNs, researchers found that an attention-based
    language model was even more powerful when the recurrent layers were deleted.
    This led to the development of the **transformer architecture**, which is the
    main topic of this chapter and the remaining sections.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，最初的Transformer架构基于一个注意力机制，这个机制最初是在RNN中使用的。最初使用注意力机制的目的是在处理长句子时提高RNN的文本生成能力。然而，仅仅几年后，在为RNN尝试注意力机制后，研究人员发现，在删除循环层后，基于注意力的语言模型甚至更强大。这导致了Transformer架构的发展，这也是本章和后续部分的主题。
- en: The transformer architecture was first proposed in the NeurIPS 2017 paper *Attention
    Is All You Need* by *A. Vaswani* and colleagues ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    Thanks to the self-attention mechanism, a transformer model can capture long-range
    dependencies among the elements in an input sequence—in an NLP context; for example,
    this helps the model better “understand” the meaning of an input sentence.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构最初是由A. Vaswani及其同事在NeurIPS 2017论文“Attention Is All You Need”中提出的（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）。由于自注意力机制，Transformer模型能够捕捉输入序列中元素之间的长距离依赖关系，例如在NLP上下文中，这有助于模型更好地“理解”输入句子的含义。
- en: 'Although this transformer architecture was originally designed for language
    translation, it can be generalized to other tasks such as English constituency
    parsing, text generation, and text classification. Later, we will discuss popular
    language models, such as BERT and GPT, which were derived from this original transformer
    architecture. *Figure 16.6*, which we adapted from the original transformer paper,
    illustrates the main architecture and components we will be discussing in this
    section:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种Transformer架构最初是为语言翻译设计的，但可以推广到其他任务，如英语成分解析、文本生成和文本分类。稍后，我们将讨论从这种原始Transformer架构衍生出的流行语言模型，如BERT和GPT。我们从原始Transformer论文中修改的*图16.6*，展示了我们将在本节讨论的主要架构和组件：
- en: '![](img/B17582_16_06.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_06.png)'
- en: 'Figure 16.6: The original transformer architecture'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.6：原始的Transformer架构
- en: 'In the following subsections, we go over this original transformer model step
    by step, by decomposing it into two main blocks: an encoder and a decoder. The
    encoder receives the original sequential input and encodes the embeddings using
    a multi-head self-attention module. The decoder takes in the processed input and
    outputs the resulting sequence (for instance, the translated sentence) using a
    *masked* form of self-attention.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们逐步详细介绍这个原始Transformer模型，将其分解为两个主要模块：编码器和解码器。编码器接收原始顺序输入并使用多头自注意力模块编码嵌入。解码器接收处理后的输入，并使用*掩码*形式的自注意力输出结果序列（例如翻译后的句子）。
- en: Encoding context embeddings via multi-head attention
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过多头注意力编码上下文嵌入
- en: The overall goal of the **encoder** block is to take in a sequential input ![](img/B17582_16_123.png)
    and map it into a continuous representation ![](img/B17582_16_124.png) that is
    then passed on to the decoder.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**块的总体目标是接收顺序输入 ![](img/B17582_16_123.png)，并将其映射到连续表示 ![](img/B17582_16_124.png)，然后传递给解码器。'
- en: 'The encoder is a stack of six identical layers. Six is not a magic number here
    but merely a hyperparameter choice made in the original transformer paper. You
    can adjust the number of layers according to the model performance. Inside each
    of these identical layers, there are two sublayers: one computes the multi-head
    self-attention, which we will discuss below, and the other one is a fully connected
    layer, which you have already encountered in previous chapters.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器是六个相同层的堆叠。这里的六不是一个魔法数字，而只是原始Transformer论文中的超参数选择。您可以根据模型性能调整层数。在这些相同层的每一层中，有两个子层：一个计算多头自注意力，我们将在下面讨论；另一个是全连接层，您在前几章已经遇到过。
- en: Let’s first talk about the **multi-head self-attention**, which is a simple
    modification of scaled dot-product attention covered earlier in this chapter.
    In the scaled dot-product attention, we used three matrices (corresponding to
    query, value, and key) to transform the input sequence. In the context of multi-head
    attention, we can think of this set of three matrices as one attention *head*.
    As indicated by its name, in multi-head attention, we now have multiple of such
    heads (sets of query, value, and key matrices) similar to how convolutional neural
    networks can have multiple kernels.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先讨论**多头自注意力**，这是对前文中介绍的缩放点积注意力的简单修改。在缩放点积注意力中，我们使用三个矩阵（对应查询、值和键）来转换输入序列。在多头注意力的背景下，我们可以将这组三个矩阵看作一个注意力*头*。正如其名称所示，在多头注意力中，我们现在有多个这样的头（一组查询、值和键矩阵），类似于卷积神经网络可以具有多个卷积核。
- en: To explain the concept of multi-head self-attention with ![](img/B17582_16_125.png)
    heads in more detail, let’s break it down into the following steps.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地解释具有 ![](img/B17582_16_125.png) 个头的多头自注意力的概念，让我们分解为以下步骤。
- en: 'First, we read in the sequential input ![](img/B17582_16_123.png). Suppose
    each element is embedded by a vector of length *d*. Here, the input can be embedded
    into a ![](img/B17582_16_127.png) matrix. Then, we create ![](img/B17582_16_125.png)
    sets of the query, key, and value learning parameter matrices:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们读取顺序输入 ![](img/B17582_16_123.png)。假设每个元素被一个长度为 *d* 的向量嵌入。在这里，输入可以嵌入成一个
    ![](img/B17582_16_127.png) 矩阵。然后，我们创建 ![](img/B17582_16_125.png) 组查询、键和值学习参数矩阵：
- en: '![](img/B17582_16_129.png)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_16_129.png)'
- en: '![](img/B17582_16_130.png)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_16_130.png)'
- en: '...'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '...'
- en: '![](img/B17582_16_131.png)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B17582_16_131.png)'
- en: Because we are using these weight matrices to project each element ![](img/B17582_16_046.png)
    for the required dimension-matching in the matrix multiplications, both ![](img/B17582_16_133.png)
    and ![](img/B17582_16_134.png) have the shape ![](img/B17582_16_099.png), and
    ![](img/B17582_16_136.png) has the shape ![](img/B17582_16_101.png). As a result,
    both resulting sequences, query and key, have length ![](img/B17582_16_096.png),
    and the resulting value sequence has length ![](img/B17582_16_139.png). In practice,
    people often choose ![](img/B17582_16_140.png) for simplicity.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用这些权重矩阵来投影每个元素 ![](img/B17582_16_046.png) 以便在矩阵乘法中进行必要的维度匹配，因此 ![](img/B17582_16_133.png)
    和 ![](img/B17582_16_134.png) 的形状都是 ![](img/B17582_16_099.png)，而 ![](img/B17582_16_136.png)
    的形状是 ![](img/B17582_16_101.png)。因此，生成的查询和键序列的长度均为 ![](img/B17582_16_096.png)，生成的值序列的长度为
    ![](img/B17582_16_139.png)。实际应用中，人们通常简化选择 ![](img/B17582_16_140.png)。
- en: 'To illustrate the multi-head self-attention stack in code, first consider how
    we created the single query projection matrix in the previous subsection, *Parameterizing
    the self-attention mechanism: scaled dot-product attention*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要在代码中说明多头自注意力堆栈，首先考虑我们如何在前一小节中创建单一查询投影矩阵，*参数化自注意力机制：缩放点积注意力*：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, assume we have eight attention heads similar to the original transformer,
    that is, ![](img/B17582_16_141.png):'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有八个类似于原始变压器的注意力头，即 ![](img/B17582_16_141.png)：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we can see in the code, multiple attention heads can be added by simply adding
    an additional dimension.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如代码中所示，可以通过简单地增加一个额外的维度来添加多个注意力头。
- en: '**Splitting data across multiple attention heads**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**将数据分配到多个注意力头**'
- en: In practice, rather than having a separate matrix for each attention head, transformer
    implementations use a single matrix for all attention heads. The attention heads
    are then organized into logically separate regions in this matrix, which can be
    accessed via Boolean masks. This makes it possible to implement multi-head attention
    more efficiently because multiple matrix multiplications can be implemented as
    a single matrix multiplication instead. However, for simplicity, we are omitting
    this implementation detail in this section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，转换器实现中并不是为每个注意力头单独使用一个矩阵，而是使用一个矩阵来处理所有注意力头。然后，这些注意力头在矩阵中被组织成逻辑上的独立区域，可以通过布尔掩码访问。这样可以更有效地实现多头注意力，因为多个矩阵乘法可以合并为单个矩阵乘法。然而，在本节中为简化起见，我们省略了这个实现细节。
- en: 'After initializing the projection matrices, we can compute the projected sequences
    similar to how it’s done in scaled dot-product attention. Now, instead of computing
    one set of query, key, and value sequences, we need to compute *h* sets of them.
    More formally, for example, the computation involving the query projection for
    the *i*th data point in the *j*th head can be written as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化投影矩阵后，我们可以计算投影序列，类似于缩放点积注意力中的方式。现在，我们不是计算一组查询、键和值序列，而是需要计算 *h* 组。更正式地说，例如，涉及到第
    *i* 个数据点在第 *j* 个头部的查询投影的计算可以写成如下形式：
- en: '![](img/B17582_16_142.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_142.png)'
- en: We then repeat this computation for all heads ![](img/B17582_16_143.png).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为所有头部 ![](img/B17582_16_143.png) 重复这个计算。
- en: 'In code, this looks like the following for the second input word as the query:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这看起来像是对第二个输入词作为查询的情况：
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `multihead_query_2` matrix has eight rows, where each row corresponds to
    the *j*th attention head.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`multihead_query_2` 矩阵有八行，每行对应第 *j* 个注意力头。'
- en: 'Similarly, we can compute key and value sequences for each head:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以计算每个头部的键和值序列：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The code output shows the key vector of the second input element via the third
    attention head.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出显示了第三个注意力头中第二个输入元素的键向量。
- en: 'However, remember that we need to repeat the key and value computations for
    all input sequence elements, not just `x_2`—we need this to compute self-attention
    later. A simple and illustrative way to do this is by expanding the input sequence
    embeddings to size 8 as the first dimension, which is the number of attention
    heads. We use the `.repeat()` method for this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请记住，我们需要为所有输入序列元素重复键和值的计算，而不仅仅是 `x_2` —— 我们需要这样做来后续计算自注意力。一个简单且生动的方法是将输入序列嵌入扩展到大小为
    8 作为第一维度，即注意力头的数量。我们使用 `.repeat()` 方法来实现这一点：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we can have a batch matrix multiplication, via `torch.bmm()`, with the
    attention heads to compute all keys:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过 `torch.bmm()` 进行批次矩阵乘法，使用注意力头来计算所有键：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this code, we now have a tensor that refers to the eight attention heads
    in its first dimension. The second and third dimensions refer to the embedding
    size and the number of words, respectively. Let us swap the second and third dimensions
    so that the keys have a more intuitive representation, that is, the same dimensionality
    as the original input sequence `embedded_sentence`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们现在有一个张量，其第一维度指向八个注意力头。第二和第三维度分别指向嵌入大小和单词数量。让我们交换第二和第三维度，以便键具有更直观的表示方式，即与原始输入序列
    `embedded_sentence` 相同的维度：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After rearranging, we can access the second key value in the second attention
    head as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排列后，我们可以按如下方式访问第二个注意力头中的第二个键值：
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can see that this is the same key value that we got via `multihead_key_2[2]`
    earlier, which indicates that our complex matrix manipulations and computations
    are correct. So, let’s repeat it for the value sequences:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这与我们之前通过 `multihead_key_2[2]` 得到的键值是相同的，这表明我们复杂的矩阵操作和计算是正确的。因此，让我们重复一下值序列的计算：
- en: '[PRE27]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We follow the steps of the single head attention calculation to calculate the
    context vectors as described in the *Parameterizing the self-attention mechanism:
    scaled dot-product attention* section. We will skip the intermediate steps for
    brevity and assume that we have computed the context vectors for the second input
    element as the query and the eight different attention heads, which we represent
    as `multihead_z_2` via random data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照单头注意力计算的步骤来计算上下文向量，如 *自注意机制的参数化：缩放点积注意力* 部分所述。出于简洁起见，我们将跳过中间步骤，并假设我们已经计算了第二个输入元素的上下文向量作为查询和八个不同注意力头，我们将其表示为
    `multihead_z_2`，通过随机数据：
- en: '[PRE28]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that the first dimension indexes over the eight attention heads, and the
    context vectors, similar to the input sentences, are 16-dimensional vectors. If
    this appears complicated, think of `multihead_z_2` as eight copies of the ![](img/B17582_16_144.png)
    shown in *Figure 16.5*; that is, we have one ![](img/B17582_16_144.png) for each
    of the eight attention heads.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一维度索引了八个注意力头，上下文向量类似于输入句子，是 16 维向量。如果这看起来很复杂，请将 `multihead_z_2` 视为 *图 16.5*
    中显示的 ![](img/B17582_16_144.png) 的八个副本；也就是说，我们为每个注意力头有一个 ![](img/B17582_16_144.png)。
- en: 'Then, we concatenate these vectors into one long vector of length ![](img/B17582_16_146.png)
    and use a linear projection (via a fully connected layer) to map it back to a
    vector of length ![](img/B17582_16_147.png). This process is illustrated in *Figure
    16.7*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些向量连接成一个长度为 ![](img/B17582_16_146.png) 的长向量，并使用线性投影（通过全连接层）将其映射回长度为 ![](img/B17582_16_147.png)
    的向量。这个过程在 *图 16.7* 中有所说明：
- en: '![Diagram  Description automatically generated](img/B17582_16_07.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_16_07.png)'
- en: 'Figure 16.7: Concatenating the scaled dot-product attention vectors into one
    vector and passing it through a linear projection'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.7：将缩放点积注意力向量连接成一个向量并通过线性投影传递
- en: 'In code, we can implement the concatenation and squashing as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以按以下方式实现连接和压缩：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To summarize, multi-head self-attention is repeating the scaled dot-product
    attention computation multiple times in parallel and combining the results. It
    works very well in practice because the multiple heads help the model to capture
    information from different parts of the input, which is very similar to how the
    multiple kernels produce multiple channels in a convolutional network, where each
    channel can capture different feature information. Lastly, while multi-head attention
    sounds computationally expensive, note that the computation can all be done in
    parallel because there are no dependencies between the multiple heads.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，多头自注意力是并行多次重复缩放点积注意力计算，并将结果合并。它在实践中表现非常好，因为多头帮助模型从输入的不同部分捕获信息，这与卷积网络中多个核产生多个通道以捕获不同特征信息的方式非常相似。最后，虽然多头注意力听起来计算量昂贵，但请注意计算可以全部并行进行，因为多头之间没有依赖关系。
- en: 'Learning a language model: decoder and masked multi-head attention'
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习语言模型：解码器和掩码多头注意力
- en: Similar to the encoder, the **decoder** also contains several repeated layers.
    Besides the two sublayers that we have already introduced in the previous encoder
    section (the multi-head self-attention layer and fully connected layer), each
    repeated layer also contains a masked multi-head attention sublayer.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与编码器类似，**解码器**也包含几个重复的层。除了我们在前述编码器部分介绍的两个子层（多头自注意力层和全连接层）之外，每个重复层还包含一个掩码多头注意力子层。
- en: 'Masked attention is a variation of the original attention mechanism, where
    masked attention only passes a limited input sequence into the model by “masking”
    out a certain number of words. For example, if we are building a language translation
    model with a labeled dataset, at sequence position *i* during the training procedure,
    we only feed in the correct output words from positions 1,…,*i*-1\. All other
    words (for instance, those that come after the current position) are hidden from
    the model to prevent the model from “cheating.” This is also consistent with the
    nature of text generation: although the true translated words are known during
    training, we know nothing about the ground truth in practice. Thus, we can only
    feed the model the solutions to what it has already generated, at position *i*.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码注意力是原始注意力机制的一种变体，其中掩码注意力只通过“掩码”屏蔽掉一定数量的词来将有限的输入序列传递给模型。例如，在使用标记数据集构建语言翻译模型时，在训练过程中的序列位置
    *i*，我们只传递来自位置 1 到 *i*-1 的正确输出词。所有其他词（例如，当前位置之后的词）对于模型是隐藏的，以防止模型“作弊”。这也与文本生成的性质一致：虽然在训练过程中我们知道真实的翻译词，但在实际应用中我们对地面真相一无所知。因此，我们只能将模型已经生成的解决方案传递给它，在位置
    *i*。
- en: '*Figure 16.8* illustrates how the layers are arranged in the decoder block:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.8* 说明了解码器块中层的排列方式：'
- en: '![Diagram  Description automatically generated](img/B17582_16_08.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_16_08.png)'
- en: 'Figure 16.8: Layer arrangement in the decoder part'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8：解码器部分中的层排列
- en: 'First, the previous output words (output embeddings) are passed into the masked
    multi-head attention layer. Then, the second layer receives both the encoded inputs
    from the encoder block and the output of the masked multi-head attention layer
    into a multi-head attention layer. Finally, we pass the multi-head attention outputs
    into a fully connected layer that generates the overall model output: a probability
    vector corresponding to the output words.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将先前的输出词（输出嵌入）传递到掩码多头注意力层。然后，第二层同时接收来自编码器块的编码输入和掩码多头注意力层的输出，传递到多头注意力层。最后，我们将多头注意力的输出传递到一个全连接层，生成整体模型输出：与输出词对应的概率向量。
- en: Note that we can use an argmax function to obtain the predicted words from these
    word probabilities similar to the overall approach we took in the recurrent neural
    network in *Chapter 15*, *Modeling Sequential Data Using Recurrent Neural Networks*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以使用 argmax 函数从这些单词概率中获取预测单词，这与我们在*第15章* *使用递归神经网络建模序列数据*中采用的整体方法类似。
- en: Comparing the decoder with the encoder block, the main difference is the range
    of sequence elements that the model can attend to. In the encoder, for each given
    word, the attention is calculated across all the words in a sentence, which can
    be considered as a form of bidirectional input parsing. The decoder also receives
    the bidirectionally parsed inputs from the encoder. However, when it comes to
    the output sequence, the decoder only considers those elements that are preceding
    the current input position, which can be interpreted as a form of unidirectional
    input parsing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 将解码器与编码器块进行比较，主要区别在于模型可以关注的序列元素范围。在编码器中，对于每个给定的单词，都会计算整个句子中所有单词的注意力，这可以被视为一种双向输入解析形式。解码器还接收来自编码器的双向解析输入。然而，在输出序列方面，解码器仅考虑那些在当前输入位置之前的元素，这可以被解释为一种单向输入解析形式。
- en: 'Implementation details: positional encodings and layer normalization'
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现细节：位置编码和层归一化
- en: In this subsection, we will discuss some of the implementation details of transformers
    that we have glanced over so far but are worth mentioning.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将讨论一些转换器的实现细节，这些细节我们迄今为止只是粗略地提及，但是值得一提。
- en: 'First, let’s consider the **positional encodings** that were part of the original
    transformer architecture from *Figure 16.6*. Positional encodings help with capturing
    information about the input sequence ordering and are a crucial part of transformers
    because both scaled dot-product attention layers and fully connected layers are
    permutation-invariant. This means, without positional encoding, the order of words
    is ignored and does not make any difference to the attention-based encodings.
    However, we know that word order is essential for understanding a sentence. For
    example, consider the following two sentences:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑原始转换器架构中的**位置编码**，这些编码是来自*图16.6*的一部分。位置编码有助于捕获输入序列顺序信息，对于转换器而言至关重要，因为缩放的点积注意力层和全连接层都是置换不变的。这意味着，没有位置编码，单词的顺序会被忽略，并且对基于注意力的编码没有任何影响。然而，我们知道单词顺序对于理解一个句子是至关重要的。例如，考虑以下两个句子：
- en: Mary gives John a flower
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 玛丽给约翰一朵花
- en: John gives Mary a flower
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 约翰给玛丽一朵花
- en: The words occurring in the two sentences are exactly the same; the meanings,
    however, are very different.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 两个句子中出现的单词完全相同；然而，它们的含义却大不相同。
- en: 'Transformers enable the same words at different positions to have slightly
    different encodings by adding a vector of small values to the input embeddings
    at the beginning of the encoder and decoder blocks. In particular, the original
    transformer architecture uses a so-called sinusoidal encoding:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器通过在编码器和解码器块的开头向输入嵌入添加一个小值向量，使同一单词在不同位置具有略微不同的编码。特别地，原始的转换器架构使用所谓的正弦编码：
- en: '![](img/B17582_16_148.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_148.png)'
- en: '![](img/B17582_16_149.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_16_149.png)'
- en: Here ![](img/B17582_16_021.png) is the position of the word and *k* denotes
    the length of the encoding vector, where we choose *k* to have the same dimension
    as the input word embeddings so that the positional encoding and word embeddings
    can be added together. Sinusoidal functions are used to prevent positional encodings
    from becoming too large. For instance, if we used absolute position 1,2,3…, *n*
    to be positional encodings, they would dominate the word encoding and make the
    word embedding values negligible.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![](img/B17582_16_021.png) 是单词的位置，*k* 表示编码向量的长度，我们选择 *k* 与输入单词嵌入的维度相同，以便将位置编码和单词嵌入相加。使用正弦函数可以防止位置编码过大。例如，如果我们使用绝对位置
    1,2,3... *n* 作为位置编码，它们会主导单词编码并使单词嵌入值变得可以忽略。
- en: In general, there are two types of positional encodings, an *absolute* one (as
    shown in the previous formula) and a *relative* one. The former will record absolute
    positions of words and is sensitive to word shifts in a sentence. That is to say,
    absolute positional encodings are fixed vectors for each given position. On the
    other hand, relative encodings only maintain the relative position of words and
    are invariant to sentence shift.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，有两种类型的位置编码，一种是*绝对*的（如前面的公式所示），另一种是*相对*的。前者记录单词的绝对位置，并对句子中的单词移动敏感。也就是说，绝对位置编码是每个给定位置的固定向量。另一方面，相对编码仅保持单词的相对位置，对句子移动是不变的。
- en: 'Next, let’s look at the **layer normalization** mechanism, which was first
    introduced by J. Ba, J.R. Kiros, and G.E. Hinton in 2016 in the same-named paper
    *Layer Normalization* (URL: [https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)).
    While batch normalization, which we will discuss in more detail in *Chapter 17*,
    *Generative Adversarial Networks for Synthesizing New Data*, is a popular choice
    in computer vision contexts, layer normalization is the preferred choice in NLP
    contexts, where sentence lengths can vary. *Figure 16.9* illustrates the main
    differences of layer and batch normalization side by side:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看**层归一化**机制，这是由 J. Ba、J.R. Kiros 和 G.E. Hinton 在 2016 年同名论文 *Layer Normalization*（URL：[https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)）中首次引入的。虽然批归一化在计算机视觉环境中很受欢迎，我们将在
    *第17章*《生成对抗网络用于合成新数据》中更详细地讨论它，但在自然语言处理（NLP）环境中，句子长度可能会变化，因此层归一化是首选。*图 16.9* 显示了层归一化和批归一化的主要区别：
- en: '![Diagram  Description automatically generated](img/B17582_16_09.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_16_09.png)'
- en: 'Figure 16.9: A comparison of batch and layer normalization'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9：批归一化与层归一化的比较
- en: While layer normalization is traditionally performed across all elements in
    a given feature for each feature independently, the layer normalization used in
    transformers extends this concept and computes the normalization statistics across
    all feature values independently for each training example.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统上层归一化是在每个特征的所有元素之间执行的，但在transformers中使用的层归一化扩展了这一概念，并为每个训练样本独立地计算所有特征值的归一化统计。
- en: Since layer normalization computes mean and standard deviation for each training
    example, it relaxes minibatch size constraints or dependencies. In contrast to
    batch normalization, layer normalization is thus capable of learning from data
    with small minibatch sizes and varying lengths. However, note that the original
    transformer architecture does not have varying-length inputs (sentences are padded
    when needed), and unlike RNNs, there is no recurrence in the model. So, how can
    we then justify the use of layer normalization over batch normalization? Transformers
    are usually trained on very large text corpora, which requires parallel computation;
    this can be challenging to achieve with batch normalization, which has a dependency
    between training examples. Layer normalization has no such dependency and is thus
    a more natural choice for transformers.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于层归一化对每个训练样本计算均值和标准差，它放宽了小批量大小的限制或依赖关系。与批归一化相比，层归一化能够从具有小批量大小和不同长度数据中学习。但需要注意的是，原始的Transformer架构并没有变长输入（需要时会对句子进行填充），并且与RNN不同，模型中没有循环。那么，我们如何能够在这种情况下证明层归一化优于批归一化的使用呢？Transformers通常在非常大的文本语料库上进行训练，这需要并行计算；这对于批归一化来说可能是具有挑战性的，因为训练样本之间存在依赖关系。层归一化没有这种依赖关系，因此对于transformers来说是一个更自然的选择。
- en: Building large-scale language models by leveraging unlabeled data
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用未标记数据构建大规模语言模型
- en: In this section, we will discuss popular large-scale transformer models that
    emerged from the original transformer. One common theme among these transformers
    is that they are pre-trained on very large, unlabeled datasets and then fine-tuned
    for their respective target tasks. First, we will introduce the common training
    procedure of transformer-based models and explain how it is different from the
    original transformer. Then, we will focus on popular large-scale language models
    including **Generative Pre-trained Transformer** (**GPT**), **Bidirectional Encoder
    Representations from Transformers** (**BERT**), and **Bidirectional and Auto-Regressive
    Transformers** (**BART**).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将讨论从原始变压器中产生的流行大规模变压器模型。这些变压器之间的一个共同主题是它们都是在非常大的未标记数据集上进行预训练，然后针对各自的目标任务进行微调的。首先，我们将介绍基于变压器的模型的常见训练过程，并解释它如何与原始变压器不同。然后，我们将重点介绍流行的大规模语言模型，包括**生成式预训练变压器**（**GPT**）、**来自变压器的双向编码器表示**（**BERT**）和**双向自回归变压器**（**BART**）。
- en: Pre-training and fine-tuning transformer models
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练和微调变压器模型
- en: 'In an earlier section, *Attention is all we need: introducing the original
    transformer architecture*, we discussed how the original transformer architecture
    can be used for language translation. Language translation is a supervised task
    and requires a labeled dataset, which can be very expensive to obtain. The lack
    of large, labeled datasets is a long-lasting problem in deep learning, especially
    for models like the transformer, which are even more data hungry than other deep
    learning architectures. However, given that large amounts of text (books, websites,
    and social media posts) are generated every day, an interesting question is how
    we can use such unlabeled data for improving the model training.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个早期的章节中，*注意力就是我们需要的：介绍原始变压器架构*，我们讨论了如何利用原始变压器架构进行语言翻译。语言翻译是一个监督任务，需要一个带标签的数据集，这可能非常昂贵。缺乏大型的标记数据集是深度学习中一个长期存在的问题，特别是对于像变压器这样更加依赖数据的模型。然而，考虑到每天会产生大量的文本（书籍、网站和社交媒体帖子），一个有趣的问题是我们如何利用这些未标记的数据来改进模型训练。
- en: 'The answer to whether we can leverage unlabeled data in transformers is *yes*,
    and the trick is a process called **self-supervised learning**: we can generate
    “labels” from supervised learning from plain text itself. For example, given a
    large, unlabeled text corpus, we train the model to perform **next-word prediction**,
    which enables the model to learn the probability distribution of words and can
    form a strong basis for becoming a powerful language model.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否可以利用变压器中的无标签数据的答案是*yes*，而技巧就是一个称为**自监督学习**的过程：我们可以从纯文本本身生成“标签”来进行监督学习。例如，给定一个大型的未标记文本语料库，我们训练模型执行**下一个词预测**，这使得模型能够学习单词的概率分布，并可以形成强大的语言模型的基础。
- en: Self-supervised learning is traditionally also referred to as **unsupervised
    pre-training** and is essential for the success of modern transformer-based models.
    The “unsupervised” in unsupervised pre-training supposedly refers to the fact
    that we use unlabeled data; however, since we use the structure of the data to
    generate labels (for example, the next-word prediction task mentioned previously),
    it is still a supervised learning process.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习传统上也被称为**无监督预训练**，对于现代基于变压器的模型的成功至关重要。无监督预训练中的“无监督”据说是指我们使用未标记的数据；然而，由于我们使用数据的结构生成标签（例如前面提到的下一个词预测任务），因此它仍然是一个监督学习过程。
- en: 'To elaborate a bit further on how unsupervised pre-training and next-word prediction
    works, if we have a sentence containing *n* words, the pre-training procedure
    can be decomposed into the following three steps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步详细说明无监督预训练和下一个词预测的工作原理，如果我们有一个包含*n*个单词的句子，预训练过程可以分解为以下三个步骤：
- en: At time *step 1*, feed in the ground-truth words 1, …, *i*-1.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤 1*时，输入地面真实的单词 1，...，*i*-1。
- en: Ask the model to predict the word at position *i* and compare it with the ground-truth
    word *i*.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要求模型预测位置*i*处的单词，并将其与地面真实单词*i*进行比较。
- en: Update the model and time step, *i*:= *i*+1\. Go back to step 1 and repeat until
    all words are processed.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型和时间步长，*i* := *i* + 1。回到步骤 1 并重复，直到所有单词都被处理完。
- en: We should note that in the next iteration, we always feed the model the ground-truth
    (correct) words instead of what the model has generated in the previous round.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，在下一次迭代中，我们总是向模型提供正确的单词而不是上一轮模型生成的内容。
- en: The main idea of pre-training is to make use of plain text and then transfer
    and fine-tune the model to perform some specific tasks for which a (smaller) labeled
    dataset is available. Now, there are many different types of pre-training techniques.
    For example, the previously mentioned next-word prediction task can be considered
    as a unidirectional pre-training approach. Later, we will introduce additional
    pre-training techniques that are utilized in different language models to achieve
    various functionalities.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的主要思想是利用普通文本，然后转移并微调模型，以执行某些具有（较小）标记数据集的特定任务。现在，有许多不同类型的预训练技术。例如，先前提到的下一个词预测任务可以被视为单向预训练方法。稍后，我们将介绍在不同语言模型中使用的其他预训练技术，以实现各种功能。
- en: 'A complete training procedure of a transformer-based model consists of two
    parts: (1) pre-training on a large, unlabeled dataset and (2) training (that is,
    fine-tuning) the model for specific downstream tasks using a labeled dataset.
    In the first step, the pre-trained model is not designed for any specific task
    but rather trained as a “general” language model. Afterward, via the second step,
    it can be generalized to any customized task via regular supervised learning on
    a labeled dataset.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基于Transformer模型的完整训练过程包括两部分：（1）在大规模未标记数据集上进行预训练，以及（2）使用标记数据集对模型进行训练（即微调），以适应特定的下游任务。在第一步中，预训练模型并不针对特定任务设计，而是被训练为一个“通用”语言模型。随后，在第二步中，通过常规监督学习在标记数据集上，它可以泛化到任何定制任务中。
- en: 'With the representations that can be obtained from the pre-trained model, there
    are mainly two strategies for transferring and adopting a model to a specific
    task: (1) a **feature-based approach** and (2) a **fine-tuning approach**. (Here,
    we can think of these representations as the hidden layer activations of the last
    layers of a model.)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 利用从预训练模型中获得的表示，主要有两种策略将模型转移并应用于特定任务：（1）基于特征的方法和（2）微调方法。（在这里，我们可以将这些表示视为模型最后层的隐藏层激活。）
- en: 'The feature-based approach uses the pre-trained representations as additional
    features to a labeled dataset. This requires us to learn how to extract sentence
    features from the pre-trained model. An early model that is well-known for this
    feature extraction approach is **ELMo** (**Embeddings from Language Models**)
    proposed by Peters and colleagues in 2018 in the paper *Deep Contextualized Word
    Representations* (URL: [https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365)).
    ELMo is a pre-trained bidirectional language model that masks words at a certain
    rate. In particular, it randomly masks 15 percent of the input words during pre-training,
    and the modeling task is to fill in these blanks, that is, predicting the missing
    (masked) words. This is different from the unidirectional approach we introduced
    previously, which hides all the future words at time step *i*. Bidirectional masking
    enables a model to learn from both ends and can thus capture more holistic information
    about a sentence. The pre-trained ELMo model can generate high-quality sentence
    representations that, later on, serve as input features for specific tasks. In
    other words, we can think of the feature-based approach as a model-based feature
    extraction technique similar to principal component analysis, which we covered
    in *Chapter 5*, *Compressing Data via Dimensionality Reduction*.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 特征驱动方法使用预训练表示作为标记数据集的附加特征。这要求我们学习如何从预训练模型中提取句子特征。一个早期以特征提取方法闻名的模型是2018年由Peters和同事在论文《深度上下文化的词表示》（URL：[https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365)）中提出的**ELMo**（从语言模型中嵌入）。ELMo是一个预训练的双向语言模型，在预训练过程中以一定比例屏蔽单词。特别地，它在预训练期间随机屏蔽输入单词的15%，建模任务是填补这些空白，即预测丢失（屏蔽）的单词。这与我们之前介绍的单向方法不同，后者在时间步*i*隐藏所有未来单词。双向屏蔽使模型能够从两端学习，因此可以捕获更全面的句子信息。预训练的ELMo模型能够生成高质量的句子表示，后续可作为特定任务的输入特征。换句话说，我们可以将特征驱动方法视为一种类似于主成分分析的基于模型的特征提取技术，我们在《第5章》，《通过降维压缩数据》中进行了讨论。
- en: The fine-tuning approach, on the other hand, updates the pre-trained model parameters
    in a regular supervised fashion via backpropagation. Unlike the feature-based
    method, we usually also add another fully connected layer to the pre-trained model,
    to accomplish certain tasks such as classification, and then update the whole
    model based on the prediction performance on the labeled training set. One popular
    model that follows this approach is BERT, a large-scale transformer model pre-trained
    as a bidirectional language model. We will discuss BERT in more detail in the
    following subsections. In addition, in the last section of this chapter, we will
    see a code example showing how to fine-tune a pre-trained BERT model for sentiment
    classification using the movie review dataset we worked with in *Chapter 8*, *Applying
    Machine Learning to Sentiment Analysis*, and *Chapter 15*, *Modeling Sequential
    Data Using Recurrent Neural Networks*.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种微调方法是通过反向传播以常规监督方式更新预训练模型参数。与基于特征的方法不同，我们通常还会向预训练模型添加另一个完全连接的层，以完成诸如分类等特定任务，然后根据在标记训练集上的预测性能更新整个模型。遵循此方法的一个流行模型是BERT，一个大规模的变压器模型，预先训练为双向语言模型。我们将在接下来的小节中更详细地讨论BERT。此外，在本章的最后一节中，我们将看到一个代码示例，展示如何使用我们在*第8章*，*应用机器学习进行情感分析*，和*第15章*，*使用递归神经网络建模序列数据*中使用的电影评论数据集，对预训练的BERT模型进行情感分类的微调。
- en: 'Before we move on to the next section and start our discussion of popular transformer-based
    language models, the following figure summarizes the two stages of training transformer
    models and illustrates the difference between the feature-based and fine-tuning
    approaches:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入下一节并开始讨论基于Transformer的流行语言模型之前，以下图表总结了Transformer模型训练的两个阶段，并说明了基于特征和微调方法之间的区别：
- en: ­­­­![Diagram  Description automatically generated](img/B17582_16_10.png)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ­­­­![自动生成的图表说明](img/B17582_16_10.png)
- en: 'Figure 16.10: The two main ways to adopt a pre-trained transformer for downstream
    tasks'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：采用预训练变压器进行下游任务的两种主要方式
- en: Leveraging unlabeled data with GPT
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用GPT进行无标签数据的操作
- en: The **Generative Pre-trained Transformer** (**GPT**) is a popular series of
    large-scale language models for generating text developed by OpenAI. The most
    recent model, GPT-3, which was released in May 2020 (*Language Models are Few-Shot
    Learners*), is producing astonishing results. The quality of the text generated
    by GPT-3 is very hard to distinguish from human-generated texts. In this section,
    we are going to discuss how the GPT model works on a high level, and how it has
    evolved over the years.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成预训练变压器**（**GPT**）是由OpenAI开发的一系列流行的大规模语言模型，用于生成文本。最近的模型GPT-3，于2020年5月发布（*语言模型是少样本学习者*），正在产生令人惊讶的结果。GPT-3生成的文本质量很难与人类生成的文本区分开。在本节中，我们将讨论GPT模型在高层次上的工作原理及其多年来的发展。'
- en: 'As listed in *Table 16.1*, one obvious evolution within the GPT model series
    is the number of parameters:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*表16.1*中所列，GPT模型系列中的一个明显演变是参数数量的增加：
- en: '| **Model** | **Release year** | **Number of parameters** | **Title** | **Paper
    link** |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **发布年份** | **参数数量** | **标题** | **论文链接** |'
- en: '| GPT-1 | 2018 | 110 million | Improving Language Understanding by Generative
    Pre-Training | [https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| GPT-1 | 2018 | 1.1亿 | 通过生成预训练来提升语言理解能力 | [链接](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)
    |'
- en: '| GPT-2 | 2019 | 1.5 billion | Language Models are Unsupervised Multitask Learners
    | [https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| GPT-2 | 2019 | 15亿 | 语言模型是无监督的多任务学习者 | [链接](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)
    |'
- en: '| GPT-3 | 2020 | 175 billion | Language Models are Few-Shot Learners | [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3 | 2020 | 1750亿 | 语言模型是少样本学习者 | [链接](https://arxiv.org/pdf/2005.14165.pdf)
    |'
- en: 'Table 16.1: Overview of the GPT models'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表16.1：GPT模型概览
- en: 'But let’s not get ahead of ourselves, and take a closer look at the GPT-1 model
    first, which was released in 2018\. Its training procedure can be decomposed into
    two stages:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，让我们先不要过于超前，首先更仔细地看看2018年发布的GPT-1模型的情况，它的训练过程可以分解为两个阶段：
- en: Pre-training on a large amount of unlabeled plain text
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大量未标记的纯文本上进行预训练
- en: Supervised fine-tuning
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督微调
- en: 'As *Figure 16.11* (adapted from the GPT-1 paper) illustrates, we can consider
    GPT-1 as a transformer consisting of (1) a decoder (and without an encoder block)
    and (2) an additional layer that is added later for the supervised fine-tuning
    to accomplish specific tasks:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*图 16.11*（改编自GPT-1论文）所示，我们可以将GPT-1视为一个由(1)解码器（没有编码器块）和(2)稍后添加的额外层组成的变压器，用于进行监督微调以完成特定任务：
- en: '![Graphical user interface  Description automatically generated](img/B17582_16_11.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  描述自动生成](img/B17582_16_11.png)'
- en: 'Figure 16.11: The GPT-1 transformer'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.11：GPT-1变压器
- en: In the figure, note that if our task is *Text Prediction* (predicting the next
    word), then the model is ready after the pre-training step. Otherwise, for example,
    if our task is related to classification or regression, then supervised fine-tuning
    is required.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，请注意，如果我们的任务是*文本预测*（预测下一个词），那么模型在预训练步骤后就已经准备好了。否则，例如，如果我们的任务与分类或回归相关，则需要进行监督微调。
- en: 'During pre-training, GPT-1 utilizes a transformer decoder structure, where,
    at a given word position, the model only relies on preceding words to predict
    the next word. GPT-1 utilizes a unidirectional self-attention mechanism, as opposed
    to a bidirectional one as in BERT (which we will cover later in this chapter),
    because GPT-1 is focused on text generation rather than classification. During
    text generation, it produces words one by one with a natural left-to-right direction.
    There is one other aspect worth highlighting here: during the training procedure,
    for each position, we always feed the correct words from the previous positions
    to the model. However, during inference, we just feed the model whatever words
    it has generated to be able to generate new texts.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练期间，GPT-1利用变压器解码器结构，在给定的词位置，模型仅依赖于前面的词来预测下一个词。GPT-1利用单向自注意机制，与BERT中的双向自注意机制相对，因为GPT-1专注于文本生成而不是分类。在文本生成期间，它以自然的从左到右的方向逐个生成单词。这里有一个值得强调的另一个方面：在训练过程中，对于每个位置，我们始终向模型提供来自前一位置的正确单词。但在推理过程中，我们只是向模型提供它已经生成的任何单词，以便生成新的文本。
- en: After obtaining the pre-trained model (the block in the previous figure labeled
    as *Transformer*), we then insert it between the input pre-processing block and
    a linear layer, where the linear layer serves as an output layer (similar to previous
    deep neural network models we discussed earlier in this book). For classification
    tasks, fine-tuning is as simple as first tokenizing the input and then feeding
    it into the pre-trained model and the newly added linear layer, which is followed
    by a softmax activation function. However, for more complicated tasks such as
    question answering, inputs are organized in a certain format that is not necessarily
    matching the pre-trained model, which requires an extra processing step customized
    for each task. Readers who are interested in specific modifications are encouraged
    to read the GPT-1 paper for additional details (the link is provided in the previous
    table).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得预训练模型（前一图中标记为*Transformer*的块）之后，我们将其插入到输入预处理块和线性层之间，其中线性层充当输出层（类似于本书前面讨论过的其他深度神经网络模型）。对于分类任务，微调就像首先对输入进行标记化，然后将其输入到预训练模型和新添加的线性层中，接着是softmax激活函数。然而，对于诸如问答之类的更复杂任务，输入以某种不一定匹配预训练模型的格式组织，这需要为每个任务定制的额外处理步骤。鼓励对特定修改感兴趣的读者阅读GPT-1论文以获取更多细节（链接在上表中提供）。
- en: GPT-1 also performs surprisingly well on **zero-shot tasks**, which proves its
    ability to be a general language model that can be customized for different types
    of tasks with minimal task-specific fine-tuning. Zero-shot learning generally
    describes a special circumstance in machine learning where during testing and
    inference, the model is required to classify samples from classes that were not
    observed during training. In the context of GPT, the zero-shot setting refers
    to unseen tasks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-1 在**零次任务**上的表现也令人惊讶，这证明了它作为一个通用语言模型的能力，可以通过最少的任务特定微调定制不同类型的任务。零次学习通常描述的是机器学习中的一个特殊情况，在测试和推理过程中，模型需要对未在训练中观察到的类别的样本进行分类。在
    GPT 的上下文中，零次设置指的是未见任务。
- en: GPT’s adaptability inspired researchers to get rid of the task-specific input
    and model setup, which led to the development of GPT-2\. Unlike its predecessor,
    GPT-2 does not require any additional modification during the input or fine-tuning
    stages anymore. Instead of rearranging sequences to match the required format,
    GPT-2 can distinguish between different types of inputs and perform the corresponding
    downstream tasks with minor hints, the so-called “contexts.” This is achieved
    by modeling output probabilities conditioned on both input and task type, ![](img/B17582_16_151.png),
    instead of only conditioning on the input. For example, the model is expected
    to recognize a translation task if the context includes `translate to French,
    English text, French text`.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 的适应能力激发了研究人员摒弃特定任务的输入和模型设置，从而推动了 GPT-2 的发展。与其前身不同，GPT-2 在输入或微调阶段不再需要任何额外的修改。模型不再需要重新排列序列以匹配所需格式，而是可以区分不同类型的输入，并在少量提示（所谓的“上下文”）下执行相应的下游任务。这是通过在输出概率上进行建模，条件是输入和任务类型，![](img/B17582_16_151.png)，而不仅仅是条件于输入。例如，如果上下文包括
    `translate to French, English text, French text`，则期望模型能识别翻译任务。
- en: This sounds much more “artificially intelligent” than GPT and is indeed the
    most noticeable improvement besides the model size. Just as the title of its corresponding
    paper indicates (*Language Models are Unsupervised Multitask Learners*), an unsupervised
    language model may be key to zero-shot learning, and GPT-2 makes full use of zero-shot
    task transfer to build this multi-task learner.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来比 GPT 更“人工智能”，实际上除了模型大小外，这是最显著的改进之一。正如其相应论文的标题所示（*语言模型是无监督多任务学习者*），无监督语言模型可能是零次学习的关键，而
    GPT-2 充分利用了零次任务转移来构建这种多任务学习器。
- en: 'Compared with GPT-2, GPT-3 is less “ambitious” in the sense that it shifts
    the focus from zero- to one-shot and **few-shot learning** via in-context learning.
    While providing no task-specific training examples seems to be too strict, few-shot
    learning is not only more realistic but also more human-like: humans usually need
    to see a few examples to be able to learn a new task. Just as its name suggests,
    few-shot learning means that the model sees a *few* examples of the task while
    one-shot learning is restricted to exactly one example.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GPT-2 相比，GPT-3 在某种意义上不太“雄心勃勃”，它将注意力从零次转移到一次和**少次学习**，通过上下文学习。虽然不提供特定任务的训练示例似乎过于严格，但少次学习不仅更现实，而且更像人类：人类通常需要看几个例子才能学会一个新任务。正如其名称所示，少次学习意味着模型看到*少量*任务示例，而一次学习则限于一个示例。
- en: '*Figure 16.12* illustrates the difference between zero-shot, one-shot, few-shot,
    and fine-tuning procedures:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 16.12* 展示了零次、一次、少次和微调过程之间的区别：'
- en: '![Diagram  Description automatically generated with medium confidence](img/B17582_16_12.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述，自动生成，中等置信度](img/B17582_16_12.png)'
- en: 'Figure 16.12: A comparison of zero-shot, one-shot, and few-shot learning'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.12：零次、一次和少次学习的比较
- en: 'The model architecture of GPT-3 is pretty much the same as GPT-2 except for
    the 100-fold parameter size increase and the use of a sparse transformer. In the
    original (dense) attention mechanism we discussed earlier, each element attends
    to all other elements in the input, which scales with ![](img/B17582_16_152.png)
    complexity. **Sparse attention** improves the efficiency by only attending to
    a subset of elements with limited size, normally proportional to ![](img/B17582_16_153.png).
    Interested readers can learn more about the specific subset selection by visiting
    the sparse transformer paper: *Generating Long Sequences with Sparse Transformers
    by Rewon Child et al*. 2019 (URL: [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的模型架构基本与 GPT-2 相同，只是参数规模增加了 100 倍，并使用了稀疏 transformer。在我们之前讨论过的原始（密集）注意力机制中，每个元素都关注输入中的所有其他元素，这会随着
    ![](img/B17582_16_152.png) 的复杂性增加。**稀疏注意力**通过仅关注大小有限的元素子集来提高效率，通常与 ![](img/B17582_16_153.png)
    成比例。有兴趣的读者可以通过访问稀疏 transformer 论文了解更多有关特定子集选择的信息：*Generating Long Sequences with
    Sparse Transformers by Rewon Child 等人，2019*（URL：[https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)）。
- en: Using GPT-2 to generate new text
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GPT-2 生成新的文本
- en: Before we move on to the next transformer architecture, let us take a look at
    how we can use the latest GPT models to generate new text. Note that GPT-3 is
    still relatively new and is currently only available as a beta version via the
    OpenAI API at [https://openai.com/blog/openai-api/](https://openai.com/blog/openai-api/).
    However, an implementation of GPT-2 has been made available by Hugging Face (a
    popular NLP and machine learning company; [http://huggingface.co](http://huggingface.co)),
    which we will use.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续下一个 transformer 架构之前，让我们看看如何使用最新的 GPT 模型生成新的文本。请注意，GPT-3 目前仍然相对较新，并且目前仅通过
    OpenAI API 的 beta 版本提供服务，网址为 [https://openai.com/blog/openai-api/](https://openai.com/blog/openai-api/)。但是，Hugging
    Face 提供了 GPT-2 的实现（一家知名的自然语言处理和机器学习公司；[http://huggingface.co](http://huggingface.co)），我们将使用它。
- en: 'We will be accessing GPT-2 via `transformers`, which is a very comprehensive
    Python library created by Hugging Face that provides various transformer-based
    models for pre-training and fine-tuning. Users can also discuss and share their
    customized models on the forum. Feel free to check out and engage with the community
    if you are interested: [https://discuss.huggingface.co](https://discuss.huggingface.co).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过 `transformers` 访问 GPT-2，这是由 Hugging Face 创建的非常全面的 Python 库，提供各种基于 transformer
    的模型进行预训练和微调。用户还可以在论坛上讨论和分享他们定制的模型。如果您有兴趣，请随时访问社区并参与其中：[https://discuss.huggingface.co](https://discuss.huggingface.co)。
- en: '**Installing transformers version 4.9.1**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 transformers 版本 4.9.1**'
- en: 'Because this package is evolving rapidly, you may not be able to replicate
    the results in the following subsections. For reference, this tutorial uses version
    4.9.1 released in June 2021\. To install the version we used in this book, you
    can execute the following command in your terminal to install it from PyPI:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个软件包正在迅速发展，您可能无法在以下子章节中复制结果。作为参考，本教程使用的是 2021 年 6 月发布的 4.9.1 版本。要安装本书中使用的版本，您可以在终端中执行以下命令从
    PyPI 安装它：
- en: '[PRE30]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We also recommend checking the latest instructions on the official installation
    page:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还建议查阅官方安装页面上的最新说明：
- en: '[https://huggingface.co/transformers/installation.html](https://huggingface.co/transformers/installation.html)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/transformers/installation.html](https://huggingface.co/transformers/installation.html)'
- en: 'Once we have installed the `transformers` library, we can run the following
    code to import a pre-trained GPT model that can generate new text:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们安装了 `transformers` 库，我们可以运行以下代码来导入一个预训练的 GPT 模型，该模型可以生成新的文本：
- en: '[PRE31]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then, we can prompt the model with a text snippet and ask it to generate new
    text based on that input snippet:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以用一个文本片段提示模型，并要求它基于该输入片段生成新的文本：
- en: '[PRE32]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As we can see from the output, the model generated three reasonable sentences
    based on our text snippet. If you want to explore more examples, please feel free
    to change the random seed and the maximum sequence length.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，基于我们的文本片段，模型生成了三个合理的句子。如果您想探索更多示例，请随意更改随机种子和最大序列长度。
- en: 'Also, as previously illustrated in *Figure 16.10*, we can use a transformer
    model to generate features for training other models. The following code illustrates
    how we can use GPT-2 to generate features based on an input text:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如在*图 16.10*中所示，我们可以使用 transformer 模型为训练其他模型生成特征。以下代码说明了如何使用 GPT-2 根据输入文本生成特征：
- en: '[PRE33]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This code encoded the input sentence text into a tokenized format for the GPT-2
    model. As we can see, it mapped the strings to an integer representation, and
    it set the attention mask to all 1s, which means that all words will be processed
    when we pass the encoded input to the model, as shown here:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输入句子文本编码成了GPT-2模型的标记化格式。正如我们所见，它将字符串映射到整数表示，并将注意力掩码设置为全1，这意味着在将编码输入传递给模型时将处理所有单词，如下所示：
- en: '[PRE34]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The `output` variable stores the last hidden state, that is, our GPT-2-based
    feature encoding of the input sentence:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`output`存储了最后的隐藏状态，即我们基于GPT-2的输入句子特征编码：
- en: '[PRE35]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: To suppress the verbose output, we only showed the shape of the tensor. Its
    first dimension is the batch size (we only have one input text), which is followed
    by the sentence length and size of the feature encoding. Here, each of the five
    words is encoded as a 768-dimensional vector.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 为了抑制冗长的输出，我们仅展示了张量的形状。其第一维是批处理大小（我们只有一个输入文本），其后是句子长度和特征编码的大小。在这里，每个五个单词被编码为一个768维向量。
- en: Now, we could apply this feature encoding to a given dataset and train a downstream
    classifier based on the GPT-2-based feature representation instead of using a
    bag-of-words model as discussed in *Chapter 8*, *Applying Machine Learning to
    Sentiment Analysis*.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这种特征编码应用于给定的数据集，并基于基于GPT-2的特征表示训练一个下游分类器，而不是使用如第8章“应用机器学习进行情感分析”中讨论的词袋模型。
- en: Moreover, an alternative approach to using large pre-trained language models
    is fine-tuning, as we discussed earlier. We will be seeing a fine-tuning example
    later in this chapter.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，另一种使用大型预训练语言模型的方法是微调，正如我们之前讨论过的。在本章稍后我们将看到一个微调的例子。
- en: 'If you are interested in additional details on using GPT-2, we recommend the
    following documentation pages:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对使用GPT-2的详细信息感兴趣，我们建议查阅以下文档页面：
- en: '[https://huggingface.co/gpt2](https://huggingface.co/gpt2)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/gpt2](https://huggingface.co/gpt2)'
- en: '[https://huggingface.co/docs/transformers/model_doc/gpt2](https://huggingface.co/docs/transformers/model_doc/gpt2)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/docs/transformers/model_doc/gpt2](https://huggingface.co/docs/transformers/model_doc/gpt2)'
- en: Bidirectional pre-training with BERT
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用BERT进行双向预训练
- en: '**BERT**, its full name being **Bidirectional Encoder Representations from
    Transformers**, was created by a Google research team in 2018 (*BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding* by *J. Devlin,
    M. Chang, K. Lee,* and *K. Toutanova*, [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)).
    For reference, even though we cannot compare GPT and BERT directly as they are
    different architectures, BERT has 345 million parameters (which makes it only
    slightly larger than GPT-1, and its size is only 1/5 of GPT-2).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**，全名为**双向编码器表示转换器**，由Google研究团队于2018年创建（*BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding* by *J. Devlin, M. Chang,
    K. Lee,* and *K. Toutanova*, [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)）。值得参考的是，虽然我们不能直接将GPT和BERT进行比较，因为它们是不同的架构，但BERT有3.45亿个参数（这使它比GPT-1略大一些，其大小仅为GPT-2的五分之一）。'
- en: As its name suggests, BERT has a transformer-encoder-based model structure that
    utilizes a bidirectional training procedure. (Or, more accurately, we can think
    of BERT as using “nondirectional” training because it reads in all input elements
    all at once.) Under this setting, the encoding of a certain word depends on both
    the preceding and the succeeding words. Recall that in GPT, input elements are
    read in with a natural left-to-right order, which helps to form a powerful generative
    language model. Bidirectional training disables BERT’s ability to generate a sentence
    word by word but provides input encodings of higher quality for other tasks, such
    as classification, since the model can now process information in both directions.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名，BERT具有基于transformer编码器的模型结构，利用了双向训练过程。（更准确地说，我们可以认为BERT使用“非定向”训练，因为它一次性读取所有输入元素。）在此设置下，某个单词的编码取决于其前后的单词。回想一下，在GPT中，输入元素按自然的从左到右顺序读取，这有助于形成强大的生成语言模型。双向训练禁用了BERT逐词生成句子的能力，但提供了更高质量的输入编码，用于其他任务，如分类，因为该模型现在可以双向处理信息。
- en: 'Recall that in a transformer’s encoder, token encoding is a summation of positional
    encodings and token embeddings. In the BERT encoder, there is an additional segment
    embedding indicating which segment this token belongs to. This means that each
    token representation contains three ingredients, as *Figure 16.13* illustrates:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在Transformer的编码器中，令牌编码是位置编码和令牌嵌入的总和。在BERT编码器中，还有一个额外的段嵌入，指示此令牌属于哪个段。这意味着每个令牌表示包含三个部分，正如*图16.13*所示：
- en: '![A screenshot of a game  Description automatically generated with medium confidence](img/B17582_16_13.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![游戏截图 描述已由中度置信度自动生成](img/B17582_16_13.png)'
- en: 'Figure 16.13: Preparing the inputs for the BERT encoder'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13：为BERT编码器准备输入
- en: Why do we need this additional segment information in BERT? The need for this
    segment information originated from the special pre-training task of BERT called
    *next-sentence prediction*. In this pre-training task, each training example includes
    two sentences and thus requires special segment notation to denote whether it
    belongs to the first or second sentence.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要BERT中的额外段信息？这段信息的需求源于BERT的特殊预训练任务，称为*下一句预测*。在此预训练任务中，每个训练示例包括两个句子，因此需要特殊的段符号来表示它是属于第一个还是第二个句子。
- en: 'Now, let us look at BERT’s pre-training tasks in more detail. Similar to all
    other transformer-based language models, BERT has two training stages: pre-training
    and fine-tuning. And pre-training includes two unsupervised tasks: *masked language
    modeling* and *next-sentence prediction*.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看一下BERT的预训练任务。与所有其他基于Transformer的语言模型类似，BERT有两个训练阶段：预训练和微调。预训练包括两个无监督任务：*掩码语言建模*和*下一句预测*。
- en: 'In the **masked language model** (**MLM**), tokens are randomly replaced by
    so-called *mask tokens*, `[MASK]`, and the model is required to predict these
    hidden words. Compared with the next-word prediction in GPT, MLM in BERT is more
    akin to “filling in the blanks” because the model can attend to all tokens in
    the sentence (except the masked ones). However, simply masking words out can result
    in inconsistencies between pre-training and fine-tuning since `[MASK]` tokens
    do not appear in regular texts. To alleviate this, there are further modifications
    to the words that are selected for masking. For instance, 15 percent of the words
    in BERT are marked for masking. These 15 percent of randomly selected words are
    then further treated as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在**掩码语言模型**（**MLM**）中，标记被随机替换为所谓的*掩码标记* `[MASK]`，模型需要预测这些隐藏的单词。与GPT中的下一个单词预测相比，BERT中的MLM更类似于“填空”，因为模型可以关注句子中的所有标记（除了掩码标记）。然而，简单地屏蔽单词可能导致预训练和微调之间的不一致，因为`[MASK]`标记不会出现在常规文本中。为了减轻这一问题，对于选定的要屏蔽的单词，还有进一步的修改。例如，在BERT中，15%的单词被标记为屏蔽。这15%的随机选择单词接下来会进一步处理为：
- en: Keep the word unchanged 10 percent of the time
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10%的时间保持单词不变
- en: Replace the original word token with a random word 10 percent of the time
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10%的时间将原始词令牌替换为随机单词
- en: Replace the original word token with a mask token, `[MASK]`, 80 percent of the
    time
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 80%的时间将原始词令牌替换为掩码令牌 `[MASK]`
- en: Besides avoiding the aforementioned inconsistency between pre-training and fine-tuning
    when introducing `[MASK]` tokens into the training procedure, these modifications
    also have other benefits. Firstly, unchanged words include the possibility of
    maintaining the information of the original token; otherwise, the model can only
    learn from the context and nothing from the masked words. Secondly, the 10 percent
    random words prevent the model from becoming lazy, for instance, learning nothing
    but returning what it is being given. The probabilities for masking, randomizing,
    and leaving words unchanged were chosen by an ablation study (see the GPT-2 paper);
    for instance, authors tested different settings and found that this combination
    worked best.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在引入`[MASK]`令牌到训练过程中避免上述预训练和微调之间的不一致性之外，这些修改还有其他好处。首先，未更改的单词包括保持原始令牌信息的可能性；否则，模型只能从上下文中学习，而不是从掩码的单词中学习。其次，10%的随机单词防止模型变得懒惰，例如，仅仅返回所给的内容而没有学到任何东西。掩码、随机化和保持单词不变的概率由消融研究选择（参见GPT-2论文）；例如，作者测试了不同的设置，并发现这种组合效果最好。
- en: '*Figure 16.14* illustrates an example where the word *fox* is masked and, with
    a certain probability, remains unchanged or is replaced by `[MASK]` or *coffee*.
    The model is then required to predict what the masked (highlighted) word is as
    illustrated in *Figure 16.14*:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*图16.14* 描述了一个示例，在这个示例中，单词*fox*被屏蔽，并且有一定概率保持不变，或者被替换为`[MASK]`或*coffee*。然后，模型需要预测屏蔽（突出显示）的单词是什么，如*图16.14*所示：'
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17582_16_14.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用 自动生成描述](img/B17582_16_14.png)'
- en: 'Figure 16.14: An example of MLM'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14：MLM示例
- en: Next-sentence prediction is a natural modification of the next-word prediction
    task considering the bidirectional encoding of BERT. In fact, many important NLP
    tasks, such as question answering, depend on the relationship of two sentences
    in the document. This kind of relationship is hard to capture via regular language
    models because next-word prediction training usually occurs on a single-sentence
    level due to input length constraints.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个句子预测是对BERT的双向编码进行自然修改，考虑到下一个词预测任务的关系。实际上，许多重要的自然语言处理任务，如问答，依赖于文档中两个句子之间的关系。由于输入长度限制，通常单句预测训练很难捕捉这种关系。
- en: 'In the next-sentence prediction task, the model is given two sentences, A and
    B, in the following format:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个句子预测任务中，模型会得到两个句子A和B，格式如下：
- en: '[CLS] A [SEP] B [SEP]'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLS] A [SEP] B [SEP]'
- en: '[CLS] is a classification token, which serves as a placeholder for the predicted
    label in the decoder output, as well as a token denoting the beginning of the
    sentences. The [SEP] token, on the other hand, is attached to denote the end of
    each sentence. The model is then required to classify whether B is the next sentence
    (“IsNext”) of A or not. To provide the model with a balanced dataset, 50 percent
    of the samples are labeled as “IsNext” while the remaining samples are labeled
    as “NotNext.”'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[CLS] 是分类标记，用作解码器输出中预测标签的占位符，同时也是表示句子开头的标记。另一方面，[SEP] 标记附加在每个句子的末尾。然后，模型需要分类是否B是A的下一个句子（“IsNext”）。为了向模型提供平衡的数据集，50%的样本标记为“IsNext”，而剩余的样本标记为“NotNext”。'
- en: BERT is pre-trained on these two tasks, masked sentences and next-sentence prediction,
    at the same time. Here, the training objective of BERT is to minimize the combined
    loss function of both tasks.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: BERT同时在这两个任务上进行预训练，即屏蔽句子和下一个句子预测。在这里，BERT的训练目标是最小化这两个任务的组合损失函数。
- en: Starting from the pre-trained model, specific modifications are required for
    different downstream tasks in the fine-tuning stage. Each input example needs
    to match a certain format; for example, it should begin with a [CLS] token and
    be separated using [SEP] tokens if it consists of more than one sentence.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 从预训练模型开始，需要针对微调阶段中不同的下游任务进行特定的修改。每个输入示例都需要匹配特定的格式；例如，如果包含多个句子，则应以[CLS]标记开头并使用[SEP]标记分隔。
- en: 'Roughly speaking, BERT can be fine-tuned on four categories of tasks: (a) sentence
    pair classification; (b) single-sentence classification; (c) question answering;
    (d) single-sentence tagging.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 大致而言，BERT可以在四类任务上进行微调：(a) 句对分类；(b) 单句分类；(c) 问答；(d) 单句标注。
- en: Among them, (a) and (b) are sequence-level classification tasks, which only
    require an additional softmax layer to be added to the output representation of
    the [CLS] token. (c) and (d), on the other hand, are token-level classification
    tasks. This means that the model passes output representations of all related
    tokens to the softmax layer to predict a class label for each individual token.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，(a) 和 (b) 是序列级分类任务，只需在输出表示的[CLS]标记上添加额外的softmax层。而 (c) 和 (d) 则是标记级分类任务。这意味着模型将所有相关标记的输出表示传递给softmax层，以预测每个单独标记的类别标签。
- en: '**Question answering**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**问答**'
- en: Task (c), question answering, appears to be less often discussed compared to
    other popular classification tasks such as sentiment classification or speech
    tagging. In question answering, each input example can be split into two parts,
    the question and the paragraph that helps to answer the question. The model is
    required to point out both the start and end token in the paragraph that forms
    a proper answer to the question. This means that the model needs to generate a
    tag for every single token in the paragraph, indicating whether this token is
    a start or end token, or neither. As a side note, it is worth mentioning that
    the output may contain an end token that appears before the start token, which
    will lead to a conflict when generating the answer. This kind of output will be
    recognized as “No Answer” to the question.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 任务（c），即问答，似乎与其他流行的分类任务（如情感分类或语音标记）相比少有讨论。在问答中，每个输入示例可以分为两部分，问题和帮助回答问题的段落。模型需要指出段落中的起始和结束标记，形成一个合适的答案。这意味着模型需要为段落中的每个单词生成一个标记，指示该单词是起始标记、结束标记还是其他。值得一提的是，输出可能包含在起始标记之前出现的结束标记，这在生成答案时可能会导致冲突。这种输出将被识别为对问题的“无答案”。
- en: 'As *Figure 16.15* indicates, the model fine-tuning setup has a very simple
    structure: an input encoder is attached to a pre-trained BERT, and a softmax layer
    is added for classification. Once the model structure is set up, all the parameters
    will be adjusted along the learning process.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图16.15*所示，模型的微调设置具有非常简单的结构：输入编码器连接到预训练的BERT，添加一个softmax层用于分类。一旦模型结构设置完成，所有参数将随学习过程进行调整。
- en: '![Diagram  Description automatically generated](img/B17582_16_15.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_16_15.png)'
- en: 'Figure 16.15: Using BERT to fine-tune different language tasks'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15：使用BERT微调不同的语言任务
- en: 'The best of both worlds: BART'
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**双赢之选：BART**'
- en: 'The **Bidirectional and Auto-Regressive Transformer**, abbreviated as **BART**,
    was developed by researchers at Facebook AI Research in 2019: *BART: Denoising
    Sequence-to-Sequence Pre-training for Natural Language Generation, Translation,
    and Comprehension*, *Lewis* and colleagues, [https://arxiv.org/abs/1910.13461](https://arxiv.org/abs/1910.13461).
    Recall that in previous sections we argued that GPT utilizes a transformer’s decoder
    structure, whereas BERT utilizes a transformer’s encoder structure. Those two
    models are thus capable of performing different tasks well: GPT’s specialty is
    generating text, whereas BERT performs better on classification tasks. BART can
    be viewed as a generalization of both GPT and BERT. As the title of this section
    suggests, BART is able to accomplish both tasks, generating and classifying text.
    The reason why it can handle both tasks well is that the model comes with a bidirectional
    encoder as well as a left-to-right autoregressive decoder.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**双向自回归变换器**，简称**BART**，由Facebook AI Research的研究人员在2019年开发：*BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练*，*Lewis*等人，[https://arxiv.org/abs/1910.13461](https://arxiv.org/abs/1910.13461)。回顾前文，我们提到GPT利用变换器的解码器结构，而BERT利用变换器的编码器结构。这两个模型因此能够很好地执行不同的任务：GPT的特长是生成文本，而BERT在分类任务上表现更好。BART可以看作是GPT和BERT的泛化。正如本节标题所示，BART能够同时完成生成和分类文本的任务。它能够处理这两个任务的原因在于该模型配备了双向编码器和从左到右的自回归解码器。'
- en: 'You may wonder how this is different from the original transformer. There are
    a few changes to the model size along with some minor changes such as activation
    function choices. However, one of the more interesting changes is that BART works
    with different model inputs. The original transformer model was designed for language
    translation so there are two inputs: the text to be translated (source sequence)
    for the encoder and the translation (target sequence) for the decoder. Additionally,
    the decoder also receives the encoded source sequence, as illustrated earlier
    in *Figure 16.6*. However, in BART, the input format was generalized such that
    it only uses the source sequence as input. BART can perform a wider range of tasks
    including language translation, where a target sequence is still required to compute
    the loss and fine-tune the model, but it is not necessary to feed it directly
    into the decoder.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道这与原始变压器的区别在哪里。模型大小有一些变化，以及一些较小的更改，如激活函数的选择。然而，其中一个更有趣的变化是，BART使用不同的模型输入。原始变压器模型是为语言翻译设计的，因此有两个输入：要翻译的文本（编码器的源序列）和翻译（解码器的目标序列）。此外，解码器还接收编码的源序列，如前文所述在*图16.6*中。然而，在BART中，输入格式被泛化，只使用源序列作为输入。BART可以执行更广泛的任务，包括语言翻译，在这种情况下仍然需要目标序列来计算损失并微调模型，但不需要直接将其馈送到解码器中。
- en: 'Now let us take a closer look at the BART’s model structure. As previously
    mentioned, BART is composed of a bidirectional encoder and an autoregressive decoder.
    Upon receiving a training example as plain text, the input will first be “corrupted”
    and then encoded by the encoder. These input encodings will then be passed to
    the decoder, along with the generated tokens. The cross-entropy loss between encoder
    output and the original text will be calculated and then optimized through the
    learning process. Think of a transformer where we have two texts in different
    languages as input to the decoder: the initial text to be translated (source text)
    and the generated text in the target language. BART can be understood as replacing
    the former with corrupted text and the latter with the input text itself.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看一下BART的模型结构。如前所述，BART由一个双向编码器和一个自回归解码器组成。在接收到一个纯文本的训练示例后，输入将首先被“污染”，然后由编码器编码。这些输入编码将随后传递给解码器，连同生成的标记一起。编码器输出与原始文本之间的交叉熵损失将被计算，然后通过学习过程进行优化。想象一个转换器，我们在解码器中有两种不同语言的文本作为输入：要翻译的初始文本（源文本）和目标语言中生成的文本。BART可以被理解为用损坏的文本替换前者，用输入文本本身替换后者。
- en: '![Diagram  Description automatically generated](img/B17582_16_16.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_16_16.png)'
- en: 'Figure 16.16: BART’s model structure'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16：BART的模型结构
- en: 'To explain the corruption step in a bit more detail, recall that BERT and GPT
    are pre-trained by reconstructing masked words: BERT is “filling in the blanks”
    and GPT is “predicting the next word.” These pre-training tasks can also be recognized
    as reconstructing corrupted sentences because masking words is one way of corrupting
    a sentence. BART provides the following corruption methods that can be applied
    to the clean text:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地解释一下损坏步骤，回想一下，BERT和GPT是通过重构掩码单词进行预训练的：BERT是“填补空白”，而GPT是“预测下一个单词”。这些预训练任务也可以被视为重构损坏的句子，因为掩盖单词是损坏句子的一种方式。BART提供以下可以应用于清洁文本的损坏方法：
- en: Token masking
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记掩盖
- en: Token deletion
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记删除
- en: Text infilling
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本填充
- en: Sentence permutation
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子排列
- en: Document rotation
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档旋转
- en: One or more of the techniques listed above can be applied to the same sentence;
    in the worst scenario, where all the information is contaminated and corrupted,
    the text becomes useless. Hence, the encoder has limited utility, and with only
    the decoder module working properly, the model will essentially become more similar
    to a unidirectional language.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 以上列出的一种或多种技术可以应用于同一句子；在最坏的情况下，所有信息都被污染和破坏，文本变得毫无用处。因此，编码器的效用有限，只有解码器模块正常工作时，模型才会更像是单向语言。
- en: BART can be fine-tuned on a wide range of downstream tasks including (a) sequence
    classification, (b) token classification, (c) sequence generation, and (d) machine
    translation. As with BERT, small changes to the inputs need to be made in order
    to perform different tasks.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: BART可以在广泛的下游任务中进行微调，包括（a）序列分类，（b）标记分类，（c）序列生成和（d）机器翻译。与BERT一样，需要对输入进行微小的更改以执行不同的任务。
- en: In the sequence classification task, an additional token needs to be attached
    to the input to serve as the generated label token, which is similar to the [CLS]
    token in BERT. Also, instead of disturbing the input, uncorrupted input is fed
    into both the encoder and decoder so that the model can make full use of the input.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列分类任务中，需要附加一个额外的令牌到输入中，作为生成的标签令牌，这类似于 BERT 中的 [CLS] 令牌。此外，不会破坏输入，而是将未损坏的输入同时馈送到编码器和解码器，以便模型能够充分利用输入。
- en: For token classification, additional tokens become unnecessary, and the model
    can directly use the generated representation for each token for classification.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于令牌分类，额外的令牌变得不必要，模型可以直接使用每个令牌的生成表示进行分类。
- en: Sequence generation in BART differs a bit from GPT because of the existence
    of the encoder. Instead of generating text from the ground up, sequence generation
    tasks via BART are more comparable to summarization, where the model is given
    a corpus of contexts and asked to generate a summary or an abstractive answer
    to certain questions. To this end, whole input sequences are fed into the encoder
    while the decoder generates output autoregressively.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: BART 中的序列生成与 GPT 有所不同，这是因为存在编码器。通过 BART 进行序列生成任务不是从头开始生成文本，而更类似于摘要，模型被给定一组上下文并要求生成摘要或对特定问题的抽象回答。为此，整个输入序列被馈送到编码器，而解码器则自回归地生成输出。
- en: 'Finally, it’s natural for BART to perform machine translation considering the
    similarity between BART and the original transformer. However, instead of following
    the exact same procedure as for training the original transformer, researchers
    considered the possibility of incorporating the entire BART model as a pre-trained
    decoder. To complete the translation model, a new set of randomly initialized
    parameters is added as a new, additional encoder. Then, the fine-tuning stage
    can be accomplished in two steps:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，考虑到 BART 与原始变压器之间的相似性，自然而然地可以进行机器翻译。但是，与训练原始变压器的完全相同过程不同，研究人员考虑了将整个 BART
    模型作为预训练解码器并将新的一组随机初始化参数作为新的附加编码器添加来完成翻译模型。然后，微调阶段可以分为两步：
- en: First, freeze all the parameters except the encoder
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，冻结除编码器外的所有参数。
- en: Then, update all parameters in the model
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，更新模型中的所有参数
- en: BART was evaluated on several benchmark datasets for various tasks, and it obtained
    very competitive results compared to other famous language models such as BERT.
    In particular, for generation tasks including abstractive question answering,
    dialogue response, and summarization tasks, BART achieved state-of-the-art results.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: BART 在几个基准数据集上进行了评估，用于各种任务，并与其他著名的语言模型（如 BERT）相比，取得了非常有竞争力的结果。特别是在生成任务中，包括抽象问答、对话回复和摘要任务中，BART
    实现了最先进的结果。
- en: Fine-tuning a BERT model in PyTorch
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中微调 BERT 模型
- en: Now that we have introduced and discussed all the necessary concepts and the
    theory behind the original transformer and popular transformer-based models, it’s
    time to take a look at the more practical part! In this section, you will learn
    how to fine-tune a BERT model for **sentiment classification** in PyTorch.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经介绍并讨论了所有必要的概念以及原始变压器和流行的基于变压器的模型背后的理论，现在是时候看看更实际的部分了！在本节中，您将学习如何在 PyTorch
    中对 BERT 模型进行**情感分类**的微调。
- en: Note that although there are many other transformer-based models to choose from,
    BERT provides a nice balance between model popularity and having a manageable
    model size so that it can be fine-tuned on a single GPU. Note also that pre-training
    a BERT from scratch is painful and quite unnecessary considering the availability
    of the `transformers` Python package provided by Hugging Face, which includes
    a bunch of pre-trained models that are ready for fine-tuning.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然有许多其他选择的基于变压器的模型，但 BERT 在模型流行度和具有可管理模型大小之间提供了良好的平衡，因此可以在单个 GPU 上进行微调。还请注意，从头开始预训练
    BERT 是痛苦且相当不必要的，考虑到 Hugging Face 提供的 `transformers` Python 包中包含了一堆准备好进行微调的预训练模型。
- en: In the following sections, you’ll see how to prepare and tokenize the IMDb movie
    review dataset and fine-tune the distilled BERT model to perform sentiment classification.
    We deliberately chose sentiment classification as a simple but classic example,
    though there are many other fascinating applications of language models. Also,
    by using the familiar IMDb movie review dataset, we can get a good idea of the
    predictive performance of the BERT model by comparing it to the logistic regression
    model in *Chapter 8*, *Applying Machine Learning to Sentiment Analysis*, and the
    RNN in*Chapter 15*, *Modeling Sequential Data Using Recurrent Neural Networks*.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，您将看到如何准备和标记化IMDb电影评论数据集，并对精炼的BERT模型进行微调以执行情感分类。尽管有许多其他有趣的语言模型应用，我们故意选择情感分类作为一个简单但经典的例子。此外，通过使用熟悉的IMDb电影评论数据集，我们可以通过将其与逻辑回归模型在*第8章*
    *应用机器学习进行情感分析*中和RNN在*第15章* *使用递归神经网络建模顺序数据*中进行比较，来获取BERT模型的预测性能。
- en: Loading the IMDb movie review dataset
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载IMDb电影评论数据集
- en: In this subsection, we will begin by loading the required packages and the dataset,
    split into train, validation, and test sets.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们将首先加载所需的包和数据集，并将其分为训练集、验证集和测试集。
- en: For the BERT-related parts of this tutorial, we will mainly use the open-source
    `transformers` library ([https://huggingface.co/transformers/](https://huggingface.co/transformers/))
    created by Hugging Face, which we installed in the previous section, *Using GPT-2
    to generate new text*.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中与BERT相关的部分，我们将主要使用Hugging Face创建的开源`transformers`库（[https://huggingface.co/transformers/](https://huggingface.co/transformers/)），该库已在前一节中安装好，*使用GPT-2生成新文本*。
- en: The **DistilBERT** model we are using in this chapter is a lightweight transformer
    model created by distilling a pre-trained BERT base model. The original uncased
    BERT base model contains over 110 million parameters while DistilBERT has 40 percent
    fewer parameters. Also, DistilBERT runs 60 percent faster and still preserves
    95 percent of BERT’s performance on the GLUE language understanding benchmark.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们使用的**DistilBERT**模型是一个轻量级的transformer模型，是通过蒸馏预训练的BERT基础模型而来。原始的不区分大小写的BERT基础模型包含超过1.1亿个参数，而DistilBERT的参数少了40%。此外，DistilBERT运行速度快了60%，同时还保留了GLUE语言理解基准测试中BERT性能的95%。
- en: 'The following code imports all the packages we will be using in this chapter
    to prepare the data and fine-tune the DistilBERT model:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码导入了本章中将要使用的所有包，以准备数据并微调DistilBERT模型：
- en: '[PRE36]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we specify some general settings, including the number of epochs we train
    the network on, the device specification, and the random seed. To reproduce the
    results, make sure to set a specific random seed such as `123`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定一些通用设置，包括网络训练的时代数、设备规范和随机种子。为了复现结果，请确保设置一个特定的随机种子，例如`123`：
- en: '[PRE37]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We will be working on the IMDb movie review dataset, which you have already
    seen in *Chapters 8* and *15*. The following code fetches the compressed dataset
    and unzips it:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理IMDb电影评论数据集，您已经在*第8章*和*第15章*中见过它。以下代码获取了压缩数据集并解压缩它：
- en: '[PRE38]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If you have the `movie_data.csv` file from *Chapter 8* still on your hard drive,
    you can skip this download and unzip procedure.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仍然在硬盘上拥有*第8章*中的`movie_data.csv`文件，可以跳过此下载和解压缩过程。
- en: 'Next, we load the data into a pandas `DataFrame` and make sure it looks all
    right:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据加载到一个pandas的`DataFrame`中，并确保一切正常：
- en: '[PRE39]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Table  Description automatically generated](img/B17582_16_17.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![表 自动描述生成](img/B17582_16_17.png)'
- en: 'Figure 16.17: The first three rows of the IMDb movie review dataset'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17：IMDb电影评论数据集的前三行
- en: 'The next step is to split the dataset into separate training, validation, and
    test sets. Here, we use 70 percent of the reviews for the training set, 10 percent
    for the validation set, and the remaining 20 percent for testing:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据集分割为单独的训练、验证和测试集。在这里，我们使用70％的评论作为训练集，10％作为验证集，剩余的20％作为测试集：
- en: '[PRE40]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Tokenizing the dataset
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对数据集进行标记化处理
- en: 'So far, we have obtained the texts and labels for the training, validation,
    and test sets. Now, we are going to tokenize the texts into individual word tokens
    using the tokenizer implementation inherited from the pre-trained model class:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经获取了训练集、验证集和测试集的文本和标签。现在，我们将使用继承自预训练模型类的分词器实现，将文本标记化为单独的单词标记：
- en: '[PRE41]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '**Choosing different tokenizers**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择不同的分词器**'
- en: If you are interested in applying different types of tokenizers, feel free to
    explore the `tokenizers` package ([https://huggingface.co/docs/tokenizers/python/latest/](https://huggingface.co/docs/tokenizers/python/latest/)),
    which is also built and maintained by Hugging Face. However, inherited tokenizers
    maintain the consistency between the pre-trained model and the dataset, which
    saves us the extra effort of finding the specific tokenizer corresponding to the
    model. In other words, using an inherited tokenizer is the recommended approach
    if you want to fine-tune a pre-trained model.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣应用不同类型的分词器，请随时探索`tokenizers`包（[https://huggingface.co/docs/tokenizers/python/latest/](https://huggingface.co/docs/tokenizers/python/latest/)），该包也由Hugging
    Face构建和维护。然而，继承的分词器保持了预训练模型与数据集之间的一致性，这样可以节省我们找到与模型对应的特定分词器的额外工作。换句话说，如果您想要微调一个预训练模型，使用继承的分词器是推荐的方法。
- en: 'Finally, let’s pack everything into a class called `IMDbDataset` and create
    the corresponding data loaders. Such a self-defined dataset class lets us customize
    all the related features and functions for our custom movie review dataset in
    `DataFrame` format:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将所有内容打包到一个名为`IMDbDataset`的类中，并创建相应的数据加载器。这样一个自定义的数据集类允许我们为我们自定义的电影评论数据集中的所有相关特征和函数进行定制：
- en: '[PRE42]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: While the overall data loader setup should be familiar from previous chapters,
    one noteworthy detail is the `item` variable in the `__getitem__` method. The
    encodings we produced previously store a lot of information about the tokenized
    texts. Via the dictionary comprehension that we use to assign the dictionary to
    the `item` variable, we are only extracting the most relevant information. For
    instance, the resulting dictionary entries include `input_ids` (unique integers
    from the vocabulary corresponding to the tokens), `labels` (the class labels),
    and `attention_mask`. Here, `attention_mask` is a tensor with binary values (0s
    and 1s) that denotes which tokens the model should attend to. In particular, 0s
    correspond to tokens used for padding the sequence to equal lengths and are ignored
    by the model; the 1s correspond to the actual text tokens.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管整体数据加载器设置应该与前几章类似，但一个值得注意的细节是 `__getitem__` 方法中的 `item` 变量。我们之前生成的编码存储了关于标记化文本的大量信息。通过我们用于将字典分配给
    `item` 变量的字典推导式，我们只提取了最相关的信息。例如，生成的字典条目包括 `input_ids`（词汇表中对应于标记的唯一整数）、`labels`（类标签）和
    `attention_mask`。这里，`attention_mask` 是一个具有二进制值（0和1）的张量，指示模型应该关注哪些标记。特别地，0对应于用于填充序列以达到相等长度的标记，并且被模型忽略；1对应于实际文本标记。
- en: Loading and fine-tuning a pre-trained BERT model
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和微调预训练的BERT模型
- en: 'Having taken care of the data preparation, in this subsection, you will see
    how to load the pre-trained DistilBERT model and fine-tune it using the dataset
    we just created. The code for loading the pre-trained model is as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据准备后，在本小节中，您将看到如何加载预训练的DistilBERT模型并使用我们刚刚创建的数据集进行微调。加载预训练模型的代码如下所示：
- en: '[PRE43]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`DistilBertForSequenceClassification` specifies the downstream task we want
    to fine-tune the model on, which is sequence classification in this case. As mentioned
    before, `''distilbert-base-uncased''` is a lightweight version of a BERT uncased
    base model with manageable size and good performance. Note that “uncased” means
    that the model does not distinguish between upper- and lower-case letters.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '`DistilBertForSequenceClassification` 指定了我们希望在其上微调模型的下游任务，即在本例中进行的序列分类。正如前面提到的，`''distilbert-base-uncased''`
    是一个轻量级的BERT小写基础模型，大小适中且性能良好。注意，“uncased”表示该模型不区分大小写字母。'
- en: '**Using other pre-trained transformers**'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用其他预训练的transformers**'
- en: The *transformers* package also provides many other pre-trained models and various
    downstream tasks for fine-tuning. Check them out at [https://huggingface.co/transformers/](https://huggingface.co/transformers/).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '*transformers* 包还提供了许多其他预训练模型和各种下游任务供微调使用。请访问 [https://huggingface.co/transformers/](https://huggingface.co/transformers/)
    查看详情。'
- en: 'Now, it’s time to train the model. We can break this up into two parts. First,
    we need to define an accuracy function to evaluate the model performance. Note
    that this accuracy function computes the conventional classification accuracy.
    Why is it so verbose? Here, we are loading the dataset batch by batch to work
    around RAM or GPU memory (VRAM) limitations when working with a large deep learning
    model:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是训练模型的时候了。我们可以将其分为两部分。首先，我们需要定义一个准确率函数来评估模型性能。请注意，这个准确率函数计算传统的分类准确率。为什么这么啰嗦？在这里，我们按批次加载数据集以解决在使用大型深度学习模型时可能遇到的RAM或GPU内存（VRAM）限制：
- en: '[PRE44]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In the `compute_accuracy` function, we load a given batch and then obtain the
    predicted labels from the outputs. While doing this, we keep track of the total
    number of examples via `num_examples`. Similarly, we keep track of the number
    of correct predictions via the `correct_pred` variable. Finally, after we iterate
    over the complete dataset, we compute the accuracy as the proportion of correctly
    predicted labels.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在`compute_accuracy`函数中，我们加载一个给定的批次，然后从输出中获取预测标签。在这个过程中，我们通过`num_examples`变量跟踪总样本数。类似地，我们通过`correct_pred`变量跟踪正确预测的数量。最后，在完整数据集上迭代完成后，我们计算准确率作为正确预测标签的比例。
- en: Overall, via the `compute_accuracy` function, you can already get a glimpse
    at how we can use the transformer model to obtain the class labels. That is, we
    feed the model the `input_ids` along with the `attention_mask` information that,
    here, denotes whether a token is an actual text token or a token for padding the
    sequences to equal length. The `model` call then returns the outputs, which is
    a transformer library-specific `SequenceClassifierOutput` object. From this object,
    we then obtain the logits that we convert into class labels via the `argmax` function
    as we have done in previous chapters.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`compute_accuracy`函数，您已经可以大致了解如何使用transformer模型获取类标签。也就是说，我们将`input_ids`和`attention_mask`信息（在这里表示一个标记是实际文本标记还是用于填充序列以达到相等长度的标记）馈送到模型中。然后，`model`调用返回一个特定于transformer库的`SequenceClassifierOutput`对象的输出。然后，我们通过`argmax`函数将这个对象中的logits转换为类标签，就像我们在前几章中所做的那样。
- en: 'Finally, let us get to the main part: the training (or rather, fine-tuning)
    loop. As you will notice, fine-tuning a model from the *transformers* library
    is very similar to training a model in pure PyTorch from scratch:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们进入主要部分：训练（或者说，微调）循环。正如您将注意到的那样，从*transformers*库微调模型与从头开始在纯PyTorch中训练模型非常相似：
- en: '[PRE45]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output produced by the preceding code is as follows (note that the code
    is not fully deterministic, which is why the results you are getting may be slightly
    different):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成的输出如下（请注意，代码并非完全确定性，因此您得到的结果可能略有不同）：
- en: '[PRE46]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In this code, we iterate over multiple epochs. In each epoch we perform the
    following steps:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们遍历多个epochs。在每个epoch中，我们执行以下步骤：
- en: Load the input into the device we are working on (GPU or CPU)
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入加载到我们正在工作的设备上（GPU或CPU）。
- en: Compute the model output and loss
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型输出和损失。
- en: Adjust the weight parameters by backpropagating the loss
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过反向传播损失来调整权重参数。
- en: Evaluate the model performance on both the training and validation set
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集和验证集上的模型性能进行评估。
- en: Note that the training time may vary on different devices. After three epochs,
    accuracy on the test dataset reaches around 93 percent, which is a substantial
    improvement compared to the 85 percent test accuracy that the RNN achieved in
    *Chapter 15*.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，不同设备上的训练时间可能会有所不同。在三个epochs后，测试数据集的准确率达到约93％，与RNN在*第15章*中达到的85％的测试准确率相比，这是一个显著的改进。
- en: Fine-tuning a transformer more conveniently using the Trainer API
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Trainer API更方便地微调transformer模型。
- en: In the previous subsection, we implemented the training loop in PyTorch manually
    to illustrate that fine-tuning a transformer model is really not that much different
    from training an RNN or CNN model from scratch. However, note that the `transformers`
    library contains several nice extra features for additional convenience, like
    the Trainer API, which we will introduce in this subsection.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的小节中，我们手动在PyTorch中实现了训练循环，以说明微调transformer模型与从头开始训练RNN或CNN模型实际上并没有太大不同。但是，请注意，`transformers`库包含一些额外的便利功能，例如我们将在本小节中介绍的Trainer
    API。
- en: The Trainer API provided by Hugging Face is optimized for transformer models
    with a wide range of training options and various built-in features. When using
    the Trainer API, we can skip the effort of writing training loops on our own,
    and training or fine-tuning a transformer model is as simple as a function (or
    method) call. Let’s see how this works in practice.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face提供的Trainer API针对具有广泛的训练选项和各种内置功能的Transformer模型进行了优化。使用Trainer API时，我们可以跳过自己编写训练循环的工作，训练或微调Transformer模型就像调用函数（或方法）一样简单。让我们看看实际操作中是如何工作的。
- en: After loading the pre-trained model via
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载预训练模型之后：
- en: '[PRE47]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The training loop from the previous section can then be replaced by the following
    code:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用以下代码替换上一节的训练循环：
- en: '[PRE48]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In the preceding code snippets, we first defined the training arguments, which
    are relatively self-explanatory settings regarding the input and output locations,
    number of epochs, and batch sizes. We tried to keep the settings as simple as
    possible; however, there are many additional settings available, and we recommend
    consulting the `TrainingArguments` documentation page for additional details:
    [https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments).'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码片段中，我们首先定义了相对简单的训练参数设置，这些设置涉及输入和输出位置、epoch数量和批处理大小。我们尽量保持设置尽可能简单；然而，还有许多其他可用的设置，建议查阅`TrainingArguments`文档页面以获取更多详细信息：[https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments)。
- en: We then passed these `TrainingArguments` settings to the `Trainer` class to
    instantiate a new `trainer` object. After initiating the `trainer` with the settings,
    the model to be fine-tuned, and the training and evaluation sets, we can train
    the model by calling the `trainer.train()` method (we will use this method further
    shortly). That’s it, using the Trainer API is as simple as shown in the preceding
    code, and no further boilerplate code is required.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这些`TrainingArguments`设置传递给`Trainer`类，以实例化一个新的`trainer`对象。在用设置初始化了`trainer`之后，需要调用`trainer.train()`方法来训练模型（稍后我们将进一步使用此方法）。使用Trainer
    API就像前面的代码示例中展示的那样简单，不需要进一步的样板代码。
- en: However, you may have noticed that the test dataset was not involved in these
    code snippets, and we haven’t specified any evaluation metrics in this subsection.
    This is because the Trainer API only shows the training loss and does not provide
    model evaluation along the training process by default. There are two ways to
    display the final model performance, which we will illustrate next.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您可能已经注意到这些代码片段中没有涉及测试数据集，并且在本小节中我们还未指定任何评估指标。这是因为Trainer API仅显示训练损失，并且默认情况下不提供模型在训练过程中的评估。有两种方法可以显示最终模型的性能，我们将在接下来进行说明。
- en: 'The first method for evaluating the final model is to define an evaluation
    function as the `compute_metrics` argument for another `Trainer` instance. The
    `compute_metrics` function operates on the models’ test predictions as logits
    (which is the default output of the model) and the test labels. To instantiate
    this function, we recommend installing Hugging Face’s `datasets` library via `pip
    install datasets` and use it as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 评估最终模型的第一种方法是为另一个`Trainer`实例定义一个评估函数作为`compute_metrics`参数。`compute_metrics`函数操作模型测试预测的logits（这是模型的默认输出）和测试标签。为了实例化此函数，建议通过`pip
    install datasets`安装Hugging Face的`datasets`库，并按以下方式使用：
- en: '[PRE49]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The updated `Trainer` instantiation (now including `compute_metrics`) is then
    as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 更新后的`Trainer`实例化（现在包括`compute_metrics`）如下所示：
- en: '[PRE50]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, let’s train the model (again, note that the code is not fully deterministic,
    which is why you might be getting slightly different results):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们再次训练模型（请注意，代码不是完全确定性的，这就是为什么可能会得到稍微不同结果的原因）：
- en: '[PRE51]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'After the training has completed, which can take up to an hour depending on
    your GPU, we can call `trainer.evaluate()` to obtain the model performance on
    the test set:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后（具体时间取决于您的GPU，可能需要长达一小时），我们可以调用`trainer.evaluate()`来获取模型在测试集上的性能：
- en: '[PRE52]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As we can see, the evaluation accuracy is around 94 percent, similar to our
    own previously used PyTorch training loop. (Note that we have skipped the training
    step, because the `model` is already fine-tuned after the previous `trainer.train()`
    call.) There is a small discrepancy between our manual training approach and using
    the `Trainer` class, because the `Trainer` class uses some different and some
    additional settings.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，评估准确率约为94%，与我们之前使用的PyTorch训练循环相似。（请注意，我们已跳过训练步骤，因为在之前的`trainer.train()`调用后，`model`已经经过微调。）我们的手动训练方法与使用`Trainer`类有一些小差异，因为`Trainer`类使用了一些不同和一些额外的设置。
- en: 'The second method we could employ to compute the final test set accuracy is
    re-using our `compute_accuracy` function that we defined in the previous section.
    We can directly evaluate the performance of the fine-tuned model on the test dataset
    by running the following code:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采用第二种方法来计算最终的测试集准确率，即重新使用我们在前一节中定义的`compute_accuracy`函数。我们可以通过运行以下代码直接评估经过微调的模型在测试数据集上的性能：
- en: '[PRE53]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In fact, if you want to check the model’s performance regularly during training,
    you can require the trainer to print the model evaluation after each epoch by
    defining the training arguments as follows:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果您想在训练过程中定期检查模型的性能，可以通过以下方式定义训练参数，要求训练器在每个epoch后打印模型评估：
- en: '[PRE54]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'However, if you are planning to change or optimize hyperparameters and repeat
    the fine-tuning procedure several times, we recommend using the validation set
    for this purpose, in order to keep the test set independent. We can achieve this
    by instantiating the `Trainer` using `valid_dataset`:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您计划更改或优化超参数，并重复几次微调过程，我们建议使用验证集来实现此目的，以保持测试集的独立性。我们可以通过使用`valid_dataset`实例化`Trainer`来实现这一点：
- en: '[PRE55]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In this section, we saw how we can fine-tune a BERT model for classification.
    This is different from using other deep learning architectures like RNNs, which
    we usually train from scratch. However, unless we are doing research and are trying
    to develop new transformer architectures—a very expensive endeavor—pre-training
    transformer models is not necessary. Since transformer models are trained on general,
    unlabeled dataset resources, pre-training them ourselves may not be a good use
    of our time and resources; fine-tuning is the way to go.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何为分类任务微调BERT模型。这与使用其他深度学习架构（如通常从头开始训练的RNN）有所不同。然而，除非我们正在进行研究并试图开发新的transformer架构（这是一项非常昂贵的工作），否则预训练transformer模型并不是必要的。由于transformer模型是在通用的未标记数据集资源上训练的，自己进行预训练可能并不是一个很好的时间和资源利用方式；微调才是正确的方法。
- en: Summary
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a whole new model architecture for natural language
    processing, the transformer architecture. The transformer architecture is built
    on a concept called self-attention, and we started introducing this concept step
    by step. First, we looked at an RNN outfitted with attention in order to improve
    its translation capabilities for long sentences. Then, we gently introduced the
    concept of self-attention and explained how it is used in the multi-head attention
    module within the transformer.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了自然语言处理的一种全新模型架构，即transformer架构。transformer架构建立在一种称为自注意力的概念之上，并逐步介绍了这一概念。首先，我们研究了带有注意力机制的RNN，以提高其对长句子的翻译能力。然后，我们温柔地引入了自注意力的概念，并解释了它在transformer中如何在多头注意力模块中使用。
- en: 'Many different derivatives of the transformer architecture have emerged and
    evolved since the original transformer was published in 2017\. In this chapter,
    we focused on a selection of some of the most popular ones: the GPT model family,
    BERT, and BART. GPT is a unidirectional model that is particularly good at generating
    new text. BERT takes a bidirectional approach, which is better suited for other
    types of tasks, for example, classification. Lastly, BART combines both the bidirectional
    encoder from BERT and the unidirectional decoder from GPT. Interested readers
    can find out about additional transformer-based architectures via the following
    two survey articles:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 自从2017年首次发布transformer架构以来，已经涌现出许多不同的衍生变体并不断演化。在本章中，我们重点介绍了一些最受欢迎的变体：GPT模型系列、BERT和BART。GPT是一个单向模型，特别擅长生成新的文本。BERT采用双向方法，更适合其他类型的任务，例如分类。最后，BART结合了BERT的双向编码器和GPT的单向解码器。有兴趣的读者可以通过以下两篇调研文章了解更多基于transformer的架构：
- en: '*Pre-trained Models for Natural Language Processing: A Survey* by *Qiu* and
    colleagues, 2020\. Available at [https://arxiv.org/abs/2003.08271](https://arxiv.org/abs/2003.08271)'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*自然语言处理的预训练模型：一项调查*，由*Qiu*和同事们，2020年。可在 [https://arxiv.org/abs/2003.08271](https://arxiv.org/abs/2003.08271)
    获取。'
- en: '*AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language
    Processing* by *Kayan* and colleagues, 2021\. Available at [https://arxiv.org/abs/2108.05542](https://arxiv.org/abs/2108.05542)'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*AMMUS：自然语言处理中基于 Transformer 预训练模型的调查*，由*Kayan*和同事们，2021年。可在 [https://arxiv.org/abs/2108.05542](https://arxiv.org/abs/2108.05542)
    获取。'
- en: Transformer models are generally more data hungry than RNNs and require large
    amounts of data for pre-training. The pre-training leverages large amounts of
    unlabeled data to build a general language model that can then be specialized
    to specific tasks by fine-tuning it on smaller labeled datasets.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型通常比 RNNs 更需要数据，并且需要大量数据进行预训练。预训练利用大量未标记数据构建一个通用语言模型，然后通过在较小的有标签的数据集上微调它，使其专门用于特定任务。
- en: To see how this works in practice, we downloaded a pre-trained BERT model from
    the Hugging Face `transformers` library and fine-tuned it for sentiment classification
    on the IMDb movie review dataset.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到这在实践中是如何运作的，我们从 Hugging Face 的 `transformers` 库中下载了一个预训练的 BERT 模型，并对其在 IMDb
    电影评论数据集上进行了情感分类微调。
- en: In the next chapter, we will discuss generative adversarial networks. As the
    name suggests, generative adversarial networks are models that can be used for
    generating new data, similar to the GPT models we discussed in this chapter. However,
    we are now leaving the natural language modeling topic behind us and will look
    at generative adversarial networks in the context of computer vision and generating
    new images, the task that these networks were originally designed for.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论生成对抗网络。正如其名称所示，生成对抗网络是可以用于生成新数据的模型，类似于我们在本章讨论的 GPT 模型。然而，我们现在将自然语言建模的主题抛在身后，将在计算机视觉的背景下研究生成对抗网络并生成新图像——这正是这些网络最初设计用于的任务。
- en: Join our book’s Discord space
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 加入这本书的 Discord 工作区，与作者进行每月一次的*问答*会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
