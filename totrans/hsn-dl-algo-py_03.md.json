["```py\npip install tensorflow==1.13.1\n```", "```py\nimport tensorflow as tf\n\nhello = tf.constant(\"Hello TensorFlow!\")\nsess = tf.Session()\nprint(sess.run(hello))\n```", "```py\nimport tensorflow as tf\n\nx = 2\ny = 3\nz = tf.add(x, y, name='Add')\n```", "```py\na = tf.multiply(8,5)\nb = tf.multiply(a,1)\n```", "```py\na = tf.multiply(8,5)\nb = tf.multiply(4,3)\n```", "```py\ngraph = tf.Graph()\n\nwith graph.as_default():\n     z = tf.add(x, y, name='Add')\n```", "```py\nsess = tf.Session()\n```", "```py\na = tf.multiply(3,3)\nprint(a)\n```", "```py\na = tf.multiply(3,3)\nwith tf.Session as sess:\n    print(sess.run(a))\n```", "```py\nx = tf.Variable(13)\n```", "```py\nW = tf.Variable(tf.random_normal([500, 111], stddev=0.35), name=\"weights\")\n```", "```py\nW2 = tf.Variable(weights.initialized_value(), name=\"weights_2\")\n```", "```py\nx = tf.Variable(1212)\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n  sess.run(init) \n  print sess.run(x)\n```", "```py\nW3 = tf.get_variable(name = 'weights', shape = [500, 111], initializer = random_normal_initializer()))\n```", "```py\nwith tf.variable_scope(\"scope\"):\n a = tf.get_variable('x', [2])\n\nwith tf.variable_scope(\"scope\", reuse = True):\n b = tf.get_variable('x', [2])\n```", "```py\n x = tf.constant(13)\n```", "```py\n x = tf.placeholder(\"float\", shape=None)\n```", "```py\nx = tf.placeholder(\"float\", None)\ny = x +3\n\nwith tf.Session() as sess:\n    result = sess.run(y)\n    print(result)\n```", "```py\nwith tf.Session() as sess:\n    result = sess.run(y, feed_dict={x: 5})\n    print(result)\n```", "```py\nwith tf.Session() as sess:\n    result = sess.run(y, feed_dict={x: [3,6,9]})\n    print(result)\n```", "```py\n[ 6\\.  9\\. 12.]\n```", "```py\nx = tf.placeholder(\"float\", [None, 2])\n```", "```py\nwith tf.Session() as sess:\n    x_val = [[1, 2,], \n              [3,4],\n              [5,6],\n              [7,8],]\n    result = sess.run(y, feed_dict={x: x_val})\n    print(result)\n```", "```py\n[[ 4\\.  5.]\n [ 6\\.  7.]\n [ 8\\.  9.]\n [10\\. 11.]]\n```", "```py\nx = tf.constant(1,name='x')\ny = tf.constant(1,name='y')\na = tf.constant(3,name='a')\nb = tf.constant(3,name='b')\n```", "```py\nprod1 = tf.multiply(x,y,name='prod1')\nprod2 = tf.multiply(a,b,name='prod2')\n```", "```py\nsum = tf.add(prod1,prod2,name='sum')\n```", "```py\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter(logdir='./graphs',graph=sess.graph)\n    print(sess.run(sum))\n```", "```py\ntensorboard --logdir=graphs --port=8000\n```", "```py\nwith tf.name_scope(\"Product\"):\n    with tf.name_scope(\"prod1\"):\n        prod1 = tf.multiply(x,y,name='prod1')\n\n    with tf.name_scope(\"prod2\"):\n        prod2 = tf.multiply(a,b,name='prod2')\n```", "```py\nwith tf.name_scope(\"sum\"):\n    sum = tf.add(prod1,prod2,name='sum')\n```", "```py\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter('./graphs', sess.graph)\n    print(sess.run(sum))\n```", "```py\ntensorboard --logdir=graphs --port=8000\n```", "```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```", "```py\nmnist = input_data.read_data_sets(\"data/mnist\", one_hot=True)\n```", "```py\nprint(\"No of images in training set {}\".format(mnist.train.images.shape))\nprint(\"No of labels in training set {}\".format(mnist.train.labels.shape))\n\nprint(\"No of images in test set {}\".format(mnist.test.images.shape))\nprint(\"No of labels in test set {}\".format(mnist.test.labels.shape))\n\nNo of images in training set (55000, 784)\nNo of labels in training set (55000, 10)\nNo of images in test set (10000, 784)\nNo of labels in test set (10000, 10)\n```", "```py\nimg1 = mnist.train.images[0].reshape(28,28)\nplt.imshow(img1, cmap='Greys')\n```", "```py\n#number of neurons in input layer\nnum_input = 784\n\n#num of neurons in hidden layer 1\nnum_hidden1 = 512\n\n#num of neurons in hidden layer 2\nnum_hidden2 = 256\n\n#num of neurons in hidden layer 3\nnum_hidden_3 = 128\n\n#num of neurons in output layer\nnum_output = 10\n```", "```py\nwith tf.name_scope('input'):\n    X = tf.placeholder(\"float\", [None, num_input])\n\nwith tf.name_scope('output'):\n    Y = tf.placeholder(\"float\", [None, num_output])\n```", "```py\nwith tf.name_scope('weights'):\n\n weights = {\n 'w1': tf.Variable(tf.truncated_normal([num_input, num_hidden1], stddev=0.1),name='weight_1'),\n 'w2': tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=0.1),name='weight_2'),\n 'w3': tf.Variable(tf.truncated_normal([num_hidden2, num_hidden_3], stddev=0.1),name='weight_3'),\n 'out': tf.Variable(tf.truncated_normal([num_hidden_3, num_output], stddev=0.1),name='weight_4'),\n }\n```", "```py\nwith tf.name_scope('biases'):\n\n    biases = {\n        'b1': tf.Variable(tf.constant(0.1, shape=[num_hidden1]),name='bias_1'),\n        'b2': tf.Variable(tf.constant(0.1, shape=[num_hidden2]),name='bias_2'),\n        'b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3]),name='bias_3'),\n        'out': tf.Variable(tf.constant(0.1, shape=[num_output]),name='bias_4')\n    }\n```", "```py\nwith tf.name_scope('Model'):\n\n    with tf.name_scope('layer1'):\n        layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['w1']), biases['b1']) ) \n\n    with tf.name_scope('layer2'):\n        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['w2']), biases['b2']))\n\n    with tf.name_scope('layer3'):\n        layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['w3']), biases['b3']))\n\n    with tf.name_scope('output_layer'):\n         y_hat = tf.nn.sigmoid(tf.matmul(layer_3, weights['out']) + biases['out'])\n```", "```py\nwith tf.name_scope('Loss'):\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=Y))\n```", "```py\nlearning_rate = 1e-4\noptimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n```", "```py\nwith tf.name_scope('Accuracy'):\n\n    predicted_digit = tf.argmax(y_hat, 1)\n    actual_digit = tf.argmax(Y, 1)\n\n    correct_pred = tf.equal(predicted_digit,actual_digit)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n```", "```py\ntf.summary.scalar(\"Accuracy\", accuracy)\ntf.summary.scalar(\"Loss\", loss)\n```", "```py\nmerge_summary = tf.summary.merge_all()\n```", "```py\ninit = tf.global_variables_initializer()\n```", "```py\nlearning_rate = 1e-4\nnum_iterations = 1000\nbatch_size = 128\n```", "```py\nwith tf.Session() as sess:\n```", "```py\n    sess.run(init)\n```", "```py\n    summary_writer = tf.summary.FileWriter('./graphs', graph=sess.graph)\n```", "```py\n    for i in range(num_iterations):\n```", "```py\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n```", "```py\n        sess.run(optimizer, feed_dict={ X: batch_x, Y: batch_y})\n```", "```py\n        if i % 100 == 0:\n\n            batch_loss, batch_accuracy,summary = sess.run(\n                [loss, accuracy, merge_summary], feed_dict={X: batch_x, Y: batch_y}\n                )\n\n            #store all the summaries    \n            summary_writer.add_summary(summary, i)\n\n            print('Iteration: {}, Loss: {}, Accuracy: {}'.format(i,batch_loss,batch_accuracy))\n```", "```py\nIteration: 0, Loss: 2.30789709091, Accuracy: 0.1171875\nIteration: 100, Loss: 1.76062202454, Accuracy: 0.859375\nIteration: 200, Loss: 1.60075569153, Accuracy: 0.9375\nIteration: 300, Loss: 1.60388696194, Accuracy: 0.890625\nIteration: 400, Loss: 1.59523034096, Accuracy: 0.921875\nIteration: 500, Loss: 1.58489584923, Accuracy: 0.859375\nIteration: 600, Loss: 1.51407408714, Accuracy: 0.953125\nIteration: 700, Loss: 1.53311181068, Accuracy: 0.9296875\nIteration: 800, Loss: 1.57677125931, Accuracy: 0.875\nIteration: 900, Loss: 1.52060437202, Accuracy: 0.9453125\n```", "```py\nx = tf.constant(11)\ny = tf.constant(11)\nz = x*y\n\nwith tf.Session() as sess:\n    print sess.run(z)\n```", "```py\nx = tf.constant(11)\ny = tf.constant(11)\nz = x*y\n\nprint z\n```", "```py\n<tf.Tensor: id=789, shape=(), dtype=int32, numpy=121>\n```", "```py\nz.numpy()\n\n121\n```", "```py\nx = tf.constant([1., 2., 3.])\ny = tf.constant([3., 2., 1.])\n```", "```py\nsum = tf.add(x,y)\nsum.numpy()\n\narray([4., 4., 4.], dtype=float32)\n```", "```py\ndifference = tf.subtract(x,y)\ndifference.numpy()\n\narray([-2.,  0.,  2.], dtype=float32)\n```", "```py\nproduct = tf.multiply(x,y)\nproduct.numpy()\n\narray([3., 4., 3.], dtype=float32)\n```", "```py\ndivision = tf.divide(x,y)\ndivision.numpy()\n\narray([0.33333334, 1\\.        , 3\\.        ], dtype=float32)\n```", "```py\ndot_product = tf.reduce_sum(tf.multiply(x, y))\ndot_product.numpy()\n\n10.0\n```", "```py\nx = tf.constant([10, 0, 13, 9])\n```", "```py\ntf.argmin(x).numpy()\n\n1\n```", "```py\ntf.argmax(x).numpy()\n\n2\n```", "```py\nx = tf.Variable([1,3,5,7,11])\ny = tf.Variable([1])\n\ntf.math.squared_difference(x,y).numpy()\n\n[  0,   4,  16,  36, 100]\n```", "```py\nprint x.dtype\n\ntf.int32\n```", "```py\nx = tf.cast(x, dtype=tf.float32)\n```", "```py\nprint x.dtype\n\ntf.float32\n```", "```py\nx = [[3,6,9], [7,7,7]]\ny = [[4,5,6], [5,5,5]]\n```", "```py\ntf.concat([x, y], 0).numpy()\n\narray([[3, 6, 9],\n       [7, 7, 7],\n       [4, 5, 6],\n       [5, 5, 5]], dtype=int32)\n```", "```py\ntf.concat([x, y], 1).numpy()\n\narray([[3, 6, 9, 4, 5, 6],\n       [7, 7, 7, 5, 5, 5]], dtype=int32)\n```", "```py\ntf.stack(x, axis=1).numpy()\n\narray([[3, 7],\n       [6, 7],\n       [9, 7]], dtype=int32)\n```", "```py\nx = tf.Variable([[1.0, 5.0], [2.0, 3.0]])\n\nx.numpy()\n\narray([[1., 5.],\n       [2., 3.]]\n```", "```py\ntf.reduce_mean(input_tensor=x).numpy() \n\n2.75\n```", "```py\ntf.reduce_mean(input_tensor=x, axis=0).numpy() \n\narray([1.5, 4\\. ], dtype=float32)\n```", "```py\ntf.reduce_mean(input_tensor=x, axis=1, keepdims=True).numpy()\n\narray([[3\\. ],\n       [2.5]], dtype=float32)\n```", "```py\ntf.random.normal(shape=(3,2), mean=10.0, stddev=2.0).numpy()\n\ntf.random.uniform(shape = (3,2), minval=0, maxval=None, dtype=tf.float32,).numpy()\n```", "```py\nx = tf.constant([7., 2., 5.])\n\ntf.nn.softmax(x).numpy()\n\narray([0.8756006 , 0.00589975, 0.11849965], dtype=float32)\n```", "```py\ndef square(x):\n  return tf.multiply(x, x)\n```", "```py\nwith tf.GradientTape(persistent=True) as tape:\n     print square(6.).numpy()\n\n36.0\n```", "```py\npip install tensorflow==2.0.0-alpha0\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n```", "```py\nmodel = Sequential()\n```", "```py\nmodel.add(Dense(13, input_dim=7, activation='relu'))\n```", "```py\nmodel.add(Dense(7, activation='relu'))\n```", "```py\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(13, input_dim=7, activation='relu'))\nmodel.add(Dense(7, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n```", "```py\ninput = Input(shape=(2,))\n```", "```py\nlayer1 = Dense(10, activation='relu')\n```", "```py\nlayer1 = Dense(10, activation='relu')(input)\n```", "```py\nlayer2 = Dense(10, activation='relu')(layer1)\n```", "```py\noutput = Dense(1, activation='sigmoid')(layer2)\n```", "```py\nmodel = Model(inputs=input, outputs=output)\n```", "```py\ninput = Input(shape=(2,))\nlayer1 = Dense(10, activation='relu')(input)\nlayer2 = Dense(10, activation='relu')(layer1)\noutput = Dense(1, activation='sigmoid')(layer2)\nmodel = Model(inputs=input, outputs=output)\n```", "```py\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n```", "```py\nmodel.fit(x=data, y=labels, epochs=100, batch_size=10)\n```", "```py\nmodel.evaluate(x=data_test,y=labels_test)\n```", "```py\nmodel.evaluate(x=data,y=labels)\n```", "```py\nmnist = tf.keras.datasets.mnist\n```", "```py\n(x_train,y_train), (x_test, y_test) = mnist.load_data()\n```", "```py\nx_train, x_test = tf.cast(x_train/255.0, tf.float32), tf.cast(x_test/255.0, tf.float32)\ny_train, y_test = tf.cast(y_train,tf.int64),tf.cast(y_test,tf.int64)\n```", "```py\nmodel = tf.keras.models.Sequential()\n```", "```py\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(256, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(128, activation=\"relu\"))\nmodel.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n```", "```py\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n```", "```py\nmodel.fit(x_train, y_train, batch_size=32, epochs=10)\n```", "```py\nmodel.evaluate(x_test, y_test)\n```"]