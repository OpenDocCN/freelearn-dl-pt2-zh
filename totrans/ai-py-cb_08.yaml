- en: Working with Moving Images
  prefs: []
  type: TYPE_NORMAL
- en: This chapter deals with video applications. While methods applied to images
    can be applied to single frames of videos, this usually comes with a loss of temporal
    consistency. We will try to strike a balance between what's possible on consumer
    hardware and what's interesting enough to show and implement.
  prefs: []
  type: TYPE_NORMAL
- en: Quite a few applications should come to mind when talking about video, such
    as object tracking, event detection (surveillance), deep fake, 3D scene reconstruction,
    and navigation (self-driving cars).
  prefs: []
  type: TYPE_NORMAL
- en: A lot of them require many hours or days of computation. We'll try to strike
    a sensible compromise between what's possible and what's interesting. This compromise
    might be felt more than in other chapters, where computations are not as demanding
    as for video. As part of this compromise, we'll work on videos frame by frame,
    rather than across the temporal domain. Still, as always, we'll try to work on
    problems by giving examples that are either representative of practical real-world
    applications, or that are at least similar.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll start with image detection, where an algorithm applies
    an image recognition model to different parts of an image in order to localize
    objects. We'll then give examples of how to apply this to a video feed. We'll
    then create videos using a deep fake model, and reference more related models
    for both creating and detecting deep fakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Localizing objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faking videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll use many standard libraries, including `keras` and `opencv`, but we'll
    see a few more libraries that we'll mention at the beginning of each recipe before
    they'll become relevant.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the notebooks for this chapter's recipes on GitHub at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter08](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Localizing objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection refers to identifying objects of particular classes in images
    and videos. For example, in self-driving cars, pedestrians and trees have to be
    identified in order to be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement an object detection algorithm in Keras. We'll
    apply it to a single image and then to our laptop camera. In the *How it works...*
    section, we'll discuss the theory and more algorithms for object detection.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this recipe, we''ll need the Python bindings for the **Open Computer Vision
    Library** (**OpenCV**) and `scikit-image`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As our example image, we''ll download an image from an object detection toolbox:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Please note that any other image will do.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use a code based on the `keras-yolo3` library, which was quick to set
    up with only a few changes. We can quickly download this as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we also need the weights for the `YOLOv3` network, which we can download
    from the darknet open source implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You should now have the example image, the `yolo3-keras` Python script, and
    the `YOLOv3` network weights in your local directory from which you run your notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll implement an object detection algorithm with Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll import the `keras-yolo3` library, load the pretrained weights, and then
    perform object detection given images or the video feed from a camera:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we have most of the object detection implemented in the `keras-yolo3`
    script, we only need to import it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then load our network with the pretrained weights as follows. Please
    note that the weight files are quite big – they''ll occupy around 237 MB of disk
    space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our model is now available as a Keras model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then perform the object detection on our example image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see our example image annotated with labels for each bounding box,
    as can be seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4a49bb5-e260-4989-a90d-c486244c41ff.png)'
  prefs: []
  type: TYPE_IMG
- en: We can extend this for videos using the `OpenCV` library. We can capture images
    frame by frame from a camera attached to our computer, run the object detection,
    and show the annotated image.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this implementation is not optimized and might run relatively
    slowly. For faster implementations, please refer to the darknet implementation
    linked in the *See also* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run the following code, please know that you can stop the camera by
    pressing `q`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We capture our image as grayscale, but then have to convert it back to RGB using
    `scikit-image` by stacking the image. Then we detect objects and show the annotated
    frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the image we obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d5e5dfc-1b88-4d31-a473-cbdc09e73ae7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we'll discuss this recipe with some background explanations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've implemented an object detection algorithm with Keras. This came out of
    the box with a standard library, but we connected it to a camera and applied it
    to an example image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main algorithms in terms of image detection are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Fast R-CNN (Ross Girshick, 2015)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single Shot MultiBox Detector** (**SSD**); Liu and others, 2015: [https://arxiv.org/abs/1512.02325](https://arxiv.org/abs/1512.02325))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**You Only Look Once** (**YOLO**); Joseph Redmon and others*, *2016: [https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLOv4 (Alexey Bochkovskiy and others, 2020: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the main requirements of object detection is speed – you don't want to
    wait to hit the tree before recognizing it.
  prefs: []
  type: TYPE_NORMAL
- en: Image detection is based on image recognition with the added complexity of searching
    through the image for candidate locations.
  prefs: []
  type: TYPE_NORMAL
- en: Fast R-CNN is an improvement over R-CNN by the same author (2014). Each region
    of interest, a rectangular image patch defined by a bounding box, is scale normalized
    by image pyramids. The convolutional network can then process these object proposals
    (from a few thousand to as many as many thousands) through a single forward pass
    of a convolutional neural network. As an implementation detail, Fast R-CNN compresses
    fully connected layers with singular value decomposition for speed.
  prefs: []
  type: TYPE_NORMAL
- en: YOLO is a single network that proposed bounding boxes and classes directly from
    images in a single evaluation. It was much faster than other detection methods
    at the time; in their experiments, the author ran different versions of YOLO at
    45 frames per second and 155 frames per second.
  prefs: []
  type: TYPE_NORMAL
- en: The SSD is a single-stage model that does away with the need for a separate
    object proposal generation, instead of opting for a discrete set of bounding boxes
    that are passed through a network. Predictions are then combined across different
    resolutions and bounding box locations.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a side note, Joseph Redmon published and maintained several incremental
    improvements of his YOLO architecture, but he has since left academia. The latest
    instantiation of the YOLO series by Bochkovskiy and others is in the same spirit,
    however, and is endorsed on Redmon''s GitHub repository: [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet).'
  prefs: []
  type: TYPE_NORMAL
- en: YOLOv4 introduces several new network features to their CNN and they exhibit
    fast processing speed, while maintaining a level of accuracy significantly superior
    to YOLOv3 (43.5% **average precision** (**AP**), for the MS COCO dataset at a
    real-time speed of about 65 frames per seconds on a Tesla V100 GPU).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different ways of interacting with a web camera, and there are even
    some mobile apps that allow you to stream your camera feed, meaning you can plug
    it into applications that run on the cloud (for example, Colab notebooks) or on
    a server.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common libraries is `matplotlib`, and it is also possible to
    live update a matplotlib figure from the web camera, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is the basic template for initiating your video feed, and showing it in
    a matplotlib subfigure. We can stop by interrupting the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: We'll mention a few more libraries to play with in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We recommend having a look at the YOLOv4 paper available on arxiv: [https://arxiv.org/abs/2004.10934](https://arxiv.org/abs/2004.10934).
  prefs: []
  type: TYPE_NORMAL
- en: 'A few libraries are available regarding object detection:'
  prefs: []
  type: TYPE_NORMAL
- en: The first author of the YOLOv4 paper is maintaining an open source framework
    supporting object detection, darknet (originally developed by Joseph Redmon) at [https://github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detectron2 is a library by Facebook AI Research implementing many algorithms
    for object detection: [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Google implementations in TensorFlow for object detection, including the
    recently published SpineNet ([https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html](https://ai.googleblog.com/2020/06/spinenet-novel-architecture-for-object.html)),
    are available on GitHub: [https://github.com/tensorflow/models/tree/master/official/vision/detection](https://github.com/tensorflow/models/tree/master/official/vision/detection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Valkka Live is an open source Python video surveillance platform: [https://elsampsa.github.io/valkka-live/_build/html/index.html](https://elsampsa.github.io/valkka-live/_build/html/index.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MMDetection is an open detection toolbox that covers many popular detection
    methods and comes with pretrained weights for about 200 network models: [https://mmdetection.readthedocs.io/en/latest/](https://mmdetection.readthedocs.io/en/latest/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SpineNet is a new model, found using a massive exploration of hyperparameters,
    for object detection in TensorFlow: [https://github.com/tensorflow/models/tree/master/official/vision/detection](https://github.com/tensorflow/models/tree/master/official/vision/detection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTracking is a library for object tracking and video object segmentation with
    many powerful, state-of-the-art models based on PyTorch, which can be directly
    plugged on top of the webcam input: [https://github.com/visionml/pytracking](https://github.com/visionml/pytracking).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PySlowFast supplies many pretrained models for video classification and detection
    tasks: [https://github.com/facebookresearch/SlowFast](https://github.com/facebookresearch/SlowFast).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An implementation of models for real-time hand gesture recognition with PyTorch is
    available here: [https://github.com/ahmetgunduz/Real-time-GesRec](https://github.com/ahmetgunduz/Real-time-GesRec).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's move on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Faking videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A deep fake is a manipulated video produced by the application of deep learning.
    Potential unethical uses have been around in the media for a while. You can imagine
    how this would end up in the hands of a propaganda mechanism trying to destabilize
    a government. Please note that we are advising against producing deep fakes for
    nefarious purposes.
  prefs: []
  type: TYPE_NORMAL
- en: There are ethical applications of the deep fake technology, and some of them
    are a lot of fun. Have you ever wondered how Sylvester Stallone may have looked
    in Terminator? Today you can!
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to create a deep fake with Python. We'll download
    public domain videos of two films, and we'll produce a deep fake by replacing
    one face with another. *Charade* was a 1963 film directed by Stanley Donen in
    a style reminiscent of a Hitchcock film. It pairs off Cary Grant in his mid-fifties
    and Audrey Hepburn in her early 30s. We thought we'd make the pairing more age-appropriate.
    After some searching, what we found was Maureen O'Hara in the 1963 John Wayne
    vehicle *McLintock!* to replace Audrey Hepburn.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Faceit is a wrapper around the `faceswap` library, which facilitates many of
    the tasks that we'll need to perform for deep fake. We've forked the faceit repository
    at [https://github.com/benman1/facei](https://github.com/benman1/faceit)[t](https://github.com/benman1/faceit).
  prefs: []
  type: TYPE_NORMAL
- en: What we have to do is download the faceit repository and install the requisite
    library.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download (`clone`) the repository with `git` (add an exclamation mark
    if you are typing this in an `ipython` notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We found that a Docker container was well-suited for the installation of dependencies
    (for this you need Docker installed). We can create a Docker container like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This should take a while to build. Please note that the Docker image is based
    on Nvidia's container, so you can use your GPU from within the container.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that, although there is a lightweight model that we could use, we'd
    highly recommend you run the deep fake on a machine with a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can enter our container as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Inside the container, we can run Python 3.6\. All of the following commands
    assume we are inside the container and in the `/project` directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to define videos and faces as inputs to our deep fake process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define our model in Python (Python 3.6) like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This makes it clear that we want to replace `hepburn` with `ohara` (this is
    how we name them inside our process). We have to put images inside the data/persons
    `directories` named `hepburn.jpg` and `ohara.jpg`, accordingly. We have provided
    these images for convenience as part of the repository.
  prefs: []
  type: TYPE_NORMAL
- en: If we don't provide the images, faceit will extract all face images irrespective
    of whom they show. We can then place two of these images for the `persons` directory,
    and delete the directories with faces under `data/processed/`.
  prefs: []
  type: TYPE_NORMAL
- en: We then need to define the videos that we want to use. We have the choice of
    using the complete films or short clips. We didn't find good clips for the *McLintock!*
    film, so we are using the whole film. As for *Charade*, we've focused on the clip
    of a single scene. We have these clips on disk as `mclintock.mp4` and `who_trust.mp4`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Please note that you should only download videos from sites that permit or
    don''t disallow downloading, even of public domain videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This defines the data used by our model as a couple of videos. Faceit allows
    an optional third parameter that can be a link to a video, from where it can be
    downloaded automatically. However, before you are downloading videos from YouTube
    or other sites, please make sure this is permitted in their terms of service and
    legal within your jurisdiction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creation of the deep fake is then initiated by a few more lines of code
    (and a lot of tweaking and waiting):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preprocess step consists of downloading the videos, extracting all the frames
    as images, and finally extracting the faces. We are providing the faces already,
    so you don't have to perform the preprocess step.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows Audrey Hepburn on the left, and Maureen O''Hara playing
    Audrey Hepburn on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/102b8d3f-7123-4445-9ec9-2d1ed37aba2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The changes might seem subtle. If you want something clearer, we can use the
    same model to replace Cary Grant with Maureen O''Hara:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53694a5a-de71-4c46-967c-67a178423c2f.png)'
  prefs: []
  type: TYPE_IMG
- en: In fact, we could produce a film, *Being Maureen O'Hara*, by disabling the face
    filter in the conversion.
  prefs: []
  type: TYPE_NORMAL
- en: We could have used more advanced models, more training to improve the deep fake,
    or we could have chosen an easier scene. However, the result doesn't look bad
    at all sometimes. We've uploaded our fake video to YouTube, where you can view
    it: [https://youtu.be/vDLxg5qXz4k](https://youtu.be/vDLxg5qXz4k).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The typical deep fake pipeline consists of a number of steps that we conveniently
    glossed over in our recipe, because of the abstractions afforded in faceit. These
    steps are the following, given person A and person B, where A is to be replaced
    by B:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose videos of A and B, and split the videos into frames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract faces from these frames for A and B using face recognition. These faces,
    or the facial expressions and face postures, should ideally be representative
    of the video you are going to fake. Make sure they are not blurred, not occluded,
    don't show anyone else other than A and B, and that they are not too repetitive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can train a model on these faces that can take face A and replace it with
    B. You should let the training run for a while. This can take several days, or
    even weeks or months, depending on the number of faces and the complexity of the
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now convert the video by running the face recognition and the model on
    top of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, the face recognition library (`face-recognition`) has a very good
    performance in terms of detection and recognition. However, it still suffers from
    high false positives, but also false negatives. This can result in a poor experience,
    especially in frames where there are several faces.
  prefs: []
  type: TYPE_NORMAL
- en: In the current version of the `faceswap` library, we would extract frames from
    our target video in order to get landmarks for all the face alignments. We can
    then use the GUI in order to manually inspect and clean up these alignments in
    order to make sure they contain the right faces. These alignments will then be
    used for the conversion: [https://forum.faceswap.dev/viewtopic.php?t=27#align](https://forum.faceswap.dev/viewtopic.php?t=27#align).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these steps requires a lot of attention. At the heart of the whole
    operation is the model. There can be different models, including a generative
    adversarial autoencoder and others. The original model in faceswap is an autoencoder
    with a twist. We''ve used autoencoders before in [Chapter 7](f386de9e-b56d-4b39-bf36-803860def385.xhtml),
    *Advanced Image Applications.* This one is relatively conventional, and we could
    have taken our autoencoder implementation from there. However, for the sake of
    completeness, we''ll show its implementation, which is based on `keras`/`tensorflow` (shortened):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This code, in itself, is not terribly interesting. We have two functions, `Decoder()`
    and `Encoder()`, which return decoder and encoder models, respectively. This is
    an encoder-decoder architecture with convolutions. The `PixelShuffle` layer in
    the upscale operation of the decoder rearranges data from depth into blocks of
    spatial data through a permutation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the more interesting part of the autoencoder is in how the training is
    performed as two models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We have two autoencoders, one to be trained on `A` faces and one on `B` faces.
    Both autoencoders are minimizing the reconstruction error (measured in mean absolute
    error) of output against input. As mentioned, we have a single encoder that forms
    part of the two models, and is therefore going to be trained both on faces `A`
    and faces `B`. The decoder models are kept separate between the two faces. This
    architecture ensures that we have a common latent representation between `A` faces
    and `B` faces. In the conversion, we can take a face from `A`, represent it, and
    then apply the decoder for `B` in order to get a `B` face corresponding to the
    latent representation.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've put together some further references relating to playing around with videos
    and deep fakes, as well as detecting deep fakes.
  prefs: []
  type: TYPE_NORMAL
- en: Deep fakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We've collated a few links relevant to deep fakes and some more links that are
    relevant to the process of creating deep fakes.
  prefs: []
  type: TYPE_NORMAL
- en: The face recognition library has been used in this recipe to select image regions
    for training and application of the transformations. It is available on GitHub
    at [https://github.com/ageitgey/face_recognition](https://github.com/ageitgey/face_recognition).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some nice examples of simple video faking applications:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV masks can be used to selectively manipulate parts of an image, for example,
    for an invisibility cloak: [https://www.geeksforgeeks.org/invisible-cloak-using-opencv-python-project/](https://www.geeksforgeeks.org/invisible-cloak-using-opencv-python-project/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A similar effort has been made to add mustaches based on detected face landmarks: [http://sublimerobots.com/2015/02/dancing-mustaches/](http://sublimerobots.com/2015/02/dancing-mustaches/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As for more complex video manipulations with deep fakes, quite a few tools
    are available, of which we''ll highlight two:'
  prefs: []
  type: TYPE_NORMAL
- en: The faceswap library has a GUI and even a few guides: [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepFaceLab is a GUI application for creating deep fakes: [https://github.com/iperov/DeepFaceLab](https://github.com/iperov/DeepFaceLab).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many different models have been proposed and implemented, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ReenactGAN** *– Learning to Reenact Faces via Boundary Transfer* (2018, [https://arxiv.org/abs/1807.11079](https://arxiv.org/abs/1807.11079))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Official implementation: [https://github.com/wywu/ReenactGAN](https://github.com/wywu/ReenactGAN).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DiscoGAN** *–* Taeksoo Kim and others, *Learning to Discover Cross-Domain
    Relations with Generative Adversarial Networks* (2017, [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrated for live video feeds here: [https://github.com/ptrblck/DiscoGAN](https://github.com/ptrblck/DiscoGAN).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A denoising adversarial autoencoder with attention mechanisms for face swapping
    can be seen here: [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The faceit live repository ([https://github.com/alew3/faceit_live](https://github.com/alew3/faceit_live))
    is a fork of faceit that can operate on live video feeds and comes with a hack
    to feed the video back to prank participants in video conferences.
  prefs: []
  type: TYPE_NORMAL
- en: Detection of deep fakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following links are relevant to detecting deep fakes:'
  prefs: []
  type: TYPE_NORMAL
- en: Yuchen Luo has collected lots of links relating to the detection of deep fakes: [https://github.com/592McAvoy/fake-face-detection](https://github.com/592McAvoy/fake-face-detection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of particular interest is detection via adversarial attacks, as can be found
    here: [https://github.com/natanielruiz/disrupting-deepfakes](https://github.com/natanielruiz/disrupting-deepfakes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google, Google Jigsaw, the Technical University of Munich, and the University
    Federico II of Naples provide an extensive dataset of deep fakes for the study
    of detection algorithms: [https://github.com/ondyari/FaceForensics/](https://github.com/ondyari/FaceForensics/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper *DeepFakes and Beyond: A Survey of **Face Manipulation and Fake Detection* (Ruben
    Tolosana and others, 2020) provides more links and more resources to datasets
    and methods.'
  prefs: []
  type: TYPE_NORMAL
