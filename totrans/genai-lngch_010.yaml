- en: 9 Generative AI in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 生成 AI 在生产中
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Discord 上加入我们的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](img/file58.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的二维码描述](img/file58.png)'
- en: 'In this book so far, we’ve talked about models, agents, and LLM apps as well
    as different use cases, but there are many issues that become important when performance
    and regulatory requirements needs to be ensured, models and applications need
    to be deployed at scale, and finally monitoring has to be in place. In this chapter,
    we’ll discuss evaluation and observability, summarizing a broad range of topics
    that encompass the governance and lifecycle management of operationalized AI and
    decision models, including generative AI models. While offline evaluation provides
    a preliminary understanding of a model''s abilities in a controlled setting, observability
    in production offers continuing insights into its performance in live environments.
    Both are crucial at different stages of a model''s life cycle and complement each
    other to ensure optimal operation and results from large language models. We’ll
    discuss a few tools for either case and we’ll give examples.We’ll also discuss
    deploying of models and applications built around LLMs giving an overview over
    available tools and examples for deployment with Fast API and Ray Serve.Throughout
    the chapter, we’ll work on … with LLMs, which you can find in the GitHub repository
    for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections of this chapter are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在本书中已经讨论了模型、代理和 LLM 应用程序以及不同的用例，但是当需要确保性能和监管要求时，模型和应用程序需要大规模部署，并且最终需要进行监视，许多问题就变得重要起来。在本章中，我们将讨论评估和可观察性，总结涵盖运行中的
    AI 和决策模型的治理和生命周期管理的广泛主题，包括生成 AI 模型。虽然离线评估在受控环境中提供了对模型能力的初步理解，但是在生产中的可观察性提供了对其在实时环境中性能的持续洞察。两者在模型生命周期的不同阶段都至关重要，并相互补充，以确保大型语言模型的最佳运行和结果。我们将讨论一些用于这两种情况的工具，并举例说明。我们还将讨论围绕
    LLMs 构建的模型和应用程序的部署，概述可用工具和使用 Fast API 和 Ray Serve 进行部署的示例。在整个章节中，我们将使用...与 LLMs
    进行合作，您可以在书的 GitHub 存储库中找到它们：[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)本章的主要内容包括：
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍
- en: How to evaluate your LLM app?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估您的 LLM 应用程序？
- en: How to deploy your LLM app?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何部署您的 LLM 应用程序？
- en: How to observe your LLM app?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何观察您的 LLM 应用程序？
- en: Let’s start by introducing MLOps for LLMs and other generative models, what
    it means and includes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从介绍 MLOps 对 LLMs 和其他生成模型开始，了解它的含义和包含内容。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'As we’ve discussed in this book, LLMs have gained significant attention in
    recent years due to their ability to generate human-like text. From creative writing
    to conversational chatbots, these generative AI models have diverse applications
    across industries. However, taking these complex neural network systems from research
    to real-world deployment comes with significant challenges. This chapter explores
    the practical considerations and best practices for productionizing generative
    AI responsibly. We discuss the computational requirements for inference and serving,
    techniques for optimization, and critical issues around data quality, bias, and
    transparency. Architectural and infrastructure decisions can make or break a generative
    AI solution when scaled to thousands of users. At the same time, maintaining rigorous
    testing, auditing, and ethical safeguards is essential for trustworthy deployment.Deploying
    applications consisting of models and agents with their tools in production comes
    with several key challenges that need to be addressed to ensure their effective
    and safe use:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所讨论的，LLMs 近年来因其生成类似人类文本的能力而受到了重视。从创意写作到对话式聊天机器人，这些生成 AI 模型在各个行业都有着多样化的应用。然而，将这些复杂的神经网络系统从研究转化为实际部署到现实世界中会面临重大挑战。本章探讨了负责任地将生成
    AI 技术投入生产的实际考虑和最佳实践。我们讨论了推理和服务的计算需求、优化技术以及围绕数据质量、偏见和透明度的关键问题。当规模扩大到数千用户时，架构和基础设施决策可能成败。与此同时，保持严格的测试、审计和道德保障对于可信赖的部署至关重要。将由模型和代理及其工具组成的应用程序部署到生产环境中涉及几个关键挑战，需要解决以确保其有效且安全的使用：
- en: '**Data Quality and Bias**: Training data can introduce biases that get reflected
    in model outputs. Careful data curation and monitoring model outputs is crucial.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量和偏见**：训练数据可能引入偏见，反映在模型输出中。谨慎的数据策划和监控模型输出至关重要。'
- en: '**Ethical/Compliance Considerations**: LLMs can generate harmful, biased or
    misleading content. Review processes and safety guidelines must be established
    to prevent misuse. Adhering to regulations like HIPAA in specialized industries
    such as healthcare.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**道德/合规考虑**：LLMs可能生成有害、偏见或误导性的内容。必须建立审查流程和安全准则，以防止滥用。遵守针对特定行业如医疗保健的HIPAA等法规。'
- en: '**Resource Requirements**: LLMs require massive compute resources for training
    and serving. Efficient infrastructure is critical for cost-effective deployment
    at scale.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源需求**：LLMs需要大量计算资源进行训练和服务。高效基础设施对成本效益的大规模部署至关重要。'
- en: '**Drift or Performance Degradation**: Models need continuous monitoring to
    detect issues like data drift or performance degradation over time.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**漂移或性能下降**：模型需要持续监控，以便检测数据漂移或随时间的性能下降等问题。'
- en: '**Lack of Interpretability**: LLMs are often black boxes, making their behaviors
    and decisions opaque. Interpretability tools are important for transparency.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性不足**：LLMs通常是黑匣子，使其行为和决策不透明。解释性工具对于透明度至关重要。'
- en: 'Taking a trained LLM from research into real-world production involves navigating
    many complex challenges around aspects like scalability, monitoring, and unintended
    behaviors. Responsibly deploying capable yet unreliable models involves diligent
    planning around scalability, interpretability, testing, and monitoring. Techniques
    like fine-tuning, safety interventions, and defensive design enable developing
    applications that are helpful, harmless, and honest. With care and preparation,
    generative AI holds immense potential to benefit industries from medicine to education.Several
    key patterns can help address the challenges highlighted above:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将经过培训的LLM从研究转化为真实世界的生产，需要解决诸多复杂挑战，涉及可扩展性、监控和意外行为等方面。负责地部署功能强大但不可靠的模型需要认真规划可扩展性、可解释性、测试和监控等方面。精细调整、安全干预和防御性设计等技术，能够开发出有益、无害和诚实的应用程序。通过细心和准备，生成式AI在医学到教育等行业中具有巨大潜力。以下几个关键模式可以帮助解决上述挑战：
- en: '**Evaluations**: Solid benchmark datasets and metrics are essential to measure
    model capabilities, regressions, and alignment with goals. Metrics should be carefully
    selected based on the task.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：可靠的基准数据集和指标对于衡量模型能力、回归和与目标的一致性至关重要。应根据任务仔细选择指标。'
- en: '**Retrieval Augmentation**: Retrieving external knowledge provides useful context
    to reduce hallucinations and add recent information beyond pre-training data.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索增强**：检索外部知识可提供有用的上下文，以减少幻觉，并在预训练数据之外添加最新信息。'
- en: '**Fine-tuning**: Further tuning LLMs on task-specific data improves performance
    on target use cases. Techniques like adapter modules reduce overhead.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精细调整**：在任务特定数据上进一步调整LLMs，可以提高目标使用案例的性能。类似适配器模块的技术可以减少开销。'
- en: '**Caching**: Storing model outputs can significantly reduce latency and costs
    for repeated queries. But cache validity needs careful consideration.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存**：存储模型输出可以显著减少重复查询的延迟和成本。但缓存有效性需要仔细考虑。'
- en: '**Guardrails**: Validating model outputs syntactically and semantically ensures
    reliability. Guidance techniques directly shape output structure.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防护栏**：在句法和语义上验证模型输出，以确保可靠性。指导技术直接塑造输出结构。'
- en: '**Defensive UX**: Design anticipating inaccuracies, such as disclaimers on
    limitations, attributions, and collecting rich user feedback.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防御性用户体验**：设计时要预见到不准确性，例如在限制、属性和收集丰富的用户反馈等方面进行免责声明。'
- en: '**Monitoring**: Continuously tracking metrics, model behaviors, and user satisfaction
    provides insight into model issues and business impact.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：持续追踪指标、模型行为和用户满意度，可以洞察模型问题和业务影响。'
- en: In Chapter 5, we’ve already covered safety-aligned techniques like Constitutional
    AI for mitigating risks like generating harmful outputs. Further, LLMs have the
    potential to generate harmful or misleading content. It is essential to establish
    ethical guidelines and review processes to prevent the dissemination of misinformation,
    hate speech, or any other harmful outputs. Human reviewers can play a crucial
    role in evaluating and filtering the generated content to ensure compliance with
    ethical standards.Not only for legal, ethical, and reputational reasons, but also
    in order to maintain performance, we need to continuously evaluate model performance
    and outputs in order to detect issues like data drift or loss of capabilities.
    We’ll be discussing techniques to interpret model behaviors and decisions. Improving
    transparency in high-stakes domains.LLMs or generative AI models require significant
    computational resources for deployment due to their size and complexity. This
    includes high-performance hardware, such as GPUs or TPUs, to handle the massive
    amount of computations involved. Scaling large language models or generative AI
    models can be challenging due to their resource-intensive nature. As the size
    of the model increases, the computational requirements for training and inference
    also increase exponentially. Distributed techniques, such as data parallelism
    or model parallelism, are often used to distribute the workload across multiple
    machines or GPUs. This allows for faster training and inference times. Scaling
    also involves managing the storage and retrieval of large amounts of data associated
    with these models. Efficient data storage and retrieval systems are required to
    handle the massive model sizes.Deployment also involves considerations for optimizing
    inference speed and latency. Techniques like model compression, quantization,
    or hardware-specific optimizations may be employed to ensure efficient deployment.
    We’ve discussed some of this in *Chapter 8*. LLMs or generative AI models are
    often considered black boxes, meaning it can be difficult to understand how they
    arrive at their decisions or generate their outputs. Interpretability techniques
    aim to provide insights into the inner workings of these models. This can involve
    methods like attention visualization, feature importance analysis, or generating
    explanations for model outputs. Interpretability is crucial in domains where transparency
    and accountability are important, such as healthcare, finance, or legal systems.As
    we discussed in *Chapter 8*, Large language models can be fine-tuned on specific
    tasks or domains to improve their performance on specific use cases. Transfer
    learning allows models to leverage pre-trained knowledge and adapt it to new tasks.
    Transfer learning and fine-tuning on domain-specific data unlocks new use cases
    while requiring additional diligence.With insightful planning and preparation,
    generative AI promises to transform industries from creative writing to customer
    service. But thoughtfully navigating the complexities of these systems remains
    critical as they continue permeating diverse domains. This chapter aims to provide
    a practical guide for teams of the pieces that we’ve left out so far aiming to
    build impactful and responsible generative AI applications. We mention strategies
    for data curation, model development, infrastructure, monitoring, and transparency.
    Before we continue our discussion, a few words on terminology is in place.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五章中，我们已经介绍了诸如宪法AI之类的与安全对齐的技术，用于减轻生成有害输出的风险。此外，LLM具有生成有害或误导性内容的潜力。建立伦理指南和审查流程以防止误导信息、仇恨言论或任何其他有害输出的传播至关重要。人类审阅员在评估和过滤生成的内容以确保符合伦理标准方面发挥着至关重要的作用。我们不仅需要不断评估模型性能和输出以便检测数据漂移或能力丧失等问题，而且还需要为了维护性能。我们将讨论解释模型行为和决策的技术。改进高风险领域的透明度。由于LLM或生成性AI模型的规模和复杂性，部署需要大量的计算资源。这包括高性能硬件，如GPU或TPU，用于处理涉及的大量计算。由于其资源密集的特性，扩展大型语言模型或生成性AI模型可能具有一定的挑战性。随着模型规模的增加，训练和推断的计算要求也将呈指数级增加。通常使用分布式技术，如数据并行性或模型并行性，来在多台机器或GPU上分发工作负载。这可以加快训练和推断时间。扩展还涉及管理与这些模型相关的大量数据的存储和检索。需要高效的数据存储和检索系统来处理庞大的模型规模。部署还涉及优化推断速度和延迟的考虑。可以采用模型压缩、量化或硬件特定优化等技术，以确保高效的部署。我们在*第8章*中已经讨论了一些这方面的内容。LLM或生成性AI模型通常被视为黑匣子，这意味着很难理解它们是如何做出决定或生成输出的。解释性技术旨在提供对这些模型内部运作的洞察。这可能涉及方法如注意力可视化、特征重要性分析，或为模型输出生成解释。在健康、金融或法律系统等领域，解释性至关重要，因此透明度和问责制是重要的。正如我们在*第8章*中讨论的那样，可以对大型语言模型进行特定任务或领域的微调，以提高其在特定用例中的性能。迁移学习允许模型利用预训练知识并将其适应新任务。迁移学习和对领域特定数据的微调在需要额外的审慎的同时，也打开了新的用例。通过深思熟虑的规划和准备，生成性AI承诺将从创意写作到客户服务等领域中改变行业。然而，在这些系统继续渗透各种领域的同时，深思熟虑地应对这些系统的复杂性仍然至关重要。本章旨在为团队提供一种实用指南，以构建具有影响力和负责任的生成性AI应用。我们提到了有关数据筛选、模型开发、基础设施、监控和透明度的策略。在我们继续讨论之前，对术语的一些解释是必要的。
- en: Terminology
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 术语
- en: '**MLOps** is a paradigm that focuses on deploying and maintaining machine learning
    models in production reliably and efficiently. It combines the practices of DevOps
    with machine learning to transition algorithms from experimental systems to production
    systems. MLOps aims to increase automation, improve the quality of production
    models, and address business and regulatory requirements. **LLMOps** is a specialized
    sub-category of MLOps. It refers to the operational capabilities and infrastructure
    necessary for fine-tuning and operationalizing large language models as part of
    a product. While it may not be drastically different from the concept of MLOps,
    the distinction lies in the specific requirements connected to handling, refining,
    and deploying massive language models like GPT-3, which houses 175 billion parameters.The
    term **LMOps** is more inclusive than LLMOps as it encompasses various types of
    language models, including both large language models and smaller generative models.
    This term acknowledges the expanding landscape of language models and their relevance
    in operational contexts.**FOMO** **(Foundational Model Orchestration)** specifically
    addresses the challenges faced when working with foundational models. It highlights
    the need for managing multi-step processes, integrating with external resources,
    and coordinating workflows involving these models.The term **ModelOps** focuses
    on the governance and lifecycle management of AI and decision models as they are
    deployed. Even more broadly, **AgentOps** involves the operational management
    of LLMs and other AI agents, ensuring their appropriate behavior, managing their
    environment and resource access, and facilitating interactions between agents
    while addressing concerns related to unintended outcomes and incompatible objectives.While
    FOMO emphasizes the unique challenges of working specifically with foundational
    models, LMOps provides a more inclusive and comprehensive coverage of a wider
    range of language models beyond just the foundational ones. LMOps acknowledges
    the versatility and increasing importance of language models in various operational
    use cases, while still falling under the broader umbrella of MLOps. Finally, AgentOps
    explicitly highlights the interactive nature of agents consisting of generative
    models operating with certain heuristics and includes tools. The emergence of
    all very specialized terms underscores the rapid evolution of the field; however,
    their long-term prevalence is unclear. MLOps is an established term widely used
    in the industry, with significant recognition and adoption. Therefore, we’ll stick
    to MLOps for the remainder of this chapter.Before productionizing any agent or
    model, we should first evaluate its output, so we should start with this. We will
    focus on the evaluation methods provided by LangChain.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLOps** 是一种范式，专注于可靠高效地在生产环境中部署和维护机器学习模型。它将DevOps的实践与机器学习结合起来，将算法从实验性系统过渡到生产系统。MLOps旨在增加自动化，提高生产模型的质量，并解决业务和监管要求。**LLMOps**
    是MLOps的一种专业子类别。它指的是作为产品的一部分对大型语言模型进行微调和运行的操作能力和基础设施。虽然它可能与MLOps的概念没有显著不同，但区别在于与处理、精炼和部署像GPT-3这样的庞大语言模型相关的具体要求。术语**LMOps**比LLMOps更具包容性，因为它涵盖了各种类型的语言模型，包括大型语言模型和较小的生成模型。该术语承认了语言模型的不断扩展的景观以及它们在操作环境中的相关性。**FOMO（基础模型编排）**专门解决了在使用基础模型时面临的挑战。它强调了管理多步骤流程、与外部资源集成以及协调涉及这些模型的工作流程的需求。术语**ModelOps**专注于部署的AI和决策模型的治理和生命周期管理。更广泛地说，**AgentOps**涉及LLMs和其他AI代理的运行管理，确保它们的适当行为，管理它们的环境和资源访问，并促进代理之间的互动，同时解决与意外结果和不兼容目标相关的问题。虽然FOMO强调了专门与基础模型合作时面临的独特挑战，但LMOps提供了对除基础模型之外更广泛范围的语言模型的更具包容性和全面的覆盖。LMOps承认了语言模型在各种操作用例中的多功能性和日益重要性，同时仍然属于更广泛的MLOps的范畴。最后，AgentOps明确强调了由带有某些启发式操作的生成模型组成的代理的互动性质，并包括工具。所有这些非常专业的术语的出现都突显了该领域的快速发展；然而，它们的长期普及尚不清楚。MLOps是一个在业界广泛使用的已建立的术语，得到了很大的认可和采用。因此，在本章的其余部分我们将坚持使用MLOps。在将任何代理或模型投入生产之前，我们应该首先评估其输出，因此我们应该从这开始。我们将重点关注LangChain提供的评估方法。'
- en: How to evaluate your LLM apps?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何评估你的LLM应用程序？
- en: Evaluating LLMs either as standalone entities or in conjunction with an agent
    chain is crucial to ensure they function correctly and produce reliable results,
    and is an integral part of the machine learning lifecycle. The evaluation process
    determines the performance of the models in terms of effectiveness, reliability,
    and efficiency. The goal of evaluating large language models is to understand
    their strengths and weaknesses, enhancing accuracy and efficiency while reducing
    errors, thereby maximizing their usefulness in solving real-world problems. This
    evaluation process typically occurs offline during the development phase. Offline
    evaluations provide initial insights into model performance under controlled test
    conditions and include aspects like hyper-parameter tuning, benchmarking against
    peer models or established standards. They offer a necessary first step towards
    refining a model before deployment.Evaluations provide insights into how well
    an LLM can generate outputs that are relevant, accurate, and helpful. In LangChain,
    there are various ways to evaluate outputs of LLMs, including comparing chain
    outputs, pairwise string comparisons, string distances, and embedding distances.
    The evaluation results can be used to determine the preferred model based on the
    comparison of outputs. Confidence intervals and p-values can also be calculated
    to assess the reliability of the evaluation results. LangChain provides several
    tools for evaluating the outputs of large language models. A common approach is
    to compare the outputs of different models or prompts using the `PairwiseStringEvaluator`.
    This prompts an evaluator model to choose between two model outputs for the same
    input and aggregates the results to determine an overall preferred model.Other
    evaluators allow assessing model outputs based on specific criteria like correctness,
    relevance, and conciseness. The `CriteriaEvalChain` can score outputs on custom
    or predefined principles without needing reference labels. Configuring the evaluation
    model is also possible by specifying a different chat model like ChatGPT as the
    evaluator.Let’s compare outputs of different prompts or LLMs with the `PairwiseStringEvaluator`,
    which prompts an LLM to select the preferred output given a specific input.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Comparing two outputs
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This evaluation requires an evaluator, a dataset of inputs, and two or more
    LLMs, chains, or agents to compare. The evaluation aggregates the results to determine
    the preferred model.The evaluation process involves several steps:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Evaluator: Load the evaluator using the `load_evaluator()` function,
    specifying the type of evaluator (in this case, `pairwise_string`).'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select Dataset: Load a dataset of inputs using the `load_dataset()` function.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define Models to Compare: Initialize the LLMs, Chains, or Agents to compare
    using the necessary configurations. This involves initializing the language model
    and any additional tools or agents required.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Generate Responses: Generate outputs for each of the models before evaluating
    them. This is typically done in batches to improve efficiency.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成响应：为每个模型生成输出，然后再进行评估。通常会批量处理以提高效率。
- en: 'Evaluate Pairs: Evaluate the results by comparing the outputs of different
    models for each input. This is often done using a random selection order to reduce
    positional bias.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估成对：通过比较每个输入的不同模型的输出来评估结果。经常使用随机选择顺序来减少位置偏差。
- en: 'Here’s an example from the documentation for pairwise string comparisons:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个来自成对字符串比较文档的示例：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output from the evaluator should look as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器的输出应如下所示：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The evaluation result includes a score between 0 and 1, indicating the effectiveness
    of the agent, sometimes along with reasoning that outlines the evaluation process
    and justifies the score.In this illustration of against the reference, both results
    are factually incorrect based on the input. We could remove the reference and
    let an LLM judge the outputs instead, however, this is potentially dangerous since
    the specified can also be incorrect.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 评估结果包括一个介于0和1之间的分数，表示代理的有效性，有时还包括概述评估过程并证明分数的推理。在这个反对参考资料的例子中，基于输入，两个结果事实上都是不正确的。我们可以移除参考资料，让一个LLM来评判输出，但是这样做可能是危险的，因为指定的也可能是不正确的。
- en: Comparing against criteria
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据标准比较
- en: 'LangChain provides several predefined evaluators for different evaluation criteria.
    These evaluators can be used to assess outputs based on specific rubrics or criteria
    sets. Some common criteria include conciseness, relevance, correctness, coherence,
    helpfulness, and controversiality.The `CriteriaEvalChain` allows you to evaluate
    model outputs against custom or predefined criteria. It provides a way to verify
    if an LLM or Chain''s output complies with a defined set of criteria. You can
    use this evaluator to assess correctness, relevance, conciseness, and other aspects
    of the generated outputs.The `CriteriaEvalChain` can be configured to work with
    or without reference labels. Without reference labels, the evaluator relies on
    the LLM''s predicted answer and scores it based on the specified criteria. With
    reference labels, the evaluator compares the predicted answer to the reference
    label and determines its compliance with the criteria.The evaluation LLM used
    in LangChain, by default, is GPT-4\. However, you can configure the evaluation
    LLM by specifying other chat models, such as ChatAnthropic or ChatOpenAI, with
    the desired settings (for example, temperature). The evaluators can be loaded
    with a custom LLM by passing the LLM object as a parameter to the `load_evaluator()`
    function.LangChain supports both custom criteria and predefined principles for
    evaluation. Custom criteria can be defined using a dictionary of `criterion_name:
    criterion_description pairs`. These criteria can be used to assess outputs based
    on specific requirements or rubrics.Here’s is a simple example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'LangChain为不同的评估标准提供了几个预定义的评估器。这些评估器可用于根据特定的评分标准或标准集合评估输出。一些常见的标准包括简洁性、相关性、正确性、连贯性、帮助性和争议性。`CriteriaEvalChain`允许您根据自定义或预定义标准评估模型输出。它提供了一种验证LLM或Chain的输出是否符合一组定义的标准的方法。您可以使用此评估器评估正确性、相关性、简洁性和生成的输出的其他方面。`CriteriaEvalChain`可以配置为使用或不使用参考标签。没有参考标签，评估器依赖于LLM的预测答案，并根据指定的标准对其进行评分。有了参考标签，评估器将预测的答案与参考标签进行比较，并确定其是否符合标准。LangChain中默认使用的评估LLM是GPT-4。但是，您可以通过指定其他聊天模型（例如ChatAnthropic或ChatOpenAI）和所需的设置（例如温度）来配置评估LLM。通过将LLM对象作为参数传递给`load_evaluator()`函数，可以使用自定义LLM加载评估器。LangChain支持自定义标准和预定义原则进行评估。可以使用`criterion_name:
    criterion_description`对的字典定义自定义标准。这些标准可以根据特定要求或标准来评估输出。这里有一个简单的示例：'
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can get a very nuanced comparison of the two outputs as this result shows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得到这两个输出的非常微妙的比较，如下结果所示：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Alternatively, you can use the predefined principles available in LangChain,
    such as those from Constitutional AI. These principles are designed to evaluate
    the ethical, harmful, and sensitive aspects of the outputs. The use of principles
    in evaluation allows for a more focused assessment of the generated text.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用LangChain中提供的预定义原则，比如来自宪法人工智能的原则。这些原则旨在评估输出的道德、有害和敏感方面。在评估中使用原则可以更专注地评估生成的文本。
- en: String and semantic comparisons
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 字符串和语义比较
- en: 'LangChain supports string comparison and distance metrics for evaluating LLM
    outputs. String distance metrics like Levenshtein and Jaro provide a quantitative
    measure of similarity between predicted and reference strings. Embedding distances
    using models like SentenceTransformers calculate semantic similarity between generated
    and expected texts.Embedding distance evaluators can use embedding models, such
    as those based on GPT-4 or Hugging Face embeddings, to compute vector distances
    between predicted and reference strings. This measures the semantic similarity
    between the two strings and can provide insights into the quality of the generated
    text. Here’s a quick example from the documentation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain支持字符串比较和距离度量来评估LLM输出。像Levenshtein和Jaro这样的字符串距离度量提供了一个量化的相似度衡量标准，用于在预测和参考字符串之间的相似性。使用像SentenceTransformers这样的模型计算语义相似性的嵌入距离，计算生成和期望文本之间的语义相似性。嵌入距离评估器可以使用嵌入模型，如基于GPT-4或Hugging
    Face嵌入的模型，来计算预测字符串和参考字符串之间的向量距离。这可以衡量两个字符串之间的语义相似性，并提供有关生成文本质量的见解。以下是文档中的一个快速示例：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The evaluator returns the score 0.0966466944859925\. You can change the embeddings
    used with the `embeddings` parameter in the `load_evaluator()` call. This often
    gives better results than older string distance metrics, but these are also available
    and allows for simple unit testing and assessment of accuracy. String comparison
    evaluators compare predicted strings against reference strings or inputs.String
    distance evaluators use distance metrics, such as Levenshtein or Jaro distance,
    to measure the similarity or dissimilarity between predicted and reference strings.
    This provides a quantitative measure of how similar the predicted string is to
    the reference string.Finally, there’s an agent trajectory evaluator, where the
    `evaluate_agent_trajectory()` method is used to evaluate the input, prediction,
    and agent trajectory.We can also use LangSmith to compare our performance against
    a dataset. We’ll talk about this companion project for LangChain – LangSmith –
    more in the section on observability.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器返回得分0.0966466944859925。您可以通过`load_evaluator()`调用中的`embeddings`参数更改所使用的嵌入。这通常比旧的字符串距离度量方法效果更好，但这些方法也可用，从而可以进行简单的单元测试和准确性评估。字符串比较评估器将预测的字符串与参考字符串或输入进行比较。字符串距离评估器使用诸如Levenshtein或Jaro距离之类的距离度量来衡量预测和参考字符串之间的相似性或不相似性。这提供了预测字符串与参考字符串的相似性的定量衡量。最后，还有一个agent轨迹评估器，其中使用`evaluate_agent_trajectory()`方法来评估输入、预测和agent轨迹。我们还可以使用LangSmith与数据集比较我们的性能。我们将在关于可观测性部分更多地讨论LangChain的伴侣项目LangSmith。
- en: Benchmark dataset
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准数据集
- en: 'With LangSmith, we can evaluate the model performance against a dataset. Let’s
    step through an example.First of all, please make sure you create an account on
    LangSmith here: [https://smith.langchain.com/](https://smith.langchain.com/) You
    can obtain an API key and set it as `LANGCHAIN_API_KEY` in your environment. We
    can also set environment variables for project id and tracing:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LangSmith，我们可以评估模型在数据集上的性能。让我们通过一个示例来了解。首先，请确保您在此处创建LangSmith账户：[https://smith.langchain.com/](https://smith.langchain.com/)
    您可以获取API密钥，并将其设置为环境中的`LANGCHAIN_API_KEY`。我们还可以设置项目ID和跟踪的环境变量：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This configures LangChain to log traces. If we don’t tell LangChain the project
    id, it will log against the `default` project. After this setup, when we run our
    LangChain agent or chain, we’ll be able to see the traces on LangSmith. Let’s
    log a run!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将配置LangChain记录跟踪。如果我们不告诉LangChain项目ID，它将对`default`项目进行记录。设置完成后，当我们运行LangChain
    agent或chain时，我们将能够在LangSmith上看到跟踪。让我们记录一次运行吧！
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We’ll see this on LangSmith like this:LangSmith allows us to list all runs
    so far on the LangSmith project page: [https://smith.langchain.com/projects](https://smith.langchain.com/projects)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在LangSmith上看到如下内容：LangSmith允许我们在LangSmith项目页面上列出到目前为止所有的运行：[https://smith.langchain.com/projects](https://smith.langchain.com/projects)
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can list runs from a specific project or with by `run_type`, for example
    "chain". Each run comes with inputs and outputs as we can see here:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照特定项目或`run_type`列出运行，例如"chain"。每个运行都有输入和输出，就像我们在这里看到的那样：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can create a dataset from existing agent runs with the `create_example_from_run()`
    function – or from anything else. Here’s how to create a dataset with a set of
    questions:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`create_example_from_run()`函数从现有的agent运行中创建数据集，或者从其他任何东西创建数据集。以下是如何使用一组问题创建数据集的方法：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can then run an LLM agent or chain on the dataset like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以以这样的方式在数据集上运行LLM agent或chain：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We use a constructor function to initialize for each input. In order to evaluate
    the model performance against this dataset, we need to define an evaluator as
    we’ve seen in the previous section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用构造函数来为每个输入进行初始化。为了评估模型在该数据集上的性能，我们需要定义一个评估器，正如我们在前一节中所见。
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We’ll pass a dataset and evaluators to `run_on_dataset()` to generate metrics
    and feedback:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个数据集和评估器传递给`run_on_dataset()`来生成指标和反馈：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similarly, we could pass a dataset and evaluators to arun_on_dataset`()` to
    generate metrics and feedback asynchronously.We can view the evaluator feedback
    in the LangSmith UI to identify areas for improvement:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以传递一个数据集和评估器给`run_on_dataset()`来异步生成指标和反馈。我们可以在 LangSmith UI 中查看评估器反馈，以确定改进的方向。
- en: '![Figure 9.1: Evaluators in LangSmith.](img/file59.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：LangSmith 中的评估器。](img/file59.png)'
- en: 'Figure 9.1: Evaluators in LangSmith.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：LangSmith 中的评估器。
- en: 'We can click on any of these evaluations to see some detail, for example, for
    the careful thinking evaluator, we get this prompt that includes the original
    answer by the LLM:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以单击其中任何一个评估器来查看一些细节，例如，对于仔细思考的评估器，我们得到了这个包含了由LLM提供的原始答案的提示：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get this evaluation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这个评估：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: A way to improve performance for a few types of problems is to do few-shot prompting.
    LangSmith can help us with this as well. You can find more examples for this in
    the LangSmith documentation.This concludes evaluation. Now that we’ve evaluated
    our agents, let’s say we are happy with the performance and we deploy it!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 提高一些类型问题的性能的一种方式是进行少量提示。LangSmith 也可以帮助我们。您可以在 LangSmith 文档中找到更多此类示例。这就结束了评估。现在我们已经评估了我们的代理，假设我们对性能感到满意，我们就部署它！
- en: How to deploy your LLM apps?
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何部署您的大型语言模型应用？
- en: 'Given the increasing use of LLMs in various sectors, it''s imperative to understand
    how to effectively deploy models and apps into production. Deployment Services
    and Frameworks can help to scale the technical hurdles. There are lots of different
    ways to productionize LLM-apps or applications with generative AI. Deployment
    for production requires research into and knowledge of the generative AI ecosystem,
    which encompasses different aspects including:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于大型语言模型在各个领域的广泛使用，了解如何有效地将模型和应用程序部署到生产环境中是至关重要的。部署服务和框架可以帮助解决技术障碍。有很多不同的方法可以将大型语言模型应用或具有生成能力的应用程序投入生产。生产部署需要对生成式人工智能生态系统进行研究和了解，这包括不同方面，包括：
- en: 'Models and LLM-as-a-Service: LLMs and other models either run directly or offered
    as an API on vendor-provided infrastructure.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型和语言模型即服务：LLMs 和其他模型可以直接运行，也可以作为 API 提供在供应商提供的基础设施上。
- en: 'Reasoning Heuristics: Retrieval Augmented Generation (RAG), Tree-of-Thought,
    and others.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理启发式：检索增强生成（RAG），思维之树等等。
- en: 'Vector Databases: Aid in retrieving contextually relevant information for prompts.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量数据库：用于检索上下文相关信息以供提示使用。
- en: 'Prompt Engineering Tools: These facilitate in-context learning without requiring
    expensive fine-tuning or sensitive data.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程工具：这些工具可以在不需要昂贵的微调或敏感数据的情况下进行上下文学习。
- en: 'Pre-training and fine-tuning: For models specialized for specific tasks or
    domains.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练和微调：适用于专门用于特定任务或领域的模型。
- en: 'Prompt Logging, Testing, and Analytics: An emerging sector inspired by the
    desire to understand and improve the performance of Large Language Models.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示日志，测试和分析：一种新兴的领域，灵感来自于对大型语言模型性能的理解和改进的愿望。
- en: 'Custom LLM Stack: A set of tools for shaping and deploying solutions built
    on open-source models.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义的语言模型堆栈：一组用于塑造和部署在开源模型上构建解决方案的工具。
- en: 'We’ve discussed models in *Chapter 1* and *Chapter 3*, reasoning heuristics
    in chapters 4-7, vector databases in Chapter 5, and prompts and fine-tuning in
    *Chapter 8*. In this chapter, we’ll focus on logging, monitoring, and custom tools
    for deployment.LLMs are typically utilized using external LLM providers or self-hosted
    models. With external providers, computational burdens are shouldered by companies
    like OpenAI or Anthropic, while LangChain facilitates business logic implementation.
    However, self-hosting open-source LLMs can significantly decrease costs, latency,
    and privacy concerns.Some tools with infrastructure offer the full package. For
    example, you can deploy LangChain agents with Chainlit creating ChatGPT-like UIs
    with Chainlit. Some of the key features include intermediary steps visualisation,
    element management & display (images, text, carousel, and others) as well as cloud
    deployment. BentoML is a framework that enables the containerization of machine
    learning applications to use them as microservices running and scaling independently
    with automatic generation of OpenAPI and gRPC endpoints.You can also deploy LangChain
    to different cloud service endpoints, for example, an Azure Machine Learning Online
    Endpoint. With Steamship, LangChain developers can rapidly deploy their apps,
    which includes: production-ready endpoints, horizontal scaling across dependencies,
    persistent storage of app state, multi-tenancy support, and more.Here is a table
    summarizing services and frameworks for deploying large language model applications:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第1章*和*第3章*讨论了模型，第4-7章的推理启发式，第5章的向量数据库以及第8章的提示和微调。在本章中，我们将重点关注部署的日志记录、监控和自定义工具。LLM通常使用外部LLM提供商或自托管模型来利用。对于外部提供商，计算负担由类似于OpenAI或Anthropic的公司承载，而LangChain则促进业务逻辑实现。然而，自托管开源LLM可以显着降低成本、延迟和隐私问题。一些拥有基础设施的工具提供完整的包。例如，您可以使用Chainlit部署LangChain代理，并创建带有Chainlit的ChatGPT
    UI。一些关键特点包括中间步骤可视化、元素管理和显示（图像、文本、走马灯等）以及云部署。BentoML是一个框架，可以将机器学习应用程序容器化，以便于使用它们作为独立运行和扩展的微服务，自动生成OpenAPI和gRPC端点。您还可以将LangChain部署到不同的云服务节点，例如Azure
    Machine Learning Online Endpoint。使用Steamship，LangChain开发人员可以快速部署其应用程序，其中包括：生产就绪的端点、依赖项的水平扩展、应用程序状态的持久性存储、多租户支持等。下表总结了部署大型语言模型应用程序的服务和框架：
- en: '| **Name** | **Description** | **Type** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **名称** | **描述** | **类型** |'
- en: '| Streamlit | Open-source Python framework for building and deploying web apps
    | Framework |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Streamlit | 用于构建和部署Web应用程序的开源Python框架 | 框架 |'
- en: '| Gradio | Lets you wrap models in an interface and host on Hugging Face |
    Framework |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| Gradio | 允许您在Hugging Face上封装模型并进行托管的框架 | 框架 |'
- en: '| Chainlit | Build and deploy conversational ChatGPT-like apps | Framework
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Chainlit | 构建和部署对话式ChatGPT应用程序 | 框架 |'
- en: '| Apache Beam | Tool for defining and orchestrating data processing workflows
    | Framework |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Apache Beam | 用于定义和编排数据处理工作流程的工具 | 框架 |'
- en: '| Vercel | Platform for deploying and scaling web apps | Cloud Service |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Vercel | 用于部署和扩展Web应用程序的平台 | 云服务 |'
- en: '| FastAPI | Python web framework for building APIs | Framework |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| FastAPI | 用于构建API的Python Web框架 | 框架 |'
- en: '| Fly.io | App hosting platform with autoscaling and global CDN | Cloud Service
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Fly.io | 具有自动缩放和全球CDN的应用程序托管平台 | 云服务 |'
- en: '| DigitalOcean App Platform | Platform to build, deploy and scale apps | Cloud
    Service |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DigitalOcean应用程序平台 | 构建、部署和扩展应用程序的平台 | 云服务 |'
- en: '| Google Cloud | Services like Cloud Run to host and scale containerized apps
    | Cloud Service |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Google Cloud | 提供像Cloud Run这样的服务，可托管和扩展容器化的应用程序 | 云服务 |'
- en: '| Steamship | ML infrastructure platform for deploying and scaling models |
    Cloud Service |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Steamship | 用于部署和扩展模型的ML基础设施平台 | 云服务 |'
- en: '| Langchain-serve | Tool to serve LangChain agents as web APIs | Framework
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Langchain-serve | 用于将LangChain代理作为Web API提供的工具 | 框架 |'
- en: '| BentoML | Framework for model serving, packaging and deployment | Framework
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| BentoML | 模型服务、打包和部署的框架 | 框架 |'
- en: '| OpenLLM | Provides open APIs to commercial LLMs | Cloud Service |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM | 提供商业LLM的开放API | 云服务 |'
- en: '| Databutton | No-code platform to build and deploy model workflows | Framework
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Databutton | 无代码平台，用于构建和部署模型工作流程 | 框架 |'
- en: '| Azure ML | Managed ML ops service on Azure for models | Cloud Service |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Azure ML | Azure上的托管ML操作服务，用于模型 | 云服务 |'
- en: 'Figure 9.2: Services and frameworks for deploying large language model applications.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：部署大型语言模型应用程序的服务和框架。
- en: 'All of these are well-documented with different use cases, often directly referencing
    LLMs. We’ve already shown examples with Streamlit and Gradio, and we’ve discussed
    how to deploy them to HuggingFace Hub as an example.There are a few main requirements
    for running LLM applications:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都有不同的用例很好地记录，通常直接引用LLM。我们已经展示了使用Streamlit和Gradio的示例，并讨论了如何以HuggingFace Hub为例部署它们。运行LLM应用程序有几个主要要求：
- en: Scalable infrastructure to handle computationally intensive models and potential
    spikes in traffic
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的基础设施，以处理计算密集型模型和潜在的流量激增。
- en: Low latency for real-time serving of model outputs
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时为模型输出提供服务的低延迟
- en: Persistent storage for managing long conversations and app state
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于管理长对话和应用程序状态的持久存储
- en: APIs for integration into end-user applications
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于集成到最终用户应用程序的API
- en: Monitoring and logging to track metrics and model behavior
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和日志记录以跟踪指标和模型行为
- en: Maintaining cost efficiency can be challenging with large volumes of user interactions
    and high costs associated with LLM services. Strategies to manage efficiency include
    self-hosting models, auto-scaling resource allocations based on traffic, using
    spot instances, independent scaling, and batching requests to better utilize GPU
    resources.The choice of the tools and the infrastructure determines trade-offs
    between these requirements. Flexibility and ease is very important, because we
    want to be able to iterate rapidly, which is vital due to the dynamic nature of
    ML and LLM landscapes. It's crucial to avoid getting tied to one solution. A flexible,
    scalable serving layer that accommodates various models is key. Model composition
    and cloud providers' selection forms part of this flexibility equation.For most
    flexibility, Infrastructure as Code (IaC) tools like Terraform, CloudFormation,
    or Kubernetes YAML files can recreate your infrastructure reliably and quickly.
    Moreover, continuous integration and continuous delivery (CI/CD) pipelines can
    automate testing and deployment processes to reduce errors and facilitate quicker
    feedback and iteration.Designing a robust LLM application service can be a complex
    task requiring an understanding the trade-offs and critical considerations when
    evaluating serving frameworks. Leveraging one of these solutions for deployment
    allows developers to focus on developing impactful AI applications rather than
    infrastructure. As mentioned LangChain plays nicely with several open-source projects
    and frameworks like Ray Serve, BentoML, OpenLLM, Modal, and Jina. In the next
    section, we’ll deploy a chat service webserver based on FastAPI.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 维持成本效益在大量用户交互和与LLM服务相关的高成本的情况下可能具有挑战性。管理效率的策略包括自托管模型、根据流量自动调整资源分配、使用抢占式实例、独立扩展和批处理请求以更好地利用GPU资源。工具和基础设施的选择决定了这些需求之间的权衡。灵活性和便利性非常重要，因为我们希望能够快速迭代，这对于ML和LLM领域的动态性是至关重要的。避免被绑定在一个解决方案上是至关重要的。一个灵活的、可扩展的服务层对各种模型都很关键。模型组合和云提供商的选择构成了这种灵活性方程的一部分。对于最大的灵活性，Infrastructure
    as Code（IaC）工具如Terraform、CloudFormation或Kubernetes YAML文件可以可靠且快速地重新创建您的基础设施。此外，持续集成和持续交付（CI/CD）流水线可以自动化测试和部署过程，以减少错误并促进更快的反馈和迭代。设计健壮的LLM应用服务可能是一个复杂的任务，需要在评估服务框架时了解权衡和关键考虑因素。利用这些解决方案之一进行部署使开发人员能够专注于开发有影响力的AI应用，而不是基础设施。如前所述，LangChain与几个开源项目和框架（如Ray
    Serve、BentoML、OpenLLM、Modal和Jina）很好地配合。在下一节中，我们将基于FastAPI部署一个聊天服务Web服务器。
- en: Fast API webserver
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fast API Web服务器
- en: FastAPI is a very popular choice for deployment of webservers. Designed to be
    fast, easy to use, and efficient, it is a modern, high-performance web framework
    for building APIs with Python. Lanarky is a small, open-source library for deploying
    LLM applications that provides convenient wrappers around Flask API as well as
    Gradio for deployment of LLM applications. This means you can get a REST API endpoint
    as well as the in-browser visualization at once and you only need a few lines
    of code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI是部署Web服务器非常受欢迎的选择。设计快速、易于使用和高效，它是一个用Python构建API的现代高性能Web框架。Lanarky是一个小型的开源库，用于部署LLM应用程序，它提供了方便的封装，使您能够同时获得REST
    API端点和浏览器中的可视化，而且您只需要几行代码。
- en: A **REST API** (Representational State Transfer Application Programming Interface)
    is a set of rules and protocols that allows different software applications to
    communicate with each other over the internet. It follows the principles of REST,
    which is an architectural style for designing networked applications. A REST API
    uses HTTP methods (such as GET, POST, PUT, DELETE) to perform operations on resources,
    and it typically sends and receives data in a standardized format, such as JSON
    or XML.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**REST API**（表征状态传输应用程序编程接口）是一组规则和协议，允许不同的软件应用程序通过互联网相互通信。它遵循 REST 的原则，这是一种用于设计网络应用程序的体系结构风格。REST
    API 使用 HTTP 方法（例如 GET、POST、PUT、DELETE）对资源执行操作，通常以标准化格式（例如 JSON 或 XML）发送和接收数据。'
- en: 'In the library documentation, there are several examples including a Retrieval
    QA with Sources Chain, a Conversational Retrieval app, and a Zero Shot Agent.
    Following another example, we’ll implement a chatbot webserver with Lanarky. We’ll
    set up a web server using Lanarky that integrates with Gradio, creates a `ConversationChain`
    instance with an LLM model and settings, and defines routes for handling HTTP
    requests.First, we’ll import the necessary dependencies, including FastAPI for
    creating the web server, `mount_gradio_app` for integrating with Gradio, `ConversationChain`
    and `ChatOpenAI` from Langchain for handling LLM conversations, and other required
    modules:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在库文档中，有几个示例，包括具有源链的检索 QA、会话检索应用程序和零射击代理。跟随另一个示例，我们将使用 Lanarky 实现一个带有聊天机器人的 Web
    服务器。我们将使用 Lanarky 设置一个与 Gradio 集成的 Web 服务器，创建一个带有 LLM 模型和设置的 `ConversationChain`
    实例，并定义用于处理 HTTP 请求的路由。首先，我们将导入必要的依赖项，包括用于创建 Web 服务器的 FastAPI，用于与 Gradio 集成的 `mount_gradio_app`，用于处理
    LLM 对话的 `ConversationChain` 和 `ChatOpenAI`，以及其他所需模块：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Please note that you need to set your environment variables as explained in
    chapter 3\. A `create_chain()` function is defined to create an instance of `ConversationChain`,
    specifying the LLM model and its settings:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要根据第 3 章的说明设置环境变量。定义了一个`create_chain()`函数来创建`ConversationChain`的一个实例，指定了LLM模型及其设置：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We set the chain as a `ConversationChain`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将链设置为`ConversationChain`。
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The app variable is assigned to `mount_gradio_app`, which creates a `FastAPI`
    instance titled *ConversationChainDemo* and integrates it with Gradio:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将变量`app`分配给`mount_gradio_app`，它创建一个名为*ConversationChainDemo*的`FastAPI`实例，并将其与Gradio集成：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The templates variable gets set to a `Jinja2Templates` class, specifying the
    directory where templates are located for rendering. This specifies how the webpage
    will be shown allowing all kind of customization:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 变量模板被设置为`Jinja2Templates`类，指定了用于渲染模板的目录。这指定了网页将如何显示，允许各种自定义：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An endpoint for handling HTTP GET requests at the root path (`/`) is defined
    using the FastAPI decorator `@app.get`. The function associated with this endpoint
    returns a template response for rendering the index.xhtml template:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FastAPI装饰器`@app.get`定义了用于处理根路径（`/`）的HTTP GET请求的端点。与此端点关联的函数返回一个模板响应，用于渲染index.xhtml模板：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The router object is created as a `LangchainRouter` class. This object is responsible
    for defining and managing the routes associated with the `ConversationChain` instance.
    We can add additional routes to the router for handling JSON-based chat that even
    work with WebSocket requests:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个`LangchainRouter`类的路由器对象。该对象负责定义和管理与`ConversationChain`实例相关联的路由。我们可以向路由器添加其他路由，用于处理基于
    JSON 的聊天，甚至可以处理 WebSocket 请求：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now our application knows how to handle requests made to the specified routes
    defined within the router, directing them to the appropriate functions or handlers
    for processing.We will use Uvicorn to run our application. Uvicorn excels in supporting
    high-performance, asynchronous frameworks like FastAPI and Starlette. It is known
    for its ability to handle a large number of concurrent connections and perform
    well under heavy loads due to its asynchronous nature.We can run the webserver
    from the terminal like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的应用程序知道如何处理请求，这些请求是针对路由器中定义的指定路由发出的，并将它们定向到适当的函数或处理程序进行处理。我们将使用Uvicorn来运行我们的应用程序。Uvicorn擅长支持高性能、异步框架，如FastAPI和Starlette。由于其异步特性，它能够处理大量的并发连接，并在负载较重时表现良好。我们可以像这样从终端运行Web服务器：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This command starts a webserver, which you can view in your browser, at this
    local address: [http://127.0.0.1:8000](http://127.0.0.1:8000)The reload switch
    (`--reload`) is particularly handy, because it means the server will be automatically
    restarted once you’ve made any changes.Here’s a snapshot of the chatbot application
    we’ve just deployed:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令启动了一个 Web 服务器，在本地地址 [http://127.0.0.1:8000](http://127.0.0.1:8000) 可以在浏览器中查看。`--reload`
    开关特别方便，因为这意味着一旦你做出任何更改，服务器将自动重新启动。以下是我们刚刚部署的聊天机器人应用的快照：
- en: '![Figure 9.3: Chatbot in Flask/Lanarky](img/file60.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3: Flask/Lanarky 中的聊天机器人](img/file60.png)'
- en: 'Figure 9.3: Chatbot in Flask/Lanarky'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.3: Flask/Lanarky 中的聊天机器人'
- en: I think this looks quite nice for little work we’ve put in. It also comes with
    a few nice features such as REST API, a web UI, and a websocket interface. While
    Uvicorn itself does not provide built-in load balancing functionality, it can
    work together with other tools or technologies such as Nginx or HAProxy to achieve
    load balancing in a deployment setup, which distribute incoming client requests
    across multiple worker processes or instances. The use of Uvicorn with load balancers
    enables horizontal scaling to handle large traffic volumes, improves response
    times for clients, enhances fault tolerance.In the next section, we’ll see how
    to build robust and cost-effective generative AI applications with Ray. We'll
    built a simple search engine using LangChain for text processing and Ray for scaling
    indexing and serving.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们的工作量相当小，这看起来相当不错。它还拥有一些很棒的功能，比如 REST API，Web UI 和 WebSocket 接口。虽然 Uvicorn
    本身并不提供内置的负载平衡功能，但它可以与其他工具或技术（如 Nginx 或 HAProxy）一起工作，以在部署设置中实现负载平衡，从而将传入的客户端请求分发给多个工作进程或实例。Uvicorn
    与负载平衡器的使用可以实现横向扩展，处理大量流量，为客户端提供改进的响应时间，增强容错能力。在下一节，我们将看到如何使用 Ray 构建稳健且经济高效的生成式
    AI 应用。我们将使用 LangChain 进行文本处理，并使用 Ray 进行索引和服务方面的扩展，构建一个简单的搜索引擎。
- en: Ray
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ray
- en: 'Ray provides a flexible framework to meet infrastructure challenges of complex
    neural networks in production by scaling out generative AI workloads across clusters.
    Ray helps with common deployment needs like low-latency serving, distributed training,
    and large-scale batch inference. Ray also makes it easy to spin up on-demand fine-tuning
    or scale existing workloads from one machine to many. Some capability includes:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 提供了一个灵活的框架，以满足在生产环境中扩展复杂神经网络的基础设施挑战，通过在集群中扩展生成式 AI 工作负载。Ray 通过在低延迟服务、分布式训练和大规模批量推断等方面帮助解决常见的部署需求。Ray
    还能够轻松地按需启动微调或将现有工作负载从一台机器扩展到多台机器。具体的功能包括：
- en: Schedule distributed training jobs across GPU clusters using Ray Train
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Train 在 GPU 集群上调度分布式训练任务
- en: Deploy pre-trained models at scale for low-latency serving with Ray Serve
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Serve 在规模上部署预训练模型，以实现低延迟服务
- en: Run large batch inference in parallel across CPUs and GPUs with Ray Data
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray Data 在 CPU 和 GPU 上并行运行大批推断
- en: Orchestrate end-to-end generative AI workflows combining training, deployment,
    and batch processing
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编排端到端的生成式 AI 工作流，包括训练、部署和批处理
- en: 'We''ll use LangChain and Ray to build a simple search engine for the Ray documentation
    following an example implemented by Waleed Kadous for the anyscale Blog and on
    the langchain-ray repository on Github. You can see this as an extension of the
    recipe in *Channel 5*. You can see the full code for this recipe under semantic
    search here: [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)
    You’ll also see how to run this as a FastAPI server.First, we''ll ingest and index
    the Ray docs so we can quickly find relevant passages for a search query:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 LangChain 和 Ray 来构建一个简单的搜索引擎，以此跟随 Waleed Kadous 在 anyscale 博客和 Github
    上的 langchain-ray 仓库实现的示例。你可以把这看作是 *Channel 5* 中实现的一个扩展。你可以在这里看到这个示例的完整代码：[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)
    你也可以看到如何将其作为 FastAPI 服务器运行。首先，我们将摄取并索引 Ray 文档，以便能够快速找到搜索查询的相关段落：
- en: '[PRE23]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This builds our search index by ingesting the docs, splitting into sentences,
    embedding the sentences, and indexing the vectors. Alternatively, we can accelerate
    the indexing by parallelizing the embedding step:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 通过摄取文档、拆分成句子、将句子嵌入和索引向量来构建我们的搜索索引。另外，我们也可以通过并行化嵌入步骤来加速索引：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'By running embedding on each shard in parallel, we can significantly reduce
    indexing time. We save the database index to disk:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在每个分片中并行运行嵌入，我们可以显著减少索引时间。然后将数据库索引保存到磁盘：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`FAISS_INDEX_PATH` is an arbitrary file name. I’ve set it to `faiss_index.db`.Next,
    we’ll see how we can serve search queries with Ray Serve.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`FAISS_INDEX_PATH` 是一个任意的文件名。我已将其设置为 `faiss_index.db`。接下来，我们将看到如何使用 Ray Serve
    提供搜索查询服务。'
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This lets us serve search queries as a web endpoint! Running this gives me
    this output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们可以将搜索查询作为 Web 端点提供！运行后，我得到了这个输出：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now query it from Python:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以从 Python 进行查询：
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For me, the server fetches the Ray use cases page at: [http://https://docs.ray.io/en/latest/ray-overview/use-cases.xhtml](http://https://docs.ray.io/en/latest/ray-overview/use-cases.xhtml)What
    I really liked was the monitoring with the Ray Dashboard, which looks like this:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对我而言，服务器在[http://https://docs.ray.io/en/latest/ray-overview/use-cases.xhtml](http://https://docs.ray.io/en/latest/ray-overview/use-cases.xhtml)获取了
    Ray 的用例页面。我真的很喜欢 Ray 仪表板上的监控，它看起来像这样：
- en: '![Figure 9.4: Ray Dashboard.](img/file61.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4：Ray 仪表板。](img/file61.png)'
- en: 'Figure 9.4: Ray Dashboard.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：Ray 仪表板。
- en: This dashboard very powerful as it can give you a whole bunch of metrics and
    other information. Collecting metrics is really easy, since all you have to do
    is setting and updating variables of type `Counter`, `Gauge`, `Histogram` or other
    types within the deployment object or actor. For time series charts you should
    have either Prometheus or Grafana server installed. As you can see in the full
    implementation on Github, we can also spin this up as a FastAPI server. This concludes
    our simple semantic search engine with LangChain and Ray. As models and LLM apps
    grow more sophisticated and highly interwoven into the fabric of business applications,
    observability and monitoring during production become necessary to ensure their
    accuracy, efficiency, and reliability ongoing. The next section focuses on the
    significance of monitoring LLMs and highlights key metrics to track for a comprehensive
    monitoring strategy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个仪表板非常强大，因为它可以为您提供大量的指标和其他信息。收集指标真的很容易，因为您所需做的就是设置和更新部署对象或者 actor 类型的变量，例如
    `Counter`、`Gauge`、`Histogram` 或其他类型。对于时序图表，您应该安装 Prometheus 或 Grafana 服务器。正如您在
    Github 上的完整实现中所看到的，我们也可以将其作为 FastAPI 服务器运行。这结束了我们带有 LangChain 和 Ray 的简单语义搜索引擎。随着模型和
    LLM 应用程序变得越来越复杂，并与业务应用程序的结构密切交织，生产中的可观察性和监控变得必不可少，以确保它们的准确性、效率和可靠性。接下来的部分将重点放在监视
    LLM 的重要性，并突出了全面监控策略中要跟踪的关键指标。
- en: How to observe LLM apps?
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何观察 LLM 应用程序？
- en: The dynamic nature of real-world operations means that the conditions assessed
    during offline evaluations hardly cover all potential scenarios that LLMs may
    encounter in production systems. Thus comes the need for observability in production
    – a more on-going, real-time observation to capture anomalies that offline tests
    could not anticipate.Observability allows monitoring behaviors and outcomes as
    the model interacts with actual input data and users in production. It includes
    logging, tracking, tracing and alerting mechanisms to ensure healthy system functioning,
    performance optimization and catching issues like model drift early.As discussed,
    LLMs have become increasingly important components of many applications in sectors
    like health, e-commerce, and education.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界运营的动态性意味着离线评估中评估的条件几乎不可能涵盖 LLM 在生产系统中可能遇到的所有潜在场景。因此，生产中需要可观察性 - 更持续的、实时的观察，以捕捉离线测试无法预料的异常情况。可观察性允许监视模型与实际输入数据和用户在生产中交互时的行为和结果。它包括日志记录、跟踪、追踪和警报机制，以确保系统功能良好、优化性能，并及早发现诸如模型漂移之类的问题。正如所讨论的，LLM
    已成为医疗、电子商务和教育等许多行业应用程序中越来越重要的组成部分。
- en: Tracking, tracing, and monitoring are three important concepts in the field
    of software operation and management. While all related to understanding and improving
    a system's performance, they each have distinct roles. While tracking and tracing
    are about keeping detailed historical records for analysis and debugging, monitoring
    is aimed at real-time observation and immediate awareness of issues to ensure
    optimal system functionality at all times. All three of these concepts fall within
    the category of observability.
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 跟踪、追踪和监控是软件运行和管理领域的三个重要概念。虽然都与理解和改进系统性能有关，但它们各自扮演着不同的角色。跟踪和追踪都是为了保持详细的历史记录以进行分析和调试，而监控旨在实时观察和及时发现问题，以确保系统功能始终处于最佳状态。这三个概念都属于可观察性的范畴。
- en: ''
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Monitoring** is the ongoing process of overseeing the performance of a system
    or application. This might involve continuously collecting and analyzing metrics
    related to system health such as memory usage, CPU utilization, network latency,
    and the overall application/service performance (like response time). Effective
    monitoring includes setting up alert systems for anomalies or unexpected behaviors
    – sending notifications when certain thresholds are exceeded. While tracking and
    tracing are about keeping detailed historical records for analysis and debugging,
    monitoring is aimed at real-time observation & immediate awareness of issues to
    ensure optimal system functionality at all times.'
  id: totrans-160
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**监控**是持续监视系统或应用程序性能的过程。这可能涉及持续收集和分析与系统健康相关的指标，如内存使用、CPU利用率、网络延迟以及整体应用程序/服务性能（如响应时间）。有效的监控包括设置异常或意外行为的警报系统
    - 在超出某些阈值时发送通知。虽然跟踪和追踪是关于保留详细的历史记录以进行分析和调试，但监控旨在实时观察和立即意识到问题，以确保系统功能始终处于最佳状态。'
- en: 'The chief aim for monitoring and observability is to provide insights into
    model performance and behavior through real-time data. This helps in:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和可观测性的主要目标是通过实时数据提供对模型性能和行为的洞察。这有助于：
- en: '**Preventing model drift**: Models can degrade over time due to changes in
    the characteristics of input data or user behavior. Regular monitoring can identify
    such situations early and apply corrective measures.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预防模型漂移**：模型可能随着时间的推移而因输入数据或用户行为特征的变化而恶化。定期监控可以早期识别这种情况并采取纠正措施。'
- en: '**Performance optimization**: By tracking metrics like inference times, resource
    usage, and throughput, you can make adjustments to improve the efficiency and
    effectiveness of LLMs in production.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能优化**：通过跟踪推理时间、资源使用情况和吞吐量等指标，您可以调整以提高LLM在生产中的效率和效果。'
- en: '**A/B Testing**: It helps compare how slight differences in models may result
    in different outcomes which aids in decision-making towards model improvements.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B测试**：它有助于比较模型中轻微差异可能导致不同结果的情况，从而有助于决策改进模型。'
- en: '**Debugging Issues**: Monitoring helps identify unforeseen problems that can
    occur during runtime, enabling rapid resolution.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调试问题**：监控有助于识别运行时可能发生的未预料问题，从而快速解决。'
- en: 'It’s important to consider the monitoring strategy that consists of a few considerations:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑监控策略时，有几点需要考虑：
- en: '**Metrics to monitor**: Define key metrics of interest such as prediction accuracy,
    latency, throughput etc. based on desired model performance.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控的指标**：根据所需的模型性能，定义关键的感兴趣的指标，如预测准确性、延迟、吞吐量等。'
- en: '**Monitoring Frequency**: Frequency should be determined based on how critical
    the model is to operations - a highly critical model may require near real-time
    monitoring.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控频率**：应根据模型对操作的关键程度来确定频率 - 高度关键的模型可能需要近实时监控。'
- en: '**Logging**: Logs should provide comprehensive details regarding every relevant
    action performed by the LLM so analysts can track back any anomalies.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日志记录**：日志应提供关于LLM执行的每个相关操作的详细信息，以便分析人员可以追踪任何异常情况。'
- en: '**Alerting Mechanism**: The system should raise alerts if it detects anomalous
    behavior or drastic performance drops.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**警报机制**：如果检测到异常行为或性能急剧下降，系统应该提出警报。'
- en: 'Monitoring LLMs serves multiple purposes, including assessing model performance,
    detecting abnormalities or issues, optimizing resource utilization, and ensuring
    consistent and high-quality outputs. By continuously evaluating the behavior and
    performance of LLMs via validation, shadow launches, and interpretation along
    with dependable offline evaluation, organizations can identify and mitigate potential
    risks, maintain user trust, and provide an optimal experience.Here’s a list of
    relevant metrics:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过验证、阴影启动和解释以及可靠的离线评估等方式持续评估LLM的行为和性能，组织可以识别和消除潜在风险，保持用户信任，并提供最佳体验。以下是相关指标的列表：
- en: '**Inference Latency**: Measure the time it takes for the LLM to process a request
    and generate a response. Lower latency ensures a faster and more responsive user
    experience.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推断延迟**：衡量LLM处理请求并生成响应所需的时间。较低的延迟确保了更快和更具响应性的用户体验。'
- en: '**Query per Second (QPS)**: Calculate the number of queries or requests that
    the LLM can handle within a given time frame. Monitoring QPS helps assess scalability
    and capacity planning.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每秒查询（QPS）**：计算 LLM 可以在给定时间内处理的查询或请求数量。监控 QPS 有助于评估可伸缩性和容量规划。'
- en: '**Token Per Second (TPS)**: Track the rate at which the LLM generates tokens.
    TPS metrics are useful for estimating computational resource requirements and
    understanding model efficiency.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每秒标记数（TPS）**：跟踪 LLM 生成标记的速率。TPS 指标对于估计计算资源需求和了解模型效率很有用。'
- en: '**Token Usage**: The number of tokens correlates with the resource usage such
    as hardware utilization, latency, and costs.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记使用**：标记数量与资源使用（如硬件利用、延迟和成本）相关。'
- en: '**Error Rate**: Monitor the occurrence of errors or failures in LLM responses,
    ensuring error rates are kept within acceptable limits to maintain the quality
    of outputs.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误率**：监控 LLM 响应中错误或失败的发生，确保错误率保持在可接受的范围内，以保持输出的质量。'
- en: '**Resource Utilization**: Measure the consumption of computational resources,
    such as CPU, memory, and GPU, to optimize resource allocation and avoid bottlenecks.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源利用**：测量计算资源（如 CPU、内存和 GPU）的消耗，以优化资源分配并避免瓶颈。'
- en: '**Model Drift**: Detect changes in LLM behavior over time by comparing its
    outputs to a baseline or ground truth, ensuring the model remains accurate and
    aligned with expected outcomes.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型漂移**：通过将其输出与基线或基本事实进行比较，以检测 LLM 行为随时间的变化，确保模型保持准确性并符合预期结果。'
- en: '**Out-of-Distribution Inputs**: Identify inputs or queries falling outside
    the intended distribution of the LLM''s training data, which can cause unexpected
    or unreliable responses.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超出分布范围的输入**：识别落在 LLM 训练数据意图分布范围之外的输入或查询，这可能导致意外或不可靠的响应。'
- en: '**User Feedback Metrics**: Monitor user feedback channels to gather insights
    on user satisfaction, identify areas for improvement, and validate the effectiveness
    of the LLM.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户反馈指标**：监控用户反馈渠道，获取关于用户满意度的见解，找到改进的领域，并验证 LLM 的有效性。'
- en: Data scientists and machine learning engineers should check for staleness, incorrect
    learning, and bias using model interpretation tools like LIME and SHAP. The most
    predictive features changing suddenly could indicate a data leak. Offline metrics
    like AUC do not always correlate with online impacts on conversion rate, so it
    is important to find dependable offline metrics that translate to online gains
    relevant to the business ideally direct metrics like clicks and purchases that
    the system impacts directly.Effective monitoring enables the successful deployment
    and utilization of LLMs, boosting confidence in their capabilities and fostering
    user trust. It should be cautioned, however, that you should study the privacy
    and data protection policy when relying on cloud service platforms.In the next
    section, we’ll look at monitoring the trajectory of an agent.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和机器学习工程师应该使用如 LIME 和 SHAP 这样的模型解释工具检查数据的陈旧性、错误学习和偏见。最具预测性的特征突然变化可能表明数据泄漏。离线指标如
    AUC 并不总是与在线转化率的影响相关，因此找到可信赖的离线指标非常重要，这些指标可以转化为业务上的在线收益，最好是系统直接影响的点击和购买等直接指标。有效的监控可以实现
    LLM 的成功部署和利用，增强对它们能力的信心，建立用户信任。不过，当依赖云服务平台时，应当注意研究隐私和数据保护政策。在接下来的部分，我们将来看看一个代理的轨迹监控。
- en: Tracking and Tracing
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪和追踪
- en: '**Tracking** generally refers to the process of recording and managing information
    about a particular operation or series of operations within an application or
    system. For example, in machine learning applications or projects, tracking can
    involve keeping a record of parameters, hyperparameters, metrics, outcomes across
    different experiments or runs. It provides a way to document the progress and
    changes over time.'
  id: totrans-183
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**跟踪** 通常指的是记录和管理应用程序或系统中特定操作或一系列操作的信息的过程。例如，在机器学习应用程序或项目中，跟踪可能涉及在不同实验或运行中记录参数、超参数、度量标准、结果等。这提供了一种记录进展和随时间变化的方式。'
- en: ''
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Tracing** is a more specialized form of tracking. It involves recording the
    execution flow through software/systems. Particularly in distributed systems where
    a single transaction might span multiple services, tracing helps in maintaining
    an audit or breadcrumb trail, a detailed information about that request path through
    the system. This granular view enables developers to understand the interaction
    between various microservices and troubleshoot issues like latency or failures
    by identifying exactly where they occurred in the transaction path.'
  id: totrans-185
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**追踪（Tracing）**是跟踪的一种更专业的形式。它涉及通过软件/系统记录执行流程。特别是在单个交易可能涉及多个服务的分布式系统中，追踪有助于保持审计或面包屑路径，详细信息包括请求路径通过系统的详细信息。这种细粒度的视图使开发人员能够理解各种微服务之间的交互，并通过确定它们在事务路径中发生的确切位置来解决延迟或故障等问题。'
- en: 'Tracking the trajectory of agents can be challenging due to their broad range
    of actions and generative capabilities. LangChain comes with functionality for
    trajectory tracking and evaluation. Seeing the traces of an agent is actually
    really easy! You just have to set the return_`intermediate_steps` parameter to
    `True` when initializing an agent or an LLM. Let’s have a quick look at this.
    I’ll skip the imports and setting up the environment. You can find the full listing
    on github under monitoring at this address: [https://github.com/benman1/generative_ai_with_langchain/](https://github.com/benman1/generative_ai_with_langchain/)We’ll
    define a tool. It’s very convenient to use the `@tool` decorator, which will use
    the function docstring as description of the tool. The first tool sends a ping
    to a website address and returns information about packages transmitted and latency
    or – in the case of an error – the error message:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪代理的轨迹可能具有挑战性，因为它们广泛的行动范围和生成能力。LangChain提供了用于轨迹跟踪和评估的功能。实际上，看到代理的迹象非常容易！你只需在初始化代理或LLM时将return_`intermediate_steps`参数设置为`True`。让我们快速看一下这个。我会跳过导入和设置环境。您可以在github上找到完整的列表，在此地址下的监控：[https://github.com/benman1/generative_ai_with_langchain/](https://github.com/benman1/generative_ai_with_langchain/)我们将定义一个工具。使用`@tool`装饰器非常方便，它将使用函数文档字符串作为工具的描述。第一个工具向网站地址发送一个ping，并返回有关传输包和延迟的信息，或者（在出现错误的情况下）返回错误消息：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now we set up an agent that uses this tool with an LLM to make the calls given
    a prompt:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们设置了一个代理，使用这个工具和LLM来调用给定的提示：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The agent reports this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 代理报告如下：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In `results[`"`intermediate_steps`"`]` we can see all lot of information about
    the agent’s actions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在`results[`"`intermediate_steps`"`]`中，我们可以看到有关代理动作的大量信息：
- en: '[PRE32]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: By providing visibility into the system and aiding in problem identification
    and optimization efforts, this kind of tracking and evaluation can be very helpful.
    The LangChain documentation demonstrates how to use a trajectory evaluator to
    examine the full sequence of actions and responses they generate, and grade an
    OpenAI functions agent. Let’s have a look beyond LangChain and see what’s out
    there for observability!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供对系统的可见性，并协助问题识别和优化工作，这种跟踪和评估可以非常有帮助。LangChain文档演示了如何使用轨迹评估器来检查它们生成的完整动作和响应序列，并对OpenAI函数代理进行评分。让我们超越LangChain，看看观测能提供什么！
- en: Observability tools
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观测工具
- en: 'There are quite a few tools available as integrations in LangChain or through
    callbacks:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain中或通过回调可用的集成工具有相当多：
- en: '**Argilla**: Argilla is an open-source data curation platform that can integrate
    user feedback (human-in-the-loop workflows) with prompts and responses to curate
    datasets for fine-tuning.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Argilla**：Argilla是一个开源数据整理平台，可以将用户反馈（人在回路工作流程）与提示和响应集成，以整理数据集进行精细调整。'
- en: '**Portkey**: Portkey adds essential MLOps capabilities like monitoring detailed
    metrics, tracing chains, caching, and reliability through automatic retries to
    LangChain.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Portkey**：Portkey将基本的MLOps功能添加到LangChain中，例如监视详细指标、追踪链、缓存和通过自动重试确保可靠性。'
- en: '**Comet.ml**: Comet offers robust MLOps capabilities for tracking experiments,
    comparing models and optimizing AI projects.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Comet.ml**：Comet为跟踪实验、比较模型和优化AI项目提供了强大的MLOps能力。'
- en: '**LLMonitor**: Tracks lots of metrics including cost and usage analytics (user
    tracking), tracing, and evaluation tools (open-source).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLMonitor**：跟踪大量指标，包括成本和使用情况分析（用户跟踪）、追踪和评估工具（开源）。'
- en: '**DeepEval**: Logs default metrics like relevance, bias, and toxicity. Can
    also help in testing and in monitoring model drift or degradation.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepEval**：记录默认指标，如相关性、偏见和毒性。还可以帮助测试和监视模型漂移或退化。'
- en: '**Aim**: An open-source visualization and debugging platform for ML models.
    It logs inputs, outputs, and the serialized state of components, enabling visual
    inspection of individual LangChain executions and comparing multiple executions
    side-by-side.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Aim**：一个用于ML模型的开源可视化和调试平台。它记录输入、输出和组件的序列化状态，可以对单个LangChain执行进行视觉检查，并将多个执行进行比较。'
- en: '**Argilla**: An open-source platform for tracking training data, validation
    accuracy, parameters, and more across machine learning experiments.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Argilla**：一个用于跟踪训练数据、验证准确性、参数等的开源平台，可跨机器学习实验进行。'
- en: '**Splunk**: Splunk''s Machine Learning Toolkit can provide observability into
    your machine learning models in production.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Splunk**：Splunk的机器学习工具包可以提供对生产中机器学习模型的可观察性。'
- en: '**ClearML**: An open-source tool for automating training pipelines, seamlessly
    moving from research to production.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClearML**：一个用于自动化训练流水线的开源工具，可无缝地从研究转向生产。'
- en: '**IBM Watson OpenScale**: A platform providing insights into AI health with
    fast problem identification and resolution to help mitigate risks.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IBM Watson OpenScale**：提供对AI健康状况的洞察，快速识别和解决问题，以帮助减轻风险的平台。'
- en: '**DataRobot MLOps**: Monitors and manages models to detect issues before they
    impact performance.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataRobot MLOps**：监控和管理模型，以在影响性能之前检测问题。'
- en: '**Datadog APM Integration**: This integration allows you to capture LangChain
    requests, parameters, prompt-completions, and visualize LangChain operations.
    You can also capture metrics such as request latency, errors, and token/cost usage.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Datadog APM Integration**：此集成允许您捕获LangChain请求、参数、提示完成，并可视化LangChain操作。您还可以捕获指标，如请求延迟、错误和令牌/成本使用。'
- en: '**Weights and Biases (W&B**) **Tracing**: We’ve already shown an example of
    using (W&B) for monitoring of fine-training convergence, but it can also fulfill
    the role of tracking other metrics and of logging and comparing prompts.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重和偏差（W&B）跟踪**：我们已经展示了使用（W&B）监控微调收敛的示例，但它还可以承担跟踪其他指标、记录和比较提示的作用。'
- en: '**Langfuse**: With this open-source tool, we can conveniently monitor detailed
    information along the traces regarding latency, cost, scores of our LangChain
    agents and tools.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Langfuse**：使用这个开源工具，我们可以方便地监视关于我们的LangChain代理和工具的跟踪的详细信息，如延迟、成本和分数。'
- en: Most of these integrations are very easy to integrate into LLM pipelines. For
    example, For W&B, you can enable tracing by setting the `LANGCHAIN_WANDB_TRACING`
    environment variable to `True`. Alternatively, you can use a context manager with
    `wandb_tracing_enabled()` to trace a specific block of code. With Langfuse, we
    can hand over a `langfuse.callback.CallbackHandler()` as an argument to the `chain.run()`
    call.Some of these tools are open-source, and what’s great about these platforms
    is that it allows full customization and on-premise deployment for use cases,
    where privacy is important. For example, Langfuse is open-source and provides
    an option of self-hosting. Choose the option that best suits your needs and follow
    the instructions provided in the LangChain documentation to enable tracing for
    your agents.Having been released only recently, I am sure there’s much more to
    come for the platform, but it’s already great to see traces of how agents execute,
    detecting loops and latency issues. It enables sharing traces and stats with collaborators
    to discuss improvements.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些集成大多很容易集成到LLM流水线中。例如，对于W&B，您可以通过将`LANGCHAIN_WANDB_TRACING`环境变量设置为`True`来启用跟踪。或者，您可以使用`wandb_tracing_enabled()`上下文管理器来跟踪特定的代码块。使用Langfuse，我们可以将`langfuse.callback.CallbackHandler()`作为参数传递给`chain.run()`调用。这些工具中的一些是开源的，而这些平台的优点在于它允许完全定制和本地部署，适用于隐私重要的用例。例如，Langfuse是开源的，并提供自托管选项。选择最适合您需求的选项，并按照LangChain文档中提供的说明启用代理的跟踪。虽然该平台只是最近发布的，但我相信它还有更多的功能，但已经很棒了，可以看到代理执行的迹象，检测到循环和延迟问题。它允许与合作者共享跟踪和统计数据，以讨论改进。
- en: LangSmith
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LangSmith
- en: 'LangSmith is a framework for debugging, testing, evaluating, and monitoring
    LLM applications developed and maintained by LangChain AI, the organization behind
    LangChain. LangSmith serves as an effective tool for MLOps, specifically for LLMs,
    by providing features that cover multiple aspects of the MLOps process. It can
    help developers take their LLM applications from prototype to production by providing
    features for debugging, monitoring, and optimizing. LangSmith aims to reduce the
    barrier to entry for those without a software background by providing a simple
    and intuitive user interface. LangSmith is a platform for debugging, testing,
    and monitoring large language models (LLMs) built with LangChain. It allows you
    to:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'LangSmith是由LangChain AI开发和维护的用于调试、测试、评估和监控LLM应用程序的框架。LangSmith通过提供覆盖MLOps过程多个方面的特性，成为LLM的MLOps的有效工具。它可以通过提供调试、监控和优化功能，帮助开发人员将LLM应用程序从原型制作阶段推进到生产。LangSmith旨在为没有软件背景的人提供简单直观的用户界面，降低进入门槛。LangSmith是构建在LangChain上的大型语言模型（LLMs）的调试、测试和监控平台。它允许你： '
- en: Log traces of runs from your LangChain agents, chains, and other components
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录你的LangChain代理、链和其他组件的运行日志
- en: Create datasets to benchmark model performance
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建数据集以对模型表现进行基准测试
- en: Configure AI-assisted evaluators to grade your models
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置AI辅助评估器来为你的模型评分
- en: View metrics, visualizations, and feedback to iterate and improve your LLMs
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看指标、可视化和反馈，迭代和改进你的LLM应用
- en: 'LangSmith fulfils the requirements for MLOps for agents by providing features
    and capabilities that enable developers to debug, test, evaluate, monitor, and
    optimize language model applications. Its integration within the LangChain framework
    enhances the overall development experience and facilitates the full potential
    of language model applications. By using both two platforms, developers can take
    their LLM applications from prototype to production stage and optimize latency,
    hardware efficiency, and cost.We can get a large set of graphs for a bunch of
    important statistics as we can see here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: LangSmith通过提供可用于调试、测试、评估、监控和优化语言模型应用程序的功能和能力，满足代理的MLOps要求。其在LangChain框架中的集成增强了整个开发体验，并促进了语言模型应用程序的全部潜力。通过同时使用两个平台，开发人员可以将LLM应用程序从原型制作阶段推进到生产阶段，并优化延迟、硬件效率和成本。我们可以获得一系列重要统计数据的大量图表，如下所示：
- en: '![Figure 9.5: Evaluator metrics in LangSmith.](img/file62.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5：LangSmith中的评估器指标。](img/file62.png)'
- en: 'Figure 9.5: Evaluator metrics in LangSmith.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：LangSmith中的评估器指标
- en: 'The monitoring dashboard includes the following graphs that can be broken down
    into different time intervals:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 监控仪表板包括以下图表，可以按不同时间间隔进行分解：
- en: '| Statistics | Category |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 统计数据 | 类别 |'
- en: '| Trace Count, LLM Call Count, Trace Success Rates, LLM Call Success Rates
    | Volume |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 溯源计数、LLM调用计数、溯源成功率、LLM调用成功率 | 体积 |'
- en: '| Trace Latency (s), LLM Latency (s), LLM Calls per Trace, Tokens / sec | Latency
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 溯源延迟（秒）、LLM延迟（秒）、每个溯源的LLM调用次数、每秒令牌数 | 延迟 |'
- en: '| Total Tokens, Tokens per Trace, Tokens per LLM Call | Tokens |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 总令牌数、每个溯源的令牌数、每个LLM调用的令牌数 | 令牌 |'
- en: '| % Traces w/ Streaming, % LLM Calls w/ Streaming, Trace Time-to-First-Token
    (ms), LLM Time-to-First-Token (ms) | Streaming |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| % 具有流处理的溯源、% 具有流处理的LLM调用，溯源第一个令牌时间（毫秒）、LLM第一个令牌时间（毫秒） | 流处理 |'
- en: 'Figure 9.6: Statistisc in LangSmith.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：LangSmith中的统计数据
- en: 'Here’s a tracing example in LangSmith for the benchmark dataset run that we’ve
    seen in the section on evaluation:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例，展示了在评估章节中看到的基准数据集运行的LangSmith跟踪：
- en: '![Figure 9.7: Tracing in LangSmith.](img/file63.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7：LangSmith中的跟踪。](img/file63.png)'
- en: 'Figure 9.7: Tracing in LangSmith.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：LangSmith中的跟踪
- en: The platform itself is not open source, however, LangChain AI, the company behind
    LangSmith and LangChain, provide some support for self-hosting for organizations
    with privacy concerns. There are however a few alternatives to LangSmith such
    as Langfuse, Weights and Biases, Datadog APM, Portkey, and PromptWatch, with some
    overlap in features. We’ll focus about LangSmith here, because it has a large
    set of features for evaluation and monitoring, and because it integrates with
    LangChain.In the next section, we’ll demonstrate the utilization of PromptWatch.io
    for prompt tracking of LLMs in production environments.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: PromptWatch
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PromptWatch records information about the prompt and the generated output during
    this interaction.Let’s get the inputs out of the way.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As mentioned in Chapter 3, I’ve set all API keys in the environment in the set_environment()
    function.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Using the `PromptTemplate` class, the prompt template is set to with one variable,
    `input`, indicating where the user input should be placed within the prompt.Inside
    the `PromptWatch` block, the `LLMChain` is invoked with an input prompt as an
    example of the model generating a response based on the provided prompt.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Figure 9.8: Prompt tracking at PromptWatch.io.](img/file64.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Prompt tracking at PromptWatch.io.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: This seems quite useful. By leveraging PromptWatch.io, developers and data scientists
    can effectively monitor and analyze LLMs’ prompts, outputs, and costs in real-world
    scenarios.PromptWatch.io offers comprehensive chain execution tracking and monitoring
    capabilities for LLMs. With PromptWatch.io, you can track all aspects of LLM chains,
    actions, retrieved documents, inputs, outputs, execution time, tool details, and
    more for complete visibility into your system. The platform allows for in-depth
    analysis and troubleshooting by providing a user-friendly, visual interface that
    enables users to identify the root causes of issues and optimize prompt templates.
    PromptWatch.io can also help with unit testing and for versioning prompt templates.Let’s
    summarize this chapter!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Successfully deploying LLMs and other generative AI models in a production setting
    is a complex but manageable task that requires careful consideration of numerous
    factors. It requires addressing challenges related to data quality, bias, ethics,
    regulatory compliance, interpretability, resource requirements, and ongoing monitoring
    and maintenance, among others.The evaluation of LLMs is an important step in assessing
    their performance and quality. LangChain supports comparative evaluation between
    models, checking outputs against criteria, simple string matching, and semantic
    similarity metrics. These provide different insights into model quality, accuracy,
    and appropriate generation. Systematic evaluation is key to ensuring large language
    models produce useful, relevant, and sensible outputs. Monitoring LLMs is a vital
    aspect of deploying and maintaining these complex systems. With the increasing
    adoption of LLMs in various applications, ensuring their performance, effectiveness,
    and reliability is of utmost importance. We’ve discussed the significance of monitoring
    LLMs, highlighted key metrics to track for a comprehensive monitoring strategy,
    and have given examples of how to track metrics in practice.LangSmith provides
    powerful capabilities to track, benchmark, and optimize large language models
    built with LangChain. Its automated evaluators, metrics, and visualizations help
    accelerate LLM development and validation.Let’s see if you remember the key points
    from this chapter!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 成功在生产环境中部署LLMs和其他生成式AI模型是一个复杂但可控的任务，需要仔细考虑许多因素。它需要解决与数据质量、偏见、伦理、法规合规性、解释性、资源需求以及持续监控和维护等挑战相关的问题。LLMs的评估是评估它们的性能和质量的重要步骤。LangChain支持模型之间的比较评估，通过对输出进行标准检查、简单的字符串匹配和语义相似性度量。这些提供了不同的模型质量、准确性和适当生成的见解。系统化的评估是确保大型语言模型产生有用、相关和合理输出的关键。监控LLMs是部署和维护这些复杂系统的重要方面。随着LLMs在各种应用中的不断采用，确保它们的性能、有效性和可靠性至关重要。我们已经讨论了监视LLMs的重要性，重点介绍了全面监控策略所需追踪的关键指标，并举例说明了如何在实践中追踪指标。LangSmith提供了强大的能力，可以跟踪、基准测试和优化使用LangChain构建的大型语言模型。它的自动评估器、指标和可视化帮助加速LLM的开发和验证。让我们看看你是否记得本章的重点！
- en: Questions
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions
    from. If you are unsure about any of them, you might want to refer to the corresponding
    section in the chapter:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请看看你能否从中得出这些问题的答案。如果你对其中任何一个不确定，你可能需要参考本章对应的部分：
- en: In your opinion what is the best term for describing the operationalization
    of language models, LLM apps, or apps that rely on generative models in general?
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你看来，描述语言模型、LLM应用程序或一般依赖于生成模型的应用程序的运行化最佳术语是什么？
- en: How can we evaluate LLMs apps?
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何评估LLMs应用程序？
- en: Which tools can help for evaluating LLM apps?
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些工具可以帮助评估LLM应用程序？
- en: What are considerations for production deployment of agents?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理的生产部署需要考虑哪些因素？
- en: Name a few tools for deployment?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说出几个部署工具的名字？
- en: What are important metrics for monitoring LLMs in production?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控LLMs在生产环境中的重要指标是什么？
- en: How can we monitor these models?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何监视这些模型？
- en: What’s LangSmith?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是LangSmith？
