["```py\n>>> import pandas as pd\n>>> df_wine = pd.read_csv(\n...     'https://archive.ics.uci.edu/ml/'\n...     'machine-learning-databases/wine/wine.data',\n...     header=None\n... ) \n```", "```py\ndf = pd.read_csv(\n    'https://archive.ics.uci.edu/ml/'\n    'machine-learning-databases/wine/wine.data',\n    header=None\n) \n```", "```py\ndf = pd.read_csv(\n    'your/local/path/to/wine.data',\n    header=None\n) \n```", "```py\n>>> from sklearn.model_selection import train_test_split\n>>> X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n>>> X_train, X_test, y_train, y_test = \\\n...     train_test_split(X, y, test_size=0.3,\n...                      stratify=y,\n...                      random_state=0)\n>>> # standardize the features\n>>> from sklearn.preprocessing import StandardScaler\n>>> sc = StandardScaler()\n>>> X_train_std = sc.fit_transform(X_train)\n>>> X_test_std = sc.transform(X_test) \n```", "```py\n>>> import numpy as np\n>>> cov_mat = np.cov(X_train_std.T)\n>>> eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n>>> print('\\nEigenvalues \\n', eigen_vals)\nEigenvalues\n[ 4.84274532  2.41602459  1.54845825  0.96120438  0.84166161\n  0.6620634   0.51828472  0.34650377  0.3131368   0.10754642\n  0.21357215  0.15362835  0.1808613 ] \n```", "```py\n>>> tot = sum(eigen_vals)\n>>> var_exp = [(i / tot) for i in\n...            sorted(eigen_vals, reverse=True)]\n>>> cum_var_exp = np.cumsum(var_exp)\n>>> import matplotlib.pyplot as plt\n>>> plt.bar(range(1,14), var_exp, align='center',\n...         label='Individual explained variance')\n>>> plt.step(range(1,14), cum_var_exp, where='mid',\n...          label='Cumulative explained variance')\n>>> plt.ylabel('Explained variance ratio')\n>>> plt.xlabel('Principal component index')\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> # Make a list of (eigenvalue, eigenvector) tuples\n>>> eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n...                 for i in range(len(eigen_vals))]\n>>> # Sort the (eigenvalue, eigenvector) tuples from high to low\n>>> eigen_pairs.sort(key=lambda k: k[0], reverse=True) \n```", "```py\n>>> w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n...                eigen_pairs[1][1][:, np.newaxis]))\n>>> print('Matrix W:\\n', w)\nMatrix W:\n[[-0.13724218   0.50303478]\n [ 0.24724326   0.16487119]\n [-0.02545159   0.24456476]\n [ 0.20694508  -0.11352904]\n [-0.15436582   0.28974518]\n [-0.39376952   0.05080104]\n [-0.41735106  -0.02287338]\n [ 0.30572896   0.09048885]\n [-0.30668347   0.00835233]\n [ 0.07554066   0.54977581]\n [-0.32613263  -0.20716433]\n [-0.36861022  -0.24902536]\n [-0.29669651   0.38022942]] \n```", "```py\n>>> X_train_std[0].dot(w)\narray([ 2.38299011,  0.45458499]) \n```", "```py\n>>> X_train_pca = X_train_std.dot(w) \n```", "```py\n>>> colors = ['r', 'b', 'g']\n>>> markers = ['o', 's', '^']\n>>> for l, c, m in zip(np.unique(y_train), colors, markers):\n...     plt.scatter(X_train_pca[y_train==l, 0],\n...                 X_train_pca[y_train==l, 1],\n...                 c=c, label=f'Class {l}', marker=m)\n>>> plt.xlabel('PC 1')\n>>> plt.ylabel('PC 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\nfrom matplotlib.colors import ListedColormap\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n    # setup marker generator and color map\n    markers = ('o', 's', '^', 'v', '<')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    lab = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    lab = lab.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, lab, alpha=0.3, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    # plot class examples\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0],\n                    y=X[y == cl, 1],\n                    alpha=0.8,\n                    c=colors[idx],\n                    marker=markers[idx],\n                    label=f'Class {cl}',\n                    edgecolor='black') \n```", "```py\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.decomposition import PCA\n>>> # initializing the PCA transformer and\n>>> # logistic regression estimator:\n>>> pca = PCA(n_components=2)\n>>> lr = LogisticRegression(multi_class='ovr',\n...                         random_state=1,\n...                         solver='lbfgs')\n>>> # dimensionality reduction:\n>>> X_train_pca = pca.fit_transform(X_train_std)\n>>> X_test_pca = pca.transform(X_test_std)\n>>> # fitting the logistic regression model on the reduced dataset:\n>>> lr.fit(X_train_pca, y_train)\n>>> plot_decision_regions(X_train_pca, y_train, classifier=lr)\n>>> plt.xlabel('PC 1')\n>>> plt.ylabel('PC 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> plot_decision_regions(X_test_pca, y_test, classifier=lr)\n>>> plt.xlabel('PC 1')\n>>> plt.ylabel('PC 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> pca = PCA(n_components=None)\n>>> X_train_pca = pca.fit_transform(X_train_std)\n>>> pca.explained_variance_ratio_\narray([ 0.36951469, 0.18434927, 0.11815159, 0.07334252,\n        0.06422108, 0.05051724, 0.03954654, 0.02643918,\n        0.02389319, 0.01629614, 0.01380021, 0.01172226,\n        0.00820609]) \n```", "```py\n>>> loadings = eigen_vecs * np.sqrt(eigen_vals) \n```", "```py\n>>> fig, ax = plt.subplots()\n>>> ax.bar(range(13), loadings[:, 0], align='center')\n>>> ax.set_ylabel('Loadings for PC 1')\n>>> ax.set_xticks(range(13))\n>>> ax.set_xticklabels(df_wine.columns[1:], rotation=90)\n>>> plt.ylim([-1, 1])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> sklearn_loadings = pca.components_.T * np.sqrt(pca.explained_variance_) \n```", "```py\n>>> fig, ax = plt.subplots()\n>>> ax.bar(range(13), sklearn_loadings[:, 0], align='center')\n>>> ax.set_ylabel('Loadings for PC 1')\n>>> ax.set_xticks(range(13))\n>>> ax.set_xticklabels(df_wine.columns[1:], rotation=90)\n>>> plt.ylim([-1, 1])\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> np.set_printoptions(precision=4)\n>>> mean_vecs = []\n>>> for label in range(1,4):\n...     mean_vecs.append(np.mean(\n...                X_train_std[y_train==label], axis=0))\n...     print(f'MV {label}: {mean_vecs[label - 1]}\\n')\nMV 1: [ 0.9066  -0.3497  0.3201  -0.7189  0.5056  0.8807  0.9589  -0.5516\n0.5416  0.2338  0.5897  0.6563  1.2075]\nMV 2: [-0.8749  -0.2848  -0.3735  0.3157  -0.3848  -0.0433  0.0635  -0.0946\n0.0703  -0.8286  0.3144  0.3608  -0.7253]\nMV 3: [ 0.1992  0.866  0.1682  0.4148  -0.0451  -1.0286  -1.2876  0.8287\n-0.7795  0.9649  -1.209  -1.3622  -0.4013] \n```", "```py\n>>> d = 13 # number of features\n>>> S_W = np.zeros((d, d))\n>>> for label, mv in zip(range(1, 4), mean_vecs):\n...     class_scatter = np.zeros((d, d))\n...     for row in X_train_std[y_train == label]:\n...         row, mv = row.reshape(d, 1), mv.reshape(d, 1)\n...         class_scatter += (row - mv).dot((row - mv).T)\n...     S_W += class_scatter\n>>> print('Within-class scatter matrix: '\n...       f'{S_W.shape[0]}x{S_W.shape[1]}')\nWithin-class scatter matrix: 13x13 \n```", "```py\n>>> print('Class label distribution:',\n...       np.bincount(y_train)[1:])\nClass label distribution: [41 50 33] \n```", "```py\n>>> d = 13 # number of features\n>>> S_W = np.zeros((d, d))\n>>> for label,mv in zip(range(1, 4), mean_vecs):\n...     class_scatter = np.cov(X_train_std[y_train==label].T)\n...     S_W += class_scatter\n>>> print('Scaled within-class scatter matrix: '\n...       f'{S_W.shape[0]}x{S_W.shape[1]}')\nScaled within-class scatter matrix: 13x13 \n```", "```py\n>>> mean_overall = np.mean(X_train_std, axis=0)\n>>> mean_overall = mean_overall.reshape(d, 1)\n>>> d = 13 # number of features\n>>> S_B = np.zeros((d, d))\n>>> for i, mean_vec in enumerate(mean_vecs):\n...     n = X_train_std[y_train == i + 1, :].shape[0]\n...     mean_vec = mean_vec.reshape(d, 1) # make column vector\n...     S_B += n * (mean_vec - mean_overall).dot(\n...     (mean_vec - mean_overall).T)\n>>> print('Between-class scatter matrix: '\n...       f'{S_B.shape[0]}x{S_B.shape[1]}')\nBetween-class scatter matrix: 13x13 \n```", "```py\n>>> eigen_vals, eigen_vecs =\\\n...     np.linalg.eig(np.linalg.inv(S_W).dot(S_B)) \n```", "```py\n>>> eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i])\n...                for i in range(len(eigen_vals))]\n>>> eigen_pairs = sorted(eigen_pairs,\n...               key=lambda k: k[0], reverse=True)\n>>> print('Eigenvalues in descending order:\\n')\n>>> for eigen_val in eigen_pairs:\n...     print(eigen_val[0])\nEigenvalues in descending order:\n349.617808906\n172.76152219\n3.78531345125e-14\n2.11739844822e-14\n1.51646188942e-14\n1.51646188942e-14\n1.35795671405e-14\n1.35795671405e-14\n7.58776037165e-15\n5.90603998447e-15\n5.90603998447e-15\n2.25644197857e-15\n0.0 \n```", "```py\n>>> tot = sum(eigen_vals.real)\n>>> discr = [(i / tot) for i in sorted(eigen_vals.real,\n...                                    reverse=True)]\n>>> cum_discr = np.cumsum(discr)\n>>> plt.bar(range(1, 14), discr, align='center',\n...         label='Individual discriminability')\n>>> plt.step(range(1, 14), cum_discr, where='mid',\n...          label='Cumulative discriminability')\n>>> plt.ylabel('\"Discriminability\" ratio')\n>>> plt.xlabel('Linear Discriminants')\n>>> plt.ylim([-0.1, 1.1])\n>>> plt.legend(loc='best')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n...                eigen_pairs[1][1][:, np.newaxis].real))\n>>> print('Matrix W:\\n', w)\nMatrix W:\n [[-0.1481  -0.4092]\n  [ 0.0908  -0.1577]\n  [-0.0168  -0.3537]\n  [ 0.1484   0.3223]\n  [-0.0163  -0.0817]\n  [ 0.1913   0.0842]\n  [-0.7338   0.2823]\n  [-0.075   -0.0102]\n  [ 0.0018   0.0907]\n  [ 0.294   -0.2152]\n  [-0.0328   0.2747]\n  [-0.3547  -0.0124]\n  [-0.3915  -0.5958]] \n```", "```py\n>>> X_train_lda = X_train_std.dot(w)\n>>> colors = ['r', 'b', 'g']\n>>> markers = ['o', 's', '^']\n>>> for l, c, m in zip(np.unique(y_train), colors, markers):\n...     plt.scatter(X_train_lda[y_train==l, 0],\n...                 X_train_lda[y_train==l, 1] * (-1),\n...                 c=c, label= f'Class {l}', marker=m)\n>>> plt.xlabel('LD 1')\n>>> plt.ylabel('LD 2')\n>>> plt.legend(loc='lower right')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> # the following import statement is one line\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n>>> lda = LDA(n_components=2)\n>>> X_train_lda = lda.fit_transform(X_train_std, y_train) \n```", "```py\n>>> lr = LogisticRegression(multi_class='ovr', random_state=1,\n...                         solver='lbfgs')\n>>> lr = lr.fit(X_train_lda, y_train)\n>>> plot_decision_regions(X_train_lda, y_train, classifier=lr)\n>>> plt.xlabel('LD 1')\n>>> plt.ylabel('LD 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> X_test_lda = lda.transform(X_test_std)\n>>> plot_decision_regions(X_test_lda, y_test, classifier=lr)\n>>> plt.xlabel('LD 1')\n>>> plt.ylabel('LD 2')\n>>> plt.legend(loc='lower left')\n>>> plt.tight_layout()\n>>> plt.show() \n```", "```py\n>>> from sklearn.datasets import load_digits\n>>> digits = load_digits() \n```", "```py\n>>> fig, ax = plt.subplots(1, 4)\n>>> for i in range(4):\n>>>     ax[i].imshow(digits.images[i], cmap='Greys')\n>>> plt.show() \n```", "```py\n>>> digits.data.shape\n(1797, 64) \n```", "```py\n>>> y_digits = digits.target\n>>> X_digits = digits.data \n```", "```py\n>>> from sklearn.manifold import TSNE\n>>> tsne = TSNE(n_components=2, init='pca',\n...             random_state=123)\n>>> X_digits_tsne = tsne.fit_transform(X_digits) \n```", "```py\n>>> import matplotlib.patheffects as PathEffects\n>>> def plot_projection(x, colors):\n...     f = plt.figure(figsize=(8, 8))\n...     ax = plt.subplot(aspect='equal')\n...     for i in range(10):\n...         plt.scatter(x[colors == i, 0],\n...                     x[colors == i, 1])\n...     for i in range(10):\n...         xtext, ytext = np.median(x[colors == i, :], axis=0)\n...         txt = ax.text(xtext, ytext, str(i), fontsize=24)\n...         txt.set_path_effects([\n...             PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n...             PathEffects.Normal()])\n>>> plot_projection(X_digits_tsne, y_digits)\n>>> plt.show() \n```"]