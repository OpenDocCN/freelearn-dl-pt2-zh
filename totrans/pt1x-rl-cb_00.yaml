- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The surge in interest in reinforcement learning is due to the fact that it revolutionizes
    automation by learning the optimal actions to take in an environment in order
    to maximize the notion of cumulative reward.
  prefs: []
  type: TYPE_NORMAL
- en: '*PyTorch 1.x Reinforcement Learning Cookbook* introduces you to important reinforcement
    learning concepts and implementations of algorithms in PyTorch. Each chapter of
    the book walks you through a different type of reinforcement learning method and
    its industry-adopted applications. With the help of recipes that contain real-world
    examples, you will find it intriguing to enhance your knowledge and proficiency
    of reinforcement learning techniques in areas such as dynamic programming, Monte
    Carlo methods, temporal difference and Q-learning, multi-armed bandit, function
    approximation, deep Q-Networks, and policy gradients—they are no more obscure
    than you thought. Interesting and easy-to-follow examples, such as Atari games,
    Blackjack, Gridworld environments, internet advertising, Mountain Car, and Flappy
    Bird, will keep you interested until you reach your goal.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this book, you will have mastered the implementation of popular
    reinforcement learning algorithms and learned the best practices of applying reinforcement
    learning techniques to solve other real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning engineers, data scientists, and AI researchers looking for
    quick solutions to different problems in reinforcement learning will find this
    book useful. Prior exposure to machine learning concepts is required, while previous
    experience with PyTorch will be a bonus.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1, *Getting Started with Reinforcement Learning and PyTorch*, is the
    starting point for readers who are looking forward to beginning this book's step-by-step
    guide to reinforcement learning with PyTorch. We will set up the working environment
    and OpenAI Gym and get familiar with reinforcement learning environments using
    the Atari and CartPole playgrounds. The chapter will also cover the implementation
    of several basic reinforcement learning algorithms, including random search, hill-climbing,
    and policy gradient. At the end, readers will also have a chance to review the
    essentials of PyTorch and get ready for the upcoming learning examples and projects.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2, *Markov Decision Process and Dynamic Programming*, starts with the
    creation of a Markov chain and a Markov Decision Process, which is the core of
    most reinforcement learning algorithms. It will then move on to two approaches
    to solve a Markov Decision Process (MDP), value iteration and policy iteration.
    We will get more familiar with MDP and the Bellman equation by practicing policy
    evaluation. We will also demonstrate how to solve the interesting coin flipping
    gamble problem step by step. At the end, we will learn how to perform dynamic
    programming to scale up the learning.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3, *Monte Carlo Methods for Making Numerical Estimations*, is focused
    on Monte Carlo methods. We will start by estimating the value of pi with Monte
    Carlo. Moving on, we will learn how to use the Monte Carlo method to predict state
    values and state-action values. We will demonstrate training an agent to win at
    Blackjack using Monte Carlo. Also, we will explore on-policy, first-visit Monte
    Carlo control and off-policy Monte Carlo control by developing various algorithms.
    Monte Carlo Control with an epsilon-greedy policy and weighted importance sampling
    will also be covered.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4, *Temporal Difference and Q-Learning*, starts by setting up the CliffWalking
    and Windy Gridworld environment playground, which will be used in temporal difference
    and Q-Learning. Through our step-by-step guide, readers will explore Temporal
    Difference for prediction, and will gain practical experience with Q-Learning
    for off-policy control, and SARSA for on-policy control. We will also work on
    an interesting project, the taxi problem, and demonstrate how to solve it using
    the Q-Learning and SARSA algorithms. Finally, we will cover the Double Q-learning
    algorithm as a bonus section.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 5, *Solving Multi-Armed Bandit Problems*, covers the multi-armed bandit
    algorithm, which is probably one of the most popular algorithms in reinforcement
    learning. This will start with the creation of a multi-armed bandit problem. We
    will see how to solve the multi-armed bandit problem using four strategies, these
    being the epsilon-greedy policy, softmax exploration, the upper confidence bound
    algorithm, and the Thompson sampling algorithm. We will also work on a billion-dollar
    problem, online advertising, and demonstrate how to solve it using the multi-armed
    bandit algorithm. Finally, we will develop a more complex algorithm, the contextual
    bandit algorithm, and use it to optimize display advertising.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 6](6371b431-5738-4267-966d-eb3be840d471.xhtml), *Scaling Up Learning
    with Function Approximation*, is focused on function approximation and will start
    with setting up the Mountain Car environment playground. Through our step-by-step
    guide, we will cover the motivation for function approximation over Table Lookup,
    and gain experience in incorporating function approximation into existing algorithms
    such as Q-Learning and SARSA. We will also cover an advanced technique, batching
    using experience replay. Finally, we will cover how to solve the CartPole problem
    using what we have learned in the chapter as a whole.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 7, *Deep Q-Networks in Action*, covers Deep Q-Learning, or **Deep Q
    Network** (**DQN**), which is considered the most modern reinforcement learning
    technique. We will develop a DQN model step by step and understand the importance
    of Experience Replay and a target network in making Deep Q-Learning work in practice.
    To help readers solve Atari games, we will demonstrate how to incorporate convolutional
    neural networks into DQNs. We will also cover two DQN variants, Double DQNs and
    Dueling DQNs. We will cover how to fine-tune a Q-Learning algorithm using Double
    DQNs as an example.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8, *Implementing Policy Gradients and Policy Optimization*, focuses
    on policy gradients and optimization and starts by implementing the REINFORCE
    algorithm. We will then develop the REINFORCE algorithm with the baseline for
    CliffWalking. We will also implement the actor-critic algorithm and apply it to
    solve the CliffWalking problem. To scale up the deterministic policy gradient
    algorithm, we apply tricks from DQN and develop the Deep Deterministic Policy
    Gradients. As a bit of fun, we train an agent based on the cross-entropy method
    to play the CartPole game. Finally, we will talk about how to scale up policy
    gradient methods using the asynchronous actor-critic method and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9, *Capstone Project – Playing Flappy Bird with DQN*, takes us through
    a capstone project – playing Flappy Bird using reinforcement learning. We will
    apply what we have learned throughout this book to build an intelligent bot. We
    will focus on building a DQN, fine-tuning model parameters, and deploying the
    model. Let's see how long the bird can fly in the air.
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data scientists, machine learning engineers, and AI researchers looking for
    quick solutions to different problems in reinforcement learning will find this
    book useful. Prior exposure to machine learning concepts is required, while previous
    experience with PyTorch is not required but will be a bonus.
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from your account at [www.packt.com](http://www.packt.com).
    If you purchased this book elsewhere, you can visit [www.packtpub.com/support](https://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code files by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in or register at [www.packt.com](http://www.packt.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Support tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Code Downloads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the book in the Search box and follow the onscreen instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  prefs: []
  type: TYPE_NORMAL
- en: WinRAR/7-Zip for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipeg/iZip/UnRarX for Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7-Zip/PeaZip for Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/PyTorch-1.x-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/PyTorch-1.x-Reinforcement-Learning-Cookbook).
    In case there's an update to the code, it will be updated on the existing GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at **[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**.
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781838551964_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781838551964_ColorImages.pdf)[.](http://www.packtpub.com/sites/default/files/downloads/Bookname_ColorImages.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    Here is an example: "By saying `empty`, it doesn''t mean all elements have a value
    of `Null`."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For example, words in menus or dialog boxes appear in the text like this. Here
    is an example: "This approach is called **random search**, since the weight is
    randomly picked in each trial with the hope that the best weight will be found
    with a large number of trials."'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Sections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, you will find several headings that appear frequently (*Getting
    ready*, *How to do it...*, *How it works...*, *There's more...*, and *See also*).
  prefs: []
  type: TYPE_NORMAL
- en: 'To give clear instructions on how to complete a recipe, use these sections
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section tells you what to expect in the recipe and describes how to set
    up any software or any preliminary settings required for the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section contains the steps required to follow the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section usually consists of a detailed explanation of what happened in
    the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section consists of additional information about the recipe in order to
    make you more knowledgeable about the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section provides helpful links to other useful information for the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email us at `customercare@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](https://www.packtpub.com/support/errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packt.com` with a link
    to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Packt, please visit [packt.com](http://www.packt.com/).
  prefs: []
  type: TYPE_NORMAL
