- en: '*Chapter 1*: From Bag-of-Words to the Transformer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*：从词袋模型到Transformer'
- en: In this chapter, we will discuss what has changed in **Natural Language Processing**
    (**NLP**) over two decades. We experienced different paradigms and finally entered
    the era of Transformer architectures. All the paradigms help us to gain a better
    representation of words and documents for problem-solving. Distributional semantics
    describes the meaning of a word or a document with vectorial representation, looking
    at distributional evidence in a collection of articles. Vectors are used to solve
    many problems in both supervised and unsupervised pipelines. For language-generation
    problems, n-gram language models have been leveraged as a traditional approach
    for years. However, these traditional approaches have many weaknesses that we
    will discuss throughout the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在过去的20年中**自然语言处理**（**NLP**）发生了什么变化。我们经历了不同的范式，最终进入了Transformer架构的时代。所有这些范式都帮助我们更好地表示单词和文档以解决问题。分布语义描述了单词或文档的意义，具有矢量表示，观察在文集中的分布证据。矢量用于在受控和非受控流程中解决许多问题。对于语言生成问题，n-gram语言模型长期以来一直被用作传统方法。然而，这些传统方法存在许多缺点，在整整一章中我们将进行讨论。
- en: We will further discuss classical **Deep Learning** (**DL**) architectures such
    as **Recurrent Neural Networks** (**RNNs**), **Feed-Forward Neural Networks**
    (**FFNNs**), and **Convolutional Neural Networks** (**CNNs**). These have improved
    the performance of the problems in the field and have overcome the limitation
    of traditional approaches. However, these models have had their own problems too.
    Recently, Transformer models have gained immense interest because of their effectiveness
    in all NLP tasks, from text classification to text generation. However, the main
    success has been that Transformers effectively improve the performance of multilingual
    and multi-task NLP problems, as well as monolingual and single tasks. These contributions
    have made **Transfer Learning** (**TL**) more possible in NLP, which aims to make
    models reusable for different tasks or different languages.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进一步讨论经典的**深度学习**（**DL**）架构，如**循环神经网络**（**RNNs**），**前馈神经网络**（**FFNNs**）和**卷积神经网络**（**CNNs**）。这些架构已经改善了该领域问题的性能，并克服了传统方法的局限。然而，这些模型也存在各自的问题。最近，由于Transformer模型在从文本分类到文本生成的所有NLP任务中的有效性，它们引起了巨大的兴趣。然而，主要的成功在于Transformer有效地提高了多语言和多任务NLP问题的性能，以及单语言和单任务。这些贡献使得**迁移学习**(**TL**)在NLP中更为可能，其目标是使模型可在不同任务或不同语言中重复使用。
- en: Starting with the attention mechanism, we will briefly discuss the Transformer
    architecture and the differences between previous NLP models. In parallel with
    theoretical discussions, we will show practical examples with the popular NLP
    framework. For the sake of simplicity, we will choose introductory code examples
    that are as short as possible.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从注意机制开始，我们将简要讨论Transformer架构和之前NLP模型的区别。与理论讨论并行，我们将展示流行的NLP框架的实际示例。为简单起见，我们将选择尽可能简短的入门代码示例。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Evolution of NLP toward Transformers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP向Transformer的演变
- en: Understanding distributional semantics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布语义
- en: Leveraging DL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用深度学习
- en: Overview of the Transformer architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer架构概述
- en: Using TL with Transformers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Transformer进行迁移学习
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using Jupyter Notebook to run our coding exercises that require
    `python >=3.6.0`, along with the following packages that need to be installed
    with the `pip install` command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Jupyter Notebook来运行需要安装`python >=3.6.0`及以下包的编码练习：
- en: '`sklearn`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`'
- en: '`nltk==3.5.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk==3.5.0`'
- en: '`gensim==3.8.3`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gensim==3.8.3`'
- en: '`fasttext`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fasttext`'
- en: '`keras>=2.3.0`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras>=2.3.0`'
- en: '`Transformers >=4.00`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transformers >=4.00`'
- en: 'All notebooks with coding exercises are available at the following GitHub link:
    [https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有带有编码练习的笔记本都可在以下GitHub链接处找到：[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01)。
- en: 'Check out the following link to see Code in Action Video: [https://bit.ly/2UFPuVd](https://bit.ly/2UFPuVd)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看视频代码示例：[https://bit.ly/2UFPuVd](https://bit.ly/2UFPuVd)
- en: Evolution of NLP toward Transformers
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen profound changes in NLP over the last 20 years. During this period,
    we experienced different paradigms and finally entered a new era dominated mostly
    by magical *Transformer* architecture. This architecture did not come out of nowhere.
    Starting with the help of various neural-based NLP approaches, it gradually evolved
    to an attention-based encoder-decoder type architecture and still keeps evolving.
    The architecture and its variants have been successful thanks to the following
    developments in the last decade:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Contextual word embeddings
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better subword tokenization algorithms for handling unseen words or rare words
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Injecting additional memory tokens into sentences, such as `Paragraph ID` in
    `Doc2vec` or a **Classification** (**CLS**) token in **Bidirectional Encoder Representations
    from Transformers** (**BERT**)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention mechanisms, which overcome the problem of forcing input sentences
    to encode all information into one context vector
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-head self-attention
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional encoding to case word order
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizable architectures that make for faster training and fine-tuning
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model compression (distillation, quantization, and so on)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TL (cross-lingual, multitask learning)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For many years, we used traditional NLP approaches such as *n-gram language
    models*, *TF-IDF-based information retrieval models*, and *one-hot encoded document-term
    matrices*. All these approaches have contributed a lot to the solution of many
    NLP problems such as *sequence classification*, *language generation*, *language
    understanding*, and so forth. On the other hand, these traditional NLP methods
    have their own weaknesses—for instance, falling short in solving the problems
    of sparsity, unseen words representation, tracking long-term dependencies, and
    others. In order to cope with these weaknesses, we developed DL-based approaches
    such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FFNNs
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several variants of RNNs, CNNs, and FFNNs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 2013, as a two-layer FFNN word-encoder model, `Word2vec`, sorted out the
    dimensionality problem by producing short and dense representations of the words,
    called **word embeddings**. This early model managed to produce fast and efficient
    static word embeddings. It transformed unsupervised textual data into supervised
    data (*self-supervised learning*) by either predicting the target word using context
    or predicting neighbor words based on a sliding window. **GloVe**, another widely
    used and popular model, argued that count-based models can be better than neural
    models. It leverages both global and local statistics of a corpus to learn embeddings
    based on word-word co-occurrence statistics. It performed well on some syntactic
    and semantic tasks, as shown in the following screenshot. The screenshot tells
    us that the embeddings offsets between the terms help to apply vector-oriented
    reasoning. We can learn the generalization of gender relations, which is a semantic
    relation from the offset between *man* and *woman* (*man-> woman*). Then, we can
    arithmetically estimate the vector of *actress* by adding the vector of the term
    *actor* and the offset calculated before. Likewise, we can learn syntactic relations
    such as word plural forms. For instance, if the vectors of **Actor**, **Actors**,
    and **Actress** are given, we can estimate the vector of **Actresses**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，作为两层FFNN单词编码器模型，`Word2vec`通过产生短而稠密的单词表示（称为**词嵌入**）解决了维度问题。这个早期模型成功地产生了快速而有效的静态词嵌入。它通过预测上下文中的目标单词或基于滑动窗口预测相邻单词，将无监督的文本数据转换为受监督的数据（*自监督学习*）。**GloVe**，另一个被广泛使用和普遍流行的模型，认为基于计数的模型可能比神经模型更好。它利用语料库的全局和局部统计数据来学习基于单词共现统计的嵌入。它在一些句法和语义任务上表现良好，如下面的截图所示。截图告诉我们，术语之间的嵌入偏移有助于应用矢量导向推理。我们可以学习性别关系的泛化，这是从*男人*和*女人*之间的偏移关系推导出来的语义关系（*男人->
    女人*）。然后，我们可以通过将*男演员*的矢量和之前计算出的偏移矢量相加来算出*女演员*的矢量。同样，我们可以学习词的复数形式。例如，如果给出**Actor**，**Actors**和**Actress**的矢量，我们可以估算**Actresses**的矢量：
- en: '![Figure 1.1 – Word embeddings offset for relation extraction ](img/B17123_01_01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 - 用于关系提取的单词嵌入偏移](img/B17123_01_01.jpg)'
- en: Figure 1.1 – Word embeddings offset for relation extraction
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 - 用于关系提取的单词嵌入偏移
- en: The recurrent and convolutional architectures such as RNN, **Long Short-Term
    Memory** (**LSTM**), and CNN started to be used as encoders and decoders in **sequence-to-sequence**
    (**seq2seq**) problems. The main challenge with these early models was polysemous
    words. The senses of the words are ignored since a single fixed representation
    is assigned to each word, which is especially a severe problem for polysemous
    words and sentence semantics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 递归和卷积架构，比如RNN、**长短期记忆**（**LSTM**）和CNN，开始在**序列到序列**（**seq2seq**）问题中被用作编码器和解码器。这些早期模型的主要挑战是多义词。由于给每个单词分配了单一的固定表示，因此忽略了单词的含义，这对多义词和句子语义尤其是一个严重的问题。
- en: The further pioneer neural network models such as **Universal Language Model
    Fine-tuning** (**ULMFit**) and **Embeddings from Language Models** (**ELMo**)
    managed to encode the sentence-level information and finally alleviate polysemy
    problems, unlike with static word embeddings. These two important approaches were
    based on LSTM networks. They also introduced the concept of pre-training and fine-tuning.
    They help us to apply TL, employing the pre-trained models trained on a general
    task with huge textual datasets. Then, we can easily perform fine-tuning by resuming
    training of the pre-trained network on a target task with supervision. The representations
    differ from traditional word embeddings such that each word representation is
    a function of the entire input sentence. The modern Transformer architecture took
    advantage of this idea.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的先驱神经网络模型，比如**通用语言模型微调**（**ULMFit**）和**语言模型嵌入**（**ELMo**），成功地对句子级信息进行编码，并最终缓解了一词多义的问题，与静态词嵌入不同。这两种重要方法基于LSTM网络。它们还引入了预训练和微调的概念。它们帮助我们应用迁移学习，利用在大量文本数据集上进行常规任务训练的预训练模型。然后，我们可以很容易地通过在目标任务上继续对预训练网络进行训练，进行微调。这些表示与传统的词嵌入不同，每个单词表示是整个输入句子的函数。现代Transformer架构充分利用了这个想法。
- en: 'In the meantime, the idea of an attention mechanism made a strong impression
    in the NLP field and achieved significant success, especially in seq2seq problems.
    Earlier methods would pass the last state (known as a `Government of Canada` in
    the input sentence for an English to Turkish translation task. In the output sentence,
    the `Kanada Hükümeti` token makes strong connections with the input phrase and
    establishes a weaker connection with the remaining words in the input, as illustrated
    in the following screenshot:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，注意力机制的想法在自然语言处理领域引起了强烈的印象，并取得了显著的成功，特别是在 seq2seq 问题上。早期的方法会传递最后一个状态（称为输入句子中的`加拿大政府`，用于英语到土耳其语的翻译任务。在输出句子中，`Kanada
    Hükümeti` 标记与输入短语建立了强连接，并与输入中的其他单词建立了较弱的连接，如下方截图所示：
- en: '![Figure 1.2 – Sketchy visualization of an attention mechanism ](img/B17123_01_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 注意力机制的草图可视化](img/B17123_01_02.jpg)'
- en: Figure 1.2 – Sketchy visualization of an attention mechanism
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 注意力机制的草图可视化
- en: So, this mechanism makes models more successful in seq2seq problems such as
    translation, question answering, and text summarization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种机制使得模型在翻译、问答和文本摘要等 seq2seq 问题中更加成功。
- en: In 2017, the Transformer-based encoder-decoder model was proposed and found
    to be successful. The design is based on an FFNN by discarding RNN recurrency
    and using only attention mechanisms (*Vaswani et al., All you need is attention,
    2017*). The Transformer-based models have so far overcome many difficulties that
    other approaches faced and have become a new paradigm. Throughout this book, you
    will be exploring and understanding how the Transformer-based models work.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，基于 Transformer 的编码器-解码器模型被提出并被发现成功。该设计基于 FFNN，丢弃了 RNN 的递归，并仅使用注意力机制（*Vaswani
    et al., All you need is attention, 2017*）。到目前为止，基于 Transformer 的模型已经克服了其他方法所面临的许多困难，并成为了一个新的范式。在本书中，你将探索并理解
    Transformer 模型的工作原理。
- en: Understanding distributional semantics
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布语义
- en: Distributional semantics describes the meaning of a word with a vectorial representation,
    preferably looking at its distributional evidence rather than looking at its predefined
    dictionary definitions. The theory suggests that words co-occurring together in
    a similar environment tend to share similar meanings. This was first formulated
    by the scholar Harris (*Distributional Structure Word, 1954*). For example, similar
    words such as *dog* and *cat* mostly co-occur in the same context. One of the
    advantages of a distributional approach is to help the researchers to understand
    and monitor the semantic evolution of words across time and domains, also known
    as the **lexical semantic change problem**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 分布语义描述了单词的含义，并通过矢量表示，最好是查看其分布证据，而不是查看其预定义的词典定义。该理论表明，在相似环境中共同出现的单词倾向于共享相似的含义。这最早由学者哈里斯提出（*Distributional
    Structure Word, 1954*）。例如，诸如*狗*和*猫*这样的相似单词大多在相同的上下文中共同出现。分布式方法的一个优点是帮助研究人员理解和监测单词随时间和领域的语义演变，也被称为**词汇语义变化问题**。
- en: Traditional approaches have applied **Bag-of-Words** (**BoW**) and n-gram language
    models to build the representation of words and sentences for many years. In a
    BoW approach, words and documents are represented with a one-hot encoding as a
    sparse way of representation, also known as the **Vector Space Model** (**VSM**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，传统方法一直应用**词袋模型**（**BoW**）和 n-gram 语言模型来构建单词和句子的表示。在词袋模型中，单词和文档以一种稀疏的方式表示为
    one-hot 编码，也被称为**向量空间模型**（**VSM**）。
- en: Text classification, word similarity, semantic relation extraction, word-sense
    disambiguation, and many other NLP problems have been solved by these one-hot
    encoding techniques for years. On the other hand, n-gram language models assign
    probabilities to sequences of words so that we can either compute the probability
    that a sequence belongs to a corpus or generate a random sequence based on a given
    corpus.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，这些 one-hot 编码技术已解决了文本分类、单词相似度、语义关系提取、单词意义消歧等许多自然语言处理问题。另一方面，n-gram 语言模型为单词序列分配概率，以便我们可以计算一个序列属于语料库的概率，或者基于给定语料库生成一个随机序列。
- en: BoW implementation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BoW 实现
- en: 'A BoW is a representation technique for documents by counting the words in
    them. The main data structure of the technique is a document-term matrix. Let''s
    see a simple implementation of BoW with Python. The following piece of code illustrates
    how to build a document-term matrix with the Python `sklearn` library for a toy
    corpus of three sentences:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一种词袋（BoW）是一种文档的表征技术，通过计算其中的单词来实现。该技术的主要数据结构是文档词项矩阵。让我们用 Python 看一个 BoW 的简单实现。以下代码片段说明了如何使用
    Python 的 `sklearn` 库为一个三句话的玩具语料库构建文档词项矩阵：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the code is a document-term matrix, as shown in the following
    screenshot. The size is (3 x 10), but in a realistic scenario the matrix size
    can grow to much larger numbers such as 10K x 10M:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出是一个文档词项矩阵，如下图所示。其大小为 (3 x 10)，但在现实场景中，矩阵的大小可以增长到更大的数字，例如 10K x 10M：
- en: '![Figure 1.3 – Document-term matrix ](img/B17123_01_03.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 文档词项矩阵](img/B17123_01_03.jpg)'
- en: Figure 1.3 – Document-term matrix
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 文档词项矩阵
- en: The table indicates a count-based mathematical matrix where the cell values
    are transformed by a **Term Frequency-Inverse Document Frequency** (**TF-IDF**)
    weighting schema. This approach does not care about the position of words. Since
    the word order strongly determines the meaning, ignoring it leads to a loss of
    meaning. This is a common problem in a BoW method, which is finally solved by
    a recursion mechanism in RNN and positional encoding in Transformers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表格表示的是一种基于计数的数学矩阵，在其中单元格的值按照**词频-逆文档频率**（**TF-IDF**）加权模式进行转换。此方法不关心单词的位置。由于单词顺序强烈决定了含义，忽略它会导致意义丧失。这是
    BoW 方法中的常见问题，最终通过 RNN 中的递归机制和变压器中的位置编码得到解决。
- en: Each column in the matrix stands for the vector of a word in the vocabulary,
    and each row stands for the vector of a document. Semantic similarity metrics
    can be applied to compute the similarity or dissimilarity of the words as well
    as documents. Most of the time, we use bigrams such as `cat_sat` and `the_street`
    to enrich the document representation. For instance, as the parameter `ngram_range=(1,2)`
    is passed to `TfidfVectorizer`, it builds a vector space containing both unigrams
    (`big, cat, dog`) and bigrams (`big_cat`, `big_dog`). Thus, such models are also
    called **bag-of-n-grams**, which is a natural extension of **BoW**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每一列代表词汇表中一个词的向量，每一行代表一个文档的向量。可以应用语义相似性指标来计算单词和文档的相似性或非相似性。大多数情况下，我们使用二元组，例如
    `cat_sat` 和 `the_street` 来丰富文档的表示。例如，当参数 `ngram_range=(1,2)` 传递给 `TfidfVectorizer`
    时，它构建一个包含 unigrams (`big, cat, dog`) 和 bigrams (`big_cat`, `big_dog`) 的向量空间。因此，这样的模型也被称为**词袋式
    n-grams**，它是**BoW**的自然扩展。
- en: If a word is commonly used in each document, it can be considered to be high-frequency,
    such as *and* *the*. Conversely, some words hardly appear in documents, called
    low-frequency (or rare) words. As high-frequency and low-frequency words may prevent
    the model from working properly, TF-IDF, which is one of the most important and
    well-known weighting mechanisms, is used here as a solution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词在每篇文章中都经常出现，那么它可以被视为高频词，例如 *and* 和 *the*。相反，一些词在文章中很少出现，称为低频（或稀有）词。由于高频和低频词可能会妨碍模型的正常工作，因此在这里使用了TF-IDF作为解决方案，这是最重要和著名的加权机制之一。
- en: '`the` has no discriminative power, `chased` can be highly informative and give
    clues about the subject of the text. This is because high-frequency words (stopwords,
    functional words) have little discriminating power in understanding the documents.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`the` 没有区分力，而 `chased` 可能具有高信息量，可以提供关于文本主题的线索。这是因为高频词（停用词，功能词）在理解文档时具有很少的区分能力。'
- en: The discriminativeness of the terms also depends on the domain—for instance,
    a list of DL articles is most likely to have the word `network` in almost every
    document. IDF can scale down the weights of all terms by using their **Document
    Frequency** (**DF**), where the DF of a word is computed by the number of documents
    in which a term appears. **Term Frequency** (**TF**) is the raw count of a term
    (word) in a document.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 词的区分度也取决于领域，例如，DL 文章列表中几乎每篇文章都可能有单词 `network`。IDF 可以通过使用单词的**文档频率**（**DF**）来缩小所有词的权值，其中单词的
    DF 通过单词出现在的文档数计算得出。**词频**（**TF**）是文档中词（术语）的原始计数。
- en: 'Some of the advantages and disadvantages of a TF-IDF based BoW model are listed
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于 TF-IDF 的 BoW 模型的一些优缺点列举如下：
- en: '![Table 1 – Advantages and disadvantages of a TF-IDF BoW model ](img/B17123_01_Table1.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![表格 1 – TF-IDF BoW 模型的优缺点](img/B17123_01_Table1.jpg)'
- en: Table 1 – Advantages and disadvantages of a TF-IDF BoW model
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Overcoming the dimensionality problem
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To overcome the dimensionality problem of the BoW model, **Latent Semantic Analysis**
    (**LSA**) is widely used for capturing semantics in a low-dimensional space. It
    is a linear method that captures pairwise correlations between terms. LSA-based
    probabilistic methods can be still considered as a single layer of hidden topic
    variables. However, current DL models include multiple hidden layers, with billions
    of parameters. In addition to that, Transformer-based models showed that they
    can discover latent representations much better than such traditional models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: For the **Natural Language Understanding** (**NLU**) tasks, the traditional
    pipeline starts with some preparation steps, such as *tokenization*, *stemming*,
    *noun phrase detection*, *chunking*, *stop-word elimination*, and much more. Afterward,
    a document-term matrix is constructed with any weighting schema, where TF-IDF
    is the most popular one. Finally, the matrix is served as a tabulated input for
    **Machine Learning** (**ML**) pipelines, sentiment analysis, document similarity,
    document clustering, or measuring the relevancy score between a query and a document.
    Likewise, terms are represented as a tabular matrix and can be input for a token
    classification problem where we can apply named-entity recognition, semantic relation
    extractions, and so on.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'The classification phase includes a straightforward implementation of supervised
    ML algorithms such as **Support Vector Machine** (**SVM**), Random forest, logistic,
    naive bayes, and Multiple Learners (Boosting or Bagging). Practically, the implementation
    of such a pipeline can simply be coded as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As seen in the preceding code, we can apply fit operations easily thanks to
    the `sklearn` **Application Programming Interface** (**API**). In order to apply
    the learned model to train data, the following code is executed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's move on to the next section!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling and generation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For language-generation problems, the traditional approaches are based on leveraging
    n-gram language models. This is also called a **Markov process**, which is a stochastic
    model in which each word (event) depends on a subset of previous words—*unigram*,
    *bigram*, or *n-gram*, outlined as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '**Unigram** (all words are independent and no chain): This estimates the probability
    of word in a vocabulary simply computed by the frequency of it to the total word
    count.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bigram** (First-order Markov process): This estimates the *P (word*i*| wordi*-1*)*.probability
    of *wordi* depending on *wordi-1*, which is simply computed by the ratio of *P
    (word*i *, wordi*-1*)* to *P (wordi*-1*)*.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ngram** (N-order Markov process): This estimates *P (wordi | word0, ...,
    wordi-1)*.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s give a simple language model implementation with the **Natural Language
    Toolkit** (**NLTK**) library. In the following implementation, we train a **Maximum
    Likelihood Estimator** (**MLE**) with order *n=2*. We can select any n-gram order
    such as *n=1* for unigrams, *n=2* for bigrams, *n=3* for trigrams, and so forth:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用**自然语言工具包**（**NLTK**）库进行一个简单的语言模型实现。在以下实现中，我们使用**最大似然估计器**（**MLE**）训练了一个*n=2*的模型。我们可以选择任何n-gram顺序，比如*n=1*代表unigrams，*n=2*代表bigrams，*n=3*代表trigrams等等：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `nltk` package first downloads the `gutenberg` corpus, which includes some
    texts from the *Project Gutenberg* electronic text archive, hosted at [https://www.gutenberg.org](https://www.gutenberg.org).
    It also downloads the `punkt` tokenizer tool for the punctuation process. This
    tokenizer divides a raw text into a list of sentences by using an unsupervised
    algorithm. The `nltk` package already includes a pre-trained English `punkt` tokenizer
    model for abbreviation words and collocations. It can be trained on a list of
    texts in any language before use. In the further chapters, we will discuss how
    to train different and more efficient tokenizers for Transformer models as well.
    The following code produces what the language model learned so far:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk`包首先下载`古腾堡`语料库，其中包括来自*古腾堡项目*电子文本存档的一些文本，托管在[https://www.gutenberg.org](https://www.gutenberg.org)。它还下载用于标点处理的`punkt`分词器工具。该分词器使用无监督算法将原始文本划分为句子列表。`nltk`包已经包含了一个预先训练的英文`punkt`分词器模型，用于缩写词和搭配词。在使用之前，可以对任何语言的一系列文本进行训练。在后续章节中，我们将讨论如何为Transformer模型训练不同和更高效的分词器。以下代码展示了语言模型目前学到的内容：'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is the output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The n-gram language model keeps *n-gram* counts and computes the conditional
    probability for sentence generation. `lm=MLE(2)` stands for MLE, which yields
    the maximum probable sentence from each token probability. The following code
    produces a random sentence of 10 words with the `<s>` starting condition given:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram语言模型保留*n-gram*计数，并计算生成句子的条件概率。`lm=MLE(2)`代表最大似然估计，在每个令牌的概率中得出最可能的句子。以下代码使用`<s>`开头条件生成一个包含10个单词的随机句子：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is shown in the following snippet:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can give a specific starting condition through the `text_seed` parameter,
    which makes the generation be conditioned on the preceding context. In our preceding
    example, the preceding context is `<s>`, which is a special token indicating the
    beginning of a sentence.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`text_seed`参数提供特定的起始条件，使得生成受到前文的影响。在我们先前的例子中，前文是`<s>`，这是一个特殊的令牌，表示句子的开头。
- en: So far, we have discussed paradigms underlying traditional NLP models and provided
    very simple implementations with popular frameworks. We are now moving to the
    DL section to discuss how neural language models shaped the field of NLP and how
    neural models overcome the traditional model limitations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了传统NLP模型的基础范式，并用流行的框架提供了非常简单的实现。现在我们将转向DL部分，讨论神经语言模型如何塑造NLP领域，以及神经模型如何克服传统模型的局限性。
- en: Leveraging DL
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用DL
- en: NLP is one of the areas where DL architectures have been widely and successfully
    used. For decades, we have witnessed successful architectures, especially in word
    and sentence representation. In this section, we will share the story of these
    different approaches with commonly used frameworks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是DL架构广泛且成功应用的领域之一。几十年来，在词和句子表示中特别出现了成功的架构。在本节中，我们将分享这些不同方法的故事，并介绍常用的框架。
- en: Learning word embeddings
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习词嵌入
- en: Neural network-based language models effectively solved feature representation
    and language modeling problems since it became possible to train more complex
    neural architecture on much larger datasets to build short and dense representations.
    In 2013, the **Word2vec model**, which is a popular word-embedding technique,
    used a simple and effective architecture to learn a high quality of continuous
    word representations. It outperformed other models for a variety of syntactic
    and semantic language tasks such as *sentiment analysis*, *paraphrase detection*,
    *relation extraction*, and so forth. The other key factor in the popularity of
    the model is its much *lower computational complexity*. It maximizes the probability
    of the current word given any surrounding context words, or vice versa.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'The following piece of code illustrates how to train word vectors for the sentences
    of the play *Macbeth*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code trains the word embeddings with a vector size of 100 by a sliding
    5-length context window. To visualize the words embeddings, we need to reduce
    the dimension to 3 by applying **Principal Component Analysis** (**PCA**) as shown
    in the following code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is the output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Visualizing word embeddings with PCA ](img/B17123_01_004.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Visualizing word embeddings with PCA
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'As the plot shows, the main characters of Shakespeare''s play—**Macbeth**,
    **Malcolm**, **Banquo**, **Macduff**, and others—are mapped close to each other.
    Likewise, auxiliary verbs **shall**, **should**, and **would** appear close to
    each other at the left-bottom of *Figure 1.4*. We can also capture an analogy
    such as *man-woman= uncle-aunt* by using an embedding offset. For more interesting
    visual examples on this topic, please check the following project: [https://projector.tensorflow.org/](https://projector.tensorflow.org/).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The Word2vec-like models learn word embeddings by employing a prediction-based
    neural architecture. They employ gradient descent on some objective functions
    and nearby word predictions. While traditional approaches apply a count-based
    method, neural models design a prediction-based architecture for distributional
    semantics. *Are count-based methods or prediction-based methods the best for distributional
    word representations?* The GloVe approach addressed this problem and argued that
    these two approaches are not dramatically different. Jeffrey Penington et al.
    even supported the idea that the count-based methods could be more successful
    by capturing global statistics. They stated that GloVe outperformed other neural
    network language models on word analogy, word similarity, and **Named Entity Recognition**
    (**NER**) tasks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: These two paradigms, however, did not provide a helpful solution for unseen
    words and word-sense problems. They do not exploit subword information, and therefore
    cannot learn the embeddings of rare and unseen words.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '**FastText**, another widely used model, proposed a new enriched approach using
    subword information, where each word is represented as a bag of character n-grams.
    The model sets a constant vector to each character n-gram and represents words
    as the sum of their sub-vectors, which is an idea that was first introduced by
    Hinrich Schütze (*Word Space, 1993*). The model can compute word representations
    even for unseen words and learn the internal structure of words such as suffixes/affixes,
    which is especially important with morphologically rich languages such as Finnish,
    Hungarian, Turkish, Mongolian, Korean, Japanese, Indonesian, and so forth. Currently,
    modern Transformer architectures use a variety of subword tokenization methods
    such as **WordPiece**, **SentencePiece**, or **Byte-Pair Encoding** (**BPE**).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastText**，另一个广泛使用的模型，提出了一种使用子词信息的新的丰富方法，其中每个单词被表示为一组字符n-gram。该模型为每个字符n-gram设置一个常量向量，并将单词表示为其子向量的和，这是Hinrich
    Schütze首次引入的一种想法（*Word Space, 1993*）。模型可以计算即使对于未见过的单词也可以学习单词的内部结构，例如后缀/词缀，这在形态丰富的语言（如芬兰语、匈牙利语、土耳其语、蒙古语、韩语、日语、印尼语等）中尤为重要。当前，现代Transformer架构使用各种子词标记化方法，例如**WordPiece**，**SentencePiece**或**字节对编码**（**BPE**）。'
- en: A brief overview of RNNs
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN的简要概述
- en: 'RNN models can learn each token representation by rolling up the information
    of other tokens at an earlier timestep and learn sentence representation at the
    last timestep. This mechanism has been found beneficial in many ways, outlined
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型可以通过在较早的时间步中滚动其他标记的信息来学习每个标记表示，并在最后一个时间步学习句子表示。这种机制在许多方面都被发现有益，概述如下：
- en: Firstly, RNN can be redesigned in a one-to-many model for language generation
    or music generation.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，RNN 可以在语言生成或音乐生成的一对多模型中进行重新设计。
- en: Secondly, many-to-one models can be used for text classification or sentiment
    analysis.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，多对一模型可用于文本分类或情感分析。
- en: And lastly, many-to-many models are used for NER problems. The second use of
    many-to-many models is to solve encoder-decoder problems such as *machine translation*,
    *question answering*, and *text summarization*.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，多对多模型用于NER问题。多对多模型的第二个用途是解决编码器-解码器问题，例如*机器翻译*，*问答*和*文本摘要*。
- en: As with other neural network models, RNN models take tokens produced by a tokenization
    algorithm that breaks down the entire raw text into atomic units also called tokens.
    Further, it associates the token units with numeric vectors—token embeddings—which
    are learned during the training. As an alternative, we can assign the embedded
    learning task to the well-known word-embedding algorithms such as Word2vec or
    FastText in advance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他神经网络模型一样，RNN模型接受通过令牌化算法生成的标记，该算法将整个原始文本分解为原子单位，也称为标记。此外，它将标记单元与数字向量（标记嵌入）相关联，这些向量在训练期间学习。作为替代方案，我们可以事先将嵌入式学习任务分配给著名的单词嵌入算法，例如Word2vec或FastText。
- en: Here is a simple example of an RNN architecture for the sentence `The cat is
    sad.`, where x0 is the vector embeddings of `the`, x1 is the vector embeddings
    of `cat`, and so forth. *Figure 1.5* illustrates an RNN being unfolded into a
    full **Deep Neural Network** (**DNN**).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是句子`The cat is sad.`的RNN架构的简单示例，其中x0是`the`的向量嵌入，x1是`cat`的向量嵌入，依此类推。*图1.5*说明了一个被展开成完整**深度神经网络**（**DNN**）的RNN。
- en: '`The cat is sad.` sequence, we take care of a sequence of five words. The hidden
    state in each layer acts as the memory of the network. It encodes information
    about what happened in all previous timesteps and in the current timestep. This
    is represented in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`The cat is sad.`序列，我们关心一个包含五个词的序列。每一层的隐藏状态充当网络的记忆。它编码了所有先前时间步和当前时间步发生了什么的信息。这在下图中表示：'
- en: '![Figure 1.5 – An RNN architecture ](img/B17123_01_05.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 - 一个RNN架构](img/B17123_01_05.jpg)'
- en: Figure 1.5 – An RNN architecture
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 - 一个RNN架构
- en: 'The following are some advantages of an RNN architecture:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是RNN架构的一些优势：
- en: '**Variable-length input**: The capacity to work on variable-length input, no
    matter the size of the sentence being input. We can feed the network with sentences
    of 3 or 300 words without changing the parameter.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变长度输入**：具有处理可变长度输入的能力，无论输入的句子大小如何。我们可以将3个或300个词的句子喂给网络而不更改参数。'
- en: '**Caring about word order**: It processes the sequence word by word in order,
    caring about the word position.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注单词顺序**：它按顺序逐个处理序列中的单词，关心单词的位置。'
- en: '**Suitable for working in various modes** (**many-to-many, one-to-many**):
    We can train a machine translation model or sentiment analysis using the same
    recurrency paradigm. Both architectures would be based on an RNN.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The disadvantages of an RNN architecture are listed here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '**Long-term dependency problem**: When we process a very long document and
    try to link the terms that are far from each other, we need to care about and
    encode all irrelevant other terms between these terms.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prone to exploding or vanishing gradient problems**: When working on long
    documents, updating the weights of the very first words is a big deal, which makes
    a model untrainable due to a vanishing gradient problem.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hard to apply parallelizable training**: Parallelization breaks the main
    problem down into a smaller problem and executes the solutions at the same time,
    but RNN follows a classic sequential approach. Each layer strongly depends on
    previous layers, which makes parallelization impossible.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The computation is slow as the sequence is long**: An RNN could be very efficient
    for short text problems. It processes longer documents very slowly, besides the
    long-term dependency problem.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although an RNN can theoretically attend the information at many timesteps
    before, in the real world, problems such as long documents and long-term dependencies
    are impossible to discover. Long sequences are represented within many deep layers.
    These problems have been addressed by many studies, some of which are outlined
    here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '*Hochreiter and Schmidhuber. Long Short-term Memory. 1997*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bengio et al. Learning long-term dependencies with gradient descent is difficult.
    1993*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K. Cho et al. Learning phrase representations using RNN encoder-decoder for
    statistical machine translation. 2014*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTMs and gated recurrent units
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LSTM (*Schmidhuber, 1997*) and **Gated Recurrent Units** (**GRUs**) (*Cho,
    2014*) are new variants of RNNs, have solved long-term dependency problems, and
    have attracted great attention. LSTMs were particularly developed to cope with
    the long-term dependency problem. The advantage of an LSTM model is that it uses
    the additional cell state, which is a horizontal sequence line on the top of the
    LSTM unit. This cell state is controlled by special purpose gates for forget,
    insert, or update operations. The complex unit of an LSTM architecture is depicted
    in the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – An LSTM unit ](img/B17123_01_06.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – An LSTM unit
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'It is able to decide the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: What kind of information we will store in the cell state
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which information will be forgotten or deleted
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the original RNN, in order to learn the state of I tokens, it recurrently
    processes the entire state of previous tokens between timestep0 and timestepi-1\.
    Carrying entire information from earlier timesteps leads to vanishing gradient
    problems, which makes the model untrainable. The gate mechanism in LSTM allows
    the architecture to skip some unrelated tokens at a certain timestep or remember
    long-range states in order to learn the current token state.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'A GRU is similar to an LSTM in many ways, the main difference being that a
    GRU does not use the cell state. Rather, the architecture is simplified by transferring
    the functionality of the cell state to the hidden state, and it only includes
    two gates: an *update gate* and a *reset gate*. The update gate determines how
    much information from the previous and current timesteps will be pushed forward.
    This feature helps the model keep relevant information from the past, which minimizes
    the risk of a vanishing gradient problem as well. The reset gate detects the irrelevant
    data and makes the model forget it.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: A gentle implementation of LSTM with Keras
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to download the **Stanford Sentiment Treebank** (**SST-2**) sentiment
    dataset from the **General Language Understanding Evaluation** (**GLUE**) benchmark.
    We can do this by running the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Important note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '**SST-2**: This is a fully labeled parse tree that allows for complete sentiment
    analysis in English. The corpus originally consists of about 12K single sentences
    extracted from movie reviews. It was parsed with the Stanford parser and includes
    over 200K unique phrases, each annotated by three human judges. For more information,
    see *Socher et al., Parsing With Compositional Vector Grammars, EMNLP. 2013* ([https://nlp.stanford.edu/sentiment](https://nlp.stanford.edu/sentiment)).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading the data, let''s read it as a pandas object, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We need to set maximum sentence length, build vocabulary and dictionaries (`word2idx`,
    `idx2words`), and finally represent each sentence as a list of indexes rather
    than strings. We can do this by running the following code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Sequences that are shorter than `max_sen_len` (maximum sentence length) are
    padded with a `PAD` value until they are `max_sen_len` in length. On the other
    hand, longer sequences are truncated so that they fit `max_sen_len`. Here is the
    implementation:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are ready to design and train an LSTM model, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model will be trained for 30 epochs. In order to plot what the LSTM model
    has learned so far, we can execute the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The code produces the following plot, which shows us the training and validation
    performance of the LSTM-based text classification:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – The classification performance of the LSTM network ](img/B17123_01_07.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – The classification performance of the LSTM network
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned before, the main problem of an RNN-based encoder-decoder model
    is that it produces a single fixed representation for a sequence. However, the
    attention mechanism allowed the RNN to focus on certain parts of the input tokens
    as it maps them to a certain part of the output tokens. This attention mechanism
    has been found to be useful and has become one of the underlying ideas of the
    Transformer architecture. We will discuss how the Transformer architecture takes
    advantage of attention in the next part and throughout the entire book.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，基于 RNN 的编码器-解码器模型的主要问题是它为一个序列生成一个固定的表示。 但是，注意机制使得 RNN 能够专注于输入标记的某些部分，并将它们映射到输出标记的某些部分。
    发现这种注意机制非常有用，并已成为 Transformer 架构的基本思想之一。 我们将在接下来的部分以及整本书的讨论中讨论 Transformer 架构如何利用注意力。
- en: A brief overview of CNNs
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN 的简要概述
- en: 'CNNs, after their success in computer vision, were ported to NLP in terms of
    modeling sentences or tasks such as semantic text classification. A CNN is composed
    of convolution layers followed by a dense neural network in many practices. A
    convolution layer performs over the data in order to extract useful features.
    As with any DL model, a convolution layer plays the feature extraction role to
    automate feature extraction. This feature layer, in the case of NLP, is fed by
    an embedding layer that takes sentences as an input in a one-hot vectorized format.
    The one-hot vectors are generated by a `token-id` for each word forming a sentence.
    The left part of the following screenshot shows a one-hot representation of a
    sentence:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 在计算机视觉方面取得成功后，也被用于 NLP，用于建模句子或语义文本分类等任务。 在许多实践中，CNN 由卷积层组成，然后是一个密集的神经网络。
    卷积层对数据进行处理以提取有用的特征。 与任何 DL 模型一样，卷积层扮演自动化特征提取的角色。 在 NLP 的情况下，这个特征层是由一个嵌入层提供输入，该层将句子以
    one-hot 矢量化格式作为输入。 这些 one-hot 矢量是通过为组成句子的每个单词生成一个`token-id`来生成的。 以下截图的左侧显示了句子的
    one-hot 表示：
- en: '![Figure 1.8 – One-hot vectors ](img/B17123_01_08.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 - One-hot 矢量](img/B17123_01_08.jpg)'
- en: Figure 1.8 – One-hot vectors
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 - One-hot 矢量
- en: 'Each token, represented by a one-hot vector, is fed to the embedding layer.
    The embedding layer can be initialized by random values or by using pre-trained
    word vectors such as GloVe, Word2vec, or FastText. A sentence will then be transformed
    into a dense matrix in the shape of NxE (where **N** is the number of tokens in
    a sentence and **E** is the embedding size). The following screenshot illustrates
    how a 1D CNN processes that dense matrix:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 每个由 one-hot 矢量表示的标记都被送入嵌入层。 嵌入层可以通过随机值或使用预训练的单词向量（如 GloVe、Word2vec 或 FastText）进行初始化。
    然后，该句子将转换为一个 NxE 形状的密集矩阵（其中**N**是句子中标记的数量，**E**是嵌入的大小）。 以下截图说明了 1D CNN 如何处理该密集矩阵：
- en: '![Figure 1.9 – 1D CNN network for a sentence of five tokens ](img/B17123_01_09.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 - 五个标记的句子的 1D CNN 网络](img/B17123_01_09.jpg)'
- en: Figure 1.9 – 1D CNN network for a sentence of five tokens
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 - 五个标记的句子的 1D CNN 网络
- en: 'Convolution will take place on top of this operation with different layers
    and kernels. Hyperparameters for the convolution layer are the kernel size and
    the number of kernels. It is also good to note that 1D convolution is applied
    here and the reason for that is token embeddings cannot be seen as partial, and
    we want to apply kernels capable of seeing multiple tokens in a sequential order
    together. You can see it as something like an n-gram with a specified window.
    Using shallow TL combined with CNN models is also another good capability of such
    models. As shown in the following screenshot, we can also propagate the networks
    with a combination of many representations of tokens, as proposed in the 2014
    study by Yoon Kim, *Convolutional Neural Networks for Sentence Classification*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积将在不同层和核之上进行。 卷积层的超参数是核大小和核的数量。 值得注意的是，这里应用的是 1D 卷积，原因是标记嵌入不能被视为局部的，我们希望应用能够依次看到多个标记的核。
    您可以将其视为具有指定窗口的 n-gram。 使用浅层 TL 结合 CNN 模型也是这些模型的另一个良好的能力。 正如以下截图所示，我们还可以用许多标记的表示的组合来传播网络，正如
    2014 年由 Yoon Kim 提出的研究中所建议的那样，《句子分类的卷积神经网络》:'
- en: '![Figure 1.10 – Combination of many representations in a CNN ](img/B17123_01_10.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10 - CNN 中许多表示的组合](img/B17123_01_10.jpg)'
- en: Figure 1.10 – Combination of many representations in a CNN
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 - CNN 中许多表示的组合
- en: 'For example, we can use three embedding layers instead of one and concatenate
    them for each token. Given this setup, a token such as **fell** will have a vector
    size of 3x128 if the embedding size is 128 for all three different embeddings.
    These embeddings can be initialized with pre-trained vectors from Word2vec, GloVe,
    and FastText. The convolution operation at each step will see N words with their
    respective three vectors (N is the convolution filter size). The type of convolution
    that is used here is a 1D convolution. The dimension here denotes possible movements
    when doing the operation. For example, a 2D convolution will move along two axes,
    while a 1D convolution just moves along one axis. The following screenshot shows
    the differences between them:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – Convolutional directions ](img/B17123_01_11.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – Convolutional directions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet is a 1D CNN implementation processing the same data
    used in an LSTM pipeline. It includes a composition of `Conv1D` and `MaxPooling`
    layers, followed by `GlobalMaxPooling` layers. We can extend the pipeline by tweaking
    the parameters and adding more layers to optimize the model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It turns out that the CNN model showed comparable performance with its LSTM
    counterpart. Although CNNs have become a standard in image processing, we have
    seen many successful applications of CNNs for NLP. While an LSTM model is trained
    to recognize patterns across time, a CNN model recognizes patterns across space.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Transformer architecture
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transformer** models have received immense interest because of their effectiveness
    in an enormous range of NLP problems, from text classification to text generation.
    The attention mechanism is an important part of these models and plays a very
    crucial role. Before Transformer models, the attention mechanism was proposed
    as a helper for improving conventional DL models such as RNNs. To have a good
    understanding of Transformers and their impact on the NLP, we will first study
    the attention mechanism.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Attention mechanism
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first variations of the attention mechanism was proposed by *Bahdanau
    et al. (2015)*. This mechanism is based on the fact that RNN-based models such
    as GRUs or LSTMs have an information bottleneck on tasks such as `token-id` and
    process it in a recurrent fashion (encoder). Afterward, the processed intermediate
    representation is fed into another recurrent unit (decoder) to extract the results.
    This avalanche-like information is like a rolling ball that consumes all the information,
    and rolling it out is hard for the decoder part because the decoder part does
    not see all the dependencies and only gets the intermediate representation (context
    vector) as an input.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'To align this mechanism, Bahdanau proposed an attention mechanism to use weights
    on intermediate hidden values. These weights align the amount of attention a model
    must pay to input in each decoding step. Such wonderful guidance assists models
    in specific tasks such as NMT, which is a many-to-many task. A diagram of a typical
    attention mechanism is provided here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – Attention mechanism ](img/B17123_01_12.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – Attention mechanism
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Different attention mechanisms have been proposed with different improvements.
    *Additive*, *multiplicative*, *general*, and *dot-product* attention appear within
    the family of these mechanisms. The latter, which is a modified version with a
    scaling parameter, is noted as scaled dot-product attention. This specific attention
    type is the foundation of Transformers models and is called a **multi-head attention
    mechanism**. Additive attention is also what was introduced earlier as a notable
    change in NMT tasks. You can see an overview of the different types of attention
    mechanisms here:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 2 – Types of attention mechanisms (Image inspired from https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
    ](img/B17123_01_Table2.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Table 2 – Types of attention mechanisms (Image inspired from https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Since attention mechanisms are not specific to NLP, they are also used in different
    use cases in various fields, from computer vision to speech recognition. The following
    screenshot shows a visualization of a multimodal approach trained for neural image
    captioning (*K Xu et al., Show, attend and tell: Neural image caption generation
    with visual attention, 2015*):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Attention mechanism in computer vision ](img/B17123_01_13.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – Attention mechanism in computer vision
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'The multi-head attention mechanism that is shown in the following diagram is
    an essential part of the Transformer architecture:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Multi-head attention mechanism ](img/B17123_01_14.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Multi-head attention mechanism
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's understand multi-head attention mechanisms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Multi-head attention mechanisms
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before jumping into scaled dot-product attention mechanisms, it''s better to
    get a good understanding of self-attention. **Self-attention**, as shown in *Figure
    1.15*, is a basic form of a scaled self-attention mechanism. This mechanism uses
    an input matrix shown as *X* and produces an attention score between various items
    in *X*. We see *X* as a 3x4 matrix where 3 represents the number of tokens and
    4 presents the embedding size. *Q* from *Figure 1.15* is also known as the **query**,
    *K* is known as the **key**, and *V* is noted as the **value**. Three types of
    matrices shown as *theta*, *phi*, and *g* are multiplied by *X* before producing
    *Q*, *K*, and *V*. The multiplied result between query (*Q*) and key (*K*) yields
    an attention score matrix. This can also be seen as a database where we use the
    query and keys in order to find out how much various items are related in terms
    of numeric evaluation. Multiplication of the attention score and the *V* matrix
    produces the final result of this type of attention mechanism. The main reason
    for it being called **self-attention** is because of its unified input *X*; *Q*,
    *K*, and *V* are computed from *X*. You can see all this depicted in the following
    diagram:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Mathematical representation for the attention mechanism (Image
    inspired from https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp)
    ](img/B17123_01_15.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Mathematical representation for the attention mechanism (Image
    inspired from https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'A scaled dot-product attention mechanism is very similar to a self-attention
    (dot-product) mechanism except it uses a scaling factor. The multi-head part,
    on the other hand, ensures the model is capable of looking at various aspects
    of input at all levels. Transformer models attend to encoder annotations and the
    hidden values from past layers. The architecture of the Transformer model does
    not have a recurrent step-by-step flow; instead, it uses positional encoding in
    order to have information about the position of each token in the input sequence.
    The concatenated values of the embeddings (randomly initialized) and the fixed
    values of positional encoding are the input fed into the layers in the first encoder
    part and are propagated through the architecture, as illustrated in the following
    diagram:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16 – A Transformer ](img/B17123_01_16.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – A Transformer
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'The positional information is obtained by evaluating sine and cosine waves
    at different frequencies. An example of positional encoding is visualized in the
    following screenshot:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.17 – Positional encoding (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
    ](img/B17123_01_17.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – Positional encoding (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example of performance on the Transformer architecture and the scaled
    dot-product attention mechanism is given in the following popular screenshot:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.18 – Attention mapping for Transformers (Image inspired from https://ai.googleblog.com/2017/08/Transformer-novel-neural-network.html)
    ](img/B17123_01_18.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – Attention mapping for Transformers (Image inspired from https://ai.googleblog.com/2017/08/Transformer-novel-neural-network.html)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'The word **it** refers to different entities in different contexts, as is seen
    from the preceding screenshot. Another improvement made by using a Transformer
    architecture is in parallelism. Conventional sequential recurrent models such
    as LSTMs and GRUs do not have such capabilities because they process the input
    token by token. Feed-forward layers, on the other hand, speed up a bit more because
    single matrix multiplication is far faster than a recurrent unit. Stacks of multi-head
    attention layers gain a better understanding of complex sentences. A good visual
    example of a multi-head attention mechanism is shown in the following screenshot:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Multi-head attention mechanism (Image inspired from https://imgur.com/gallery/FBQqrxw)
    ](img/B17123_01_19.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Multi-head attention mechanism (Image inspired from https://imgur.com/gallery/FBQqrxw)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: On the decoder side of the attention mechanism, a very similar approach to the
    encoder is utilized with small modifications. A multi-head attention mechanism
    is the same, but the output of the encoder stack is also used. This encoding is
    given to each decoder stack in the second multi-head attention layer. This little
    modification introduces the output of the encoder stack while decoding. This modification
    lets the model be aware of the encoder output while decoding and at the same time
    help it during training to have a better gradient flow over various layers. The
    final softmax layer at the end of the decoder layer is used to provide outputs
    for various use cases such as NMT, for which the original Transformer architecture
    was introduced.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: This architecture has two inputs, noted as inputs and outputs (shifted right).
    One is always present (the inputs) in both training and inference, while the other
    is just present in training and in inference, which is produced by the model.
    The reason we do not use model predictions in inference is to stop the model from
    going too wrong by itself. But what does it mean? Imagine a neural translation
    model trying to translate a sentence from English to French—at each step, it makes
    a prediction for a word, and it uses that predicted word to predict the next one.
    But if it goes wrong at some step, all the following predictions will be wrong
    too. To stop the model from going wrong like this, we provide the correct words
    as a shifted-right version.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'A visual example of a Transformer model is given in the following diagram.
    It shows a Transformer model with two encoders and two decoder layers. The **Add
    & Normalize** layer from this diagram adds and normalizes the input it takes from
    the **Feed Forward** layer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Transformer model (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
    ](img/B17123_01_20.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Transformer model (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Another major improvement that is used by a Transformer-based architecture is
    based on a simple universal text-compression scheme to prevent unseen tokens on
    the input side. This approach, which takes place by using different methods such
    as byte-pair encoding and sentence-piece encoding, improves a Transformer's performance
    in dealing with unseen tokens. It also guides the model when the model encounters
    morphologically close tokens. Such tokens were unseen in the past and are rarely
    used in the training, and yet, an inference might be seen. In some cases, chunks
    of it are seen in training; the latter happens in the case of morphologically
    rich languages such as Turkish, German, Czech, and Latvian. For example, a model
    might see the word *training* but not *trainings*. In such cases, it can tokenize
    *trainings* as *training+s*. These two are commonly seen when we look at them
    as two parts.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based models have quite common characteristics—for example, they
    are all based on this original architecture with differences in which steps they
    use and don't use. In some cases, minor differences are made—for example, improvements
    to the multi-head attention mechanism taking place.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Using TL with Transformers
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TL** is a field of **Artificial Intelligence** (**AI**) and ML that aims
    to make models reusable for different tasks—for example, a model trained on a
    given task such as *A* is reusable (fine-tuning) on a different task such as *B*.
    In an NLP field, this is achievable by using Transformer-like architectures that
    can capture the understanding of language itself by language modeling. Such models
    are called language models—they provide a model for the language they have been
    trained on. TL is not a new technique, and it has been used in various fields
    such as computer vision. ResNet, Inception, VGG, and EfficientNet are examples
    of such models that can be used as pre-trained models able to be fine-tuned on
    different computer-vision tasks.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Shallow TL using models such as *Word2vec*, *GloVe*, and *Doc2vec* is also possible
    in NLP. It is called *shallow* because there is no model behind this kind of TL
    and instead, the pre-trained vectors for words/tokens are utilized. You can use
    these token- or document-embedding models followed by a classifier or use them
    combined with other models such as RNNs instead of using random embeddings.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: TL in NLP using Transformer models is also possible because these models can
    learn a language itself without any labeled data. Language modeling is a task
    used to train transferable weights for various problems. Masked language modeling
    is one of the methods used to learn a language itself. As with Word2vec's window-based
    model for predicting center tokens, in masked language modeling, a similar approach
    takes place, with key differences. Given a probability, each word is masked and
    replaced with a special token such as *[MASK]*. The language model (a Transformer-based
    model, in our case) must predict the masked words. Instead of using a window,
    unlike with Word2vec, a whole sentence is given, and the output of the model must
    be the same sentence with masked words filled.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: One of the first models that used the Transformer architecture for language
    modeling is **BERT**, which is based on the encoder part of the Transformer architecture.
    Masked language modeling is accomplished by BERT by using the same method described
    before and after training a language model. BERT is a transferable language model
    for different NLP tasks such as token classification, sequence classification,
    or even question answering.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these tasks is a fine-tuning task for BERT once a language model is
    trained. BERT is best known for its key characteristics on the base Transformer
    encoder model, and by altering these characteristics, different versions of it—small,
    tiny, base, large, and extra-large—are proposed. Contextual embedding enables
    a model to have the correct meaning of each word based on the context in which
    it is given—for example, the word *Cold* can have different meanings in two different
    sentences: *Cold-hearted killer* and *Cold weather*. The number of layers at the
    encoder part, the input dimension, the output embedding dimension, and the number
    of multi-head attention mechanisms are these key characteristics, as illustrated
    in the following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21 – Pre-training and fine-tuning procedures for BERT (Image inspired
    from J. Devlin et al., Bert: Pre-training of deep bidirectional Transformers for
    language understanding, 2018) ](img/B17123_01_21.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.21 – Pre-training and fine-tuning procedures for BERT (Image inspired
    from J. Devlin et al., Bert: Pre-training of deep bidirectional Transformers for
    language understanding, 2018)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 1.21*, the pre-training phase also consists of another
    objective known as **next-sentence prediction**. As we know, each document is
    composed of sentences followed by each other, and another important part of training
    for a model to grasp the language is to understand the relations of sentences
    to each other—in other words, whether they are related or not. To achieve these
    tasks, BERT introduced special tokens such as *[CLS]* and *[SEP]*. A *[CLS]* token
    is an initially meaningless token used as a start token for all tasks, and it
    contains all information about the sentence. In sequence-classification tasks
    such as NSP, a classifier on top of the output of this token (output position
    of *0*) is used. It is also useful in evaluating the sense of a sentence or capturing
    its semantics—for example, when using a Siamese BERT model, comparing these two
    *[CLS]* tokens for different sentences by a metric such as cosine-similarity is
    very helpful. On the other hand, *[SEP]* is used to distinguish between two sentences,
    and it is only used to separate two sentences. After pre-training, if someone
    aims to fine-tune BERT on a sequence-classification task such as sentiment analysis,
    which is a sequence-classification task, they will use a classifier on top of
    the output embedding of *[CLS]*. It is also notable that all TL models can be
    frozen during fine-tuning or freed; frozen means seeing all weights and biases
    inside the model as constants and stopping training on them. In the example of
    sentiment analysis, just the classifier will be trained, not the model if it is
    frozen.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this, we now come to the end of the chapter. You should now have an understanding
    of the evolution of NLP methods and approaches, from BoW to Transformers. We looked
    at how to implement BoW-, RNN-, and CNN-based approaches and understood what Word2vec
    is and how it helps improve the conventional DL-based methods using shallow TL.
    We also looked into the foundation of the Transformer architecture, with BERT
    as an example. By the end of the chapter, we had learned about TL and how it is
    utilized by BERT.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have learned basic information that is necessary to continue
    to the next chapters. We understood the main idea behind Transformer-based architectures
    and how TL can be applied using this architecture.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how it is possible to run a simple Transformer
    example from scratch. The related information about the installation steps will
    be given, and working with datasets and benchmarks is also investigated in detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Mikolov, T., Chen, K., Corrado, G. & Dean, J. (2013). Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781.*'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bahdanau, D., Cho, K. & Bengio, Y. (2014). Neural machine translation by jointly
    learning to align and translate. arXiv preprint arXiv:1409.0473.*'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pennington, J., Socher, R. & Manning, C. D. (2014, October). GloVe: Global
    vectors for word representation. In Proceedings of the 2014 conference on empirical
    methods in natural language processing (EMNLP) (pp. 1532-1543).*'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hochreiter, S. & Schmidhuber, J. (1997). Long short-term memory. Neural computation,
    9(8), 1735-1780.*'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bengio, Y., Simard, P, & Frasconi, P. (1994). Learning long-term dependencies
    with gradient descent is difficult. IEEE transactions on neural networks, 5(2),
    157-166.*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
    H. & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. arXiv preprint arXiv:1406.1078.*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kim, Y. (2014). Convolutional neural networks for sentence classification.
    CoRR abs/1408.5882 (2014). arXiv preprint arXiv:1408.5882.*'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
    N. & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.*'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training
    of deep bidirectional Transformers for language understanding. arXiv preprint
    arXiv:1810.04805.*'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
