- en: '*Chapter 1*: From Bag-of-Words to the Transformer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*：从词袋模型到Transformer'
- en: In this chapter, we will discuss what has changed in **Natural Language Processing**
    (**NLP**) over two decades. We experienced different paradigms and finally entered
    the era of Transformer architectures. All the paradigms help us to gain a better
    representation of words and documents for problem-solving. Distributional semantics
    describes the meaning of a word or a document with vectorial representation, looking
    at distributional evidence in a collection of articles. Vectors are used to solve
    many problems in both supervised and unsupervised pipelines. For language-generation
    problems, n-gram language models have been leveraged as a traditional approach
    for years. However, these traditional approaches have many weaknesses that we
    will discuss throughout the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在过去的20年中**自然语言处理**（**NLP**）发生了什么变化。我们经历了不同的范式，最终进入了Transformer架构的时代。所有这些范式都帮助我们更好地表示单词和文档以解决问题。分布语义描述了单词或文档的意义，具有矢量表示，观察在文集中的分布证据。矢量用于在受控和非受控流程中解决许多问题。对于语言生成问题，n-gram语言模型长期以来一直被用作传统方法。然而，这些传统方法存在许多缺点，在整整一章中我们将进行讨论。
- en: We will further discuss classical **Deep Learning** (**DL**) architectures such
    as **Recurrent Neural Networks** (**RNNs**), **Feed-Forward Neural Networks**
    (**FFNNs**), and **Convolutional Neural Networks** (**CNNs**). These have improved
    the performance of the problems in the field and have overcome the limitation
    of traditional approaches. However, these models have had their own problems too.
    Recently, Transformer models have gained immense interest because of their effectiveness
    in all NLP tasks, from text classification to text generation. However, the main
    success has been that Transformers effectively improve the performance of multilingual
    and multi-task NLP problems, as well as monolingual and single tasks. These contributions
    have made **Transfer Learning** (**TL**) more possible in NLP, which aims to make
    models reusable for different tasks or different languages.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进一步讨论经典的**深度学习**（**DL**）架构，如**循环神经网络**（**RNNs**），**前馈神经网络**（**FFNNs**）和**卷积神经网络**（**CNNs**）。这些架构已经改善了该领域问题的性能，并克服了传统方法的局限。然而，这些模型也存在各自的问题。最近，由于Transformer模型在从文本分类到文本生成的所有NLP任务中的有效性，它们引起了巨大的兴趣。然而，主要的成功在于Transformer有效地提高了多语言和多任务NLP问题的性能，以及单语言和单任务。这些贡献使得**迁移学习**(**TL**)在NLP中更为可能，其目标是使模型可在不同任务或不同语言中重复使用。
- en: Starting with the attention mechanism, we will briefly discuss the Transformer
    architecture and the differences between previous NLP models. In parallel with
    theoretical discussions, we will show practical examples with the popular NLP
    framework. For the sake of simplicity, we will choose introductory code examples
    that are as short as possible.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从注意机制开始，我们将简要讨论Transformer架构和之前NLP模型的区别。与理论讨论并行，我们将展示流行的NLP框架的实际示例。为简单起见，我们将选择尽可能简短的入门代码示例。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Evolution of NLP toward Transformers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP向Transformer的演变
- en: Understanding distributional semantics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布语义
- en: Leveraging DL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用深度学习
- en: Overview of the Transformer architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer架构概述
- en: Using TL with Transformers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Transformer进行迁移学习
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using Jupyter Notebook to run our coding exercises that require
    `python >=3.6.0`, along with the following packages that need to be installed
    with the `pip install` command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Jupyter Notebook来运行需要安装`python >=3.6.0`及以下包的编码练习：
- en: '`sklearn`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`'
- en: '`nltk==3.5.0`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nltk==3.5.0`'
- en: '`gensim==3.8.3`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gensim==3.8.3`'
- en: '`fasttext`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fasttext`'
- en: '`keras>=2.3.0`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`keras>=2.3.0`'
- en: '`Transformers >=4.00`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Transformers >=4.00`'
- en: 'All notebooks with coding exercises are available at the following GitHub link:
    [https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所有带有编码练习的笔记本都可在以下GitHub链接处找到：[https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-Transformers/tree/main/CH01)。
- en: 'Check out the following link to see Code in Action Video: [https://bit.ly/2UFPuVd](https://bit.ly/2UFPuVd)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以查看视频代码示例：[https://bit.ly/2UFPuVd](https://bit.ly/2UFPuVd)
- en: Evolution of NLP toward Transformers
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP向Transformer的发展
- en: 'We have seen profound changes in NLP over the last 20 years. During this period,
    we experienced different paradigms and finally entered a new era dominated mostly
    by magical *Transformer* architecture. This architecture did not come out of nowhere.
    Starting with the help of various neural-based NLP approaches, it gradually evolved
    to an attention-based encoder-decoder type architecture and still keeps evolving.
    The architecture and its variants have been successful thanks to the following
    developments in the last decade:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 过去20年来，我们已经看到了自然语言处理（NLP）领域的深刻变化。在这段时间里，我们经历了不同的范式，最终进入了一个主要由神奇的*Transformer*架构主导的新时代。这种架构并非从天而降。从各种基于神经网络的NLP方法开始，它逐渐演变成了基于注意力的编码-解码类型的架构，并且仍在不断发展。过去十年中，由于以下发展，该架构及其各种变体取得了成功：
- en: Contextual word embeddings
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文词嵌入
- en: Better subword tokenization algorithms for handling unseen words or rare words
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的子词标记算法，用于处理未知单词或稀有单词
- en: Injecting additional memory tokens into sentences, such as `Paragraph ID` in
    `Doc2vec` or a **Classification** (**CLS**) token in **Bidirectional Encoder Representations
    from Transformers** (**BERT**)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将额外的记忆标记注入到句子中，比如`Doc2vec`中的`段落ID`或**来自Transformer的双向编码器表示**（**BERT**）中的**分类**（**CLS**）标记
- en: Attention mechanisms, which overcome the problem of forcing input sentences
    to encode all information into one context vector
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制，克服将输入句子强制编码到一个上下文向量中的问题
- en: Multi-head self-attention
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头自注意力
- en: Positional encoding to case word order
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于处理单词顺序的位置编码
- en: Parallelizable architectures that make for faster training and fine-tuning
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可并行化的架构，使训练和微调更快
- en: Model compression (distillation, quantization, and so on)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型压缩（蒸馏、量化等）
- en: TL (cross-lingual, multitask learning)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TL（跨语言、多任务学习）
- en: 'For many years, we used traditional NLP approaches such as *n-gram language
    models*, *TF-IDF-based information retrieval models*, and *one-hot encoded document-term
    matrices*. All these approaches have contributed a lot to the solution of many
    NLP problems such as *sequence classification*, *language generation*, *language
    understanding*, and so forth. On the other hand, these traditional NLP methods
    have their own weaknesses—for instance, falling short in solving the problems
    of sparsity, unseen words representation, tracking long-term dependencies, and
    others. In order to cope with these weaknesses, we developed DL-based approaches
    such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，我们使用了传统的NLP方法，例如*n-gram语言模型*、*基于TF-IDF的信息检索模型*和*one-hot编码的文档-术语矩阵*。所有这些方法都为解决许多NLP问题（如*序列分类*、*语言生成*、*语言理解*等）做出了很大贡献。另一方面，这些传统的NLP方法也有其自身的弱点—例如，在解决稀疏性、未知单词表示、跟踪长期依赖关系等问题上存在不足。为了应对这些弱点，我们开发了基于深度学习的方法，如以下所示：
- en: RNNs
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNNs
- en: CNNs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs
- en: FFNNs
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FFNNs
- en: Several variants of RNNs, CNNs, and FFNNs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几种RNNs、CNNs和FFNNs的变种
- en: 'In 2013, as a two-layer FFNN word-encoder model, `Word2vec`, sorted out the
    dimensionality problem by producing short and dense representations of the words,
    called **word embeddings**. This early model managed to produce fast and efficient
    static word embeddings. It transformed unsupervised textual data into supervised
    data (*self-supervised learning*) by either predicting the target word using context
    or predicting neighbor words based on a sliding window. **GloVe**, another widely
    used and popular model, argued that count-based models can be better than neural
    models. It leverages both global and local statistics of a corpus to learn embeddings
    based on word-word co-occurrence statistics. It performed well on some syntactic
    and semantic tasks, as shown in the following screenshot. The screenshot tells
    us that the embeddings offsets between the terms help to apply vector-oriented
    reasoning. We can learn the generalization of gender relations, which is a semantic
    relation from the offset between *man* and *woman* (*man-> woman*). Then, we can
    arithmetically estimate the vector of *actress* by adding the vector of the term
    *actor* and the offset calculated before. Likewise, we can learn syntactic relations
    such as word plural forms. For instance, if the vectors of **Actor**, **Actors**,
    and **Actress** are given, we can estimate the vector of **Actresses**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年，作为两层FFNN单词编码器模型，`Word2vec`通过产生短而稠密的单词表示（称为**词嵌入**）解决了维度问题。这个早期模型成功地产生了快速而有效的静态词嵌入。它通过预测上下文中的目标单词或基于滑动窗口预测相邻单词，将无监督的文本数据转换为受监督的数据（*自监督学习*）。**GloVe**，另一个被广泛使用和普遍流行的模型，认为基于计数的模型可能比神经模型更好。它利用语料库的全局和局部统计数据来学习基于单词共现统计的嵌入。它在一些句法和语义任务上表现良好，如下面的截图所示。截图告诉我们，术语之间的嵌入偏移有助于应用矢量导向推理。我们可以学习性别关系的泛化，这是从*男人*和*女人*之间的偏移关系推导出来的语义关系（*男人->
    女人*）。然后，我们可以通过将*男演员*的矢量和之前计算出的偏移矢量相加来算出*女演员*的矢量。同样，我们可以学习词的复数形式。例如，如果给出**Actor**，**Actors**和**Actress**的矢量，我们可以估算**Actresses**的矢量：
- en: '![Figure 1.1 – Word embeddings offset for relation extraction ](img/B17123_01_01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 - 用于关系提取的单词嵌入偏移](img/B17123_01_01.jpg)'
- en: Figure 1.1 – Word embeddings offset for relation extraction
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 - 用于关系提取的单词嵌入偏移
- en: The recurrent and convolutional architectures such as RNN, **Long Short-Term
    Memory** (**LSTM**), and CNN started to be used as encoders and decoders in **sequence-to-sequence**
    (**seq2seq**) problems. The main challenge with these early models was polysemous
    words. The senses of the words are ignored since a single fixed representation
    is assigned to each word, which is especially a severe problem for polysemous
    words and sentence semantics.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 递归和卷积架构，比如RNN、**长短期记忆**（**LSTM**）和CNN，开始在**序列到序列**（**seq2seq**）问题中被用作编码器和解码器。这些早期模型的主要挑战是多义词。由于给每个单词分配了单一的固定表示，因此忽略了单词的含义，这对多义词和句子语义尤其是一个严重的问题。
- en: The further pioneer neural network models such as **Universal Language Model
    Fine-tuning** (**ULMFit**) and **Embeddings from Language Models** (**ELMo**)
    managed to encode the sentence-level information and finally alleviate polysemy
    problems, unlike with static word embeddings. These two important approaches were
    based on LSTM networks. They also introduced the concept of pre-training and fine-tuning.
    They help us to apply TL, employing the pre-trained models trained on a general
    task with huge textual datasets. Then, we can easily perform fine-tuning by resuming
    training of the pre-trained network on a target task with supervision. The representations
    differ from traditional word embeddings such that each word representation is
    a function of the entire input sentence. The modern Transformer architecture took
    advantage of this idea.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的先驱神经网络模型，比如**通用语言模型微调**（**ULMFit**）和**语言模型嵌入**（**ELMo**），成功地对句子级信息进行编码，并最终缓解了一词多义的问题，与静态词嵌入不同。这两种重要方法基于LSTM网络。它们还引入了预训练和微调的概念。它们帮助我们应用迁移学习，利用在大量文本数据集上进行常规任务训练的预训练模型。然后，我们可以很容易地通过在目标任务上继续对预训练网络进行训练，进行微调。这些表示与传统的词嵌入不同，每个单词表示是整个输入句子的函数。现代Transformer架构充分利用了这个想法。
- en: 'In the meantime, the idea of an attention mechanism made a strong impression
    in the NLP field and achieved significant success, especially in seq2seq problems.
    Earlier methods would pass the last state (known as a `Government of Canada` in
    the input sentence for an English to Turkish translation task. In the output sentence,
    the `Kanada Hükümeti` token makes strong connections with the input phrase and
    establishes a weaker connection with the remaining words in the input, as illustrated
    in the following screenshot:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，注意力机制的想法在自然语言处理领域引起了强烈的印象，并取得了显著的成功，特别是在 seq2seq 问题上。早期的方法会传递最后一个状态（称为输入句子中的`加拿大政府`，用于英语到土耳其语的翻译任务。在输出句子中，`Kanada
    Hükümeti` 标记与输入短语建立了强连接，并与输入中的其他单词建立了较弱的连接，如下方截图所示：
- en: '![Figure 1.2 – Sketchy visualization of an attention mechanism ](img/B17123_01_02.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 注意力机制的草图可视化](img/B17123_01_02.jpg)'
- en: Figure 1.2 – Sketchy visualization of an attention mechanism
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 注意力机制的草图可视化
- en: So, this mechanism makes models more successful in seq2seq problems such as
    translation, question answering, and text summarization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种机制使得模型在翻译、问答和文本摘要等 seq2seq 问题中更加成功。
- en: In 2017, the Transformer-based encoder-decoder model was proposed and found
    to be successful. The design is based on an FFNN by discarding RNN recurrency
    and using only attention mechanisms (*Vaswani et al., All you need is attention,
    2017*). The Transformer-based models have so far overcome many difficulties that
    other approaches faced and have become a new paradigm. Throughout this book, you
    will be exploring and understanding how the Transformer-based models work.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，基于 Transformer 的编码器-解码器模型被提出并被发现成功。该设计基于 FFNN，丢弃了 RNN 的递归，并仅使用注意力机制（*Vaswani
    et al., All you need is attention, 2017*）。到目前为止，基于 Transformer 的模型已经克服了其他方法所面临的许多困难，并成为了一个新的范式。在本书中，你将探索并理解
    Transformer 模型的工作原理。
- en: Understanding distributional semantics
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布语义
- en: Distributional semantics describes the meaning of a word with a vectorial representation,
    preferably looking at its distributional evidence rather than looking at its predefined
    dictionary definitions. The theory suggests that words co-occurring together in
    a similar environment tend to share similar meanings. This was first formulated
    by the scholar Harris (*Distributional Structure Word, 1954*). For example, similar
    words such as *dog* and *cat* mostly co-occur in the same context. One of the
    advantages of a distributional approach is to help the researchers to understand
    and monitor the semantic evolution of words across time and domains, also known
    as the **lexical semantic change problem**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 分布语义描述了单词的含义，并通过矢量表示，最好是查看其分布证据，而不是查看其预定义的词典定义。该理论表明，在相似环境中共同出现的单词倾向于共享相似的含义。这最早由学者哈里斯提出（*Distributional
    Structure Word, 1954*）。例如，诸如*狗*和*猫*这样的相似单词大多在相同的上下文中共同出现。分布式方法的一个优点是帮助研究人员理解和监测单词随时间和领域的语义演变，也被称为**词汇语义变化问题**。
- en: Traditional approaches have applied **Bag-of-Words** (**BoW**) and n-gram language
    models to build the representation of words and sentences for many years. In a
    BoW approach, words and documents are represented with a one-hot encoding as a
    sparse way of representation, also known as the **Vector Space Model** (**VSM**).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，传统方法一直应用**词袋模型**（**BoW**）和 n-gram 语言模型来构建单词和句子的表示。在词袋模型中，单词和文档以一种稀疏的方式表示为
    one-hot 编码，也被称为**向量空间模型**（**VSM**）。
- en: Text classification, word similarity, semantic relation extraction, word-sense
    disambiguation, and many other NLP problems have been solved by these one-hot
    encoding techniques for years. On the other hand, n-gram language models assign
    probabilities to sequences of words so that we can either compute the probability
    that a sequence belongs to a corpus or generate a random sequence based on a given
    corpus.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，这些 one-hot 编码技术已解决了文本分类、单词相似度、语义关系提取、单词意义消歧等许多自然语言处理问题。另一方面，n-gram 语言模型为单词序列分配概率，以便我们可以计算一个序列属于语料库的概率，或者基于给定语料库生成一个随机序列。
- en: BoW implementation
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BoW 实现
- en: 'A BoW is a representation technique for documents by counting the words in
    them. The main data structure of the technique is a document-term matrix. Let''s
    see a simple implementation of BoW with Python. The following piece of code illustrates
    how to build a document-term matrix with the Python `sklearn` library for a toy
    corpus of three sentences:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一种词袋（BoW）是一种文档的表征技术，通过计算其中的单词来实现。该技术的主要数据结构是文档词项矩阵。让我们用 Python 看一个 BoW 的简单实现。以下代码片段说明了如何使用
    Python 的 `sklearn` 库为一个三句话的玩具语料库构建文档词项矩阵：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the code is a document-term matrix, as shown in the following
    screenshot. The size is (3 x 10), but in a realistic scenario the matrix size
    can grow to much larger numbers such as 10K x 10M:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出是一个文档词项矩阵，如下图所示。其大小为 (3 x 10)，但在现实场景中，矩阵的大小可以增长到更大的数字，例如 10K x 10M：
- en: '![Figure 1.3 – Document-term matrix ](img/B17123_01_03.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 文档词项矩阵](img/B17123_01_03.jpg)'
- en: Figure 1.3 – Document-term matrix
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 文档词项矩阵
- en: The table indicates a count-based mathematical matrix where the cell values
    are transformed by a **Term Frequency-Inverse Document Frequency** (**TF-IDF**)
    weighting schema. This approach does not care about the position of words. Since
    the word order strongly determines the meaning, ignoring it leads to a loss of
    meaning. This is a common problem in a BoW method, which is finally solved by
    a recursion mechanism in RNN and positional encoding in Transformers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表格表示的是一种基于计数的数学矩阵，在其中单元格的值按照**词频-逆文档频率**（**TF-IDF**）加权模式进行转换。此方法不关心单词的位置。由于单词顺序强烈决定了含义，忽略它会导致意义丧失。这是
    BoW 方法中的常见问题，最终通过 RNN 中的递归机制和变压器中的位置编码得到解决。
- en: Each column in the matrix stands for the vector of a word in the vocabulary,
    and each row stands for the vector of a document. Semantic similarity metrics
    can be applied to compute the similarity or dissimilarity of the words as well
    as documents. Most of the time, we use bigrams such as `cat_sat` and `the_street`
    to enrich the document representation. For instance, as the parameter `ngram_range=(1,2)`
    is passed to `TfidfVectorizer`, it builds a vector space containing both unigrams
    (`big, cat, dog`) and bigrams (`big_cat`, `big_dog`). Thus, such models are also
    called **bag-of-n-grams**, which is a natural extension of **BoW**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的每一列代表词汇表中一个词的向量，每一行代表一个文档的向量。可以应用语义相似性指标来计算单词和文档的相似性或非相似性。大多数情况下，我们使用二元组，例如
    `cat_sat` 和 `the_street` 来丰富文档的表示。例如，当参数 `ngram_range=(1,2)` 传递给 `TfidfVectorizer`
    时，它构建一个包含 unigrams (`big, cat, dog`) 和 bigrams (`big_cat`, `big_dog`) 的向量空间。因此，这样的模型也被称为**词袋式
    n-grams**，它是**BoW**的自然扩展。
- en: If a word is commonly used in each document, it can be considered to be high-frequency,
    such as *and* *the*. Conversely, some words hardly appear in documents, called
    low-frequency (or rare) words. As high-frequency and low-frequency words may prevent
    the model from working properly, TF-IDF, which is one of the most important and
    well-known weighting mechanisms, is used here as a solution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词在每篇文章中都经常出现，那么它可以被视为高频词，例如 *and* 和 *the*。相反，一些词在文章中很少出现，称为低频（或稀有）词。由于高频和低频词可能会妨碍模型的正常工作，因此在这里使用了TF-IDF作为解决方案，这是最重要和著名的加权机制之一。
- en: '`the` has no discriminative power, `chased` can be highly informative and give
    clues about the subject of the text. This is because high-frequency words (stopwords,
    functional words) have little discriminating power in understanding the documents.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`the` 没有区分力，而 `chased` 可能具有高信息量，可以提供关于文本主题的线索。这是因为高频词（停用词，功能词）在理解文档时具有很少的区分能力。'
- en: The discriminativeness of the terms also depends on the domain—for instance,
    a list of DL articles is most likely to have the word `network` in almost every
    document. IDF can scale down the weights of all terms by using their **Document
    Frequency** (**DF**), where the DF of a word is computed by the number of documents
    in which a term appears. **Term Frequency** (**TF**) is the raw count of a term
    (word) in a document.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 词的区分度也取决于领域，例如，DL 文章列表中几乎每篇文章都可能有单词 `network`。IDF 可以通过使用单词的**文档频率**（**DF**）来缩小所有词的权值，其中单词的
    DF 通过单词出现在的文档数计算得出。**词频**（**TF**）是文档中词（术语）的原始计数。
- en: 'Some of the advantages and disadvantages of a TF-IDF based BoW model are listed
    as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于 TF-IDF 的 BoW 模型的一些优缺点列举如下：
- en: '![Table 1 – Advantages and disadvantages of a TF-IDF BoW model ](img/B17123_01_Table1.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![表格 1 – TF-IDF BoW 模型的优缺点](img/B17123_01_Table1.jpg)'
- en: Table 1 – Advantages and disadvantages of a TF-IDF BoW model
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表1 - TF-IDF BoW 模型的优缺点
- en: Overcoming the dimensionality problem
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克服维度问题
- en: To overcome the dimensionality problem of the BoW model, **Latent Semantic Analysis**
    (**LSA**) is widely used for capturing semantics in a low-dimensional space. It
    is a linear method that captures pairwise correlations between terms. LSA-based
    probabilistic methods can be still considered as a single layer of hidden topic
    variables. However, current DL models include multiple hidden layers, with billions
    of parameters. In addition to that, Transformer-based models showed that they
    can discover latent representations much better than such traditional models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 BoW 模型的维度问题，**潜在语义分析**（**LSA**）被广泛用于在低维空间中捕捉语义。它是一种线性方法，捕捉术语之间的成对相关性。基于
    LSA 的概率方法仍然可以被看作是一个隐藏主题变量的单层。然而，当前的 DL 模型包括多个隐藏层，参数量达到数十亿。除此之外，基于 Transformer
    的模型表明，它们可以比传统模型更好地发现潜在表示。
- en: For the **Natural Language Understanding** (**NLU**) tasks, the traditional
    pipeline starts with some preparation steps, such as *tokenization*, *stemming*,
    *noun phrase detection*, *chunking*, *stop-word elimination*, and much more. Afterward,
    a document-term matrix is constructed with any weighting schema, where TF-IDF
    is the most popular one. Finally, the matrix is served as a tabulated input for
    **Machine Learning** (**ML**) pipelines, sentiment analysis, document similarity,
    document clustering, or measuring the relevancy score between a query and a document.
    Likewise, terms are represented as a tabular matrix and can be input for a token
    classification problem where we can apply named-entity recognition, semantic relation
    extractions, and so on.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**自然语言理解**（**NLU**）任务，传统的流水线从一些准备步骤开始，如 *tokenization*、*stemming*、*noun phrase
    detection*、*chunking*、*stop-word elimination* 等等。之后，使用任何加权模式构建文档-术语矩阵，其中 TF-IDF
    是最流行的。最后，该矩阵作为 **机器学习**（**ML**）流水线、情感分析、文档相似性、文档聚类或测量查询与文档之间关联分数的表格化输入。同样，术语被表示为一个表格矩阵，并且可以作为一个
    token 分类问题的输入，其中我们可以应用命名实体识别、语义关系提取等。
- en: 'The classification phase includes a straightforward implementation of supervised
    ML algorithms such as **Support Vector Machine** (**SVM**), Random forest, logistic,
    naive bayes, and Multiple Learners (Boosting or Bagging). Practically, the implementation
    of such a pipeline can simply be coded as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 分类阶段包括对监督式机器学习算法的直接实现，如**支持向量机**（**SVM**）、随机森林、逻辑回归、朴素贝叶斯和多个学习器（Boosting 或 Bagging）。实际上，这样一个流水线的实现可以简单地编码如下：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As seen in the preceding code, we can apply fit operations easily thanks to
    the `sklearn` **Application Programming Interface** (**API**). In order to apply
    the learned model to train data, the following code is executed:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们可以轻松地应用 `sklearn` **应用程序编程接口**（**API**）来进行适配操作。为了将学习到的模型应用于训练数据，执行以下代码：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let's move on to the next section!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一节吧！
- en: Language modeling and generation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模与生成
- en: 'For language-generation problems, the traditional approaches are based on leveraging
    n-gram language models. This is also called a **Markov process**, which is a stochastic
    model in which each word (event) depends on a subset of previous words—*unigram*,
    *bigram*, or *n-gram*, outlined as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言生成问题，传统方法是基于利用 n-gram 语言模型。这也被称为**马尔可夫过程**，它是一种随机模型，其中每个词（事件）都依赖于前面的一部分词—*unigram*、*bigram*
    或 *n-gram*，如下所述：
- en: '**Unigram** (all words are independent and no chain): This estimates the probability
    of word in a vocabulary simply computed by the frequency of it to the total word
    count.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一元组**（所有词都是独立的，没有链）：这估计了词汇表中每个词的概率，简单地通过它在总词数中的频率计算得到。'
- en: '**Bigram** (First-order Markov process): This estimates the *P (word*i*| wordi*-1*)*.probability
    of *wordi* depending on *wordi-1*, which is simply computed by the ratio of *P
    (word*i *, wordi*-1*)* to *P (wordi*-1*)*.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元组**（一阶马尔可夫过程）：这估计了 *P（wordi | wordi-1）*，即 *wordi* 取决于 *wordi-1* 的概率，简单地通过
    *P（wordi，wordi-1）* 与 *P（wordi-1）* 的比率计算得到。'
- en: '**Ngram** (N-order Markov process): This estimates *P (wordi | word0, ...,
    wordi-1)*.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N 元组**（N 阶马尔可夫过程）：这估计了 *P（wordi | word0, ..., wordi-1）*。'
- en: 'Let''s give a simple language model implementation with the **Natural Language
    Toolkit** (**NLTK**) library. In the following implementation, we train a **Maximum
    Likelihood Estimator** (**MLE**) with order *n=2*. We can select any n-gram order
    such as *n=1* for unigrams, *n=2* for bigrams, *n=3* for trigrams, and so forth:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用**自然语言工具包**（**NLTK**）库进行一个简单的语言模型实现。在以下实现中，我们使用**最大似然估计器**（**MLE**）训练了一个*n=2*的模型。我们可以选择任何n-gram顺序，比如*n=1*代表unigrams，*n=2*代表bigrams，*n=3*代表trigrams等等：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `nltk` package first downloads the `gutenberg` corpus, which includes some
    texts from the *Project Gutenberg* electronic text archive, hosted at [https://www.gutenberg.org](https://www.gutenberg.org).
    It also downloads the `punkt` tokenizer tool for the punctuation process. This
    tokenizer divides a raw text into a list of sentences by using an unsupervised
    algorithm. The `nltk` package already includes a pre-trained English `punkt` tokenizer
    model for abbreviation words and collocations. It can be trained on a list of
    texts in any language before use. In the further chapters, we will discuss how
    to train different and more efficient tokenizers for Transformer models as well.
    The following code produces what the language model learned so far:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk`包首先下载`古腾堡`语料库，其中包括来自*古腾堡项目*电子文本存档的一些文本，托管在[https://www.gutenberg.org](https://www.gutenberg.org)。它还下载用于标点处理的`punkt`分词器工具。该分词器使用无监督算法将原始文本划分为句子列表。`nltk`包已经包含了一个预先训练的英文`punkt`分词器模型，用于缩写词和搭配词。在使用之前，可以对任何语言的一系列文本进行训练。在后续章节中，我们将讨论如何为Transformer模型训练不同和更高效的分词器。以下代码展示了语言模型目前学到的内容：'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This is the output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The n-gram language model keeps *n-gram* counts and computes the conditional
    probability for sentence generation. `lm=MLE(2)` stands for MLE, which yields
    the maximum probable sentence from each token probability. The following code
    produces a random sentence of 10 words with the `<s>` starting condition given:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram语言模型保留*n-gram*计数，并计算生成句子的条件概率。`lm=MLE(2)`代表最大似然估计，在每个令牌的概率中得出最可能的句子。以下代码使用`<s>`开头条件生成一个包含10个单词的随机句子：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is shown in the following snippet:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can give a specific starting condition through the `text_seed` parameter,
    which makes the generation be conditioned on the preceding context. In our preceding
    example, the preceding context is `<s>`, which is a special token indicating the
    beginning of a sentence.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`text_seed`参数提供特定的起始条件，使得生成受到前文的影响。在我们先前的例子中，前文是`<s>`，这是一个特殊的令牌，表示句子的开头。
- en: So far, we have discussed paradigms underlying traditional NLP models and provided
    very simple implementations with popular frameworks. We are now moving to the
    DL section to discuss how neural language models shaped the field of NLP and how
    neural models overcome the traditional model limitations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了传统NLP模型的基础范式，并用流行的框架提供了非常简单的实现。现在我们将转向DL部分，讨论神经语言模型如何塑造NLP领域，以及神经模型如何克服传统模型的局限性。
- en: Leveraging DL
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用DL
- en: NLP is one of the areas where DL architectures have been widely and successfully
    used. For decades, we have witnessed successful architectures, especially in word
    and sentence representation. In this section, we will share the story of these
    different approaches with commonly used frameworks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是DL架构广泛且成功应用的领域之一。几十年来，在词和句子表示中特别出现了成功的架构。在本节中，我们将分享这些不同方法的故事，并介绍常用的框架。
- en: Learning word embeddings
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习词嵌入
- en: Neural network-based language models effectively solved feature representation
    and language modeling problems since it became possible to train more complex
    neural architecture on much larger datasets to build short and dense representations.
    In 2013, the **Word2vec model**, which is a popular word-embedding technique,
    used a simple and effective architecture to learn a high quality of continuous
    word representations. It outperformed other models for a variety of syntactic
    and semantic language tasks such as *sentiment analysis*, *paraphrase detection*,
    *relation extraction*, and so forth. The other key factor in the popularity of
    the model is its much *lower computational complexity*. It maximizes the probability
    of the current word given any surrounding context words, or vice versa.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 自从能够在更大的数据集上训练更复杂的神经架构以构建短而密集的表示以来，基于神经网络的语言模型有效地解决了特征表示和语言建模问题。2013年，**Word2vec
    模型**，这是一种流行的词嵌入技术，使用了简单而有效的架构来学习高质量的连续词表示。它在各种句法和语义语言任务上的表现优于其他模型，如*情感分析*、*释义检测*、*关系提取*等等。该模型的另一个关键因素是其*更低的计算复杂性*。它最大化了给定任何周围上下文词的当前词的概率，或者反之亦然。
- en: 'The following piece of code illustrates how to train word vectors for the sentences
    of the play *Macbeth*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码段说明了如何为剧作 *麦克白* 的句子训练词向量：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code trains the word embeddings with a vector size of 100 by a sliding
    5-length context window. To visualize the words embeddings, we need to reduce
    the dimension to 3 by applying **Principal Component Analysis** (**PCA**) as shown
    in the following code snippet:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码通过一个长度为 5 的滑动窗口训练具有 100 个向量大小的词嵌入。为了可视化词嵌入，我们需要通过应用**主成分分析**（**PCA**）将维度降低到
    3，如下面的代码片段所示：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is the output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 1.4 – Visualizing word embeddings with PCA ](img/B17123_01_004.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 用 PCA 可视化词嵌入](img/B17123_01_004.jpg)'
- en: Figure 1.4 – Visualizing word embeddings with PCA
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 用 PCA 可视化词嵌入
- en: 'As the plot shows, the main characters of Shakespeare''s play—**Macbeth**,
    **Malcolm**, **Banquo**, **Macduff**, and others—are mapped close to each other.
    Likewise, auxiliary verbs **shall**, **should**, and **would** appear close to
    each other at the left-bottom of *Figure 1.4*. We can also capture an analogy
    such as *man-woman= uncle-aunt* by using an embedding offset. For more interesting
    visual examples on this topic, please check the following project: [https://projector.tensorflow.org/](https://projector.tensorflow.org/).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，莎士比亚剧作的主要角色——**麦克白**、**马尔科姆**、**班克**、**麦克达夫**等——被映射到彼此附近。同样，辅助动词**shall**、**should**
    和 **would** 出现在*图 1.4*的左下方，彼此靠近。我们还可以通过使用嵌入偏移来捕获类似 *man-woman= uncle-aunt* 的类比。有关此主题的更多有趣视觉示例，请查看以下项目：[https://projector.tensorflow.org/](https://projector.tensorflow.org/)。
- en: The Word2vec-like models learn word embeddings by employing a prediction-based
    neural architecture. They employ gradient descent on some objective functions
    and nearby word predictions. While traditional approaches apply a count-based
    method, neural models design a prediction-based architecture for distributional
    semantics. *Are count-based methods or prediction-based methods the best for distributional
    word representations?* The GloVe approach addressed this problem and argued that
    these two approaches are not dramatically different. Jeffrey Penington et al.
    even supported the idea that the count-based methods could be more successful
    by capturing global statistics. They stated that GloVe outperformed other neural
    network language models on word analogy, word similarity, and **Named Entity Recognition**
    (**NER**) tasks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 Word2vec 的模型通过采用基于预测的神经架构来学习词嵌入。它们对一些目标函数和附近词预测进行梯度下降。虽然传统方法采用基于计数的方法，但神经模型设计了一个基于预测的架构用于分布语义。*基于计数的方法还是基于预测的方法对于分布式词表示更好？*
    GloVe 方法解决了这个问题，并认为这两种方法并没有明显的区别。Jeffrey Penington 等人甚至支持基于计数的方法可能更成功，因为它们捕捉了全局统计信息。他们指出
    GloVe 在词类比、词相似性和**命名实体识别**（**NER**）任务上胜过其他神经网络语言模型。
- en: These two paradigms, however, did not provide a helpful solution for unseen
    words and word-sense problems. They do not exploit subword information, and therefore
    cannot learn the embeddings of rare and unseen words.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两种范式对于未知单词和单词语义问题并没有提供有效的解决方案。它们不利用子词信息，因此无法学习稀有和未知单词的嵌入。
- en: '**FastText**, another widely used model, proposed a new enriched approach using
    subword information, where each word is represented as a bag of character n-grams.
    The model sets a constant vector to each character n-gram and represents words
    as the sum of their sub-vectors, which is an idea that was first introduced by
    Hinrich Schütze (*Word Space, 1993*). The model can compute word representations
    even for unseen words and learn the internal structure of words such as suffixes/affixes,
    which is especially important with morphologically rich languages such as Finnish,
    Hungarian, Turkish, Mongolian, Korean, Japanese, Indonesian, and so forth. Currently,
    modern Transformer architectures use a variety of subword tokenization methods
    such as **WordPiece**, **SentencePiece**, or **Byte-Pair Encoding** (**BPE**).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastText**，另一个广泛使用的模型，提出了一种使用子词信息的新的丰富方法，其中每个单词被表示为一组字符n-gram。该模型为每个字符n-gram设置一个常量向量，并将单词表示为其子向量的和，这是Hinrich
    Schütze首次引入的一种想法（*Word Space, 1993*）。模型可以计算即使对于未见过的单词也可以学习单词的内部结构，例如后缀/词缀，这在形态丰富的语言（如芬兰语、匈牙利语、土耳其语、蒙古语、韩语、日语、印尼语等）中尤为重要。当前，现代Transformer架构使用各种子词标记化方法，例如**WordPiece**，**SentencePiece**或**字节对编码**（**BPE**）。'
- en: A brief overview of RNNs
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN的简要概述
- en: 'RNN models can learn each token representation by rolling up the information
    of other tokens at an earlier timestep and learn sentence representation at the
    last timestep. This mechanism has been found beneficial in many ways, outlined
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型可以通过在较早的时间步中滚动其他标记的信息来学习每个标记表示，并在最后一个时间步学习句子表示。这种机制在许多方面都被发现有益，概述如下：
- en: Firstly, RNN can be redesigned in a one-to-many model for language generation
    or music generation.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，RNN 可以在语言生成或音乐生成的一对多模型中进行重新设计。
- en: Secondly, many-to-one models can be used for text classification or sentiment
    analysis.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，多对一模型可用于文本分类或情感分析。
- en: And lastly, many-to-many models are used for NER problems. The second use of
    many-to-many models is to solve encoder-decoder problems such as *machine translation*,
    *question answering*, and *text summarization*.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，多对多模型用于NER问题。多对多模型的第二个用途是解决编码器-解码器问题，例如*机器翻译*，*问答*和*文本摘要*。
- en: As with other neural network models, RNN models take tokens produced by a tokenization
    algorithm that breaks down the entire raw text into atomic units also called tokens.
    Further, it associates the token units with numeric vectors—token embeddings—which
    are learned during the training. As an alternative, we can assign the embedded
    learning task to the well-known word-embedding algorithms such as Word2vec or
    FastText in advance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他神经网络模型一样，RNN模型接受通过令牌化算法生成的标记，该算法将整个原始文本分解为原子单位，也称为标记。此外，它将标记单元与数字向量（标记嵌入）相关联，这些向量在训练期间学习。作为替代方案，我们可以事先将嵌入式学习任务分配给著名的单词嵌入算法，例如Word2vec或FastText。
- en: Here is a simple example of an RNN architecture for the sentence `The cat is
    sad.`, where x0 is the vector embeddings of `the`, x1 is the vector embeddings
    of `cat`, and so forth. *Figure 1.5* illustrates an RNN being unfolded into a
    full **Deep Neural Network** (**DNN**).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是句子`The cat is sad.`的RNN架构的简单示例，其中x0是`the`的向量嵌入，x1是`cat`的向量嵌入，依此类推。*图1.5*说明了一个被展开成完整**深度神经网络**（**DNN**）的RNN。
- en: '`The cat is sad.` sequence, we take care of a sequence of five words. The hidden
    state in each layer acts as the memory of the network. It encodes information
    about what happened in all previous timesteps and in the current timestep. This
    is represented in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`The cat is sad.`序列，我们关心一个包含五个词的序列。每一层的隐藏状态充当网络的记忆。它编码了所有先前时间步和当前时间步发生了什么的信息。这在下图中表示：'
- en: '![Figure 1.5 – An RNN architecture ](img/B17123_01_05.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 - 一个RNN架构](img/B17123_01_05.jpg)'
- en: Figure 1.5 – An RNN architecture
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 - 一个RNN架构
- en: 'The following are some advantages of an RNN architecture:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是RNN架构的一些优势：
- en: '**Variable-length input**: The capacity to work on variable-length input, no
    matter the size of the sentence being input. We can feed the network with sentences
    of 3 or 300 words without changing the parameter.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变长度输入**：具有处理可变长度输入的能力，无论输入的句子大小如何。我们可以将3个或300个词的句子喂给网络而不更改参数。'
- en: '**Caring about word order**: It processes the sequence word by word in order,
    caring about the word position.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注单词顺序**：它按顺序逐个处理序列中的单词，关心单词的位置。'
- en: '**Suitable for working in various modes** (**many-to-many, one-to-many**):
    We can train a machine translation model or sentiment analysis using the same
    recurrency paradigm. Both architectures would be based on an RNN.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于各种模式**（**多对多，一对多**）：我们可以使用相同的循环范式训练机器翻译模型或情感分析。这两种架构都将基于RNN。'
- en: 'The disadvantages of an RNN architecture are listed here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: RNN架构的缺点如下：
- en: '**Long-term dependency problem**: When we process a very long document and
    try to link the terms that are far from each other, we need to care about and
    encode all irrelevant other terms between these terms.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长期依赖问题**：当我们处理一个非常长的文档并尝试链接相距很远的术语时，我们需要关心并编码这些术语之间的所有不相关的其他术语。'
- en: '**Prone to exploding or vanishing gradient problems**: When working on long
    documents, updating the weights of the very first words is a big deal, which makes
    a model untrainable due to a vanishing gradient problem.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容易出现梯度爆炸或消失问题**：当处理长文档时，更新最初几个单词的权重是一个大问题，这会导致模型由于梯度消失问题而无法训练。'
- en: '**Hard to apply parallelizable training**: Parallelization breaks the main
    problem down into a smaller problem and executes the solutions at the same time,
    but RNN follows a classic sequential approach. Each layer strongly depends on
    previous layers, which makes parallelization impossible.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**难以应用并行训练**：并行化将主问题分解为更小的问题，并同时执行解决方案，但是RNN遵循经典的顺序方法。每一层都强烈依赖于前面的层，这使得并行化变得不可能。'
- en: '**The computation is slow as the sequence is long**: An RNN could be very efficient
    for short text problems. It processes longer documents very slowly, besides the
    long-term dependency problem.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算速度随序列长度增加而变慢**：对于短文本问题，RNN可能非常高效。但它在处理长文档时速度非常慢，而且存在长期依赖问题。'
- en: 'Although an RNN can theoretically attend the information at many timesteps
    before, in the real world, problems such as long documents and long-term dependencies
    are impossible to discover. Long sequences are represented within many deep layers.
    These problems have been addressed by many studies, some of which are outlined
    here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上RNN可以在许多时间步骤上处理信息，但在现实世界中，长文档和长期依赖等问题是不可能发现的。长序列在许多深层中被表示。许多研究已经解决了这些问题，其中一些概述如下：
- en: '*Hochreiter and Schmidhuber. Long Short-term Memory. 1997*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hochreiter和Schmidhuber。长短期记忆。1997年*。'
- en: '*Bengio et al. Learning long-term dependencies with gradient descent is difficult.
    1993*.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bengio等人。用梯度下降学习长期依赖性是困难的。1993年*。'
- en: '*K. Cho et al. Learning phrase representations using RNN encoder-decoder for
    statistical machine translation. 2014*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K. Cho等人。使用RNN编码器-解码器学习短语表示的统计机器翻译。2014年*。'
- en: LSTMs and gated recurrent units
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM和门控循环单元
- en: 'LSTM (*Schmidhuber, 1997*) and **Gated Recurrent Units** (**GRUs**) (*Cho,
    2014*) are new variants of RNNs, have solved long-term dependency problems, and
    have attracted great attention. LSTMs were particularly developed to cope with
    the long-term dependency problem. The advantage of an LSTM model is that it uses
    the additional cell state, which is a horizontal sequence line on the top of the
    LSTM unit. This cell state is controlled by special purpose gates for forget,
    insert, or update operations. The complex unit of an LSTM architecture is depicted
    in the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（*Schmidhuber，1997*）和**门控循环单元**（**GRUs**）（*Cho，2014*）是RNN的新变种，已经解决了长期依赖问题，并引起了极大关注。LSTM特别是针对长期依赖问题进行了开发。LSTM模型的优势在于它使用了额外的细胞状态，这是LSTM单元顶部的水平序列线。该细胞状态由专用门控制，用于忘记、插入或更新操作。LSTM架构的复杂单元如下图所示：
- en: '![Figure 1.6 – An LSTM unit ](img/B17123_01_06.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图1.6 – 一个LSTM单元](img/B17123_01_06.jpg)'
- en: Figure 1.6 – An LSTM unit
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 一个LSTM单元
- en: 'It is able to decide the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它能够决定以下内容：
- en: What kind of information we will store in the cell state
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在细胞状态中存储什么样的信息
- en: Which information will be forgotten or deleted
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些信息将被遗忘或删除
- en: In the original RNN, in order to learn the state of I tokens, it recurrently
    processes the entire state of previous tokens between timestep0 and timestepi-1\.
    Carrying entire information from earlier timesteps leads to vanishing gradient
    problems, which makes the model untrainable. The gate mechanism in LSTM allows
    the architecture to skip some unrelated tokens at a certain timestep or remember
    long-range states in order to learn the current token state.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始 RNN 中，为了学习 I 令牌的状态，它会在 timestep0 和 timestepi-1 之间递归处理先前令牌的整个状态。携带来自较早时间步的完整信息会导致梯度消失问题，使得模型无法训练。LSTM
    中的门机制允许架构在某个时间步跳过一些不相关的令牌或记住长期状态以便学习当前令牌状态。
- en: 'A GRU is similar to an LSTM in many ways, the main difference being that a
    GRU does not use the cell state. Rather, the architecture is simplified by transferring
    the functionality of the cell state to the hidden state, and it only includes
    two gates: an *update gate* and a *reset gate*. The update gate determines how
    much information from the previous and current timesteps will be pushed forward.
    This feature helps the model keep relevant information from the past, which minimizes
    the risk of a vanishing gradient problem as well. The reset gate detects the irrelevant
    data and makes the model forget it.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 在很多方面与 LSTM 相似，主要区别在于 GRU 不使用细胞状态。相反，该架构简化了，将细胞状态的功能转移到隐藏状态，并且只包括两个门：一个
    *更新门* 和一个 *重置门*。更新门确定了来自先前和当前时间步的信息将被推送多远。这个特性帮助模型保留了过去的相关信息，从而最小化了梯度消失问题的风险。重置门检测到不相关的数据并使模型忘记它们。
- en: A gentle implementation of LSTM with Keras
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个使用 Keras 的 LSTM 的温和实现
- en: 'We need to download the **Stanford Sentiment Treebank** (**SST-2**) sentiment
    dataset from the **General Language Understanding Evaluation** (**GLUE**) benchmark.
    We can do this by running the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从 **通用语言理解评估**（**GLUE**）基准中下载 **斯坦福情感树库**（**SST-2**）情感数据集。我们可以通过运行以下代码来实现这一点：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Important note
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**SST-2**: This is a fully labeled parse tree that allows for complete sentiment
    analysis in English. The corpus originally consists of about 12K single sentences
    extracted from movie reviews. It was parsed with the Stanford parser and includes
    over 200K unique phrases, each annotated by three human judges. For more information,
    see *Socher et al., Parsing With Compositional Vector Grammars, EMNLP. 2013* ([https://nlp.stanford.edu/sentiment](https://nlp.stanford.edu/sentiment)).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**SST-2**：这是一个完全标记的解析树，允许在英语中进行完整的情感分析。该语料库最初包括约 12K 个从电影评论中提取的单个句子。它使用斯坦福解析器进行解析，并包含由三名人类评审员注释的
    200K 多个唯一短语。有关更多信息，请参见 *Socher 等人，用组合矢量语法解析，EMNLP。2013* ([https://nlp.stanford.edu/sentiment](https://nlp.stanford.edu/sentiment))。'
- en: 'After downloading the data, let''s read it as a pandas object, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下载数据后，让我们将其作为 pandas 对象读取，如下所示：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We need to set maximum sentence length, build vocabulary and dictionaries (`word2idx`,
    `idx2words`), and finally represent each sentence as a list of indexes rather
    than strings. We can do this by running the following code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置最大句子长度，构建词汇表和字典（`word2idx`，`idx2words`），最后将每个句子表示为索引列表而不是字符串。我们可以通过运行以下代码来实现这一点：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Sequences that are shorter than `max_sen_len` (maximum sentence length) are
    padded with a `PAD` value until they are `max_sen_len` in length. On the other
    hand, longer sequences are truncated so that they fit `max_sen_len`. Here is the
    implementation:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 比 `max_sen_len`（最大句子长度）短的序列将使用 `PAD` 值填充，直到它们的长度达到 `max_sen_len`。另一方面，更长的序列将被截断，以使其适合
    `max_sen_len`。以下是实现：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are ready to design and train an LSTM model, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备设计和训练一个 LSTM 模型，如下所示：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model will be trained for 30 epochs. In order to plot what the LSTM model
    has learned so far, we can execute the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将训练 30 个 epochs。为了绘制 LSTM 模型到目前为止学到了什么，我们可以执行以下代码：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The code produces the following plot, which shows us the training and validation
    performance of the LSTM-based text classification:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码生成以下图表，显示了基于 LSTM 的文本分类的训练和验证性能：
- en: '![Figure 1.7 – The classification performance of the LSTM network ](img/B17123_01_07.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – LSTM 网络的分类性能](img/B17123_01_07.jpg)'
- en: Figure 1.7 – The classification performance of the LSTM network
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – LSTM 网络的分类性能
- en: As we mentioned before, the main problem of an RNN-based encoder-decoder model
    is that it produces a single fixed representation for a sequence. However, the
    attention mechanism allowed the RNN to focus on certain parts of the input tokens
    as it maps them to a certain part of the output tokens. This attention mechanism
    has been found to be useful and has become one of the underlying ideas of the
    Transformer architecture. We will discuss how the Transformer architecture takes
    advantage of attention in the next part and throughout the entire book.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，基于 RNN 的编码器-解码器模型的主要问题是它为一个序列生成一个固定的表示。 但是，注意机制使得 RNN 能够专注于输入标记的某些部分，并将它们映射到输出标记的某些部分。
    发现这种注意机制非常有用，并已成为 Transformer 架构的基本思想之一。 我们将在接下来的部分以及整本书的讨论中讨论 Transformer 架构如何利用注意力。
- en: A brief overview of CNNs
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN 的简要概述
- en: 'CNNs, after their success in computer vision, were ported to NLP in terms of
    modeling sentences or tasks such as semantic text classification. A CNN is composed
    of convolution layers followed by a dense neural network in many practices. A
    convolution layer performs over the data in order to extract useful features.
    As with any DL model, a convolution layer plays the feature extraction role to
    automate feature extraction. This feature layer, in the case of NLP, is fed by
    an embedding layer that takes sentences as an input in a one-hot vectorized format.
    The one-hot vectors are generated by a `token-id` for each word forming a sentence.
    The left part of the following screenshot shows a one-hot representation of a
    sentence:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 在计算机视觉方面取得成功后，也被用于 NLP，用于建模句子或语义文本分类等任务。 在许多实践中，CNN 由卷积层组成，然后是一个密集的神经网络。
    卷积层对数据进行处理以提取有用的特征。 与任何 DL 模型一样，卷积层扮演自动化特征提取的角色。 在 NLP 的情况下，这个特征层是由一个嵌入层提供输入，该层将句子以
    one-hot 矢量化格式作为输入。 这些 one-hot 矢量是通过为组成句子的每个单词生成一个`token-id`来生成的。 以下截图的左侧显示了句子的
    one-hot 表示：
- en: '![Figure 1.8 – One-hot vectors ](img/B17123_01_08.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 - One-hot 矢量](img/B17123_01_08.jpg)'
- en: Figure 1.8 – One-hot vectors
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 - One-hot 矢量
- en: 'Each token, represented by a one-hot vector, is fed to the embedding layer.
    The embedding layer can be initialized by random values or by using pre-trained
    word vectors such as GloVe, Word2vec, or FastText. A sentence will then be transformed
    into a dense matrix in the shape of NxE (where **N** is the number of tokens in
    a sentence and **E** is the embedding size). The following screenshot illustrates
    how a 1D CNN processes that dense matrix:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 每个由 one-hot 矢量表示的标记都被送入嵌入层。 嵌入层可以通过随机值或使用预训练的单词向量（如 GloVe、Word2vec 或 FastText）进行初始化。
    然后，该句子将转换为一个 NxE 形状的密集矩阵（其中**N**是句子中标记的数量，**E**是嵌入的大小）。 以下截图说明了 1D CNN 如何处理该密集矩阵：
- en: '![Figure 1.9 – 1D CNN network for a sentence of five tokens ](img/B17123_01_09.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 - 五个标记的句子的 1D CNN 网络](img/B17123_01_09.jpg)'
- en: Figure 1.9 – 1D CNN network for a sentence of five tokens
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 - 五个标记的句子的 1D CNN 网络
- en: 'Convolution will take place on top of this operation with different layers
    and kernels. Hyperparameters for the convolution layer are the kernel size and
    the number of kernels. It is also good to note that 1D convolution is applied
    here and the reason for that is token embeddings cannot be seen as partial, and
    we want to apply kernels capable of seeing multiple tokens in a sequential order
    together. You can see it as something like an n-gram with a specified window.
    Using shallow TL combined with CNN models is also another good capability of such
    models. As shown in the following screenshot, we can also propagate the networks
    with a combination of many representations of tokens, as proposed in the 2014
    study by Yoon Kim, *Convolutional Neural Networks for Sentence Classification*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '卷积将在不同层和核之上进行。 卷积层的超参数是核大小和核的数量。 值得注意的是，这里应用的是 1D 卷积，原因是标记嵌入不能被视为局部的，我们希望应用能够依次看到多个标记的核。
    您可以将其视为具有指定窗口的 n-gram。 使用浅层 TL 结合 CNN 模型也是这些模型的另一个良好的能力。 正如以下截图所示，我们还可以用许多标记的表示的组合来传播网络，正如
    2014 年由 Yoon Kim 提出的研究中所建议的那样，《句子分类的卷积神经网络》:'
- en: '![Figure 1.10 – Combination of many representations in a CNN ](img/B17123_01_10.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.10 - CNN 中许多表示的组合](img/B17123_01_10.jpg)'
- en: Figure 1.10 – Combination of many representations in a CNN
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 - CNN 中许多表示的组合
- en: 'For example, we can use three embedding layers instead of one and concatenate
    them for each token. Given this setup, a token such as **fell** will have a vector
    size of 3x128 if the embedding size is 128 for all three different embeddings.
    These embeddings can be initialized with pre-trained vectors from Word2vec, GloVe,
    and FastText. The convolution operation at each step will see N words with their
    respective three vectors (N is the convolution filter size). The type of convolution
    that is used here is a 1D convolution. The dimension here denotes possible movements
    when doing the operation. For example, a 2D convolution will move along two axes,
    while a 1D convolution just moves along one axis. The following screenshot shows
    the differences between them:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用三个嵌入层而不是一个，并为每个令牌连接它们。在这种设置下，如果所有三个不同的嵌入的大小为128，则诸如**fell**的令牌将具有大小为3x128的向量。这些嵌入可以使用来自Word2vec、GloVe和FastText的预训练向量进行初始化。每一步的卷积运算将使用其各自的三个向量来查看N个单词（N是卷积滤波器的大小）。这里使用的卷积类型是1D卷积。这里的维度表示进行操作时可能的移动。例如，2D卷积将沿两个轴移动，而1D卷积只沿一个轴移动。下面的屏幕截图显示了它们之间的区别：
- en: '![Figure 1.11 – Convolutional directions ](img/B17123_01_11.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11 – 卷积方向](img/B17123_01_11.jpg)'
- en: Figure 1.11 – Convolutional directions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 – 卷积方向
- en: 'The following code snippet is a 1D CNN implementation processing the same data
    used in an LSTM pipeline. It includes a composition of `Conv1D` and `MaxPooling`
    layers, followed by `GlobalMaxPooling` layers. We can extend the pipeline by tweaking
    the parameters and adding more layers to optimize the model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是一个处理与LSTM管道中使用的相同数据的1D CNN实现。它包括`Conv1D`和`MaxPooling`层的组合，以及`GlobalMaxPooling`层。我们可以通过调整参数并添加更多层来扩展该管道以优化模型：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It turns out that the CNN model showed comparable performance with its LSTM
    counterpart. Although CNNs have become a standard in image processing, we have
    seen many successful applications of CNNs for NLP. While an LSTM model is trained
    to recognize patterns across time, a CNN model recognizes patterns across space.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，CNN模型与其LSTM对应物表现出可比较的性能。虽然CNN在图像处理中已成为标准，但我们已经看到了许多成功应用CNN在NLP中。而LSTM模型被训练用于识别跨越时间的模式，CNN模型则识别跨越空间的模式。
- en: Overview of the Transformer architecture
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer架构概述
- en: '**Transformer** models have received immense interest because of their effectiveness
    in an enormous range of NLP problems, from text classification to text generation.
    The attention mechanism is an important part of these models and plays a very
    crucial role. Before Transformer models, the attention mechanism was proposed
    as a helper for improving conventional DL models such as RNNs. To have a good
    understanding of Transformers and their impact on the NLP, we will first study
    the attention mechanism.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer**模型因其在从文本分类到文本生成等巨大范围的NLP问题中的有效性而受到极大关注。注意力机制是这些模型的重要部分，并发挥着非常关键的作用。在Transformer模型出现之前，注意力机制被提出作为帮助改进传统DL模型（如RNNs）的工具。为了更好地理解Transformer及其对NLP的影响，我们将首先研究注意力机制。'
- en: Attention mechanism
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力机制
- en: One of the first variations of the attention mechanism was proposed by *Bahdanau
    et al. (2015)*. This mechanism is based on the fact that RNN-based models such
    as GRUs or LSTMs have an information bottleneck on tasks such as `token-id` and
    process it in a recurrent fashion (encoder). Afterward, the processed intermediate
    representation is fed into another recurrent unit (decoder) to extract the results.
    This avalanche-like information is like a rolling ball that consumes all the information,
    and rolling it out is hard for the decoder part because the decoder part does
    not see all the dependencies and only gets the intermediate representation (context
    vector) as an input.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bahdanau等人（2015年）*提出了注意力机制的第一个变体之一。该机制基于这样一个事实，即基于RNN的模型（如GRUs或LSTMs）在诸如`token-id`之类的任务上存在信息瓶颈，并以递归方式处理它（编码器）。然后，处理过的中间表示被馈送到另一个递归单元（解码器）以提取结果。这种雪崩般的信息就像一个滚动的球，消耗了所有的信息，而将其滚动出来对于解码器部分来说是困难的，因为解码器部分并没有看到所有的依赖关系，只获得中间表示（上下文向量）作为输入。'
- en: 'To align this mechanism, Bahdanau proposed an attention mechanism to use weights
    on intermediate hidden values. These weights align the amount of attention a model
    must pay to input in each decoding step. Such wonderful guidance assists models
    in specific tasks such as NMT, which is a many-to-many task. A diagram of a typical
    attention mechanism is provided here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整这个机制，Bahdanau提出了一种注意机制，在中间隐藏值上使用权重。这些权重可以调整模型在每个解码步骤中对输入的关注程度。这种出色的指导帮助模型在特定任务中，比如NMT（许多对许多任务），做出了很大的帮助。这里提供了一个典型注意机制的示意图：
- en: '![Figure 1.12 – Attention mechanism ](img/B17123_01_12.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图1.12 – 注意机制](img/B17123_01_12.jpg)'
- en: Figure 1.12 – Attention mechanism
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12 – 注意机制
- en: 'Different attention mechanisms have been proposed with different improvements.
    *Additive*, *multiplicative*, *general*, and *dot-product* attention appear within
    the family of these mechanisms. The latter, which is a modified version with a
    scaling parameter, is noted as scaled dot-product attention. This specific attention
    type is the foundation of Transformers models and is called a **multi-head attention
    mechanism**. Additive attention is also what was introduced earlier as a notable
    change in NMT tasks. You can see an overview of the different types of attention
    mechanisms here:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的注意机制提出了不同的改进。 *加性*，*乘性*，*一般* 和 *点积* 注意机制出现在这些机制家族中。后者，这是一个带有缩放参数的修改版本，被称为缩放点积注意力。这种特定的注意类型是Transformer模型的基础，被称为**多头注意力机制**。加性注意力也是之前在NMT任务中引入的一项显着变化。你可以在这里看到不同类型的注意机制的概述：
- en: '![Table 2 – Types of attention mechanisms (Image inspired from https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
    ](img/B17123_01_Table2.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![表2 – 注意机制的类型（图片灵感来自https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html）](img/B17123_01_Table2.jpg)'
- en: Table 2 – Types of attention mechanisms (Image inspired from https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表2 – 注意机制的类型（图片灵感来自https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html）
- en: 'Since attention mechanisms are not specific to NLP, they are also used in different
    use cases in various fields, from computer vision to speech recognition. The following
    screenshot shows a visualization of a multimodal approach trained for neural image
    captioning (*K Xu et al., Show, attend and tell: Neural image caption generation
    with visual attention, 2015*):'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '由于注意机制不仅限于NLP，它们也被用于各种领域的不同用例，从计算机视觉到语音识别。以下截图展示了一个用于神经图像字幕训练的多模态方法的可视化（*K
    Xu等，Show, attend and tell: Neural image caption generation with visual attention,
    2015*）：'
- en: '![Figure 1.13 – Attention mechanism in computer vision ](img/B17123_01_13.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图1.13 – 计算机视觉中的注意机制](img/B17123_01_13.jpg)'
- en: Figure 1.13 – Attention mechanism in computer vision
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13 – 计算机视觉中的注意机制
- en: 'The multi-head attention mechanism that is shown in the following diagram is
    an essential part of the Transformer architecture:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下图所示的多头注意机制是Transformer架构中的一个重要部分：
- en: '![Figure 1.14 – Multi-head attention mechanism ](img/B17123_01_14.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图1.14 – 多头注意机制](img/B17123_01_14.jpg)'
- en: Figure 1.14 – Multi-head attention mechanism
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14 – 多头注意机制
- en: Next, let's understand multi-head attention mechanisms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解多头注意机制。
- en: Multi-head attention mechanisms
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意机制
- en: 'Before jumping into scaled dot-product attention mechanisms, it''s better to
    get a good understanding of self-attention. **Self-attention**, as shown in *Figure
    1.15*, is a basic form of a scaled self-attention mechanism. This mechanism uses
    an input matrix shown as *X* and produces an attention score between various items
    in *X*. We see *X* as a 3x4 matrix where 3 represents the number of tokens and
    4 presents the embedding size. *Q* from *Figure 1.15* is also known as the **query**,
    *K* is known as the **key**, and *V* is noted as the **value**. Three types of
    matrices shown as *theta*, *phi*, and *g* are multiplied by *X* before producing
    *Q*, *K*, and *V*. The multiplied result between query (*Q*) and key (*K*) yields
    an attention score matrix. This can also be seen as a database where we use the
    query and keys in order to find out how much various items are related in terms
    of numeric evaluation. Multiplication of the attention score and the *V* matrix
    produces the final result of this type of attention mechanism. The main reason
    for it being called **self-attention** is because of its unified input *X*; *Q*,
    *K*, and *V* are computed from *X*. You can see all this depicted in the following
    diagram:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究缩放点积注意力机制之前，最好先对自注意力有一个良好的理解。如*图1.15*所示，**自注意力**是一种缩放自注意力机制的基本形式。该机制使用一个显示为*X*的输入矩阵，并在*X*中的各个项目之间产生注意力分数。我们将*X*视为一个3x4矩阵，其中3代表令牌的数量，4代表嵌入大小。*图1.15*中的*Q*也称为**查询**，*K*称为**键**，*V*称为**值**。在产生*Q*、*K*和*V*之前，三种类型的矩阵被称为*theta*、*phi*和*g*，它们与*X*相乘。查询（*Q*）和键（*K*）之间的乘积结果产生一个注意力分数矩阵。这也可以看作是一个数据库，我们使用查询和键来了解各种项目在数值评估方面的关联程度。注意力分数和*V*矩阵的乘积产生这种类型的注意力机制的最终结果。其被称为**自注意力**的主要原因是因为它的统一输入*X*;
    *Q*、*K*和*V*是从*X*计算出来的。你可以在下图中看到所有这些的描述：
- en: '![Figure 1.15 – Mathematical representation for the attention mechanism (Image
    inspired from https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp)
    ](img/B17123_01_15.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图1.15 – 注意力机制的数学表示（图片来源于 https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp）](img/B17123_01_15.jpg)'
- en: Figure 1.15 – Mathematical representation for the attention mechanism (Image
    inspired from https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp)
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15 – 注意力机制的数学表示（图片来源于 https://blogs.oracle.com/datascience/multi-head-self-attention-in-nlp）
- en: 'A scaled dot-product attention mechanism is very similar to a self-attention
    (dot-product) mechanism except it uses a scaling factor. The multi-head part,
    on the other hand, ensures the model is capable of looking at various aspects
    of input at all levels. Transformer models attend to encoder annotations and the
    hidden values from past layers. The architecture of the Transformer model does
    not have a recurrent step-by-step flow; instead, it uses positional encoding in
    order to have information about the position of each token in the input sequence.
    The concatenated values of the embeddings (randomly initialized) and the fixed
    values of positional encoding are the input fed into the layers in the first encoder
    part and are propagated through the architecture, as illustrated in the following
    diagram:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缩放点积注意力机制与自注意力（点积）机制非常相似，只是它使用了一个缩放因子。另一方面，多头部分确保模型能够在各个层面上查看输入的各个方面。Transformer
    模型关注编码器注释和来自过去层的隐藏值。Transformer 模型的架构没有逐步流程; 相反，它使用位置编码来获取有关输入序列中每个令牌位置的信息。嵌入值（随机初始化）和位置编码的固定值的串联值是输入传递到第一个编码器部分中的层，并通过体系结构传播，如下图所示：
- en: '![Figure 1.16 – A Transformer ](img/B17123_01_16.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图1.16 – 一个 Transformer](img/B17123_01_16.jpg)'
- en: Figure 1.16 – A Transformer
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16 – 一个 Transformer
- en: 'The positional information is obtained by evaluating sine and cosine waves
    at different frequencies. An example of positional encoding is visualized in the
    following screenshot:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通过评估不同频率的正弦和余弦波来获取位置信息。位置编码的一个示例在以下截图中可视化：
- en: '![Figure 1.17 – Positional encoding (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
    ](img/B17123_01_17.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图1.17 – 位置编码（图片来源于 http://jalammar.github.io/illustrated-Transformer/）](img/B17123_01_17.jpg)'
- en: Figure 1.17 – Positional encoding (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17 – 位置编码（图片来源于 http://jalammar.github.io/illustrated-Transformer/）
- en: 'A good example of performance on the Transformer architecture and the scaled
    dot-product attention mechanism is given in the following popular screenshot:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的流行截图中给出了 Transformer 架构和缩放点积注意力机制的性能的一个很好的例子：
- en: '![Figure 1.18 – Attention mapping for Transformers (Image inspired from https://ai.googleblog.com/2017/08/Transformer-novel-neural-network.html)
    ](img/B17123_01_18.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.18 – Transformer 的注意力映射（图片灵感来自 https://ai.googleblog.com/2017/08/Transformer-novel-neural-network.html）](img/B17123_01_18.jpg)'
- en: Figure 1.18 – Attention mapping for Transformers (Image inspired from https://ai.googleblog.com/2017/08/Transformer-novel-neural-network.html)
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18 – Transformer 的注意力映射（图片灵感来自 https://ai.googleblog.com/2017/08/Transformer-novel-neural-network.html）
- en: 'The word **it** refers to different entities in different contexts, as is seen
    from the preceding screenshot. Another improvement made by using a Transformer
    architecture is in parallelism. Conventional sequential recurrent models such
    as LSTMs and GRUs do not have such capabilities because they process the input
    token by token. Feed-forward layers, on the other hand, speed up a bit more because
    single matrix multiplication is far faster than a recurrent unit. Stacks of multi-head
    attention layers gain a better understanding of complex sentences. A good visual
    example of a multi-head attention mechanism is shown in the following screenshot:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**它** 这个词在不同的语境中指代不同的实体，正如前面的截图所示。使用 Transformer 架构的另一个改进在于并行性。传统的顺序循环模型如 LSTM
    和 GRU 并没有这样的能力，因为它们逐个处理输入标记。另一方面，前馈层的速度会更快一些，因为单个矩阵乘法比循环单元要快得多。多头注意力层的堆叠可以更好地理解复杂的句子。一个很好的多头注意力机制的可视化示例如下截图所示：'
- en: '![Figure 1.19 – Multi-head attention mechanism (Image inspired from https://imgur.com/gallery/FBQqrxw)
    ](img/B17123_01_19.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.19 – 多头注意力机制（图片灵感来自 https://imgur.com/gallery/FBQqrxw）](img/B17123_01_19.jpg)'
- en: Figure 1.19 – Multi-head attention mechanism (Image inspired from https://imgur.com/gallery/FBQqrxw)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19 – 多头注意力机制（图片灵感来自 https://imgur.com/gallery/FBQqrxw）
- en: On the decoder side of the attention mechanism, a very similar approach to the
    encoder is utilized with small modifications. A multi-head attention mechanism
    is the same, but the output of the encoder stack is also used. This encoding is
    given to each decoder stack in the second multi-head attention layer. This little
    modification introduces the output of the encoder stack while decoding. This modification
    lets the model be aware of the encoder output while decoding and at the same time
    help it during training to have a better gradient flow over various layers. The
    final softmax layer at the end of the decoder layer is used to provide outputs
    for various use cases such as NMT, for which the original Transformer architecture
    was introduced.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制的解码器侧，采用了与编码器非常相似的方法，但有一些小的修改。多头注意力机制是相同的，但也使用了编码器堆栈的输出。这个编码被提供给第二个多头注意力层中的每个解码器堆栈。这个小修改在解码时引入了编码器堆栈的输出。这个修改让模型在解码时意识到编码器输出，并同时在训练期间帮助它在各个层之间有更好的梯度流动。解码器层末端的最终
    softmax 层用于为各种用例提供输出，例如原始 Transformer 架构引入的 NMT。
- en: This architecture has two inputs, noted as inputs and outputs (shifted right).
    One is always present (the inputs) in both training and inference, while the other
    is just present in training and in inference, which is produced by the model.
    The reason we do not use model predictions in inference is to stop the model from
    going too wrong by itself. But what does it mean? Imagine a neural translation
    model trying to translate a sentence from English to French—at each step, it makes
    a prediction for a word, and it uses that predicted word to predict the next one.
    But if it goes wrong at some step, all the following predictions will be wrong
    too. To stop the model from going wrong like this, we provide the correct words
    as a shifted-right version.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构有两个输入，分别标注为输入和输出（向右移动）。一个始终存在（输入），无论是训练还是推断，而另一个只存在于训练和推断中，它由模型产生。我们之所以不在推断中使用模型预测，是为了防止模型自行出错。但这是什么意思呢？想象一个神经机器翻译模型试图将一句英文翻译成法文——在每个步骤中，它都会对一个词进行预测，并使用该预测的词来预测下一个词。但如果在某个步骤出错了，那么后面的所有预测都会错误。为了防止模型像这样出错，我们提供正确的词作为右移版本。
- en: 'A visual example of a Transformer model is given in the following diagram.
    It shows a Transformer model with two encoders and two decoder layers. The **Add
    & Normalize** layer from this diagram adds and normalizes the input it takes from
    the **Feed Forward** layer:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了一个 Transformer 模型的可视化示例。它展示了一个具有两个编码器和两个解码器层的 Transformer 模型。这张图中的 **加法
    & 标准化** 层从这张图中的 **前馈** 层接收输入后添加和标准化它：
- en: '![Figure 1.20 – Transformer model (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
    ](img/B17123_01_20.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.20 – Transformer 模型（图片灵感来自 http://jalammar.github.io/illustrated-Transformer/）](img/B17123_01_20.jpg)'
- en: Figure 1.20 – Transformer model (Image inspired from http://jalammar.github.io/illustrated-Transformer/)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20 – Transformer 模型（图片灵感来自 http://jalammar.github.io/illustrated-Transformer/）
- en: Another major improvement that is used by a Transformer-based architecture is
    based on a simple universal text-compression scheme to prevent unseen tokens on
    the input side. This approach, which takes place by using different methods such
    as byte-pair encoding and sentence-piece encoding, improves a Transformer's performance
    in dealing with unseen tokens. It also guides the model when the model encounters
    morphologically close tokens. Such tokens were unseen in the past and are rarely
    used in the training, and yet, an inference might be seen. In some cases, chunks
    of it are seen in training; the latter happens in the case of morphologically
    rich languages such as Turkish, German, Czech, and Latvian. For example, a model
    might see the word *training* but not *trainings*. In such cases, it can tokenize
    *trainings* as *training+s*. These two are commonly seen when we look at them
    as two parts.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个由基于 Transformer 架构使用的主要改进是基于一种简单的通用文本压缩方案，以防止输入端出现未见标记。这种方法通过使用不同的方法（如字节对编码和句子片段编码）进行，提高了
    Transformer 在处理未见标记时的性能。它还在模型遇到形态接近的标记时指导模型。这样的标记在过去是不可见的，并且在训练中很少使用，然而，在推理中可能会看到。在某些情况下，训练中会看到其部分内容；在形态丰富的语言（如土耳其语、德语、捷克语和拉脱维亚语）的情况下会发生后者。例如，模型可能看到单词
    *training*，但没有看到 *trainings*。在这种情况下，它可以将 *trainings* 标记为 *training+s*。当我们将它们视为两个部分时，这两者是常见的。
- en: Transformer-based models have quite common characteristics—for example, they
    are all based on this original architecture with differences in which steps they
    use and don't use. In some cases, minor differences are made—for example, improvements
    to the multi-head attention mechanism taking place.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的模型具有相当普遍的特征——例如，它们都是基于这种原始架构的，不同之处在于它们使用和不使用的步骤。在某些情况下，会做出较小的改进——例如，改进多头注意力机制。
- en: Using TL with Transformers
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Transformer 进行 TL
- en: '**TL** is a field of **Artificial Intelligence** (**AI**) and ML that aims
    to make models reusable for different tasks—for example, a model trained on a
    given task such as *A* is reusable (fine-tuning) on a different task such as *B*.
    In an NLP field, this is achievable by using Transformer-like architectures that
    can capture the understanding of language itself by language modeling. Such models
    are called language models—they provide a model for the language they have been
    trained on. TL is not a new technique, and it has been used in various fields
    such as computer vision. ResNet, Inception, VGG, and EfficientNet are examples
    of such models that can be used as pre-trained models able to be fine-tuned on
    different computer-vision tasks.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**TL** 是人工智能（**AI**）和机器学习（ML）的一个领域，旨在使模型可在不同任务中重用，例如，在给定任务（如 *A*）上训练的模型可在不同任务（如
    *B*）上重用（微调）。在 NLP 领域，通过使用可以捕获语言理解的 Transformer-like 架构来实现这一目标。这种模型称为语言模型——它们为其训练的语言提供了一个模型。TL
    不是一种新技术，它已经被用于各种领域，如计算机视觉。ResNet、Inception、VGG 和 EfficientNet 是可以用作预训练模型的示例，可在不同的计算机视觉任务上进行微调。'
- en: Shallow TL using models such as *Word2vec*, *GloVe*, and *Doc2vec* is also possible
    in NLP. It is called *shallow* because there is no model behind this kind of TL
    and instead, the pre-trained vectors for words/tokens are utilized. You can use
    these token- or document-embedding models followed by a classifier or use them
    combined with other models such as RNNs instead of using random embeddings.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 浅层 TL 使用诸如 *Word2vec*、*GloVe* 和 *Doc2vec* 这样的模型在 NLP 中也是可能的。它被称为 *浅层*，因为这种 TL
    背后没有模型，而是利用了预训练的单词/标记向量。你可以使用这些标记或文档嵌入模型，接着使用分类器，或者将它们与其他模型（如 RNNs）结合使用，而不是使用随机嵌入。
- en: TL in NLP using Transformer models is also possible because these models can
    learn a language itself without any labeled data. Language modeling is a task
    used to train transferable weights for various problems. Masked language modeling
    is one of the methods used to learn a language itself. As with Word2vec's window-based
    model for predicting center tokens, in masked language modeling, a similar approach
    takes place, with key differences. Given a probability, each word is masked and
    replaced with a special token such as *[MASK]*. The language model (a Transformer-based
    model, in our case) must predict the masked words. Instead of using a window,
    unlike with Word2vec, a whole sentence is given, and the output of the model must
    be the same sentence with masked words filled.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中使用 Transformer 模型进行 TL 也是可能的，因为这些模型可以在没有任何标注数据的情况下学习一门语言本身。语言建模是一种用于为各种问题训练可转移权重的任务。掩码语言建模是用于学习一门语言本身的方法之一。与
    Word2vec 的基于窗口的模型预测中心词元相似，掩码语言建模采用类似的方法，但有关键差异。给定一个概率，每个词都被掩码并替换为特殊标记，如 *[MASK]*。语言模型（在我们的情况下是基于
    Transformer 的模型）必须预测被掩码的词。与 Word2vec 不同，不是使用一个窗口，而是给出整个句子，模型的输出必须是相同的带有掩码词的句子。
- en: One of the first models that used the Transformer architecture for language
    modeling is **BERT**, which is based on the encoder part of the Transformer architecture.
    Masked language modeling is accomplished by BERT by using the same method described
    before and after training a language model. BERT is a transferable language model
    for different NLP tasks such as token classification, sequence classification,
    or even question answering.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Transformer 架构进行语言建模的第一个模型之一是**BERT**，它基于 Transformer 架构的编码器部分。通过在训练语言模型之前和之后使用相同的方法，BERT
    完成了掩码语言建模。BERT 是一个可转移的语言模型，适用于不同的 NLP 任务，如标记分类、序列分类，甚至问答任务。
- en: 'Each of these tasks is a fine-tuning task for BERT once a language model is
    trained. BERT is best known for its key characteristics on the base Transformer
    encoder model, and by altering these characteristics, different versions of it—small,
    tiny, base, large, and extra-large—are proposed. Contextual embedding enables
    a model to have the correct meaning of each word based on the context in which
    it is given—for example, the word *Cold* can have different meanings in two different
    sentences: *Cold-hearted killer* and *Cold weather*. The number of layers at the
    encoder part, the input dimension, the output embedding dimension, and the number
    of multi-head attention mechanisms are these key characteristics, as illustrated
    in the following screenshot:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个任务都是对 BERT 进行微调的任务，一旦一个语言模型被训练完成。BERT 最为人所知的是其在基础 Transformer 编码器模型上的关键特性，通过改变这些特性，提出了不同版本的它——小型、微型、基础、大型和超大型。上下文嵌入使得模型能够根据所处的上下文正确理解每个单词的含义——例如，单词
    *冷* 在两个不同的句子中可能有不同的含义：*冷酷无情的杀手* 和 *寒冷的天气*。编码器部分的层数、输入维度、输出嵌入维度和多头注意机制的数量是这些关键特性，如下面的截图所示：
- en: '![Figure 1.21 – Pre-training and fine-tuning procedures for BERT (Image inspired
    from J. Devlin et al., Bert: Pre-training of deep bidirectional Transformers for
    language understanding, 2018) ](img/B17123_01_21.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.21 – BERT 的预训练和微调流程（图片灵感来自 J. Devlin 等人，《Bert: Pre-training of deep bidirectional
    Transformers for language understanding》，2018年）](img/B17123_01_21.jpg)'
- en: 'Figure 1.21 – Pre-training and fine-tuning procedures for BERT (Image inspired
    from J. Devlin et al., Bert: Pre-training of deep bidirectional Transformers for
    language understanding, 2018)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1.21 – BERT 的预训练和微调流程（图片灵感来自 J. Devlin 等人，《Bert: Pre-training of deep bidirectional
    Transformers for language understanding》，2018年）'
- en: As you can see in *Figure 1.21*, the pre-training phase also consists of another
    objective known as **next-sentence prediction**. As we know, each document is
    composed of sentences followed by each other, and another important part of training
    for a model to grasp the language is to understand the relations of sentences
    to each other—in other words, whether they are related or not. To achieve these
    tasks, BERT introduced special tokens such as *[CLS]* and *[SEP]*. A *[CLS]* token
    is an initially meaningless token used as a start token for all tasks, and it
    contains all information about the sentence. In sequence-classification tasks
    such as NSP, a classifier on top of the output of this token (output position
    of *0*) is used. It is also useful in evaluating the sense of a sentence or capturing
    its semantics—for example, when using a Siamese BERT model, comparing these two
    *[CLS]* tokens for different sentences by a metric such as cosine-similarity is
    very helpful. On the other hand, *[SEP]* is used to distinguish between two sentences,
    and it is only used to separate two sentences. After pre-training, if someone
    aims to fine-tune BERT on a sequence-classification task such as sentiment analysis,
    which is a sequence-classification task, they will use a classifier on top of
    the output embedding of *[CLS]*. It is also notable that all TL models can be
    frozen during fine-tuning or freed; frozen means seeing all weights and biases
    inside the model as constants and stopping training on them. In the example of
    sentiment analysis, just the classifier will be trained, not the model if it is
    frozen.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在*图1.21*中，预训练阶段还包括另一个称为**下一句预测**的目标。我们知道，每个文档由相互跟随的句子组成，而模型理解语言的另一个重要部分是理解句子之间的关系，换句话说，它们是否相关。为了完成这些任务，BERT引入了特殊的标记，如*[CLS]*和*[SEP]*。*[CLS]*标记是一个最初没有意义的标记，用作所有任务的起始标记，并包含关于句子的所有信息。在诸如NSP之类的序列分类任务中，会在此标记的输出（*0*位置的输出）之上使用分类器。它还有助于评估句子的意义或捕获其语义，例如，当使用孪生BERT模型时，通过诸如余弦相似度之类的度量来比较不同句子的这两个*[CLS]*标记非常有帮助。另一方面，*[SEP]*用于区分两个句子，它仅用于分隔两个句子。在预训练之后，如果有人打算在情感分析等序列分类任务上对BERT进行微调，那么他们将在*[CLS]*的输出嵌入之上使用一个分类器。值得注意的是，在微调期间，所有TL模型都可以被冻结或释放；冻结意味着将模型内的所有权重和偏差视为常量，并停止对它们进行训练。在情感分析的示例中，如果模型被冻结，只有分类器会被训练，而不是模型。
- en: Summary
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: With this, we now come to the end of the chapter. You should now have an understanding
    of the evolution of NLP methods and approaches, from BoW to Transformers. We looked
    at how to implement BoW-, RNN-, and CNN-based approaches and understood what Word2vec
    is and how it helps improve the conventional DL-based methods using shallow TL.
    We also looked into the foundation of the Transformer architecture, with BERT
    as an example. By the end of the chapter, we had learned about TL and how it is
    utilized by BERT.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一章，我们现在来到了结束。你现在应该对NLP方法和方法的演变有所了解，从BoW到Transformers。我们看了如何实现基于BoW、RNN和CNN的方法，并了解了Word2vec是什么，以及它如何通过浅层TL改进传统的DL方法。我们还深入了解了Transformer架构的基础，以BERT为例。到本章结束时，我们已经了解了TL以及BERT如何利用它。
- en: At this point, we have learned basic information that is necessary to continue
    to the next chapters. We understood the main idea behind Transformer-based architectures
    and how TL can be applied using this architecture.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '到目前为止，我们已经学习了继续阅读下一章所必需的基本信息。我们了解了基于Transformer的架构的主要思想以及如何使用此架构应用TL。  '
- en: In the next section, we will see how it is possible to run a simple Transformer
    example from scratch. The related information about the installation steps will
    be given, and working with datasets and benchmarks is also investigated in detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何从头开始运行一个简单的Transformer示例。将提供有关安装步骤的相关信息，并且还将详细调查与数据集和基准的工作。
- en: References
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Mikolov, T., Chen, K., Corrado, G. & Dean, J. (2013). Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781.*'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mikolov, T., Chen, K., Corrado, G. & Dean, J. (2013). Efficient estimation
    of word representations in vector space. arXiv preprint arXiv:1301.3781.*'
- en: '*Bahdanau, D., Cho, K. & Bengio, Y. (2014). Neural machine translation by jointly
    learning to align and translate. arXiv preprint arXiv:1409.0473.*'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bahdanau, D., Cho, K. & Bengio, Y. (2014). Neural machine translation by jointly
    learning to align and translate. arXiv preprint arXiv:1409.0473.*'
- en: '*Pennington, J., Socher, R. & Manning, C. D. (2014, October). GloVe: Global
    vectors for word representation. In Proceedings of the 2014 conference on empirical
    methods in natural language processing (EMNLP) (pp. 1532-1543).*'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Pennington, J., Socher, R. & Manning, C. D. (2014, 十月). GloVe: 用于词表示的全局向量.
    在2014年自然语言处理会议（EMNLP）论文集中的论文 (pp. 1532-1543).*'
- en: '*Hochreiter, S. & Schmidhuber, J. (1997). Long short-term memory. Neural computation,
    9(8), 1735-1780.*'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hochreiter, S. & Schmidhuber, J. (1997). 长短期记忆网络. 神经计算, 9(8), 1735-1780.*'
- en: '*Bengio, Y., Simard, P, & Frasconi, P. (1994). Learning long-term dependencies
    with gradient descent is difficult. IEEE transactions on neural networks, 5(2),
    157-166.*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bengio, Y., Simard, P, & Frasconi, P. (1994). 使用梯度下降学习长期依赖关系困难. IEEE 神经网络交易,
    5(2), 157-166.*'
- en: '*Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
    H. & Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder
    for statistical machine translation. arXiv preprint arXiv:1406.1078.*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk,
    H. & Bengio, Y. (2014). 使用 RNN 编码器-解码器学习短语表示进行统计机器翻译. arXiv 预印本 arXiv:1406.1078.*'
- en: '*Kim, Y. (2014). Convolutional neural networks for sentence classification.
    CoRR abs/1408.5882 (2014). arXiv preprint arXiv:1408.5882.*'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kim, Y. (2014). 句子分类的卷积神经网络. CoRR abs/1408.5882 (2014). arXiv 预印本 arXiv:1408.5882.*'
- en: '*Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
    N. & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.*'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.
    N. & Polosukhin, I. (2017). 注意力就是一切. arXiv 预印本 arXiv:1706.03762.*'
- en: '*Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: Pre-training
    of deep bidirectional Transformers for language understanding. arXiv preprint
    arXiv:1810.04805.*'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). Bert: 深度双向 Transformer
    的预训练用于语言理解. arXiv 预印本 arXiv:1810.04805.*'
