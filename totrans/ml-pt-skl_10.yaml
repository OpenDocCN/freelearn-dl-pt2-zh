- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Working with Unlabeled Data – Clustering Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用未标记数据进行工作——聚类分析
- en: In the previous chapters, we used supervised learning techniques to build machine
    learning models, using data where the answer was already known—the class labels
    were already available in our training data. In this chapter, we will switch gears
    and explore cluster analysis, a category of **unsupervised learning** techniques
    that allows us to discover hidden structures in data where we do not know the
    right answer upfront. The goal of **clustering** is to find a natural grouping
    in data so that items in the same cluster are more similar to each other than
    to those from different clusters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用监督学习技术构建机器学习模型，使用的数据是已知答案的数据——在我们的训练数据中，类标签已经是可用的。在本章中，我们将转向探索聚类分析，这是一类**无监督学习**技术，允许我们在数据中发现隐藏的结构，我们并不预先知道正确答案。**聚类**的目标是在数据中找到自然的分组，使得同一聚类中的项彼此之间的相似性比与不同聚类中的项更高。
- en: 'Given its exploratory nature, clustering is an exciting topic, and in this
    chapter, you will learn about the following concepts, which can help us to organize
    data into meaningful structures:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于其探索性质，聚类是一个令人兴奋的话题，在本章中，您将学习以下概念，这些概念可以帮助我们将数据组织成有意义的结构：
- en: Finding centers of similarity using the popular **k-means** algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流行的**k-means**算法找到相似性中心
- en: Taking a bottom-up approach to building hierarchical clustering trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用自下而上的方法构建层次聚类树
- en: Identifying arbitrary shapes of objects using a density-based clustering approach
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于密度的聚类方法识别对象的任意形状
- en: Grouping objects by similarity using k-means
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k-means 将对象按相似性分组
- en: In this section, we will learn about one of the most popular clustering algorithms,
    k-means, which is widely used in academia as well as in industry. Clustering (or
    cluster analysis) is a technique that allows us to find groups of similar objects
    that are more related to each other than to objects in other groups. Examples
    of business-oriented applications of clustering include the grouping of documents,
    music, and movies by different topics, or finding customers that share similar
    interests based on common purchase behaviors as a basis for recommendation engines.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习其中一种最流行的聚类算法之一，即 k-means，它在学术界以及工业界广泛使用。聚类（或聚类分析）是一种技术，允许我们找到彼此相关性更高的相似对象组。聚类的业务应用示例包括按不同主题对文档、音乐和电影进行分组，或者基于共同购买行为找到具有相似兴趣的客户作为推荐引擎的基础。
- en: k-means clustering using scikit-learn
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 进行 k-means 聚类
- en: As you will see in a moment, the k-means algorithm is extremely easy to implement,
    but it is also computationally very efficient compared to other clustering algorithms,
    which might explain its popularity. The k-means algorithm belongs to the category
    of **prototype-based clustering**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在接下来看到的，k-means 算法非常易于实现，但与其他聚类算法相比，在计算效率上也非常高，这也许解释了它的流行性。k-means 算法属于**原型聚类**的范畴。
- en: We will discuss two other categories of clustering, **hierarchical** and **density-based
    clustering**, later in this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后将讨论另外两种聚类方法，**层次聚类**和**基于密度的聚类**，在本章的后面部分。
- en: Prototype-based clustering means that each cluster is represented by a prototype,
    which is usually either the **centroid** (*average*) of similar points with continuous
    features, or the **medoid** (the most *representative* or the point that minimizes
    the distance to all other points that belong to a particular cluster) in the case
    of categorical features. While k-means is very good at identifying clusters with
    a spherical shape, one of the drawbacks of this clustering algorithm is that we
    have to specify the number of clusters, *k*, *a priori*. An inappropriate choice
    for *k* can result in poor clustering performance. Later in this chapter, we will
    discuss the **elbow** method and **silhouette plots**, which are useful techniques
    to evaluate the quality of a clustering to help us determine the optimal number
    of clusters, *k*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 原型聚类意味着每个聚类由一个原型表示，通常是具有连续特征的相似点的**质心**（*平均值*），或者在分类特征情况下的**中心点**（最*具代表性*或者最小化到属于特定聚类的所有其他点之间距离的点）。虽然
    k-means 在识别球形聚类方面非常出色，但这种聚类算法的缺点之一是我们必须预先指定聚类数*k*。不恰当的*k*选择可能导致聚类性能不佳。本章后面，我们将讨论**肘部**方法和**轮廓图**，这些是评估聚类质量的有用技术，帮助我们确定最优聚类数*k*。
- en: 'Although k-means clustering can be applied to data in higher dimensions, we
    will walk through the following examples using a simple two-dimensional dataset
    for the purpose of visualization:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然k均值聚类可以应用于高维数据，但出于可视化目的，我们将通过一个简单的二维数据集来演示以下示例：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset that we just created consists of 150 randomly generated points
    that are roughly grouped into three regions with higher density, which is visualized
    via a two-dimensional scatterplot:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的数据集包含了150个随机生成的点，大致分成了三个密度较高的区域，这通过二维散点图进行了可视化：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_10_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![Chart, scatter chart  Description automatically generated](img/B17582_10_01.png)'
- en: 'Figure 10.1: A scatterplot of our unlabeled dataset'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：我们未标记数据集的散点图
- en: 'In real-world applications of clustering, we do not have any ground-truth category
    information (information provided as empirical evidence as opposed to inference)
    about those examples; if we were given class labels, this task would fall into
    the category of supervised learning. Thus, our goal is to group the examples based
    on their feature similarities, which can be achieved using the k-means algorithm,
    as summarized by the following four steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类的实际应用中，我们没有任何关于这些示例的地面真实类别信息（作为实证证据而非推断提供的信息）；如果我们有类标签，这个任务就属于监督学习的范畴。因此，我们的目标是根据它们的特征相似性对这些示例进行分组，这可以通过使用k均值算法来实现，如下所总结的四个步骤：
- en: Randomly pick *k* centroids from the examples as initial cluster centers
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从示例中随机选择*k*个质心作为初始聚类中心
- en: Assign each example to the nearest centroid, ![](img/B17582_10_001.png)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个示例分配到最近的质心，![](img/B17582_10_001.png)
- en: Move the centroids to the center of the examples that were assigned to it
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将质心移动到分配给它的示例的中心
- en: Repeat *steps 2* and *3* until the cluster assignments do not change or a user-defined
    tolerance or maximum number of iterations is reached
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*和*3*直到簇分配不再改变或达到用户定义的容差或最大迭代次数
- en: 'Now, the next question is, *how do we measure similarity between objects*?
    We can define similarity as the opposite of distance, and a commonly used distance
    for clustering examples with continuous features is the **squared Euclidean distance**
    between two points, **x** and **y**, in *m*-dimensional space:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，下一个问题是，*我们如何衡量对象之间的相似性*？我们可以将相似性定义为距离的相反数，对于具有连续特征的聚类示例，常用的距离是**欧氏距离的平方**，即在*m*维空间中两点**x**和**y**之间的距离：
- en: '![](img/B17582_10_002.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_002.png)'
- en: Note that, in the preceding equation, the index *j* refers to the *j*th dimension
    (feature column) of the example inputs, **x** and **y**. In the rest of this section,
    we will use the superscripts *i* and *j* to refer to the index of the example
    (data record) and cluster index, respectively.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前述方程中，指数*j*指的是示例输入的第*j*维（特征列），**x**和**y**。在本节的其余部分，我们将使用上标*i*和*j*来分别指代示例（数据记录）和簇索引的索引。
- en: 'Based on this Euclidean distance metric, we can describe the k-means algorithm
    as a simple optimization problem, an iterative approach for minimizing the within-cluster
    **sum of squared errors** (**SSE**), which is sometimes also called **cluster
    inertia**:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个欧氏距离度量，我们可以将k均值算法描述为一个简单的优化问题，这是一种迭代方法，用于最小化**簇内平方误差和**（**SSE**），有时也称为**簇惯性**：
- en: '![](img/B17582_10_003.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_003.png)'
- en: Here, ![](img/B17582_10_004.png) is the representative point (centroid) for
    cluster *j*. *w*^(^i^(, )^j^) = 1 if the example, **x**^(^i^), is in cluster *j*,
    or 0 otherwise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B17582_10_004.png)是簇*j*的代表点（质心）。*w*^(^i^(, )^j^) = 1如果示例**x**^(^i^)在簇*j*中，否则为0。
- en: '![](img/B17582_10_005.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_005.png)'
- en: 'Now that you have learned how the simple k-means algorithm works, let’s apply
    it to our example dataset using the `KMeans` class from scikit-learn’s `cluster`
    module:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了简单的k均值算法的工作原理，让我们使用scikit-learn的`cluster`模块中的`KMeans`类将其应用于我们的示例数据集：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the preceding code, we set the number of desired clusters to `3`; having
    to specify the number of clusters *a priori* is one of the limitations of k-means.
    We set `n_init=10` to run the k-means clustering algorithms 10 times independently,
    with different random centroids to choose the final model as the one with the
    lowest SSE. Via the `max_iter` parameter, we specify the maximum number of iterations
    for each single run (here, `300`). Note that the k-means implementation in scikit-learn
    stops early if it converges before the maximum number of iterations is reached.
    However, it is possible that k-means does not reach convergence for a particular
    run, which can be problematic (computationally expensive) if we choose relatively
    large values for `max_iter`. One way to deal with convergence problems is to choose
    larger values for `tol`, which is a parameter that controls the tolerance with
    regard to the changes in the within-cluster SSE to declare convergence. In the
    preceding code, we chose a tolerance of `1e-04` (=0.0001).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述代码，我们将所需聚类数设置为`3`；需要事先指定聚类数是k-means的限制之一。我们设置`n_init=10`，以独立运行k-means聚类算法10次，每次选择不同的随机质心，选择最终模型为SSE最低的一个。通过`max_iter`参数，我们指定每次单独运行的最大迭代次数（这里是`300`）。请注意，如果scikit-learn中的k-means实现在达到最大迭代次数之前已经收敛，它将会提前停止。然而，有可能k-means在特定运行中无法收敛，这可能是一个问题（计算上昂贵），特别是当我们为`max_iter`选择相对较大的值时。解决收敛问题的一种方法是选择更大的`tol`值，这是一个控制在群内SSE变化方面宣布收敛的参数。在前述代码中，我们选择了`1e-04`（=0.0001）的容差。
- en: A problem with k-means is that one or more clusters can be empty. Note that
    this problem does not exist for k-medoids or fuzzy C-means, an algorithm that
    we will discuss later in this section. However, this problem is accounted for
    in the current k-means implementation in scikit-learn. If a cluster is empty,
    the algorithm will search for the example that is farthest away from the centroid
    of the empty cluster. Then, it will reassign the centroid to be this farthest
    point.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: k-means的一个问题是一个或多个聚类可能为空。请注意，这个问题在k-medoids或模糊C均值算法中并不存在，我们稍后将在本节讨论这个算法。然而，在scikit-learn中的当前k-means实现中已经解决了这个问题。如果一个聚类为空，算法将会寻找离空聚类质心最远的样本，然后将质心重新分配为这个最远的点。
- en: '**Feature scaling**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征缩放**'
- en: When we are applying k-means to real-world data using a Euclidean distance metric,
    we want to make sure that the features are measured on the same scale and apply
    z-score standardization or min-max scaling if necessary.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将k-means应用于现实世界的数据时，使用欧氏距离度量，我们希望确保特征在相同的尺度上测量，并在必要时应用z-score标准化或最小-最大缩放。
- en: 'Having predicted the cluster labels, `y_km`, and discussed some of the challenges
    of the k-means algorithm, let’s now visualize the clusters that k-means identified
    in the dataset together with the cluster centroids. These are stored under the
    `cluster_centers_` attribute of the fitted `KMeans` object:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 预测了聚类标签`y_km`之后，并讨论了k-means算法的一些挑战，现在让我们来可视化k-means在数据集中识别出的聚类以及聚类中心。这些信息存储在已拟合的`KMeans`对象的`cluster_centers_`属性下：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In *Figure 10.2*, you can see that k-means placed the three centroids at the
    center of each sphere, which looks like a reasonable grouping given this dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图10.2*中，你可以看到k-means将三个质心放置在每个球体的中心位置，这看起来是对数据集合理的分组：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_10_02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 自动生成的描述](img/B17582_10_02.png)'
- en: 'Figure 10.2: The k-means clusters and their centroids'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：k-means聚类及其质心
- en: Although k-means worked well on this toy dataset, we still have the drawback
    of having to specify the number of clusters, *k*, *a priori*. The number of clusters
    to choose may not always be so obvious in real-world applications, especially
    if we are working with a higher-dimensional dataset that cannot be visualized.
    The other properties of k-means are that clusters do not overlap and are not hierarchical,
    and we also assume that there is at least one item in each cluster. Later in this
    chapter, we will encounter different types of clustering algorithms, hierarchical
    and density-based clustering. Neither type of algorithm requires us to specify
    the number of clusters upfront or assume spherical structures in our dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管k-means在这个玩具数据集上表现良好，但我们仍然存在一个缺点，即需要事先指定簇的数量*k*。在现实应用中，特别是当我们处理无法可视化的高维数据集时，要选择的簇数并不总是那么明显。k-means的其他特性包括簇不重叠，不具有层次结构，同时我们还假设每个簇中至少有一项。在本章后面，我们将遇到不同类型的聚类算法，包括分层和基于密度的聚类。这两种算法都不要求我们预先指定簇的数量或假设数据集中存在球形结构。
- en: In the next subsection, we will cover a popular variant of the classic k-means
    algorithm called **k-means++**. While it doesn’t address those assumptions and
    drawbacks of k-means that were discussed in the previous paragraph, it can greatly
    improve the clustering results through more clever seeding of the initial cluster
    centers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将介绍经典k-means算法的一种流行变体称为**k-means++**。虽然它没有解决前一段讨论的k-means的假设和缺点，但通过更智能地选择初始聚类中心，它可以极大地改善聚类结果。
- en: A smarter way of placing the initial cluster centroids using k-means++
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用k-means++更智能地放置初始簇质心
- en: So far, we have discussed the classic k-means algorithm, which uses a random
    seed to place the initial centroids, which can sometimes result in bad clusterings
    or slow convergence if the initial centroids are chosen poorly. One way to address
    this issue is to run the k-means algorithm multiple times on a dataset and choose
    the best-performing model in terms of the SSE.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了经典的k-means算法，该算法使用随机种子来放置初始质心，如果选择的初始质心不好，有时可能导致簇的不良聚类或收敛缓慢。解决这个问题的一种方法是在数据集上多次运行k-means算法，并选择在SSE方面表现最佳的模型。
- en: 'Another strategy is to place the initial centroids far away from each other
    via the k-means++ algorithm, which leads to better and more consistent results
    than the classic k-means (*k-means++: The Advantages of Careful Seeding* by *D.
    Arthur* and *S. Vassilvitskii* in *Proceedings of the eighteenth annual ACM-SIAM
    symposium on Discrete algorithms*, pages 1027-1035\. *Society for Industrial and
    Applied Mathematics*, 2007).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是通过k-means++算法将初始质心放置在彼此远离的位置，这比经典的k-means方法（*k-means++：仔细种子的优势*由*D. Arthur*和*S.
    Vassilvitskii*在*第十八届年度ACM-SIAM离散算法研讨会论文集*中提出，页面1027-1035\. *工业和应用数学学会*，2007年）可以得到更好和更一致的结果。
- en: 'The initialization in k-means++ can be summarized as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: k-means++中的初始化可以总结如下：
- en: Initialize an empty set, **M**, to store the *k* centroids being selected.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空集合，**M**，用于存储正在选择的*k*个质心。
- en: Randomly choose the first centroid, ![](img/B17582_10_004.png), from the input
    examples and assign it to **M**.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择第一个质心，![](img/B17582_10_004.png)，从输入示例中，并将其分配给**M**。
- en: For each example, **x**^(^i^), that is not in **M**, find the minimum squared
    distance, *d*(**x**^(^i^), **M**)², to any of the centroids in **M**.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于不在**M**中的每个示例，**x**^(^i^)，找到到**M**中任何质心的最小平方距离，*d*(**x**^(^i^), **M**)²。
- en: To randomly select the next centroid, ![](img/B17582_10_007.png), use a weighted
    probability distribution equal to ![](img/B17582_10_008.png). For instance, we
    collect all points in an array and choose a weighted random sampling, such that
    the larger the squared distance, the more likely a point gets chosen as the centroid.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要随机选择下一个质心，![](img/B17582_10_007.png)，使用等于![](img/B17582_10_008.png)的加权概率分布。例如，我们将所有点收集到一个数组中，并选择加权随机抽样，使得距离平方较大的点更有可能被选择为质心。
- en: Repeat *steps* *3* and *4* until *k* centroids are chosen.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤* *3*和*4*，直到选择*k*个质心。
- en: Proceed with the classic k-means algorithm.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续使用经典的k-means算法。
- en: To use k-means++ with scikit-learn’s `KMeans` object, we just need to set the
    `init` parameter to `'k-means++'`. In fact, `'k-means++'` is the default argument
    to the `init` parameter, which is strongly recommended in practice. The only reason
    we didn’t use it in the previous example was to not introduce too many concepts
    all at once. The rest of this section on k-means will use k-means++, but you are
    encouraged to experiment more with the two different approaches (classic k-means
    via `init='random'` versus k-means++ via `init='k-means++'`) for placing the initial
    cluster centroids.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 scikit-learn 的`KMeans`对象中使用 k-means++，我们只需将`init`参数设置为`'k-means++'`。实际上，`'k-means++'`是`init`参数的默认参数，强烈推荐在实践中使用。之前的示例中没有使用它的唯一原因是为了不一次引入太多概念。本节的其余部分将使用
    k-means++，但鼓励您更多地尝试这两种不同的方法（通过`init='random'`进行经典的 k-means 或通过`init='k-means++'`进行
    k-means++）来放置初始簇质心。
- en: Hard versus soft clustering
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬聚类与软聚类
- en: '**Hard clustering** describes a family of algorithms where each example in
    a dataset is assigned to exactly one cluster, as in the k-means and k-means++
    algorithms that we discussed earlier in this chapter. In contrast, algorithms
    for **soft clustering** (sometimes also called **fuzzy clustering**) assign an
    example to one or more clusters. A popular example of soft clustering is the **fuzzy
    C-means** (**FCM**) algorithm (also called **soft k-means** or **fuzzy k-means**).
    The original idea goes back to the 1970s, when Joseph C. Dunn first proposed an
    early version of fuzzy clustering to improve k-means (*A Fuzzy Relative of the
    ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters*, 1973).
    Almost a decade later, James C. Bedzek published his work on the improvement of
    the fuzzy clustering algorithm, which is now known as the FCM algorithm (*Pattern
    Recognition with Fuzzy Objective Function Algorithms*, *Springer Science+Business
    Media*, 2013).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬聚类**描述了一个算法族，其中数据集中的每个示例被分配到一个且仅一个簇中，就像我们之前在本章讨论过的 k-means 和 k-means++ 算法一样。相反，**软聚类**（有时也称为**模糊聚类**）的算法将一个示例分配给一个或多个簇。软聚类的一个流行示例是**模糊C均值**（**FCM**）算法（也称为**软k-means**或**模糊k-means**）。最初的想法可以追溯到20世纪70年代，当时Joseph
    C. Dunn首次提出了模糊聚类的早期版本，以改进k-means（*A Fuzzy Relative of the ISODATA Process and
    Its Use in Detecting Compact Well-Separated Clusters*, 1973）。几乎10年后，James C. Bezdek发表了他关于改进模糊聚类算法的工作，现在被称为FCM算法（*Pattern
    Recognition with Fuzzy Objective Function Algorithms*, *Springer Science+Business
    Media*, 2013）。'
- en: 'The FCM procedure is very similar to k-means. However, we replace the hard
    cluster assignment with probabilities for each point belonging to each cluster.
    In k-means, we could express the cluster membership of an example, *x*, with a
    sparse vector of binary values:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: FCM过程与k-means非常相似。但是，我们用每个点属于每个簇的概率替换了硬聚类分配。在k-means中，我们可以用稀疏的二进制值向量表示示例*x*的簇成员资格。
- en: '![](img/B17582_10_009.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_009.png)'
- en: 'Here, the index position with value 1 indicates the cluster centroid, ![](img/B17582_10_004.png),
    that the example is assigned to (assuming *k* = 3, ![](img/B17582_10_011.png)).
    In contrast, a membership vector in FCM could be represented as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，索引位置为1的值表示簇质心，![](img/B17582_10_004.png)，示例被分配到（假设*k* = 3，![](img/B17582_10_011.png)）。相比之下，在FCM中，成员向量可以表示如下：
- en: '![](img/B17582_10_012.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_012.png)'
- en: 'Here, each value falls in the range [0, 1] and represents a probability of
    membership of the respective cluster centroid. The sum of the memberships for
    a given example is equal to 1\. As with the k-means algorithm, we can summarize
    the FCM algorithm in four key steps:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个值都落在范围[0, 1]内，表示相应簇质心的成员概率。给定示例的成员总和等于1。与k-means算法类似，我们可以用四个关键步骤总结FCM算法：
- en: Specify the number of *k* centroids and randomly assign the cluster memberships
    for each point
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定*k*个质心，并为每个点随机分配簇成员资格。
- en: Compute the cluster centroids, ![](img/B17582_10_013.png)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算簇质心，![](img/B17582_10_013.png)
- en: Update the cluster memberships for each point
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每个点的簇成员资格
- en: Repeat *steps 2* and *3* until the membership coefficients do not change or
    a user-defined tolerance or maximum number of iterations is reached
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 2* 和 *3*，直到成员系数不再改变或达到用户定义的容差或最大迭代次数。
- en: 'The objective function of FCM—we abbreviate it as *J*[m]—looks very similar
    to the within-cluster SSE that we minimize in k-means:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: FCM 的目标函数——我们将其缩写为*J*[m]——看起来与我们在 k-means 中最小化的簇内平方和误差（SSE）非常相似：
- en: '![](img/B17582_10_014.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_014.png)'
- en: However, note that the membership indicator, *w*^(^i^(, )^j^), is not a binary
    value as in k-means (![](img/B17582_10_015.png)), but a real value that denotes
    the cluster membership probability (![](img/B17582_10_016.png)). You also may
    have noticed that we added an additional exponent to *w*^(^i^(, )^j^); the exponent
    *m*, any number greater than or equal to one (typically *m* = 2), is the so-called
    **fuzziness coefficient** (or simply **fuzzifier**), which controls the degree
    of *fuzziness*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，成员指示器* w ^（^i^（，^j^）*不像k均值中的二进制值（![](img/B17582_10_015.png)），而是一个表示聚类成员资格概率的实数值（![](img/B17582_10_016.png)）。您可能还注意到，我们为*
    w ^（^i^（，^j^）*添加了一个额外的指数；指数*m*，大于或等于1（通常*m*=2），被称为**模糊系数**（或简称**模糊化器**），它控制*模糊度*的程度。
- en: 'The larger the value of *m*, the smaller the cluster membership, *w*^(^i^(, )^j^),
    becomes, which leads to fuzzier clusters. The cluster membership probability itself
    is calculated as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*m*值越大，聚类成员资格* w ^（^i^（，^j^）*的值越小，这导致聚类变得更加模糊。聚类成员资格概率本身的计算方法如下：'
- en: '![](img/B17582_10_017.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_017.png)'
- en: 'For example, if we chose three cluster centers, as in the previous k-means
    example, we could calculate the membership of ![](img/B17582_10_018.png) belonging
    to the ![](img/B17582_10_004.png) cluster as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在前面的k均值示例中，如果我们选择了三个聚类中心，我们可以计算![](img/B17582_10_018.png)属于![](img/B17582_10_004.png)聚类的成员资格如下：
- en: '![](img/B17582_10_020.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_020.png)'
- en: 'The center, ![](img/B17582_10_004.png), of a cluster itself is calculated as
    the mean of all examples weighted by the degree to which each example belongs
    to that cluster (![](img/B17582_10_022.png)):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类中心![](img/B17582_10_004.png)本身是通过加权平均所有示例来计算的，加权系数是每个示例属于该聚类的程度（![](img/B17582_10_022.png)）：
- en: '![](img/B17582_10_023.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_023.png)'
- en: 'Just by looking at the equation to calculate the cluster memberships, we can
    say that each iteration in FCM is more expensive than an iteration in k-means.
    On the other hand, FCM typically requires fewer iterations overall to reach convergence.
    However, it has been found, in practice, that both k-means and FCM produce very
    similar clustering outputs, as described in a study (*Comparative Analysis of
    k-means and Fuzzy C-Means Algorithms* by *S. Ghosh* and *S. K. Dubey*, *IJACSA*,
    4: 35–38, 2013). Unfortunately, the FCM algorithm is not implemented in scikit-learn
    currently, but interested readers can try out the FCM implementation from the
    scikit-fuzzy package, which is available at [https://github.com/scikit-fuzzy/scikit-fuzzy](https://github.com/scikit-fuzzy/scikit-fuzzy).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '光看计算聚类成员资格的方程，我们可以说FCM中的每次迭代比k均值中的迭代更昂贵。另一方面，FCM通常需要更少的迭代才能达到收敛。然而，实际上发现，k均值和FCM产生非常相似的聚类输出，正如一项研究（*《比较分析k均值和模糊c均值算法》*，由*S.
    Ghosh*和*S. K. Dubey*，*IJACSA*，4: 35–38，2013年）所描述的那样。不幸的是，目前scikit-learn中未实现FCM算法，但有兴趣的读者可以尝试来自scikit-fuzzy软件包的FCM实现，该软件包可在[https://github.com/scikit-fuzzy/scikit-fuzzy](https://github.com/scikit-fuzzy/scikit-fuzzy)获取。'
- en: Using the elbow method to find the optimal number of clusters
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用肘方法找到最优聚类数
- en: One of the main challenges in unsupervised learning is that we do not know the
    definitive answer. We don’t have the ground-truth class labels in our dataset
    that allow us to apply the techniques that we used in *Chapter 6*, *Learning Best
    Practices for Model Evaluation and Hyperparameter Tuning*, to evaluate the performance
    of a supervised model. Thus, to quantify the quality of clustering, we need to
    use intrinsic metrics—such as the within-cluster SSE (distortion)—to compare the
    performance of different k-means clustering models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的主要挑战之一是我们不知道确切的答案。在我们的数据集中，我们没有地面真实类标签，这些标签允许我们应用在*第6章* *学习模型评估和超参数调整的最佳实践*
    中使用的技术来评估监督模型的性能。因此，为了量化聚类的质量，我们需要使用内在度量标准，例如在集群内的SSE（畸变）来比较不同k均值聚类模型的性能。
- en: 'Conveniently, we don’t need to compute the within-cluster SSE explicitly when
    we are using scikit-learn, as it is already accessible via the `inertia_` attribute
    after fitting a `KMeans` model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 方便地，当我们使用scikit-learn时，我们不需要显式计算在集群内的SSE，因为在拟合了`KMeans`模型后，它已经通过`inertia_`属性访问：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Based on the within-cluster SSE, we can use a graphical tool, the so-called
    **elbow method**, to estimate the optimal number of clusters, *k*, for a given
    task. We can say that if *k* increases, the distortion will decrease. This is
    because the examples will be closer to the centroids they are assigned to. The
    idea behind the elbow method is to identify the value of *k* where the distortion
    begins to increase most rapidly, which will become clearer if we plot the distortion
    for different values of *k*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聚类内 SSE，我们可以使用一个名为 **肘部法** 的图形工具来估计给定任务的最佳聚类数 *k*。我们可以说，如果 *k* 增加，失真将减少。这是因为示例将更接近它们被分配到的质心。肘部法的思想是识别失真开始最快增加的
    *k* 值，如果我们为不同的 *k* 值绘制失真图，这将变得更清晰：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see in *Figure 10.3*, the *elbow* is located at *k* = 3, so this
    is supporting evidence that *k* = 3 is indeed a good choice for this dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 *图 10.3* 中所看到的，*肘部* 位于 *k* = 3，这是支持 *k* = 3 对于此数据集确实是一个好选择的证据：
- en: '![](img/B17582_10_03.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_03.png)'
- en: 'Figure 10.3: Finding the optimal number of clusters using the elbow method'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：使用肘部法找到最佳聚类数
- en: Quantifying the quality of clustering via silhouette plots
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过轮廓图量化聚类质量
- en: 'Another intrinsic metric to evaluate the quality of a clustering is **silhouette
    analysis**, which can also be applied to clustering algorithms other than k-means
    that we will discuss later in this chapter. Silhouette analysis can be used as
    a graphical tool to plot a measure of how tightly grouped the examples in the
    clusters are. To calculate the **silhouette coefficient** of a single example
    in our dataset, we can apply the following three steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个评估聚类质量的内在度量是 **轮廓分析**，它也可以应用于我们稍后将讨论的除 k-means 外的其他聚类算法。轮廓分析可用作绘制集群示例紧密程度的度量的图形工具。要计算数据集中单个示例的
    **轮廓系数**，我们可以应用以下三个步骤：
- en: Calculate the **cluster cohesion**, *a*^(^i^), as the average distance between
    an example, **x**^(^i^), and all other points in the same cluster.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 **聚类内聚性** *a*^(^i^) ，作为示例 **x**^(^i^) 和同一聚类中所有其他点之间的平均距离。
- en: Calculate the **cluster separation**, *b*^(^i^), from the next closest cluster
    as the average distance between the example, **x**^(^i^), and all examples in
    the nearest cluster.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 **聚类分离度** *b*^(^i^) ，作为示例 **x**^(^i^) 和最近聚类中所有示例之间的平均距离。
- en: 'Calculate the silhouette, *s*^(^i^), as the difference between cluster cohesion
    and separation divided by the greater of the two, as shown here:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算轮廓系数 *s*^(^i^) ，如下所示，作为聚类内聚性和分离性之间差异的差值，除以两者中的较大者：
- en: '![](img/B17582_10_024.png)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_10_024.png)'
- en: The silhouette coefficient is bounded in the range –1 to 1\. Based on the preceding
    equation, we can see that the silhouette coefficient is 0 if the cluster separation
    and cohesion are equal (*b*^(^i^) = *a*^(^i^)). Furthermore, we get close to an
    ideal silhouette coefficient of 1 if *b*^(^i^) >> *a*^(^i^), since *b*^(^i^) quantifies
    how dissimilar an example is from other clusters, and *a*^(^i^) tells us how similar
    it is to the other examples in its own cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数的范围为 -1 到 1。根据前述方程，我们可以看到如果聚类分离和内聚相等（*b*^(^i^) = *a*^(^i^)），则轮廓系数为 0。此外，如果
    *b*^(^i^) >> *a*^(^i^)，我们接近理想的轮廓系数 1，因为 *b*^(^i^) 量化了示例与其他聚类的不相似性，而 *a*^(^i^)
    告诉我们它与同一聚类中其他示例的相似性。
- en: 'The silhouette coefficient is available as `silhouette_samples` from scikit-learn’s
    `metric` module, and optionally, the `silhouette_scores` function can be imported
    for convenience. The `silhouette_scores` function calculates the average silhouette
    coefficient across all examples, which is equivalent to `numpy.mean(silhouette_samples(...))`.
    By executing the following code, we will now create a plot of the silhouette coefficients
    for a k-means clustering with *k* = 3:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`silhouette_samples` 是 scikit-learn 的 `metric` 模块提供的轮廓系数，可选地，为了方便起见，可以导入 `silhouette_scores`
    函数。`silhouette_scores` 函数计算所有示例的平均轮廓系数，这相当于 `numpy.mean(silhouette_samples(...))`。通过执行以下代码，我们现在将创建一个
    k-means 聚类的轮廓系数图，其中 *k* = 3：'
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Through a visual inspection of the silhouette plot, we can quickly scrutinize
    the sizes of the different clusters and identify clusters that contain *outliers*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对轮廓图的视觉检查，我们可以快速审查不同聚类的大小，并识别包含 *异常值* 的聚类：
- en: '![](img/B17582_10_04.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_04.png)'
- en: 'Figure 10.4: A silhouette plot for a good example of clustering'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：一个良好聚类的轮廓图示例
- en: However, as you can see in the preceding silhouette plot, the silhouette coefficients
    are not close to 0 and are approximately equally far away from the average silhouette
    score, which is, in this case, an indicator of *good* clustering. Furthermore,
    to summarize the goodness of our clustering, we added the average silhouette coefficient
    to the plot (dotted line).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如前面的轮廓图所示，轮廓系数与平均轮廓分数并不接近，并且在本例中，这是*好*聚类的指标。此外，为了总结我们聚类的好坏，我们将平均轮廓系数添加到图中（虚线）。
- en: 'To see what a silhouette plot looks like for a relatively *bad* clustering,
    let’s seed the k-means algorithm with only two centroids:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看相对*糟糕*聚类的轮廓图是什么样子，请用仅两个质心种子化k均值算法：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see in *Figure 10.5*, one of the centroids falls between two of the
    three spherical groupings of the input data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在*图10.5*中所见，三个球形数据组之间有一个质心。
- en: 'Although the clustering does not look completely terrible, it is suboptimal:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管聚类看起来并非完全糟糕，但仍然次优：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_10_05.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17582_10_05.png)'
- en: 'Figure 10.5: A suboptimal example of clustering'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '图10.5: 一个聚类次优示例'
- en: 'Please keep in mind that we typically do not have the luxury of visualizing
    datasets in two-dimensional scatterplots in real-world problems, since we typically
    work with data in higher dimensions. So, next, we will create the silhouette plot
    to evaluate the results:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在真实世界的问题中，我们通常没有奢侈地在二维散点图中可视化数据集，因为我们通常使用更高维度的数据。因此，接下来，我们将创建轮廓图来评估结果：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see in *Figure 10.6*, the silhouettes now have visibly different
    lengths and widths, which is evidence of a relatively *bad* or at least *suboptimal*
    clustering:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在*图10.6*中所见，轮廓现在具有明显不同的长度和宽度，这表明相对*糟糕*或至少*次优*的聚类：
- en: '![](img/B17582_10_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_06.png)'
- en: 'Figure 10.6: A silhouette plot for a suboptimal example of clustering'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '图10.6: 一个聚类次优示例的轮廓图'
- en: Now, after we have gained a good understanding of how clustering works, the
    next section will introduce hierarchical clustering as an alternative approach
    to k-means.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经对聚类的工作原理有了很好的理解，接下来的部分将介绍层次聚类作为k均值的替代方法。
- en: Organizing clusters as a hierarchical tree
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将群集组织成层次树
- en: 'In this section, we will look at an alternative approach to prototype-based
    clustering: **hierarchical clustering**. One advantage of the hierarchical clustering
    algorithm is that it allows us to plot **dendrograms** (visualizations of a binary
    hierarchical clustering), which can help with the interpretation of the results
    by creating meaningful taxonomies. Another advantage of this hierarchical approach
    is that we do not need to specify the number of clusters upfront.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看一种基于原型的聚类的替代方法：**层次聚类**。层次聚类算法的一个优点是它允许我们绘制**树状图**（二叉层次聚类的可视化），这可以通过创建有意义的分类体系来帮助解释结果。这种层次方法的另一个优点是我们不需要预先指定群集的数量。
- en: The two main approaches to hierarchical clustering are **agglomerative** and
    **divisive** hierarchical clustering. In divisive hierarchical clustering, we
    start with one cluster that encompasses the complete dataset, and we iteratively
    split the cluster into smaller clusters until each cluster only contains one example.
    In this section, we will focus on agglomerative clustering, which takes the opposite
    approach. We start with each example as an individual cluster and merge the closest
    pairs of clusters until only one cluster remains.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的两种主要方法是**聚合**和**分裂**层次聚类。在分裂层次聚类中，我们从包含完整数据集的一个群集开始，并迭代地将群集分成较小的群集，直到每个群集只包含一个示例。在本节中，我们将重点关注聚合聚类，它采用相反的方法。我们从每个示例作为单独的群集开始，并合并最接近的群集对，直到只剩下一个群集。
- en: Grouping clusters in a bottom-up fashion
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以自底向上的方式对群集进行分组
- en: 'The two standard algorithms for agglomerative hierarchical clustering are **single
    linkage** and **complete linkage**. Using single linkage, we compute the distances
    between the most similar members for each pair of clusters and merge the two clusters
    for which the distance between the most similar members is the smallest. The complete
    linkage approach is similar to single linkage but, instead of comparing the most
    similar members in each pair of clusters, we compare the most dissimilar members
    to perform the merge. This is shown in *Figure 10.7*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 凝聚式层次聚类的两种标准算法是**单链接**和**完全链接**。使用单链接时，我们计算每对聚类中最相似成员之间的距离，并合并两个距离最小的聚类。完全链接方法类似于单链接，但我们比较每对聚类中最不相似成员，以执行合并。这在
    *图 10.7* 中显示：
- en: '![](img/B17582_10_07.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_07.png)'
- en: 'Figure 10.7: The complete linkage approach'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：完全链接方法
- en: '**Alternative types of linkages**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**替代链接类型**'
- en: Other commonly used algorithms for agglomerative hierarchical clustering include
    average linkage and Ward’s linkage. In average linkage, we merge the cluster pairs
    based on the minimum average distances between all group members in the two clusters.
    In Ward’s linkage, the two clusters that lead to the minimum increase of the total
    within-cluster SSE are merged.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 用于凝聚式层次聚类的其他常用算法包括平均链接和Ward链接。在平均链接中，我们基于两个聚类中所有组成员的最小平均距离合并聚类对。在Ward链接中，合并导致总内部簇平方和增加最小的两个聚类。
- en: 'In this section, we will focus on agglomerative clustering using the complete
    linkage approach. Hierarchical complete linkage clustering is an iterative procedure
    that can be summarized by the following steps:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于使用完全链接方法进行凝聚式聚类。层次完全链接聚类是一个迭代过程，可以总结为以下步骤：
- en: Compute a pair-wise distance matrix of all examples.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有示例的成对距离矩阵。
- en: Represent each data point as a singleton cluster.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点表示为单例聚类。
- en: Merge the two closest clusters based on the distance between the most dissimilar
    (distant) members.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据最不相似（最远）成员之间的距离合并两个最接近的聚类。
- en: Update the cluster linkage matrix.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新聚类链接矩阵。
- en: Repeat *steps 2-4* until one single cluster remains.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2-4* 直到只剩下一个单一的聚类。
- en: 'Next, we will discuss how to compute the distance matrix (*step 1*). But first,
    let’s generate a random data sample to work with. The rows represent different
    observations (IDs 0-4), and the columns are the different features (`X`, `Y`,
    `Z`) of those examples:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何计算距离矩阵（*步骤 1*）。但首先，让我们生成一个随机数据样本来使用。行代表不同的观察（ID 0-4），列是这些示例的不同特征（`X`、`Y`、`Z`）：
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After executing the preceding code, we should now see the following `DataFrame`
    containing the randomly generated examples:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行上述代码后，我们现在应该看到包含随机生成示例的以下 `DataFrame`：
- en: '![](img/B17582_10_08.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_08.png)'
- en: 'Figure 10.8: A randomly generated data sample'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：一个随机生成的数据样本
- en: Performing hierarchical clustering on a distance matrix
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在距离矩阵上执行层次聚类
- en: 'To calculate the distance matrix as input for the hierarchical clustering algorithm,
    we will use the `pdist` function from SciPy’s `spatial.distance` submodule:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算作为层次聚类算法输入的距离矩阵，我们将使用 SciPy 的 `spatial.distance` 子模块中的 `pdist` 函数：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using the preceding code, we calculated the Euclidean distance between each
    pair of input examples in our dataset based on the features `X`, `Y`, and `Z`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码，我们根据特征 `X`、`Y` 和 `Z` 计算了数据集中每对输入示例之间的欧氏距离。
- en: 'We provided the condensed distance matrix—returned by `pdist`—as input to the
    `squareform` function to create a symmetrical matrix of the pair-wise distances,
    as shown here:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了由 `pdist` 返回的压缩距离矩阵作为 `squareform` 函数的输入，以创建成对距离的对称矩阵，如下所示：
- en: '![](img/B17582_10_09.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_09.png)'
- en: 'Figure 10.9: The calculated pair-wise distances of our data'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：我们数据的计算成对距离
- en: Next, we will apply the complete linkage agglomeration to our clusters using
    the `linkage` function from SciPy’s `cluster.hierarchy` submodule, which returns
    a so-called **linkage matrix**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 SciPy 的 `cluster.hierarchy` 子模块中的 `linkage` 函数将完全链接聚合应用于我们的聚类，该函数返回所谓的**链接矩阵**。
- en: 'However, before we call the `linkage` function, let’s take a careful look at
    the function documentation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 `linkage` 函数之前，让我们仔细查看函数文档：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Based on the function description, we understand that we can use a condensed
    distance matrix (upper triangular) from the `pdist` function as an input attribute.
    Alternatively, we could also provide the initial data array and use the `''euclidean''`
    metric as a function argument in `linkage`. However, we should not use the `squareform`
    distance matrix that we defined earlier, since it would yield different distance
    values than expected. To sum it up, the three possible scenarios are listed here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 根据函数描述，我们理解可以使用来自`pdist`函数的简化距离矩阵（上三角形式）作为输入属性。或者，我们也可以提供初始数据数组，并在`linkage`函数中使用`'euclidean'`度量作为函数参数。然而，我们不应该使用之前定义的`squareform`距离矩阵，因为它会产生与预期不同的距离值。总结来说，这里列出了三种可能的情景：
- en: '**Incorrect approach**: Using the `squareform` distance matrix as shown in
    the following code snippet leads to incorrect results:'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误的方法**：如下所示使用`squareform`距离矩阵的代码片段会导致错误的结果：'
- en: '[PRE11]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Correct approach**: Using the condensed distance matrix as shown in the following
    code example yields the correct linkage matrix:'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确的方法**：如下所示使用简化的距离矩阵的代码示例可以产生正确的联接矩阵：'
- en: '[PRE12]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Correct approach**: Using the complete input example matrix (the so-called
    design matrix) as shown in the following code snippet also leads to a correct
    linkage matrix similar to the preceding approach:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确的方法**：如下所示使用完整的输入示例矩阵（即所谓的设计矩阵）的代码片段也会导致与前述方法类似的正确联接矩阵：'
- en: '[PRE13]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To take a closer look at the clustering results, we can turn those results
    into a pandas `DataFrame` (best viewed in a Jupyter notebook) as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地查看聚类结果，我们可以将这些结果转换为 pandas `DataFrame`（最好在 Jupyter 笔记本中查看）如下所示：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As shown in *Figure 10.10*, the linkage matrix consists of several rows where
    each row represents one merge. The first and second columns denote the most dissimilar
    members in each cluster, and the third column reports the distance between those
    members.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 10.10*所示，联接矩阵由多行组成，每行代表一个合并。第一列和第二列表示每个簇中最不相似的成员，第三列报告这些成员之间的距离。
- en: 'The last column returns the count of the members in each cluster:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一列返回每个簇中成员的计数：
- en: '![](img/B17582_10_10.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![img/B17582_10_10.png](img/B17582_10_10.png)'
- en: 'Figure 10.10: The linkage matrix'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：联接矩阵
- en: 'Now that we have computed the linkage matrix, we can visualize the results
    in the form of a dendrogram:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出联接矩阵，我们可以以树状图的形式可视化结果：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you are executing the preceding code or reading an e-book version of this
    book, you will notice that the branches in the resulting dendrogram are shown
    in different colors. The color scheme is derived from a list of Matplotlib colors
    that are cycled for the distance thresholds in the dendrogram. For example, to
    display the dendrograms in black, you can uncomment the respective sections that
    were inserted in the preceding code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在执行上述代码或阅读本书的电子书版本，您会注意到生成的树状图中的分支显示为不同颜色。颜色方案源自 Matplotlib 的颜色列表，这些颜色按照树状图中的距离阈值循环。例如，要将树状图显示为黑色，您可以取消注释前述代码中插入的相应部分：
- en: '![Chart, box and whisker chart  Description automatically generated](img/B17582_10_11.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![img/B17582_10_11.png](img/B17582_10_11.png)'
- en: 'Figure 10.11: A dendrogram of our data'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：我们数据的树状图
- en: Such a dendrogram summarizes the different clusters that were formed during
    the agglomerative hierarchical clustering; for example, you can see that the examples
    `ID_0` and `ID_4`, followed by `ID_1` and `ID_2`, are the most similar ones based
    on the Euclidean distance metric.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的树状图总结了在聚合层次聚类期间形成的不同簇；例如，您可以看到基于欧几里德距离度量，示例`ID_0`和`ID_4`，接着是`ID_1`和`ID_2`是最相似的。
- en: Attaching dendrograms to a heat map
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附加树状图到热图
- en: In practical applications, hierarchical clustering dendrograms are often used
    in combination with a **heat map**, which allows us to represent the individual
    values in the data array or matrix containing our training examples with a color
    code. In this section, we will discuss how to attach a dendrogram to a heat map
    plot and order the rows in the heat map correspondingly.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，层次聚类的树状图通常与热图结合使用，这使我们能够用颜色代码表示包含训练示例的数据数组或矩阵中的个别值。在本节中，我们将讨论如何将树状图附加到热图中并相应地对热图的行进行排序。
- en: 'However, attaching a dendrogram to a heat map can be a little bit tricky, so
    let’s go through this procedure step by step:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将树状图附加到热图可能有些棘手，所以让我们一步步进行此过程：
- en: 'We create a new `figure` object and define the *x* axis position, *y* axis
    position, width, and height of the dendrogram via the `add_axes` attribute. Furthermore,
    we rotate the dendrogram 90 degrees counterclockwise. The code is as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个新的 `figure` 对象，并通过 `add_axes` 属性定义树状图的*x*轴位置、*y*轴位置、宽度和高度。此外，我们将树状图逆时针旋转90度。代码如下：
- en: '[PRE16]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we reorder the data in our initial `DataFrame` according to the clustering
    labels that can be accessed from the `dendrogram` object, which is essentially
    a Python dictionary, via the `leaves` key. The code is as follows:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们根据从 `dendrogram` 对象（实质上是一个Python字典）通过 `leaves` 键访问的聚类标签重新排序我们初始 `DataFrame`
    中的数据。代码如下：
- en: '[PRE17]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we construct the heat map from the reordered `DataFrame` and position
    it next to the dendrogram:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们从重新排序的 `DataFrame` 构建热图，并将其放置在树状图旁边：
- en: '[PRE18]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we modify the aesthetics of the dendrogram by removing the axis ticks
    and hiding the axis spines. Also, we add a color bar and assign the feature and
    data record names to the *x* and *y* axis tick labels, respectively:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过移除轴刻度和隐藏轴脊梁来修改树状图的美学。此外，我们添加了一个色条，并将特征和数据记录名称分配给*x*和*y*轴刻度标签：
- en: '[PRE19]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After following the previous steps, the heat map should be displayed with the
    dendrogram attached:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行前述步骤之后，热图应显示在附加的树状图上。
- en: '![Chart  Description automatically generated](img/B17582_10_12.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_10_12.png)'
- en: 'Figure 10.12: A heat map and dendrogram of our data'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12：我们数据的热图和树状图
- en: As you can see, the order of rows in the heat map reflects the clustering of
    the examples in the dendrogram. In addition to a simple dendrogram, the color-coded
    values of each example and feature in the heat map provide us with a nice summary
    of the dataset.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，热图中行的顺序反映了树状图中示例的聚类情况。除了简单的树状图外，热图中每个示例和特征的色彩编码值为我们提供了数据集的一个良好总结。
- en: Applying agglomerative clustering via scikit-learn
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 scikit-learn 应用凝聚层次聚类
- en: In the previous subsection, you saw how to perform agglomerative hierarchical
    clustering using SciPy. However, there is also an `AgglomerativeClustering` implementation
    in scikit-learn, which allows us to choose the number of clusters that we want
    to return. This is useful if we want to prune the hierarchical cluster tree.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的小节中，您看到了如何使用 SciPy 执行凝聚层次聚类。然而，scikit-learn 中也有一个 `AgglomerativeClustering`
    实现，允许我们选择要返回的聚类数目。如果我们想要修剪层次聚类树，这将非常有用。
- en: 'By setting the `n_cluster` parameter to `3`, we will now cluster the input
    examples into three groups using the same complete linkage approach based on the
    Euclidean distance metric as before:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `n_cluster` 参数设置为 `3` 后，我们将使用与之前相同的完全连接方法和欧几里得距离度量将输入示例聚类成三组：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Looking at the predicted cluster labels, we can see that the first and the
    fifth examples (`ID_0` and `ID_4`) were assigned to one cluster (label `1`), and
    the examples `ID_1` and `ID_2` were assigned to a second cluster (label `0`).
    The example `ID_3` was put into its own cluster (label `2`). Overall, the results
    are consistent with the results that we observed in the dendrogram. We should
    note, though, that `ID_3` is more similar to `ID_4` and `ID_0` than to `ID_1`
    and `ID_2`, as shown in the preceding dendrogram figure; this is not clear from
    scikit-learn’s clustering results. Let’s now rerun the `AgglomerativeClustering`
    using `n_cluster=2` in the following code snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 查看预测的聚类标签，我们可以看到第一个和第五个示例（`ID_0` 和 `ID_4`）被分配到一个簇（标签 `1`），示例 `ID_1` 和 `ID_2`
    被分配到第二个簇（标签 `0`）。示例 `ID_3` 被放入其自己的簇（标签 `2`）。总体而言，这些结果与我们在树状图中观察到的结果一致。但是，需要注意的是，`ID_3`
    与 `ID_4` 和 `ID_0` 更相似，而不是与 `ID_1` 和 `ID_2`，如前面的树状图所示；这一点在 scikit-learn 的聚类结果中并不明显。现在让我们在以下代码片段中使用
    `n_cluster=2` 重新运行 `AgglomerativeClustering`：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see, in this *pruned* clustering hierarchy, label `ID_3` was assigned
    to the same cluster as `ID_0` and `ID_4`, as expected.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在这个*修剪*的聚类层次结构中，标签 `ID_3` 被分配到与 `ID_0` 和 `ID_4` 相同的簇中，正如预期的那样。
- en: Locating regions of high density via DBSCAN
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 DBSCAN 定位高密度区域
- en: 'Although we can’t cover the vast number of different clustering algorithms
    in this chapter, let’s at least include one more approach to clustering: **density-based
    spatial clustering of applications with noise** (**DBSCAN**), which does not make
    assumptions about spherical clusters like k-means, nor does it partition the dataset
    into hierarchies that require a manual cut-off point. As its name implies, density-based
    clustering assigns cluster labels based on dense regions of points. In DBSCAN,
    the notion of density is defined as the number of points within a specified radius,
    ![](img/B17582_10_025.png).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们无法在本章节中涵盖大量不同的聚类算法，但至少让我们再包括一种聚类方法：**基于密度的空间聚类应用与噪声**（**DBSCAN**），它不像k-means那样假设球形簇，也不将数据集分割成需要手动切断点的层次结构。DBSCAN根据点的密集区域分配聚类标签。在DBSCAN中，密度的概念被定义为指定半径内的点数，如图所示：![](img/B17582_10_025.png)。
- en: 'According to the DBSCAN algorithm, a special label is assigned to each example
    (data point) using the following criteria:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据DBSCAN算法，根据以下标准为每个示例（数据点）分配特殊标签：
- en: A point is considered a **core point** if at least a specified number (MinPts)
    of neighboring points fall within the specified radius, ![](img/B17582_07_004.png)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在指定半径内有至少指定数量（MinPts）的相邻点，则称该点为**核心点**，如图所示：![](img/B17582_07_004.png)
- en: A **border point** is a point that has fewer neighbors than MinPts within ![](img/B17582_07_004.png),
    but lies within the ![](img/B17582_07_004.png) radius of a core point
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界点**是指在![](img/B17582_07_004.png)半径内具有少于MinPts相邻点，但位于核心点的![](img/B17582_07_004.png)半径内的点。'
- en: All other points that are neither core nor border points are considered **noise
    points**
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有既不是核心点也不是边界点的其他点被视为**噪声点**。
- en: 'After labeling the points as core, border, or noise, the DBSCAN algorithm can
    be summarized in two simple steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将点标记为核心点、边界点或噪声点之后，DBSCAN算法可以总结为两个简单步骤：
- en: Form a separate cluster for each core point or connected group of core points.
    (Core points are connected if they are no farther away than ![](img/B17582_10_029.png).)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 形成每个核心点或连接的核心点组的单独簇。（如果它们的距离不超过![](img/B17582_10_029.png)。）
- en: Assign each border point to the cluster of its corresponding core point.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个边界点分配到其对应核心点的簇中。
- en: 'To get a better understanding of what the result of DBSCAN can look like, before
    jumping to the implementation, let’s summarize what we have just learned about
    core points, border points, and noise points in *Figure 10.13*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入实施之前，为了更好地理解DBSCAN的结果可能如何，让我们总结一下关于核心点、边界点和噪声点的内容，参见*图10.13*：
- en: '![](img/B17582_10_13.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_10_13.png)'
- en: 'Figure 10.13: Core, noise, and border points for DBSCAN'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.13：DBSCAN的核心点、噪声点和边界点
- en: One of the main advantages of using DBSCAN is that it does not assume that the
    clusters have a spherical shape as in k-means. Furthermore, DBSCAN is different
    from k-means and hierarchical clustering in that it doesn’t necessarily assign
    each point to a cluster but is capable of removing noise points.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DBSCAN的主要优势之一是，它不假设聚类像k-means中的球形那样。此外，DBSCAN与k-means和层次聚类不同之处在于，它不一定将每个点分配到一个簇中，但能够移除噪声点。
- en: 'For a more illustrative example, let’s create a new dataset of half-moon-shaped
    structures to compare k-means clustering, hierarchical clustering, and DBSCAN:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具说明性的例子，让我们创建一个新的半月形结构数据集，以比较k-means聚类、层次聚类和DBSCAN：
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you can see in the resulting plot, there are two visible, half-moon-shaped
    groups consisting of 100 examples (data points) each:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示的结果绘图中，可以看到两个明显的半月形簇，每个簇包含100个示例（数据点）：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_10_14.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17582_10_14.png)'
- en: 'Figure 10.14: A two-feature half-moon-shaped dataset'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：一个双特征半月形数据集
- en: 'We will start by using the k-means algorithm and complete linkage clustering
    to see if one of those previously discussed clustering algorithms can successfully
    identify the half-moon shapes as separate clusters. The code is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用k-means算法和完全链接聚类来查看这些先前讨论的聚类算法是否能够成功识别半月形状作为单独的簇。代码如下：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Based on the visualized clustering results, we can see that the k-means algorithm
    was unable to separate the two clusters, and also, the hierarchical clustering
    algorithm was challenged by those complex shapes:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 根据可视化的聚类结果，我们可以看到k-means算法无法分离两个簇，而且层次聚类算法在面对这些复杂形状时也面临挑战：
- en: '![A picture containing arrow  Description automatically generated](img/B17582_10_15.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图，箭头  由系统自动生成描述](img/B17582_10_15.png)'
- en: 'Figure 10.15: k-means and agglomerative clustering on the half-moon-shaped
    dataset'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：半月形数据集上的k-means和聚合聚类
- en: 'Finally, let’s try the DBSCAN algorithm on this dataset to see if it can find
    the two half-moon-shaped clusters using a density-based approach:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试在这个数据集上应用DBSCAN算法，看看它是否能够通过密度方法找到两个半月形簇：
- en: '[PRE24]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The DBSCAN algorithm can successfully detect the half-moon shapes, which highlights
    one of the strengths of DBSCAN—clustering data of arbitrary shapes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN算法可以成功检测半月形状，这突显了DBSCAN的一个优势——可以对任意形状的数据进行聚类：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_10_16.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图，散点图  由系统自动生成描述](img/B17582_10_16.png)'
- en: 'Figure 10.16: DBSCAN clustering on the half-moon-shaped dataset'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16：半月形数据集上的DBSCAN聚类
- en: 'However, we should also note some of the disadvantages of DBSCAN. With an increasing
    number of features in our dataset—assuming a fixed number of training examples—the
    negative effect of the **curse of dimensionality** increases. This is especially
    a problem if we are using the Euclidean distance metric. However, the problem
    of the curse of dimensionality is not unique to DBSCAN: it also affects other
    clustering algorithms that use the Euclidean distance metric, for example, k-means
    and hierarchical clustering algorithms. In addition, we have two hyperparameters
    in DBSCAN (MinPts and ![](img/B17582_07_004.png)) that need to be optimized to
    yield good clustering results. Finding a good combination of MinPts and ![](img/B17582_07_004.png)
    can be problematic if the density differences in the dataset are relatively large.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也应该注意一些DBSCAN的缺点。随着数据集中特征数量的增加——假设训练示例数量固定——**维度灾难**的负面影响增加。如果我们使用欧氏距离度量尤其是问题。然而，维度灾难问题并不局限于DBSCAN：它也影响其他使用欧氏距离度量的聚类算法，例如k-means和层次聚类算法。此外，DBSCAN中有两个超参数（MinPts和！[](img/B17582_07_004.png)）需要优化以产生良好的聚类结果。如果数据集中的密度差异相对较大，找到MinPts和！[](img/B17582_07_004.png)的良好组合可能会成为一个问题。
- en: '**Graph-based clustering**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于图的聚类**'
- en: 'So far, we have seen three of the most fundamental categories of clustering
    algorithms: prototype-based clustering with k-means, agglomerative hierarchical
    clustering, and density-based clustering via DBSCAN. However, there is also a
    fourth class of more advanced clustering algorithms that we have not covered in
    this chapter: graph-based clustering. Probably the most prominent members of the
    graph-based clustering family are the spectral clustering algorithms.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了三种最基本的聚类算法类别：基于原型的k-means聚类，聚合层次聚类和基于密度的DBSCAN聚类。然而，在本章中我们还没有涉及的是第四类更高级的聚类算法：基于图的聚类。可能是基于图的聚类家族中最显著的成员是光谱聚类算法。
- en: 'Although there are many different implementations of spectral clustering, what
    they all have in common is that they use the eigenvectors of a similarity or distance
    matrix to derive the cluster relationships. Since spectral clustering is beyond
    the scope of this book, you can read the excellent tutorial by Ulrike von Luxburg
    to learn more about this topic (*A tutorial on spectral clustering*, *Statistics
    and Computing*, 17(4): 395-416, 2007). It is freely available from arXiv at [http://arxiv.org/pdf/0711.0189v1.pdf](http://arxiv.org/pdf/0711.0189v1.pdf).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管光谱聚类有许多不同的实现，它们的共同之处在于利用相似性或距离矩阵的特征向量来推导聚类关系。由于光谱聚类超出了本书的范围，您可以阅读乌尔里克·冯·吕克斯堡（Ulrike
    von Luxburg）的优秀教程以了解更多关于这个主题的内容（*光谱聚类教程*，*统计与计算*，17（4）：395-416，2007）。它可以在arXiv上免费获取：[http://arxiv.org/pdf/0711.0189v1.pdf](http://arxiv.org/pdf/0711.0189v1.pdf)。
- en: Note that, in practice, it is not always obvious which clustering algorithm
    will perform best on a given dataset, especially if the data comes in multiple
    dimensions that make it hard or impossible to visualize. Furthermore, it is important
    to emphasize that a successful clustering does not only depend on the algorithm
    and its hyperparameters; rather, the choice of an appropriate distance metric
    and the use of domain knowledge that can help to guide the experimental setup
    can be even more important.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在实践中，不同聚类算法在给定数据集上表现最佳并不总是显而易见，特别是如果数据以多个维度呈现，这会使得可视化变得困难或不可能。此外，强调成功的聚类不仅取决于算法及其超参数；选择合适的距离度量标准以及使用能够帮助指导实验设置的领域知识可能更为重要。
- en: In the context of the curse of dimensionality, it is thus common practice to
    apply dimensionality reduction techniques prior to performing clustering. Such
    dimensionality reduction techniques for unsupervised datasets include principal
    component analysis and t-SNE, which we covered in *Chapter 5*, *Compressing Data
    via Dimensionality Reduction*. Also, it is particularly common to compress datasets
    down to two-dimensional subspaces, which allows us to visualize the clusters and
    assigned labels using two-dimensional scatterplots, which are particularly helpful
    for evaluating the results.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在维度诅咒的背景下，因此在执行聚类之前常见的做法是应用降维技术。这些面向无监督数据集的降维技术包括主成分分析和 t-SNE，我们在*第五章*《通过降维压缩数据》中涵盖了这些技术。此外，将数据集压缩到二维子空间特别常见，这样可以使用二维散点图可视化聚类和分配的标签，这对于评估结果特别有帮助。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about three different clustering algorithms that
    can help us with the discovery of hidden structures or information in data. We
    started with a prototype-based approach, k-means, which clusters examples into
    spherical shapes based on a specified number of cluster centroids. Since clustering
    is an unsupervised method, we do not enjoy the luxury of ground-truth labels to
    evaluate the performance of a model. Thus, we used intrinsic performance metrics,
    such as the elbow method or silhouette analysis, as an attempt to quantify the
    quality of clustering.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，您学习了三种不同的聚类算法，这些算法可以帮助我们发现数据中隐藏的结构或信息。我们从基于原型的方法开始，即 k-means 算法，该算法根据指定数量的聚类中心将示例聚类成球形。由于聚类是一种无监督方法，我们没有地面真实标签来评估模型的性能。因此，我们使用了内在性能度量标准，例如肘部法则或轮廓分析，试图量化聚类的质量。
- en: 'We then looked at a different approach to clustering: agglomerative hierarchical
    clustering. Hierarchical clustering does not require specifying the number of
    clusters upfront, and the result can be visualized in a dendrogram representation,
    which can help with the interpretation of the results. The last clustering algorithm
    that we covered in this chapter was DBSCAN, an algorithm that groups points based
    on local densities and is capable of handling outliers and identifying non-globular
    shapes.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们看了一种不同的聚类方法：凝聚层次聚类。层次聚类不需要预先指定聚类数，其结果可以在树形图表示中可视化，这有助于解释结果。我们在本章中介绍的最后一个聚类算法是
    DBSCAN，这是一种根据局部密度分组点的算法，能够处理异常值并识别非球形形状。
- en: 'After this excursion into the field of unsupervised learning, it is now time
    to introduce some of the most exciting machine learning algorithms for supervised
    learning: multilayer artificial neural networks. After their recent resurgence,
    neural networks are once again the hottest topic in machine learning research.
    Thanks to recently developed deep learning algorithms, neural networks are considered
    state of the art for many complex tasks such as image classification, natural
    language processing, and speech recognition. In *Chapter 11*, *Implementing a
    Multilayer Artificial Neural Network from Scratch*, we will construct our own
    multilayer neural network. In *Chapter 12*, *Parallelizing Neural Network Training
    with PyTorch*, we will work with the PyTorch library, which specializes in training
    neural network models with multiple layers very efficiently by utilizing graphics
    processing units.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次涉足无监督学习领域之后，现在是介绍一些最激动人心的监督学习机器学习算法的时候了：多层人工神经网络。随着它们最近的复苏，神经网络再次成为机器学习研究中最热门的话题。由于最近开发的深度学习算法，神经网络被认为是许多复杂任务的最先进技术，如图像分类、自然语言处理和语音识别。在*第11章*，*从头开始实现多层人工神经网络*中，我们将构建自己的多层神经网络。在*第12章*，*使用PyTorch并行化神经网络训练*中，我们将使用PyTorch库，该库专门用于利用图形处理单元高效训练多层神经网络模型。
- en: Join our book’s Discord space
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作空间，参与每月的*问答*活动与作者交流：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
