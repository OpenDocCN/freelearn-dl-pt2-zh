["```py\n    %pip install -U \"gym==0.26.2\"\n    import numpy as np\n    import gym\n    import random \n    ```", "```py\n    from gym import envs\n    print('\\n'.join([str(env) for env in envs.registry])) \n    ```", "```py\n    env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='rgb_array') \n    ```", "```py\n    env.render() \n    ```", "```py\n    env.observation_space.n \n    ```", "```py\n    env.action_space.n \n    ```", "```py\n    env.action_space.sample() \n    ```", "```py\n    env.reset() \n    ```", "```py\n    env.step(env.action_space.sample()) \n    ```", "```py\n    %pip install torch-snippets \"gym==0.26.2\"\n    import numpy as np\n    import gym\n    import random\n    env = gym.make('FrozenLake-v0', is_slippery=False, \n                                    render_mode='rgb_array') \n    ```", "```py\n    action_size=env.action_space.n\n    state_size=env.observation_space.n\n    qtable=np.zeros((state_size,action_size)) \n    ```", "```py\n    episode_rewards = []\n    for i in range(10000):\n        state, *_ =env.reset() \n    ```", "```py\n     total_rewards = 0\n        for step in range(50): \n    ```", "```py\n     action=env.action_space.sample()\n            new_state,reward,done,*_=env.step(action) \n    ```", "```py\n     qtable[state,action]+=0.1*(reward+0.9*np.max(qtable[new_state,:]) \\\n                                                       -qtable[state,action]) \n    ```", "```py\n     state=new_state\n            total_rewards+=reward \n    ```", "```py\n     episode_rewards.append(total_rewards)\n    print(qtable) \n    ```", "```py\nepisode_rewards = []\n**epsilon=****1**\n**max_epsilon=****1**\n**min_epsilon=****0.01**\n**decay_rate=****0.005**\nfor episode in range(1000):\n    state, *_=env.reset()\n    total_rewards = 0\n    for step in range(50):\n        **exp_exp_tradeoff=random.uniform(****0****,****1****)**\n        **## Exploitation:**\n        **if** **exp_exp_tradeoff>epsilon:**\n            **action=np.argmax(qtable[state,:])**\n        **else****:**\n            **## Exploration**\n            **action=env.action_space.sample()**\n        new_state,reward,done,*_=env.step(action)\n        qtable[state,action]+=0.9*(reward+0.9*np.max(\\\n                                  qtable[new_state,:])\\\n                                   -qtable[state,action])\n        state=new_state\n        total_rewards+=reward\n    episode_rewards.append(total_rewards)\n    epsilon=min_epsilon+(max_epsilon-min_epsilon)\\\n                            ***np.exp(decay_rate*episode)**\nprint(qtable) \n```", "```py\nenv.reset()\nfor episode in range(1):\n    state, *_=env.reset()\n    step=0\n    done=False\n    print(\"-----------------------\")\n    print(\"Episode\",episode)\n    for step in range(50):\n        env.render()\n        action=np.argmax(qtable[state,:])\n        print(action)\n        new_state,reward,done,*_­­­=env.step(action)\n        if done:\n            print(\"Number of Steps\",step+1)\n            break\n        state=new_state\nenv.close() \n```", "```py\n    %pip install \"gym==0.26.2\"\n    import gym\n    import numpy as np\n    import cv2import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import random\n    from collections import namedtuple, deque\n    import torch.optim as optim\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    env = gym.make('CartPole-v1') \n    ```", "```py\n    class DQNetwork(nn.Module):\n        def __init__(self, state_size, action_size):\n            super(DQNetwork, self).__init__()\n\n            self.fc1 = nn.Linear(state_size, 24)\n            self.fc2 = nn.Linear(24, 24)\n            self.fc3 = nn.Linear(24, action_size)\n\n        def forward(self, state):     \n            x = F.relu(self.fc1(state))\n            x = F.relu(self.fc2(x))\n            x = self.fc3(x)\n            return x \n    ```", "```py\n    class Agent():\n        def __init__(self, state_size, action_size):\n\n            self.state_size = state_size\n            self.action_size = action_size\n            self.seed = random.seed(0)\n            ## hyperparameters\n            self.buffer_size = 2000\n            self.batch_size = 64\n            self.gamma = 0.99\n            self.lr = 0.0025\n            self.update_every = 4\n            # Q-Network\n            self.local = DQNetwork(state_size, action_size).to(device)\n            self.optimizer=optim.Adam(self.local.parameters(), lr=self.lr)\n            # Replay memory\n            self.memory = deque(maxlen=self.buffer_size)\n            self.experience = namedtuple(\"Experience\", \\\n                                field_names=[\"state\", \"action\",\n                                \"reward\", \"next_state\", \"done\"])\n            self.t_step = 0 \n    ```", "```py\n     def step(self, state, action, reward, next_state, done):\n            # Save experience in replay memory\n            self.memory.append(self.experience(state, action,\n                                               reward, next_state, done))\n            # Learn once every 'update_every' number of time steps.\n            self.t_step = (self.t_step + 1) % self.update_every\n            if self.t_step == 0:\n            # If enough samples are available in memory,\n            # get random subset and learn\n                if len(self.memory) > self.batch_size:\n                    experiences = self.sample_experiences()\n                    self.learn(experiences, self.gamma) \n    ```", "```py\n     def act(self, state, eps=0.):\n            # Epsilon-greedy action selection\n            if random.random() > eps:\n                state = torch.from_numpy(state).float()\\\n                                               .unsqueeze(0).to(device)\n                self.local.eval()\n                with torch.no_grad():\n                    action_values = self.local(state)\n                self.local.train()\n                return np.argmax(action_values.cpu().data.numpy())\n            else:\n                return random.choice(np.arange(self.action_size)) \n    ```", "```py\n     def learn(self, experiences, gamma):\n            states,actions,rewards,next_states,dones= experiences\n            # Get expected Q values from local model\n            Q_expected = self.local(states).gather(1, actions)\n            # Get max predicted Q values (for next states)\n            # from local model\n            Q_targets_next = self.local(next_states).detach()\\\n                                                    .max(1)[0].unsqueeze(1)\n            # Compute Q targets for current states\n            Q_targets = rewards+(gamma*Q_targets_next*(1-dones))\n\n            # Compute loss\n            loss = F.mse_loss(Q_expected, Q_targets)\n            # Minimize the loss\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step() \n    ```", "```py\n     def sample_experiences(self):\n            experiences = random.sample(self.memory,\n                                        k=self.batch_size)\n            states = torch.from_numpy(np.vstack([e.state \\\n                        for e in experiences if e is not \\\n                                    None])).float().to(device)\n            actions = torch.from_numpy(np.vstack([e.action \\\n                        for e in experiences if e is not \\\n                                    None])).long().to(device)\n            rewards = torch.from_numpy(np.vstack([e.reward \\\n                        for e in experiences if e is not \\\n                                    None])).float().to(device)\n            next_states=torch.from_numpy(np.vstack([e.next_state \\\n                        for e in experiences if e is not \\\n                                      None])).float().to(device)\n            dones = torch.from_numpy(np.vstack([e.done \\\n                        for e in experiences if e is not None])\\\n                           .astype(np.uint8)).float().to(device)\n            return (states, actions, rewards, next_states,dones) \n    ```", "```py\n    agent = Agent(env.observation_space.shape[0], env.action_space.n) \n    ```", "```py\n    scores = [] # list containing scores from each episode\n    scores_window = deque(maxlen=100) # last 100 scores\n    n_episodes=5000\n    max_t=5000\n    eps_start=1.0\n    eps_end=0.001\n    eps_decay=0.9995\n    eps = eps_start \n    ```", "```py\n    for i_episode in range(1, n_episodes+1):\n        state, *_ = env.reset()\n        state_size = env.observation_space.shape[0]\n        state = np.reshape(state, [1, state_size])\n        score = 0 \n    ```", "```py\n     for i in range(max_t):\n            action = agent.act(state, eps)\n            next_state, reward, done, *_ = env.step(action)\n            next_state = np.reshape(next_state, [1, state_size]) \n    ```", "```py\n     reward = reward if not done or score == 499 else -10\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            score += reward\n            if done:\n                break \n    ```", "```py\n     scores_window.append(score) # save most recent score\n        scores.append(score) # save most recent score\n        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n        print('\\rEpisode {:.2f}\\tReward {:.2f} \\tAverage Score: {:.2f} \\tEpsilon: {:.2f}'.format(i_episode, score, np.mean(scores_window), eps), end=\"\")\n        if i_episode % 100 == 0:\n            print('\\rEpisode {:.2f}\\tAverage Score: {:.2f} \\tEpsilon: {:.2f}'.format(i_episode, np.mean(scores_window), eps))\n        if i_episode>10 and np.mean(scores[-10:])>450:\n            break \n    ```", "```py\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    plt.plot(scores)\n    plt.title('Scores over increasing episodes') \n    ```", "```py\n    %pip install -qqU \"gym[atari, accept-rom-license]==0.26.2\"\n    import gym\n    import numpy as np\n    import cv2\n    from collections import deque\n    import matplotlib.pyplot as plt\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import random\n    from collections import namedtuple, deque\n    import torch.optim as optim\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    env = gym.make('PongDeterministic-v0') \n    ```", "```py\n    state_size = env.observation_space.shape[0]\n    action_size = env.action_space.n \n    ```", "```py\n    def preprocess_frame(frame):\n        bkg_color = np.array([144, 72, 17])\n        img = np.mean(frame[34:-16:2,::2]-bkg_color,axis=-1)/255.\n        resized_image = img\n        return resized_image \n    ```", "```py\n    def stack_frames(stacked_frames, state, is_new_episode):\n        # Preprocess frame\n        frame = preprocess_frame(state)\n        stack_size = 4 \n    ```", "```py\n     if is_new_episode:\n            # Clear our stacked_frames\n            stacked_frames = deque([np.zeros((80,80), dtype=np.uint8) \\\n                             for i in range(stack_size)], maxlen=4)\n            # Because we're in a new episode,\n            # copy the same frame 4x\n            for i in range(stack_size):\n                stacked_frames.append(frame)\n            # Stack the frames\n            stacked_state = np.stack(stacked_frames, \\\n                                     axis=2).transpose(2, 0, 1) \n    ```", "```py\n     else:\n            # Append frame to deque,\n            # automatically removes the #oldest frame\n            stacked_frames.append(frame)\n            # Build the stacked state\n            # (first dimension specifies #different frames)\n            stacked_state = np.stack(stacked_frames, \\\n                                     axis=2).transpose(2, 0, 1)\n        return stacked_state, stacked_frames \n    ```", "```py\n    class DQNetwork(nn.Module):\n        def __init__(self, states, action_size):\n            super(DQNetwork, self).__init__()\n\n            self.conv1 = nn.Conv2d(4, 32, (8, 8), stride=4)\n            self.conv2 = nn.Conv2d(32, 64, (4, 4), stride=2)\n            self.conv3 = nn.Conv2d(64, 64, (3, 3), stride=1)\n            self.flatten = nn.Flatten()\n            self.fc1 = nn.Linear(2304, 512)\n            self.fc2 = nn.Linear(512, action_size)\n\n        def forward(self, state):\n            x = F.relu(self.conv1(state))\n            x = F.relu(self.conv2(x))\n            x = F.relu(self.conv3(x))\n            x = self.flatten(x)\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return x \n    ```", "```py\n    class Agent():\n        **def****__init__****(self, state_size, action_size):**      \n            self.state_size = state_size\n            self.action_size = action_size\n            self.seed = random.seed(0)\n            ## hyperparameters\n            self.buffer_size = 1000\n            self.batch_size = 32\n            self.gamma = 0.99\n            self.lr = 0.0001\n            self.update_every = 4\n            self.update_every_target = 1000\n            self.learn_every_target_counter = 0\n            # Q-Network\n            self.local = DQNetwork(state_size, action_size).to(device)\n            self.target = DQNetwork(state_size, action_size).to(device)\n            self.optimizer=optim.Adam(self.local.parameters(), lr=self.lr)\n            # Replay memory\n            self.memory = deque(maxlen=self.buffer_size)\n            self.experience = namedtuple(\"Experience\", \\\n                               field_names=[\"state\", \"action\", \\\n                                \"reward\", \"next_state\", \"done\"])\n            # Initialize time step (for updating every few steps)\n            self.t_step = 0 \n    ```", "```py\n     def step(self, state, action, reward, next_state, done):\n            # Save experience in replay memory\n            self.memory.append(self.experience(state[None], \\\n                                        action, reward, \\\n                                        next_state[None], done))\n\n            # Learn every update_every time steps.\n            self.t_step = (self.t_step + 1) % self.update_every\n            if self.t_step == 0:\n        # If enough samples are available in memory, get  \n        # random subset and learn\n                if len(self.memory) > self.batch_size:\n                    experiences = self.sample_experiences()\n                    self.learn(experiences, self.gamma) \n    ```", "```py\n     def act(self, state, eps=0.):\n            # Epsilon-greedy action selection\n            if random.random() > eps:\n                state = torch.from_numpy(state).float()\\\n                             .unsqueeze(0).to(device)\n                self.local.eval()\n                with torch.no_grad():\n                    action_values = self.local(state)\n                self.local.train()\n                return np.argmax(action_values.cpu().data.numpy())\n            else:\n                return random.choice(np.arange(self.action_size)) \n    ```", "```py\n     def learn(self, experiences, gamma):\n            self.learn_every_target_counter+=1\n            states,actions,rewards,next_states,dones = experiences\n            # Get expected Q values from local model\n            Q_expected = self.local(states).gather(1, actions)\n            # Get max predicted Q values (for next states)\n            # from target model\n            **Q_targets_next =** **self****.target(next_states).detach()\\**\n                                 **.****max****(****1****)[****0****].unsqueeze(****1****)**\n            # Compute Q targets for current state\n            Q_targets = rewards+(gamma*Q_targets_next*(1-dones))\n\n            # Compute loss\n            loss = F.mse_loss(Q_expected, Q_targets)\n            # Minimize the loss\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            # ------------ update target network ------------- #\n            if self.learn_every_target_counter%1000 ==0:\n                self.target_update() \n    ```", "```py\n     def target_update(self):\n            print('target updating')\n            self.target.load_state_dict(self.local.state_dict()) \n    ```", "```py\n     def sample_experiences(self):\n            experiences = random.sample(self.memory, k=self.batch_size)\n            states = torch.from_numpy(np.vstack([e.state \\\n                        for e in experiences if e is not \\\n                               None])).float().to(device)\n            actions = torch.from_numpy(np.vstack([e.action \\\n                        for e in experiences if e is not \\\n                               None])).long().to(device)\n            rewards = torch.from_numpy(np.vstack([e.reward \\\n                        for e in experiences if e is not \\\n                               None])).float().to(device)\n            next_states=torch.from_numpy(np.vstack([e.next_state \\\n                         for e in experiences if e is not \\\n                                      None])).float().to(device)\n            dones = torch.from_numpy(np.vstack([e.done \\\n                        for e in experiences if e is not None])\\\n                           .astype(np.uint8)).float().to(device)\n            return (states, actions, rewards, next_states,dones) \n    ```", "```py\n    agent = Agent(state_size, action_size) \n    ```", "```py\n    n_episodes=5000\n    max_t=5000\n    eps_start=1.0\n    eps_end=0.02\n    eps_decay=0.995\n    scores = [] # list containing scores from each episode\n    scores_window = deque(maxlen=100) # last 100 scores\n    eps = eps_start\n    stack_size = 4\n    stacked_frames = deque([np.zeros((80,80), dtype=np.int) \\\n                            for i in range(stack_size)], maxlen=stack_size) \n    ```", "```py\n    for i_episode in range(1, n_episodes+1):\n        state, *_ = env.reset()\n        state, frames = stack_frames(stacked_frames, state, True)\n        score = 0\n        for i in range(max_t):\n            action = agent.act(state, eps)\n            next_state, reward, done, *_ = env.step(action)\n            next_state, frames = stack_frames(frames, next_state, False)\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            score += reward\n            if done:\n                break\n        scores_window.append(score) # save most recent score\n        scores.append(score) # save most recent score\n        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n        print('\\rEpisode {}\\tReward {} \\tAverage Score: {:.2f} \\\n        \\tEpsilon: {}'.format(i_episode,score,\\\n                              np.mean(scores_window),eps),end=\"\")\n        if i_episode % 100 == 0:\n            print('\\rEpisode {}\\tAverage Score: {:.2f} \\\n            \\tEpsilon: {}'.format(i_episode, \\\n                                  np.mean(scores_window), eps)) \n    ```", "```py\n    $ mv CARLA_0.9.6.tar.gz ~/Documents/\n    $ cd ~/Documents/\n    $ tar -xf CARLA_0.9.6.tar.gz\n    $ cd CARLA_0.9.6/ \n    ```", "```py\n    $ echo \"export PYTHONPATH=$PYTHONPATH:/home/$(whoami)/Documents/CARLA_0.9.6/PythonAPI/carla/dist/carla-0.9.6-py3.5-linux-x86_64.egg\" >> ~/.bashrc \n    ```", "```py\n    $ chmod +x /home/$(whoami)/Documents/CARLA_0.9.6/CarlaUE4.sh\n    $ ./home/$(whoami)/Documents/CARLA_0.9.6/CarlaUE4.sh \n    ```", "```py\n    $ cd /location/to/clone/repo/to\n    $ git clone https://github.com/cjy1992/gym-carla\n    $ cd gym-carla\n    $ pip install -r requirements.txt\n    $ pip install -e . \n    ```", "```py\n    $ python test.py \n    ```", "```py\nfrom torch_snippets import *\nclass DQNetworkImageSensor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.n_outputs = 9\n        self.image_branch = nn.Sequential(\n                            nn.Conv2d(3, 32, (8, 8), stride=4),\n                            nn.ReLU(inplace=True),\n                            nn.Conv2d(32, 64, (4, 4), stride=2),\n                            nn.ReLU(inplace=True),\n                            nn.Conv2d(64,128,(3, 3),stride=1),\n                            nn.ReLU(inplace=True),\n                            nn.AvgPool2d(8),\n                            nn.ReLU(inplace=True),\n                            nn.Flatten(),\n                            nn.Linear(1152, 512),\n                            nn.ReLU(inplace=True),\n                            nn.Linear(512, self.n_outputs)\n                        )\n        self.lidar_branch = nn.Sequential(\n                            nn.Conv2d(3, 32, (8, 8), stride=4),\n                            nn.ReLU(inplace=True),\n                            nn.Conv2d(32,64,(4, 4),stride=2),\n                            nn.ReLU(inplace=True),\n                            nn.Conv2d(64,128,(3, 3),stride=1),\n                            nn.ReLU(inplace=True),\n                            nn.AvgPool2d(8),\n                            nn.ReLU(inplace=True),\n                            nn.Flatten(),\n                            nn.Linear(1152, 512),\n                            nn.ReLU(inplace=True),\n                            nn.Linear(512, self.n_outputs)\n                        )\n        self.sensor_branch = nn.Sequential(\n                                nn.Linear(4, 64),\n                                nn.ReLU(inplace=True),\n                                nn.Linear(64, self.n_outputs)\n                            )\n    def forward(self, image, lidar=None, sensor=None):\n        x = self.image_branch(image)\n        if lidar is None:\n            y = 0\n        else:\n            y = self.lidar_branch(lidar)\n        z = self.sensor_branch(sensor)\n        return x + y + z \n```", "```py\n    import numpy as np\n    import random\n    from collections import namedtuple, deque\n    import torch\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from model1 import DQNetworkImageSensor\n    BUFFER_SIZE = int(1e3) # replay buffer size\n    BATCH_SIZE = 256 # minibatch size\n    GAMMA = 0.99 # discount factor\n    TAU = 1e-2 # for soft update of target parameters\n    LR = 5e-4 # learning rate\n    UPDATE_EVERY = 50 # how often to update the network\n    ACTION_SIZE = 2\n    device = 'cuda' if torch.cuda.is_available() else 'cpu' \n    ```", "```py\n    class Actor():\n        def __init__(self):      \n            # Q-Network\n            self.qnetwork_local=DQNetworkImageSensor().to(device)\n            self.qnetwork_target=DQNetworkImageSensor().to(device)\n            self.optimizer = optim.Adam(self.qnetwork_local.parameters(),\n                                                                  lr=LR)\n            # Replay memory\n            self.memory= ReplayBuffer(ACTION_SIZE,BUFFER_SIZE, \\\n                                       BATCH_SIZE, 10)\n            # Initialize time step\n            # (for updating every UPDATE_EVERY steps)\n            self.t_step = 0\n\n        def step(self, state, action, reward, next_state, done):\n            # Save experience in replay memory\n            self.memory.add(state, action, reward, next_state, done)\n\n            # Learn every UPDATE_EVERY time steps.\n            self.t_step = (self.t_step + 1) % UPDATE_EVERY\n            if self.t_step == 0:\n      # If enough samples are available in memory,\n      # get random subset and learn\n                if len(self.memory) > BATCH_SIZE:\n                    experiences = self.memory.sample()\n                    self.learn(experiences, GAMMA) \n    ```", "```py\n     def act(self, state, eps=0.):\n            images,lidars sensors=state['image'], \\\n                                  state['lidar'],state['sensor']\n            images = torch.from_numpy(images).float()\\\n                          .unsqueeze(0).to(device)\n            lidars = torch.from_numpy(lidars).float()\\\n                          .unsqueeze(0).to(device)\n            sensors = torch.from_numpy(sensors).float()\\\n                           .unsqueeze(0).to(device)\n            self.qnetwork_local.eval()\n            with torch.no_grad():\n                action_values = self.qnetwork_local(images, \\\n                                  lidar=lidars, sensor=sensors)\n            self.qnetwork_local.train()\n            # Epsilon-greedy action selection\n            if random.random() > eps:\n                return np.argmax(action_values.cpu().data.numpy())\n            else:\n                return random.choice(np.arange(self.qnetwork_local.n_outputs)) \n    ```", "```py\n     def learn(self, experiences, gamma):\n            states,actions,rewards,next_states,dones= experiences\n            images, lidars, sensors = states\n            next_images, next_lidars, next_sensors = next_states\n            # Get max predicted Q values (for next states)\n            # from target model\n            Q_targets_next = self.qnetwork_target(next_images,\n                                            lidar=next_lidars,\n                                         sensor=next_sensors)\\\n                                .detach().max(1)[0].unsqueeze(1)\n            # Compute Q targets for current states\n            Q_targets= rewards +(gamma*Q_targets_next*(1-dones))\n            # Get expected Q values from local model\n            # import pdb; pdb.set_trace()\n            Q_expected=self.qnetwork_local(images,lidar=lidars,\n                                     sensor=sensors).gather(1,actions.long())\n            # Compute loss\n            loss = F.mse_loss(Q_expected, Q_targets)\n            # Minimize the loss\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            # ------------ update target network ------------- #\n            self.soft_update(self.qnetwork_local,\n                             self.qnetwork_target, TAU)\n        def soft_update(self, local_model, target_model, tau):\n            for target_param, local_param in \\\n                zip(target_model.parameters(), \\\n                local_model.parameters()):\n                target_param.data.copy_(tau*local_param.data + \\\n                                    (1.0-tau)*target_param.data) \n    ```", "```py\n    class ReplayBuffer:\n        \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n        def __init__(self, action_size, buffer_size, batch_size, seed):\n            self.action_size = action_size\n            self.memory = deque(maxlen=buffer_size)\n            self.batch_size = batch_size\n            self.experience = namedtuple(\"Experience\", \\\n                              field_names=[\"state\", \"action\", \\\n                              \"reward\",\"next_state\", \"done\"])\n            self.seed = random.seed(seed)\n\n        def add(self, state, action, reward, next_state, done):\n            \"\"\"Add a new experience to memory.\"\"\"\n            e = self.experience(state, action, reward,\n                                next_state, done)\n            self.memory.append(e)\n\n        def sample(self):\n            experiences = random.sample(self.memory,\n                                        k=self.batch_size)\n            images = torch.from_numpy(np.vstack([e.state['image'][None] \\\n                     for e in experiences if e is not None]))\\\n                        .float().to(device)\n            lidars = torch.from_numpy(np.vstack([e.state['lidar'][None] \\\n                     for e in experiences if e is not None]))\\\n                        .float().to(device)\n            sensors = torch.from_numpy(np.vstack([e.state['sensor'] \\\n                     for e in experiences if e is not None]))\\\n                        .float().to(device)\n            **states = [images, lidars, sensors]**\n            actions = torch.from_numpy(np.vstack(\\\n                        [e.action for e in experiences \\\n                         if e is not None])).long().to(device)\n            rewards = torch.from_numpy(np.vstack(\\\n                        [e.reward for e in experiences \\\n                         if e is not None])).float().to(device)\n            next_images = torch.from_numpy(np.vstack(\\\n                        [e.next_state['image'][None] \\\n                         for e in experiences if e is not\n                         None])).float().to(device)\n            next_lidars = torch.from_numpy(np.vstack(\\\n                        [e.next_state['lidar'][None] \\\n                         for e in experiences if e is not \\\n                         None])).float().to(device)\n            next_sensors = torch.from_numpy(np.vstack(\\\n                        [e.next_state['sensor'] \\\n                         for e in experiences if e is not \\\n                         None])).float().to(device)\n            next_states = [next_images, next_lidars, next_sensors]\n            dones = torch.from_numpy(np.vstack([e.done \\\n                         for e in experiences if e is not \\\n                     None]).astype(np.uint8)).float().to(device)     \n            return (states,actions, rewards, next_states, dones)\n        def __len__(self):\n            \"\"\"Return the current size of internal memory.\"\"\"\n            return len(self.memory) \n    ```", "```py\n    %pip install -U \"gym==0.26.2\"\n    import gym\n    import gym_carla\n    import carla\n    from model import DQNetworkState\n    from actor import Actor\n    from torch_snippets import *\n    params = {\n        'number_of_vehicles': 10,\n        'number_of_walkers': 0,\n        'display_size': 256, # screen size of bird-eye render\n        'max_past_step': 1, # the number of past steps to draw\n        'dt': 0.1, # time interval between two frames\n        'discrete': True, # whether to use discrete control space\n        # discrete value of accelerations\n        'discrete_acc': [-1, 0, 1],\n        # discrete value of steering angles\n        'discrete_steer': [-0.3, 0.0, 0.3],\n        # define the vehicle\n        'ego_vehicle_filter': 'vehicle.lincoln*',\n        'port': 2000, # connection port\n        'town': 'Town03', # which town to simulate\n        'task_mode': 'random', # mode of the task\n        'max_time_episode': 1000, # maximum timesteps per episode\n        'max_waypt': 12, # maximum number of waypoints\n        'obs_range': 32, # observation range (meter)\n        'lidar_bin': 0.125, # bin size of lidar sensor (meter)\n        'd_behind': 12, # distance behind the ego vehicle (meter)\n        'out_lane_thres': 2.0, # threshold for out of lane\n        'desired_speed': 8, # desired speed (m/s)\n        'max_ego_spawn_times': 200, # max times to spawn vehicle\n        'display_route': True, # whether to render desired route\n        'pixor_size': 64, # size of the pixor labels\n        'pixor': False, # whether to output PIXOR observation\n    }\n    # Set gym-carla environment\n    env = gym.make('carla-v0', params=params) \n    ```", "```py\n    load_path = None # 'car-v1.pth'\n    # continue training from an existing model\n    save_path = 'car-v2.pth'\n    actor = Actor()\n    if load_path is not None:\n        actor.qnetwork_local.load_state_dict(torch.load(load_path))\n        actor.qnetwork_target.load_state_dict(torch.load(load_path))\n    else:\n        pass \n    ```", "```py\n    n_episodes = 100000\n    def dqn(n_episodes=n_episodes, max_t=1000, eps_start=1, \\\n            eps_end=0.01, eps_decay=0.995):\n        scores = [] # list containing scores from each episode\n        scores_window = deque(maxlen=100) # last 100 scores\n        eps = eps_start # Initialize epsilon\n        for i_episode in range(1, n_episodes+1):\n            state, *_ = env.reset() \n    ```", "```py\n     image, lidar, sensor = state['camera'], \\\n                                   state['lidar'], \\\n                                   state['state']\n            image, lidar = preprocess(image), preprocess(lidar)\n            state_dict = {'image': image, 'lidar': lidar, \\\n                          'sensor': sensor}\n            score = 0\n            for t in range(max_t):\n                action = actor.act(state_dict, eps) \n    ```", "```py\n     next_state, reward, done, *_ = env.step(action)\n                image, lidar, sensor = next_state['camera'], \\\n                                       next_state['lidar'], \\\n                                       next_state['state']\n                image,lidar = preprocess(image), preprocess(lidar)\n                next_state_dict= {'image':image,'lidar':lidar, \\\n                                  'sensor': sensor}\n                actor.step(state_dict, action, reward, \\\n                           next_state_dict, done)\n                state_dict = next_state_dict\n                score += reward\n                if done:\n                    break\n            scores_window.append(score) # save most recent score\n            scores.append(score) # save most recent score\n            eps = max(eps_end, eps_decay*eps) # decrease epsilon\n            if i_episode % 100 == 0:\n                log.record(i_episode, mean_score=np.mean(scores_window))\n                torch.save(actor.qnetwork_local.state_dict(), save_path) \n    ```", "```py\n    dqn() \n    ```"]