["```py\nprint(\"rows:\",data.shape[0],\" columns:\", data.shape[1])\n```", "```py\n    data_clean = data.drop(columns=[\"ID\", \"SEX\"])\n    data_clean.head()\n    ```", "```py\n    total = data_clean.isnull().sum()\n    percent = (data_clean.isnull().sum()/\n                     data_clean.isnull().count()*100)\n    pd.concat([total, percent], axis=1, \n    keys=['Total', 'Percent']).transpose()\n    ```", "```py\n    target = data_clean[\"default payment next month\"]\n    yes = target[target == 1].count()\n    no = target[target == 0].count()\n    print(\"yes %: \" + str(yes/len(target)*100) + \" - no %: \" + str(no/len(target)*100))\n    ```", "```py\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots()\n    plt.bar(\"yes\", yes)\n    plt.bar(\"no\", no)\n    ax.set_yticks([yes,no])\n    plt.show()\n    ```", "```py\ndata_yes = data_clean[data_clean[\"default payment next month\"]                       == 1]\ndata_no = data_clean[data_clean[\"default payment next month\"]                      == 0]\nover_sampling = data_yes.sample(no, replace=True, random_state                                 = 0)\ndata_resampled = pd.concat([data_no, over_sampling], axis=0)\n```", "```py\n    X = data_clean.drop(columns=[\"default payment next month\"])\n    y = data_clean[\"default payment next month\"] \n    ```", "```py\n    X = (X - X.min())/(X.max() - X.min())\n    X.head()\n    ```", "```py\nfinal_data = pd.concat([X, y], axis=1)\nfinal_data.to_csv(\"dccc_prepared.csv\", index=False)\n```", "```py\nimport torch.nn as nn\nmodel = nn.Sequential(nn.Linear(D_i, D_h),\n                      nn.ReLU(),\n                      nn.Linear(D_h, D_o),\n                      nn.Softmax())\n```", "```py\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nclass Classifier(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.hidden_1 = nn.Linear(input_size, 100)\n        self.hidden_2 = nn.Linear(100, 100)\n        self.hidden_3 = nn.Linear(100, 50)\n        self.hidden_4 = nn.Linear(50,50)\n        self.output = nn.Linear(50, 2)\n\n        self.dropout = nn.Dropout(p=0.1)\n        #self.dropout_2 = nn.Dropout(p=0.1)\n\n    def forward(self, x):\n        z = self.dropout(F.relu(self.hidden_1(x)))\n        z = self.dropout(F.relu(self.hidden_2(z)))\n        z = self.dropout(F.relu(self.hidden_3(z)))\n        z = self.dropout(F.relu(self.hidden_4(z)))\n        out = F.log_softmax(self.output(z), dim=1)\n\n        return out\n```", "```py\nmodel = Classifier()\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.005)\nepochs = 10\nbatch_size = 100\n```", "```py\n        train_losses, dev_losses, train_acc, dev_acc= [], [], [], []\n        for e in range(epochs):\n            X, y = shuffle(X_train, y_train)\n            running_loss = 0\n            running_acc = 0\n            iterations = 0\n            for i in range(0, len(X), batch_size):\n                iterations += 1\n                b = i + batch_size\n                X_batch = torch.tensor(X.iloc[i:b,:].values).float()\n                y_batch = torch.tensor(y.iloc[i:b].values)\n                pred = model(X_batch)\n                loss = criterion(pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item()\n                ps = torch.exp(pred)\n                top_p, top_class = ps.topk(1, dim=1)\n                running_acc += accuracy_score(y_batch, top_class)\n            dev_loss = 0\n            acc = 0\n            with torch.no_grad():\n                pred_dev = model(X_dev_torch)\n                dev_loss = criterion(pred_dev, y_dev_torch)\n                ps_dev = torch.exp(pred_dev)\n                top_p, top_class_dev = ps_dev.topk(1, dim=1)\n                acc = accuracy_score(y_dev_torch, top_class_dev)\n            train_losses.append(running_loss/iterations)\n            dev_losses.append(dev_loss)\n            train_acc.append(running_acc/iterations)\n            dev_acc.append(acc)\n            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n                  \"Training Loss: {:.3f}.. \".format(running_loss/iterations),\n                  \"Validation Loss: {:.3f}.. \".format(dev_loss),\n                  \"Training Accuracy: {:.3f}.. \".format(running_acc/iterations),\n                  \"Validation Accuracy: {:.3f}\".format(acc))\n        ```", "```py\n    import pandas as pd\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import shuffle\n    from sklearn.metrics import accuracy_score\n    import torch\n    from torch import nn, optim\n    import torch.nn.functional as F\n    import matplotlib.pyplot as plt\n    ```", "```py\n    Bayes error (BE) = 0.15\n    Training set error (TSE) = 1 – 0.715 = 0.285\n    Validation set error (VSE) = 1 – 0.706 = 0.294\n    ```", "```py\n    Bias = TSE – BE = 0.135\n    Variance = VSE – TSE = 0.009\n    ```", "```py\n    checkpoint = {\"input\": X_train.shape[1],\n                  \"state_dict\": model.state_dict()}\n    ```", "```py\n    torch.save(checkpoint, \"checkpoint.pth\")\n    ```", "```py\n    def load_model_checkpoint(path):\n        checkpoint = torch.load(path)\n        model = final_model.Classifier(checkpoint[\"input\"],\n                           checkpoint[\"output\"],\n                          checkpoint[\"hidden\"])\n        model.load_state_dict(checkpoint[\"state_dict\"])\n        return model\n    model = load_model_checkpoint(\"checkpoint.pth\")\n    ```", "```py\ntraced_script = torch.jit.trace(model, example)\n```", "```py\nprediction = traced_script(input)\n```", "```py\n    torch.tensor([[0.0606, 0.5000, 0.3333, 0.4828, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.1651, 0.0869, 0.0980, 0.1825, 0.1054, 0.2807, 0.0016, 0.0000, 0.0033, 0.0027, 0.0031, 0.0021]]).float()\n    ```"]