- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image Generation Using GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about manipulating an image using neural
    style transfer and super-imposed the expression in one image on another. However,
    what if we give the network a bunch of images and ask it to come up with an entirely
    new image, all on its own?
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks (GANs) are a step toward achieving the feat
    of generating an image given a collection of images. In this chapter, we will
    start by learning about the idea behind what makes GANs work, before building
    one from scratch. This is a vast field that is expanding even as we write this
    book. This chapter will lay the foundation of GANs by covering three variants;
    we will learn about more advanced GANs and their applications in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using GANs to generate handwritten digits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using DCGANs to generate face images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing conditional GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code snippets within this chapter are available in the `Chapter12` folder
    of the Github repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand GANs, we need to understand two terms: generator and discriminator.
    First, we should have a reasonable sample of images (100-1000 images) of an object.
    A **generative network** (generator) learns representation from a sample of images
    and then generates images similar to the sample of images. A **discriminator network**
    (discriminator) is one that looks at the image generated by the generator network
    and the original sample of images and classifies images as original or generated
    (fake) ones.'
  prefs: []
  type: TYPE_NORMAL
- en: The generator network tries to generate images in such a way that the discriminator
    classifies the images as real. The discriminator network tried to classify the
    generated images as fake and the images in the original sample as real.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, the adversarial term in GAN represents the opposite nature of the
    two networks—*a generator network, which generates images to fool the discriminator
    network, and a discriminator network that classifies each image by saying whether
    the image is generated or is an original*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the process employed by GANs through the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18457_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Typical GAN workflow'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the generator network is generating images from random
    noise as input. A discriminator network is looking at the images generated by
    the generator and comparing them with real data (a sample of images that are provided)
    to specify whether the generated image is real or fake. The generator tries to
    generate as many realistic images as possible, while the discriminator tries to
    detect which of the images that are generated by the generator are fake. This
    way, the generator learns to generate as many realistic images as possible by
    learning from what the discriminator looks at to identify whether an image is
    fake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, the generator and discriminator are trained alternately within each
    step of training. This way, it becomes a cops-and-robbers game, where the generator
    is the robber trying to generate fake data, while the discriminator is the cop
    trying to identify the available data as real or fake. The steps involved in training
    GANs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the generator (and not the discriminator) to generate images such that
    the discriminator classifies the images as real.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the discriminator (and not the generator) to classify the images that
    the generator generates as fake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the process until an equilibrium is achieved.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s now understand how we compute the loss values for both the generator
    and discriminator to train both the networks together using the following diagram
    and steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: GAN training workflow (dotted lines represent training, solid
    lines process)'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding scenario, when the discriminator can detect generated images
    really well, the loss corresponding to the generator is much higher when compared
    to the loss corresponding to the discriminator. Thus, the gradients adjust in
    such a way that the generator would have a lower loss. However, it would tip the
    discriminator loss to a higher side. In the next iteration, the gradients adjust
    so that the discriminator loss is lower. This way, the generator and discriminator
    keep getting trained until a point where the generator generates realistic images
    and the discriminator cannot distinguish between a real or a generated image.
  prefs: []
  type: TYPE_NORMAL
- en: With this understanding, let’s generate images relating to the MNIST dataset
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using GANs to generate handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To generate images of handwritten digits, we will leverage the same network
    as we learned about in the previous section. The strategy we will adopt is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Import MNIST data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize random noise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the generator model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the discriminator model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the two models alternately.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the model train until the generator and discriminator losses are largely
    the same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s execute each of the preceding steps in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `Handwritten_digit_generation_using_GAN.ipynb`
    in the `Chapter12` folder in this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code is moderately lengthy. We strongly recommend you execute the notebook
    in GitHub to reproduce the results while you understand the steps to perform and
    the explanation of various code components from the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and define the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `MNIST` data and define the dataloader with built-in data transformation
    so that the input data is scaled to a mean of 0.5 and a standard deviation of
    0.5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Discriminator` model class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A summary of the discriminator network is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, in the preceding code, in place of `ReLU`, we have used `LeakyReLU`
    (an empirical evaluation of different types of ReLU activations is available here:
    [https://arxiv.org/pdf/1505.00853](https://arxiv.org/pdf/1505.00853)) as the activation
    function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18457_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Summary of the discriminator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `Generator` model class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the generator takes a 100-dimensional input (which is of random noise)
    and generates an image from the input. A summary of the generator model is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18457_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Summary of the generator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function to generate random noise and register it to the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to train the discriminator, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The discriminator training function (`discriminator_train_step`) takes real
    data (`real_data`) and fake data (`fake_data`) as input:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the gradients:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict on the real data (`real_data`) and calculate the loss (`error_real`)
    before performing backpropagation on the loss value:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we calculate the discriminator loss on real data, we expect the discriminator
    to predict an output of `1`. Hence, the discriminator loss on real data is calculated
    by expecting the discriminator to predict the output to be `1` using `torch.ones`
    during discriminator training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Predict on the fake data (`fake_data`) and calculate the loss (`error_fake`)
    before performing backpropagation on the loss value:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we calculate the discriminator loss on fake data, we expect the discriminator
    to predict an output of `0`. Hence, the discriminator loss on fake data is calculated
    by expecting the discriminator to predict an output of `0` using `torch.zeros`
    during discriminator training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Update the weights and return the overall loss (summing up the loss values
    of `error_real` on `real_data` and `error_fake` on `fake_data`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the generator model in the following way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the generator training function (`generator_train_step`) that takes
    fake data (`fake_data`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the gradients of the generator optimizer:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the output of the discriminator on fake data (`fake_data`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the generator loss value by passing `prediction` and the expected
    value as `torch.ones` since we want to fool the discriminator to output a value
    of `1` when training the generator:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform backpropagation, update the weights, and return the error:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the model objects, the optimizer for each generator and discriminator,
    and the loss function to optimize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the models over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loop through 200 epochs (`num_epochs`) over the `data_loader` function obtained
    in *step 2*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load real data (`real_data`) and fake data, where fake data (`fake_data`) is
    obtained by passing `noise` (with a batch size of the number of data points in
    `real_data`: `len(real_data)`) through the `generator` network. Note that it is
    important to run `fake_data.detach()`, or else training will not work. On detaching,
    we are creating a fresh copy of the tensor so that when `error.backward()` is
    called in `discriminator_train_step`, the tensors associated with the generator
    (which create `fake_data`) are not affected:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the discriminator using the `discriminator_train_step` function defined
    in *step 6*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have trained the discriminator, let’s train the generator in this
    step. Generate a new set of fake images (`fake_data`) from noisy data and train
    the generator using `generator_train_step` defined in *step 6*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Record the losses:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The discriminator and generator losses over increasing epochs are as follows
    (you can refer to the digital version of the book for the colored image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Discriminator and Generator loss over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visualize the fake data post-training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shape, arrow  Description automatically generated](img/B18457_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Generated digits'
  prefs: []
  type: TYPE_NORMAL
- en: From this, we can see that we can leverage GANs to generate images that are
    realistic, but still with some scope for improvement. In the next section, we
    will learn about using deep convolutional GANs to generate more realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: Using DCGANs to generate face images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned about generating images using GANs. However,
    we have already seen in *Chapter 4*, *Introducing Convolutional Neural Networks*,
    that Convolutional Neural Networks (CNNs) perform better in the context of images
    when compared to vanilla neural networks. In this section, we will learn about
    generating images using Deep Convolutional Generative Adversarial Networks (DCGANs),
    which use convolution and pooling operations in the model.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s understand the technique we will leverage to generate an image
    using a set of 100 random numbers (we chose 100 random numbers so that the network
    has a reasonable number of values to generate images. We encourage readers to
    experiment with different amounts of random numbers and see the result). We will
    first convert noise into a shape of *batch size x 100 x 1 x 1*.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for appending additional channel information in DCGANs and not doing
    it in the GAN section is that we will leverage CNNs in this section, which requires
    inputs in the form of batch size x channels x height x width.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we convert the generated noise into an image by leveraging
  prefs: []
  type: TYPE_NORMAL
- en: '`ConvTranspose2d`. As we learned in *Chapter 9*, *Image Segmentation*, `ConvTranspose2d`
    does the opposite of a convolution operation, which is to take input with a smaller
    feature map size (height x width) and upsample it to that of a larger size using
    a predefined kernel size, stride, and padding. This way, we would gradually convert
    a vector from a shape of batch size x 100 x 1 x 1 into a shape of batch size x
    3 x 64 x 64\. With this, we have taken a random noise vector of size 100 and converted
    it into an image of a face.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this understanding, let’s now build a model to generate images of faces:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `Face_generation_using_DCGAN.ipynb` in the
    `Chapter12` folder in this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    understand the steps to perform and the explanation of various code components
    from the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download and extract the face images (a dataset that we have collated by generating
    faces of random people):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A sample of the images is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, posing, person, different  Description automatically
    generated](img/B18457_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: Sample male and female images'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the dataset and dataloader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ensure that we crop the images so that we retain only the faces and discard
    additional details in the image. First, we will download the cascade filter (more
    on cascade filters can be found in the *Using OpenCV Utilities for Image Analysis*
    PDF on GitHub), which will help with identifying faces within an image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new folder and dump all the cropped face images into the new folder:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A sample of the cropped faces is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of people''s faces  Description automatically generated with medium
    confidence](img/B18457_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Cropped male and female faces'
  prefs: []
  type: TYPE_NORMAL
- en: Note that by cropping and keeping the faces only, we are retaining only the
    information that we want to generate. This way, we are reducing the complexities
    that the DCGAN would have to learn about.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the transformation to perform on each image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the `Faces` dataset class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Create the dataset object: `ds`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the `dataloader` class as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define weight initialization so that the weights have a smaller spread as mentioned
    in the details of the adversarial training section at [https://arxiv.org/pdf/1511.06434](https://arxiv.org/pdf/1511.06434):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Discriminator` model class, which takes an image of a shape of
    batch size x 3 x 64 x 64 and predicts whether it is real or fake:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain a summary of the defined model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Summary of the discriminator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `Generator` model class that generates fake images from an input
    of shape batch size x 100 x 1 x 1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain a summary of the defined model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Summary of the generator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have leveraged `ConvTranspose2d` to gradually upsample an array
    so that it closely resembles an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the functions to train the generator (`generator_train_step`) and the
    discriminator (`discriminator_train_step`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we are performing a `.squeeze` operation on top of the
    prediction as the output of the model has a shape of batch size x 1 x 1 x 1 and
    it needs to be compared to a tensor that has a shape of batch size x 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the generator and discriminator model objects, the optimizers, and the
    loss function of the discriminator to be optimized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the models over increasing epochs, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loop through 25 epochs over the `dataloader` function defined in *step 3*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load real data (`real_data`) and generate fake data (`fake_data`) by passing
    through the generator network:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the major difference between vanilla GANs and DCGANs when generating
    `real_data` is that we did not have to flatten `real_data` in the case of DCGANs
    as we are leveraging CNNs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the discriminator using the `discriminator_train_step` function defined
    in *step 7*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a new set of images (`fake_data`) from the noisy data (`torch.randn(len(real_data))`)
    and train the generator using the `generator_train_step` function defined in *step
    7*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Record the losses:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output (you can refer to the digital
    version of the book for the colored image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Discriminator and generator loss over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in this setting, the variation in generator and discriminator losses
    does not follow the pattern that we have seen in the case of handwritten digit
    generation on account of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We are dealing with bigger images (images that are 64 x 64 x 3 in shape when
    compared to images of 28 x 28 x 1 shape, which we saw in the previous section).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Digits have fewer variations when compared to the features that are present
    in the image of a face.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information in handwritten digits is available in only a minority of pixels
    when compared to the information in images of a face.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the training process is complete, generate a sample of images using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following set of images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person''s face  Description automatically generated](img/B18457_12_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Generated images from the trained DCGAN model'
  prefs: []
  type: TYPE_NORMAL
- en: Note that while the generator generated images of a face from random noise,
    the images are decent but still not sufficiently realistic. One potential reason
    is that not all input images have the same face alignment. As an exercise, we
    suggest you train the DCGAN only on those images where there is no tilted face
    and the person is looking straight into the camera in the original image.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we suggest you try and contrast the generated images with high
    discriminator scores to the ones with low discriminator scores.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned about generating images of a face. However,
    we cannot specify the generation of an image that is of interest to us (for example,
    a man with a beard). In the next section, we will work toward generating images
    of a specific class.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing conditional GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a scenario where we want to generate an image of a class of our interest;
    for example, an image of a cat, a dog, or a man with spectacles. How do we specify
    that we want to generate an image of interest to us? **Conditional GANs** come
    to the rescue in this scenario. For now, let’s assume that we have the images
    of male and female faces only, along with their corresponding labels. In this
    section, we will learn about generating images of a specified class of interest
    from random noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy we adopt is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the label of the image we want to generate as a one-hot-encoded version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the label through an embedding layer to generate a multi-dimensional representation
    of each class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate random noise and concatenate with the embedding layer generated in
    the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model just like we did in the previous sections, but this time with
    the noise vector concatenated with the embedding of the class of image we wish
    to generate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we will code up the preceding strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `Face_generation_using_Conditional_GAN.ipynb`
    in the `Chapter12` folder in this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    We strongly recommend you execute the notebook in GitHub to reproduce the results
    while you understand the steps to perform and the explanation of various code
    components from the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the images and the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the dataset and dataloader, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Store the male and female image paths:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Ensure that we crop the images so that we retain only faces and discard additional
    details in an image. First, we will download the cascade filter (more on cascade
    filters can be found in the *Using OpenCV Utilities for Image Analysis* PDF on
    GitHub, which will help in identifying faces within an image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create two new folders (one corresponding to male and another for female images)
    and dump all the cropped face images into the respective folders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the transformation to perform on each image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `Faces` dataset class that returns the image and the corresponding
    gender of the person in it:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `ds` dataset and `dataloader`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the weight initialization method (just like we did in the *Using DCGANs
    to generate face images* section) so that we do not have a widespread variation
    across randomly initialized weight values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Discriminator` model class as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the model architecture:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the model class, we have an additional parameter, `emb_size`, present
    in conditional GANs and not in DCGANs. `emb_size` represents the number of embeddings
    into which we convert the input class label (the class of image we want to generate),
    which is stored as `label_embeddings`. The reason we convert the input class label
    from a one-hot-encoded version to embeddings of a higher dimension is that the
    model has a higher degree of freedom to learn and adjust to deal with different
    classes.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: While the model class, to a large extent, remains the same as what we have seen
    in DCGANs, we are initializing another model (`model2`) that does the classification
    exercise. There will be more about how the second model helps after we discuss
    the `forward` method next. You will also understand the reason why `self.model2`
    has 288 values as input after you go through the following `forward` method and
    the summary of the model.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the `forward` method that takes the image and the label of the image
    as input:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the `forward` method defined, we are fetching the output of the first model
    (`self.model(input)`) and the output of passing `labels` through `label_embeddings`
    and then concatenating the outputs. Next, we are passing the concatenated outputs
    through the second model (`self.model2`) we have defined earlier that fetches
    us the discriminator output.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Obtain the summary of the defined model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18457_12_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: Summary of the discriminator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `self.model2` takes an input of 288 values as the output of `self.model`
    has 256 values per data point, which is then concatenated with the 32 embedding
    values of the input class label, resulting in 256 + 32 = 288 input values to `self.model2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `Generator` network class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `__init__` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding code, we are using `nn.Embedding` to convert the
    2D input (which is of classes) to a 32-dimensional vector (`self.emb_size`).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding code, we have leveraged `nn.ConvTranspose2d` to upscale
    toward fetching an image as output.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apply weight initialization:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `forward` method, which takes the noise values (`input_noise`) and
    input label (`labels`) as input and generates the output of the image:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain a summary of the defined `generator` function:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18457_12_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: Summary of Generator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function (`noise`) to generate random noise with 100 values and register
    it to the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the function to train the discriminator:: `discriminator_train_step`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The discriminator takes four inputs—real images (`real_data`), real labels
    (`real_labels`), fake images (`fake_data`), and fake labels (`fake_labels`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are resetting the gradient corresponding to the discriminator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the loss value corresponding to predictions on the real data (`prediction_real`).
    The loss value output when `real_data` and `real_labels` are passed through the
    `discriminator` network is compared with the expected value of `(torch.ones(len(real_data),1).to(device))`
    to obtain `error_real` before performing backpropagation:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the loss value corresponding to predictions on the fake data (`prediction_fake`).
    The loss value output when `fake_data` and `fake_labels` are passed through the
    `discriminator` network is compared with the expected value of `(torch.zeros(len(fake_data),1).to(device))`
    to obtain `error_fake` before performing backpropagation:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update weights and return the loss values:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training steps for the generator where we pass the fake images (`fake_data`)
    along with the fake labels (`fake_labels`) as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the `generator_train_step` function is similar to `discriminator_train_step`,
    with the exception that this has an expectation of `torch.ones(len(fake_data),1).to(device))`
    as output in place of zeros given that we are training the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `generator` and `discriminator` model objects, the loss optimizers,
    and the `loss` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, `while` defining `fixed_fake_labels`, we are specifying
    that half of the images correspond to one class (class 0) and the rest to another
    class (class 1). Additionally, we are defining `fixed_noise`, which will be used
    to generate images from random noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the model over increasing epochs (`n_epochs`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Specify the length of `dataloader`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through the batch of images along with their labels:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify `real_data` and `real_labels`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize `fake_data` and `fake_labels`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the discriminator using the `discriminator_train_step` function defined
    in *step 7* to calculate discriminator loss (`d_loss`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Regenerate fake images (`fake_data`) and fake labels (`fake_labels`) and train
    the generator using the `generator_train_step` function defined in *step 8* to
    calculate the generator loss (`g_loss`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Log the loss metrics as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we train the model, generate the male and female images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code, we are passing the noise (`fixed_noise`) and labels
    (`fixed_fake_labels`) to the generator to fetch the `fake` images, which are as
    follows at the end of 25 epochs of training the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person''s face  Description automatically generated](img/B18457_12_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: Generated male and female faces'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding image, we can see that the first 32 images correspond to
    male images, while the next 32 correspond to female images, which substantiates
    the fact that the conditional GANs performed as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about leveraging two different neural networks to
    generate new images of handwritten digits using GANs. Next, we generated realistic
    faces using DCGANs. Finally, we learned about conditional GANs, which help us
    in generating images of a certain class. Having generated images using different
    techniques, we could still see that the generated images were not sufficiently
    realistic. Furthermore, while we generated images by specifying the class of images
    we want to generate in conditional GANs, we are still not in a position to perform
    image translation, where we ask to replace one object in the image with another
    one, with everything else left as is. In addition, we are yet to have an image
    generation mechanism where the number of classes (styles) to generate is more
    unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about generating images that are more realistic
    using some of the latest variants of GANs. In addition, we will learn about generating
    images of different styles in a more unsupervised manner.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What happens if the learning rate of generator and discriminator models is high?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a scenario where the generator and discriminator are very well trained, what
    is the probability of a given image being real?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use `ConvTranspose2d` in generating images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we have embeddings with a high embedding size than the number of classes
    in conditional GANs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can we generate images of men with beards?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we have Tanh activation in the last layer in the generator and not ReLU
    or sigmoid?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why did we get realistic images even though we did not denormalize the generated
    data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens if we do not crop faces corresponding to images before training
    the GAN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do the weights of the discriminator not get updated when the training generator
    is updated (as the `generator_train_step` function involves the discriminator
    network)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we fetch losses on both real and fake images while training the discriminator
    but only the loss on fake images while training the generator?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
