- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Image Generation Using GANs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 生成图像
- en: In the previous chapter, we learned about manipulating an image using neural
    style transfer and super-imposed the expression in one image on another. However,
    what if we give the network a bunch of images and ask it to come up with an entirely
    new image, all on its own?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何使用神经风格转移操作图片，并将一个图像中的表情叠加到另一个图像上。但是，如果我们给网络一堆图片，并要求它自行创造出一个全新的图片，会怎样呢？
- en: Generative adversarial networks (GANs) are a step toward achieving the feat
    of generating an image given a collection of images. In this chapter, we will
    start by learning about the idea behind what makes GANs work, before building
    one from scratch. This is a vast field that is expanding even as we write this
    book. This chapter will lay the foundation of GANs by covering three variants;
    we will learn about more advanced GANs and their applications in the next chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）是实现给定一组图片生成图像的尝试。在本章中，我们将从理解使得 GANs 起作用的原理开始，然后从头开始构建一个。这是一个不断扩展的广阔领域，即使我们在撰写本书时，也在扩展中。本章将通过介绍三种变体来奠定
    GANs 的基础；在下一章中，我们将学习更高级的 GANs 及其应用。
- en: 'In this chapter, we will explore the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: Introducing GANs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入 GAN
- en: Using GANs to generate handwritten digits
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GAN 生成手写数字
- en: Using DCGANs to generate face images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DCGAN 生成人脸图像
- en: Implementing conditional GANs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现条件 GANs
- en: All code snippets within this chapter are available in the `Chapter12` folder
    of the Github repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有代码片段均可在 Github 仓库的 `Chapter12` 文件夹中找到，链接为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Introducing GANs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 GAN
- en: 'To understand GANs, we need to understand two terms: generator and discriminator.
    First, we should have a reasonable sample of images (100-1000 images) of an object.
    A **generative network** (generator) learns representation from a sample of images
    and then generates images similar to the sample of images. A **discriminator network**
    (discriminator) is one that looks at the image generated by the generator network
    and the original sample of images and classifies images as original or generated
    (fake) ones.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 GAN，我们需要理解两个术语：生成器和判别器。首先，我们应该有一个合理数量的对象图片样本（100-1000 张图片）。**生成网络**（生成器）从图片样本中学习表示，并生成类似样本图片的图片。**判别网络**（判别器）则会查看生成器网络生成的图片和原始图片样本，并将图片分类为原始或生成的（伪造的）。
- en: The generator network tries to generate images in such a way that the discriminator
    classifies the images as real. The discriminator network tried to classify the
    generated images as fake and the images in the original sample as real.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络试图以一种方式生成图片，使得判别器将这些图片分类为真实的。判别器网络试图将生成的图片分类为假的，并将原始样本中的图片分类为真实的。
- en: Essentially, the adversarial term in GAN represents the opposite nature of the
    two networks—*a generator network, which generates images to fool the discriminator
    network, and a discriminator network that classifies each image by saying whether
    the image is generated or is an original*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，GAN 中的对抗术语表示了两个网络相反性质的对立面——*一个生成器网络，生成图像以欺骗判别器网络，和一个判别器网络，通过判断图像是生成的还是原始的来分类每张图片*。
- en: 'Let’s understand the process employed by GANs through the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过下图了解 GANs 的工作过程：
- en: '![Graphical user interface  Description automatically generated](img/B18457_12_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  自动生成的描述](img/B18457_12_01.png)'
- en: 'Figure 12.1: Typical GAN workflow'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：典型的 GAN 工作流程
- en: In the preceding diagram, the generator network is generating images from random
    noise as input. A discriminator network is looking at the images generated by
    the generator and comparing them with real data (a sample of images that are provided)
    to specify whether the generated image is real or fake. The generator tries to
    generate as many realistic images as possible, while the discriminator tries to
    detect which of the images that are generated by the generator are fake. This
    way, the generator learns to generate as many realistic images as possible by
    learning from what the discriminator looks at to identify whether an image is
    fake.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图表中，生成器网络从随机噪声生成图像作为输入。鉴别器网络查看生成器生成的图像，并将其与真实数据（提供的图像样本）进行比较，以确定生成的图像是真实还是伪造的。生成器试图生成尽可能多的逼真图像，而鉴别器则试图检测生成器生成的图像中哪些是伪造的。这样一来，生成器通过学习鉴别器的观察对象，学会尽可能多地生成逼真图像。
- en: 'Typically, the generator and discriminator are trained alternately within each
    step of training. This way, it becomes a cops-and-robbers game, where the generator
    is the robber trying to generate fake data, while the discriminator is the cop
    trying to identify the available data as real or fake. The steps involved in training
    GANs are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在每一步训练中，生成器和鉴别器交替训练。这样做类似于警察和小偷的游戏，生成器是试图生成伪造数据的小偷，而鉴别器则是试图辨别现有数据是真实还是伪造的警察。训练
    GAN 的步骤如下：
- en: Train the generator (and not the discriminator) to generate images such that
    the discriminator classifies the images as real.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器（而不是鉴别器）以生成被鉴别器分类为真实的图像。
- en: Train the discriminator (and not the generator) to classify the images that
    the generator generates as fake.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练鉴别器（而不是生成器）以将生成器生成的图像分类为伪造的。
- en: Repeat the process until an equilibrium is achieved.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复该过程直到达到平衡。
- en: 'Let’s now understand how we compute the loss values for both the generator
    and discriminator to train both the networks together using the following diagram
    and steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们理解如何使用以下图表和步骤计算生成器和鉴别器的损失值，从而同时训练两个网络：
- en: '![Diagram  Description automatically generated](img/B18457_12_02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18457_12_02.png)'
- en: 'Figure 12.2: GAN training workflow (dotted lines represent training, solid
    lines process)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：GAN 训练工作流程（虚线表示训练，实线表示过程）
- en: In the preceding scenario, when the discriminator can detect generated images
    really well, the loss corresponding to the generator is much higher when compared
    to the loss corresponding to the discriminator. Thus, the gradients adjust in
    such a way that the generator would have a lower loss. However, it would tip the
    discriminator loss to a higher side. In the next iteration, the gradients adjust
    so that the discriminator loss is lower. This way, the generator and discriminator
    keep getting trained until a point where the generator generates realistic images
    and the discriminator cannot distinguish between a real or a generated image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述场景中，当鉴别器能够非常好地检测生成的图像时，与生成器相关的损失要比与鉴别器相关的损失高得多。因此，梯度调整得使得生成器的损失更低。然而，这会使得鉴别器的损失偏高。在下一次迭代中，梯度会调整，以使得鉴别器的损失更低。这样一来，生成器和鉴别器不断训练，直到生成器生成逼真图像，而鉴别器无法区分真实图像和生成图像。
- en: With this understanding, let’s generate images relating to the MNIST dataset
    in the next section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解这些之后，让我们在下一节生成与 MNIST 数据集相关的图像。
- en: Using GANs to generate handwritten digits
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 生成手写数字。
- en: 'To generate images of handwritten digits, we will leverage the same network
    as we learned about in the previous section. The strategy we will adopt is as
    follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成手写数字图像，我们将利用前一节学习的同一网络。我们将采用以下策略：
- en: Import MNIST data.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 MNIST 数据。
- en: Initialize random noise.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化随机噪声。
- en: Define the generator model.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器模型。
- en: Define the discriminator model.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义鉴别器模型。
- en: Train the two models alternately.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交替训练两个模型。
- en: Let the model train until the generator and discriminator losses are largely
    the same.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让模型训练，直到生成器和鉴别器的损失大致相同。
- en: 'Let’s execute each of the preceding steps in the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下面的代码中执行上述每个步骤。
- en: 'The following code is available as `Handwritten_digit_generation_using_GAN.ipynb`
    in the `Chapter12` folder in this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code is moderately lengthy. We strongly recommend you execute the notebook
    in GitHub to reproduce the results while you understand the steps to perform and
    the explanation of various code components from the text.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本书GitHub存储库中的`Chapter12`文件夹中提供了`Handwritten_digit_generation_using_GAN.ipynb`代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。代码相当冗长。我们强烈建议您在GitHub上执行此笔记本，以重现结果，并在理解步骤和文本中各种代码组件的解释时进行操作。
- en: 'Import the relevant packages and define the device:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并定义设备：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the `MNIST` data and define the dataloader with built-in data transformation
    so that the input data is scaled to a mean of 0.5 and a standard deviation of
    0.5:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`MNIST`数据，并使用内置数据转换定义数据加载器，以便将输入数据缩放到均值0.5和标准差0.5：
- en: '[PRE1]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the `Discriminator` model class:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Discriminator`模型类：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A summary of the discriminator network is as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络摘要如下：
- en: 'Note that, in the preceding code, in place of `ReLU`, we have used `LeakyReLU`
    (an empirical evaluation of different types of ReLU activations is available here:
    [https://arxiv.org/pdf/1505.00853](https://arxiv.org/pdf/1505.00853)) as the activation
    function.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，请注意，我们使用了`LeakyReLU`（关于不同类型ReLU激活的实证评估可在此处找到：[https://arxiv.org/pdf/1505.00853](https://arxiv.org/pdf/1505.00853)）作为激活函数，而不是`ReLU`。
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code generates the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![Table  Description automatically generated](img/B18457_12_03.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成表格描述](img/B18457_12_03.png)'
- en: 'Figure 12.3: Summary of the discriminator architecture'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：鉴别器架构摘要
- en: 'Define the `Generator` model class:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Generator`模型类：
- en: '[PRE4]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that the generator takes a 100-dimensional input (which is of random noise)
    and generates an image from the input. A summary of the generator model is as
    follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，生成器接受100维输入（即随机噪声），并从输入生成图像。生成器模型摘要如下：
- en: '[PRE5]'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code generates the following output:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![Table  Description automatically generated](img/B18457_12_04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成表格描述](img/B18457_12_04.png)'
- en: 'Figure 12.4: Summary of the generator architecture'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：生成器架构摘要
- en: 'Define a function to generate random noise and register it to the device:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个生成随机噪声并将其注册到设备的函数：
- en: '[PRE6]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define a function to train the discriminator, as follows:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练鉴别器的函数如下：
- en: 'The discriminator training function (`discriminator_train_step`) takes real
    data (`real_data`) and fake data (`fake_data`) as input:'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴别器训练函数(`discriminator_train_step`)接受真实数据(`real_data`)和假数据(`fake_data`)作为输入：
- en: '[PRE7]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Reset the gradients:'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置梯度：
- en: '[PRE8]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Predict on the real data (`real_data`) and calculate the loss (`error_real`)
    before performing backpropagation on the loss value:'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进行损失值反向传播之前，在真数据(`real_data`)上进行预测并计算损失(`error_real`)：
- en: '[PRE9]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When we calculate the discriminator loss on real data, we expect the discriminator
    to predict an output of `1`. Hence, the discriminator loss on real data is calculated
    by expecting the discriminator to predict the output to be `1` using `torch.ones`
    during discriminator training.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们计算真实数据上的鉴别器损失时，我们期望鉴别器预测输出为`1`。因此，在鉴别器训练期间，通过使用`torch.ones`来计算真实数据上的鉴别器损失，预期鉴别器预测输出为`1`。
- en: 'Predict on the fake data (`fake_data`) and calculate the loss (`error_fake`)
    before performing backpropagation on the loss value:'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在进行损失值反向传播之前，在假数据(`fake_data`)上进行预测并计算损失(`error_fake`)：
- en: '[PRE10]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When we calculate the discriminator loss on fake data, we expect the discriminator
    to predict an output of `0`. Hence, the discriminator loss on fake data is calculated
    by expecting the discriminator to predict an output of `0` using `torch.zeros`
    during discriminator training.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们计算假数据上的鉴别器损失时，我们期望鉴别器预测输出为`0`。因此，在鉴别器训练期间，通过使用`torch.zeros`来计算假数据上的鉴别器损失，预期鉴别器预测输出为`0`。
- en: 'Update the weights and return the overall loss (summing up the loss values
    of `error_real` on `real_data` and `error_fake` on `fake_data`):'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重并返回总体损失（将`error_real`在`real_data`上的损失值与`error_fake`在`fake_data`上的损失值相加）：
- en: '[PRE11]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Train the generator model in the following way:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以以下方式训练生成器模型：
- en: 'Define the generator training function (`generator_train_step`) that takes
    fake data (`fake_data`):'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器训练函数(`generator_train_step`)，接受假数据(`fake_data`)作为输入：
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reset the gradients of the generator optimizer:'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置生成器优化器的梯度：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Predict the output of the discriminator on fake data (`fake_data`):'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测鉴别器在虚假数据（`fake_data`）上的输出：
- en: '[PRE14]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Calculate the generator loss value by passing `prediction` and the expected
    value as `torch.ones` since we want to fool the discriminator to output a value
    of `1` when training the generator:'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过传递`prediction`和期望值作为`torch.ones`来计算生成器损失值，因为我们希望在训练生成器时愚弄鉴别器输出值为`1`：
- en: '[PRE15]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Perform backpropagation, update the weights, and return the error:'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行反向传播，更新权重，并返回错误：
- en: '[PRE16]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the model objects, the optimizer for each generator and discriminator,
    and the loss function to optimize:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型对象，每个生成器和鉴别器的优化器，以及优化损失函数：
- en: '[PRE17]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Run the models over increasing epochs:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加的epochs上运行模型：
- en: 'Loop through 200 epochs (`num_epochs`) over the `data_loader` function obtained
    in *step 2*:'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环通过200个epochs（`num_epochs`）使用第2步获得的`data_loader`函数：
- en: '[PRE18]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load real data (`real_data`) and fake data, where fake data (`fake_data`) is
    obtained by passing `noise` (with a batch size of the number of data points in
    `real_data`: `len(real_data)`) through the `generator` network. Note that it is
    important to run `fake_data.detach()`, or else training will not work. On detaching,
    we are creating a fresh copy of the tensor so that when `error.backward()` is
    called in `discriminator_train_step`, the tensors associated with the generator
    (which create `fake_data`) are not affected:'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载真实数据（`real_data`）和虚假数据，其中虚假数据（`fake_data`）是通过将`noise`（批处理大小为`real_data`中的数据点数：`len(real_data)`）通过`generator`网络来获得的。注意，重要的是运行`fake_data.detach()`，否则训练将无法进行。在分离时，我们正在创建张量的新副本，以便在`discriminator_train_step`中调用`error.backward()`时，与生成器相关联的张量（用于创建`fake_data`）不受影响：
- en: '[PRE19]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Train the discriminator using the `discriminator_train_step` function defined
    in *step 6*:'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在第6步中定义的`discriminator_train_step`函数训练鉴别器：
- en: '[PRE20]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now that we have trained the discriminator, let’s train the generator in this
    step. Generate a new set of fake images (`fake_data`) from noisy data and train
    the generator using `generator_train_step` defined in *step 6*:'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经训练了鉴别器，让我们在这一步中训练生成器。从噪声数据生成一组新的虚假图像（`fake_data`），并使用第6步中定义的`generator_train_step`训练生成器：
- en: '[PRE21]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Record the losses:'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录损失：
- en: '[PRE22]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The discriminator and generator losses over increasing epochs are as follows
    (you can refer to the digital version of the book for the colored image):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 随着epochs的增加，鉴别器和生成器的损失如下（您可以参考书籍的数字版本获取彩色图像）：
- en: '![Chart, line chart  Description automatically generated](img/B18457_12_05.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图 自动生成的描述](img/B18457_12_05.png)'
- en: 'Figure 12.5: Discriminator and Generator loss over increasing epochs'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：随着epochs增加的鉴别器和生成器损失
- en: 'Visualize the fake data post-training:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化训练后的虚假数据：
- en: '[PRE23]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code generates the following output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![Shape, arrow  Description automatically generated](img/B18457_12_06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![形状，箭头 自动生成的描述](img/B18457_12_06.png)'
- en: 'Figure 12.6: Generated digits'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：生成的数字
- en: From this, we can see that we can leverage GANs to generate images that are
    realistic, but still with some scope for improvement. In the next section, we
    will learn about using deep convolutional GANs to generate more realistic images.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以看出，我们可以利用GAN生成逼真的图像，但仍然有一些改进的空间。在下一节中，我们将学习如何使用深度卷积GAN生成更逼真的图像。
- en: Using DCGANs to generate face images
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DCGAN生成面部图像
- en: In the previous section, we learned about generating images using GANs. However,
    we have already seen in *Chapter 4*, *Introducing Convolutional Neural Networks*,
    that Convolutional Neural Networks (CNNs) perform better in the context of images
    when compared to vanilla neural networks. In this section, we will learn about
    generating images using Deep Convolutional Generative Adversarial Networks (DCGANs),
    which use convolution and pooling operations in the model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了如何使用GAN生成图像。但是，我们已经在*第4章*，*引入卷积神经网络*中看到，卷积神经网络（CNNs）在图像背景下的表现比普通神经网络更好。在本节中，我们将学习如何使用深度卷积生成对抗网络（DCGANs）生成图像，这些网络在模型中使用卷积和池化操作。
- en: First, let’s understand the technique we will leverage to generate an image
    using a set of 100 random numbers (we chose 100 random numbers so that the network
    has a reasonable number of values to generate images. We encourage readers to
    experiment with different amounts of random numbers and see the result). We will
    first convert noise into a shape of *batch size x 100 x 1 x 1*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解我们将利用的技术，使用一组 100 个随机数生成图像（我们选择了 100 个随机数，以便网络有合理数量的值来生成图像。我们鼓励读者尝试不同数量的随机数并查看结果）。我们将首先将噪声转换为*批量大小
    x 100 x 1 x 1*的形状。
- en: The reason for appending additional channel information in DCGANs and not doing
    it in the GAN section is that we will leverage CNNs in this section, which requires
    inputs in the form of batch size x channels x height x width.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DCGAN 中附加额外的通道信息而不在 GAN 部分中执行的原因是，我们将在本节中利用 CNN，它需要以批量大小 x 通道 x 高度 x 宽度的形式输入。
- en: Next, we convert the generated noise into an image by leveraging
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过利用
- en: '`ConvTranspose2d`. As we learned in *Chapter 9*, *Image Segmentation*, `ConvTranspose2d`
    does the opposite of a convolution operation, which is to take input with a smaller
    feature map size (height x width) and upsample it to that of a larger size using
    a predefined kernel size, stride, and padding. This way, we would gradually convert
    a vector from a shape of batch size x 100 x 1 x 1 into a shape of batch size x
    3 x 64 x 64\. With this, we have taken a random noise vector of size 100 and converted
    it into an image of a face.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`ConvTranspose2d`。正如我们在*第 9 章*，*图像分割*中学到的那样，`ConvTranspose2d` 执行的是卷积操作的相反操作，即使用预定义的内核大小、步长和填充来将输入的较小特征映射大小（高度
    x 宽度）上采样到较大的大小。通过这种方式，我们逐渐将大小为批量大小 x 100 x 1 x 1 的随机噪声向量转换为批量大小 x 3 x 64 x 64
    的图像。有了这个，我们已经将大小为 100 的随机噪声向量转换为一个人脸图像。 '
- en: 'With this understanding, let’s now build a model to generate images of faces:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了这一点之后，现在让我们构建一个模型来生成人脸图像：
- en: 'The following code is available as `Face_generation_using_DCGAN.ipynb` in the
    `Chapter12` folder in this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    understand the steps to perform and the explanation of various code components
    from the text.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书 GitHub 存储库的 `Chapter12` 文件夹中的 `Face_generation_using_DCGAN.ipynb` 中作为可用。代码包含用于下载数据的
    URL，并且长度适中。我们强烈建议您在 GitHub 上执行笔记本以重现结果，同时理解执行步骤和文本中各种代码组件的解释。
- en: 'Download and extract the face images (a dataset that we have collated by generating
    faces of random people):'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并提取人脸图像（我们通过生成随机人物的面孔来整理的数据集）：
- en: '[PRE24]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'A sample of the images is shown here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一些图像的样本：
- en: '![A picture containing text, posing, person, different  Description automatically
    generated](img/B18457_12_07.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、姿势、人物、不同的图片 自动产生的描述](img/B18457_12_07.png)'
- en: 'Figure 12.7: Sample male and female images'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7：示例男性和女性图像
- en: 'Import the relevant packages:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包：
- en: '[PRE25]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define the dataset and dataloader:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集和数据加载器：
- en: 'Ensure that we crop the images so that we retain only the faces and discard
    additional details in the image. First, we will download the cascade filter (more
    on cascade filters can be found in the *Using OpenCV Utilities for Image Analysis*
    PDF on GitHub), which will help with identifying faces within an image:'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保我们裁剪图像时仅保留人脸，并且丢弃图像中的额外细节。首先，我们将下载级联过滤器（有关级联过滤器的更多信息可以在 GitHub 上的《使用 OpenCV
    工具进行图像分析》PDF 中找到），这将有助于识别图像中的人脸：
- en: '[PRE26]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a new folder and dump all the cropped face images into the new folder:'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新文件夹，并将所有裁剪的人脸图像倒入到这个新文件夹中：
- en: '[PRE27]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'A sample of the cropped faces is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪后的人脸样本如下：
- en: '![A collage of people''s faces  Description automatically generated with medium
    confidence](img/B18457_12_08.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![人们面孔的拼贴 图片上显示了中等置信度自动生成的描述](img/B18457_12_08.png)'
- en: 'Figure 12.8: Cropped male and female faces'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8：裁剪的男性和女性面孔
- en: Note that by cropping and keeping the faces only, we are retaining only the
    information that we want to generate. This way, we are reducing the complexities
    that the DCGAN would have to learn about.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过裁剪并仅保留面部，我们保留了我们希望生成的信息。这样，我们可以减少 DCGAN 需要学习的复杂性。
- en: 'Specify the transformation to perform on each image:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定在每个图像上执行的转换：
- en: '[PRE28]'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the `Faces` dataset class:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `Faces` 数据集类：
- en: '[PRE29]'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create the dataset object: `ds`:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据集对象：`ds`：
- en: '[PRE30]'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the `dataloader` class as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示定义`dataloader`类：
- en: '[PRE31]'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define weight initialization so that the weights have a smaller spread as mentioned
    in the details of the adversarial training section at [https://arxiv.org/pdf/1511.06434](https://arxiv.org/pdf/1511.06434):'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义权重初始化，使得权重的分布更小，正如在对抗训练部分的详细说明中提到的那样，参见[https://arxiv.org/pdf/1511.06434](https://arxiv.org/pdf/1511.06434)：
- en: '[PRE32]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the `Discriminator` model class, which takes an image of a shape of
    batch size x 3 x 64 x 64 and predicts whether it is real or fake:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Discriminator`模型类，接受形状为批量大小 x 3 x 64 x 64的图像，并预测其真实性或伪造性：
- en: '[PRE33]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Obtain a summary of the defined model:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 获取定义模型的摘要：
- en: '[PRE34]'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code generates the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成如下输出：
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_12_09.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成描述](img/B18457_12_09.png)'
- en: 'Figure 12.9: Summary of the discriminator architecture'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.9: 鉴别器架构摘要'
- en: 'Define the `Generator` model class that generates fake images from an input
    of shape batch size x 100 x 1 x 1:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Generator`模型类，从形状为批量大小 x 100 x 1 x 1的输入生成虚假图像：
- en: '[PRE35]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Obtain a summary of the defined model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 获取定义模型的摘要：
- en: '[PRE36]'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code generates the following output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成如下输出：
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_12_10.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成描述](img/B18457_12_10.png)'
- en: 'Figure 12.10: Summary of the generator architecture'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.10: 生成器架构摘要'
- en: Note that we have leveraged `ConvTranspose2d` to gradually upsample an array
    so that it closely resembles an image.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们利用`ConvTranspose2d`逐渐上采样数组，使其尽可能地类似于图像。
- en: 'Define the functions to train the generator (`generator_train_step`) and the
    discriminator (`discriminator_train_step`):'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于训练生成器（`generator_train_step`）和鉴别器（`discriminator_train_step`）的函数：
- en: '[PRE37]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the preceding code, we are performing a `.squeeze` operation on top of the
    prediction as the output of the model has a shape of batch size x 1 x 1 x 1 and
    it needs to be compared to a tensor that has a shape of batch size x 1.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们在模型输出上执行了`.squeeze`操作，因为模型的输出形状为批量大小 x 1 x 1 x 1，需要与形状为批量大小 x 1的张量进行比较。
- en: 'Create the generator and discriminator model objects, the optimizers, and the
    loss function of the discriminator to be optimized:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建生成器和鉴别器模型对象、优化器和要优化的鉴别器损失函数：
- en: '[PRE38]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Run the models over increasing epochs, as follows:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在逐步增加的周期上运行模型，如下所示：
- en: 'Loop through 25 epochs over the `dataloader` function defined in *step 3*:'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环通过*步骤3*中定义的`dataloader`函数25个周期：
- en: '[PRE39]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Load real data (`real_data`) and generate fake data (`fake_data`) by passing
    through the generator network:'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过生成器网络传递真实数据（`real_data`）加载真实数据，并生成虚假数据（`fake_data`）：
- en: '[PRE40]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note that the major difference between vanilla GANs and DCGANs when generating
    `real_data` is that we did not have to flatten `real_data` in the case of DCGANs
    as we are leveraging CNNs.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，当生成`real_data`时，普通GAN和DCGAN之间的主要区别在于，在DCGAN的情况下，我们不必展平`real_data`，因为我们正在利用CNN。
- en: 'Train the discriminator using the `discriminator_train_step` function defined
    in *step 7*:'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*步骤7*中定义的`discriminator_train_step`函数训练鉴别器：
- en: '[PRE41]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Generate a new set of images (`fake_data`) from the noisy data (`torch.randn(len(real_data))`)
    and train the generator using the `generator_train_step` function defined in *step
    7*:'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从嘈杂数据（`torch.randn(len(real_data))`）生成一组新图像（`fake_data`），并使用*步骤7*中定义的`generator_train_step`函数训练生成器：
- en: '[PRE42]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Record the losses:'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录损失：
- en: '[PRE43]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The preceding code generates the following output (you can refer to the digital
    version of the book for the colored image):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成如下输出（您可以参考书籍的数字版本查看彩色图像）：
- en: '![Chart, line chart  Description automatically generated](img/B18457_12_11.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图  自动生成描述](img/B18457_12_11.png)'
- en: 'Figure 12.11: Discriminator and generator loss over increasing epochs'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '图12.11: 随着训练周期增加，鉴别器和生成器的损失'
- en: 'Note that in this setting, the variation in generator and discriminator losses
    does not follow the pattern that we have seen in the case of handwritten digit
    generation on account of the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种设置中，生成器和鉴别器损失的变化不遵循我们在手写数字生成情况下看到的模式，原因如下：
- en: We are dealing with bigger images (images that are 64 x 64 x 3 in shape when
    compared to images of 28 x 28 x 1 shape, which we saw in the previous section).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们处理更大的图像（图像形状为64 x 64 x 3，与前一节中的28 x 28 x 1形状的图像相比）。
- en: Digits have fewer variations when compared to the features that are present
    in the image of a face.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information in handwritten digits is available in only a minority of pixels
    when compared to the information in images of a face.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the training process is complete, generate a sample of images using the
    following code:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code generates the following set of images:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person''s face  Description automatically generated](img/B18457_12_12.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: Generated images from the trained DCGAN model'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Note that while the generator generated images of a face from random noise,
    the images are decent but still not sufficiently realistic. One potential reason
    is that not all input images have the same face alignment. As an exercise, we
    suggest you train the DCGAN only on those images where there is no tilted face
    and the person is looking straight into the camera in the original image.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: In addition, we suggest you try and contrast the generated images with high
    discriminator scores to the ones with low discriminator scores.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned about generating images of a face. However,
    we cannot specify the generation of an image that is of interest to us (for example,
    a man with a beard). In the next section, we will work toward generating images
    of a specific class.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Implementing conditional GANs
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a scenario where we want to generate an image of a class of our interest;
    for example, an image of a cat, a dog, or a man with spectacles. How do we specify
    that we want to generate an image of interest to us? **Conditional GANs** come
    to the rescue in this scenario. For now, let’s assume that we have the images
    of male and female faces only, along with their corresponding labels. In this
    section, we will learn about generating images of a specified class of interest
    from random noise.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy we adopt is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Specify the label of the image we want to generate as a one-hot-encoded version.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the label through an embedding layer to generate a multi-dimensional representation
    of each class.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate random noise and concatenate with the embedding layer generated in
    the previous step.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model just like we did in the previous sections, but this time with
    the noise vector concatenated with the embedding of the class of image we wish
    to generate.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following code, we will code up the preceding strategy:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `Face_generation_using_Conditional_GAN.ipynb`
    in the `Chapter12` folder in this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    We strongly recommend you execute the notebook in GitHub to reproduce the results
    while you understand the steps to perform and the explanation of various code
    components from the text.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the images and the relevant packages:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create the dataset and dataloader, as follows:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Store the male and female image paths:'
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Ensure that we crop the images so that we retain only faces and discard additional
    details in an image. First, we will download the cascade filter (more on cascade
    filters can be found in the *Using OpenCV Utilities for Image Analysis* PDF on
    GitHub, which will help in identifying faces within an image:'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保裁剪图像，仅保留面部并丢弃图像中的其他细节。首先，我们将下载级联滤波器（有关级联滤波器的更多信息可在 GitHub 上的 *Using OpenCV
    Utilities for Image Analysis* PDF 中找到，这将帮助识别图像中的面部）：
- en: '[PRE47]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create two new folders (one corresponding to male and another for female images)
    and dump all the cropped face images into the respective folders:'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个新文件夹（一个对应男性图像，另一个对应女性图像），并将所有裁剪后的面部图像转储到相应的文件夹中：
- en: '[PRE48]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Specify the transformation to perform on each image:'
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定要在每个图像上执行的转换：
- en: '[PRE49]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Create the `Faces` dataset class that returns the image and the corresponding
    gender of the person in it:'
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `Faces` 数据集类，返回图像及其中人物的性别：
- en: '[PRE50]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Define the `ds` dataset and `dataloader`:'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `ds` 数据集和 `dataloader`：
- en: '[PRE51]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Define the weight initialization method (just like we did in the *Using DCGANs
    to generate face images* section) so that we do not have a widespread variation
    across randomly initialized weight values:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义权重初始化方法（就像我们在 *Using DCGANs to generate face images* 部分中所做的那样），以便随机初始化的权重值没有普遍变化：
- en: '[PRE52]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Define the `Discriminator` model class as follows:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下定义 `Discriminator` 模型类：
- en: 'Define the model architecture:'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: '[PRE53]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note that in the model class, we have an additional parameter, `emb_size`, present
    in conditional GANs and not in DCGANs. `emb_size` represents the number of embeddings
    into which we convert the input class label (the class of image we want to generate),
    which is stored as `label_embeddings`. The reason we convert the input class label
    from a one-hot-encoded version to embeddings of a higher dimension is that the
    model has a higher degree of freedom to learn and adjust to deal with different
    classes.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在模型类中，我们有一个额外的参数 `emb_size`，它存在于条件 GAN 中，而不是 DCGAN 中。`emb_size` 表示我们将输入类标签（我们要生成的图像类的类别）转换为的嵌入数量，这些嵌入存储为
    `label_embeddings`。我们将输入类标签从一热编码版本转换为更高维度的嵌入的原因是，模型具有更高的自由度来学习和调整以处理不同的类别。
- en: While the model class, to a large extent, remains the same as what we have seen
    in DCGANs, we are initializing another model (`model2`) that does the classification
    exercise. There will be more about how the second model helps after we discuss
    the `forward` method next. You will also understand the reason why `self.model2`
    has 288 values as input after you go through the following `forward` method and
    the summary of the model.
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然模型类在很大程度上与我们在 DCGAN 中看到的相同，我们正在初始化另一个模型 (`model2`)，这个模型执行分类任务。在我们讨论完 `forward`
    方法后，将详细介绍第二个模型如何帮助解释。在你阅读下面的 `forward` 方法和模型摘要后，你也会理解为什么 `self.model2` 有 288 个输入值。
- en: 'Define the `forward` method that takes the image and the label of the image
    as input:'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `forward` 方法，接受图像及其标签作为输入：
- en: '[PRE54]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In the `forward` method defined, we are fetching the output of the first model
    (`self.model(input)`) and the output of passing `labels` through `label_embeddings`
    and then concatenating the outputs. Next, we are passing the concatenated outputs
    through the second model (`self.model2`) we have defined earlier that fetches
    us the discriminator output.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义的 `forward` 方法中，我们获取第一个模型的输出 (`self.model(input)`) 和通过 `label_embeddings`
    传递 `labels` 的输出，然后连接这些输出。接下来，我们通过早期定义的第二个模型 (`self.model2`) 传递连接的输出，它获取我们的判别器输出。
- en: 'Obtain the summary of the defined model:'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取定义模型的摘要：
- en: '[PRE55]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The preceding code generates the following output:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![Table  Description automatically generated](img/B18457_12_13.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![表 自动生成描述](img/B18457_12_13.png)'
- en: 'Figure 12.13: Summary of the discriminator architecture'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：判别器架构摘要
- en: Note that `self.model2` takes an input of 288 values as the output of `self.model`
    has 256 values per data point, which is then concatenated with the 32 embedding
    values of the input class label, resulting in 256 + 32 = 288 input values to `self.model2`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`self.model2` 接受 288 个值作为输入，因为 `self.model` 的输出每个数据点有 256 个值，然后与输入类标签的 32
    个嵌入值连接，结果是 256 + 32 = 288 个输入值给 `self.model2`。
- en: 'Define the `Generator` network class:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `Generator` 网络类：
- en: 'Define the `__init__` method:'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `__init__` 方法：
- en: '[PRE56]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note that in the preceding code, we are using `nn.Embedding` to convert the
    2D input (which is of classes) to a 32-dimensional vector (`self.emb_size`).
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在前述代码中，我们使用`nn.Embedding`将2D输入（即类别）转换为32维向量(`self.emb_size`)。
- en: '[PRE57]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that in the preceding code, we have leveraged `nn.ConvTranspose2d` to upscale
    toward fetching an image as output.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，在前述代码中，我们利用了`nn.ConvTranspose2d`来向上缩放以获取图像作为输出。
- en: 'Apply weight initialization:'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用权重初始化：
- en: '[PRE58]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define the `forward` method, which takes the noise values (`input_noise`) and
    input label (`labels`) as input and generates the output of the image:'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`forward`方法，该方法接受噪声值(`input_noise`)和输入标签(`labels`)作为输入，并生成图像的输出：
- en: '[PRE59]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Obtain a summary of the defined `generator` function:'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取所定义的`generator`函数的摘要：
- en: '[PRE60]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The preceding code generates the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成如下输出：
- en: '![Table  Description automatically generated](img/B18457_12_14.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的表格说明](img/B18457_12_14.png)'
- en: 'Figure 12.14: Summary of Generator architecture'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14：生成器架构摘要
- en: 'Define a function (`noise`) to generate random noise with 100 values and register
    it to the device:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数(`noise`)，用于生成具有100个值的随机噪声，并将其注册到设备上：
- en: '[PRE61]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Define the function to train the discriminator:: `discriminator_train_step`:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练鉴别器的函数：`discriminator_train_step`：
- en: 'The discriminator takes four inputs—real images (`real_data`), real labels
    (`real_labels`), fake images (`fake_data`), and fake labels (`fake_labels`):'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴别器接受四个输入——真实图像(`real_data`)、真实标签(`real_labels`)、虚假图像(`fake_data`)和虚假标签(`fake_labels`)：
- en: '[PRE62]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here, we are resetting the gradient corresponding to the discriminator.
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们正在重置鉴别器对应的梯度。
- en: 'Calculate the loss value corresponding to predictions on the real data (`prediction_real`).
    The loss value output when `real_data` and `real_labels` are passed through the
    `discriminator` network is compared with the expected value of `(torch.ones(len(real_data),1).to(device))`
    to obtain `error_real` before performing backpropagation:'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在真实数据(`prediction_real`通过`discriminator`网络时的损失值。将输出的损失值与期望值`(torch.ones(len(real_data),1).to(device))`进行比较，以获取`error_real`，然后执行反向传播：
- en: '[PRE63]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Calculate the loss value corresponding to predictions on the fake data (`prediction_fake`).
    The loss value output when `fake_data` and `fake_labels` are passed through the
    `discriminator` network is compared with the expected value of `(torch.zeros(len(fake_data),1).to(device))`
    to obtain `error_fake` before performing backpropagation:'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在虚假数据(`prediction_fake`通过`discriminator`网络时的损失值。将输出的损失值与期望值`(torch.zeros(len(fake_data),1).to(device))`进行比较，以获取`error_fake`，然后执行反向传播：
- en: '[PRE64]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Update weights and return the loss values:'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重并返回损失值：
- en: '[PRE65]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define the training steps for the generator where we pass the fake images (`fake_data`)
    along with the fake labels (`fake_labels`) as input:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器的训练步骤，其中我们将虚假图像(`fake_data`)与虚假标签(`fake_labels`)作为输入传递：
- en: '[PRE66]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Note that the `generator_train_step` function is similar to `discriminator_train_step`,
    with the exception that this has an expectation of `torch.ones(len(fake_data),1).to(device))`
    as output in place of zeros given that we are training the generator.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`generator_train_step`函数类似于`discriminator_train_step`，唯一的区别在于这里期望输出为`torch.ones(len(fake_data),1).to(device))`，因为我们正在训练生成器。
- en: 'Define the `generator` and `discriminator` model objects, the loss optimizers,
    and the `loss` function:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`generator`和`discriminator`模型对象、损失优化器和`loss`函数：
- en: '[PRE67]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: In the preceding code, `while` defining `fixed_fake_labels`, we are specifying
    that half of the images correspond to one class (class 0) and the rest to another
    class (class 1). Additionally, we are defining `fixed_noise`, which will be used
    to generate images from random noise.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，在定义`fixed_fake_labels`时，我们指定一半图像对应于一类（类0），其余图像对应于另一类（类1）。此外，我们定义了`fixed_noise`，将用于从随机噪声生成图像。
- en: 'Train the model over increasing epochs (`n_epochs`):'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加的epochs (`n_epochs`)上训练模型：
- en: 'Specify the length of `dataloader`:'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`dataloader`的长度：
- en: '[PRE68]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Loop through the batch of images along with their labels:'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环遍历图像批次及其标签：
- en: '[PRE69]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Specify `real_data` and `real_labels`:'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`real_data`和`real_labels`：
- en: '[PRE70]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Initialize `fake_data` and `fake_labels`:'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`fake_data`和`fake_labels`：
- en: '[PRE71]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Train the discriminator using the `discriminator_train_step` function defined
    in *step 7* to calculate discriminator loss (`d_loss`):'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在*步骤7*中定义的`discriminator_train_step`函数训练鉴别器以计算鉴别器损失(`d_loss`)：
- en: '[PRE72]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Regenerate fake images (`fake_data`) and fake labels (`fake_labels`) and train
    the generator using the `generator_train_step` function defined in *step 8* to
    calculate the generator loss (`g_loss`):'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新生成虚假图像 (`fake_data`) 和虚假标签 (`fake_labels`)，并使用在 *步骤 8* 中定义的 `generator_train_step`
    函数来计算生成器损失 (`g_loss`)：
- en: '[PRE73]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Log the loss metrics as follows:'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录如下损失指标：
- en: '[PRE74]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Once we train the model, generate the male and female images:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练模型，生成男性和女性图像：
- en: '[PRE75]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'In the preceding code, we are passing the noise (`fixed_noise`) and labels
    (`fixed_fake_labels`) to the generator to fetch the `fake` images, which are as
    follows at the end of 25 epochs of training the models:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们将噪声 (`fixed_noise`) 和标签 (`fixed_fake_labels`) 传递给生成器，以获取在训练模型 25 个周期后的
    `fake` 图像，结果如下：
- en: '![A collage of a person''s face  Description automatically generated](img/B18457_12_15.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![一个人脸拼贴图  Description automatically generated](img/B18457_12_15.png)'
- en: 'Figure 12.15: Generated male and female faces'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.15：生成的男性和女性面孔
- en: From the preceding image, we can see that the first 32 images correspond to
    male images, while the next 32 correspond to female images, which substantiates
    the fact that the conditional GANs performed as expected.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图像中，我们可以看到前 32 张图像对应于男性图像，接下来的 32 张图像对应于女性图像，这证实了条件 GAN 的表现符合预期。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about leveraging two different neural networks to
    generate new images of handwritten digits using GANs. Next, we generated realistic
    faces using DCGANs. Finally, we learned about conditional GANs, which help us
    in generating images of a certain class. Having generated images using different
    techniques, we could still see that the generated images were not sufficiently
    realistic. Furthermore, while we generated images by specifying the class of images
    we want to generate in conditional GANs, we are still not in a position to perform
    image translation, where we ask to replace one object in the image with another
    one, with everything else left as is. In addition, we are yet to have an image
    generation mechanism where the number of classes (styles) to generate is more
    unsupervised.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何利用两个不同的神经网络使用 GAN 生成手写数字的新图像。接下来，我们使用 DCGAN 生成逼真的面孔。最后，我们学习了条件 GAN，这有助于我们生成特定类别的图像。尽管我们使用不同的技术生成图像，但我们仍然发现生成的图像不够逼真。此外，在条件
    GAN 中，虽然我们通过指定要生成的图像类别来生成图像，但我们仍无法执行图像翻译，即要求替换图像中的一个对象，并保留其他一切。此外，我们还没有一种图像生成机制，其中要生成的类别（风格）更无监督。
- en: In the next chapter, we will learn about generating images that are more realistic
    using some of the latest variants of GANs. In addition, we will learn about generating
    images of different styles in a more unsupervised manner.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习使用一些最新变体的 GAN 生成更逼真的图像。此外，我们将学习以更无监督的方式生成不同风格的图像。
- en: Questions
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What happens if the learning rate of generator and discriminator models is high?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果生成器和鉴别器模型的学习率很高会发生什么？
- en: In a scenario where the generator and discriminator are very well trained, what
    is the probability of a given image being real?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成器和鉴别器都经过充分训练的情况下，给定图像是真实的概率是多少？
- en: Why do we use `ConvTranspose2d` in generating images?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在生成图像时要使用 `ConvTranspose2d`？
- en: Why do we have embeddings with a high embedding size than the number of classes
    in conditional GANs?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在条件 GAN 中的嵌入大小比类别数高？
- en: How can we generate images of men with beards?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何生成带胡须的男性图像？
- en: Why do we have Tanh activation in the last layer in the generator and not ReLU
    or sigmoid?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在生成器的最后一层使用 Tanh 激活而不是 ReLU 或 sigmoid？
- en: Why did we get realistic images even though we did not denormalize the generated
    data?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 即使我们没有对生成的数据进行反归一化，为什么我们仍然得到逼真的图像？
- en: What happens if we do not crop faces corresponding to images before training
    the GAN?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在训练 GAN 前不裁剪与图像对应的面部会发生什么？
- en: Why do the weights of the discriminator not get updated when the training generator
    is updated (as the `generator_train_step` function involves the discriminator
    network)?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练生成器时，为什么鉴别器的权重不会得到更新（因为 `generator_train_step` 函数涉及鉴别器网络）？
- en: Why do we fetch losses on both real and fake images while training the discriminator
    but only the loss on fake images while training the generator?
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练鉴别器时为什么要获取真实图像和虚假图像的损失，而在训练生成器时只获取虚假图像的损失？
- en: Learn more on Discord
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多信息
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
