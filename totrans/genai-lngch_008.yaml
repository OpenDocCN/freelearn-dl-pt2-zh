- en: 7 LLMs for Data Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7个用于数据科学的LLMs
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Discord上加入我们的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](img/file49.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的Qr代码说明](img/file49.png)'
- en: 'This chapter is about how generative AI can automate data science. Generative
    AI, in particular large language models (LLMs) have the potential to accelerate
    scientific progress across various domains, especially by providing efficient
    analysis of research data and aiding in literature review processes. A lot of
    current approaches that fall within the domain of AutoML can help data scientists
    increase their productivity and help make the data science more repeatable. I’ll
    first give an overview over automation in data science and then we’ll discuss
    how data science is affected by generative AI.Next, we’ll discuss how we can use
    code generation and tools in different ways to answer questions related to data
    science. This can come in the form of doing a simulation or of enriching out dataset
    with additional information. Finally, we put the focus on exploratory analysis
    of structured datasets. We can set up agents to run SQL or tabular data in Pandas.
    We’ll see how we can ask questions about the dataset, statistical questions about
    the data, or ask for visualizations.Throughout the chapter, we’ll work on different
    approaches to doing data science with LLMs, which you can find in the `data_science`
    directory in the Github repository for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章介绍了生成式人工智能如何自动化数据科学。生成式人工智能，特别是大型语言模型（LLMs），有望加速各种领域的科学进展，尤其是通过提供对研究数据的高效分析和帮助文献综述过程。许多现有的自动机器学习（AutoML）方法可以帮助数据科学家提高生产力，并帮助使数据科学更具可重复性。我将先概述数据科学中的自动化，然后我们将讨论生成式人工智能如何影响数据科学。接下来，我们将讨论如何使用代码生成和工具以不同方式回答与数据科学相关的问题。这可以采用模拟的形式或通过附加信息来丰富数据集。最后，我们将关注结构化数据集的探索性分析。我们可以设置代理人在Pandas中运行SQL或表格数据。我们将看到如何对数据集提出问题、有关数据的统计问题或要求可视化。在整个章节中，我们将使用LLMs的不同方法进行数据科学，您可以在本书的Github存储库中的`data_science`目录中找到。[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)。主要部分包括：
- en: Automated data science
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化数据科学
- en: Agents can answer data science questions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理人可以回答数据科学问题
- en: Data exploration with LLMs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs的数据探索
- en: Let’s start by discussing how data science can be automated and which parts
    of it, and how generative AI will impact data science.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论数据科学如何自动化，以及自动化的哪些部分，以及生成式人工智能将如何影响数据科学。
- en: Automated data science
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化数据科学
- en: 'Data science is a field that combines computer science, statistics, and business
    analytics to extract knowledge and insights from data. Data scientists use a variety
    of tools and techniques to collect, clean, analyze, and visualize data. They then
    use this information to help businesses make better decisions.The work of a data
    scientist can vary depending on the specific role and industry. However, some
    common tasks that data scientists might perform include:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是将计算机科学、统计学和商业分析相结合，从数据中提取知识和洞见的领域。数据科学家使用各种工具和技术来收集、清理、分析和可视化数据。然后，他们利用这些信息帮助企业做出更好的决策。数据科学家的工作可能因具体职位和行业而有所不同。但是，数据科学家可能执行的一些常见任务包括：
- en: Collecting data: Data scientists need to collect data from a variety of sources,
    such as databases, social media, and sensors.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集数据：数据科学家需要从各种来源收集数据，例如数据库、社交媒体和传感器。
- en: Cleaning data: Data scientists need to clean data to remove errors and inconsistencies.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理数据：数据科学家需要清理数据以删除错误和不一致性。
- en: Analyzing data: Data scientists use a variety of statistical and machine learning
    techniques to analyze data.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析数据：数据科学家使用各种统计和机器学习技术来分析数据。
- en: Visualizing data: Data scientists use data visualizations to communicate insights
    to stakeholders.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化数据：数据科学家使用数据可视化来向利益相关者传达见解。
- en: Building models: Data scientists build models to predict future outcomes or
    make recommendations.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立模型：数据科学家建立模型来预测未来结果或做出建议。
- en: 'Data analysis is a subset of data science that focuses on extracting insights
    from data. Data analysts use a variety of tools and techniques to analyze data,
    but they typically do not build models.The overlap between data science and data
    analysis is that both fields involve working with data to extract insights. However,
    data scientists typically have a more technical skillset than data analysts. Data
    scientists are also more likely to build models and sometimes deploy models into
    production. Data scientists sometimes deploy models into production so that they
    can be used to make decisions in real time, however, we’ll avoid automatic deployment
    of models in this discussion.Here is a table that summarizes the key differences
    between data science and data analysis:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析是数据科学的一个子集，专注于从数据中提取洞察。数据分析师使用各种工具和技术来分析数据，但是他们通常不构建模型。数据科学和数据分析的重叠在于两个领域都涉及使用数据提取洞察。然而，与数据分析师相比，数据科学家通常具有更高技术水平的技能。数据科学家也更有可能构建模型，有时将模型部署到生产环境中。数据科学家有时会将模型部署到生产环境中，以便实时使用它们来做决策。但是，在本文中，我们将避免自动部署模型。下面是总结数据科学和数据分析之间关键差异的表格：
- en: '| **Feature** | **Data Science** | **Data Analysis** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 数据科学 | 数据分析 |'
- en: '| Technical skillset | More technical | Less technical |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 技术技能 | 更高 | 较低 |'
- en: '| Machine learning | Yes | No |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习 | 是 | 否 |'
- en: '| Model deployment | Sometimes | No |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 模型部署 | 有时 | 否 |'
- en: '| Focus | Extracting insights and building models | Extracting insights |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 重点 | 提取洞察和建模 | 提取洞察 |'
- en: 'Figure 7.1: Comparison of Data Science and Data Analysis.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：数据科学和数据分析的比较。
- en: 'The common denominator between the two is collecting data, cleaning data, analyzing
    data, visualizing data, all of which fall into the category of extracting insights.
    Data science, additionally is about training machine learning models and usually
    it has a stronger focus on statistics. In some cases, depending on the setup in
    the company and industry practices, deploying models and writing software can
    be added to the list for data science. Automatic data analysis and data science
    aims to automate many of the tedious, repetitive tasks involved in working with
    data. This includes data cleaning, feature engineering, model training, tuning,
    and deployment. The goal is to make data scientists and analysts more productive
    by enabling faster iterations and less manual coding for common workflows.A lot
    of these tasks can be automated to some degree. Some of the tasks for data science
    are similar to those of a software developer that we talked about in chapter 6,
    *Developing Software*, namely writing and deploying software although with a narrower
    focus, on models.Data science platforms such as Weka, H2O, KNIME, RapidMiner,
    and Alteryx are unified machine learning and analytics engines that can be used
    for a variety of tasks, including preprocessing of large volumes of data and feature
    extraction. All of these come with a graphical user interface (GUI), have the
    capability to integrate 3^(rd) party data source, and write custom plug-ins. KNIME
    is mostly open-source, however also offers a commercial product called KNIME Server. Apache
    Spark is a versatile tool that can be used for a variety of tasks involved in
    data science. It can be used to to clean, transform, extract features, and prepare
    high-volume data for analysis and also to train and deploy machine learning models,
    both in streaming scenarios, when it’s about real-time decisions or monitoring
    events.Further, at its most fundamental, libraries for scientific computing such
    as NumPy can be serve for all tasks involved in automated data science. Deep learning
    and machine learning libraries such as TensorFlow, Pytorch, and Scikit-Learn can
    be used for a variety of tasks beyond creating complex machine learning models,
    including data preprocessing and feature extraction. Orchestration tools such
    as Airflow, Kedro, or others can help in all these tasks, and include a lot of
    integrations with specific tools related to all steps in data science.Several
    data science tools have generative AI support. In *Chapter 6*, *Developing Software*,
    we’ve already mentioned GitHub Copilot, but there are others such as the PyCharm
    AI Assistant, and even more to the point, Jupyter AI, which is a subproject of
    Project Jupyter that brings generative artificial intelligence to Jupyter notebooks.
    Jupyter AI allows users to generate code, fix errors, summarize content, and even
    create entire notebooks using natural language prompts. The tool connects Jupyter
    with LLMs from various providers, allowing users to choose their preferred model
    and embedding.Jupyter AI prioritizes responsible AI and data privacy. The underlying
    prompts, chains, and components are open source, ensuring transparency. It saves
    metadata about model-generated content, making it easy to track AI-generated code
    within the workflow. Jupyter AI respects user data privacy and only contacts LLMs
    when explicitly requested, which is done through LangChain integrations.To use
    Jupyter AI, users can install the appropriate version for JupyterLab and access
    it through a chat UI or the magic command interface. The chat interface features
    Jupyternaut, an AI assistant that can answer questions, explain code, modify code,
    and identify errors. Users can also generate entire notebooks from text prompts.The
    software allows users to teach Jupyternaut about local files and interact with
    LLMs using magic commands in notebook environments. It supports multiple providers
    and offers customization options for the output format. This screenshot from the
    documentation shows the chat feature, the Jupyternaut chat:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两者之间的共同点是收集数据、清理数据、分析数据、可视化数据，所有这些都属于提取洞见的范畴。此外，数据科学还涉及训练机器学习模型，通常更加注重统计学。在某些情况下，根据公司的设置和行业惯例，部署模型和编写软件可能会被添加到数据科学的任务清单中。自动化数据分析和数据科学旨在自动化与数据处理相关的许多乏味重复的任务。这包括数据清理、特征工程、模型训练、调优和部署。目标是通过实现更快的迭代和减少常见工作流程的手动编码来提高数据科学家和分析师的生产力。许多这些任务可以在一定程度上自动化。数据科学的一些任务与我们在第6章《软件开发》中谈到的软件开发者的任务类似，即编写和部署软件，尽管焦点更窄，专注于模型。数据科学平台（如Weka、H2O、KNIME、RapidMiner和Alteryx）是统一的机器学习和分析引擎，可用于各种任务，包括大容量数据的预处理和特征提取。所有这些都配有图形用户界面（GUI），具有集成第三方数据源和编写自定义插件的能力。KNIME主要是开源的，但也提供了一个名为KNIME
    Server的商业产品。Apache Spark是一种多功能工具，可用于数据科学中涉及的各种任务。它可用于清理、转换、提取特征和准备高容量数据进行分析，还可用于训练和部署机器学习模型，无论是在流式处理方案中，还是在实时决策或监视事件时。此外，在其最基本的层面上，科学计算库（如NumPy）可以用于自动化数据科学中涉及的所有任务。深度学习和机器学习库（如TensorFlow、Pytorch和Scikit-Learn）可以用于各种任务，包括数据预处理和特征提取。编排工具（如Airflow、Kedro或其他工具）可以帮助完成所有这些任务，并与数据科学的各个步骤相关的特定工具进行了大量集成。几种数据科学工具都支持生成式AI。在《第6章》《软件开发》中，我们已经提到了GitHub
    Copilot，但还有其他工具，如PyCharm AI助手，甚至更加具体的Jupyter AI，这是Project Jupyter的一个子项目，将生成式人工智能引入到Jupyter笔记本中。Jupyter
    AI允许用户生成代码、修复错误、总结内容，甚至使用自然语言提示创建整个笔记本。该工具将Jupyter与来自各种提供者的LLM连接起来，使用户可以选择其首选模型和嵌入。Jupyter
    AI优先考虑负责任的人工智能和数据隐私。底层提示、链和组件是开源的，确保透明度。它保存有关模型生成内容的元数据，使得在工作流程中跟踪AI生成的代码变得容易。Jupyter
    AI尊重用户数据隐私，只有在明确请求时才会与LLM联系，这是通过LangChain集成完成的。要使用Jupyter AI，用户可以安装适用于JupyterLab的适当版本，并通过聊天界面或魔术命令界面访问它。聊天界面具有Jupyternaut，一个可以回答问题、解释代码、修改代码和识别错误的AI助手。用户还可以从文本提示中生成整个笔记本。该软件允许用户教Jupyternaut有关本地文件，并在笔记本环境中使用魔术命令与LLM进行交互。它支持多个提供者，并为输出格式提供了定制选项。文档中的此截图显示了聊天功能，Jupyternaut聊天：
- en: '![Figure 7.2: Jupyter AI – Jupyternaut chat.](img/file50.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2：Jupyter AI – Jupyternaut 聊天。](img/file50.png)'
- en: 'Figure 7.2: Jupyter AI – Jupyternaut chat.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：Jupyter AI – Jupyternaut 聊天。
- en: It should be plain to see that having a chat like that at your fingertips to
    ask questions, create simple functions, or change existing functions can be a
    boon to data scientists. The benefits of using these tools include improved efficiency,
    reduced manual effort in tasks like model building or feature selection, enhanced
    interpretability of models, identification and fixing of data quality issues,
    integration with other scikit-learn pipelines (pandas_dq), and overall improvement
    in the reliability of results.Overall, automated data science can greatly accelerate
    analytics and ML application development. It allows data scientists to focus on
    higher value and creative aspects of the process. Democratizing data science for
    business analysts is also a key motivation behind automating these workflows.
    In the following sections, we’ll look into these steps in turn, and we’ll discuss
    automating them, and we’ll highlight how generative AI can make a contribution
    to improving the workflow and create efficiency gains.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，像这样的聊天工具能方便地提问问题、创建简单函数或更改现有函数，对于数据科学家来说是一种福音。使用这些工具的好处包括提高效率，在模型构建或特征选择等任务中减少手动工作，增强模型的可解释性，识别和修复数据质量问题，与其他
    scikit-learn 管道（pandas_dq）集成，以及结果可靠性的整体改进。总的来说，自动化数据科学可以极大加速分析和机器学习应用开发。它让数据科学家集中精力在流程的更高价值和创造性方面。对于商业分析师来说，使数据科学普及化也是自动化这些工作流的一个关键动机。在接下来的章节中，我们将依次讨论这些步骤，并讨论如何自动化它们，以及高效率的生成
    AI 如何为工作流的改进做出贡献。
- en: Data collection
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'Automated data collection is the process of collecting data without human intervention.
    Automatic data collection can be a valuable tool for businesses. It can help businesses
    to collect data more quickly and efficiently, and it can free up human resources
    to focus on other tasks. Generally, in the context of data science or analytics
    we refer to ETL (Extract, Transform, and Load) as the process that not only takes
    data from one or more sources (the data collection), but also prepares it for
    specific use cases. The ETL process typically follows these steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化数据收集是在无需人工干预的情况下进行数据收集的过程。自动数据收集对企业来说是一种有价值的工具。它可以帮助企业更快速、更高效地收集数据，并且可以释放人力资源来专注于其他任务。通常，在数据科学或分析的背景下，我们将
    ETL（抽取、转换和加载）视为不仅仅是从一个或多个来源获取数据（数据收集）的过程，还包括准备数据以满足特定用例的过程。ETL 过程通常遵循以下步骤：
- en: Extract: The data is extracted from the source systems. This can be done using
    a variety of methods, such as web scraping, API integration, or database queries.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取：数据从源系统中提取出来。这可以通过多种方法来完成，比如网页抓取、API 集成或数据库查询。
- en: Transform: The data is transformed into a format that can be used by the data
    warehouse or data lake. This may involve cleaning the data, removing duplicates,
    and standardizing the data format.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换：数据被转换为数据仓库或数据湖可以使用的格式。这可能涉及数据清洗、去重和标准化数据格式。
- en: Load: The data is loaded into the data warehouse or data lake. This can be done
    using a variety of methods, such as bulk loading or incremental loading.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 载入：数据被载入数据仓库或数据湖中。这可以通过多种方法来完成，比如批量载入或增量载入。
- en: 'ETL and data collection can be done using a variety of tools and techniques,
    such as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ETL 和数据收集可以使用多种工具和技术来完成，比如：
- en: Web scraping: Web scraping is the process of extracting data from websites.
    This can be done using a variety of tools, such as Beautiful Soup, Scrapy, Octoparse,
    .
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页抓取：网页抓取是从网站中提取数据的过程。这可以使用多种工具来完成，比如 Beautiful Soup、Scrapy、Octoparse。
- en: 'APIs (Application Programming Interfaces): These are a way for software applications
    to talk to each other. Businesses can use APIs to collect data from other companies
    without having to build their own systems.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API（应用程序接口）：这是软件应用程序进行交流的一种方法。企业可以使用 API 从其他公司收集数据，而无需建立自己的系统。
- en: 'Query languages: Any database can serve as data source including of the SQL
    (Structured Query Language) or the no-SQL variety.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询语言：任何数据库都可以作为数据源，包括 SQL（结构化查询语言）或非 SQL 类型。
- en: Machine learning: Machine learning can be used to automate the process of data
    collection. For example, businesses can use machine learning to identify patterns
    in data and then collect data based on those patterns.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习：机器学习可以用于自动化数据收集过程。例如，企业可以利用机器学习来识别数据中的模式，然后根据这些模式收集数据。
- en: 'Once the data has been collected, it can be processed to prepare it for use
    in a data warehouse or data lake. The ETL process will typically clean the data,
    remove duplicates, and standardize the data format. The data will then be loaded
    into the data warehouse or data lake, where it can be used by data analysts or
    data scientists to gain insights into the business.There are many ETL tools including
    commercial ones such as AWS glue, Google Dataflow, Amazon Simple Workflow Service
    (SWF), dbt, Fivetran, Microsoft SSIS, IBM InfoSphere DataStage, Talend Open Studio
    or open-source tools such as Airflow, Kafka, and Spark. In Python are many more
    tools, too many to list all, such as Pandas for data extraction and processing,
    and even celery and joblib, which can serve as ETL orchestration tools. In LangChain,
    there’s an integration with Zapier, which is an automation tool that can be used
    to connect different applications and services. This can be used to automate the
    process of data collection from a variety of sources.Here are some of the benefits
    of using automated ETL tools:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被收集，就可以对其进行处理，以便在数据仓库或数据湖中使用。ETL过程通常会清理数据、删除重复项并标准化数据格式。然后，数据将被加载到数据仓库或数据湖中，数据分析师或数据科学家可以利用这些数据来获取业务见解。有许多ETL工具，包括商业工具如AWS
    glue、Google Dataflow、Amazon Simple Workflow Service (SWF)、dbt、Fivetran、Microsoft
    SSIS、IBM InfoSphere DataStage、Talend Open Studio，或者开源工具如Airflow、Kafka和Spark。在Python中有更多的工具，太多了无法列举出来，比如用于数据提取和处理的Pandas，甚至是celery和joblib，它们可以作为ETL编排工具。在LangChain中，有与Zapier的集成，这是一种可以用于连接不同应用程序和服务的自动化工具。这可以用于自动化来自各种来源的数据收集过程。以下是使用自动化ETL工具的一些好处：
- en: Increased accuracy: Automated ETL tools can help to improve the accuracy of
    the data extraction, transformation, and loading process. This is because the
    tools can be programmed to follow a set of rules and procedures, which can help
    to reduce human error.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高准确性：自动化ETL工具可以帮助提高数据提取、转换和加载过程的准确性。这是因为这些工具可以编程遵循一组规则和程序，可以帮助减少人为错误。
- en: Reduced time to market: Automated ETL tools can help to reduce the time it takes
    to get data into a data warehouse or data lake. This is because the tools can
    automate the repetitive tasks involved in the ETL process, such as data extraction
    and loading.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩短上市时间：自动化ETL工具可以帮助缩短将数据放入数据仓库或数据湖中所需的时间。这是因为这些工具可以自动化ETL过程中涉及的重复任务，比如数据提取和加载。
- en: Improved scalability: Automated ETL tools can help to improve the scalability
    of the ETL process. This is because the tools can be used to process large volumes
    of data, and they can be easily scaled up or down to meet the needs of the business.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高可伸缩性：自动化的ETL工具可以帮助提高ETL过程的可伸缩性。这是因为这些工具可以用于处理大量的数据，并且它们可以轻松地根据业务需求进行横向或纵向扩展。
- en: Improved compliance: Automated ETL tools can help to improve compliance with
    regulations such as GDPR and CCPA. This is because the tools can be programmed
    to follow a set of rules and procedures, which can help to ensure that data is
    processed in a compliant manner.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善合规性：自动化ETL工具可以帮助改善符合GDPR和CCPA等法规的程度。这是因为这些工具可以编程遵循一组规则和程序，可以帮助确保数据以符合法规的方式进行处理。
- en: The best tool for automatic data collection will depend on the specific needs
    of the business. Businesses should consider the type of data they need to collect,
    the volume of data they need to collect, and the budget they have available.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化数据收集的最佳工具将取决于企业的具体需求。企业应考虑他们需要收集的数据类型、需要收集的数据量以及他们可用的预算。
- en: Visualization and EDA
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化和EDA
- en: 'Automated EDA (Exploratory Data Analysis) and visualization refer to the process
    of using software tools and algorithms to automatically analyze and visualize
    data, without significant manual intervention. Traditional EDA involves manually
    exploring and summarizing data to understand its various aspects before performing
    machine learning or deep learning tasks. It helps in identifying patterns, detecting
    inconsistencies, testing assumptions, and gaining insights. However, with the
    advent of large datasets and the need for efficient analysis, automated EDA has
    become important.Automated EDA and visualization tools provide several benefits.
    They can speed up the data analysis process, reducing the time spent on tasks
    like data cleaning, handling missing values, outlier detection, and feature engineering.
    These tools also enable a more efficient exploration of complex datasets by generating
    interactive visualizations that provide a comprehensive overview of the data.Several
    tools are available for automated EDA and visualization, including:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化的探索性数据分析（EDA）和可视化是指利用软件工具和算法自动分析和可视化数据的过程，无需显著的手动干预。传统的探索性数据分析涉及手动探索和总结数据，以了解在执行机器学习或深度学习任务之前的各个方面。它有助于识别模式，检测不一致性，测试假设并获得洞见。然而，随着大型数据集的出现和对高效分析的需求，自动化的EDA变得越来越重要。自动化的EDA和可视化工具提供了几个好处。它们可以加快数据分析过程，减少在数据清理、处理缺失值、异常值检测和特征工程等任务上花费的时间。这些工具还通过生成交互式可视化来更有效地探索复杂数据集，从而提供对数据的全面概述。有几种工具可用于自动化的EDA和可视化，包括：
- en: 'D-Tale: A library that facilitates easy visualization of pandas data frames.
    It supports interactive plots, 3D plots, heatmaps, correlation analysis, custom
    column creation.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D-Tale：一个库，方便地可视化pandas数据框。它支持交互式图形、3D图形、热图、相关分析、自定义列创建。
- en: 'ydata-profiling (previously pandas profiling): An open-source library that
    generates interactive HTML reports (`ProfileReport`) summarizing different aspects
    of the dataset such as missing values statistics, variable types distribution
    profiles, correlations between variables. It works with Pandas as well as Spark
    DataFrames.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ydata-profiling（之前是pandas profiling）：一个生成交互式HTML报告（`ProfileReport`）的开源库，总结数据集的不同方面，例如缺失值统计、变量类型分布概况、变量之间的相关性。它可以与Pandas以及Spark
    DataFrames一起使用。
- en: 'Sweetviz: A Python library that provides visualization capabilities for exploratory
    data analysis with minimal code required. It allows for comparisons between variables
    or datasets.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sweetviz：一个Python库，提供了对探索性数据分析的可视化能力，只需很少的代码。它允许在变量或数据集之间进行比较。
- en: 'Autoviz: This library automatically generates visualizations for datasets regardless
    of their size with just a few lines of code.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Autoviz：该库只需几行代码就可以自动生成各种大小的数据集的可视化。
- en: 'DataPrep: With just a few lines you can collect data from common data sources
    do EDA and data cleaning such as standardization of column names or entries.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataPrep：只需几行代码，您就可以从常见的数据源中收集数据，进行EDA和数据清理，例如标准化列名或条目。
- en: 'Lux: Displays a set of visualizations with interesting trends and patterns
    in the dataset displayed via an interactive widget that users can quickly browse
    in order to gain insights.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lux：通过交互式小部件显示数据集中有趣的趋势和模式的一组可视化，用户可以快速浏览以获得洞见。
- en: The use of generative AI in data visualization adds another dimension to automated
    EDA by allowing algorithms to generate new visualizations based on existing ones
    or specific user prompts. Generative AI has the potential to enhance creativity
    by automating part of the design process while maintaining human control over
    the final output.Overall, automated EDA and visualization tools offer significant
    advantages in terms of time efficiency, comprehensive analysis, and the generation
    of meaningful visual representations of data. Generative AI has the potential
    to revolutionize data visualization in a number of ways. For example, it can be
    used to create more realistic and engaging visualizations, which can help in business
    communication and to communicate data more effectively to stakeholders to provide
    each user with the information they need to gain insights and make informed decisions.Generative
    AI can enhance and extend the creation that traditional tools are capable of by
    making personalized visualizations tailored to the individual needs of each user.
    Further, Generative AI can be used to create interactive visualizations that allow
    users to explore data in new and innovative ways.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据可视化中使用生成式人工智能为自动化探索性数据分析增加了另一个维度，它允许算法基于现有的可视化结果或特定用户提示生成新的可视化结果。生成式人工智能有潜力通过自动化设计过程的一部分来增强创造力，同时保持对最终输出的人类控制。总体而言，自动化探索性数据分析和可视化工具在时间效率、全面分析和生成有意义的数据可视化方面具有显著优势。生成式人工智能有潜力以多种方式改变数据可视化。例如，它可以用于创建更真实和吸引人的可视化效果，这有助于业务沟通，并更有效地向利益相关者传达数据，以向每个用户提供他们需要获取见解并做出知情决策所需的信息。生成式人工智能可以通过为每个用户量身定制的个性化可视化效果增强和扩展传统工具的创建能力。此外，生成式人工智能可以用于创建交互式可视化效果，允许用户以新颖和创新的方式探索数据。
- en: Pre-processing and feature extraction
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理和特征提取
- en: Automated data preprocessing is the process of automating the tasks involved
    in data preprocessing. This can include tasks such as data cleaning, data integration,
    data transformation, and feature extraction. It is related to the transform step
    in ETL, so there’s a lot of overlap in tools and techniques.Data preprocessing
    is important because it ensures that data is in a format that can be used by data
    analysts and machine learning models. This includes removing errors and inconsistencies
    from the data, as well as converting it into a format that is compatible with
    the analytical tools that will be used.Manually engineering features can be tedious
    and time consuming, so automating this process is valuable. Recently, several
    open source Python libraries have emerged to help auto-generate useful features
    from raw data as we’ll see.Featuretools offers a general-purpose framework that
    can synthesize many new features from transactional and relational data. It integrates
    across multiple ML frameworks making it flexible. Feature Engine provides a simpler
    set of transformers focused on common data transformations like handling missing
    data. For optimizing feature engineering specifically for tree-based models, ta
    from Microsoft shows strong performance through techniques like automatic crossing.AutoGluon
    Features applies neural network style automatic feature generation and selection
    to boost model accuracy. It is tightly integrated with the AutoGluon autoML capabilities.
    Finally, TensorFlow Transform operates directly on Tensorflow pipelines to prepare
    data for models during training. It has progressed rapidly with diverse open source
    options now available. Featuretools provides the most automation and flexibility
    while integrating across ML frameworks. For tabular data, ta and Feature Engine
    offer easy-to-use transformers optimized for different models. Tf.transform is
    ideal for TensorFlow users, while AutoGluon specializes in the Apache MXNet deep
    learning software framework.As for time series data, Tsfel is a library that extracts
    features from time series data. It allows users to specify the window size for
    feature extraction and can analyze the temporal complexity of the features. It
    computes statistical, spectral, and temporal features.On the other hand, tsflex
    is a time series feature extraction toolkit that is flexible and efficient for
    sequence data. It makes few assumptions about the data structure and can handle
    missing data and unequal lengths. It also computes rolling features.Both libraries
    offer more modern options for automated time series feature engineering compared
    to tsfresh. Tsfel is more full-featured, while tsflex emphasizes flexibility on
    complex sequence data.There are a few tools that focus on data quality for machine
    learning and data science that come with data profiling and automatic data transformations.
    For example, the pandas-dq library, which can be integrated with scikit-learn
    pipelines, offers a range of useful features for data profiling, train-test comparison,
    data cleaning, data imputation (filling missing values), and data transformation
    (e.g., skewness correction). It helps improve the quality of data analysis by
    addressing potential issues before modeling.More focused on improved reliability
    through early identification of potential issues or errors are tools like Great
    Expectations and Deequ. Great Expectations is a tool for validating, documenting,
    and profiling data to maintain quality and improve communication between teams.
    It allows users to assert expectations on the data, catch issues quickly through
    unit tests for data, create documentation and reports based on expectations. Deequ
    is built on top of Apache Spark for defining unit tests for data quality in large
    datasets. It lets users explicitly state assumptions about the dataset and verifies
    them through checks or constraints on attributes. By ensuring adherence to these
    assumptions, it prevents crashes or wrong outputs in downstream applications.All
    these libraries allow data scientists to shorten feature preparation and expand
    the feature space to improve model quality. Automated feature engineering is becoming
    essential to leveraging the full power of ML algorithms on complex real-world
    data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化数据预处理是自动化数据预处理任务的过程。 这可以包括诸如数据清洗、数据集成、数据转换和特征提取等任务。 它与ETL中的转换步骤相关，因此在工具和技术上有很多重叠。数据预处理很重要，因为它确保数据处于可以被数据分析师和机器学习模型使用的格式。
    这包括从数据中删除错误和不一致性，以及将其转换为与将要使用的分析工具兼容的格式。手动工程特征可能很烦琐且耗时，因此自动化此过程非常有价值。 最近，出现了几个开源Python库，以帮助从原始数据中自动生成有用的特征，我们将看到。Featuretools提供了一个通用框架，可以从事务性和关系性数据中合成许多新特征。
    它集成了多个ML框架，使其灵活。 Feature Engine提供了一组更简单的转换器，专注于处理缺失数据等常见数据转换。 为了针对基于树的模型专门优化特征工程，来自Microsoft的ta通过诸如自动交叉等技术表现出强大的性能。AutoGluon
    Features将神经网络风格的自动特征生成和选择应用于提高模型准确性。 它与AutoGluon autoML功能紧密集成。 最后，TensorFlow Transform直接在Tensorflow管道上运行，以准备模型在训练期间使用的数据。
    它已经迅速发展，现在有多种开源选项。Featuretools提供了最多的自动化和灵活性，同时集成了ML框架。 对于表格数据，ta和Feature Engine提供了易于使用的针对不同模型进行优化的转换器。
    Tf.transform非常适合TensorFlow用户，而AutoGluon专门针对Apache MXNet深度学习软件框架。至于时间序列数据，Tsfel是一个从时间序列数据中提取特征的库。
    它允许用户指定特征提取的窗口大小，并可以分析特征的时间复杂性。 它计算统计、频谱和时间特征。另一方面，tsflex是一个灵活高效的时间序列特征提取工具包，适用于序列数据。
    它对数据结构做出了少量假设，并且可以处理缺失数据和长度不等的情况。 它还计算滚动特征。与tsfresh相比，这两个库提供了更现代的自动时间序列特征工程选项。
    Tsfel更全面，而tsflex强调对复杂序列数据的灵活性。有一些工具专注于机器学习和数据科学的数据质量，带有数据概要文件和自动数据转换。 例如，pandas-dq库可以与scikit-learn管道集成，为数据概要文件、训练测试比较、数据清理、数据填充（填充缺失值）和数据转换（例如，偏斜校正）提供一系列有用的功能。
    它通过在建模之前解决潜在问题来改善数据分析的质量。更专注于通过及早识别潜在问题或错误来改进可靠性的工具包括Great Expectations和Deequ。
    Great Expectations是一个用于验证、记录和分析数据的工具，以保持质量并改进团队之间的沟通。 它允许用户对数据断言期望，并通过对数据进行单元测试迅速捕获问题，根据期望创建文档和报告。
    Deequ是建立在Apache Spark之上的工具，用于为大型数据集定义数据质量的单元测试。 它让用户明确陈述关于数据集的假设，并通过对属性的检查或约束验证它们。
    通过确保遵守这些假设，它可以防止下游应用程序中的崩溃或错误输出。所有这些库都允许数据科学家缩短特征准备时间并扩展特征空间以改进模型质量。自动特征工程对于利用复杂现实世界数据上ML算法的全部潜力变得至关重要。
- en: AutoML
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoML
- en: 'Automated Machine Learning (AutoML) frameworks are tools that automate the
    process of machine learning model development. They can be used to automate tasks
    such as data cleaning, feature selection, model training, and hyperparameter tuning.
    This can save data scientists a lot of time and effort, and it can also help to
    improve the quality of machine learning models.The basic idea of AutoML is illustrated
    in this diagram from the Github repo of the mljar autoML library (source: https://github.com/mljar/mljar-supervised):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化机器学习（AutoML）框架是自动化机器学习模型开发过程的工具。它们可用于自动化诸如数据清洗、特征选择、模型训练和超参数调整等任务。这可以节省数据科学家大量的时间和精力，还可以帮助提高机器学习模型的质量。AutoML
    的基本思想在 mljar autoML 库的 Github 仓库中通过这张图解释（来源：https://github.com/mljar/mljar-supervised）：
- en: '![Figure 7.3: How AutoML works.](img/file51.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3：AutoML 的工作原理。](img/file51.png)'
- en: 'Figure 7.3: How AutoML works.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：AutoML 的工作原理。
- en: 'Load some data, try different combinations of preprocessing methods, ML algorithms,
    training and model parameters, create explanations, compare results in a leaderboard
    together with visualizations. The main value proposition of an AutoML framework
    is the ease-of-use of and an increased developer productivity in finding a machine
    learning model, understanding it, and getting it to production. AutoML tools have
    been around for a long time. One of the first broader frameworks was AutoWeka,
    written in Java, and it was designed to automate the process of machine learning
    model development for tabular data in the Weka (Waikato Environment for Knowledge
    Analysis) machine learning suite, which is developed at the University of Waikato.In
    the years since AutoWeka was released, there have been many other AutoML frameworks
    developed. Some of the most popular AutoML frameworks today include auto-sklearn,
    autokeras, NASLib, Auto-Pytorch, tpot, optuna, autogluon, and ray (tune). These
    frameworks are written in a variety of programming languages, and they support
    a variety of machine learning tasks.Recent advances in autoML and neural architecture
    search have allowed tools to automate large parts of the machine learning pipeline.
    Leading solutions like Google AutoML, Azure AutoML, and H2O AutoML/Driverless
    AI can automatically handle data prep, feature engineering, model selection, hyperparameter
    tuning, and deployment based on the dataset and problem type. These make machine
    learning more accessible to non-experts as well.Current autoML solutions can handle
    structured data like tables and time series data very effectively. They can automatically
    generate relevant features, select algorithms like tree ensembles, neural networks
    or SVMs, and tune hyperparameters. Performance is often on par or better than
    manual process due to massive hyperparameter search. AutoML for unstructured data
    like images, video and audio is also advancing rapidly with neural architecture
    search techniques.Open source libraries like AutoKeras, AutoGluon, and AutoSklearn
    provide accessible autoML capabilities as well. However, most autoML tools still
    require some coding and data science expertise. Fully automating data science
    remains challenging and autoML does have limitations in flexibility and controllability.
    But rapid progress is being made with more user-friendly and performant solutions
    coming to market.Here’s a tabular summary of frameworks:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 载入一些数据，尝试不同的预处理方法、ML算法、训练和模型参数的组合，创建解释，将结果与可视化内容一起在排行榜中进行比较。 AutoML 框架的主要价值主张是易用性和增加开发者在找到机器学习模型、理解它并将其投入生产中的生产力。
    AutoML 工具已经存在很长时间了。 最早的广泛框架之一是 AutoWeka，它是用 Java 编写的，并且旨在自动化 Weka（Waikato 知识分析环境）机器学习套件中用于表格数据的机器学习模型开发过程，该套件是在
    Waikato 大学开发的。自 AutoWeka 发布以来的几年里，已经开发出了许多其他 AutoML 框架。 如今一些最流行的 AutoML 框架包括 auto-sklearn、autokeras、NASLib、Auto-Pytorch、tpot、optuna、autogluon
    和 ray（调整）。 这些框架用多种编程语言编写，并支持多种机器学习任务。自动化机器学习和神经架构搜索的最新进展使得工具能够自动化机器学习管道的大部分内容。
    领先的解决方案如 Google AutoML、Azure AutoML 和 H2O AutoML/Driverless AI 可以根据数据集和问题类型自动处理数据准备、特征工程、模型选择、超参数调整和部署。这使得机器学习更容易被非专家所接受。目前的自动机器学习解决方案可以非常有效地处理结构化数据，如表格和时间序列数据。它们可以自动生成相关特征，选择算法，如树集成、神经网络或
    SVM，并调整超参数。 由于大量的超参数搜索，性能通常与手动过程相当甚至更好。针对图像、视频和音频等非结构化数据的自动机器学习也正在迅速发展，其中包括神经架构搜索技术。
    AutoKeras、AutoGluon 和 AutoSklearn 等开源库也提供了可访问的自动机器学习能力。 但是，大多数自动机器学习工具仍然需要一些编码和数据科学专业知识。
    完全自动化数据科学仍然具有挑战性，而且自动机器学习在灵活性和可控性方面存在局限性。 但是，随着更加用户友好和性能更好的解决方案不断问世，进展正在迅速取得。以下是框架的表格摘要：
- en: '| **Framework** | **Language** | **ML Frameworks** | **First Release** | **Key
    Features** | **Data Types** | **Maintainer** | **Github stars** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **框架** | **语言** | **ML 框架** | **首次发布** | **关键特性** | **数据类型** | **维护者** |
    **Github 星标** |'
- en: '| Auto-Keras | Python | Keras | 2017 | Neural architecture search, easy to
    use | Images, text, tabular | Keras Team (DATA Lab, Texas A&M) | 8896 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Keras | Python | Keras | 2017 | 神经架构搜索，易于使用 | 图像、文本、表格 | Keras 团队（DATA
    实验室，德克萨斯农工大学） | 8896 |'
- en: '| Auto-PyTorch | Python | PyTorch | 2019 | Neural architecture search, hyperparameter
    tuning | Tabular, text, image, time series | AutoML Group, Univ. of Freiburg |
    2105 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Auto-PyTorch | Python | PyTorch | 2019 | 神经架构搜索，超参数调整 | 表格、文本、图像、时间序列 | AutoML
    Group，弗莱堡大学 | 2105 |'
- en: '| Auto-Sklearn | Python | Scikit-learn | 2015 | Automated scikit-learn workflows
    | Tabular | AutoML Group, Univ. of Freiburg | 7077 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Sklearn | Python | Scikit-learn | 2015 | 自动化的scikit-learn工作流 | 表格数据
    | 弗赖堡大学AutoML小组 | 7077 |'
- en: '| Auto-WEKA | Java* | WEKA | 2012 | Bayesian optimization | Tabular | University
    of British Columbia | 315 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Auto-WEKA | Java* | WEKA | 2012 | 贝叶斯优化 | 表格数据 | 英属哥伦比亚大学 | 315 |'
- en: '| AutoGluon | Python | MXNet, PyTorch | 2019 | Optimized for deep learning
    | Text, image, tabular | Amazon | 6032 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| AutoGluon | Python | MXNet，PyTorch | 2019 | 专为深度学习优化 | 文本，图像，表格数据 | 亚马逊 |
    6032 |'
- en: '| AWS SageMaker Autopilot | Python | XGBoost, sklearn | 2020 | Cloud-based,
    simple | Tabular | Amazon | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| AWS SageMaker Autopilot | Python | XGBoost，sklearn | 2020 | 基于云的，简单 | 表格数据
    | 亚马逊 | - |'
- en: '| Azure AutoML | Python | Scikit-learn, PyTorch | 2018 | Explainable models
    | Tabular | Microsoft | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Azure AutoML | Python | Scikit-learn，PyTorch | 2018 | 可解释性模型 | 表格数据 | 微软
    | - |'
- en: '| DataRobot | Python, R | Multiple | 2012 | Monitoring, explainability | Text,
    image, tabular | DataRobot | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| DataRobot | Python, R | 多种 | 2012 | 监控，可解释性 | 文本，图像，表格数据 | DataRobot | -
    |'
- en: '| Google AutoML | Python | TensorFlow | 2018 | Easy to use, cloud-based | Text,
    image, video, tabular | Google | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Google AutoML | Python | TensorFlow | 2018 | 易于使用，基于云的 | 文本，图像，视频，表格数据 |
    谷歌 | - |'
- en: '| H2O AutoML | Python, R | XGBoost, GBMs | 2017 | Automatic workflow, ensembling
    | Tabular, time series, images | h2o.ai | 6430 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| H2O AutoML | Python, R | XGBoost，GBMs | 2017 | 自动化工作流，集成学习 | 表格数据，时间序列，图像
    | h2o.ai | 6430 |'
- en: '| hyperopt-sklearn | Python | Scikit-learn | 2014 | Hyperparameter tuning |
    Tabular | Hyperopt team | 1451 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| hyperopt-sklearn | Python | Scikit-learn | 2014 | 超参数调整 | 表格数据 | Hyperopt团队
    | 1451 |'
- en: '| Ludwig | Python | Transformers/Pytorch | 2019 | Low-code framework for building
    and tuning custom LLMs and deep neural networks | Multiple | Linux Foundation
    | 9083 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Ludwig | Python | Transformers/Pytorch | 2019 | 无代码框架，用于构建和调整自定义LLM和深度神经网络
    | 多种 | Linux Foundation | 9083 |'
- en: '| MLJar | Python | Multiple | 2019 | Explainable, customizable | Tabular |
    MLJar | 2714 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| MLJar | Python | 多种 | 2019 | 可解释，可定制 | 表格数据 | MLJar | 2714 |'
- en: '| NASLib | Python | PyTorch, TensorFlow/Keras | 2020 | Neural architecture
    search | Images, text | AutoML Group, Univ. of Freiburg | 421 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| NASLib | Python | PyTorch，TensorFlow/Keras | 2020 | 神经结构搜索 | 图像，文本 | 弗赖堡大学AutoML小组
    | 421 |'
- en: '| Optuna | Python | Agnostic | 2019 | Hyperparameter tuning | Agnostic | Preferred
    Networks Inc | 8456 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Optuna | Python | 通用 | 2019 | 超参数调整 | 通用 | Preferred Networks公司 | 8456 |'
- en: '| Ray (Tune) | Python | Agnostic | 2018 | Distributed hyperparameter tuning;
    Accelerating ML workloads | Agnostic | University of California, Berkeley | 26906
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Ray (Tune) | Python | 通用 | 2018 | 分布式超参数调整；加速ML工作负载 | 通用 | 加州大学伯克利分校 | 26906
    |'
- en: '| TPOT | Python | Scikit-learn, XGBoost | 2016 | Genetic programming, pipelines
    | Tabular | Epistasis Lab, Penn State | 9180 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| TPOT | Python | Scikit-learn，XGBoost | 2016 | 遗传编程，管道 | 表格数据 | Penn State大学的Epistasis实验室
    | 9180 |'
- en: '| TransmogrifAI | Scala | Spark ML | 2018 | AutoML on Spark | Text, tabular
    | Salesforce | 2203 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| TransmogrifAI | Scala | Spark ML | 2018 | 基于Spark的AutoML | 文本，表格数据 | Salesforce
    | 2203 |'
- en: 'Figure 7.4: Comparison of open-source AutoML frameworks. Weka can be accessed
    from Python as pyautoweka. Stars for Ray Tune and H2O concern the whole project
    rather than only the automl part. The H2O commercial product related to AutoML
    is Driverless AI. Most projects are maintained by a community of contributors
    not affiliated with just one company'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：开源AutoML框架的比较。Weka可以通过pyautoweka从Python访问。Ray Tune和H2O的星号涉及整个项目，而不仅仅是automl部分。与AutoML相关的H2O商业产品是Driverless
    AI。大多数项目由一组与任何一个公司无关的贡献者维护
- en: 'I have only included the biggest frameworks, libraries or products – omitting
    a few. Although the focus is on open-source frameworks in Python, I’ve included
    a few big commercial products. Github stars aim to show the popularity of the
    framework – they are not relevant to proprietary products. Pycaret is another
    big project (7562 stars) that gives the option to train several models simultaneously
    and compare them with relatively low amounts of code. Projects like Nixtla’s Statsforecast
    and MLForecast, or Darts have similar functionality specific to time series data.Libraries
    like Auto-ViML and deep-autoviml handle various types of variables and are built
    on scikit-learn and keras, respectively. They aim to make it easy for both novices
    and experts to experiment with different kinds of models and deep learning. However,
    users are advised to exercise their own judgement for accurate and explainable
    results.Important features of AutoML frameworks include the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我只列出了最大的框架、库或产品，省略了一些。虽然焦点在于 Python 中的开源框架，但我也包括了一些大的商业产品。Github 星标旨在展示框架的受欢迎程度
    - 它们与专有产品无关。Pycaret 是另一个大型项目（7562 颗星），它提供了同时训练多个模型并用相对较少的代码进行比较的选项。像 Nixtla 的
    Statsforecast 和 MLForecast，或者是 Darts 这样的项目，具有特定于时间序列数据的类似功能。像 Auto-ViML 和 deep-autoviml
    这样的库处理各种类型的变量，分别基于 scikit-learn 和 keras 构建。它们旨在使初学者和专家都能轻松尝试不同类型的模型和深度学习。然而，用户应该行使自己的判断以获得准确和可解释的结果。AutoML
    框架的重要功能包括以下内容：
- en: 'Deployment: Some solutions, especially those in the cloud, can be directly
    deployed to production. Others export to tensorflow or other formats.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署：有些解决方案，特别是云端解决方案，可以直接部署到生产环境。其他的则导出到 tensorflow 或其他格式。
- en: 'Types of data: Most solutions focus on tabular datasets; deep learning automl
    frameworks often work with different types of data. For example, autogluon facilitates
    rapid comparison and prototyping of ml solutions for images, text, time series
    in addition to tabular data. A few that focus on hyperparameter optimization such
    as optuna and ray tune, are totally agnostic to the format.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型：大多数解决方案都专注于制表数据集；深度学习自动化框架经常处理不同类型的数据。例如，autogluon 除了制表数据外，还促进了针对图像、文本和时间序列的机器学习解决方案的快速比较和原型制作。像
    optuna 和 ray tune 这样专注于超参数优化的几个工具是对格式完全无偏见的。
- en: 'Explainability: This can be very important depending on the industry, related
    to regulation (for example, healthcare or insurance) or reliability (finance).
    For a few solutions, this is a unique selling point.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性：这可能非常重要，具体取决于行业，与法规（例如医疗保险）或可靠性（金融）有关。对于一些解决方案，这是一个独特的卖点。
- en: 'Monitoring: After deployment, the model performance can deteriorate (drift).
    A few providers provide monitoring of performance.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控：部署后，模型性能可能会恶化（漂移）。少数提供者提供性能监视。
- en: 'Accessibility: Some providers require coding or at least basic data science
    understanding, others are turnkey solutions that require very little to no code.
    Typically, low and no-code solutions are less customizable.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可访问性：有些提供者需要编码或至少具备基本的数据科学理解，而其他的则是开箱即用的解决方案，几乎不需要编写任何代码。通常，低代码和无代码解决方案的可自定义性较低。
- en: 'Open Source: The advantage of open-source platforms is that they are fully
    transparent about the implementation and the availability of methods and their
    parameters, and that they are fully extensible.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源：开源平台的优点在于它们完全透明地公开了实现和方法及其参数的可用性，并且它们是完全可扩展的。
- en: 'Transfer Learning: This capability means being able to extend or customize
    existing foundation models.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转移学习：这种能力意味着能够扩展或自定义现有的基础模型。
- en: 'There is a lot more to cover here that would go beyond the scope of this chapter
    such as the number of available methods. Less-well supported are features such
    as self-supervised learning, reinforcement learning, or generative image and audio
    models. For deep learning, a few libraries focus on the backend being specialized
    in Tensorflow, Pytorch, or MXNet. Auto-Keras, NASLib, and Ludwig have broader
    support, especially because they work with Keras. Starting with version 3.0, which
    is scheduled for release in fall 2023, Keras supports the three major backends
    TensorFlow, JAX, and PyTorch. Sklearn has its own hyperparameter optimization
    tools such as grid search, random search, successive halving. More specialized
    libraries such as auto-sklearn and hyperopt-sklearn go beyond this by offering
    methods for Bayesian Optimization. Optuna can integrate with a broad variety of
    ML frameworks such as AllenNLP, Catalyst, Catboost, Chainer, FastAI, Keras, LightGBM,
    MXNet, PyTorch, PyTorch, Ignite, PyTorch, Lightning, TensorFlow, and XGBoost.
    Ray Tune comes with its own integrations among which is optuna. Both of them come
    with cutting edge parameter optimization algorithms and mechanisms for scaling
    (distributed training).In addition to the features listed above, some of these
    frameworks can automatically perform feature engineering tasks, such as data cleaning
    and feature selection, for example removing highly correlated features, and generating
    performance results graphically. Each of the listed tools has their own implementations
    for each step of the process such as feature selection and feature transformations
    – what differs is the extent to which this is automated. More specifically, the
    advantages of using AutoML frameworks include:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里还有很多内容需要涵盖，这将超出本章的范围，比如可用方法的数量。支持较少的功能包括自监督学习、强化学习或生成图像和音频模型。对于深度学习，一些库专注于后端，专门使用Tensorflow、Pytorch或MXNet。Auto-Keras、NASLib和Ludwig具有更广泛的支持，特别是因为它们与Keras一起工作。从计划于2023年秋季发布的版本3.0开始，Keras支持三个主要的后端TensorFlow、JAX和PyTorch。Sklearn拥有自己的超参数优化工具，如网格搜索、随机搜索、连续二分法。更专业的库，如auto-sklearn和hyperopt-sklearn，提供了贝叶斯优化的方法。Optuna可以与各种ML框架集成，如AllenNLP、Catalyst、Catboost、Chainer、FastAI、Keras、LightGBM、MXNet、PyTorch、PyTorch
    Ignite、PyTorch Lightning、TensorFlow和XGBoost。Ray Tune具有其自身的集成，其中包括optuna。它们都具有领先的参数优化算法和用于扩展（分布式训练）的机制。除了上述列出的功能外，这些框架中的一些可以自动执行特征工程任务，例如数据清理和特征选择，例如删除高度相关的特征，并以图形方式生成性能结果。每个列出的工具都有它们各自的实现，如特征选择和特征转换的每个步骤-不同之处在于自动化的程度。更具体地说，使用AutoML框架的优势包括：
- en: Time savings: AutoML frameworks can save data scientists a lot of time by automating
    the process of machine learning model development.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间节约：AutoML框架可以通过自动化机器学习模型开发的过程，为数据科学家节省大量时间。
- en: Improved accuracy: AutoML frameworks can help to improve the accuracy of machine
    learning models by automating the process of hyperparameter tuning.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高准确性：AutoML框架可以通过自动化超参数调整的过程来帮助提高机器学习模型的准确性。
- en: Increased accessibility: AutoML frameworks make machine learning more accessible
    to people who do not have a lot of experience with machine learning.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强可访问性：AutoML框架使那些对机器学习经验不多的人更容易接触机器学习。
- en: 'However, there are also some disadvantages to using AutoML frameworks:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用AutoML框架也存在一些缺点：
- en: Black box: AutoML frameworks can be "black boxes," meaning that it can be difficult
    to understand how they work. This can make it difficult to debug problems with
    AutoML models.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒子：AutoML框架可以是“黑盒子”，这意味着它的工作原理可能难以理解。这可能会使得调试AutoML模型的问题变得困难。
- en: Limited flexibility: AutoML frameworks can be limited in terms of the types
    of machine learning tasks that they can automate.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限的灵活性：AutoML框架在能够自动化的机器学习任务类型方面可能会有所限制。
- en: A lot of the above tools have at least some kind of automatic feature engineering
    or preprocessing functionality, however, there are a few more specialized tools
    for this.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述工具中有很多都至少具有某种自动特征工程或预处理功能，但是也有一些更专业化的工具可以实现这一点。
- en: The impact of generative models
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成模型的影响
- en: 'Generative AI and LLMs like GPT-3 have brought about significant changes to
    the field of data science and analysis. These models, particularly LLMs have the
    potential to revolutionize all steps involved in data science in a number of ways
    offering exciting opportunities for researchers and analysts. Generative AI models,
    such as ChatGPT, have the ability to understand and generate human-like responses,
    making them valuable tools for enhancing research productivity.Generative AI can
    play a crucial role in analyzing and interpreting research data. These models
    can assist in data exploration, uncover hidden patterns or correlations, and provide
    insights that may not be apparent through traditional methods. By automating certain
    aspects of data analysis, generative AI saves time and resources, allowing researchers
    to focus on higher-level tasks.Another area where generative AI can benefit researchers
    is in performing literature reviews and identifying research gaps. ChatGPT and
    similar models can summarize vast amounts of information from academic papers
    or articles, providing a concise overview of existing knowledge. This helps researchers
    identify gaps in the literature and guide their own investigations more efficiently.
    We’ve looked at this aspect of using generative AI models in *Chapter 4*, *Question
    Answering*.Other use cases for generative AI can be:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能和像GPT-3这样的LLMs已经给数据科学和分析领域带来了重大变革。这些模型，特别是LLMs，有潜力以多种方式彻底改变数据科学的所有步骤，为研究人员和分析师提供令人兴奋的机会。生成式人工智能模型，比如ChatGPT，能够理解和生成类似人类的回应，成为增强研究生产力的有价值工具。生成式人工智能在分析和解释研究数据方面起着关键作用。这些模型可以协助进行数据探索，发现隐藏的模式或相关性，并提供通过传统方法可能不明显的见解。通过自动化数据分析的某些方面，生成式人工智能节省了时间和资源，使研究人员能够专注于更高级别的任务。生成式人工智能可以在帮助研究人员进行文献综述和识别研究空白方面起到关键作用。ChatGPT和类似模型可以总结学术论文或文章中的大量信息，提供现有知识的简洁概述。这有助于研究人员更有效地识别文献中的空白并指导他们自己的调查。我们在*第4章*
    *问题回答*中研究了使用生成式人工智能模型的这一方面。生成式人工智能的其他用例可能包括：
- en: Automatically generate synthetic data: Generative AI can be used to automatically
    generate synthetic data that can be used to train machine learning models. This
    can be helpful for businesses that do not have access to large amounts of real-world
    data.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动产生合成数据：生成式人工智能可用于自动生成合成数据，可用于训练机器学习模型。这对于没有大量真实世界数据的企业非常有帮助。
- en: Identify patterns in data: Generative AI can be used to identify patterns in
    data that would not be visible to human analysts. This can be helpful for businesses
    that are looking to gain new insights from their data.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别数据中的模式：生成式人工智能可以用于识别人类分析员无法看到的数据中的模式。这对于希望从数据中获得新见解的企业非常有帮助。
- en: Create new features from existing data: Generative AI can be used to create
    new features from existing data. This can be helpful for businesses that are looking
    to improve the accuracy of their machine learning models.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有数据中创建新特征：生成式人工智能可以用于从现有数据中创建新特征。这对于希望改善他们的机器学习模型准确性的企业非常有帮助。
- en: 'According to recent reports by the likes of McKinsey and KPMG, the consequences
    of AI relate to what data scientists will work on, how they will work, and who
    can work on data science tasks. The main areas of key impact include:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '根据像麦肯锡和毕马威这样的最近报告所述，人工智能的后果涉及数据科学家将工作的内容，他们如何工作以及谁能完成数据科学任务。主要影响包括:'
- en: 'Democratization of AI: Generative models allow many more people to leverage
    AI by generating text, code, and data from simple prompts. This expands the use
    of AI beyond data scientists.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能的民主化：生成式模型让更多人通过简单提示生成文本、代码和数据来利用人工智能。这将人工智能的使用扩展到数据科学家以外的范围。
- en: 'Increased productivity: By auto-generating code, data, and text, generative
    AI can accelerate development and analysis workflows. This allows data scientists
    and analysts to focus on higher-value tasks.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高生产力：通过自动生成代码、数据和文本，生成式人工智能可以加速开发和分析工作流程。这使数据科学家和分析师能够专注于更高价值的任务。
- en: 'Innovation in data science: Generative AI is bringing about is the ability
    to explore data in new and more creative ways, and generate new hypotheses and
    insights that would not have been possible with traditional methods'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学的创新：生成式人工智能正在带来更创新的方式来探索数据，并生成以传统方法不可能的新假设和见解
- en: 'Disruption of industries: New applications of generative AI could disrupt industries
    by automating tasks or enhancing products and services. Data teams will need to
    identify high-impact use cases.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行业的颠覆：生成式人工智能的新应用可能通过自动化任务或增强产品和服务来颠覆行业。数据团队将需要确定高影响力的用例。
- en: 'Limitations remain: Current models still have accuracy limitations, bias issues,
    and lack of controllability. Data experts are needed to oversee responsible development.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仍然存在限制：当前模型仍然存在准确性限制、偏见问题和缺乏可控性。需要数据专家监督负责的发展。
- en: 'Importance of governance: Rigorous governance over development and ethical
    use of generative AI models will be critical to maintaining stakeholder trust.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 治理的重要性：对生成式人工智能模型的发展和道德使用进行严格的治理将对维护利益相关者的信任至关重要。
- en: Need for partnerships - Companies will need to build ecosystems with partners,
    communities and platform providers to effectively leverage generative AI capabilities.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合作伙伴关系的需求 - 公司将需要与合作伙伴、社区和平台提供商建立生态系统，以有效利用生成式人工智能的能力。
- en: Changes to data science skills - Demand may shift from coding expertise to abilities
    in data governance, ethics, translating business problems, and overseeing AI systems.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学技能的变化 - 需求可能从编码专业知识转向数据治理、伦理、将业务问题转化为语言以及监督人工智能系统等能力。
- en: 'Regarding democratization and innovation of data science, more specifically,
    generative AI is also having an impact on the way that data is visualized. In
    the past, data visualizations were often static and two-dimensional. However,
    generative AI can be used to create interactive and three-dimensional visualizations
    that can help to make data more accessible and understandable. This is making
    it easier for people to understand and interpret data, which can lead to better
    decision-making.Again, one of the biggest changes that generative AI is bringing
    about is the democratization of data science. In the past, data science was a
    very specialized field that required a deep understanding of statistics and machine
    learning. However, generative AI is making it possible for people with less technical
    expertise to create and use data models. This is opening up the field of data
    science to a much wider range of people.LLMs and generative AI can play a crucial
    role in automated data science by offering several benefits:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据科学的民主化和创新，更具体地说，生成式人工智能也正在影响数据可视化的方式。过去，数据可视化通常是静态的和二维的。然而，生成式人工智能可以用于创建交互式和三维的可视化，这有助于使数据更易于访问和理解。这使得人们更容易理解和解释数据，从而促进更好的决策。同样，生成式人工智能带来的最大变化之一是数据科学的民主化。过去，数据科学是一个非常专业化的领域，需要对统计学和机器学习有深入的理解。然而，生成式人工智能使得不具备较高技术专业知识的人们能够创建和使用数据模型。这使得数据科学领域对更广泛的人群开放。LLMs和生成式人工智能可以在自动化数据科学中发挥关键作用，提供几个优势：
- en: 'Natural Language Interaction: LLMs allow for natural language interaction,
    enabling users to communicate with the model using plain English or other languages.
    This makes it easier for non-technical users to interact with and explore the
    data using everyday language, without requiring expertise in coding or data analysis.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言交互：LLMs允许进行自然语言交互，使用户能够使用普通英语或其他语言与模型进行交流。这使得非技术用户可以使用日常语言与数据进行交互和探索，而无需具备编码或数据分析方面的专业知识。
- en: 'Code Generation: Generative AI can automatically generate code snippets to
    perform specific analysis tasks during EDA. For example, it can generate code
    to retrieve data (for example, SQL) clean data, handle missing values, or create
    visualizations (for example in Python). This feature saves time and reduces the
    need for manual coding.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码生成：生成式人工智能可以自动生成代码片段，以执行探索性数据分析期间的特定分析任务。例如，它可以生成检索数据的代码（例如，SQL）、清理数据、处理缺失值或创建可视化（例如，在Python中）。此功能节省时间，减少了手动编码的需求。
- en: 'Automated Report Generation: LLMs can generate automated reports summarizing
    the key findings of EDA. These reports provide insights into various aspects of
    the dataset such as statistical summary, correlation analysis, feature importance,
    etc., making it easier for users to understand and present their findings.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动报告生成：LLM（大型语言模型）可以生成自动化报告，总结探索性数据分析的关键发现。这些报告提供了关于数据集各个方面的见解，例如统计摘要、相关性分析、特征重要性等，使用户更容易理解和展示他们的发现。
- en: 'Data Exploration and Visualization: Generative AI algorithms can explore large
    datasets comprehensively and generate visualizations that reveal underlying patterns,
    relationships between variables, outliers or anomalies in the data automatically.
    This helps users gain a holistic understanding of the dataset without manually
    creating each visualization.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索和可视化：生成式人工智能算法可以全面地探索大型数据集，并自动生成可视化图表，揭示数据中的基本模式、变量之间的关系、离群值或异常值。这有助于用户全面理解数据集，而无需手动创建每个可视化图表。
- en: Further, we could think that generative AI algorithms should be able to learn
    from user interactions and adapt their recommendations based on individual preferences
    or past behaviors. They improve over time through continuous adaptive learning
    and user feedback, providing more personalized and useful insights during automated
    EDA.Finally, generative AI models can identify errors or anomalies in the data
    during EDA by learning patterns from existing datasets (Intelligent Error Identification).
    They can detect inconsistencies and highlight potential issues quickly and accurately.Overall,
    LLMs and generative AI can enhance automated EDA by simplifying user interaction,
    generating code snippets, identifying errors/ anomalies efficiently, automating
    report generation, facilitating comprehensive data exploration, visualization
    creation, and adapting to user preferences for more effective analysis of large
    and complex datasets.However, while these models offer immense potential to enhance
    research and aiding in literature review processes, they should not be treated
    as infallible sources. As we’ve seen earlier, LLMs work by analogy and struggle
    with reasoning and math. Their strength is creativity, not accuracy, and therefore,
    researchers must exercise critical thinking and ensure that the outputs generated
    by these models are accurate, unbiased, and aligned with rigorous scientific standards.One
    notable example is Microsoft's Fabric, which incorporates a chat interface powered
    by generative AI. This allows users to ask data-related questions using natural
    language and receive instant answers without having to wait in a data request
    queue. By leveraging LLMs like OpenAI models, Fabric enables real-time access
    to valuable insights.Fabric stands out among other analytics products due to its
    comprehensive approach. It addresses various aspects of an organization's analytics
    needs and provides role-specific experiences for different teams involved in the
    analytics process, such as data engineers, warehousing professionals, scientists,
    analysts, and business users.With the integration of Azure OpenAI Service at every
    layer, Fabric harnesses generative AI's power to unlock the full potential of
    data. Features like Copilot in Microsoft Fabric provide conversational language
    experiences, allowing users to create dataflows, generate code or entire functions,
    build machine learning models, visualize results, and even develop custom conversational
    language experiences.Anecdotally, ChatGPT is (and Fabric in extension) often produces
    incorrect SQL queries. This is fine when used by analysts who can check the validity
    of the output, but a total disaster as a self-service analytics tool for non-technical
    business users. Therefore, organizations must ensure that they have reliable data
    pipelines in place and employ data quality management practices while using Fabric
    for analysis.While the possibilities of generative AI in data analytics are promising,
    caution must be exercised. The reliability and accuracy of LLMs should be verified
    using first-principled reasoning and rigorous analysis. While these models have
    shown their potential in ad-hoc analysis, idea generation during research, and
    summarizing complex analyses, they may not always be suitable for self-service
    analytical tools for non-technical users due to the need for validation by domain
    experts.Let’s start to use agents to run code or call other tools to answer questions!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以认为生成式 AI 算法应该能够从用户交互中学习，根据个人偏好或过去行为调整推荐内容。它们通过持续自适应学习和用户反馈而不断优化，为自动化
    EDA 提供更加个性化和有用的见解。最后，生成式 AI 模型可以通过从现有数据集中学习模式（智能错误识别）在 EDA 过程中识别数据中的错误或异常。它们能够快速准确地检测不一致性并突出潜在问题。总的来说，LLM
    和生成式 AI 可以通过简化用户互动、生成代码片段、高效识别错误/异常、自动化报告生成、实现全面数据探索和可视化建设以及适应用户偏好来增强自动化 EDA，以便更有效地分析大型和复杂数据集。然而，虽然这些模型可以极大地增强研究和文献综述过程，但它们不应被视为绝对可靠的来源。如前所述，LLM
    是通过类比工作的，在推理和数学方面会遇到困难。它们的优势在于创造力，而不是准确性，因此研究人员必须运用批判性思维，并确保这些模型生成的输出准确、公正并符合严格的科学标准。其中一个著名的例子是微软的
    Fabric，它包含由生成式 AI 驱动的聊天界面，使用户能够使用自然语言提出数据相关问题，并在无需等待数据请求队列的情况下立即获得答案。通过利用像 OpenAI
    模型这样的 LLM，Fabric 实现了对有价值的见解的实时访问。Fabric 在其他分析产品中脱颖而出，因为它采用全面的方法。它解决了组织在分析过程中各个方面的需求，并为参与分析过程的不同团队（如数据工程师、数据仓库专业人员、科学家、分析人员和业务用户）提供角色专用的体验。借助每个层面的
    Azure OpenAI 服务的集成，Fabric 利用生成式 AI 的能力来发掘数据的全部潜力。类似 Microsoft Fabric 中的 Copilot
    等特性提供了对话式语言体验，使用户能够创建数据流、生成代码或整个函数、构建机器学习模型、可视化结果，甚至开发自定义的对话式语言体验。有趣的是，ChatGPT（以及它的扩展
    Fabric）经常会产生不正确的 SQL 查询。虽然对于可以检查输出有效性的分析人员来说这没问题，但对于非技术业务用户而言，这是一场灾难性的自助式分析工具。因此，在使用
    Fabric 进行分析时，组织必须确保有可靠的数据管道，并采取数据质量管理实践。虽然生成式 AI 在数据分析中的潜力很大，但仍需要谨慎。必须通过第一原理推理和严格的分析来验证
    LLM 的可靠性和准确性。尽管这些模型在临时分析、研究中的思想生成和复杂分析的概括方面表现出了其潜力，但由于需要领域专家的验证，它
- en: Agents can answer data science questions
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理人可以回答数据科学问题
- en: 'As we’ve seen with Jupyter AI (Jupyternaut chat) – and in chapter 6, *Developing
    Software* – there’s a lot of potential to increase efficiency creating and creating
    software with generative AI (code LLMs). This is a good starting point for the
    practical part of this chapter as we look into the use of generative AI in data
    science.We’ve already seen different agents with tools before. For example, the
    LLMMathChain can execute Python to answer math questions as illustrated here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在Jupyter AI（Jupyternaut chat）中所看到的 - 还有第6章的*开发软件* - 通过生成式AI（代码LLMs）来增加创建和编写软件的效率有很大的潜力。这是我们研究数据科学中使用生成式AI的实际部分的很好的起点。我们之前已经看到不同的带有工具的代理人。例如，LLMMathChain可以执行Python来回答数学问题，就像这里所示：
- en: '[PRE0]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'While this is useful to extract information and feed it back, it’s less obvious
    to see how to plug this into a traditional EDA process. Similarly, the CPAL (`CPALChain`)
    and PAL (`PALChain`) chains can answer more complex reasoning questions while
    keeping hallucinations in check, but it’s hard to come up with real-life use cases
    for them.With the `PythonREPLTool` we can create simple visualizations of toy
    data or train with synthetic data, which can be nice for illustration or bootstrapping
    of a project. This is an example from the LangChain documentation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这对于提取信息并将其反馈是有用的，但如何将其插入传统的EDA过程中却不太明显。同样，CPAL(`CPALChain`)和PAL(`PALChain`)链可以回答更复杂的推理问题，同时保持幻觉受控，但很难想出它们的真实用例。通过`PythonREPLTool`，我们可以使用玩具数据创建简单的可视化，或者用合成数据进行训练，这对说明或启动项目可能很好。这是LangChain文档中的一个例子：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Please note that this should be executed with caution since the Python code
    is executed directly on the machine without any safeguards in place. This actually
    works, and creates a dataset, trains a model, and we get a prediction back:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这应谨慎执行，因为Python代码直接在机器上执行而且没有任何防护措施。实际上这是有效的，可以创建数据集，训练模型，然后得到预测结果：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Again, this is very cool, but it’s hard to see how that would scale up without
    more serious engineering similar to what we did in chapter 6, *Developing Software*.LLMs
    and tools can be useful if we want to enrich our data with category or geographic
    information. For example, if our company offers flights from Tokyo, and we want
    to know the distances of our customers from Tokyo, we can use Wolfram Alpha as
    a tool. Here’s a simplistic example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这非常酷，但很难看出如何在没有更严谨的工程的情况下扩展。如果我们想要丰富我们的数据以获取类别或地理信息，LLMs和工具会很有用。例如，如果我们的公司从东京提供航班，而我们想要知道我们的客户距离东京的距离，我们可以使用Wolfram
    Alpha作为一个工具。这是一个简单的例子：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Please make sure you’ve set the OPENAI_API_KEY and WOLFRAM_ALPHA_APPID environment
    variables as discussed in chapter 3, *Getting Started with LangChain*. Here’s
    the output:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您已经设置了OPENAI_API_KEY和WOLFRAM_ALPHA_APPID环境变量，正如在第3章*开始使用LangChain*中所讨论的那样。这是输出结果：
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, a lot of these questions are very simple. However, we can give agents datasets
    to work with and here’s where it can get very powerful when we connect more tools.
    Let’s start with asking and answering questions about structured datasets!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，很多这些问题都非常简单。然而，我们可以给予代理人数据集进行处理，这就是当我们连接更多工具时，它变得非常强大的地方。让我们开始问答关于结构化数据集的问题吧！
- en: Data exploration with LLMs
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLMs进行数据探索
- en: 'Data exploration is a crucial and foundational step in data analysis, allowing
    researchers to gain a comprehensive understanding of their datasets and uncover
    significant insights. With the emergence of LLMs like ChatGPT, researchers can
    harness the power of natural language processing to facilitate data exploration.As
    we’ve mentioned earlier Generative AI models, such as ChatGPT, have the ability
    to understand and generate human-like responses, making them valuable tools for
    enhancing research productivity. Asking our questions in natural language and
    getting responses in digestible pieces and shape can be a great boost to analysis.LLMs
    can assist in exploring not only textual data but also other forms of data such
    as numerical datasets or multimedia content. Researchers can leverage ChatGPT''s
    capabilities to ask questions about statistical trends in numerical datasets or
    even query visualizations for image classification tasks.Let’s load up a dataset
    and work with that. We can quickly get a dataset from scikit-learn:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索是数据分析中至关重要且基础的步骤，使研究人员能够全面了解其数据集并发现重要信息。随着类似ChatGPT这样的LLM的出现，研究人员可以利用自然语言处理的能力促进数据探索。正如我们之前提到的生成式AI模型如ChatGPT具有理解和生成类人回答的能力，使它们成为增强研究生产力的有价值的工具。以自然语言提出问题并获得易消化的响应可以极大地促进分析。LLM不仅可用于探索文本数据，还可用于探索其他形式的数据，如数字数据集或多媒体内容。研究人员可以利用ChatGPT的能力，询问数值数据集中的统计趋势或查询图像分类任务的可视化。让我们加载一个数据集并进行处理。我们可以从scikit-learn快速获取一个数据集：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The Iris dataset is well-known – it’s a toy dataset, but it will help us illustrate
    the capabilities of using generative AI for data exploration. We’ll use the DataFrame
    in the following. We can create a Pandas dataframe agent now and we’ll see how
    easy it is to get simple stuff done!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Iris数据集是众所周知的-它是玩具数据集，但它将帮助我们说明使用生成式AI进行数据探索的能力。我们将在接下来使用DataFrame。我们现在可以创建一个Pandas
    dataframe代理，看看如何轻松地完成一些简单的事情！
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I’ve put the instruction for the model to say it doesn’t know when in doubt
    and thinking step by step, both to reduce hallucinations. Now we can query our
    agent against the DataFrame:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经为模型制定了当怀疑时告诉它自己不知道以及逐步思考的指示，这两者都可以减少产生幻觉的可能性。现在我们可以针对DataFrame查询我们的代理：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get the answer `''This dataset is about the measurements of some type of
    flower.`'' which is correct. Let’s show how to get a visualization:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了答案 "这个数据集是关于某种类型的花的测量"，是正确的。让我们展示如何获得一个可视化：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s not perfect, but we are getting a nice-looking plot:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它并不完美，但我们得到了一个好看的图表：
- en: '![Figure 7.5: Iris dataset barplots.](img/file52.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5：Iris数据集的条形图。](img/file52.png)'
- en: 'Figure 7.5: Iris dataset barplots.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.5: Iris数据集的条形图。'
- en: 'We can also ask to see the distributions of the columns visually, which will
    give us this neat plot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以请求以可视化方式查看列的分布，从而获得这个整洁的图表：
- en: '![Figure 7.6: Iris dataset boxplots.](img/file53.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6：Iris数据集箱线图。](img/file53.png)'
- en: 'Figure 7.6: Iris dataset boxplots.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.6: Iris数据集箱线图。'
- en: 'We can request the plot to use other plotting backends such as seaborn, however,
    please note that these have to be installed. We can also ask more questions about
    the dataset like which row has the biggest difference between petal length and
    petal width. We get the answer with the intermediate steps as follows (shortened):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以请求图表使用其他绘图后端，例如seaborn，但请注意，这些必须安装。我们还可以询问关于数据集的更多问题，比如哪一行在花瓣长度和花瓣宽度之间有最大差异。我们得到了具有中间步骤的答案（缩短后）。
- en: '[PRE9]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I think that’s worth a pat on the back, LLM!Next steps could be adding more
    instructions to the prompt about plotting such about the sizes of plots. It’s
    a bit harder to implement the same plotting logic in a streamlit app, because
    we need to use the plotting functionality in corresponding streamlit functions,
    for example, `st.bar_chart()`, however, this can be done as well. You can find
    explanations for this on the Streamlit blog (“Building a Streamlit and scikit-learn
    app with ChatGPT”).What about statistical tests?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这值得称赞，LLM！下一步可能是给提示添加更多关于绘图的指示，例如绘图大小等。在Streamlit应用程序中实现相同的绘图逻辑有点困难，因为我们需要使用适当的Streamlit函数的绘图功能，例如`st.bar_chart()`，但是这也可以完成。您可以在Streamlit博客上找到有关此内容的解释（“使用ChatGPT构建Streamlit和scikit-learn应用程序”）。那么统计测试呢？
- en: '[PRE10]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We get this response:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了这个回答：
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '''The p-value of 6.639808432803654e-32 indicates that the two variables come
    from different distributions.''That’s check for statistical test! That’s cool.
    We can ask fairly complex questions about the dataset with simple prompts in plain
    English. There’s also the pandas-ai library, which uses LangChain under the hood
    and provides similar functionality. Here’s an example from the documentation with
    an example dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '''6.639808432803654e-32的p值表明两个变量来自不同的分布。''这是统计检验！很酷。我们可以用简单的提示用普通英语询问关于数据集的相当复杂的问题。还有pandas-ai库，它在内部使用LangChain并提供类似的功能。以下是文档中的一个例子，案例数据集：'
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will give us the requested result similarly to before when we were using
    LangChain directly. Please note that pandas-ai is not part of the setup for the
    book, so you’ll have to install it separately if you want to use it.For data in
    SQL-databases, we can connect with a `SQLDatabaseChain`. The LangChain documentation
    shows this example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们直接使用LangChain时，这将给我们提供所请求的结果。请注意，pandas-ai并不是本书的设置的一部分，所以如果你想使用它，你需要单独安装它。对于SQL数据库中的数据，我们可以使用`SQLDatabaseChain`进行连接。LangChain的文档展示了这个例子：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are connecting to a database first. Then we can ask questions about the data
    in natural language. This can also be quite powerful. An LLM will create the queries
    for us. I would expect this to be particularly useful when we don’t know about
    the schema of the database.The `SQLDatabaseChain` can also check queries and autocorrect
    them if the `use_query_checker` option is set.Let’s summarize!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先连接到数据库。然后我们可以用自然语言提出关于数据的问题。这也可以非常强大。LLM将为我们创建查询。我期望当我们不了解数据库模式时，这将特别有用。`SQLDatabaseChain`还可以在`use_query_checker`选项设置时检查查询并自动更正它们。让我们做个总结！
- en: Summary
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve explored the state-of-the-art in automated data analysis
    and data science. There are quite a few areas, where LLMs can benefit data science,
    mostly as coding assistants or in data exploration.We’ve started off with an overview
    over frameworks that cover each step in the data science process such as AutoML
    methods, and we’ve discussed how LLMs can help us further increasing productivity
    and making data science and data analysis more accessible, both to stakeholders
    and to developers or users. We’ve then investigated how code generation and tools,
    similar to code LLMs *Chapter 6*, *Developing Software*, can help in data science
    tasks by creating functions or models that we can query, or how we can enrich
    data using LLMs or third-party tools like Wolfram Alpha.We then had a look at
    using LLMs in data exploration. In *Chapter 4*, *Question Answering*, we looked
    at ingesting large amounts of textual data for analysis. In this chapter, we focused
    on exploratory analysis of structured datasets in SQL or tabular form. In conclusion,
    AI technology has the potential to revolutionize the way we can analyze data,
    and ChatGPT plugins or Microsoft Fabric are examples of this. However, at the
    current state-of-affairs, AI can’t replace data scientists, only help enable them.Let’s
    see if you remember some of the key takeaways from this chapter!
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了自动化数据分析和数据科学的最新技术。有很多领域，LLM可以使数据科学受益，主要是作为编码助手或数据探索。我们从概述了覆盖数据科学流程中每个步骤的框架开始，比如AutoML方法，然后讨论了LLM如何帮助我们进一步提高生产力，使数据科学和数据分析更加容易访问，无论是对利益相关者还是对开发人员或用户。我们随后研究了代码生成和类似代码LLM
    *第六章* *软件开发*中的工具如何在数据科学任务中帮助我们通过创建我们可以查询的函数或模型，或者如何利用LLM或第三方工具如沃尔夫拉姆阿尔法来丰富数据。然后，我们关注了在数据探索中使用LLM。在*第四章*
    *问答*中，我们研究了摄取大量文本数据以进行分析。在本章中，我们聚焦于SQL或表格形式的结构化数据集的探索性分析。总之，人工智能技术有潜力彻底改变我们分析数据的方式，ChatGPT插件或微软Fabric就是例子。然而，在当前的状况下，人工智能不能取代数据科学家，只能帮助他们。让我们看看你是否记住了本章的一些关键要点！
- en: Questions
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请看看你是否能够凭记忆回答这些问题。如果对任何问题不确定，我建议您回到本章的相应部分查看：
- en: What’s the difference between data science and data analysis?
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学和数据分析之间有什么区别？
- en: What steps are involved in data science?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据科学涉及哪些步骤？
- en: Why would we want to automate data science/analysis?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们想要自动化数据科学/分析？
- en: What frameworks exist for automating data science tasks and what can they do?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存在用于自动化数据科学任务的框架以及它们能做什么？
- en: How can generative AI help data scientists?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成式人工智能如何帮助数据科学家？
- en: What kind of agents and tools can we use to answer simple questions?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用什么样的代理和工具来回答简单的问题？
- en: How can we get an LLM to work with data?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何让LLM处理数据？
