["```py\nimport tensorflow as tf\nmirrored_strategy = tf.distribute.MirroredStrategy()\n# or \n# mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:3\"])\n# if you want to use only specific devices \nwith mirrored_strategy.scope():\n    # define your model \n    # …\nmodel.compile(... )\nmodel.fit(... ) \n```", "```py\n    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    ```", "```py\nif tf.config.list_physical_devices('GPU'):\n    strategy = tf.distribute.MirroredStrategy()\nelse:  # Use the Default Strategy\n    strategy = tf.distribute.get_strategy()\n```", "```py\ncpu = torch.device(cpu')\ncuda = torch.device('cuda')     # Default CUDA device\ncuda0 = torch.device('cuda:0')\nx = torch.tensor([1., 2.], device=cuda0)\n# x.device is device(type='cuda', index=0)\ny = torch.tensor([1., 2.]).cuda()\n# y.device is device(type='cuda', index=0)\n# transfers a tensor from CPU to GPU 1\na = torch.tensor([1., 2.]).cuda()\n# a.device are device(type='cuda', index=1)\n# to function of a Tensor instance can be used to move the tensor to different devices\nb = torch.tensor([1., 2.]).to(device=cuda)\n# b.device are device(type='cuda', index=1)\n```", "```py\n# Train using CPU\nTrainer()\n# Specify how many GPUs to use\nTrainer(gpus=k)\n# Specify which GPUs to use\nTrainer(gpus=[0, 1])\n# To use all available GPUs put -1 or '-1'\nTrainer(gpus=-1)\n```", "```py\nwith tf.device('GPU:0'): \n    layer1 = layers.Dense(16, input_dim=8) \nwith tf.device('GPU:1'): \n    layer2 = layers.Dense(4, input_dim=16)\n```", "```py\ntf_config = {\n    'cluster': {\n        'worker': ['localhost:12345', 'localhost:23456']\n    },\n    'task': {'type': 'worker', 'index': 0}\n}\njs_tf = json.dumps(tf_config)\nwith open(\"tf_config.json\", \"w\") as outfile:\n    outfile.write(js_tf)\n```", "```py\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\nwith strategy.scope():\n    model = … \n```", "```py\n# On the first node\nTF_CONFIG='{\"cluster\": {\"worker\": ['localhost:12345', 'localhost:23456']}, \"task\": {\"index\": 0, \"type\": \"worker\"}}' python training.py\n# On the second node\nTF_CONFIG='{\"cluster\": {\"worker\": ['localhost:12345', 'localhost:23456']}, \"task\": {\"index\": 1, \"type\": \"worker\"}}' python training.py\n```", "```py\n# train on 8 GPUs (same machine)\ntrainer = Trainer(gpus=8, accelerator='ddp')\n# train on 32 GPUs (4 nodes)\ntrainer = Trainer(gpus=8, accelerator='ddp', num_nodes=4)\n```", "```py\npython -m torch.distributed.run\n    --nnodes=2 # number of nodes you'd like to run with\n    --master_addr <MASTER_ADDR>\n    --master_port <MASTER_PORT>\n    --node_rank <NODE_RANK>\n    train.py (--arg1 ... train script args...)\n```", "```py\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n# Initializes SageMaker session\nsagemaker_session = sagemaker.Session()\nbucket = 's3://dataset/'\ntf_estimator = TensorFlow(entry_point='training_script.py', \n              source_dir='.',\n              role=sagemaker.get_execution_role(),\n              instance_count=1, \n              instance_type='ml.c5.18xlarge',\n              framework_version=tf_version, \n              py_version='py3',\n              script_mode=True,\n              hyperparameters={'epochs': 30} )\n```", "```py\ntf_estimator.fit({'training': 's3://bucket/training',\n                  'validation': 's3://bucket/validation'})   \n```", "```py\nimport sagemaker\nfrom sagemaker.pytorch import PyTorch\n# Initializes SageMaker session\nsagemaker_session = sagemaker.Session()\nbucket = 's3://dataset/'\npytorch_estimator = PyTorch(\n                      entry_point='train.py',\n                      source_dir='.',\n                      role=sagemaker.get_execution_role(),\n                      framework_version='1.10.0',\n                      train_instance_count=1,\n                      train_instance_type='ml.c5.18xlarge',\n                      hyperparameters={'epochs': 6})\n…\npytorch_estimator.fit({\n                        'training': bucket+'/training',\n                        'validation': bucket+'/validation'})   \n```", "```py\ndistribution = {\"smdistributed\": {\"dataparallel\": { \"enabled\": True}} \n```", "```py\ntf_estimator = TensorFlow(\n                 entry_point='training_script.py', \n                 source_dir='.',\n                 role=sagemaker.get_execution_role(),\n                 instance_count=4, \n                 instance_type='ml.c5.18xlarge',\n                 framework_version=tf_version, \n                 py_version='py3',\n                 script_mode=True,\n                 hyperparameters={'epochs': 30}\n                 distributions={'smdistributed':\n                 \"dataparallel\": {\"enabled\": True}})\n```", "```py\nfrom sagemaker_tensorflow import PipeModeDataset\nds = PipeModeDataset(channel='training', record_format='TFRecord') \n```", "```py\ndistribution={\"mpi\": {\"enabled\":True, \n                        \"processes_per_host\":2 }}\n```", "```py\nmpirun -np 8 -H server1:2,server2:2,server3:2,server4:2 … (other parameters) python train.py  \n```", "```py\nscp -i <your_pem_key_path> ubuntu@<IPv4_Public_IP>:/home/ubuntu/.ssh/ \n```", "```py\neval 'ssh-agent'\nssh-add <your_pem_key>\n```", "```py\nimport tensorflow as tf\nimport horovod.tensorflow.keras as hvd\n# Initialize Horovod\nhvd.init()\n```", "```py\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n```", "```py\ndataset = np.array_split(dataset, hvd.size())[hvd.rank()]\n```", "```py\nmodel = …\n```", "```py\nopt = tf.optimizers.Adam(0.001 * hvd.size())\nopt = hvd.DistributedOptimizer(opt)\n```", "```py\nmodel.compile(loss=tf.losses.SparseCategoricalCrossentropy(),\n              optimizer=opt,\n              metrics=['accuracy'],\n              experimental_run_tf_function=False)\n```", "```py\ncallbacks=[\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0)\n]\n```", "```py\n# Save checkpoints only on the instance with rank 0 to prevent other workers from corrupting them.\nIf hvd.rank()==0:\n    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n```", "```py\nif hvd.rank()==0:\n    ver = 1\nelse:\n    ver = 0\nmodel.fit(dataset,\n          steps_per_epoch=hvd.size(),\n          callbacks=callbacks,\n          epochs=num_epochs,\n          verbose=ver)\n```", "```py\nimport torch\nimport horovod.torch as hvd\n# Initialize Horovod\nhvd.init()\n```", "```py\ntorch.cuda.set_device(hvd.local_rank())\n```", "```py\n# Define dataset...\ntrain_dataset = ...\n# Partition dataset among workers using DistributedSampler\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)\n# Build model...\nmodel = ...\nmodel.cuda()\noptimizer = optim.SGD(model.parameters())\n# Add Horovod Distributed Optimizer\noptimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n# Broadcast parameters from rank 0 to all other processes.\nhvd.broadcast_parameters(model.state_dict(), root_rank=0)\n```", "```py\nfor epoch in range(num_ephos):\n   for batch_idx, (data, target) in enumerate(train_loader):\n       optimizer.zero_grad()\n       output = model(data)\n       loss = F.nll_loss(output, target)\n       loss.backward()\n       optimizer.step()\n```", "```py\nhorovodrun -np 4 -H localhost:4 python train.py\nmpirun -np 4 python train.py\n```", "```py\nhorovodrun --gloo -np 4 -H localhost:4 python train.py\n```", "```py\nhorovodrun -np 4 -H server1:1,server2:1,server3:1,server4:1 python train.py \n```", "```py\nmpirun -np 4 -H server1:1,server2:1,server3:1,server4:1 python train.py\n```", "```py\ncluster_name: BookDL\nmax_workers: 5\nupscaling_speed: 1.0\n```", "```py\nprovider:\n    type: aws\n    region: us-east-1\n    availability_zone: us-east-1c, us-east-1b, us-east-1a\n    cache_stopped_nodes: True \n    ssh_user: ubuntu\n    ssh_private_key: /Users/BookDL/.ssh/BookDL.pem\n```", "```py\navailable_node_types:\n    ray.head.default:\n        node_config:\n            KeyName:\"BookDL.pem\"\n```", "```py\n            SecurityGroupIds:\n                - sg-XXXXX\n                - sg-XXXXX\n            SubnetIds: [subnet-XXXXX]\n```", "```py\n            InstanceType: m5.8xlarge\n            ImageId: ami-09ac68f361e5f4a13\n```", "```py\n            BlockDeviceMappings:\n                  - DeviceName: /dev/sda1\n                    Ebs:\n                    VolumeSize: 580\n```", "```py\n            TagSpecifications:\n                - ResourceType:\"instance\"\n                  Tags:\n                      - Key:\"Developer\"\n                        Value:\"BookDL\"\n```", "```py\n            IamInstanceProfile:\n                Arn:arn:aws:iam::XXXXX\n```", "```py\n    ray.worker.default:\n            min_workers: 2\n            max_workers: 4\n```", "```py\n        node_config:\n            KeyName: \"BookDL.pem\"\n            SecurityGroupIds:\n                - sg-XXXXX\n                - sg-XXXXX\n            SubnetIds: [subnet-XXXXX]\n            InstanceType: p2.8xlarge\n            ImageId: ami-09ac68f361e5f4a13\n            TagSpecifications:\n                - ResourceType: \"instance\"\n                  Tags:\n                      - Key: \"Developer\"\n                        Value: \"BookDL\"\n            IamInstanceProfile:\n                Arn: arn:aws:iam::XXXXX\n            BlockDeviceMappings:\n              - DeviceName: /dev/sda1\n                Ebs:\n                  VolumeSize: 120\n```", "```py\nsetup_commands:\n    - (stat $HOME/anaconda3/envs/tensorflow2_p38/ &> /dev/null && echo 'export PATH=\"$HOME/anaconda3/envs/tensorflow2_p38/bin:$PATH\"' >> ~/.bashrc) || true\n    - source activate tensorflow2_p38 && pip install --upgrade pip\n    - pip install awscli\n    - pip install Cython\n    - pip install -U ray\n    - pip install -U ray[rllib] ray[tune] ray\n    - pip install mlflow\n    - pip install dvc\n```", "```py\nhead_start_ray_commands:\n    - ray stop\n    - source activate tensorflow2_p38 && ray stop\n    - ulimit -n 65536; source activate tensorflow2_p38 && ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\nworker_start_ray_commands:      \n    - ray stop\n    - source activate tensorflow2_p38 && ray stop\n    - ulimit -n 65536; source activate tensorflow2_p38 && ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n```", "```py\nray start --head --redis-port=6379  \n```", "```py\nray start --address=<redis server ip address>\n```", "```py\nray up your_cluster_setting_file.yaml\n```", "```py\nray attach your_cluster_setting_file.yaml\n```", "```py\nray down your_cluster_setting_file.yaml\n```", "```py\ndef train_func_distributed():\n    per_worker_batch_size = 64\n    tf_config = json.loads(os.environ['TF_CONFIG'])\n    num_workers = len(tf_config['cluster']['worker'])\n    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n    global_batch_size = per_worker_batch_size * num_workers\n    multi_worker_dataset = dataset(global_batch_size)\n    with strategy.scope():\n        multi_worker_model = build_and_compile_your_model()\n    multi_worker_model.fit(multi_worker_dataset, epochs=20, steps_per_epoch=50)\n```", "```py\nimport ray\nfrom ray.train import Trainer\nray.init()\ntrainer = Trainer(backend=\"tensorflow\", num_workers=4, use_gpu=True)\ntrainer.start()\ntrainer.run(train_func_distributed)\ntrainer.shutdown()\n```", "```py\nFROM tensorflow/tensorflow:latest-gpu-jupyter\nRUN pip install minio –upgrade\nRUN pip install –upgrade pip \nRUN pip install pandas –upgrade \n… \nRUN mkdir -p /opt/kubeflow\nCOPY train.py /opt/kubeflow\nENTRYPOINT [\"python\", \"/opt/kubeflow/train.py\"]\n```", "```py\ndocker build -t kubeflow/tf_example_job:1.0\ndocker push kubeflow/tf_example_job:1.0\n```", "```py\napiVersion: \"kubeflow.org/v1\" \nkind: \"TFJob\"\nmetadata:\n    name: \"tf_example_job\"\nspec:\n    tfReplicaSpecs:\n        Worker:\n            replicas: 1\n        restartPolicy: Never\n        template:\n            specs:\n                containers:\n                    - name: tensorflow \n                      image: kubeflow/tf_example_job:1.0\n```", "```py\nkubectl apply -f tf_example_job.yaml\n```", "```py\nkubectl describe tfjob tf_example_job \n```", "```py\napiVersion: \"kubeflow.org/v1\"\nkind: \"TFJob\"\nmetadata:\n    name: \"tf_example_job_dist\"\nspec:\n    cleanPodPolicy: None\n    tfReplicaSpecs:\n        Worker:\n            replicas: 4\n            restartPolicy: Never\n            template:\n                specs:\n                    containers:\n                        - name: tensorflow \n                          image: kubeflow/tf_example_job:1.1\n```", "```py\nkubectl apply -f tf_example_job_dist.yaml\n```", "```py\napiVersion: \"kubeflow.org/v1 \nkind: \"PyTorchJob\"\nmetadata:\n    name: \"pt_example_job_dist\"\nspec:\n    pytorchReplicaSpecs:\n        Master:\n            replicas: 1\n            restartPolicy: Never\n            template:\n                specs:\n                    containers:\n                        - name: pytorch \n                          image: kubeflow/pt_example_job:1.0\n        Worker:\n            replicas: 5\n            restartPolicy: OnFailure\n            template:\n                specs:\n                    containers:\n                        - name: pytorch \n                          image: kubeflow/pt_example_job:1.0\n```"]