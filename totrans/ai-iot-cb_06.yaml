- en: Computer Vision
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision has come a long way in recent years. Unlike many other forms
    of machine learning that require complex analysis, the vast majority of computer
    vision problems come from simple RGB cameras. Machine learning frameworks such
    as Keras and OpenCV have standard and high-accuracy neural networks built-in.
    A few years ago, implementing a facial recognition neural net, for example, was
    complex and challenging to set up in Python, let alone on a high-speed device
    using C++ or CUDA. Today, this process is easier and more accessible than ever
    before. In this chapter, we are going to talk about implementing computer vision
    in the cloud, as well as on Edge devices such as NVIDIA Jetson Nano.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Connecting cameras through OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Microsoft's custom vision to train and label your images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting faces with deep neural nets and Caffe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects using YOLO on Raspberry Pi 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting objects using GPUs on NVIDIA Jetson Nano
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training vision with PyTorch on GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting cameras through OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Connecting a camera through OpenCV is fairly straightforward. The issue is often
    in installing OpenCV. OpenCV installs easily on a desktop computer, but on more
    constrained devices, it may require extra work. In a Raspberry Pi 3, for example,
    you may need to enable swap space. This allows the system to use the SD card as
    a temporary memory store. Depending on the device, there are various instructions
    available online on how to get OpenCV onto a challenging device.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will connect OpenCV to a camera application on the Raspberry
    Pi Zero, but if you do not have the hardware, you can run the code on a PC. In
    future recipes, we will assume knowledge of this and breeze by the explanation of
    what is going on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a coding perspective, using OpenCV abstracts the hardware away. It does
    not matter if you are using a $5 Raspberry Pi Zero or a $120 LattePanda; the only
    things required for this recipe are a computer and a camera. Most laptops have
    built-in cameras, but for a desktop computer or a **single board computer** (**SBC**),
    such as a Raspberry Pi or LattePanda, you will need a USB web camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you will need to install OpenCV. As mentioned earlier, there are ways
    of getting OpenCV on constrained devices. These are all unique to the device in
    question. In our case, we will put a PiCam module on a Raspberry Pi Zero. The
    following is an image of a PiCam module for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1692aaf3-8b87-40d9-a062-d36d1501715c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To add the PiCam to the Pi Zero, you simply pull the black tabs from the connector,
    insert the PiCam module, and then push in the tab, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cec335f-5a4a-4e9a-8c6f-c78bb1146298.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From here you need to enable the cameras in your Raspberry Pi. You will need
    to plug a monitor, keyboard, and mouse into your Raspberry Pi. Then, make sure
    that your system is up to date by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you will enable the camera by going into the **Rasp Config** menu. In
    your terminal, type in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, select **Camera** and then enable it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62237be4-5d76-4068-8374-a608f7be400d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There are three different libraries that we can `pip` install: `opencv-contrib-python`
    for all of the OpenCV extras, `opencv-python` for a faster, but shorter, list
    of features, and finally `opencv-cython` for a faster Python experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this book, I would recommend performing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Check whether the camera is available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Capture, save, and show frames from the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Release the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, first, we import OpenCV. We then select the first camera it
    finds (`camera(0)`). If we were looking for the second camera it finds, then we
    would increment the camera number (`camera(1)`). Next, we check whether the camera
    is available. There can be several reasons why a camera might not be available.
    First, it could be opened by something else. You could, for example, open the
    camera in a different application to see whether it is working and this would
    prevent the Python application from detecting and connecting to the camera. Another
    common issue is that releasing the camera step in the code does not get executed
    and the camera needs to be reset. Next, we capture the video frames and present
    them on the screen until someone presses the *Q* key. Finally, after someone has
    exited the application, we release the camera and close the open window.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCV has many tools for writing text to the screen or drawing bounding boxes
    around an identified object. In addition, it has the ability to downsample or
    change an RGB image to black and white. Filtering and downsampling are techniques
    that machine learning engineers perform on constrained devices that allow them
    to operate efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Using Microsoft's custom vision to train and label your images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft's cognitive services offer a one-stop shop for everything you need
    for training images and deploying models. First, it provides a way of uploading
    images. Then, it has a UI for drawing bounding boxes around images, and finally,
    it allows you to deploy and expose an API endpoint you can use for computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use Microsoft's custom vision service, you will require an Azure subscription.
    Then you will need to spin up a new custom vision project. There is a free tier
    for testing out small models and a paid tier for larger models and serving models
    at scale. After creating the custom vision project in the Azure portal, you will
    see two new projects in the resource group. The first will be for training, and
    the second will have a `-prediction` label appended to the name, which will be
    used for the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Then you will require images of what you are classifying. In our case, we are
    identifying beverages in an environment with lead and carcinogen exposure. If
    you have completed the previous recipe, you will have a camera capturing images
    at 1 second intervals. To make an object detection model in cognitive services,
    you will need at least 30 images of each thing you are trying to classify. More
    images will improve accuracy. To get good accuracy, you should vary the light,
    background, angle, size, and type, and use individual and grouped images of the
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'You also need to install Microsoft''s cognitive services computer vision Python
    package. To do that, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the Azure portal where you created your custom vision project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate your browser to [https://customvision.ai](https://customvision.ai)
    and log in with your Azure credential. This will take you to the Projects page.
    There are some sample projects, but you will want to create your own. Click on
    the New project tile. Then, fill out the Create new project wizard. For this recipe,
    we are taking pictures of food and drink items so that we can use them in a workplace
    safety computer vision project. This type of computer vision could be used in
    an electronics shop, where people are eating in an environment with contaminants
    such as lead or carcinogens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the main page of the project, you will see a Tags button. Click on the Untagged
    option (as shown in the following screenshot) and you will see all of the images
    that you uploaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3e4a3009-3598-4155-a343-b3f656d4e4b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on the image and use the tools to draw a bounding box around the images.
    From here, you can draw bounding boxes around your images and tag them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/50909e0a-021e-47f5-a662-f6f14682bfb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, click on the green Train button to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a7df02d4-bb8c-415d-bfae-e6b7e1769fe7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After you click on Train, it will start to train a model. This could take quite
    some time. Once it completes, click on the iteration and then click on the Prediction URL
    button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/be747990-8bf6-4314-8d4a-4459ffb7a0d6.png)'
  prefs: []
  type: TYPE_IMG
- en: This will give you a window with everything you require in order to send an
    image to the object detection service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for testing the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Cognitive services use the tagged images to create a model that finds those
    images within a larger picture. As the number of images increases, so does the
    accuracy. There will be a point where, however, when the accuracy reaches convergence or,
    in layman''s terms, does not improve. To find this convergence, add and tag more
    images until the iteration metrics of Precision, Recall, and mAP do not improve.
    The custom vision dashboard in the following screenshot shows the three factors
    that we measure to show the model''s accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b9e86f1-826e-4db9-9f47-007cce8173d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Detecting faces with deep neural nets and Caffe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One advantage of using OpenCV's implementation of visual neural networks is
    that they are available on different platforms. For the sake of clarity and brevity,
    we are using Python on an environment that has Python installed. However, the
    same results could be used with OpenCV's C++ implementation on an ARM-CortexM3
    or OpenCV's Java implementation on an Android system. In this recipe, we are going
    to use a face detection neural network that OpenCV implemented based on the **Caffe** machine
    learning framework. The output of this recipe will be a window on the PC that
    has the image with bounding boxes around the face.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run this recipe, you will need a web camera attached to your device. You
    will need to install OpenCV, NumPy, and Imutils, if you have already not done
    so. Installing OpenCV can be challenging on very constrained devices. There are
    several ways in which you can attempt to do this if you are unable to install
    it natively on a device. Many devices with extra storage space will allow you
    to use the disk as swap space for the memory. If the device in question supports
    dockerization, then you can compile on a computer and run the container on the
    device. This recipe uses a pretrained model that can be found in the GitHub companion
    to this book in the `Ch6` directory.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Import a neural network from the pretrained model in the `Ch6` GitHub repo
    and then initialize OpenCV''s camera operator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a function that downsamples the image and transforms it into a predefined
    shape for our neural network and then perform the inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw the bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the image with bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a never-ending loop that reads an image from the camera, performs the
    inference and gets the overlay, and then outputs the image to the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, clean up and destroy all the windows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After importing our libraries, we import the pretrained face detection model
    into our `net` variable. We then open the first camera (`0`). Then we use `FacNN`
    to predict the image and draw the bounding box. Then we shrink the image to an
    appropriate dimension. We then use `imutils` to resize our large image from the
    camera. We then set the image in the network and get the face detections. Next,
    we get the face detections and retrieve the confidence that the object it finds
    is really a face. In our case, we are using a `.8` or `80%` threshold. We also
    filter out faces with low confidence. We then draw bounding boxes around the faces
    and put confidence text on the boxes. We then return those images to our main
    `while True` loop and display them on the screen. We also wait for a *Q* key to
    quit. Finally, we release the camera and destroy the UI window.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects using YOLO on Raspberry Pi 4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**YOLO** stands for **you only look once**. It is a fast image classification
    library that is optimized for GPU processing. YOLO tends to outperform all other
    computer vision libraries. In this recipe, we are going to implement a computer
    vision object detection using OpenCV''s implementation of YOLO. In this example,
    we are going to use a pretrained model that has 40 common objects already trained.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get ready, you will need to clone the GitHub repo for this book. In the
    `Ch6` section, you will find the `yolov3.cfg` config file and the `yolov3.txt` text
    file of the classes. Next, you will need to download the large `weights` file.
    To do this, you will need to open a command prompt and `cd` into the `Ch6` directory
    and then download the `weights` file with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, you will need to install OpenCV and NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define our output layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw bounding boxes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Process the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the camera:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, clean up and destroy all the windows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: YOLO looks at the image once and divides the image up into a grid. It then uses
    bounding boxes to divide up the grid. YOLO first determines whether the bounding
    box has an object and then determines the class of object. By incorporating a
    prefilter on the algorithm, this screens out parts of the images that are not
    objects and YOLO is then able to dramatically speed up its search.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, after importing our libraries, we set our variables. First,
    we open `yolov3.txt`. This file contains the classes of the pretrained library
    we will be using. Next, we create a random `color` array to denote our different
    objects as different colors. Then we import our libraries and set our camera to
    the first camera on the computer. We then set thresholds and scale images so that
    the image sizes are something that would be recognizable to the classifier. If
    we, for example, add a high-resolution image, the classifier might only recognize
    very small things as objects while ignoring larger things. This is because YOLO
    tries to determine bounding boxes around objects to filter out things that are
    objects. Next, we define our output layers and then create bounding boxes based
    on our confidence threshold. We then use these bounding boxes to draw rectangles
    around the images and passed that image in addition to the labeled text back to
    our image processor. Our main image processing loop calls the `Yolo` function.
    Finally, before cleaning up our resources, we run through the main loop that performs
    YOLO analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting objects using GPUs on NVIDIA Jetson Nano
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NVIDIA makes a series of GPU-enabled SBCs. Some of these, such as the TX2, are
    used on drones because they are lightweight and can pack a lot of power under
    their GPU-enabled systems. GPUs, along with **Tensor Processing Units** (**TPUs**),
    are able to deliver multiple times the computer vision capabilities compared with
    standard CPUs. In this recipe, we will use NVIDIA Jetson Nano, which is their
    least expensive development board at $99\. The Jetson has an ecosystem of libraries
    that only work on their products.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you will need an NVIDIA Jetson. Then you will need to install the operating
    system. To do this, you will need to flash a micro USB with NVIDIA's Jetpack image.
    The Jetpack image consists of a base Ubuntu image with many of the dev tools you
    will need in order to get going. Once you have your OS image, put it into the
    Jetson with a monitor, keyboard, mouse, and network-attached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then you will update the OS, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, you will need to install the extra software to run the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have done that, you will need to download the starter project from
    Jetson:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you will make and navigate to the `build` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'From here we will make, install, and link the code from the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After running `make`, you will get a dialog in your terminal offering you the
    ability to download some different pretrained models and also PyTorch so that
    you can train your own models. Use the wizard to first select the models you want
    to download:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6bb20ab8-2bf3-42d0-b4f3-905e788d4b64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The tool will download all the models you selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3cc4a518-9752-4c81-8dbf-37ac48dd6b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: For this recipe, you can keep the default models. After selecting OK, it will
    ask you to install PyTorch so that you can train your own models. Select PyTorch
    and then select OK.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the Jetson libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the camera display loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we added the libraries and then we cloned the Jetson inference
    repository. We then ran a series of make and linker tools to get the install working
    correctly. During this process, we downloaded a large set of pretrained models.
    We then started writing our code. Because the Jetson is limited in terms of its
    capabilities and memory, installing a full-featured IDE can be wasteful. One workaround
    for this is to use an IDE that supports SSH, such as Visual Studio Code, and remoting
    into the box via the IDE. You can then work with the device without tying up resources
    on the Jetson Nano.
  prefs: []
  type: TYPE_NORMAL
- en: To build out this project, first, we import the Jetson inference and `utils` libraries.
    In the previous recipes, we did a lot of the low-level work ourselves as far as
    using OpenCV to get the camera and then used other libraries to manipulate the
    images and draw bounding boxes. With Jetson's libraries, the vast majority of
    that code is handled for you. After we imported the libraries, we imported the
    models we downloaded earlier and set a threshold. Then we entered it in the camera
    dimensions and set the camera to `/dev/video0`. Next, we set our visual display.
    Finally, we grabbed the camera image, ran the detection algorithm, and then output
    that camera image to the screen with the bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, NVIDIA has an ecosystem for their products. They have
    helpful containers, models, and tutorials to work efficiently with their hardware.
    To aid you, they have a product website that gets you started with training models
    and building out containerized notebooks. They have dozens of prebuilt containers
    for different libraries, including PyTorch and TensorFlow to name a few. They
    also have dozens of pretrained models using everything from pose detection to
    specific industry models. They even have their own cloud where you can train your
    models if you wish. You can, however, run locally as well. Their website is [https://ngc.nvidia.com/](https://ngc.nvidia.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Training vision with PyTorch on GPUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we implemented an object classifier using GPU and an
    NVIDIA Jetson Nano. There are other types of GPU-enabled devices. These range
    from the NVIDIA TX2, which can be put on a drone to do real-time analysis of pipelines,
    to industrial PCs running GPUs and using computer vision to perform analyses on
    workplace safety. In this recipe, we are going to train and add to an existing
    image classification model by adding our own images to it.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges that the IoT faces include **over-the-air** (**OTA**) updates and
    fleet management. IoT Edge is a conceptual framework that solves this. In OTA
    updates, Docker containers are used as an update mechanism. The underlying systems
    can be updated without having to worry about complete device failure. If an update
    does not work, the system can be rolled back because container failures do not
    affect the main OS and the Docker daemon can perform the update and roll back.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to use NVIDIA Docker containers to build our models.
    Later, we will use that model for inference.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To get ready, we are going to use Docker with an application version greater
    than 19\. In Docker 19, the `--gpu` tag was added, allowing you to use Docker
    to access the GPU natively. Depending on your GPUs, you may need to install additional
    drivers to make the GPUs work on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also going to be using **Visual Studio Code** (**VS Code**), which,
    with the help of a plugin, allows you to write code directly in NVIDIA''s GPU
    PyTorch container. You will need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and install VS Code and then use the extension manager to add the **Remote
    Development Extension Pack** by clicking on the extension icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, you can sign up for NVIDIA GPU Cloud, which has a catalog of containers
    and models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pull the NVIDIA Docker image for PyTorch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Create a folder where you want to map the code to on your computer. Then, in
    a terminal window, navigate to the directory you created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the Docker container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Open VS Code and connect to your container by clicking on the ![](img/2e681bf2-3c35-409a-9d18-fa8a87afe775.png) button
    and then, in the dialog box, enter `Remote-Containers: Attach to a running container`.
    This will give you a list of the running containers. Then, open the `/data` folder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put your images in a data folder with the folder labeled as the class name.
    There is an example of this, complete with images, in the GitHub repo for this
    recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Test the container to make sure that the container is up and running and that
    all of the drivers are installed. In the terminal window that you started the
    container with, type in `python` and then execute the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If it returns `True`, you are ready to train with GPUs. If not, you may need
    to troubleshoot your environment.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Declare your variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Make an accuracy printer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Save your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used a Docker container from NVIDIA to bypass the many steps
    it requires to install NVIDIA GPU on a local computer. We used VS Code to connect
    to the running Docker container and we tested it to make sure the container was
    capable of using the GPUs. We then developed our code.
  prefs: []
  type: TYPE_NORMAL
- en: First, as always, we imported our libraries. Then we declared our variables.
    The first variable is the location of the training data, the split amount, the
    number of epochs, and the steps run. We then made a function that prints the results
    on screen so that we could see whether our model was improving with changes to
    the hyperparameters. We then imported the images from our training folder. After
    that, we set up our neural network. Next, we imported the ResNet 50 model. We
    set the model's `requires_grad` parameters to `false` so that our code would not
    affect the already existing model. We are using a sequential linear neural network
    using ReLU for our activation function with a dropout of 20%. We then added a
    smaller network as our output layer using softmax as our activation function.
    We use `Adam` to perform stochastic optimization. We then ran it through our epochs
    and trained the model. Finally, the model was saved.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may want to test your newly trained image classifier. There is an inference
    tester in the GitHub repo for this book under `Ch6 -> pyImage -> inferance.py`.
    In the NVIDIA developer portal, you will find everything you need, from how to
    manage GPU usage effectively across a Kubernetes cluster to information on how
    to deploy the model you just created on a device for a drone such as a TX2.
  prefs: []
  type: TYPE_NORMAL
