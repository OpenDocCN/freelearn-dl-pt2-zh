- en: Computer Vision
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Computer vision has come a long way in recent years. Unlike many other forms
    of machine learning that require complex analysis, the vast majority of computer
    vision problems come from simple RGB cameras. Machine learning frameworks such
    as Keras and OpenCV have standard and high-accuracy neural networks built-in.
    A few years ago, implementing a facial recognition neural net, for example, was
    complex and challenging to set up in Python, let alone on a high-speed device
    using C++ or CUDA. Today, this process is easier and more accessible than ever
    before. In this chapter, we are going to talk about implementing computer vision
    in the cloud, as well as on Edge devices such as NVIDIA Jetson Nano.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机视觉取得了长足的进步。与许多其他需要复杂分析的机器学习形式不同，大多数计算机视觉问题来自简单的RGB摄像头。诸如Keras和OpenCV之类的机器学习框架内置了标准和高精度的神经网络。几年前，在Python中实现面部识别神经网络，例如，是复杂的，并且在高速设备上使用C++或CUDA设置更是挑战。如今，这一过程比以往任何时候都更加简单和可访问。在本章中，我们将讨论在云中实现计算机视觉，以及在NVIDIA
    Jetson Nano等边缘设备上的应用。
- en: 'We will cover the following recipes in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章节中涵盖以下的配方：
- en: Connecting cameras through OpenCV
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过OpenCV连接摄像头
- en: Using Microsoft's custom vision to train and label your images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Microsoft的自定义视觉来训练和标记您的图像
- en: Detecting faces with deep neural nets and Caffe
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度神经网络和Caffe检测面部
- en: Detecting objects using YOLO on Raspberry Pi 4
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在树莓派4上使用YOLO检测物体
- en: Detecting objects using GPUs on NVIDIA Jetson Nano
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NVIDIA Jetson Nano上使用GPU检测物体
- en: Training vision with PyTorch on GPUs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch在GPU上训练视觉
- en: Connecting cameras through OpenCV
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过OpenCV连接摄像头
- en: Connecting a camera through OpenCV is fairly straightforward. The issue is often
    in installing OpenCV. OpenCV installs easily on a desktop computer, but on more
    constrained devices, it may require extra work. In a Raspberry Pi 3, for example,
    you may need to enable swap space. This allows the system to use the SD card as
    a temporary memory store. Depending on the device, there are various instructions
    available online on how to get OpenCV onto a challenging device.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过OpenCV连接摄像头非常简单。问题通常出现在安装OpenCV上。在台式电脑上，OpenCV安装很容易，但在资源受限的设备上，可能需要额外的工作。例如，在树莓派3上，您可能需要启用交换空间。这允许系统将SD卡用作临时内存存储。根据设备的不同，有各种在线说明可以帮助您在具有挑战性的设备上安装OpenCV。
- en: In this recipe, we will connect OpenCV to a camera application on the Raspberry
    Pi Zero, but if you do not have the hardware, you can run the code on a PC. In
    future recipes, we will assume knowledge of this and breeze by the explanation of
    what is going on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将OpenCV连接到树莓派Zero上的摄像头应用程序，但如果您没有硬件，您也可以在PC上运行代码。在接下来的配方中，我们将假设您已掌握这些知识，并简略解释正在进行的事情。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: From a coding perspective, using OpenCV abstracts the hardware away. It does
    not matter if you are using a $5 Raspberry Pi Zero or a $120 LattePanda; the only
    things required for this recipe are a computer and a camera. Most laptops have
    built-in cameras, but for a desktop computer or a **single board computer** (**SBC**),
    such as a Raspberry Pi or LattePanda, you will need a USB web camera.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从编码的角度来看，使用OpenCV可以屏蔽硬件的差异。无论您使用的是$5的树莓派Zero还是$120的LattePanda，这个配方所需的唯一物品是一台电脑和一个摄像头。大多数笔记本电脑都有内置摄像头，但对于台式电脑或者如树莓派或LattePanda这样的**单板计算机（SBC）**，您将需要一个USB网络摄像头。
- en: 'Next, you will need to install OpenCV. As mentioned earlier, there are ways
    of getting OpenCV on constrained devices. These are all unique to the device in
    question. In our case, we will put a PiCam module on a Raspberry Pi Zero. The
    following is an image of a PiCam module for reference:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要安装OpenCV。如前所述，有多种方法可以在受限设备上获取OpenCV。这些方法都是根据具体设备的特性而定。在我们的情况中，我们将在树莓派Zero上安装PiCam模块。以下是PiCam模块的参考图像：
- en: '![](img/1692aaf3-8b87-40d9-a062-d36d1501715c.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1692aaf3-8b87-40d9-a062-d36d1501715c.jpg)'
- en: 'To add the PiCam to the Pi Zero, you simply pull the black tabs from the connector,
    insert the PiCam module, and then push in the tab, as shown in the following image:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要将PiCam添加到Pi Zero上，您只需从连接器中拉出黑色标签，插入PiCam模块，然后将标签推入，如下图所示：
- en: '![](img/6cec335f-5a4a-4e9a-8c6f-c78bb1146298.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cec335f-5a4a-4e9a-8c6f-c78bb1146298.jpg)'
- en: 'From here you need to enable the cameras in your Raspberry Pi. You will need
    to plug a monitor, keyboard, and mouse into your Raspberry Pi. Then, make sure
    that your system is up to date by executing the following commands:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，您需要在树莓派上启用摄像头。您需要将显示器、键盘和鼠标插入树莓派中。然后，通过执行以下命令确保您的系统是最新的：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, you will enable the camera by going into the **Rasp Config** menu. In
    your terminal, type in the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以通过进入 **Rasp Config** 菜单来启用摄像头。在终端中，键入以下内容：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From there, select **Camera** and then enable it:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里选择 **Camera** 然后启用它：
- en: '![](img/62237be4-5d76-4068-8374-a608f7be400d.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62237be4-5d76-4068-8374-a608f7be400d.jpg)'
- en: There are three different libraries that we can `pip` install: `opencv-contrib-python`
    for all of the OpenCV extras, `opencv-python` for a faster, but shorter, list
    of features, and finally `opencv-cython` for a faster Python experience.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个不同的库可以使用 `pip` 安装：`opencv-contrib-python` 包含所有的 OpenCV 扩展功能，`opencv-python`
    提供更快但功能更少的特性列表，最后 `opencv-cython` 提供更快的 Python 使用体验。
- en: 'For this book, I would recommend performing the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书，我建议执行以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How to do it...
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps for this recipe are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的步骤如下：
- en: 'Import OpenCV:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 OpenCV：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Select the camera:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择摄像头：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Check whether the camera is available:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查摄像头是否可用：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Capture, save, and show frames from the camera:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获、保存并显示摄像头帧：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Release the camera:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 释放摄像头：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, first, we import OpenCV. We then select the first camera it
    finds (`camera(0)`). If we were looking for the second camera it finds, then we
    would increment the camera number (`camera(1)`). Next, we check whether the camera
    is available. There can be several reasons why a camera might not be available.
    First, it could be opened by something else. You could, for example, open the
    camera in a different application to see whether it is working and this would
    prevent the Python application from detecting and connecting to the camera. Another
    common issue is that releasing the camera step in the code does not get executed
    and the camera needs to be reset. Next, we capture the video frames and present
    them on the screen until someone presses the *Q* key. Finally, after someone has
    exited the application, we release the camera and close the open window.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，首先我们导入 OpenCV。然后我们选择它找到的第一个摄像头（`camera(0)`）。如果我们要找到第二个找到的摄像头，那么我们会增加摄像头号码（`camera(1)`）。接下来，我们检查摄像头是否可用。摄像头可能不可用的原因有几种。首先，它可能被其他东西打开了。例如，您可以在不同的应用程序中打开摄像头以查看其是否工作，这将阻止
    Python 应用程序检测和连接到摄像头。另一个常见的问题是，代码中释放摄像头的步骤未执行，需要重新设置摄像头。接下来，我们捕获视频帧并在屏幕上显示，直到有人按下
    *Q* 键。最后，在有人退出应用程序后，我们释放摄像头并关闭打开的窗口。
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: OpenCV has many tools for writing text to the screen or drawing bounding boxes
    around an identified object. In addition, it has the ability to downsample or
    change an RGB image to black and white. Filtering and downsampling are techniques
    that machine learning engineers perform on constrained devices that allow them
    to operate efficiently.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 具有许多工具，可以将文本写入屏幕或在识别对象周围绘制边界框。此外，它还能够对 RGB 图像进行降采样或转换为黑白图像。过滤和降采样是机器学习工程师在允许它们高效运行的受限设备上执行的技术。
- en: Using Microsoft's custom vision to train and label your images
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Microsoft 的自定义视觉来训练和标记您的图像
- en: Microsoft's cognitive services offer a one-stop shop for everything you need
    for training images and deploying models. First, it provides a way of uploading
    images. Then, it has a UI for drawing bounding boxes around images, and finally,
    it allows you to deploy and expose an API endpoint you can use for computer vision.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的认知服务为训练图像和部署模型提供了一站式解决方案。首先，它提供了上传图像的方法。然后，它有一个用户界面，可以在图像周围绘制边界框，最后，它允许您部署和公开
    API 端点，用于计算机视觉。
- en: Getting ready
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To use Microsoft's custom vision service, you will require an Azure subscription.
    Then you will need to spin up a new custom vision project. There is a free tier
    for testing out small models and a paid tier for larger models and serving models
    at scale. After creating the custom vision project in the Azure portal, you will
    see two new projects in the resource group. The first will be for training, and
    the second will have a `-prediction` label appended to the name, which will be
    used for the predictions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Microsoft的自定义视觉服务，您需要一个Azure订阅。然后，您需要启动一个新的自定义视觉项目。有一个免费层用于测试小型模型，有一个付费层用于更大的模型和规模化服务模型。在Azure门户中创建自定义视觉项目后，您将在资源组中看到两个新项目。第一个用于训练，第二个名称后附有“-prediction”标签，将用于预测。
- en: Then you will require images of what you are classifying. In our case, we are
    identifying beverages in an environment with lead and carcinogen exposure. If
    you have completed the previous recipe, you will have a camera capturing images
    at 1 second intervals. To make an object detection model in cognitive services,
    you will need at least 30 images of each thing you are trying to classify. More
    images will improve accuracy. To get good accuracy, you should vary the light,
    background, angle, size, and type, and use individual and grouped images of the
    objects.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您将需要所分类物体的图像。在我们的情况下，我们正在识别含有铅和致癌物暴露的饮料。如果您完成了上一个配方，您将会有一个摄像机以每秒1次的间隔捕捉图像。要在认知服务中创建一个对象检测模型，您需要至少30张您要分类的每样东西的图像。更多图像将提高准确性。为了获得良好的准确性，您应该变化光线、背景、角度、大小和类型，并使用单独和组合的物体图像。
- en: 'You also need to install Microsoft''s cognitive services computer vision Python
    package. To do that, execute the following command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要安装Microsoft的认知服务计算机视觉Python包。为此，请执行以下命令：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How to do it...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方的步骤如下：
- en: Go to the Azure portal where you created your custom vision project.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到您创建自定义视觉项目的Azure门户。
- en: Navigate your browser to [https://customvision.ai](https://customvision.ai)
    and log in with your Azure credential. This will take you to the Projects page.
    There are some sample projects, but you will want to create your own. Click on
    the New project tile. Then, fill out the Create new project wizard. For this recipe,
    we are taking pictures of food and drink items so that we can use them in a workplace
    safety computer vision project. This type of computer vision could be used in
    an electronics shop, where people are eating in an environment with contaminants
    such as lead or carcinogens.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将浏览器导航到[https://customvision.ai](https://customvision.ai)，并使用您的Azure凭据登录。这将带您到项目页面。有一些示例项目，但您会想创建自己的项目。点击“新项目”磁贴。然后，填写创建新项目向导。对于本配方，我们拍摄食品和饮料项目的照片，以便我们可以在工作场所安全计算机视觉项目中使用它们。这种计算机视觉可以在电子店中使用，在那里人们在含有铅或致癌物质等污染物的环境中进食。
- en: 'On the main page of the project, you will see a Tags button. Click on the Untagged
    option (as shown in the following screenshot) and you will see all of the images
    that you uploaded:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在项目的主页上，您会看到一个“标签”按钮。点击“未标记”选项（如下图所示），您将看到您上传的所有图像：
- en: '![](img/3e4a3009-3598-4155-a343-b3f656d4e4b9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e4a3009-3598-4155-a343-b3f656d4e4b9.png)'
- en: 'Click on the image and use the tools to draw a bounding box around the images.
    From here, you can draw bounding boxes around your images and tag them:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击图像，使用工具在图像周围绘制一个边界框。从这里开始，你可以在图像周围绘制边界框并打标签：
- en: '![](img/50909e0a-021e-47f5-a662-f6f14682bfb3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50909e0a-021e-47f5-a662-f6f14682bfb3.png)'
- en: 'Next, click on the green Train button to train the model:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，点击绿色的“训练”按钮来训练模型：
- en: '![](img/a7df02d4-bb8c-415d-bfae-e6b7e1769fe7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7df02d4-bb8c-415d-bfae-e6b7e1769fe7.png)'
- en: 'After you click on Train, it will start to train a model. This could take quite
    some time. Once it completes, click on the iteration and then click on the Prediction URL
    button:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“训练”后，它将开始训练一个模型。这可能需要相当长的时间。一旦完成，点击迭代，然后点击“预测URL”按钮：
- en: '![](img/be747990-8bf6-4314-8d4a-4459ffb7a0d6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be747990-8bf6-4314-8d4a-4459ffb7a0d6.png)'
- en: This will give you a window with everything you require in order to send an
    image to the object detection service.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您提供一个窗口，其中包含发送图像到对象检测服务所需的一切。
- en: 'The code for testing the model is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型的代码如下：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: 'Cognitive services use the tagged images to create a model that finds those
    images within a larger picture. As the number of images increases, so does the
    accuracy. There will be a point where, however, when the accuracy reaches convergence or,
    in layman''s terms, does not improve. To find this convergence, add and tag more
    images until the iteration metrics of Precision, Recall, and mAP do not improve.
    The custom vision dashboard in the following screenshot shows the three factors
    that we measure to show the model''s accuracy:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 认知服务使用标记的图像创建一个模型，以在更大的图像中查找这些图像。随着图像数量的增加，准确性也会提高。然而，随着准确性达到收敛点或者俗称的不再改善，会有一点。要找到这个收敛点，添加和标记更多图像，直到精度、召回率和mAP的迭代指标不再改善为止。下面的自定义视觉仪表板显示了我们用来显示模型准确性的三个因素：
- en: '![](img/6b9e86f1-826e-4db9-9f47-007cce8173d7.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b9e86f1-826e-4db9-9f47-007cce8173d7.png)'
- en: Detecting faces with deep neural nets and Caffe
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度神经网络和Caffe检测人脸
- en: One advantage of using OpenCV's implementation of visual neural networks is
    that they are available on different platforms. For the sake of clarity and brevity,
    we are using Python on an environment that has Python installed. However, the
    same results could be used with OpenCV's C++ implementation on an ARM-CortexM3
    or OpenCV's Java implementation on an Android system. In this recipe, we are going
    to use a face detection neural network that OpenCV implemented based on the **Caffe** machine
    learning framework. The output of this recipe will be a window on the PC that
    has the image with bounding boxes around the face.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenCV的视觉神经网络实现的一个优势是它们适用于不同的平台。为了清晰和简洁起见，我们在安装了Python的环境中使用Python。然而，在ARM-CortexM3上使用OpenCV的C++实现或在Android系统上使用OpenCV的Java实现也可以得到相同的结果。在本示例中，我们将使用基于**Caffe**机器学习框架的OpenCV实现的人脸检测神经网络。本示例的输出将是PC上的一个窗口，其中有围绕面部的边界框。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To run this recipe, you will need a web camera attached to your device. You
    will need to install OpenCV, NumPy, and Imutils, if you have already not done
    so. Installing OpenCV can be challenging on very constrained devices. There are
    several ways in which you can attempt to do this if you are unable to install
    it natively on a device. Many devices with extra storage space will allow you
    to use the disk as swap space for the memory. If the device in question supports
    dockerization, then you can compile on a computer and run the container on the
    device. This recipe uses a pretrained model that can be found in the GitHub companion
    to this book in the `Ch6` directory.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要将网络摄像头连接到设备上。如果尚未安装OpenCV、NumPy和Imutils，您需要先安装它们。在资源非常有限的设备上安装OpenCV可能会有挑战性。如果您无法在设备上本地安装它，可以尝试几种方法。许多具有额外存储空间的设备将允许您将磁盘用作内存的交换空间。如果相关设备支持docker化，那么可以在计算机上编译并在设备上运行容器。本示例使用了一个预训练模型，可以在本书的GitHub附录中的`Ch6`目录中找到。
- en: How to do it...
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的步骤如下：
- en: 'Import the libraries:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Import a neural network from the pretrained model in the `Ch6` GitHub repo
    and then initialize OpenCV''s camera operator:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`Ch6` GitHub仓库的预训练模型中导入一个神经网络，然后初始化OpenCV的摄像头操作员：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create a function that downsamples the image and transforms it into a predefined
    shape for our neural network and then perform the inference:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，对图像进行降采样并将其转换为我们神经网络的预定义形状，然后执行推理：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Draw the bounding boxes:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制边界框：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Return the image with bounding boxes:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回带有边界框的图像：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a never-ending loop that reads an image from the camera, performs the
    inference and gets the overlay, and then outputs the image to the screen:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个无休止的循环，从摄像头读取图像，进行推理并获取叠加效果，然后将图像输出到屏幕上：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, clean up and destroy all the windows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，清理并销毁所有窗口：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: After importing our libraries, we import the pretrained face detection model
    into our `net` variable. We then open the first camera (`0`). Then we use `FacNN`
    to predict the image and draw the bounding box. Then we shrink the image to an
    appropriate dimension. We then use `imutils` to resize our large image from the
    camera. We then set the image in the network and get the face detections. Next,
    we get the face detections and retrieve the confidence that the object it finds
    is really a face. In our case, we are using a `.8` or `80%` threshold. We also
    filter out faces with low confidence. We then draw bounding boxes around the faces
    and put confidence text on the boxes. We then return those images to our main
    `while True` loop and display them on the screen. We also wait for a *Q* key to
    quit. Finally, we release the camera and destroy the UI window.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库后，我们将预训练的人脸检测模型导入到我们的 `net` 变量中。然后我们打开第一个摄像头（`0`）。接着我们使用 `FacNN` 来预测图像并绘制边界框。然后我们将图像缩小到适当的尺寸。接下来我们使用
    `imutils` 来调整来自摄像头的大图像大小。然后我们将图像设置在网络中并获取人脸检测结果。接着我们获取人脸检测结果并获取对象确实是脸的置信度。在我们的例子中，我们使用了
    `.8` 或 `80%` 的阈值。我们还过滤掉置信度较低的脸部。然后我们在脸部周围绘制边界框并在框上放置置信度文本。最后，我们将这些图像返回到我们的主 `while
    True` 循环并在屏幕上显示它们。我们还等待按下 *Q* 键来退出。最后，我们释放摄像头并销毁 UI 窗口。
- en: Detecting objects using YOLO on Raspberry Pi 4
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在树莓派 4 上使用 YOLO 检测物体
- en: '**YOLO** stands for **you only look once**. It is a fast image classification
    library that is optimized for GPU processing. YOLO tends to outperform all other
    computer vision libraries. In this recipe, we are going to implement a computer
    vision object detection using OpenCV''s implementation of YOLO. In this example,
    we are going to use a pretrained model that has 40 common objects already trained.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**YOLO** 代表 **you only look once**。它是一个快速的图像分类库，专为 GPU 处理进行了优化。YOLO 往往优于所有其他计算机视觉库。在本教程中，我们将使用
    OpenCV 实现的 YOLO 进行计算机视觉对象检测。在这个例子中，我们将使用一个已经训练好的模型，其中包含 40 种常见对象。'
- en: Getting ready
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To get ready, you will need to clone the GitHub repo for this book. In the
    `Ch6` section, you will find the `yolov3.cfg` config file and the `yolov3.txt` text
    file of the classes. Next, you will need to download the large `weights` file.
    To do this, you will need to open a command prompt and `cd` into the `Ch6` directory
    and then download the `weights` file with the following command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好之后，您需要克隆本书的 GitHub 仓库。在 `Ch6` 部分，您会找到 `yolov3.cfg` 配置文件和 `yolov3.txt` 类别文本文件。接下来，您需要下载大的
    `weights` 文件。为此，您需要打开命令提示符并 `cd` 到 `Ch6` 目录，然后使用以下命令下载 `weights` 文件：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Additionally, you will need to install OpenCV and NumPy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您需要安装 OpenCV 和 NumPy。
- en: How to do it...
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'The steps for this recipe are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此教程的步骤如下：
- en: 'Import the libraries:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Set the variables:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置变量：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define our output layers:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们的输出层：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create bounding boxes:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建边界框：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Draw bounding boxes:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制边界框：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Process the images:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理图像：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Read the camera:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取摄像头：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, clean up and destroy all the windows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，清理和销毁所有窗口：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理…
- en: YOLO looks at the image once and divides the image up into a grid. It then uses
    bounding boxes to divide up the grid. YOLO first determines whether the bounding
    box has an object and then determines the class of object. By incorporating a
    prefilter on the algorithm, this screens out parts of the images that are not
    objects and YOLO is then able to dramatically speed up its search.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 一次性查看图像并将图像分割成网格。然后它使用边界框来划分网格。YOLO 首先确定边界框是否包含对象，然后确定对象的类别。通过在算法中加入预过滤器，可以筛选掉不是对象的图像部分，从而显著加快搜索速度。
- en: In this example, after importing our libraries, we set our variables. First,
    we open `yolov3.txt`. This file contains the classes of the pretrained library
    we will be using. Next, we create a random `color` array to denote our different
    objects as different colors. Then we import our libraries and set our camera to
    the first camera on the computer. We then set thresholds and scale images so that
    the image sizes are something that would be recognizable to the classifier. If
    we, for example, add a high-resolution image, the classifier might only recognize
    very small things as objects while ignoring larger things. This is because YOLO
    tries to determine bounding boxes around objects to filter out things that are
    objects. Next, we define our output layers and then create bounding boxes based
    on our confidence threshold. We then use these bounding boxes to draw rectangles
    around the images and passed that image in addition to the labeled text back to
    our image processor. Our main image processing loop calls the `Yolo` function.
    Finally, before cleaning up our resources, we run through the main loop that performs
    YOLO analysis.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，在导入我们的库之后，我们设置我们的变量。首先，我们打开`yolov3.txt`。这个文件包含我们将使用的预训练库的类别。接下来，我们创建一个随机的`color`数组来表示我们的不同对象为不同的颜色。然后，我们导入我们的库并设置我们的摄像头为计算机上的第一个摄像头。然后，我们设置阈值并缩放图像，以便图像大小适合分类器能够识别。例如，如果我们添加一个高分辨率图像，分类器可能只会识别非常小的物体而忽略较大的物体。这是因为YOLO试图确定围绕对象的边界框，以过滤掉物体。接下来，我们定义我们的输出层，并基于我们的置信度阈值创建边界框。然后，我们使用这些边界框在图像周围绘制矩形，并将该图像及其标签文本传回我们的图像处理器。我们的主要图像处理循环调用`Yolo`函数。最后，在清理资源之前，我们通过执行执行YOLO分析的主循环。
- en: Detecting objects using GPUs on NVIDIA Jetson Nano
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NVIDIA Jetson Nano在GPU上检测对象
- en: NVIDIA makes a series of GPU-enabled SBCs. Some of these, such as the TX2, are
    used on drones because they are lightweight and can pack a lot of power under
    their GPU-enabled systems. GPUs, along with **Tensor Processing Units** (**TPUs**),
    are able to deliver multiple times the computer vision capabilities compared with
    standard CPUs. In this recipe, we will use NVIDIA Jetson Nano, which is their
    least expensive development board at $99\. The Jetson has an ecosystem of libraries
    that only work on their products.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA制造了一系列带GPU的SBC。其中一些，如TX2，因为它们轻便且能在其GPU启用系统下提供大量计算机视觉功能，所以被用于无人机上。与标准CPU相比，GPU与**张量处理单元**（**TPUs**）能够提供多倍的计算机视觉能力。在本示例中，我们将使用NVIDIA
    Jetson Nano，这是他们售价最低的开发板，售价为99美元。Jetson有一个只能在他们产品上运行的库生态系统。
- en: Getting ready
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: First, you will need an NVIDIA Jetson. Then you will need to install the operating
    system. To do this, you will need to flash a micro USB with NVIDIA's Jetpack image.
    The Jetpack image consists of a base Ubuntu image with many of the dev tools you
    will need in order to get going. Once you have your OS image, put it into the
    Jetson with a monitor, keyboard, mouse, and network-attached.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要一台NVIDIA Jetson。接下来需要安装操作系统。为此，你需要使用NVIDIA的Jetpack映像来刷写一个Micro USB。Jetpack映像包含了一个基于Ubuntu的基础映像，并且包含了你启动所需的许多开发工具。一旦你有了操作系统映像，就将其放入Jetson中，并连接显示器、键盘、鼠标和网络。
- en: 'Then you will update the OS, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将按以下步骤更新操作系统：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After that, you will need to install the extra software to run the code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你需要安装额外的软件来运行代码：
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once you have done that, you will need to download the starter project from
    Jetson:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成上述步骤，你将需要从Jetson下载起始项目：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then you will make and navigate to the `build` directory:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来你将创建并导航至`build`目录：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'From here we will make, install, and link the code from the repository:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们将编译、安装并链接存储库中的代码：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After running `make`, you will get a dialog in your terminal offering you the
    ability to download some different pretrained models and also PyTorch so that
    you can train your own models. Use the wizard to first select the models you want
    to download:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`make`后，你将在终端中收到一个对话框，询问你是否要下载一些不同的预训练模型，以及PyTorch，以便你可以训练自己的模型。使用向导首先选择你想要下载的模型：
- en: '![](img/6bb20ab8-2bf3-42d0-b4f3-905e788d4b64.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bb20ab8-2bf3-42d0-b4f3-905e788d4b64.png)'
- en: 'The tool will download all the models you selected:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 工具将下载你选择的所有模型：
- en: '![](img/3cc4a518-9752-4c81-8dbf-37ac48dd6b8f.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cc4a518-9752-4c81-8dbf-37ac48dd6b8f.png)'
- en: For this recipe, you can keep the default models. After selecting OK, it will
    ask you to install PyTorch so that you can train your own models. Select PyTorch
    and then select OK.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，您可以保留默认的模型。选择“确定”后，它将要求您安装PyTorch，以便您可以训练自己的模型。选择PyTorch，然后选择“确定”。
- en: How to do it...
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的步骤如下：
- en: 'Import the Jetson libraries:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Jetson库：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Set the variables:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置变量：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, run the camera display loop:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，运行摄像头显示循环：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How it works...
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we added the libraries and then we cloned the Jetson inference
    repository. We then ran a series of make and linker tools to get the install working
    correctly. During this process, we downloaded a large set of pretrained models.
    We then started writing our code. Because the Jetson is limited in terms of its
    capabilities and memory, installing a full-featured IDE can be wasteful. One workaround
    for this is to use an IDE that supports SSH, such as Visual Studio Code, and remoting
    into the box via the IDE. You can then work with the device without tying up resources
    on the Jetson Nano.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们添加了库，然后克隆了Jetson推理存储库。然后，我们运行了一系列的制作和链接工具，以确保安装正确运行。在此过程中，我们下载了大量预训练模型。然后我们开始编写我们的代码。由于Jetson在功能和内存方面有限，安装一个功能齐全的IDE可能会浪费资源。这个问题的一个解决方法是使用支持SSH的IDE，比如Visual
    Studio Code，并通过IDE远程连接到设备上。这样您就可以在不占用Jetson Nano资源的情况下与设备一起工作。
- en: To build out this project, first, we import the Jetson inference and `utils` libraries.
    In the previous recipes, we did a lot of the low-level work ourselves as far as
    using OpenCV to get the camera and then used other libraries to manipulate the
    images and draw bounding boxes. With Jetson's libraries, the vast majority of
    that code is handled for you. After we imported the libraries, we imported the
    models we downloaded earlier and set a threshold. Then we entered it in the camera
    dimensions and set the camera to `/dev/video0`. Next, we set our visual display.
    Finally, we grabbed the camera image, ran the detection algorithm, and then output
    that camera image to the screen with the bounding boxes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这个项目，首先我们要导入Jetson推理和`utils`库。在之前的示例中，我们自己处理了许多低级工作，如使用OpenCV获取摄像头，然后使用其他库来操作图像并绘制边界框。使用Jetson的库，这些大部分代码都已为您处理好了。在导入了库之后，我们导入了之前下载的模型并设置了阈值。然后我们设置了摄像头的尺寸并将摄像头设置为`/dev/video0`。接下来，我们设置了视觉显示。最后，我们获取摄像头图像，运行检测算法，然后将带有边界框的摄像头图像输出到屏幕上。
- en: There's more...
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: As we mentioned earlier, NVIDIA has an ecosystem for their products. They have
    helpful containers, models, and tutorials to work efficiently with their hardware.
    To aid you, they have a product website that gets you started with training models
    and building out containerized notebooks. They have dozens of prebuilt containers
    for different libraries, including PyTorch and TensorFlow to name a few. They
    also have dozens of pretrained models using everything from pose detection to
    specific industry models. They even have their own cloud where you can train your
    models if you wish. You can, however, run locally as well. Their website is [https://ngc.nvidia.com/](https://ngc.nvidia.com/).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，NVIDIA为他们的产品生态系统提供了支持。他们有帮助的容器、模型和教程，可以有效地与他们的硬件一起工作。为了帮助您，他们有一个产品网站，可以帮助您开始训练模型和构建容器化的笔记本电脑。他们提供了数十个预构建的容器，涵盖了不同的库，包括PyTorch和TensorFlow等。他们还有数十个使用从姿势检测到特定行业模型的预训练模型。他们甚至有自己的云，如果您愿意，可以在那里训练您的模型。但是您也可以在本地运行。他们的网站是[https://ngc.nvidia.com/](https://ngc.nvidia.com/)。
- en: Training vision with PyTorch on GPUs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch在GPU上进行视觉训练
- en: In the previous recipe, we implemented an object classifier using GPU and an
    NVIDIA Jetson Nano. There are other types of GPU-enabled devices. These range
    from the NVIDIA TX2, which can be put on a drone to do real-time analysis of pipelines,
    to industrial PCs running GPUs and using computer vision to perform analyses on
    workplace safety. In this recipe, we are going to train and add to an existing
    image classification model by adding our own images to it.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个示例中，我们使用GPU和NVIDIA Jetson Nano实现了对象分类器。还有其他类型的启用GPU的设备。从能够放置在无人机上以进行实时管道分析的NVIDIA
    TX2，到运行GPU并使用计算机视觉来执行工作场所安全性分析的工业PC。在这个示例中，我们将通过向其添加我们自己的图像来训练并增加现有的图像分类模型。
- en: Challenges that the IoT faces include **over-the-air** (**OTA**) updates and
    fleet management. IoT Edge is a conceptual framework that solves this. In OTA
    updates, Docker containers are used as an update mechanism. The underlying systems
    can be updated without having to worry about complete device failure. If an update
    does not work, the system can be rolled back because container failures do not
    affect the main OS and the Docker daemon can perform the update and roll back.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网面临的挑战包括**空中升级（OTA）**和车队管理。物联网边缘是一个解决这些问题的概念框架。在OTA升级中，Docker 容器被用作升级机制。在不必担心设备完全失效的情况下，可以更新底层系统。如果更新不起作用，可以回滚系统，因为容器故障不会影响主操作系统，Docker
    守护进程可以执行更新和回滚。
- en: In this recipe, we are going to use NVIDIA Docker containers to build our models.
    Later, we will use that model for inference.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 NVIDIA Docker 容器来构建我们的模型。稍后，我们将使用该模型进行推断。
- en: Getting ready
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To get ready, we are going to use Docker with an application version greater
    than 19\. In Docker 19, the `--gpu` tag was added, allowing you to use Docker
    to access the GPU natively. Depending on your GPUs, you may need to install additional
    drivers to make the GPUs work on your machine.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做好准备，我们将使用版本大于19的 Docker 应用程序。在 Docker 19 中，添加了`--gpu`标签，允许您本地访问 GPU。根据您的GPU，您可能需要安装额外的驱动程序以使GPU在您的计算机上工作。
- en: 'We are also going to be using **Visual Studio Code** (**VS Code**), which,
    with the help of a plugin, allows you to write code directly in NVIDIA''s GPU
    PyTorch container. You will need to perform the following steps:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用**Visual Studio Code**（**VS Code**），借助插件，允许您直接在 NVIDIA 的 GPU PyTorch 容器中编写代码。您需要执行以下步骤：
- en: Download and install VS Code and then use the extension manager to add the **Remote
    Development Extension Pack** by clicking on the extension icon.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并安装 VS Code，然后使用扩展管理器通过点击扩展图标添加**Remote Development Extension Pack**。
- en: Optionally, you can sign up for NVIDIA GPU Cloud, which has a catalog of containers
    and models.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可选地，您可以注册 NVIDIA GPU Cloud，它具有容器和模型的目录。
- en: 'Pull the NVIDIA Docker image for PyTorch:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拉取用于 PyTorch 的 NVIDIA Docker 镜像：
- en: '[PRE34]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Create a folder where you want to map the code to on your computer. Then, in
    a terminal window, navigate to the directory you created.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您希望将代码映射到的计算机上创建一个文件夹。然后，在终端窗口中，导航到您创建的目录。
- en: 'Run the Docker container:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 Docker 容器：
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Open VS Code and connect to your container by clicking on the ![](img/2e681bf2-3c35-409a-9d18-fa8a87afe775.png) button
    and then, in the dialog box, enter `Remote-Containers: Attach to a running container`.
    This will give you a list of the running containers. Then, open the `/data` folder.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '打开 VS Code 并通过点击 ![](img/2e681bf2-3c35-409a-9d18-fa8a87afe775.png) 按钮连接到您的容器，然后在对话框中输入`Remote-Containers:
    Attach to a running container`。这将为您列出正在运行的容器。接着，打开`/data`文件夹。'
- en: Put your images in a data folder with the folder labeled as the class name.
    There is an example of this, complete with images, in the GitHub repo for this
    recipe.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的图像放在一个数据文件夹中，文件夹以类名标记。在 GitHub 仓库中有一个包含完整示例的文件夹及其图像。
- en: 'Test the container to make sure that the container is up and running and that
    all of the drivers are installed. In the terminal window that you started the
    container with, type in `python` and then execute the following code:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试容器以确保容器已启动并运行，并安装了所有驱动程序。在您启动容器的终端窗口中，输入`python`，然后执行以下代码：
- en: '[PRE36]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If it returns `True`, you are ready to train with GPUs. If not, you may need
    to troubleshoot your environment.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果返回`True`，您可以准备使用GPU进行训练。如果没有，您可能需要排查您的环境问题。
- en: How to do it...
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps for this recipe are as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的步骤如下：
- en: 'Import the libraries:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库：
- en: '[PRE37]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Declare your variables:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明您的变量：
- en: '[PRE38]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Make an accuracy printer:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个准确率打印机：
- en: '[PRE39]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Import the images:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入图像：
- en: '[PRE40]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Set up the network:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置网络：
- en: '[PRE41]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Train the model:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE42]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Save your model:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存您的模型：
- en: '[PRE43]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we used a Docker container from NVIDIA to bypass the many steps
    it requires to install NVIDIA GPU on a local computer. We used VS Code to connect
    to the running Docker container and we tested it to make sure the container was
    capable of using the GPUs. We then developed our code.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了 NVIDIA 的 Docker 容器来跳过在本地计算机上安装 NVIDIA GPU 所需的许多步骤。我们使用 VS Code
    连接到正在运行的 Docker 容器，并测试确保容器能够使用 GPU。然后，我们开发了我们的代码。
- en: First, as always, we imported our libraries. Then we declared our variables.
    The first variable is the location of the training data, the split amount, the
    number of epochs, and the steps run. We then made a function that prints the results
    on screen so that we could see whether our model was improving with changes to
    the hyperparameters. We then imported the images from our training folder. After
    that, we set up our neural network. Next, we imported the ResNet 50 model. We
    set the model's `requires_grad` parameters to `false` so that our code would not
    affect the already existing model. We are using a sequential linear neural network
    using ReLU for our activation function with a dropout of 20%. We then added a
    smaller network as our output layer using softmax as our activation function.
    We use `Adam` to perform stochastic optimization. We then ran it through our epochs
    and trained the model. Finally, the model was saved.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像往常一样，我们导入了我们的库。然后我们声明了我们的变量。第一个变量是训练数据的位置，分割量，epochs数量和运行步骤。然后我们制作了一个在屏幕上打印结果的功能，以便我们能够看到我们的模型是否随着超参数的更改而改进。然后我们从我们的训练文件夹导入了图像。之后，我们设置了我们的神经网络。接下来，我们导入了ResNet
    50模型。我们将模型的`requires_grad`参数设置为`false`，这样我们的代码就不会影响已有的模型。我们正在使用一个使用ReLU作为激活函数的序列线性神经网络，丢失率为20%。然后，我们添加了一个较小的网络作为我们的输出层，使用softmax作为我们的激活函数。我们使用`Adam`进行随机优化。然后我们通过我们的epochs运行它并训练模型。最后，模型被保存了。
- en: There's more...
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: You may want to test your newly trained image classifier. There is an inference
    tester in the GitHub repo for this book under `Ch6 -> pyImage -> inferance.py`.
    In the NVIDIA developer portal, you will find everything you need, from how to
    manage GPU usage effectively across a Kubernetes cluster to information on how
    to deploy the model you just created on a device for a drone such as a TX2.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想测试你新训练的图像分类器。在本书的GitHub存储库中的`Ch6 -> pyImage -> inferance.py`目录下有一个推理测试器。在NVIDIA开发者门户网站上，你会找到一切所需信息，从如何在Kubernetes集群中有效管理GPU使用，到如何将刚刚创建的模型部署到像TX2这样的无人机设备上。
