- en: Transfer Learning with Modern Network Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored how deep learning algorithms can be used
    to create artistic images, create new images based on existing datasets, and generate
    text. In this chapter, we will introduce you to different network architectures
    that power modern computer vision applications and natural language systems. We
    will also cover how transfer learning can be incorporated into these models.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is a method in machine learning where a model that's been
    developed for a particular task is reused for another. If, for example, we wanted
    to learn how to drive a motorbike but we already know how to drive a car, we would
    transfer our knowledge about what driving a car involves to the new task rather
    than starting from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: To transfer such knowledge from one task to another, some of the layers in the
    network need to be frozen. Freezing a layer means that the weights of the layer
    won't update during training. The benefit of transfer learning is that it can
    speed up the time that's taken to develop and train a new model by reusing what
    was learned by the pretrained model, thereby helping to accelerate the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the architectures that we will look at in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Residual network** (**ResNet**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder-decoder architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Modern network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Densely connected convolutional networks – DenseNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model ensembling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder-decoder architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern network architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best things we do when the deep learning model fails to learn is
    add more layers to the model. As you add layers, the model's accuracy improves
    and then starts saturating. However, adding more layers beyond a certain number
    will introduce certain challenges, such as vanishing or exploding gradients. This
    is partially solved by carefully initializing weights and introducing intermediate
    normalizing layers. Modern architectures, such as ResNet and Inception, try to
    solve this problem by introducing different techniques, such as residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ResNet was first introduced in 2015 in a paper called *Deep Residual Learning
    for Image Recognition* by Kaiming He and et al. ([https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)).
    It makes it possible for us to train thousands of layers and achieve high performance.
    The core concept of ResNet is to introduce an identity shortcut connection that
    skips one or more of the layers. The following diagram depicts how ResNet works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec6af942-3b2b-4c91-8133-7850c3bc892b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This identity mapping doesn''t have any parameters. It is simply there to add
    the output from the previous layer to the next layer. However, sometimes, x and
    F(x) will not have the same dimensions. A convolution operation typically shrinks
    the spatial resolution of an image. For example, a 3 x 3 convolution on a 32 x
    32 image results in a 30 x 30 image. This identity mapping is multiplied by a
    linear projection, *W*, in order to expand the channels of the shortcut to match
    the residual. As such, the input, x, and F(x) need to be combined to create the
    input for the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19c4f7c5-4ffa-48e4-8f02-0f182357dad8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code demonstrates what a simple ResNet block would look like
    in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `ResNetBasicBlock` class contains an `init` method that initializes all
    the different layers, such as the convolution layer and batch normalization. The
    forward method is almost identical to what we have seen up until now, except that
    the input is added to the layer's output just before it is returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch `torchvision` package provides an out-of-the-box ResNet model with
    different layers. Some of the different models that are available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ResNet-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also use any of these models for transfer learning. The `torchvision`
    instance allows us to simply create one of these models and use it as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows what a 34-layer ResNet model would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70fe2fcf-af96-4e53-96a1-172475925d0d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that this network consists of multiple ResNet blocks. A key
    advantage of these modern networks is that they need very few parameters compared
    to models such as VGG since they avoid using fully connected layers that need
    lots of parameters to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will train a ResNet model on the dogs and cats dataset. We will use
    the data that we used in [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml),
    *Diving Deep into Neural Networks*, and will quickly train a model based on the
    features that have been calculated from the ResNet. As usual, we will follow these
    steps to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the PyTorch datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the loaders for training and validation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the ResNet model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the convolutional features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a custom PyTorch dataset class for the pre-convoluted features and loader.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a simple linear model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and validate the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once done, we are going to repeat these steps for Inception and DenseNet. Finally,
    we will explore the ensembling technique, where we combine these powerful models
    to build a new model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating PyTorch datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we need to create a transformation object that contains all the basic
    transformations required and use the `ImageFolder` function to load the images
    from the data directory that we created in [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml),
    *Diving Deep into Neural Networks*. In the following code, we create the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By now, most of the preceding code will be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating loaders for training and validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use PyTorch loaders to load the data provided by the dataset in the form
    of batches, along with all of its advantages, such as shuffling the data and using
    multithreads, to speed up the process. The following code demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We need to maintain the exact sequence of the data while calculating the pre-convoluted
    features. When we allow the data to be shuffled, we won't be able to maintain
    the labels. So, ensure that `shuffle` is `False`; otherwise, the required logic
    needs to be handled inside the code.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ResNet model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will consider a coded example for creating a ResNet model. First,
    we initiate the pretrained `resnet34` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we discard the last linear layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model has been created, we set the `requires_grad` parameter to `False` so
    that PyTorch doesn''t have to maintain any space for holding gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Extracting convolutional features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we pass the data from the train and validation data loaders through the
    model and store the results in a list for further computation. By calculating
    the pre-convoluted features, we can save a lot of time in training the model as
    we won''t be calculating these features in every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate through the train data and store the calculated features and the labels
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For validation data, iterate through the validation data and store the calculated
    features and the labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Creating a custom PyTorch dataset class for the pre-convoluted features and
    loader
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have calculated the pre-convoluted features, we need to create
    a custom dataset that can select data from them. Here, we will create a custom
    dataset and loader for the pre-convoluted features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the custom dataset for the pre-convoluted features has been created, we
    can use the `DataLoader` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This will create a data loader for training and validation.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple linear model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we need to create a simple linear model that will map the pre-convoluted
    features to the respective categories. In this example, there are two categories
    (dogs and cats):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are ready to train our new model and validate the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code shows how we can train the model. Note that the `fit` function
    is the same as the one discussed in [Chapter 3](f93f665d-9a2a-4d36-b442-75e7fb89d9cd.xhtml),
    *Diving Deep into Neural Networks*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Inception
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Inception network was an important milestone in the development of CNN
    classifiers as it was shown to improve both speed and accuracy. There have been
    a number of versions of Inception, with some of the most noteworthy being the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inception v1 ([https://arxiv.org/pdf/1409.4842v1.pdf](https://arxiv.org/pdf/1409.4842v1.pdf)),
    commonly known as GoogLeNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v2 and v2 ([https://arxiv.org/pdf/1512.00567v3.pdf](https://arxiv.org/pdf/1512.00567v3.pdf))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inception v4 and Inception-ResNet ([https://arxiv.org/pdf/1602.07261.pdf](https://arxiv.org/pdf/1602.07261.pdf))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows how the naive Inception network is structured (v1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7243507d-5680-4082-af07-6fca7a80b89c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the convolution of different sizes is applied to the input, and the outputs
    of all these layers are concatenated. This is the simplest version of an Inception
    module. There is another variant of an Inception block where we pass the input
    through a 1 x 1 convolution before passing it through 3 x 3 and 5 x 5 convolutions.
    A 1 x 1 convolution is used for dimensionality reduction. It helps in solving
    computational bottlenecks. A 1 x 1 convolution looks at one value at a time and
    across the channels. For example, using a 10 x 1 x 1 filter on an input size of
    100 x 64 x 64 would result in 10 x 64 x 64\. The following diagram shows the Inception
    block with dimensionality reductions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43677ad4-6c85-43cb-817a-29b9f9a21663.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://arxiv.org/pdf/1409.4842.pdf](https://arxiv.org/pdf/1409.4842.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at a PyTorch example of what the preceding Inception block
    would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code contains two classes: `BasicConv2d` and `InceptionBasicBlock`.
    `BasicConv2d` acts like a custom layer that applies a two-dimensional convolution
    layer, batch normalization, and a ReLU layer to the input that is passed through.
    It is good practice to create a new layer when we have a repeating code structure,
    in order to make the code look elegant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `InceptionBasicBlock` class implements what we have in the second Inception
    diagram. Let''s go through each smaller snippet and try to understand how it is
    implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code transforms the input by applying a 1 x 1 convolution block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block, followed by a 5 x 5 convolution block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we transform the input by applying a 1 x 1 convolution
    block, followed by a 3 x 3 convolution block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we apply an average pool, along with a 1 x 1 convolution
    block. Finally, we concatenate all the results together. An Inception network
    consists of several Inception blocks. The following diagram shows what an Inception
    architecture would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6f49a1f-4121-458d-bbee-f7e4034f24b6.png)'
  prefs: []
  type: TYPE_IMG
- en: The Inception architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchvision` package has an Inception network that can be used in the
    same way as we used the ResNet network. Many improvements were made to the initial
    Inception block, and the current implementation that''s available from PyTorch
    is Inception v3\. Let''s look at how we can use the Inception v3 model from `torchvision`
    to calculate pre-computed features. We won''t go through the data loading process
    as we will be using the same data loaders from the *Creating a ResNet model* section.
    We will look at the following important topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Inception model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting convolutional features using `register_forward_hook`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a new dataset for the convoluted features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a fully connected model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an Inception model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Inception v3 model has two branches, each of which generates an output,
    and in the original model training, we would merge the losses, just like we did
    for style transfer. At the moment, we are interested in using only one branch
    to calculate pre-convoluted features using Inception. Going into the details of
    this is outside the scope of this book. If you are interested in finding out more
    about how this works, then going through the paper and the source code ([https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py))
    of the Inception model would help. We can disable one of the branches by setting
    the `aux_logits` parameter to `False`. The following code explains how to create
    a model and how to set the `aux_logits` parameter to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Extracting the convolution features from the Inception model isn't straightforward,
    so we will use the `register_forward_hook` function to extract the activations.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting convolutional features using register_forward_hook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the same techniques that we used to calculate activations
    for style transfer. The following is the `LayerActivations` class with some minor
    modifications since we are only interested in extracting the outputs of a particular
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Apart from the `hook` function, the rest of the code is similar to what we
    used for style transfer. Since we are capturing the outputs of all the images
    and storing them, we won''t be able to hold the data on the **graphics processing
    unit** (**GPU**) memory. Therefore, we need to extract the tensors from the GPU
    and CPU and just store the tensors instead of `Variable`. We are converting them
    back into tensors since the data loaders will only work with tensors. In the following
    code, we''re using the objects of `LayerActivations` to extract the output of
    the Inception model from the last layer, excluding the average pooling layer,
    the dropout layer, and the linear layer. We are skipping the average pooling layer
    to avoid losing useful information in the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let's create the datasets and loaders that are required for the new convoluted
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new dataset for the convoluted features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use the same `FeaturesDataset` class to create the new dataset and data
    loaders. In the following code, we''re creating the datasets and the loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a new model that we can train on the pre-convoluted features.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a fully connected model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A simple model may end up overfitting, so let''s include dropout in the model.
    Dropout will help us avoid overfitting. In the following code, we are creating
    our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Once the model has been created, we can train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will use the same fit and training logic that we used in our ResNet
    example. We''re only going to look at the training code and the results it returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the results, the Inception model achieves 99% accuracy on the training
    dataset and 97.8% accuracy on the validation dataset. Since we are precomputing
    and holding all the features in memory, it takes less than a few minutes to train
    the models. If you are running out of memory when you run the program on your
    machine, then you may need to avoid holding the features in memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at another interesting architecture, DenseNet,
    which has become very popular in the last year.
  prefs: []
  type: TYPE_NORMAL
- en: Densely connected convolutional networks – DenseNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the most successful and popular architectures, such as ResNet and Inception,
    have shown the importance of deeper and wider networks. ResNet uses shortcut connections
    to build deeper networks. DenseNet has taken this to a whole new level by allowing
    connections to be made from each layer to the subsequent layers, that is, the
    layers where we can receive all the feature maps from the previous layers. Symbolically,
    this would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/adc8b7d8-c009-4882-83bf-55122b79de47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram describes what a five-layer dense block would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4491f48d-c888-46ae-acfe-ae908a038f43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)'
  prefs: []
  type: TYPE_NORMAL
- en: There is also a DenseNet implementation of `torchvision` ([https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py](https://github.com/pytorch/vision/blob/master/torchvision/models/densenet.py)).
    Let's look at two of its major functionalities, that is, `_DenseBlock` and `_DenseLayer`.
  prefs: []
  type: TYPE_NORMAL
- en: The _DenseBlock object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the code for `_DenseBlock` and then walk through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`_DenseBlock` is a sequential module where we add layers in a sequential order.
    Based on the number of layers (`number_layers`) in the block, we add that number
    of `_DenseLayer` objects, along with a name, to it. All the magic happens inside
    the `_DenseLayer` object. Let''s look at what goes on inside the `DenseLayer`
    object.'
  prefs: []
  type: TYPE_NORMAL
- en: The _DenseLayer object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One way to learn how a particular network works is to look at the source code.
    PyTorch has a very clean implementation and most of the time it is easily readable.
    Let''s look at the `_DenseLayer` implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: If you are new to inheritance in Python, then the preceding code may not look
    intuitive. The `_DenseLayer` object is a subclass of `nn.Sequential`; let's look
    at what goes on inside each method.
  prefs: []
  type: TYPE_NORMAL
- en: In the `__init__` method, we add all the layers that the input data needs to
    be passed to. It is quite similar to all the other network architectures we have
    seen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The magic happens in the `forward` method. We pass the input to the `forward`
    method of the super class, which is `nn.Sequential`. Let''s look at what happens
    in the `forward` method of the sequential class ([https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py](https://github.com/pytorch/pytorch/blob/409b1c8319ecde4bd62fcf98d0a6658ae7a4ab23/torch/nn/modules/container.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The input is passed through all the layers that were previously added to the
    sequential block and the output is concatenated to the input. This process is
    repeated for the required number of layers in a block.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how a DenseNet block works, let's explore how we can
    use DenseNet to calculate pre-convoluted features and build a classifier model
    on top of it. At a high level, the DenseNet implementation is similar to the VGG
    implementation. The DenseNet implementation also has a features module, which
    contains all the dense blocks, and a classifier module, which contains the fully
    connected model. We will be going through the following steps for building the
    model in this section but will be skipping most of the parts that are similar
    to what we have seen for Inception and ResNet, such as creating the data loader
    and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will discuss the following steps in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DenseNet model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting DenseNet features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a dataset and loaders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a fully connected model and training it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By now, most of the code will be self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a DenseNet model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Torchvision has a pretrained DenseNet model with different layer options (121,
    169, 201, and 161). Here, we have chosen the model with 121 layers. As we mentioned
    previously, the DenseNet model has two modules: `features` (containing the dense
    blocks) and `classifier` (fully connected block). Since we are using DenseNet
    as an image feature extractor, we will only use the `features` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let's extract the DenseNet features from the images.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting DenseNet features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This process is similar to what we did for Inception, except we aren''t using
    `register_forward_hook` to extract features. The following code shows how the
    DenseNet features are extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is similar to what we have seen for Inception and ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset and loaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the same `FeaturesDataset` class that we created for ResNet and
    use it to create data loaders for the train and validation datasets. We will use
    the following code to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Now, it's time to create the model and train it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a fully connected model and training it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will use a simple linear model, similar to what we used in ResNet and
    Inception. The following code shows the network architecture that we will be using
    to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the same `fit` method to train the preceding model. The following
    code snippet shows the training code, along with the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding algorithm was able to achieve a maximum training accuracy of 99%,
    and 99% validation accuracy. Your results may be different since the validation
    dataset you will have created may have different images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the advantages of DenseNet are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It substantially reduces the number of parameters required.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It alleviates the vanishing gradient problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It encourages feature reuse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this next section, we will explore how we can build a model that combines
    the advantages of the convoluted features we've computed using ResNet, Inception,
    and DenseNet.
  prefs: []
  type: TYPE_NORMAL
- en: Model ensembling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There could be times when we need to try to combine multiple models to build
    a very powerful model. There are many techniques we can use to build an ensemble
    model. In this section, we will learn how to combine outputs using the features
    generated by three different models (ResNet, Inception, and DenseNet) to build
    a powerful model. We will be using the same dataset that we used for the other
    examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture for the ensemble model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80ace9cb-9435-4e05-8590-af55a99dd66c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows what we are going to do in the ensemble model,
    which can be summarized in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create three models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the image features using the created models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a custom dataset that returns features of all the three models, along
    with the labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a model that's similar to the architecture shown in the preceding diagram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and validate the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's explore each of these steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Creating models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's create all the three required models, as shown in the following code blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for creating the ResNet model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for creating the Inception model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for creating the DenseNet model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all the models, let's extract the features from the images.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the image features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will combine all the logic that we have seen individually for the algorithms
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for ResNet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for Inception is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The code for DenseNet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have created image features using all the models. If you are facing
    memory issues, then you can either remove one of the models or stop storing features
    that are slow to train in memory. If you are running this on a CUDA instance,
    then you can go for a more powerful instance.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom dataset, along with data loaders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We won''t be able to use the `FeaturesDataset` class as it is since it was
    developed to pick from the output of only one model. Due to this, the following
    implementation contains minor changes that have been made to the `FeaturesDataset`
    class so that we can accommodate all three generated features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have made changes to the `__init__` method so that we can store all
    the features that are generated from the different models. We also changed the `__getitem__`
    method so that we can retrieve the features and labels of an image. Using the
    `FeatureDataset` class, we created dataset instances for the training and validation
    data. Once the dataset has been created, we can use the same data loader for batching
    data, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Creating an ensembling model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we need to create a model that works like the architecture diagram we
    saw previously. The following code implements this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we created three linear layers that take the features
    that will be generated by the different models. We sum up all the outputs from
    these three linear layers and pass them to another linear layer, which maps them
    to the required categories. To prevent the model from overfitting, we used dropouts.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to make some minor changes to the `fit` method to accommodate the three
    input values we generated from the data loader. The following code implements
    the new `fit` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, most of the code remains the same, except that the loader returns
    three inputs and one label. Therefore, we have to make changes to the function,
    which is self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The ensemble model achieves 99.6% training accuracy and a validation accuracy
    of 99.3%. Though ensemble models are powerful, they are computationally expensive.
    They are good techniques to use when you are solving problems in competitions
    such as Kaggle.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Almost all the deep learning algorithms we have seen in this book are good
    at learning how to map training data to their corresponding labels. We cannot
    use them directly for tasks where the model needs to learn from a sequence and
    generate another sequence or an image. Some example applications are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Language translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image generation (`seq2img`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these problems can be seen as forms of sequence-to-sequence mapping,
    and these can be solved using a family of architectures called encoder-decoder
    architectures. In this section, we will learn about the intuition behind these
    architectures. We will not be looking at the implementation of these networks
    as they need to be studied in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the encoder-decoder architecture looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7dc27de-9ac7-4f14-9531-ef4ec4be7236.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An encoder is usually a **recurrent neural network** (**RNN**) (for sequential
    data) or a **convolutional neural network** (**CNN**) (for images) that takes
    in an image or a sequence and converts it into a fixed-length vector that encodes
    all the information. The decoder is another RNN or CNN, which learns to decode
    the vector that was generated by the encoder and generates a new sequence of data.
    The following diagram shows what the encoder-decoder architecture looks like for
    an image captioning system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1dc80559-4b82-4a28-884d-5ec94e46ac29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at what happens inside an encoder and a decoder architecture
    for an image captioning system.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For an image captioning system, we should use a trained architecture, such as
    ResNet or Inception, to extract features from the image. Like we did for the ensemble
    model, we can output a fixed vector length by using a linear layer and then make
    that linear layer trainable.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decoder is a **long short-term memory** (**LSTM**) layer that will generate
    a caption for an image. To build a simple model, we can just pass the encoder
    embedding as input to the LSTM. However, this could be quite challenging for the
    decoder to learn; instead, it is common practice to provide the encoder embedding
    at every step of the decoder. Intuitively, a decoder learns to generate a sequence
    of text that best describes the caption of a given image.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder with attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In 2017, a paper titled *Attention Is All You Need*, by Ashish Vaswani and
    co. ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)),
    was published, and it incorporates an attention mechanism. At each timestep, the
    attention network computes the weights of the pixels. It considers the sequence
    of words that have been generated so far and outputs what should be described
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e74e5dd3-69f8-454c-af67-0880f5004aae.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding example, we can see that it is the LSTM's ability to retain
    information that can help it to learn that it is logical to write* "is holding
    a dog"* after "*a man"*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored some modern architectures, such as ResNet, Inception,
    and DenseNet. We also explored how we can use these models for transfer learning
    and ensembling, and also introduced the encoder-decoder architecture, which powers
    a lot of systems, such as language translation systems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be diving into deep reinforcement learning and
    learning how models can be applied to solve problems in the real world. We will
    also look at some PyTorch implementations that can help with this.
  prefs: []
  type: TYPE_NORMAL
