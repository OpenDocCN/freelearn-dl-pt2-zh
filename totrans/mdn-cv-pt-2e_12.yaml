- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Applications of Object Detection and Segmentation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物体检测与分割的应用
- en: In previous chapters, we learned about various object detection techniques,
    such as the R-CNN family of algorithms, YOLO, SSD, and the U-Net and Mask R-CNN
    image segmentation algorithms. In this chapter, we will take our learning a step
    further – we will work on more realistic scenarios and learn about frameworks/architectures
    that are more optimized to solve detection and segmentation problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了各种物体检测技术，如R-CNN系列算法、YOLO、SSD以及U-Net和Mask R-CNN图像分割算法。在本章中，我们将进一步学习更适合解决检测和分割问题的框架/架构，处理更现实的场景。
- en: We will start by leveraging the Detectron2 framework to train and detect custom
    objects present in an image. We will also predict the pose of humans present in
    an image using a pre-trained model. Furthermore, we will learn how to count the
    number of people in a crowd in an image and then learn about leveraging segmentation
    techniques to perform image colorization. Next, we will learn about a modified
    version of YOLO to predict 3D bounding boxes around objects by using point clouds
    obtained from a LIDAR sensor. Finally, we will learn about recognizing actions
    from a video.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从利用Detectron2框架开始，训练和检测图像中存在的自定义对象。我们还将使用预训练模型预测图像中人类的姿势。此外，我们将学习如何统计图像中人群的人数，然后学习如何利用分割技术进行图像着色。接下来，我们将学习一个修改版的YOLO，通过使用来自LIDAR传感器的点云来预测物体周围的3D边界框。最后，我们将学习如何从视频中识别动作。
- en: 'By the end of this chapter, you will have learned about the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将学习以下内容：
- en: Multi-object instance segmentation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对象实例分割
- en: Human pose detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人体姿势检测
- en: Crowd counting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人群计数
- en: Image colorization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像着色
- en: 3D object detection with point clouds
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用点云进行3D物体检测
- en: Action recognition from video
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从视频中识别动作
- en: All code snippets within this chapter are available in the `Chapter10` folder
    of the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本章所有代码片段均可在GitHub存储库的`Chapter10`文件夹中找到，链接为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: As the field evolves, we will periodically add valuable supplements to the GitHub
    repository. Do check the `supplementary_sections` folder within each chapter’s
    directory for new and useful content.
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着领域的发展，我们将定期向GitHub存储库添加有价值的补充内容。请查看每个章节目录下的`supplementary_sections`文件夹获取新的有用内容。
- en: Multi-object instance segmentation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多对象实例分割
- en: In previous chapters, we learned about various object detection algorithms.
    In this section, we will learn about the Detectron2 platform ([https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/))
    before we implement multi-object instance segmentation using the Google Open Images
    dataset. Detectron2 is a platform built by the Facebook team. Detectron2 includes
    high-quality implementations of state-of-the-art object detection algorithms,
    including DensePose of the Mask R-CNN model family. The original Detectron framework
    was written in Caffe2, while the Detectron2 framework is written using PyTorch.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了各种物体检测算法。在本节中，我们将学习Detectron2平台（链接：[https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/](https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/)），然后实现使用Google开放图像数据集进行多对象实例分割。Detectron2是由Facebook团队开发的平台。Detectron2包含了最先进的物体检测算法的高质量实现，包括Mask
    R-CNN模型系列的DensePose。原始Detectron框架使用Caffe2编写，而Detectron2框架则使用PyTorch编写。
- en: Detectron2 supports a range of tasks related to object detection. Like the original
    Detectron, it supports object detection with boxes and instance segmentation masks,
    as well as human pose prediction. Beyond that, Detectron2 adds support for semantic
    segmentation and panoptic segmentation (a task that combines both semantic and
    instance segmentation). By leveraging Detectron2, we are able to build object
    detection, segmentation, and pose estimation in a few lines of code.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Detectron2支持与物体检测相关的一系列任务。与原始Detectron类似，它支持具有框和实例分割掩码的物体检测，以及人体姿势预测。除此之外，Detectron2还增加了对语义分割和全景分割（将语义分割和实例分割结合在一起的任务）的支持。通过利用Detectron2，我们能够在几行代码中构建物体检测、分割和姿势估计。
- en: 'In this section, we will:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将：
- en: Fetch the open-images dataset
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取开放图像数据集
- en: Convert the dataset to COCO format
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集转换为COCO格式
- en: Train a model using Detectron2
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Detectron2训练模型
- en: Infer new images on the trained model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模型上推断新图像
- en: Let’s go through each of these.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一进行这些步骤。
- en: Fetching and preparing data
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取和准备数据
- en: We will be working on the images that are available in the Open Images dataset
    (which contains millions of images along with their annotations) provided by Google
    at [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将处理由Google提供的Open Images数据集中可用的图像（其中包含数百万张图像及其注释），网址为[https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)。
- en: 'In this part of the code, we will learn about fetching only the required images
    and not the entire dataset. Note that this step is required, as the dataset size
    prohibits a typical user who might not have extensive resources from building
    a model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码部分，我们将了解仅获取所需图像而不是整个数据集的方法。请注意，此步骤是必需的，因为数据集大小限制了可能没有广泛资源的典型用户构建模型的能力。
- en: The following code can be found in the `Multi_object_segmentation.ipynb` file
    located in the `Chapter10` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub的`Chapter10`文件夹中找到名为`Multi_object_segmentation.ipynb`的文件，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Install the required packages:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的包：
- en: '[PRE0]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Download the required annotations files:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载所需的注释文件：
- en: '[PRE1]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Specify the classes that we want our model to predict (you can visit the Open
    Images website to see the list of all classes):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定我们希望模型预测的类别（您可以访问Open Images网站查看所有类别的列表）：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Fetch the image IDs and masks corresponding to `required_classes`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取与`required_classes`对应的图像ID和掩码：
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Given the vast amount of data, we are only fetching 500 images per class in
    `subset_data`. It is up to you whether you fetch a smaller or larger set of files
    per class and the list of unique classes (`required_classes`).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于数据量巨大，我们只获取`subset_data`中每类500张图像。您可以选择是否获取更小或更大的文件集合以及唯一类别的列表（`required_classes`）。
- en: So far, we only have the `ImageId` and `MaskPath` values corresponding to an
    image. In the next steps, we will go ahead and download the actual images and
    masks from `open-images`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只有与图像对应的`ImageId`和`MaskPath`值。在接下来的步骤中，我们将继续从`open-images`下载实际的图像和掩码。
- en: 'Now that we have the subset of masks data to download, let’s start the download.
    Open Images has 16 ZIP files for training masks. Each ZIP file will have only
    a few masks from `subset_masks`, so we will delete the rest after moving the required
    masks into a separate folder. This *download* -> *move* -> *delete* action will
    keep the memory footprint relatively small. We will have to run this step once
    for each of the 16 files:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好下载子集掩码数据，让我们开始下载。Open Images有16个用于训练掩码的ZIP文件。每个ZIP文件仅包含来自`subset_masks`的少量掩码，因此我们将在将所需掩码移动到单独文件夹后删除其余部分。这一*下载*
    -> *移动* -> *删除*操作将保持内存占用相对较小。我们将对这16个文件每一个运行此步骤一次：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Download the images corresponding to `ImageId`:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载与`ImageId`对应的图像：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Zip all the images, masks, and ground truths and save them – just in case your
    session crashes, it is helpful to save and retrieve the file for later training.
    Once the ZIP file is created, ensure you save the file in your drive or download
    it. The file size ends up being around 2.5 GB:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有图像、掩码和地面真相打包并保存它们——以防会话崩溃时，保存和检索文件以便后续训练是有帮助的。创建ZIP文件后，请确保将文件保存在您的驱动器上或下载它。文件大小最终约为2.5
    GB：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, move the data into a single directory:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将数据移动到单个目录中：
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Given that there are so many moving components in object detection code, as
    a standardization method, Detectron accepts a rigid data format for training.
    While it is possible to write a dataset definition and feed it to Detectron, it
    is easier (and more profitable) to save the entire training data in COCO format.
    This way, you can leverage other training algorithms, such as **detectron transformers**
    (**DETR**), with no change to the data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于目标检测代码中有如此多的移动组件，作为标准化方法，Detectron接受严格的训练数据格式。虽然可以编写数据集定义并将其提供给Detectron，但将整个训练数据保存为COCO格式更简单（也更有利）。这样，您可以利用其他训练算法，例如**detectron
    transformers**（**DETR**），而无需更改数据。
- en: 'First, we will start by defining the categories of classes:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将定义类别：
- en: 'Define the required categories in COCO format:'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以COCO格式定义所需的类别：
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code, in the definition of `CATEGORIES`, we are creating a
    new key called `supercategory`. To understand `supercategory`, let’s go through
    an example: the `Man` and `Woman` classes are categories belonging to the `Person`
    supercategory. In our case, given that we are not interested in supercategories,
    we will specify it as `none`.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上述代码中，在 `CATEGORIES` 的定义中，我们正在创建一个名为 `supercategory` 的新键。要理解 `supercategory`，让我们通过一个例子来看：`Man`
    和 `Woman` 类别属于 `Person` 超类。在我们的情况下，由于我们不关心超类，我们将其指定为 `none`。
- en: 'Import the relevant packages and create an empty dictionary with the keys needed
    to save the COCO JSON file:'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并创建一个带有保存 COCO JSON 文件所需键的空字典：
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Set a few variables in place that contain the information on the image locations
    and annotation file locations:'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置几个包含图像位置信息和注释文件位置信息的变量：
- en: '[PRE10]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Loop through each image filename and populate the `images` key in the `coco_output`
    dictionary:'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环遍历每个图像文件名并填充 `coco_output` 字典中的 `images` 键：
- en: '[PRE11]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Loop through each segmentation annotation and populate the `annotations` key
    in the `coco_output` dictionary:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环遍历每个分割注释并填充 `coco_output` 字典中的 `annotations` 键：
- en: '[PRE12]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Save `coco_output` in a JSON file:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `coco_output` 保存在 JSON 文件中：
- en: '[PRE13]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With this, we have our files in COCO format. Now, we are ready to train our
    model using the Detectron2 framework.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的文件已经按照 COCO 格式准备好了。现在，我们可以使用 Detectron2 框架来训练我们的模型。
- en: Training the model for instance segmentation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型进行实例分割
- en: To train our model, we need to download the required packages, modify the configuration
    file to reflect the dataset paths, and then train the model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练我们的模型，我们需要下载所需的包，修改配置文件以反映数据集路径，然后训练模型。
- en: 'Let’s go through the steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步进行：
- en: 'Install the required Detectron2 packages:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的 Detectron2 包：
- en: '[PRE14]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note: You’ll need to restart Colab before proceeding to the next step.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在进行下一步之前，您需要重新启动 Colab。
- en: 'Import the relevant `detectron2` packages:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的 `detectron2` 包：
- en: '[PRE15]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Given that we have restarted Colab, let’s re-fetch the required classes:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们已重新启动 Colab，请重新获取所需的类：
- en: '[PRE16]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Register the created datasets using `register_coco_instances`:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `register_coco_instances` 注册创建的数据集：
- en: '[PRE17]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define all the parameters in the `cfg` configuration file. Configuration (`cfg`)
    is a special Detectron object that holds all the relevant information for training
    a model:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `cfg` 配置文件中定义所有参数。配置 (`cfg`) 是一个特殊的 Detectron 对象，保存了训练模型所需的所有相关信息：
- en: '[PRE18]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see in the preceding code, you can set up all the major hyperparameters
    needed for training the model. `merge_from_file` imports all the core parameters
    from a pre-existing configuration file that was used for pre-training `mask_rccnn`
    with `FPN` as the backbone. This will also contain additional information on the
    pre-training experiment, such as the optimizer and loss functions. The hyperparameters
    that have been set, for our purpose, in `cfg` are self-explanatory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的代码中所看到的，您可以设置所有用于训练模型所需的主要超参数。`merge_from_file` 从用于使用 `FPN` 作为骨干预训练 `mask_rcnn`
    的预先存在的配置文件中导入所有核心参数。这还将包含有关预训练实验的其他信息，例如优化器和损失函数。在我们的目的中，已在 `cfg` 中设置的超参数是不言自明的。
- en: 'Train the model:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE19]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: With the preceding lines of code, we can train a model to predict classes, bounding
    boxes, and also the segmentation of objects belonging to the defined classes within
    our custom dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述代码行，我们可以训练一个模型来预测属于我们自定义数据集中定义类的对象的类、边界框和分割。
- en: 'Save the model in a folder:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型保存在一个文件夹中：
- en: '[PRE20]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By this point, we have trained our model. In the next section, we will make
    inferences on a new image so that we are able to identify objects in a given image
    using our pre-trained model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经训练好了我们的模型。在下一节中，我们将对新图像进行推断，以便使用我们预训练的模型识别给定图像中的对象。
- en: Making inferences on a new image
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对新图像进行推断
- en: 'To perform inference on a new image, we load the path, set the probability
    threshold, and pass it through the `DefaultPredictor` method, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要对新图像执行推断，加载路径，设置概率阈值，并通过 `DefaultPredictor` 方法传递，如下所示：
- en: 'Load the weights with the trained model. Use the same `cfg` and load the model
    weights as shown in the following code:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型权重加载权重。使用相同的 `cfg` 并按照以下代码加载模型权重：
- en: '[PRE21]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set the threshold for the probability of the object belonging to a certain
    class:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置对象属于某一类的概率阈值：
- en: '[PRE22]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the `predictor` method:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `predictor` 方法：
- en: '[PRE23]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Perform segmentation on the image of interest and visualize it. In the following
    code, we are randomly plotting 30 training images (note that we haven’t created
    validation data; we have left this as an exercise for you), but you can also load
    your own image path in place of `choose(files)`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在感兴趣的图像上执行分割并进行可视化。在以下代码中，我们随机绘制了 30 张训练图像（注意我们尚未创建验证数据；这是你的练习），但你也可以加载自己的图像路径来代替
    `choose(files)`：
- en: '[PRE24]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Visualizer` is Detectron2’s way of plotting object instances. Given that the
    predictions (present in the `outputs` variable) are a mere dictionary of tensors,
    `Visualizer` converts them into pixel information and draws them on an image.
    Let’s see what each input means:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`Visualizer` 是 Detectron2 绘制对象实例的方式。鉴于预测（存在于 `outputs` 变量中）仅是张量字典，`Visualizer`
    将其转换为像素信息并将其绘制在图像上。让我们看看每个输入的含义：'
- en: '`im`: The image we want to visualize.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`im`：我们要可视化的图像。'
- en: '`scale`: The size of the image when plotted. Here, we are asking it to shrink
    the image down to 50%.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`scale`：绘制时图像的大小。在这里，我们要求将图像缩小到 50%。'
- en: '`metadata`: We need class-level information for the dataset, mainly the index-to-class
    mapping so that when we send the raw tensors as input to be plotted, the class
    will decode them into actual human-readable classes.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`metadata`：我们需要数据集的类级信息，主要是索引到类的映射，以便当我们将原始张量作为输入发送时，类将解码为实际可读的类别。'
- en: '`instance_mode`: We are asking the model to only highlight the segmented pixels.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`instance_mode`：我们要求模型仅突出显示分段像素。'
- en: Finally, once the class is created (in our example, it is `v`), we can ask it
    to draw instance predictions coming from the model and show the image.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，一旦类被创建（在我们的例子中是 `v`），我们可以要求它从模型中获取实例预测并显示图像。
- en: 'The preceding code gives the following output. Notice that we are able to identify
    the pixels corresponding to the elephants fairly accurately:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码给出了以下输出。请注意，我们能够准确识别与大象相对应的像素：
- en: '![A picture containing diagram  Description automatically generated](img/B18457_10_01.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![包含图解的图片 自动生成的描述](img/B18457_10_01.png)'
- en: 'Figure 10.1: Instance segmentation prediction'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：实例分割预测
- en: Now that we have learned about leveraging Detectron2 to identify the pixels
    corresponding to classes within an image, in the next section, we will learn about
    leveraging Detectron2 to perform the detection of human poses present in an image.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何利用 Detectron2 来识别图像中类别对应的像素，接下来我们将学习如何利用 Detectron2 执行图像中人体姿势的检测。
- en: Human pose detection
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人体姿势检测
- en: 'In the previous section, we learned about detecting multiple objects and segmenting
    them. Now we will learn about detecting multiple people in an image, as well as
    detecting the keypoints of various body parts of the people present in the image
    using Detectron2\. Detecting keypoints comes in handy in multiple use cases, such
    as in sports analytics and security. For this exercise, we will be leveraging
    the pre-trained keypoint model that is available in the configuration file:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们了解了检测多个对象并对它们进行分割。现在我们将学习在图像中检测多个人物，以及使用 Detectron2 检测图像中各个人体部位的关键点。检测关键点在多种用例中非常有用，例如体育分析和安全领域。对于此练习，我们将利用配置文件中提供的预训练关键点模型：
- en: The following code can be found in the `Human_pose_detection.ipynb` file located
    in the `Chapter10` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 可在 GitHub 上的 `Chapter10` 文件夹中的 `Human_pose_detection.ipynb` 文件中找到以下代码，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Install all the requirements as shown in the previous section:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所有前面部分中显示的要求。
- en: '[PRE25]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Fetch the configuration file and load the pre-trained keypoint detection model
    present in Detectron2:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取配置文件并加载在 Detectron2 中存在的预训练关键点检测模型：
- en: '[PRE26]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Specify the configuration parameters:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定配置参数：
- en: '[PRE27]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Load the image that we want to predict:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载我们想要预测的图像：
- en: '[PRE28]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Predict on the image and plot the keypoints:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行预测并绘制关键点：
- en: '[PRE29]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code gives output as follows. We can see that the model is able
    to identify the various body pose keypoints corresponding to the people in the
    image accurately:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码给出了以下输出。我们可以看到模型能够准确识别图像中人物的各个身体姿势关键点：
- en: '![A group of people sitting in a bus  Description automatically generated with
    low confidence](img/B18457_10_02.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![一群人坐在公共汽车上 自动以低置信度生成的描述](img/B18457_10_02.png)'
- en: 'Figure 10.2: (Left) Input image (Right) Predicted keypoints overlaid on original
    image'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：（左）输入图像（右）预测的关键点叠加在原始图像上
- en: In this section, we have learned how to perform keypoint detection using the
    Detectron2 platform. In the next section, we will learn about implementing a modified
    VGG architecture from scratch to estimate the number of people present in an image.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用Detectron2平台执行关键点检测。在下一节中，我们将学习如何从头开始实现修改后的VGG架构来估计图像中的人数。
- en: Crowd counting
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人群计数
- en: Imagine a scenario where you are given a picture of a crowd and are asked to
    estimate the number of people present in the image. A crowd counting model comes
    in handy in such a scenario. Before we go ahead and build a model to perform crowd
    counting, let’s understand the data available and the model architecture first.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种场景，你被给予一张人群的图片，并被要求估计图片中的人数。在这种情况下，人群计数模型非常有用。在我们继续构建用于执行人群计数的模型之前，让我们先了解一下可用的数据和模型架构。
- en: 'In order to train a model that predicts the number of people in an image, we
    will have to load the images first. The images should constitute the location
    of the center of the heads of all the people present in the image. A sample of
    the input image and the location of the center of the heads of the respective
    people in the image is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个能够预测图像中人数的模型，我们首先需要加载图像。图像应包含图像中所有人的头部中心位置。以下是输入图像及其各自人员头部中心位置的样本：
- en: '![A group of people running  Description automatically generated with medium
    confidence](img/B18457_10_03.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![一群人奔跑 描述自动生成，置信度中等](img/B18457_10_03.png)'
- en: 'Figure 10.3: (Left) Original image (Right) Plot of the center of the heads
    of people in the image'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：（左）原始图像（右）图像中人头中心的绘图
- en: 'Source: ShanghaiTech dataset licensed under BSD 2-Clause “Simplified” License
    (https://github.com/desenzhou/ShanghaiTechDataset)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：上海科技数据集，根据BSD 2-Clause “简化”许可证许可 (https://github.com/desenzhou/ShanghaiTechDataset)
- en: 'In the preceding example, the image representing ground truth (on the right
    – the center of the heads of the people present in the image) is extremely sparse.
    There are exactly *N* white pixels, where *N* is the number of people in the image.
    Let’s zoom in to the top-left corner of the image and see the same map again:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，代表真实数据的图像（右边 - 图像中存在的人头中心）非常稀疏。确切地说，有 *N* 个白色像素，其中 *N* 是图像中的人数。让我们放大到图像的左上角，并再次看看同样的地图：
- en: '![A group of people holding guns  Description automatically generated with
    low confidence](img/B18457_10_04.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![一群人手持枪 描述自动生成，置信度低](img/B18457_10_04.png)'
- en: 'Figure 10.4: Zoomed in version of Figure 10.3'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：图 10.3 的放大版本
- en: 'In the next step, we transform the ground truth sparse image into a density
    map that represents the number of people in that region of the image:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将将地面实况稀疏图像转换为表示该图像区域中人数的密度图：
- en: '![](img/B18457_10_05.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_05.png)'
- en: 'Figure 10.5: (Left) Plot of centers of heads in image (Right) Density map'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：（左）图像中头部中心的绘图（右）密度图
- en: 'The final input-output pair of the same crop would look like this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 同一裁剪的最终输入输出对将如下所示：
- en: '![A group of people  Description automatically generated with low confidence](img/B18457_10_06.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![一群人 描述自动生成，置信度低](img/B18457_10_06.png)'
- en: 'Figure 10.6: (Left) Input (Right) Output of the zoomed in image'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：（左）放大后的输入图像（右）输出图像
- en: 'The final input-output pair for the entire image would look like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 整个图像的最终输入输出对将如下所示：
- en: '![A group of people running in a race  Description automatically generated
    with low confidence](img/B18457_10_07.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![一群人赛跑 描述自动生成，置信度低](img/B18457_10_07.png)'
- en: 'Figure 10.7: (Left) Original input image (Right) Density map'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：（左）原始输入图像（右）密度图
- en: Note that, in the preceding image, when two people are close to each other,
    the pixel intensity is high. However, when a person is far away from the rest,
    the pixel density corresponding to the person is more evenly spread out, resulting
    in a lower pixel intensity corresponding to the person who is far away from the
    rest. Essentially, the heatmap is generated in a way that the sum of the pixel
    values is equal to the number of people present in the image.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在上述图像中，当两个人靠近时，像素强度较高。然而，当一个人远离其他人时，与该人对应的像素密度更均匀分布，导致对该人的像素强度较低。本质上，热图是以这样一种方式生成的，即像素值的总和等于图像中出现的人数。
- en: 'Now that we are in a position to accept an input image and the location of
    the center of the heads of the people in the image (which is processed to fetch
    the ground truth output heatmap), we will leverage the architecture detailed in
    the paper titled *CSRNet: Dilated Convolutional Neural Networks for Understanding
    the Highly Congested Scenes* ([https://arxiv.org/pdf/1802.10062.pdf](https://arxiv.org/pdf/1802.10062.pdf))
    to predict the number of people present in an image. The model architecture is
    as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，我们已经准备好接受输入图像以及图像中人员头部中心位置的位置（这些位置已经被处理以获取地面真实输出热图），我们将利用文章*CSRNet: Dilated
    Convolutional Neural Networks for Understanding the Highly Congested Scenes*（[https://arxiv.org/pdf/1802.10062.pdf](https://arxiv.org/pdf/1802.10062.pdf)）中详细描述的架构来预测图像中出现的人数。模型架构如下：'
- en: '![Table  Description automatically generated](img/B18457_10_08.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述 自动生成](img/B18457_10_08.png)'
- en: 'Figure 10.8: CSRNet architecture'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：CSRNet架构
- en: Notice in the preceding structure of the model architecture that we are passing
    the image through four additional layers of convolutions after first passing it
    through the standard VGG-16 backbone. This output is passed through one of the
    four configurations and finally through a 1 x 1 x 1 convolution layer. We will
    be using the *A* configuration as it is the smallest.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在模型架构的上述结构中，我们首先将图像通过标准的VGG-16骨干网络传递，然后通过四个额外的卷积层。这个输出通过四种配置之一最终通过一个1 x 1
    x 1卷积层。我们将使用*A*配置，因为它是最小的。
- en: 'Next, we perform **Mean Squared Error** (**MSE**) loss minimization on the
    output image to arrive at the optimal weight values while keeping track of the
    actual crowd count using MAE. One additional detail of the architecture is that
    the authors used **dilated convolution** instead of normal convolution. A typical
    dilated convolution looks as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对输出图像执行**均方误差**（**MSE**）损失最小化，以获得最佳的权重值，同时使用MAE跟踪实际的人群计数。架构的一个额外细节是作者使用了**扩张卷积**而不是普通卷积。典型的扩张卷积如下所示：
- en: '![A picture containing table  Description automatically generated](img/B18457_10_09.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![包含表格描述的图片 自动生成](img/B18457_10_09.png)'
- en: 'Figure 10.9: Examples of dilated kernels'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：扩张卷积核示例
- en: 'Source: arXiv:1802.10062 [cs.CV]'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：arXiv:1802.10062 [cs.CV]
- en: The preceding diagram on the left represents a typical kernel that we have been
    working on so far. The diagrams in the middle and on the right represent the dilated
    kernels, which have a gap between individual pixels. This way, the kernel has
    a larger receptive field, which can come in handy as we need to understand the
    number of people near a given person in order to estimate the pixel density corresponding
    to the person. We are using a dilated kernel (of nine parameters) instead of a
    normal kernel (which will have 49 parameters to be equivalent to a dilation rate
    of three kernels) to capture more information with fewer parameters.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的前图表示到目前为止我们一直在工作的典型核心。中间和右侧的图表示的是扩张卷积核，这些核心之间存在像素间隙。这样，卷积核具有更大的感受野，这在我们需要理解靠近给定人员的人数以估算与人员对应的像素密度时非常方便。我们使用扩张卷积核（具有九个参数），而不是正常卷积核（要达到与三个卷积核的膨胀率等效的49个参数），以使用更少的参数捕获更多信息。
- en: With an understanding of how the model is to be architected in place, let’s
    go ahead and code the model to perform crowd counting next.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何设计模型的架构，让我们继续编写模型来进行人群计数。
- en: 'For those of you looking to understand the working details, we suggest you
    go through the paper here: [https://arxiv.org/pdf/1802.10062.pdf](https://arxiv.org/pdf/1802.10062.pdf).
    The model we will be training in the following section is inspired by this paper.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些希望了解工作详细信息的人，我们建议阅读此处的论文：[https://arxiv.org/pdf/1802.10062.pdf](https://arxiv.org/pdf/1802.10062.pdf)。我们将在接下来的部分中训练的模型受到此论文的启发。
- en: Implementing crowd counting
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施人群计数
- en: 'The strategy that we’ll adopt to perform crowd counting is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略执行人群计数如下：
- en: Import the relevant packages and dataset.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集。
- en: The dataset that we will be working on – the ShanghaiTech dataset – already
    has the center of faces converted into a distribution based on Gaussian filter
    density, so we need not perform it again. Map the input image and the output Gaussian
    density map using a network.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将要处理的数据集 – 上海科技数据集 – 已经将人脸中心转换为基于高斯滤波器密度的分布，因此我们无需再次执行。使用网络映射输入图像和输出高斯密度图。
- en: Define a function to perform dilated convolution.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义执行扩张卷积的函数。
- en: Define the network model and train on batches of data to minimize the MSE.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络模型并在数据批次上进行训练以最小化 MSE。
- en: 'Let’s go ahead and code up our strategy as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续按照以下策略进行编码：
- en: The following code can be found in the `crowd_counting.ipynb` file located in
    the `Chapter10` folder on GitHub at `https://bit.ly/mcvp-2e`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码可以在 GitHub 上的 `Chapter10` 文件夹中的 `crowd_counting.ipynb` 文件中找到，链接为 `https://bit.ly/mcvp-2e`。
- en: 'Import the packages and download the dataset:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包并下载数据集：
- en: '[PRE30]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Provide the location of the images (`image_folder`), the ground truth (`gt_folder`),
    and the heatmap folders (`heatmap_folder`):'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供图像的位置（`image_folder`）、地面实况（`gt_folder`）和热图文件夹（`heatmap_folder`）：
- en: '[PRE31]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the training and validation datasets and dataloaders:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和验证数据集以及数据加载器：
- en: '[PRE32]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that the only addition to the typical dataset `class` that we have written
    so far is the lines of code in bold in the preceding code. We are resizing the
    ground truth as the output of our network would be shrunk to 1/8th of the original
    size, and hence we are multiplying the `map` by `64` so that the `sum` of the
    image pixels will be scaled back to the original crowd count.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，到目前为止，我们已编写的典型数据集 `class` 的唯一补充是前述代码中粗体的代码行。我们正在调整地面实况，因为我们网络的输出将缩小到原始大小的
    1/8，并且我们将地图乘以 `64`，以便图像像素的总和将缩放回原始人群计数。
- en: 'Define the network architecture by implementing the following steps:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实现以下步骤来定义网络架构：
- en: 'Define the function that enables dilated convolutions (`make_layers`):'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义使扩张卷积（`make_layers`）成为可能的函数：
- en: '[PRE33]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the network architecture – `CSRNet`:'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络架构 – `CSRNet`：
- en: '[PRE34]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the functions to train and validate a batch of data:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于训练和验证数据批次的函数：
- en: '[PRE35]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Train the model over increasing epochs:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加周期的训练模型：
- en: '[PRE36]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code results in a variation in the training and validation loss
    (here, the loss is the MAE of the crowd count), as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码导致训练和验证损失的变化（这里损失是人群计数的 MAE），如下：
- en: '![Chart, line chart  Description automatically generated](img/B18457_10_10.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图 描述自动生成](img/B18457_10_10.png)'
- en: 'Figure 10.10: Training and validation loss over increasing epochs'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.10: 随着周期增加的训练和验证损失'
- en: 'From the preceding plot, we can see that we are off in our predictions by around
    150 people. We can improve the model in the following two ways:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图表可以看出，我们的预测与实际相差约150人。我们可以通过以下两种方式改进模型：
- en: By using data augmentation and training on crops of the original image
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用数据增强并在原始图像的裁剪上进行训练
- en: By using a larger network (we used the *A* configuration, while *B*, *C*, and
    *D* are larger)
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用更大的网络（我们使用了配置 *A*，而 *B*、*C* 和 *D* 更大）
- en: 'Make inferences on a new image by fetching a test image and normalizing it:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过获取测试图像并对其进行归一化来对新图像进行推断：
- en: '[PRE37]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then pass the image through the trained model:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过训练模型传递图像：
- en: '[PRE38]'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code results in a heatmap (right image) of the input image (left
    image). We can see that the model predicted the heatmap reasonably accurately
    and the prediction count of people is close to the actual value:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码导致输入图像的热图（右图），我们可以看到模型预测的热图相当准确，并且人数预测值接近实际值：
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B18457_10_11.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图 描述自动生成，置信度低](img/B18457_10_11.png)'
- en: 'Figure 10.11: (Left) Input image (Right) Predicted density map and the count
    of people'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图10.11: (左) 输入图像 (右) 预测的密度图和人数计数'
- en: In the next section, we will continue to work on additional applications and
    will leverage a U-Net architecture to colorize an image.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续探讨其他应用，并利用U-Net架构对图像进行着色。
- en: Image colorization
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像着色
- en: Imagine a scenario where you are given a bunch of black-and-white images and
    are asked to turn them into color images. How would you solve this problem? One
    way to solve this is by using a pseudo-supervised pipeline where we take a raw
    image, convert it into black and white images, and treat them as input-output
    pairs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下这样的场景：您被要求将一堆黑白图像变成彩色图像。您会如何解决这个问题？一种解决方法是使用伪监督流水线，其中我们将原始图像转换为黑白图像，并将其视为输入-输出对。
- en: 'We will demonstrate this by leveraging the CIFAR-10 dataset to perform colorization
    on images. The strategy that we will adopt as we code up the image colorization
    network is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用CIFAR-10数据集演示如何对图像进行着色。我们在编写图像着色网络的代码时将采取的策略如下：
- en: Take the original color image in the training dataset and convert it into grayscale
    to fetch the input (grayscale) and output (original colored image) combination.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中获取原始彩色图像，并将其转换为灰度图像以获取输入（灰度）和输出（原始彩色图像）组合。
- en: Normalize the input and output.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 规范化输入和输出。
- en: Build a U-Net architecture.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建U-Net架构。
- en: Train the model over increasing epochs.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加的epochs上训练模型。
- en: 'With the preceding strategy in place, let’s go ahead and code up the model
    as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 有了上述策略，让我们继续编写模型如下：
- en: The following code can be found in the `Image colorization.ipynb` file located
    in the `Chapter10` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub的`Chapter10`文件夹中的`Image colorization.ipynb`文件中找到以下代码，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Install the required packages and import them:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的包并导入它们：
- en: '[PRE39]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Download the dataset and define the training and validation datasets and dataloaders:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集并定义训练和验证数据集以及数据加载器：
- en: 'Download the dataset:'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集：
- en: '[PRE40]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define the training and validation datasets and dataloaders:'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和验证数据集以及数据加载器：
- en: '[PRE41]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'A sample of the input and output images is as follows:'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入和输出图像的示例如下所示：
- en: '[PRE42]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding code results in the following output:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '![A picture containing text, blurry, cat, brick  Description automatically
    generated](img/B18457_10_12.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、模糊、猫、砖块的图片 自动生成描述](img/B18457_10_12.png)'
- en: 'Figure 10.12: (Left) Input image (Right) Colorized image'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '图10.12: (左) 输入图像 (右) 着色后的图像'
- en: Note that CIFAR-10 has images that are 32 x 32 in shape.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 注意CIFAR-10的图像形状为32 x 32。
- en: 'Define the network architecture:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络架构：
- en: '[PRE43]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the model, optimizer, and loss function:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型、优化器和损失函数：
- en: '[PRE44]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the functions to train and validate a batch of data:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和验证数据集以及数据加载器：
- en: '[PRE45]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Train the model over increasing epochs:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加的epochs上训练模型：
- en: '[PRE46]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code generates an output, as follows. We can see that the model
    is able to color the grayscale image reasonably well:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出。我们可以看到模型能够相当好地对灰度图像进行着色：
- en: '![A picture containing text  Description automatically generated](img/B18457_10_13.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本 自动生成描述](img/B18457_10_13.png)'
- en: 'Figure 10.13: (Left) Input image (Middle) Original image (Right) Predicted
    image'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图10.13: (左) 输入图像 (中) 原始图像 (右) 预测图像'
- en: So far, we have learned about leveraging Detectron2 for segmentation and keypoint
    detection, dilated convolutions in crowd counting, and U-Net in image colorization.
    In the next section, we will learn about leveraging YOLO for 3D object detection.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了如何利用Detectron2进行分割和关键点检测，如何在人群计数中使用扩张卷积，以及在图像着色中使用U-Net。在下一节中，我们将学习如何利用YOLO进行3D物体检测。
- en: 3D object detection with point clouds
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用点云进行3D物体检测
- en: We now know how to predict a bounding rectangle on 2D images using algorithms
    that have the core underlying concept of anchor boxes. Let’s learn how the same
    concept can be extended to predict 3D bounding boxes around objects.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何使用具有锚框核心概念的算法来预测2D图像上的边界矩形。让我们学习如何将相同的概念扩展到预测围绕物体的3D边界框。
- en: In a self-driving car, tasks such as pedestrian/obstacle detection and route
    planning cannot happen without knowing the environment. Predicting 3D object locations
    along with their orientations is an important task. Not only is the 2D bounding
    box around obstacles important but knowing the distance from the object, height,
    width, and orientation of the obstacle are also critical to navigating safely
    in the 3D world.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶汽车中，如行人/障碍物检测和路径规划等任务在不了解环境的情况下无法完成。预测3D物体位置及其方向是一项重要任务。不仅2D障碍物周围的边界框重要，还需要知道距离、高度、宽度和障碍物的方向，这对安全导航至关重要。
- en: In this section, we will learn how YOLO is used to predict the 3D orientation
    and position of cars and pedestrians on a real-world dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用YOLO来预测实际数据集中汽车和行人的3D方向和位置。
- en: 'The instructions for downloading the data, training, and testing sets are all
    given in this GitHub repo: [https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions](https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions).
    Given that there are very few openly available 3D datasets, we have chosen the
    most-used dataset for this exercise, which you still need to register for to download.
    We have provided the instructions for registration at the preceding link as well.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据下载、训练和测试集的说明均包含在此GitHub仓库中：[https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions](https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions)。考虑到仅有少数公开可用的3D数据集，我们选择了本练习中最常用的数据集，您仍需要注册才能下载。我们还在前述链接中提供了注册说明。
- en: Theory
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: One of the well-known sensors for collecting real-time 3D data is **Light Detection
    and Ranging** (**LIDAR**). It is a laser mounted on a rotating apparatus that
    fires beams of lasers hundreds of times every second. Another sensor receives
    the reflection of the laser from surrounding objects and calculates how far the
    laser has traveled before encountering an obstruction. Doing this in all directions
    of a car will result in a 3D point cloud of distances that is reflective of the
    environment itself. In the dataset that we will learn about, we have obtained
    the 3D point clouds from specific hardware developed by Velodyne. Let’s understand
    how the input and output are encoded for 3D object detection.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 收集实时3D数据的众所周知传感器之一是**光探测与测距仪**（**LIDAR**）。它是安装在旋转装置上的激光器，每秒发射数百次激光束。另一个传感器接收来自周围物体的激光反射，并计算激光在遇到障碍物前走过的距离。在汽车的所有方向上执行此操作将产生反映环境本身的距离3D点云。在我们即将了解的数据集中，我们从Velodyne开发的特定硬件获得了3D点云。让我们了解3D物体检测的输入和输出是如何编码的。
- en: Input encoding
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入编码
- en: 'Our raw inputs are going to be 3D point clouds presented to us in the form
    of `.bin` files. Each can be loaded as a NumPy array using `np.fromfile(<filepath>)`
    and here’s how the data looks for a sample file:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的原始输入将是以`.bin`文件形式呈现给我们的3D点云。可以使用`np.fromfile(<filepath>)`将每个文件加载为NumPy数组，以下是一个样本文件的数据查看：
- en: '[PRE47]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: These files are found in the `dataset/.../training/velodyne` directory after
    downloading and moving the raw files as per the GitHub repo instructions.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并按照GitHub仓库的说明移动原始文件后，这些文件位于`dataset/.../training/velodyne`目录中。
- en: 'The preceding code gives the following output:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码得到以下输出：
- en: '![Text  Description automatically generated](img/B18457_10_14.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![文本 描述自动生成](img/B18457_10_14.png)'
- en: 'Figure 10.14: Input array'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：输入数组
- en: 'This can be visualized as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化如下所示：
- en: '[PRE48]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code results in the following output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的结果如下：
- en: '![A map of a city  Description automatically generated with low confidence](img/B18457_10_15.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![一个城市地图 描述自动生成（置信度低）](img/B18457_10_15.png)'
- en: 'Figure 10.15: Visualization of input array'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：输入数组的可视化
- en: 'We can convert this information into an image of a bird’s-eye view by performing
    the following steps:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下步骤将这些信息转换为鸟瞰视图图像：
- en: Project the 3D point cloud onto the *XY* plane (ground) and split it into a
    grid with a resolution of 8 cm² per grid cell.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 把3D点云投影到*XY*平面（地面）并分割成8平方厘米分辨率的网格单元。
- en: 'For each cell, compute the following and associate them with the specified
    channel:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个单元格，计算以下内容并将其与指定的通道关联：
- en: 'Red channel: The height of the highest point in the grid'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 红色通道：网格中最高点的高度
- en: 'Green channel: The intensity of the highest point in the grid'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绿色通道：网格中最高点的强度
- en: 'Blue channel: The number of points in the grid divided by 64 (which is a normalizing
    factor)'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 蓝色通道：网格中点数除以 64（这是一个标准化因子）
- en: 'For example, the reconstructed top view of the cloud may look like this:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，重建的云顶视图可能如下所示：
- en: '![](img/B18457_10_16.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_16.png)'
- en: 'Figure 10.16: Bird’s eye view of input image'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 第 10.16 图：图像输入的鸟瞰视图
- en: You can clearly see the “shadows” in the image, indicating that there is an
    obstacle. This is how we create an image from the LIDAR point cloud data.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以清楚地看到图像中的“阴影”，表明存在障碍物。这是我们从 LIDAR 点云数据创建图像的方式。
- en: We have taken 3D point clouds as the raw input and obtained the bird’s-eye image
    as the output. This is the preprocessing step necessary to create the image that
    is going to be the input for the YOLO model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将 3D 点云作为原始输入，并获得了鸟瞰图像作为输出。这是创建将作为 YOLO 模型输入的图像所必需的预处理步骤。
- en: Output encoding
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出编码
- en: 'Now that we have the bird’s-eye image (of the 3D point cloud) as input to the
    model, the model needs to predict the following real-world features:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了鸟瞰图像（3D 点云）作为模型的输入，模型需要预测以下真实世界的特征：
- en: What the object (**class**) present in the image is
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中目标物体的**类别**
- en: How far the object is (in meters) from the car on the east-west axis (**x**)
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标物体距车辆东西轴（**x** 轴）的距离（以米为单位）
- en: How far the object is (in meters) from the car on the north-south axis (**y**)
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标物体距车辆南北轴（**y** 轴）的距离（以米为单位）
- en: What the orientation of the object (**yaw**)is
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标物体的方向（**偏航角**）
- en: How big the object is (the **length** and **width** of the object in meters)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标物体的大小（以米为单位的**长度**和**宽度**）
- en: It is possible to predict the bounding box in the pixel coordinate system (of
    the bird’s-eye image), but it does not have any real-world significance as the
    predictions would still be in pixel space (in a bird’s-eye view). In this case,
    we need to convert these pixel coordinate (of the bird’s-eye view) bounding box
    predictions into real-world coordinates in meters. To avoid additional steps during
    postprocessing, we are directly predicting the real-world values.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在鸟瞰图像的像素坐标系中预测边界框，但这些预测仍然在像素空间中，没有真实世界的意义。在这种情况下，我们需要将这些鸟瞰图像的像素坐标边界框预测转换为以米为单位的真实世界坐标。为了避免后处理过程中的额外步骤，我们直接预测真实世界的值。
- en: 'Furthermore, in a realistic scenario, the object could be oriented in any direction.
    If we only calculate the length and width, it will not be sufficient to describe
    the tight bounding box. An example of such a scenario is as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在现实场景中，目标物体可以朝任意方向定位。如果我们仅计算长度和宽度，则无法足以描述紧凑边界框。一个这样的场景示例如下：
- en: '![Diagram, engineering drawing  Description automatically generated](img/B18457_10_17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图，工程图 描述自动生成](img/B18457_10_17.png)'
- en: 'Figure 10.17: Bounding box representation'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 第 10.17 图：边界框表示
- en: To get a tight bounding box for the object, we also need the information on
    which direction the obstacle is facing, and hence we also need the additional
    yaw parameter. Formally, it is the orientation of the object with the north-south
    axis.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得目标物体的紧凑边界框，我们还需要了解障碍物面向的方向信息，因此还需要额外的偏航参数。严格来说，它是物体相对南北轴的方向。
- en: 'First, the YOLO model uses an anchor grid of 32 x 64 cells (more width than
    height), taking into consideration that the car’s dashcam (and hence LIDAR) views
    are wider than they are tall. The model uses two losses for the task. The first
    one is the normal YOLO loss (which is responsible for predicting the *x*, *y*,
    *l*, *w* and class) we learned about in *Chapter 8*, *Advanced Object Detection*,
    and another loss called the Euler loss, which exclusively predicts the yaw. Formally,
    the set of equations to predict the final bounding boxes from the model’s outputs
    is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，YOLO 模型使用一个 32 x 64 的锚点网格（宽度大于高度），考虑到汽车的仪表板摄像头（因此也包括 LIDAR）视野比高度更宽。该模型为此任务使用了两种损失。第一种是正常的
    YOLO 损失（负责预测 *x*、*y*、*l*、*w* 和类别），我们在第 8 章的 *高级目标检测* 中学到的，还有一种称为欧拉损失，专门预测偏航角。正式来说，从模型输出预测最终边界框的方程组如下：
- en: '![](img/B18457_10_001.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_001.png)'
- en: '![](img/B18457_10_002.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_002.png)'
- en: '![](img/B18457_10_003.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_003.png)'
- en: '![](img/B18457_10_004.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_004.png)'
- en: '![](img/B18457_10_005.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_005.png)'
- en: Here, *b*[x], *b*[y], *b*[w], *b*[l], and *b*[φ] are the *x* and *y* coordinate
    values, the width, the length, and the yaw of the obstacle, respectively. *t*[x],
    *t*[y], *t*[w], *t*[l], *t*[Im], and *t*[Re] are the six regression values that
    are being predicted from Note that even though there are only 5 values to be predicted,
    the angle φ is being predicted using two auxilliary values *t*[Im] and *t*[Re],
    which represent imaginary and real targets, respectively. These are just names
    used by the official implementation and are essentially trying to calculate *b*[φ]
    using the preceding arctan formula. *c*[x] and *c*[y] are the positions of the
    center of the grid cell within the 32 x 64 matrix and *p**w* and *p**l* are pre-defined
    priors chosen by taking the average widths and lengths of cars and pedestrians.
    Furthermore, there are five priors (anchor boxes) in the implementation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*b*[x]、*b*[y]、*b*[w]、*b*[l]和*b*[φ]分别是障碍物的*x*和*y*坐标值、宽度、长度和偏航角。*t*[x]、*t*[y]、*t*[w]、*t*[l]、*t*[Im]和*t*[Re]是从中预测的六个回归值。请注意，虽然只有5个值需要预测，但角度φ使用两个辅助值*t*[Im]和*t*[Re]进行预测，分别代表虚部和实部目标。这些只是官方实现中使用的名称，实质上是在尝试使用前述的arctan公式计算*b*[φ]。*c*[x]和*c*[y]是32
    x 64矩阵内网格单元中心的位置，*p**w*和*p**l*是预定义的先验，通过取汽车和行人的平均宽度和长度确定。此外，实现中还有五个先验（锚定框）。
- en: The height of each object of the same class is assumed to be a fixed number.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 假设相同类别的每个物体的高度为固定数值。
- en: 'Refer to the illustration given here, which shows this pictorially:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 参考此处给出的插图，以图像方式展示：
- en: '![Diagram  Description automatically generated](img/B18457_10_18.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18457_10_18.png)'
- en: 'Figure 10.18: Bounding box regression Source: arXiv:1803.06199v2 [cs.CV]'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.18：边界框回归 源自：arXiv:1803.06199v2 [cs.CV]
- en: 'The total loss is calculated as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 总损失计算如下：
- en: '![](img/B18457_10_006.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_006.png)'
- en: 'You already know *Loss*[YOLO] from the previous chapter (using *t*[x], *t*[y],
    *t*[w], and *t*[l] as the targets). Also, note the following:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经从上一章了解到*Loss*[YOLO]（使用*t*[x]、*t*[y]、*t*[w]和*t*[l]作为目标）。同时，请注意以下内容：
- en: '![](img/B18457_10_007.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_007.png)'
- en: '![](img/B18457_10_008.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_008.png)'
- en: Now that we have understood that the fundamentals of 3D object detection are
    the same as those of 2D object detection (but with more parameters to predict)
    and the input-output pairs of this task, let’s leverage an existing GitHub repo
    to train our model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了3D物体检测的基础与2D物体检测相同（但要预测更多参数），以及此任务的输入输出对，让我们利用现有的GitHub仓库来训练我们的模型。
- en: For more details on 3D object detection, refer to the paper *Complex-YOLO* at
    [https://arxiv.org/pdf/1803.06199.pdf](https://arxiv.org/pdf/1803.06199.pdf).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于3D物体检测的细节，请参考论文*Complex-YOLO*，链接地址为[https://arxiv.org/pdf/1803.06199.pdf](https://arxiv.org/pdf/1803.06199.pdf)。
- en: Training the YOLO model for 3D object detection
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练YOLO模型进行3D物体检测
- en: The coding effort is largely taken away from the user due to the standardized
    code. Much like Detectron2, we can train and test the algorithm by ensuring that
    the data is in the right format in the right location. Once that is ensured, we
    can train and test the code with a minimal number of lines.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的代码大大减少了用户的编码工作量。与Detectron2类似，通过确保数据位于正确位置并处于正确格式，我们可以用最少的代码行训练和测试算法。一旦确保了这些，我们就可以进行训练和测试。
- en: 'We need to clone the `Complex-YOLOv4-Pytorch` repository first:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要克隆`Complex-YOLOv4-Pytorch`代码库：
- en: '[PRE49]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Follow the instructions in the `README.md` file to download and move the datasets
    to the right locations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 按照`README.md`文件中的说明下载并移动数据集到正确的位置。
- en: 'The instructions for downloading the data, training, and testing sets are all
    given in this GitHub repo: [https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions](https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions).
    Given that there are very few openly available 3D datasets, we have chosen the
    most-used dataset for this exercise, which you still need to register in order
    to download. We also give the instructions for registration at the preceding link.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 此GitHub仓库中提供了关于下载数据、训练和测试集的所有说明：[https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions](https://github.com/sizhky/Complex-YOLOv4-Pytorch/blob/master/README.md#training-instructions)。考虑到目前仅有少量开放的3D数据集，我们选择了这个练习中最常用的数据集，但您仍需注册才能下载。我们还在前述链接提供了注册说明。
- en: Data format
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据格式
- en: We can use any 3D point cloud data with ground truths for this exercise. Refer
    to the `README` file in the GitHub repository for more instructions on how to
    download and move the data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用任何具有地面真实值的3D点云数据来进行此练习。有关如何下载和移动数据的更多说明，请参阅GitHub存储库中的`README`文件。
- en: 'The data needs to be stored in the following format in the root directory:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 数据需要以以下格式存储在根目录中：
- en: '![Text  Description automatically generated](img/B18457_10_19.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![文本说明自动生成](img/B18457_10_19.png)'
- en: 'Figure 10.19: Data storage format'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.19：数据存储格式
- en: 'The three folders that are new to us are `velodyne`, `calib`, and `label_2`:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们而言，三个新文件夹是`velodyne`、`calib`和`label_2`：
- en: '`velodyne` contains a list of `.bin` files that encode 3D point cloud information
    for corresponding images present in the `image_2` folder.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`velodyne`包含编码为`.bin`文件的3D点云信息列表，这些信息对应于`image_2`文件夹中的图像。'
- en: '`calib` contains calibration files corresponding to each point cloud. The 3D
    coordinates from the LIDAR point cloud coordinate system can be projected onto
    the camera coordinate system – that is, the image – by using the 3 x 4 projection
    matrix present in each file in the `calib` folder. Essentially, the LIDAR sensor
    captures the points that are slightly offset from what the camera is capturing.
    This offset is due to the fact that the sensors are mounted a few inches apart
    from each other. Knowing the right offsets will help us to correctly project bounding
    boxes and 3D points onto the image from the camera.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calib`包含每个点云对应的校准文件。来自LIDAR点云坐标系的三维坐标可以通过位于`calib`文件夹中每个文件中的3 x 4投影矩阵投影到相机坐标系（即图像）上。基本上，LIDAR传感器捕捉的点略微偏离相机捕捉的点。这种偏移是因为传感器彼此之间相距几英寸。知道正确的偏移量将帮助我们正确地从相机将边界框和三维点投影到图像上。'
- en: '`label_2` contains the ground truths (one ground truth per line) for each image
    in the form of 15 values that are explained in the following table:'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label_2`包含每个图像的地面真实值（每行一个地面真实值），格式为15个值，这些值在下表中有解释：'
- en: '![Table  Description automatically generated](img/B18457_10_20.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![表格说明自动生成](img/B18457_10_20.png)'
- en: 'Figure 10.20: Sample ground truth values'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.20：样本地面真实值
- en: Note that our target columns are type (class), *w*, *l*, *x*, *z*, and *ry*
    (yaw) among the ones seen here. We will ignore the rest of the values for this
    task.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的目标列是类型（类）、*w*、*l*、*x*、*z*和*y*（偏航），如此处所示。对于此任务，我们将忽略其余的值。
- en: Data inspection
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据检查
- en: 'We can verify that the data is downloaded properly by running the following:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下内容来验证数据是否已正确下载：
- en: '[PRE50]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The preceding code shows multiple images, one image at a time. The following
    is one such example (image source: [https://arxiv.org/pdf/1803.06199.pdf](https://arxiv.org/pdf/1803.06199.pdf)):'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码显示了多个图像，每次显示一张图像。以下是这样一个示例（图像来源：[https://arxiv.org/pdf/1803.06199.pdf](https://arxiv.org/pdf/1803.06199.pdf)）：
- en: '![](img/B18457_10_21.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_21.png)'
- en: 'Figure 10.21: Input image with the corresponding ground truth'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.21：输入图像及其对应的地面真实值
- en: 'Source: arXiv:1803.06199v2 [cs.CV]'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：arXiv:1803.06199v2 [cs.CV]
- en: Now that we are able to download and view a few images, in the next section,
    we will learn about training the model to predict 3D bounding boxes.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经能够下载并查看一些图像，在下一节中，我们将学习如何训练模型以预测3D边界框。
- en: Training
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练
- en: 'The training code is wrapped in a single Python file and can be called as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码包装在一个单独的Python文件中，并可按以下方式调用：
- en: '[PRE51]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The default number of epochs is 300, but the results are fairly reasonable starting
    at the fifth epoch itself. Each epoch takes 30 to 45 minutes on a GTX 1070 GPU.
    You can use `--resume_path` to resume training if training cannot be done in a
    single stretch. The code saves a new checkpoint every five epochs.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的训练周期数为300，但从第五个周期开始结果就相当合理了。每个周期在GTX 1070 GPU上需要30到45分钟的时间。如果无法一次性完成训练，可以使用`--resume_path`来恢复训练。代码每五个周期保存一个新的检查点。
- en: Testing
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试
- en: 'Just like in the preceding *Data inspection* section, the trained model can
    be tested with the following code:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面的*数据检查*部分一样，训练好的模型可以使用以下代码进行测试：
- en: '[PRE52]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The main inputs to the code are the checkpoint path and the model configuration
    path. After giving them and running the code, the following output pops up (image
    source: [https://arxiv.org/pdf/1803.06199.pdf](https://arxiv.org/pdf/1803.06199.pdf)):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的主要输入是检查点路径和模型配置路径。在指定它们并运行代码后，会出现以下输出（图像来源：[https://arxiv.org/pdf/1803.06199.pdf](https://arxiv.org/pdf/1803.06199.pdf)）：
- en: '![](img/B18457_10_22.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_10_22.png)'
- en: 'Figure 10.22: Input image with the corresponding predicted labels and bounding
    boxes'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: arXiv:1803.06199v2 [cs.CV]'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Because of the simplicity of the model, we can use it in real-time scenarios
    with a normal GPU, getting about 15–20 predictions per second.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learnt about scenarios where we take an image/frame as input
    and predict the class/ object/bounding box. What if we want to recognize an event
    from a video (or a sequence of frames)? Let’s focus on this in the next section.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Action recognition from video
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s now learn how to use the MMAction toolbox ([https://github.com/open-mmlab/mmaction](https://github.com/open-mmlab/mmaction))
    from the open-mmlab project to perform action recognition. The major features
    of MMAction are:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Action recognition on trimmed videos (portion of the video that has an action)
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temporal action detection (action localization) in untrimmed videos
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial (parts of a frame that indicate an action) and temporal (variation of
    action across frames) action detection in untrimmed videos
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for various action datasets
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for multiple action understanding frameworks
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let us understand how action recognition works. A video is a collection
    of images that are spaced over time (frames). We have two options of model input
    – 2D and 3D. 2D model input has a dimension of FxCHW where F is the number of
    frames and C, H, W are channels, height, and width respectively. 3D model input
    has an input dimension of CFHW.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: In the case of 2D model input, we pass the video (set of frames) through the
    backbones that we learned about in *Chapter 5* (VGG16, ResNet50) to obtain the
    intermediate layers. We next pass the intermediate outputs through temporal convolution
    to aggregate the information of what is happening in each frame. In the case of
    3D model input, we pass it through a 3D model backbone like ResNet3D that can
    inherently process the temporal dimension along with spatial dimensions to fetch
    the intermediate layers.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: Next, we pass the output (across all frames) through a pooling layer (so that
    the model processes on a reduced dimension that captures the key features) to
    obtain the penultimate layer, which is then used to predict the different classes.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, font, diagram  Description automatically
    generated](img/B18457_10_23.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.23: Action recognition workflow'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'A survey of the different methods of performing action recognition is provided
    here: [https://arxiv.org/pdf/2010.11757.pdf](https://arxiv.org/pdf/2010.11757.pdf)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an understanding of how action classification is done on a
    video, let’s perform the following:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Leverage MMAction out of the box to identify an action in a given video
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train MMAction to recognize an action in a custom dataset
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s get started using MMAction.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Identifying an action in a given video
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To identify an action in a given video using MMAction, perform the following
    steps:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `action_recognition.ipynb` file located
    in the `Chapter10` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以在 GitHub 上的 `Chapter10` 文件夹中的 `action_recognition.ipynb` 文件中找到，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Install the dependencies. We’ll install `pytorch` (version 2.2.1+u121) and
    install MIM 0.3.9\. MIM provides a unified interface for launching and installing
    OpenMMLab projects and their extensions and managing the OpenMMLab model zoo.
    First, install `openmim`, `mmengine`, and `mmcv`. We will then install `mmaction`
    and its dependencies:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装依赖项。我们将安装 `pytorch`（版本 2.2.1+u121）并安装 MIM 0.3.9。MIM 提供了一个统一的界面，用于启动和安装 OpenMMLab
    项目及其扩展，并管理 OpenMMLab 模型库。首先安装 `openmim`，`mmengine` 和 `mmcv`。然后我们将安装 `mmaction`
    及其依赖项：
- en: '[PRE53]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Download the pre-trained checkpoints:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载预训练的检查点：
- en: '[PRE54]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Import the required packages:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE55]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Initialize the recognizer:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化识别器：
- en: '[PRE56]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Much like `detectron2` and `huggingface`, `mmaction` is a library that uses
    config files to create models, dataloaders, and pipelines, as well as trainers.
    We will continue to leverage it in this and the next section.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 像 `detectron2` 和 `huggingface` 一样，`mmaction` 是一个使用配置文件创建模型、数据加载器和流水线以及训练器的库。在这一部分和下一部分中，我们将继续利用它。
- en: '[PRE57]'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The model essentially has two components – a ResNet backbone that takes in a
    tensor of shape [F x 3 x H x W] and returns a feature vector of shape [F x 2048
    x 7 x 7]. The head converts this tensor by first averaging each of the 7x7 feature
    maps. This returns [F x 2048 x 1 x 1]. In the next step, the frames are averaged
    out and the computed tensor will be of shape [1 x 2048 x 1 x 1]. This is flattened
    into [1 x 2048], which is sent through a linear layer that finally returns the
    [1 x 400] tensor where 400 is the number of classes.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 模型基本上有两个组件 - 一个 ResNet 主干，接受形状为 [F x 3 x H x W] 的张量并返回形状为 [F x 2048 x 7 x 7]
    的特征向量。头部通过首先对每个 7x7 特征图进行平均来转换这个张量。这将返回形状为 [F x 2048 x 1 x 1] 的张量。在下一步中，帧被平均，计算出的张量将是形状为
    [1 x 2048 x 1 x 1]。这被展平为 [1 x 2048]，然后通过一个线性层，最终返回形状为 [1 x 400] 的张量，其中 400 是类别数。
- en: 'Use the recognizer to perform inference:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用识别器进行推理：
- en: '[PRE58]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `inference_recognizer` function is a wrapper around video preprocessing
    and model.forward where the video is loaded as a NumPy array, the frames are resized
    and reshaped, and the dimensions are set appropriately for the model to accept
    a [F x 3 x H x W] tensor.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '`inference_recognizer` 函数是对视频预处理和模型前向传播的封装，其中视频被加载为 NumPy 数组，帧被调整大小和重塑，尺寸被设置为模型接受的
    [F x 3 x H x W] 张量格式。'
- en: 'Print the predictions:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印预测结果：
- en: '[PRE59]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![A picture containing text, font, white, receipt  Description automatically
    generated](img/B18457_10_24.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、字体、白色、收据 由算法自动生成的说明](img/B18457_10_24.png)'
- en: 'Figure 10.24: Predicted actions'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.24：预测的动作
- en: We used the simple trick of treating the frames as batch dimensions. This way,
    the functionality of ResNet does not change. A new head is used to average out
    the frames into a single value and the tensor can be simply used to perform classification
    with cross-entropy loss. As you can see, we used a relatively straightforward
    path to extend our knowledge of image processing to also perform video classification.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个简单的技巧，将帧视为批处理维度。这样，ResNet 的功能不会改变。一个新的头部用于将帧平均为单个值，然后可以简单地使用张量执行带有交叉熵损失的分类。正如您所看到的，我们使用了一个相对简单的路径，将我们对图像处理的知识扩展到了视频分类。
- en: Training a recognizer on a custom dataset
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在自定义数据集上训练识别器
- en: Now that we have learned how to leverage existing architecture for video classification,
    let’s take this a step further and train the same model on a binary classification
    video dataset of our own. Note that this can be extended to video classification
    of any number of classes.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何利用现有的架构进行视频分类，让我们进一步在我们自己的二分类视频数据集上训练相同的模型。请注意，这可以扩展到任意数量的类别的视频分类。
- en: 'To train a new recognizer, we need to perform the following:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个新的识别器，我们需要执行以下步骤：
- en: 'Let’s download a small subset of the dataset present at [https://research.google/pubs/the-kinetics-human-action-video-dataset/](https://research.google/pubs/the-kinetics-human-action-video-dataset/):'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们下载位于 [https://research.google/pubs/the-kinetics-human-action-video-dataset/](https://research.google/pubs/the-kinetics-human-action-video-dataset/)
    的数据集的一个小子集：
- en: '[PRE60]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The preceding code downloads 40 videos – 30 into the training dataset and 10
    into the validation dataset.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码下载了 40 个视频 - 30 个用于训练数据集，10 个用于验证数据集。
- en: The task is binary video classification where there are two classes – “Climbing
    Rope” (0) and “Blowing Glass” (1)
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是二进制视频分类，其中有两个类别 - “攀登绳索”（0）和“吹玻璃”（1）。
- en: 'Check the annotation format:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查注释格式：
- en: '[PRE61]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![A picture containing text, font, white, typography  Description automatically
    generated](img/B18457_10_25.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、字体、白色、排版说明自动生成的图片](img/B18457_10_25.png)'
- en: 'Figure 10.25: Input video path and the corresponding class'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.25：输入视频路径及其对应的类别
- en: Each line above indicates the file path and the label corresponding to it.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 上述每行均指示文件路径及其相应的标签。
- en: 'Modify the config file for training by implementing the following:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实施以下内容修改配置文件以进行训练：
- en: 'Initialize the configuration file:'
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化配置文件：
- en: '[PRE62]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As mentioned in the previous section, we will use the config file to create
    the trainer class (also called the runner).
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前文所述，我们将使用配置文件创建训练器类（也称为运行器）。
- en: 'We will modify the configuration values by changing the defaults to fit our
    use case. The names of the variables should be self-explanatory:'
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过更改默认值来修改配置值以适应我们的用例。这些变量的名称应该是自说明的：
- en: '[PRE63]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, we’ll create a runner class and use it to train the recognizer:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个运行器类，并使用它来训练识别器：
- en: '[PRE64]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The preceding code results in a training accuracy of 100%.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码导致训练准确率达到100%。
- en: Note
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Classes like `Runner` in `mmaction`; `Trainer` in `huggingface`, `pytorch lightning`,
    `pytorch-ignite`, `tensorflow`, and `detectron2`; and `Learner` in `fastai` are
    all wrappers for the core training components.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 `mmaction` 中的 `Runner`，`huggingface`、`pytorch lightning`、`pytorch-ignite`、`tensorflow`
    和 `detectron2` 中的 `Trainer`，以及 `fastai` 中的 `Learner`，都是核心训练组件的封装。
- en: '`optimizer.zero_grad()`'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer.zero_grad()`'
- en: '`model.train()`'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.train()`'
- en: '`pred = model(inputs)`'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pred = model(inputs)`'
- en: '`loss = loss_fn(pred, target)`'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss = loss_fn(pred, target)`'
- en: '`loss.backward()`'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss.backward()`'
- en: '`optimizer.step()`'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer.step()`'
- en: The differences in each of the libraries’ functionalities are merely in the
    way they were implemented. Their functionalities are almost identical and exploring
    them is a good exercise for you to understand how to write good deep learning
    code.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 每个库的功能差异仅仅在于它们的实现方式。它们的功能几乎相同，探索它们是你理解如何编写优秀深度学习代码的好方法。
- en: 'Finally, we test the recognizer:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们测试识别器：
- en: '[PRE65]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The preceding results in an accuracy of 90% in Top1-accuracy and 100% in Top5-accuracy.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 上述操作导致Top1准确率为90%，Top5准确率为100%。
- en: Summary
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the various practical aspects of dealing with
    object localization and segmentation. Specifically, we learned about how the Detectron2
    platform is leveraged to perform image segmentation and detection, and keypoint
    detection. In addition, we also learned about some of the intricacies involved
    in working with large datasets when we were working on fetching images from the
    Open Images dataset. Next, we worked on leveraging the VGG and U-Net architectures
    for crowd counting and image colorization, respectively. Then, we understood the
    theory and implementation steps behind 3D object detection using point cloud images.
    Finally, we understood ways of performing classification exercises on a sequence
    of frames (video). As you can see from all these examples, the underlying basics
    are the same as those described in the previous chapters, with modifications only
    in the input/output of the networks to accommodate the task at hand.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了处理目标定位和分割的各种实用方面。具体来说，我们学习了如何利用Detectron2平台进行图像分割和检测以及关键点检测。此外，我们还了解了在处理从Open
    Images数据集获取图像时涉及的某些复杂性。接下来，我们利用VGG和U-Net架构分别进行了人群计数和图像着色的工作。然后，我们理解了使用点云图像进行3D对象检测的理论和实现步骤。最后，我们了解了如何对一系列帧（视频）执行分类练习的方法。正如从所有这些示例中可以看出的那样，底层基础知识与前几章中描述的基本相同，只是网络的输入/输出有所修改以适应手头的任务。
- en: In the next chapter, we will switch gears and learn about image encoding, which
    helps in identifying similar images as well as generating new images.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向学习图像编码，这有助于识别相似图像以及生成新图像。
- en: Questions
  id: totrans-394
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why is it important to convert datasets into a specific format for Detectron2?
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为何将数据集转换为Detectron2特定格式是重要的？
- en: It is hard to directly perform a regression of the number of people in an image.
    What is the key insight that allowed the VGG architecture to perform crowd counting?
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接对图像中的人数进行回归是困难的。VGG架构之所以能够进行人群计数的关键见解是什么？
- en: Explain self-supervision in the case of image-colorization.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释在图像着色案例中的自监督学习。
- en: How did we convert a 3D point cloud into an image that is compatible with YOLO?
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们是如何将3D点云转换成与YOLO兼容的图像的？
- en: What is a simple way to handle videos using architectures that work only with
    images?
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何使用仅适用于图像的架构处理视频的简单方法？
- en: Learn more on Discord
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多信息。
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
