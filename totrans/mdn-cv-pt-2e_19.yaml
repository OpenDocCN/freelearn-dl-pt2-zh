- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining Computer Vision and NLP Techniques
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about applications that combine reinforcement
    learning and computer vision. In this chapter, we will switch gears and learn
    about how a **convolutional neural network** (**CNN**) can be used in conjunction
    with algorithms in the broad family of **transformers**, which are heavily used
    (as of the time of writing this book) in **natural language processing** (**NLP**)
    to develop solutions that leverage both computer vision and NLP.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: To understand combining CNNs and transformers, we will first learn how **vision
    transformers** (**ViTs**) work and how they help in performing image classification.
    After that, we will learn about leveraging transformers to perform the transcription
    of handwritten images using **Transformer optical character recognition** (**TrOCR**).
    Next, we will learn about combining transformers and OCR to perform question answering
    on document images using a technique named **LayoutLM**. Finally, we will learn
    about performing visual question answering using a transformer architecture named
    **Bootstrapping Language Image Pre-training** (**BLIP2**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will have learned about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ViTs for image classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing LayoutLM for document question answering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcribing handwritten images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual question answering using BLIP2
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are advised to go through the supplemental chapter on training with minimal
    data points to get familiarity with word embeddings, available in the `Extra chapters
    from first edition` folder on GitHub.
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The code in this chapter is available in the `Chapter15` folder of this book’s
    GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: As the field evolves, we will periodically add valuable supplements to the GitHub
    repository. Do check the `supplementary_sections` folder within each chapter’s
    directory for new and useful content.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transformers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we learn about ViTs, let us understand transformers from an NLP perspective.
    A transformer helps in generating a representation (word/vector embedding) that
    best describes the word given its context (surrounding words). Some of the major
    limitations of **recurrent neural networks** (**RNNs**) and **long short-term
    memory** (**LSTM**) architecture (detailed information about which is provided
    in the associated GitHub repository) are:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: A word embedding corresponding to a word is not dependent on the context in
    which the word appears (the word *apple* will have the same embedding irrespective
    of whether the context is about the fruit or the company).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden state calculation during training happens sequentially (a word’s hidden
    state is dependent on the previous word’s hidden state and thus can only be calculated
    after the previous hidden state is calculated), resulting in a considerable time
    taken to process text.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers address these two major limitations, which results in:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The ability to pre-train transformers on a large corpus of data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用大型数据语料库对变压器进行预训练的能力
- en: Fine-tuning transformers to a variety of downstream tasks (including leveraging
    them for vision tasks)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将变压器微调到各种下游任务中（包括利用它们进行视觉任务）
- en: Leveraging the intermediate states/hidden states in a variety of architectures
    – encoder-only, decoder-only, or encoder-decoder architectures (more on encoders
    and decoders in the following section)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在各种架构中利用中间状态/隐藏状态 – 仅编码器、仅解码器或编码器-解码器架构（更多关于编码器和解码器的内容在以下章节中）
- en: The ability to train transformer outputs in parallel when compared to sequentially
    in RNN
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比于RNN的顺序训练，训练变压器输出可以并行进行
- en: With the advantages of transformers in place, let us understand how they work.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 充分利用变压器的优势，让我们了解它们的工作原理。
- en: Basics of transformers
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器的基础
- en: To understand transformers, let us go through a scenario of machine translation
    – where the source language is English and the target language is French.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解变压器，让我们通过一个机器翻译的场景来了解 – 源语言为英语，目标语言为法语。
- en: 'A transformer architecture can be illustrated as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构可以如下所示：
- en: '![Diagram  Description automatically generated](img/B18475_15_01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18475_15_01.png)'
- en: 'Figure 15.1: Transformer architecture'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1：变压器架构
- en: '(Source: [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762))'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)）
- en: In the above architecture, the left-hand-side block is the encoder and the right-hand-side
    one is the decoder. Let us first understand the encoder block.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述架构中，左侧块是编码器，右侧块是解码器。让我们先了解编码器块。
- en: Encoder block
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器块
- en: The first step is to fetch the tokens corresponding to the input sentence in
    English. In traditional language modeling, we assign an “unknown” token to rare
    words and fetch the embeddings corresponding to the remaining (frequent) words.
    However, during the tokenization process of transformer architecture, we perform
    byte pair encoding (tokenization) in such a way that we break individual words
    (for example, the word `anand` could be broken into `###an`, `###and`, while a
    frequent word like `computer` would remain as is). This way, we would not have
    any unknown words. Additionally, each token would then have an embedding associated
    with it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取对应于英文输入句子的标记。在传统语言建模中，我们给罕见单词分配一个“未知”标记，并获取与其余（频繁的）单词对应的嵌入。然而，在变压器架构的标记化过程中，我们执行字节对编码（标记化），以一种使个别单词分解的方式（例如，单词
    `anand` 可能会分解为 `###an`、`###and`，而像 `computer` 这样的频繁单词则保持原样）。这样，我们就不会有任何未知单词。此外，每个标记随后将具有相关联的嵌入。
- en: Thus, we leverage tokenization to obtain the input word embeddings.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们利用标记化来获得输入单词嵌入。
- en: 'Let’s now learn about the **self-attention** module, which is at the heart
    of a transformer. It takes three two-dimensional matrices – called **query** (**Q**),
    **key** (**K**), and **value** (**V**) matrices – as input. The matrices can have
    very large embedding sizes (as they would contain vocabulary x embedding size
    number of values), so they are split up into smaller components first (*Step 1*
    in the following diagram), before running through the scaled dot-product attention
    (*Step 2* in the following diagram):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习一下位于变压器核心的**自注意力**模块。它以三个二维矩阵作为输入，分别称为**查询**（**Q**）、**键**（**K**）和**值**（**V**）矩阵。这些矩阵可以具有非常大的嵌入大小（因为它们将包含词汇
    x 嵌入大小数量的值），因此首先被分成较小的组件（*以下图表中的步骤1*），然后再运行经过缩放的点积注意力（*以下图表中的步骤2*）：
- en: '![Diagram  Description automatically generated](img/B18475_15_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B18475_15_02.png)'
- en: 'Figure 15.2: Workflow of scaled-dot product attention and multi-head attention'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2：缩放点积注意力和多头注意力的工作流程
- en: 'Let’s understand how self-attention works. Let’s imagine a hypothetical scenario
    where the sequence length of input is 3 tokens – i.e., we have three word/token
    embeddings (*W1*, *W2*, and *W3*) as input. Say each embedding is of size 512
    values. The following steps can be performed:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解自注意力是如何工作的。假设我们有一个输入序列长度为3个标记 - 即，我们有三个词/标记嵌入（*W1*，*W2*，和 *W3*）。假设每个嵌入的大小为512个值。可以执行以下步骤：
- en: 'Each of these embeddings is individually converted into three additional vectors,
    which are the `Q`, `K`, and `V` vectors corresponding to each input. In the following
    image, `512` is the embedding dimension of the Q, K, and V vectors and `3` is
    the sequence length (3 words/tokens):'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A picture containing calendar  Description automatically generated](img/B18475_15_03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Initializing Q, K, and V vectors'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a multi-head approach where we split each of these vectors into smaller
    “heads” (eight in this example), with eight sets of vectors (of size 64 x 3) for
    each key, query, and value tensor. Here, `64` is obtained by dividing 512 (embedding
    size) by 8 (number of heads), and `3` is the sequence length:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A group of black letters  Description automatically generated](img/B18475_15_04.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: Q, K, and V values of each head'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Note that there will be eight sets of tensors of key, query, and value as there
    are eight heads. Furthermore, each head could learn about different aspects of
    a word.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'In each head, we first perform matrix multiplication between the key transpose
    and query matrices. This way, we end up with a 3 x 3 matrix. Divide the resulting
    matrix by the square root of the number of dimensions of vectors (`d = 64` in
    this case). Pass it through softmax activation. Now, we have a matrix showing
    how important each word is in relation to every other word:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18457_15_002.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: Operations on Q and K vectors'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we perform matrix multiplication of the preceding tensor output with
    the value tensor to get the output of our self-attention operation:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18457_15_003.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Self-attention calculation'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, this scaled-dot product attention calculation can be written as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_15_001.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, *d* refers to the dimension of vector (64 in this
    case) and k represents the index of head.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: A step-by-step calculation on a sample input and randomly initialized Q, K,
    and V weight matrices is provided in the associated GitHub repository as `self-attention.ipynb`
    in the `Chapter15` folder at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'We then combine the eight outputs of this step using the concat layer (*Step
    3* in *Figure 15.2*), and end up with a single tensor of size 512 x 3\. As there
    are eight heads (i.e., eight Q, K, and V matrices), the layer is also called **multi-head
    self-attention** (source: *Attention Is All You Need*, [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)).'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For our example in computer vision, when searching for an object such as a horse,
    the query would contain information to search for an object that is large in dimension
    and is brown, black, or white in general. The softmax output of scaled dot-product
    attention will reflect those parts of the key matrix that contain this color (brown,
    black, white, and so on) in the image. Thus, the values output from the self-attention
    layer will have those parts of the image that are roughly of the desired color
    and are present in the values matrix.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在计算机视觉中，当搜索马等对象时，查询将包含信息以搜索大尺寸且通常为棕色、黑色或白色的对象。经缩放的点积注意力的softmax输出将反映包含这种颜色（棕色、黑色、白色等）的键矩阵的部分。因此，来自自注意力层的值输出将包含图像中大致符合所需颜色的部分，并存在于值矩阵中。
- en: We then pass the output of multi-head attention through a residual block, in
    which we first add the inputs and output of multi-head attention, and then perform
    normalization of this final output.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过残差块传递多头注意力的输出，在这个残差块中，我们首先将多头注意力的输入和输出相加，然后对最终输出进行归一化。
- en: Then, we pass the output through a linear network to get the output, which has
    similar dimensions as that of the input.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过线性网络传递输出，其维度与输入的维度相似。
- en: We use the self-attention block several times (`Nx` in *Figure 15.1*).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次使用自注意力块（如*图15.1*中的`Nx`）。
- en: Decoder block
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器块
- en: 'While the decoder block is very similar to the encoder block, there are two
    additions to the architecture:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然解码器块与编码器块非常相似，但架构上有两个附加部分：
- en: Masked multi-head attention
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掩盖的多头注意力
- en: Cross-attention
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉注意力
- en: While multi-head attention works in a manner similar to the encoder block, we
    mask the tokens in future timesteps while calculating the attention of a token
    at a given timestep. This way, we are building the network so that it does not
    peek into future tokens. We mask future tokens by adding a mask/identity matrix,
    which ensures that future tokens are not considered during attention calculation.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然多头注意力的工作方式与编码器块类似，但在计算给定时间步的令牌的注意力时，我们掩盖了未来时间步中的令牌。这样，我们构建网络，使其不会窥视未来的令牌。我们通过添加掩码/单位矩阵来掩盖未来的令牌，以确保在注意力计算期间不考虑未来的令牌。
- en: The key and value matrices of the encoder are used as the key and query inputs
    for the cross-head attention of the decoding half, while the value input is learned
    by the neural network, independent of the encoding half. We call it cross-attention
    as key and value matrices are fetched from the encoder layer while the query is
    fetched from the decoder layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的键和值矩阵用作解码器的交叉头部注意力的键和查询输入，而值输入则由神经网络学习，独立于编码器的输入。我们将其称为交叉注意力，因为键和值矩阵从编码器层获取，而查询从解码器层获取。
- en: Finally, even though this is a sequence of inputs, there’s no sign of which
    token (word) is first and which is next. Positional encodings are learnable embeddings
    that we add to each input as a function of its position in the sequence. This
    is done so that the network understands which word embedding is first in the sequence,
    which is second, and so on.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管这是一系列输入，但没有迹象表明哪个令牌（单词）是第一个，哪个是下一个。位置编码是可学习的嵌入，我们将其添加到每个输入中，作为其在序列中位置的函数。这样做是为了让网络理解序列中哪个单词嵌入是第一个，哪个是第二个，依此类推。
- en: 'The way to create a transformer network in PyTorch is very simple. There is
    a built-in transformer block that you can create, like so:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中创建Transformer网络的方法非常简单。您可以像这样创建一个内置的transformer块：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `hidden_dim` is the size of the embeddings, `nheads` is the number of
    heads in the multi-head self-attention, and `num_encoder_layers` and `num_decoder_layers`
    are the number of encoding and decoding blocks in the network, respectively.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`hidden_dim`是嵌入的大小，`nheads`是多头自注意力中的头数，而`num_encoder_layers`和`num_decoder_layers`分别是网络中编码和解码块的数量。
- en: The working details of transformers and their implementation from scratch is
    provided in the `Transformers_from_scratch.ipynb` file within the `Chapter15`
    folder of the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从头实现transformers的工作细节可以在GitHub存储库的`Chapter15`文件夹中的`Transformers_from_scratch.ipynb`文件中找到，具体链接为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Now that we know how transformers work, in the next section, let us learn how
    ViTs work.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道transformers如何工作，在下一节中，让我们了解ViTs如何工作。
- en: How ViTs work
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ViTs的工作原理
- en: 'A ViT can be easily understood with the following diagram:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下图示轻松理解ViT：
- en: '![A diagram of a process  Description automatically generated](img/B18475_15_07.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: ViT architecture (source: [https://arxiv.org/pdf/2010.11929](https://arxiv.org/pdf/2010.11929))'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the workflow that is being followed in the preceding figure:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: We are taking an image and extracting 9 (3 x 3) patches from it. For example,
    let us assume the original image is of size 300 x 300 and each of the 9 patches
    would be 100 x 100 in shape.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we take each of the patches, flatten them, and pass them through a linear
    projection. This exercise is like extracting word embeddings corresponding to
    a word (token).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we add the positional embedding corresponding to the patch. We add a positional
    embedding as we need to preserve the information of the location of the patch
    in original image. In addition, we are also initializing a class token that would
    be helpful in the final classification of image.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, all the embeddings are passed through the transformer encoder. In a transformer,
    these embeddings pass through a sequence of normalization, multi-head attention,
    and a skip connection with a linear head (MLP stands for multi-layer perceptron).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we have output embeddings corresponding to each patch. We now attach the
    final head, depending on the problem we are trying to solve. In the case of image
    classification, we would attach a linear layer only for the **classification**
    (**CLS**) token. The outputs of remaining patches can be used as a feature extractor
    for downstream tasks like object recognition or image captioning.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we understand how ViTs work, we will go ahead and implement transformers
    on a dataset.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ViTs
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will implement ViTs on the cats vs dogs dataset that we leveraged in *Chapters
    4* and *5*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available in the `ViT_Image_classification.ipynb` file
    in the `Chapter15` folder of this book’s GitHub repository at `https://bit.ly/mcvp-2e`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and import the required packages:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that we will be leveraging the pre-trained ViT model (checkpoint location
    provided above).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dataset:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Specify the training data location:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Specify the dataset class as we did in *Chapters 4* and *5*:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the ViT model architecture:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the preceding architecture, we are fetching the features corresponding to
    each of the patches, fetching the feature of the first one (CLS token) and then
    passing it through a sigmoid layer because we want to classify it as one of the
    possible classes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the model, loss function, and optimizer:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the functions to perform training, calculate accuracy, and fetch data:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Train the model over increasing epochs:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Training and validation accuracy are as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![A graph with a line graph  Description automatically generated](img/B18475_15_08.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: Training loss and accuracy over increasing epochs'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note that the accuracy is similar to the accuracy that we saw with VGG and ResNet
    in *Chapter 5*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about leveraging an encoder-only transformer to
    perform image classification. In the next section, we will learn about leveraging
    encoder-decoder architecture-based transformers to transcribe images containing
    handwritten words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何利用仅编码器的变压器来执行图像分类。在下一节中，我们将学习如何利用基于编码器-解码器架构的变压器来转录包含手写单词的图像。
- en: Transcribing handwritten images
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转录手写图像
- en: 'Imagine a scenario where you must extract information from a scanned document
    (extracting keys and values from a picture of an ID card or a picture of a manually
    filled-in form). You’ll have to extract (transcribe) text from the image. This
    problem gets tricky due to variety in the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情况，您必须从扫描文档中提取信息（从身份证或手动填写表格的图片中提取键和值）。您将不得不从图像中提取（转录）文本。由于以下变化，这个问题变得棘手：
- en: Handwriting
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手写
- en: Quality of the scan/picture
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扫描/图片质量
- en: Lighting conditions
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 光线条件
- en: In this section, we will learn about the technique to transcribe handwritten
    images.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习有关转录手写图像的技术。
- en: Let us first understand how an encoder-decoder architecture of a transformer
    can be applied to transcribe a handwritten image.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解一下如何将变压器的编码器-解码器架构应用于手写图像的转录。
- en: Handwriting transcription workflow
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手写转录工作流程
- en: 'We will leverage the TrOCR architecture (source: [https://arxiv.org/abs/2109.10282](https://arxiv.org/abs/2109.10282))
    to transcribe handwritten information.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用TrOCR架构（来源：[https://arxiv.org/abs/2109.10282](https://arxiv.org/abs/2109.10282)）来转录手写信息。
- en: 'The following diagram shows the workflow that is followed:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了所遵循的工作流程：
- en: '![A screenshot of a computer program  Description automatically generated](img/B18475_15_09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图的图片](img/B18475_15_09.png)'
- en: 'Figure 15.9: TrOCR workflow'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9：TrOCR工作流程
- en: 'As shown in the preceding picture, the workflow is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图片所示，工作流程如下：
- en: We take an input and resize it to a fixed height and width.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接收输入并将其调整为固定的高度和宽度。
- en: Then, we divide the image into a set of patches.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将图像分成一组补丁。
- en: We then flatten the patches and fetch embeddings corresponding to each patch.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将补丁展平并获取与每个补丁对应的嵌入。
- en: We combine the patch embeddings with position embeddings and pass them through
    an encoder.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将补丁嵌入与位置嵌入结合，并通过编码器传递它们。
- en: The key and value vectors of the encoder are fed into the cross-attention of
    the decoder to fetch the outputs in the final layer.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器的键和值向量被送入解码器的交叉注意力层，以在最终层中获取输出。
- en: The tokenizer and the model that we will use to train the model will leverage
    the `trocr-base-handwritten` model released by Microsoft. Let us go ahead and
    code up handwriting recognition in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Microsoft发布的`trocr-base-handwritten`模型来训练模型和标记器。让我们继续编写下一节的手写识别代码。
- en: Handwriting transcription in code
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码中的手写转录
- en: 'The following steps can be followed for handwriting transcription:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 可以遵循以下步骤进行手写转录：
- en: This code is available as `TrOCR_fine_tuning.ipynb` in the `Chapter15` folder
    of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码作为`TrOCR_fine_tuning.ipynb`在本书GitHub存储库的`Chapter15`文件夹中提供，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Download and import the dataset of images:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并导入图像数据集：
- en: '[PRE10]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code, we have downloaded the dataset in which the images are
    provided; the filename of the image contains the ground truth of transcription
    corresponding to that image.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们已经下载了包含图像的数据集；图像的文件名包含了与该图像对应的转录的真实信息。
- en: 'A sample from the images that were downloaded is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从下载的图像中选取的样本如下：
- en: '![A picture containing chart  Description automatically generated](img/B18475_15_10.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表描述的图片](img/B18475_15_10.png)'
- en: 'Figure 15.10: Sample image along with ground truth (as title)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：样本图像及其真实信息（作为标题）
- en: 'Install the required packages and import them:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装所需的包并导入它们：
- en: '[PRE11]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Specify the location of the images and the function to fetch the ground truth
    from the images:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定图像的位置和从图像中提取真实信息的函数：
- en: '[PRE12]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that we are creating the `fname2label` function as the ground truth of
    an image is available after the `@` symbol in the filename. A sample of the filenames
    is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们正在创建`fname2label`函数，因为在文件名中`@`符号后面的图像的真实信息是可用的。文件名的示例如下：
- en: '![Text  Description automatically generated](img/B18475_15_11.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: Sample filenames'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch 5,000 samples (images and their corresponding labels) from the 25K image
    dataset for faster training:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Specify the training and test DataFrames:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the `Dataset` class:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding code, we are fetching the image, passing through a processor
    to fetch the pixel values. Furthermore, we are passing the label through the tokenizer
    of the model to fetch the tokens of labels.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Define `TrOCRProcessor`:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, we are defining the processor that preprocesses the images
    and performs the tokenization of labels.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the training and evaluation datasets:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the `TrOCR` model:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define the model configuration parameters and training arguments:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the function to calculate the character error rate:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Train the model:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding code results in the following output:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![A table of numbers with numbers on it  Description automatically generated](img/B18475_15_12.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: Training and validation loss along with the character error rate
    over increasing epochs'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Note that the error rate kept reducing over increasing steps.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform inference on a sample image from our dataset:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code results in images as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![A black text on a white background  Description automatically generated](img/B18475_15_13.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: Sample prediction'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about using the encoder-decoder architecture for handwriting
    recognition. In the next section, we will learn about leveraging the transformer
    architecture to fetch keys and values within a document.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Document layout analysis
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a scenario where you are tasked with extracting the values for the various
    keys present in a passport (like name, date of birth, issue date, and expiry date).
    In certain passports, values are present below the keys; in others, they are present
    on the right side of keys, while others have them on the left side. How do we
    build a single model that is able to assign a value corresponding to each text
    within the document image? LayoutLM comes in handy in such a scenario.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LayoutLM
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LayoutLM is a pre-trained model that is trained on a huge corpus of document
    images. The architecture of LayoutLM is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](img/B18475_15_14.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: LayoutLM architecture (source: [https://arxiv.org/pdf/1912.13318](https://arxiv.org/pdf/1912.13318))'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding diagram, the corresponding workflow consists of the
    following steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: We take an image of the document and extract the various words and their bounding-box
    coordinates (x0, x1, y0, and y1) – this is done using tools that help in OCR where
    they provide not only the text but also the bounding box in which the text is
    present in the document.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We take the position embeddings corresponding to these bounding-box coordinates
    – position embeddings are calculated based on the bounding-box coordinates.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We add the embeddings corresponding to the various texts extracted (**text embeddings**
    in the above picture) where we pass the text through a tokenizer, which in turn
    is passed through a pre-trained **Bi-directional Encoder Representation of Transformers**
    (**BERT**)-like model.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从上述图片中提取的各种文本对应的嵌入（**文本嵌入**）添加到一起，其中我们通过标记器传递文本，进而通过预训练的**双向编码器转换器**（**BERT**）模型。
- en: During the training phase of the pre-trained LayoutLM, we randomly mask certain
    words (but not the position embeddings of those words) and predict the masked
    words given the context (surrounding words and their corresponding position embeddings).
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预训练的 LayoutLM 的训练阶段中，我们会随机屏蔽某些单词（但不是这些单词的位置嵌入），并根据上下文（周围单词及其对应的位置嵌入）预测屏蔽的单词。
- en: Once the pre-trained LayoutLM model is fine-tuned, we extract the embeddings
    corresponding to each word by summing up the text embeddings of the word with
    the position embeddings corresponding to the word.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦预训练的 LayoutLM 模型被微调，我们通过将单词的文本嵌入与对应的单词位置嵌入相加来提取每个单词对应的嵌入。
- en: Next, we leverage Faster R-CNN to obtain the image embedding corresponding to
    the location of the word. We leverage image embedding so that we obtain key information
    regarding the text style (for example, bold, italics, or underlined) that is not
    available with OCR.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们利用 Faster R-CNN 获取与单词位置对应的图像嵌入。我们利用图像嵌入以获取关于文本样式（例如粗体、斜体或下划线）的关键信息，这是
    OCR 无法提供的。
- en: Finally, we perform the downstream task of extracting the keys and values corresponding
    to the image. In the case of document key value extraction, it translates to the
    task of named entity recognition, where each output word is classified as one
    of the possible keys or a value associated with a key.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们执行下游任务，提取与图像对应的键和值。在文档键值提取的情况下，这对应于命名实体识别的任务，其中每个输出单词被分类为可能的键或与键相关联的值之一。
- en: 'LayoutLMv3 is an improvement over LayoutLM, and its architecture is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: LayoutLMv3 是 LayoutLM 的改进版本，其架构如下：
- en: '![A diagram of a computer program  Description automatically generated](img/B18475_15_15.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序图表描述自动生成](img/B18475_15_15.png)'
- en: 'Figure 15.15: LayoutLMv3 architecture (source: [https://arxiv.org/pdf/2204.08387](https://arxiv.org/pdf/2204.08387))'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.15：LayoutLMv3 架构（来源：[https://arxiv.org/pdf/2204.08387](https://arxiv.org/pdf/2204.08387)）
- en: 'The preceding architecture shows the following steps:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 前述架构展示了以下步骤：
- en: Words are obtained from an image using a typical OCR parser.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单词是使用典型的 OCR 解析器从图像中获取的。
- en: 'The words are then converted into embeddings using the `RoBERTa` model (more
    details here: [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)).'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用 `RoBERTa` 模型将单词转换为嵌入（更多细节请见：[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)）。
- en: The document is resized into a fixed shape and then converted into multiple
    patches.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文档被调整为固定形状，然后转换为多个补丁。
- en: Each patch is flattened and passed through a linear layer to obtain embeddings
    corresponding to the patch.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个补丁都被展平并通过线性层传递，以获得与补丁对应的嵌入。
- en: The 1D position embeddings correspond to the index of the word/patch while the
    2D position embeddings correspond to the bounding box/segment.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1D 位置嵌入对应于单词/补丁的索引，而 2D 位置嵌入对应于边界框/段落。
- en: Once the embeddings are in place, we perform masked pre-training (`MLM Head`)
    in a manner similar to that of LayoutLM, where we mask certain words and predict
    them using the context. Similarly in **masked image modeling** (**MIM Head**),
    we mask certain blocks and predict the tokens within the block.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦嵌入就位，我们执行掩码预训练（`MLM Head`），方式类似于 LayoutLM，其中我们掩码某些单词并使用上下文预测它们。类似地，在**掩码图像建模**（**MIM
    Head**）中，我们掩码某些块并预测块内的标记。
- en: '**Word patch alignment** (**WPA Head**) is then performed, which refers to
    the task of predicting whether a masked image patch has the corresponding tokens
    masked. If a token is masked and the corresponding image patch is masked, it is
    aligned; it is unaligned if one of these is masked and the other isn’t.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后执行**单词补丁对齐**（**WPA Head**），这指的是预测掩码图像补丁是否具有相应的掩码标记。如果一个标记被掩码，并且相应的图像补丁也被掩码，则它是对齐的；如果其中一个被掩码而另一个没有，则它是不对齐的。
- en: In the following section, we’ll learn how to implement this.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习如何实现这一点。
- en: Implementing LayoutLMv3
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施 LayoutLMv3
- en: 'Let us code up `LayoutLMv3` using a passport dataset – where we try to associate
    each token in the image to the corresponding key or value:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available in `LayoutLMv3_passports.ipynb` in the `Chapter15`
    folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the required packages and import them:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import a dataset of synthetic passports:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The input dataset contains words, boxes, and labels, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a number  Description automatically generated](img/B18475_15_16.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: Sample expected output'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample image is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a passport  Description automatically generated](img/B18475_15_17.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.17: Sample synthetic passport'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the train and test split:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Specify the `label2id` and `id2label` mapping:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the processor and prepare the function to encode inputs:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the preceding code, we are leveraging the pre-trained `LayoutLMv3` model’s
    processor, which we are fine-tuning for our dataset. We are then passing the image,
    words, and boxes through the processor to get the corresponding encoding.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the train and evaluation datasets:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the evaluation metric:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the function to calculate the metrics:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In the preceding code, we are fetching the output that is not padding in ground
    truth and computing the precision, recall, F1 score, and accuracy metrics.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the pre-trained `LayoutLMv3` model by importing the relevant module
    from the transformers library:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code, we are defining the base model that we will use to fine-tune
    the model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the training parameters:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Initialize the trainer and train the model:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we perform inference (the code for which is provided in the associated
    GitHub repository) to get results for an input image. A sample inference with
    the predicted keys and values (present as bounding boxes within the image) is
    as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18475_15_18.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.18: Predicted output with bounding boxes of different keys and values
    extracted'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve learned about extracting keys and values from a document.
    In the next section, we will learn about performing question answering given a
    generic image.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Visual question answering
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a scenario where you are given an image and are asked to answer certain
    questions by looking at that image. This is a task of **visual question answering**
    (**VQA**). A high-level strategy for VQA could be to leverage a pre-trained image
    to encode image information, encode the question (text) using a **large language
    model** (**LLM**), and then use the image and text representations to generate
    (decode) the answer – essentially, a multimodal model, which has input in both
    text and image mode.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: One way of performing visual question answering is by getting the caption corresponding
    to the image and then performing question answering on the caption.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the reason why we cannot use this, let’s look at the following
    image:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '![A cat wearing sunglasses  Description automatically generated](img/B18475_15_19.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![一只戴着墨镜的猫 由系统自动生成的描述](img/B18475_15_19.png)'
- en: 'Figure 15.19: Sample image'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.19：示例图像
- en: 'A set of possible questions for the same caption of the original image are:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一组关于原始图像标题的可能问题：
- en: '| **Extracted caption** | **Question** |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| **提取的标题** | **问题** |'
- en: '| A cat wearing sunglasses | What is the subject of the image? |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 一只戴着墨镜的猫 | 图像的主题是什么？ |'
- en: '| A cat wearing sunglasses | What is the background color in the image? |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 一只戴着墨镜的猫 | 图像的背景颜色是什么？ |'
- en: 'Table 15.1: Some questions for the given image'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表15.1：给定图像的一些问题
- en: In the preceding scenario, while we can answer the first question, we are unable
    to answer the second as the information related to the question is not extracted
    in the context (extracted caption).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的情景中，虽然我们可以回答第一个问题，但我们无法回答第二个问题，因为与问题相关的信息没有在上下文中提取出来（提取的标题）。
- en: Let’s learn about BLIP2 – a model that helps in addressing this problem.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下BLIP2 - 一个帮助解决这个问题的模型。
- en: Introducing BLIP2
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入BLIP2
- en: One of the problems associated with encoder-decoder models (as discussed in
    the high-level strategy we just discussed, where captioning is the process of
    encoding, and question answering on the caption is the process of decoding) is
    that the model is likely to result in **catastrophic** **forgetting** when VQA
    models integrate both visual perception and language understanding. When these
    models are updated with new visual or linguistic patterns, the complex interplay
    between these two domains can lead to the forgetting of previously learned patterns.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚讨论的高级策略中讨论的编码器-解码器模型的一个问题是，当VQA模型整合视觉感知和语言理解时，模型可能会导致灾难性遗忘。当这些模型使用新的视觉或语言模式进行更新时，这两个领域之间复杂的相互作用可能会导致先前学习的模式被遗忘。
- en: 'BLIP with frozen image encoders and LLMs (BLIP2) addresses this with a unique
    architecture, as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用冻结图像编码器和LLMs（BLIP2）通过独特的架构解决了这个问题：
- en: '![A diagram of a language model  Description automatically generated](img/B18475_15_20.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![语言模型图示 由系统自动生成的描述](img/B18475_15_20.png)'
- en: 'Figure 15.20: BLIP2 architecture (source: [https://ar5iv.labs.arxiv.org/html/2301.12597](https://ar5iv.labs.arxiv.org/html/2301.12597))'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.20：BLIP2架构（来源：[https://ar5iv.labs.arxiv.org/html/2301.12597](https://ar5iv.labs.arxiv.org/html/2301.12597)）
- en: The **querying transformer** (**Q-Former**) acts as a bridge between the image
    encoder and the LLM.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 查询变换器（Q-Former）充当了从图像编码器到LLM的桥梁。
- en: The Q-Former extracts information from the image that is most relevant to the
    question that is asked; so, in the scenario presented at the start of this section,
    it would extract visual information from the image that is most relevant to the
    question asked. Now, we append the context (information extracted by Q-Former)
    to the question asked and provide it as an input to the LLM.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Former从图像中提取与所问问题最相关的信息；因此，在本节开头呈现的情景中，它将从图像中提取与所问问题最相关的视觉信息。现在，我们将上下文（Q-Former提取的信息）附加到问题并将其作为输入提供给LLM。
- en: Essentially, Q-Former acts as a bridge between the image encoder and LLM to
    modify the features extracted from the image in such a way that they are most
    relevant to the question asked.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，Q-Former充当了从图像编码器到LLM的桥梁，以修改从图像中提取的特征，使其与所问问题最相关。
- en: 'There are two stages in which Q-Former is trained:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: Q-Former训练有两个阶段：
- en: Bootstrapping vision-language **representation learning** from a frozen encoder
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从冻结编码器中引导视觉-语言表示学习
- en: Bootstrapping vision-language **generative learning** from a frozen LLM
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从冻结的LLM中引导视觉-语言生成学习
- en: Let’s look at these stages in detail.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些阶段。
- en: Representation learning
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表示学习
- en: In the representation learning stage, we connect Q-Former to a frozen image
    encoder and perform pre-training using image-text pairs. We aim to train the Q-Former
    such that the queries (32 learnable query vectors) can learn to extract the visual
    representation that is most relevant to the text.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在表示学习阶段，我们将Q-Former连接到冻结的图像编码器，并使用图像-文本对进行预训练。我们的目标是训练Q-Former，使得这些查询（32个可学习查询向量）能够学习提取与文本最相关的视觉表示。
- en: We jointly optimize three pre-training objectives that share the same input
    format and model parameters. Each objective employs a different attention masking
    strategy between queries and text to control their interaction.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们共同优化了三个预训练目标，它们共享相同的输入格式和模型参数。每个目标使用不同的注意屏蔽策略来控制查询和文本之间的交互。
- en: 'The following diagram illustrates this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了这一点：
- en: '![A diagram of a process  Description automatically generated with medium confidence](img/B18475_15_21.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![一个过程的流程图 由中等置信度自动生成的描述](img/B18475_15_21.png)'
- en: 'Figure 15.21: Details of representation learning (source: [https://arxiv.org/pdf/2301.12597.pdf](https://arxiv.org/pdf/2301.12597.pdf))'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.21：表示学习的详细信息（来源：[https://arxiv.org/pdf/2301.12597.pdf](https://arxiv.org/pdf/2301.12597.pdf)）
- en: 'As shown in the preceding image, the three objectives are:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，三个目标如下：
- en: '**Image-text matching:** In this task, within the self-attention module, query
    tokens can attend to text tokens and the text tokens can attend to query vectors.
    The objective of this pre-training is to perform a binary classification of whether
    the image and text pair match.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像-文本匹配：** 在这个任务中，在自注意模块内，查询标记可以关注文本标记，而文本标记可以关注查询向量。这个预训练的目标是执行图像和文本对是否匹配的二元分类。'
- en: '**Image-grounded text generation:** In this task, within the self-attention
    module, query vectors do not have access (are masked) to the text tokens while
    the text tokens have access to the query vectors and also the previously generated
    tokens. The objective of this training is to generate text that matches the image.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于图像的文本生成：** 在这个任务中，在自注意模块中，查询向量不能访问（被屏蔽）文本标记，而文本标记可以访问查询向量以及先前生成的标记。此训练的目标是生成与图像匹配的文本。'
- en: '**Image-text contrastive learning:** In this task, we have the self-attention
    module that is shared between the learned queries and the input text. The learned
    queries interact with the image encoder to get an output vector. The input text
    is converted to embedding vectors and interacts with the self-attention and feed-forward
    network to generate an embedding corresponding to the text. We pre-train the Q-Former
    to have a high similarity for matching image-text pairs and a low similarity for
    an image and a different text (similar to how CLIP is trained). Note that while
    the self-attention layer is common, text tokens are masked from image tokens (learned
    queries) so that information does not leak between the two.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像-文本对比学习：** 在这个任务中，我们有一个共享的自注意模块，用于学习的查询和输入文本之间。学习的查询与图像编码器交互以获取输出向量。输入文本转换为嵌入向量，并与自注意和前馈网络交互以生成对应文本的嵌入。我们预训练Q-Former，使其对匹配的图像-文本对具有高相似度，并对图像与不同文本的情况具有低相似度（类似于CLIP的训练方式）。请注意，虽然自注意层是共享的，但文本标记被屏蔽，以防止两者之间信息泄漏。'
- en: With the above three pre-training objectives, we have completed the representation
    learning exercise, where we extract the visual information that is most informative
    of the text.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述三种预训练目标，我们完成了表示学习练习，从中提取了对文本最具信息性的视觉信息。
- en: Generative learning
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成学习
- en: 'In the generative learning stage, we pass the learned queries through the Q-Former
    to get an output vector, which is then passed through a fully connected layer
    to get an embedding that has the same dimensions as that of the text embedding
    dimensions. This can be illustrated as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成学习阶段，我们通过 Q-Former 传递学习的查询以获得输出向量，然后通过全连接层传递以获得与文本嵌入维度相同的嵌入。可以如下所示：
- en: '![A diagram of a computer process  Description automatically generated](img/B18475_15_22.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![计算机流程图示例 由自动生成的描述](img/B18475_15_22.png)'
- en: 'Figure 15.22: Details of generative learning (source: [https://ar5iv.labs.arxiv.org/html/2301.12597](https://ar5iv.labs.arxiv.org/html/2301.12597))'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.22：生成学习的详细信息（来源：[https://ar5iv.labs.arxiv.org/html/2301.12597](https://ar5iv.labs.arxiv.org/html/2301.12597)）
- en: We now have two ways of generating text through the frozen LLM. A decoder-based
    LLM takes the output of the **fully connected** (**FC**) layer and generates the
    output text, while an encoder-decoder-based model takes the concatenation of FC
    output and prefix text to generate subsequent text.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两种通过冻结的LLM生成文本的方式。基于解码器的LLM接收**全连接**（**FC**）层的输出并生成输出文本，而基于编码器-解码器的模型接收FC输出和前缀文本的连接以生成后续文本。
- en: With the above, we are done with the two stages of pre-training BLIP2 and getting
    the context that is most relevant to the question asked. This is how we provide
    the most relevant image context to a question and generate an answer. Let’s go
    ahead and implement BLIP2 in the next section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述方法，我们完成了预训练BLIP2的两个阶段，并获得了与所提问题最相关的上下文。这是我们如何为问题提供最相关的图像背景并生成答案的方法。让我们继续在下一节实现BLIP2。
- en: Implementing BLIP2
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施BLIP2
- en: 'In this section, we will leverage BLIP2 to perform the following tasks:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用BLIP2执行以下任务：
- en: Image captioning
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕
- en: Image question answering
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像问题回答
- en: 'You can use the following steps to achieve this:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下步骤来实现此操作：
- en: This code is available in the `Visual_Question_answering.ipynb` file in the
    `Chapter15` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码在本书GitHub存储库的`Chapter15`文件夹中的`Visual_Question_answering.ipynb`文件中提供，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Import the required packages and load the model:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包并加载模型：
- en: '[PRE34]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that the model requires a high VRAM and thus we have used a V100 machine
    on Colab.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该模型需要高显存，因此我们在Colab上使用了V100机器。
- en: 'Load the image – you can use any image of your choice:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像 - 您可以使用任何您选择的图像：
- en: '[PRE35]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![A baby wearing a yellow crown  Description automatically generated](img/B18475_15_23.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![一个戴着黄色皇冠的婴儿  自动生成的描述](img/B18475_15_23.png)'
- en: 'Figure 15.23: Sample image'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.23：示例图像
- en: 'Pass the image through the processor to generate a caption corresponding to
    the image:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像通过处理器以生成与图像相对应的标题：
- en: '[PRE36]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This gives us the following output:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: '[PRE37]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perform question answering on the image:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像执行问题回答：
- en: '[PRE38]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The preceding code results in the output `blue`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出结果为`blue`。
- en: Note that when performing question answering, we should explicitly mention the
    start of the question and the start of the answer, as provided in the prompt.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在执行问题回答时，我们应明确提及问题的开始和答案的开始，如提示所提供。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how transformers work. Furthermore, we learned about
    leveraging ViTs to perform image classification. We then learned about document
    understanding while learning about leveraging TrOCR for handwriting transcription
    and LayoutLM for key-value extraction from documents. Finally, we learned about
    visual question answering using the pre-trained BLIP2 model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了变压器的工作原理。此外，我们还学习了如何利用ViT进行图像分类。然后，我们学习了如何在学习手写转录时利用TrOCR进行文档理解，以及如何从文档中利用LayoutLM提取键值对。最后，我们了解了如何使用预训练的BLIP2模型进行视觉问题回答。
- en: With this, you should be comfortable in tackling some of the most common real-world
    use cases, such as OCR on documents, extracting key-value pairs from documents,
    and visual question answering on an image (handling multimodal data). Furthermore,
    with the understanding of transformers, you are now in a good position to dive
    deep into foundation models in the next chapter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，您应该可以轻松处理一些最常见的真实世界用例，例如文档上的OCR，从文档中提取键值对，以及图像上的视觉问题回答（处理多模态数据）。此外，通过对变换器的理解，您现在可以深入了解基础模型的下一章。
- en: Questions
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the inputs, steps for calculation, and outputs of self-attention?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自注意力的输入、计算步骤和输出是什么？
- en: How is an image transformed into a sequence input in a vision transformer?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像如何在视觉变换器中转换为序列输入？
- en: What are the inputs to the BERT transformer in a LayoutLM model?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在LayoutLM模型中，BERT变换器的输入是什么？
- en: What are the three objectives of BLIP?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BLIP的三个目标是什么？
- en: Learn more on Discord
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多信息
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
