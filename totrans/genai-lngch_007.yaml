- en: 6 Developing Software
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 开发软件
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们在Discord上的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](img/file42.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![Qr code Description automatically generated](img/file42.png)'
- en: 'While this book is about integrating generative AI and, in particular, large
    language models (LLMs) into software applications, in this chapter, we’ll talk
    about how we can leverage LLMs to help in software development. This is a big
    topic and software development was highlighted in reports by several consultancies
    such as KPMG and McKinsey as one of the domains impacted most by generative AI.We’ll
    first discuss how LLMs could help in coding tasks, and we’ll go through a lot
    of literature as an overview to see how far we have come in automating software
    engineers. We’ll also discuss a lot of the recent progress and new models. Then,
    we’ll play around with a few models evaluating the generated code qualitatively.
    Next, we’ll implement a fully-automated agent for software development tasks.
    We go through the design choices and show a bit of the results that we got in
    an agent implementation of only a few lines of Python with LangChain. There are
    a lot of possible extensions to this approach, which we’ll also go through.Throughout
    the chapter, we’ll work on different approaches to software development, which
    you can find in the `software_development` directory in the Github repository
    for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这本书讨论整合生成式人工智能，特别是大型语言模型（LLMs）到软件应用程序中，但在本章中，我们将讨论如何利用LLMs来帮助软件开发。这是一个重要的话题，软件开发被几家咨询公司（如毕马威和麦肯锡）的报告强调为受生成式人工智能影响最大的领域之一。我们将首先讨论LLMs如何帮助编码任务，并概述我们在自动化软件工程师方面取得的进展。我们还将讨论许多最新的进展和新模型。然后，我们将测试一些模型，定性地评估生成的代码。接下来，我们将实现一个完全自动化的软件开发代理。我们将讨论设计选择，并展示我们通过LangChain仅使用几行Python代码在代理实现中得到的结果。这种方法有许多可能的扩展，我们也会详细介绍。在整个章节中，我们将探讨软件开发的不同方法，您可以在Github存储库中的`software_development`目录中找到书籍的链接：[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)主要包括以下几个部分：
- en: Software development and AI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件开发和人工智能
- en: Writing code with LLMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLMs编写代码
- en: Automated software development
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化软件开发
- en: We'll begin the chapter by giving a broad overview over the state-of-the-art
    of using AI for software development.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以对使用人工智能进行软件开发的最新技术做一个广泛概述作为这一章的开端。
- en: Software development and AI
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件开发和人工智能
- en: 'The emergence of powerful AI systems like ChatGPT has sparked great interest
    in using AI as a tool to assist software developers. A June 2023 report by KPMG
    estimated that about 25% of software development tasks could be automated away.
    A McKinsey report from the same month highlighted software development as a function,
    where generative AI can have a significant impact in terms of cost reduction and
    efficiency gain. The idea of utilizing artificial intelligence to aid programming
    is not new, but has rapidly evolved alongside advances in computing and AI. The
    two areas are intertwined as we’ll see.Early efforts in language and compiler
    design the 1950s and 60s sought to make it easier to write software. Data processing
    languages like **FLOW-MATIC** (also known as: **Business Language version 0**),
    designed under Grace Hopper at Remington Rand in 1955, generated code from English-like
    statements. Similarly, programming languages such as **BASIC** (**Beginners’ All-purpose
    Symbolic Instruction Code**), created at Dartmouth College in 1963, aimed to make
    it easier to write software in an interpreted environment.Other efforts further
    simplified and standardized the programming syntax and interfaces. The **flow-based
    programming** (**FBP**) paradigm, invented by J. Paul Morrison in the early 1970s,
    allows to define applications as connected black box processes, which exchange
    data by message passing. Visual low-code or no-code platforms followed in the
    same mold with popular proponents such as LabVIEW, extensively used for system
    design in Electronical Engineering, and the KNIME extract, transform, load tool
    for data science.Some of the earliest efforts to automate coding itself through
    AI were **expert systems**, which emerged in the 1980s. As a form of narrow AI,
    they focused on encoding domain knowledge and rules to provide guidance. These
    would be formulated in a very specific syntax and executed in rule engines. These
    encoded best practices for programming tasks like debugging, though their usefulness
    was constrained by the need for meticulous rule-based programming.For software
    development, from command line editors such as ed (1969), to vim and emacs (1970s),
    to today’s integrated development environment (IDEs) such as Visual Studio (first
    released in 1997) and PyCharm (since 2010), these tools have helped developers
    write code, navigate in complex projects, refactor, get highlighting and setup
    and run tests. IDE’s also integrated and provide feedback from code validation
    tools, some of which have been around since the 1970s. Prominently, Lint, written
    by Stephen Curtis Johnson in 1978 at Bell Labs can flag bugs, stylistic errors
    and suspicious constructs. Many tools apply formal methods; however, machine learning
    has been applied including genetic programming and neural network based approaches
    for at least 20 years. In this chapter, we’ll how far we’ve come with analyzing
    code using deep neural networks, especially transformers.This brings us to the
    present day, where models have been trained to produce full or partial programs
    based on natural language descriptions (in coding assistants or chatbots) or some
    code inputs (completion).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的人工智能系统如ChatGPT的出现引发了将AI作为辅助软件开发的工具的极大兴趣。卡特彼马集团(KPMG)在2023年6月的一份报告估计，约有25%的软件开发任务可以自动化完成。这个月的麦肯锡报告强调了软件开发作为一个功能，其中生成AI可以在成本降低和效率提高方面产生显著影响。利用人工智能辅助编程的想法并不是新鲜事物，但随着计算和AI的进步，这两个领域已迅速发展。正如我们将看到的那样，这两个领域是相互交织的。20世纪50年代和60年代早期的语言和编译器设计早期努力旨在使编写软件变得更容易。数据处理语言像**FLOW-MATIC**（也称为：**商业语言版本0**），由Grace
    Hopper于1955年在Remington Rand设计，可以从类似于英语的语句中生成代码。同样，编程语言如Dartmouth College在1963年创建的**BASIC**（初学者通用符号指令代码）旨在使在解释环境中编写软件更容易。其他努力进一步简化和规范化编程语法和接口。
    **流程驱动编程**（**FBP**）范例，由J. Paul Morrison在20世纪70年代早期发明，允许将应用程序定义为连接的黑盒进程，其通过消息传递交换数据。可视化的低代码或无代码平台遵循相同的模式，其中一些流行的支持者包括LabVIEW，广泛用于电子工程中的系统设计，以及KNIME数据科学中的提取、转换、加载工具。通过AI自动编码本身的最早的一些努力是**专家系统**，它们出现在20世纪80年代。作为狭义AI的一种形式，它们专注于编码领域知识和规则以提供指导。这些规则将以非常特殊的语法公式化，并在规则引擎中执行。这些编码了编程任务的最佳实践，如调试，但它们的有用性受到了需要精细的基于规则的编程的限制。对于软件开发，从命令行编辑器如ed(1969)，到vim和emacs（1970年代），再到今天的集成开发环境(IDEs)，如Visual
    Studio（1997年首次发布）和PyCharm（自2010年以来），这些工具帮助开发人员编写代码，导航在复杂的项目中，重构，获得高亮和设置和运行测试。IDE也集成并提供来自代码验证工具的反馈，其中一些工具已经存在自20世纪70年代以来。其中，贝尔实验室1978年由Stephen
    Curtis Johnson编写的Lint可以标记错误，风格上的错误和可疑的结构。许多工具应用正式方法; 然而，机器学习至少20年来已经应用，包括遗传编程和基于神经网络的方法。在本章中，我们将看到使用深度神经网络，特别是变压器来分析代码的进展。这带我们来到了现在，已经训练出模型根据自然语言描述（在编码助手或
- en: Present day
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 现在的一天
- en: 'Researchers at DeepMind published two papers in the journals Nature and Science,
    respectively, that represent important milestones in using AI to transform foundational
    computing, in particular using reinforcement learning to discover optimized algorithms.
    In October 2022, they released algorithms discovered by their model **AlphaTensor**
    for matrix multiplication problems, which can speed up this essential computation
    required by deep learning models, but also in many other applications. **AlphaDev**
    uncovered novel sorting algorithms that were integrated into widely used C++ libraries,
    improving performance for millions of developers. It also generalized its capabilities,
    discovering a 30% faster hashing algorithm now used billions of times daily. These
    discoveries demonstrate AlphaDev''s ability to surpass human-refined algorithms
    and unlock optimizations difficult at higher programming levels.Their model **AlphaCode**,
    published as a paper in February 2022, showcases an AI-powered coding engine that
    creates computer programs at a rate comparable to that of an average programmer.
    They report results on different datasets including HumanEval and others, which
    we’ll come to in the next section. The DeepMind researchers highlight the large-scale
    sampling of candidate pool of algorithms and a filtering step to select from it.
    The model was celebrated as a breakthrough achievement; however, the practicality
    and scalability of their approach is unclear.Today, new code LLMs such as ChatGPT
    and Microsoft''s Copilot are highly popular generative AI models, with millions
    of users and significant productivity-boosting capabilities. There are different
    tasks related to programming that LLMs can tackle such as these:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的研究人员分别在《自然》和《科学》期刊上发表了两篇论文，这代表了使用AI改变基础计算的重要里程碑，特别是使用强化学习来发现优化算法。2022年10月，他们发布了由他们的模型**AlphaTensor**发现的用于矩阵乘法问题的算法，这可以加速深度学习模型需要的这种重要计算，也适用于许多其他应用。**AlphaDev**发现了集成到广泛使用的C++库中的新型排序算法，提高了数百万开发人员的性能。它还推广了它的能力，发现了一个比目前每天数十亿次使用的速度快30%的哈希算法。这些发现证明了AlphaDev的能力超越了人工精细调整的算法，并解锁了在较高编程级别难以进行的优化。他们的模型**AlphaCode**，在2022年2月作为一篇论文发表，展示了一个由人工智能驱动的编码引擎，其创建计算机程序的速度可与普通程序员相媲美。他们报告了不同数据集上的结果，包括HumanEval等，我们将在下一节讨论。DeepMind的研究人员强调了大规模采样算法候选池和从中选择的过滤步骤。该模型被誉为突破性成就；然而，他们的方法的实用性和可扩展性尚不清楚。如今，像ChatGPT和微软的Copilot这样的新代码LLM已经成为备受欢迎的生成式AI模型，拥有数百万用户和显著的提高生产力的能力。LLM可以处理与编程相关的不同任务，比如：
- en: 'Code completion: This task involves predicting the next code element based
    on the surrounding code. It is commonly used in integrated development environments
    (IDEs) to assist developers in writing code.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码补全：这项任务涉及基于周围代码预测下一个代码元素。它通常用于集成开发环境（IDE）中，以帮助开发人员编写代码。
- en: 'Code summarization/documentation: This task aims to generate a natural language
    summary or documentation for a given block of source code. This summary helps
    developers understand the purpose and function of the code without having to read
    the actual code.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码摘要/文档：这项任务旨在为给定的源代码块生成自然语言摘要或文档。这个摘要帮助开发人员理解代码的目的和功能，而不必阅读实际的代码。
- en: 'Code search: The objective of code search is to find the most relevant code
    snippets based on a given natural language query. This task involves learning
    the joint embeddings of the query and code snippets to return the expected ranking
    order of code snippets. Neural code search is specifically focused on in the experiment
    mentioned in the text.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码搜索：代码搜索的目标是根据给定的自然语言查询找到最相关的代码片段。这项任务涉及学习查询和代码片段的联合嵌入，以返回预期的代码片段排名顺序。在文本中提到的实验，神经代码搜索专门侧重于这一点。
- en: 'Bug finding/fixing: AI systems can reduce manual debugging efforts and enhance
    software reliability and security. Many bugs and vulnerabilities are hard to find
    for programmers, although there are typical patterns for which code validation
    tools exist. As an alternative, LLMs can spot problems with a code and (when prompted)
    correct them. Thus, these systems can reduce manual debugging efforts and help
    improve software reliability and security.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bug 查找/修复：AI 系统可以减少手动调试工作量，增强软件的可靠性和安全性。对于程序员来说，许多错误和漏洞很难找到，尽管存在用于代码验证的典型模式。作为替代方案，LLM
    可以发现代码中的问题，并在提示时进行修正。因此，这些系统可以减少手动调试工作量，并有助于提高软件的可靠性和安全性。
- en: 'Test generation: Similar to code completion, LLMs can generate unit tests (compare
    Bei Chen and others, 2022) and other types of tests enhancing the maintainability
    of a code base.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试生成：类似于代码补全，LLM（大型语言模型）可以生成单元测试（参见Bei Chen等人，2022年）和增强代码库可维护性的其他类型的测试。
- en: AI programming assistants combine the interactivity of earlier systems with
    cutting-edge natural language processing. Developers can query bugs in plain English
    or describe desired functions, receiving generated code or debugging tips. However,
    risks remain around code quality, security, and excessive dependence. Striking
    the right balance of computer augmentation while maintaining human oversight is
    an ongoing challenge.Let’s look at the current performance of AI systems for coding,
    particularly code LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: AI 编程助手结合了早期系统的互动性和前沿的自然语言处理。开发人员可以用简单的英语查询 bug 或描述所需的功能，接收生成的代码或调试提示。然而，围绕代码质量、安全性和过度依赖仍存在风险。在保持人类监督的同时实现正确的计算机增强的平衡是一个持续的挑战。让我们来看看目前用于编码的AI系统的性能，特别是代码LLMs。
- en: Code LLMs
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码LLMs
- en: 'Quite a few AI models have emerged, each with their own strengths and weaknesses,
    which are continuously competing with each other to improve and deliver better
    results. This comparison should give an overview over some of the largest and
    most popular models:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 出现了相当多的 AI 模型，每个模型都有其优点和缺点，并不断竞争以改进并提供更好的结果。这个比较应该概述一些最大和最流行的模型：
- en: '| **Model** | **Reads files** | **Runs code** | **Tokens** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **读取文件** | **运行代码** | **标记** |'
- en: '| ChatGPT; GPT 3.5/4 | No | No | up to 32k |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT；GPT 3.5/4 | 否 | 否 | 最多32k |'
- en: '| ChatGPT: Code interpreter | Yes | Yes | up to 32k |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT：代码解释器 | 是 | 是 | 最多32k |'
- en: '| Claude 2 | Yes | No | 100k |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Claude 2 | 是 | 否 | 100k |'
- en: '| Bard | No | Yes | 1k |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Bard | 否 | 是 | 1k |'
- en: '| Bing | Yes | No | 32k |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 必应 | 是 | 否 | 32k |'
- en: 'Figure 6.1: Public chat interfaces for software development.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：软件开发的公共聊天界面。
- en: While this competition benefits users by providing a wider range of options,
    it also means that relying solely on ChatGPT may no longer be the optimal choice.
    Users now face the decision of selecting the most suitable model for each specific
    task.The latest wave leverages machine learning and neural networks for more flexible
    intelligence. Powerful pre-trained models like GPT-3 enable context-aware, conversational
    support. Deep learning approaches also empower bug detection, repair recommendations,
    automated testing tools, and code search.Microsoft's GitHub Copilot, which is
    based on OpenAI’s Codex, draws on open source code to suggest full code blocks
    in real-time. According to a Github report in June 2023, developers accepted the
    AI assistant’s suggestions about 30 percent of the time, which suggests that the
    tool can provide useful suggestions, with less experienced developers profiting
    the most.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种竞争为用户提供了更广泛的选择，但这也意味着仅仅依赖 ChatGPT 可能不再是最佳选择。用户现在面临选择为每个特定任务选择最合适的模型的决策。最新的浪潮利用机器学习和神经网络实现了更灵活的智能。强大的预训练模型如
    GPT-3 使得上下文感知、对话支持成为可能。深度学习方法还赋予了 Bug 检测、修复建议、自动化测试工具和代码搜索更强大的能力。微软的 GitHub Copilot
    基于 OpenAI 的 Codex，利用开源代码实时建议完整的代码块。根据2023年6月的 GitHub 报告，开发人员大约30%的时间接受了AI助手的建议，这表明该工具可以提供有用的建议，而经验不足的开发人员受益最多。
- en: '**Codex** is a model, developed by OpenAI. It is capable of parsing natural
    language and generating code and powers GitHub Copilot. A descendant of the GPT-3
    model, it has been fine-tuned on publicly available code from GitHub, 159 gigabytes
    of Python code from 54 million GitHub repositories, for programming applications.'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Codex** 是由 OpenAI 开发的一个模型。它能够解析自然语言并生成代码，为 GitHub Copilot 提供动力。作为 GPT-3 模型的后裔，它已经在
    GitHub 上公开可用的代码上进行了微调，包括来自 5400 万 GitHub 仓库的 159 千兆字节的 Python 代码，用于编程应用。'
- en: 'To illustrate the progress made in creating software, let’s look at quantitative
    results in a benchmark: the **HumanEval dataset**, introduced in the Codex paper
    (“*Evaluating Large Language Models Trained on Code*”, 2021) is designed to test
    the ability of large language models to complete functions based on their signature
    and docstring. It evaluates the functional correctness of synthesizing programs
    from docstrings. The dataset includes 164 programming problems that cover various
    aspects such as language comprehension, algorithms, and simple mathematics. Some
    of the problems are comparable to simple software interview questions. A common
    metric on HumanEval is pass@k (pass@1) – this refers to the fraction of correct
    samples when generating k code samples per problem.This table summarizes the progress
    of AI models on the HumanEval task (source: Suriya Gunasekar and others, “*Textbooks
    Are All You Need*”, 2023; [https://arxiv.org/pdf/2306.11644.pdf](https://arxiv.org/pdf/2306.11644.pdf)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明在创建软件方面取得的进展，让我们看一下基准中的定量结果：HumanEval 数据集，由 Codex 论文介绍（“评估基于代码的大型语言模型的性能”，2021
    年），旨在测试大型语言模型根据其签名和文档字符串完成函数的能力。它评估了从文档字符串合成程序的功能正确性。数据集包括 164 个编程问题，涵盖了语言理解、算法和简单数学等各个方面。其中一些问题类似于简单的软件面试问题。在
    HumanEval 上的一个常见指标是 pass@k（pass@1）-这是指在每个问题生成 k 个代码样本时的正确样本的分数。这张表总结了 AI 模型在 HumanEval
    任务上的进展（来源：Suriya Gunasekar 等人，“*仅需教科书*”，2023 年；[https://arxiv.org/pdf/2306.11644.pdf](https://arxiv.org/pdf/2306.11644.pdf)）：
- en: '![Figure 6.2: Model comparison on coding task benchmarks (HumanEval and MBPP).
    The performance metrics are self-reported. This table only includes models as
    opposed to other approaches, for example reasoning strategies. Llama2’s self-reported
    performance on HumanEval is 29.9%.](img/file43.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2：编码任务基准上的模型比较（HumanEval 和 MBPP）。性能指标是自报的。这张表只包括模型，而不是其他方法，例如推理策略。Llama2
    在 HumanEval 上的自报性能为 29.9%。](img/file43.png)'
- en: 'Figure 6.2: Model comparison on coding task benchmarks (HumanEval and MBPP).
    The performance metrics are self-reported. This table only includes models as
    opposed to other approaches, for example reasoning strategies. Llama2’s self-reported
    performance on HumanEval is 29.9%.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：编码任务基准上的模型比较（HumanEval 和 MBPP）。性能指标是自报的。这张表只包括模型，而不是其他方法，例如推理策略。Llama2
    在 HumanEval 上的自报性能为 29.9%。
- en: 'Please note that the data used in training most LLM models includes some amount
    of source code. For example, The Pile dataset, which was curated by EleutherAI''s
    GPT-Neo for training open-source alternatives of the GPT models, GPT-Neo, includes
    at least about 11% of code from Github (102.18GB). The Pile was used in training
    of Meta’s Llama, Yandex''s YaLM 100B, and many others.Although, HumanEval has
    been broadly used as a benchmark for code LLMs, there are a multitude of benchmarks
    for programming. Here’s an example question and the response from an advanced
    computer science test given to Codex:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练大多数 LLM 模型的数据中包含一定数量的源代码。例如，由 EleutherAI 的 GPT-Neo 等人策划的 Pile 数据集，用于训练
    GPT 模型的开源替代品，其中至少包含来自 Github 的约 11% 代码（102.18GB）。Pile 被用于 Meta 的 Llama、Yandex
    的 YaLM 100B 等的训练。尽管 HumanEval 已广泛用作代码 LLM 的基准，但有多种编程基准可供选择。以下是一个示例问题以及给 Codex
    的一个高级计算机科学测试的响应：
- en: '![Figure 6.3: A question given in a CS2 exam (left) and the Codex response
    (source “My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex
    on CS2 Programming Exercises” James Finnie-Ansley and others, 2023).](img/file44.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3：CS2 考试中的一个问题（左）和 Codex 的响应（来源：“我的 AI 想知道这会不会出现在考试中：测试 OpenAI 的 Codex
    对 CS2 编程练习” James Finnie-Ansley 等人，2023 年）。](img/file44.png)'
- en: 'Figure 6.3: A question given in a CS2 exam (left) and the Codex response (source
    “My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2
    Programming Exercises” James Finnie-Ansley and others, 2023).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：CS2 考试中的一个问题（左）和 Codex 的响应（来源：“我的 AI 想知道这会不会出现在考试中：测试 OpenAI 的 Codex 对
    CS2 编程练习” James Finnie-Ansley 等人，2023 年）。
- en: 'There are many interesting studies that shed a light on AI’s capability to
    help software developers or that expand on that capability as summarized in this
    table:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多有趣的研究揭示了 AI 帮助软件开发人员的能力，或者扩展了该能力，如在这个表格中总结的那样：
- en: '| **Authors** | **Publication Date** | **Conclusions** | **Task** | **Model/Strategy
    Analyzed** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **作者** | **出版日期** | **结论** | **任务** | **分析的模型/策略** |'
- en: '| Abdullah Al Ishtiaq and others | April 2021 | Pre-trained language models
    like BERT can enhance code search through improved semantic understanding. | Code
    search | BERT |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Abdullah Al Ishtiaq 等人 | 2021 年 4 月 | 类似 BERT 的预训练语言模型可以通过改进语义理解来增强代码搜索。
    | 代码搜索 | BERT |'
- en: '| Mark Chen et al. (OpenAI) | July 2021 | Evaluates Codex on code generation,
    shows potential to advance program synthesis | Code generation | Codex |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Mark Chen 等人（OpenAI） | 2021 年 7 月 | 对 Codex 进行代码生成评估，显示提升程序合成的潜力 | 代码生成 |
    Codex |'
- en: '| Ankita Sontakke and others | March 2022 | Even state-of-the-art models produce
    poor quality code summaries, indicating they may not understand code. | Code summarization
    | Transformer models |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Ankita Sontakke 等人 | 2022 年 3 月 | 即使是最先进的模型也会产生质量低劣的代码摘要，表明它们可能不理解代码。 | 代码摘要
    | Transformer 模型 |'
- en: '| Bei Chen et al. (Microsoft) | July 2022 | CODE-T leverages LLMs to auto-generate
    test cases, reducing human effort and improving code evaluation. It achieves 65.8%
    HumanEval pass@1. | Code generation, testing | CODET |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| Bei Chen 等人（微软） | 2022 年 7 月 | CODE-T 利用 LLM 自动生成测试用例，减少人力成本并提高代码评估。它达到了
    65.8% 的 HumanEval pass@1。| 代码生成、测试 | CODET |'
- en: '| Eric Zelikman et al. (Stanford) | December 2022 | Parsel framework enables
    LLMs to decompose problems and leverage strengths, improving performance on hierarchical
    reasoning | Program synthesis, planning | Codex |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| Eric Zelikman 等人（斯坦福大学） | 2022 年 12 月 | Parsel 框架使 LLM 能够分解问题并利用其优势，在层次化推理方面提高性能
    | 程序合成、规划 | Codex |'
- en: '| James Finnie-Ansley and others | January 2023 | Codex outperforms most students
    on advanced CS2 programming exams. | CS2 programming | Codex |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| James Finnie-Ansley 等人 | 2023 年 1 月 | Codex 在高级 CS2 编程考试中表现优于大多数学生。 | CS2
    编程 | Codex |'
- en: '| Yue Liu and others | February 2023 | Existing automated code generation has
    limitations in robustness and reliability. | Code generation | 5 NMT models |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 刘越 等人 | 2023 年 2 月 | 现有的自动化代码生成在鲁棒性和可靠性方面存在限制。 | 代码生成 | 5 个 NMT 模型 |'
- en: '| Mingyang Geng and others | February 2023 | A two-stage approach significantly
    increased effectiveness of code summarization. | Code summarization | LLM + reinforcement
    learning |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 耿明扬 等人 | 2023 年 2 月 | 两阶段方法显著增加了代码摘要的有效性。 | 代码摘要 | LLM + 强化学习 |'
- en: '| Noah Shinn et al. | March 2023 | Reflexion enables trial-and-error learning
    via verbal reflection, achieving 91% HumanEval pass@1 | Coding, reasoning | Reflexion
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Noah Shinn 等人 | 2023 年 3 月 | 通过口头反思，Reflexion 实现了试错学习，达到了 91% 的 HumanEval
    pass@1 | 编码、推理 | Reflexion |'
- en: '| Haoye Tian and others | April 2023 | ChatGPT shows promise for programming
    assistance but has limitations in robustness, generalization, and attention span.
    | Code generation, program repair, code summarization | ChatGPT |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 田昊烨 等人 | 2023 年 4 月 | ChatGPT 在编程辅助方面表现出潜力，但在鲁棒性、泛化性和注意力跨度方面存在限制。 | 代码生成、程序修复、代码摘要
    | ChatGPT |'
- en: '| Chuqin Geng and others | April 2023 | ChatGPT demonstrates impressive capabilities
    for intro programming education but would only get a B- grade as a student. |
    Intro functional programming course | ChatGPT |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 耿初琴 等人 | 2023 年 4 月 | ChatGPT 在初级编程教育中展现出令人印象深刻的能力，但作为学生只能获得 B- 的成绩。 | 初级函数式编程课程
    | ChatGPT |'
- en: '| Xinyun Chen and others | April 2023 | Self-debugging technique enables language
    models to identify and correct mistakes in generated code. | Code generation |
    Self-Debugging |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 陈欣云 等人 | 2023 年 4 月 | 自调试技术使语言模型能够识别和纠正生成代码中的错误。 | 代码生成 | Self-Debugging
    |'
- en: '| Masum Hasan and others | April 2023 | Transforming text to an intermediate
    formal language enabled more efficient app code generation from descriptions.
    | App code generation | Seq2seq networks |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Masum Hasan 等人 | 2023 年 4 月 | 将文本转换为中间形式语言可从描述中更高效地生成应用程序代码。 | 应用程序代码生成 |
    Seq2seq 网络 |'
- en: '| Anis Koubaa and others | May 2023 | ChatGPT struggles with complex programming
    problems and is not yet suitable for fully automated programming. It performs
    much worse than human programmers. | Programming problem solving | ChatGPT |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Anis Koubaa 等人 | 2023 年 5 月 | ChatGPT 在复杂编程问题上表现困难，尚不适合完全自动化编程。它的表现远远不如人类程序员。
    | 编程问题解决 | ChatGPT |'
- en: '| Wei Ma and others | May 2023 | ChatGPT understands code syntax but is limited
    in analyzing dynamic code behavior. | Complex code analysis | ChatGPT |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 马伟 等人 | 2023 年 5 月 | ChatGPT 理解代码语法，但在分析动态代码行为方面受到限制。 | 复杂代码分析 | ChatGPT
    |'
- en: '| Raymond Li et al. (BigCode) | May 2023 | Introduces 15.5B parameter StarCoder
    trained on 1 trillion GitHub tokens, achieves 40% HumanEval pass@1 | Code generation,
    multiple languages | StarCoder |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Raymond Li 等人（BigCode）| 2023 年 5 月 | 推出基于 1 万亿个 GitHub token 训练的 155 亿参数
    StarCoder，达到了 40% 的人类审核一致性 | 代码生成，多种语言 | StarCoder |'
- en: '| Amos Azaria and others | June 2023 | ChatGPT has errors and limitations,
    so outputs should be independently verified. It is best used by experts well-versed
    in the domain. | General capabilities and limitations | ChatGPT |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Amos Azaria 等人 | 2023 年 6 月 | ChatGPT 存在错误和限制，因此输出应独立验证。最好由精通领域的专家使用。 | 总体能力和限制
    | ChatGPT |'
- en: '| Adam Hörnemalm | June 2023 | ChatGPT increased efficiency for coding and
    planning but struggled with communication. Developers wanted more integrated tooling.
    | Software development | ChatGPT |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Adam Hörnemalm | 2023 年 6 月 | ChatGPT 在编码和规划方面提高了效率，但在交流方面遇到了困难。开发者希望有更多集成的工具。
    | 软件开发 | ChatGPT |'
- en: '| Suriya Gunasekar et al. (Microsoft) | June 2023 | High-quality data enables
    smaller models to match larger models, altering scaling laws | Code generation
    | Phi-1 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Suriya Gunasekar 等人（微软）| 2023 年 6 月 | 高质量的数据使得较小的模型能够匹配较大的模型，改变了缩放定律 | 代码生成
    | Phi-1 |'
- en: 'Figure 6.2: Literature review of AI for programming tasks. The publication
    dates refer mostly to the preprint releases.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：针对编程任务的人工智能文献综述。发布日期主要指发行的未辑集论文。
- en: 'This is just a small subset of studies, but hopefully this helps to shed a
    light on some of the developments in the field. Recent research explored how ChatGPT
    can support programmers’ daily work activities like coding, communication, and
    planning. Other research describes new models (such as Codex, StarCoder, or Phi-1)
    or approaches for planning or reasoning to execute these models.Most recently,
    the paper “*Textbooks Are All You Need*” by Suriya Gunasekar and others at Microsoft
    Research (2023) introduced phi-1, a 1.3B parameter Transformer-based language
    model for code. The paper demonstrates how high-quality data can enable smaller
    models to match larger models for code tasks. The authors start with a 3 TB corpus
    of code from The Stack and StackOverflow. A large language model (LLM) filters
    this to select 6B high-quality tokens. Separately, GPT-3.5 generates 1B tokens
    mimicking textbook style. A small 1.3B parameter model phi-1 is trained on this
    filtered data. Phi-1 is then fine-tuned on exercises synthesized by GPT-3.5\.
    Results show phi-1 matches or exceeds the performance of models over 10x its size
    on benchmarks like HumanEval and MBPP.The core conclusion is that high-quality
    data significantly impacts model performance, potentially altering scaling laws.
    Instead of brute force scaling, data quality should take precedence. The authors
    reduce costs by using a smaller LLM to select data, rather than expensive full
    evaluation. Recursively filtering and retraining on selected data could enable
    further improvements. It’s important to appreciate that there’s a massive step
    change in difficulty between short code snippets, where task specifications are
    translated directly into code and the right API calls have to be issued in a sequence
    specific to the task, and generating complete programs, which relies on a much
    deeper understanding and reasoning about the task, the concepts behind, and planning
    how to accomplish it. However, reasoning strategies can make a big difference
    for short snippets as well as the paper “*Reflexion: Language Agents with Verbal
    Reinforcement Learning*” by Noah Shinn and others (2023) shows. The authors propose
    a framework called Reflexion that enables LLM agents (implemented in LangChain)
    to learn quickly and efficiently from trial-and-error using verbal reinforcement.
    The agents verbally reflect on task feedback signals and store their reflective
    text in an episodic memory buffer, which helps the agents make better decisions
    in subsequent trials. The authors demonstrate the effectiveness of Reflexion in
    improving decision-making in diverse tasks such as sequential decision-making,
    coding, and language reasoning. Reflexion has the potential to outperform previous
    state-of-the-art models, such as GPT-4, in specific tasks, as shown by its 91%
    pass@1 accuracy on the HumanEval coding benchmark, which beats any approach previously
    published including GPT-4’s 67% (as reported by OpenAI).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是研究中的一个小部分，但希望这能帮助启发一些该领域的发展。最近的研究探讨了ChatGPT如何支持程序员的日常工作活动，如编码、沟通和规划。其他研究描述了新模型（如Codex、StarCoder或Phi-1）或用于规划或推理执行这些模型的方法。最近，微软研究部门的陈茜亚·古纳塞卡等人在《*仅需教科书*》一文中介绍了
    phi-1，这是一个基于Transformer的语言模型，具有 13 亿参数。该论文展示了高质量数据如何使较小的模型匹配用于代码任务的更大模型。作者首先从
    The Stack 和 StackOverflow 中获得 3TB 代码语料库。大语言模型（LLM）对其进行筛选，选择了 60 亿高质量的 token。另外，GPT-3.5
    生成了 10 亿模拟教科书风格的 token。一个小的 13 亿参数模型 phi-1 在此筛选数据上进行训练。然后，phi-1 在由 GPT-3.5 合成的练习上进行微调。结果显示，phi-1
    在 HumanEval 和 MBPP 等基准测试中与其 10 倍大小的模型相匹配或超越了其性能。核心结论是高质量数据显著影响模型性能，潜在地改变了标度律。数据质量应该优先于蛮力标度。作者通过使用较小的LLM来选择数据，而不是昂贵的全面评估，降低了成本。递归过滤和在选定数据上重新训练可能会带来进一步的改进。需要注意的是，短代码片段与生成完整程序存在着巨大的困难程度差异，其中任务规范直接转化为代码，正确的
    API 调用必须根据任务特定的顺序发出，而生成完整程序则依赖对任务、背后的概念以及计划如何完成的更深入理解和推理。然而，推理策略也可以对短代码片段产生重大影响，正如《*反思：具有口头强化学习的语言代理*》一文所示。作者提出了一个名为
    Reflexion 的框架，该框架使 LLM 代理（使用LangChain实现）能够通过试错快速、有效地学习。代理人口头反思任务反馈信号，并将其反思文本存储在一个片段性记忆缓冲区中，这有助于代理在随后的实验中做出更好的决策。作者展示了
    Reflexion 在改善顺序决策、编码和语言推理等各种任务中决策的有效性。如其在 HumanEval 编码基准测试中的 91% 的一次通过准确率所示，Reflexion
    有潜力在特定任务中胜过之前的最先进模型，即使包括 GPT-4 在内也是如此。 （根据 OpenAI 报告的数据，GPT-4 的准确率为 67%。）
- en: Outlook
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Outlook
- en: Looking forward, advances in multimodal AI may further evolve programming tools.
    Systems capable of processing code, documentation, images, and more could enable
    a more natural workflow. The future of AI as a programming partner is bright,
    but requires thoughtful coordination of human creativity and computer-enabled
    productivity.While promising, effectively leveraging AI programming assistants
    requires establishing standards through workshops to create useful prompts and
    pre-prompts for tasks. Focused training ensures proper validation of generated
    code. Integrating AI into existing environments rather than stand-alone browsers
    improves developer experience. As research continues, AI programming assistants
    present opportunities to increase productivity if thoughtfully implemented with
    an understanding of limitations. With careful oversight, AI stands to automate
    tedious tasks, freeing developers to focus on complex programming problems.Legal
    and ethical concerns arise during the pre-training phase, specifically regarding
    the rights of content creators whose data is used to train the models. Copyright
    laws and fair use exemptions are debated in relation to the use of copyrighted
    data by machine learning models.For example, the Free Software Foundation has
    raised concerns about potential copyright violations associated with code snippets
    generated by Copilot and Codex. They question whether training on public repositories
    falls within fair use, how developers can identify infringing code, the nature
    of machine learning models as modifiable source code or compilations of training
    data, and the copyrightability of machine learning models. Further, an internal
    GitHub study found that a small percentage of generated code contained direct
    copies from the training data, including incorrect copyright notices. OpenAI recognizes
    the legal uncertainty surrounding these copyright implications and calls for authoritative
    resolution. The situation has been compared to the Authors Guild, Inc. v. Google,
    Inc. court case regarding fair use of text snippets in Google Books. Ideally,
    we want to be able to do this without relying on a cloud-based service that charges
    us for a request and that may force us to give up the ownership of our data. However,
    it’s very convenient to outsource the AI so that all we have to implement are
    the prompts and the strategies of how to issue calls with our client. Many of
    the open-source models have made impressive progress on coding tasks, and they
    have the advantage of full transparency and openness about their development process.
    Most of them have been trained on code that’s been released under permissive licenses,
    therefore they are not coming with the same legal concerns as other commercial
    products.There is a broader impact of these systems beyond coding itself on education
    and the ecosystem around software development. For example, the emergence of ChatGPT
    resulted in a massive traffic decline for the popular Stack Overflow question-and-answer
    forum for programmers. After initially blocking any contributions generated using
    large language models (LLMs), Stack Overflow launched Overflow AI to bring enhanced
    search, knowledge ingestion, and other AI features to Stack products. New semantic
    search is to provide intelligent, conversational results using Stack’s knowledge
    base.Large language models like Codex and ChatGPT excel in code generation for
    common problems, but struggle with new ones and long prompts. Most importantly,
    ChatGPT understands syntax well but has limitations in analyzing dynamic code
    behavior. In programming education, AI models surpass many students but have a
    lot of room for improvement, however, they haven’t yet reached the level of being
    able to replace programmers and human intelligence. Scrutiny is necessary as mistakes
    can occur, making expert supervision crucial. The potential of AI tools in coding
    is encouraging but challenges remain in robustness, generalization, attention
    span, and true semantic understanding. Further development is needed to ensure
    reliable and transparent AI programming tools that can augment developers, allowing
    them to write code faster with fewer bugs.In the next section, we’ll see how we
    can generate software code with LLMs and how we can execute this from within LangChain.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Writing code with LLMs
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLMs编写代码
- en: 'Let’s start off by applying a model to write code for us. We can use one of
    the publicly available models for generating code. I’ve listed a few examples
    before such as ChatGPT or Bard. From LangChain, we can call OpenAI’s LLMs, PaLM’s
    code-bison, or a variety of open-source models for example through Replicate,
    HuggingFace Hub, or – for local models – Llama.cpp, GPT4All, or HuggingFace Pipeline
    integrations.Let’s have a look at StarCoder This screenshot shows the model in
    a playground on HuggingFace Spaces:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始应用一个模型为我们编写代码。我们可以使用公开可用的模型来生成代码。我之前列举过一些示例，如 ChatGPT 或 Bard。从 LangChain，我们可以调用
    OpenAI 的LLMs，PaLM 的 code-bison，或者通过 Replicate、HuggingFace Hub 等各种开源模型，或者 – 对于本地模型
    – Llama.cpp，GPT4All，或者 HuggingFace Pipeline 集成。让我们看看StarCoder。这个屏幕截图显示了在HuggingFace
    Spaces上游玩的模型：
- en: '![Figure 6.3: StarCoder Models Playground. We can choose between different
    models: StarCoder, StarCoderPlus, StarCoderBase.This is available at https://huggingface.co/spaces/bigcode/bigcode-playground](img/file45.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3: StarCoder 模型游乐场。我们可以在不同的模型之间选择：StarCoder，StarCoderPlus，StarCoderBase。这是在
    https://huggingface.co/spaces/bigcode/bigcode-playground 上提供的。](img/file45.png)'
- en: 'Figure 6.3: StarCoder Models Playground. We can choose between different models:
    StarCoder, StarCoderPlus, StarCoderBase.This is available at https://huggingface.co/spaces/bigcode/bigcode-playground'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '图6.3: StarCoder 模型游乐场。我们可以在不同的模型之间选择：StarCoder，StarCoderPlus，StarCoderBase。这是在
    https://huggingface.co/spaces/bigcode/bigcode-playground 上提供的。'
- en: 'Please note that, as the description says, the StarCoder model is not instruction-tuned,
    which means that we can’t give it tasks to do. We can’t say “write a class that…”
    but we can ask it to complete a text as shown in the screenshot, where we prompt
    the model with “`# dataclass of customer including an alphanumeric id, a name,
    and a birthday`” – let’s try this!We can toggle settings for temperature, max
    new tokens, top-n, and a repetition penalty. For anything non-trivial, we need
    to get the max new tokens setting up.I am getting this code, which gives us a
    useful data model for our hypothetical customer:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如描述所说，StarCoder 模型没有经过指导调整，这意味着我们不能给它任务去做。我们不能说“写一个类…”，但我们可以要求它完成一个文本，就像屏幕截图中显示的那样，我们用“`#
    dataclass of customer including an alphanumeric id, a name, and a birthday`”来提示模型
    – 让我们试试这个！我们可以切换温度、最大新标记、前n个和重复惩罚的设置。对于任何非平凡的事情，我们需要设置最大新标记。我得到了这段代码，它为我们的假想客户提供了一个有用的数据模型：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is missing the imports, which would usually come before the comment prompt,
    so I can’t fault the model for it, we need to add these two lines to the top:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 缺少导入语句，这些通常会出现在注释提示之前，所以我不能指责模型，我们需要将这两行添加到顶部：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This doesn’t throw an error when we run it as is, which is impressive, however,
    there are a few things not ideal or even wrong with it when we look a bit more
    in detail.On the first glance, I personally don’t like the commenting style and
    the lack of docstrings. There are some `print()` statements left in the code;
    often this is done for debugging purposes. This shouldn’t be in production code.
    It’s probably not ideal to have empty strings as defaults for `firstname` and
    `lastname`. I would expect the customer id to be assigned based on a hashing function
    – I can’t see why the id should have exactly 8 characters as enforced in the validation
    function for the property. There are more problems with this code that affect
    its correctness and readability, but there are a few more crucial problems in
    the code including attempting to write read-only attributes. `firstname` and `lastname`
    in the dataclass – `frozen=True` in a dataclass means that attributes can’t be
    changed after creation. The logic about creating a variable name from `firstname`
    and `lastname` using regular expressions in a non-standard syntax is strange to
    say the least – and incorrect. The last name gets dropped in the process. The
    filters on the reversed name is also highly suspect. I leave it here. This is
    giving us some good ideas and a structure to start with, but it’s not production
    code. It doesn’t even work. You can see this code as `customer.py` in the book’s
    Github repo.Let’s try this again. Perhaps we started off on a bad foot. We started
    a code snippet in bad syntax expected for beginners and expected code that works.
    That’s not realistic. Let’s try again, and start with a prompt that is more up
    to standard:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们按原样运行它时，这不会引发错误，这令人印象深刻，然而，当我们更仔细地观察时，会发现有一些不太理想甚至错误的地方。乍一看，我个人不喜欢注释风格和缺少文档字符串。代码中留有一些`print()`语句；通常这是为了调试目的而做的。这不应该出现在生产代码中。将空字符串作为`firstname`和`lastname`的默认值可能并不理想。我希望客户
    ID 是基于哈希函数分配的 – 我不明白为什么 ID 必须有 8 个字符，正如在属性的验证函数中强制执行的那样。这段代码存在更多问题，影响了其正确性和可读性，但代码中还存在一些更为关键的问题，包括尝试编写只读属性。在数据类中的`firstname`和`lastname`
    – 在数据类中`frozen=True`意味着属性在创建后不能被更改。使用正则表达式从`firstname`和`lastname`创建变量名的逻辑在非标准语法中是奇怪的，至少是不正确的。在反转名称上的过滤器也非常可疑。我就说到这里。这给了我们一些好的想法和一个开始的结构，但这不是生产代码。它甚至都不工作。您可以在书的
    Github 仓库中看到此代码为`customer.py`。让我们再试一次。也许我们一开始就走错了路。我们以期望适合初学者的错误语法开始了代码片段，并期望能够工作的代码。这不现实。让我们再试一次，并从一个更符合标准的提示开始。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the following result:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It’s good to see that the customer id is created using a hash as expected.
    There’s some boilerplate code for comparing two different customer objects. However,
    again there are problems, similar ones to before. First, it’s missing the imports,
    which I don’t understand after our prompt which would be a module docstring to
    be found at the start of a file, and the imports would come right after. Second,
    it’s again attempting to set an attribute after initialization of the class that’s
    supposed to be frozen. After fixing these two problems, we get our first `Customer()`,
    then there’s a problem, where the customer id is referenced with the wrong name.
    After fixing this, we can initialize our customer, look at the attributes, and
    compare one customer to another. I can see how this approach is starting to become
    useful for writing boilerplate code. You can see this code as `customer2.py` in
    the book’s Github repo. Let’s try an instruction-tuned model so we can give it
    tasks! StarChat, which is based on StarCoder, is available on HuggingFace under
    [https://huggingface.co/spaces/HuggingFaceH4/starchat-playground](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)
    This screenshot shows an example with StarChat:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴看到客户 ID 如预期般使用哈希创建了。有一些模板代码用于比较两个不同的客户对象。然而，问题又出现了，与之前类似。首先，它缺少了导入，我不明白我们提示后会在文件开头找到一个模块文档字符串，导入会紧随其后。其次，它又一次在类初始化后尝试设置一个应该是冻结的属性。修复了这两个问题后，我们得到了我们的第一个`Customer()`，然后有一个问题，客户
    ID 使用了错误的名称引用。修复了这个问题后，我们可以初始化我们的客户，查看属性，并将一个客户与另一个进行比较。我能看到这种方法开始对编写模板代码变得有用。您可以在书的
    Github 仓库中看到此代码为`customer2.py`。让我们尝试一种指令导向的模型，以便我们可以给它任务！基于 StarCoder 的 StarChat
    可在 HuggingFace 的 [https://huggingface.co/spaces/HuggingFaceH4/starchat-playground](https://huggingface.co/spaces/HuggingFaceH4/starchat-playground)
    下载。这个屏幕截图显示了一个使用 StarChat 的示例：
- en: '![Figure 6.4: StarChat implementing a function in Python for calculating prime
    numbers. Please note that not all the code is visible in the screenshot.](img/file46.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4: StarChat 在 Python 中实现计算素数功能。请注意，截图中并非所有代码都可见。](img/file46.png)'
- en: 'Figure 6.4: StarChat implementing a function in Python for calculating prime
    numbers. Please note that not all the code is visible in the screenshot.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.4: StarChat 在 Python 中实现计算素数功能。请注意，截图中并非所有代码都可见。'
- en: 'You can find the complete code listing on Github.For this very example that
    should be well-known in first year Computer Science courses, no imports are needed.
    The algorithm’s implementation is straightforward. It executes right away and
    gives the expected result. Within LangChain, we can use the `HuggingFaceHub` integration
    like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Github 上找到完整的代码清单。对于这个在第一年计算机科学课程中应该很熟悉的例子，不需要导入任何内容。算法的实现是直接的。它立即执行并给出了预期的结果。在
    LangChain 中，我们可以像这样使用 `HuggingFaceHub` 集成：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As of August 2023, this LangChain integration has some issues with timeouts
    – hopefully, this is going to get fixed soon. We are not going to use it here.Llama2
    is not one of the best models for coding with a pass@1 of about 29 as mentioned
    earlier, however, we can try it out on HuggingFace chat:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2023 年 8 月，该 LangChain 集成在超时方面存在一些问题 - 希望很快能够解决。我们这里不打算使用它。正如之前提到的，Llama2
    并不是最好的编码模型，其 pass@1 约为 29，然而，我们可以在 HuggingFace 聊天上尝试一下：
- en: '![Figure 6.5: HuggingFace chat with Llama2 at https://huggingface.co/chat/](img/file47.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5: 在 https://huggingface.co/chat/ 上与 Llama2 聊天](img/file47.png)  '
- en: 'Figure 6.5: HuggingFace chat with Llama2 at https://huggingface.co/chat/'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.5: 在 https://huggingface.co/chat/ 上与 Llama2 聊天'
- en: 'Please note that this is only the beginning of the output. Llama2 finds a good
    implementation and the explanations are spot on. Well done, StarCoder and Llama2!
    – Or perhaps, this was just too easy? There so many ways to get code completion
    or generation. We can run even try a small local model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这只是输出的开始部分。Llama2 找到了一个良好的实现，解释非常到位。干得好，StarCoder 和 Llama2！ - 或者，这太容易了？有很多种方法可以获取代码完成或生成代码。我们甚至可以尝试一个小的本地模型：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'CodeGen is a model by Salesforce AI Research. CodeGen 350 Mono performed 12.76%
    pass@1 in HumanEval. As of July 2023, new versions of CodeGen was released with
    only 6B parameters that are very competitive, which clocks in at a performance
    of 26.13%. This last model was trained on the BigQuery dataset containing C, C++,
    Go, Java, Javascript, and Python, as well as the BigPython dataset, which consists
    of 5.5TB of Python code. Another interesting, small model is Microsoft’s CodeBERT
    (2020), a model for program synthesis that has been trained and tested on Ruby,
    Javascript, Go, Python, Java, and PHP.. Since this model was released before the
    HumanEval benchmark, the performance statistics for the benchmark were not part
    of the initial publication.We can now get the output from the pipeline directly
    like this:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CodeGen 是 Salesforce AI 研究的一个模型。CodeGen 350 Mono 在 HumanEval 中的 pass@1 达到了 12.76%。截至
    2023 年 7 月，发布了新版本的 CodeGen，仅使用了 60 亿参数，非常具有竞争力，性能达到了 26.13%。这个最新模型是在包含 C、C++、Go、Java、Javascript
    和 Python 的 BigQuery 数据集以及包含 5.5TB Python 代码的 BigPython 数据集上训练的。另一个有趣的小模型是微软的 CodeBERT（2020），这是一个用于程序合成的模型，已经在
    Ruby、Javascript、Go、Python、Java 和 PHP 上进行了训练和测试。由于这个模型在 HumanEval 基准发布之前发布，所以基准测试的性能统计数据并未包含在最初的发布中。我们现在可以直接从流程中获取输出，方法如下：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Alternatively, we can wrap this pipeline via the LangChain integration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过 LangChain 集成来包装这个流程：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a bit verbose. There’s also the more convenient constructor method `HuggingFacePipeline.from_model_id()`.I
    am getting something similar to the StarCoder output. I had to add an `import
    math`, but the function works. This pipeline we could use in a LangChain agent,
    however, please note that this model is not instruction-tuned, so you cannot give
    it tasks, only completion tasks. You can also use all of these models for code
    embeddings. Other models that have been instruction-tuned and are available for
    chat, can act as your techie assistant to help with advice, document and explain
    existing code, or translate code into other programming languages – for the last
    task they need to have been trained on enough samples in these languages.Please
    note that the approach taken here is a bit naïve. For example, we could be taking
    more samples and choose between them such as in the a few of the papers we’ve
    discussed.Let’s now try to implement a feedback cycle for code development, where
    we validate and run the code and change it based on feedback.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点啰嗦。还有更方便的构造方法`HuggingFacePipeline.from_model_id()`。我得到了类似于StarCoder输出的东西。我不得不添加一个`import
    math`，但函数确实有效。这个管道我们可以在LangChain代理中使用，但请注意，这个模型不是经过指令调整的，所以你不能给它任务，只能完成任务。你也可以将所有这些模型用于代码嵌入。已经经过指令调整并且可用于聊天的其他模型可以作为你的技术助手，帮助提供建议，解释和说明现有代码，或将代码翻译成其他编程语言
    - 对于最后一个任务，它们需要在这些语言中经过足够多的样本训练。请注意，这里采用的方法有点天真。例如，我们可以采集更多样本并在它们之间进行选择，就像我们讨论过的一些论文中的情况一样。现在让我们尝试实现一个代码开发的反馈循环，其中我们根据反馈验证和运行代码并进行更改。
- en: Automated software development
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化软件开发
- en: We’ll now going to write a fully-automated agent that will write code for us
    and fix any problems responding to feedback.In LangChain, we have several integrations
    for code execution like these the `LLMMathChain`, which executes Python code to
    solve math questions, and the `BashChain` that executes Bash terminal commands,
    which can help with system administration tasks. However, these are for problem
    solving with code rather than creating a software.This can, however, work quite
    well.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要编写一个完全自动化的代理，它将为我们编写代码并根据反馈修复任何问题。在LangChain中，我们有几个用于代码执行的集成，比如`LLMMathChain`，它执行Python代码来解决数学问题，以及`BashChain`，它执行Bash终端命令，可以帮助处理系统管理任务。但是，这些是用于通过代码解决问题而不是创建软件的。但是，这样做可能效果很好。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see how the prime number calculations get processed quite well under
    the hood between OpenAI’s LLM and the Python interpreter:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在OpenAI的LLM和Python解释器之间，质数计算是如何在内部进行处理的：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We get to the right answer about the prime numbers, however, it’s not entirely
    clear how this approach would scale for building software products, where it is
    about modules, abstractions, separation of concerns, and maintainable code. There
    are a few interesting implementations for this around. The MetaGPT library approaches
    this with an agent simulation, where different agents represent job roles in a
    company or IT department:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了关于质数的正确答案，但是这种方法如何扩展到构建软件产品方面并不完全清楚，其中涉及模块、抽象、关注点分离和可维护代码。这方面有一些有趣的实现。MetaGPT库采用了一个代理模拟，其中不同的代理代表公司或IT部门中的职务角色：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is a really inspiring use case of an agent simulation. The llm-strategy
    library by Andreas Kirsch generates code for dataclasses using decorator patterns.
    Other examples for automatic software development include AutoGPT and BabyGPT
    although these often tend to get stuck in loops or stop because of failures. A
    simple planning and feedback loop like this can be implemented in LangChain with
    a ZeroShot Agent and a planner. The Code-It project by Paolo Rechia and Gpt-Engineer
    by AntonOsika both follows such as pattern as illustrated in this graph for Code-It:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常鼓舞人心的代理模拟用例。Andreas Kirsch的llm-strategy库使用装饰器模式为数据类生成代码。自动软件开发的其他示例包括AutoGPT和BabyGPT，尽管它们通常会陷入循环或因失败而停止。像这样的简单规划和反馈循环可以在LangChain中通过ZeroShot
    Agent和一个规划器来实现。Paolo Rechia的Code-It项目和AntonOsika的Gpt-Engineer都遵循这样的模式，如此图表所示：
- en: '![Figure 6.6: Code-It control flow (source: https://github.com/ChuloAI/code-it).](img/file48.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6：代码流程（来源：https://github.com/ChuloAI/code-it）。](img/file48.jpg)'
- en: 'Figure 6.6: Code-It control flow (source: https://github.com/ChuloAI/code-it).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：代码流程（来源：https://github.com/ChuloAI/code-it）。
- en: 'Many of these steps consist of specific prompts that are sent to LLMs with
    instructions to break down the project or to set up the environment. It’s quite
    impressive to implement the full feedback loop with all the tools. We can implement
    a relatively simple feedback loop in different ways in LangChain, for example
    using `PlanAndExecute` chain, a `ZeroShotAgent`, or `BabyAGI`. Let’s go with `PlanAndExecute`!The
    main idea is to set up a chain and execute it with the objective of writing a
    software, like this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤中的许多是发送给LLMs的具体提示，指示它们拆解项目或设置环境。使用所有工具实现完整的反馈循环是非常令人印象深刻的。在LangChain中，我们可以以不同的方式实现相对简单的反馈循环，例如使用`PlanAndExecute`链，`ZeroShotAgent`或`BabyAGI`。让我们选择`PlanAndExecute`！主要思路是设置一个链并执行它，目的是编写软件，就像这样：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'I am omitting the imports here, but you can find the full implementation in
    the Github repo of the book. The other options can be found there as well. There
    are a few more pieces to this, but this could already write some code, depending
    on the instructions that we give. One thing we need is clear instructions for
    a language model to write Python code in a certain form:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我这里省略了导入，但你可以在书的Github仓库中找到完整的实现。其他选项也可以在那里找到。还有一些其他部分，但根据我们的指示，这已经可以编写一些代码。我们需要的一件事是为语言模型提供明确的指导，以便以某种形式撰写Python代码：
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We need to make sure, we take a model that is able to come up with code. We’ve
    discussed already the models we can choose between for this. I’ve chosen a longer
    context so we don’t get cut off in the middle of a function, and a low temperature,
    so it doesn’t get too wild.However, on its own this model wouldn’t be able to
    store it to file, do anything meaningful with it, and act on the feedback from
    the execution. We need to come up with code and then test it, and see if it works.
    Let’s see how we can implement this – that’s in the `tools` argument to the agent
    executor, let’s see how this is defined!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保选择一个可以生成代码的模型。我们已经讨论过可以选择的模型。我选择了一个较长的上下文，这样我们就不会在函数的中间中断了，还有一个较低的温度，这样就不会变得太疯狂。然而，单独来看，这个模型是无法将其存储到文件中，也无法对其进行任何有意义的操作，并根据执行的反馈进行行动的。我们需要想出代码，然后测试它，看它是否有效。让我们看看我们可以如何实现这个
    - 那就是在代理执行器的`tools`参数中，让我们看看这是如何定义的！
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `PythonDeveloper` class has all the logic about taking tasks given in any
    form and translating them into code. I won’t go into all the detail here, however,
    the main idea is here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`PythonDeveloper`类包含了将任务以任何形式给出并将其转换为代码的所有逻辑。我不会在这里详细介绍所有的细节，不过，主要的思路在这里：'
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'I am again leaving out a few pieces. The error handling if very simplistic
    here. In the implementation on Github, we can distinguish different kinds of errors
    we are getting, such as these:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我再次略去了一些部分。这里的错误处理非常简单。在Github上的实现中，我们可以区分我们得到的不同种类的错误，比如这些：
- en: '`ModuleNotFoundError`: this means that the code tries to work with packages
    that we don’t have installed. I’ve implemented logic to install these packages.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ModuleNotFoundError`：这意味着代码尝试使用我们没有安装的包。我已经实现了安装这些包的逻辑。'
- en: '`NameError`: using variable names that don’t exist.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NameError`：使用不存在的变量名。'
- en: '`SyntaxError`: the code often doesn’t close parentheses or is not even code'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SyntaxError`：代码经常不关闭括号，或者根本不是代码'
- en: '`FileNotFoundError`: the code relies on files that don’t exist. I’ve found
    a few times that the code tried showing images that were made up.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FileNotFoundError`：代码依赖于不存在的文件。我发现有几次代码试图显示虚构的图像。'
- en: '`SystemExit`: if something more dramatic happens and Python crashes.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SystemExit`：如果发生了更严重的情况，Python崩溃了。'
- en: 'I’ve implemented logic to install packages for `ModuleNotFoundError`, and clearer
    messages for some of these problems. In the case of missing images, we could add
    a generative image model to create these. Returning all this as enriched feedback
    to the code generation, results in increasingly specific output such as this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经实现了逻辑来安装`ModuleNotFoundError`的包，以及在某些问题上更清晰的消息。在缺少图像的情况下，我们可以添加一个生成图像模型来创建这些图像。将所有这些作为丰富的反馈返回给代码生成，会产生越来越具体的输出，比如这样：
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The Python code itself gets compiled and executed in a subdirectory and we
    take redirect the output of the Python execution in order to capture it – both
    of this is implemented as Python contexts. Please be cautious with executing code
    on your system, because some of these approaches are quite sensitive to security,
    because they lack a sandboxed environment, although tools and frameworks exists
    such as codebox-api, RestrictedPython, pychroot, or setuptools’ DirectorySandbox
    to just name a few of these for Python.So let’s set tools:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Python代码本身被编译并在一个子目录中执行，我们将Python执行的输出重定向以捕获它——这两者都是作为Python上下文实现的。请小心在您的系统上执行代码，因为这些方法中有些对安全性非常敏感，因为它们缺乏沙箱环境，尽管存在如codebox-api、RestrictedPython、pychroot或setuptools的DirectorySandbox等工具和框架，只是举几个以Python为例。
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'An internet search is definitely worth adding to make sure we are implementing
    something that has to do with our objective. I’ve seen a few implementations of
    Rock, Paper, Scissors instead of Tetris.We can define additional tools such as
    a planner that breaks down the tasks into functions. You can see this in the repo.Running
    our agent executor with the objective to implement tetris, every time the results
    are a bit different. I see several searches for requirements and game mechanics,
    and several times a code is produced and run. The pygame library is installed.
    The final code snippet is not the final product, but it brings up a window:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过网络搜索，可以确保我们正在实现与我们目标相关的内容。我看到有一些实现的Rock, Paper, Scissors而不是Tetris。我们可以定义额外的工具，比如将任务分解为函数的计划器。你可以在代码库中看到这一点。每次运行我们的代理执行器来实现俄罗斯方块的目标时，结果都有些不同。我看到了几次搜索需求和游戏机制，几次生成和运行代码。安装了pygame库。最终的代码片段不是最终产品，但它会弹出一个窗口：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The code is not too bad in terms of syntax – I guess the prompt must have helped.
    However, in terms of functionality it’s very far from Tetris. This implementation
    of a fully-automated agent for software development is still quite experimental.
    It’s also very simple and basic, consisting only of about 340 lines of Python
    including the imports, which you can find on Github.I think a better approach
    could be to break down all the functionality into functions and maintain a list
    of functions to call, which can be used in all subsequent generations of code.
    We could also try a test-driven development approach or have a human give feedback
    rather than a fully automated process.An advantage to our approach is however
    that it’s easy to debug, since all steps including searches and generated code
    are written to a log file in the implementation.Let’s summarize!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在语法方面，这段代码并不算太糟糕——我猜提示可能有所帮助。但就功能而言，它离俄罗斯方块相去甚远。这种用于软件开发的全自动代理的实现仍然相当试验性。它也非常简单和基础，包括大约340行Python代码，包括导入的部分，在Github上可以找到。我认为一个更好的方法可能是将所有功能分解为函数，并维护一个要调用的函数列表，这可以用于所有后续的代码生成。我们还可以尝试测试驱动的开发方法，或者让人类给出反馈，而不是完全自动化的流程。我们方法的一个优势在于，易于调试，因为所有步骤，包括搜索和生成的代码都写入了日志文件中。让我们做个总结！
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve discussed LLMs for source code, and how they can help
    in developing software. There are quite a few areas, where LLMs can benefit software
    development, mostly as coding assistants.We’ve applied a few models for code generation
    using naïve approaches and we’ve evaluated them qualitatively. We’ve seen that
    the suggested solutions seem superficially correct but don’t actually perform
    the task or are full of bugs. This could particularly affect beginners, and could
    have significant implications regarding safety and reliability.In previous chapters,
    we’ve seen LLMs used as goal-driven agents to interact with external environments.
    In coding, compiler errors, results of code execution can be used to provide feedback
    as we’ve seen. Alternatively, we could have used human feedback or implemented
    tests. Let’s see if you remember some of the key takeaways from this chapter!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了用于源代码的LLM以及它们如何在开发软件中起到帮助。LLM在许多领域都能够对软件开发产生益处，主要是作为编码助手。我们应用了一些模型来使用天真的方法进行代码生成，并对它们进行了定性评估。我们看到，所建议的解决方案表面上看起来是正确的，但实际上并没有执行任务，或者充满了错误。这可能特别影响初学者，并且对安全性和可靠性可能产生重要影响。在之前的章节中，我们已经看到LLM被用作目标驱动代理来与外部环境进行交互。在编码中，编译器错误、代码执行的结果可以用来提供反馈，就像我们看到的那样。或者，我们可以使用人类的反馈，或者实施测试。让我们看看你是否记得本章的一些关键要点！
- en: Questions
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看一下，看看您是否能够凭记忆给出这些问题的答案。如果您对其中任何一个不确定，我建议您回到本章的相应部分：
- en: What can LLMs do to help in software development?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLMs能为软件开发提供什么帮助？
- en: How do you measure a code LLM’s performance on coding tasks?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何衡量代码LLM在编码任务上的表现？
- en: Which code LLM models are available, both open- and closed-source?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些代码LLM模型可供选择，包括开源和闭源？
- en: How does the Reflexion strategy work?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反思策略是如何工作的？
- en: What options do we have available to establish a feedback loop for writing code?
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有哪些选项可用于建立编写代码的反馈循环？
- en: What do you think is the impact of generative AI on software development?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您认为生成式人工智能对软件开发的影响是什么？
