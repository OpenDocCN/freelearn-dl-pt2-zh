- en: '*Chapter 10*: Serving Transformer Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：提供Transformer模型'
- en: So far, we've explored many aspects surrounding Transformers, and you've learned
    how to train and use a Transformer model from scratch. You also learned how to
    fine-tune them for many tasks. However, we still don't know how to serve these
    models in production. Like any other real-life and modern solution, **Natural
    Language Processing** (**NLP**)-based solutions must be able to be served in a
    production environment. However, metrics such as response time must be taken into
    consideration while developing such solutions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了围绕Transformer的许多方面，你已经学会了如何从头开始训练和使用Transformer模型。您还学会了如何为许多任务进行微调。然而，我们仍然不知道如何在生产中提供这些模型。与任何其他现实生活和现代解决方案一样，基于**自然语言处理**（**NLP**）的解决方案必须能够在生产环境中提供服务。然而，在开发这种解决方案时必须考虑响应时间等指标。
- en: This chapter will explain how to serve a Transformer-based NLP solution in environments
    where CPU/GPU is available. **TensorFlow Extended** (**TFX**) for machine learning
    deployment as a solution will be described here. Also, other solutions for serving
    Transformers as APIs such as FastAPI will be illustrated. You will also learn
    about the basics of Docker, as well as how to dockerize your service and make
    it deployable. Lastly, you will learn how to perform speed and load tests on Transformer-based
    solutions using Locust.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何在具有CPU/GPU的环境中提供基于Transformer的NLP解决方案。将在此处描述用于机器学习部署的**TensorFlow Extended**（**TFX**）解决方案。还将说明用于作为API提供Transformer的其他解决方案，例如FastAPI。您还将了解Docker的基础知识，以及如何将您的服务docker化并使其可部署。最后，您将学习如何使用Locust对基于Transformer的解决方案进行速度和负载测试。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: fastAPI Transformer model serving
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速API变换器模型服务
- en: Dockerizing APIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker化APIs
- en: Faster Transformer model serving using TFX
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TFX进行更快的变换器模型服务
- en: Load testing using Locust
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Locust进行负载测试
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using Jupyter Notebook, Python, and Dockerfile to run our coding
    exercises, which will require Python 3.6.0\. The following packages need to be
    installed:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Jupyter Notebook、Python和Dockerfile来运行我们的编码练习，这将需要Python 3.6.0。需要安装以下软件包：
- en: TensorFlow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: PyTorch
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Transformer >=4.00
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer >=4.00
- en: fastAPI
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速API
- en: Docker
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: Locust
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Locust
- en: Now, let's get started!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: 'All the notebooks for the coding exercises in this chapter will be available
    at the following GitHub link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH10](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH10).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '本章中的所有编码练习笔记本将在以下GitHub链接中提供: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH10](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH10)。'
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/375TOPO](https://bit.ly/375TOPO)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '点击以下链接查看动态演示视频: [https://bit.ly/375TOPO](https://bit.ly/375TOPO)'
- en: fastAPI Transformer model serving
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速API变换器模型服务
- en: There are many web frameworks we can use for serving. Sanic, Flask, and fastAPI
    are just some examples. However, fastAPI has recently gained so much attention
    because of its speed and reliability. In this section, we will use fastAPI and
    learn how to build a service according to its documentation. We will also use
    `pydantic` to define our data classes. Let's begin!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多我们可以用来提供服务的web框架。 Sanic、Flask和fastAPI只是一些例子。然而，fastAPI最近因其速度和可靠性而备受关注。在本节中，我们将使用fastAPI并根据其文档学习如何构建服务。我们还将使用`pydantic`来定义我们的数据类。让我们开始吧！
- en: 'Before we start, we must install `pydantic` and fastAPI:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始之前，我们必须安装`pydantic`和fastAPI：
- en: '[PRE0]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The next step is to make the data model for decorating the input of the API
    using `pydantic`. But before forming the data model, we must know what our model
    is and identify its input.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用`pydantic`为装饰API输入的数据模型进行建模。但是在形成数据模型之前，我们必须了解我们的模型是什么，并确定其输入。
- en: We are going to use a **Question Answering** (**QA**) model for this. As you
    know from [*Chapter 6*](B17123_06_Epub_AM.xhtml#_idTextAnchor090), *Fine-Tuning
    Language Models for Token Classification*, the input is in the form of a question
    and a context.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将使用**问答**（**QA**）模型。正如你从[*第6章*](B17123_06_Epub_AM.xhtml#_idTextAnchor090)中所知，*Fine-Tuning
    Language Models for Token Classification*，输入是问题和上下文的形式。
- en: 'By using the following data model, you can make the QA data model:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用以下数据模型，您可以创建QA数据模型：
- en: '[PRE1]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We must load the model once and not load it for each request; instead, we will
    preload it once and reuse it. Because the endpoint function is called each time
    we send a request to the server, this will result in the model being loaded each
    time:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须只加载模型一次，而不是为每个请求加载它；相反，我们将预加载它一次并重复使用它。因为每次我们向服务器发送请求时，端点函数都会被调用，这将导致模型每次都被加载：
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to make a fastAPI instance for moderating the application:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是为调节应用程序创建一个 fastAPI 实例：
- en: '[PRE3]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Afterward, you must make a fastAPI endpoint using the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您必须使用以下代码创建一个 fastAPI 端点：
- en: '[PRE4]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It is important to use `async` for the function to make this function run in
    asynchronous mode; this will be parallelized for requests. You can also use the
    `workers` parameter to increase the number of workers for the API, as well as
    making it answer different and independent API calls at once.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于使该函数以异步模式运行，重要的是使用 `async`；这将使请求并行运行。您还可以使用 `workers` 参数来增加 API 的工作线程数，并使其同时回答不同和独立的
    API 调用。
- en: 'Using `uvicorn`, you can run your application and serve it as an API. **Uvicorn**
    is a lightning-fast server implementation for Python-based APIs that makes them
    run as fast as possible. Use the following code for this:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `uvicorn`，您可以运行应用程序并将其作为 API 提供。**Uvicorn** 是用于 Python API 的高速服务器实现，使其尽可能快速运行。使用以下代码：
- en: '[PRE5]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It is important to remember that the preceding code must be saved in a `.py`
    file (`main.py`, for example). You can run it by using the following command:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请记住，上述代码必须保存在 `.py` 文件中（例如 `main.py`）。您可以使用以下命令运行它：
- en: '[PRE6]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As a result, you will see the following output in your terminal:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下，您将在终端中看到以下输出：
- en: '![Figure 10.1 – fastAPI in action ](img/B17123_10_001.jpg)'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 10.1 – fastAPI 实战](img/B17123_10_001.jpg)'
- en: Figure 10.1 – fastAPI in action
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.1 – fastAPI 实战
- en: 'The next step is to use and test it. There are many tools we can use for this
    but Postman is one of the best. Before we learn how to use Postman, use the following
    code:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用并测试它。我们可以使用许多工具来做这件事，但 Postman 是其中之一。在学习如何使用 Postman 之前，请使用以下代码：
- en: '[PRE7]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As a result, you will get the following output:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE8]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Curl is a useful tool but not as handy as Postman. Postman comes with a GUI
    and is easier to use compared to curl, which is a CLI tool. To use Postman, install
    it from [https://www.postman.com/downloads/](https://www.postman.com/downloads/).
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Curl 是一个有用的工具，但不如 Postman 方便。Postman 带有图形用户界面，比起是一个 CLI 工具的 curl 更易于使用。要使用 Postman，请从[https://www.postman.com/downloads/](https://www.postman.com/downloads/)安装它。
- en: After installing Postman, you can easily use it, as shown in the following screenshot:![Figure
    10.2 – Postman usage ](img/B17123_10_002.jpg)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完 Postman 后，您可以轻松使用它，如下截图所示：![图 10.2 – Postman 使用](img/B17123_10_002.jpg)
- en: Figure 10.2 – Postman usage
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.2 – Postman 使用
- en: 'Each step for setting up Postman for your service is numbered in the preceding
    screenshot. Let''s take a look at them:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 Postman 以使用您的服务的每个步骤在上述截图中都有编号。让我们来看看它们：
- en: Select **POST** as your method.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **POST** 作为您的方法。
- en: Enter your full endpoint URL.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入完整的端点 URL。
- en: Select **Body**.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **Body**。
- en: Set **Body** to **raw**.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 **Body** 设置为 **raw**。
- en: Select the **JSON** data type.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **JSON** 数据类型。
- en: Enter your input data in JSON format.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以 JSON 格式输入您的输入数据。
- en: Click **Send**.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单击 **Send**。
- en: You will see the result in the bottom section of Postman.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您将在 Postman 的底部部分看到结果。
- en: In the next section, you will learn how to dockerize your fastAPI-based API.
    It is essential to learn Docker basics to make your APIs packageable and easier
    for deployment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何将基于 fastAPI 的 API docker 化。学习 Docker 基础知识对于使您的 API 可打包并更容易部署至关重要。
- en: Dockerizing APIs
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 化 API
- en: 'To save time during production and ease the deployment process, it is essential
    to use Docker. It is very important to isolate your service and application. Also,
    note that the same code can be run anywhere, regardless of the underlying OS.
    To achieve this, Docker provides great functionality and packaging. Before using
    it, you must install it using the steps recommended in the Docker documentation
    ([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在生产过程中节省时间并简化部署过程，使用 Docker 是至关重要的。对于隔离您的服务和应用程序非常重要。此外，请注意，相同的代码可以在任何地方运行，而不受底层操作系统的限制。为了实现这一点，Docker
    提供了出色的功能和打包。在使用它之前，您必须按照 Docker 文档中推荐的步骤安装它([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/))：
- en: First, put the `main.py` file in the app directory.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将 `main.py` 文件放置在 app 目录中。
- en: 'Next, you must eliminate the last part from your code by specifying the following:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您必须通过指定以下内容来删除代码的最后一部分：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next step is to make a Dockerfile for your fastAPI; you made this previously.
    To do so, you must create a Dockerfile that contains the following content:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是为您的fastAPI创建一个Dockerfile；您之前已经创建过了。为此，您必须创建一个包含以下内容的Dockerfile：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Afterward, you can build your Docker container:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以构建您的Docker容器：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As a result, you can now access your API using port `8000`. However, you can
    still use Postman, as described in the previous section, *fastAPI Transformer
    model serving*.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，您现在可以使用`8000`端口访问您的API。但是，您仍然可以使用Postman，如前一节所述，*fastAPI Transformer model
    serving*。
- en: So far, you have learned how to make your own API based on a Transformer model
    and serve it using fastAPI. You then learned how to dockerize it. It is important
    to know that there are many options and setups you must learn about regarding
    Docker; we only covered the basics of Docker here.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何基于Transformer模型创建自己的API，并使用fastAPI提供服务。然后学习了如何dockerize它。重要的是要知道，关于Docker，您必须学习许多选项和设置；我们这里只覆盖了Docker的基础知识。
- en: In the next section, you will learn how to improve your model serving using
    TFX.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何使用TFX来改进您的模型服务。
- en: Faster Transformer model serving using TFX
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TFX进行更快的Transformer模型服务
- en: 'TFX provides a faster and more efficient way to serve deep learning-based models.
    But it has some important key points you must understand before you use it. The
    model must be a saved model type from TensorFlow so that it can be used by TFX
    Docker or the CLI. Let''s take a look:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: TFX提供了一种更快速和更高效的方式来提供基于深度学习的模型。但是在使用之前，您必须了解一些重要的关键点。模型必须是来自TensorFlow的保存模型类型，以便它可以被TFX
    Docker或CLI使用。让我们来看一看：
- en: 'You can perform TFX model serving by using a saved model format from TensorFlow.
    For more information about TensorFlow saved models, you can read the official
    documentation at [https://www.tensorflow.org/guide/saved_model](https://www.tensorflow.org/guide/saved_model).
    To make a saved model from Transformers, you can simply use the following code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过使用来自TensorFlow的保存模型格式来执行TFX模型服务。有关TensorFlow保存模型的更多信息，请阅读官方文档：[https://www.tensorflow.org/guide/saved_model](https://www.tensorflow.org/guide/saved_model)。要从Transformers创建保存模型，您只需使用以下代码：
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Before we understand how to use it to serve Transformers, it is required to
    pull the Docker image for TFX:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在理解如何使用它来为Transformers提供服务之前，需要拉取TFX的Docker镜像：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will pull the Docker container of the TFX being served. The next step
    is to run the Docker container and copy the saved model into it:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将拉取正在提供的TFX Docker容器。下一步是运行Docker容器并将保存的模型复制到其中：
- en: '[PRE14]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can copy the saved file into the Docker container using the following code:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下代码将保存的文件复制到Docker容器中：
- en: '[PRE15]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will copy the saved model files into the container. However, you must
    commit the changes:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将把保存的模型文件复制到容器中。但是，您必须提交更改：
- en: '[PRE16]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that everything is ready, you can kill the Docker container:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在一切都准备就绪，您可以终止Docker容器：
- en: '[PRE17]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This will stop the container from running.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将停止容器的运行。
- en: Now that the model is ready and can be served by the TFX Docker, you can simply
    use it with another service. The reason we need another service to call TFX is
    that the Transformer-based models have a special input format provided by tokenizers.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在模型已经准备好，并且可以通过TFX Docker提供服务，您可以简单地与另一个服务一起使用它。我们需要另一个服务来调用TFX的原因是，基于Transformer的模型有一个由tokenizer提供的特殊输入格式。
- en: 'To do so, you must make a fastAPI service that will model the API that was
    served by the TensorFlow serving container. Before you code your service, you
    should start the Docker container by giving it parameters to run the BERT-based
    model. This will help you fix bugs in case there are any errors:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此，您必须创建一个fastAPI服务，该服务将模拟由TensorFlow服务容器提供的API。在编写代码之前，您应该通过给予它运行BERT-based模型的参数来启动Docker容器。这将帮助您修复任何错误：
- en: '[PRE18]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following code contains the content of the `main.py` file:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的代码包含了`main.py`文件的内容：
- en: '[PRE19]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have loaded the `config` file because the labels are stored in it, and we
    need them to return it in the result. You can simply run this file using `python`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了`config`文件，因为标签存储在其中，我们需要它们以在结果中返回。您可以简单地使用`python`运行此文件：
- en: '[PRE20]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, your service is up and ready to use. You can access it using Postman,
    as shown in the following screenshot:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，您的服务已经启动并准备就绪。您可以使用Postman访问它，如下图所示：
- en: '![Figure 10.3 – Postman output of a TFX-based service ](img/B17123_10_003.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – TFX-based服务的Postman输出](img/B17123_10_003.jpg)'
- en: Figure 10.3 – Postman output of a TFX-based service
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3—基于TFX服务的Postman输出
- en: 'The overall architecture of the new service within TFX Docker is shown in the
    following diagram:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TFX Docker中新服务的整体架构如下图所示：
- en: '![Figure 10.4 – TFX-based service architecture ](img/B17123_10_004.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4—基于TFX服务的架构](img/B17123_10_004.jpg)'
- en: Figure 10.4 – TFX-based service architecture
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4—基于TFX服务的架构
- en: So far, you have learned how to serve a model using TFX. However, you need to
    learn how to load test your service using Locust. It is important to know the
    limits of your service and when to optimize it by using quantization or pruning.
    In the next section, we will describe how to test model performance under heavy
    load using Locust.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何使用TFX提供模型。然而，您还需要学会如何使用Locust进行负载测试。了解服务的限制以及何时通过量化或修剪进行优化是非常重要的。在下一节中，我们将描述如何使用Locust在高负载下测试模型性能。
- en: Load testing using Locust
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Locust进行负载测试
- en: 'There are many applications we can use to load test services. Most of these
    applications and libraries provide useful information about the response time
    and delay of the service. They also provide information about the failure rate.
    Locust is one of the best tools for this purpose. We will use it to load test
    three methods for serving a Transformer-based model: using fastAPI only, using
    dockerized fastAPI, and TFX-based serving using fastAPI. Let''s get started:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用许多应用程序来对服务进行负载测试。这些应用程序和库中的大多数都提供了有关服务的响应时间和延迟的有用信息。它们还提供了有关故障率的信息。Locust是这一目的最好的工具之一。我们将使用它来对三种用于提供基于Transformer的模型的方法进行负载测试：仅使用fastAPI、使用docker化的fastAPI以及使用fastAPI进行TFX-based服务。让我们开始吧：
- en: 'First, we must install Locust:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须安装Locust：
- en: '[PRE21]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This command will install Locust. The next step is to make all the services
    serving an identical task use the same model. Fixing two of the most important
    parameters of this test will ensure that all the services have been designed identically
    to serve a single purpose. Using the same model will help us freeze anything else
    and focus on the deployment performance of the methods.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此命令将安装Locust。下一步是使提供相同任务的所有服务使用相同的模型。修正此测试的最重要的两个参数将确保所有服务均被设计成满足单一目的。使用相同的模型将帮助我们凝固其他任何内容，并集中于方法的部署性能。
- en: 'Once everything is ready, you can start load testing your APIs. You must prepare
    a `locustfile` to define your user and its behavior. The following code is of
    a simple `locustfile`:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切准备就绪后，您可以开始测试API的负载。您必须准备一个`locustfile`来定义您的用户及其行为。以下代码是一个简单的`locustfile`：
- en: '[PRE22]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: By using `HttpUser` and creating the `User` class that's inherited from it,
    we can define an `HttpUser` class. The `@task` decorator is essential for defining
    the task that the user must perform after spawning. The `predict` function is
    the actual task that the user will perform repeatedly after spawning. It will
    generate a random string that's `20` in length and send it to your API.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用`HttpUser`并创建从中继承的`User`类，我们可以定义一个`HttpUser`类。`@task`装饰器对于定义用户生成后必须执行的任务至关重要。`predict`函数是用户生成后将重复执行的实际任务。它将生成一个长度为`20`的随机字符串并发送到您的API。
- en: 'To start the test, you must start your service. Once you''ve started your service,
    run the following code to start the Locust load test:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始测试，您必须启动您的服务。一旦您启动了服务，运行以下代码以启动Locust负载测试：
- en: '[PRE23]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Locust will start with the settings you provided in your `locustfile`. You
    will see the following in your Terminal:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Locust将根据您在`locustfile`中提供的设置启动。您的终端将显示以下内容：
- en: '![Figure 10.5 – Terminal after starting a Locust load test ](img/B17123_10_005.jpg)'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图10.5—启动Locust负载测试后的终端](img/B17123_10_005.jpg)'
- en: Figure 10.5 – Terminal after starting a Locust load test
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.5—启动Locust负载测试后的终端
- en: As you can see, you can open the URL where the load web interface is located;
    that is, http://0.0.0.0:8089.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您所看到的，您可以打开网络接口的URL，即http://0.0.0.0:8089。
- en: After opening the URL, you will see an interface, as shown in the following
    screenshot:![Figure 10.6 – Locust web interface ](img/B17123_10_006.jpg)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开URL之后，您将看到一个界面，如下截图所示：![图10.6—Locust网络接口](img/B17123_10_006.jpg)
- en: Figure 10.6 – Locust web interface
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图10.6—Locust网络接口
- en: We are going to set **Number of total users to simulate** to **10**, **Spawn
    rate** to **1**, and **Host** to **http://127.0.0.1:8000**, which is where our
    service is running. After setting these parameters, click **Start swarming**.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把**要模拟的总用户数**设定为**10**，**生成速率**设定为**1**，**主机**设定为**http://127.0.0.1:8000**，这是我们服务运行的地方。在设置这些参数之后，点击**开始swarming**。
- en: At this point, the UI will change, and the test will begin. To stop the test
    at any time, click the **Stop** button.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，界面将会改变，测试将开始。要随时停止测试，点击**停止**按钮。
- en: You can also click the **Charts** tab to see a visualization of the results:![Figure
    10.7 – Locust test results from the Charts tab ](img/B17123_10_007.jpg)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以点击**Charts**选项卡查看结果的可视化：![图 10.7 – 来自 Charts 选项卡的 Locust 测试结果](img/B17123_10_007.jpg)
- en: Figure 10.7 – Locust test results from the Charts tab
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10.7 – 来自 Charts 选项卡的 Locust 测试结果
- en: Now that the test is ready for the API, let's test all three versions and compare
    the results to see which one performs better. Remember that the services must
    be tested independently on the machine where you want to serve them. In other
    words, you must run one service at a time and test that, close the service, run
    the other one and test it, and so on.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在 API 的测试准备好了，让我们测试所有三个版本并比较结果，看哪个执行效果更好。请记住，服务必须在你要提供服务的机器上独立测试。换句话说，你必须一次运行一个服务并测试它，关闭服务，运行另一个服务并测试它，依此类推。
- en: 'The results are shown in the following table:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示在下表中：
- en: '![Table 1 – Comparing the results of different implementations ](img/B17123_10_Table_01.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![表 1 – 比较不同实现的结果](img/B17123_10_Table_01.jpg)'
- en: Table 1 – Comparing the results of different implementations
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1 – 比较不同实现的结果
- en: In the preceding table, **Requests Per Second** (**RPS**) means the number of
    requests per second that the API answers, while the **Average Response Time**
    (**RT**) means the milliseconds that service takes to respond to a given call.
    These results shows that the TFX-based fastAPI is the fastest. It has a higher
    RPS and a lower average RT. All these tests were performed on a machine with an
    Intel(R) Core(TM) i7-9750H CPU with 32 GB RAM, and GPU disabled.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，**每秒请求数**（**RPS**）表示 API 每秒响应的请求数，而**平均响应时间**（**RT**）表示服务响应给定调用所需的毫秒数。这些结果显示了基于
    TFX 的 fastAPI 是最快的。它具有更高的 RPS 和较低的平均 RT。所有这些测试都是在一台配有 Intel(R) Core(TM) i7-9750H
    CPU 和 32 GB RAM、GPU 禁用的机器上进行的。
- en: In this section, you learned how to test your API and measure its performance
    in terms of important parameters such as RPS and RT. However, there are many other
    stress tests a real-world API can perform, such as increasing the number of users
    to make them behave like real users. To perform such tests and report their results
    in a more realistic way, it is important to read Locust's documentation and learn
    how to perform more advanced tests.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，你学习了如何测试你的 API 并测量其性能，重要参数如 RPS 和 RT。然而，真实世界的 API 还可以执行许多其他压力测试，比如增加用户数量使其表现得像真实用户一样。要执行这样的测试并以更真实的方式报告其结果，重要的是阅读
    Locust 的文档并学习如何执行更高级的测试。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the basics of serving Transformer models using
    fastAPI. You also learned how to serve models in a more advanced and efficient
    way, such as by using TFX. You then studied the basics of load testing and creating
    users. Making these users spawn in groups or one by one, and then reporting the
    results of stress testing, was another major topic of this chapter. After that,
    you studied the basics of Docker and how to package your application in the form
    of a Docker container. Finally, you learned how to serve Transformer-based models.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学习了使用 fastAPI 提供 Transformer 模型的基础知识。你还学会了如何以更高级和更有效的方式提供模型，比如使用 TFX。然后，你学习了负载测试和创建用户的基础知识。让这些用户分组生成或逐个生成，然后报告压力测试的结果，是本章的另一个主要主题。之后，你学习了
    Docker 的基础知识以及如何将你的应用程序打包成 Docker 容器的形式。最后，你学会了如何提供基于 Transformer 的模型。
- en: In the next chapter, you will learn about Transformer deconstruction, the model
    view, and monitoring training using various tools and techniques.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习 Transformer 的解构、模型视图，并使用各种工具和技术监视训练。
- en: References
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: 'Locust documentation: [https://docs.locust.io](https://docs.locust.io)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Locust 文档：[https://docs.locust.io](https://docs.locust.io)
- en: 'TFX documentation: [https://www.tensorflow.org/tfx/guide](https://www.tensorflow.org/tfx/guide)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TFX 文档：[https://www.tensorflow.org/tfx/guide](https://www.tensorflow.org/tfx/guide)
- en: 'FastAPI documentation: [https://fastapi.tiangolo.com](https://fastapi.tiangolo.com)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastAPI 文档：[https://fastapi.tiangolo.com](https://fastapi.tiangolo.com)
- en: 'Docker documentation: [https://docs.docker.com](https://docs.docker.com)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 文档：[https://docs.docker.com](https://docs.docker.com)
- en: 'HuggingFace TFX serving: [https://huggingface.co/blog/tf-serving](https://huggingface.co/blog/tf-serving)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HuggingFace TFX 服务：[https://huggingface.co/blog/tf-serving](https://huggingface.co/blog/tf-serving)
