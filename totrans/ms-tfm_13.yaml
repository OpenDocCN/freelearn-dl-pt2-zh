- en: '*Chapter 10*: Serving Transformer Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've explored many aspects surrounding Transformers, and you've learned
    how to train and use a Transformer model from scratch. You also learned how to
    fine-tune them for many tasks. However, we still don't know how to serve these
    models in production. Like any other real-life and modern solution, **Natural
    Language Processing** (**NLP**)-based solutions must be able to be served in a
    production environment. However, metrics such as response time must be taken into
    consideration while developing such solutions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain how to serve a Transformer-based NLP solution in environments
    where CPU/GPU is available. **TensorFlow Extended** (**TFX**) for machine learning
    deployment as a solution will be described here. Also, other solutions for serving
    Transformers as APIs such as FastAPI will be illustrated. You will also learn
    about the basics of Docker, as well as how to dockerize your service and make
    it deployable. Lastly, you will learn how to perform speed and load tests on Transformer-based
    solutions using Locust.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: fastAPI Transformer model serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dockerizing APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster Transformer model serving using TFX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load testing using Locust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using Jupyter Notebook, Python, and Dockerfile to run our coding
    exercises, which will require Python 3.6.0\. The following packages need to be
    installed:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer >=4.00
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: fastAPI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locust
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'All the notebooks for the coding exercises in this chapter will be available
    at the following GitHub link: [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH10](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH10).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video: [https://bit.ly/375TOPO](https://bit.ly/375TOPO)'
  prefs: []
  type: TYPE_NORMAL
- en: fastAPI Transformer model serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many web frameworks we can use for serving. Sanic, Flask, and fastAPI
    are just some examples. However, fastAPI has recently gained so much attention
    because of its speed and reliability. In this section, we will use fastAPI and
    learn how to build a service according to its documentation. We will also use
    `pydantic` to define our data classes. Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, we must install `pydantic` and fastAPI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next step is to make the data model for decorating the input of the API
    using `pydantic`. But before forming the data model, we must know what our model
    is and identify its input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to use a **Question Answering** (**QA**) model for this. As you
    know from [*Chapter 6*](B17123_06_Epub_AM.xhtml#_idTextAnchor090), *Fine-Tuning
    Language Models for Token Classification*, the input is in the form of a question
    and a context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By using the following data model, you can make the QA data model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We must load the model once and not load it for each request; instead, we will
    preload it once and reuse it. Because the endpoint function is called each time
    we send a request to the server, this will result in the model being loaded each
    time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to make a fastAPI instance for moderating the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Afterward, you must make a fastAPI endpoint using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is important to use `async` for the function to make this function run in
    asynchronous mode; this will be parallelized for requests. You can also use the
    `workers` parameter to increase the number of workers for the API, as well as
    making it answer different and independent API calls at once.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using `uvicorn`, you can run your application and serve it as an API. **Uvicorn**
    is a lightning-fast server implementation for Python-based APIs that makes them
    run as fast as possible. Use the following code for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is important to remember that the preceding code must be saved in a `.py`
    file (`main.py`, for example). You can run it by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a result, you will see the following output in your terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.1 – fastAPI in action ](img/B17123_10_001.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 10.1 – fastAPI in action
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to use and test it. There are many tools we can use for this
    but Postman is one of the best. Before we learn how to use Postman, use the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a result, you will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Curl is a useful tool but not as handy as Postman. Postman comes with a GUI
    and is easier to use compared to curl, which is a CLI tool. To use Postman, install
    it from [https://www.postman.com/downloads/](https://www.postman.com/downloads/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After installing Postman, you can easily use it, as shown in the following screenshot:![Figure
    10.2 – Postman usage ](img/B17123_10_002.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.2 – Postman usage
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Each step for setting up Postman for your service is numbered in the preceding
    screenshot. Let''s take a look at them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **POST** as your method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter your full endpoint URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Body**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set **Body** to **raw**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **JSON** data type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter your input data in JSON format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Send**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will see the result in the bottom section of Postman.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, you will learn how to dockerize your fastAPI-based API.
    It is essential to learn Docker basics to make your APIs packageable and easier
    for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerizing APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To save time during production and ease the deployment process, it is essential
    to use Docker. It is very important to isolate your service and application. Also,
    note that the same code can be run anywhere, regardless of the underlying OS.
    To achieve this, Docker provides great functionality and packaging. Before using
    it, you must install it using the steps recommended in the Docker documentation
    ([https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)):'
  prefs: []
  type: TYPE_NORMAL
- en: First, put the `main.py` file in the app directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, you must eliminate the last part from your code by specifying the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step is to make a Dockerfile for your fastAPI; you made this previously.
    To do so, you must create a Dockerfile that contains the following content:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Afterward, you can build your Docker container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As a result, you can now access your API using port `8000`. However, you can
    still use Postman, as described in the previous section, *fastAPI Transformer
    model serving*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So far, you have learned how to make your own API based on a Transformer model
    and serve it using fastAPI. You then learned how to dockerize it. It is important
    to know that there are many options and setups you must learn about regarding
    Docker; we only covered the basics of Docker here.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to improve your model serving using
    TFX.
  prefs: []
  type: TYPE_NORMAL
- en: Faster Transformer model serving using TFX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TFX provides a faster and more efficient way to serve deep learning-based models.
    But it has some important key points you must understand before you use it. The
    model must be a saved model type from TensorFlow so that it can be used by TFX
    Docker or the CLI. Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can perform TFX model serving by using a saved model format from TensorFlow.
    For more information about TensorFlow saved models, you can read the official
    documentation at [https://www.tensorflow.org/guide/saved_model](https://www.tensorflow.org/guide/saved_model).
    To make a saved model from Transformers, you can simply use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before we understand how to use it to serve Transformers, it is required to
    pull the Docker image for TFX:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will pull the Docker container of the TFX being served. The next step
    is to run the Docker container and copy the saved model into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can copy the saved file into the Docker container using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will copy the saved model files into the container. However, you must
    commit the changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that everything is ready, you can kill the Docker container:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will stop the container from running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the model is ready and can be served by the TFX Docker, you can simply
    use it with another service. The reason we need another service to call TFX is
    that the Transformer-based models have a special input format provided by tokenizers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To do so, you must make a fastAPI service that will model the API that was
    served by the TensorFlow serving container. Before you code your service, you
    should start the Docker container by giving it parameters to run the BERT-based
    model. This will help you fix bugs in case there are any errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code contains the content of the `main.py` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have loaded the `config` file because the labels are stored in it, and we
    need them to return it in the result. You can simply run this file using `python`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, your service is up and ready to use. You can access it using Postman,
    as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Postman output of a TFX-based service ](img/B17123_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Postman output of a TFX-based service
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall architecture of the new service within TFX Docker is shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – TFX-based service architecture ](img/B17123_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – TFX-based service architecture
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned how to serve a model using TFX. However, you need to
    learn how to load test your service using Locust. It is important to know the
    limits of your service and when to optimize it by using quantization or pruning.
    In the next section, we will describe how to test model performance under heavy
    load using Locust.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing using Locust
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many applications we can use to load test services. Most of these
    applications and libraries provide useful information about the response time
    and delay of the service. They also provide information about the failure rate.
    Locust is one of the best tools for this purpose. We will use it to load test
    three methods for serving a Transformer-based model: using fastAPI only, using
    dockerized fastAPI, and TFX-based serving using fastAPI. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must install Locust:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will install Locust. The next step is to make all the services
    serving an identical task use the same model. Fixing two of the most important
    parameters of this test will ensure that all the services have been designed identically
    to serve a single purpose. Using the same model will help us freeze anything else
    and focus on the deployment performance of the methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once everything is ready, you can start load testing your APIs. You must prepare
    a `locustfile` to define your user and its behavior. The following code is of
    a simple `locustfile`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: By using `HttpUser` and creating the `User` class that's inherited from it,
    we can define an `HttpUser` class. The `@task` decorator is essential for defining
    the task that the user must perform after spawning. The `predict` function is
    the actual task that the user will perform repeatedly after spawning. It will
    generate a random string that's `20` in length and send it to your API.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To start the test, you must start your service. Once you''ve started your service,
    run the following code to start the Locust load test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Locust will start with the settings you provided in your `locustfile`. You
    will see the following in your Terminal:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Terminal after starting a Locust load test ](img/B17123_10_005.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 10.5 – Terminal after starting a Locust load test
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, you can open the URL where the load web interface is located;
    that is, http://0.0.0.0:8089.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After opening the URL, you will see an interface, as shown in the following
    screenshot:![Figure 10.6 – Locust web interface ](img/B17123_10_006.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.6 – Locust web interface
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are going to set **Number of total users to simulate** to **10**, **Spawn
    rate** to **1**, and **Host** to **http://127.0.0.1:8000**, which is where our
    service is running. After setting these parameters, click **Start swarming**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, the UI will change, and the test will begin. To stop the test
    at any time, click the **Stop** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can also click the **Charts** tab to see a visualization of the results:![Figure
    10.7 – Locust test results from the Charts tab ](img/B17123_10_007.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.7 – Locust test results from the Charts tab
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that the test is ready for the API, let's test all three versions and compare
    the results to see which one performs better. Remember that the services must
    be tested independently on the machine where you want to serve them. In other
    words, you must run one service at a time and test that, close the service, run
    the other one and test it, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The results are shown in the following table:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Table 1 – Comparing the results of different implementations ](img/B17123_10_Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 – Comparing the results of different implementations
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding table, **Requests Per Second** (**RPS**) means the number of
    requests per second that the API answers, while the **Average Response Time**
    (**RT**) means the milliseconds that service takes to respond to a given call.
    These results shows that the TFX-based fastAPI is the fastest. It has a higher
    RPS and a lower average RT. All these tests were performed on a machine with an
    Intel(R) Core(TM) i7-9750H CPU with 32 GB RAM, and GPU disabled.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to test your API and measure its performance
    in terms of important parameters such as RPS and RT. However, there are many other
    stress tests a real-world API can perform, such as increasing the number of users
    to make them behave like real users. To perform such tests and report their results
    in a more realistic way, it is important to read Locust's documentation and learn
    how to perform more advanced tests.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the basics of serving Transformer models using
    fastAPI. You also learned how to serve models in a more advanced and efficient
    way, such as by using TFX. You then studied the basics of load testing and creating
    users. Making these users spawn in groups or one by one, and then reporting the
    results of stress testing, was another major topic of this chapter. After that,
    you studied the basics of Docker and how to package your application in the form
    of a Docker container. Finally, you learned how to serve Transformer-based models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about Transformer deconstruction, the model
    view, and monitoring training using various tools and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Locust documentation: [https://docs.locust.io](https://docs.locust.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TFX documentation: [https://www.tensorflow.org/tfx/guide](https://www.tensorflow.org/tfx/guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FastAPI documentation: [https://fastapi.tiangolo.com](https://fastapi.tiangolo.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker documentation: [https://docs.docker.com](https://docs.docker.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HuggingFace TFX serving: [https://huggingface.co/blog/tf-serving](https://huggingface.co/blog/tf-serving)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
