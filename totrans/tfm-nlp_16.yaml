- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Emergence of Transformer-Driven Copilots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When **Industry 4.0** (**I4.0**) reaches maturity, it will all be about machine-to-machine
    connections, communication, and decision-making. AI will be primarily embedded
    in ready-to-use pay-as-you-go cloud AI solutions. Big tech will absorb the most
    talented AI specialists to create APIs, interfaces, and integration tools.
  prefs: []
  type: TYPE_NORMAL
- en: AI specialists will go from development to design to becoming architects, integrators,
    and cloud AI pipeline administrators. Thus, AI is becoming a job for engineer
    consultants more than engineer developers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 1*, *What Are Transformers?*, introduced foundation models, transformers
    that can do NLP tasks they were not trained for. *Chapter 15*, *From NLP to Task-Agnostic
    Transformer Models*, expanded foundation model transformers to task-agnostic models
    that can perform vision tasks, NLP tasks, and much more.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will extend task-agnostic OpenAI GPT-3 models to a wide range of
    copilot tasks. A new generation of AI specialists and data scientists will learn
    how to work with AI copilots to help them generate source code automatically and
    make decisions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter begins by exploring prompt engineering in more detail. The example
    task consists of converting meeting notes into a summary. Transformers boost our
    productivity. However, we will see how natural language remains a challenge for
    AI.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to use OpenAI Codex as a copilot. GitHub Copilot suggests
    source code as we write our programs using Codex. Codex can also convert natural
    language into code.
  prefs: []
  type: TYPE_NORMAL
- en: We will then discover new AI methods with domain-specific GPT-3 engines. This
    chapter will show how to generate embeddings with 12,288 dimensions and plug them
    into machine learning algorithms. We will also see how to ask a transformer to
    produce instructions automatically.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how to filter biased input and output before looking into transformer-driven
    recommenders. AI of the 2020s must be built with ethical methods.
  prefs: []
  type: TYPE_NORMAL
- en: Recommender systems have permeated every social media platform to suggest videos,
    posts, messages, books, and many other products we might want to consume. We will
    build an educational multi-purpose transformer-based recommender system using
    ML in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models analyze sequences. They began with NLP but have successfully
    expanded to computer vision. We will explore a transformer-based computer vision
    program developed in JAX.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will see AI copilots contribute to the transition of virtual systems
    into metaverses, which will expand in this decade. You are the pilot when you
    develop your applications. However, when you have code to develop, the activated
    completions are limited to methods, not lines of code. An IDE might suggest a
    list of methods. Copilots can produce completions of whole paragraphs of code!
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub Copilot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codex language to source code models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedded-driven machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruct series
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content filter models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring transformer-based recommenders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending NLP sequence learning to behavior predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transformer models in JAX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying transformer models to computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin with prompt engineering, which is a critical ability to acquire.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Speaking a specific language is not hereditary. There is not a language center
    in our brain containing the language of our parents. Our brain engineers our neurons
    early in our lives to speak, read, write, and understand a language. Each human
    has a different language circuitry depending on their cultural background and
    how they were communicated with in their early years.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we grow up, we discover that much of what we hear is chaos: unfinished sentences,
    grammar mistakes, misused words, bad pronunciation, and many other distortions.'
  prefs: []
  type: TYPE_NORMAL
- en: We use language to convey a message. We quickly find that we need to adapt our
    language to the person or audience we address. We might have to try additional
    “inputs” or “prompts” to obtain the result (“output”) we expect. Foundation-level
    transformer models such as GPT-3 can perform hundreds of tasks in an indefinite
    number of ways. *We must learn the language of transformer prompts and responses
    as we would any other language*. Effective communication with a person or near-human-level
    transformer must contain a minimum amount of information to maximize results.
    We represent the minimum input information to obtain a result as *minI* and the
    maximum output of any system as *maxR*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent this chain of communication as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*minI(input) ![](img/B17948_16_22.png) maxR(output)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will replace “input” with “prompt” for transformers to show that our input
    influences how the model will react. The output is the “response.” The dialogue
    with transformers, *d(T)*, can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d(T)=minI(prompt)* *![](img/B17948_16_22.png) maxR(response)*'
  prefs: []
  type: TYPE_NORMAL
- en: When *minI(prompt)![](img/B17948_16_22.png)*`1`, the probability of *maxR*(response)
    *![](img/B17948_16_22.png)*`1`.
  prefs: []
  type: TYPE_NORMAL
- en: When *minI(prompt)* *![](img/B17948_16_22.png)*`0`, the probability of *maxR*(response)
    *![](img/B17948_16_22.png)*`0`.
  prefs: []
  type: TYPE_NORMAL
- en: The quality *d(T)* depends on how well we can define *minI(prompt)*.
  prefs: []
  type: TYPE_NORMAL
- en: If your prompt tends to reach `1`, then it will produce probabilities that tend
    to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: If your prompt tends to reach `0`, then it will produce output probabilities
    that tend to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Your prompt is part of the content that impacts the probabilities! Why? Because
    the transformer will include the prompt and the response in its estimations.
  prefs: []
  type: TYPE_NORMAL
- en: It takes many years to learn a language as a child or an adult. It also takes
    quite some time to learn the language of transformers and how to design *minI(prompt)*
    effectively. We need to understand them, their architecture, and the way the algorithms
    calculate predictions. Then we need to spend quite some time understanding how
    to design the input, the prompt, for the transformers to behave as we expect.
  prefs: []
  type: TYPE_NORMAL
- en: This section focuses on oral language. The prompt for OpenAI GPT-3 for an NLP
    task will often be taken from meeting notes or conversations, which tend to be
    unstructured. Transforming meeting notes or conversations into a summary can be
    quite challenging. This section will focus on summarizing notes of the same conversation
    in seven situations that go from casual English to casual or formal English with
    limited context.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with casual English with a meaningful context.
  prefs: []
  type: TYPE_NORMAL
- en: Casual English with a meaningful context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Casual English is spoken with shorter sentences and limited vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s ask OpenAI GPT-3 to perform a “notes to summary” task. Go to [www.openai.](http://www.openai.com)com.
    Log in or sign up. Then go to the **Examples** page and select **Notes to summary**.
  prefs: []
  type: TYPE_NORMAL
- en: We will give GPT-3 all the information required to summarize a casual conversation
    between Jane and Tom. Jane and Tom are two developers starting work. Tom offers
    Jane coffee. Jane declines the offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, *minI(prompt)=1* since the input information is fine, as shown
    in *Figure 16.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Summarizing well-documented notes'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we click on **Generate**, we get a surprisingly good answer, as shown
    in *Figure 16.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B17948_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: An acceptable summary is provided by GPT-3'
  prefs: []
  type: TYPE_NORMAL
- en: Can we conclude that AI can find structures in our chaotic daily conversations,
    meetings, and aimless chatter? The answer isn’t easy. We will now complicate the
    input by adding a metonymy.
  prefs: []
  type: TYPE_NORMAL
- en: Casual English with a metonymy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tom mentioned the word `coffee`, setting GPT-3 on track. But what if Tom used
    the word `java` instead of `coffee`. Coffee refers to the beverage, but `java`
    is an ingredient that comes from the island of Java. A metonymy is when we use
    an attribute of an object, such as `java` for `coffee`. Java is also a programming
    language with a logo that is a cup of coffee.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are facing three possible definitions of `java`: an ingredient of coffee
    meaning coffee (metonymy), the island of Java, and the name of a programming language.
    GPT-3 now has a polysemy (several meanings of the same word) issue to solve.'
  prefs: []
  type: TYPE_NORMAL
- en: Humans master polysemy. We learn the different meanings of words. We know that
    a word doesn’t mean much without context. In this case, Jane and Tom are developers,
    complicating the situation. Are they talking about coffee or the language?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is easy for a human since Tom then talks about his wife, who stopped
    drinking it. GPT-3 can be confused by this polysemy when the word `java` replaces
    `coffee`, and it produces an incorrect answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text  Description automatically generated](img/B17948_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: Incorrect GPT-3 response when prompt is confusing'
  prefs: []
  type: TYPE_NORMAL
- en: We thus confirm that when *minI(prompt)**![](img/B17948_16_22.png)*`0`, the
    probability of *maxR(response)![](img/B17948_16_22.png)*`0`.
  prefs: []
  type: TYPE_NORMAL
- en: Human conversations can become even more difficult to analyze if we add an ellipsis.
  prefs: []
  type: TYPE_NORMAL
- en: Casual English with an ellipsis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The situation can get worse. Let’s suppose Tom is drinking a cup of coffee,
    and Jane looks at him and the cup of coffee as she casually greets him.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of asking Jane if she wants coffee or java, Tom says:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"Want some?"`'
  prefs: []
  type: TYPE_NORMAL
- en: Tom left out the word `coffee`, which is an ellipsis. Jane can still understand
    what Tom means by looking at him holding a cup of coffee.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI GPT-3 detects the word `drinking` and still manages to associate this
    verb with the question `Want some?`. We don’t `want some` of a programming language.
    The following summary produced by GPT-3 is still correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: Correct response produced by GPT-3'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see what happens when there is vague context that a human can understand
    but remains a challenge for AI.
  prefs: []
  type: TYPE_NORMAL
- en: Casual English with vague context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we take this further, `Tom` doesn’t need to mention his wife for `Jane` to
    understand what he is talking about since he is holding a cup of coffee.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s remove `Tom`''s reference to his wife and the verb `drinking`. Let’s
    leave `want some` in instead of coffee or java:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Vague input context'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output reflects the apparent chaos of the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: Poor GPT-3 response'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prompt was too vague, leading to an inadequate response that we can sum
    up as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d(T)![](img/B17948_16_22.png)0 because when* *minI(prompt)![](img/B17948_16_22.png)*`0`,
    the probability of *maxR(response)![](img/B17948_16_22.png)*`0`'
  prefs: []
  type: TYPE_NORMAL
- en: 'When humans communicate, they bring their culture, past relationships, visual
    situations, and other *invisible factors into a conversation*. These invisible
    factors for third parties can be:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading a text without seeing what the people were doing (actions, facial expressions,
    body language, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listening to people refer to things they know about, but we don’t (movies, sports,
    problems in a factory, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cultural events from a culture that’s different from ours
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list is endless!
  prefs: []
  type: TYPE_NORMAL
- en: We can see that these *invisible* factors make AI *blind*.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now introduce sensors into the situation.
  prefs: []
  type: TYPE_NORMAL
- en: Casual English with sensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We now introduce video sensors into the room for a thought experiment. Imagine
    we can use image captioning with a video feed and supply a context early in the
    dialogue such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Humans sometimes generate dialogue that only people that know each other well
    understand. Consider the following dialogue between Jane and Tom. The video feed
    produces image captioning showing that Tom is drinking a cup of coffee and Jane
    is typing on her keyboard. Jane and Tom are two developers, mumbling their way
    through the day while getting down to work in an open space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we provide the following chaotic chat as a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tom: "hi" Jane: "yeah sure" Tom: "Want some?" Jane: "Nope" Tom: "Cool. You''re
    trying then." Jane: "Yup" Tom: "Sleep better?" Jane: "Yeah. Sure. "`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of GPT-3 is acceptable though important semantic words are missing
    at the start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The results may change from one run to another. GPT-3 looks at the top probabilities
    and selects one of the best. GPT-3 made it through this experiment because image
    captioning provided the context.
  prefs: []
  type: TYPE_NORMAL
- en: However, what if Tom is not holding a cup of coffee, depriving GPT-3 of visual
    context?
  prefs: []
  type: TYPE_NORMAL
- en: Casual English with sensors but no visible context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most difficult situation for AI is if Tom refers to an event every day
    but not today. Suppose that Tom comes in with a cup of coffee every morning. He
    comes in now and asks Jane if she `wants some` *before* getting some coffee. Our
    thought experiment is to imagine all the possible cases. In that case, the video
    feed in our thought experiment will reveal nothing, and we are back to chaos again.
    Also, the video feed can’t see if they are developers, accountants, or consultants.
    So, let’s take that part of context out, which leaves us with the following context.
    Let’s go further. The dialogue contains `Tom`: and `Jane:`. So we don’t need to
    mention that context. We are left with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tom: "hi" Jane: "yeah sure" Tom: "Want some?" Jane: "Nope" Tom: "Cool. You''re
    trying then." Jane: "Yup" Tom: "Sleep better?" Jane: "Yeah. Sure."`'
  prefs: []
  type: TYPE_NORMAL
- en: The output is quite astonishing. The casual language used by Jane and Tom leads
    GPT-3 to absurd conclusions. Remember, GPT-3 is a stochastic algorithm. The slightest
    change in the inputs can lead to quite different outputs. GPT-3 is trying to guess
    what they are talking about. GPT-3 detects that the conversation is about consuming
    something. Their casual language leads to nonsensical predictions about illegal
    substances that I am not reproducing in this section for ethical reasons.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-3 determines the level of language used and associates it with related situations.
  prefs: []
  type: TYPE_NORMAL
- en: What will happen if we reproduce this same experiment using formal English?
  prefs: []
  type: TYPE_NORMAL
- en: Formal English conversation with no context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now keep all of the context out but provide formal English. Formal English
    contains longer sentences, good grammar, and manners. We can express the same
    conversation that contains no context using formal English:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tom: "Good morning, Jane" Jane: "Good morning, Tom" Tom: "Want some as well?"
    Jane: "No, thank you. I''m fine." Tom: "Excellent. You are on the right track!"
    Jane: "Yes, I am" Tom: "Do you sleep better these days?" Jane: "Yes, I do. Thank
    you. "`'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3 naturally understands what Tom refers to with “drinking” with this level
    of English and good manners. The output is quite satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Summarize: Tom says "good morning" to Jane. Tom offers her some of what he''s
    drinking. Jane says "no, thank you. I''m fine." Tom says "excellent" and that
    she is on the right track. Jane says, "yes, I am." Tom asks if she sleeps better
    these days.`'
  prefs: []
  type: TYPE_NORMAL
- en: We could imagine an endless number of variations on this same conversation by
    introducing other people into the dialogue and other objects and generating an
    endless number of situations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s sum these experiments up.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our thoughts are often chaotic. Humans use many methods to reconstruct unstructured
    sentences. *Humans often need to ask additional questions to understand what somebody
    is talking about. You need to accept this when interacting with a trained transformer
    such as OpenAI GPT-3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that a *dialogue d(T)* with a transformer and the response, *maxR(response)*,
    depends on the quality of your inputs, *minI(prompt)*, as defined at the beginning
    of this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d(T)=minI(prompt) ![](img/B17948_16_22.png) maxR(response)*'
  prefs: []
  type: TYPE_NORMAL
- en: When *minI(prompt)![](img/B17948_16_22.png)*`1`, the probability of *maxR(response)![](img/B17948_16_22.png)*`1`.
  prefs: []
  type: TYPE_NORMAL
- en: When *minI(prompt)![](img/B17948_16_22.png)*`0`, the probability of *maxR(response)![](img/B17948_16_22.png)*`0`.
  prefs: []
  type: TYPE_NORMAL
- en: Practice prompt engineering and measure your progress in time. Prompt engineering
    is a new skill that will take you to the next level of AI.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt engineering abilities lead to being able to master copilots.
  prefs: []
  type: TYPE_NORMAL
- en: Copilots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the world of AI-driven development copilots powered by OpenAI and
    available in Visual Studio.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Copilot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin with [GitHub Copilot:](https://github.com/github/copilot-docs)
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/git](https://github.com/github/copilot-docs)hub/copilot-docs'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use GitHub Copilot with PyCh[arm (JetBrains):](https://github.com/github/copilot-docs/tree/main/docs/jetbrains)
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/github/copilot-docs/tree/ma](https://github.com/github/copilot-docs/tree/main/docs/jetbrains)in/docs/jetbrains'
  prefs: []
  type: TYPE_NORMAL
- en: Follow the instructions in the documentation to install JetBrains and activate
    OpenAI GitHub Copilot in PyCharm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working withGitHub Copilot is a four-step process (see *Figure 16.7*):'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Codex is trained on public code and text on the internet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The trained model is plugged into the GitHub Copilot service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GitHub service manages back-and-forth flows between code we write in an
    editor (in this case PyCharm) and OpenAI Codex. The GitHub Service Manager makes
    suggestions and then sends the interactions back for improvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code editor is our development workspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B17948_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: GitHub Copilot’s four-step process'
  prefs: []
  type: TYPE_NORMAL
- en: Follow the instructions provided by GitHub Copilot, log in to GitHub when you
    are in PyCharm. For any trou[bleshooting, read https://copilo](https://copilot.github.com/#faqs)t.github.com/#faqs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are all set in the PyCharm editor, simply type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as the code is typed, you can open the OpenAI GitHub suggestion pane
    and see the suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Suggestions for the code you typed'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you choose the copilotsuggestion you prefer, it will appear in the editor.
    You can confirm suggestions with the *Tab* key. You can wait for another suggestion,
    such as drawing a scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot will be displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: A GitHub Copilot scatterplot'
  prefs: []
  type: TYPE_NORMAL
- en: You can run the result on your machine with `GitHub_Copilot.py`, which is in
    the `Chapter16` folder in this book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: The technology is seamless, invisible, and will progressively expand into all
    areas of development. The system is packed with GPT-3 functionality along with
    other pipelines. The technology is available for Python, JavaScript, and more.
  prefs: []
  type: TYPE_NORMAL
- en: It will take training with prompt engineering to get used to working with GitHub
    Copilot driven by OpenAI Codex.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go directly to OpenAI Codex, which can be a good place to train using
    copilots.
  prefs: []
  type: TYPE_NORMAL
- en: Codex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI Codex does more than suggest source code. Codex can translate natural
    language into source code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the OpenAI website and click on the link to the Codex interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://beta.openai.com/codex-javascript-sandbox](https://beta.openai.com/codex-javascript-sandbox)'
  prefs: []
  type: TYPE_NORMAL
- en: Provide instructions and the result will be displayed in the Codex window and
    the source code on the right pane of the interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The example in this section contains a few sequences using the prompt pane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_16_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.10: Codex’s JavaScript sandbox'
  prefs: []
  type: TYPE_NORMAL
- en: We enter natural language in the **Provide instructions…** pane, generating
    JavaScript that will run.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, I entered two instructions in natural language:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Draw 50 small ping pong balls of all sorts of colors`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Make the balls round`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Codex is derived from GPT-3\. In some cases, the engine is referred to as “davinci-codex.”
    This means Codex inherits the properties of a GPT-3 engine. To understand this,
    let’s take one parameter among others: `top_p`. The engine samples the output
    of the engine. For example, if `top_p` is set to `0.1`, it will only take the
    top 10% of the sampling into account. It will retrieve the tokens in that top
    10% probability mass. Since the whole computation is stochastic, it might take
    some top choices and another set of top choices to run after. Be patient and take
    your time designing the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your AI engine learning path:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Learn to understand the behavior of the Codex engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept the free creative nature of stochastic algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get used to piloting them with better prompts. You will then be able to grow
    with Codex as the engines improve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The script will appear in the JavaScript pane with commented instructions and
    the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_16_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.11: Creating fifty multicolored balls'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s ask the program to move the balls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the balls move, export the code to HTML by clicking on the **Export to**
    **JSFiddle** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing logo  Description automatically generated](img/B17948_16_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.12: The Export to JSFiddle button'
  prefs: []
  type: TYPE_NORMAL
- en: 'JSFiddle creates an HTML page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_16_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.13: JSFiddle creating an HTML page'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the code was saved to `codex.html`, which is in this chapter’s
    folder in the GitHub repository of the book. You can open and watch the innovative
    result of creating an HTML page with language natural to code source code.
  prefs: []
  type: TYPE_NORMAL
- en: Domain-specific GPT-3 engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section explores GPT-3 engines that can perform domain-specific tasks.
    We will run three models in the three subsections of this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding2ML to use GPT-3 to provide embeddings for ML algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruct series to ask GPT-3 to provide instructions for any task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content filter to filter bias or any form of unacceptable input and output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open `Domain_Specific_GPT_3_Functionality.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with embedding2ML (embeddings as an input to ML).
  prefs: []
  type: TYPE_NORMAL
- en: Embedding2ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI has trained several embedding models with different dimensions with
    different capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Ada (1,024 dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Babbage (2,048 dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Curie (4,096 dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davinci (12,288 dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more explanations on each engine, you will find more information on OpenAI’s
    website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://beta.openai.com/docs/guides/embeddings](https://beta.openai.com/docs/guides/embeddings).'
  prefs: []
  type: TYPE_NORMAL
- en: The Davinci model offers embedding with 12,288 dimensions. In this section,
    we will use the power of Davinci to generate the embeddings of a supply chain
    dataset. However, we will not send the embeddings to the embedding sublayer of
    the transformer!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will send the embeddings to a clustering machine learning program from the
    scikit-learn library in six steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Installing and importing OpenAI, and entering the API key*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 2: Loading the dataset*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 3: Combining the columns*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 4: Running the GPT-3 embedding*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 5: Clustering (k-means) with the embeddings*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 6: Visualizing the clusters (t-SNE)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process is summed up in *Figure 16.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shape, polygon  Description automatically generated](img/B17948_16_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.14: Six-step process for sending embeddings to a clustering algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Google Colab file, `Domain_Specific_GPT_3_Functionality.ipynb` and
    go to the `Embedding2ML with GPT-3 engine` section of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The steps described in this section match the notebook cells. Let’s go through
    a summary of each step of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Installing and importing OpenAI'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with the following substeps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cell
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart the runtime
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the cell again to make sure since you restarted the runtime:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Enter the API key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now load the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Loading the dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Load your file before running the cell. I uploaded `tracking.csv` (available
    in the GitHub repository of this book), which contains SCM data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The data contains seven fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Id`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Product`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`User`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Score`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Summary`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Text`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s print the first few lines using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can combine columns to build the clusters we wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Combining the columns'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can combine the `Product` column with `Summary` to obtain a view of products
    and their delivery status. Remember that this is only an experimental exercise.
    In a real-life project, carefully analyze and decide on the columns you wish to
    combine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example code can be replaced with any choice you make for your
    experimentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see a new column named `combined`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We will now run the embedding model on the `combined` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Running the GPT-3 embedding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now run the `davinci-similarity` model to obtain 12,288 dimensions
    for the `combined` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is impressive. We have 12,288 dimensions for the combined column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to convert the result into a `numpy` matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix has a shape of 1,053 records x 12,288 dimensions, which is quite
    impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The matrix is now ready to be sent to a scikit-learn machine learning clustering
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Clustering (k-means clustering) with the embeddings'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We usually send classical datasets to a k-means clustering algorithm. We will
    send a 12,288-dimension dataset to the ML algorithm, not to the next sublayer
    of the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import k-means from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now run a classical k-means clustering algorithm with our 12,288-dimension
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is four clusters as requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the labels for the content of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now visualize the clusters using t-SNE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Visualizing the clusters (t-SNE)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: t-SNE keeps local similarities. PCA maximizes large pairwise distances. In this
    case, smaller pairwise distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook will use `matplotlib` to display t-SNE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Before visualizing we need to run the t-SNE algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now display the results in `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot shows the clusters with many data points piled up around them. There
    are also many data points circled around the clusters attached to the closest
    centroid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_16_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.15: Clusters of embeddings-t-SNE'
  prefs: []
  type: TYPE_NORMAL
- en: We ran a large GPT-3 model to embed 12,288 dimensions. Then we plugged the result
    into a clustering algorithm. The potential of combining transformers and machine
    learning is endless!
  prefs: []
  type: TYPE_NORMAL
- en: You can go to the `Peeking into the embeddings` section of the notebook if you
    wish to peek into the data frames.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now have a look at the instruct series.
  prefs: []
  type: TYPE_NORMAL
- en: Instruct series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Personal assistants, avatars in metaverses, websites, and many other domains
    will increasingly need to provide clear instructions when a user asks for help.
    Go to the `instruct series` section of `Domain_Specific_GPT_3_Functionality.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will ask a transformer to explain how to set up parent
    control in Microsoft Edge with the following prompt: `Explain how to set up parent
    control in Edge`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first run the completion cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is a list of instructions as requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The number of instructions you can ask for is unlimited! Use your creativity
    and imagination to find more examples!
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes the input or the output is not acceptable. Let’s see how to implement
    a content filter.
  prefs: []
  type: TYPE_NORMAL
- en: Content filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias, unacceptable language, and any form of unethical input should be excluded
    from your AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: One of OpenAI’s trained models is a content filter. We will run an example in
    this section. Go to the `content filter` section of `Domain_Specific_GPT_3_Functionality.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'My recommendation is to filter the input and the output, as shown in *Figure
    16.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17948_16_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.16: Implementing a content filter'
  prefs: []
  type: TYPE_NORMAL
- en: 'My recommendation is to implement a three-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply a content filter to ALL input data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the AI algorithm run as trained
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a content filter to ALL output data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, the input and output data will be named `content`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take an obnoxious input as the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This input is unacceptable! School is not the NBA. Basketball should remain
    a nice exercise for *everyone*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now run the content filter in the cell - `content-filter-alpha`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The content filter stores the result in `response`, a dictionary object. We
    retrieve the value of choice to obtain the level of acceptability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The content filter sends one of three values back:'
  prefs: []
  type: TYPE_NORMAL
- en: '`0` – Safe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1` – Sensitive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2` – Unsafe'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the result is 2, of course:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The content filter might not be sufficient. I recommend adding other algorithms
    to control and filter input/output content: rule bases, dictionaries, and other
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored domain-specific models, let’s build a transformer-based
    recommender system.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based recommender systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer models learn sequences. Learning language sequences is a great place
    to start considering the billions of messages posted on social media and cloud
    platforms each day. Consumer behaviors, images, and sounds can also be represented
    in sequences.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will first create a general-purpose sequence graph and then
    build a general-purpose transformer-based recommender in Google Colaboratory.
    We will then see how to deploy them in metahumans.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first define general-purpose sequences.
  prefs: []
  type: TYPE_NORMAL
- en: General-purpose sequences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many activities can be represented by entities and links between them. They
    are thus organized in sequences. For example, a video on YouTube can be an entity
    A, and the link can be the behavior of a person going from video A to video E.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is a bad fever being an entity F, and the link being the inference
    a doctor may make leading to a micro-decision B. The purchase of product D on
    Amazon by a consumer can generate a link to a suggestion C or another product.
    The examples are infinite!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the entities in this section with six letters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E={A,B,C,D,E,F}*'
  prefs: []
  type: TYPE_NORMAL
- en: When we speak a language, we follow grammar rules and cannot escape them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose *A =”I”*, *E =”eat”*, and *D =”candy”*. There is only
    one proper sequence to express the fact that I consume candy: “I eat candy.”'
  prefs: []
  type: TYPE_NORMAL
- en: If somebody says “eat candy I,” it will sound slightly off.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sequence, the links representing those rules are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A->E (I eat)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*E->D(eat candy)*'
  prefs: []
  type: TYPE_NORMAL
- en: We can automatically infer rules in any domain by observing behaviors, learning
    datasets with ML, or manually listening to experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will suppose that we have observed a YouTube user for several
    months who spends several hours watching videos. We have noticed that the user
    systematically goes from one type of video to another. For example, from the video
    of singer B to the video of singer D. The behavior rules *X* of this person *P*
    seem to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*X(P)={AE,BD,BF,C,CD,DB,DC,DE,EA,ED,FB}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent this system of entities as vertices in a graph and the links
    as edges. For example, if we apply *X(P)* to the vertices, we obtain the following
    undirected graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clock  Description automatically generated](img/B17948_16_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.17: Graph of YouTube user’s video combinations'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the vertices are videos of a viewer’s favorite singers and that *C*
    is the singer the viewer prefers. We can give a value of 1 to the statistical
    transitions (links or edges) the viewer made in the past weeks. We can also give
    a value of 100 to the viewer’s favorite singer’s videos.
  prefs: []
  type: TYPE_NORMAL
- en: For this viewer, the path is represented by the (edges, vertices) values *V(R(P)):*
  prefs: []
  type: TYPE_NORMAL
- en: '*V(X(P))={AE=1,BD=1,BF=1,C=100,CD=1,DB=1,DE=1,EA=1,ED=1,FB=1}*'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a recommender is thus to suggest sequences that lead to videos of
    singer *C* or suggest *C* directly in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent the undirected graph in reward matrix `R`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Let’s use this reward matrix to simulate the activity of a viewer *X* over several
    months.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset pipeline simulation with RL using an MDP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will simulate the behavior *X* of a person *P* watching
    videos of songs on YouTube, which we define as *X(P)*. We will determine the values
    of the behavior of *P* as *V(X(P))*. We will then organize the values in a reward
    matrix *R* for a **Markov Decision Process** (**MDP**) that we will now implement
    in a Bellman equation.
  prefs: []
  type: TYPE_NORMAL
- en: Open `KantaiBERT_Recommender.ipynb`, which is in this chapter’s folder in the
    book’s GitHub repository. The notebook is a modification of `KantaiBERT.ipynb`
    described in *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 4*, we trained a transformer using `kant.txt`, which contained some
    of the works of Immanuel Kant. In this section, we will generate thousands of
    sequences of a person’s behavior through **reinforcement learning** (**RL**).
    RL is not in the scope of this book, but this section contains some reminders.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to train a transformer model to learn and simulate a person’s
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Training customer behaviors with an MDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`KantaiBERT.ipynb` in *Chapter 4* began by loading `kant.txt` to train a RoBERTa
    with a DistilBERT architecture. `kant.txt` contained works by Immanuel Kant. In
    this section, we will generate sequences using the reward matrix *R* defined in
    the *General-purpose sequences* section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The first cell of the program is thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Step 1A Training: Dataset Pipeline Simulation with RL using an MDP:`'
  prefs: []
  type: TYPE_NORMAL
- en: 'This cell implements an MDP using the Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`R` is the original reward matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q` is the updated matrix, which is the same size as `R`. However, it is updated
    through reinforcement learning to compute the relative value of the link (edge)
    between each entity (vertex).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma` is a learning rate set to `0.8` to avoid overfitting the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxValue` is the maximum value of the next vertex. For example, if the viewer
    `P` of YouTube videos is viewing singer `A`, the program might increase the value
    of `E` so that this suggestion can appear as a recommendation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little by little, the program will try to find the best values to help a viewer
    find the best videos to watch. Once the reinforcement program has learned the
    best links (edges), it can recommend the best viewing sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'The original reward matrix has trained to become an operational matrix. If
    we add the original entities, the trained values clearly appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The original sequences of values *V* of the behaviors *X* of person *P* were:'
  prefs: []
  type: TYPE_NORMAL
- en: '*V(X(P))={AE=1,BD=1,BF=1,C=100, CD=1,DB=1,DE=1,EA=1,ED=1,FB=1}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'They have been trained to become:'
  prefs: []
  type: TYPE_NORMAL
- en: '*V(X(P))={AE=259.44, BD=321.8 ,BF=207.752, C=500, CD=321.8 ,DB=258.44, DE=258.44,
    EA=207.752, ED=321.8, FB=258.44}*'
  prefs: []
  type: TYPE_NORMAL
- en: This is quite a change!
  prefs: []
  type: TYPE_NORMAL
- en: Now, it becomes possible to recommend a sequence of exciting videos of *P*’s
    preferred singers. Suppose *P* views a video of singer *E*. Line *E* of the trained
    matrix will recommend a video of the highest value of that line, which is *D=321.8*.
    Thus, a video of singer *D* will appear in the YouTube feed of person *P*.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section is not to stop at this phase. Instead, this section
    uses an MDP to create meaningful sequences to create a dataset for the transformer
    to use for training.
  prefs: []
  type: TYPE_NORMAL
- en: YouTube does not need to generate sequences to create a dataset. YouTube stores
    all the behaviors of all viewers in big data. Then Google’s powerful algorithms
    take over to recommend the best videos in the video feed of a viewer.
  prefs: []
  type: TYPE_NORMAL
- en: Other platforms use cosine similarity as implemented in *Chapter 9*, *Matching
    Tokenizers and Datasets*, to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The MDP could have been trained for YouTube viewers, Amazon buyers, Google search
    results, a doctor’s diagnosis path, a supply chain, and any type of sequence.
    Transformers are taking sequence learning, training, and predictions to another
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a simulation to create behavior sequences for a transformer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating consumer behavior with an MDP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the RL part of the program is trained in cell 1, cell 2,`Step 1B Applying:
    Dataset Pipeline Simulation with MDP` will simulate a YouTube viewer’s behavior
    over several months. It will also include similar viewer profiles adding up to
    a simulation of 10,000 sequences of video watching.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cell 2 begins by creating the `kant.txt` file that will be used to train the
    KantaiBERT transformer model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the entities (vertices) are introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The function chooses a random start vertex named `origin`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The program uses the trained matrix to select the best sequence for any domain
    from that point of origin. In this case, we suppose they are the favorite singers
    of a person such as the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Once 10,000 sequences have been calculated, `kant.txt` contains a dataset for
    the transformer.
  prefs: []
  type: TYPE_NORMAL
- en: With `kant.txt`, the remaining cells of the program are the same as in `KantaiBERT.ipynb`
    described in *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer is now ready to make recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Making recommendations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In *Chapter 4*, `KantaiBERT.ipynb` contained the following masked sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This sequence is specific and related to *Immanuel Kant*’s works. This notebook
    has a general-purpose dataset that can be used in any domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this notebook, the input is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains duplicates. It will take a cleaning function to filter
    them to obtain two non-duplicate sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The sequences make sense. Sometimes a viewer will watch the same videos, sometimes
    not. The behavior can be chaotic. That’s where machine learning comes in and how
    AI can be used in metahumans.
  prefs: []
  type: TYPE_NORMAL
- en: Metahuman recommenders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the sequences have been generated, they will be converted back into natural
    language for user interfaces. Metahuman, in this section, refers to a recommender
    that takes large amounts of features that:'
  prefs: []
  type: TYPE_NORMAL
- en: Exceed human capacity to reason with that many parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lead to more accurate predictions than a human can make
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These pragmatic metahumans are not digital humans yet in this context but powerful
    computing tools. We will go through metahumans in the digital sense in the *Humans
    and AI copilots in metaverses* section.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the BDC sequences could be a song by singer *B*, followed by singer
    *D*, and then *P*’s favorite singer *C*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the sequence is converted into natural language, several options are possible:'
  prefs: []
  type: TYPE_NORMAL
- en: The sequence can be sent to a bot or a digital human.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an emerging technology appears, jump on the train and take the ride! You
    will get to know this technology and evolve with it. You can google other metahuman
    platforms. In any case, you can remain on the cutting edge by learning how to
    circumvent limits and find ways to use new technology.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can use the metahuman as an educational video while waiting for an API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metahuman can be inserted into an interface as a voice message. For example,
    when using Google Maps in a car, you listen to the voice. It sounds like a human.
    We even slip and sometimes think it’s a person, but it isn’t. It’s a machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can also be an invisible embedded suggestion in Amazon. It remains something
    that makes recommendations that lead us to make micro-decisions. It influences
    us as a salesperson would do. It’s an invisible metahuman.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the general-purpose sequences were created by an MDP and trained
    by a RoBERTa transformer. This shows that transformers can be applied to any type
    of sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how transformers are applied to computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about NLP, not computer vision. However, in the previous section,
    we implemented general purpose sequences that can be applied to many domains.
    Computer vision is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The title of the article by *Dosovitskiy* et al. (2021) says it all: *An image
    is worth 16x16 words: Transformers for Image Recognition at Scale*. The authors
    processed an image as sequences. The results proved their point.'
  prefs: []
  type: TYPE_NORMAL
- en: Google has made vision transformers available in a Colaboratory notebook. Open
    `Vision_Transformer_MLP_Mixer.ipynb` in the `Chapter16` directory of this book’s
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Open `Vision_Transformer_MLP_Mixer.ipynb` contains a transformer computer vision
    model in `JAX()`. JAX combines Autograd and XLA. JAX can differentiate Python
    and NumPy functions. JAX speeds up Python and NumPy by using compilation techniques
    and parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is self-explanatory. You can explore it to see how it works. However,
    bear in mind that when Industry 4.0 reaches maturity and Industry 5.0 kicks in,
    the best implementations will be obtained by integrating your data on Cloud AI
    platforms. Local development will diminish, and companies will turn to Cloud AI
    without bearing local development, maintenance, and support.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook’s table of contents contains a transformer process we have gone
    through several times in this book. However, this time, it’s simply applied to
    sequences of digital image information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_16_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.18: Our vision transformer notebook'
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook follows standard deep learning methods. It shows some images with
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_16_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.19: Images with labels'
  prefs: []
  type: TYPE_NORMAL
- en: 'The images in this chapter are from *Learning Multiple Layers of Features from
    Tiny Images*, *Alex Krizhevsky*, 2009: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
    They are part of the `CIFAR-10` and `CIFAR-100` datasets (`toronto.edu`): https://www.cs.toronto.edu/~kriz/cifar.html.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook contains the standard transformer process and then displays the
    training images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![A picture containing screenshot  Description automatically generated](img/B17948_16_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.20: Trained data'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer programs can classify random pictures. It seems like a miracle to
    take a transformer model originally designed for NLP and use it for general-purpose
    sequences for recommenders and then for computer vision. However, we are just
    beginning to explore the generalization of sequences training.
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity of the model is surprising! The vision transformer relies on
    the architecture of transformers. It does not contain the complexity of convolutional
    neural networks. Yet, it produces comparable results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, robots and bots can be equipped with transformer models to understand language
    and interpret images to understand the world around them.
  prefs: []
  type: TYPE_NORMAL
- en: Vision transformers can be implemented in metahumans and metaverses.
  prefs: []
  type: TYPE_NORMAL
- en: Humans and AI copilots in metaverses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans and metahuman AI are merging into metaverses. Exploring metaverses is
    beyond the scope of this book. The toolbox provided by this book shows the path
    to metaverses populated by humans and metahuman AI.
  prefs: []
  type: TYPE_NORMAL
- en: Avatars, computer vision, and video game experience will make our communication
    with others *immersive*. We will go from looking at smartphones to being in locations
    with others.
  prefs: []
  type: TYPE_NORMAL
- en: From looking at to being in
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evolution from looking at to being in is a natural one. We invented computers,
    added screens, then invented smartphones, and now use apps for video meetings.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can enter virtual reality for all types of meetings and activities.
  prefs: []
  type: TYPE_NORMAL
- en: We will use Facebook’s metaverse, for example, on our smartphone to *feel* present
    in the same location as the people (personal and professional) we meet. Feeling
    *present* will no doubt be a major evolution in smartphone communication.
  prefs: []
  type: TYPE_NORMAL
- en: '*Feeling present* somewhere is quite different from looking at a small screen
    on a mobile.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The metaverse will make the impossible possible: a spacewalk, surfing on huge
    waves, walking in a forest, visiting dinosaurs, and wherever our imagination takes
    us.'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, there are limits, dangers, threats, and everything that goes with human
    technology. However, we can use AI to control AI, as we saw with content filtering.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer tools in this book added to the emerging metaverse technology
    will take us literally to another world.
  prefs: []
  type: TYPE_NORMAL
- en: Make good use of the knowledge and skills you acquired in this book to create
    your ethical future in a metaverse or the physical world!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter described the rise of AI copilots with human-decision-making-level
    capability. Industry 4.0 has opened the door to machine interconnectivity. Machine-to-machine
    micro-decision making will speed up transactions. AI copilots will boost our productivity
    in a wide range of domains.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to use OpenAI Codex to generate source code while we code and even
    with natural language instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We built a transformer-based recommender system using a dataset generated by
    the MDP program to train a RoBERTa transformer model. The dataset structure was
    a multi-purpose sequence model. A metahuman can thus acquire multi-domain recommender
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter then showed how a vision transformer could classify images processed
    as sequences of information.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw that the metaverse would make recommendations visible through
    a metahuman interface or invisible in deeply embedded functions in social media,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have emerged with innovating copilots and models in an incredibly
    complex new era. The journey will prove both challenging and exciting!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI copilots that can generate code automatically do not exist. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AI copilots will never replace humans. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-3 engines can only do one task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can be trained to be recommenders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can only process language. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer sequence can only contain words. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vision transformers cannot equal CNNs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AI robots with computer vision do not exist. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is impossible to produce Python source code automatically. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We might one day become the copilots of robots. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI platf[orm for GPT-3: htt](https://openai.com)ps://openai.com'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI model[s and engines: https://beta.openai.c](https://beta.openai.com/docs/engines)om/docs/engines'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vision Transformers: *Alexey Dosovitskiy*, *Lucas Beyer*, *Alexander Kolesnikov*,
    *Dirk Weissenborn*, *Xiaohua Zhai*, *Thomas Unterthiner*, *Mostafa Dehghani*,
    *Matthias Minderer*, *Georg Heigold*, *Sylvain Gelly*, *Jakob Uszkoreit*, *Neil
    Houlsby*, 2020, *An Image is Worth 16x16 Words: Transformers for Image Recogni*[*tion
    at Scale*: https://arxiv.org](https://arxiv.org/abs/2010.11929)/abs/2010.11929'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JAX for vision [transformers: https://github](https://github.com/google/jax).com/google/jax'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI, Visual S[tudio Copilot: https://copi](https://copilot.github.com/)lot.github.com/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Facebook metaverse: https://www.facebook.com/Meta/videos/577658430179350'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Markov Decision Process (MDP), examples and graph: *Denis Rothman*, 2020, *Artificial
    Intelligence by Example*[, 2^(nd) Edition: https://www.amazon.com/Artificial-Intelligence-Example-advanced-learning/dp/1839211539/ref=sr_1_3?crid=238SF8FPU7BB0&keywords=denis+rothman&qid=1644008912&sprefix=denis+rothman%2Ca](https://www.amazon.com/Artificial-Intelligence-Example-advanced-learning/dp/1839211539/ref=sr_1_3?crid=238SF8FPU7BB0&keywords=denis+rothman&qid=1644008912&sprefix=denis+rothman%2Caps%2C143&sr=8-3)ps%2C143&sr=8-3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: https://www.packt.link/Transformers
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
