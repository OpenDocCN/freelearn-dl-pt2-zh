- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: The Emergence of Transformer-Driven Copilots
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器驱动副驾驶员的出现
- en: When **Industry 4.0** (**I4.0**) reaches maturity, it will all be about machine-to-machine
    connections, communication, and decision-making. AI will be primarily embedded
    in ready-to-use pay-as-you-go cloud AI solutions. Big tech will absorb the most
    talented AI specialists to create APIs, interfaces, and integration tools.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当**工业4.0**（**I4.0**）达到成熟时，一切都将关于机器对机器的连接、通信和决策。人工智能将主要嵌入成熟的按需付费云AI解决方案中。大型科技公司将吸收大部分才华横溢的AI专家，创建API、接口和集成工具。
- en: AI specialists will go from development to design to becoming architects, integrators,
    and cloud AI pipeline administrators. Thus, AI is becoming a job for engineer
    consultants more than engineer developers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: AI专家将从开发转变为设计，成为架构师、集成者和云AI管道管理员。因此，AI更像是工程顾问的工作，而不是工程开发者的工作。
- en: '*Chapter 1*, *What Are Transformers?*, introduced foundation models, transformers
    that can do NLP tasks they were not trained for. *Chapter 15*, *From NLP to Task-Agnostic
    Transformer Models*, expanded foundation model transformers to task-agnostic models
    that can perform vision tasks, NLP tasks, and much more.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1章*，*变压器是什么？*，介绍了基础模型，变压器可以完成它们未经训练的自然语言处理任务。*第15章*，*从自然语言处理到任务无关的变压器模型*，将基础模型变压器扩展到可以执行视觉任务、自然语言处理任务等的任务无关模型。'
- en: This chapter will extend task-agnostic OpenAI GPT-3 models to a wide range of
    copilot tasks. A new generation of AI specialists and data scientists will learn
    how to work with AI copilots to help them generate source code automatically and
    make decisions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将把OpenAI GPT-3模型扩展到各种副驾驶员任务。新一代AI专家和数据科学家将学习如何与AI副驾驶员合作，帮助他们自动生成源代码并做出决策。
- en: This chapter begins by exploring prompt engineering in more detail. The example
    task consists of converting meeting notes into a summary. Transformers boost our
    productivity. However, we will see how natural language remains a challenge for
    AI.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将首先更详细地探讨提示工程。示例任务包括将会议记录转换为摘要。变压器提高了我们的生产力。但是，我们将看到自然语言对AI仍然是一个挑战。
- en: We will learn how to use OpenAI Codex as a copilot. GitHub Copilot suggests
    source code as we write our programs using Codex. Codex can also convert natural
    language into code.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何将OpenAI Codex作为副驾驶员。GitHub Copilot在我们编写程序时使用Codex为我们建议源代码。Codex也可以将自然语言转换为代码。
- en: We will then discover new AI methods with domain-specific GPT-3 engines. This
    chapter will show how to generate embeddings with 12,288 dimensions and plug them
    into machine learning algorithms. We will also see how to ask a transformer to
    produce instructions automatically.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将发现具有特定领域GPT-3引擎的新AI方法。本章将展示如何生成具有12288维度的嵌入并将其插入机器学习算法中。我们还将看到如何要求转换器自动生成说明。
- en: We will see how to filter biased input and output before looking into transformer-driven
    recommenders. AI of the 2020s must be built with ethical methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究变压器驱动的推荐系统之前，我们将看到如何过滤偏见的输入和输出。2020年代的人工智能必须以道德方法构建。
- en: Recommender systems have permeated every social media platform to suggest videos,
    posts, messages, books, and many other products we might want to consume. We will
    build an educational multi-purpose transformer-based recommender system using
    ML in the process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统渗透到每个社交媒体平台，推荐视频、帖子、消息、书籍和其他我们可能想消费的产品。我们将在这个过程中构建一个基于ML的教育多用途变压器推荐系统。
- en: Transformer models analyze sequences. They began with NLP but have successfully
    expanded to computer vision. We will explore a transformer-based computer vision
    program developed in JAX.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型分析序列。它们起初用于自然语言处理，但已成功扩展到计算机视觉。我们将探索在JAX中开发的基于变压器的计算机视觉程序。
- en: Finally, we will see AI copilots contribute to the transition of virtual systems
    into metaverses, which will expand in this decade. You are the pilot when you
    develop your applications. However, when you have code to develop, the activated
    completions are limited to methods, not lines of code. An IDE might suggest a
    list of methods. Copilots can produce completions of whole paragraphs of code!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将看到AI副驾驶员如何促进虚拟系统向元宇宙的过渡，在本十年中将会扩展。当你开发应用程序时，你是飞行员。但是，当你有要开发的代码时，激活的完成只限于方法，而不是代码行。IDE
    可能会建议方法列表。副驾驶员可以生成整段代码的完成！
- en: 'This chapter covers the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Prompt engineering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: GitHub Copilot
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub Copilot
- en: Codex language to source code models
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codex语言到源代码模型
- en: Embedding datasets
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入数据集
- en: Embedded-driven machine learning
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入式驱动的机器学习
- en: Instruct series
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指导系列
- en: Content filter models
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容过滤模型
- en: Exploring transformer-based recommenders
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索基于转换器的推荐系统
- en: Extending NLP sequence learning to behavior predictions
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将自然语言处理序列学习扩展到行为预测
- en: Implementing transformer models in JAX
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在JAX中实现转换器模型
- en: Applying transformer models to computer vision
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将转换器模型应用于计算机视觉
- en: Let’s begin with prompt engineering, which is a critical ability to acquire.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从提示工程开始，这是一种必须获得的关键能力。
- en: Prompt engineering
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程
- en: Speaking a specific language is not hereditary. There is not a language center
    in our brain containing the language of our parents. Our brain engineers our neurons
    early in our lives to speak, read, write, and understand a language. Each human
    has a different language circuitry depending on their cultural background and
    how they were communicated with in their early years.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 说一种特定的语言不是遗传的。我们大脑中没有一个包含我们父母语言的语言中心。我们的大脑在我们的生活早期工程化我们的神经元，以便说、阅读、写作和理解一种语言。每个人的语言电路都取决于他们的文化背景以及在早年如何与他们交流。
- en: 'As we grow up, we discover that much of what we hear is chaos: unfinished sentences,
    grammar mistakes, misused words, bad pronunciation, and many other distortions.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们成长，我们发现我们听到的很多东西都是混乱的：未完整的句子、语法错误、词语误用、发音错误以及许多其他扭曲。
- en: We use language to convey a message. We quickly find that we need to adapt our
    language to the person or audience we address. We might have to try additional
    “inputs” or “prompts” to obtain the result (“output”) we expect. Foundation-level
    transformer models such as GPT-3 can perform hundreds of tasks in an indefinite
    number of ways. *We must learn the language of transformer prompts and responses
    as we would any other language*. Effective communication with a person or near-human-level
    transformer must contain a minimum amount of information to maximize results.
    We represent the minimum input information to obtain a result as *minI* and the
    maximum output of any system as *maxR*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用语言传达信息。我们很快发现我们需要将我们的语言适应我们所面向的人或观众。我们可能需要尝试额外的“输入”或“提示”来得到我们期望的结果（“输出”）。像GPT-3这样的基础级转换器模型可以以无限多种方式执行数百种任务。*我们必须像学习任何其他语言一样学习转换器提示和响应的语言*。与人或接近人类水平的转换器进行有效沟通必须包含最少的信息以最大化结果。我们代表获得结果所需的最小输入信息为*minI*，以及任何系统的最大输出为*maxR*。
- en: 'We can represent this chain of communication as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种沟通链表示为：
- en: '*minI(input) ![](img/B17948_16_22.png) maxR(output)*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*minI(input) ![](img/B17948_16_22.png) maxR(output)*'
- en: 'We will replace “input” with “prompt” for transformers to show that our input
    influences how the model will react. The output is the “response.” The dialogue
    with transformers, *d(T)*, can be expressed as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用“提示”取代“输入”，以便转换器表明我们的输入会影响模型的反应。输出是“响应”。与转换器的对话，*d(T)*，可以表示为：
- en: '*d(T)=minI(prompt)* *![](img/B17948_16_22.png) maxR(response)*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*d(T)=minI(prompt)* *![](img/B17948_16_22.png) maxR(response)*'
- en: When *minI(prompt)![](img/B17948_16_22.png)*`1`, the probability of *maxR*(response)
    *![](img/B17948_16_22.png)*`1`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当*minI(prompt)![](img/B17948_16_22.png)*`1`时，*maxR*(response)的概率也*![](img/B17948_16_22.png)*`1`。
- en: When *minI(prompt)* *![](img/B17948_16_22.png)*`0`, the probability of *maxR*(response)
    *![](img/B17948_16_22.png)*`0`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当*minI(prompt)* *![](img/B17948_16_22.png)*`0`时，*maxR*(response)的概率也*![](img/B17948_16_22.png)*`0`。
- en: The quality *d(T)* depends on how well we can define *minI(prompt)*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*d(T)*的质量取决于我们能够定义*minI(prompt)*的程度。'
- en: If your prompt tends to reach `1`, then it will produce probabilities that tend
    to `1`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提示倾向于达到`1`，那么它将产生倾向于`1`的概率。
- en: If your prompt tends to reach `0`, then it will produce output probabilities
    that tend to `0`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提示趋向于达到`0`，那么它将产生趋向于`0`的输出概率。
- en: Your prompt is part of the content that impacts the probabilities! Why? Because
    the transformer will include the prompt and the response in its estimations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你的提示是影响概率的内容的一部分！为什么？因为转换器将在其估计中包含提示和响应。
- en: It takes many years to learn a language as a child or an adult. It also takes
    quite some time to learn the language of transformers and how to design *minI(prompt)*
    effectively. We need to understand them, their architecture, and the way the algorithms
    calculate predictions. Then we need to spend quite some time understanding how
    to design the input, the prompt, for the transformers to behave as we expect.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为孩子或成年人学习一门语言需要很多年的时间。学习变压器语言和如何有效设计*minI(prompt)*也需要相当长的时间。我们需要了解它们，它们的架构以及算法计算预测的方式。然后我们需要花相当多的时间来理解如何设计输入，提示，以使变压器表现出我们期望的行为。
- en: This section focuses on oral language. The prompt for OpenAI GPT-3 for an NLP
    task will often be taken from meeting notes or conversations, which tend to be
    unstructured. Transforming meeting notes or conversations into a summary can be
    quite challenging. This section will focus on summarizing notes of the same conversation
    in seven situations that go from casual English to casual or formal English with
    limited context.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍口头语言。OpenAI GPT-3的NLP任务的提示通常来自会议记录或对话，这些记录往往是非结构化的。将会议记录或对话转换成摘要可能是相当具有挑战性的。本节将专注于总结相同对话的七种情况下的笔记，从随意英语到具有有限上下文的随意或正式英语。
- en: We will begin with casual English with a meaningful context.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从具有有意义上下文的随意英语开始。
- en: Casual English with a meaningful context
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有有意义上下文的随意英语
- en: Casual English is spoken with shorter sentences and limited vocabulary.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随意英语是用较短的句子和有限的词汇说话的。
- en: Let’s ask OpenAI GPT-3 to perform a “notes to summary” task. Go to [www.openai.](http://www.openai.com)com.
    Log in or sign up. Then go to the **Examples** page and select **Notes to summary**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们要求 OpenAI GPT-3 执行“笔记到摘要”的任务。转到 [www.openai.](http://www.openai.com)com。登录或注册。然后转到**示例**页面并选择**笔记到摘要**。
- en: We will give GPT-3 all the information required to summarize a casual conversation
    between Jane and Tom. Jane and Tom are two developers starting work. Tom offers
    Jane coffee. Jane declines the offer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将给 GPT-3 提供所有必要的信息，以总结简和汤姆之间的随意对话。简和汤姆是两位开始工作的开发人员。汤姆提供咖啡给简。简拒绝了这个提议。
- en: 'In this case, *minI(prompt)=1* since the input information is fine, as shown
    in *Figure 16.1*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*minI(prompt)=1*，因为输入信息是正确的，如*图16.1*所示：
- en: '![](img/B17948_16_01.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_16_01.png)'
- en: 'Figure 16.1: Summarizing well-documented notes'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：总结了详细记录的笔记
- en: 'When we click on **Generate**, we get a surprisingly good answer, as shown
    in *Figure 16.2*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们点击**生成**时，我们得到了一个令人惊讶的好答案，如*图16.2*所示：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B17948_16_02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、文本、应用程序、电子邮件  自动生成的描述](img/B17948_16_02.png)'
- en: 'Figure 16.2: An acceptable summary is provided by GPT-3'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：GPT-3提供了一个可以接受的摘要
- en: Can we conclude that AI can find structures in our chaotic daily conversations,
    meetings, and aimless chatter? The answer isn’t easy. We will now complicate the
    input by adding a metonymy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否得出结论，AI可以在我们混乱的日常对话、会议和漫无目的的闲聊中找到结构？答案并不容易。我们现在将通过添加一个代称来使输入复杂化。
- en: Casual English with a metonymy
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以代称为特征的随意英语
- en: Tom mentioned the word `coffee`, setting GPT-3 on track. But what if Tom used
    the word `java` instead of `coffee`. Coffee refers to the beverage, but `java`
    is an ingredient that comes from the island of Java. A metonymy is when we use
    an attribute of an object, such as `java` for `coffee`. Java is also a programming
    language with a logo that is a cup of coffee.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 汤姆提到了`coffee`这个词，让 GPT-3 保持正确的方向。但如果汤姆用`java`代替`coffee`呢？咖啡是指饮料，而`java`是一种来自爪哇岛的成分。代称是当我们使用一个对象的属性时，比如用`java`代替`coffee`。Java也是一种编程语言，其标志是一杯咖啡。
- en: 'We are facing three possible definitions of `java`: an ingredient of coffee
    meaning coffee (metonymy), the island of Java, and the name of a programming language.
    GPT-3 now has a polysemy (several meanings of the same word) issue to solve.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临着`java`的三种可能定义：指咖啡的成分咖啡（代称），爪哇岛和一种编程语言的名称。GPT-3现在有一个多义性（同一个词的几个含义）问题需要解决。
- en: Humans master polysemy. We learn the different meanings of words. We know that
    a word doesn’t mean much without context. In this case, Jane and Tom are developers,
    complicating the situation. Are they talking about coffee or the language?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 人类掌握了多义性。我们学会了单词的不同含义。我们知道一个词没有上下文是没有多大意义的。在这种情况下，简和汤姆是开发人员，使情况复杂化。他们是在谈论咖啡还是语言？
- en: 'The answer is easy for a human since Tom then talks about his wife, who stopped
    drinking it. GPT-3 can be confused by this polysemy when the word `java` replaces
    `coffee`, and it produces an incorrect answer:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说答案很容易，因为汤姆随后谈到了他的妻子，她已经停止喝咖啡了。但是当词语`java`代替`coffee`时，GPT-3可能会因为这种多义性而产生错误的答案：
- en: '![Graphical user interface, text  Description automatically generated](img/B17948_16_03.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本 自动生成的描述](img/B17948_16_03.png)'
- en: 'Figure 16.3: Incorrect GPT-3 response when prompt is confusing'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：当提示混淆时GPT-3的错误响应
- en: We thus confirm that when *minI(prompt)**![](img/B17948_16_22.png)*`0`, the
    probability of *maxR(response)![](img/B17948_16_22.png)*`0`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们证实了当*minI(prompt)**![](img/B17948_16_22.png)*`0`时，*maxR(response)![](img/B17948_16_22.png)*`0`的概率。
- en: Human conversations can become even more difficult to analyze if we add an ellipsis.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们加入省略号，人类的对话会变得更加难以分析。
- en: Casual English with an ellipsis
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有省略号的非正式英语
- en: The situation can get worse. Let’s suppose Tom is drinking a cup of coffee,
    and Jane looks at him and the cup of coffee as she casually greets him.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 情况可能会变得更糟。假设汤姆正在喝一杯咖啡，简看着他和咖啡杯，然后随意地打了个招呼。
- en: 'Instead of asking Jane if she wants coffee or java, Tom says:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 汤姆没有问简是否想要咖啡或者Java，而是说：
- en: '`"Want some?"`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`"想要一些吗？"`'
- en: Tom left out the word `coffee`, which is an ellipsis. Jane can still understand
    what Tom means by looking at him holding a cup of coffee.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 汤姆省略了`coffee`这个词，这是一个省略号。简仍然可以通过看着他拿着一杯咖啡来理解汤姆的意思。
- en: 'OpenAI GPT-3 detects the word `drinking` and still manages to associate this
    verb with the question `Want some?`. We don’t `want some` of a programming language.
    The following summary produced by GPT-3 is still correct:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI GPT-3检测到了`drinking`这个词，并成功地将这个动词与问题`想要一些吗？`联系起来。我们不需要某种编程语言的`想要一些`。由GPT-3产生的以下摘要仍然是正确的：
- en: '![Text  Description automatically generated](img/B17948_16_04.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![文本 自动生成的描述](img/B17948_16_04.png)'
- en: 'Figure 16.4: Correct response produced by GPT-3'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：GPT-3产生的正确响应
- en: Now, let’s see what happens when there is vague context that a human can understand
    but remains a challenge for AI.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看当存在人类可以理解但对人工智能仍然是一个挑战的模糊背景时会发生什么。
- en: Casual English with vague context
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有模糊背景的非正式英语
- en: If we take this further, `Tom` doesn’t need to mention his wife for `Jane` to
    understand what he is talking about since he is holding a cup of coffee.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们进一步思考，`汤姆`并不需要提及他的妻子才能让`简`理解他在说什么，因为他正在拿着一杯咖啡。
- en: 'Let’s remove `Tom`''s reference to his wife and the verb `drinking`. Let’s
    leave `want some` in instead of coffee or java:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去掉`汤姆`提到他的妻子和动词`drinking`。让我们保留`想要一些`而不是咖啡或Java：
- en: '![](img/B17948_16_05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_16_05.png)'
- en: 'Figure 16.5: Vague input context'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：模糊的输入情境
- en: 'The output reflects the apparent chaos of the conversation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出反映了对话的明显混乱：
- en: '![](img/B17948_16_06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_16_06.png)'
- en: 'Figure 16.6: Poor GPT-3 response'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：GPT-3的不良响应
- en: 'The prompt was too vague, leading to an inadequate response that we can sum
    up as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 提示太模糊了，导致了一条不足的回答，我们可以总结为：
- en: '*d(T)![](img/B17948_16_22.png)0 because when* *minI(prompt)![](img/B17948_16_22.png)*`0`,
    the probability of *maxR(response)![](img/B17948_16_22.png)*`0`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*d(T)![](img/B17948_16_22.png)0，因为当* *minI(prompt)![](img/B17948_16_22.png)*`0`时，*maxR(response)![](img/B17948_16_22.png)*`0`的概率'
- en: 'When humans communicate, they bring their culture, past relationships, visual
    situations, and other *invisible factors into a conversation*. These invisible
    factors for third parties can be:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类进行交流时，他们会将文化、过去的关系、视觉情境和其他*不可见因素带入对话*。对于第三方来说，这些不可见因素可能包括：
- en: Reading a text without seeing what the people were doing (actions, facial expressions,
    body language, etc.)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读文字而看不到人们正在做什么（行动、面部表情、身体语言等）。
- en: Listening to people refer to things they know about, but we don’t (movies, sports,
    problems in a factory, etc.)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 听人们提到我们不了解的事物（电影、体育、工厂的问题等）。
- en: Cultural events from a culture that’s different from ours
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自与我们不同文化的文化事件
- en: The list is endless!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列举的例子还有很多！
- en: We can see that these *invisible* factors make AI *blind*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这些*不可见*因素让人工智能*失明*。
- en: Let’s now introduce sensors into the situation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将传感器引入到情境中。
- en: Casual English with sensors
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用传感器的非正式英语
- en: 'We now introduce video sensors into the room for a thought experiment. Imagine
    we can use image captioning with a video feed and supply a context early in the
    dialogue such as:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将视频传感器引入房间进行思想实验。想象一下我们可以使用图像字幕处理视频源，并在对话开始时提供上下文，比如：
- en: Humans sometimes generate dialogue that only people that know each other well
    understand. Consider the following dialogue between Jane and Tom. The video feed
    produces image captioning showing that Tom is drinking a cup of coffee and Jane
    is typing on her keyboard. Jane and Tom are two developers, mumbling their way
    through the day while getting down to work in an open space.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 人类有时会产生只有彼此了解的对话。考虑一下简和汤姆之间的对话。视频镜头显示汤姆正在喝咖啡，简在键盘上打字。简和汤姆是两个开发者，在开放空间里摸索着努力工作的一天。
- en: 'Then we provide the following chaotic chat as a prompt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们以以下混乱的聊天作为提示：
- en: '`Tom: "hi" Jane: "yeah sure" Tom: "Want some?" Jane: "Nope" Tom: "Cool. You''re
    trying then." Jane: "Yup" Tom: "Sleep better?" Jane: "Yeah. Sure. "`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tom: "hi" Jane: "yeah sure" Tom: "Want some?" Jane: "Nope" Tom: "Cool. You''re
    trying then." Jane: "Yup" Tom: "Sleep better?" Jane: "Yeah. Sure."`'
- en: 'The output of GPT-3 is acceptable though important semantic words are missing
    at the start:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的输出是可以接受的，虽然在开头缺少了重要的语义词：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The results may change from one run to another. GPT-3 looks at the top probabilities
    and selects one of the best. GPT-3 made it through this experiment because image
    captioning provided the context.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能会在每一次运行中有所不同。GPT-3查看了最高的概率并选择了最好的其中之一。GPT-3完成了这个实验，因为图像标题提供了上下文。
- en: However, what if Tom is not holding a cup of coffee, depriving GPT-3 of visual
    context?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果汤姆没有拿着一杯咖啡，剥夺了GPT-3的视觉背景，那该怎么办？
- en: Casual English with sensors but no visible context
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有可见的上下文的随意英语
- en: 'The most difficult situation for AI is if Tom refers to an event every day
    but not today. Suppose that Tom comes in with a cup of coffee every morning. He
    comes in now and asks Jane if she `wants some` *before* getting some coffee. Our
    thought experiment is to imagine all the possible cases. In that case, the video
    feed in our thought experiment will reveal nothing, and we are back to chaos again.
    Also, the video feed can’t see if they are developers, accountants, or consultants.
    So, let’s take that part of context out, which leaves us with the following context.
    Let’s go further. The dialogue contains `Tom`: and `Jane:`. So we don’t need to
    mention that context. We are left with:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AI来说最困难的情况是如果汤姆每天都提到一个事件，但今天不是。假设汤姆每天早晨都拿着一杯咖啡进来。他现在进来了，问简是否在拿一杯咖啡之前想要一些。我们的思维实验是想象所有可能的情况。在这种情况下，我们的思维实验中的视频镜头将什么也没有揭示，我们又回到了混乱中。此外，视频镜头无法看到他们是开发者、会计还是顾问。所以，让我们剔除掉这部分上下文，只剩下以下上下文。让我们再进一步。对话包含`Tom:`和`Jane:`。所以我们不需要提及上下文。我们得到的是：
- en: '`Tom: "hi" Jane: "yeah sure" Tom: "Want some?" Jane: "Nope" Tom: "Cool. You''re
    trying then." Jane: "Yup" Tom: "Sleep better?" Jane: "Yeah. Sure."`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tom: "hi" Jane: "yeah sure" Tom: "Want some?" Jane: "Nope" Tom: "Cool. You''re
    trying then." Jane: "Yup" Tom: "Sleep better?" Jane: "Yeah. Sure."`'
- en: The output is quite astonishing. The casual language used by Jane and Tom leads
    GPT-3 to absurd conclusions. Remember, GPT-3 is a stochastic algorithm. The slightest
    change in the inputs can lead to quite different outputs. GPT-3 is trying to guess
    what they are talking about. GPT-3 detects that the conversation is about consuming
    something. Their casual language leads to nonsensical predictions about illegal
    substances that I am not reproducing in this section for ethical reasons.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是相当惊人的。简和汤姆使用的随意语言导致GPT-3得出荒谬的结论。请记住，GPT-3是一种随机算法。输入中最微小的变化可能会导致完全不同的输出。GPT-3试图猜测他们在谈论什么。GPT-3发现这段对话是关于消费某物。他们的随意语言导致了关于非法物质的荒谬预测，出于道德原因，我不在本节中重现这些。
- en: GPT-3 determines the level of language used and associates it with related situations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3确定语言水平并将其与相关情境关联起来。
- en: What will happen if we reproduce this same experiment using formal English?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用正式英语重现相同的实验会发生什么？
- en: Formal English conversation with no context
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有上下文的正式英语对话
- en: 'Let’s now keep all of the context out but provide formal English. Formal English
    contains longer sentences, good grammar, and manners. We can express the same
    conversation that contains no context using formal English:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把所有的上下文都剔除掉，但提供正式的英语。正式英语包含更长的句子、良好的语法和礼貌。我们可以使用正式的英语表达不包含上下文的相同对话：
- en: '`Tom: "Good morning, Jane" Jane: "Good morning, Tom" Tom: "Want some as well?"
    Jane: "No, thank you. I''m fine." Tom: "Excellent. You are on the right track!"
    Jane: "Yes, I am" Tom: "Do you sleep better these days?" Jane: "Yes, I do. Thank
    you. "`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`汤姆：“早上好，简” 简：“早上好，汤姆” 汤姆：“你也要来点吗？” 简：“不，谢谢。我很好。” 汤姆：“太棒了。你走在了正确的道路上！” 简：“是的，我是的”
    汤姆：“这些日子你睡得更好吗？” 简：“是的，谢谢。”`'
- en: 'GPT-3 naturally understands what Tom refers to with “drinking” with this level
    of English and good manners. The output is quite satisfactory:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 自然地理解了汤姆所说的“喝”与这种程度的英语和良好的礼貌。输出结果相当令人满意：
- en: '`Summarize: Tom says "good morning" to Jane. Tom offers her some of what he''s
    drinking. Jane says "no, thank you. I''m fine." Tom says "excellent" and that
    she is on the right track. Jane says, "yes, I am." Tom asks if she sleeps better
    these days.`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`总结：汤姆对简说“早上好”。汤姆给了她他喝的东西。简说“不，谢谢。我很好。” 汤姆说“太棒了”并且她走在了正确的道路上。简说：“是的，我是。” 汤姆问她这些日子她睡得更好吗。`'
- en: We could imagine an endless number of variations on this same conversation by
    introducing other people into the dialogue and other objects and generating an
    endless number of situations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过引入其他人物和其他对象并生成无数的情况，想象出同一对话的无尽变化。
- en: Let’s sum these experiments up.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下这些实验。
- en: Prompt engineering training
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程培训
- en: Our thoughts are often chaotic. Humans use many methods to reconstruct unstructured
    sentences. *Humans often need to ask additional questions to understand what somebody
    is talking about. You need to accept this when interacting with a trained transformer
    such as OpenAI GPT-3*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的思维常常是混乱的。人类使用许多方法来重构无结构的句子。*人类经常需要询问额外的问题才能理解别人在说什么。与 OpenAI GPT-3 这样的经过训练的转换器交互时，您需要接受这一点*。
- en: 'Keep in mind that a *dialogue d(T)* with a transformer and the response, *maxR(response)*,
    depends on the quality of your inputs, *minI(prompt)*, as defined at the beginning
    of this section:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，与转换器的 *对话 d(T)* 和响应 *maxR(response)*，取决于您的输入的质量 *minI(prompt)*，如本节开始时所定义的：
- en: '*d(T)=minI(prompt) ![](img/B17948_16_22.png) maxR(response)*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*d(T)=minI(prompt) ![](img/B17948_16_22.png) maxR(response)*'
- en: When *minI(prompt)![](img/B17948_16_22.png)*`1`, the probability of *maxR(response)![](img/B17948_16_22.png)*`1`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *minI(prompt)![](img/B17948_16_22.png)*`1` 时，*maxR(response)![](img/B17948_16_22.png)*`1`
    的概率。
- en: When *minI(prompt)![](img/B17948_16_22.png)*`0`, the probability of *maxR(response)![](img/B17948_16_22.png)*`0`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *minI(prompt)![](img/B17948_16_22.png)*`0` 时，*maxR(response)![](img/B17948_16_22.png)*`0`
    的概率。
- en: Practice prompt engineering and measure your progress in time. Prompt engineering
    is a new skill that will take you to the next level of AI.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 练习提示工程，并以时间来衡量您的进步。提示工程是一项将您带入下一个 AI 水平的新技能。
- en: The prompt engineering abilities lead to being able to master copilots.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程能力导致能够掌握协助工具。
- en: Copilots
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协助工具
- en: Welcome to the world of AI-driven development copilots powered by OpenAI and
    available in Visual Studio.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到由 OpenAI 提供并在 Visual Studio 中提供的 AI 驱动的开发协助工具的世界。
- en: GitHub Copilot
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GitHub Copilot
- en: Let’s begin with [GitHub Copilot:](https://github.com/github/copilot-docs)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 [GitHub Copilot:](https://github.com/github/copilot-docs) 开始
- en: '[https://github.com/git](https://github.com/github/copilot-docs)hub/copilot-docs'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/git](https://github.com/github/copilot-docs)hub/copilot-docs'
- en: In this section, we will use GitHub Copilot with PyCh[arm (JetBrains):](https://github.com/github/copilot-docs/tree/main/docs/jetbrains)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将使用 GitHub Copilot 与 PyCh[arm (JetBrains):](https://github.com/github/copilot-docs/tree/main/docs/jetbrains)
- en: '[https://github.com/github/copilot-docs/tree/ma](https://github.com/github/copilot-docs/tree/main/docs/jetbrains)in/docs/jetbrains'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/github/copilot-docs/tree/ma](https://github.com/github/copilot-docs/tree/main/docs/jetbrains)in/docs/jetbrains'
- en: Follow the instructions in the documentation to install JetBrains and activate
    OpenAI GitHub Copilot in PyCharm.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 按照文档中的说明在 PyCharm 中安装 JetBrains 并激活 OpenAI GitHub Copilot。
- en: 'Working withGitHub Copilot is a four-step process (see *Figure 16.7*):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GitHub Copilot 合作是一个四步骤的过程（见 *图16.7*）：
- en: OpenAI Codex is trained on public code and text on the internet.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI Codex 是在互联网上公开的代码和文本上进行训练的。
- en: The trained model is plugged into the GitHub Copilot service.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型被插入到 GitHub Copilot 服务中。
- en: The GitHub service manages back-and-forth flows between code we write in an
    editor (in this case PyCharm) and OpenAI Codex. The GitHub Service Manager makes
    suggestions and then sends the interactions back for improvement.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub 服务管理编辑器中编写的代码（在本例中为 PyCharm）和 OpenAI Codex 之间的双向流动。 GitHub 服务管理器提出建议，然后将交互发送回进行改进。
- en: The code editor is our development workspace.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码编辑器是我们的开发工作空间。
- en: '![A picture containing diagram  Description automatically generated](img/B17948_16_07.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片 说明自动生成](img/B17948_16_07.png)'
- en: 'Figure 16.7: GitHub Copilot’s four-step process'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.7：GitHub Copilot 的四步流程
- en: Follow the instructions provided by GitHub Copilot, log in to GitHub when you
    are in PyCharm. For any trou[bleshooting, read https://copilo](https://copilot.github.com/#faqs)t.github.com/#faqs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 GitHub Copilot 提供的说明，在 PyCharm 中登录 GitHub。对于任何问题，请阅读 [https://copilot.github.com/#faqs](https://copilot.github.com/#faqs)。
- en: 'Once you are all set in the PyCharm editor, simply type:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在 PyCharm 编辑器中设置好，只需键入：
- en: '[PRE1]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As soon as the code is typed, you can open the OpenAI GitHub suggestion pane
    and see the suggestions:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码被输入，您可以打开 OpenAI GitHub 建议窗格并查看建议：
- en: '![Text  Description automatically generated](img/B17948_16_08.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![文本描述自动生成](img/B17948_16_08.png)'
- en: 'Figure 16.8: Suggestions for the code you typed'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8：您键入的代码的建议
- en: 'Once you choose the copilotsuggestion you prefer, it will appear in the editor.
    You can confirm suggestions with the *Tab* key. You can wait for another suggestion,
    such as drawing a scatterplot:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了您喜欢的副驾驶建议，它将出现在编辑器中。您可以使用 *Tab* 键确认建议。您可以等待另一个建议，比如绘制散点图：
- en: '[PRE2]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The plot will be displayed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图将被显示：
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_16_09.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 说明自动生成](img/B17948_16_09.png)'
- en: 'Figure 16.9: A GitHub Copilot scatterplot'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9：GitHub Copilot 散点图
- en: You can run the result on your machine with `GitHub_Copilot.py`, which is in
    the `Chapter16` folder in this book’s GitHub repository.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的 GitHub 存储库的 `Chapter16` 文件夹中找到的 `GitHub_Copilot.py` 上运行结果。
- en: The technology is seamless, invisible, and will progressively expand into all
    areas of development. The system is packed with GPT-3 functionality along with
    other pipelines. The technology is available for Python, JavaScript, and more.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术是无缝的、不可见的，并将逐渐扩展到所有开发领域。该系统装载了 GPT-3 功能以及其他流水线。该技术适用于 Python、JavaScript 等。
- en: It will take training with prompt engineering to get used to working with GitHub
    Copilot driven by OpenAI Codex.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要通过提示工程进行培训，以适应由 OpenAI Codex 驱动的 GitHub Copilot 的工作方式。
- en: Let’s go directly to OpenAI Codex, which can be a good place to train using
    copilots.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入 OpenAI Codex，这可能是使用副驾驶进行训练的好地方。
- en: Codex
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Codex
- en: OpenAI Codex does more than suggest source code. Codex can translate natural
    language into source code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Codex 不仅提供源代码建议。Codex 还可以将自然语言转换为源代码。
- en: 'Go to the OpenAI website and click on the link to the Codex interface:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 转到 OpenAI 网站并点击链接到 Codex 界面：
- en: '[https://beta.openai.com/codex-javascript-sandbox](https://beta.openai.com/codex-javascript-sandbox)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://beta.openai.com/codex-javascript-sandbox](https://beta.openai.com/codex-javascript-sandbox)'
- en: Provide instructions and the result will be displayed in the Codex window and
    the source code on the right pane of the interface.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 提供说明，结果将显示在 Codex 窗口和界面右侧的源代码中。
- en: 'The example in this section contains a few sequences using the prompt pane:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例包含使用提示窗格的几个序列：
- en: '![](img/B17948_16_10.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_16_10.png)'
- en: 'Figure 16.10: Codex’s JavaScript sandbox'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.10：Codex 的 JavaScript 沙盒
- en: We enter natural language in the **Provide instructions…** pane, generating
    JavaScript that will run.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 **提供说明...** 窗格中输入自然语言，生成将运行的 JavaScript。
- en: 'For example, I entered two instructions in natural language:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我用自然语言输入了两条说明：
- en: '`Draw 50 small ping pong balls of all sorts of colors`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`绘制 50 个各种颜色的小乒乓球`'
- en: '`Make the balls round`'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`使球变圆`'
- en: 'Codex is derived from GPT-3\. In some cases, the engine is referred to as “davinci-codex.”
    This means Codex inherits the properties of a GPT-3 engine. To understand this,
    let’s take one parameter among others: `top_p`. The engine samples the output
    of the engine. For example, if `top_p` is set to `0.1`, it will only take the
    top 10% of the sampling into account. It will retrieve the tokens in that top
    10% probability mass. Since the whole computation is stochastic, it might take
    some top choices and another set of top choices to run after. Be patient and take
    your time designing the prompt.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Codex 源自 GPT-3。在某些情况下，该引擎被称为“davinci-codex”。这意味着 Codex 继承了 GPT-3 引擎的属性。为了理解这一点，让我们看看其中的一个参数：`top_p`。引擎对引擎的输出进行采样。例如，如果
    `top_p` 设置为 `0.1`，它将仅考虑采样中的前 10%。它将检索该前 10% 概率质量中的标记。由于整个计算是随机的，可能需要一些前选项和另一组前选项来运行。请耐心等待，花时间设计提示。
- en: 'Your AI engine learning path:'
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 您的 AI 引擎学习路径：
- en: Learn to understand the behavior of the Codex engine.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习了解 Codex 引擎的行为。
- en: Accept the free creative nature of stochastic algorithms.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受随机算法的自由创造性本质。
- en: Get used to piloting them with better prompts. You will then be able to grow
    with Codex as the engines improve.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 习惯于用更好的提示来驾驭它们。然后，随着引擎的改进，您将能够随着 Codex 的成长而成长。
- en: 'The script will appear in the JavaScript pane with commented instructions and
    the code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将出现在 JavaScript 窗格中，带有注释的说明和代码：
- en: '[PRE3]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is displayed:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示如下：
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_16_11.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17948_16_11.png)'
- en: 'Figure 16.11: Creating fifty multicolored balls'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.11：创建五十个多彩的球
- en: 'Now let’s ask the program to move the balls:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们要求程序移动球：
- en: '[PRE4]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The code is generated:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已生成：
- en: '[PRE5]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the balls move, export the code to HTML by clicking on the **Export to**
    **JSFiddle** button:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦球移动，通过单击 **导出到 JSFiddle** 按钮将代码导出到 HTML：
- en: '![A picture containing logo  Description automatically generated](img/B17948_16_12.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![包含标志的图片  自动生成的描述](img/B17948_16_12.png)'
- en: 'Figure 16.12: The Export to JSFiddle button'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.12：导出到 JSFiddle 按钮
- en: 'JSFiddle creates an HTML page:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: JSFiddle 创建了一个 HTML 页面：
- en: '![Text  Description automatically generated](img/B17948_16_13.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![文本  自动生成的描述](img/B17948_16_13.png)'
- en: 'Figure 16.13: JSFiddle creating an HTML page'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.13：JSFiddle 创建 HTML 页面
- en: In this case, the code was saved to `codex.html`, which is in this chapter’s
    folder in the GitHub repository of the book. You can open and watch the innovative
    result of creating an HTML page with language natural to code source code.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，代码已保存到 `codex.html`，它位于本章文件夹中的 GitHub 存储库中。您可以打开并观看使用自然编程语言创建 HTML 页面的创新结果。
- en: Domain-specific GPT-3 engines
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 领域特定的 GPT-3 引擎
- en: 'This section explores GPT-3 engines that can perform domain-specific tasks.
    We will run three models in the three subsections of this section:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨可以执行领域特定任务的 GPT-3 引擎。我们将在本节的三个子部分中运行三个模型：
- en: Embedding2ML to use GPT-3 to provide embeddings for ML algorithms
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Embedding2ML 使用 GPT-3 为 ML 算法提供嵌入
- en: Instruct series to ask GPT-3 to provide instructions for any task
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指示系列要求 GPT-3 为任何任务提供说明
- en: Content filter to filter bias or any form of unacceptable input and output
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容过滤器，用于过滤偏见或任何形式的不可接受的输入和输出
- en: Open `Domain_Specific_GPT_3_Functionality.ipynb`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `Domain_Specific_GPT_3_Functionality.ipynb`。
- en: We will begin with embedding2ML (embeddings as an input to ML).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从embedding2ML（将嵌入作为 ML 输入）开始。
- en: Embedding2ML
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Embedding2ML
- en: 'OpenAI has trained several embedding models with different dimensions with
    different capabilities:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 已经训练了几个具有不同维度和不同功能的嵌入模型：
- en: Ada (1,024 dimensions)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艾达（1,024 维度）
- en: Babbage (2,048 dimensions)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴贝奇（2,048 维度）
- en: Curie (4,096 dimensions)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 居里（4,096 维度）
- en: Davinci (12,288 dimensions)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达芬奇（12,288 维度）
- en: 'For more explanations on each engine, you will find more information on OpenAI’s
    website:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个引擎的更多解释，您可以在 OpenAI 的网站上找到更多信息：
- en: '[https://beta.openai.com/docs/guides/embeddings](https://beta.openai.com/docs/guides/embeddings).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://beta.openai.com/docs/guides/embeddings](https://beta.openai.com/docs/guides/embeddings).'
- en: The Davinci model offers embedding with 12,288 dimensions. In this section,
    we will use the power of Davinci to generate the embeddings of a supply chain
    dataset. However, we will not send the embeddings to the embedding sublayer of
    the transformer!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 达芬奇模型提供具有 12,288 维度的嵌入。在本节中，我们将利用达芬奇的力量生成供应链数据集的嵌入。但是，我们不会将嵌入发送到变压器的嵌入子层！
- en: 'We will send the embeddings to a clustering machine learning program from the
    scikit-learn library in six steps:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将嵌入发送到 scikit-learn 库中的聚类机器学习程序中的六个步骤：
- en: '*Step 1: Installing and importing OpenAI, and entering the API key*'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1：安装和导入 OpenAI，并输入 API 密钥*'
- en: '*Step 2: Loading the dataset*'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 2：加载数据集*'
- en: '*Step 3: Combining the columns*'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 3：合并列*'
- en: '*Step 4: Running the GPT-3 embedding*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 4：运行 GPT-3 嵌入*'
- en: '*Step 5: Clustering (k-means) with the embeddings*'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 5：使用嵌入进行聚类（k-means）*'
- en: '*Step 6: Visualizing the clusters (t-SNE)*'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 6：可视化聚类（t-SNE）*'
- en: 'The process is summed up in *Figure 16.14*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 过程总结在 *图 16.14* 中：
- en: '![Shape, polygon  Description automatically generated](img/B17948_16_14.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![形状，多边形  自动生成的描述](img/B17948_16_14.png)'
- en: 'Figure 16.14: Six-step process for sending embeddings to a clustering algorithm'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.14：将嵌入发送到聚类算法的六个步骤流程
- en: Open the Google Colab file, `Domain_Specific_GPT_3_Functionality.ipynb` and
    go to the `Embedding2ML with GPT-3 engine` section of the notebook.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The steps described in this section match the notebook cells. Let’s go through
    a summary of each step of the process.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Installing and importing OpenAI'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start with the following substeps:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Run the cell
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Restart the runtime
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the cell again to make sure since you restarted the runtime:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Enter the API key:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We now load the dataset.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Loading the dataset'
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Load your file before running the cell. I uploaded `tracking.csv` (available
    in the GitHub repository of this book), which contains SCM data:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The data contains seven fields:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '`Id`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Time`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Product`'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`User`'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Score`'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Summary`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Text`'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s print the first few lines using the following command:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can combine columns to build the clusters we wish.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Combining the columns'
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can combine the `Product` column with `Summary` to obtain a view of products
    and their delivery status. Remember that this is only an experimental exercise.
    In a real-life project, carefully analyze and decide on the columns you wish to
    combine.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example code can be replaced with any choice you make for your
    experimentation:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now see a new column named `combined`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We will now run the embedding model on the `combined` column.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Running the GPT-3 embedding'
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will now run the `davinci-similarity` model to obtain 12,288 dimensions
    for the `combined` column:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The result is impressive. We have 12,288 dimensions for the combined column:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We now need to convert the result into a `numpy` matrix:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The matrix has a shape of 1,053 records x 12,288 dimensions, which is quite
    impressive:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The matrix is now ready to be sent to a scikit-learn machine learning clustering
    algorithm.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Clustering (k-means clustering) with the embeddings'
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We usually send classical datasets to a k-means clustering algorithm. We will
    send a 12,288-dimension dataset to the ML algorithm, not to the next sublayer
    of the transformer.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import k-means from scikit-learn:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We now run a classical k-means clustering algorithm with our 12,288-dimension
    dataset:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is four clusters as requested:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can print the labels for the content of the dataset:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s now visualize the clusters using t-SNE.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Visualizing the clusters (t-SNE)'
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: t-SNE keeps local similarities. PCA maximizes large pairwise distances. In this
    case, smaller pairwise distances.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook will use `matplotlib` to display t-SNE:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Before visualizing we need to run the t-SNE algorithm:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now display the results in `matplotlib`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The plot shows the clusters with many data points piled up around them. There
    are also many data points circled around the clusters attached to the closest
    centroid:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B17948_16_15.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.15: Clusters of embeddings-t-SNE'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: We ran a large GPT-3 model to embed 12,288 dimensions. Then we plugged the result
    into a clustering algorithm. The potential of combining transformers and machine
    learning is endless!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行了一个大型的GPT-3模型，嵌入了12,288维度。然后我们将结果送入聚类算法。将Transformer和机器学习相结合的潜力是无限的！
- en: You can go to the `Peeking into the embeddings` section of the notebook if you
    wish to peek into the data frames.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望查看数据框，请转到笔记本的`Peeking into the embeddings`部分。
- en: Let’s now have a look at the instruct series.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下指示系列。
- en: Instruct series
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指示系列
- en: Personal assistants, avatars in metaverses, websites, and many other domains
    will increasingly need to provide clear instructions when a user asks for help.
    Go to the `instruct series` section of `Domain_Specific_GPT_3_Functionality.ipynb`.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 个人助手、元宇宙中的头像、网站等在用户请求帮助时需要提供清晰的指示。转到`Domain_Specific_GPT_3_Functionality.ipynb`的`instruct
    series`部分。
- en: 'In this section, we will ask a transformer to explain how to set up parent
    control in Microsoft Edge with the following prompt: `Explain how to set up parent
    control in Edge`.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将要求一个Transformer解释如何在Microsoft Edge中设置家长控制，使用以下提示：`解释如何在Edge中设置家长控制`。
- en: 'We first run the completion cell:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先运行完成单元：
- en: '[PRE25]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The response is a list of instructions as requested:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是根据请求的指示列表：
- en: '[PRE26]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The number of instructions you can ask for is unlimited! Use your creativity
    and imagination to find more examples!
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以要求的指示数量是无限的！发挥您的创造力和想象力找到更多例子！
- en: Sometimes the input or the output is not acceptable. Let’s see how to implement
    a content filter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 有时输入或输出是不可接受的。让我们看看如何实施内容过滤器。
- en: Content filter
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内容过滤器
- en: Bias, unacceptable language, and any form of unethical input should be excluded
    from your AI applications.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见、不可接受的语言和任何形式的不道德输入都应该从您的AI应用程序中排除。
- en: One of OpenAI’s trained models is a content filter. We will run an example in
    this section. Go to the `content filter` section of `Domain_Specific_GPT_3_Functionality.ipynb`.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI训练的模型之一是内容过滤器。我们将在本节中运行一个示例。转到`Domain_Specific_GPT_3_Functionality.ipynb`的内容过滤器部分。
- en: 'My recommendation is to filter the input and the output, as shown in *Figure
    16.16*:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是过滤输入和输出，如*图16.16*所示：
- en: '![Diagram  Description automatically generated](img/B17948_16_16.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17948_16_16.png)'
- en: 'Figure 16.16: Implementing a content filter'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16：实现内容过滤器
- en: 'My recommendation is to implement a three-step process:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是实施一个三步过程：
- en: Apply a content filter to ALL input data
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有输入数据应用内容过滤器
- en: Let the AI algorithm run as trained
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让AI算法根据训练运行
- en: Apply a content filter to ALL output data
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有输出数据应用内容过滤器
- en: In this section, the input and output data will be named `content`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，输入和输出数据将被命名为`content`。
- en: 'Take an obnoxious input as the following one:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 以以下输入为例：
- en: '[PRE27]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This input is unacceptable! School is not the NBA. Basketball should remain
    a nice exercise for *everyone*.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输入是不可接受的！学校不是NBA。篮球应该保持每个人的一个良好的锻炼。
- en: 'Let’s now run the content filter in the cell - `content-filter-alpha`:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在单元`content-filter-alpha`中运行内容过滤器：
- en: '[PRE28]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The content filter stores the result in `response`, a dictionary object. We
    retrieve the value of choice to obtain the level of acceptability:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 内容过滤器将结果存储在`response`中，一个字典对象。我们检索选择的值以获取可接受性的级别：
- en: '[PRE29]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The content filter sends one of three values back:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 内容过滤器返回三个值之一：
- en: '`0` – Safe'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0` – 安全'
- en: '`1` – Sensitive'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1` – 敏感'
- en: '`2` – Unsafe'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2` – 不安全'
- en: 'In this case, the result is 2, of course:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，结果当然是2：
- en: '[PRE30]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The content filter might not be sufficient. I recommend adding other algorithms
    to control and filter input/output content: rule bases, dictionaries, and other
    methods.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 内容过滤器可能不够。我建议添加其他算法来控制和过滤输入/输出内容：规则库、字典和其他方法。
- en: Now that we have explored domain-specific models, let’s build a transformer-based
    recommender system.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了特定领域的模型，让我们构建一个基于Transformer的推荐系统。
- en: Transformer-based recommender systems
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Transformer的推荐系统
- en: Transformer models learn sequences. Learning language sequences is a great place
    to start considering the billions of messages posted on social media and cloud
    platforms each day. Consumer behaviors, images, and sounds can also be represented
    in sequences.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型学习序列。学习语言序列是一个很好的开始，考虑到每天在社交媒体和云平台上发布的数十亿条消息。消费者行为、图像和声音也可以用序列表示。
- en: In this section, we will first create a general-purpose sequence graph and then
    build a general-purpose transformer-based recommender in Google Colaboratory.
    We will then see how to deploy them in metahumans.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first define general-purpose sequences.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: General-purpose sequences
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many activities can be represented by entities and links between them. They
    are thus organized in sequences. For example, a video on YouTube can be an entity
    A, and the link can be the behavior of a person going from video A to video E.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Another example is a bad fever being an entity F, and the link being the inference
    a doctor may make leading to a micro-decision B. The purchase of product D on
    Amazon by a consumer can generate a link to a suggestion C or another product.
    The examples are infinite!
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the entities in this section with six letters:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '*E={A,B,C,D,E,F}*'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: When we speak a language, we follow grammar rules and cannot escape them.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose *A =”I”*, *E =”eat”*, and *D =”candy”*. There is only
    one proper sequence to express the fact that I consume candy: “I eat candy.”'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: If somebody says “eat candy I,” it will sound slightly off.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'In this sequence, the links representing those rules are:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '*A->E (I eat)*'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '*E->D(eat candy)*'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: We can automatically infer rules in any domain by observing behaviors, learning
    datasets with ML, or manually listening to experts.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will suppose that we have observed a YouTube user for several
    months who spends several hours watching videos. We have noticed that the user
    systematically goes from one type of video to another. For example, from the video
    of singer B to the video of singer D. The behavior rules *X* of this person *P*
    seem to be:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '*X(P)={AE,BD,BF,C,CD,DB,DC,DE,EA,ED,FB}*'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent this system of entities as vertices in a graph and the links
    as edges. For example, if we apply *X(P)* to the vertices, we obtain the following
    undirected graph:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clock  Description automatically generated](img/B17948_16_17.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.17: Graph of YouTube user’s video combinations'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the vertices are videos of a viewer’s favorite singers and that *C*
    is the singer the viewer prefers. We can give a value of 1 to the statistical
    transitions (links or edges) the viewer made in the past weeks. We can also give
    a value of 100 to the viewer’s favorite singer’s videos.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: For this viewer, the path is represented by the (edges, vertices) values *V(R(P)):*
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '*V(X(P))={AE=1,BD=1,BF=1,C=100,CD=1,DB=1,DE=1,EA=1,ED=1,FB=1}*'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a recommender is thus to suggest sequences that lead to videos of
    singer *C* or suggest *C* directly in some cases.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent the undirected graph in reward matrix `R`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let’s use this reward matrix to simulate the activity of a viewer *X* over several
    months.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Dataset pipeline simulation with RL using an MDP
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will simulate the behavior *X* of a person *P* watching
    videos of songs on YouTube, which we define as *X(P)*. We will determine the values
    of the behavior of *P* as *V(X(P))*. We will then organize the values in a reward
    matrix *R* for a **Markov Decision Process** (**MDP**) that we will now implement
    in a Bellman equation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Open `KantaiBERT_Recommender.ipynb`, which is in this chapter’s folder in the
    book’s GitHub repository. The notebook is a modification of `KantaiBERT.ipynb`
    described in *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 4*, we trained a transformer using `kant.txt`, which contained some
    of the works of Immanuel Kant. In this section, we will generate thousands of
    sequences of a person’s behavior through **reinforcement learning** (**RL**).
    RL is not in the scope of this book, but this section contains some reminders.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to train a transformer model to learn and simulate a person’s
    behavior.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Training customer behaviors with an MDP
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`KantaiBERT.ipynb` in *Chapter 4* began by loading `kant.txt` to train a RoBERTa
    with a DistilBERT architecture. `kant.txt` contained works by Immanuel Kant. In
    this section, we will generate sequences using the reward matrix *R* defined in
    the *General-purpose sequences* section of this chapter:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The first cell of the program is thus:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '`Step 1A Training: Dataset Pipeline Simulation with RL using an MDP:`'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'This cell implements an MDP using the Bellman equation:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In this equation:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '`R` is the original reward matrix.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q` is the updated matrix, which is the same size as `R`. However, it is updated
    through reinforcement learning to compute the relative value of the link (edge)
    between each entity (vertex).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma` is a learning rate set to `0.8` to avoid overfitting the training process.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MaxValue` is the maximum value of the next vertex. For example, if the viewer
    `P` of YouTube videos is viewing singer `A`, the program might increase the value
    of `E` so that this suggestion can appear as a recommendation.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little by little, the program will try to find the best values to help a viewer
    find the best videos to watch. Once the reinforcement program has learned the
    best links (edges), it can recommend the best viewing sequences.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'The original reward matrix has trained to become an operational matrix. If
    we add the original entities, the trained values clearly appear:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The original sequences of values *V* of the behaviors *X* of person *P* were:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '*V(X(P))={AE=1,BD=1,BF=1,C=100, CD=1,DB=1,DE=1,EA=1,ED=1,FB=1}*'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'They have been trained to become:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '*V(X(P))={AE=259.44, BD=321.8 ,BF=207.752, C=500, CD=321.8 ,DB=258.44, DE=258.44,
    EA=207.752, ED=321.8, FB=258.44}*'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: This is quite a change!
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Now, it becomes possible to recommend a sequence of exciting videos of *P*’s
    preferred singers. Suppose *P* views a video of singer *E*. Line *E* of the trained
    matrix will recommend a video of the highest value of that line, which is *D=321.8*.
    Thus, a video of singer *D* will appear in the YouTube feed of person *P*.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section is not to stop at this phase. Instead, this section
    uses an MDP to create meaningful sequences to create a dataset for the transformer
    to use for training.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: YouTube does not need to generate sequences to create a dataset. YouTube stores
    all the behaviors of all viewers in big data. Then Google’s powerful algorithms
    take over to recommend the best videos in the video feed of a viewer.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: Other platforms use cosine similarity as implemented in *Chapter 9*, *Matching
    Tokenizers and Datasets*, to make predictions.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The MDP could have been trained for YouTube viewers, Amazon buyers, Google search
    results, a doctor’s diagnosis path, a supply chain, and any type of sequence.
    Transformers are taking sequence learning, training, and predictions to another
    level.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Let’s implement a simulation to create behavior sequences for a transformer
    model.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Simulating consumer behavior with an MDP
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the RL part of the program is trained in cell 1, cell 2,`Step 1B Applying:
    Dataset Pipeline Simulation with MDP` will simulate a YouTube viewer’s behavior
    over several months. It will also include similar viewer profiles adding up to
    a simulation of 10,000 sequences of video watching.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Cell 2 begins by creating the `kant.txt` file that will be used to train the
    KantaiBERT transformer model:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then the entities (vertices) are introduced:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The function chooses a random start vertex named `origin`:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The program uses the trained matrix to select the best sequence for any domain
    from that point of origin. In this case, we suppose they are the favorite singers
    of a person such as the following examples:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Once 10,000 sequences have been calculated, `kant.txt` contains a dataset for
    the transformer.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: With `kant.txt`, the remaining cells of the program are the same as in `KantaiBERT.ipynb`
    described in *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: The transformer is now ready to make recommendations.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Making recommendations
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In *Chapter 4*, `KantaiBERT.ipynb` contained the following masked sequence:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This sequence is specific and related to *Immanuel Kant*’s works. This notebook
    has a general-purpose dataset that can be used in any domain.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'In this notebook, the input is:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output contains duplicates. It will take a cleaning function to filter
    them to obtain two non-duplicate sequences:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The sequences make sense. Sometimes a viewer will watch the same videos, sometimes
    not. The behavior can be chaotic. That’s where machine learning comes in and how
    AI can be used in metahumans.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Metahuman recommenders
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once the sequences have been generated, they will be converted back into natural
    language for user interfaces. Metahuman, in this section, refers to a recommender
    that takes large amounts of features that:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Exceed human capacity to reason with that many parameters
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lead to more accurate predictions than a human can make
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These pragmatic metahumans are not digital humans yet in this context but powerful
    computing tools. We will go through metahumans in the digital sense in the *Humans
    and AI copilots in metaverses* section.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: For example, the BDC sequences could be a song by singer *B*, followed by singer
    *D*, and then *P*’s favorite singer *C*.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the sequence is converted into natural language, several options are possible:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: The sequence can be sent to a bot or a digital human.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an emerging technology appears, jump on the train and take the ride! You
    will get to know this technology and evolve with it. You can google other metahuman
    platforms. In any case, you can remain on the cutting edge by learning how to
    circumvent limits and find ways to use new technology.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can use the metahuman as an educational video while waiting for an API.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metahuman can be inserted into an interface as a voice message. For example,
    when using Google Maps in a car, you listen to the voice. It sounds like a human.
    We even slip and sometimes think it’s a person, but it isn’t. It’s a machine.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can also be an invisible embedded suggestion in Amazon. It remains something
    that makes recommendations that lead us to make micro-decisions. It influences
    us as a salesperson would do. It’s an invisible metahuman.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, the general-purpose sequences were created by an MDP and trained
    by a RoBERTa transformer. This shows that transformers can be applied to any type
    of sequence.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how transformers are applied to computer vision.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about NLP, not computer vision. However, in the previous section,
    we implemented general purpose sequences that can be applied to many domains.
    Computer vision is one of them.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'The title of the article by *Dosovitskiy* et al. (2021) says it all: *An image
    is worth 16x16 words: Transformers for Image Recognition at Scale*. The authors
    processed an image as sequences. The results proved their point.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Google has made vision transformers available in a Colaboratory notebook. Open
    `Vision_Transformer_MLP_Mixer.ipynb` in the `Chapter16` directory of this book’s
    GitHub repository.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Open `Vision_Transformer_MLP_Mixer.ipynb` contains a transformer computer vision
    model in `JAX()`. JAX combines Autograd and XLA. JAX can differentiate Python
    and NumPy functions. JAX speeds up Python and NumPy by using compilation techniques
    and parallelization.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: The notebook is self-explanatory. You can explore it to see how it works. However,
    bear in mind that when Industry 4.0 reaches maturity and Industry 5.0 kicks in,
    the best implementations will be obtained by integrating your data on Cloud AI
    platforms. Local development will diminish, and companies will turn to Cloud AI
    without bearing local development, maintenance, and support.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook’s table of contents contains a transformer process we have gone
    through several times in this book. However, this time, it’s simply applied to
    sequences of digital image information:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_16_18.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.18: Our vision transformer notebook'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook follows standard deep learning methods. It shows some images with
    labels:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_16_19.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.19: Images with labels'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'The images in this chapter are from *Learning Multiple Layers of Features from
    Tiny Images*, *Alex Krizhevsky*, 2009: https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
    They are part of the `CIFAR-10` and `CIFAR-100` datasets (`toronto.edu`): https://www.cs.toronto.edu/~kriz/cifar.html.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook contains the standard transformer process and then displays the
    training images:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![A picture containing screenshot  Description automatically generated](img/B17948_16_20.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.20: Trained data'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Transformer programs can classify random pictures. It seems like a miracle to
    take a transformer model originally designed for NLP and use it for general-purpose
    sequences for recommenders and then for computer vision. However, we are just
    beginning to explore the generalization of sequences training.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: The simplicity of the model is surprising! The vision transformer relies on
    the architecture of transformers. It does not contain the complexity of convolutional
    neural networks. Yet, it produces comparable results.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Now, robots and bots can be equipped with transformer models to understand language
    and interpret images to understand the world around them.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Vision transformers can be implemented in metahumans and metaverses.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Humans and AI copilots in metaverses
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans and metahuman AI are merging into metaverses. Exploring metaverses is
    beyond the scope of this book. The toolbox provided by this book shows the path
    to metaverses populated by humans and metahuman AI.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: Avatars, computer vision, and video game experience will make our communication
    with others *immersive*. We will go from looking at smartphones to being in locations
    with others.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: From looking at to being in
  id: totrans-433
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The evolution from looking at to being in is a natural one. We invented computers,
    added screens, then invented smartphones, and now use apps for video meetings.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Now we can enter virtual reality for all types of meetings and activities.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: We will use Facebook’s metaverse, for example, on our smartphone to *feel* present
    in the same location as the people (personal and professional) we meet. Feeling
    *present* will no doubt be a major evolution in smartphone communication.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '*Feeling present* somewhere is quite different from looking at a small screen
    on a mobile.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'The metaverse will make the impossible possible: a spacewalk, surfing on huge
    waves, walking in a forest, visiting dinosaurs, and wherever our imagination takes
    us.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Yes, there are limits, dangers, threats, and everything that goes with human
    technology. However, we can use AI to control AI, as we saw with content filtering.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: The transformer tools in this book added to the emerging metaverse technology
    will take us literally to another world.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: Make good use of the knowledge and skills you acquired in this book to create
    your ethical future in a metaverse or the physical world!
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter described the rise of AI copilots with human-decision-making-level
    capability. Industry 4.0 has opened the door to machine interconnectivity. Machine-to-machine
    micro-decision making will speed up transactions. AI copilots will boost our productivity
    in a wide range of domains.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to use OpenAI Codex to generate source code while we code and even
    with natural language instructions.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: We built a transformer-based recommender system using a dataset generated by
    the MDP program to train a RoBERTa transformer model. The dataset structure was
    a multi-purpose sequence model. A metahuman can thus acquire multi-domain recommender
    functionality.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: The chapter then showed how a vision transformer could classify images processed
    as sequences of information.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw that the metaverse would make recommendations visible through
    a metahuman interface or invisible in deeply embedded functions in social media,
    for example.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have emerged with innovating copilots and models in an incredibly
    complex new era. The journey will prove both challenging and exciting!
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI copilots that can generate code automatically do not exist. (True/False)
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AI copilots will never replace humans. (True/False)
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-3 engines can only do one task. (True/False)
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can be trained to be recommenders. (True/False)
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers can only process language. (True/False)
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A transformer sequence can only contain words. (True/False)
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vision transformers cannot equal CNNs. (True/False)
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AI robots with computer vision do not exist. (True/False)
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is impossible to produce Python source code automatically. (True/False)
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We might one day become the copilots of robots. (True/False)
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI platf[orm for GPT-3: htt](https://openai.com)ps://openai.com'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI model[s and engines: https://beta.openai.c](https://beta.openai.com/docs/engines)om/docs/engines'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vision Transformers: *Alexey Dosovitskiy*, *Lucas Beyer*, *Alexander Kolesnikov*,
    *Dirk Weissenborn*, *Xiaohua Zhai*, *Thomas Unterthiner*, *Mostafa Dehghani*,
    *Matthias Minderer*, *Georg Heigold*, *Sylvain Gelly*, *Jakob Uszkoreit*, *Neil
    Houlsby*, 2020, *An Image is Worth 16x16 Words: Transformers for Image Recogni*[*tion
    at Scale*: https://arxiv.org](https://arxiv.org/abs/2010.11929)/abs/2010.11929'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'JAX for vision [transformers: https://github](https://github.com/google/jax).com/google/jax'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI, Visual S[tudio Copilot: https://copi](https://copilot.github.com/)lot.github.com/'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Facebook metaverse: https://www.facebook.com/Meta/videos/577658430179350'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Markov Decision Process (MDP), examples and graph: *Denis Rothman*, 2020, *Artificial
    Intelligence by Example*[, 2^(nd) Edition: https://www.amazon.com/Artificial-Intelligence-Example-advanced-learning/dp/1839211539/ref=sr_1_3?crid=238SF8FPU7BB0&keywords=denis+rothman&qid=1644008912&sprefix=denis+rothman%2Ca](https://www.amazon.com/Artificial-Intelligence-Example-advanced-learning/dp/1839211539/ref=sr_1_3?crid=238SF8FPU7BB0&keywords=denis+rothman&qid=1644008912&sprefix=denis+rothman%2Caps%2C143&sr=8-3)ps%2C143&sr=8-3'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: https://www.packt.link/Transformers
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
