["```py\n>>> import torch\n>>> T = torch.tensor([[0.4, 0.6],\n...                   [0.8, 0.2]])\n```", "```py\n>>> T_2 = torch.matrix_power(T, 2)\n>>> T_5 = torch.matrix_power(T, 5)\n>>> T_10 = torch.matrix_power(T, 10)\n>>> T_15 = torch.matrix_power(T, 15)\n>>> T_20 = torch.matrix_power(T, 20)\n```", "```py\n>>> v = torch.tensor([[0.7, 0.3]])\n```", "```py\n>>> v_1 = torch.mm(v, T)\n>>> v_2 = torch.mm(v, T_2)\n>>> v_5 = torch.mm(v, T_5)\n>>> v_10 = torch.mm(v, T_10)\n>>> v_15 = torch.mm(v, T_15)\n>>> v_20 = torch.mm(v, T_20)\n```", "```py\n>>> print(\"Transition probability after 2 steps:\\n{}\".format(T_2))\nTransition probability after 2 steps:\ntensor([[0.6400, 0.3600],\n [0.4800, 0.5200]])\n>>> print(\"Transition probability after 5 steps:\\n{}\".format(T_5))\nTransition probability after 5 steps:\ntensor([[0.5670, 0.4330],\n [0.5773, 0.4227]])\n>>> print(\n\"Transition probability after 10 steps:\\n{}\".format(T_10))\nTransition probability after 10 steps:\ntensor([[0.5715, 0.4285],\n [0.5714, 0.4286]])\n>>> print(\n\"Transition probability after 15 steps:\\n{}\".format(T_15))\nTransition probability after 15 steps:\ntensor([[0.5714, 0.4286],\n [0.5714, 0.4286]])\n>>> print(\n\"Transition probability after 20 steps:\\n{}\".format(T_20))\nTransition probability after 20 steps:\ntensor([[0.5714, 0.4286],\n [0.5714, 0.4286]])\n```", "```py\n>>> print(\"Distribution of states after 1 step:\\n{}\".format(v_1))\nDistribution of states after 1 step:\ntensor([[0.5200, 0.4800]])\n>>> print(\"Distribution of states after 2 steps:\\n{}\".format(v_2))\nDistribution of states after 2 steps:\ntensor([[0.5920, 0.4080]])\n>>> print(\"Distribution of states after 5 steps:\\n{}\".format(v_5))\nDistribution of states after 5 steps:\ntensor([[0.5701, 0.4299]])\n>>> print(\n \"Distribution of states after 10 steps:\\n{}\".format(v_10))\nDistribution of states after 10 steps:\ntensor([[0.5714, 0.4286]])\n>>> print(\n \"Distribution of states after 15 steps:\\n{}\".format(v_15))\nDistribution of states after 15 steps:\ntensor([[0.5714, 0.4286]])\n>>> print(\n \"Distribution of states after 20 steps:\\n{}\".format(v_20))\nDistribution of states after 20 steps:\ntensor([[0.5714, 0.4286]])\n```", "```py\n >>> import torch\n >>> T = torch.tensor([[[0.8, 0.1, 0.1],\n ...                    [0.1, 0.6, 0.3]],\n ...                   [[0.7, 0.2, 0.1],\n ...                    [0.1, 0.8, 0.1]],\n ...                   [[0.6, 0.2, 0.2],\n ...                    [0.1, 0.4, 0.5]]]\n ...                  )\n```", "```py\n >>> R = torch.tensor([1., 0, -1.])\n >>> gamma = 0.5\n```", "```py\n>>> action = 0\n```", "```py\n >>> def cal_value_matrix_inversion(gamma, trans_matrix, rewards):\n ...     inv = torch.inverse(torch.eye(rewards.shape[0]) \n - gamma * trans_matrix)\n ...     V = torch.mm(inv, rewards.reshape(-1, 1))\n ...     return V\n```", "```py\n >>> trans_matrix = T[:, action]\n >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)\n >>> print(\"The value function under the optimal \n policy is:\\n{}\".format(V))\n The value function under the optimal policy is:\n tensor([[ 1.6787],\n [ 0.6260],\n [-0.4820]])\n```", "```py\n >>> gamma = 0\n >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)\n >>> print(\"The value function under the optimal policy is:\\n{}\".format(V))\n The value function under the optimal policy is:\n tensor([[ 1.],\n [ 0.],\n [-1.]])\n```", "```py\n >>> gamma = 0.99\n >>> V = cal_value_matrix_inversion(gamma, trans_matrix, R)\n >>> print(\"The value function under the optimal policy is:\\n{}\".format(V))\n The value function under the optimal policy is:\n tensor([[65.8293],\n [64.7194],\n [63.4876]])\n```", "```py\n >>> import torch\n >>> T = torch.tensor([[[0.8, 0.1, 0.1],\n ...                    [0.1, 0.6, 0.3]],\n ...                   [[0.7, 0.2, 0.1],\n ...                    [0.1, 0.8, 0.1]],\n ...                   [[0.6, 0.2, 0.2],\n ...                    [0.1, 0.4, 0.5]]]\n ...                  )\n```", "```py\n >>> R = torch.tensor([1., 0, -1.])\n >>> gamma = 0.5\n```", "```py\n >>> threshold = 0.0001\n```", "```py\n >>> policy_optimal = torch.tensor([[1.0, 0.0],\n ...                                [1.0, 0.0],\n ...                                [1.0, 0.0]])\n```", "```py\n>>> def policy_evaluation(\n policy, trans_matrix, rewards, gamma, threshold):\n...     \"\"\"\n...     Perform policy evaluation\n...     @param policy: policy matrix containing actions and their \n probability in each state\n...     @param trans_matrix: transformation matrix\n...     @param rewards: rewards for each state\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values \n for all states are less than the threshold\n...     @return: values of the given policy for all possible states\n...     \"\"\"\n...     n_state = policy.shape[0]\n...     V = torch.zeros(n_state)\n...     while True:\n...         V_temp = torch.zeros(n_state)\n...         for state, actions in enumerate(policy):\n...             for action, action_prob in enumerate(actions):\n...                 V_temp[state] += action_prob * (R[state] + \n gamma * torch.dot(\n trans_matrix[state, action], V))\n...         max_delta = torch.max(torch.abs(V - V_temp))\n...         V = V_temp.clone()\n...         if max_delta <= threshold:\n...             break\n...     return V\n```", "```py\n>>> V = policy_evaluation(policy_optimal, T, R, gamma, threshold)\n>>> print(\n \"The value function under the optimal policy is:\\n{}\".format(V)) The value function under the optimal policy is:\ntensor([ 1.6786,  0.6260, -0.4821])\n```", "```py\n>>> policy_random = torch.tensor([[0.5, 0.5],\n...                               [0.5, 0.5],\n...                               [0.5, 0.5]])\n```", "```py\n>>> V = policy_evaluation(policy_random, T, R, gamma, threshold)\n>>> print(\n \"The value function under the random policy is:\\n{}\".format(V))\nThe value function under the random policy is:\ntensor([ 1.2348,  0.2691, -0.9013])\n```", "```py\n>>> def policy_evaluation_history(\n policy, trans_matrix, rewards, gamma, threshold):\n...     n_state = policy.shape[0]\n...     V = torch.zeros(n_state)\n...     V_his = [V]\n...     i = 0\n...     while True:\n...         V_temp = torch.zeros(n_state)\n...         i += 1\n...         for state, actions in enumerate(policy):\n...             for action, action_prob in enumerate(actions):\n...                 V_temp[state] += action_prob * (R[state] + gamma * \n torch.dot(trans_matrix[state, action], V))\n...         max_delta = torch.max(torch.abs(V - V_temp))\n...         V = V_temp.clone()\n...         V_his.append(V)\n...         if max_delta <= threshold:\n...             break\n...     return V, V_his\n```", "```py\n>>> V, V_history = policy_evaluation_history(\n policy_optimal, T, R, gamma, threshold)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> s0, = plt.plot([v[0] for v in V_history])\n>>> s1, = plt.plot([v[1] for v in V_history])\n>>> s2, = plt.plot([v[2] for v in V_history])\n>>> plt.title('Optimal policy with gamma = {}'.format(str(gamma)))\n>>> plt.xlabel('Iteration')\n>>> plt.ylabel('Policy values')\n>>> plt.legend([s0, s1, s2],\n...            [\"State s0\",\n...             \"State s1\",\n...             \"State s2\"], loc=\"upper left\")\n>>> plt.show()\n```", "```py\n>>> import gym\n>>> import torch\n>>> env = gym.make(\"FrozenLake-v0\")\n>>> n_state = env.observation_space.n\n>>> print(n_state)\n16\n>>> n_action = env.action_space.n\n>>> print(n_action)\n4\n```", "```py\n>>> env.reset()\n0\n```", "```py\n>>> env.render()\n```", "```py\n>>> new_state, reward, is_done, info = env.step(1)\n>>> env.render()\n```", "```py\n>>> print(new_state)\n4\n>>> print(reward)\n0.0\n>>> print(is_done)\nFalse\n>>> print(info)\n{'prob': 0.3333333333333333}\n```", "```py\n>>> def run_episode(env, policy):\n...     state = env.reset()\n...     total_reward = 0\n...     is_done = False\n...     while not is_done:\n...         action = policy[state].item()\n...         state, reward, is_done, info = env.step(action)\n...         total_reward += reward\n...         if is_done:\n...             break\n...     return total_reward\n```", "```py\n>>> n_episode = 1000\n>>> total_rewards = []\n>>> for episode in range(n_episode):\n...     random_policy = torch.randint(\n high=n_action, size=(n_state,))\n...     total_reward = run_episode(env, random_policy)\n...     total_rewards.append(total_reward)\n...\n>>> print('Average total reward under random policy: {}'.format(\n sum(total_rewards) / n_episode))\nAverage total reward under random policy: 0.014\n```", "```py\n>>> while True:\n...     random_policy = torch.randint(\n high=n_action, size=(n_state,))\n...     total_reward = run_episode(env, random_policy)\n...     if total_reward == 1:\n...         best_policy = random_policy\n...         break\n```", "```py\n>>> print(best_policy)\ntensor([0, 3, 2, 2, 0, 2, 1, 1, 3, 1, 3, 0, 0, 1, 1, 1])\n```", "```py\n>>> total_rewards = []\n>>> for episode in range(n_episode):\n...     total_reward = run_episode(env, best_policy)\n...     total_rewards.append(total_reward)\n...\n>>> print('Average total reward under random search \n     policy: {}'.format(sum(total_rewards) / n_episode))\nAverage total reward under random search policy: 0.208\n```", "```py\n>>> print(env.env.P[6])\n{0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}\n```", "```py\n>>> print(env.env.P[11])\n{0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('FrozenLake-v0')\n```", "```py\n>>> gamma = 0.99\n>>> threshold = 0.0001\n```", "```py\n>>> def value_iteration(env, gamma, threshold):\n...     \"\"\"\n...     Solve a given environment with value iteration algorithm\n...     @param env: OpenAI Gym environment\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values for \n all states are less than the threshold\n...     @return: values of the optimal policy for the given \n environment\n...     \"\"\"\n...     n_state = env.observation_space.n\n...     n_action = env.action_space.n\n...     V = torch.zeros(n_state)\n...     while True:\n...         V_temp = torch.empty(n_state)\n...         for state in range(n_state):\n...             v_actions = torch.zeros(n_action)\n...             for action in range(n_action):\n...                 for trans_prob, new_state, reward, _ in \n env.env.P[state][action]:\n...                     v_actions[action] += trans_prob * (reward \n + gamma * V[new_state])\n...             V_temp[state] = torch.max(v_actions)\n...         max_delta = torch.max(torch.abs(V - V_temp))\n...         V = V_temp.clone()\n...         if max_delta <= threshold:\n...             break\n...     return V\n```", "```py\n>>> V_optimal = value_iteration(env, gamma, threshold)\n>>> print('Optimal values:\\n{}'.format(V_optimal))\nOptimal values:\ntensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,\n 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])\n```", "```py\n>>> def extract_optimal_policy(env, V_optimal, gamma):\n...     \"\"\"\n...     Obtain the optimal policy based on the optimal values\n...     @param env: OpenAI Gym environment\n...     @param V_optimal: optimal values\n...     @param gamma: discount factor\n...     @return: optimal policy\n...     \"\"\"\n...     n_state = env.observation_space.n\n...     n_action = env.action_space.n\n...     optimal_policy = torch.zeros(n_state)\n...     for state in range(n_state):\n...         v_actions = torch.zeros(n_action)\n...         for action in range(n_action):\n...             for trans_prob, new_state, reward, _ in \n                                   env.env.P[state][action]:\n...                 v_actions[action] += trans_prob * (reward \n + gamma * V_optimal[new_state])\n...         optimal_policy[state] = torch.argmax(v_actions)\n...     return optimal_policy\n```", "```py\n>>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n>>> print('Optimal policy:\\n{}'.format(optimal_policy))\nOptimal policy:\ntensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])\n```", "```py\n>>> n_episode = 1000\n>>> total_rewards = []\n>>> for episode in range(n_episode):\n...     total_reward = run_episode(env, optimal_policy)\n...     total_rewards.append(total_reward)\n>>> print('Average total reward under the optimal \n policy: {}'.format(sum(total_rewards) / n_episode))\nAverage total reward under the optimal policy: 0.75\n```", "```py\n>>> gammas = [0, 0.2, 0.4, 0.6, 0.8, .99, 1.]\n```", "```py\n>>> avg_reward_gamma = []\n>>> for gamma in gammas:\n...     V_optimal = value_iteration(env, gamma, threshold)\n...     optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n...     total_rewards = []\n...     for episode in range(n_episode):\n...         total_reward = run_episode(env, optimal_policy)\n...         total_rewards.append(total_reward)\n...     avg_reward_gamma.append(sum(total_rewards) / n_episode)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(gammas, avg_reward_gamma)\n>>> plt.title('Success rate vs discount factor')\n>>> plt.xlabel('Discount factor')\n>>> plt.ylabel('Average success rate')\n>>> plt.show()\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('FrozenLake-v0')\n```", "```py\n>>> gamma = 0.99\n>>> threshold = 0.0001\n```", "```py\n>>> def policy_evaluation(env, policy, gamma, threshold):\n...     \"\"\"\n...     Perform policy evaluation\n...     @param env: OpenAI Gym environment\n...     @param policy: policy matrix containing actions and \n their probability in each state\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values \n for all states are less than the threshold\n...     @return: values of the given policy\n...     \"\"\"\n...     n_state = policy.shape[0]\n...     V = torch.zeros(n_state)\n...     while True:\n...         V_temp = torch.zeros(n_state)\n...         for state in range(n_state):\n...             action = policy[state].item()\n...             for trans_prob, new_state, reward, _ in \n env.env.P[state][action]:\n...                 V_temp[state] += trans_prob * (reward \n + gamma * V[new_state])\n...         max_delta = torch.max(torch.abs(V - V_temp))\n...         V = V_temp.clone()\n...         if max_delta <= threshold:\n...             break\n...     return V\n```", "```py\n>>> def policy_improvement(env, V, gamma):\n...     \"\"\"\n...     Obtain an improved policy based on the values\n...     @param env: OpenAI Gym environment\n...     @param V: policy values\n...     @param gamma: discount factor\n...     @return: the policy\n...     \"\"\"\n...     n_state = env.observation_space.n\n...     n_action = env.action_space.n\n...     policy = torch.zeros(n_state)\n...     for state in range(n_state):\n...         v_actions = torch.zeros(n_action)\n...         for action in range(n_action):\n...             for trans_prob, new_state, reward, _ in \n env.env.P[state][action]:\n...                 v_actions[action] += trans_prob * (reward \n + gamma * V[new_state])\n...         policy[state] = torch.argmax(v_actions)\n...     return policy\n```", "```py\n>>> def policy_iteration(env, gamma, threshold):\n...     \"\"\"\n...     Solve a given environment with policy iteration algorithm\n...     @param env: OpenAI Gym environment\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values \n for all states are less than the threshold\n...     @return: optimal values and the optimal policy for the given \n environment\n...     \"\"\"\n...     n_state = env.observation_space.n\n...     n_action = env.action_space.n\n...     policy = torch.randint(high=n_action, size=(n_state,)).float()\n...     while True:\n...         V = policy_evaluation(env, policy, gamma, threshold)\n...         policy_improved = policy_improvement(env, V, gamma)\n...         if torch.equal(policy_improved, policy):\n...             return V, policy_improved\n...         policy = policy_improved\n```", "```py\n>>> V_optimal, optimal_policy = \n policy_iteration(env, gamma, threshold)\n```", "```py\n>>> print('Optimal values:\\n{}'.format(V_optimal))\nOptimal values:\ntensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,\n 0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])\n>>> print('Optimal policy:\\n{}'.format(optimal_policy))\nOptimal policy:\ntensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])\n```", "```py\n>>> import torch\n```", "```py\n>>> gamma = 1\n>>> threshold = 1e-10\n```", "```py\n>>> capital_max = 100\n>>> n_state = capital_max + 1\n```", "```py\n>>> rewards = torch.zeros(n_state)\n>>> rewards[-1] = 1\n>>> print(rewards)\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n```", "```py\n>>> head_prob = 0.4\n```", "```py\n>>> env = {'capital_max': capital_max,\n...        'head_prob': head_prob,\n...        'rewards': rewards,\n...        'n_state': n_state}\n```", "```py\n>>> def value_iteration(env, gamma, threshold):\n...     \"\"\"\n...     Solve the coin flipping gamble problem with \n value iteration algorithm\n...     @param env: the coin flipping gamble environment\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values \n for all states are less than the threshold\n...     @return: values of the optimal policy for the given \n environment\n...     \"\"\"\n...     head_prob = env['head_prob']\n...     n_state = env['n_state']\n...     capital_max = env['capital_max']\n...     V = torch.zeros(n_state)\n...     while True:\n...         V_temp = torch.zeros(n_state)\n...         for state in range(1, capital_max):\n...             v_actions = torch.zeros(\n min(state, capital_max - state) + 1)\n...             for action in range(\n 1, min(state, capital_max - state) + 1):\n...                 v_actions[action] += head_prob * (\n rewards[state + action] +\n gamma * V[state + action])\n...                 v_actions[action] += (1 - head_prob) * (\n rewards[state - action] +\n gamma * V[state - action])\n...             V_temp[state] = torch.max(v_actions)\n...         max_delta = torch.max(torch.abs(V - V_temp))\n...         V = V_temp.clone()\n...         if max_delta <= threshold:\n...             break\n...     return V\n```", "```py\n>>> def extract_optimal_policy(env, V_optimal, gamma):\n...     \"\"\"\n...     Obtain the optimal policy based on the optimal values\n...     @param env: the coin flipping gamble environment\n...     @param V_optimal: optimal values\n...     @param gamma: discount factor\n...     @return: optimal policy\n...     \"\"\"\n...     head_prob = env['head_prob']\n...     n_state = env['n_state']\n...     capital_max = env['capital_max']\n...     optimal_policy = torch.zeros(capital_max).int()\n...     for state in range(1, capital_max):\n...         v_actions = torch.zeros(n_state)\n...         for action in range(1, \n min(state, capital_max - state) + 1):\n...             v_actions[action] += head_prob * (\n rewards[state + action] +\n gamma * V_optimal[state + action])\n...             v_actions[action] += (1 - head_prob) * (\n rewards[state - action] +\n gamma * V_optimal[state - action])\n...         optimal_policy[state] = torch.argmax(v_actions)\n...     return optimal_policy\n```", "```py\n>>> import time\n>>> start_time = time.time()\n>>> V_optimal = value_iteration(env, gamma, threshold)\n>>> optimal_policy = extract_optimal_policy(env, V_optimal, gamma)\n>>> print(\"It takes {:.3f}s to solve with value \n iteration\".format(time.time() - start_time))\nIt takes 4.717s to solve with value iteration\n```", "```py\n>>> print('Optimal values:\\n{}'.format(V_optimal))\n>>> print('Optimal policy:\\n{}'.format(optimal_policy))\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(V_optimal[:100].numpy())\n>>> plt.title('Optimal policy values')\n>>> plt.xlabel('Capital')\n>>> plt.ylabel('Policy value')\n>>> plt.show()\n```", "```py\n>>> def policy_evaluation(env, policy, gamma, threshold):\n...     \"\"\"\n...     Perform policy evaluation\n...     @param env: the coin flipping gamble environment\n...     @param policy: policy tensor containing actions taken \n for individual state\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values \n for all states are less than the threshold\n...     @return: values of the given policy\n...     \"\"\"\n...     head_prob = env['head_prob']\n...     n_state = env['n_state']\n...     capital_max = env['capital_max']\n...     V = torch.zeros(n_state)\n...     while True:\n...         V_temp = torch.zeros(n_state)\n...         for state in range(1, capital_max):\n...             action = policy[state].item()\n...             V_temp[state] += head_prob * (\n rewards[state + action] +\n gamma * V[state + action])\n...             V_temp[state] += (1 - head_prob) * (\n rewards[state - action] +\n gamma * V[state - action])\n...         max_delta = torch.max(torch.abs(V - V_temp))\n...         V = V_temp.clone()\n...         if max_delta <= threshold:\n...             break\n...     return V\n```", "```py\n>>> def policy_improvement(env, V, gamma):\n...     \"\"\"\n...     Obtain an improved policy based on the values\n...     @param env: the coin flipping gamble environment\n...     @param V: policy values\n...     @param gamma: discount factor\n...     @return: the policy\n...     \"\"\"\n...     head_prob = env['head_prob']\n...     n_state = env['n_state']\n...     capital_max = env['capital_max']\n...     policy = torch.zeros(n_state).int()\n...     for state in range(1, capital_max):\n...         v_actions = torch.zeros(\n min(state, capital_max - state) + 1)\n...         for action in range(\n 1, min(state, capital_max - state) + 1):\n...             v_actions[action] += head_prob * (\n rewards[state + action] + \n gamma * V[state + action])\n...             v_actions[action] += (1 - head_prob) * (\n rewards[state - action] +\n gamma * V[state - action])\n...         policy[state] = torch.argmax(v_actions)\n...     return policy\n```", "```py\n>>> def policy_iteration(env, gamma, threshold):\n...     \"\"\"\n...     Solve the coin flipping gamble problem with policy \n iteration algorithm\n...     @param env: the coin flipping gamble environment\n...     @param gamma: discount factor\n...     @param threshold: the evaluation will stop once values\n for all states are less than the threshold\n...     @return: optimal values and the optimal policy for the \n given environment\n...     \"\"\"\n...     n_state = env['n_state']\n...     policy = torch.zeros(n_state).int()\n...     while True:\n...         V = policy_evaluation(env, policy, gamma, threshold)\n...         policy_improved = policy_improvement(env, V, gamma)\n...         if torch.equal(policy_improved, policy):\n...             return V, policy_improved\n...         policy = policy_improved\n```", "```py\n>>> start_time = time.time()\n>>> V_optimal, optimal_policy \n = policy_iteration(env, gamma, threshold)\n>>> print(\"It takes {:.3f}s to solve with policy \n iteration\".format(time.time() - start_time))\nIt takes 2.002s to solve with policy iteration\n```", "```py\n>>> print('Optimal values:\\n{}'.format(V_optimal))\n>>> print('Optimal policy:\\n{}'.format(optimal_policy))\n```", "```py\nOptimal values:\ntensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,\n 0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,\n 0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,\n 0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,\n 0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,\n 0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,\n 0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,\n 0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,\n 0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,\n 0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,\n 0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,\n 0.9643, 0.0000])\n```", "```py\nOptimal policy:\ntensor([ 0,  1, 2, 3, 4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 22, 29, 30, 31, 32, 33,  9, 35,\n 36, 37, 38, 11, 40,  9, 42, 43, 44, 5, 4,  3, 2, 1, 50, 1, 2, 47,\n 4, 5, 44,  7, 8, 9, 10, 11, 38, 12, 36, 35, 34, 17, 32, 19, 30,  4,\n 3, 2, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,\n 10, 9, 8,  7, 6, 5, 4,  3, 2, 1], dtype=torch.int32)\n```", "```py\nOptimal values:\ntensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,\n 0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,\n 0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,\n 0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,\n 0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,\n 0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,\n 0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,\n 0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,\n 0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,\n 0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,\n 0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,\n 0.9643, 0.0000])\n```", "```py\nOptimal policy:\ntensor([ 0,  1, 2, 3, 4,  5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 22, 29, 30, 31, 32, 33,  9, 35,\n 36, 37, 38, 11, 40,  9, 42, 43, 44, 5, 4,  3, 2, 1, 50, 1, 2, 47,\n 4, 5, 44,  7, 8, 9, 10, 11, 38, 12, 36, 35, 34, 17, 32, 19, 30,  4,\n 3, 2, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11,\n 10, 9, 8,  7, 6, 5, 4,  3, 2, 1, 0], dtype=torch.int32)\n```", "```py\n>>> def optimal_strategy(capital):\n...     return optimal_policy[capital].item()\n```", "```py\n>>> def conservative_strategy(capital):\n...     return 1\n```", "```py\n>>> def random_strategy(capital):\n...     return torch.randint(1, capital + 1, (1,)).item()\n```", "```py\n>>> def run_episode(head_prob, capital, policy):\n...     while capital > 0:\n...         bet = policy(capital)\n...         if torch.rand(1).item() < head_prob:\n...             capital += bet\n...             if capital >= 100:\n...                 return 1\n...         else:\n...             capital -= bet\n...     return 0\n```", "```py\n>>> capital = 50\n>>> n_episode = 10000\n```", "```py\n>>> n_win_random = 0\n>>> n_win_conservative = 0\n>>> n_win_optimal = 0\n>>> for episode in range(n_episode):\n...     n_win_random += run_episode(\n head_prob, capital, random_strategy)\n...     n_win_conservative += run_episode(\n head_prob, capital, conservative_strategy)\n...     n_win_optimal += run_episode(\n head_prob, capital, optimal_strategy)\n```", "```py\n>>> print('Average winning probability under the random \n policy: {}'.format(n_win_random/n_episode))\nAverage winning probability under the random policy: 0.2251\n>>> print('Average winning probability under the conservative \n policy: {}'.format(n_win_conservative/n_episode))\nAverage winning probability under the conservative policy: 0.0\n>>> print('Average winning probability under the optimal \n policy: {}'.format(n_win_optimal/n_episode))\nAverage winning probability under the optimal policy: 0.3947\n```"]