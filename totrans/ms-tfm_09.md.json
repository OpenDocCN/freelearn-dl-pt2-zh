["```py\n    from datasets import load_metric, load_dataset\n    metric = load_metric('glue', 'mrpc')\n    mrpc = load_dataset('glue', 'mrpc')\n    ```", "```py\n    labels = [i['label'] for i in dataset['test']]\n    metric.compute(predictions=predictions, references=labels)\n    ```", "```py\n    metric = load_metric('glue', 'stsb')\n    metric.compute(predictions=[1,2,3],references=[5,2,2])\n    ```", "```py\n    pip install tensorflow-hub\n    pip install sentence-transformers\n    ```", "```py\n    from datasets import load_metric, load_dataset\n    stsb_metric = load_metric('glue', 'stsb')\n    stsb = load_dataset('glue', 'stsb')\n    ```", "```py\n    import tensorflow_hub as hub\n    use_model = hub.load(\n       \"https://tfhub.dev/google/universal-sentence-encoder/4\")\n    from sentence_transformers import SentenceTransformer\n    distilroberta = SentenceTransformer(\n                          'stsb-distilroberta-base-v2')\n    ```", "```py\n    import tensorflow as tf\n    import math\n    def use_sts_benchmark(batch):\n      sts_encode1 = \\\n      tf.nn.l2_normalize(use_model(tf.constant(batch['sentence1'])), axis=1)\n      sts_encode2 = \\\n      tf.nn.l2_normalize(use_model(tf.constant(batch['sentence2'])),   axis=1)\n      cosine_similarities = \\\n                  tf.reduce_sum(tf.multiply(sts_encode1,sts_encode2),axis=1)\n      clip_cosine_similarities = \\\n              tf.clip_by_value(cosine_similarities,-1.0, 1.0)\n      scores = 1.0 - \\\n               tf.acos(clip_cosine_similarities) / math.pi\n    return scores\n    ```", "```py\n    def roberta_sts_benchmark(batch):\n      sts_encode1 = \\\n      tf.nn.l2_normalize(distilroberta.encode(batch['sentence1']), axis=1)\n      sts_encode2 = \\\n        tf.nn.l2_normalize(distilroberta.encode(batch['sentence2']), axis=1)\n      cosine_similarities = \\\n              tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2),  axis=1)\n      clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n      scores = 1.0  - tf.acos(clip_cosine_similarities) / math.pi\n    return scores\n    ```", "```py\n    use_results = use_sts_benchmark(stsb['validation'])\n    distilroberta_results = roberta_sts_benchmark(\n                                          stsb['validation'])\n    ```", "```py\n    results = {\n          \"USE\":stsb_metric.compute(\n                    predictions=use_results,\n                    references=references),\n          \"DistillRoberta\":stsb_metric.compute(\n                    predictions=distilroberta_results,\n                    references=references)\n    }\n    ```", "```py\n    import pandas as pd\n    pd.DataFrame(results)\n    ```", "```py\nfrom transformers import pipeline\nimport pandas as pd\nclassifier = pipeline(\"zero-shot-classification\",\n                      model=\"facebook/bart-large-mnli\")\nsequence_to_classify = \"one day I will see the world\"\ncandidate_labels = ['travel',\n                    'cooking',\n                    'dancing',\n                    'exploration']\nresult = classifier(sequence_to_classify, candidate_labels)\npd.DataFrame(result)\n```", "```py\nresult = classifier(sequence_to_classify, \n                      candidate_labels, \n                      multi_label=True)\nPd.DataFrame(result)\n```", "```py\nfrom transformers \\\n      import AutoModelForSequenceClassification,\\\n      AutoTokenizer\nnli_model = AutoModelForSequenceClassification\\\n                .from_pretrained(\n                    \"facebook/bart-large-mnli\")\ntokenizer = AutoTokenizer\\\n                .from_pretrained(\n           \"facebook/bart-large-mnli\")\npremise = \"one day I will see the world\"\nlabel = \"travel\"\nhypothesis = f'This example is {label}.'\nx = tokenizer.encode(\n    premise,\n    hypothesis,\n    return_tensors='pt',\n    truncation_strategy='only_first')\nlogits = nli_model(x)[0]\nentail_contradiction_logits = logits[:,[0,2]]\nprobs = entail_contradiction_logits.softmax(dim=1)\nprob_label_is_true = probs[:,1]\nprint(prob_label_is_true)\n```", "```py\ntensor([0.9945], grad_fn=<SelectBackward>)\n```", "```py\n!pip install sentence-transformers\n!pip install dataset\n!pip install flair\n```", "```py\nimport pandas as pd\nsimilar=[\n (\"A black dog walking beside a pool.\",\n \"A black dog is walking along the side of a pool.\"),\n (\"A blonde woman looks for medical supplies for work in a  suitcase. \",\n \" The blond woman is searching for medical supplies in a  suitcase.\"),\n (\"A doubly decker red bus driving down the road.\",\n \"A red double decker bus driving down a street.\"),\n (\"There is a black dog jumping into a swimming pool.\",\n \"A black dog is leaping into a swimming pool.\"),\n (\"The man used a sword to slice a plastic bottle.\",\n \"A man sliced a plastic bottle with a sword.\")]\npd.DataFrame(similar, columns=[\"sen1\", \"sen2\"])\n```", "```py\nimport pandas as pd\ndissimilar= [\n(\"A little girl and boy are reading books. \",\n \"An older child is playing with a doll while gazing out the window.\"),\n (\"Two horses standing in a field with trees in the background.\",\n \"A black and white bird on a body of water with grass in the background.\"),\n (\"Two people are walking by the ocean.\",\n \"Two men in fleeces and hats looking at the camera.\"),\n (\"A cat is pouncing on a trampoline.\",\n \"A man is slicing a tomato.\"),\n(\"A woman is riding on a horse.\",\n \"A man is turning over tables in anger.\")]\npd.DataFrame(dissimilar, columns=[\"sen1\", \"sen2\"])\n```", "```py\nimport torch, numpy as np\ndef sim(s1,s2):\n    s1=s1.embedding.unsqueeze(0)\n    s2=s2.embedding.unsqueeze(0)\n    sim=torch.cosine_similarity(s1,s2).item() \n    return np.round(sim,2)\n```", "```py\nfrom flair.data import Sentence\ndef evaluate(embeddings, myPairList):\n    scores=[]\n    for s1, s2 in myPairList:\n        s1,s2=Sentence(s1), Sentence(s2)\n        embeddings.embed(s1)\n        embeddings.embed(s2)\n        score=sim(s1,s2)\n        scores.append(score)\n    return scores, np.round(np.mean(scores),2)\n```", "```py\nfrom flair.data import Sentence\nfrom flair.embeddings\\\n     import WordEmbeddings, DocumentPoolEmbeddings\nglove_embedding = WordEmbeddings('glove')\nglove_pool_embeddings = DocumentPoolEmbeddings(\n                                      [glove_embedding]\n                                      )\n```", "```py\n>>> evaluate(glove_pool_embeddings, similar)\n([0.97, 0.99, 0.97, 0.99, 0.98], 0.98)\n```", "```py\n>>> evaluate(glove_pool_embeddings, dissimilar)\n([0.94, 0.97, 0.94, 0.92, 0.93], 0.94)\n```", "```py\nfrom flair.embeddings \\\n      import WordEmbeddings, DocumentRNNEmbeddings\ngru_embeddings = DocumentRNNEmbeddings([glove_embedding])\n```", "```py\n>>> evaluate(gru_embeddings, similar)\n([0.99, 1.0, 0.94, 1.0, 0.92], 0.97)\n>>> evaluate(gru_embeddings, dissimilar)\n([0.86, 1.0, 0.91, 0.85, 0.9], 0.9)\n```", "```py\nfrom flair.embeddings import TransformerDocumentEmbeddings\nfrom flair.data import Sentence\nbert_embeddings = TransformerDocumentEmbeddings(\n                                      'bert-base-uncased')\n```", "```py\n>>> evaluate(bert_embeddings, similar)\n([0.85, 0.9, 0.96, 0.91, 0.89], 0.9)\n>>> evaluate(bert_embeddings, dissimilar)\n([0.93, 0.94, 0.86, 0.93, 0.92], 0.92)\n```", "```py\n    !pip install sentence-transformers\n    ```", "```py\n    from flair.data import Sentence\n    from flair.embeddings \\\n          import SentenceTransformerDocumentEmbeddings\n    sbert_embeddings = SentenceTransformerDocumentEmbeddings(\n                       'bert-base-nli-mean-tokens')\n    ```", "```py\n    >>> evaluate(sbert_embeddings, similar)\n    ([0.98, 0.95, 0.96, 0.99, 0.98], 0.97)\n    >>> evaluate(sbert_embeddings, dissimilar)\n    ([0.48, 0.41, 0.19, -0.05, 0.0], 0.21)\n    ```", "```py\n    >>> tricky_pairs=[\n    (\"An elephant is bigger than a lion\",\n    \"A lion is bigger than an elephant\") ,\n    (\"the cat sat on the mat\",\n    \"the mat sat on the cat\")]\n    >>> evaluate(glove_pool_embeddings, tricky_pairs)\n    ([1.0, 1.0], 1.0)\n    >>> evaluate(gru_embeddings, tricky_pairs)\n    ([0.87, 0.65], 0.76)\n    >>> evaluate(bert_embeddings, tricky_pairs)\n    ([1.0, 0.98], 0.99) \n    >>> evaluate(sbert_embeddings, tricky_pairs)\n    ([0.93, 0.97], 0.95)\n    ```", "```py\n    from transformers \\\n    Import AutoModelForSequenceClassification, AutoTokenizer\n    nli_model = AutoModelForSequenceClassification\\\n                    .from_pretrained(\n                          'joeddav/xlm-roberta-large-xnli')\n    tokenizer = AutoTokenizer\\\n                    .from_pretrained(\n                          'joeddav/xlm-roberta-large-xnli')\n    import numpy as np\n    for permise, hypothesis in tricky_pairs:\n      x = tokenizer.encode(premise, \n                          hypothesis,\n                          return_tensors='pt',\n                          truncation_strategy='only_first')\n      logits = nli_model(x)[0]\n      print(f\"Permise: {permise}\")\n      print(f\"Hypothesis: {hypothesis}\")\n      print(\"Top Class:\")\n     print(nli_model.config.id2label[np.argmax(\n                           logits[0].detach().numpy()). ])\n      print(\"Full softmax scores:\")\n      for i in range(3):\n        print(nli_model.config.id2label[i],\n               logits.softmax(dim=1)[0][i].detach().numpy())\n      print(\"=\"*20)\n    ```", "```py\n    Permise: An elephant is bigger than a lion\n    Hypothesis: A lion is bigger than an elephant\n    Top Class:\n    contradiction\n    Full softmax scores:\n    contradiction 0.7731286\n    neutral 0.2203285\n    entailment 0.0065428796\n    ====================\n    Permise: the cat sat on the mat\n    Hypothesis: the mat sat on the cat\n    Top Class:\n    entailment\n    Full softmax scores:\n    contradiction 0.49365467\n    neutral 0.007260764\n    entailment 0.49908453\n    ====================\n    ```", "```py\n    import pandas as pd, numpy as np\n    import torch, os, scipy\n    from datasets import load_dataset\n    dataset = load_dataset(\"amazon_polarity\",split=\"train\")\n    corpus=dataset.shuffle(seed=42)[:10000]['content']\n    ```", "```py\n    from sentence_transformers import SentenceTransformer\n    model_path=\"paraphrase-distilroberta-base-v1\"\n    model = SentenceTransformer(model_path)\n    ```", "```py\n    >>> corpus_embeddings = model.encode(corpus)\n    >>> corpus_embeddings.shape\n    (10000, 768)\n    ```", "```py\n    >>> from sklearn.cluster import KMeans\n    >>> K=5\n    >>> kmeans = KMeans(\n               n_clusters=5,\n               random_state=0).fit(corpus_embeddings)\n    >>> cls_dist=pd.Series(kmeans.labels_).value_counts()\n    >>> cls_dist\n    3 2772 \n    4 2089 \n    0 1911 \n    2 1883 \n    1 1345 \n    ```", "```py\n    distances = \\\n    scipy.spatial.distance.cdist(kmeans.cluster_centers_, corpus_embeddings)\n    centers={}\n    print(\"Cluster\", \"Size\", \"Center-idx\", \n                          \"Center-Example\", sep=\"\\t\\t\")\n    for i,d in enumerate(distances):\n        ind = np.argsort(d, axis=0)[0]\n        centers[i]=ind\n        print(i,cls_dist[i], ind, corpus[ind] ,sep=\"\\t\\t\")\n    ```", "```py\n    !pip install umap-learn\n    ```", "```py\n    import matplotlib.pyplot as plt\n    import umap\n    X = umap.UMAP(\n               n_components=2,\n               min_dist=0.0).fit_transform(corpus_embeddings)\n    labels= kmeans.labels_fig, ax = plt.subplots(figsize=(12,\n    8))\n    plt.scatter(X[:,0], X[:,1], c=labels, s=1, cmap='Paired')\n    for c in centers:\n        plt.text(X[centers[c],0], X[centers[c], 1],\"CLS-\"+ str(c), fontsize=18) \n        plt.colorbar()\n    ```", "```py\n    !pip install bertopic\n    ```", "```py\n    from bertopic import BERTopic\n    sentence_model = SentenceTransformer(\n                      \"paraphrase-distilroberta-base-v1\")\n    topic_model = BERTopic(embedding_model=sentence_model)\n         topics, _ = topic_model.fit_transform(corpus)\n         topic_model.get_topic_info()[:6]\n    ```", "```py\ntopic_model.get_topic(5)\n```", "```py\n    from sentence_transformers import SentenceTransformer\n    model = SentenceTransformer('quora-distilbert-base')\n    ```", "```py\n    faq_embeddings = model.encode(wwf_faq)\n    ```", "```py\n    test_questions=[\"What should be done, if the adoption pack did not reach to me?\",\n    \" How fast is my adoption pack delivered to me?\",\n    \"What should I do to renew my adoption?\",\n    \"What should be done to change address and contact details ?\",\n    \"I live outside of the UK, Can I still adopt an animal?\"]\n    test_q_emb= model.encode(test_questions)\n    ```", "```py\n    from scipy.spatial.distance import cdist\n    for q, qe in zip(test_questions, test_q_emb):\n        distances = cdist([qe], faq_embeddings, \"cosine\")[0]\n        ind = np.argsort(distances, axis=0)[:3]\n        print(\"\\n Test Question: \\n \"+q)\n        for i,(dis,text) in enumerate(\n                               zip(\n                               distances[ind],\n                               [wwf_faq[i] for i in ind])):\n            print(dis,ind[i],text, sep=\"\\t\")\n    ```", "```py\n    def get_best(query, K=5):\n        query_emb = model.encode([query])\n        distances = cdist(query_emb,faq_embeddings,\"cosine\")[0]\n        ind = np.argsort(distances, axis=0)\n        print(\"\\n\"+query)\n        for c,i in list(zip(distances[ind], ind))[:K]:\n            print(c,wwf_faq[i], sep=\"\\t\")\n    ```", "```py\n    get_best(\"How do I change my contact info?\",3)\n    ```", "```py\n    get_best(\"How do I get my plane ticket \\\n        if I bought it online?\")\n    ```"]