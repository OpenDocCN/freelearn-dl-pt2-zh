["```py\n>>> import gym\n>>> import torch\n>>> import torch.nn as nn >>> env = gym.make('CartPole-v0')\n```", "```py\n>>> class PolicyNetwork():\n ...     def __init__(self, n_state, n_action, n_hidden=50, lr=0.001):\n ...         self.model = nn.Sequential(\n ...                         nn.Linear(n_state, n_hidden),\n ...                         nn.ReLU(),\n ...                         nn.Linear(n_hidden, n_action),\n ...                         nn.Softmax(),\n ...                 )\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n>>>     def predict(self, s):\n ...         \"\"\"\n ...         Compute the action probabilities of state s using \n                 the learning model\n ...         @param s: input state\n ...         @return: predicted policy\n ...         \"\"\"\n ...         return self.model(torch.Tensor(s))\n```", "```py\n>>>     def update(self, returns, log_probs):\n ...         \"\"\"\n ...         Update the weights of the policy network given \n                 the training samples\n ...         @param returns: return (cumulative rewards) for \n                 each step in an episode\n ...         @param log_probs: log probability for each step\n ...         \"\"\"\n ...         policy_gradient = []\n ...         for log_prob, Gt in zip(log_probs, returns):\n ...             policy_gradient.append(-log_prob * Gt)\n ...\n ...         loss = torch.stack(policy_gradient).sum()\n ...         self.optimizer.zero_grad()\n ...         loss.backward()\n ...         self.optimizer.step()\n```", "```py\n>>>     def get_action(self, s):\n ...         \"\"\"\n ...         Estimate the policy and sample an action, \n                 compute its log probability\n ...         @param s: input state\n ...         @return: the selected action and log probability\n ...         \"\"\"\n ...         probs = self.predict(s)\n ...         action = torch.multinomial(probs, 1).item()\n ...         log_prob = torch.log(probs[action])\n ...         return action, log_prob\n```", "```py\n>>> def reinforce(env, estimator, n_episode, gamma=1.0):\n ...     \"\"\"\n ...     REINFORCE algorithm\n ...     @param env: Gym environment\n ...     @param estimator: policy network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         log_probs = []\n ...         rewards = []\n ...         state = env.reset()\n ...         while True:\n ...             action, log_prob = estimator.get_action(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             log_probs.append(log_prob)\n ...             rewards.append(reward)\n ...\n ...             if is_done:\n ...                 returns = []\n ...                 Gt = 0\n ...                 pw = 0\n ...                 for reward in rewards[::-1]:\n ...                     Gt += gamma ** pw * reward\n ...                     pw += 1\n ...                     returns.append(Gt)\n ...                 returns = returns[::-1]\n ...                 returns = torch.tensor(returns)\n ...                 returns = (returns - returns.mean()) / (\n ...                     returns.std() + 1e-9)\n ...                 estimator.update(returns, log_probs)\n ...                 print('Episode: {}, total reward: {}'.format( episode, total_reward_episode[episode]))\n ...                 break\n ...\n ...             state = next_state\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n>>> n_hidden = 128\n>>> lr = 0.003\n>>> policy_net = PolicyNetwork(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> gamma = 0.9\n```", "```py\n>>> n_episode = 500\n>>> total_reward_episode = [0] * n_episode\n>>> reinforce(env, policy_net, n_episode, gamma)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode') >>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\nEpisode: 0, total reward: 12.0\n Episode: 1, total reward: 18.0\n Episode: 2, total reward: 23.0\n Episode: 3, total reward: 23.0\n Episode: 4, total reward: 11.0\n ……\n ……\n Episode: 495, total reward: 200.0\n Episode: 496, total reward: 200.0\n Episode: 497, total reward: 200.0\n Episode: 498, total reward: 200.0\n Episode: 499, total reward: 200.0\n```", "```py\n>>> import gym\n>>> import torch\n>>> import torch.nn as nn >>> from torch.autograd import Variable\n>>> env = gym.make('CartPole-v0')\n```", "```py\n>>> def update(self, advantages, log_probs):\n ...     \"\"\"\n ...     Update the weights of the policy network given \n             the training samples\n ...     @param advantages: advantage for each step in an episode\n ...     @param log_probs: log probability for each step\n ...     \"\"\"\n ...     policy_gradient = []\n ...     for log_prob, Gt in zip(log_probs, advantages):\n ...         policy_gradient.append(-log_prob * Gt)\n ...\n ...     loss = torch.stack(policy_gradient).sum()\n ...     self.optimizer.zero_grad()\n ...     loss.backward()\n ...     self.optimizer.step()\n```", "```py\n>>> class ValueNetwork():\n ...     def __init__(self, n_state, n_hidden=50, lr=0.05):\n ...         self.criterion = torch.nn.MSELoss()\n ...         self.model = torch.nn.Sequential(\n ...                         torch.nn.Linear(n_state, n_hidden),\n ...                         torch.nn.ReLU(),\n ...                         torch.nn.Linear(n_hidden, 1)\n ...                 )\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n...     def update(self, s, y):\n ...         \"\"\"\n ...         Update the weights of the DQN given a training sample\n ...         @param s: states\n ...         @param y: target values\n ...         \"\"\"\n ...         y_pred = self.model(torch.Tensor(s))\n ...         loss = self.criterion(y_pred, Variable(torch.Tensor(y)))\n ...         self.optimizer.zero_grad()\n ...         loss.backward()\n ...         self.optimizer.step()\n```", "```py\n...     def predict(self, s):\n ...         \"\"\"\n ...         Compute the Q values of the state for all actions \n                 using the learning model\n ...         @param s: input state\n ...         @return: Q values of the state for all actions\n ...         \"\"\"\n ...         with torch.no_grad():\n ...             return self.model(torch.Tensor(s))\n```", "```py\n>>> def reinforce(env, estimator_policy, estimator_value, \n                     n_episode, gamma=1.0):\n ...     \"\"\"\n ...     REINFORCE algorithm with baseline\n ...     @param env: Gym environment\n ...     @param estimator_policy: policy network\n ...     @param estimator_value: value network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         log_probs = []\n ...         states = []\n ...         rewards = []\n ...         state = env.reset()\n ...         while True:\n ...             states.append(state)\n ...             action, log_prob = \n                     estimator_policy.get_action(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             log_probs.append(log_prob)\n ...             rewards.append(reward)\n ...\n ...             if is_done:\n ...                 Gt = 0\n ...                 pw = 0\n ...                 returns = []\n ...                 for t in range(len(states)-1, -1, -1):\n ...                     Gt += gamma ** pw * rewards[t]\n ...                     pw += 1\n ...                     returns.append(Gt)\n ...                 returns = returns[::-1]\n ...                 returns = torch.tensor(returns)\n ...                 baseline_values = \n                         estimator_value.predict(states)\n ...                 advantages = returns - baseline_values\n ...                 estimator_value.update(states, returns)\n ...                 estimator_policy.update(advantages, log_probs)\n ...                 print('Episode: {}, total reward: {}'.format( episode, total_reward_episode[episode]))\n ...                 break\n ...             state = next_state\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n>>> n_hidden_p = 64\n>>> lr_p = 0.003\n>>> policy_net = PolicyNetwork(n_state, n_action, n_hidden_p, lr_p)\n```", "```py\n>>> n_hidden_v = 64\n>>> lr_v = 0.003\n>>> value_net = ValueNetwork(n_state, n_hidden_v, lr_v)\n```", "```py\n>>> gamma = 0.9\n```", "```py\n>>> n_episode = 2000\n>>> total_reward_episode = [0] * n_episode\n>>> reinforce(env, policy_net, value_net, n_episode, gamma)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\n>>> import gym\n>>> import torch\n>>> import torch.nn as nn\n>>> import torch.nn.functional as F >>> env = gym.make('CartPole-v0')\n```", "```py\n>>> class ActorCriticModel(nn.Module):\n ...     def __init__(self, n_input, n_output, n_hidden):\n ...         super(ActorCriticModel, self).__init__()\n ...         self.fc = nn.Linear(n_input, n_hidden)\n ...         self.action = nn.Linear(n_hidden, n_output)\n ...         self.value = nn.Linear(n_hidden, 1)\n ...\n ...     def forward(self, x):\n ...         x = torch.Tensor(x)\n ...         x = F.relu(self.fc(x))\n ...         action_probs = F.softmax(self.action(x), dim=-1)\n ...         state_values = self.value(x)\n ...         return action_probs, state_values\n```", "```py\n>>> class PolicyNetwork():\n ...     def __init__(self, n_state, n_action, \n                     n_hidden=50, lr=0.001):\n ...         self.model = ActorCriticModel( n_state, n_action, n_hidden)\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n ...         self.scheduler = torch.optim.lr_scheduler.StepLR( self.optimizer, step_size=10, gamma=0.9)\n```", "```py\n>>>     def predict(self, s):\n ...         \"\"\"\n ...         Compute the output using the Actor Critic model\n ...         @param s: input state\n ...         @return: action probabilities, state_value\n ...         \"\"\"\n ...         return self.model(torch.Tensor(s))\n```", "```py\n>>>     def update(self, returns, log_probs, state_values):\n ...         \"\"\"\n ...         Update the weights of the Actor Critic network \n                 given the training samples\n ...         @param returns: return (cumulative rewards) for \n                 each step in an episode\n ...         @param log_probs: log probability for each step\n ...         @param state_values: state-value for each step\n ...         \"\"\"\n ...         loss = 0\n ...         for log_prob, value, Gt in zip( log_probs, state_values, returns):\n ...             advantage = Gt - value.item()\n ...             policy_loss = -log_prob * advantage\n ...             value_loss = F.smooth_l1_loss(value, Gt)\n ...             loss += policy_loss + value_loss\n ...         self.optimizer.zero_grad()\n ...         loss.backward()\n ...         self.optimizer.step()\n```", "```py\n>>>     def get_action(self, s):\n ...         \"\"\"\n ...         Estimate the policy and sample an action, \n                     compute its log probability\n ...         @param s: input state\n ...         @return: the selected action and log probability\n ...         \"\"\"\n ...         action_probs, state_value = self.predict(s)\n ...         action = torch.multinomial(action_probs, 1).item()\n ...         log_prob = torch.log(action_probs[action])\n ...         return action, log_prob, state_value\n```", "```py\n>>> def actor_critic(env, estimator, n_episode, gamma=1.0):\n ...     \"\"\"\n ...     Actor Critic algorithm\n ...     @param env: Gym environment\n ...     @param estimator: policy network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         log_probs = []\n ...         rewards = []\n ...         state_values = []\n ...         state = env.reset()\n ...         while True:\n ...             action, log_prob, state_value = \n                         estimator.get_action(state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             log_probs.append(log_prob)\n ...             state_values.append(state_value)\n ...             rewards.append(reward)\n ...\n ...             if is_done:\n ...                 returns = []\n ...                 Gt = 0\n ...                 pw = 0\n ...                 for reward in rewards[::-1]:\n ...                     Gt += gamma ** pw * reward\n ...                     pw += 1\n ...                     returns.append(Gt)\n ...                 returns = returns[::-1]\n ...                 returns = torch.tensor(returns)\n ...                 returns = (returns - returns.mean()) / \n                                     (returns.std() + 1e-9)\n ...                 estimator.update( returns, log_probs, state_values)\n ...                 print('Episode: {}, total reward: {}'.format( episode, total_reward_episode[episode]))\n ...                 if total_reward_episode[episode] >= 195:\n ...                     estimator.scheduler.step()\n ...                 break\n ...\n ...             state = next_state\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = env.action_space.n\n>>> n_hidden = 128\n>>> lr = 0.03\n>>> policy_net = PolicyNetwork(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> gamma = 0.9\n```", "```py\n>>> n_episode = 1000\n>>> total_reward_episode = [0] * n_episode\n>>> actor_critic(env, policy_net, n_episode, gamma)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\nEpisode: 0, total reward: 18.0\nEpisode: 1, total reward: 9.0\nEpisode: 2, total reward: 9.0\nEpisode: 3, total reward: 10.0\nEpisode: 4, total reward: 10.0\n...\n...\nEpisode: 995, total reward: 200.0\nEpisode: 996, total reward: 200.0\nEpisode: 997, total reward: 200.0\nEpisode: 998, total reward: 200.0\nEpisode: 999, total reward: 200.0\n```", "```py\n>>> import gym\n>>> import torch\n>>> import torch.nn as nn\n>>> import torch.nn.functional as F >>> env = gym.make('CliffWalking-v0')\n```", "```py\n>>> class ActorCriticModel(nn.Module):\n ...     def __init__(self, n_input, n_output, n_hidden):\n ...         super(ActorCriticModel, self).__init__()\n ...         self.fc1 = nn.Linear(n_input, n_hidden[0])\n ...         self.fc2 = nn.Linear(n_hidden[0], n_hidden[1])\n ...         self.action = nn.Linear(n_hidden[1], n_output)\n ...         self.value = nn.Linear(n_hidden[1], 1)\n ...\n ...     def forward(self, x):\n ...         x = torch.Tensor(x)\n ...         x = F.relu(self.fc1(x))\n ...         x = F.relu(self.fc2(x))\n ...         action_probs = F.softmax(self.action(x), dim=-1)\n ...         state_values = self.value(x)\n ...         return action_probs, state_values\n```", "```py\n>>> def actor_critic(env, estimator, n_episode, gamma=1.0):\n ...     \"\"\"\n ...     Actor Critic algorithm\n ...     @param env: Gym environment\n ...     @param estimator: policy network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         log_probs = []\n ...         rewards = []\n ...         state_values = []\n ...         state = env.reset()\n ...         while True:\n ...             one_hot_state = [0] * 48\n ...             one_hot_state[state] = 1\n ...             action, log_prob, state_value = \n                     estimator.get_action(one_hot_state)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             log_probs.append(log_prob)\n ...             state_values.append(state_value)\n ...             rewards.append(reward)\n ...\n ...             if is_done:\n ...                 returns = []\n ...                 Gt = 0\n ...                 pw = 0\n ...                 for reward in rewards[::-1]:\n ...                     Gt += gamma ** pw * reward\n ...                     pw += 1\n ...                     returns.append(Gt)\n ...                 returns = returns[::-1]\n ...                 returns = torch.tensor(returns)\n ...                 returns = (returns - returns.mean()) / \n                                 (returns.std() + 1e-9)\n ...                 estimator.update( returns, log_probs, state_values)\n ...                 print('Episode: {}, total reward: {}'.format( episode, total_reward_episode[episode]))\n ...                 if total_reward_episode[episode] >= -14:\n ...                     estimator.scheduler.step()\n ...                 break\n ...\n ...             state = next_state\n```", "```py\n>>> n_state = 48\n>>> n_action = env.action_space.n\n>>> n_hidden = [128, 32]\n>>> lr = 0.03\n>>> policy_net = PolicyNetwork(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> gamma = 0.9\n```", "```py\n>>> n_episode = 1000\n>>> total_reward_episode = [0] * n_episode\n>>> actor_critic(env, policy_net, n_episode, gamma)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(range(100, n_episode), total_reward_episode[100:])\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\nEpisode: 0, total reward: -85355\n Episode: 1, total reward: -3103\n Episode: 2, total reward: -1002\n Episode: 3, total reward: -240\n Episode: 4, total reward: -118\n ...\n ...\n Episode: 995, total reward: -13\n Episode: 996, total reward: -13\n Episode: 997, total reward: -13\n Episode: 998, total reward: -13\n Episode: 999, total reward: -13\n```", "```py\n>>> import gym\n>>> import torch\n>>> env = gym.envs.make(\"MountainCarContinuous-v0\")\n```", "```py\n>>> print(env.action_space.low[0])\n-1.0\n>>> print(env.action_space.high[0])\n1.0\n```", "```py\n>>> env.reset()\narray([-0.56756635,  0\\. ])\n```", "```py\n>>> is_done = False\n>>> while not is_done:\n ...     random_action = torch.rand(1) * 2 - 1\n ...     next_state, reward, is_done, info = env.step(random_action)\n ...     print(next_state, reward, is_done)\n ...     env.render()\n >>> env.render()\n [-0.5657432   0.00182313] -0.09924464356736849 False\n [-0.5622848   0.00345837] -0.07744002014160288 False\n [-0.55754507  0.00473979] -0.04372991690837722 False\n ......\n ......\n```", "```py\n>>> import gym\n>>> import torch\n>>> import torch.nn as nn\n>>> import torch.nn.functional as F >>> env = gym.make('MountainCarContinuous-v0')\n```", "```py\n>>> class ActorCriticModel(nn.Module):\n ...     def __init__(self, n_input, n_output, n_hidden):\n ...         super(ActorCriticModel, self).__init__()\n ...         self.fc = nn.Linear(n_input, n_hidden)\n ...         self.mu = nn.Linear(n_hidden, n_output)\n ...         self.sigma = nn.Linear(n_hidden, n_output)\n ...         self.value = nn.Linear(n_hidden, 1)\n ...         self.distribution = torch.distributions.Normal\n ...\n ...     def forward(self, x):\n ...         x = F.relu(self.fc(x))\n ...         mu = 2 * torch.tanh(self.mu(x))\n ...         sigma = F.softplus(self.sigma(x)) + 1e-5\n ...         dist = self.distribution( mu.view(1, ).data, sigma.view(1, ).data)\n ...         value = self.value(x)\n ...         return dist, value\n```", "```py\n>>> class PolicyNetwork():\n ...     def __init__(self, n_state, n_action, \n                     n_hidden=50, lr=0.001):\n ...         self.model = ActorCriticModel( n_state, n_action, n_hidden)\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n```", "```py\n>>>     def predict(self, s):\n ...         \"\"\"\n ...         Compute the output using the continuous Actor Critic model\n ...         @param s: input state\n ...         @return: Gaussian distribution, state_value\n ...         \"\"\"\n ...         self.model.training = False\n ...         return self.model(torch.Tensor(s))\n```", "```py\n>>>     def get_action(self, s):\n ...         \"\"\"\n ...         Estimate the policy and sample an action, \n                 compute its log probability\n ...         @param s: input state\n ...         @return: the selected action, log probability, \n                 predicted state-value\n ...         \"\"\"\n ...         dist, state_value = self.predict(s)\n ...         action = dist.sample().numpy()\n ...         log_prob = dist.log_prob(action[0])\n ...         return action, log_prob, state_value\n```", "```py\n>>> def actor_critic(env, estimator, n_episode, gamma=1.0):\n ...     \"\"\"\n ...     continuous Actor Critic algorithm\n ...     @param env: Gym environment\n ...     @param estimator: policy network\n ...     @param n_episode: number of episodes\n ...     @param gamma: the discount factor\n ...     \"\"\"\n ...     for episode in range(n_episode):\n ...         log_probs = []\n ...         rewards = []\n ...         state_values = []\n ...         state = env.reset()\n ...         while True:\n ...             state = scale_state(state)\n ...             action, log_prob, state_value = \n                         estimator.get_action(state)\n ...             action = action.clip(env.action_space.low[0],\n ...                                  env.action_space.high[0])\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             total_reward_episode[episode] += reward\n ...             log_probs.append(log_prob)\n ...             state_values.append(state_value)\n ...             rewards.append(reward)\n ...             if is_done:\n ...                 returns = []\n ...                 Gt = 0\n ...                 pw = 0\n ...                 for reward in rewards[::-1]:\n ...                     Gt += gamma ** pw * reward\n ...                     pw += 1\n ...                     returns.append(Gt)\n ...                 returns = returns[::-1]\n ...                 returns = torch.tensor(returns)\n ...                 returns = (returns - returns.mean()) / \n                                 (returns.std() + 1e-9)\n ...                 estimator.update( returns, log_probs, state_values)\n ...                 print('Episode: {}, total reward: {}'.format( episode, total_reward_episode[episode]))\n ...                 break\n ...             state = next_state\n```", "```py\n>>> import sklearn.preprocessing\n >>> import numpy as np\n >>> state_space_samples = np.array(\n ...     [env.observation_space.sample() for x in range(10000)])\n >>> scaler = sklearn.preprocessing.StandardScaler()\n >>> scaler.fit(state_space_samples)\n```", "```py\n>>> def scale_state(state):\n ...     scaled = scaler.transform([state])\n ...     return scaled[0]\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> n_action = 1\n>>> n_hidden = 128\n>>> lr = 0.0003\n>>> policy_net = PolicyNetwork(n_state, n_action, n_hidden, lr)\n```", "```py\n>>> gamma = 0.9\n```", "```py\n>>> n_episode = 200\n>>> total_reward_episode = [0] * n_episode\n>>> actor_critic(env, policy_net, n_episode, gamma)\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```", "```py\nEpisode: 0, total reward: 89.46417524456328\n Episode: 1, total reward: 89.54226159679301\n Episode: 2, total reward: 89.91828341346695\n Episode: 3, total reward: 90.04199470314816\n Episode: 4, total reward: 86.23157467747066\n ...\n ...\n Episode: 194, total reward: 92.71676277432059\n Episode: 195, total reward: 89.97484988523927\n Episode: 196, total reward: 89.26063135086025\n Episode: 197, total reward: 87.19460382302674\n Episode: 198, total reward: 79.86081433777699\n Episode: 199, total reward: 88.98075638481279\n```", "```py\n>>> import gym\n>>> import torch\n>>> import torch.nn as nn\n>>> from torch.autograd import Variable >>> env = gym.make('CartPole-v0')\n```", "```py\n>>> class Estimator():\n ...     def __init__(self, n_state, lr=0.001):\n ...         self.model = nn.Sequential(\n ...                         nn.Linear(n_state, 1),\n ...                         nn.Sigmoid()\n ...                 )\n ...         self.criterion = torch.nn.BCELoss()\n ...         self.optimizer = torch.optim.Adam( self.model.parameters(), lr)\n ...\n ...     def predict(self, s):\n ...         return self.model(torch.Tensor(s))\n ...\n ...     def update(self, s, y):\n ...         \"\"\"\n ...         Update the weights of the estimator given \n                 the training samples\n ...         \"\"\"\n ...         y_pred = self.predict(s)\n ...         loss = self.criterion( y_pred, Variable(torch.Tensor(y)))\n ...         self.optimizer.zero_grad()\n ...         loss.backward()\n ...         self.optimizer.step()\n```", "```py\n>>> def cross_entropy(env, estimator, n_episode, n_samples):\n ...     \"\"\"\n ...     Cross-entropy algorithm for policy learning\n ...     @param env: Gym environment\n ...     @param estimator: binary estimator\n ...     @param n_episode: number of episodes\n ...     @param n_samples: number of training samples to use\n ...     \"\"\"\n ...     experience = []\n ...     for episode in range(n_episode):\n ...         rewards = 0\n ...         actions = []\n ...         states = []\n ...         state = env.reset()\n ...         while True:\n ...             action = env.action_space.sample()\n ...             states.append(state)\n ...             actions.append(action)\n ...             next_state, reward, is_done, _ = env.step(action)\n ...             rewards += reward\n ...             if is_done:\n ...                 for state, action in zip(states, actions):\n ...                     experience.append((rewards, state, action))\n ...                 break\n ...             state = next_state\n ...\n ...     experience = sorted(experience, \n                         key=lambda x: x[0], reverse=True)\n ...     select_experience = experience[:n_samples]\n ...     train_states = [exp[1] for exp in select_experience]\n ...     train_actions = [exp[2] for exp in select_experience]\n ...\n ...     for _ in range(100):\n ...         estimator.update(train_states, train_actions)\n```", "```py\n>>> n_state = env.observation_space.shape[0]\n>>> lr = 0.01\n```", "```py\n>>> estimator = Estimator(n_state, lr)\n```", "```py\n>>> n_episode = 5000\n>>> n_samples = 10000\n>>> cross_entropy(env, estimator, n_episode, n_samples)\n```", "```py\n>>> n_episode = 100\n>>> total_reward_episode = [0] * n_episode\n>>> for episode in range(n_episode):\n ...     state = env.reset()\n ...     is_done = False\n ...     while not is_done:\n ...         action = 1 if estimator.predict(state).item() >= 0.5 else 0\n ...         next_state, reward, is_done, _ = env.step(action)\n ...         total_reward_episode[episode] += reward\n ...         state = next_state\n```", "```py\n>>> import matplotlib.pyplot as plt\n>>> plt.plot(total_reward_episode)\n>>> plt.title('Episode reward over time')\n>>> plt.xlabel('Episode')\n>>> plt.ylabel('Total reward')\n>>> plt.show()\n```"]