["```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \nplt.style.use('ggplot')\n\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n```", "```py\ndf = pd.read_csv('data/btc.csv')\n```", "```py\ndf.head()\n```", "```py\ndata = df['Close'].values\n```", "```py\nscaler = StandardScaler()\ndata = scaler.fit_transform(data.reshape(-1, 1))\n```", "```py\nplt.plot(data)\nplt.xlabel('Days')\nplt.ylabel('Price')\nplt.grid()\n```", "```py\ndef get_data(data, window_size):\n    X = []\n    y = []\n\n    i = 0\n\n    while (i + window_size) <= len(data) - 1:\n        X.append(data[i:i+window_size])\n        y.append(data[i+window_size])\n\n        i += 1\n    assert len(X) == len(y)\n    return X, y\n```", "```py\nX, y = get_data(data, window_size = 7)\n```", "```py\n#train set\nX_train = np.array(X[:1000])\ny_train = np.array(y[:1000])\n\n#test set\nX_test = np.array(X[1000:])\ny_test = np.array(y[1000:])\n```", "```py\nX_train.shape\n\n(1000,7,1)\n```", "```py\nbatch_size = 7\nwindow_size = 7\nhidden_layer = 256\nlearning_rate = 0.001\n```", "```py\ninput = tf.placeholder(tf.float32, [batch_size, window_size, 1])\ntarget = tf.placeholder(tf.float32, [batch_size, 1])\n```", "```py\nU_i = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))\nW_i = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))\nb_i = tf.Variable(tf.zeros([hidden_layer]))\n```", "```py\nU_f = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))\nW_f = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))\nb_f = tf.Variable(tf.zeros([hidden_layer]))\n```", "```py\nU_o = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))\nW_o = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))\nb_o = tf.Variable(tf.zeros([hidden_layer]))\n```", "```py\nU_g = tf.Variable(tf.truncated_normal([1, hidden_layer], stddev=0.05))\nW_g = tf.Variable(tf.truncated_normal([hidden_layer, hidden_layer], stddev=0.05))\nb_g = tf.Variable(tf.zeros([hidden_layer]))\n```", "```py\nV = tf.Variable(tf.truncated_normal([hidden_layer, 1], stddev=0.05))\nb_v = tf.Variable(tf.zeros([1]))\n```", "```py\ndef LSTM_cell(input, prev_hidden_state, prev_cell_state):\n\n    it = tf.sigmoid(tf.matmul(input, U_i) + tf.matmul(prev_hidden_state, W_i) + b_i)\n\n    ft = tf.sigmoid(tf.matmul(input, U_f) + tf.matmul(prev_hidden_state, W_f) + b_f)\n\n    ot = tf.sigmoid(tf.matmul(input, U_o) + tf.matmul(prev_hidden_state, W_o) + b_o)\n\n    gt = tf.tanh(tf.matmul(input, U_g) + tf.matmul(prev_hidden_state, W_g) + b_g)\n\n    ct = (prev_cell_state * ft) + (it * gt)\n\n    ht = ot * tf.tanh(ct)\n\n    return ct, ht\n```", "```py\ny_hat = []\n```", "```py\nfor i in range(batch_size):\n```", "```py\n    hidden_state = np.zeros([1, hidden_layer], dtype=np.float32) \n    cell_state = np.zeros([1, hidden_layer], dtype=np.float32)\n```", "```py\n    for t in range(window_size):\n        cell_state, hidden_state = LSTM_cell(tf.reshape(input[i][t], (-1, 1)), hidden_state, cell_state)\n```", "```py\n    y_hat.append(tf.matmul(hidden_state, V) + b_v)\n```", "```py\nlosses = []\n\nfor i in range(len(y_hat)):\n    losses.append(tf.losses.mean_squared_error(tf.reshape(target[i], (-1, 1)), y_hat[i]))\n\nloss = tf.reduce_mean(losses)\n```", "```py\ngradients = tf.gradients(loss, tf.trainable_variables())\nclipped, _ = tf.clip_by_global_norm(gradients, 4.0)\n```", "```py\noptimizer = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(gradients, tf.trainable_variables()))\n```", "```py\nsession = tf.Session()\nsession.run(tf.global_variables_initializer())\n```", "```py\nepochs = 100\n```", "```py\nfor i in range(epochs):\n\n    train_predictions = []\n    index = 0\n    epoch_loss = []\n```", "```py\n    while(index + batch_size) <= len(X_train):\n\n        X_batch = X_train[index:index+batch_size]\n        y_batch = y_train[index:index+batch_size]\n\n        #predict the price and compute the loss\n        predicted, loss_val, _ = session.run([y_hat, loss, optimizer], feed_dict={input:X_batch, target:y_batch})\n\n        #store the loss in the epoch_loss list\n        epoch_loss.append(loss_val)\n\n        #store the predictions in the train_predictions list\n        train_predictions.append(predicted)\n        index += batch_size\n```", "```py\n     if (i % 10)== 0:\n        print 'Epoch {}, Loss: {} '.format(i,np.mean(epoch_loss))\n```", "```py\nEpoch 0, Loss: 0.0402321927249 \nEpoch 10, Loss: 0.0244581680745 \nEpoch 20, Loss: 0.0177710317075 \nEpoch 30, Loss: 0.0117778982967 \nEpoch 40, Loss: 0.00901956297457 \nEpoch 50, Loss: 0.0112476013601 \nEpoch 60, Loss: 0.00944950990379 \nEpoch 70, Loss: 0.00822851061821 \nEpoch 80, Loss: 0.00766260037199 \nEpoch 90, Loss: 0.00710930628702 \n```", "```py\npredicted_output = []\ni = 0\nwhile i+batch_size <= len(X_test): \n\n    output = session.run([y_hat],feed_dict={input:X_test[i:i+batch_size]})\n    i += batch_size\n    predicted_output.append(output)\n```", "```py\npredicted_output[0]\n```", "```py\n[[array([[-0.60426176]], dtype=float32),\n  array([[-0.60155034]], dtype=float32),\n  array([[-0.60079575]], dtype=float32),\n  array([[-0.599668]], dtype=float32),\n  array([[-0.5991149]], dtype=float32),\n  array([[-0.6008351]], dtype=float32),\n  array([[-0.5970466]], dtype=float32)]]\n```", "```py\npredicted_values_test = []\nfor i in range(len(predicted_output)):\n  for j in range(len(predicted_output[i][0])):\n    predicted_values_test.append(predicted_output[i][0][j])\n```", "```py\npredicted_values_test[0]\n\narray([[-0.60426176]], dtype=float32)\n```", "```py\npredictions = []\nfor i in range(1280):\n      if i >= 1000:\n        predictions.append(predicted_values_test[i-1019])\n      else:\n        predictions.append(None)\n```", "```py\nplt.figure(figsize=(16, 7))\nplt.plot(data, label='Actual')\nplt.plot(predictions, label='Predicted')\nplt.legend()\nplt.xlabel('Days')\nplt.ylabel('Price')\nplt.grid()\nplt.show()\n```", "```py\n Uz = tf.get_variable(\"Uz\", [vocab_size, hidden_size], initializer=init)\n Wz = tf.get_variable(\"Wz\", [hidden_size, hidden_size], initializer=init)\n bz = tf.get_variable(\"bz\", [hidden_size], initializer=init)\n```", "```py\nUr = tf.get_variable(\"Ur\", [vocab_size, hidden_size], initializer=init)\nWr = tf.get_variable(\"Wr\", [hidden_size, hidden_size], initializer=init)\nbr = tf.get_variable(\"br\", [hidden_size], initializer=init)\n```", "```py\nUc = tf.get_variable(\"Uc\", [vocab_size, hidden_size], initializer=init)\nWc = tf.get_variable(\"Wc\", [hidden_size, hidden_size], initializer=init)\nbc = tf.get_variable(\"bc\", [hidden_size], initializer=init)\n```", "```py\nV = tf.get_variable(\"V\", [hidden_size, vocab_size], initializer=init)\nby = tf.get_variable(\"by\", [vocab_size], initializer=init)\n```", "```py\nzt = tf.sigmoid(tf.matmul(x_t, Uz) + tf.matmul(h_t, Wz) + bz)\n```", "```py\nrt = tf.sigmoid(tf.matmul(x_t, Ur) + tf.matmul(h_t, Wr) + br)\n```", "```py\nct = tf.tanh(tf.matmul(x_t, Uc) + tf.matmul(tf.multiply(rt, h_t), Wc) + bc)\n```", "```py\n h_t = tf.multiply((1 - zt), ct) + tf.multiply(zt, h_t)\n```", "```py\n y_hat_t = tf.matmul(h_t, V) + by\n```", "```py\nfrom tensorflow.contrib import rnn\n```", "```py\nforward_hidden_layer = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n\nbackward_hidden_layer = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n```", "```py\noutputs, forward_states, backward_states = rnn.static_bidirectional_rnn(forward_hidden_layer, backward_hidden_layer, input)                                         \n```"]