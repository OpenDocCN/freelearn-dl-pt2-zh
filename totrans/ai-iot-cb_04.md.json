["```py\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(1385)\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n```", "```py\nfile_location = \"/FileStore/tables/train_FD001.txt\"\nfile_type = \"csv\"\n\nfrom pyspark.sql.types import *\nschema = StructType([\n  StructField(\"engine_id\", IntegerType()),\n  StructField(\"cycle\", IntegerType()),\n  StructField(\"setting1\", DoubleType()),\n  StructField(\"setting2\", DoubleType()),\n  StructField(\"setting3\", DoubleType()),\n  StructField(\"s1\", DoubleType()),\n  StructField(\"s2\", DoubleType()),\n  StructField(\"s3\", DoubleType()),\n  StructField(\"s4\", DoubleType()),\n  StructField(\"s5\", DoubleType()),\n  StructField(\"s6\", DoubleType()),\n  StructField(\"s7\", DoubleType()),\n  StructField(\"s8\", DoubleType()),\n  StructField(\"s9\", DoubleType()),\n  StructField(\"s10\", DoubleType()),\n  StructField(\"s11\", DoubleType()),\n  StructField(\"s12\", DoubleType()),\n  StructField(\"s13\", DoubleType()),\n  StructField(\"s14\", DoubleType()),\n  StructField(\"s15\", DoubleType()),\n  StructField(\"s16\", DoubleType()),\n  StructField(\"s17\", IntegerType()),\n  StructField(\"s18\", IntegerType()),\n  StructField(\"s19\", DoubleType()),\n  StructField(\"s20\", DoubleType()),\n  StructField(\"s21\", DoubleType())\n  ])\n```", "```py\ndf = spark.read.option(\"delimiter\",\" \").csv(file_location, \n                                            schema=schema, \n                                            header=False)\n```", "```py\ndf.createOrReplaceTempView(\"raw_engine\")\n```", "```py\n%sql\n\ndrop table if exists engine;\n\ncreate table engine as\n(select e.*\n,mc - e.cycle as rul\n, CASE WHEN mc - e.cycle < 14 THEN 1 ELSE 0 END as needs_maintenance \nfrom raw_engine e \njoin (select max(cycle) mc, engine_id from raw_engine group by engine_id) m\non e.engine_id = m.engine_id)\n```", "```py\ndf = spark.sql(\"select * from engine\")\n```", "```py\nmy_window = Window.partitionBy('engine_id').orderBy(\"cycle\")\ndf = df.withColumn(\"roc_s9\", \n                   ((F.lag(df.s9).over(my_window)/df.s9) -1)*100)\ndf = df.withColumn(\"roc_s20\", \n                   ((F.lag(df.s20).over(my_window)/df.s20) -1)*100)\ndf = df.withColumn(\"roc_s2\", \n                   ((F.lag(df.s2).over(my_window)/df.s2) -1)*100)\ndf = df.withColumn(\"roc_s14\", \n                   ((F.lag(df.s14).over(my_window)/df.s14) -1)*100)\n```", "```py\npdf = df.toPandas()\npdf.describe().transpose()\n```", "```py\ncolumns_to_drop = ['s1', 's5', 's10', 's16', 's18', 's19', \n                   'op_setting3', 'setting3']\ndf = df.drop(*columns_to_drop)\n```", "```py\ncorr = pdf.corr().round(1)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nf, ax = plt.subplots(figsize=(20, 20))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, \n            annot=True)\n\ndisplay(plt.tight_layout())\n```", "```py\ncolumns_to_drop = ['s14']\ndf = df.drop(*columns_to_drop)\n```", "```py\npdf = df.toPandas()\n\nplt.figure(figsize = (16, 8))\nplt.title('Example temperature sensor', fontsize=16)\nplt.xlabel('# Cycles', fontsize=16)\nplt.ylabel('Degrees', fontsize=16)\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\npdf.hist(bins=50, figsize=(18,16))\ndisplay(plt.show())\n```", "```py\nvalues = pdf[pdf.engine_id==1].values\ngroups = [5, 6, 7, 8, 9, 10, 11,12,13]\ni = 1\nplt.figure(figsize=(10,20))\nfor group in groups:\n    plt.subplot(len(groups), 1, i)\n    plt.plot(values[:, group])\n    plt.title(pdf.columns[group], y=0.5, loc='right')\n    i += 1\ndisplay(plt.show())\n```", "```py\nw = (Window.partitionBy('engine_id').orderBy(\"cycle\")\\\n     .rangeBetween(-7,0))\ndf = df.withColumn('rolling_average_s2', F.avg(\"s2\").over(w))\ndf = df.withColumn('rolling_average_s3', F.avg(\"s3\").over(w))\ndf = df.withColumn('rolling_average_s4', F.avg(\"s4\").over(w))\ndf = df.withColumn('rolling_average_s7', F.avg(\"s7\").over(w))\ndf = df.withColumn('rolling_average_s8', F.avg(\"s8\").over(w))\n\npdf = df.toPandas()\nvalues = pdf[pdf.engine_id==1].values\ngroups = [5, 25, 6, 26, 8, 27]\ni = 1\nplt.figure(figsize=(10,20))\nfor group in groups:\n    plt.subplot(len(groups), 1, i)\n    plt.plot(values[:, group])\n    plt.title(pdf.columns[group], y=0.5, loc='right')\n    i += 1\ndisplay(plt.show())\n```", "```py\ndf.write.mode(\"overwrite\").saveAsTable(\"engine_ml_ready\") \n\n```", "```py\n# File location and type\nfile_location = \"/FileStore/tables/test_FD001.txt\"\ndf = spark.read.option(\"delimiter\",\" \").csv(file_location, \n                                            schema=schema, \n                                            header=False)\ndf.write.mode(\"overwrite\").saveAsTable(\"engine_test\")\n\n```", "```py\nfile_location = \"/FileStore/tables/RUL_FD001.txt\"\nRULschema = StructType([StructField(\"RUL\", IntegerType())])\ndf = spark.read.option(\"delimiter\",\" \").csv(file_location,\n                                            schema=RULschema, \n                                            header=False)\ndf.write.mode(\"overwrite\").saveAsTable(\"engine_RUL\")\n```", "```py\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\n\nimport pandas as pd\nimport numpy as np\nimport io\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, LeakyReLU, Dropout\n\nimport pickle\nimport mlflow\n```", "```py\nX_train = spark.sql(\"select rolling_average_s2, rolling_average_s3, \n                    rolling_average_s4, rolling_average_s7, \n                    rolling_average_s8 from \\\n                    engine_ml_ready\").toPandas()\n\ny_train = spark.sql(\"select needs_maintenance from \\\n                    engine_ml_ready\").toPandas()\n\nX_test = spark.sql(\"select rolling_average_s2, rolling_average_s3, \n                    rolling_average_s4, rolling_average_s7, \n                    rolling_average_s8 from \\\n                    engine_test_ml_ready\").toPandas()\n\ny_test = spark.sql(\"select needs_maintenance from \\\n                    engine_test_ml_ready\").toPandas()\n```", "```py\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_train.iloc[:,1:6] = scaler.fit_transform(X_train.iloc[:,1:6])\nX_test.iloc[:,1:6] = scaler.fit_transform(X_test.iloc[:,1:6])\ndim = X_train.shape[1]\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(32, input_dim = dim))\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.25))\n```", "```py\nmodel.add(Dense(32))\nmodel.add(LeakyReLU())\nmodel.add(Dropout(0.25))\n```", "```py\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nmodel.compile(optimizer ='rmsprop', loss ='binary_crossentropy',\n              metrics = ['accuracy'])\n```", "```py\nmodel.fit(X_train, y_train, batch_size = 32, epochs = 5, \n          verbose = 1, validation_data = (X_test, y_test))\n```", "```py\ny_pred = model.predict(X_test)\npre_score = precision_score(y_test,y_pred, average='micro')\nprint(\"Neural Network:\",pre_score)\n```", "```py\nwith mlflow.start_run():\n    mlflow.set_experiment(\"/Shared/experiments/Predictive_Maintenance\")\n    mlflow.log_param(\"model\", 'Neural Network')\n    mlflow.log_param(\"Inputactivation\", 'Leaky ReLU')\n    mlflow.log_param(\"Hiddenactivation\", 'Leaky ReLU')\n    mlflow.log_param(\"optimizer\", 'rmsprop')\n    mlflow.log_param(\"loss\", 'binary_crossentropy')\n```", "```py\n    mlflow.log_metric(\"precision_score\", pre_score)\n    filename = 'NeuralNet.pickel'\n    pickle.dump(model, open(filename, 'wb'))\n    mlflow.log_artifact(filename)\n```", "```py\nimport pandas as pd\nimport numpy as np\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Activation\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, recall_score, precision_score\nimport pickle\nimport mlflow\n```", "```py\nweek1 = 7\nweek2 = 14\nsequence_length = 100\nsensor_cols = ['s' + str(i) for i in range(1,22)]\nsequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\nsequence_cols.extend(sensor_cols)\n```", "```py\ntrain = spark.sql(\"select * from engine\").toPandas()\ntrain.drop(columns=\"label\" , inplace=True)\ntest = spark.sql(\"select * from engine_test2\").toPandas()\ntruth = spark.sql(\"select * from engine_rul\").toPandas()\n```", "```py\nrul = pd.DataFrame(train.groupby('engine_id')['cycle']\\\n                   .max()).reset_index()\nrul.columns = ['engine_id', 'max']\ntrain = train.merge(rul, on=['engine_id'], how='left')\ntrain['RUL'] = train['max'] - train['cycle']\ntrain.drop('max', axis=1, inplace=True)\ntrain['label1'] = np.where(train['RUL'] <= week2, 1, 0 )\ntrain['label2'] = train['label1']\ntrain.loc[train['RUL'] <= week1, 'label2'] = 2\n```", "```py\nrul = pd.DataFrame(test.groupby('engine_id')['cycle'].max())\\\n                   .reset_index()\nrul.columns = ['engine_id', 'max']\ntruth.columns = ['more']\ntruth['engine_id'] = truth.index + 1\ntruth['max'] = rul['max'] + truth['more']\ntruth.drop('more', axis=1, inplace=True)\n\ntest = test.merge(truth, on=['engine_id'], how='left')\ntest['RUL'] = test['max'] - test['cycle']\ntest.drop('max', axis=1, inplace=True)\n\ntest['label1'] = np.where(test['RUL'] <= week2, 1, 0 )\ntest['label2'] = test['label1']\ntest.loc[test['RUL'] <= week1, 'label2'] = 2\n```", "```py\ntrain['cycle_norm'] = train['cycle']\ncols_normalize = train.columns.difference(['engine_id','cycle','RUL',\n                                           'label1','label2'])\nmin_max_scaler = preprocessing.MinMaxScaler()\nnorm_train = \\\npd.DataFrame(min_max_scaler.fit_transform(train[cols_normalize]), \n                                          columns=cols_normalize, \n                                          index=train.index)\njoin = \\\ntrain[train.columns.difference(cols_normalize)].join(norm_train)\ntrain = join.reindex(columns = train.columns)\n\ntest['cycle_norm'] = test['cycle']\nnorm_test = \\\npd.DataFrame(min_max_scaler.transform(test[cols_normalize]), \n                                      columns=cols_normalize, \n                                      index=test.index)\ntest_join = \\\ntest[test.columns.difference(cols_normalize)].join(norm_test)\ntest = test_join.reindex(columns = test.columns)\ntest = test.reset_index(drop=True)\n```", "```py\ndef gen_sequence(id_df, seq_length, seq_cols):\n    data_array = id_df[seq_cols].values\n    num_elements = data_array.shape[0]\n    for start, stop in zip(range(0, num_elements-seq_length), \n                           range(seq_length, num_elements)):\n        yield data_array[start:stop, :]\n\nseq_gen = (list(gen_sequence(train[train['engine_id']==engine_id],\n                             sequence_length, sequence_cols)) \n           for engine_id in train['engine_id'].unique())\n\nseq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n```", "```py\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\nmodel = Sequential()\n\nmodel.add(LSTM(input_shape=(sequence_length, nb_features), \n               units=100, return_sequences=True))\nmodel.add(Dropout(0.25))\n```", "```py\nmodel.add(LSTM(units=50, return_sequences=False))\nmodel.add(Dropout(0.25))\n```", "```py\nmodel.add(Dense(units=nb_out, activation='sigmoid'))\n```", "```py\nmodel.compile(loss='binary_crossentropy', optimizer='adam', \n              metrics=['accuracy'])\nprint(model.summary())\n```", "```py\nmodel.fit(seq_array, label_array, epochs=10, batch_size=200, \n          validation_split=0.05, verbose=1, \n          callbacks = \\\n          [keras.callbacks.EarlyStopping(monitor='val_loss', \n                                         min_delta=0, patience=0, \n                                         verbose=0, mode='auto')])\n```", "```py\nscores = model.evaluate(seq_array, label_array, verbose=1, \n                        batch_size=200)\nprint('Accuracy: {}'.format(scores[1]))\n```", "```py\ny_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\ny_true = label_array\nprint('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true, y_pred)\ncm\n```", "```py\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nprint( 'precision = ', precision, '\\n', 'recall = ', recall)\n```", "```py\nseq_array_test_last = [test[test['engine_id']==engine_id]\\\n[sequence_cols].values[-sequence_length:] for engine_id in \\\ntest['engine_id'].unique() if \\\nlen(test[test['engine_id']==engine_id]) >= sequence_length]\n\nseq_array_test_last = \\\nnp.asarray(seq_array_test_last).astype(np.float32)\n\ny_mask = [len(test[test['engine_id']==engine_id]) >= \\\n          sequence_length for engine_id in \\\n          test['engine_id'].unique()]\n\nlabel_array_test_last = \\\ntest.groupby('engine_id')['label1'].nth(-1)[y_mask].values\nlabel_array_test_last = label_array_test_last.reshape(\n    label_array_test_last.shape[0],1).astype(np.float32)\n```", "```py\nscores_test = model.evaluate(seq_array_test_last, \n                             label_array_test_last, verbose=2)\nprint('Accuracy: {}'.format(scores_test[1]))\ny_pred_test = model.predict_classes(seq_array_test_last)\ny_true_test = label_array_test_last\nprint('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true_test, y_pred_test)\nprint(cm)\n\npre_score = precision_score(y_true_test, y_pred_test)\nrecall_test = recall_score(y_true_test, y_pred_test)\nf1_test = 2 * (pre_score * recall_test) / (pre_score + recall_test)\nprint('Precision: ', pre_score, '\\n', 'Recall: ', recall_test,\n      '\\n', 'F1-score:', f1_test )\n```", "```py\nwith mlflow.start_run():\n    mlflow.set_experiment(\"/Shared/experiments/Predictive_Maintenance\")\n    mlflow.log_param(\"type\", 'LSTM')\n    mlflow.log_metric(\"precision_score\", pre_score)\n    filename = 'model.sav'\n    pickle.dump(model, open(filename, 'wb'))\n    mlflow.log_artifact(filename)\n```", "```py\nfrom flask import Flask, request, jsonify\nimport os\nimport pickle\nimport pandas as pd\nimport xgboost as xgb\n```", "```py\napplication = Flask(__name__)\nmodel_filename = os.path.join(os.getcwd(), 'bst.sav')\nloaded_model = pickle.load(open(model_filename, \"rb\"))\n```", "```py\n@application.route('/predict', methods=['POST']) \ndef predict():\n    x_test = pd.DataFrame(request.json)\n    y_pred = loaded_model.predict(xgb.DMatrix(x_test))\n    y_pred[y_pred > 0.5] = 1\n    y_pred[y_pred <= 0.5] = 0\n    return int(y_pred[0])\n```", "```py\nif __name__ == '__main__':\n    application.run(host='0.0.0.0', port=8000)\n```", "```py\nflask\npandas\nxgboost\npickle-mixin\ngunicorn\n```", "```py\nFROM python:3.7.5\nADD . /app\nWORKDIR /app\n\nRUN pip install -r requirements.txt\n\nEXPOSE 8000\nCMD [\"gunicorn\", \"-b\", \"0.0.0.0:8000\", \"app\"]\n```", "```py\ndocker build -t ch4 .\n```", "```py\ndocker run -it -p 8000:8000 ch4\n```", "```py\ndocker login\n\ndocker tag ch4 [Your container path]:v1\ndocker push [Your container path]:v1\n```"]