- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Rise of Suprahuman Transformers with GPT-3 Engines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2020, *Brown* et al. (2020) described the training of an OpenAI GPT-3 model
    containing 175 billion parameters that learned using huge datasets such as the
    400 billion byte-pair-encoded tokens extracted from Common Crawl data. OpenAI
    ran the training on a Microsoft Azure supercomputer with 285,00 CPUs and 10,000
    GPUs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: The machine intelligence of OpenAI’s GPT-3 engines and their supercomputer led
    *Brown* et al. (2020) to zero-shot experiments. The idea was to use a trained
    model for downstream tasks without further training the parameters. The goal would
    be for a trained model to go directly into multi-task production with an API that
    could even perform tasks it wasn’t trained for.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The era of suprahuman cloud AI engines was born. OpenAI’s API requires no high-level
    software skills or AI knowledge. You might wonder why I used the term “suprahuman.”
    You will discover that a GPT-3 engine can perform many tasks as well as a human
    in many cases. For the moment, *it is essential to understand how GPT models are
    built and run to appreciate the magic*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will first examine the architecture and the evolution of the size
    of the transformer model. We will investigate the zero-shot challenge of using
    trained transformer models with little to no fine-tuning of the model’s parameters
    for downstream tasks. We will explore the innovative architecture of GPT transformer
    models. OpenAI provides specially trained versions of their models named engines.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We will use a 345M parameter GPT-2 transformer in TensorFlow from OpenAI’s repository.
    We must get our hands dirty to understand GPT models. We will interact with the
    model to produce text completion with general conditioning sentences.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We will continue by using a 117M parameter customized GPT-2 model. We will tokenize
    the high-level conceptual `Kant` dataset we used to train the RoBERTa model in
    *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will then explore using a GPT-3 engine that does not require a data
    scientist, an artificial specialist, or even an experienced developer to *get
    started*. However, that does not mean that a data scientist or an AI specialist
    will not be required down the line.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: We will see that GPT-3 engines do sometimes require fine-tuning. We will run
    a Google Colab notebook to fine-tune a GPT-3 Ada engine.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will end with the new mindset and skillset of an Industry 4.0 AI
    specialist.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will know how a GPT model is built and how to
    use a seamless GPT-3 API. You will understand the gratifying tasks an Industry
    4.0 AI specialist can accomplish in the 2020s!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with a GPT-3 model
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of OpenAI GPT models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining zero-shot transformer models
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The path from few-shots to one-shot
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a near-human GPT-2 text completion model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a 345M parameter model and running it
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with GPT-2 with a standard model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与一个标准模型交互使用 GPT-2
- en: Training a language modeling GPT-2 117M parameter model
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个具有 117M 参数模型的语言建模 GPT-2
- en: Importing a customized and specific dataset
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入一个定制的特定数据集
- en: Encoding a customized dataset
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对一个定制数据集进行编码
- en: Conditioning the model
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行条件调节
- en: Conditioning a GPT-2 model for specific text completion tasks
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件转换一个 GPT-2 模型以执行特定的文本完成任务
- en: Fine-tuning a GPT-3 model
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对一个 GPT-3 模型进行微调
- en: The role of an Industry 4.0 AI specialist
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工业 4.0 人工智能专家的角色
- en: Let’s begin our journey by exploring GPT-3 transformer models.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索 GPT-3 转换器模型开始我们的旅程。
- en: Suprahuman NLP with GPT-3 transformer models
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-3 转换器模型进行超人类 NLP
- en: GPT-3 is built on the GPT-2 architecture. However, a fully trained GPT-3 transformer
    is a foundation model. A foundation model can do many tasks it wasn’t trained
    for. GPT-3 completion applies all NLP tasks and even programming tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 建立在 GPT-2 架构之上。然而，一个完全训练的 GPT-3 转换器是一个基础模型。基础模型可以执行许多它没有训练过的任务。GPT-3 完成应用于所有
    NLP 任务甚至编程任务。
- en: GPT-3 is one of the few fully trained transformer models that qualify as a foundation
    models. GPT-3 will no doubt lead to more powerful OpenAI models. Google will produce
    foundation models beyond the Google BERT version they trained on their supercomputers.
    Foundation models represent a new way of thinking about AI.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 是少数几个符合基础模型资格的完全训练的转换器模型之一。GPT-3 毫无疑问将产生更强大的 OpenAI 模型。Google 将为其在超级计算机上训练的
    Google BERT 版本之外的基础模型提供更多。基础模型代表了对人工智能的一种新思考方式。
- en: It will not take long for companies to realize they do not need a data scientist
    or an AI specialist to start an NLP project with an API like the one that OpenAI
    provides.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 公司很快就会意识到，他们不需要数据科学家或人工智能专家才能开始一个具有像 OpenAI 提供的 API 那样的 NLP 项目。
- en: Why bother with any other tool? An OpenAI API is available with access to one
    of the most efficient transformer models trained on one of the most powerful supercomputers
    in the world.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么还要麻烦使用任何其他工具？拥有访问世界上最强大的超级计算机之一上训练过的最有效的转换器模型之一的 OpenAI API。
- en: Why develop tools, download libraries, or use any other tool if an API exists
    that only deep pockets and the best research teams in the world can design, such
    as Google or OpenAI?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在只有深腰包和世界上最好的研究团队才能设计的 API，比如 Google 或 OpenAI，为什么要开发工具、下载库或使用任何其他工具？
- en: The answer to these questions is quite simple. It’s easy to start a GPT-3 engine,
    just as it is to start a Formula 1 or Indy 500 race car. No problem. But then,
    trying to drive such a car is nearly impossible without months of training! GPT-3
    engines are powerful AI race cars. You can get them to run in a few clicks. However,
    mastering their incredible horsepower requires the knowledge you have acquired
    from the beginning of this book up to now and what you will discover in the following
    chapters!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些问题的答案非常简单。启动 GPT-3 引擎就像启动一辆一级方程式赛车或印第500赛车一样简单。没问题。但是，要驾驶这样的车几乎是不可能的，如果没有几个月的培训！GPT-3
    引擎是强大的人工智能赛车。你可以通过几次点击让它们运行。然而，要掌握它们令人难以置信的动力需要你从本书的开始到现在所学到的知识，以及你将在接下来的章节中发现的东西！
- en: We first need to understand the architecture of GPT models to see where developers,
    AI specialists, and data scientists fit in the era of suprahuman NLP models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要了解 GPT 模型的架构，以了解开发者、人工智能专家和数据科学家在超人类 NLP 模型时代的定位。
- en: The architecture of OpenAI GPT transformer models
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI GPT 转换器模型的架构
- en: Transformers went from training, to fine-tuning, and finally to zero-shot models
    in less than three years between the end of 2017 and the first part of 2020\.
    A zero-shot GPT-3 transformer model requires no fine-tuning. The trained model
    parameters are not updated for downstream multi-tasks, which opens a new era for
    NLP/NLU tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2017 年底至 2020 年初的不到三年时间里，转换器模型从训练、微调，最终到零冲击模型。零冲击 GPT-3 转换器模型不需要微调。训练的模型参数不会为下游多任务更新，这为
    NLP/NLU 任务开启了一个新时代。
- en: In this section, we will first learn about the motivation of the OpenAI team
    that designed GPT models. We will begin by going through the fine-tuning of zero-shot
    models. Then we will see how to condition a transformer model to generate mind-blowing
    text completion. Finally, we will explore the architecture of GPT models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将首先了解设计 GPT 模型的 OpenAI 团队的动机。我们将从零冲击模型的微调开始。然后我们将看到如何使一个转换器模型调节以生成令人惊叹的文本完成。最后，我们将探索
    GPT 模型的架构。
- en: We will first go through the creation process of the OpenAI team.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将了解 OpenAI 团队的创建过程。
- en: The rise of billion-parameter transformer models
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The speed at which transformers went from small models trained for NLP tasks
    to models that require little to no fine-tuning is staggering.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) introduced the Transformer, which surpassed CNNs and
    RNNs on BLEU tasks. *Radford* et al. (2018) introduced the **Generative Pre-Training**
    (**GPT**) model, which could perform downstream tasks with fine-tuning. *Devlin*
    et al. (2019) perfected fine-tuning with the BERT model. *Radford* et al. (2019)
    went further with GPT-2 models.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '*Brown* et al. (2020) defined a GPT-3 zero-shot approach to transformers that
    does not require fine-tuning!'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, *Wang* et al. (2019) created GLUE to benchmark NLP models.
    But transformer models evolved so quickly that they surpassed human baselines!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '*Wang* et al. (2019, 2020) rapidly created SuperGLUE, set the human baselines
    much higher, and made the NLU/NLP tasks more challenging. Transformers are rapidly
    progressing, and some have already surpassed Human Baselines on the SuperGLUE
    leaderboards at the time of writing.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: How did this happen so quickly?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: We will look at one aspect, the models’ sizes, to understand how this evolution
    happened.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The increasing size of transformer models
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From 2017 to 2020 alone, the number of parameters increased from 65M parameters
    in the original Transformer model to 175B parameters in the GPT-3 model, as shown
    in *Table 7.1*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '| **Transformer Model** | **Paper** | **Parameters** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: '| Transformer Base | *Vaswani* et al. (2017) | 65M |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
- en: '| Transformer Big | *Vaswani* et al. (2017) | 213M |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
- en: '| BERT-Base | *Devlin* et al. (2019) | 110M |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
- en: '| BERT-Large | *Devlin* et al. (2019) | 340M |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | *Radford* et al. (2019) | 117M |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | *Radford* et al. (2019) | 345M |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
- en: '| GPT-2 | *Radford* et al. (2019) | 1.5B |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 | *Brown* et al. (2020) | 175B |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: The evolution of the number of transformer parameters'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 7.1* only contains the main models designed during that short time.
    The dates of the publications come after the date the models were actually designed.
    Also, the authors updated the papers. For example, once the original Transformer
    set the market in motion, transformers emerged from Google Brain and Research,
    OpenAI, and Facebook AI, which all produced new models in parallel.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, some GPT-2 models are larger than the smaller GPT-3 models. For
    example, the GPT-3 Small model contains 125M parameters, which is smaller than
    the 345M parameter GPT-2 model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: 'The size of the architecture evolved at the same time:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: The number of layers of a model went from 6 layers in the original Transformer
    to 96 layers in the GPT-3 model
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of heads of a layer went from 8 in the original Transformer model
    to 96 in the GPT-3 model
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context size went from 512 tokens in the original Transformer model to 12,288
    in the GPT-3 model
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture’s size explains why GPT-3 175B, with its 96 layers, produces
    more impressive results than GPT-2 1,542M, with only 40 layers. The parameters
    of both models are comparable, but the number of layers has doubled.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on the context size to understand another aspect of the rapid evolution
    of transformers.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Context size and maximum path length
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cornerstone of transformer models resides in the attention sub-layers. In
    turn, the key property of attention sub-layers is the method used to process context
    size.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The context size is one of the main ways humans and machines can learn languages.
    The larger the context size, the more we can understand a sequence presented to
    us.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: However, the drawback of context size is the distance it takes to understand
    what a word refers to. The path taken to analyze long-term dependencies requires
    changing from recurrent to attention layers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sentence requires a long path to find what the pronoun “it” refers
    to:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: “Our *house* was too small to fit a big couch, a large table, and other furniture
    we would have liked in such a tiny space. We thought about staying for some time,
    but finally, we decided to sell *it*.”
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The meaning of “it” can only be explained if we take a long path back to the
    word “house” at the beginning of the sentence. That’s quite a path for a machine!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'The order of function that defines the maximum path length can be summed up
    as shown in *Table 7.2* in Big *O* notation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '| **Layer Type** | **Maximum Path Length** | **Context Size** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| Self-Attention | 0(1) | 1 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| Recurrent | 0(n) | 100 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: 'Table 7.2: Maximum path length'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) optimized the design of context analysis in the original
    Transformer model. Attention brings the operations down to a one-to-one token
    operation. The fact that all of the layers are identical makes it much easier
    to scale up the size of transformer models. A GPT-3 model with a size 100 context
    window has the same maximum length path as a size 10 context window.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: For example, a recurrent layer in an RNN has to store the total length of the
    context step by step. The maximum path length is the context size. The maximum
    length size for an RNN that would process the context size of a GPT-3 model would
    be 0(n) times longer. Furthermore, an RNN cannot split the context into 96 heads
    running on a parallelized machine architecture, distributing the operations over
    96 GPUs, for example.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'The flexible and optimized architecture of transformers has led to an impact
    on several other factors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) trained a state-of-the-art transformer model with 36M
    sentences. *Brown* et al. (2020) trained a GPT-3 model with 400 billion byte-pair-encoded
    tokens extracted from Common Crawl data.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training large transformer models requires machine power that is only available
    to a small number of teams in the world. It took a total of 2.14*10^(23) FLOPS
    for *Brown* et al. (2020) to train GPT-3 175B.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the architecture of transformers requires highly qualified teams that
    can only be funded by a small number of organizations in the world.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计转换器的架构需要高素质的团队，这些团队只能由世界上少数机构资助。
- en: The size and architecture will continue to evolve and probably increase to trillion-parameter
    models in the near future. Supercomputers will continue to provide the necessary
    resources to train transformers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 大小和架构将继续发展，可能在不久的将来增加到万亿参数模型。超级计算机将继续提供必要的资源来训练转换器。
- en: We will now see how zero-shot models were achieved.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到零-shot模型是如何实现的。
- en: From fine-tuning to zero-shot models
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从微调到零-shot模型
- en: From the start, OpenAI’s research teams, led by *Radford* et al. (2018), wanted
    to take transformers from trained models to GPT models. The goal was to train
    transformers on unlabeled data. Letting attention layers learn a language from
    unsupervised data was a smart move. Instead of teaching transformers to do specific
    NLP tasks, OpenAI decided to train transformers to learn a language.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从一开始，由*Radford*等人(2018年)带领的OpenAI研究团队就希望把转换器从经过训练的模型转变为GPT模型。其目标是在未标记数据上训练转换器。让注意层从无监督数据中学习语言是一个聪明的举措。OpenAI决定训练转换器学习语言，而不是教会它们执行特定的NLP任务。
- en: OpenAI wanted to create a task-agnostic model. So they began to train transformer
    models on raw data instead of relying on labeled data by specialists. Labeling
    data is time-consuming and considerably slows down the transformer’s training
    process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI希望创建一个任务无关的模型。因此，他们开始对原始数据进行转换器模型的训练，而不是依赖专家标记的数据。标记数据耗时，并且显著减慢了转换器的训练过程。
- en: The first step was to start with unsupervised training in a transformer model.
    Then, they would only fine-tune the model’s supervised learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从一个转换器模型的无监督训练开始。然后，他们只会对模型进行监督学习的微调。
- en: OpenAI opted for a decoder-only transformer described in the Stacking decoder
    layers section. The metrics of the results were convincing and quickly reached
    the level of the best NLP models of fellow NLP research labs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI选择了在“叠加解码层”部分描述的只解码转换器。结果的度量是令人信服的，并很快达到了同行NLP研究实验室最佳NLP模型水平。
- en: 'The promising results of the first version of GPT transformer models soon led
    *Radford* et al. (2019) to come up with zero-shot transfer models. The core of
    their philosophy was to continue training GPT models to learn from raw text. They
    then took their research a step further, focusing on language modeling through
    examples of unsupervised distributions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个版本的GPT转换器模型的有希望的结果很快就促使*Radford*等人(2019年)提出了零-shot转移模型。他们的核心哲学是继续训练GPT模型从原始文本中学习。然后，他们进一步深入研究，聚焦于通过无监督分布的语言建模示例：
- en: Examples=(*x*[1], *x*[2], *x*[3], ,*x*[n])
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 示例=(*x*[1], *x*[2], *x*[3], ,*x*[n])
- en: 'The examples are composed of sequences of symbols:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 示例由符号序列组成：
- en: Sequences=(*s*[1], *s*[2], *s*[3], ,*s*[n])
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 序列=(*s*[1], *s*[2], *s*[3], ,*s*[n])
- en: 'This led to a metamodel that can be expressed as a probability distribution
    for any type of input:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了一个可以表示为任何类型输入的概率分布的元模型：
- en: '*p* (*output*/*input*)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* (*输出*/*输入*)'
- en: The goal was to generalize this concept to any type of downstream task once
    the trained GPT model understands a language through intensive training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是一旦训练的GPT模型通过深入训练理解了语言，就将这个概念推广到任何类型的下游任务。
- en: The GPT models rapidly evolved from 117M parameters to 345M parameters, to other
    sizes, and then to 1,542M parameters. 1,000,000,000+ parameter transformers were
    born. The amount of fine-tuning was sharply reduced. The results reached state-of-the-art
    metrics again.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型从117M参数迅速发展到345M参数，然后到其他大小，然后到1,542M参数。10亿+参数的转换器诞生了。微调量急剧减少。结果再次达到了最先进的指标。
- en: 'This encouraged OpenAI to go further, much further. *Brown* et al. (2020) went
    on the assumption that conditional probability transformer models could be trained
    in-depth and were able to produce excellent results with little to no fine-tuning
    for downstream tasks:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励OpenAI更进一步，更远。*Brown*等人(2020年)假设有条件概率转换器模型可以进行深度训练，并且能够在几乎不进行进一步微调的情况下产生出色的结果：
- en: '*p* (*output*/*multi-tasks*)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* (*输出*/*多任务*)'
- en: 'OpenAI was reaching its goal of training a model and then running downstream
    tasks directly without further fine-tuning. This phenomenal progress can be described
    in four phases:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI正在实现其目标，即训练模型，然后直接运行下游任务，而无需进一步微调。这一巨大的进步可以分为四个阶段：
- en: '**Fine-Tuning** (**FT**) is meant to be performed in the sense we have been
    exploring in previous chapters. A transformer model is trained and then fine-tuned
    on downstream tasks. *Radford* et al. (2018) designed many fine-tuning tasks.
    The OpenAI team then reduced the number of tasks progressively to `0` in the following
    steps.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Few-Shot** (**FS**) represents a huge step forward. The GPT is trained. When
    the model needs to make inferences, it is presented with demonstrations of the
    task to perform as conditioning. Conditioning replaces weight updating, which
    the GPT team excluded from the process. We will be applying conditioning to our
    model through the context we provide to obtain text completion in the notebooks
    we will go through in this chapter.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-Shot** (**1S**) takes the process further. The trained GPT model is presented
    with only one demonstration of the downstream task to perform. No weight updating
    is permitted either.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zero-Shot** (**ZS**) is the ultimate goal. The trained GPT model is presented
    with no demonstration of the downstream task to perform.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these approaches has various levels of efficiency. The OpenAI GPT team
    has worked hard to produce these state-of-the-art transformer models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now explain the motivations that led to the architecture of the GPT
    models:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Teaching transformer models how to learn a language through extensive training.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focusing on language modeling through context conditioning.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformer takes the context and generates text completion in a novel way.
    Instead of consuming resources on learning downstream tasks, it works on understanding
    the input and making inferences no matter what the task is.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding efficient ways to train models by masking portions of the input sequences
    forces the transformer to think with machine intelligence. Thus, machine intelligence,
    though not human, is efficient.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We understand the motivations that led to the architecture of GPT models. Let’s
    now have a look at the decoder-layer-only GPT model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Stacking decoder layers
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now understand that the OpenAI team focused on language modeling. Therefore,
    it makes sense to keep the masked attention sublayer. Hence, the choice to retain
    the decoder stacks and exclude the encoder stacks. *Brown* et al. (2020) dramatically
    increased the size of the decoder-only transformer models to get excellent results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: GPT models have the same structure as the decoder stacks of the original Transformer
    designed by *Vaswani* et al. (2017). We described the decoder stacks in *Chapter
    2*, *Getting Started with the Architecture of the Transformer Model*. If necessary,
    take a few minutes to go back through the architecture of the original Transformer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The GPT model has a decoder-only architecture, as shown in *Figure 7.1*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_07_01.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: GPT decoder-only architecture'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: We can recognize the text and position embedding sub-layer, the masked multi-head
    self-attention layer, the normalization sub-layers, the feedforward sub-layer,
    and the outputs. In addition, there is a version of GPT-2 with both text prediction
    and task classification.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以识别文本和位置嵌入子层、蒙版多头自注意层、规范化子层、前馈子层和输出。此外，还有一种同时进行文本预测和任务分类的 GPT-2 版本。
- en: The OpenAI team customized and tweaked the decoder model by model. *Radford*
    et al. (2019) presented no fewer than four GPT models, and *Brown* et al. (2020)
    described no fewer than eight models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 团队通过模型定制和调整了解码模型。*Radford* 等人（2019）给出了至少四种 GPT 模型，*Brown* 等人（2020）描述了至少八种模型。
- en: 'The GPT-3 175B model has reached a unique size that requires computer resources
    that few teams in the world can access:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 175B 模型已经达到了很少团队可以访问的独特规模，需要大量的计算资源：
- en: '*n*[params] = 175.0B, *n*[layers] = 96, *d*[model] = 12288, *n*[heads] = 96'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*[参数] = 175.0B，*n*[层数] = 96，*d*[模型] = 12288，*n*[头] = 96'
- en: Let’s look into the growing number of GPT-3 engines.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看越来越多的 GPT-3 引擎。
- en: GPT-3 engines
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3 引擎
- en: 'A GPT-3 model can be trained to accomplish specific tasks of different sizes.
    The list of engines available at this time is documented by OpenAI: [https://beta.openai.com/docs/engines](https://beta.openai.com/docs/engines)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 模型可以被训练来完成不同规模的特定任务。目前 OpenAI 记录了可用的引擎列表：[https://beta.openai.com/docs/engines](https://beta.openai.com/docs/engines)
- en: 'The base series of engines have different functions – for example:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基本系列的引擎具有不同的功能 - 例如：
- en: The Davinci engine can analyze complex intent
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达芬奇引擎能够分析复杂意图
- en: The Curie engine is fast and has good summarization
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 居里引擎速度快，具有很好的摘要能力
- en: The Babbage engine is good at semantic search
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 巴比奇引擎擅长语义检索
- en: The Ada engine is good at parsing text
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿达引擎擅长解析文本
- en: 'OpenAI is producing more engines to put on the market:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 正在生产更多的引擎投放市场：
- en: The Instruct series provides instructions based on a description. An example
    is available in the *More GPT-3 examples* section of this chapter.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Instruct 系列根据描述提供指令。此章节的 *更多 GPT-3 示例* 中有一个例子可供参考。
- en: The Codex series can translate language to code. We will explore this series
    in *Chapter 16*, *The Emergence of Transformer-Driven Copilots*.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Codex 系列可以将语言翻译成代码。我们将在 *第 16 章* *Transformer 驱动合作伙伴的出现* 中探索这个系列。
- en: The Content filter series filters unsafe or sensitive text. We will explore
    this series in *Chapter 16*, *The Emergence of Transformer-Driven Copilots*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内容过滤系列可以过滤不安全或敏感的文本。我们将在 *第 16 章* *Transformer 驱动合作伙伴的出现* 中探索这个系列。
- en: We have explored the process that led us from fine-tuning to zero-shot GPT-3
    models. We have seen that GPT-3 can produce a wide range of engines.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了从微调到零样本 GPT-3 模型的过程。我们已经看到 GPT-3 可以生成各种引擎。
- en: It is now time to see how the source code of GPT models is built. Although the
    GPT-3 transformer model source code is not publicly available at this time, GPT-2
    models are sufficiently powerful to understand the inner workings of GPT models.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看 GPT 模型的源代码是如何构建的。尽管目前 GPT-3 变压器模型的源代码并不公开，但 GPT-2 模型已经足够强大，可以理解 GPT
    模型的内部工作原理。
- en: We are ready to interact with a GPT-2 model and train it.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好与 GPT-2 模型进行交互和训练。
- en: We will first use a trained GPT-2 345M model for text completion with 24 decoder
    layers with self-attention sublayers of 16 heads.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将使用训练好的 GPT-2 345M 模型进行文本完成，它包括 24 个解码层和 16 个自注意子层的自注意。
- en: We will then train a GPT-2 117M model for customized text completion with 12
    decoder layers with self-attention layers of 12 heads.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对 12 个解码层和 12 个自注意层的 GPT-2 117M 模型进行定制文本完成训练。
- en: Let’s start by interacting with a pretrained 345M parameter GPT-2 model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先与一个预训练的 345M 参数 GPT-2 模型进行交互。
- en: Generic text completion with GPT-2
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GPT-2 进行通用文本完成
- en: We will explore an example with a GPT-2 generic model from top to bottom. *The
    goal of the example we will run is to determine the level of abstract reasoning
    a GPT model can attain*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从上到下探讨使用 GPT-2 通用模型的例子。*我们要运行的例子的目标是确定 GPT 模型能达到的抽象推理水平*。
- en: This section describes the interaction with a GPT-2 model for text completion.
    We will focus on *Step 9* of the `OpenAI_GPT_2.ipynb` notebook described in detail
    in *Appendix III*, *Generic Text Completion with GPT-2*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了与 GPT-2 模型进行文本完成的交互。我们将重点关注 *OpenAI_GPT_2.ipynb* 中详细描述的 *附录 III* 中的 *使用
    GPT-2 进行通用文本完成的第 9 步*。
- en: You can first read this section to see how the generic pretrained GPT-2 model
    will react to a specific example. Then read *Appendix III*, *Generic Text Completion
    with GPT-2*, to go into the details of how a generic GPT-2 model is implemented
    in a Google Colab notebook.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以先阅读本节，看看通用预训练的 GPT-2 模型对特定示例的反应如何。然后阅读 *附录 III*，*使用 GPT-2 进行通用文本补全*，以深入了解通用
    GPT-2 模型如何在 Google Colab 笔记本中实现。
- en: You can also read *Appendix III* directly, which contains the interaction of
    *Step 9* described below.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以直接阅读 *附录 III*，其中包含下面描述的 *Step 9* 的交互。
- en: First, let’s understand the specific example of the pretrained GPT-2 being applied.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解应用预训练 GPT-2 的具体示例。
- en: 'Step 9: Interacting with GPT-2'
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9步：与 GPT-2 交互
- en: In this section, we will interact with the GPT-2 345M model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将与 GPT-2 345M 模型进行交互。
- en: 'To interact with the model, run the `interact_model` cell:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要与模型交互，请运行 `interact_model` 单元格：
- en: '[PRE0]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will be prompted to enter some context:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你将被提示输入一些上下文：
- en: '![](img/B17948_07_02.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_07_02.png)'
- en: 'Figure 7.2: Context input for text completion'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：文本补全的上下文输入
- en: You can try any type of context you wish since this is a standard GPT-2 model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试任何类型的上下文，因为这是一个标准的 GPT-2 模型。
- en: 'We can try a sentence written by Immanuel Kant:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试一句由康德写的句子：
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Press *Enter* to generate text. The output will be relatively random since the
    GPT-2 model was not trained on our dataset, and we are running a stochastic model
    anyway.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 按下 *Enter* 生成文本。由于 GPT-2 模型未在我们的数据集上进行训练，并且我们无论如何都在运行随机模型，因此输出将相对随机。
- en: 'Let’s have a look at the first few lines the GPT model generated at the time
    I ran it:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我运行时 GPT 模型生成的前几行：
- en: '[PRE2]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To stop the cell, double-click on the run button of the cell.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 要停止单元格，请双击单元格的运行按钮。
- en: You can also press *Ctrl* + *M* to stop generating text, but it may transform
    the code into text, and you will have to copy it back into a program cell.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以按下 *Ctrl* + *M* 停止生成文本，但这可能会将代码转换为文本，你将不得不将其复制回程序单元格。
- en: 'The output is rich. We can observe several facts:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出丰富。我们可以观察到几个事实：
- en: The context we entered *conditioned* the output generated by the model.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们输入的上下文 *条件了* 模型生成的输出。
- en: The context was a demonstration of the model. It learned what to say from the
    context without modifying its parameters.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文是模型的演示。它从上下文中学习要说什么，而不修改其参数。
- en: Text completion is conditioned by context. This opens the door to transformer
    models that do not require fine-tuning.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本补全受上下文条件限制。这为不需要微调的转换器模型打开了大门。
- en: From a semantic perspective, the output could be more interesting.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语义角度来看，输出可能更有趣。
- en: From a grammatical perspective, the output is convincing.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从语法角度来看，输出是令人信服的。
- en: Can we do better? The following section presents the interaction of custom text
    completion.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？下一节介绍了自定义文本补全的交互。
- en: Training a custom GPT-2 language model
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练自定义 GPT-2 语言模型
- en: We will continue our top-to-bottom approach in this section by exploring an
    example with a GPT-2 custom model that we will train on a specific dataset. *The
    goal remains to determine the level of abstract reasoning a GPT model can attain*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将继续采用自上而下的方法，探讨一个在特定数据集上训练的 GPT-2 自定义模型的示例。*目标仍然是确定 GPT 模型可以达到的抽象推理水平*。
- en: This section describes the interaction with a GPT-2 model for text completion
    trained on a specific dataset. We will focus on *Step 12* of the `Training_OpenAI_GPT_2.ipynb`
    notebook described in detail in *Appendix IV*, *Custom Text Completion with GPT-2*.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了与特定数据集训练的 GPT-2 模型进行文本补全的交互。我们将重点放在详细描述在 *附录 IV* 中的 *Training_OpenAI_GPT_2.ipynb*
    笔记本中的 *Step 12*，即 *自定义文本补全与 GPT-2*。
- en: You can read this section first to see how an example with a custom GPT-2 model
    will improve responses. Then read *Appendix IV*, *Custom Text Completion with
    GPT-2*, to understand how to train a GPT-2 to obtain specific responses.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以先阅读本节，看看带有自定义 GPT-2 模型的示例将如何改进响应。然后阅读 *附录 IV*，*自定义文本补全与 GPT-2*，以了解如何训练 GPT-2
    来获得特定的响应。
- en: You can also decide to read *Appendix IV* directly, which also contains the
    interaction of *Step 12* described below.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以决定直接阅读 *附录 IV*，其中也包含下面描述的 *Step 12* 的交互。
- en: First, let’s understand how the interaction with GPT-2 improved by training
    it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解通过训练 GPT-2 改进的交互方式。
- en: 'Step 12: Interactive context and completion examples'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第12步：交互式上下文和补全示例
- en: We will now run a conditional sample. The context we enter will condition the
    model to think as we want it to, to complete the text by generating tailor-made
    paragraphs.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the cell and explore the magic:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If necessary, take a few minutes to go back to *Step 9*, *Interacting with
    GPT-2* of *Appendix III*, *Generic Text Completion with GPT-2*, to see the differences
    in the responses. The program prompts us to enter the context:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_07_03.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Context input for text completion'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s enter the same paragraph written by Immanuel Kant as we did in *Step
    9* of the *Generic* *text completion with GPT-2* section of this chapter:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Press *Enter* to generate text as we did previously. Though structured and logical,
    the outputs might change from one run to another, making transformers attractive.
    This time, the result is not random and is impressive.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the first few lines the GPT-2 model produced:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To stop the cell, double-click on the run button of the cell or press *Ctrl*
    + *M*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Wow! I doubt anybody could see the difference between the text completion produced
    by our trained GPT-2 model and a human. It might also generate different outputs
    at each run.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: In fact, I think our model could outperform many humans in this abstract exercise
    in philosophy, reason, and logic!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'We can draw some conclusions from our experiment:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: A well-trained transformer model can produce text completion at a human level
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A GPT-2 model can almost reach human level in text generation on complex and
    abstract reasoning
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text context is an efficient way of conditioning a model by demonstrating what
    is expected
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text completion is text generation based on text conditioning if context sentences
    are provided
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can enter conditioning text context examples to experiment with text completion.
    You can also train the model on your own data. Just replace the content of the
    `dset.txt` file with your own data and see what happens!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Remember that our trained GPT-2 model will react like a human. If you enter
    a short, incomplete, uninteresting, or tricky context, you will obtain puzzled
    or bad results. This is because GPT-2 expects the best out of us, as in real life!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go to the GPT-3 playground to see how a trained GPT-3 reacts to the example
    tested with GPT-2.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Running OpenAI GPT-3 tasks
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will run GPT-3 in two different ways:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: We will first run the GPT-3 tasks online with no code
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will then implement GPT-3 in Google Colab notebook
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using GPT-3 engines in this book. When you sign up for the GPT-3
    API, OpenAI gives you a free budget to get started. This free budget should cover
    most of the cost, if not all of the cost, of running the examples in this book
    once or twice.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by running NLP tasks online.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Running NLP tasks online
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now go through some Industry 4.0 examples without an API, directly asking
    GPT-3 to do something for us.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us define a standard structure of a prompt and response as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '*N* = name of the NLP task (INPUT).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* = NLP 任务的名称（输入）。'
- en: '*E* = explanation for the GPT-3 engine. *E* precedes *T* (INPUT).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E* = GPT-3 引擎的解释。*E* 在 *T* 之前（输入）。'
- en: '*T* = the text or content we wish GPT-3 to look into (INPUT).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T* = 我们希望 GPT-3 查看的文本或内容（输入）。'
- en: '*S* = showing GPT-3 what is expected. *S* follows *T* and is added when necessary
    (INPUT).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S* = 显示给 GPT-3 期望的内容。*S* 在必要时跟随 *T* 并添加（输入）。'
- en: '*R* = GPT-3’s response (OUTPUT).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R* = GPT-3 的回应（输出）。'
- en: The structure of the prompt described above is a guideline. However, GPT-3 is
    very flexible, and many variations are possible.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提示的结构是一个指南。然而，GPT-3 非常灵活，有许多变体可行。
- en: 'We are now ready to run some educational examples online with no API:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备在线运行一些教育示例，无需 API：
- en: 'Questions and answers (**Q&A**) on existing knowledge:'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有知识的问题与答案（**Q&A**）：
- en: '*E* = `Q`'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E* = `Q`'
- en: '*T* = `Who was the president of the United States in 1965?`'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*T* = `1965年美国总统是谁？`'
- en: '*S* = None'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = 无'
- en: '*R* = `A`'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R* = `A`'
- en: 'Prompts and responses:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示和回答：
- en: '`Q: Who was the president of the United States in 1965?`'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Q: 1965年美国总统是谁？`'
- en: '`A: Lyndon B. Johnson was president of the United States in 1965.`'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`A: 1965年林登·约翰逊是美国总统。`'
- en: '`Q: Who was the first human on the moon?`'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Q: 谁是第一个登上月球的人类？`'
- en: '`A: Neil Armstrong was the first human on the moon.`'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`A: 尼尔·阿姆斯特朗是第一个登上月球的人类。`'
- en: '**Movie to Emoji**:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电影到表情符号**：'
- en: '*E* = Some examples of movie titles'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E* = 一些电影标题的示例'
- en: '*T* = None'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*T* = 无'
- en: '*S* = Implicit through examples'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = 通过例子隐含'
- en: '*R* = Some examples of emojis'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R* = 一些表情符号的示例'
- en: 'Prompts and responses:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示和回答：
- en: '![A picture containing text  Description automatically generated](img/B17948_07_08.png)'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![包含文本的图片的图片描述 自动生成](img/B17948_07_08.png)'
- en: 'A new prompt and response:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新的提示和回答：
- en: '![](img/B17948_07_09.png)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17948_07_09.png)'
- en: 'Summarizing for a second grader (**Summarize for a 2nd grader**):'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给二年级学生总结（**给二年级学生总结**）：
- en: '*E* = `My second grader asked me what this passage means:`'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*E* = `我的二年级孩子问我这段话是什么意思：`'
- en: '*T* = `"""The initial conclusions…."""`'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*T* = `"""初始结论……。"""`'
- en: '*S* = `I rephrased it for him, in plain language a second grader can understand:
    """`'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = `我用通俗易懂的语言给他解释了一下："""`'
- en: '*R* = The summary'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*R* = 摘要'
- en: 'Prompt and response:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示和回答：
- en: '`My second grader asked me what this passage means:`'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`我的二年级孩子问我这段话是什么意思：`'
- en: '`""" The initial conclusions can be divided into two categories: facts and
    fiction. The facts are that OpenAI has one of the most powerful NLP services in
    the world. The main facts are: OpenAI engines are powerful zero-shot that require
    no hunting for all kinds of transformer models, no pre-training, and no fine-tuning.
    The supercomputers used to train the models are unique. If the prompt is well-designed,
    we obtain surprisingly accurate responses. Implementing the NLP tasks in this
    section required a copy and paste action that any software beginner can perform.
    Fiction begins with dystopian and hype assertions AI will replace data scientists
    and AI specialists. Is that true? Before answering that question, first ask yourself
    the following questions about the example we just ran: How do we know the sentence
    was incorrect in the first place? How do we know the answer is correct without
    us humans reading and confirming this? How did the engine know it was a grammar
    correction task? If the response is incorrect, how can we understand what happened
    to help improve the prompt or revert to manual mode in a well-designed human interface?
    The truth is that humans will need to intervene to answers these questions manually,
    with rule-bases, quality control automated pipelines, and many other tools. The
    facts are convincing. It is true that running an NLP task requires little development.
    The fiction is not convincing. Humans are still required. OpenAI engines are not
    there to replace humans but to help them perform more high-level gratifying tasks.
    You can now fly a jet without having to build it! """`'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`I rephrased it for him, in plain language a second grader can understand:
    """`'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`The first conclusion is that OpenAI has one of the most powerful NLP services
    in the world. The most important facts are: OpenAI engines are very powerful zero-shot
    that require no hunting for all kinds of transformer models, no pre-training,
    and no fine-tuning. The supercomputers used to train the models are unique. If
    the prompt is well-designed, we get surprisingly accurate responses.`'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The results are impressive and a huge step forward in the history of NLP.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: The number of NLP tasks we can implement with the zero-shot GPT-3 engines provided
    by OpenAI is endless since we control how we show what we expect of the transformer
    models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: However, a true Industry 4.0 AI guru must get their hands dirty before implementing
    ready-to-use APIs. We will now explore the architecture of OpenAI GPT models and
    then build GPT-2 models to see how these engines work.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The more we know about GPT models, the better an Industry 4.0 NLP expert can
    implement them in real-life projects.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue our top-to-bottom approach and drill down into the architecture
    of OpenAI GPT transformer models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with GPT-3 engines
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI has some of the most powerful transformer engines in the world. One GPT-3
    model can perform hundreds of tasks. GPT-3 can do many tasks it wasn’t trained
    for.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: This section will use the API in `Getting_Started_GPT_3.ipynb`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: To use a GPT-3, you must first go to OpenAI’s website, [https://openai.com/](https://openai.com/),
    and sign up.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI has a playground for everybody to try, just like Google Translate or
    any user-friendly online service. So, let’s try some tasks.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Running our first NLP task with GPT-3
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s start using GPT-3 in a few steps.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Go to Google Colab and open `Getting_Started_GPT_3.ipynb`, which is the chapter
    directory of the book on GitHub.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: You do not need to change the settings of the notebook. We are using an API,
    so we will not need much local computing power for the tasks in this section.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The steps of this section are the same ones as in the notebook.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Running an NLP is done in three simple steps:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Installing OpenAI'
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Install `openai` using the following command:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If `openai` is not installed, you must restart the runtime. A message will
    indicate when to do this, as shown in the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_07_10.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Restart the runtime and then run this cell again to make sure `openai` is imported.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Entering the API key'
  id: totrans-267
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An API key is given that can be used with Python, C#, Java, and many other
    options. We will be using Python in this section:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can now update the next cell with your API key:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let’s now run an NLP task.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Running an NLP task with the default parameters'
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We copy and paste an OpenAI example for a **grammar correction** task:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The task is to correct this grammar mistake: `She no went to the market`.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'We can process the response as we wish by parsing it. OpenAI’s response is
    a dictionary object. The OpenAI object contains detailed information on the task.
    We can ask the object to be displayed:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can explore the object:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The “created” number and “id”, and “model” name can vary with each run.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then ask the object dictionary to display `"text"` and to print the
    processed output:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output of “`text`" in the dictionary is the grammatically correct sentence:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: NLP tasks and examples
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will cover an industrial approach to GPT-3 engine usage. For example,
    OpenAI provides an interactive educational interface that does not require an
    API. So a school teacher, a consultant, a linguist, a philosopher, or anybody
    that wishes to use a GPT-3 engine for educational purposes can do so with no experience
    at all in AI.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: We will first begin by using an API in a notebook.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Grammar correction
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If we go back to `Getting_Started_GPT_3.ipynb`, which we began to explore in
    the *Getting started with GPT-3 engines* section of this chapter, we can experiment
    with grammar correction with different prompts.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the notebook and go to *Step 4: Example 1: Grammar correction*:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The request body is not limited to the prompt. The body contains several key
    parameters:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '`engine="davinci"`. The choice of the OpenAI GPT-3 engine to use and possibly
    other models in the future.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature=0`. A higher value such as `0.9` will force the model to take
    more risks. Do not modify the temperature and `top_p` at the same time.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tokens=60`. The maximum number of tokens of the response.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_p=1.0`. Another way to control sampling like `temperature`. In this case,
    the `top_p` percentage of tokens of the probability mass will be considered. `0.2`
    would make the system only take 20% of the top probability mass.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frequency_penalty=0.0`. A value between `0` and `1` limits the frequency of
    tokens in a given response.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`presence_penalty=0.0`. A value between `0` and `1` forces the system to use
    new tokens and produce new ideas.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stop=["\n"]`. A signal to the model to stop producing new tokens.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of these parameters are described at the source code level in the *Steps
    7b-8: Importing and defining the model* section of *Appendix III*, *Generic Text
    Completion with GPT-2*.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: You can play around with these parameters in the GPT-3 model if you gain access
    or in the GPT-2 model in *Appendix III*, *Generic Text Completion with GPT-2*.
    The concepts are the same in both cases.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'This section will focus on the prompt:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The prompt can be divided into three parts:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '`Original`: This signals to the model that what follows is the original text,
    which the model will do something with'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`She no went to the market.\n`: This part of the prompt shows the model that
    this is the original text'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Standard American English`: This shows the model what task is expected'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see how far we can get by changing the task:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard American English produces:'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt="Original: She no went to the market.\n Standard American English:"`'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The text in response is:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"text": " She didn''t go to the market."`'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: That is fine, but what if we do not want a contraction in the sentence?
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'English with no contractions produces:'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt="Original: She no went to the market.\n English with no contractions:"`'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The text in response is:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`"text": " She did not go to the market."`'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Wow! This is impressive. Let’s try another language.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'French with no contractions produces:'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`"text": " Elle n''est pas all\u00e9e au march\u00e9."`'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is impressive. `\u00e9` simply needs to be post-processed into `é`.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Many more options are possible. Your Industry 4.0 cross-disciplinary imagination
    is the limit!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: More GPT-3 examples
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'OpenAI contains many examples. OpenAI provides an online playground to explore
    tasks. OpenAI also provides source code for each example: [https://beta.openai.com/examples](https://beta.openai.com/examples)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Just click on an example such as the grammar example we explored in the *Grammar
    correction* section:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_07_04.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: The Grammar correction section of OpenAI'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI will describe the prompt and the sample response for each task.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_07_05.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: The sample response corrects the prompt'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose to go to the playground and run it online as we did in this
    chapter’s *Running NLP tasks online* section. To do so, click on the **Open in
    Playground** button:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, chat or text message  Description
    automatically generated](img/B17948_07_06.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: The Open in Playground button'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose to copy and paste the code to run the API as we are doing in
    the Google Colab notebook of this chapter:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17948_07_07.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Running code using the Davinci engine'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '`Getting_Started_GPT_3.ipynb` contains ten examples that you can run to practice
    implementing the OpenAI GPT-3.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'For each example:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: You can first read the link to the explanation provided by OpenAI. A link to
    the documentation is provided above each cell.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can then run the cell to observe GPT-3’s behavior.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run these ten examples in the notebook:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: Grammar correction'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 2: English-to-French translation'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 3: Instruct series that provides instructions'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 4: Movie to emoji'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 5: Programming language to another language. For example, Python to
    JavaScript. Warning: you may need to obtain special permission from OpenAI to
    run this example, which uses the Davinci Codex engine, the code generator. If
    this example does not run in your notebook, please contact OpenAI to request access
    to Codex.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 6: Advanced tweet classifier'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 7: Q&A'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example 8 Summarize a text
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 9: Parse unstructured data'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 10: Calculate time complexity'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can run many other tasks on the Examples page: [https://beta.openai.com/examples](https://beta.openai.com/examples)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now compare the output of GPT-2 and GPT-3.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the output of GPT-2 and GPT-3
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our curiosity must be satisfied before we move on. What can the powerful GPT-3
    model produce with the example we submitted to a pretrained GPT-2 model and then
    our custom-trained GPT-2 model?
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Our example used for the GPT-2 model:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result is mind-blowing! It explains what the text means, including some
    deep philosophical reflections!
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: We have proven our point in this chapter. Transformer models can attain to abstract
    reasoning, which can help make micro-decisions in our fast-moving world.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI GPT-3 is a fully trained model. However, GPT-3 can be fine-tuned. Let’s
    see how.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning GPT-3
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section shows how to fine-tune GPT-3 to learn logic. Transformers need
    to learn logic, inferences, and entailment to understand language at a human level.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is the key to making GPT-3 your own application, to customizing
    it to make it fit the needs of your project. It’s a ticket to AI freedom to rid
    your application of bias, teach it things you want it to know, and leave your
    footprint on AI.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: In this section, GPT-3 will be trained on the works of Immanuel Kant using `kantgpt.csv`.
    We used a similar file to train the BERT-type model in *Chapter 4*, *Pretraining
    a RoBERTa Model from Scratch*.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Once you master fine-tuning GPT-3, you can use other types of data to teach
    it specific domains, knowledge graphs, and texts.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI provides an efficient, well-documented service to fine-tune GPT-3 engines.
    It has trained GPT-3 models to become different types of engines, as seen in the
    *The rise of billion-parameter transformer models* section of this chapter.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: The Davinci engine is powerful but can be more expensive to use. The Ada engine
    is less expensive and produces sufficient results to explore GPT-3 in our experiment.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning GPT-3 involves two phases:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a GPT-3 model
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open `Fine_Tuning_GPT_3.ipynb` in Google Colab in the GitHub chapter directory.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI has documented the data preparation process in detail:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data](https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Installing OpenAI'
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Step 1* is to install and import `openai`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Restart the runtime once the installation is complete and run the cell again
    to make sure import openai has been executed
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can also install wand to visualize the logs:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will now enter the API key
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Entering the API key'
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Step 2* is to enter your key:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Step 3: Activating OpenAI’s data preparation module'
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, load your file. In this section, load `kantgpt.csv`. Now, `kantgpt.csv`.
    is a raw unstructured file. OpenAI has an inbuilt data cleaner that will ask questions
    at each step.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI detects that the file is a CSV file and will convert it to a `JSONL`
    file. `JSONL` contains lines in plain structured text.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI tracks all the changes we approve:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: OpenAI saves the converted file to `kantgpt_prepared.jsonl`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to fine-tune GPT-3.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning GPT-3
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can split the notebook into two separate notebooks: one for data preparation
    and one for fine-tuning.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Creating an OS environment'
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Step 4* in the fine-tuning process creates an `os` environment for the API
    key:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Step 5: Fine-tuning OpenAI’s Ada engine'
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Step 5* triggers fine-tuning the OpenAI Ada engine with the JSONL file that
    was saved after data preparation:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: OpenAI has many requests.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'If your steam is interrupted, OpenAI will indicate the instruction to continue
    fine-tuning. `Execute fine_tunes.follow` instruction:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Step 6: Interacting with the fine-tuned model'
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Step 6* is interacting with the fine-tuned model. The prompt is a sequence
    that is close to what *Immanuel Kant* might say:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The instruction to run a completion task with `[YOUR_MODEL INFO]` is often displayed
    by OpenAI at the end of your fine-tune task. You can copy and paste it in a cell(add
    `"!"` to run the command line) or insert your `[YOUR_MODEL INFO]` in the following
    cell.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'The completion is quite convincing:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have fine-tuned GPT-3, which shows the importance of understanding transformers
    and designing AI pipelines with APIs. Let’s see how this changes the role of AI
    specialists.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: The role of an Industry 4.0 AI specialist
  id: totrans-412
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a nutshell, the role of an Industry 4.0 developer is to become a cross-disciplinary
    AI guru. Developers, data scientists, and AI specialists will progressively learn
    more about linguistics, business goals, subject matter expertise, and more. An
    Industry 4.0 AI specialist will guide teams with practical cross-disciplinary
    knowledge and experience.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'Human experts are mandatory in three domains when implementing transformers:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '**Morals and ethics**'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Industry 4.0 AI guru ensures moral and ethical practices are enforced when
    implementing humanlike transformers. European regulations, for example, are strict
    and require that automated decisions be explained to the users when necessary.
    The US has anti-discrimination laws to protect citizens from automated bias.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Prompts and responses**'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users and UI developers will need Industry 4.0 AI gurus to explain how to create
    the right prompts for NLP tasks, show a transformer model how to do a task, and
    verify the response.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Quality control and understanding the model**'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens when the model does not behave as expected even after tweaking
    its hyperparameters? We will go deeper into such issues in *Chapter 14*, *Interpreting
    Black Box Transformer Models*.
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Initial conclusions
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The initial conclusions can be divided into two categories: facts and fiction.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'One fact is that OpenAI has one of the most powerful NLP services in the world.
    Other facts include:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI engines are powerful zero-shot engines that require no hunting for all
    kinds of transformer models, no pre-training, and no fine-tuning
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The supercomputers used to train the models are unique
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a prompt is well designed, we can get surprisingly accurate responses
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the NLP tasks in this chapter only required a copy and paste action
    that any software beginner can perform
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many people believe AI will replace data scientists and AI specialists. Is
    that true? Before answering that question, first, ask yourself the following questions
    about the examples we ran in this chapter:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: How do we know if a sentence is incorrect?
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we know an answer is correct without us humans reading and confirming
    this?
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How did the engine know it was a grammar correction task?
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a response is incorrect, how can we understand what happened to help improve
    the prompt or revert to manual mode in a well-designed human interface?
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The truth is that humans will need to intervene to answers these questions manually,
    with rule bases, quality controlled automated pipelines, and many other tools.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: The facts are convincing. Running an NLP task with a transformer requires little
    development in many cases.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Humans are still required. OpenAI engines are not there to replace humans but
    to help them perform more high-level gratifying tasks. You can now fly a jet without
    having to build it!
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: We need to answer the exciting questions we brought up in this section. So let’s
    now explore your new fascinating Industry 4.0 role on a wonderful path into the
    future of AI!
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: Let’s sum up the chapter and move on to the next exploration!
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discovered the new era of transformer models training billions
    of parameters on supercomputers. OpenAI’s GPT models are taking NLU beyond the
    reach of most NLP development teams.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: We saw how a GPT-3 zero-shot model performs many NLP tasks through an API and
    even directly online without an API. The online version of Google Translate has
    already paved the way for mainstream online usage of AI.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: We explored the design of GPT models, which are all built on the original transformer’s
    decoder stack. The masked attention sub-layer continues the philosophy of left-to-right
    training. However, the sheer power of the calculations and the subsequent self-attention
    sub-layer makes it highly efficient.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: We then implemented a 345M parameter GPT-2 model with TensorFlow. The goal was
    to interact with a trained model to see how far we could get with it. We saw that
    the context provided conditioned the outputs. However, it did not reach the results
    expected when entering a specific input from the `Kant` dataset.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: We trained a 117M parameter GPT-2 model on a customized dataset. The interactions
    with this relatively small trained model produced fascinating results.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: We ran NLP tasks online with OpenAI’s API and fine-tuned a GPT-3 model. This
    chapter showed that the fully pretrained transformers and their engines can automatically
    accomplish many tasks with little help from engineers.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: Does this mean that users will not need AI NLP developers, data scientists,
    and AI specialists anymore in the future? Instead, will users simply upload the
    task definition and input text to cloud transformer models and download the results?
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: No, it doesn’t mean that at all. Industry 4.0 data scientists and AI specialists
    will evolve into pilots of powerful AI systems. They will be increasingly necessary
    to ensure the inputs are ethical and secure. These modern-age AI pilots will also
    understand how transformers are built and adjust the hyperparameters of an AI
    ecosystem.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Applying Transformers to Legal and Financial Documents
    for AI Text Summarization*, we will take transformer models to their limits as
    multi-task models and explore new frontiers.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A zero-shot method trains the parameters once. (True/False)
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gradient updates are performed when running zero-shot models. (True/False)
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT models only have a decoder stack. (True/False)
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is impossible to train a 117M GPT model on a local machine. (True/False)
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is impossible to train the GPT-2 model with a specific dataset. (True/False)
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A GPT-2 model cannot be conditioned to generate text. (True/False)
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A GPT-2 model can analyze the context of an input and produce completion content.
    (True/False)
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We cannot interact with a 345M-parameter GPT model on a machine with less than
    8 GPUs. (True/False)
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supercomputers with 285,000 CPUs do not exist. (True/False)
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Supercomputers with thousands of GPUs are game-changers in AI. (True/False)
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI and GPT-3 engines: [https://beta.openai.com/docs/engines/engines](https://beta.openai.com/docs/engines/engines)'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BertViz` GitHub Repository by *Jesse Vig*: [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI’s supercomputer: [https://blogs.microsoft.com/ai/openai-azure-supercomputer/](https://blogs.microsoft.com/ai/openai-azure-supercomputer/)'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, *Illia Polosukhin*, 2017, *Attention
    is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alec Radford*, *Karthik Narasimhan*, *Tim Salimans*, *Ilya Sutskever*, 2018,
    *Improving* *Language Understanding by Generative Pre-Training*: [https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, and *Kristina Toutanova*, 2019,
    *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*:
    [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alec Radford*, *Jeffrey Wu*, *Rewon Child*, *David Luan*, *Dario Amodei*,
    *Ilya Sutskever*, 2019, *Language Models are Unsupervised Multitask Learners*:
    [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tom B. Brown*, *Benjamin Mann*, *Nick Ryder*, *Melanie Subbiah*, *Jared Kaplany*,
    *Prafulla Dhariwal*, *Arvind Neelakantan*, *Pranav Shyam*, *Girish Sastry*, *Amanda
    Askell*, *Sandhini Agarwal*, *Ariel Herbert-Voss*, *Gretchen Krueger*, *Tom Henighan*,
    *Rewon Child*, *Aditya Ramesh*, *Daniel M. Ziegler*, *Jeffrey Wu*, *Clemens Winter*,
    *Christopher Hesse*, *Mark Chen*, *Eric Sigler*, *Mateusz Litwin*, *Scott Gray*,
    *Benjamin Chess*, *Jack Clark*, *Christopher Berner*, *Sam McCandlish*, *Alec
    Radford*, *Ilya Sutskever*, *Dario Amodei*, 2020, *Language Models are Few-Shot
    Learners*: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *SuperGLUE: A Stickier
    Benchmark for General-Purpose Language Understanding Systems*: [https://w4ngatang.github.io/static/papers/superglue.pdf](https://w4ngatang.github.io/static/papers/superglue.pdf)'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alex Wang*, *Yada Pruksachatkun*, *Nikita Nangia*, *Amanpreet Singh*, *Julian
    Michael*, *Felix Hill*, *Omer Levy*, *Samuel R. Bowman*, 2019, *GLUE: A Multi-Task
    Benchmark and Analysis Platform for Natural Language Understanding*: [https://arxiv.org/pdf/1804.07461.pdf](https://arxiv.org/pdf/1804.07461.pdf)'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI GPT-2 GitHub Repository: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2)'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'N. Shepperd’s GitHub Repository: [https://github.com/nshepperd/gpt-2](https://github.com/nshepperd/gpt-2)'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Common Crawl data: [https://commoncrawl.org/big-picture/](https://commoncrawl.org/big-picture/)'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-473
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
