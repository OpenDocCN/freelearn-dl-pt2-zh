- en: '*Chapter 9*:Cross-Lingual and Multilingual Language Modeling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：跨语言和多语言语言建模'
- en: Up to this point, you have learned a lot about transformer-based architectures,
    from encoder-only models to decoder-only models, from efficient transformers to
    long-context transformers. You also learned about semantic text representation
    based on a Siamese network. However, we discussed all these models in terms of
    monolingual problems. We assumed that these models just understand a single language
    and are not capable of having a general understanding of text, regardless of the
    language itself. In fact, some of these models have multilingual variants; **Multilingual
    Bidirectional Encoder Representations from Transformers** (**mBERT**), **Multilingual
    Text-to-Text Transfer Transformer** (**mT5**), and **Multilingual Bidirectional
    and Auto-Regressive Transformer** (**mBART**), to name but a few. On the other
    hand, some models are specifically designed for multilingual purposes trained
    with cross-lingual objectives. For example, **Cross-lingual Language Model** (**XLM**)
    is such a method, and this will be described in detail in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了很多关于基于transformer的架构的知识，从仅编码器模型到仅解码器模型，从高效的transformers到长上下文的transformers。你还学习了基于Siamese网络的语义文本表示。然而，我们讨论了所有这些模型都是从单语问题的角度出发的。我们假设这些模型只是理解单一语言，并且无法对文本进行通用的理解，无论语言本身如何。事实上，其中一些模型具有多语言变体；**多语言双向编码器表示模型**（**mBERT**），**多语言文本到文本转换transformer**（**mT5**），以及**多语言双向和自回归transformer**（**mBART**），仅举几例。另一方面，一些模型专门设计用于多语言目的，采用跨语言目标进行训练。例如，**跨语言语言模型**（**XLM**）就是这样一种方法，本章将详细介绍。
- en: In this chapter, the concept of knowledge sharing between languages will be
    presented, and the impact of **Byte-Pair Encoding** (**BPE**) on the tokenization
    part is also another important subject to cover in order to achieve better input.
    Cross-lingual sentence similarity using the **Cross-Lingual Natural Language Inference**
    (**XNLI**) corpus will be detailed. Tasks such as cross-lingual classification
    and utilization of cross-lingual sentence representation for training on one language
    and testing on another one will be presented by concrete examples of real-life
    problems in **Natural Language Processing** (**NLP**), such as multilingual intent
    classification.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将介绍语言之间的知识共享概念，以及**Byte-Pair编码**（**BPE**）对分词部分的影响，也是另一个重要的主题，为了实现更好的输入。将详细说明使用**跨语言自然语言推理**（**XNLI**）语料库进行跨语言句子相似性的任务。将通过实际的现实问题的具体例子，如多语言意图分类等**自然语言处理**（**NLP**）中的跨语言分类和跨语言句子表示利用另一种语言进行训练和测试的任务。
- en: 'In short, you will learn the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你将在本章学习以下主题：
- en: Translation language modeling and cross-lingual knowledge sharing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译语言建模和跨语言知识共享
- en: XLM and mBERT
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLM 和 mBERT
- en: Cross-lingual similarity tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨语言相似性任务
- en: Cross-lingual classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨语言分类
- en: Cross-lingual zero-shot learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨语言零-shot学习
- en: Fundamental limitations of multilingual models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语言模型的基本局限性
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code for this chapter is found in the repo at [https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH09](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH09),
    which is in the GitHub repository for this book. We will be using Jupyter Notebook
    to run our coding exercises that require Python 3.6.0+, and the following packages
    will need to be installed:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码在该书的GitHub仓库中的[https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH09](https://github.com/PacktPublishing/Mastering-Transformers/tree/main/CH09)下找到。我们将使用Jupyter
    Notebook来运行我们的编码练习，需要Python 3.6.0+，并且需要安装以下软件包：
- en: '`tensorflow`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow`'
- en: '`pytorch`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`transformers >=4.00`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers >=4.00`'
- en: '`datasets`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets`'
- en: '`sentence-transformers`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentence-transformers`'
- en: '`umap-learn`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`umap-learn`'
- en: '`openpyxl`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openpyxl`'
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看《代码实战》视频：
- en: '[https://bit.ly/3zASz7M](https://bit.ly/3zASz7M)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3zASz7M](https://bit.ly/3zASz7M)'
- en: Translation language modeling and cross-lingual knowledge sharing
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 翻译语言建模和跨语言知识共享
- en: 'So far, you have learned about **Masked Language Modeling** (**MLM**) as a
    cloze task. However, language modeling using neural networks is divided into three
    categories based on the approach itself and its practical usage, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，你已经了解了**遮罩语言建模**（**MLM**）作为填空任务。然而，基于神经网络的语言建模根据方法本身和其实际用途分为三类，如下所示：
- en: MLM
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLM
- en: '**Causal Language Modeling** (**CLM**)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因果语言建模**（**CLM**）'
- en: '**Translation Language Modeling** (**TLM**)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译语言建模**（**TLM**）'
- en: It is also important to note that there are other pre-training approaches such
    as **Next Sentence Prediction** (**NSP**) and **Sentence Order Prediction** (**SOP**)
    too, but we just considered token-based language modeling. These three are the
    main approaches that are used in the literature. **MLM**, described and detailed
    in previous chapters, is a very close concept to a cloze task in language learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 也很重要的是，还有其他预训练方法，例如**下一句预测**（**NSP**）和**句子顺序预测**（**SOP**），但我们只考虑了基于令牌的语言建模。这三种方法是文献中使用的主要方法。如前几章中描述和详细介绍的**MLM**，与语言学习中的填空任务非常接近。
- en: '**CLM** is defined by predicting the next token, which is followed by some
    previous tokens. For example, if you see the following context, you can easily
    predict the next token:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**CLM**的定义是预测下一个令牌，后面跟着一些前面的令牌。例如，如果你看到以下上下文，你可以轻松预测下一个令牌：'
- en: '*<s> Transformers changed the natural language …*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*<s> 变压器改变了自然语言...*'
- en: 'As you see, only the last token is masked, and the previous tokens are given
    to the model to predict that last one. This token would be *processing* and if
    the context with this token is given to you again, you might end it with an *"</s>"*
    token. In order to have good training on this approach, it is required to not
    mask the first token, because the model would have just a sentence start token
    to make a sentence out of it. This sentence can be anything! Here''s an example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，只有最后一个令牌被屏蔽，前面的令牌都给了模型，让它预测最后一个。这个令牌将会是*processing*，如果再次给你这个令牌的上下文，你可能会以一个*"</s>"*令牌结束。为了在这种方法上进行良好的训练，需要不屏蔽第一个令牌，因为模型只有一个句子的开始令牌来组成一个句子。这个句子可以是任何东西！下面是一个例子：
- en: '*<s> …*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*<s> ...*'
- en: 'What would you predict out of this? It can be literally anything. To have better
    training and better results, it is required to give at least the first token,
    such as this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你会预测出什么？可以是任何东西。为了更好的训练和更好的结果，需要至少给出第一个令牌，如下所示：
- en: '*<s> Transformers …*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*<s> 变压器...*'
- en: And the model is required to predict the *change*; after giving it *Transformers
    changed ...* it is required to predict *the*, and so on. This approach is very
    similar to N-grams and `P(wn|wn-1, wn-2 ,…,w0)` where `wn` is the token to be
    predicted and the rest is the tokens before it. The token with the maximum probability
    is the predicted one.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 而模型需要预测*变化*；在给出*变压器改变...*后，需要预测*the*，依此类推。这种方法非常类似于N-grams和`P(wn|wn-1, wn-2
    ,…,w0)`，其中`wn`是要预测的令牌，而其余令牌是它前面的令牌。具有最大概率的令牌是预测的令牌。
- en: These are the objectives used for monolingual models. So, what can be done for
    cross-lingual models? The answer is **TLM**, which is very similar to MLM, with
    a few changes. Instead of giving a sentence from a single language, a sentence
    pair is given to a model in different languages, separated by a special token.
    The model is required to predict the masked tokens, which are randomly masked
    in any of these languages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是用于单语模型的目标。那么，对于跨语言模型可以做些什么？答案是**TLM**，它与MLM非常相似，但有一些变化。不是给定一个单一语言的句子，而是给模型一个包含不同语言的句子对，用一个特殊的令牌分隔开。模型需要预测这些语言中任意一个中的随机屏蔽的令牌。
- en: 'The following sentence pair is an example of such a task:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的句子对就是这样一个任务的例子：
- en: '![Figure 9.1 – Cross-lingual relation example between Turkish and English ](img/B17123_09_001.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 - 土耳其和英语之间跨语言关系示例](img/B17123_09_001.jpg)'
- en: Figure 9.1 – Cross-lingual relation example between Turkish and English
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 - 土耳其和英语之间的跨语言关系示例
- en: Given these two masked sentences, the model is required to predict the missing
    tokens. In this task, on some occasions, the model has access to the tokens (for
    example, **doğal** and **language** respectively in the sentence pair in *Figure
    9.1*) that are missing from one of the languages in the pair.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这两个屏蔽的句子，模型需要预测缺失的令牌。在这个任务中，有些场合下，模型可以访问到其中一个语言中缺失的令牌（例如，在*图9.1*中的句子对中分别为**doğal**和**language**）。
- en: 'As another example, you can see the same pair from Persian and Turkish sentences.
    In the second sentence, the **değiştirdiler** token can be attended by the multiple
    tokens (one is masked) in the first sentence. In the following example, the word
    **تغییر** is missing but the meaning of **değiştirdiler** is **تغییر دادند****.**
    :'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子，你可以看到波斯语和土耳其语句子的同一对。在第二个句子中，**değiştirdiler** 标记可以被第一个句子中的多个标记（一个被掩码）关注到。在下面的例子中，单词
    **تغییر** 缺失，但 **değiştirdiler** 的含义是 **تغییر دادند****。**
- en: '![Figure 9.2 – Cross-lingual relation example between Persian and Turkish ](img/B17123_09_002.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 波斯语和土耳其语之间的跨语言关系示例](img/B17123_09_002.jpg)'
- en: Figure 9.2 – Cross-lingual relation example between Persian and Turkish
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 波斯语和土耳其语之间的跨语言关系示例
- en: Accordingly, a model can learn the mapping between these meanings. Just as with
    a translation model, our TLM must also learn these complexities between languages
    because **Machine Translation** (**MT**) is more than a token-to-token mapping.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个模型可以学习这些含义之间的映射。就像一个翻译模型一样，我们的 TLM 也必须学习这些语言之间的复杂性，因为**机器翻译**（**MT**）不仅仅是一个标记到标记的映射。
- en: XLM and mBERT
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLM 和 mBERT
- en: 'We have picked two models to explain in this section: mBERT and XLM. We selected
    these models because they correspond to the two best multilingual types as of
    writing this article. mBERT is a multilingual model trained on a different corpus
    of various languages using MLM modeling. It can operate separately for many languages.
    On the other hand, XLM is trained on different corpora using MLM, CLM, and TLM
    language modeling, and can solve cross-lingual tasks. For instance, it can measure
    the similarity of the sentences in two different languages by mapping them in
    a common vector space, which is not possible with mBERT.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中我们选择了两个模型进行解释：mBERT 和 XLM。我们选择这些模型是因为它们对应于写作本文时最好的两种多语言类型。mBERT 是一个在不同语言语料库上使用
    MLM 建模训练的多语言模型。它可以单独为许多语言进行操作。另一方面，XLM 是使用 MLM、CLM 和 TLM 语言建模在不同语料库上训练的，可以解决跨语言任务。例如，它可以通过将它们映射到共同的向量空间来测量两种不同语言句子的相似度，而这在
    mBERT 中是不可能的。
- en: mBERT
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: mBERT
- en: You are familiar with the BERT autoencoder model from [*Chapter 3*](B17123_03_Epub_AM.xhtml#_idTextAnchor050),
    *Autoencoding Language Models*, and how to train it using MLM on a specified corpus.
    Imagine a case where a wide and huge corpus is provided not from a single language,
    but from 104 languages instead. Training on such a corpus would result in a multilingual
    version of BERT. However, training on such a wide variety of languages would increase
    the model size, and this is inevitable in the case of BERT. The vocabulary size
    would be increased and, accordingly, the size of the embedding layer would be
    larger because of more vocabulary.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你对来自[*第 3 章*](B17123_03_Epub_AM.xhtml#_idTextAnchor050)的 BERT 自动编码器模型以及如何在指定语料库上使用
    MLM 进行训练已经很熟悉了。想象一下提供了一个广泛且庞大的语料库，不是来自单一语言，而是来自 104 种语言。在这样的语料库上进行训练将会产生一个多语言版本的
    BERT。然而，在如此广泛的语言上进行训练将会增加模型的大小，在 BERT 的情况下是不可避免的。词汇量会增加，因此嵌入层的大小会因为更多的词汇而变大。
- en: Compared to a monolingual pre-trained BERT, this new version is capable of handling
    multiple languages inside a single model. However, the downside for this kind
    of modeling is that this model is not capable of mapping between languages. This
    means that the model, in the pre-training phase, does not learn anything about
    these mappings between semantic meanings of the tokens from different languages.
    In order to provide cross-lingual mapping and understanding for this model, it
    is necessary to train it on some of the cross-lingual supervised tasks, such as
    those available in the `XNLI` dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与单语言预训练的 BERT 相比，这个新版本能够在一个模型内处理多种语言。然而，这种建模的缺点是，该模型不能在语言之间进行映射。这意味着在预训练阶段，模型并未学习到关于不同语言的这些标记的语义含义之间的映射。为了为这个模型提供跨语言映射和理解，有必要在一些跨语言监督任务上对其进行训练，比如`XNLI`数据集中提供的任务。
- en: 'Using this model is as easy as working with the models you have used in the
    previous chapters (see [https://huggingface.co/bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased)
    for more details). Here''s the code you''ll need to get started:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个模型与你在之前章节中使用的模型一样简单（更多详情请参见 [https://huggingface.co/bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased)）。以下是你需要开始的代码：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will then be presented, as shown in the following code snippet:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果将如下代码片段所示呈现：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, it can perform `fill-mask` for various languages.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，它可以为各种语言执行 `fill-mask`。
- en: XLM
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XLM
- en: Cross-lingual pre-training of language models, such as that shown with an XLM
    approach, is based on three different pre-training objectives. MLM, CLM, and TLM
    are used to pre-train the XLM model. The sequential order of this pre-training
    is performed using a shared BPE tokenizer between all languages. The reason that
    tokens are shared is that the shared tokens provide fewer tokens in the case of
    languages that have similar tokens or subwords, and on the other hand, these tokens
    can provide shared semantics in the pre-training process. For example, some tokens
    have remarkably similar writing and meaning across many languages, and accordingly,
    these tokens are shared by BPE for all. On the other hand, some tokens spelled
    the same in different languages can have different meanings—for example, *was*
    is shared in German and English contexts. Luckily, self-attention mechanisms help
    us to disambiguate the meaning of *was* using the surrounding context.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的跨语言预训练，例如 XLM 方法所示，基于三种不同的预训练目标。MLM、CLM 和 TLM 被用来预训练 XLM 模型。这种预训练的顺序是使用所有语言之间共享的
    BPE 分词器执行的。共享标记的原因在于，在具有相似标记或子词的语言情况下，共享标记提供了较少的标记，而另一方面，这些标记可以在预训练过程中提供共享的语义。例如，一些标记在许多语言中的书写和含义非常相似，因此这些标记对所有语言都使用
    BPE 进行共享。另一方面，一些在不同语言中拼写相同的标记可能具有不同的含义——例如，在德语和英语环境中 *was* 是共享的。幸运的是，自注意力机制帮助我们使用周围的上下文消除
    *was* 的含义歧义。
- en: Another major improvement of this cross-lingual modeling is that it is also
    pre-trained on CLM, which makes it more reasonable for inferences where sentence
    prediction or completion is required. In other words, this model has an understanding
    of the languages and is capable of completing sentences, predicting missing tokens,
    and predicting missing tokens by using the other language source.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种跨语言建模的另一个主要改进是它还在 CLM 上进行了预训练，这使得它在需要句子预测或完成时更加合理。换句话说，这个模型对语言有一定的理解，能够完成句子、预测缺失的标记，并利用其他语言源预测缺失的标记。
- en: 'The following diagram shows the overall structure of cross-lingual modeling.
    You can read more at [https://arxiv.org/pdf/1901.07291.pdf](https://arxiv.org/pdf/1901.07291.pdf):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了跨语言建模的整体结构。你可以在 [https://arxiv.org/pdf/1901.07291.pdf](https://arxiv.org/pdf/1901.07291.pdf)
    上阅读更多内容。
- en: '![Figure 9.3 – MLM and TLM pre-training for cross-lingual modeling ](img/B17123_09_003.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – MLM 和 TLM 跨语言建模的预训练](img/B17123_09_003.jpg)'
- en: Figure 9.3 – MLM and TLM pre-training for cross-lingual modeling
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – MLM 和 TLM 跨语言建模的预训练
- en: 'A newer version of the XLM model is also released as **XLM-R**, which has minor
    changes in the training and corpus used. XLM-R is identical to the XLM model but
    is trained on more languages and a much bigger corpus. The **CommonCrawl** and
    **Wikipedia** corpus is aggregated, and the XLM-R is trained for MLM on it. However,
    the XNLI dataset is also used for TLM. The following diagram shows the amount
    of data used by XLM-R pre-training:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: XLM 模型的更新版本也被发布为**XLM-R**，其在训练和使用的语料库中有轻微的更改。XLM-R 与 XLM 模型相同，但在更多的语言和更大的语料库上进行了训练。CommonCrawl
    和 Wikipedia 语料库被聚合，XLM-R 在其中进行 MLM 的训练。然而，XNLI 数据集也用于 TLM。下图显示了 XLM-R 预训练使用的数据量：
- en: '![Figure 9.4 – Amount of data in gigabytes (GB) (log-scale) ](img/B17123_09_004.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 数据量（GB）（对数刻度）](img/B17123_09_004.jpg)'
- en: Figure 9.4 – Amount of data in gigabytes (GB) (log-scale)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 数据量（GB）（对数刻度）
- en: There are many upsides and downsides when adding new languages for training
    data— for example, adding new languages may not always improve the overall model
    of **Natural Language Inference** (**NLI**). The **XNLI dataset** is usually used
    for multilingual and cross-lingual NLI. From previous chapters, you have seen
    the **Multi-Genre NLI** (**MNLI**) dataset for English; the XNLI dataset is almost
    identical to it but has more languages, and it also has sentence pairs. However,
    training only on this task is not enough, and it will not cover TLM pre-training.
    For TLM pre-training, much broader datasets such as the parallel corpus of **OPUS**
    (short for **Open Source Parallel Corpus**) are used. This dataset contains subtitles
    from different languages, aligned and cleaned, with the translations provided
    by many software sources such as Ubuntu, and so on.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加新语言进行训练数据时，有许多优点和缺点——例如，添加新语言并不总是会提高**自然语言推理**（**NLI**）的整体模型。**XNLI 数据集**通常用于多语言和跨语言
    NLI。从前面的章节中，您已经看到了**多样化体裁 NLI**（**MNLI**）数据集用于英语；XNLI 数据集与之几乎相同，但具有更多的语言，而且还有句子对。然而，仅在此任务上进行训练是不够的，它不会涵盖
    TLM 预训练。对于 TLM 预训练，会使用更广泛的数据集，例如**OPUS**（**开源平行语料库**的缩写）的平行语料库。该数据集包含来自不同语言的字幕，经过对齐和清理，由许多软件来源提供的翻译，如
    Ubuntu 等。
- en: 'The following screenshot shows OPUS ([https://opus.nlpl.eu/trac/](https://opus.nlpl.eu/trac/))
    and its components for searching and getting information about the dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 OPUS ([https://opus.nlpl.eu/trac/](https://opus.nlpl.eu/trac/)) 及其用于搜索和获取数据集信息的组件：
- en: '![Figure 9.5 – OPUS ](img/B17123_09_005(new).jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – OPUS](img/B17123_09_005(new).jpg)'
- en: Figure 9.5 – OPUS
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – OPUS
- en: 'The steps for using cross-lingual models are described here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 描述使用跨语言模型的步骤如下：
- en: 'Simple changes to the previous code can show you how XLM-R performs mask filling.
    First, you must change the model, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对先前的代码进行简单更改，您就可以看到 XLM-R 如何执行掩码填充。首先，您必须更改模型，如下所示：
- en: '[PRE2]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Afterward, you need to change the mask token from `[MASK]` to `<mask>`, which
    is a special token for XLM-R (or simply call `tokenizer.mask_token`). Here''s
    the code to accomplish this:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，您需要将掩码标记从`[MASK]`更改为`<mask>`，这是 XLM-R 的特殊标记（或简称为`tokenizer.mask_token`）。以下是完成此操作的代码：
- en: '[PRE3]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, you can run the same code, as follows:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以运行相同的代码，如下所示：
- en: '[PRE4]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The results will appear, like so:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将如此显示：
- en: '[PRE5]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'But as you see from the Turkish and Persian examples, the model still made
    mistakes; for example, in the Persian text, it just added `ی`, and in the Turkish
    version, it added `,`. For the English sentence, it added `human`, which is not
    what was expected. The sentences are not wrong, but not what we expected. However,
    this time, we have a cross-lingual model that is trained using TLM; so, let''s
    use it by concatenating two sentences and giving the model some extra hints. Here
    we go:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但从土耳其语和波斯语的例子中可以看出，模型仍然犯了错误；例如，在波斯文本中，它只是添加了`ی`，在土耳其版本中，它添加了`,`。对于英文句子，它添加了`human`，这不是我们所期望的。这些句子并没有错，但不是我们所期望的。然而，这一次，我们有一个使用
    TLM 训练的跨语言模型；所以，让我们通过连接两个句子并给模型一些额外的提示来使用它。我们开始吧：
- en: '[PRE6]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The results will be shown, as follows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE7]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'That''s it! The model has now made the right choice. Let''s play with it a
    bit more and see how it performs, as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就这样！模型现在已经做出了正确的选择。让我们再玩一会儿，看看它的表现如何，如下所示：
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is the result:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Well done! So far, you have learned about multilingual and cross-lingual models
    such as mBERT and XLM. In the next section, you will learn how to use such models
    for multilingual text similarity. You will also see some use cases, such as multilingual
    plagiarism detection.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！到目前为止，您已经了解了诸如 mBERT 和 XLM 等多语言和跨语言模型。在下一节中，您将学习如何使用这些模型进行多语言文本相似性。您还将看到一些用例，例如多语言抄袭检测。
- en: Cross-lingual similarity tasks
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨语言相似性任务
- en: Cross-lingual models are capable of representing text in a unified form, where
    sentences are from different languages but those with close meaning are mapped
    to similar vectors in vector space. XLM-R, as was detailed in the previous section,
    is one of the successful models in this scope. Now, let's look at some applications
    on this.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言模型能够以统一的形式表示文本，其中句子来自不同的语言，但意思相近的句子在向量空间中被映射到相似的向量上。XLM-R，正如前一节详细介绍的那样，是这个领域中成功的模型之一。现在，让我们看一些在这方面的应用。
- en: Cross-lingual text similarity
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨语言文本相似性
- en: 'In the following example, you will see how it is possible to use a cross-lingual
    language model pre-trained on the XNLI dataset to find similar texts from different
    languages. A use-case scenario is where a plagiarism detection system is required
    for this task. We will use sentences from the Azerbaijani language and see whether
    XLM-R finds similar sentences from English—if there are any. The sentences from
    both languages are identical. Here are the steps to take:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，您将看到如何使用在 XNLI 数据集上预训练的跨语言语言模型来从不同语言中找到相似的文本。一个使用情景是需要为此任务提供抄袭检测系统。我们将使用来自阿塞拜疆语的句子，并查看
    XLM-R 是否能找到相似的英语句子——如果有的话。两种语言的句子是相同的。下面是要执行的步骤：
- en: 'First, you need to load a model for this task, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要为此任务加载一个模型，如下所示：
- en: '[PRE10]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Afterward, we assume that we have sentences ready in the form of two separate
    lists, as illustrated in the following code snippet:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，我们假设已经准备好两个单独列表形式的句子，如下代码段所示：
- en: '[PRE11]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And the next step is to represent these sentences in vector space by using
    the XLM-R model. You can do this by simply using the `encode` function of the
    model, as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的步骤是利用 XLM-R 模型将这些句子表示为向量空间。您只需简单地使用模型的`encode`函数即可实现这一点，如下所示：
- en: '[PRE12]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At the final step, we will search for semantically similar sentences of the
    first language on the other language''s representations, as follows:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步，我们将在另一种语言表示中搜索第一种语言的语义相似句子，如下所示：
- en: '[PRE13]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In order to see a clear form of these results, you can use a pandas DataFrame,
    as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了看到这些结果的清晰形式，您可以使用 pandas DataFrame，如下所示：
- en: '[PRE14]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And you will see the results with their matching score, as follows:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，您将看到结果及其匹配分数，如下所示：
- en: '![Figure 9.6 – Plagiarism detection results (XLM-R) ](img/B17123_09_006.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 抄袭检测结果（XLM-R）](img/B17123_09_006.jpg)'
- en: Figure 9.6 – Plagiarism detection results (XLM-R)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 抄袭检测结果（XLM-R）
- en: The model made mistakes in one case (row number *4*) if we accept the maximum
    scored sentence to be paraphrased or translated, but it is useful to have a threshold
    and accept values higher than it. We will show more comprehensive experimentation
    in the following sections.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们接受最高分句子被改写或翻译的话，在一种情况下模型会错（第 *4* 行），但是有一个阈值并接受比它高的值是有用的。我们将在接下来的章节中展示更全面的实验。
- en: On the other hand, there are alternative bi-encoders available too. Such approaches
    provide a pair encoding of two sentences and classify the result to train the
    model. In such cases, **Language-Agnostic BERT Sentence Embedding** (**LaBSE**)
    may be a good choice too, and it is available in the **sentence-transformers**
    library and in **TensorFlow Hub** too. LaBSE is a dual encoder based on Transformers,
    which is similar to Sentence-BERT, where two encoders that have the same parameters
    are combined with a loss function based on the dual similarity of two sentences.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有可用的双编码器。这些方法提供了两个句子的对编码，并对结果进行分类以训练模型。在这种情况下，**语言无关 BERT 句子嵌入**（**LaBSE**）也可能是一个不错的选择，并且在**sentence-transformers**库和**TensorFlow
    Hub**中也有提供。LaBSE 是基于 Transformers 的双编码器，类似于 Sentence-BERT，其中两个具有相同参数的编码器与基于双语句子相似性的损失函数相结合。
- en: 'Using the same example, you can change the model to LaBSE in a very simple
    way and rerun the previous code (*Step 1*), as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的示例，您可以很简单地将模型更改为 LaBSE，并重新运行之前的代码（*步骤 1*），如下所示：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下面的屏幕截图中：
- en: '![Figure 9.7 – Plagiarism detection results (LaBSE) ](img/B17123_09_007.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 抄袭检测结果（LaBSE）](img/B17123_09_007.jpg)'
- en: Figure 9.7 – Plagiarism detection results (LaBSE)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 抄袭检测结果（LaBSE）
- en: As you see, LaBSE performs better in this case, and the result in row number
    *4* is correct this time. LaBSE authors claim that it works very well in finding
    translations of sentences, but it is not so good at finding sentences that are
    not completely identical. For this purpose, it is a very useful tool for finding
    plagiarism in cases where a translation is used to steal intellectual material.
    However, there are many other factors that change the results too—for example,
    the resource size for the pre-trained model in each language and the nature of
    the language pairs is also important. For a reasonable comparison, we need a more
    comprehensive experiment, and we should consider many factors.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，LaBSE 在这种情况下表现更好，第 *4* 行的结果这次是正确的。LaBSE 的作者声称它在查找句子的翻译方面效果非常好，但在查找不完全相同的句子方面则不那么好。对于在翻译用于窃取知识产权材料的情况下查找抄袭，它是一个非常有用的工具。然而，还有许多其他因素会改变结果—例如，每种语言的预训练模型的资源大小和语言对的性质也很重要。为了进行合理的比较，我们需要进行更全面的实验，并考虑许多因素。
- en: Visualizing cross-lingual textual similarity
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化跨语言文本相似性
- en: 'Now, we will measure and visualize the degree of textual similarity between
    two sentences, one of which is a translation of the other. **Tatoeba** is a free
    collection of such sentences and translations, and it is part of the XTREME benchmark.
    The community aims to get high-quality sentence translation with the support of
    many participants. We''ll now take the following steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将测量和可视化两个句子之间的文本相似度程度，其中一个句子是另一个的翻译。**Tatoeba**是这样的句子和翻译的免费集合，它是 XTREME
    基准测试的一部分。社区的目标是在许多参与者的支持下获得高质量的句子翻译。我们现在将采取以下步骤：
- en: 'We will get Russian and English sentences out of this collection. Make sure
    the following libraries are installed before you start working:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从这个集合中获取俄语和英语句子。在开始工作之前，请确保安装了以下库：
- en: '[PRE16]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Load the sentence pairs, as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载句子对，如下所示：
- en: '[PRE17]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s look at the output, as follows:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们来看一下输出，如下所示：
- en: '![Figure 9.8 – Russian-English sentence pairs ](img/B17123_09_008.jpg)'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 9.8 – 俄语-英语句子对](img/B17123_09_008.jpg)'
- en: Figure 9.8 – Russian-English sentence pairs
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.8 – 俄语-英语句子对
- en: 'First, we will take the first *K=30* sentence pairs for visualization, and
    later, we will run an experiment for the entire set. Now, we will encode them
    with sentence transformers that we already used for the previous example. Here
    is the execution of the code:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将取前 *K=30* 个句子对进行可视化，稍后我们将对整个集合运行实验。现在，我们将使用我们已经在上一个示例中使用过的句子转换器对它们进行编码。以下是代码的执行：
- en: '[PRE18]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We now have 60 vectors of length 768\. We will reduce the dimensionality to
    2 with `id 12`) insist on not getting close.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了长度为 768 的 60 个向量。我们将用 `id 12`）坚持不靠近。
- en: 'For a comprehensive analysis, let''s now measure the entire dataset. We encode
    all of the source and target sentences—1K pairs—as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了全面分析，现在让我们测量整个数据集。我们将对所有源和目标句子—1K 对—进行如下编码：
- en: '[PRE19]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We calculate the cosine similarity between all pairs, save them in the `sims`
    variable, and plot a histogram, as follows:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算所有对之间的余弦相似度，将它们保存在 `sims` 变量中，并绘制一个直方图，如下所示：
- en: '[PRE20]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the output:'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 9.10 – Similarity histogram for the English and Russian sentence pairs
    ](img/B17123_09_010.jpg)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 9.10 – 英语和俄语句子对的相似性直方图](img/B17123_09_010.jpg)'
- en: Figure 9.10 – Similarity histogram for the English and Russian sentence pairs
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.10 – 英语和俄语句子对的相似性直方图
- en: 'As can be seen, the scores are very close to 1\. This is what we expect from
    a good cross-lingual model. The mean and standard deviation of all similarity
    measurements also support the cross-lingual model performance, as follows:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以看到，得分非常接近 1。这是我们对一个良好的跨语言模型的期望。所有相似性测量的平均值和标准差也支持跨语言模型的性能，如下所示：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You can run the same code yourself for languages other than Russian. As you
    run it with `fra`), `tam`), and so on, you will get the following resulting table.
    The table indicates that you will see in your experiment that the model works
    well in many languages but fails in others, such as **Afrikaans** or **Tamil**:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以自行运行相同的代码以用于除俄语外的其他语言。当您将其与 `fra`）、`tam`）等一起运行时，您将得到以下结果表。该表表明，在您的实验中，该模型在许多语言中表现良好，但在其他语言中失败，例如**南非荷兰语**或**泰米尔语**：
- en: '![Table 1 – Cross-lingual model performance for other languages  ](img/B17123_09_Table_1.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![表 1 – 其他语言的跨语言模型性能](img/B17123_09_Table_1.jpg)'
- en: Table 1 – Cross-lingual model performance for other languages
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1 - 其他语言的跨语言模型性能
- en: In this section, we applied cross-lingual models to measure similarity between
    different languages. In the next section, we'll make use of cross-lingual models
    in a supervised way.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将跨语言模型应用于衡量不同语言之间的相似性。在下一节中，我们将以监督的方式利用跨语言模型。
- en: Cross-lingual classification
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨语言分类
- en: So far, you have learned that cross-lingual models are capable of understanding
    different languages in semantic vector space where similar sentences, regardless
    of their language, are close in terms of vector distance. But how it is possible
    to use this capability in use cases where we have few samples available?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解到跨语言模型能够在语义向量空间中理解不同语言，其中相似的句子无论其语言如何，在向量距离上都是接近的。但是在我们只有少量样本可用的用例中，如何利用这种能力呢？
- en: For example, you are trying to develop an intent classification for a chatbot
    in which there are few samples or no samples available for the second language;
    but for the first language—let's say English—you do have enough samples. In such
    cases, it is possible to freeze the cross-lingual model itself and just train
    a classifier for the task. A trained classifier can be tested on a second language
    instead of the language it is trained on.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你正在尝试为一个聊天机器人开发意图分类，其中第二语言只有少量或没有样本可用；但是对于第一语言——比如英语——你有足够的样本。在这种情况下，可以冻结跨语言模型本身，只需为任务训练一个分类器。训练好的分类器可以在第二语言上进行测试，而不是在其训练语言上进行测试。
- en: In this section, you will learn how to train a cross-lingual model in English
    for text classification and test it in other languages. We have selected a very
    low-resource language known as **Khmer** ([https://en.wikipedia.org/wiki/Khmer_language](https://en.wikipedia.org/wiki/Khmer_language)),
    which is spoken by 16 million people in Cambodia, Thailand, and Vietnam. It has
    few resources on the internet, and it is hard to find good datasets to train your
    model on it. However, we have access to a good **Internet Movie Database** (**IMDb**)
    sentiment dataset of movie reviews for sentiment analysis. We will use that dataset
    to find out how our model performs on the language it is not trained on.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何在英语中训练跨语言模型进行文本分类，并在其他语言中进行测试。我们选择了一个被称为**高棉语**（[https://en.wikipedia.org/wiki/Khmer_language](https://en.wikipedia.org/wiki/Khmer_language)）的非常低资源语言，该语言在柬埔寨、泰国和越南共有
    1600 万人口使用。网络上几乎没有相关资源，很难找到用于训练模型的好数据集。然而，我们可以使用一个好的**互联网电影数据库**（**IMDb**）情感数据集，其中包含了电影评论的情感分析数据。我们将使用该数据集来评估我们的模型在其未经训练的语言上的表现。
- en: 'The following diagram nicely depicts the kind of flow we will follow. The model
    is trained with train data on the left, and this model is applied to the test
    sets on the right. Please notice that MT and sentence-encoder mappings play a
    significant role in the flow:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下图清晰地展示了我们将要遵循的流程。模型在左侧使用训练数据进行训练，然后将该模型应用于右侧的测试集。请注意，MT 和句子编码器映射在流程中起着重要作用：
- en: '![Figure 9.11 – Flow of cross-lingual classification ](img/B17123_09_011.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 - 跨语言分类流程 ](img/B17123_09_011.jpg)'
- en: Figure 9.11 – Flow of cross-lingual classification
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 - 跨语言分类流程
- en: 'The required steps to load and train a model for cross-lingual testing are
    outlined here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 加载和训练用于跨语言测试的模型所需的步骤概述如下：
- en: 'The first step is to load the dataset, as follows:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是加载数据集，如下所示：
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You need to shuffle the dataset to shuffle the samples before using them, as
    follows:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用数据之前，你需要对数据集进行洗牌以打乱样本顺序，如下所示：
- en: '[PRE23]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to make a good test split out of this dataset, which is in
    the Khmer language. In order to do so, you can use a translation service such
    as Google Translate. First, you should save this dataset in Excel format, as follows:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是从该数据集中制作一个良好的测试分割，该数据集使用的是高棉语。为了做到这一点，你可以使用诸如谷歌翻译之类的翻译服务。首先，你应该将该数据集保存为
    Excel 格式，如下所示：
- en: '[PRE24]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Afterward, you can upload it to Google Translate and get the Khmer translation
    of this dataset ([https://translate.google.com/?sl=en&tl=km&op=docs](https://translate.google.com/?sl=en&tl=km&op=docs)),
    as illustrated in the following screenshot:![Figure 9.12 – Google document translator
    ](img/B17123_09_012.jpg)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后，你可以将其上传到谷歌翻译，并获取该数据集的高棉语翻译（[https://translate.google.com/?sl=en&tl=km&op=docs](https://translate.google.com/?sl=en&tl=km&op=docs)），如下面的截图所示：![图
    9.12 - 谷歌文档翻译器 ](img/B17123_09_012.jpg)
- en: Figure 9.12 – Google document translator
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.12 - 谷歌文档翻译器
- en: 'After selecting and uploading the document, it will give you the translated
    version in Khmer, which you can copy and paste into an Excel file. It is also
    required to save it in Excel format again. The result would be an Excel document
    that is a translation of the original spam/ham English dataset. You can read it
    using pandas by running the following command:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并上传文档后，它将为您提供高棉语的翻译版本，您可以将其复制并粘贴到Excel文件中。还需要将其再次保存为Excel格式。结果将是一个Excel文档，是原始垃圾邮件/正常邮件英文数据集的翻译。您可以通过运行以下命令使用pandas读取它：
- en: '[PRE25]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And the result will be seen, as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '![Figure 9.13 – IMDB dataset in KHMER language. ](img/B17123_09_013.jpg)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图9.13 – IMDB数据集高棉语版。](img/B17123_09_013.jpg)'
- en: Figure 9.13 – IMDB dataset in KHMER language.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.13 – IMDb数据集高棉语版。
- en: 'However, it is required to get only text, so you should use the following code:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是，只需要文本，因此您应该使用以下代码：
- en: '[PRE26]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now that you have text for both languages and the labels, you can split the
    train and test validations, as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您有了两种语言和标签的文本，您可以按如下方式拆分训练和测试验证：
- en: '[PRE27]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next step is to provide the representation of these sentences using the
    XLM-R cross-lingual model. First, you should load the model, as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用XLM-R跨语言模型提供这些句子的表征。首先，您应该加载模型，如下所示：
- en: '[PRE28]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'And now, you can get the representations, like this:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以像这样获取表征：
- en: '[PRE29]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'But you should not forget to convert the labels to `numpy` format because TensorFlow
    and Keras only deal with `numpy` arrays when using the `fit` function of the Keras
    models. Here''s how to do it:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但是，您不应忘记将标签转换为`numpy`格式，因为当使用Keras模型的`fit`函数时，TensorFlow和Keras只处理`numpy`数组。以下是如何执行此操作的方式：
- en: '[PRE30]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that everything is ready, let''s make a very simple model for classifying
    the representations, as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，让我们为分类表征制作一个非常简单的模型，如下所示：
- en: '[PRE31]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can fit your model using the following function:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下函数拟合您的模型：
- en: '[PRE32]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And the results for `20` epochs of training are shown, as follows:![Figure 9.14
    – Training results on the English version of the IMDb dataset ](img/B17123_09_014.jpg)
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且显示了`20`个时期的训练结果，如下所示：![图9.14 – IMDb数据集英文版的训练结果](img/B17123_09_014.jpg)
- en: Figure 9.14 – Training results on the English version of the IMDb dataset
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.14 – IMDb数据集英文版的训练结果
- en: 'As you have seen, we used an English test set to see the model performance
    across epochs, and it is reported as follows in the final epoch:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如您所见，我们使用了英文测试集来查看模型在不同时期的性能，并且在最终时期报告如下：
- en: '[PRE33]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we have trained our model and tested it on English, let''s test it on the
    Khmer test set, as our model never saw any of the samples either in English or
    in Khmer. Here''s the code to accomplish this:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的模型并在英文上进行了测试，让我们在高棉语测试集上进行测试，因为我们的模型从未见过任何英文或高棉语的样本。以下是完成此操作的代码：
- en: '[PRE34]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here are the results:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE35]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So far, you have learned how it is possible to leverage the capabilities of
    cross-lingual models in low-resource languages. It makes a huge impact and difference
    when you can use such a capability in cases where there are very few samples or
    no samples to train the model on. In the next section, you will learn how it is
    possible to use zero-shot learning where there are no samples available, even
    for high-resource languages such as English.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学会了如何利用跨语言模型在资源稀缺的语言中发挥作用。当您可以在很少的样本或没有样本的情况下使用这种能力时，它会产生巨大的影响和差异。在接下来的部分中，您将学习如何在没有样本可用的情况下使用零样本学习，即使是对于像英语这样的高资源语言也是如此。
- en: Cross-lingual zero-shot learning
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨语言零样本学习
- en: In previous sections, you learned how to perform zero-shot text classification
    using monolingual models. Using XLM-R for multilingual and cross-lingual zero-shot
    classification is identical to the approach and code used previously, so we will
    use **mT5** here.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的部分中，您已经学会了如何使用单语模型执行零样本文本分类。使用XLM-R进行多语言和跨语言零样本分类与先前使用的方法和代码相同，因此我们将在这里使用**mT5**。
- en: mT5, which is a massively multilingual pre-trained language model, is based
    on the encoder-decoder architecture of Transformers and is also identical to **T5**.
    T5 is pre-trained on English and mT5 is trained on 101 languages from **Multilingual
    Common Crawl** (**mC4**).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: mT5是基于Transformers的编码器-解码器架构的大规模多语言预训练语言模型，也与**T5**相同。 T5在英语上预训练，而mT5在来自**多语言通用爬虫**（**mC4**）的101种语言上进行了训练。
- en: The fine-tuned version of mT5 on the XNLI dataset is available from the HuggingFace
    repository ([https://huggingface.co/alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli](https://huggingface.co/alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在HuggingFace存储库（[https://huggingface.co/alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli](https://huggingface.co/alan-turing-institute/mt5-large-finetuned-mnli-xtreme-xnli)）上可以找到经过XNLI数据集精调的mT5模型。
- en: 'The T5 model and its variant, mT5, is a completely text-to-text model, which
    means it will produce text for any task it is given, even if the task is classification
    or NLI. So, in the case of inferring this model, extra steps are required. We''ll
    take the following steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: T5模型及其变体mT5是完全的文本-to-文本模型，这意味着它将为它所给的任何任务生成文本，即使任务是分类或NLI。因此，在推断这个模型的情况下，需要额外的步骤。我们将采取以下步骤：
- en: 'The first step is to load the model and the tokenizer, as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是加载模型和分词器，如下所示：
- en: '[PRE36]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the next step, let''s provide samples to be used in zero-shot classification—a
    sentence and labels, as follows:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们提供用于零样本分类的样本 - 一个句子和标签，如下所示：
- en: '[PRE37]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As you see, the sequence itself is in German (`"Who will you vote for in the
    next election?"`) but the labels are written in Turkish (`"spor"`, `"ekonomi"`,
    `"politika"`). The `hypothesis_template` says: `"this example is ..."` in German.'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您所看到的，序列本身是德语的（`"在下次选举中你会投票给谁？"`），但标签是用土耳其语写的（`"spor"`，`"ekonomi"`，`"politika"`）。`hypothesis_template`说：`"这个例子是
    ..."`，用德语表达。
- en: 'The next step is to set the label `IDs`) of the entailment, `CONTRADICTS`,
    and `NEUTRAL`, which will be used later in inferring the generated results. Here''s
    the code you''ll need to do this:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是设置蕴涵、`CONTRADICTS`和`NEUTRAL`的标签`IDs`，这将在将来推断生成的结果时使用。以下是您需要执行此操作的代码：
- en: '[PRE38]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'As you''ll recall, the T5 model uses prefixes to know the task that it is supposed
    to perform. The following function provides the XNLI prefix, along with the premise
    and hypothesis in the proper format:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您会记得，T5模型使用前缀来知道它应该执行的任务。以下函数提供了XNLI前缀，以及用适当格式表示的前提和假设：
- en: '[PRE39]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In the next step, for each label, a sentence will be generated, as illustrated
    in the following code snippet:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，对于每个标签，将生成一个句子，如下面的代码片段所示：
- en: '[PRE40]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can see the resulting sequences by printing them, as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过打印它们来查看生成的序列，如下所示：
- en: '[PRE41]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: These sequences simply say that the task is XNLI-coded by `xnli:`; the premise
    sentence is `"Who will you vote for in the next election?"` (in German) and the
    hypothesis is `"this example is politics"`, `"this example is a sport"`, or `"this
    example is economy"`.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些序列简单地表明任务是由`xnli:`编码的；前提句子是`"在下次选举中你会投票给谁？"`（德语），而假设是`"这个例子是政治"`, `"这个例子是体育"`,
    或者`"这个例子是经济"`。
- en: 'In the next step, you can tokenize the sequences and give them to the model
    to generate the text according to it, as follows:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，您可以对序列进行标记化，并将它们提供给模型根据其生成文本，如下所示：
- en: '[PRE42]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The generated text actually gives scores for each token in the vocabulary,
    and what we are looking for is the entailment, contradiction, and neutral scores.
    You can get their score using their token IDs, as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的文本实际上为词汇表中的每个标记给出了分数，而我们要找的是蕴涵、矛盾和中性分数。您可以使用它们的标记`IDs`获取它们的分数，如下所示：
- en: '[PRE43]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can see these scores by printing them, like this:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过打印它们来查看这些分数，就像这样：
- en: '[PRE44]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The neutral score is not required for our purpose, and we only need contradiction
    compared to entailment. So, you can use the following code to get only these scores:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的目的，中性分数是不需要的，我们只需要与蕴涵相比较的矛盾。因此，您可以使用以下代码仅获取这些分数：
- en: '[PRE45]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now that you have these scores for each sequence of the samples, you can apply
    a `softmax` layer on it to get the probabilities, as follows:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您有了这些对样本序列的每个分数，您可以在其上应用`softmax`层来获得概率，如下所示：
- en: '[PRE46]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'To see these probabilities, you can use `print`, like this:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看这些概率，您可以使用`print`，像这样：
- en: '[PRE47]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, you can compare the entailment probability of these three samples by selecting
    them and applying a `softmax` layer over them, as follows:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以通过选择它们并在其上应用`softmax`层来比较这三个样本的蕴涵概率，如下所示：
- en: '[PRE48]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'And to see the values, use `print`, as follows:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看值，可以使用`print`，如下所示：
- en: '[PRE49]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result means the highest probability belongs to the third sequence. In
    order to see it in a better shape, use the following code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果意味着最高概率属于第三个序列。为了更好地看到它，使用以下代码：
- en: '[PRE50]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The whole process can be summarized as follows: each label is given to the
    model with the premise, and the model generates scores for each token in the vocabulary.
    We use these scores to find out how much the entailment token scores over the
    contradiction.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程可以总结如下：每个标签都赋予模型前提，并且模型为词汇表中的每个标记生成分数。我们使用这些分数来找出蕴涵标记在矛盾上的分数有多少。
- en: Fundamental limitations of multilingual models
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多语言模型的基本限制
- en: Although the multilingual and cross-lingual models are promising and will affect
    the direction of NLP work, they still have some limitations. Many recent works
    addressed these limitations. Currently, the mBERT model slightly underperforms
    in many tasks compared with its monolingual counterparts and may not be a potential
    substitute for a well-trained monolingual model, which is why monolingual models
    are still widely used.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多语言和跨语言模型前景可期，将影响自然语言处理工作的方向，但它们仍然存在一些局限性。许多最近的工作都解决了这些局限性。目前，与其单语言对应模型相比，mBERT模型在许多任务中表现略逊一筹，并且可能并不是训练良好的单语言模型的潜在替代品，这就是为什么单语言模型仍然被广泛使用的原因。
- en: Studies in the field indicate that multilingual models suffer from the so-called
    *curse of multilingualism* as they seek to appropriately represent all languages.
    Adding new languages to a multilingual model improves its performance, up to a
    certain point. However, it is also seen that adding it after this point degrades
    performance, which may be due to shared vocabulary. Compared to monolingual models,
    multilingual models are significantly more limited in terms of the parameter budget.
    They need to allocate their vocabulary to each one of more than 100 languages.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的研究表明，多语言模型遭受所谓的*多语言诅咒*，因为它们试图适当地代表所有语言。将新语言添加到多语言模型会提高其性能，直到某一点为止。然而，也有观察到，在此点之后添加语言会降低性能，这可能是由于共享的词汇表。与单语言模型相比，多语言模型在参数预算方面受到了显著的限制。它们需要将其词汇表分配给100多种语言中的每一种。
- en: The existing performance differences between mono- and multilingual models can
    be attributed to the capability of the designated tokenizer. The study *How Good
    is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models*
    (2021) by Rust et al. ([https://arxiv.org/abs/2012.15613](https://arxiv.org/abs/2012.15613))
    showed that when a dedicated language-specific tokenizer rather than a general-purpose
    one (a shared multilingual tokenizer) is attached to a multilingual model, it
    improves the performance for that language.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定的分词器的能力，可以归因于现有的单语和多语言模型之间的性能差异。Rust等人在2021年的研究*How Good is Your Tokenizer?
    On the Monolingual Performance of Multilingual Language Models*（[https://arxiv.org/abs/2012.15613](https://arxiv.org/abs/2012.15613)）表明，当在多语言模型上附加专用的语言特定分词器而不是通用分词器（共享的多语言分词器）时，它会提高该语言的性能。
- en: Some other findings indicate that it is not currently possible to represent
    all the world's languages in a single model due to an imbalance in resource distribution
    of different languages. As a solution, low-resource languages can be oversampled,
    while high-resource languages can be undersampled. Another observation is that
    knowledge transfer between two languages can be more efficient if those languages
    are close. If they are distant languages, this transfer may have little effect.
    This observation may explain why we got worse results for Afrikaans and Tamil
    languages in the previous cross-lingual sentence-pair experiment part.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他的发现表明，目前不可能通过一个单一模型来代表世界上所有的语言，原因是不同语言的资源分配不平衡。作为解决方案，可以过度采样资源稀缺语言，而对资源充足的语言进行欠采样。另一个观察结果是，两种语言之间的知识转移如果这些语言相近会更加高效。如果它们是远离的语言，这种转移可能几乎没有效果。这个观察结果可能解释了我们在先前的跨语言句对实验中为南非荷兰语和泰米尔语获得更差的结果。
- en: However, there is a lot of work on this subject, and these limitations may be
    overcome at any time. As of writing this article, the team of XML-R recently proposed
    two new models—namely, XLM-R XL and XLM-R XXL—that outperform the original XLM-R
    model by 1.8% and 2.4% average accuracies respectively on XNLI.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关于这个主题有大量的工作，并且这些局限性随时可能被克服。在撰写本文时，XML-R团队最近提出了两个新模型——XLM-R XL和XLM-R XXL，它们分别在XNLI上比原始XLM-R模型提高了1.8%和2.4%的平均准确性。
- en: Fine-tuning the performance of multilingual models
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调优多语言模型的性能
- en: 'Now, let''s check whether the fine-tuned performance of the multilingual models
    is actually worse than the monolingual models or not. As an example, let''s recall
    the example of Turkish text classification with seven classes in [*Chapter 5*](B17123_05_Epub_AM.xhtml#_idTextAnchor081),
    *Fine-Tuning Language Models for Text Classification*. In that experiment, we
    fine-tuned a Turkish-specific monolingual model and achieved a good result. We
    will repeat the same experiment, keeping everything as-is but replacing the Turkish
    monolingual model with the mBERT and XLM-R models, respectively. Here''s how we''ll
    do this:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查多语言模型的微调性能是否真的比单语言模型差。例如，让我们回想一下在[*第5章*](B17123_05_Epub_AM.xhtml#_idTextAnchor081)中的七个类别的土耳其文本分类示例，*文本分类的语言模型微调*。在那个实验中，我们对土耳其特定的单语言模型进行了微调，并取得了良好的结果。我们将重复相同的实验，保持一切不变，但将土耳其单语模型分别替换为
    mBERT 和 XLM-R 模型。我们将这样做：
- en: 'Let''s recall the codes in that example again. We had fine-tuned the `"dbmdz/bert-base-turkish-uncased"`
    model, as follows:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下那个例子中的代码。我们对`"dbmdz/bert-base-turkish-uncased"`模型进行了微调，如下所示：
- en: '[PRE51]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'With the monolingual model, we got the following performance values:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用单语言模型，我们得到了以下性能值：
- en: '![Figure 9.15 – Monolingual text classification performance (from Chapter 5,
    Fine-Tuning Language Models for Text Classification) ](img/B17123_09_015.jpg)'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图9.15 – 单语言文本分类性能（来自第5章，文本分类的语言模型微调） ](img/B17123_09_015.jpg)'
- en: Figure 9.15 – Monolingual text classification performance (from Chapter 5, Fine-Tuning
    Language Models for Text Classification)
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.15 – 单语言文本分类性能（来自第5章，文本分类的语言模型微调）
- en: 'To fine-tune with mBERT, we need to only replace the preceding model instantiation
    lines. Now, we will use the `"bert-base-multilingual-uncased"` multilingual model.
    We instantiate it like this:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用 mBERT 进行微调，我们只需替换前面的模型实例化行。现在，我们将使用`"bert-base-multilingual-uncased"`多语言模型。我们像这样实例化它：
- en: '[PRE52]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: There is not much difference in coding. When we run the experiment keeping all
    other parameters and settings the same, we get the following performance values:![Figure
    9.16 – mBERT fine-tuned performance ](img/B17123_09_016.jpg)
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码上没有太大的区别。当我们保持所有其他参数和设置完全相同运行实验时，我们得到以下性能值：![图9.16 – mBERT 微调性能 ](img/B17123_09_016.jpg)
- en: Figure 9.16 – mBERT fine-tuned performance
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.16 – mBERT 微调性能
- en: Hmm! The multilingual model underperforms compared with its monolingual counterpart
    roughly by 2.2% on all metrics.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哦！与单语言模型相比，多语言模型在所有指标上的表现差约2.2%。
- en: 'Let''s fine-tune the `"xlm-roberta-base"` XLM-R model for the same problem.
    We''ll execute the XLM-R model initialization code, as follows:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们为相同问题微调`"xlm-roberta-base"` XLM-R 模型。我们将执行以下 XLM-R 模型初始化代码：
- en: '[PRE53]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Again, we keep all other settings exactly the same. We get the following performance
    values with the XML-R model:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们保持所有其他设置完全相同。我们得到以下 XML-R 模型的性能值：
- en: '![Figure 9.17 – XLM-R fine-tuned performance ](img/B17123_09_017.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – XLM-R 微调性能 ](img/B17123_09_017.jpg)'
- en: Figure 9.17 – XLM-R fine-tuned performance
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – XLM-R 微调性能
- en: 'Not bad! The XLM model did give comparable results. The obtained results are
    quite close to the monolingual model, with a roughly 1.0% difference. Therefore,
    although monolingual results can be better than multilingual models in certain
    tasks, we can achieve promising results with multilingual models. Think of it
    this way: we may not want to train a whole monolingual model for a 1% performance
    that lasts 10 days and more. Such small performance differences may be negligible
    for us.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！XLM 模型确实给出了可比较的结果。获得的结果与单语言模型相当接近，差异约为1.0%。因此，虽然单语言结果在某些任务中可能比多语言模型更好，但我们可以通过多语言模型获得令人满意的结果。这样想吧：我们可能不想为了持续10天或更长时间的1%的性能而训练整个单语言模型。对我们来说，这种小的性能差异可能是可以忽略不计的。
- en: Summary
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about multilingual and cross-lingual language model
    pre-training and the difference between monolingual and multilingual pre-training.
    CLM and TLM were also covered, and you gained knowledge about them. You learned
    how it is possible to use cross-lingual models on various use cases, such as semantic
    search, plagiarism, and zero-shot text classification. You also learned how it
    is possible to train on a dataset from a language and test on a completely different
    language using cross-lingual models. Fine-tuning the performance of multilingual
    models was evaluated, and we concluded that some multilingual models can be a
    substitute for monolingual models, remarkably keeping performance loss to a minimum.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了多语言和跨语言语言模型预训练，以及单语言和多语言预训练之间的区别。还涵盖了CLM和TLM，并且您对它们有了了解。您了解了如何在各种用例中使用跨语言模型，比如语义搜索，抄袭和零样本文本分类。您还了解到，可以使用跨语言模型在来自一种语言的数据集上进行训练，然后在完全不同的语言上进行测试。评估了多语言模型的微调性能，并得出结论，一些多语言模型可以替代单语言模型，显著地将性能损失降至最低。
- en: In the next chapter, you will learn how to deploy transformer models for real
    problems and train them for production at an industrial scale.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习如何部署变压器模型以解决实际问题，并以工业规模进行生产训练。
- en: References
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Conneau, A., Lample, G., Rinott, R., Williams, A., Bowman, S. R., Schwenk,
    H. and Stoyanov, V. (2018). XNLI: Evaluating cross-lingual sentence representations.
    arXiv preprint arXiv:1809.05053.*'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Conneau, A., Lample, G., Rinott, R., Williams, A., Bowman, S. R., Schwenk,
    H. and Stoyanov, V. (2018). XNLI: 评估跨语言句子表示. arXiv预印本arXiv:1809.05053.*'
- en: '*Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A. and
    Raffel, C. (2020). mT5: A massively multilingual pre-trained text-to-text transformer.
    arXiv preprint arXiv:2010.11934.*'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*薛，L.，康斯坦特，N.，罗伯茨，A.，卡勒，M.，阿尔-福，R.，西达，A. and 拉菲尔，C. (2020). mT5：一个大规模多语种预训练文本到文本变压器.
    arXiv预印本arXiv:2010.11934.*'
- en: '*Lample, G. and Conneau, A. (2019). Cross-lingual language model pretraining.
    arXiv preprint arXiv:1901.07291.*'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Lample, G. and Conneau, A. (2019). 跨语言语言模型预训练. arXiv预印本arXiv:1901.07291.*'
- en: '*Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán,
    F. and Stoyanov, V. (2019). Unsupervised cross-lingual representation learning
    at scale. arXiv preprint arXiv:1911.02116.*'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán,
    F. and Stoyanov, V. (2019). 在规模上无监督的跨语言表示学习. arXiv预印本arXiv:1911.02116.*'
- en: '*Feng, F., Yang, Y., Cer, D., Arivazhagan, N. and Wang, W. (2020). Language-agnostic
    bert sentence embedding. arXiv preprint arXiv:2007.01852.*'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*冯，F.，杨，Y.，Cer，D.，阿里瓦扎甘，N.和王，W.（2020）. 无语言bert句子嵌入. arXiv预印本arXiv:2007.01852.*'
- en: '*Rust, P., Pfeiffer, J., Vulić, I., Ruder, S. and Gurevych, I. (2020). How
    Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language
    Models. arXiv preprint arXiv:2012.15613.*'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Rust, P., Pfeiffer, J., Vulić, I., Ruder, S. and Gurevych, I. (2020). 你的分词器有多好？关于多语言语言模型的单语性能.
    arXiv预印本arXiv:2012.15613.*'
- en: '*Goyal, N., Du, J., Ott, M., Anantharaman, G. and Conneau, A. (2021). Larger-Scale
    Transformers for Multilingual Masked Language Modeling. arXiv preprint arXiv:2105.00572.*'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Goyal, N., Du, J., Ott, M., Anantharaman, G. and Conneau, A. (2021). 用于多语言掩模语言建模的大规模变压器.
    arXiv预印本arXiv:2105.00572.*'
