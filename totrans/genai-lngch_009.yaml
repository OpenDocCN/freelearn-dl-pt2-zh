- en: 8 Customizing LLMs and their output
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8自定义LLMs及其输出
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Discord上加入我们的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](img/file54.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![二维码说明会自动生成](img/file54.png)'
- en: 'This chapter is about techniques and best practices to improve the reliability
    and performance of large language models (LLMs) in certain scenarios such as on
    complex reasoning and problem-solving tasks. Generally, this process of adapting
    a model for a certain task or making sure that our model output corresponds to
    what we expect is called conditioning. In this chapter, we’ll discuss fine-tuning
    and prompting as methods for conditioning.Fine-tuning involves training the pre-trained
    base model on specific tasks or datasets relevant to the desired application.
    This process allows the model to adapt and become more accurate and contextually
    aligned for the intended use case. Similarly, by providing additional input or
    context at inference time, large language models (LLMs) can generate text tailored
    to a particular tasks or style.Prompt design is highly significant for unlocking
    LLM reasoning capabilities, the potential for future advancements in models and
    prompting techniques, and these principles and techniques form a valuable toolkit
    for researchers and practitioners working with large language models. Understanding
    how LLMs generate text token-by-token helps create better reasoning prompts.Prompting
    is still an empirical art - trying variations to see what works is often needed.
    But some prompt engineering insights can transfer across models and tasks. We’ll
    discuss the tools in LangChain to enable advanced prompt engineering strategies
    like few-shot learning, dynamic example selection, and chained reasoning.Throughout
    the chapter, we’ll work on fine-tuning and prompting with LLMs, which you can
    find in the `notebooks` directory in the Github repository for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讨论改进大型语言模型（LLMs）在复杂推理和问题解决任务等特定情况下的可靠性和性能的技术和最佳实践。一般来说，使模型适应特定任务或确保模型输出与我们期望的一致的过程称为条件。在本章中，我们将讨论微调和提示作为条件的方法。微调涉及在特定任务或与所需应用相关的数据集上训练预训练基础模型。这个过程允许模型适应并在预期用例中变得更准确和上下文对齐。同样，通过在推断时提供额外的输入或上下文，大型语言模型（LLMs）可以生成适合特定任务或风格的文本。提示设计对于释放LLM的推理能力，模型和提示技术未来进展的潜力非常重要，这些原则和技术构成了研究人员和从业者使用大型语言模型的宝贵工具包。了解LLMs如何逐个标记地生成文本有助于创建更好的推理提示。提示仍然是一个经验性的艺术——通常需要尝试各种变化来看看什么有效。但一些提示工程的见解可以在模型和任务之间转移。我们将讨论LangChain中的工具，以实现高级提示工程策略，如少样本学习、动态示例选择和链式推理。在本章的整个过程中，我们将使用LLMs进行微调和提示，您可以在书的Github存储库的`notebooks`目录中找到，网址是[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)主要章节包括：
- en: Conditioning and Alignment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件和对齐
- en: Fine-Tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调
- en: Prompt Engineering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Let’s start by discussing conditioning and alignment, why it’s important, and
    how we can achieve it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先讨论条件和对齐，为什么重要，以及如何实现它。
- en: Conditioning and alignment
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件和对齐
- en: Alignment, in the context of generative AI models, refers to ensuring that the
    outputs of these models are consistent with human values, intentions, or desired
    outcomes. It involves guiding the model's behavior to be in line with what is
    considered ethical, appropriate, or relevant within a given context. The concept
    of alignment is crucial to avoid generating outputs that may be biased, harmful,
    or deviate from the intended purpose. Addressing alignment involves careful attention
    to the biases present in training data, iterative feedback loops involving human
    reviewers, refining objective functions during training/fine-tuning stages, leveraging
    user feedback, and continuous monitoring during deployment to ensure ongoing alignment.There
    are several reasons one may want to condition a large language model. The first
    is to control the content and style of the outputs. For example, conditioning
    on certain keywords or attributes like formality level can produce more relevant
    and high-quality text. Conditioning also encompasses safety measures to prevent
    the generation of malicious or harmful content. For example, avoiding generating
    misleading information, inappropriate suggestions, or potentially dangerous instructions,
    or – more generally aligning the model with certain values.The potential benefits
    of conditioning large language models are numerous. By providing more specific
    and relevant input, we can achieve outputs that are tailored to our needs. For
    instance, in a customer support chatbot, conditioning the model with user queries
    allows it to generate responses that address their concerns accurately. Conditioning
    also helps control biased or inappropriate outputs by constraining the model's
    creativity within specific boundaries.Furthermore, by conditioning large language
    models, we can make them more controllable and adaptable. We can fine-tune and
    shape their behavior according to our requirements and create AI systems that
    are reliable in specific domains such as legal advice or technical writing.However,
    there are potential downsides to consider as well. Conditioning models too heavily
    might result in overfitting, where they become excessively reliant on specific
    inputs and struggle with generating creative or diverse outputs in different contexts.
    Moreover, conditioning should be utilized responsibly since large language models
    have the tendency to amplify biases present in the training data. Care must be
    taken not to exacerbate issues related to bias or controversial topics when conditioning
    these models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式人工智能模型的背景下，"alignment" 意味着确保这些模型的输出与人类的价值观、意图或期望结果一致。它涉及指导模型的行为与在特定上下文中被认为道德、适当或相关的内容保持一致。"alignment"
    概念对于避免生成可能存在偏见、有害或偏离预期目的的输出至关重要。解决"alignment" 问题需要注意训练数据中存在的偏见，涉及人类审阅者的反馈循环，在训练/微调阶段改进客观函数，利用用户反馈，并在部署过程中持续监测以确保持续对齐。一个可能想对大型语言模型进行条件的原因有几个。第一个是控制输出的内容和风格。例如，根据某些关键词或属性如正式程度进行条件，可以产生更相关和高质量的文本。条件还包括安全措施，以防止生成恶意或有害内容。例如，避免生成误导性信息、不当建议或潜在危险指示，或者更一般地对齐模型与某些价值观。对大型语言模型进行条件的潜在好处很多。通过提供更具体和相关的输入，我们可以得到符合我们需求的输出。例如，在客服聊天机器人中，对模型进行条件，允许它生成准确解决用户问题的回应。条件还有助于通过限制模型创造性在特定边界内来控制有偏见或不当的输出。此外，通过对大型语言模型进行条件，我们可以使它们更可控和适应性更强。我们可以根据我们的需求对其进行微调和塑造行为，并创建在特定领域如法律建议或技术写作中可靠的人工智能系统。然而，也有潜在的不利因素需要考虑。过于强烈地对模型进行条件可能导致过拟合，导致模型过分依赖特定输入，在不同环境中难以产生创造性或多样化的输出。此外，应该有责任地利用条件，因为大型语言模型有放大训练数据中存在偏见的倾向。在对这些模型进行条件时，必须小心不要加剧与偏见或有争议话题相关的问题。
- en: '**Benefits of alignment** include:'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对齐的好处** 包括：'
- en: 'Enhanced User Experience: Aligned models generate outputs that are relevant
    to user queries or prompts.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强用户体验：对齐的模型生成与用户查询或提示相关的输出。
- en: 'Trust-building: Ensuring ethical behavior helps build trust among users/customers.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立信任：确保道德行为有助于在用户/客户之间建立信任。
- en: 'Brand Reputation: By aligning with business goals regarding branding consistency
    and desired tone/style guidelines.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌声誉：通过与关于品牌一致性和期望的语气/风格指南的业务目标保持一致。
- en: 'Mitigating Harmful Effects: Alignment with safety, security, and privacy considerations
    helps prevent the generation of harmful or malicious content.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解有害影响：与安全、保密和隐私考虑的对齐有助于防止生成有害或恶意内容。
- en: '**Potential downsides** include:'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**潜在的缺点**包括：'
- en: 'Challenging Balance: Striking a balance between extreme alignment (overly conservative)
    and creative freedom (overly permissive) can be difficult.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡挑战：在极端对齐（过于保守）和创造性自由（过于宽松）之间取得平衡可能很困难。
- en: 'Limitations of Automated Metrics: Quantitative evaluation metrics might not
    capture alignment nuances fully.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动度量标准的限制：定量评估指标可能无法完全捕捉对齐的微妙差异。
- en: 'Subjectivity: Alignment judgments are often subjective, requiring careful consideration
    and consensus building on desired values and guidelines.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主观性：对齐判断往往是主观的，需要对所需价值观和指导方针进行认真考虑和共识建立。
- en: Pre-training a large model on diverse data to learn patterns and language understanding
    results in a base model that has a broad understanding of various topics but lacks
    specificity or alignment to any particular context. While base models such as
    GPT-4 are capable of generating impressive text on a wide range of topics, conditioning
    them can enhance their capabilities in terms of task relevance, specificity, and
    coherence, and make their outputs more relevant and on-topic. Without conditioning,
    these models tend to generate text that may not always align perfectly with the
    desired context. By conditioning them, we can guide the language models to produce
    outputs that are more closely related to the given input or instructions. The
    major advantage of conditioning is that it allows guiding the model without extensive
    retraining. It also enables interactive control and switching between modes. Conditioning
    can happen at different stages of the model development cycle—from fine-tuning
    to output generation in various contexts. There are several options for achieving
    alignment of large language models. One approach is to condition during fine-tuning,
    by training the model on a dataset reflective of the desired outputs. This allows
    the model to specialize, but requires access to relevant training data. Another
    option is to dynamically condition the model at inference time by providing conditional
    input along with the main prompt. This is more flexible but introduces some complexity
    during deployment.In the next section, I will summarize key methods for alignment
    such as fine-tuning and prompt engineering, discuss the rationale, and examine
    their relative pros and cons.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练大型模型以学习模式和语言理解会导致一个基础模型，该模型具有对各种主题的广泛理解，但缺乏特定上下文的特定性或对齐性。尽管像GPT-4这样的基础模型能够在各种主题上生成令人印象深刻的文本，但通过对它们进行调节可以增强它们在任务相关性、特定性和连贯性方面的能力，并使它们的输出更相关和贴切。没有条件的话，这些模型往往会生成与所需上下文不完全一致的文本。通过对其进行调节，我们可以引导语言模型生成更与给定输入或指令相关的输出。调节的主要优势在于它允许在不进行大量重新训练的情况下引导模型。它还可以实现交互式控制和在不同模式之间切换。调节可以在模型开发周期的不同阶段进行——从微调到在各种上下文中生成输出。有几种实现大型语言模型对齐的选择。一种方法是在微调过程中进行条件化，通过对模型进行反映所需输出的数据集进行训练。这样可以使模型专业化，但需要访问相关的训练数据。另一种选择是在推理时动态地对模型进行条件化，通过提供条件输入和主提示。这样更灵活，但在部署过程中引入了一些复杂性。在下一节中，我将总结关键的对齐方法，如微调和提示工程，讨论其原理，并分析它们的相对优缺点。
- en: Methods for alignment
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对齐方法
- en: 'With the advent of large pre-trained language models like GPT-3, there has
    been growing interest in techniques to adapt these models for downstream tasks.
    This process is known as fine-tuning. Fine-tuning allows pre-trained models to
    be customized for specific applications while leveraging the vast linguistic knowledge
    acquired during pre-training. The idea of adapting pre-trained neural networks
    originated in computer vision research in the early 2010s. In NLP, Howard and
    Ruder (2018) demonstrated the effectiveness of fine-tuning pre-trained contextual
    representations like ELMo and ULMFit on downstream tasks. The seminal BERT model
    (Devlin and others., 2019) established fine-tuning of pre-trained transformers
    as the de facto approach in NLP.The need for fine-tuning arises because pre-trained
    LMs are designed to model general linguistic knowledge, not specific downstream
    tasks. Their capabilities manifest only when adapted to particular applications.
    Fine-tuning allows pre-trained weights to be updated for target datasets and objectives.
    This enables knowledge transfer from the general model while customizing it for
    specialized tasks. Several approaches have been proposed for alignment, with trade-offs
    in efficacy and efficiency and it’s worth to delve a bit more into the details
    of each of these alignment methods. **Full Fine-Tuning** involves updating all
    the parameters of the pre-trained language model during fine-tuning. The model
    is trained end-to-end on the downstream tasks, allowing the weights to be updated
    globally to maximize performance on the target objectives. FFT consistently achieves
    strong results across tasks but requires extensive computational resources and
    large datasets to avoid overfitting or forgetting.In **Adapter Tuning** additional
    trainable adapter layers are inserted, usually bottleneck layers, into the pre-trained
    model while keeping the original weights frozen. Only the newly added adapter
    layers are trained on the downstream tasks. This makes tuning parameter-efficient
    as only a small fraction of weights are updated. However, as the pre-trained weights
    remain fixed, adapter tuning has a risk of underfitting to the tasks. The insertion
    points and capacity of the adapters impact overall efficacy.**Prefix Tuning**:
    This prepends trainable vectors to each layer of the LM, which are optimized during
    fine-tuning while the base weights remain frozen. The prefixes allow injection
    of inductive biases into the model. Prefix tuning has a smaller memory footprint
    compared to adapters but has not been found to be as effective. The length and
    initialization of prefixes impact efficacy.In **Prompt Tuning**, the input text
    is appended with trainable prompt tokens which provide a soft prompt to induce
    the desired behavior from the LM. For example, a task description can be provided
    as a prompt to steer the model. Only the added prompt tokens are updated during
    training while the pre-trained weights are frozen. Performance is heavily influenced
    by prompt engineering. Automated prompting methods are being explored.**Low-Rank
    Adaptation (LoRA)** adds pairs of low-rank trainable weight matrices to the frozen
    LM weights. For example, to each weight W, low-rank matrices B and A are added
    such that the forward pass uses W + BA. Only B and A are trained, keeping the
    base W frozen. LoRA achieves reasonable efficacy with greater parameter efficiency
    than full tuning. The choice of rank r impacts tradeoffs. LoRA enables tuning
    giant LMs on limited hardware.Another way to ensure proper alignment of outputs
    is through **human oversight** methods like human-in-the-loop systems. These systems
    involve human reviewers who provide feedback and make corrections if necessary.
    Human involvement helps align generated outputs with desired values or guidelines
    set by humans.Here is a table summarizing the different techniques for steering
    generative AI outputs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着像 GPT-3 这样的大型预训练语言模型的出现，人们对调整这些模型以适应下游任务的技术越来越感兴趣。这个过程被称为微调。微调允许预训练模型在利用预训练期间获得的广泛语言知识的同时，为特定应用定制。在
    2010 年代初，调整预训练神经网络的想法起源于计算机视觉研究。在自然语言处理领域，Howard 和 Ruder（2018）展示了微调预训练上下文表示（如
    ELMo 和 ULMFit）在下游任务上的有效性。开创性的 BERT 模型（Devlin 等人，2019）将微调预训练 transformer 确立为自然语言处理中的事实标准。微调的需求产生的原因是预训练语言模型旨在建模一般语言知识，而不是特定的下游任务。只有当适应特定应用程序时，它们的能力才会显现出来。微调允许更新预训练权重以适应目标数据集和目标。这样可以在定制化专业任务的同时从一般模型中转移知识。已经提出了几种用于对齐的方法，其中在效果和效率方面存在权衡，值得深入研究每种对齐方法的细节。**完全微调**涉及在微调期间更新预训练语言模型的所有参数。该模型在下游任务上端到端地进行训练，允许全局更新权重以最大化目标性能。FFT
    在各个任务上一直表现出色，但需要大量的计算资源和大型数据集来避免过拟合或遗忘。在**适配器调整**中，额外的可训练适配器层被插入到预训练模型中，通常是瓶颈层，同时保持原始权重冻结。只有新增的适配器层在下游任务上进行训练。这使得调整参数效率高，因为只有一小部分权重被更新。然而，由于预训练权重保持不变，适配器调整有着低配合任务的风险。适配器的插入点和容量会影响整体的有效性。**前缀调整**：这种方法将可训练向量预置到
    LM 的每一层中，在微调期间优化这些向量，而基本权重保持冻结。前缀允许向模型注入归纳偏见。与适配器相比，前缀调整的内存占用较小，但效果没有被发现那么有效。前缀的长度和初始化会影响效果。在**提示调整**中，输入文本附加了可训练提示标记，这些标记提供了对
    LM 所需行为的软提示。例如，任务描述可以作为提示提供给模型。只有新增的提示标记在训练期间进行更新，而预训练权重被冻结。性能受提示工程的影响很大。自动提示方法正在被探索。**低秩调整（LoRA）**将一对低秩可训练权重矩阵添加到冻结的
    LM 权重中。例如，对于每个权重 W，添加低秩矩阵 B 和 A，使得前向传播使用 W + BA。只有 B 和 A 被训练，基本的 W 保持冻结。LoRA 实现了合理的有效性，而且参数效率比全面调整更高。秩
    r 的选择会影响权衡。LoRA 可以在有限的硬件上调整巨大的 LM。确保生成的输出正确对齐的另一种方法是通过**人工监督**方法，例如人机协同系统。这些系统涉及人类审阅员提供反馈，并在必要时进行更正。人的参与有助于使生成的输出与人类设定的期望值或指南相一致。下表总结了不同的技术用于引导生成式
    AI 输出：
- en: '| **Stage** | **Technique** | **Examples** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **阶段** | **技术** | **示例** |'
- en: '| Training | Pre-training | Training on diverse data |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 预训练 | 在多样化数据上进行训练 |'
- en: '|  | Objective Function | Careful design of training objective |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标函数 | 训练目标的谨慎设计 |'
- en: '|  | Architecture and Training Process | Optimizing model structure and training
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | 架构和训练过程 | 优化模型结构和训练 |'
- en: '| Fine-Tuning | Specialization | Training on specific datasets/tasks |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 专业化 | 在特定数据集/任务上进行训练 |'
- en: '| Inference-Time Conditioning | Dynamic Inputs | Prefixes, control codes, context
    examples |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 推断时间调整 | 动态输入 | 前缀、控制代码、上下文示例 |'
- en: '| Human Oversight | Human-in-the-Loop | Human review and feedback |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 人类监督 | 人机协同 | 人工审查和反馈 |'
- en: 'Figure 8.1: Steering generative AI outputs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：引导生成型 AI 输出。
- en: Combining these techniques provides developers with more control over the behavior
    and outputs of generative AI systems. The ultimate goal is to ensure that human
    values are incorporated at all stages, from training to deployment, in order to
    create responsible and aligned AI systems.Furthermore, careful design choices
    in the pre-training objective function also impact what behaviors and patterns
    the language model learns initially. By incorporating ethical considerations into
    these objective functions, developers can influence the initial learning process
    of large language models.We can distinguish a few more approaches in fine-tuning
    such as online and offline. InstructGPT was considered a game-changer because
    it demonstrated the potential to significantly improve language models, such as
    GPT-3, by incorporating reinforcement learning from human feedback (RLHF). Let’s
    talk about the reasons why InstructGPT had such a transformative impact.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些技术可以为生成型 AI 系统的行为和输出提供更多的控制。最终目标是确保人类价值观在所有阶段，从训练到部署，都得到了体现，以创建负责任和对齐的 AI
    系统。此外，在预训练目标函数中进行谨慎设计选择也会影响语言模型最初学习的行为和模式。通过将伦理考量纳入这些目标函数中，开发者可以影响大型语言模型的初始学习过程。我们还可以区分微调的几种方法，例如在线和离线。InstructGPT
    被认为是一个改变游戏规则的因素，因为它展示了通过将来自人类反馈的强化学习（RLHF）纳入到语言模型（如 GPT-3）中，可以显著改进语言模型的潜力。让我们谈谈
    InstructGPT 为什么会产生如此革命性的影响的原因。
- en: Reinforcement learning with human feedback
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 利用人类反馈进行强化学习
- en: 'In their March 2022 paper, Ouyang and others from OpenAI demonstrated using
    reinforcement learning from human feedback (RLHF) with proximal policy optimization
    (PPO) to align large language models like GPT-3 with human preferences. Reinforcement
    learning from human feedback (RLHF) is an online approach that fine-tunes LMs
    using human preferences. It has three main steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们 2022 年 3 月的论文中，来自 OpenAI 的欧阳等人展示了使用来自人类反馈的强化学习（RLHF）和近端策略优化（PPO）来调整大型语言模型（如
    GPT-3）与人类偏好一致。从人类反馈中进行强化学习（RLHF）是一种在线方法，它使用人类偏好对语言模型进行微调。它有三个主要步骤：
- en: 'Supervised pre-training: The LM is first trained via standard supervised learning
    on human demonstrations.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督预训练：首先通过标准的监督学习在人类示范中对语言模型进行训练。
- en: 'Reward model training: A reward model is trained on human ratings of LM outputs
    to estimate reward.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励模型训练：一个奖励模型根据语言模型输出的人类评分进行训练，以估计奖励。
- en: 'RL fine-tuning: The LM is fine-tuned via reinforcement learning to maximize
    expected reward from the reward model using an algorithm like PPO.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RL 微调：通过强化学习对语言模型进行微调，以最大化奖励模型的预期奖励，使用像 PPO 这样的算法。
- en: 'The main change, RLHF, allows incorporating nuanced human judgments into language
    model training through a learned reward model. As a result, human feedback can
    steer and improve language model capabilities beyond standard supervised fine-tuning.
    This new model can be used to follow instructions that are given in natural language,
    and it can answer questions in a way that’s more accurate and relevant than GPT-3\.
    InstructGPT outperformed GPT-3 on user preference, truthfulness, and harm reduction,
    despite having 100x fewer parameters.Starting in March 2022, OpenAI started releasing
    the GPT-3.5 series models, upgraded versions of GPT-3, which include fine-tuning
    with RLHF.There are three advantages of fine-tuning that were immediately obvious
    to users of these models:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 主要变化RLHF，通过学习的奖励模型将细微的人类判断纳入语言模型的训练中。因此，人类反馈可以引导和改进语言模型的能力，超越标准的监督微调。这种新模型可以用于遵循用自然语言给出的指示，并且可以回答问题的准确性和相关性比GPT-3更高。尽管参数少了100倍，但InstructGPT在用户偏好、真实性和减少伤害方面都优于GPT-3。从2022年3月开始，OpenAI开始发布GPT-3.5系列模型，这些模型是GPT-3的升级版本，包括与RLHF微调。这些模型的用户立即发现了微调的三个优势：
- en: 'Steerability: the capability of models to follow instructions (instruction-tuning)'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Steerability: 模型遵循指示的能力（指令调整）'
- en: 'Reliable output-formatting: this became important, for example, for API calls/function
    calling)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可靠的输出格式：这对于API调用/函数调用变得重要。
- en: 'Custom tone: this makes it possible to adapt the output style as appropriate
    to task and audience.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义语气：这使得可以根据任务和受众的需求调整输出风格。
- en: InstructGPT opened up new avenues for improving language models by incorporating
    reinforcement learning from human feedback methods beyond traditional fine-tuning
    approaches. RL training can be unstable and computationally expensive, notwithstanding,
    its success inspired further research into refining RLHF techniques, reducing
    data requirements for alignment, and developing more powerful and accessible models
    for a wide range of applications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT通过将人类反馈的强化学习方法纳入传统微调方法之外，开辟了改进语言模型的新途径。尽管强化学习训练可能不稳定且计算成本高昂，但其成功激发了进一步研究，以改进强化学习人机反馈技术，减少对齐的数据需求，以及为各种应用开发更强大和更易于访问的模型。
- en: Offline Approaches
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离线方法
- en: 'Offline methods circumvent the complexity of online RL by directly utilizing
    human feedback. We can distinguish between ranking-based on language-based approaches:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 离线方法通过直接利用人类反馈来绕过在线强化学习的复杂性。我们可以区分基于排名和基于语言的方法：
- en: 'Ranking-based: Human rankings of LM outputs are used to define optimization
    objectives for fine-tuning, avoiding RL entirely. This includes methods like Preference
    Ranking Optimization (PRO; Song et al., 2023) and Direct Preference Optimization
    (DPO; Rafailov et al., 2023).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于排名的：人类对语言模型输出进行排名，用于定义微调的优化目标，完全避免了强化学习。这包括Preference Ranking Optimization
    (PRO; Song等人，2023)和Direct Preference Optimization (DPO; Rafailov等人，2023)等方法。
- en: 'Language-based: Human feedback is provided in natural language and utilized
    via standard supervised learning. For example, Chain of Hindsight (CoH; Liu et
    al., 2023) converts all types of feedback into sentences and uses them to fine-tune
    the model, taking advantage of the language comprehension capabilities of language
    models.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于语言：人类通过自然语言提供反馈，并通过标准监督学习利用。例如，Chain of Hindsight (CoH; Liu等人，2023)将所有类型的反馈转换为句子，并用它们微调模型，利用语言模型的语言理解能力。
- en: 'Direct Preference Optimization (DPO) is a simple and effective method for training
    language models to adhere to human preferences without needing to explicitly learn
    a reward model or use reinforcement learning. While it optimizes the same objective
    as existing RLHF methods, it is much simpler to implement, more stable, and achieves
    strong empirical performance.Researchers from Meta, in the paper “*LIMA: Less
    Is More for Alignment*”, simplified alignment by minimizing a supervised loss
    on only 1,000 carefully curated prompts in fine-training the LLaMa model. Based
    on the favorable human preferences when comparing their outputs to DaVinci003
    (GPT-3.5), they conclude that fine-training has only minimal importance. This
    they refer to as the superficial alignment hypothesis.Offline approaches offer
    more stable and efficient tuning. However, they are limited by static human feedback.
    Recent methods try to combine offline and online learning.While both DPO and RLHF
    with PPO aim to align LLMs with human preferences, they differ in terms of complexity,
    data requirements, and implementation details. DPO offers simplicity but achieves
    strong performance by directly optimizing probability ratios. On the other hand,
    RLHF with PPO in InstructGPT introduces more complexity but allows for nuanced
    alignment through reward modeling and reinforcement learning optimization.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 直接优化偏好（DPO）是一种简单有效的方法，用于训练语言模型以符合人类偏好，无需显式学习奖励模型或使用强化学习。虽然它优化与现有RLHF方法相同的目标，但它实现起来要简单得多，更稳定，并且具有强大的经验性能。Meta的研究人员在论文“*LIMA：对齐的少即多*”中，通过在对LLaMa模型进行精细训练时仅在1000个精心策划的提示上最小化监督损失来简化对齐。通过将它们的输出与DaVinci003（GPT-3.5）进行比较时所得到的有利人类偏好，他们得出结论，精细训练仅具有极小的重要性。他们将这称为表面对齐假设。离线方法提供了更稳定和高效的调整。然而，它们受限于静态人类反馈。最近的方法尝试将离线和在线学习相结合。尽管DPO和带有PPO的RLHF都旨在将LLMs与人类偏好对齐，但它们在复杂性、数据需求和实现细节方面存在差异。DPO提供了简单性，但通过直接优化概率比率实现了强大的性能。另一方面，带有PPO的RLHF在InstructGPT中引入了更多复杂性，但允许通过奖励建模和强化学习优化进行细微的对齐。
- en: Low-Rank Adaptation
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 低秩适应
- en: LLMs have achieved impressive results in Natural Language Processing and are
    now being used in other domains such as Computer Vision and Audio. However, as
    these models become larger, it becomes difficult to train them on consumer hardware
    and deploying them for each specific task becomes expensive. There are a few methods
    that reduce computational, memory, and storage costs, while improving performance
    in low-data and out-of-domain scenarios.Low-Rank Adaptation (LoRA) freezes the
    pre-trained model weights and introduces trainable rank decomposition matrices
    into each layer of the Transformer architecture to reduce the number of trainable
    parameters. LoRA achieves comparable or better model quality compared to fine-tuning
    on various language models (RoBERTa, DeBERTa, GPT-2, and GPT-3) while having fewer
    trainable parameters and higher training throughput.The QLORA method is an extension
    of LoRA, which enables efficient fine-tuning of large models by backpropagating
    gradients through a frozen 4-bit quantized model into learnable low-rank adapters.
    This allows fine-tuning a 65B parameter model on a single GPU. QLORA models achieve
    99% of ChatGPT performance on Vicuna using innovations like new data types and
    optimizers. In particular, QLORA reduces the memory requirements for fine-tuning
    a 65B parameter model from >780GB to <48GB without affecting runtime or predictive
    performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLM（大型语言模型）在自然语言处理方面取得了令人印象深刻的成果，现在也被应用于计算机视觉和音频等其他领域。然而，随着这些模型变得越来越庞大，用消费者硬件训练它们变得困难，并且为每个特定任务部署它们变得昂贵。有几种方法可以减少计算、内存和存储成本，同时提高在低数据和领域外场景下的性能。低秩适应（LoRA）冻结预训练模型的权重，并在Transformer架构的每一层中引入可训练的秩分解矩阵，以减少可训练参数的数量。LoRA在各种语言模型（RoBERTa、DeBERTa、GPT-2和GPT-3）上与微调相比，实现了可比较或更好的模型质量，同时具有更少的可训练参数和更高的训练吞吐量。QLORA方法是LoRA的扩展，通过将梯度通过冻结的4位量化模型传播到可学习的低秩适配器，实现了对大型模型的有效微调，从而使得在单个GPU上对650亿参数模型进行微调成为可能。QLORA模型在Vicuna上实现了与ChatGPT性能99%相当的性能，采用了新的数据类型和优化器等创新。特别是，QLORA将对650亿参数模型进行微调的内存需求从大于780GB降低到小于48GB，而不影响运行时或预测性能。
- en: '**Quantization** refers to techniques for reducing the numerical precision
    of weights and activations in neural networks like large language models (LLMs).
    The main purpose of quantization is to reduce the memory footprint and computational
    requirements of large models.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**量化**指的是降低神经网络中权重和激活的数值精度的技术，例如大型语言模型（LLMs）。量化的主要目的是减少大型模型的内存占用和计算需求。'
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some key points about quantization of LLMs:'
  id: totrans-51
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于LLM量化的一些关键点：
- en: It involves representing weights and activations using fewer bits than standard
    single-precision floating point (FP32). For example, weights could be quantized
    to 8-bit integers.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它涉及使用比标准单精度浮点（FP32）更少的比特来表示权重和激活。例如，权重可以量化为8位整数。
- en: This allows shrinking model size by up to 4x and improving throughput on specialized
    hardware.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以通过将可训练向量前置到LLM层来缩小模型大小最多4倍，并提高专用硬件的吞吐量。
- en: Quantization typically has a minor impact on model accuracy, especially with
    re-training.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化通常对模型准确度影响不大，尤其是在重新训练时。
- en: Common quantization methods include scalar, vector, and product quantization
    which quantize weights separately or in groups.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的量化方法包括标量、向量和乘积量化，这些方法将权重分别或分组量化。
- en: Activations can also be quantized by estimating their distribution and binning
    appropriately.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过估计激活的分布并适当分组，激活也可以进行量化。
- en: Quantization-aware training adjusts weights during training to minimize quantization
    loss.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化感知训练在训练过程中调整权重以最小化量化损失。
- en: LLMs like BERT and GPT-3 have been shown to work well with 4-8 bit quantization
    via fine-tuning.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像BERT和GPT-3之类的LLM已经通过微调展示了对4-8位量化的良好适应性。
- en: Parameter-Efficient Fine-tuning (PEFT) methods enable the use of small checkpoints
    for each task, making the models more portable. These small trained weights can
    be added on top of the LLM, allowing the same model to be used for multiple tasks
    without replacing the entire model.In the next section, we’ll discuss methods
    for conditioning large language models (LLMs) at inference time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）方法使得可以为每个任务使用小检查点，使得模型更具可移植性。这些小训练权重可以添加到LLM之上，允许同一模型用于多个任务而无需替换整个模型。在下一节中，我们将讨论在推理时如何为大型语言模型（LLMs）进行条件化的方法。
- en: Inference-Time conditioning
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理时条件化
- en: 'One commonly used approach is **conditioning at inference time** (output generation
    phase) where specific inputs or conditions are provided dynamically to guide the
    output generation process. LLM fine-tuning may not always be feasible or beneficial
    in certain scenarios:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的方法是在推理时进行条件化（输出生成阶段），在这个过程中动态提供特定的输入或条件以引导输出生成过程。在某些场景中，LLM微调并不总是可行或有益的：
- en: 'Limited Fine-Tuning Services: Some models are only accessible through APIs
    that lack or have restricted fine-tuning capabilities.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有限的微调服务：一些模型只能通过缺乏或具有受限微调能力的API访问。
- en: 'Insufficient Data: In cases where there is a lack of data for fine-tuning,
    either for the specific downstream task or relevant application domain.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据不足：在特定下游任务或相关应用领域缺乏微调所需的数据的情况下。
- en: 'Dynamic Data: Applications with frequently changing data, such as news-related
    platforms, may struggle to fine-tune models frequently, leading to potential drawbacks.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态数据：像新闻相关平台这样频繁更改数据的应用可能难以频繁微调模型，导致潜在的缺点。
- en: 'Context-Sensitive Applications: Dynamic and context-specific applications like
    personalized chatbots cannot perform fine-tuning based on individual user data.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文敏感应用：像个性化聊天机器人这样的动态和特定上下文的应用无法根据个人用户数据执行微调。
- en: 'For conditioning at inference time, most commonly, we provide a textual prompt
    or instruction at the beginning of the text generation process. This prompt can
    be a few sentences or even a single word, acting as an explicit indication of
    the desired output. Some common techniques for dynamic inference-time conditioning
    include:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段进行条件化时，最常见的做法是在文本生成过程开始时提供文本提示或指令。这个提示可以是几句话甚至是一个单词，作为所需输出的明确指示。一些常见的动态推理时条件化技术包括：
- en: 'Prompt tuning: Providing natural language guidance for intended behavior. Sensitive
    to prompt design.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示调整：为预期行为提供自然语言指导。对提示设计敏感。
- en: 'Prefix tuning: Prepending trainable vectors to LLM layers.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀调整：在LLM层前面添加可训练向量。
- en: 'Constraining tokens: Forcing inclusion/exclusion of certain words'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制令牌：强制包含/排除某些单词
- en: 'Metadata: Providing high-level info like genre, target audience, etc.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据：提供类似类型、目标受众等高层次信息。
- en: Prompts can facilitate generating text that adheres to specific themes, styles,
    or even mimics a particular author's writing style. These techniques involve providing
    contextual information during inference time such as for in-context learning or
    retrieval augmentation.An example of prompt tuning is prefixing prompts, where
    instructions like "Write a child-friendly story about..." are prepended to the
    prompt. For example, in chatbot applications, conditioning the model with user
    messages helps it generate responses that are personalized and pertinent to the
    ongoing conversation. Further examples include prepending relevant documents to
    prompts to assist LLMs with writing tasks (e.g., news reports, Wikipedia pages,
    company documents), or retrieving and prepending user-specific data (financial
    records, health data, emails) before prompting an LLM to ensure personalized answers.
    By conditioning LLM outputs on contextual information at runtime, these methods
    can guide models without relying on traditional fine-tuning processes.Often demonstrations
    are part of the instructions for reasoning tasks, where few-shot examples are
    provided to induce desired behavior. Powerful LLMs, such as GPT-3, can solve tasks
    without further training through prompting techniques. In this approach, the problem
    to be solved is presented to the model as a text prompt, possibly with some text
    examples of similar problems and their solutions. The model must provide a completion
    of the prompt via inference. **Zero-shot prompting** involves no solved examples,
    while few-shot prompting includes a small number of examples of similar (problem,
    solution) pairs. It has shown that prompting provides easy control over large
    frozen models like GPT-3 and allows steering model behavior without extensive
    fine-tuning. Prompting enables conditioning models on new knowledge with low overhead,
    but careful prompt engineering is needed for best results. This is what we’ll
    discuss as part of this chapter.In prefix tuning, continuous task-specific vectors
    are trained and supplied to models at inference time. continuous task-specific
    vectors. Similar ideas have been proposed for adapter-approaches such as parameter
    efficient transfer learning (PELT) or Ladder Side-Tuning (LST).Conditioning at
    inference time can also happen during sampling such as grammar-based sampling,
    where the output can be constrained to be compatible with certain well-defined
    patterns, such as a programming language syntax.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以促进生成符合特定主题、风格甚至模仿特定作者写作风格的文本。这些技术涉及在推理时提供上下文信息，比如针对上下文学习或检索增强。一个提示调整的例子是在提示前添加前缀，比如“写一个适合儿童的故事...”，这些指示会被添加到提示的前面。例如，在聊天机器人应用中，用用户消息来调整模型有助于生成与正在进行的对话相关且个性化的回复。更进一步的例子包括在提示之前添加相关文档，以帮助语言模型完成写作任务（如新闻报道、维基百科页面、公司文件），或者在提示LLM之前检索并添加用户特定数据（财务记录、健康数据、电子邮件）以确保个性化回答。通过在运行时将LLM输出与上下文信息相结合，这些方法可以引导模型而不依赖于传统的微调过程。通常演示是推理任务说明的一部分，其中提供少量示例来诱导期望的行为。强大的LLM，如GPT-3，可以通过提示技术解决问题而无需进一步的训练。在这种方法中，要解决的问题被呈现给模型作为文本提示，可能包括一些类似问题及其解决方案的文本示例。模型必须通过推理提供提示的完成。**零次提示**不包含已解决的示例，而少量提示包括一小部分类似（问题、解决方案）对的示例。已经表明提示可以轻松控制像GPT-3这样的大型冻结模型，并且可以在不需要大量微调的情况下引导模型行为。提示使模型可以在新知识上进行调整，而开销较低，但是需要谨慎地进行提示工程以获得最佳结果。这将作为本章的一部分进行讨论。在前缀调整中，连续的任务特定向量在推理时被训练并提供给模型。类似的想法也被提出用于适配器方法，例如参数高效的迁移学习（PELT）或梯子侧调整（LST）。在推理时的调整也可以发生在采样过程中，比如基于语法的采样，其中输出可以受到某些定义良好模式的约束，比如编程语言语法。
- en: Conclusions
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结论
- en: Full fine-tuning consistently achieves strong results but often requires extensive
    resources, and trade-offs exist between efficacy and efficiency. Methods like
    adapters, prompts, and LoRA reduce this burden via sparsity or freezing, but can
    be less effective. The optimal approach depends on constraints and objectives.
    Future work on improved techniques tailor-made for large LMs could push the boundaries
    of both efficacy and efficiency. Recent work blends offline and online learning
    for improved stability. Integrating world knowledge and controllable generation
    remain open challenges.Prompt-based techniques allow flexible conditioning of
    LLMs to induce desired behaviors without intensive training. Careful prompt design,
    optimization, and evaluation is key to effectively controlling LLMs. Prompt-based
    techniques allow conditioning LLMs on specific behaviors or knowledge in a flexible,
    low-resource manner.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 全面的微调一贯取得强大的结果，但往往需要大量资源，效率与功效之间存在着权衡。像适配器、提示和LoRA这样的方法通过稀疏化或冻结减轻了这一负担，但可能效果较差。最佳方法取决于约束条件和目标。未来改进的技术针对大型语言模型定制可能会推动功效和效率的边界。最近的工作将离线和在线学习相结合，以提高稳定性。整合世界知识和可控生成仍是开放挑战。基于提示的技术允许对大型语言模型进行灵活的调节，以引导所需行为而无需进行密集训练。精心设计、优化和评估提示是有效控制大型语言模型的关键。基于提示的技术可通过灵活、低资源的方式对语言模型进行特定行为或知识的调节。
- en: Evaluations
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: 'Alignment is evaluated on alignment benchmarks like HUMAN and generalization
    tests like FLAN. There are a few core benchmarks with high differentiability to
    accurately assess model strengths and weaknesses such as these:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐在类似HUMAN的对齐基准和FLAN等泛化测试上进行评估。有一些核心基准可以准确评估模型的优势和劣势，例如：
- en: 'English knowledge: MMLU'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英文知识：MMLU
- en: 'Chinese knowledge: C-Eval'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中文知识：C-Eval
- en: 'Reasoning: GSM8k / BBH (Algorithmic)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理：GSM8k / BBH（算法）
- en: 'Coding: HumanEval / MBPP'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码：HumanEval / MBPP
- en: After balancing these directions, additional benchmarks like MATH (high-difficulty
    reasoning) and Dialog could be pursued.A particularly interesting evaluation is
    in math or reasoning, where generalization abilities would be expected to be very
    strong. The MATH benchmark demonstrates high-level difficulty, and GPT-4 achieves
    varying scores based on prompting methods. Results range from naive prompting
    via few-shot evaluations to PPO + process-based reward modeling. If fine-tuning
    involves dialog data only, it might negatively affect existing capabilities such
    as MMLU or BBH. Prompt engineering is essential, as biases and query difficulty
    impact evaluations.There are quantitative metrics like perplexity (measuring how
    well a model predicts data) or BLEU score (capturing similarity between generated
    text and reference text). These metrics provide rough estimates but may not fully
    capture semantic meaning or alignment with higher-level goalsOther metrics include
    user preferences ratings through human evaluation, pairwise preference, utilizing
    pre-trained reward model for online small/medium models or automated LLM-based
    assessments (for example GPT-4). Human evaluations can sometimes be problematic
    since humans can be swayed by subjective criteria such as an authoritative tone
    in the response rather than the actual accuracy. Conducting evaluations where
    users assess the quality, relevance, appropriateness of generated text against
    specific criteria set beforehand provides more nuanced insights into alignment.
    Fine-tuning is not intended to solely improve user preferences on a given set
    of prompts. Its primary purpose is to address AI safety concerns by reducing the
    occurrence of undesirable outputs such as illegal, harmful, abusive, false, or
    deceptive content. This focus on mitigating risky behavior is crucial in ensuring
    the safety and reliability of AI systems. Evaluating and comparing models based
    purely on user preferences without considering the potential harm they may cause
    can be misleading and prioritize suboptimal models over safer alternatives. In
    summary, evaluating LLM alignment requires careful benchmark selection, consideration
    of differentiability, and a mix of automatic evaluation methods and human judgments.
    Attention to prompt engineering and specific evaluation aspects is necessary to
    ensure accurate assessment of model performance.In the next section, we’ll fine-tune
    a small open-source LLM (OpenLLaMa) for a question answering with PEFT and quantization,
    and we’ll deploy it on HuggingFace.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在平衡了这些方向之后，可以追求像MATH（高难度推理）和Dialog等额外的基准。尤其有趣的评估在数学或推理方面，人们预期泛化能力会非常强大。MATH基准展示了高级别的难度，而GPT-4根据提示方法的不同实现了不同的分数。结果从通过少量样本评估的朴素提示到PPO
    + 基于过程的奖励建模都有。如果微调只涉及对话数据，可能会对现有的能力产生负面影响，例如MMLU或BBH。提示工程至关重要，因为偏见和查询难度会影响评估。还有一些定量指标，如困惑度（衡量模型预测数据的能力）或BLEU分数（捕捉生成文本与参考文本之间的相似性）。这些指标提供了粗略的估计，但可能无法完全捕捉语义含义或与更高级别目标的一致性。其他指标包括通过人类评估的用户偏好评分，成对偏好，利用预训练奖励模型进行在线小/中型模型的评估或自动化LLM评估（例如GPT-4）。人类评估有时可能存在问题，因为人类可能会受到主观标准的影响，例如回答中的权威语气而不是实际的准确性。进行评估时，用户根据事先设置的特定标准来评估生成文本的质量、相关性和适当性，可以提供更加细致入微的洞见。微调的目的不仅仅是为了改善对给定提示集的用户偏好。其主要目的是通过减少不良输出（如非法、有害、滥用、虚假或欺骗性内容）来解决AI安全问题。关注减轻风险行为对于确保AI系统的安全性和可靠性至关重要。仅基于用户偏好评估和比较模型，而不考虑它们可能导致的潜在危害，可能会产生误导，并将次优模型优先于更安全的替代方案。总之，评估LLM的一致性需要仔细选择基准、考虑可区分性，并结合自动评估方法和人类判断。注意提示工程和特定评估方面对于确保准确评估模型性能至关重要。在下一节中，我们将使用PEFT和量化微调一个小型开源LLM（OpenLLaMa）以进行问答，并将其部署在HuggingFace上。
- en: Fine-Tuning
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: 'As we’ve discussed in the first section of this chapter, the goal of model
    fine-tuning for LLMs is to optimize a model to generate outputs that are more
    specific to a task and context than the original foundation model. Amongst the
    multitude of tasks and scenarios, where we might want to apply this approach are
    these:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章第一节中讨论的那样，对LLM进行模型微调的目标是优化模型，使其生成的输出比原始基础模型更具体于任务和上下文。在我们可能想要应用这种方法的众多任务和场景中，包括以下几种：
- en: Software Development
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件开发
- en: Document classification
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类
- en: Question-Answering
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答系统
- en: Information Retrieval
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息检索
- en: Customer Support
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户支持
- en: In this section, we’ll fine-tune a model for question answering. This recipe
    is not specific to LangChain, but we’ll point out a few customizations, where
    LangChain could come in. For performance reasons, we'll run this on Google Colab
    instead of the usual local environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将微调一个用于问答的模型。这个步骤不专门针对 LangChain，但我们将指出一些自定义内容，其中 LangChain 可能会起作用。出于性能原因，我们将在谷歌
    Colab 上运行，而不是通常的本地环境。
- en: '**Google Colab** is a computation environment that provides different means
    for hardware acceleration of computation tasks such as Tensor Processing Units
    (TPUs) and Graphical Processing Units (GPUs). These are available both in free
    and professional tiers. For the purpose of the task in this section, the free
    tier is completely sufficient. You can sign into a Colab environment at this url:
    [https://colab.research.google.com/](https://colab.research.google.com/)'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**谷歌 Colab** 是一个计算环境，提供了不同的硬件加速计算任务的手段，如张量处理单元（TPUs）和图形处理单元（GPUs）。这些在免费和专业版中都可用。对于本节中的任务目的，免费版完全足够。您可以在此网址登录到
    Colab 环境：[https://colab.research.google.com/](https://colab.research.google.com/)'
- en: 'Please make sure you set your google colab machine settings in the top menu
    to TPU or GPU in order to make sure you have sufficient resources to run this
    and that the training doesn''t take too long. We’ll install all required libraries
    in the Google Colab environment – I am adding the versions of these libraries
    that I’ve used in order to make our fine-tuning repeatable:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保您在顶部菜单中将您的谷歌 Colab 机器设置为 TPU 或 GPU，以确保您有足够的资源运行此操作并且训练不会太长时间。我们将在谷歌 Colab
    环境中安装所有必需的库 - 我正在添加这些库的版本，以便我们的微调是可重复的：
- en: 'peft: Parameter-Efficient Fine-Tuning (PEFT; version 0.5.0)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'peft: Parameter-Efficient Fine-Tuning（PEFT；版本 0.5.0）'
- en: 'trl: Proximal Policy Optimization (0.6.0)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'trl: Proximal Policy Optimization (0.6.0)'
- en: 'bitsandbytes: k-bit optimizers and matrix multiplication routines, needed for
    quantization (0.41.1)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'bitsandbytes: k 位优化器和矩阵乘法例程，用于量化 (0.41.1)'
- en: 'accelerate: train and use PyTorch models with multi-GPU, TPU, mixed-precision
    (0.22.0)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'accelerate: 使用多 GPU、TPU、混合精度训练和使用 PyTorch 模型 (0.22.0)'
- en: 'transformers: HuggingFace transformers library with backends in JAX, PyTorch
    and TensorFlow (4.32.0)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'transformers: HuggingFace transformers library with backends in JAX, PyTorch
    and TensorFlow (4.32.0)'
- en: 'datasets: community-driven open-source library of datasets (2.14.4)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'datasets: 社区驱动的开源数据集库 (2.14.4)'
- en: 'sentencepiece: Python wrapper for fast tokenization (0.1.99)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'sentencepiece: 快速标记化的 Python 包装器 (0.1.99)'
- en: 'wandb: for monitoring the training progress on Weights and Biases (0.15.8)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'wandb: 用于监视 Weights and Biases（W&B）训练进度的工具 (0.15.8)'
- en: langchain for loading the model back as a langchain llm after training (0.0.273)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: langchain 用于在训练后将模型加载回 langchain llm (0.0.273)
- en: 'We can install these libraries from the Colab notebook as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从 Colab 笔记本中安装这些库，如下所示：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to download and train models from HuggingFace, we need to authenticate
    with the platform. Please note that if you want to push your model to HuggingFace
    later, you need to generate a new API token with write permissions on HuggingFace:
    [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从 HuggingFace 下载和训练模型，我们需要在该平台上进行身份验证。请注意，如果您稍后想要将模型推送到 HuggingFace，则需要在 HuggingFace
    上生成一个具有写入权限的新 API 令牌：[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
- en: '![Figure 8.3: Creating a new API token on HuggingFace write permissions.](img/file55.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3：在 HuggingFace 上创建新的 API 令牌并授予写入权限。](img/file55.png)'
- en: 'Figure 8.3: Creating a new API token on HuggingFace write permissions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：在 HuggingFace 上创建新的 API 令牌并授予写入权限。
- en: 'We can authenticate from the notebook like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样从笔记本进行身份验证：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When prompted, paste your HuggingFace access token.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示时，请粘贴您的 HuggingFace 访问令牌。
- en: 'A note of caution before, we start: when executing the code, you need to log
    into different services so make sure you pay attention when running the notebook!'
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要注意一点：在执行代码时，您需要登录到不同的服务中，所以在运行笔记本时请注意！
- en: 'Weights and Biases (W&B) is an MLOps platform that can help developers to monitor
    and document ML training workflows from end to end. As mentioned earlier, we will
    use W&B to get an idea of how well the training is working, in particular if the
    model is improving over time.For W&B, we need to name the project; alternatively,
    we can use wandb''s `init()` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Weights and Biases（W&B）是一个 MLOps 平台，可以帮助开发人员从头到尾地监视和记录 ML 训练工作流程。正如前面提到的，我们将使用
    W&B 来了解训练的情况，特别是模型是否随时间改进。对于 W&B，我们需要为项目命名；或者，我们可以使用 wandb 的 `init()` 方法：
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In order to authenticate with W&B, you need to create a free account with them
    for this at [https://www.wandb.ai](https://www.wandb.ai) You can find your API
    key on the Authorize page: [https://wandb.ai/authorize](https://wandb.ai/authorize)Again,
    we need to paste in our API token. If the previous training run is still active
    – this could be from a previous execution of the notebook if you are running a
    second time –, let''s make sure we start a new one! This will ensure that we get
    new reports and dashboard on W&B:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用W&B进行身份验证，您需要在他们这里创建一个免费账户：[https://www.wandb.ai](https://www.wandb.ai)您可以在“授权”页面找到您的API密钥：[https://wandb.ai/authorize](https://wandb.ai/authorize)再次，我们需要粘贴我们的API令牌。如果之前的训练仍然处于活动状态——如果您第二次运行笔记本可能是因为之前的执行——让我们确保开始一个新的！这将确保我们在W&B上获得新的报告和仪表板：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we’ll need to choose a dataset against which we want to optimize. We
    can use lots of different datasets here that are appropriate for coding, storytelling,
    tool use, SQL generation, grade-school math questions (GSM8k), or many other tasks.
    HuggingFace provides a wealth of datasets, which can be viewed at this url: [https://huggingface.co/datasets](https://huggingface.co/datasets)These
    cover a lot of different, even the most niche tasks. We can also customize our
    own dataset. For example, we can use langchain to set up training data. There
    are quite a few methods available for filtering that could help reduce redundancy
    in the dataset. It would have been appealing to show data collection as a practical
    recipe in this chapter. However, because of the complexity I am leaving it out
    of scope for the book.It might be harder to filter for quality from web data,
    but there are a lot of possibilities. For code models, we could apply code validation
    techniques to score segments as a quality filter. If the code comes from Github,
    we can filter by stars or by stars by repo owner. For texts in natural language,
    quality filtering is not trivial. Search engine placement could serve as a popularity
    filter since it''s often based on user engagement with the content. Further, knowledge
    distillation techniques could be tweaked as a filter by fact density and accuracy.In
    this recipe, we are fine-tuning for question answering performance with the Squad
    V2 dataset. You can see a detailed dataset description on HuggingFace: [https://huggingface.co/spaces/evaluate-metric/squad_v2](https://huggingface.co/spaces/evaluate-metric/squad_v2)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择一个数据集进行优化。我们可以使用许多不同的数据集，这些数据集适用于编码、叙述、工具使用、SQL生成、小学数学问题（GSM8k）或许多其他任务。HuggingFace提供了丰富的数据集，可以在此网址查看：[https://huggingface.co/datasets](https://huggingface.co/datasets)，这些数据集涵盖了许多不同，甚至是最小众的任务。我们也可以定制自己的数据集。例如，我们可以使用langchain来设置训练数据。有许多可用的过滤方法可以帮助减少数据集中的冗余。本章原本很想展示数据收集作为一个实用的配方。但是，因为复杂性的原因，我决定不在本书范围内讨论这个问题。过滤网络数据的质量可能更难一些，但有很多可能性。对于代码模型，我们可以应用代码验证技术来将部分代码分数化为质量过滤器。如果代码来自Github，我们可以按星和存储库所有者筛选。对于自然语言的文本，质量过滤并不容易。搜索引擎排名可以作为受欢迎程度的过滤器，因为它通常基于用户对内容的参与度。此外，知识蒸馏技术可以通过事实的密度和准确性调整为一个过滤器。在这个配方中，我们在Squad
    V2数据集上进行了问答性能微调。您可以在HuggingFace上查看详细的数据集描述：[https://huggingface.co/spaces/evaluate-metric/squad_v2](https://huggingface.co/spaces/evaluate-metric/squad_v2)
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are taking both training and validation splits. The Squad V2 dataset bas
    a part that’s supposed to be used in training and another one in validation as
    we can see in the output of `load_dataset(dataset_name)`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在采用训练和验证集。Squad V2数据集有一个部分应该在训练中使用，另一个部分应该在验证中使用，正如我们可以在`load_dataset(dataset_name)`的输出中看到的：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’ll use the validation splits for early stopping. Early stopping will allow
    us to stop training when the validation error begins to degrade.The Squad V2 dataset
    is composed of various features, which we can see here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用验证集进行提前停止。当验证错误开始恶化时，提前停止将允许我们停止训练。Squad V2数据集由各种特征组成，我们可以在这里看到：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The basic idea in training is prompting the model with a question and comparing
    the answer to the dataset.We want a small model that we can run locally at a decent
    token rate. LLaMa-2 models require signing a license agreement with your email
    address and to get confirmed (which, to be fair, can be very fast) as it comes
    with restrictions to commercial use. LLaMa derivates such as OpenLLaMa have been
    performing quite well as can be evidenced on the HF leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)OpenLLaMa
    version 1 cannot be used for coding tasks, because of the tokenizer. Therefore,
    let''s use v2! We’ll use a 3 billion parameter model, which we’ll be able to use
    even on older hardware:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中的基本思想是用一个问题提示模型，并将答案与数据集进行比较。我们需要一个可以在本地以不错的令牌速率运行的小型模型。LLaMa-2 模型需要您使用您的电子邮件地址签署许可协议并获得确认（公平地说，这可能非常快），因为它受到商业使用限制。LLaMa
    衍生产品，如 OpenLLaMa，在 HF 排行榜上表现得相当不错：[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。OpenLLaMa
    版本 1 不能用于编码任务，因为分词器的原因。因此，让我们使用 v2！我们将使用一个 30 亿参数的模型，即使在旧硬件上也能使用：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can use even smaller models such as `EleutherAI/gpt-neo-125m` which can
    also give a very good compromise between resource use and performance.Let’s load
    the model:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以使用更小的模型，比如 `EleutherAI/gpt-neo-125m`，它也可以在资源使用和性能之间达到很好的折衷。让我们加载模型：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The Bits and Bytes configuration makes it possible to quantize our model in
    8, 4, 3 or even 2 bits with a much-accelerated inference and lower memory footprint
    without a incurring a big cost in terms of performance. We are going to store
    model checkpoints on Google Drive; you need to confirm your login to your google
    account:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Bits and Bytes 配置，我们可以将模型量化为 8、4、3 甚至 2 位，具有更快的推断速度和更低的内存占用，而不会在性能方面产生大的代价。我们将在
    Google Drive 上存储模型检查点；您需要确认您的 Google 账号登录：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We’ll need to authenticate with Google for this to work. We can set our output
    directory for model checkpoints and logs to our Google Drive:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过 Google 进行身份验证才能让它起作用。我们可以将模型检查点和日志的输出目录设置为我们的 Google Drive：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you don''t want to use google drive, just set this to a directory on your
    computer.For training, we need to set up a tokenizer:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想使用 Google Drive，只需将其设置为计算机上的目录。对于训练，我们需要设置一个分词器：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we’ll define our training configuration. We’ll set up LORA and other training
    arguments:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义我们的训练配置。我们将设置 LORA 和其他训练参数：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A few comments to explain some of these parameters are in order. The `push_to_hub`
    argument means that we can push the model checkpoints to the HuggingSpace Hub
    regularly during training. For this to work you need to set up the HuggingSpace
    authentication (with write permissions as mentioned). If we opt for this, as `output_dir`
    we can use `new_model_name`. This will be the repository name under which the
    model will be available here on HuggingFace: [https://huggingface.co/models](https://huggingface.co/models)Alternatively,
    as I’ve done here, we can save your model locally or to the cloud, for example
    google drive to a directory.I’ve set `max_steps` and `num_train_epochs` very high,
    because I’ve noticed that training can still improve after many steps. We are
    using early stepping together with a high number of maximum training steps to
    get the model to converge to higher performance. For early stopping, we need to
    set the `evaluation_strategy` as `"steps"` and `load_best_model_at_end=True`.`eval_steps`
    is the number of update steps between two evaluations. `save_total_limit=5` means
    that only last 5 models are saved. Finally, `report_to="wandb"` means that we’ll
    send training stats, some model metadata, and hardware information to W&B, where
    we can look at graphs and dashboards for each run. The training can then use our
    configuration:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些注释来解释其中一些参数是有必要的。`push_to_hub` 参数意味着我们可以在训练期间定期将模型检查点推送到 HuggingSpace Hub。为此，您需要设置
    HuggingSpace 认证（如前所述，具有写权限）。如果我们选择这样做，作为 `output_dir` 我们可以使用 `new_model_name`。这将是模型在
    HuggingFace 上可用的仓库名称：[https://huggingface.co/models](https://huggingface.co/models)
    或者，如我在这里所做的，我们可以将模型保存在本地或云端，例如将其保存到 Google Drive 上的一个目录。我将 `max_steps` 和 `num_train_epochs`
    设置得非常高，因为我注意到训练仍然可以在许多步骤之后进行改进。我们使用提前停止与最大训练步数的高数量来使模型收敛到更高的性能。对于提前停止，我们需要将 `evaluation_strategy`
    设置为 `"steps"` 并将 `load_best_model_at_end=True`。`eval_steps` 是两次评估之间的更新步数。`save_total_limit=5`
    意味着只保存最后 5 个模型。最后，`report_to="wandb"` 意味着我们将向 W&B 发送训练统计信息、一些模型元数据和硬件信息，我们可以在那里查看每次运行的图形和仪表板。然后，训练可以使用我们的配置：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The training can take quite a while, even running on TPU device. The evaluating
    and early stopping slows the training down by a lot. If you disable the early
    stopping, you can make this much faster.We should see some statistics as the training
    progresses, but it’s nicer to show the graph of performance as we can see it on
    W&B:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要相当长的时间，即使在 TPU 设备上运行也是如此。评估和提前停止会大大减慢训练速度。如果禁用提前停止，您可以加快训练速度。随着训练的进行，我们应该看到一些统计数据，但是通过
    W&B，将性能图形化展示更好：
- en: '![Figure 8.4: Fine-tuning training loss over time (steps).](img/file56.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4：时间（步骤）上的微调训练损失。](img/file56.png)'
- en: 'Figure 8.4: Fine-tuning training loss over time (steps).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：时间（步骤）上的微调训练损失。
- en: 'After training is done, we can save the final checkpoint on disk for re-loading:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以将最终的检查点保存在磁盘上以便重新加载：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now share our final model with friends in order to brag about the performance
    we''ve achieved by manually pushing to HuggingFace:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将最终模型与朋友共享，以炫耀我们通过手动推送到 HuggingFace 所实现的性能：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now load the model back using the combination of our HuggingFace username
    and the repository name (new model name).Let’s quickly show how to use this model
    in LangChain. usually, the peft model is stored as an adapter, not as a full model,
    therefore the loading is a bit different:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用我们的 HuggingFace 用户名和仓库名称（新模型名称）的组合来加载模型。让我们快速展示如何在 LangChain 中使用这个模型。通常情况下，peft
    模型被存储为适配器，而不是完整模型，因此加载方式略有不同：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’ve done everything so far on Google Colab, but we can equally execute this
    locally, just note that you need to have the huggingface peft library installed!So
    far, we’ve shown how to fine-tune and deploy an open-source LLM. Some commercial
    models can be fine-tuned on custom data as well. For example, both OpenAI’s GPT-3.5
    and Google’s PaLM model offer this capability. This has been integrated with a
    few Python libraries. With the Scikit-LLM library, this is only a few lines of
    code in either case:Fine-tuning a PaLM model for text classification can be done
    like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在 Google Colab 上完成所有操作，但我们同样可以在本地执行这些操作，只需注意您需要安装 HuggingFace peft
    库！到目前为止，我们已经展示了如何微调和部署开源 LLM。一些商业模型也可以根据自定义数据进行微调。例如，OpenAI 的 GPT-3.5 和 Google
    的 PaLM 模型都具备这个功能。这已经集成到了一些 Python 库中。使用 Scikit-LLM 库，在任何情况下，这都只是几行代码：像这样对 PaLM
    模型进行文本分类的微调：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, you can fine-tune the GPT-3.5 model for text classification like
    this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，您可以这样微调 GPT-3.5 模型进行文本分类：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Interestingly, in the fine-tuning available on OpenAI, all inputs are passed
    through a moderation system to make sure that the inputs are compatible with safety
    standards.This concludes fine-tuning. On the extreme end, LLMs can be deployed
    and queried without any task-specific tuning. By prompting, we can accomplish
    few-shot learning or even zero-shot learning as we’ll discuss in the next section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在OpenAI提供的微调中，所有输入都要通过一套审查系统，以确保输入符合安全标准。这样完成了微调。而在极端情况下，LLMs可以在没有任何任务特定调整的情况下部署和查询。通过提示，我们可以实现少样本学习甚至零样本学习，这将在下一节讨论。
- en: Prompt Engineering
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: 'Prompts are important for steering the behavior of large language models (LLMs)
    because they allow aligning the model outputs to human intentions without expensive
    retraining. Carefully engineered prompts can make LLMs suitable for a wide variety
    of tasks beyond what they were originally trained for. Prompts act as instructions
    that demonstrate to the LLM what is the desired input-output mapping. The picture
    below shows a few examples for prompting different language models (source: “Pre-train,
    Prompt, and Predict - A Systematic Survey of Prompting Methods in Natural Language
    Processing” by Liu and colleagues, 2021):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 提示对于引导大型语言模型（LLMs）的行为很重要，因为它们可以在不需要昂贵的重新训练的情况下，将模型的输出与人类意图对齐。精心设计的提示可以使LLMs适用于各种超出其原始训练范围的任务。提示充当指示，向LLM展示所需的输入-输出映射。下面的图片展示了为不同语言模型提供提示的几个示例（来源：“预训练、提示和预测-自然语言处理中提示方法的系统调查”作者刘和同事，2021年）：
- en: '![Figure 8.5: Prompt examples, particularly knowledge probing in cloze form,
    and summarization.](img/file57.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：提示示例，特别是使用填空形式的知识调查和摘要。](img/file57.png)'
- en: 'Figure 8.5: Prompt examples, particularly knowledge probing in cloze form,
    and summarization.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：提示示例，特别是使用填空形式的知识调查和摘要。
- en: Prompt engineering, also known as in-context learning, refers to techniques
    for steering large language model (LLM) behavior through carefully designed prompts,
    without changing the model weights. The goal is to align the model outputs with
    human intentions for a given task. By designing good prompt templates, models
    can achieve strong results, sometimes comparable to fine-tuning. But how do good
    prompts look like?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程，也被称为上下文学习，指的是通过精心设计的提示技术来引导大型语言模型（LLM）的行为，而无需改变模型权重。其目标是使模型输出与给定任务的人类意图一致。通过设计良好的提示模板，模型可以取得良好的结果，有时可以与微调相媲美。但良好的提示是什么样子呢？
- en: Structure of Prompts
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示的结构
- en: 'Prompts consist of three main components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 提示由三个主要组成部分组成：
- en: Instructions that describe the task requirements, goals and format of inputs/outputs
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述任务要求、目标和输入/输出格式的指示
- en: Examples that demonstrate the desired input-output pairs
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 展示所需的输入-输出对的例子
- en: The input that the model must act on to generate the output
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型必须作用于以生成输出的输入
- en: 'Instructions explain the task to the model unambiguously. Examples provide
    diverse demonstrations of how different inputs should map to outputs. The input
    is what the model must generalize to.Basic prompting methods include zero-shot
    prompting with just the input text, and few-shot prompting with a few demonstration
    examples showing desired input-output pairs. Researchers have identified biases
    like majority label bias and recency bias that contribute to variability in few-shot
    performance. Careful prompt design through example selection, ordering, and formatting
    can help mitigate these issues.More advanced prompting techniques include instruction
    prompting, where the task requirements are described explicitly rather than just
    demonstrated. Self-consistency sampling generates multiple outputs and selects
    the one that aligns best with the examples. Chain-of-thought (CoT) prompting generates
    explicit reasoning steps leading to the final output. This is especially beneficial
    for complex reasoning tasks. CoT prompts can be manually written or generated
    automatically via methods like augment-prune-select.This table gives a brief overview
    of a few methods of prompting compared to fine-tuning:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 说明清楚地向模型解释任务。示例提供了不同输入应该如何映射到输出的多样化演示。输入是模型必须进行泛化的内容。基本提示方法包括零样本提示，仅使用输入文本，以及少样本提示，其中有几个演示示例显示所需的输入输出对。研究人员已经确定了诸如多数标签偏差和最近偏差等偏见，这些偏见导致少样本性能的可变性。通过示例选择、排序和格式化等仔细的提示设计可以帮助减轻这些问题。更高级的提示技术包括指令提示，其中任务要求明确描述而不仅仅是演示。自洽性抽样生成多个输出，并选择与示例最匹配的输出。思维链（CoT）提示生成导致最终输出的明确推理步骤。这对于复杂的推理任务特别有益。思维链提示可以通过手动编写或通过增广-修剪-选择等方法自动生成。这个表格简要概述了几种提示方法与微调的比较：
- en: '| **Technique** | **Method** | **Key Idea** | **Results** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **方法** | **关键想法** | **结果** |'
- en: '| Fine-tuning | Fine-tune on explanation dataset generated via prompting |
    Improves model''s reasoning abilities | 73% accuracy on commonsense QA dataset
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 在通过提示生成的解释数据集上进行微调 | 提高模型的推理能力 | 在常识问答数据集上的准确率为73% |'
- en: '| Zero-shot prompting | Simply feeding the task text to the model and asking
    for results. | Text: "i''ll bet the video game is a lot more fun than the film."<br>-
    Sentiment: |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 零样本提示 | 简单地将任务文本提供给模型并要求结果。 | 文本："我打赌视频游戏比电影更有趣。"<br>- 情感： |  |'
- en: '| Chain-of-Thought (CoT) | Prefix responses with "Let''s think step by step"
    | Gives model space to reason before answering | Quadrupled accuracy on math dataset
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 思维链（CoT） | 在响应前加上"让我们一步一步地思考" | 给模型提供推理空间以在回答之前推理 | 在数学数据集上的准确率增加了四倍 |'
- en: '| Few-shot prompting | Provide few demos consisting of input and desired output
    to help the model understand | Shows desired reasoning format | Tripled accuracy
    on grade school math |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 少样本提示 | 提供由输入和期望输出组成的少量演示来帮助模型理解 | 显示所需的推理格式 | 在小学数学上的准确率提高了两倍 |'
- en: '| Least-to-most prompting | Prompt model for simpler subtasks to solve incrementally.
    "To solve {question}, we need to first solve: " | Decomposes problems into smaller
    pieces | Boosted accuracy from 16% to 99.7% on some tasks |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 由浅入深提示 | 逐步提示模型解决较简单的子任务。"要解决{问题}，我们首先需要解决：" | 将问题分解为较小的部分 | 在某些任务中的准确度从16%提高到99.7%
    |'
- en: '| Selection-inference prompting | Alternate selection and inference prompts
    | Guides model through reasoning steps | Lifts performance on long reasoning tasks
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 选择推理提示 | 交替选择和推理提示 | 引导模型通过推理步骤 | 提高了长推理任务的性能 |'
- en: '| Self-consistency | Pick most frequent answer from multiple samples | Increases
    redundancy | Gained 1-24 percentage points across benchmarks |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 自洽性抽样 | 从多个样本中选择最频繁的答案 | 增加冗余性 | 在各项基准测试中提升了1-24个百分点 |'
- en: '| Verifiers | Train separate model to evaluate responses | Filters out incorrect
    responses | Lifted grade school math accuracy ~20 percentage points |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 核查器 | 训练独立模型评估响应 | 过滤出错误响应 | 提高了小学数学准确度约20个百分点 |'
- en: 'Figure 8.6: Pronpting techniques for LLMs.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：LLMs的提示技术。
- en: 'Some prompting techniques incorporate external information retrieval to provide
    missing context to the LLM before generating the output. For open-domain QA, relevant
    paragraphs can be retrieved via search engines and incorporated into the prompt.
    For closed-book QA, few-shot examples with evidence-question-answer format work
    better than question-answer format.There are different techniques to improve the
    reliability of large language models (LLMs) in complex reasoning tasks:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一些提示技术包括获取外部信息来提供缺失的上下文给LLM，然后在生成输出之前。对于开放领域的问答，可以通过搜索引擎检索相关段落并纳入提示。对于闭卷问答，拥有证据-问题-答案格式的少样本示例比问题-答案格式更有效。对于复杂推理任务，有不同的技术来提高大型语言模型（LLMs）的可靠性：
- en: 'Prompting and Explanation: prompting the model to explain its reasoning step-by-step
    before answering using prompts like "Let''s think step by step" (as in CoT) significantly
    improves accuracy in reasoning tasks.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提问和解释：在回答之前，通过提示模型一步一步解释其推理过程，例如使用"让我们一步一步思考"（如在CoT中），可以显著提高推理任务的准确性。
- en: Providing few-shot examples of reasoning chains helps demonstrate the desired
    format and guides LLMs in generating coherent explanations.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供少样本推理链的例子有助于展示所需的格式，并指导LLM生成连贯的解释。
- en: 'Alternate Selection and Inference Prompts: Utilizing a combination of specialized
    selection prompts (narrow down the answer space) and inference prompts (generate
    the final response) leads to better results compared to generic reasoning prompts
    alone.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交替选择和推理提示：利用专门的选择提示（缩小答案空间）和推理提示（生成最终响应）的组合，与仅使用通用推理提示相比，会得到更好的结果。
- en: 'Problem Decomposition: Breaking down complex problems into smaller subtasks
    or components using a least-to-most prompting approach helps improve reliability,
    as it allows for a more structured and manageable problem-solving process.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题分解：使用逐步提示的方法，将复杂问题分解为较小的子任务或组件，有助于提高可靠性，因为这样可以进行更有结构化和可管理的问题解决过程。
- en: 'Sampling Multiple Responses: Sampling multiple responses from LLMs during generation
    and picking the most common answer increases consistency, reducing reliance on
    a single output. In particular, training separate verifier models that evaluate
    candidate responses generated by LLMs helps filter out incorrect or unreliable
    answers, improving overall reliability.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多次响应采样：在生成过程中从LLMs中采样多个响应，并选择最常见的答案，可以增加一致性，减少对单一输出的依赖。特别是，训练单独的验证模型来评估LLMs生成的候选响应有助于过滤出不正确或不可靠的答案，提高总体可靠性。
- en: Finally, fine-tuning LLMs on explanation datasets generated through prompting
    enhances their performance and reliability in reasoning tasks
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过提示在解释数据集上进行LLMs的微调，增强其在推理任务中的表现和可靠性。
- en: '**Few-shot learning** presents the LLM with just a few input-output examples
    relevant to the task, without explicit instructions. This allows the model to
    infer the intentions and goals purely from demonstrations. Carefully selected,
    ordered and formatted examples can greatly improve the model''s inference abilities.
    However, few shot learning can be prone to biases and variability across trials.
    Adding explicit instructions can make the intentions more transparent to the model
    and improve robustness. Overall, prompts combine the strengths of instructions
    and examples to maximize steering of the LLM for the task at hand.'
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**少样本学习**向LLM提供与任务相关的仅有少量输入-输出示例，而不包含明确的说明。这使得模型能够纯粹从示范中推断出意图和目标。精心选择、排序和格式化的示例可以极大地提高模型的推理能力。然而，少样本学习可能容易受到偏见和试验间的变异影响。添加明确的说明可以使模型更透明地了解意图并提高鲁棒性。总的来说，提示结合了说明和示例的优势，以最大程度地引导LLM完成手头的任务。'
- en: Instead of hand-engineering prompts, methods like automatic prompt tuning learn
    optimal prompts by directly optimizing prefix tokens on the embedding space. The
    goal is to increase the likelihood of desired outputs given inputs. Overall, prompt
    engineering is an active area of research for aligning large pre-trained LLMs
    with human intentions for a wide variety of tasks. Careful prompt design can steer
    models without expensive retraining.In this section, we’ll go through a few (but
    not all) of the techniques mentioned beforehand. Let’s discuss the tools that
    LangChain provides tools to create prompt templates in Python!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与手工设计提示不同，诸如自动提示调整之类的方法通过直接优化嵌入空间上的前缀标记来学习最佳提示。其目标是增加在给定输入情况下所需输出的可能性。总的来说，提示工程是一个积极研究的领域，用于将大型预训练的
    LLM 与各种任务的人类意图保持一致。仔细设计提示可以在不昂贵的重新训练的情况下引导模型。在本节中，我们将讨论之前提到的一些（但不是所有）技术。让我们讨论
    LangChain 提供的用于在 Python 中创建提示模板的工具！
- en: Templating
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模板化
- en: 'Prompts are the instructions and examples we provide to language models to
    steer their behavior. Prompt templating refers to creating reusable templates
    for prompts that can be configured with different parameters.LangChain provides
    tools to create prompt templates in Python. Templates allow prompts to be dynamically
    generated with variable input. We can create a basic prompt template like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们向语言模型提供的指令和示例，以引导其行为。提示模板化是指创建可重用的提示模板，可以使用不同的参数进行配置。LangChain 提供了用于在 Python
    中创建提示模板的工具。模板允许使用变量输入动态生成提示。我们可以像这样创建一个基本提示模板：
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This template has two input variables - {adjective} and {topic}. We can format
    these with values:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此模板有两个输入变量 - {adjective} 和 {topic}。我们可以使用值格式化这些变量：
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The template format defaults to Python f-strings, but Jinja2 is also supported.Prompt
    templates can be composed into pipelines, where the output of one template is
    passed as input to the next. This allows modular reuse.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模板格式默认为 Python f-strings，但也支持 Jinja2。提示模板可以组合成管道，其中一个模板的输出作为下一个模板的输入传递。这允许模块化重用。
- en: Chat Prompt Templates
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 聊天提示模板
- en: 'For conversational agents, we need chat prompt templates:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于会话代理，我们需要聊天提示模板：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This formats a list of chat messages instead of a string. This can be useful
    for taking the history of a conversation into account. We’ve looked at different
    memory methods in Chapter 5\. These are similarly relevant in this context to
    make sure model outputs are relevant and on point.Prompt templating enables reusable,
    configurable prompts. LangChain provides a Python API for conveniently creating
    templates and formatting them dynamically. Templates can be composed into pipelines
    for modularity. Advanced prompt engineering can further optimize prompting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这会格式化聊天消息列表而不是字符串。这对考虑对话历史可能会有所帮助。我们在第五章已经研究了不同的记忆方法。在此上下文中，这些方法同样相关，以确保模型输出相关且准确。提示模板化可以实现可重用的、可配置的提示。LangChain
    提供了一个 Python API，方便地创建模板并动态格式化它们。模板可以组合成管道以实现模块化。高级提示工程可以进一步优化提示。
- en: Advanced Prompt Engineering
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级提示工程
- en: LangChain provides tools to enable advanced prompt engineering strategies like
    few-shot learning, dynamic example selection, and chained reasoning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 提供了一些工具，以实现高级提示工程策略，如少样本学习、动态示例选择和链式推理。
- en: Few-Shot Learning
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少样本学习
- en: 'The `FewShotPromptTemplate` allows showing the model just a few demonstration
    examples of the task to prime it, without explicit instructions. For instance:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`FewShotPromptTemplate` 允许仅向模型展示任务的几个演示示例以引导它，而无需明确的说明。例如：'
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The model must infer what to do from the examples alone.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须仅通过示例推断出要做什么。
- en: Dynamic Example Selection
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动态示例选择
- en: 'To choose examples tailored to each input, `FewShotPromptTemplate` can accept
    an `ExampleSelector` rather than hardcoded examples:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择适合每个输入的示例，`FewShotPromptTemplate` 可以接受一个 `ExampleSelector` 而不是硬编码的示例：
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`ExampleSelector` implementations like `SemanticSimilarityExampleSelector`
    automatically find the most relevant examples for each input.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExampleSelector` 的实现，如 `SemanticSimilarityExampleSelector`，会自动为每个输入找到最相关的示例。'
- en: Chained Reasoning
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 链式推理
- en: 'When asking an LLM to reason through a problem, it is often more effective
    to have it explain its reasoning before stating the final answer. For example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在要求 LLM 推理问题时，让其在陈述最终答案之前解释其推理通常更有效。例如：
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This encourages the LLM to logically think through the problem first, rather
    than just guessing the answer and trying to justify it after. This is called **Zero-Shot
    Chain of Thought**. Asking an LLM to explain its thought process aligns well with
    its core capabilities.**Few-Shot Chain of Thought** prompting is a few-shot prompt,
    where the reasoning is explained as part of the example solutions, with the idea
    to encourage an LLM to explain its reasoning before making a decision. It has
    been shown that this kind of prompting can lead to more accurate results, however,
    this performance boost was found to be proportional to the size of the model,
    and the improvements seemed to be negligible or even negative in smaller models.In
    **Tree of Thoughts (ToT)** prompting, we are generating multiple problem-solving
    steps or approaches for a given prompt and then using the AI model to critique
    these steps. The critique will be based on the model’s judgment of the solution’s
    suitability to the problem. Let''s walk through a more detailed example of implementing
    ToT using LangChain.First, we''ll define our 4 chain components with `PromptTemplates`.
    We need a solution template, an evaluation template, a reasoning template, and
    a ranking template. Let’s first generate solutions:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这鼓励LLM首先通过逻辑来思考问题，而不是只是猜测答案，然后试图在之后证明它。这被称为**零射链思维**。要求LLM解释其思维过程与其核心能力很好地吻合。**少射链思维**提示是少射提示，其中推理被解释为示例解决方案的一部分，旨在鼓励LLM在做出决定之前解释其推理。已经证明这种提示可以导致更准确的结果，但是，发现这种性能提升与模型大小成正比，而且在较小的模型中改进似乎是微不足道甚至是负面的。在**Tree
    of Thoughts (ToT)**提示中，我们正在为给定提示生成多个解决步骤或方法，然后使用AI模型对这些步骤进行批判。评价将基于模型对解决方案是否适用于问题的判断。让我们通过使用LangChain来实现ToT的更详细示例。首先，我们将使用`PromptTemplates`定义我们的4个链组件。我们需要一个解决方案模板、一个评估模板、一个推理模板和一个排名模板。让我们首先生成解决方案：
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s ask the LLM to evaluate these solutions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让LLM评估这些解决方案：
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we’ll reason a bit more about them:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将更多地推理一下它们：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can rank these solutions given our reasoning so far:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，根据我们迄今为止的推理，我们可以对这些解决方案进行排名：
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we create chains from these templates before we’ll put it all together:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从这些模板创建链，然后我们将所有内容放在一起：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally. we connect these chains into a `SequentialChain`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这些链连接成一个`SequentialChain`：
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This allows us to leverage the LLM at each stage of the reasoning process. The
    ToT approach helps avoid dead-ends by fostering exploration.These techniques collectively
    enhance the accuracy, consistency, and reliability of large language models' reasoning
    capabilities on complex tasks by providing clearer instructions, fine-tuning with
    targeted data, employing problem breakdown strategies, incorporating diverse sampling
    approaches, integrating verification mechanisms, and adopting probabilistic modeling
    frameworks.Prompt design is highly significant for unlocking LLM reasoning capabilities,
    the potential for future advancements in models and prompting techniques, and
    these principles and techniques form a valuable toolkit for researchers and practitioners
    working with large language models.Let’s summarize!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够在推理过程的每个阶段利用LLM。 ToT方法有助于通过促进探索来避免死胡同。这些技术共同增强了大型语言模型在复杂任务上推理能力的准确性、一致性和可靠性，方法包括提供更清晰的指导、使用定向数据进行微调、采用问题分解策略、结合多样化的抽样方法、整合验证机制，以及采用概率建模框架。Prompt设计对于释放LLM的推理能力、模型和Prompt技术未来发展的潜力以及这些原则和技术对于与大型语言模型一起工作的研究人员和从业者都具有重要意义。让我们总结一下！
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In Chapter 1, we discussed the basic principles of generative models, particularly
    LLMs, and their training. We focused mostly on the pre-training step, which is
    – generally speaking – adjusting the models to the correlations within words and
    wider segments of texts. Alignment it the assessment of model outputs against
    expectations and conditioning is the process of making sure the output is according
    to expectations. Conditioning allows steering generative AI to improve safety
    and quality, but it is not a complete solution. In this chapter, the focus is
    on conditioning, in particular through fine-tuning and prompting. In fine-tuning
    the language model is trained on many examples of tasks formulated as natural
    language instructions, along with appropriate responses. Often this is done through
    reinforcement learning with human feedback (RLHF), which involves training on
    a dataset of human-generated (prompt, response) pairs, followed by reinforcement
    learning from human feedback, however, other techniques have been developed that
    have been shown to produce competitive results with lower resource footprints.
    In the first recipe of this chapter, we’ve implemented a fine-tuning of a small
    open-source model for question answering.There are many techniques to improve
    the reliability of LLMs in complex reasoning tasks including step-by-step prompting,
    alternate selection, and inference prompts, problem decomposition, sampling multiple
    responses, and employing separate verifier models. These methods have shown to
    enhance accuracy and consistency in reasoning tasks. We’ve discussed and compared
    several techniques. LangChain provides building blocks to unlock advanced prompting
    strategies like few-shot learning, dynamic example selection, and chained reasoning
    decomposition as we’ve shown in the examples.Careful prompt engineering is key
    to aligning language models with complex objectives. Reliability in reasoning
    can be improved by breaking down problems and adding redundancy. The principles
    and techniques that we’ve discussed in this chapter provide a toolkit for experts
    working with LLMs. We can expect future advancements in both model training and
    prompting techniques. As these methods and LLMs continue to develop, they will
    likely become even more effective and useful for a broader range of applications.Let’s
    see if you remember some more key points from this chapter!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们讨论了生成模型的基本原理，特别是LLMs（大型语言模型）及其训练。我们主要关注的是预训练阶段，通常是调整模型以符合单词和文本更广泛段落之间的相关性。对齐（alignment）是对模型输出进行评估，确保输出符合预期，而条件化（conditioning）是确保输出符合预期的过程。条件化可以引导生成式人工智能以提高安全性和质量，但它并不是一个完整的解决方案。本章重点介绍了条件化，特别是通过微调和提示进行。在微调中，语言模型通过许多以自然语言指令形式表述的任务示例以及相应的回答进行训练。通常情况下，这是通过强化学习和人类反馈（RLHF）来完成的，这涉及对人类生成的（提示，回答）对数据集进行训练，然后通过人类反馈进行强化学习，然而，已经开发出其他技术，证明其能够以更低的资源占用量产生具有竞争力的结果。在本章的第一个配方中，我们实现了一个小型开源模型用于问答的微调。有许多技术可以提高LLMs在复杂推理任务中的可靠性，包括分步提示，备选选择和推理提示，问题分解，抽样多个响应以及使用单独的验证模型。这些方法已被证明可以提高推理任务中的准确性和一致性。我们已经讨论并比较了几种技术。LangChain提供了解锁高级提示策略的构建模块，例如少样本学习，动态示例选择和链式推理分解，正如我们在示例中所展示的。仔细的提示工程是对齐语言模型与复杂目标的关键。通过分解问题和添加冗余可以提高推理的可靠性。本章讨论的原则和技术为与LLMs一起工作的专家提供了工具包。我们可以期待模型训练和提示技术的未来发展。随着这些方法和LLMs的持续发展，它们可能会变得更加有效，并且对更广泛的应用范围更加有用。让我们看看你是否记得本章的一些关键点！
- en: Questions
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 请看一下，看看你是否能够凭记忆回答这些问题。如果你对其中任何一个不确定，我建议你回到本章的相应部分去查看一下：
- en: What’s alignment in the context of LLMs?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM（语言模型）的背景下，什么是对齐（alignment）？
- en: What are different methods of conditioning and how can we distinguish them?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些不同的条件化方法，我们如何区分它们？
- en: How’s moderation related to conditioning?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调节（moderation）与条件化有何关联？
- en: What is instruction tuning and what’s its importance?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是指令调整（instruction tuning），以及它的重要性是什么？
- en: What is quantization?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 量化是什么？
- en: What are a few methods for fine-tuning?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些微调的方法？
- en: What is few-shot learning?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是少样本学习（few-shot learning）？
- en: What is Chain of Thought prompting?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是思维链提示（Chain of Thought prompting）？
- en: Explain Tree of Thought prompting!
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释一下思维树提示（Tree of Thought prompting）！
