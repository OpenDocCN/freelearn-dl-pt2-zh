- en: 8 Customizing LLMs and their output
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8自定义LLMs及其输出
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Discord上加入我们的书籍社区
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![Qr code Description automatically generated](img/file54.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![二维码说明会自动生成](img/file54.png)'
- en: 'This chapter is about techniques and best practices to improve the reliability
    and performance of large language models (LLMs) in certain scenarios such as on
    complex reasoning and problem-solving tasks. Generally, this process of adapting
    a model for a certain task or making sure that our model output corresponds to
    what we expect is called conditioning. In this chapter, we’ll discuss fine-tuning
    and prompting as methods for conditioning.Fine-tuning involves training the pre-trained
    base model on specific tasks or datasets relevant to the desired application.
    This process allows the model to adapt and become more accurate and contextually
    aligned for the intended use case. Similarly, by providing additional input or
    context at inference time, large language models (LLMs) can generate text tailored
    to a particular tasks or style.Prompt design is highly significant for unlocking
    LLM reasoning capabilities, the potential for future advancements in models and
    prompting techniques, and these principles and techniques form a valuable toolkit
    for researchers and practitioners working with large language models. Understanding
    how LLMs generate text token-by-token helps create better reasoning prompts.Prompting
    is still an empirical art - trying variations to see what works is often needed.
    But some prompt engineering insights can transfer across models and tasks. We’ll
    discuss the tools in LangChain to enable advanced prompt engineering strategies
    like few-shot learning, dynamic example selection, and chained reasoning.Throughout
    the chapter, we’ll work on fine-tuning and prompting with LLMs, which you can
    find in the `notebooks` directory in the Github repository for the book at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)The
    main sections are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讨论改进大型语言模型（LLMs）在复杂推理和问题解决任务等特定情况下的可靠性和性能的技术和最佳实践。一般来说，使模型适应特定任务或确保模型输出与我们期望的一致的过程称为条件。在本章中，我们将讨论微调和提示作为条件的方法。微调涉及在特定任务或与所需应用相关的数据集上训练预训练基础模型。这个过程允许模型适应并在预期用例中变得更准确和上下文对齐。同样，通过在推断时提供额外的输入或上下文，大型语言模型（LLMs）可以生成适合特定任务或风格的文本。提示设计对于释放LLM的推理能力，模型和提示技术未来进展的潜力非常重要，这些原则和技术构成了研究人员和从业者使用大型语言模型的宝贵工具包。了解LLMs如何逐个标记地生成文本有助于创建更好的推理提示。提示仍然是一个经验性的艺术——通常需要尝试各种变化来看看什么有效。但一些提示工程的见解可以在模型和任务之间转移。我们将讨论LangChain中的工具，以实现高级提示工程策略，如少样本学习、动态示例选择和链式推理。在本章的整个过程中，我们将使用LLMs进行微调和提示，您可以在书的Github存储库的`notebooks`目录中找到，网址是[https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain)主要章节包括：
- en: Conditioning and Alignment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件和对齐
- en: Fine-Tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调
- en: Prompt Engineering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Let’s start by discussing conditioning and alignment, why it’s important, and
    how we can achieve it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先讨论条件和对齐，为什么重要，以及如何实现它。
- en: Conditioning and alignment
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件和对齐
- en: Alignment, in the context of generative AI models, refers to ensuring that the
    outputs of these models are consistent with human values, intentions, or desired
    outcomes. It involves guiding the model's behavior to be in line with what is
    considered ethical, appropriate, or relevant within a given context. The concept
    of alignment is crucial to avoid generating outputs that may be biased, harmful,
    or deviate from the intended purpose. Addressing alignment involves careful attention
    to the biases present in training data, iterative feedback loops involving human
    reviewers, refining objective functions during training/fine-tuning stages, leveraging
    user feedback, and continuous monitoring during deployment to ensure ongoing alignment.There
    are several reasons one may want to condition a large language model. The first
    is to control the content and style of the outputs. For example, conditioning
    on certain keywords or attributes like formality level can produce more relevant
    and high-quality text. Conditioning also encompasses safety measures to prevent
    the generation of malicious or harmful content. For example, avoiding generating
    misleading information, inappropriate suggestions, or potentially dangerous instructions,
    or – more generally aligning the model with certain values.The potential benefits
    of conditioning large language models are numerous. By providing more specific
    and relevant input, we can achieve outputs that are tailored to our needs. For
    instance, in a customer support chatbot, conditioning the model with user queries
    allows it to generate responses that address their concerns accurately. Conditioning
    also helps control biased or inappropriate outputs by constraining the model's
    creativity within specific boundaries.Furthermore, by conditioning large language
    models, we can make them more controllable and adaptable. We can fine-tune and
    shape their behavior according to our requirements and create AI systems that
    are reliable in specific domains such as legal advice or technical writing.However,
    there are potential downsides to consider as well. Conditioning models too heavily
    might result in overfitting, where they become excessively reliant on specific
    inputs and struggle with generating creative or diverse outputs in different contexts.
    Moreover, conditioning should be utilized responsibly since large language models
    have the tendency to amplify biases present in the training data. Care must be
    taken not to exacerbate issues related to bias or controversial topics when conditioning
    these models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式人工智能模型的背景下，"alignment" 意味着确保这些模型的输出与人类的价值观、意图或期望结果一致。它涉及指导模型的行为与在特定上下文中被认为道德、适当或相关的内容保持一致。"alignment"
    概念对于避免生成可能存在偏见、有害或偏离预期目的的输出至关重要。解决"alignment" 问题需要注意训练数据中存在的偏见，涉及人类审阅者的反馈循环，在训练/微调阶段改进客观函数，利用用户反馈，并在部署过程中持续监测以确保持续对齐。一个可能想对大型语言模型进行条件的原因有几个。第一个是控制输出的内容和风格。例如，根据某些关键词或属性如正式程度进行条件，可以产生更相关和高质量的文本。条件还包括安全措施，以防止生成恶意或有害内容。例如，避免生成误导性信息、不当建议或潜在危险指示，或者更一般地对齐模型与某些价值观。对大型语言模型进行条件的潜在好处很多。通过提供更具体和相关的输入，我们可以得到符合我们需求的输出。例如，在客服聊天机器人中，对模型进行条件，允许它生成准确解决用户问题的回应。条件还有助于通过限制模型创造性在特定边界内来控制有偏见或不当的输出。此外，通过对大型语言模型进行条件，我们可以使它们更可控和适应性更强。我们可以根据我们的需求对其进行微调和塑造行为，并创建在特定领域如法律建议或技术写作中可靠的人工智能系统。然而，也有潜在的不利因素需要考虑。过于强烈地对模型进行条件可能导致过拟合，导致模型过分依赖特定输入，在不同环境中难以产生创造性或多样化的输出。此外，应该有责任地利用条件，因为大型语言模型有放大训练数据中存在偏见的倾向。在对这些模型进行条件时，必须小心不要加剧与偏见或有争议话题相关的问题。
- en: '**Benefits of alignment** include:'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**对齐的好处** 包括：'
- en: 'Enhanced User Experience: Aligned models generate outputs that are relevant
    to user queries or prompts.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强用户体验：对齐的模型生成与用户查询或提示相关的输出。
- en: 'Trust-building: Ensuring ethical behavior helps build trust among users/customers.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立信任：确保道德行为有助于在用户/客户之间建立信任。
- en: 'Brand Reputation: By aligning with business goals regarding branding consistency
    and desired tone/style guidelines.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 品牌声誉：通过与关于品牌一致性和期望的语气/风格指南的业务目标保持一致。
- en: 'Mitigating Harmful Effects: Alignment with safety, security, and privacy considerations
    helps prevent the generation of harmful or malicious content.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解有害影响：与安全、保密和隐私考虑的对齐有助于防止生成有害或恶意内容。
- en: '**Potential downsides** include:'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**潜在的缺点**包括：'
- en: 'Challenging Balance: Striking a balance between extreme alignment (overly conservative)
    and creative freedom (overly permissive) can be difficult.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡挑战：在极端对齐（过于保守）和创造性自由（过于宽松）之间取得平衡可能很困难。
- en: 'Limitations of Automated Metrics: Quantitative evaluation metrics might not
    capture alignment nuances fully.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动度量标准的限制：定量评估指标可能无法完全捕捉对齐的微妙差异。
- en: 'Subjectivity: Alignment judgments are often subjective, requiring careful consideration
    and consensus building on desired values and guidelines.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主观性：对齐判断往往是主观的，需要对所需价值观和指导方针进行认真考虑和共识建立。
- en: Pre-training a large model on diverse data to learn patterns and language understanding
    results in a base model that has a broad understanding of various topics but lacks
    specificity or alignment to any particular context. While base models such as
    GPT-4 are capable of generating impressive text on a wide range of topics, conditioning
    them can enhance their capabilities in terms of task relevance, specificity, and
    coherence, and make their outputs more relevant and on-topic. Without conditioning,
    these models tend to generate text that may not always align perfectly with the
    desired context. By conditioning them, we can guide the language models to produce
    outputs that are more closely related to the given input or instructions. The
    major advantage of conditioning is that it allows guiding the model without extensive
    retraining. It also enables interactive control and switching between modes. Conditioning
    can happen at different stages of the model development cycle—from fine-tuning
    to output generation in various contexts. There are several options for achieving
    alignment of large language models. One approach is to condition during fine-tuning,
    by training the model on a dataset reflective of the desired outputs. This allows
    the model to specialize, but requires access to relevant training data. Another
    option is to dynamically condition the model at inference time by providing conditional
    input along with the main prompt. This is more flexible but introduces some complexity
    during deployment.In the next section, I will summarize key methods for alignment
    such as fine-tuning and prompt engineering, discuss the rationale, and examine
    their relative pros and cons.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练大型模型以学习模式和语言理解会导致一个基础模型，该模型具有对各种主题的广泛理解，但缺乏特定上下文的特定性或对齐性。尽管像GPT-4这样的基础模型能够在各种主题上生成令人印象深刻的文本，但通过对它们进行调节可以增强它们在任务相关性、特定性和连贯性方面的能力，并使它们的输出更相关和贴切。没有条件的话，这些模型往往会生成与所需上下文不完全一致的文本。通过对其进行调节，我们可以引导语言模型生成更与给定输入或指令相关的输出。调节的主要优势在于它允许在不进行大量重新训练的情况下引导模型。它还可以实现交互式控制和在不同模式之间切换。调节可以在模型开发周期的不同阶段进行——从微调到在各种上下文中生成输出。有几种实现大型语言模型对齐的选择。一种方法是在微调过程中进行条件化，通过对模型进行反映所需输出的数据集进行训练。这样可以使模型专业化，但需要访问相关的训练数据。另一种选择是在推理时动态地对模型进行条件化，通过提供条件输入和主提示。这样更灵活，但在部署过程中引入了一些复杂性。在下一节中，我将总结关键的对齐方法，如微调和提示工程，讨论其原理，并分析它们的相对优缺点。
- en: Methods for alignment
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对齐方法
- en: 'With the advent of large pre-trained language models like GPT-3, there has
    been growing interest in techniques to adapt these models for downstream tasks.
    This process is known as fine-tuning. Fine-tuning allows pre-trained models to
    be customized for specific applications while leveraging the vast linguistic knowledge
    acquired during pre-training. The idea of adapting pre-trained neural networks
    originated in computer vision research in the early 2010s. In NLP, Howard and
    Ruder (2018) demonstrated the effectiveness of fine-tuning pre-trained contextual
    representations like ELMo and ULMFit on downstream tasks. The seminal BERT model
    (Devlin and others., 2019) established fine-tuning of pre-trained transformers
    as the de facto approach in NLP.The need for fine-tuning arises because pre-trained
    LMs are designed to model general linguistic knowledge, not specific downstream
    tasks. Their capabilities manifest only when adapted to particular applications.
    Fine-tuning allows pre-trained weights to be updated for target datasets and objectives.
    This enables knowledge transfer from the general model while customizing it for
    specialized tasks. Several approaches have been proposed for alignment, with trade-offs
    in efficacy and efficiency and it’s worth to delve a bit more into the details
    of each of these alignment methods. **Full Fine-Tuning** involves updating all
    the parameters of the pre-trained language model during fine-tuning. The model
    is trained end-to-end on the downstream tasks, allowing the weights to be updated
    globally to maximize performance on the target objectives. FFT consistently achieves
    strong results across tasks but requires extensive computational resources and
    large datasets to avoid overfitting or forgetting.In **Adapter Tuning** additional
    trainable adapter layers are inserted, usually bottleneck layers, into the pre-trained
    model while keeping the original weights frozen. Only the newly added adapter
    layers are trained on the downstream tasks. This makes tuning parameter-efficient
    as only a small fraction of weights are updated. However, as the pre-trained weights
    remain fixed, adapter tuning has a risk of underfitting to the tasks. The insertion
    points and capacity of the adapters impact overall efficacy.**Prefix Tuning**:
    This prepends trainable vectors to each layer of the LM, which are optimized during
    fine-tuning while the base weights remain frozen. The prefixes allow injection
    of inductive biases into the model. Prefix tuning has a smaller memory footprint
    compared to adapters but has not been found to be as effective. The length and
    initialization of prefixes impact efficacy.In **Prompt Tuning**, the input text
    is appended with trainable prompt tokens which provide a soft prompt to induce
    the desired behavior from the LM. For example, a task description can be provided
    as a prompt to steer the model. Only the added prompt tokens are updated during
    training while the pre-trained weights are frozen. Performance is heavily influenced
    by prompt engineering. Automated prompting methods are being explored.**Low-Rank
    Adaptation (LoRA)** adds pairs of low-rank trainable weight matrices to the frozen
    LM weights. For example, to each weight W, low-rank matrices B and A are added
    such that the forward pass uses W + BA. Only B and A are trained, keeping the
    base W frozen. LoRA achieves reasonable efficacy with greater parameter efficiency
    than full tuning. The choice of rank r impacts tradeoffs. LoRA enables tuning
    giant LMs on limited hardware.Another way to ensure proper alignment of outputs
    is through **human oversight** methods like human-in-the-loop systems. These systems
    involve human reviewers who provide feedback and make corrections if necessary.
    Human involvement helps align generated outputs with desired values or guidelines
    set by humans.Here is a table summarizing the different techniques for steering
    generative AI outputs:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '| **Stage** | **Technique** | **Examples** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| Training | Pre-training | Training on diverse data |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '|  | Objective Function | Careful design of training objective |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '|  | Architecture and Training Process | Optimizing model structure and training
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| Fine-Tuning | Specialization | Training on specific datasets/tasks |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| Inference-Time Conditioning | Dynamic Inputs | Prefixes, control codes, context
    examples |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| Human Oversight | Human-in-the-Loop | Human review and feedback |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: 'Figure 8.1: Steering generative AI outputs.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Combining these techniques provides developers with more control over the behavior
    and outputs of generative AI systems. The ultimate goal is to ensure that human
    values are incorporated at all stages, from training to deployment, in order to
    create responsible and aligned AI systems.Furthermore, careful design choices
    in the pre-training objective function also impact what behaviors and patterns
    the language model learns initially. By incorporating ethical considerations into
    these objective functions, developers can influence the initial learning process
    of large language models.We can distinguish a few more approaches in fine-tuning
    such as online and offline. InstructGPT was considered a game-changer because
    it demonstrated the potential to significantly improve language models, such as
    GPT-3, by incorporating reinforcement learning from human feedback (RLHF). Let’s
    talk about the reasons why InstructGPT had such a transformative impact.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning with human feedback
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In their March 2022 paper, Ouyang and others from OpenAI demonstrated using
    reinforcement learning from human feedback (RLHF) with proximal policy optimization
    (PPO) to align large language models like GPT-3 with human preferences. Reinforcement
    learning from human feedback (RLHF) is an online approach that fine-tunes LMs
    using human preferences. It has three main steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised pre-training: The LM is first trained via standard supervised learning
    on human demonstrations.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reward model training: A reward model is trained on human ratings of LM outputs
    to estimate reward.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RL fine-tuning: The LM is fine-tuned via reinforcement learning to maximize
    expected reward from the reward model using an algorithm like PPO.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main change, RLHF, allows incorporating nuanced human judgments into language
    model training through a learned reward model. As a result, human feedback can
    steer and improve language model capabilities beyond standard supervised fine-tuning.
    This new model can be used to follow instructions that are given in natural language,
    and it can answer questions in a way that’s more accurate and relevant than GPT-3\.
    InstructGPT outperformed GPT-3 on user preference, truthfulness, and harm reduction,
    despite having 100x fewer parameters.Starting in March 2022, OpenAI started releasing
    the GPT-3.5 series models, upgraded versions of GPT-3, which include fine-tuning
    with RLHF.There are three advantages of fine-tuning that were immediately obvious
    to users of these models:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 主要变化RLHF，通过学习的奖励模型将细微的人类判断纳入语言模型的训练中。因此，人类反馈可以引导和改进语言模型的能力，超越标准的监督微调。这种新模型可以用于遵循用自然语言给出的指示，并且可以回答问题的准确性和相关性比GPT-3更高。尽管参数少了100倍，但InstructGPT在用户偏好、真实性和减少伤害方面都优于GPT-3。从2022年3月开始，OpenAI开始发布GPT-3.5系列模型，这些模型是GPT-3的升级版本，包括与RLHF微调。这些模型的用户立即发现了微调的三个优势：
- en: 'Steerability: the capability of models to follow instructions (instruction-tuning)'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Steerability: 模型遵循指示的能力（指令调整）'
- en: 'Reliable output-formatting: this became important, for example, for API calls/function
    calling)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可靠的输出格式：这对于API调用/函数调用变得重要。
- en: 'Custom tone: this makes it possible to adapt the output style as appropriate
    to task and audience.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自定义语气：这使得可以根据任务和受众的需求调整输出风格。
- en: InstructGPT opened up new avenues for improving language models by incorporating
    reinforcement learning from human feedback methods beyond traditional fine-tuning
    approaches. RL training can be unstable and computationally expensive, notwithstanding,
    its success inspired further research into refining RLHF techniques, reducing
    data requirements for alignment, and developing more powerful and accessible models
    for a wide range of applications.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGPT通过将人类反馈的强化学习方法纳入传统微调方法之外，开辟了改进语言模型的新途径。尽管强化学习训练可能不稳定且计算成本高昂，但其成功激发了进一步研究，以改进强化学习人机反馈技术，减少对齐的数据需求，以及为各种应用开发更强大和更易于访问的模型。
- en: Offline Approaches
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 离线方法
- en: 'Offline methods circumvent the complexity of online RL by directly utilizing
    human feedback. We can distinguish between ranking-based on language-based approaches:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 离线方法通过直接利用人类反馈来绕过在线强化学习的复杂性。我们可以区分基于排名和基于语言的方法：
- en: 'Ranking-based: Human rankings of LM outputs are used to define optimization
    objectives for fine-tuning, avoiding RL entirely. This includes methods like Preference
    Ranking Optimization (PRO; Song et al., 2023) and Direct Preference Optimization
    (DPO; Rafailov et al., 2023).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于排名的：人类对语言模型输出进行排名，用于定义微调的优化目标，完全避免了强化学习。这包括Preference Ranking Optimization
    (PRO; Song等人，2023)和Direct Preference Optimization (DPO; Rafailov等人，2023)等方法。
- en: 'Language-based: Human feedback is provided in natural language and utilized
    via standard supervised learning. For example, Chain of Hindsight (CoH; Liu et
    al., 2023) converts all types of feedback into sentences and uses them to fine-tune
    the model, taking advantage of the language comprehension capabilities of language
    models.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于语言：人类通过自然语言提供反馈，并通过标准监督学习利用。例如，Chain of Hindsight (CoH; Liu等人，2023)将所有类型的反馈转换为句子，并用它们微调模型，利用语言模型的语言理解能力。
- en: 'Direct Preference Optimization (DPO) is a simple and effective method for training
    language models to adhere to human preferences without needing to explicitly learn
    a reward model or use reinforcement learning. While it optimizes the same objective
    as existing RLHF methods, it is much simpler to implement, more stable, and achieves
    strong empirical performance.Researchers from Meta, in the paper “*LIMA: Less
    Is More for Alignment*”, simplified alignment by minimizing a supervised loss
    on only 1,000 carefully curated prompts in fine-training the LLaMa model. Based
    on the favorable human preferences when comparing their outputs to DaVinci003
    (GPT-3.5), they conclude that fine-training has only minimal importance. This
    they refer to as the superficial alignment hypothesis.Offline approaches offer
    more stable and efficient tuning. However, they are limited by static human feedback.
    Recent methods try to combine offline and online learning.While both DPO and RLHF
    with PPO aim to align LLMs with human preferences, they differ in terms of complexity,
    data requirements, and implementation details. DPO offers simplicity but achieves
    strong performance by directly optimizing probability ratios. On the other hand,
    RLHF with PPO in InstructGPT introduces more complexity but allows for nuanced
    alignment through reward modeling and reinforcement learning optimization.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Low-Rank Adaptation
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLMs have achieved impressive results in Natural Language Processing and are
    now being used in other domains such as Computer Vision and Audio. However, as
    these models become larger, it becomes difficult to train them on consumer hardware
    and deploying them for each specific task becomes expensive. There are a few methods
    that reduce computational, memory, and storage costs, while improving performance
    in low-data and out-of-domain scenarios.Low-Rank Adaptation (LoRA) freezes the
    pre-trained model weights and introduces trainable rank decomposition matrices
    into each layer of the Transformer architecture to reduce the number of trainable
    parameters. LoRA achieves comparable or better model quality compared to fine-tuning
    on various language models (RoBERTa, DeBERTa, GPT-2, and GPT-3) while having fewer
    trainable parameters and higher training throughput.The QLORA method is an extension
    of LoRA, which enables efficient fine-tuning of large models by backpropagating
    gradients through a frozen 4-bit quantized model into learnable low-rank adapters.
    This allows fine-tuning a 65B parameter model on a single GPU. QLORA models achieve
    99% of ChatGPT performance on Vicuna using innovations like new data types and
    optimizers. In particular, QLORA reduces the memory requirements for fine-tuning
    a 65B parameter model from >780GB to <48GB without affecting runtime or predictive
    performance.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantization** refers to techniques for reducing the numerical precision
    of weights and activations in neural networks like large language models (LLMs).
    The main purpose of quantization is to reduce the memory footprint and computational
    requirements of large models.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**量化**指的是降低神经网络中权重和激活的数值精度的技术，例如大型语言模型（LLMs）。量化的主要目的是减少大型模型的内存占用和计算需求。'
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Some key points about quantization of LLMs:'
  id: totrans-51
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 关于LLM量化的一些关键点：
- en: It involves representing weights and activations using fewer bits than standard
    single-precision floating point (FP32). For example, weights could be quantized
    to 8-bit integers.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它涉及使用比标准单精度浮点（FP32）更少的比特来表示权重和激活。例如，权重可以量化为8位整数。
- en: This allows shrinking model size by up to 4x and improving throughput on specialized
    hardware.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以通过将可训练向量前置到LLM层来缩小模型大小最多4倍，并提高专用硬件的吞吐量。
- en: Quantization typically has a minor impact on model accuracy, especially with
    re-training.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化通常对模型准确度影响不大，尤其是在重新训练时。
- en: Common quantization methods include scalar, vector, and product quantization
    which quantize weights separately or in groups.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的量化方法包括标量、向量和乘积量化，这些方法将权重分别或分组量化。
- en: Activations can also be quantized by estimating their distribution and binning
    appropriately.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过估计激活的分布并适当分组，激活也可以进行量化。
- en: Quantization-aware training adjusts weights during training to minimize quantization
    loss.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化感知训练在训练过程中调整权重以最小化量化损失。
- en: LLMs like BERT and GPT-3 have been shown to work well with 4-8 bit quantization
    via fine-tuning.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像BERT和GPT-3之类的LLM已经通过微调展示了对4-8位量化的良好适应性。
- en: Parameter-Efficient Fine-tuning (PEFT) methods enable the use of small checkpoints
    for each task, making the models more portable. These small trained weights can
    be added on top of the LLM, allowing the same model to be used for multiple tasks
    without replacing the entire model.In the next section, we’ll discuss methods
    for conditioning large language models (LLMs) at inference time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 参数高效微调（PEFT）方法使得可以为每个任务使用小检查点，使得模型更具可移植性。这些小训练权重可以添加到LLM之上，允许同一模型用于多个任务而无需替换整个模型。在下一节中，我们将讨论在推理时如何为大型语言模型（LLMs）进行条件化的方法。
- en: Inference-Time conditioning
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理时条件化
- en: 'One commonly used approach is **conditioning at inference time** (output generation
    phase) where specific inputs or conditions are provided dynamically to guide the
    output generation process. LLM fine-tuning may not always be feasible or beneficial
    in certain scenarios:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的方法是在推理时进行条件化（输出生成阶段），在这个过程中动态提供特定的输入或条件以引导输出生成过程。在某些场景中，LLM微调并不总是可行或有益的：
- en: 'Limited Fine-Tuning Services: Some models are only accessible through APIs
    that lack or have restricted fine-tuning capabilities.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有限的微调服务：一些模型只能通过缺乏或具有受限微调能力的API访问。
- en: 'Insufficient Data: In cases where there is a lack of data for fine-tuning,
    either for the specific downstream task or relevant application domain.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据不足：在特定下游任务或相关应用领域缺乏微调所需的数据的情况下。
- en: 'Dynamic Data: Applications with frequently changing data, such as news-related
    platforms, may struggle to fine-tune models frequently, leading to potential drawbacks.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动态数据：像新闻相关平台这样频繁更改数据的应用可能难以频繁微调模型，导致潜在的缺点。
- en: 'Context-Sensitive Applications: Dynamic and context-specific applications like
    personalized chatbots cannot perform fine-tuning based on individual user data.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上下文敏感应用：像个性化聊天机器人这样的动态和特定上下文的应用无法根据个人用户数据执行微调。
- en: 'For conditioning at inference time, most commonly, we provide a textual prompt
    or instruction at the beginning of the text generation process. This prompt can
    be a few sentences or even a single word, acting as an explicit indication of
    the desired output. Some common techniques for dynamic inference-time conditioning
    include:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段进行条件化时，最常见的做法是在文本生成过程开始时提供文本提示或指令。这个提示可以是几句话甚至是一个单词，作为所需输出的明确指示。一些常见的动态推理时条件化技术包括：
- en: 'Prompt tuning: Providing natural language guidance for intended behavior. Sensitive
    to prompt design.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示调整：为预期行为提供自然语言指导。对提示设计敏感。
- en: 'Prefix tuning: Prepending trainable vectors to LLM layers.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前缀调整：在LLM层前面添加可训练向量。
- en: 'Constraining tokens: Forcing inclusion/exclusion of certain words'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制令牌：强制包含/排除某些单词
- en: 'Metadata: Providing high-level info like genre, target audience, etc.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据：提供类似类型、目标受众等高层次信息。
- en: Prompts can facilitate generating text that adheres to specific themes, styles,
    or even mimics a particular author's writing style. These techniques involve providing
    contextual information during inference time such as for in-context learning or
    retrieval augmentation.An example of prompt tuning is prefixing prompts, where
    instructions like "Write a child-friendly story about..." are prepended to the
    prompt. For example, in chatbot applications, conditioning the model with user
    messages helps it generate responses that are personalized and pertinent to the
    ongoing conversation. Further examples include prepending relevant documents to
    prompts to assist LLMs with writing tasks (e.g., news reports, Wikipedia pages,
    company documents), or retrieving and prepending user-specific data (financial
    records, health data, emails) before prompting an LLM to ensure personalized answers.
    By conditioning LLM outputs on contextual information at runtime, these methods
    can guide models without relying on traditional fine-tuning processes.Often demonstrations
    are part of the instructions for reasoning tasks, where few-shot examples are
    provided to induce desired behavior. Powerful LLMs, such as GPT-3, can solve tasks
    without further training through prompting techniques. In this approach, the problem
    to be solved is presented to the model as a text prompt, possibly with some text
    examples of similar problems and their solutions. The model must provide a completion
    of the prompt via inference. **Zero-shot prompting** involves no solved examples,
    while few-shot prompting includes a small number of examples of similar (problem,
    solution) pairs. It has shown that prompting provides easy control over large
    frozen models like GPT-3 and allows steering model behavior without extensive
    fine-tuning. Prompting enables conditioning models on new knowledge with low overhead,
    but careful prompt engineering is needed for best results. This is what we’ll
    discuss as part of this chapter.In prefix tuning, continuous task-specific vectors
    are trained and supplied to models at inference time. continuous task-specific
    vectors. Similar ideas have been proposed for adapter-approaches such as parameter
    efficient transfer learning (PELT) or Ladder Side-Tuning (LST).Conditioning at
    inference time can also happen during sampling such as grammar-based sampling,
    where the output can be constrained to be compatible with certain well-defined
    patterns, such as a programming language syntax.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Full fine-tuning consistently achieves strong results but often requires extensive
    resources, and trade-offs exist between efficacy and efficiency. Methods like
    adapters, prompts, and LoRA reduce this burden via sparsity or freezing, but can
    be less effective. The optimal approach depends on constraints and objectives.
    Future work on improved techniques tailor-made for large LMs could push the boundaries
    of both efficacy and efficiency. Recent work blends offline and online learning
    for improved stability. Integrating world knowledge and controllable generation
    remain open challenges.Prompt-based techniques allow flexible conditioning of
    LLMs to induce desired behaviors without intensive training. Careful prompt design,
    optimization, and evaluation is key to effectively controlling LLMs. Prompt-based
    techniques allow conditioning LLMs on specific behaviors or knowledge in a flexible,
    low-resource manner.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 全面的微调一贯取得强大的结果，但往往需要大量资源，效率与功效之间存在着权衡。像适配器、提示和LoRA这样的方法通过稀疏化或冻结减轻了这一负担，但可能效果较差。最佳方法取决于约束条件和目标。未来改进的技术针对大型语言模型定制可能会推动功效和效率的边界。最近的工作将离线和在线学习相结合，以提高稳定性。整合世界知识和可控生成仍是开放挑战。基于提示的技术允许对大型语言模型进行灵活的调节，以引导所需行为而无需进行密集训练。精心设计、优化和评估提示是有效控制大型语言模型的关键。基于提示的技术可通过灵活、低资源的方式对语言模型进行特定行为或知识的调节。
- en: Evaluations
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: 'Alignment is evaluated on alignment benchmarks like HUMAN and generalization
    tests like FLAN. There are a few core benchmarks with high differentiability to
    accurately assess model strengths and weaknesses such as these:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐在类似HUMAN的对齐基准和FLAN等泛化测试上进行评估。有一些核心基准可以准确评估模型的优势和劣势，例如：
- en: 'English knowledge: MMLU'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英文知识：MMLU
- en: 'Chinese knowledge: C-Eval'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中文知识：C-Eval
- en: 'Reasoning: GSM8k / BBH (Algorithmic)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理：GSM8k / BBH（算法）
- en: 'Coding: HumanEval / MBPP'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码：HumanEval / MBPP
- en: After balancing these directions, additional benchmarks like MATH (high-difficulty
    reasoning) and Dialog could be pursued.A particularly interesting evaluation is
    in math or reasoning, where generalization abilities would be expected to be very
    strong. The MATH benchmark demonstrates high-level difficulty, and GPT-4 achieves
    varying scores based on prompting methods. Results range from naive prompting
    via few-shot evaluations to PPO + process-based reward modeling. If fine-tuning
    involves dialog data only, it might negatively affect existing capabilities such
    as MMLU or BBH. Prompt engineering is essential, as biases and query difficulty
    impact evaluations.There are quantitative metrics like perplexity (measuring how
    well a model predicts data) or BLEU score (capturing similarity between generated
    text and reference text). These metrics provide rough estimates but may not fully
    capture semantic meaning or alignment with higher-level goalsOther metrics include
    user preferences ratings through human evaluation, pairwise preference, utilizing
    pre-trained reward model for online small/medium models or automated LLM-based
    assessments (for example GPT-4). Human evaluations can sometimes be problematic
    since humans can be swayed by subjective criteria such as an authoritative tone
    in the response rather than the actual accuracy. Conducting evaluations where
    users assess the quality, relevance, appropriateness of generated text against
    specific criteria set beforehand provides more nuanced insights into alignment.
    Fine-tuning is not intended to solely improve user preferences on a given set
    of prompts. Its primary purpose is to address AI safety concerns by reducing the
    occurrence of undesirable outputs such as illegal, harmful, abusive, false, or
    deceptive content. This focus on mitigating risky behavior is crucial in ensuring
    the safety and reliability of AI systems. Evaluating and comparing models based
    purely on user preferences without considering the potential harm they may cause
    can be misleading and prioritize suboptimal models over safer alternatives. In
    summary, evaluating LLM alignment requires careful benchmark selection, consideration
    of differentiability, and a mix of automatic evaluation methods and human judgments.
    Attention to prompt engineering and specific evaluation aspects is necessary to
    ensure accurate assessment of model performance.In the next section, we’ll fine-tune
    a small open-source LLM (OpenLLaMa) for a question answering with PEFT and quantization,
    and we’ll deploy it on HuggingFace.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve discussed in the first section of this chapter, the goal of model
    fine-tuning for LLMs is to optimize a model to generate outputs that are more
    specific to a task and context than the original foundation model. Amongst the
    multitude of tasks and scenarios, where we might want to apply this approach are
    these:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Software Development
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question-Answering
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information Retrieval
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer Support
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we’ll fine-tune a model for question answering. This recipe
    is not specific to LangChain, but we’ll point out a few customizations, where
    LangChain could come in. For performance reasons, we'll run this on Google Colab
    instead of the usual local environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Colab** is a computation environment that provides different means
    for hardware acceleration of computation tasks such as Tensor Processing Units
    (TPUs) and Graphical Processing Units (GPUs). These are available both in free
    and professional tiers. For the purpose of the task in this section, the free
    tier is completely sufficient. You can sign into a Colab environment at this url:
    [https://colab.research.google.com/](https://colab.research.google.com/)'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Please make sure you set your google colab machine settings in the top menu
    to TPU or GPU in order to make sure you have sufficient resources to run this
    and that the training doesn''t take too long. We’ll install all required libraries
    in the Google Colab environment – I am adding the versions of these libraries
    that I’ve used in order to make our fine-tuning repeatable:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'peft: Parameter-Efficient Fine-Tuning (PEFT; version 0.5.0)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'trl: Proximal Policy Optimization (0.6.0)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bitsandbytes: k-bit optimizers and matrix multiplication routines, needed for
    quantization (0.41.1)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'accelerate: train and use PyTorch models with multi-GPU, TPU, mixed-precision
    (0.22.0)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'transformers: HuggingFace transformers library with backends in JAX, PyTorch
    and TensorFlow (4.32.0)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'datasets: community-driven open-source library of datasets (2.14.4)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'sentencepiece: Python wrapper for fast tokenization (0.1.99)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'wandb: for monitoring the training progress on Weights and Biases (0.15.8)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: langchain for loading the model back as a langchain llm after training (0.0.273)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can install these libraries from the Colab notebook as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to download and train models from HuggingFace, we need to authenticate
    with the platform. Please note that if you want to push your model to HuggingFace
    later, you need to generate a new API token with write permissions on HuggingFace:
    [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Creating a new API token on HuggingFace write permissions.](img/file55.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Creating a new API token on HuggingFace write permissions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'We can authenticate from the notebook like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When prompted, paste your HuggingFace access token.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'A note of caution before, we start: when executing the code, you need to log
    into different services so make sure you pay attention when running the notebook!'
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Weights and Biases (W&B) is an MLOps platform that can help developers to monitor
    and document ML training workflows from end to end. As mentioned earlier, we will
    use W&B to get an idea of how well the training is working, in particular if the
    model is improving over time.For W&B, we need to name the project; alternatively,
    we can use wandb''s `init()` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In order to authenticate with W&B, you need to create a free account with them
    for this at [https://www.wandb.ai](https://www.wandb.ai) You can find your API
    key on the Authorize page: [https://wandb.ai/authorize](https://wandb.ai/authorize)Again,
    we need to paste in our API token. If the previous training run is still active
    – this could be from a previous execution of the notebook if you are running a
    second time –, let''s make sure we start a new one! This will ensure that we get
    new reports and dashboard on W&B:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用W&B进行身份验证，您需要在他们这里创建一个免费账户：[https://www.wandb.ai](https://www.wandb.ai)您可以在“授权”页面找到您的API密钥：[https://wandb.ai/authorize](https://wandb.ai/authorize)再次，我们需要粘贴我们的API令牌。如果之前的训练仍然处于活动状态——如果您第二次运行笔记本可能是因为之前的执行——让我们确保开始一个新的！这将确保我们在W&B上获得新的报告和仪表板：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we’ll need to choose a dataset against which we want to optimize. We
    can use lots of different datasets here that are appropriate for coding, storytelling,
    tool use, SQL generation, grade-school math questions (GSM8k), or many other tasks.
    HuggingFace provides a wealth of datasets, which can be viewed at this url: [https://huggingface.co/datasets](https://huggingface.co/datasets)These
    cover a lot of different, even the most niche tasks. We can also customize our
    own dataset. For example, we can use langchain to set up training data. There
    are quite a few methods available for filtering that could help reduce redundancy
    in the dataset. It would have been appealing to show data collection as a practical
    recipe in this chapter. However, because of the complexity I am leaving it out
    of scope for the book.It might be harder to filter for quality from web data,
    but there are a lot of possibilities. For code models, we could apply code validation
    techniques to score segments as a quality filter. If the code comes from Github,
    we can filter by stars or by stars by repo owner. For texts in natural language,
    quality filtering is not trivial. Search engine placement could serve as a popularity
    filter since it''s often based on user engagement with the content. Further, knowledge
    distillation techniques could be tweaked as a filter by fact density and accuracy.In
    this recipe, we are fine-tuning for question answering performance with the Squad
    V2 dataset. You can see a detailed dataset description on HuggingFace: [https://huggingface.co/spaces/evaluate-metric/squad_v2](https://huggingface.co/spaces/evaluate-metric/squad_v2)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择一个数据集进行优化。我们可以使用许多不同的数据集，这些数据集适用于编码、叙述、工具使用、SQL生成、小学数学问题（GSM8k）或许多其他任务。HuggingFace提供了丰富的数据集，可以在此网址查看：[https://huggingface.co/datasets](https://huggingface.co/datasets)，这些数据集涵盖了许多不同，甚至是最小众的任务。我们也可以定制自己的数据集。例如，我们可以使用langchain来设置训练数据。有许多可用的过滤方法可以帮助减少数据集中的冗余。本章原本很想展示数据收集作为一个实用的配方。但是，因为复杂性的原因，我决定不在本书范围内讨论这个问题。过滤网络数据的质量可能更难一些，但有很多可能性。对于代码模型，我们可以应用代码验证技术来将部分代码分数化为质量过滤器。如果代码来自Github，我们可以按星和存储库所有者筛选。对于自然语言的文本，质量过滤并不容易。搜索引擎排名可以作为受欢迎程度的过滤器，因为它通常基于用户对内容的参与度。此外，知识蒸馏技术可以通过事实的密度和准确性调整为一个过滤器。在这个配方中，我们在Squad
    V2数据集上进行了问答性能微调。您可以在HuggingFace上查看详细的数据集描述：[https://huggingface.co/spaces/evaluate-metric/squad_v2](https://huggingface.co/spaces/evaluate-metric/squad_v2)
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are taking both training and validation splits. The Squad V2 dataset bas
    a part that’s supposed to be used in training and another one in validation as
    we can see in the output of `load_dataset(dataset_name)`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在采用训练和验证集。Squad V2数据集有一个部分应该在训练中使用，另一个部分应该在验证中使用，正如我们可以在`load_dataset(dataset_name)`的输出中看到的：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We’ll use the validation splits for early stopping. Early stopping will allow
    us to stop training when the validation error begins to degrade.The Squad V2 dataset
    is composed of various features, which we can see here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用验证集进行提前停止。当验证错误开始恶化时，提前停止将允许我们停止训练。Squad V2数据集由各种特征组成，我们可以在这里看到：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The basic idea in training is prompting the model with a question and comparing
    the answer to the dataset.We want a small model that we can run locally at a decent
    token rate. LLaMa-2 models require signing a license agreement with your email
    address and to get confirmed (which, to be fair, can be very fast) as it comes
    with restrictions to commercial use. LLaMa derivates such as OpenLLaMa have been
    performing quite well as can be evidenced on the HF leaderboard: [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)OpenLLaMa
    version 1 cannot be used for coding tasks, because of the tokenizer. Therefore,
    let''s use v2! We’ll use a 3 billion parameter model, which we’ll be able to use
    even on older hardware:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can use even smaller models such as `EleutherAI/gpt-neo-125m` which can
    also give a very good compromise between resource use and performance.Let’s load
    the model:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The Bits and Bytes configuration makes it possible to quantize our model in
    8, 4, 3 or even 2 bits with a much-accelerated inference and lower memory footprint
    without a incurring a big cost in terms of performance. We are going to store
    model checkpoints on Google Drive; you need to confirm your login to your google
    account:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We’ll need to authenticate with Google for this to work. We can set our output
    directory for model checkpoints and logs to our Google Drive:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you don''t want to use google drive, just set this to a directory on your
    computer.For training, we need to set up a tokenizer:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we’ll define our training configuration. We’ll set up LORA and other training
    arguments:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A few comments to explain some of these parameters are in order. The `push_to_hub`
    argument means that we can push the model checkpoints to the HuggingSpace Hub
    regularly during training. For this to work you need to set up the HuggingSpace
    authentication (with write permissions as mentioned). If we opt for this, as `output_dir`
    we can use `new_model_name`. This will be the repository name under which the
    model will be available here on HuggingFace: [https://huggingface.co/models](https://huggingface.co/models)Alternatively,
    as I’ve done here, we can save your model locally or to the cloud, for example
    google drive to a directory.I’ve set `max_steps` and `num_train_epochs` very high,
    because I’ve noticed that training can still improve after many steps. We are
    using early stepping together with a high number of maximum training steps to
    get the model to converge to higher performance. For early stopping, we need to
    set the `evaluation_strategy` as `"steps"` and `load_best_model_at_end=True`.`eval_steps`
    is the number of update steps between two evaluations. `save_total_limit=5` means
    that only last 5 models are saved. Finally, `report_to="wandb"` means that we’ll
    send training stats, some model metadata, and hardware information to W&B, where
    we can look at graphs and dashboards for each run. The training can then use our
    configuration:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The training can take quite a while, even running on TPU device. The evaluating
    and early stopping slows the training down by a lot. If you disable the early
    stopping, you can make this much faster.We should see some statistics as the training
    progresses, but it’s nicer to show the graph of performance as we can see it on
    W&B:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Fine-tuning training loss over time (steps).](img/file56.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Fine-tuning training loss over time (steps).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'After training is done, we can save the final checkpoint on disk for re-loading:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now share our final model with friends in order to brag about the performance
    we''ve achieved by manually pushing to HuggingFace:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now load the model back using the combination of our HuggingFace username
    and the repository name (new model name).Let’s quickly show how to use this model
    in LangChain. usually, the peft model is stored as an adapter, not as a full model,
    therefore the loading is a bit different:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We’ve done everything so far on Google Colab, but we can equally execute this
    locally, just note that you need to have the huggingface peft library installed!So
    far, we’ve shown how to fine-tune and deploy an open-source LLM. Some commercial
    models can be fine-tuned on custom data as well. For example, both OpenAI’s GPT-3.5
    and Google’s PaLM model offer this capability. This has been integrated with a
    few Python libraries. With the Scikit-LLM library, this is only a few lines of
    code in either case:Fine-tuning a PaLM model for text classification can be done
    like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, you can fine-tune the GPT-3.5 model for text classification like
    this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Interestingly, in the fine-tuning available on OpenAI, all inputs are passed
    through a moderation system to make sure that the inputs are compatible with safety
    standards.This concludes fine-tuning. On the extreme end, LLMs can be deployed
    and queried without any task-specific tuning. By prompting, we can accomplish
    few-shot learning or even zero-shot learning as we’ll discuss in the next section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prompts are important for steering the behavior of large language models (LLMs)
    because they allow aligning the model outputs to human intentions without expensive
    retraining. Carefully engineered prompts can make LLMs suitable for a wide variety
    of tasks beyond what they were originally trained for. Prompts act as instructions
    that demonstrate to the LLM what is the desired input-output mapping. The picture
    below shows a few examples for prompting different language models (source: “Pre-train,
    Prompt, and Predict - A Systematic Survey of Prompting Methods in Natural Language
    Processing” by Liu and colleagues, 2021):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Prompt examples, particularly knowledge probing in cloze form,
    and summarization.](img/file57.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Prompt examples, particularly knowledge probing in cloze form,
    and summarization.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering, also known as in-context learning, refers to techniques
    for steering large language model (LLM) behavior through carefully designed prompts,
    without changing the model weights. The goal is to align the model outputs with
    human intentions for a given task. By designing good prompt templates, models
    can achieve strong results, sometimes comparable to fine-tuning. But how do good
    prompts look like?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Structure of Prompts
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prompts consist of three main components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Instructions that describe the task requirements, goals and format of inputs/outputs
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples that demonstrate the desired input-output pairs
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input that the model must act on to generate the output
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instructions explain the task to the model unambiguously. Examples provide
    diverse demonstrations of how different inputs should map to outputs. The input
    is what the model must generalize to.Basic prompting methods include zero-shot
    prompting with just the input text, and few-shot prompting with a few demonstration
    examples showing desired input-output pairs. Researchers have identified biases
    like majority label bias and recency bias that contribute to variability in few-shot
    performance. Careful prompt design through example selection, ordering, and formatting
    can help mitigate these issues.More advanced prompting techniques include instruction
    prompting, where the task requirements are described explicitly rather than just
    demonstrated. Self-consistency sampling generates multiple outputs and selects
    the one that aligns best with the examples. Chain-of-thought (CoT) prompting generates
    explicit reasoning steps leading to the final output. This is especially beneficial
    for complex reasoning tasks. CoT prompts can be manually written or generated
    automatically via methods like augment-prune-select.This table gives a brief overview
    of a few methods of prompting compared to fine-tuning:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '| **Technique** | **Method** | **Key Idea** | **Results** |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning | Fine-tune on explanation dataset generated via prompting |
    Improves model''s reasoning abilities | 73% accuracy on commonsense QA dataset
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot prompting | Simply feeding the task text to the model and asking
    for results. | Text: "i''ll bet the video game is a lot more fun than the film."<br>-
    Sentiment: |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Chain-of-Thought (CoT) | Prefix responses with "Let''s think step by step"
    | Gives model space to reason before answering | Quadrupled accuracy on math dataset
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Few-shot prompting | Provide few demos consisting of input and desired output
    to help the model understand | Shows desired reasoning format | Tripled accuracy
    on grade school math |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Least-to-most prompting | Prompt model for simpler subtasks to solve incrementally.
    "To solve {question}, we need to first solve: " | Decomposes problems into smaller
    pieces | Boosted accuracy from 16% to 99.7% on some tasks |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Selection-inference prompting | Alternate selection and inference prompts
    | Guides model through reasoning steps | Lifts performance on long reasoning tasks
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Self-consistency | Pick most frequent answer from multiple samples | Increases
    redundancy | Gained 1-24 percentage points across benchmarks |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Verifiers | Train separate model to evaluate responses | Filters out incorrect
    responses | Lifted grade school math accuracy ~20 percentage points |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: 'Figure 8.6: Pronpting techniques for LLMs.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'Some prompting techniques incorporate external information retrieval to provide
    missing context to the LLM before generating the output. For open-domain QA, relevant
    paragraphs can be retrieved via search engines and incorporated into the prompt.
    For closed-book QA, few-shot examples with evidence-question-answer format work
    better than question-answer format.There are different techniques to improve the
    reliability of large language models (LLMs) in complex reasoning tasks:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompting and Explanation: prompting the model to explain its reasoning step-by-step
    before answering using prompts like "Let''s think step by step" (as in CoT) significantly
    improves accuracy in reasoning tasks.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing few-shot examples of reasoning chains helps demonstrate the desired
    format and guides LLMs in generating coherent explanations.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternate Selection and Inference Prompts: Utilizing a combination of specialized
    selection prompts (narrow down the answer space) and inference prompts (generate
    the final response) leads to better results compared to generic reasoning prompts
    alone.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Problem Decomposition: Breaking down complex problems into smaller subtasks
    or components using a least-to-most prompting approach helps improve reliability,
    as it allows for a more structured and manageable problem-solving process.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sampling Multiple Responses: Sampling multiple responses from LLMs during generation
    and picking the most common answer increases consistency, reducing reliance on
    a single output. In particular, training separate verifier models that evaluate
    candidate responses generated by LLMs helps filter out incorrect or unreliable
    answers, improving overall reliability.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, fine-tuning LLMs on explanation datasets generated through prompting
    enhances their performance and reliability in reasoning tasks
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '**Few-shot learning** presents the LLM with just a few input-output examples
    relevant to the task, without explicit instructions. This allows the model to
    infer the intentions and goals purely from demonstrations. Carefully selected,
    ordered and formatted examples can greatly improve the model''s inference abilities.
    However, few shot learning can be prone to biases and variability across trials.
    Adding explicit instructions can make the intentions more transparent to the model
    and improve robustness. Overall, prompts combine the strengths of instructions
    and examples to maximize steering of the LLM for the task at hand.'
  id: totrans-175
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead of hand-engineering prompts, methods like automatic prompt tuning learn
    optimal prompts by directly optimizing prefix tokens on the embedding space. The
    goal is to increase the likelihood of desired outputs given inputs. Overall, prompt
    engineering is an active area of research for aligning large pre-trained LLMs
    with human intentions for a wide variety of tasks. Careful prompt design can steer
    models without expensive retraining.In this section, we’ll go through a few (but
    not all) of the techniques mentioned beforehand. Let’s discuss the tools that
    LangChain provides tools to create prompt templates in Python!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Templating
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Prompts are the instructions and examples we provide to language models to
    steer their behavior. Prompt templating refers to creating reusable templates
    for prompts that can be configured with different parameters.LangChain provides
    tools to create prompt templates in Python. Templates allow prompts to be dynamically
    generated with variable input. We can create a basic prompt template like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This template has two input variables - {adjective} and {topic}. We can format
    these with values:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The template format defaults to Python f-strings, but Jinja2 is also supported.Prompt
    templates can be composed into pipelines, where the output of one template is
    passed as input to the next. This allows modular reuse.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Chat Prompt Templates
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For conversational agents, we need chat prompt templates:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This formats a list of chat messages instead of a string. This can be useful
    for taking the history of a conversation into account. We’ve looked at different
    memory methods in Chapter 5\. These are similarly relevant in this context to
    make sure model outputs are relevant and on point.Prompt templating enables reusable,
    configurable prompts. LangChain provides a Python API for conveniently creating
    templates and formatting them dynamically. Templates can be composed into pipelines
    for modularity. Advanced prompt engineering can further optimize prompting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Prompt Engineering
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LangChain provides tools to enable advanced prompt engineering strategies like
    few-shot learning, dynamic example selection, and chained reasoning.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Few-Shot Learning
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `FewShotPromptTemplate` allows showing the model just a few demonstration
    examples of the task to prime it, without explicit instructions. For instance:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The model must infer what to do from the examples alone.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Example Selection
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To choose examples tailored to each input, `FewShotPromptTemplate` can accept
    an `ExampleSelector` rather than hardcoded examples:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`ExampleSelector` implementations like `SemanticSimilarityExampleSelector`
    automatically find the most relevant examples for each input.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Chained Reasoning
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When asking an LLM to reason through a problem, it is often more effective
    to have it explain its reasoning before stating the final answer. For example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This encourages the LLM to logically think through the problem first, rather
    than just guessing the answer and trying to justify it after. This is called **Zero-Shot
    Chain of Thought**. Asking an LLM to explain its thought process aligns well with
    its core capabilities.**Few-Shot Chain of Thought** prompting is a few-shot prompt,
    where the reasoning is explained as part of the example solutions, with the idea
    to encourage an LLM to explain its reasoning before making a decision. It has
    been shown that this kind of prompting can lead to more accurate results, however,
    this performance boost was found to be proportional to the size of the model,
    and the improvements seemed to be negligible or even negative in smaller models.In
    **Tree of Thoughts (ToT)** prompting, we are generating multiple problem-solving
    steps or approaches for a given prompt and then using the AI model to critique
    these steps. The critique will be based on the model’s judgment of the solution’s
    suitability to the problem. Let''s walk through a more detailed example of implementing
    ToT using LangChain.First, we''ll define our 4 chain components with `PromptTemplates`.
    We need a solution template, an evaluation template, a reasoning template, and
    a ranking template. Let’s first generate solutions:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s ask the LLM to evaluate these solutions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now we’ll reason a bit more about them:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can rank these solutions given our reasoning so far:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we create chains from these templates before we’ll put it all together:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally. we connect these chains into a `SequentialChain`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This allows us to leverage the LLM at each stage of the reasoning process. The
    ToT approach helps avoid dead-ends by fostering exploration.These techniques collectively
    enhance the accuracy, consistency, and reliability of large language models' reasoning
    capabilities on complex tasks by providing clearer instructions, fine-tuning with
    targeted data, employing problem breakdown strategies, incorporating diverse sampling
    approaches, integrating verification mechanisms, and adopting probabilistic modeling
    frameworks.Prompt design is highly significant for unlocking LLM reasoning capabilities,
    the potential for future advancements in models and prompting techniques, and
    these principles and techniques form a valuable toolkit for researchers and practitioners
    working with large language models.Let’s summarize!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Chapter 1, we discussed the basic principles of generative models, particularly
    LLMs, and their training. We focused mostly on the pre-training step, which is
    – generally speaking – adjusting the models to the correlations within words and
    wider segments of texts. Alignment it the assessment of model outputs against
    expectations and conditioning is the process of making sure the output is according
    to expectations. Conditioning allows steering generative AI to improve safety
    and quality, but it is not a complete solution. In this chapter, the focus is
    on conditioning, in particular through fine-tuning and prompting. In fine-tuning
    the language model is trained on many examples of tasks formulated as natural
    language instructions, along with appropriate responses. Often this is done through
    reinforcement learning with human feedback (RLHF), which involves training on
    a dataset of human-generated (prompt, response) pairs, followed by reinforcement
    learning from human feedback, however, other techniques have been developed that
    have been shown to produce competitive results with lower resource footprints.
    In the first recipe of this chapter, we’ve implemented a fine-tuning of a small
    open-source model for question answering.There are many techniques to improve
    the reliability of LLMs in complex reasoning tasks including step-by-step prompting,
    alternate selection, and inference prompts, problem decomposition, sampling multiple
    responses, and employing separate verifier models. These methods have shown to
    enhance accuracy and consistency in reasoning tasks. We’ve discussed and compared
    several techniques. LangChain provides building blocks to unlock advanced prompting
    strategies like few-shot learning, dynamic example selection, and chained reasoning
    decomposition as we’ve shown in the examples.Careful prompt engineering is key
    to aligning language models with complex objectives. Reliability in reasoning
    can be improved by breaking down problems and adding redundancy. The principles
    and techniques that we’ve discussed in this chapter provide a toolkit for experts
    working with LLMs. We can expect future advancements in both model training and
    prompting techniques. As these methods and LLMs continue to develop, they will
    likely become even more effective and useful for a broader range of applications.Let’s
    see if you remember some more key points from this chapter!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please have a look to see if you can come up with the answers to these questions
    from memory. I’d recommend you go back to the corresponding sections of this chapter,
    if you are unsure about any of them:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: What’s alignment in the context of LLMs?
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are different methods of conditioning and how can we distinguish them?
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How’s moderation related to conditioning?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is instruction tuning and what’s its importance?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is quantization?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are a few methods for fine-tuning?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is few-shot learning?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is Chain of Thought prompting?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain Tree of Thought prompting!
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
