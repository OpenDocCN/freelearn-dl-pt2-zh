- en: Generative Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成网络
- en: All the examples that we have seen in the previous chapters were focused on
    solving problems such as classification or regression. This chapter is very interesting
    and important for understanding how deep learning is being evolved to solve problems
    in unsupervised learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到的所有示例都集中在解决分类或回归等问题上。这一章对理解深度学习如何发展以解决无监督学习问题非常有趣和重要。
- en: 'In this chapter, we will train networks that learn how to create:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练网络，学习如何创建：
- en: Images based on content and a particular artistic style, popularly called **style
    transfer**
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于内容和特定艺术风格生成的图像，通常称为**风格迁移**
- en: Generating faces of new persons using a particular type of **generative adversarial
    network** (**GAN**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特定类型的**生成对抗网络（GAN）**生成新人物面孔
- en: Generating new text using language modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言建模生成新文本
- en: These techniques form the basis of most of the advanced research that is happening
    in the deep learning space. Going into the exact specifics of each of the subfields,
    such as GANs and language modeling is out of the scope of this book, as they deserve
    a separate book for themselves. We will learn how they work in general and the
    process of building them in PyTorch.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术构成了深度学习领域大部分高级研究的基础。深入探讨GAN和语言建模等子领域的确切细节超出了本书的范围，它们值得有专门的书籍来介绍。我们将学习它们的一般工作原理以及在PyTorch中构建它们的过程。
- en: Neural style transfer
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经风格迁移
- en: We humans generate artwork with different levels of accuracy and complexity.
    Though the process of creating art could be a very complex process, it can be
    seen as a combination of the two most important factors, namely, what to draw
    and how to draw. What to draw is inspired by what we see around us, and how we
    draw will also take influences from certain things that are found around us. This
    could be an oversimplification from an artist's perspective, but for understanding
    how we can create artwork using deep learning algorithms, it is very useful. We
    will train a deep learning algorithm to take content from one image and then draw
    it according to a specific artistic style. If you are an artist or in the creative
    industry, you can directly use the amazing research that has gone on in recent
    years to improve this and create something cool within the domain you work in.
    Even if you are not, it still introduces you to the field of generative models,
    where networks generate new content.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类以不同的准确度和复杂性生成艺术作品。尽管创作艺术的过程可能非常复杂，但它可以被视为两个最重要因素的结合，即要绘制什么和如何绘制。绘制什么受我们周围所见的启发，而如何绘制也将受到我们周围某些事物的影响。从艺术家的角度来看，这可能是一种过度简化，但对于理解如何使用深度学习算法创建艺术作品，它非常有用。我们将训练一个深度学习算法，从一幅图像中获取内容，然后按照特定的艺术风格进行绘制。如果您是艺术家或从事创意行业，您可以直接利用近年来进行的惊人研究来改进并在您工作的领域内创造出一些很酷的东西。即使您不是，它也会向您介绍生成模型领域，其中网络生成新内容。
- en: 'Let''s understand what is done in neural style transfer at a high-level, and
    then dive into details, along with the PyTorch code required to build it. The
    style transfer algorithm is provided with a content *image (C)* and a style *image
    (S);* the algorithm has to generate a new image (O) which has the content from
    the content image and the style from the style image. This process of creating
    neural style transfer was introduced by Leon Gates and others in 2015 ([A Neural
    Algorithm of Artistic Style)](https://arxiv.org/pdf/1508.06576.pdf). The following
    is the content image (C) that we will be using:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在高层次理解神经风格迁移的过程，然后深入细节，以及构建它所需的PyTorch代码。风格迁移算法提供了一个内容*图像（C）*和一个风格*图像（S）*，算法必须生成一个新图像（O），该图像具有来自内容图像的内容和来自风格图像的风格。这一创建神经风格迁移的过程由Leon
    Gates等人于2015年介绍（[艺术风格的神经算法](https://arxiv.org/pdf/1508.06576.pdf)）。以下是我们将使用的内容图像（C）：
- en: '![](img/641807cd-e2af-47f7-a88f-e5a9b09b2cb0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/641807cd-e2af-47f7-a88f-e5a9b09b2cb0.png)'
- en: 'Image source: https://arxiv.org/pdf/1508.06576.pdf'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：https://arxiv.org/pdf/1508.06576.pdf
- en: 'And the following is the style image (S):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是样式图像（S）：
- en: '![](img/92227a6d-1e3f-42ee-98a9-5ba6bb7c9d0d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92227a6d-1e3f-42ee-98a9-5ba6bb7c9d0d.png)'
- en: 'Image source: https://arxiv.org/pdf/1508.06576.pdf'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：https://arxiv.org/pdf/1508.06576.pdf
- en: 'And this is the image that we are going to generate:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将要生成的图像：
- en: '![](img/2dc624aa-47c9-454a-bf16-b2a170dfb1cd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dc624aa-47c9-454a-bf16-b2a170dfb1cd.png)'
- en: 'Image source: https://arxiv.org/pdf/1508.06576.pdf'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：https://arxiv.org/pdf/1508.06576.pdf
- en: The idea behind style transfer becomes intuitive from understanding how **Convolutional
    Neural Networks** (**CNNs**) work. When CNNs are trained for object recognition,
    the early layers of a trained CNN learn very generic information like lines, curves,
    and shapes. The last layers in a CNN capture the higher-level concepts from an
    image such as eyes, buildings, and trees. So the values of the last layers of
    similar images tend to be closer. We take the same concept and apply it for content
    loss. The last layer for the content image and the generated image should be similar,
    and we calculate the similarity using **mean square error **(**MSE**). We use
    our optimization algorithms to bring down the loss value.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从理解**卷积神经网络**（**CNNs**）工作方式的角度来看，样式转移的背后思想变得直观。当CNN用于对象识别训练时，训练的CNN的早期层学习非常通用的信息，如线条、曲线和形状。CNN的最后层捕捉图像的更高级概念，如眼睛、建筑物和树木。因此，类似图像的最后层的值倾向于更接近。我们将相同的概念应用于内容损失。内容图像和生成图像的最后一层应该是相似的，我们使用**均方误差**（**MSE**）来计算相似性。我们使用优化算法降低损失值。
- en: The style of the image is generally captured across multiple layers in a CNN
    by a technique called **gram matrix**. Gram matrix calculates the correlation
    between the feature maps captured across multiple layers. Gram matrix gives a
    measure of calculating the style. Similarly styled images have similar values
    for gram matrix. The style loss is also calculated using MSE between the gram
    matrix of the style image and the generated image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过称为**格拉姆矩阵**的技术，CNN通常在多个层次上捕获图像的样式。格拉姆矩阵计算跨多个层次捕获的特征映射之间的相关性。格拉姆矩阵提供计算样式的一种方法。具有类似风格的图像对于格拉姆矩阵具有相似的值。样式损失也是使用样式图像和生成图像的格拉姆矩阵之间的MSE计算的。
- en: 'We will use a pretrained VGG19 model, provided in the torchvision models. The
    steps required for training a style transfer model are similar to any other deep
    learning model, except for the fact that calculating losses is more involved than
    for a classification or a regression model. The training of the neural style algorithm
    can be broken down to the following steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用提供在torchvision模型中的预训练VGG19模型。训练样式转移模型所需的步骤与任何其他深度学习模型类似，唯一不同的是计算损失比分类或回归模型更复杂。神经风格算法的训练可以分解为以下步骤：
- en: Loading data.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Creating a VGG19 model.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建VGG19模型。
- en: Defining content loss.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义内容损失。
- en: Defining style loss.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义样式损失。
- en: Extracting losses across layers from VGG model.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从VGG模型中提取跨层的损失。
- en: Creating an optimizer.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建优化器。
- en: Training—generating an image similar to the content image, and style similar
    to the style image.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 - 生成与内容图像类似的图像，并且风格与样式图像类似。
- en: Loading the data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Loading data is similar to what we have seen for solving image classification
    problems in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep Learning
    for Computer Vision*. We will be using the pretrained VGG model, so we have to
    normalize the images using the same values on which the pretrained model is trained.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据与我们在[第5章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)中解决图像分类问题所见的方式类似，《计算机视觉深度学习》。我们将使用预训练的VGG模型，因此必须使用与预训练模型相同的值对图像进行标准化。
- en: 'The following code shows how we can do this. The code is mostly self-explanatory
    as we have already discussed it in detail in the previous chapters:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们如何做到这一点。代码大部分是自解释的，因为我们已经在前几章中详细讨论过：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this code, we defined three functionalities, `prep` does all the preprocessing
    required and uses the same values for normalization as those with which the VGG
    model was trained. The output of the model needs to be normalized back to its
    original values; the `postpa` function does the processing required. The generated
    model may be out of the range of accepted values, and the `postp` function limits
    all the values greater than 1 to 1 and values that are less than 0 to 0\. Finally,
    the `image_loader` function loads the image, applies the preprocessing transformation,
    and converts it into a variable. The following function loads the style and content
    image:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们定义了三个功能，`prep`执行所有所需的预处理，并使用与训练VGG模型相同的值进行标准化。模型的输出需要恢复到其原始值；`postpa`函数执行所需的处理。生成的模型可能超出接受值的范围，`postp`函数将所有大于1的值限制为1，所有小于0的值限制为0。最后，`image_loader`函数加载图像，应用预处理转换，并将其转换为变量。以下功能加载样式和内容图像：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can either create an image with noise (random numbers) or we can use the
    same content image. We will use the content image in this case. The following
    code creates the content image:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建一个带有噪声（随机数）的图像，也可以使用相同的内容图像。在这种情况下，我们将使用内容图像。以下代码创建内容图像：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will use an optimizer to tune the values of the `opt_img` in order for the
    image to be closer to the content image and style image. For that reason, we are
    asking PyTorch to maintain the gradients by mentioning `requires_grad=True`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用优化器调整 `opt_img` 的值，以便图像更接近内容图像和样式图像。因此，我们通过指定 `requires_grad=True` 要求 PyTorch
    保持梯度。
- en: Creating the VGG model
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建 VGG 模型
- en: 'We will load a pretrained model from `torchvisions.models`. We will be using
    this model only for extracting features, and the PyTorch VGG model is defined
    in such a way that all the convolutional blocks will be in the `features` module
    and the fully connected, or linear, layers are in the `classifier` module. Since
    we will not be training any of the weights or parameters in the VGG model, we
    will also freeze the model. The following code demonstrates the same:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 `torchvisions.models` 中加载一个预训练模型。我们将仅使用此模型来提取特征，PyTorch 的 VGG 模型被定义为所有卷积块在
    `features` 模块中，而全连接或线性层在 `classifier` 模块中。由于我们不会训练 VGG 模型中的任何权重或参数，我们还将冻结该模型。以下代码演示了同样的操作：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this code, we created a VGG model, used only its convolution blocks and froze
    all the parameters of the model, as we will be using it only for extracting features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们创建了一个 VGG 模型，仅使用其卷积块，并冻结了模型的所有参数，因为我们只会用它来提取特征。
- en: Content loss
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容损失
- en: 'The **content loss** is the MSE calculated on the output of a particular layer,
    extracted by passing two images through the network. We extract the outputs of
    the intermediate layers from the VGG by using the `register_forward_hook` functionality,
    by passing in the content image and the image to be optimized. We calculate the
    MSE obtained from the outputs of these layers, as described in the following code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**内容损失** 是在通过网络传递两个图像后提取的特定层输出上计算的均方误差（MSE）。我们通过使用 `register_forward_hook`
    功能从 VGG 中提取中间层的输出来计算这些层的输出的 MSE，如下代码所述。'
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will implement the `dummy_fn` of this code—in the coming sections. For now,
    all we know is, that the `dummy_fn` function returns the outputs of particular
    layers by passing an image. We pass the outputs generated by passing the content
    image and noise image to the MSE `loss` function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将实现这段代码中的 `dummy_fn` 函数。目前我们只知道，`dummy_fn` 函数通过传递图像返回特定层的输出。我们将通过将内容图像和噪声图像传递给
    MSE `loss` 函数来传递生成的输出。
- en: Style loss
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样式损失
- en: The **style loss** is calculated across multiple layers. Style loss is the MSE
    of the gram matrix generated for each feature map. The gram matrix represents
    the correlation value of its features. Let's understand how gram matrix works
    by using the following diagram and a code implementation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**样式损失** 在多个层次上计算。样式损失是每个特征图生成的格拉姆矩阵的均方误差（MSE）。格拉姆矩阵表示其特征的相关性值。让我们通过以下图表和代码实现来理解格拉姆矩阵的工作方式。'
- en: 'The following table shows the output of a feature map of dimension [2, 3, 3,
    3], having the column attributes `Batch_size`, `Channels`, and `Values`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了具有列属性 `Batch_size`、`Channels` 和 `Values` 的特征图维度为 [2, 3, 3, 3] 的输出：
- en: '![](img/94d34d01-e524-467e-a53e-3d298fa4cd44.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94d34d01-e524-467e-a53e-3d298fa4cd44.png)'
- en: 'To calculate the gram matrix, we flatten all the values per channel and then
    find correlation by multiplying with its transpose, as shown in the following
    table:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算格拉姆矩阵，我们将所有通道的值展平，然后通过与其转置相乘找到相关性，如下表所示：
- en: '![](img/d265a566-7480-49d0-8639-aff72d4d6a81.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d265a566-7480-49d0-8639-aff72d4d6a81.png)'
- en: 'All we did is flatten all the values, with respect to each channel, to a single
    vector or tensor. The following code implements this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的只是将所有通道的值展平为单个向量或张量。以下代码实现了这一点：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We implement the `GramMatrix` as another PyTorch module with a `forward` function
    so that we can use it like a PyTorch layer. We are extracting the different dimensions
    from the input image in this line:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `GramMatrix` 实现为另一个 PyTorch 模块，并具有一个 `forward` 函数，以便我们可以像使用 PyTorch 层一样使用它。在此行中，我们从输入图像中提取不同的维度：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, `b` represents batch, `c` represents filters or channels, `h` represents
    height, and `w` represents width. In the next step, we will use the following
    code to keep the batch and channel dimensions intact and flatten all the values
    along the height and width dimension, as shown in the preceding figure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`b`代表批量大小，`c`代表过滤器或通道数，`h`代表高度，`w`代表宽度。在下一步中，我们将使用以下代码保持批量和通道维度不变，并展平所有高度和宽度维度的值，如前图所示：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The gram matrix is calculated by multiplying the flattening values along with
    its transposed vector. We can do it by using the PyTorch batch matrix multiplication
    function, provided as `torch.bmm()`, as shown in the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Gram矩阵通过将其展平的值与其转置向量相乘来计算。我们可以使用PyTorch提供的批量矩阵乘法函数`torch.bmm()`来执行此操作，如下代码所示：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We finish normalizing the values of the gram matrix by dividing it by the number
    of elements. This prevents a particular feature map with a lot of values dominating
    the score. Once `GramMatrix` is calculated, it becomes simple to calculate style
    loss, which is implemented in this code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将Gram矩阵的值除以元素数量来完成Gram矩阵的值归一化。这可以防止具有大量值的特定特征图支配得分。一旦计算了`GramMatrix`，就可以简单地计算风格损失，该损失在以下代码中实现：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `StyleLoss` is implemented as another PyTorch layer. It calculates the MSE
    between the input `GramMatrix` values and the style image `GramMatrix` values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`StyleLoss`作为另一个PyTorch层实现。它计算输入`GramMatrix`值与风格图像`GramMatrix`值之间的均方误差。'
- en: Extracting the losses
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取损失
- en: 'Just like we extracted the activation of a convolution layer using the `register_forward_hook()`
    function in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep Learning
    for Computer Vision,* we can extract losses of different convolutional layers
    required to calculate style loss and content loss. The one difference in this
    case is that instead of extracting from one layer, we need to extract outputs
    of multiple layers. The following class integrates the required change:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在《深度学习计算机视觉》[第5章](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml)中使用`register_forward_hook()`函数提取卷积层的激活一样，我们可以提取不同卷积层的损失，用于计算风格损失和内容损失。这种情况的一个不同之处在于，我们不是从一个层中提取，而是需要从多个层中提取输出。以下类整合了所需的变更：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `__init__` method takes the model on which we need to call the `register_forward_hook`
    method and the layer numbers for which we need to extract the outputs. The `for`
    loop in the `__init__` method iterates through the layer numbers and registers
    the forward hook required to pull the outputs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`方法接受我们需要调用`register_forward_hook`方法的模型以及我们需要提取输出的层编号。`__init__`方法中的`for`循环遍历层编号并注册所需的前向钩子以提取输出。'
- en: The `hook_fn` passed to the `register_forward_hook` method is called by PyTorch
    after that layer on which the `hook_fn` function is registered. Inside the function,
    we capture the output and store it in the `features` array.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`register_forward_hook`方法的`hook_fn`将在注册`hook_fn`函数的层之后由PyTorch调用。在函数内部，我们捕获输出并将其存储在`features`数组中。
- en: We need to call the `remove` function once when we don't want to capture the
    outputs. Forgetting to invoke the `remove` methods can cause out-of-memory exceptions
    as all the outputs get accumulated.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不想捕获输出时，我们需要调用`remove`函数一次。忘记调用`remove`方法可能导致内存不足异常，因为所有输出都会累积。
- en: 'Let''s write another utility function which can extract the outputs required
    for style and content images. The following function does the same:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写另一个实用函数，它可以提取用于风格和内容图像的输出。以下函数执行相同操作：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Inside the `extract_layers` function, we create objects for the `LayerActivations`
    class by passing in the model and the layer numbers. The features list may contain
    outputs from previous runs, so we are reinitiating to an empty list. Then we pass
    in the image through the model, and we are not going to use the outputs. We are
    more interested in the outputs generated in the `features` array. We call the
    `remove` method to remove all the registered hooks from the model and return the
    features. The following code shows how we extract the targets required for style
    and content image:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在`extract_layers`函数内部，我们通过传入模型和层编号来创建`LayerActivations`类的对象。特征列表可能包含先前运行的输出，因此我们将其重新初始化为空列表。然后我们通过模型传入图像，我们不会使用输出。我们更感兴趣的是生成在`features`数组中的输出。我们调用`remove`方法从模型中移除所有注册的钩子并返回特征。以下代码展示了我们如何提取风格和内容图像所需的目标：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Once we extract the targets, we need to detach the outputs from the graphs
    that created them. Remember that all these outputs are PyTorch variables which
    maintain information of how they are created. But, for our case, we are interested
    in only the output values and not the graph, as we are not going to update either
    `style` image or the `content` image. The following code illustrates this technique:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提取了目标，我们需要从创建它们的图中分离输出。请记住，所有这些输出都是保持它们如何创建的PyTorch变量。但对于我们的情况，我们只对输出值感兴趣，而不是图，因为我们不会更新`style`图像或`content`图像。以下代码说明了这种技术：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once we have detached, let''s add all the targets into one list. The following
    code illustrates this technique:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们分离了目标，让我们将所有目标添加到一个列表中。以下代码说明了这种技术：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When calculating the style loss and content loss, we passed on two lists called
    content layers and style layers. Different layer choices will have an impact on
    the quality of the image generated. Let''s pick the same layers as the authors
    of the paper have mentioned. The following code shows the choice of layers that
    we are using here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算风格损失和内容损失时，我们传递了称为内容层和风格层的两个列表。不同的层选择将影响生成图像的质量。让我们选择与论文作者提到的相同层。以下代码显示了我们在这里使用的层的选择：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The optimizer expects a single scalar quantity to minimize. To achieve a single
    scalar value, we sum up all the losses that have arrived at different layers.
    It is common practice to do a weighted sum of these losses, and again we pick
    the same weights as used in the paper''s implementation in the GitHub repository
    ([https://github.com/leongatys/PytorchNeuralStyleTransfer](https://github.com/leongatys/PytorchNeuralStyleTransfer)).
    Our implementation is a slightly modified version of the author''s implementation.
    The following code describes the weights being used, which are calculated by the
    number of filters in the selected layers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器期望最小化单一标量量。为了实现单一标量值，我们将所有到达不同层的损失求和。习惯上，对这些损失进行加权求和是常见做法，我们选择的权重与GitHub仓库中论文实现中使用的相同（[https://github.com/leongatys/PytorchNeuralStyleTransfer](https://github.com/leongatys/PytorchNeuralStyleTransfer)）。我们的实现是作者实现的略微修改版本。以下代码描述了正在使用的权重，这些权重由所选层中的滤波器数量计算得出：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To visualize this, we can print the VGG layers. Take a minute to observe what
    layers we are picking, and you can experiment with different layer combinations.
    We will use the following code to `print` the VGG layers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化这一点，我们可以打印VGG层。花一分钟观察我们选择了哪些层，并尝试不同的层组合。我们将使用以下代码来`print` VGG层：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We have to define the `loss` functions and the `optimizer` to generate artistic
    images. We will initialize both of them in the following section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须定义`loss`函数和`optimizer`来生成艺术图像。我们将在以下部分初始化它们两个。
- en: Creating loss function for each layers
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为每一层创建损失函数
- en: 'We have already defined `loss` functions as PyTorch layers. So, let''s create
    the loss layers for different style losses and content losses. The following code
    defines the function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了PyTorch层作为`loss`函数。现在，让我们为不同的风格损失和内容损失创建损失层。以下代码定义了该函数：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `loss_fns` is a list containing a bunch of style loss objects and content
    loss objects based on the lengths of the arrays created.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss_fns`是一个包含一系列风格损失对象和内容损失对象的列表，基于创建的数组长度。'
- en: Creating the optimizer
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建优化器
- en: 'In general, we pass in the parameters of a network like VGG to be trained.
    But, in this example, we are using VGG models as feature extractors and, hence,
    we cannot pass the VGG parameters. Here, we will only provide the parameters of
    the `opt_img` variable that we will optimize to make the image have the required
    content and style. The following code creates the `optimizer` that optimizes its
    values:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们会传递网络（如VGG）的参数进行训练。但在这个例子中，我们将VGG模型作为特征提取器使用，因此不能传递VGG的参数。在这里，我们只会提供`opt_img`变量的参数，我们将优化它们以使图像具有所需的内容和风格。以下代码创建了优化器来优化它的值：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now we have all the components for training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了所有的训练组件。
- en: Training
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: 'The `training` method is different compared to the other models that we have
    trained till now. Here, we need to calculate loss at multiple layers, and every
    time the optimizer is called, it will change the input image so that its content
    and style gets close to the target''s content and style. Let''s look at the code
    used for training, and then we will walk through the important steps in the training:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前训练的其他模型相比，`training`方法有所不同。在这里，我们需要在多个层次计算损失，并且每次调用优化器时，它都会改变输入图像，使其内容和样式接近目标的内容和样式。让我们看一下用于训练的代码，然后我们将详细介绍训练的重要步骤：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We are running the training loop for `500` iterations. For every iteration,
    we calculate the output from different layers of the VGG model using our `extract_layers`
    function. In this case, the only thing that changes is the values of `opt_img`,
    which will contain our style image. Once the outputs are calculated, we are calculating
    the losses by iterating through the outputs and passing them to the corresponding
    `loss` functions along with their respective targets. We sum up all the losses
    and call the `backward` function. At the end of the `closure` function, loss is
    returned. The `closure` method is called along with the `optimizer.step` method
    for `max_iter`. If you are running on a GPU, it could take a few minutes to run;
    if you are running on a CPU, try reducing the size of the image to make it run
    faster.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在运行为期`500`次迭代的训练循环。对于每一次迭代，我们使用我们的`extract_layers`函数计算VGG模型不同层的输出。在这种情况下，唯一变化的是`opt_img`的值，它将包含我们的样式图像。一旦计算出输出，我们通过迭代输出并将它们传递给相应的`loss`函数及其各自的目标来计算损失。我们将所有损失相加并调用`backward`函数。在`closure`函数的末尾，返回损失。`closure`方法与`optimizer.step`方法一起调用`max_iter`次。如果在GPU上运行，可能需要几分钟；如果在CPU上运行，请尝试减小图像大小以加快运行速度。
- en: 'After running for 500 epochs, the resulting image on my machine looks as shown
    here. Try different combinations of content and style to generate interesting
    images:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 运行500个周期后，在我的设备上生成的图像如下所示。尝试不同的内容和样式组合来生成有趣的图像：
- en: '![](img/e447d6b9-9bae-4cf2-8100-24a329cdea64.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e447d6b9-9bae-4cf2-8100-24a329cdea64.png)'
- en: In the next section, let's go ahead and generate human faces using **deep convolutional
    generative adversarial networks** (**DCGANs**).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们使用**深度卷积生成对抗网络**（**DCGANs**）生成人脸。
- en: Generative adversarial networks
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: GANs have become very popular in the last few years. Every week there are some
    advancements being made in the area of GANs. It has become one of the important
    subfields of deep learning, with a very active research community. GAN was introduced
    by Ian Goodfellow in 2014\. The GAN addresses the problem of unsupervised learning
    by training two deep neural networks, called *generator* and *discriminator*,
    which compete with each other. In the course of training, both eventually become
    better at the tasks that they perform.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: GANs在过去几年变得非常流行。每周都有一些GAN领域的进展。它已成为深度学习的重要子领域之一，拥有非常活跃的研究社区。GAN是由Ian Goodfellow于2014年引入的。GAN通过训练两个深度神经网络，称为*生成器*和*判别器*，它们相互竞争来解决无监督学习的问题。在训练过程中，它们最终都变得更擅长它们所执行的任务。
- en: GANs are intuitively understood using the case of counterfeiter (generator)
    and the police (discriminator). Initially, the counterfeiter shows the police
    fake money. The police identifies it as fake and explains to the counterfeiter
    why it is fake. The counterfeiter makes new fake money based on the feedback it
    received. The police finds it fake and informs the counterfeiter why it is fake.
    It repeats this a huge number of times until the counterfeiter is able to make
    fake money which the police is unable to recognize. In the GAN scenario, we end
    up with a generator that generates fake images which are quite similar to the
    real ones, and a classifier becomes great at identifying a fake from the real
    thing.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用仿冒者（生成器）和警察（判别器）的案例直观理解GAN。最初，仿冒者向警察展示假钞。警察识别出它是假的，并解释为什么是假的。仿冒者根据收到的反馈制作新的假钞。警察发现它是假的，并告知仿冒者为什么是假的。这个过程重复了很多次，直到仿冒者能够制作出警察无法辨别的假钞。在GAN场景中，我们最终得到一个生成器生成的假图像与真实图像非常相似，而分类器则变得擅长辨别真假。
- en: GAN is a combination of a forger network and an expert network, each being trained
    to beat the other. The generator network takes a random vector as input and generates
    a synthetic image. The discriminator network takes an input image and predicts
    whether the image is real or fake. We pass the discriminator network either a
    real image or a fake image.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 是一个伪造网络和专家网络的组合，每个网络都被训练来击败对方。生成器网络以随机向量作为输入并生成合成图像。鉴别器网络接收输入图像并预测图像是真实的还是伪造的。我们向鉴别器网络传递的是真实图像或伪造图像。
- en: The generator network is trained to produce images and fool the discriminator
    network into believing they are real. The discriminator network is also constantly
    improving at not getting fooled, as we pass the feedback whilst training it. Though
    the idea of GANs sounds simple in theory, training a GAN model that actually works
    is very difficult. Training a GAN is also challenging, as there are two deep neural
    networks that need to be trained.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络被训练来生成图像，并欺骗鉴别器网络认为它们是真实的。鉴别器网络也在不断提高其不被欺骗的能力，因为我们在训练中传递反馈。尽管 GAN 的理念在理论上听起来很简单，但训练一个真正有效的
    GAN 模型却非常困难。训练 GAN 也很具挑战性，因为需要训练两个深度神经网络。
- en: 'The DCGAN is one of the early models that demonstrated how to build a GAN model
    that learns by itself and generates meaningful images. You can learn more about
    it here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 是早期展示如何构建自学习并生成有意义图像的 GAN 模型之一。您可以在此了解更多：
- en: '[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)'
- en: 'The following diagram shows the architecture of a GAN model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 GAN 模型的架构：
- en: '![](img/4c01284d-d850-40c2-bc9d-75ad1910f2d3.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c01284d-d850-40c2-bc9d-75ad1910f2d3.png)'
- en: We will walk through each of the components of this architecture, and some of
    the reasoning behind them, and then we will implement the same flow in PyTorch
    in the next section. By the end of this implementation, we will have basic knowledge
    of how DCGANs work.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步介绍这种架构的每个组件及其背后的一些理论，然后我们将在下一节中用 PyTorch 实现相同的流程。通过此实现，我们将对 DCGAN 的工作原理有基本的了解。
- en: Deep convolutional GAN
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积 GAN
- en: 'In this section, we will implement different parts of training a GAN architecture,
    based on the *DCGAN paper I* mentioned in the preceding information box. Some
    of the important parts of training a DCGAN include:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将基于我在前面信息框中提到的 *DCGAN 论文* 来实现训练 GAN 架构的不同部分。训练 DCGAN 的一些重要部分包括：
- en: A generator network, which maps a latent vector (list of numbers) of some fixed
    dimension to images of some shape. In our implementation, the shape is (3, 64,
    64).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个生成器网络，将某个固定维度的潜在向量（数字列表）映射到某种形状的图像。在我们的实现中，形状是 (3, 64, 64)。
- en: A discriminator network, which takes as input an image generated by the generator
    or from the actual dataset, and maps to that a score estimating if the input image
    is real or fake.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个鉴别器网络，它将生成器生成的图像或来自实际数据集的图像作为输入，并映射到一个评分，用于估计输入图像是真实还是伪造的。
- en: Defining loss functions for generator and discriminator.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为生成器和鉴别器定义损失函数。
- en: Defining an optimizer.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义优化器。
- en: Training a GAN.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个 GAN。
- en: 'Let''s explore each of these sections in detail. The implementation is based
    on the code, which is available in the PyTorch examples at:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨这些部分的每一个。这些实现基于 PyTorch 示例代码，可在以下位置找到：
- en: '[https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/pytorch/examples/tree/master/dcgan](https://github.com/pytorch/examples/tree/master/dcgan)'
- en: Defining the generator network
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义生成器网络
- en: The generator network takes a random vector of fixed dimension as input, and
    applies a set of transposed convolutions, batch normalization, and ReLu activation
    to it, and generates an image of the required size. Before looking into the generator
    implementation, let's look at defining transposed convolution and batch normalization.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络以固定维度的随机向量作为输入，并对其应用一组转置卷积、批归一化和 ReLU 激活，生成所需尺寸的图像。在查看生成器实现之前，让我们来定义一下转置卷积和批归一化。
- en: Transposed convolutions
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转置卷积
- en: 'Transposed convolutions are also called **fractionally strided convolutions**.
    They work in the opposite way to how convolution works. Intuitively, they try
    to calculate how the input vector can be mapped to higher dimensions. Let''s look
    at the following figure to understand it better:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积也称为**分数步进卷积**。它们的工作方式与卷积相反。直观地说，它们尝试计算如何将输入向量映射到更高的维度。让我们看下图以更好地理解它：
- en: '![](img/947539f3-39cd-42ce-959e-dfee7ce90a10.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/947539f3-39cd-42ce-959e-dfee7ce90a10.png)'
- en: This diagram is referenced from Theano (another popular deep learning framework)
    documentation ([http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)).
    If you want to explore more about how strided convolutions work, I strongly recommend
    you read this article from the Theano documentation. What is important for us
    is, that it helps to convert a vector to a tensor of required dimensions, and
    we can train the values of the kernels by backpropagation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此图来自Theano（另一个流行的深度学习框架）的文档（[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)）。如果您想深入了解步进卷积的工作原理，我强烈建议您阅读Theano文档中的这篇文章。对我们来说重要的是，它有助于将一个向量转换为所需维度的张量，并且我们可以通过反向传播来训练核的值。
- en: Batch normalization
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批归一化
- en: 'We have already observed a couple of times that all the features that are being
    passed to either machine learning or deep learning algorithms are normalized;
    that is, the values of the features are centered to zero by subtracting the mean
    from the data, and giving the data a unit standard deviation by dividing the data
    by its standard deviation. We would generally do this by using the PyTorch `torchvision.Normalize`
    method. The following code shows an example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次观察到，所有传递给机器学习或深度学习算法的特征都是经过归一化的；也就是说，特征的值通过减去数据的均值使其居中于零，并通过除以其标准差使数据具有单位标准差。通常我们会使用PyTorch的`torchvision.Normalize`方法来实现这一点。以下代码展示了一个示例：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In all the examples we have seen, the data is normalized just before it enters
    a neural network; there is no guarantee that the intermediate layers get a normalized
    input. The following figure shows how the intermediate layers in the neural network
    fail to get normalized data:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们所看到的所有示例中，数据在进入神经网络之前都是归一化的；不能保证中间层获得归一化的输入。以下图显示了神经网络中间层无法获取归一化数据的情况：
- en: '![](img/e372928f-ab49-46db-984a-1f70580ec361.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e372928f-ab49-46db-984a-1f70580ec361.png)'
- en: 'Batch normalization acts like an intermediate function, or a layer which normalizes
    the intermediate data when the mean and variance are changing over time during
    training. Batch normalization was introduced in 2015 by Ioffe and Szegedy ([https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)).
    Batch normalization behaves differently during training and validation or testing.
    During training, the mean and variance is calculated for the data in the batch.
    For validation and testing, the global values are used. All we need to understand
    to use it is that it normalizes the intermediate data. Some of the key advantages
    of using batch normalization are that it:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 批归一化就像是一个中间函数，或者一个在训练过程中当均值和方差随时间变化时规范化中间数据的层。批归一化由Ioffe和Szegedy在2015年提出（[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)）。批归一化在训练和验证或测试时的行为是不同的。训练时，批内数据的均值和方差被计算。验证和测试时，使用全局值。我们需要理解的是它如何规范化中间数据。使用批归一化的一些关键优势是：
- en: Improves gradient flow through the network, thus helping us build deeper networks
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升网络中的梯度流，从而帮助我们构建更深的网络
- en: Allows higher learning rates
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许更高的学习率
- en: Reduces the strong dependency of initialization
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少初始化的强依赖
- en: Acts as a form of regularization and reduces the dependency of dropout
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一种正则化形式，减少了dropout的依赖性
- en: 'Most of the modern architectures, such as ResNet and Inception, extensively
    use batch normalization in their architectures. Batch normalization layers are
    introduced after a convolution layer or linear/fully connected layers, as shown
    in the following image:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代架构，如ResNet和Inception，在其架构中广泛使用批归一化。批归一化层通常在卷积层或线性/全连接层之后引入，如下图所示：
- en: '![](img/c4e85ded-f3db-4d45-a984-ef54a8a6aeed.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4e85ded-f3db-4d45-a984-ef54a8a6aeed.png)'
- en: By now, we have an intuitive understanding of the key components of a generator
    network.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们对生成网络的关键组成部分有了直观的理解。
- en: Generator
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: 'Let''s quickly look at the following generator network code, and then discuss
    the key features of the generator network:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看以下生成器网络代码，然后讨论生成器网络的关键特性：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In most of the code examples we have seen, we use a bunch of different layers
    and then define the flow in the `forward` method. In the generator network, we
    define the layers and the flow of the data inside the `__init__` method using
    a sequential model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到的大多数代码示例中，我们使用了许多不同的层，并在`forward`方法中定义了流程。在生成器网络中，我们使用顺序模型在`__init__`方法中定义层和数据流动。
- en: The model takes as input a tensor of size `nz`, and then passes it on to a transposed
    convolution to map the input to the image size that it needs to generate. The
    `forward` function passes on the input to the sequential module and returns the
    output.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接受尺寸为`nz`的张量作为输入，然后通过转置卷积将输入映射到需要生成的图像大小。`forward`函数将输入传递给顺序模块并返回输出。
- en: The last layer of the generator network is a `tanh` layer, which limits the
    range of values the network can generate.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络的最后一层是一个`tanh`层，它限制了网络可以生成的值的范围。
- en: 'Instead of using the same random weights, we initialize the model with weights
    as defined in the paper. The following is the weight initialization code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用相同的随机权重，而是使用在论文中定义的权重初始化模型。以下是权重初始化代码：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We call the `weight` function by passing the function to the generator object, `netG`.
    Each layer is passed on to the function; if the layer is a convolution layer we
    initialize the weights differently, and if it is a `BatchNorm`, then we initialize
    it a bit differently. We call the function on the network object using the following
    code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将函数传递给生成器对象`netG`调用`weight`函数。每一层都通过函数传递；如果层是卷积层，我们以不同的方式初始化权重，如果是`BatchNorm`，我们稍微不同地初始化它。我们使用以下代码在网络对象上调用函数：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Defining the discriminator network
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义鉴别器网络
- en: 'Let''s quickly look at the following discriminator network code, and then discuss
    the key features of the discriminator network:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看以下鉴别器网络代码，然后讨论鉴别器网络的关键特性：
- en: '[PRE25]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: There are two important things in the previous network, namely, the usage of
    **leaky ReLU** as an activation function, and the usage of sigmoid as the last
    activation layer. First, let's understand what Leaky ReLU is.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 前面网络中有两个重要的事情，即使用**leaky ReLU**作为激活函数，以及在最后一个激活层中使用sigmoid。首先，让我们了解一下什么是Leaky
    ReLU。
- en: Leaky ReLU is an attempt to fix the dying ReLU problem. Instead of the function
    returning zero when the input is negative, leaky ReLU will output a very small
    number like 0.001\. In the paper, it is shown that using leaky ReLU improves the
    efficiency of the discriminator.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU是解决ReLU死亡问题的一种尝试。Leaky ReLU不像在输入为负时函数返回零，而是输出一个非常小的数值，如0.001。论文表明，使用Leaky
    ReLU可以提高鉴别器的效率。
- en: Another important difference is not using fully connected layers at the end
    of the discriminator. It is common to see the last fully connected layers being
    replaced by global average pooling. But using global average pooling reduces the
    rate of the convergence speed (number of iterations to build an accurate classifier).
    The last convolution layer is flattened and passed to a sigmoid layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的区别是在鉴别器末端不使用全连接层。通常看到最后的全连接层被全局平均池化层替换。但使用全局平均池化会减少收敛速度（构建准确分类器所需的迭代次数）。最后的卷积层被展平并传递给sigmoid层。
- en: Other than these two differences, the rest of the network is similar to the
    other image classifier networks we have seen in the book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两个差异之外，网络的其余部分与我们在本书中看到的其他图像分类器网络非常相似。
- en: Defining loss and optimizer
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义损失和优化器
- en: 'We will define a binary cross-entropy loss and two optimizers, one for the
    generator and another one for the discriminator, in the following code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下面的代码中定义一个二元交叉熵损失和两个优化器，一个用于生成器，另一个用于鉴别器：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Up to this point, it is very similar to what we have seen in all our previous
    examples. Let's explore how we can train the generator and discriminator.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这与我们在所有先前的例子中看到的非常相似。让我们探讨一下如何训练生成器和鉴别器。
- en: Training the discriminator
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练鉴别器
- en: 'The loss of the discriminator network depends on how it performs on real images
    and how it performs on fake images generated by the generator network. The loss
    can be defined as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络的损失取决于其在真实图像上的表现和在生成器网络生成的假图像上的表现。损失可以定义为：
- en: '*loss = maximize log(D(x)) + log(1-D(G(z)))*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*loss = 最大化 log(D(x)) + log(1-D(G(z)))*'
- en: So, we need to train the discriminator with real images and the fake images
    generated by the generator network.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要使用真实图像和生成器网络生成的假图像来训练鉴别器。
- en: Training the discriminator with real images
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用真实图像训练鉴别器网络
- en: Let's pass some real images as ground truth to train discriminator.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们传递一些真实图像作为地面实况来训练鉴别器。
- en: 'First, we will take a look at the code for doing the same and then explore
    the important features:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看执行相同操作的代码，然后探索重要特性：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the previous code, we are calculating the loss and the gradients required
    for the discriminator image. The `inputv` and `labelv` represent the input image
    from the `CIFAR10` dataset and labels, which is one for real images. It is pretty
    straightforward, as it is similar to what we do for other image classifier networks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们计算了用于鉴别器图像的损失和梯度。`inputv` 和 `labelv` 表示来自 `CIFAR10` 数据集的输入图像和标签，其中真实图像的标签为1。这非常直观，因为它类似于我们为其他图像分类器网络所做的工作。
- en: Training the discriminator with fake images
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用假图像训练鉴别器
- en: Now pass some random images to train discriminator.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在传递一些随机图像来训练鉴别器。
- en: 'Let''s look at the code for it and then explore the important features:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看代码，并探索重要特性：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The first line in this code passes a vector with a size of 100, and the generator
    network (`netG`) generates an image. We pass on the image to the discriminator
    for it to identify whether the image is real or fake. We do not want the generator
    to get trained, as the discriminator is getting trained. So, we remove the fake
    image from its graph by calling the `detach` method on its variable. Once all
    the gradients are calculated, we call the `optimizer` to train the discriminator.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的第一行传递了一个大小为100的向量，生成器网络 (`netG`) 生成一幅图像。我们将图像传递给鉴别器以确定图像是真实还是假的。我们不希望生成器被训练，因为鉴别器正在被训练。因此，我们通过在其变量上调用
    `detach` 方法从其图中移除假图像。一旦计算出所有梯度，我们调用 `optimizer` 训练鉴别器。
- en: Training the generator network
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练生成器网络
- en: 'Let''s look at the code for it and then explore the important features:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看代码，并探索重要特性：
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It looks similar to what we did while we trained the discriminator on fake images,
    except for some key differences. We are passing the same fake images created by
    the generator, but this time we are not detaching it from the graph that produced
    it, because we want the generator to be trained. We calculate the loss (`errG`)
    and calculate the gradients. Then we call the generator optimizer, as we want
    only the generator to be trained, and we repeat this entire process for several
    iterations before we have the generator producing slightly realistic images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来与我们在训练鉴别器使用假图像时所做的类似，除了一些关键区别。我们传递了由生成器创建的同样的假图像，但这次我们没有从生成图中分离它，因为我们希望训练生成器。我们计算损失
    (`errG`) 并计算梯度。然后我们调用生成器优化器，因为我们只想训练生成器，并在生成器生成略微逼真图像之前重复整个过程多次。
- en: Training the complete network
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练完整网络
- en: 'We looked at individual pieces of how a GAN is trained. Let''s summarize them
    as follows and look at the complete code that will be used to train the GAN network
    we created:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们逐个分析了GAN的训练过程。让我们总结如下，并查看用于训练我们创建的GAN网络的完整代码：
- en: Train the discriminator network with real images
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实图像训练鉴别器网络
- en: Train the discriminator network with fake images
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用假图像训练鉴别器网络
- en: Optimize the discriminator
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化鉴别器
- en: Train the generator based on the discriminator feedback
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于鉴别器反馈训练生成器
- en: Optimize the generator network alone
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化生成器网络
- en: 'We will use the following code to train the network:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来训练网络：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `vutils.save_image` will take a tensor and save it as an image. If provided
    with a mini-batch of images, then it saves them as a grid of images.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`vutils.save_image` 将把一个张量保存为图像。如果提供一个小批量的图像，则将它们保存为图像网格。'
- en: In the following sections, we will take a look at what the generated images
    and the real images look like.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将看看生成的图像和真实图像的外观。
- en: Inspecting the generated images
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查生成的图像
- en: So, let's compare the generated images and the real images.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们比较生成的图像和真实图像。
- en: 'The generated images will be as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像如下：
- en: '![](img/79744b19-1a7f-40b0-be82-18bafe8b7da7.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79744b19-1a7f-40b0-be82-18bafe8b7da7.png)'
- en: 'The real images will be as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 真实图像如下：
- en: '![](img/def80ece-1b79-4a27-bfc4-fc16222bf31f.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/def80ece-1b79-4a27-bfc4-fc16222bf31f.png)'
- en: Comparing both sets of images, we can see that our GAN was able to learn how
    to generate images. Apart from training to generate new images, we also have a
    discriminator, which can be used for classification problems. The discriminator
    learns important features about the images which can be used for classification
    tasks when there is a limited amount of labeled data available. When there is
    limited labeled data, we can train a GAN that will give us a classifier, which
    can be used to extract features—and a classifier module can be built on top of
    it.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两组图像，我们可以看到我们的GAN能够学习如何生成图像。除了训练生成新图像之外，我们还有一个判别器，可用于分类问题。当有限数量的标记数据可用时，判别器学习关于图像的重要特征，这些特征可以用于分类任务。当有限的标记数据时，我们可以训练一个GAN，它将为我们提供一个分类器，可以用于提取特征，并且可以在其上构建一个分类器模块。
- en: In the next section, we will train a deep learning algorithm to generate text.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将训练一个深度学习算法来生成文本。
- en: Language modeling
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言建模
- en: 'We will learn how to teach a **recurrent neural network** (**RNN**) how it
    can create a sequence of text. In simple words, the RNN model that we will build
    now will be able to predict the next word, given some context. This is just like
    the *Swift* app on your phone, which guesses the next word that you are typing.
    The ability to generate sequential data has applications in many different areas,
    such as:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何教授**循环神经网络**（**RNN**）如何创建文本序列。简单来说，我们现在要构建的RNN模型将能够根据一些上下文预测下一个单词。这就像你手机上的*Swift*应用程序，它猜测你正在输入的下一个单词一样。生成序列数据的能力在许多不同领域都有应用，例如：
- en: Image captioning
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕
- en: Speech recognition
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: Language translation
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言翻译
- en: Automatic email reply
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动电子邮件回复
- en: We learnt in [Chapter 6](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml), *Deep
    Learning with Sequence Data and Text,* that RNNs are tough to train. So, we will
    be using a variant of RNN called **Long Short-Term Memory** (**LSTM**). The development
    of the LSTM algorithm started in 1997 but became popular in the last few years.
    It became popular due to the availability of powerful hardware and quality data,
    and some advancements such as dropout also helped in training better LSTM models
    much more easily than previously.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [第6章](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml) 中学到，*使用序列数据和文本的深度学习*，RNN
    难以训练。因此，我们将使用一种称为**长短期记忆网络**（**LSTM**）的变体。LSTM 算法的开发始于1997年，但在过去几年变得流行起来。它因为强大的硬件和高质量数据的可用性，以及一些进展（如dropout），使得训练更好的LSTM模型比以前更容易。
- en: 'It is quite popular to use LSTM models to generate either a character-level
    language model or a word-level language model. In character-level language modeling,
    we give one character and the LSTM model is trained to predict the next character,
    whereas in word-level language modeling, we give a word and the LSTM model predicts
    the next word. In this section, we will be building a word-level language model
    using the PyTorch LSTM model. Just like training any other module, we will be
    following the standard steps:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSTM模型生成字符级语言模型或单词级语言模型非常流行。在字符级语言建模中，我们提供一个字符，LSTM模型被训练来预测下一个字符，而在单词级语言建模中，我们提供一个单词，LSTM模型预测下一个单词。在本节中，我们将使用PyTorch
    LSTM模型构建一个单词级语言模型。就像训练任何其他模块一样，我们将遵循标准步骤：
- en: Preparing the data
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Generating batches of data
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成数据批次
- en: Defining a model based on LSTM
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于LSTM定义模型
- en: Training the model
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Testing the model
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: This section is inspired from a slightly simplified version of the word language
    modeling example available in PyTorch at [https://github.com/pytorch/examples/tree/master/word_language_model](https://github.com/pytorch/examples/tree/master/word_language_model).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容灵感来自于 PyTorch 中稍微简化的词语言建模示例，详情请见 [https://github.com/pytorch/examples/tree/master/word_language_model](https://github.com/pytorch/examples/tree/master/word_language_model)。
- en: Preparing the data
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: For this example, we use a dataset called `WikiText2`. The `WikiText language
    modeling` dataset is a collection of over 100 million tokens extracted from the
    set of verified Good and Featured articles on Wikipedia. Compared to the preprocessed
    version of **Penn Treebank** (**PTB**), another popularly-used dataset, `WikiText-2`
    is over two times larger. The `WikiText` dataset also features a far larger vocabulary
    and retains the original case, punctuation, and numbers. The dataset contains
    full articles and, as a result, it is well suited for models that take advantage
    of long term dependency.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本例，我们使用名为`WikiText2`的数据集。`WikiText语言建模`数据集包含从维基百科上的验证过的`Good`和`Featured`文章中提取的1亿多个标记。与另一个广泛使用的数据集**Penn
    Treebank**（**PTB**）的预处理版本相比，`WikiText-2`大约大两倍。`WikiText`数据集还具有更大的词汇表，并保留了原始大小写、标点和数字。该数据集包含完整的文章，因此非常适合利用长期依赖的模型。
- en: The dataset was introduced in a paper called *Pointer Sentinel Mixture Models*
    ([https://arxiv.org/abs/1609.07843](https://arxiv.org/abs/1609.07843)). The paper
    talks about solutions that can be used for solving a specific problem, where the
    LSTM with a softmax layer has difficulty in predicting rare words, though the
    context is unclear. Let's not worry about this for now, as it is an advanced concept
    and out of the scope of this book.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是在一篇名为*Pointer Sentinel混合模型*的论文中介绍的（[https://arxiv.org/abs/1609.07843](https://arxiv.org/abs/1609.07843)）。该论文讨论了用于解决特定问题的解决方案，其中LSTM与softmax层在预测稀有单词时存在困难，尽管上下文不清楚。暂时不要担心这个问题，因为这是一个高级概念，超出了本书的范围。
- en: 'The following screenshot shows what the data looks like inside the WikiText
    dump:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了WikiText转储文件内部的数据样式：
- en: '![](img/f6b3f8a6-97b7-48a4-b2c7-58d38ef172af.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![数据样式](img/f6b3f8a6-97b7-48a4-b2c7-58d38ef172af.png)'
- en: 'As usual, `torchtext` makes it easier to use the dataset, by providing abstractions
    over downloading and reading the dataset. Let''s look at the code that does that:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，`torchtext`通过提供下载和读取数据集的抽象，使使用数据集变得更加容易。让我们看看执行此操作的代码：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The previous code takes care of downloading the `WikiText2` data and splits
    it into `train`, `valid`, and `test` datasets. The key difference in language
    modeling is how the data is processed. All the text data that we had in `WikiText2`
    is stored in one long tensor. Let''s look at the following code and the results,
    to understand how the data is processed better:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的代码负责下载`WikiText2`数据并将其分成`train`、`valid`和`test`数据集。语言建模的关键区别在于如何处理数据。我们在`WikiText2`中有的所有文本数据都存储在一个长张量中。让我们看一下下面的代码和结果，以更好地理解数据的处理方式：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As we can see from the previous results, we have only one example field and
    it contains all the text. Let''s also quickly look at how the text is represented:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的结果显示，我们只有一个示例字段，其中包含所有文本。让我们快速看一下文本的表示方式：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now, take a quick look at the image that showed the initial text and how it
    is being tokenized. Now we have a long sequence, of length `2088628`, representing `WikiText2`.
    The next important thing is how we batch the data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，快速查看显示初始文本及其如何被标记化的图像。现在我们有一个长度为`2088628`的长序列，表示为`WikiText2`。接下来重要的事情是如何对数据进行分批处理。
- en: Generating the batches
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成批次
- en: 'Let''s take a look at the code and understand the two key things involved in
    the batching of sequential data:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下代码，了解序列数据分批过程中涉及的两个关键要素：
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: There are two important things that are going through this method. One is `batch_size`,
    and another is `bptt_len`, called **backpropagation** **through time**. It gives
    a brief idea about how data is transformed through each phase.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法中有两个重要的事情。一个是`batch_size`，另一个是称为**时间反向传播**（**backpropagation through time**，简称`bptt_len`）。它简要说明了数据在每个阶段如何转换。
- en: Batches
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批次
- en: Processing the entire data as a sequence is quite challenging and not computationally
    efficient. So, we break the sequence data into multiple batches, and treat each
    as a separate sequence. Though it may not sound straightforward, it works a lot
    better, as the model can learn quicker from batches of data. Let's take the example
    where the English alphabet is sequenced and we split it into batches.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据作为一个序列进行处理相当具有挑战性，而且计算效率不高。因此，我们将序列数据分成多个批次，将每个批次视为单独的序列。尽管这听起来可能并不直接，但效果要好得多，因为模型可以从数据批次中更快地学习。让我们以英文字母顺序为例进行分组。
- en: 'Sequence: a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v,
    w, x, y, z.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 序列：a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y,
    z.
- en: 'When we convert the preceding alphabet sequence into four batches, we get:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将前述字母序列转换为四批时，我们得到：
- en: '*a    g    m    s    y*'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '*a    g    m    s    y*'
- en: '*b    h    n    t    z*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*b    h    n    t    z*'
- en: '*c    i     o    u*'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '*c    i     o    u*'
- en: '*d   j     p     v*'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*d   j     p     v*'
- en: '*e   k    q     w*'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*e   k    q     w*'
- en: '*f    l     r     x*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*f    l     r     x*'
- en: In most of the cases, we would end up trimming the last extra words or tokens
    that form a small batch, since it doesn't have a great effect on text modeling.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们会剔除最后多余的单词或标记，因为它对文本建模没有太大影响。
- en: For the example `WikiText2`, when we split the data into 20 batches, we would
    get each batch with elements 104431.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于例子`WikiText2`，当我们将数据分成20批次时，我们会得到每个批次104431个元素。
- en: Backpropagation through time
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: The other important variable that we see go through the iterator is **backpropagation
    through time** (**BPTT**). What it actually means is, the sequence length the
    model needs to remember. The higher the number, the better—but the complexity
    of the model and the GPU memory required for the model also increase.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到通过迭代器传递的另一个重要变量是**时间反向传播**（**BPTT**）。它实际上是指模型需要记住的序列长度。数字越大，效果越好——但模型的复杂性和需要的GPU内存也会增加。
- en: 'To understand it better, let''s look at how we can split the previous batched
    alphabet data into sequences of length two:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解它，让我们看看如何将前面批量化的字母数据分成长度为两的序列：
- en: '*a    g    m    s*'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '*a    g    m    s*'
- en: '*b    h    n    t*'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '*b    h    n    t*'
- en: 'The previous example will be passed to the model as input, and the output will
    be from the sequence but containing the next values:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例将作为输入传递给模型，并且输出将是来自序列但包含下一个值的：
- en: '*b    h    n    t*'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*b    h    n    t*'
- en: '*c    I      o    u*'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '*c    I      o    u*'
- en: For the example `WikiText2`, when we split the batched data, we get data of
    size 30, 20 for each batch where *30* is the sequence length.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于例子`WikiText2`，当我们将批量数据分割时，我们得到大小为30的数据，每个批次为20，其中*30*是序列长度。
- en: Defining a model based on LSTM
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LSTM定义一个模型
- en: 'We defined a model that is a bit similar to the networks that we saw in [Chapter
    6](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml), *Deep Learning with Sequence Data
    and Text,* but it has some key differences. The high-level architecture of the
    network looks like the following image:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个模型，它有点类似于我们在[第6章](64a06d7f-a912-46cd-a059-e0e8e1092b63.xhtml)中看到的网络，*Deep
    Learning with Sequence Data and Text*，但它有一些关键的不同之处。网络的高级架构如下图所示：
- en: '![](img/e8dd59aa-8d2c-4a98-87d7-23641c1d5387.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8dd59aa-8d2c-4a98-87d7-23641c1d5387.png)'
- en: 'As usual, let''s take a look at the code and then walk through the key parts
    of it:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们先看看代码，然后逐步讲解其中的关键部分：
- en: '[PRE35]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the `__init__` method, we create all the layers such as embedding, dropout,
    RNN, and decoder. In earlier language models, embeddings were not generally used
    in the last layer. The use of embeddings, and tying the initial embedding along
    with the embeddings of the final output layer, improves the accuracy of the language
    model. This concept was introduced in the papers *Using the Output Embedding to
    Improve Language Models* ([https://arxiv.org/abs/1608.05859](https://arxiv.org/abs/1608.05859))
    by Press and Wolf in 2016, and *Tying Word Vectors and Word Classifiers: A Loss
    Framework for Language Modeling* ([https://arxiv.org/abs/1611.01462](https://arxiv.org/abs/1611.01462))
    by Inan and his co-authors in 2016\. Once we have made the weights of encoder
    and decoder tied, we call the `init_weights` method to initialize the weights
    of the layer.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '在`__init__`方法中，我们创建所有的层，如嵌入层、dropout、RNN和解码器。在早期的语言模型中，通常不会在最后一层使用嵌入。使用嵌入，并且将初始嵌入与最终输出层的嵌入进行绑定，可以提高语言模型的准确性。这个概念是由Press和Wolf在2016年的论文*Using
    the Output Embedding to Improve Language Models*([https://arxiv.org/abs/1608.05859](https://arxiv.org/abs/1608.05859))以及Inan和他的合著者在2016年的论文*Tying
    Word Vectors and Word Classifiers: A Loss Framework for Language Modeling*([https://arxiv.org/abs/1611.01462](https://arxiv.org/abs/1611.01462))中引入的。一旦我们将编码器和解码器的权重绑定在一起，我们调用`init_weights`方法来初始化层的权重。'
- en: The `forward` function stitches all the layers together. The last linear layers
    map all the output activations from the LSTM layer to the embeddings that are
    of the size of the vocabulary. The flow of the `forward` function input is passed
    through the embedding layer and then passed on to an RNN (in this case, an LSTM),
    and then to the decoder, another linear layer.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`函数将所有层连接在一起。最后的线性层将LSTM层的所有输出激活映射到与词汇表大小相同的嵌入中。`forward`函数的流程是通过嵌入层传递输入，然后传递给一个RNN（在本例中是LSTM），然后传递给解码器，另一个线性层。'
- en: Defining the train and evaluate functions
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义训练和评估函数
- en: 'The training of the model is very similar to what we saw in all the previous
    examples in this book. There are a few important changes that we need to make
    so that the trained model works better. Let''s look at the code and its key parts:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练与本书中之前所有示例中看到的非常相似。我们需要进行一些重要的更改，以便训练后的模型运行得更好。让我们来看看代码及其关键部分：
- en: '[PRE36]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Since we are using dropout in our model, we need to use it differently during
    training and for validation/test datasets. Calling `train()` on the model will
    ensure dropout is active during training, and calling `eval()` on the model will
    ensure that dropout is used differently:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在模型中使用了 dropout，因此在训练和验证/测试数据集中需要以不同方式使用它。在模型上调用 `train()` 将确保在训练期间启用 dropout，在模型上调用
    `eval()` 将确保在验证/测试期间以不同方式使用 dropout：
- en: '[PRE37]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: For an LSTM model, along with the input, we also need to pass the hidden variables.
    The `init_hidden` function will take the batch size as input and then return a
    hidden variable, which can be used along with the inputs. We can iterate through
    the training data and pass the input data to the model. Since we are processing
    sequence data, starting with a new hidden state (randomly initialized) for every
    iteration will not make sense. So, we will use the hidden state from the previous
    iteration after removing it from the graph by calling the `detach` method. If
    we do not call the `detach` method, then we end up calculating gradients for a
    very long sequence until we run out of GPU memory.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LSTM 模型，除了输入外，我们还需要传递隐藏变量。`init_hidden` 函数将批量大小作为输入，并返回一个隐藏变量，可以与输入一起使用。我们可以迭代训练数据并将输入数据传递给模型。由于我们处理序列数据，每次迭代都从新的隐藏状态（随机初始化）开始是没有意义的。因此，我们将使用前一次迭代的隐藏状态，在通过调用
    `detach` 方法从图中移除它后使用。如果不调用 `detach` 方法，那么我们将计算一个非常长的序列的梯度，直到 GPU 内存耗尽。
- en: 'We then pass on the input to the LSTM model and calculate loss using `CrossEntropyLoss`.
    Using the previous values of the hidden state is implemented in the following `repackage_hidden`
    function:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将输入传递给 LSTM 模型，并使用 `CrossEntropyLoss` 计算损失。使用前一个隐藏状态的值是通过以下 `repackage_hidden`
    函数实现的：
- en: '[PRE38]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'RNNs and their variants, such as LSTM and the **Gated Recurrent Unit **(**GRU**),
    suffer from a problem called **exploding gradients**. One simple trick to avoid
    the problem is to clip the gradients, which is done in the following code:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 及其变体，例如 LSTM 和 **门控循环单元（GRU）**，存在一个称为 **梯度爆炸** 的问题。避免这个问题的一个简单技巧是裁剪梯度，以下是实现这个技巧的代码：
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We manually adjust the values of the parameters by using the following code.
    Implementing an optimizer manually gives more flexibility than using a prebuilt
    optimizer:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下代码手动调整参数值。手动实现优化器比使用预建优化器更灵活：
- en: '[PRE40]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We are iterating through all the parameters and adding up the value of the gradients,
    multiplied by the learning rate. Once we update all the parameters, we log all
    the statistics such as time, loss, and perplexity.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在遍历所有参数，并将梯度值乘以学习率相加。一旦更新了所有参数，我们记录所有统计信息，如时间、损失和困惑度。
- en: 'We write a similar function for validation, where we call the `eval` method
    on the model. The `evaluate` function is defined using the following code:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证，我们编写了类似的函数，在模型上调用 `eval` 方法。使用以下代码定义了 `evaluate` 函数：
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Most of the training logic and evaluation logic is similar, except for calling
    `eval` and not updating the parameters of the model.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数的训练逻辑和评估逻辑是相似的，除了调用 `eval` 并且不更新模型参数。
- en: Training the model
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We train the model for multiple epochs and validate it using the following
    code:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行多次 epoch 的训练，并使用以下代码进行验证：
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The previous code is training the model for `40` epochs, and we start with
    a high-learning rate of 20 and reduce it further when the validation loss saturates.
    Running the model for 40 epochs gives a `ppl` score of approximately `108.45`.
    The following code block contains the logs when the model was last run:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码训练模型 `40` 个 epoch，我们从一个较高的学习率 20 开始，并在验证损失饱和时进一步减少。模型运行 40 个 epoch 后得到的
    `ppl` 分数约为 `108.45`。以下代码块包含了上次运行模型时的日志：
- en: '[PRE43]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the last few months, researchers started exploring the previous approach
    to create a language model for creating pretrained embeddings. If you are more
    interested in this approach, I would strongly recommend you read the paper *Fine-tuned
    Language Models for Text Classification *[(https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)) by
    Jeremy Howard and Sebastian Ruder, where they go into a lot of detail on how language
    modeling techniques can be used to prepare domain-specific word embeddings, which
    can later be used for different NLP tasks, such as text classification problems.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月中，研究人员开始探索先前的方法，创建一个语言模型来生成预训练的嵌入。如果您对这种方法更感兴趣，我强烈推荐您阅读Jeremy Howard和Sebastian
    Ruder撰写的论文*Fine-tuned Language Models for Text Classification*[(https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146))，他们在其中详细介绍了如何使用语言建模技术来准备特定领域的词嵌入，后者可以用于不同的NLP任务，如文本分类问题。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we covered how to train deep learning algorithms that can generate
    artistic style transfers using generative networks, new images using GAN and DCGAN,
    and generate text using LSTM networks.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何训练能够使用生成网络生成艺术风格转换、使用GAN和DCGAN生成新图像以及使用LSTM网络生成文本的深度学习算法。
- en: In the next chapter, we will cover some of the modern architectures, such as
    ResNet and Inception, for building better computer vision models and models such
    as sequence-to-sequence, which can be used for building language translation and
    image captioning.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一些现代架构，例如ResNet和Inception，用于构建更好的计算机视觉模型，以及像序列到序列这样的模型，这些模型可以用于构建语言翻译和图像标题生成等任务。
