["```py\ntorch.save(model, PATH_TO_MODEL)\n```", "```py\nmodel = torch.load(PATH_TO_MODEL)\n```", "```py\ntorch.save(model.state_dict(), PATH_TO_MODEL)\n```", "```py\nmodel = ConvNet()\nmodel.load_state_dict(torch.load(PATH_TO_MODEL))\n```", "```py\nPATH_TO_MODEL = \"./convnet.pth\"\ntorch.save(model.state_dict(), PATH_TO_MODEL)\n```", "```py\nimport torch\n```", "```py\nfrom cnn_model import ConvNet\nmodel = ConvNet()\n```", "```py\nclass ConvNet(nn.Module):\n    def __init__(self):\n       …\n    def forward(self, x):\n        …\nmodel = ConvNet()\n```", "```py\nPATH_TO_MODEL = \"./convnet.pth\"\nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))\n```", "```py\nmodel.eval()\n```", "```py\nimage = Image.open(\"./digit_image.jpg\")\n```", "```py\ndef image_to_tensor(image):\n    gray_image = transforms.functional.to_grayscale(image)\n    resized_image = transforms.functional.resize(gray_image, (28, 28))\n    input_image_tensor = transforms.functional.to_tensor(resized_image)\n    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n    return input_image_tensor_norm\n```", "```py\ninput_tensor = image_to_tensor(image)\n```", "```py\ndef run_model(input_tensor):\n    model_input = input_tensor.unsqueeze(0)\n    with torch.no_grad():\n        model_output = model(model_input)[0]\n    model_prediction = model_output.detach().numpy().argmax()\n    return model_prediction\n```", "```py\noutput = run_model(input_tensor)\nprint(output)\nprint(type(output))\n```", "```py\ndef debug_model(input_tensor):\n    model_input = input_tensor.unsqueeze(0)\n    with torch.no_grad():\n        model_output = model(model_input)[0]\n    model_prediction = model_output.detach().numpy()\n    return np.exp(model_prediction)\n```", "```py\nprint(debug_model(input_tensor))\n```", "```py\ndef post_process(output):\n    return str(output)\n```", "```py\nfinal_output = post_process(output)\nprint(final_output)\nprint(type(final_output))\n```", "```py\nfrom flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef hello_world():\n    return 'Hello, World!'\nif __name__ == '__main__':\n    app.run(host='localhost', port=8890)\n```", "```py\npython example.py\n```", "```py\nhttp://localhost:8890/\n```", "```py\nfrom flask import Flask, request\nimport torch\n```", "```py\nclass ConvNet(nn.Module):\n    def __init__(self):\n    def forward(self, x):\n```", "```py\nmodel = ConvNet()\nPATH_TO_MODEL = \"./convnet.pth\"\nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))\nmodel.eval()\n```", "```py\ndef run_model(input_tensor):\n    …\n    return model_prediction\n```", "```py\ndef post_process(output):\n    return str(output)\n```", "```py\napp = Flask(__name__)\n```", "```py\n@app.route(\"/test\", methods=[\"POST\"])\ndef test():\n    data = request.files['data'].read()\n    md = json.load(request.files['metadata'])\n    input_array = np.frombuffer(data, dtype=np.float32)\n    input_image_tensor = torch.from_numpy(input_array).view(md[\"dims\"])\n    output = run_model(input_image_tensor)\n    final_output = post_process(output)\n    return final_output\n```", "```py\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8890)\n```", "```py\npython server.py\n```", "```py\nimport requests\nfrom PIL import Image\nfrom torchvision import transforms\n```", "```py\nimage = Image.open(\"./digit_image.jpg\")\n```", "```py\ndef image_to_tensor(image):\n    gray_image = transforms.functional.to_grayscale(image)\n    resized_image = transforms.functional.resize(gray_image, (28, 28))\n    input_image_tensor = transforms.functional.to_tensor(resized_image)\n    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n    return input_image_tensor_norm\n```", "```py\nimage_tensor = image_to_tensor(image)\n```", "```py\ndimensions = io.StringIO(json.dumps({'dims': list(image_tensor.shape)}))\ndata = io.BytesIO(bytearray(image_tensor.numpy()))\n```", "```py\nr = requests.post('http://localhost:8890/test',\n                  files={'metadata': dimensions,                          'data' : data})\n```", "```py\nresponse = json.loads(r.content)\n```", "```py\nprint(\"Predicted digit :\", response)\n```", "```py\npython make_request.py\n```", "```py\ntorch==1.5.0\ntorchvision==0.5.0\nPillow==6.2.2\nFlask==1.1.1\n```", "```py\nFROM python:3.8-slim\n```", "```py\nRUN apt-get -q update && apt-get -q install -y wget\n```", "```py\nCOPY ./server.py ./\nCOPY ./requirements.txt ./\n```", "```py\nRUN wget -q https://github.com/arj7192/MasteringPyTorchV2/raw/main/Chapter13/convnet.pth \n```", "```py\nRUN pip install -r requirements.txt\n```", "```py\nUSER root\n```", "```py\nENTRYPOINT [\"python\", \"server.py\"]\n```", "```py\ndocker build -t digit_recognizer .\n```", "```py\ndocker run -p 8890:8890 digit_recognizer\n```", "```py\npython make_request.py\n```", "```py\ndocker rm $(docker ps -a -q | head -1)\n```", "```py\ndocker rmi $(docker images -q \"digit_recognizer\")\n```", "```py\nsudo apt-get install openjdk-11-jdk\n```", "```py\nbrew tap AdoptOpenJDK/openjdk\nbrew      install --cask adoptopenjdk11\n```", "```py\npip install torchserve==0.6.0 torch-model-archiver==0.6.0\n```", "```py\n==========================convnet.py===========================\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass ConvNet(nn.Module):\n    def __init__(self):\n        …\n    def forward(self, x):\n        …\n```", "```py\n========================convnet_handler.py=======================\nfrom torchvision import transforms\nfrom ts.torch_handler.image_classifier import ImageClassifier\nclass ConvNetClassifier(ImageClassifier):\n    image_processing = transforms.Compose([\n        transforms.Grayscale(), transforms.Resize((28, 28)),\n        transforms.ToTensor(),  transforms.Normalize((0.1302,), (0.3069,))])\n    def postprocess(self, output):\n        return output.argmax(1).tolist()\n```", "```py\ntorch-model-archiver --model-name convnet --version 1.0 --model-file ./convnet.py --serialized-file ./convnet.pth --handler  ./convnet_handler.py\n```", "```py\nmkdir model_store\nmv convnet.mar model_store/\n```", "```py\ntorchserve --start --ncs --model-store model_store --models convnet.mar\n```", "```py\ncurl http://localhost:8081/models\n```", "```py\ncurl http://127.0.0.1:8080/predictions/convnet -T ./digit_image.jpg\n```", "```py\ntorchserve --stop\n```", "```py\nimport torch\n...\n```", "```py\nclass ConvNet(nn.Module):\n    def __init__(self):\n       …\n    def forward(self, x):\n        …\nmodel = ConvNet()\n```", "```py\nPATH_TO_MODEL = \"./convnet.pth\"\nmodel.load_state_dict(torch.load(PATH_TO_MODEL, map_location=\"cpu\"))\nmodel.eval()\n```", "```py\nimage = Image.open(\"./digit_image.jpg\")\n```", "```py\ndef image_to_tensor(image):\n    gray_image = transforms.functional.to_grayscale(image)\n    resized_image = transforms.functional.resize(gray_image, (28, 28))\n    input_image_tensor = transforms.functional.to_tensor(resized_image)\n    input_image_tensor_norm = transforms.functional.normalize(input_image_tensor, (0.1302,), (0.3069,))\n    return input_image_tensor_norm\n```", "```py\ninput_tensor = image_to_tensor(image)\n```", "```py\nfor p in model.parameters():\n    p.requires_grad_(False)\n```", "```py\ndemo_input = torch.ones(1, 1, 28, 28)\ntraced_model = torch.jit.trace(model, demo_input)\n```", "```py\nprint(traced_model.graph)\n```", "```py\nprint(traced_model.code)\n```", "```py\ntorch.jit.save(traced_model, 'traced_convnet.pt')\n```", "```py\nloaded_traced_model = torch.jit.load('traced_convnet.pt')\n```", "```py\nloaded_traced_model(input_tensor.unsqueeze(0))\n```", "```py\nmodel(input_tensor.unsqueeze(0))\n```", "```py\nscripted_model = torch.jit.script(model)\n```", "```py\nprint(scripted_model.graph)\n```", "```py\nprint(scripted_model.code)\n```", "```py\ntorch.jit.save(scripted_model, 'scripted_convnet.pt')\nloaded_scripted_model = torch.jit.load('scripted_convnet.pt')\n```", "```py\nloaded_scripted_model(input_tensor.unsqueeze(0))\n```", "```py\n#include <torch/script.h>\n...\nint main(int argc, char **argv) {\n    Mat img = imread(argv[2], IMREAD_GRAYSCALE);\n```", "```py\nresize(img, img, Size(28, 28));\n```", "```py\nauto input_ = torch::from_blob(img.data, { img.rows, img.cols, img.channels() }, at::kByte);\n```", "```py\n    auto input = input_.permute({2,0,1}).unsqueeze_(0).reshape({1, 1, img.rows, img.cols}).toType(c10::kFloat).div(255);\n    input = (input – 0.1302) / 0.3069;\n```", "```py\n    auto module = torch::jit::load(argv[1]);\n    std::vector<torch::jit::IValue> inputs;\n    inputs.push_back(input);\n```", "```py\nauto output_ = module.forward(inputs).toTensor();\n```", "```py\nauto output = output_.argmax(1);\ncout << output << '\\n';\n```", "```py\n    return 0;\n}\n```", "```py\ncmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(cpp_convnet)\nfind_package(Torch REQUIRED)\nfind_package(OpenCV REQUIRED)\nadd_executable(cpp_convnet cpp_convnet.cpp)\n...\n```", "```py\nexport OpenCV_DIR=/Users/ashish.jha/code/personal/MasteringPyTorchV2     /     Chapter13     /cpp_convnet/build_opencv/\n```", "```py\nmkdir build\ncd build\ncmake -DCMAKE_PREFIX_PATH=/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages/torch     /share/cmake/ ..\ncmake --build . --config Release\n```", "```py\nimport torch; torch.__path__\n```", "```py\n['/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages/torch     ']_\n```", "```py\n./cpp_convnet ../../scripted_convnet.pt ../../digit_image.jpg\n```", "```py\n./cpp_convnet ../../traced_convnet.pt ../../digit_image.jpg\n```", "```py\nconda create -n <env_name> python=3.7\nsource activate <env_name> \n```", "```py\ndemo_input = torch.ones(1, 1, 28, 28)\ntorch.onnx.export(model, demo_input, \"convnet.onnx\")\n```", "```py\nimport onnx\nfrom onnx_tf.backend import prepare\nmodel_onnx = onnx.load(\"./convnet.onnx\")\ntf_rep = prepare(model_onnx)\ntf_rep.export_graph(\"./convnet.pb\")\n```", "```py\nwith tf.gfile.GFile(\"./convnet.pb\", \"rb\") as f:\n    graph_definition = tf.GraphDef()\n    graph_definition.ParseFromString(f.read())\nwith tf.Graph().as_default() as model_graph:\n    tf.import_graph_def(graph_definition, name=\"\")\nfor op in model_graph.get_operations():\n    print(op.values())\n```", "```py\nmodel_output = model_graph.get_tensor_by_name('18:0')\nmodel_input = model_graph.get_tensor_by_name('input.1:0')\nsess = tf.Session(graph=model_graph)\noutput = sess.run(model_output, feed_dict={model_input: input_tensor.unsqueeze(0)})\nprint(output)\n```", "```py\nimport torch\n```", "```py\ngit clone https://github.com/arj7192/MasteringPyTorchV2.git \n```", "```py\nchmod 400 downloaded-key-pair-file.pem\n```", "```py\nssh -i downloaded-key-pair-file.pem ubuntu@<Public IP address>\n```", "```py\ngit clone https://github.com/arj7192/MasteringPyTorchV2.git \n```", "```py\n<user>@<deployment-name>-vm:~/\n```"]