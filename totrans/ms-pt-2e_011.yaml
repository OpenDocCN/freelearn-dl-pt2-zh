- en: 17 PyTorch and Explainable AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17 PyTorch 和可解释 AI
- en: Join our book community on Discord
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在我们的 Discord 书籍社区中加入我们
- en: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/EarlyAccessCommunity](https://packt.link/EarlyAccessCommunity)'
- en: '![img](img/file141.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![img](img/file141.png)'
- en: Throughout this book, we have built several deep learning models that can perform
    different kinds of tasks for us. For example, a handwritten digit classifier,
    an image-caption generator, a sentiment classifier, and more. Although we have
    mastered how to train and evaluate these models using PyTorch, we do not know
    what precisely is happening inside these models while they make predictions. Model
    interpretability or explainability is that field of machine learning where we
    aim to answer the question, why did the model make that prediction? More elaborately,
    what did the model see in the input data to make that particular prediction?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们构建了几个可以为我们执行不同任务的深度学习模型。例如，手写数字分类器，图像字幕生成器，情感分类器等等。尽管我们已经掌握了如何使用 PyTorch
    训练和评估这些模型，但我们不知道这些模型在做出预测时内部究竟发生了什么。模型可解释性或解释性是机器学习的一个领域，我们在这个领域的目标是回答这样一个问题，为什么模型做出了那个预测？更详细地说，模型在输入数据中看到了什么，以做出特定的预测？
- en: In this chapter, we will use the handwritten digit classification model from
    *Chapter 1*, *Overview of Deep Learning Using PyTorch*, to understand its inner
    workings and thereby explain why the model makes a certain prediction for a given
    input. We will first dissect the model using only PyTorch code. Then, we will
    use a specialized model interpretability toolkit, called **Captum**, to further
    investigate what is happening inside the model. Captum is a dedicated third-party
    library for PyTorch that provides model interpretability tools for deep learning
    models, including image- and text-based models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用来自 *第1章*《使用 PyTorch 概述深度学习》的手写数字分类模型，来理解其内部工作原理，并因此解释模型为给定输入做出特定预测的原因。我们将首先使用纯粹的
    PyTorch 代码来解剖模型。然后，我们将使用一种专门的模型可解释性工具包，称为**Captum**，进一步调查模型内部发生的情况。Captum 是一个专门为
    PyTorch 提供模型解释工具的第三方库，包括基于图像和文本的模型。
- en: This chapter should provide you with the skills that are necessary to uncover
    the internals of a deep learning model. Looking inside a model this way can help
    you to reason about the model's predictive behavior. At the end of this chapter,
    you will be able to use the hands-on experience to start interpreting your own
    deep learning models using PyTorch (and Captum).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章应该为您提供解开深度学习模型内部的技能所必需的知识。以这种方式查看模型内部可以帮助您理解模型的预测行为。在本章的结尾，您将能够利用实践经验开始解释您自己的深度学习模型，使用
    PyTorch（和 Captum）。
- en: 'This chapter is broken down into the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分解为以下主题：
- en: Model interpretability in PyTorch
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的模型可解释性
- en: Using Captum to interpret models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Captum 解释模型
- en: Model interpretability in PyTorch
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 中的模型可解释性
- en: In this section, we will dissect a trained handwritten digits classification
    model using PyTorch in the form of an exercise. More precisely, we will be looking
    at the details of the convolutional layers of the trained handwritten digits classification
    model to understand what visual features the model is learning from the handwritten
    digit images. We will look at the convolutional filters/kernels along with the
    feature maps produced by those filters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 PyTorch 对已训练的手写数字分类模型进行解剖，作为一项练习。更确切地说，我们将查看训练的手写数字分类模型的卷积层的详细信息，以了解模型从手写数字图像中学到了哪些视觉特征。我们将查看卷积滤波器/核心以及这些滤波器产生的特征图。
- en: Such details will help us to understand how the model is processing input images
    and, therefore, making predictions. The full code for the exercise can be found
    in our github repository [13.1] .
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些细节将帮助我们理解模型如何处理输入图像，从而进行预测。练习的完整代码可以在我们的 github 仓库 [13.1] 中找到。
- en: Training the handwritten digits classifier – a recap
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练手写数字分类器 - 重温
- en: 'We will quickly revisit the steps involved in training the handwritten digits
    classification model, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速回顾涉及训练手写数字分类模型的步骤，如下所示：
- en: 'First, we import the relevant libraries, and then set the random seeds to be
    able to reproduce the results of this exercise:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入相关的库，然后设置随机种子，以便能够重现这次练习的结果：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will define the model architecture:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义模型架构：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will define the model training and testing routine:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义模型的训练和测试过程：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then define the training and testing dataset loaders:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义训练和测试数据集加载器：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we instantiate our model and define the optimization schedule:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实例化我们的模型，并定义优化计划：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we start the model training loop where we train our model for 20 epochs:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们开始模型训练循环，训练我们的模型进行 20 个 epochs：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This should output the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.1 – Model training logs](img/file142.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1 – 模型训练日志](img/file142.jpg)'
- en: Figure 13.1 – Model training logs
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – 模型训练日志
- en: 'Finally, we can test the trained model on a sample test image. The sample test
    image is loaded as follows:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以在一个样本测试图像上测试训练好的模型。这个样本测试图像的加载方式如下：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This should output the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.2 – An example of a handwritten image](img/file143.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 一个手写图像示例](img/file143.jpg)'
- en: Figure 13.2 – An example of a handwritten image
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 一个手写图像示例
- en: 'Then, we use this sample test image to make a model prediction, as follows:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用这个样本测试图像进行模型预测，如下所示：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This should output the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.3 – Model prediction](img/file144.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3 – 模型预测](img/file144.jpg)'
- en: Figure 13.3 – Model prediction
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – 模型预测
- en: Therefore, we have trained a handwritten digits classification model and used
    it to make inference on a sample image. We will now look at the internals of the
    trained model. We will also investigate what convolutional filters have been learned
    by this model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经训练了一个手写数字分类模型，并用它对一个样本图像进行了推断。现在我们将看看训练模型的内部结构。我们还将研究这个模型学习到了哪些卷积滤波器。
- en: Visualizing the convolutional filters of the model
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化模型的卷积滤波器
- en: 'In this section, we will go through the convolutional layers of the trained
    model and look at the filters that the model has learned during training. This
    will tell us how the convolutional layers are operating on the input image, what
    kinds of features are being extracted, and more:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细讨论已训练模型的卷积层，并查看模型在训练期间学习到的滤波器。这将告诉我们卷积层在输入图像上的操作方式，正在提取哪些特征等等：
- en: 'First, we need to obtain a list of all the layers in the model, as follows:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要获取模型中所有层的列表，如下所示：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This should output the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.4 – Model layers](img/file145.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.4 – 模型层](img/file145.jpg)'
- en: Figure 13.4 – Model layers
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – 模型层
- en: As you can see, there are 2 convolutional layers that both have 3x3-sized filters.
    The first convolutional layer uses **16** such filters, whereas the second convolutional
    layer uses **32**. We are focusing on visualizing convolutional layers in this
    exercise because they are visually more intuitive. However, you can similarly
    explore the other layers, such as linear layers, by visualizing their learned
    weights.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，这里有 2 个卷积层，它们都有 3x3 大小的滤波器。第一个卷积层使用了**16**个这样的滤波器，而第二个卷积层使用了**32**个。我们重点在本练习中可视化卷积层，因为它们在视觉上更直观。然而，您可以通过类似的方式探索其他层，比如线性层，通过可视化它们学到的权重。
- en: 'Next, we select only the convolutional layers from the model and store them
    in a separate list:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从模型中选择只有卷积层，并将它们存储在一个单独的列表中：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this process, we also make sure to store the parameters or weights learned
    in each convolutional layer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们还确保存储每个卷积层中学到的参数或权重。
- en: 'We are now ready to visualize the learned filters of the convolutional layers.
    We begin with the first layer, which has 16 filters of size 3x3 each. The following
    code visualizes those filters for us:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备好可视化卷积层学到的滤波器。我们从第一层开始，该层每个都有 16 个 3x3 大小的滤波器。下面的代码为我们可视化了这些滤波器：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This should output the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.5 – The first convolutional layer''s filters](img/file146.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 第一个卷积层的滤波器](img/file146.jpg)'
- en: Figure 13.5 – The first convolutional layer's filters
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 第一个卷积层的滤波器
- en: Firstly, we can see that all the learned filters are slightly different from
    each other, which is a good sign. These filters usually have contrasting values
    inside them so that they can extract some types of gradients when convolved around
    an image. During model inference, each of these 16 filters operates independently
    on the input grayscale image and produces 16 different feature maps, which we
    will visualize in the next section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到所有学习到的滤波器都略有不同，这是一个好迹象。这些滤波器通常在内部具有对比值，以便在图像周围卷积时提取某些类型的梯度。在模型推断期间，这
    16 个滤波器中的每一个都会独立地在输入的灰度图像上操作，并产生 16 个不同的特征图，我们将在下一节中进行可视化。
- en: 'Similarly, we can visualize the 32 filters learned in the second convolutional
    layer using the same code, as in the preceding step, but with the following change:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用与前一步骤相同的代码来可视化第二个卷积层学习到的32个滤波器，但需要进行以下更改：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should output the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.6 – The second convolutional layer''s filters](img/file147.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图13.6 – 第二个卷积层的滤波器](img/file147.jpg)'
- en: Figure 13.6 – The second convolutional layer's filters
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – 第二个卷积层的滤波器
- en: Once again, we have 32 different filters/kernels that have contrasting values
    aimed at extracting gradients from the image. These filters are already applied
    to the output of the first convolutional layer, and hence produce even higher
    levels of output feature maps. The usual goal of CNN models with multiple convolutional
    layers is to keep producing more and more complex, or higher-level, features that
    can represent complex visual elements such as a nose on a face, traffic lights
    on the road, and more.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们有32个不同的滤波器/内核，它们具有对比值，旨在从图像中提取梯度。这些滤波器已经应用于第一个卷积层的输出，因此产生了更高级别的输出特征图。具有多个卷积层的CNN模型通常的目标是持续生成更复杂或更高级别的特征，可以表示复杂的视觉元素，例如面部的鼻子，道路上的交通灯等。
- en: Next, we will take a look at what comes out of these convolutional layers as
    these filters operate/convolve on their given inputs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看这些卷积层在它们的输入上操作/卷积时产生了什么。
- en: Visualizing the feature maps of the model
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化模型的特征图
- en: 'In this section, we will run a sample handwritten image through the convolutional
    layers and visualize the outputs of these layers:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将通过卷积层运行一个样本手写图像，并可视化这些层的输出：
- en: 'First, we need to gather the results of every convolutional layer output in
    the form of a list, which is achieved using the following code:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将每个卷积层输出的结果收集到一个列表中，可以通过以下代码实现：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that we call the forward pass for each convolutional layer separately
    while ensuring that the *n*th convolutional layer receives as input the output
    of the (*n-1*)th convolutional layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们分别为每个卷积层调用前向传播，同时确保第n个卷积层接收第（n-1）个卷积层的输出作为输入。
- en: 'We can now visualize the feature maps produced by the two convolutional layers.
    We will begin with the first layer by running the following code:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以可视化由这两个卷积层产生的特征图。我们将从第一层开始运行以下代码：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This should output the following:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.7 – The first convolutional layer''s feature maps](img/file148.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图13.7 – 第一个卷积层的特征图](img/file148.jpg)'
- en: Figure 13.7 – The first convolutional layer's feature maps
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7 – 第一个卷积层的特征图
- en: The numbers, **(16, 26, 26)**, represent the output dimensions of the first
    convolution layer. Essentially, the sample image size is (28, 28), the filter
    size is (3,3), and there is no padding. Therefore, the resulting feature map size
    will be (26, 26). Because there are 16 such feature maps produced by the 16 filters
    (please refer to *Figure 13.5*), the overall output dimension is (16, 26, 26).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数字**(16, 26, 26)**表示第一卷积层的输出维度。实际上，样本图像尺寸为(28, 28)，滤波器尺寸为(3,3)，并且没有填充。因此，生成的特征图大小为(26,
    26)。由于有16个这样的特征图由16个滤波器产生（请参考*图13.5*），因此总体输出维度为(16, 26, 26)。
- en: As you can see, each filter produces a feature map from the input image. Additionally,
    each feature map represents a different visual feature in the image. For example,
    the top-left feature map essentially inverts the pixel values in the image (please
    refer to *Figure 13.2*), whereas the bottom-right feature map represents some
    form of edge detection.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，每个滤波器从输入图像中生成一个特征图。此外，每个特征图代表图像中的不同视觉特征。例如，左上角的特征图基本上颠倒了图像中的像素值（请参考*图13.2*），而右下角的特征图表示某种形式的边缘检测。
- en: These 16 feature maps are then passed on to the second convolutional layer,
    where yet another 32 filters convolve separately on these 16 feature maps to produce
    32 new feature maps. We will look at these next.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些16个特征图然后传递到第二个卷积层，其中另外32个滤波器分别在这16个特征图上卷积，产生32个新的特征图。我们接下来将查看这些特征图。
- en: 'We can use the same code as the preceding one with minor changes (as highlighted
    in the following code) to visualize the 32 feature maps produced by the next convolutional
    layer:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用与前面类似的代码，稍作更改（如下面的代码所示），来可视化下一个卷积层产生的32个特征图：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This should output the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.8 – The second convolutional layer''s feature maps](img/file149.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.8 – 第二个卷积层的特征图](img/file149.jpg)'
- en: Figure 13.8 – The second convolutional layer's feature maps
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – 第二个卷积层的特征图
- en: Compared to the earlier 16 feature maps, these 32 feature maps are evidently
    more complex. They seem to be doing more than just edge detection, and this is
    because they are already operating on the outputs of the first convolutional layer
    instead of the raw input image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的16个特征图相比，这32个特征图显然更复杂。它们似乎不仅仅是边缘检测，这是因为它们已经在第一个卷积层的输出上操作，而不是原始输入图像。
- en: In this model, the 2 convolutional layers are followed by 2 linear layers with
    (4,608x64) and (64x10) number of parameters, respectively. Although the linear
    layer weights are also useful to visualize, the sheer number of parameters (4,608x64)
    is, visually, a lot to get your head around. Therefore, in this section, we will
    restrict our visual analysis to convolutional weights only.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，2个卷积层之后是2个线性层，分别有（4,608x64）和（64x10）个参数。虽然线性层的权重也有助于可视化，但参数数量（4,608x64）的视觉化分析看起来实在太多了。因此，在本节中，我们将仅限于卷积权重的视觉分析。
- en: And thankfully, we have more sophisticated ways of interpreting model prediction
    without having to look at such a large number of parameters. In the next section,
    we will explore Captum, which is a machine learning model interpretability toolkit
    that works with PyTorch and helps us to explain model decisions within a few lines
    of code.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们有更复杂的方法来解释模型预测，而不需要查看如此多的参数。在下一节中，我们将探讨 Captum，这是一个与 PyTorch 配合使用的机器学习模型解释工具包，可以在几行代码内帮助我们解释模型决策。
- en: Using Captum to interpret models
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Captum 解释模型
- en: '**Captum** [13.2] is an open source model interpretability library built by
    Facebook on top of PyTorch, and it is currently (at the time of writing) under
    active development. In this section, we will use the handwritten digits classification
    model that we had trained in the preceding section. We will also use some of the
    model interpretability tools offered by Captum to explain the predictions made
    by this model. The full code for the following exercise can be found in our github
    repository [13.3] .'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**Captum** [13.2] 是由 Facebook 在 PyTorch 上构建的开源模型解释库，目前（撰写本文时）正在积极开发中。在本节中，我们将使用前面章节中训练过的手写数字分类模型。我们还将使用
    Captum 提供的一些模型解释工具来解释该模型所做的预测。此练习的完整代码可以在我们的 github 代码库 [13.3] 中找到。'
- en: Setting up Captum
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Captum
- en: 'The model training code is similar to the code shown under the *Training the
    handwritten digits classifier – a recap* section. In the following steps, we will
    use the trained model and a sample image to understand what happens inside the
    model while making a prediction for the given image:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练代码类似于“训练手写数字分类器 – 总结”部分中显示的代码。在接下来的步骤中，我们将使用训练好的模型和一个样本图像，来理解模型在为给定图像进行预测时内部发生了什么：
- en: 'There are few extra imports related to Captum that we need to perform in order
    to use Captum''s built-in model interpretability functions:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有几个与 Captum 相关的额外导入，我们需要执行，以便使用 Captum 的内置模型解释功能：
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In order to do a model forward pass with the input image, we reshape the input
    image to match the model input size:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要对输入图像进行模型的前向传递，我们将输入图像重塑为与模型输入大小相匹配：
- en: '[PRE16]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As per Captum's requirements, the input tensor (image) needs to be involved
    in gradient computation. Therefore, we set the `requires_grad` flag for input
    to `True`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 Captum 的要求，输入张量（图像）需要参与梯度计算。因此，我们将输入的 `requires_grad` 标志设置为 `True`。
- en: 'Next, we prepare the sample image to be processed by the model interpretability
    methods using the following code:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们准备样本图像，以便通过模型解释方法进行处理，使用以下代码：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This should output the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出以下内容：
- en: '![Figure 13.9 – The original image](img/file150.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.9 – 原始图像](img/file150.jpg)'
- en: Figure 13.9 – The original image
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – 原始图像
- en: We have tiled the grayscale image across the depth dimension so that it can
    be consumed by the Captum methods, which expect a 3-channel image.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在深度维度上平铺了灰度图像，以便Captum方法能够处理，这些方法期望一个3通道图像。
- en: Next, we will actually apply some of Captum's interpretability methods to the
    forward pass of the prepared grayscale image through the pretrained handwritten
    digits classification model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实际应用一些 Captum 的解释性方法，通过预训练的手写数字分类模型对准备的灰度图像进行前向传递。
- en: Exploring Captum's interpretability tools
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 Captum 的可解释性工具
- en: In this section, we will be looking at some of the model interpretability methods
    offered by Captum.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 Captum 提供的一些模型可解释性方法。
- en: 'One of the most fundamental methods of interpreting model results is by looking
    at saliency, which represents the gradients of the output (class 0, in this example)
    with respect to the input (that is, the input image pixels). The larger the gradients
    with respect to a particular input, the more important that input is. You can
    read more about how these gradients are exactly calculated in the original saliency
    paper [13.4] . Captum provides an implementation of the saliency method:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 解释模型结果的最基本方法之一是观察显著性，它表示输出（在本例中是类别 0）关于输入（即输入图像像素）的梯度。对于特定输入，梯度越大，该输入越重要。您可以在原始的显著性论文[13.4]中详细了解这些梯度的计算方式。Captum
    提供了显著性方法的实现：
- en: 'In the following code, we use Captum''s `Saliency` module to compute the gradients:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用 Captum 的 `Saliency` 模块计算梯度：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This should output the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出如下结果：
- en: '![Figure 13.10 – Overlayed gradients](img/file151.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.10 – 叠加梯度](img/file151.jpg)'
- en: Figure 13.10 – Overlayed gradients
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 – 叠加梯度
- en: 'In the preceding code, we reshaped the obtained gradients to size `(28,28,1)`
    in order to overlay them on the original image, as shown in the preceding diagram.
    Captum''s `viz` module takes care of the visualizations for us. We can further
    visualize only the gradients, without the original image, using the following
    code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将获得的梯度重塑为 `(28,28,1)` 的大小，以便在原始图像上叠加显示，如前面的图示所示。Captum 的 `viz` 模块为我们处理了可视化。我们还可以使用以下代码仅可视化梯度，而不显示原始图像：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We will get the following output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下输出：
- en: '![Figure 13.11 – Gradients](img/file152.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.11 – 梯度](img/file152.jpg)'
- en: Figure 13.11 – Gradients
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 – 梯度
- en: As you can see, the gradients a re spread across those pixel regions in the
    image that are likely to contain the digit `0`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，梯度分布在图像中那些可能包含数字`0`的像素区域。
- en: Next, using a similar code fashion, we will look at another interpretability
    method – integrated gradients. With this method, we will look for **feature attribution**
    or **feature importance**. That is, we'll look for what pixels are important to
    use when making predictions. Under the integrated gradients technique, apart from
    the input image, we also need to specify a baseline image, which is usually set
    to an image with all of the pixel values set to zero.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将采用类似的代码方式，研究另一种可解释性方法 - 综合梯度。通过这种方法，我们将寻找**特征归因**或**特征重要性**。也就是说，我们将寻找在进行预测时使用的哪些像素是重要的。在综合梯度技术下，除了输入图像外，我们还需要指定一个基线图像，通常将其设置为所有像素值均为零的图像。
- en: 'An integral of gradients is then calculated with respect to the input image
    along the path from the baseline image to the input image. Details of the implementation
    of integrated gradients technique can be found in the original paper [13.5] .
    The following code uses Captum''s `IntegratedGradients` module to derive the importance
    of each input image pixel:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，沿着从基线图像到输入图像的路径计算梯度的积分。关于综合梯度技术的实现细节可以在原始论文[13.5]中找到。以下代码使用 Captum 的 `IntegratedGradients`
    模块推导每个输入图像像素的重要性：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This should output the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该输出如下结果：
- en: '![Figure 13.12 – Overlayed integrated gradients](img/file153.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.12 – 叠加的综合梯度](img/file153.jpg)'
- en: Figure 13.12 – Overlayed integrated gradients
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12 – 叠加的综合梯度
- en: As expected, the gradients are high in the pixel regions that contain the digit
    `0`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，梯度在包含数字`0`的像素区域中较高。
- en: 'Finally, we will look at yet another gradient-based attribution technique,
    called **deeplift**. Deeplift also requires a baseline image besides the input
    image. Once again for the baseline, we use an image with all the pixel values
    set to zero. Deeplift computes the change in non-linear activation outputs with
    respect to the change in input from the baseline image to the input image (*Figure
    13.9*). The following code uses the `DeepLift` module provided by Captum to compute
    the gradients and displays these gradients overlayed on the original input image:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将研究另一种基于梯度的归因技术，称为**深度提升**。除了输入图像外，深度提升还需要一个基线图像。再次，我们使用所有像素值设置为零的图像作为基线图像。深度提升计算非线性激活输出相对于从基线图像到输入图像的输入变化的梯度（*图
    13.9*）。以下代码使用 Captum 提供的 `DeepLift` 模块计算梯度，并将这些梯度叠加显示在原始输入图像上：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You should see the following output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 13.13 – Overlayed deeplift](img/file154.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.13 – 覆盖的 deeplift](img/file154.jpg)'
- en: Figure 13.13 – Overlayed deeplift
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13 – 覆盖的 deeplift
- en: Once again, the gradient values are extreme around the pixels that contain the
    digit `0`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，梯度值在包含数字`0`的像素周围是极端的。
- en: This brings us to the end of this exercise and this section. There are more
    model interpretability techniques provided by Captum, such as *LayerConductance*,
    *GradCAM*, and *SHAP [13.6]* . Model interpretability is an active area of research,
    and hence libraries such as Captum are likely to evolve rapidly. More such libraries
    are likely to be developed in the near future, which will enable us to make model
    interpretability a standard component of the machine learning life cycle.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了本练习和本节。Captum 提供了更多的模型解释技术，例如*LayerConductance*、*GradCAM*和*SHAP [13.6]*。模型解释性是一个活跃的研究领域，因此像Captum这样的库可能会迅速发展。在不久的将来，可能会开发出更多类似的库，这些库将使模型解释成为机器学习生命周期的标准组成部分。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have briefly explored how to explain or interpret the decisions
    made by deep learning models using PyTorch.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要探讨了如何使用PyTorch解释或解读深度学习模型所做决策的方法。
- en: In the next chapter of this book, we will learn how to rapidly train and test
    machine learning models on PyTorch – a skill that is useful for quickly iterating
    over various machine learning ideas. We will also discuss a few deep learning
    libraries and frameworks that enable rapid prototyping with PyTorch.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章中，我们将学习如何在PyTorch上快速训练和测试机器学习模型——这是一个用于快速迭代各种机器学习想法的技能。我们还将讨论一些能够使用PyTorch进行快速原型设计的深度学习库和框架。
