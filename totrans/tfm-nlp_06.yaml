- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Translation with the Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans master sequence transduction, transferring a representation to another
    object. We can easily imagine a mental representation of a sequence. If somebody
    says *The flowers in my garden are beautiful*, we can easily visualize a garden
    with flowers in it. We see images of the garden, although we might never have
    seen that garden. We might even imagine chirping birds and the scent of flowers.
  prefs: []
  type: TYPE_NORMAL
- en: A machine must learn transduction from scratch with numerical representations.
    Recurrent or convolutional approaches have produced interesting results but have
    not reached significant BLEU translation evaluation scores. Translating requires
    the representation of language *A* transposed into language *B*.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer model’s self-attention innovation increases the analytic ability
    of machine intelligence. A sequence in language *A* is adequately represented
    before attempting to translate it into language *B*. Self-attention brings the
    level of intelligence required by a machine to obtain better BLEU scores.
  prefs: []
  type: TYPE_NORMAL
- en: The seminal *Attention Is All You Need* Transformer obtained the best results
    for English-German and English-French translations in 2017\. Since then, the scores
    have been improved by other transformers.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point in the book, we have covered the essential aspects of transformers:
    the *architecture* of the Transformer, *training* a RoBERTa model from scratch,
    *fine-tuning* a BERT, *evaluating* a fine-tuned BERT, and exploring *downstream
    tasks* with some transformer examples.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through machine translation in three additional
    topics. We will first define what machine translation is. We will then preprocess
    a **Workshop on Machine Translation** (**WMT**) dataset. Finally, we will see
    how to implement machine translations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human transductions and translations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine transductions and translations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing a WMT dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating machine translation with BLEU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric evaluations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chencherry smoothing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Google Translate’s API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing the English-German problem with Trax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to define machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Defining machine translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) tackled one of the most difficult NLP problems when
    designing the Transformer. The human baseline for machine translation seems out
    of reach for us human-machine intelligence designers. This did not stop *Vaswani*
    et al. (2017) from publishing the Transformer’s architecture and achieving state-of-the-art
    BLEU results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will define machine translation. Machine translation is
    the process of reproducing human translation by machine transductions and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Machine translation process'
  prefs: []
  type: TYPE_NORMAL
- en: 'The general idea in *Figure 6.1* is for the machine to do the following in
    a few steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a sentence to translate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how words relate to each other with hundreds of millions of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn the many ways in which words refer to each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use machine transduction to transfer the learned parameters to new sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose a candidate translation for a word or sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process always starts with a sentence to translate from a source language,
    *A*. The process ends with an output containing a translated sentence in language
    *B*. The intermediate calculations involve transductions.
  prefs: []
  type: TYPE_NORMAL
- en: Human transductions and translations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A human interpreter at the European Parliament, for instance, will not translate
    a sentence word by word. Word-by-word translations often make no sense because
    they lack the proper *grammatical structure* and cannot produce the right translation
    because the *context* of each word is ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Human transduction takes a sentence in language *A* and builds a cognitive *representation*
    of the sentence’s meaning. An interpreter (oral translations) or a translator
    (written translations) at the European Parliament will only then transform that
    transduction into an interpretation of that sentence in language *B*.
  prefs: []
  type: TYPE_NORMAL
- en: We will name the translation done by the interpreter or translator in language
    *B* a *reference* sentence.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice several references in the *Machine translation process* described
    in *Figure 6.1*.
  prefs: []
  type: TYPE_NORMAL
- en: A human translator will not translate sentence *A* into sentence *B* several
    times but only once in real life. However, more than one translator could translate
    sentence *A* in real life. For example, you can find several French to English
    translations of *Les Essais* by Montaigne. If you take one sentence, *A*, out
    of the original French version, you will thus find several versions of sentence
    *B* noted as references `1` to `n`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you go to the European Parliament one day, you might notice that the interpreters
    only translate for a limited time of two hours, for example. Then another interpreter
    takes over. No two interpreters have the same style, just like writers have different
    styles. Sentence *A* in the source language might be repeated by the same person
    several times in a day but be translated into several reference sentence *B* versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*reference* ={*reference 1*, *reference 2*,…*reference n*}'
  prefs: []
  type: TYPE_NORMAL
- en: Machines have to find a way to think the same way as human translators.
  prefs: []
  type: TYPE_NORMAL
- en: Machine transductions and translations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transduction process of the original Transformer architecture uses the encoder
    stack, the decoder stack, and all the model’s parameters to represent a *reference
    sequence*. We will refer to that output sequence as the *reference*.
  prefs: []
  type: TYPE_NORMAL
- en: Why not just say “output prediction”? The problem is that there is no single
    output prediction. The Transformer, like humans, will produce a result we can
    refer to, but that can change if we train it differently or use different transformer
    models!
  prefs: []
  type: TYPE_NORMAL
- en: We immediately realize that the human baseline of human transduction, representations
    of a language sequence, is quite a challenge. However, much progress has been
    made.
  prefs: []
  type: TYPE_NORMAL
- en: An evaluation of machine translation proves that NLP has progressed. To determine
    that one solution is better than another, each NLP challenger, lab, or organization
    must refer to the same datasets for the comparison to be valid.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore a WMT dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing a WMT dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) present the Transformer’s achievements on the WMT 2014
    English-to-German translation task and the WMT 2014 English-to-French translation
    task. The Transformer achieves a state-of-the-art BLEU score. BLEU will be described
    in the *Evaluating machine translation with BLEU* section of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The 2014 WMT contained several European language datasets. One of the datasets
    contained data taken from version 7 of the Europarl corpus. We will be using the
    French-English dataset from the *European Parliament Proceedings Parallel Corpus*,
    1996-2011 ([https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the files and have extracted them, we will preprocess
    the two parallel files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`europarl-v7.fr-en.en`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`europarl-v7.fr-en.fr`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will load, clear, and reduce the size of the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start the preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the raw data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will preprocess `europarl-v7.fr-en.en` and `europarl-v7.fr-en.fr`.
  prefs: []
  type: TYPE_NORMAL
- en: Open `read.py`, which is in this chapter’s GitHub directory. Ensure that the
    two europarl files are in the same directory as `read.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program begins using standard Python functions and `pickle` to dump the
    serialized output files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we define the function to load the file into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The loaded document is then split into sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The shortest and the longest lengths are retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The imported sentence lines must be cleaned to avoid training useless and noisy
    tokens. The lines are normalized, tokenized on white spaces, and converted to
    lowercase. The punctuation is removed from each token, non-printable characters
    are removed, and tokens containing numbers are excluded. The cleaned line is stored
    as a string.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program runs the cleaning function and returns clean appended strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We have defined the key functions we will call to prepare the datasets. The
    English data is loaded and cleaned first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is now clean, and `pickle` dumps it into a serialized file named
    `English.pkl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the key statistics and confirms that `English.pkl` is saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We now repeat the same process with the French data and dump it into a serialized
    file named `French.pkl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The output shows the key statistics for the French dataset and confirms that
    `French.pkl` is saved.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The main preprocessing is done. But we still need to make sure the datasets
    do not contain noisy and confusing tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Finalizing the preprocessing of the datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, open `read_clean.py` in the same directory as `read.py`. Our process now
    defines the function that will load the datasets that were cleaned up in the previous
    section and then save them once the preprocessing is finalized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We now define a function that will create a vocabulary counter. It is important
    to know how many times a word is used in the sequences we will parse. For example,
    if a word is only used once in a dataset containing two million lines, we will
    waste our energy using precious GPU resources to learn it. Let’s define the counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The vocabulary counter will detect words with a frequency that is below `min_occurrence`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `min_occurrence=5` and the words below or equal to this threshold
    have been removed to avoid wasting the training model’s time analyzing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have to deal with **Out-Of-Vocabulary** (**OOV**) words. OOV words can
    be misspelled words, abbreviations, or any word that does not fit standard vocabulary
    representations. We could use automatic spelling, but it would not solve all of
    the problems. For this example, we will simply replace OOV words with the `unk`
    (unknown) token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now run the functions for the English dataset, save the output, and
    then display `20` lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output functions first show the vocabulary compression obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The preprocessed dataset is saved. The output function then displays `20` lines,
    as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now run the functions for the French dataset, save the output, and then
    display `20` lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output functions first show the vocabulary compression obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preprocessed dataset is saved. The output function then displays `20` lines,
    as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This section shows how raw data must be processed before training. The datasets
    are now ready to be plugged into a transformer to be trained.
  prefs: []
  type: TYPE_NORMAL
- en: Each line of the French dataset is the *sentence* to translate. Each line of
    the English dataset is the *reference* for a machine translation model. The machine
    translation model must produce an *English candidate translation* that matches
    the *reference*.
  prefs: []
  type: TYPE_NORMAL
- en: BLEU provides a method to evaluate `candidate` translations produced by machine
    translation models.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating machine translation with BLEU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Papineni* et al. (2002) came up with an efficient way to evaluate a human
    translation. The human baseline was difficult to define. However, they realized
    that we could obtain efficient results if we compared human translation with machine
    translation, word for word.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Papineni* et al. (2002) named their method the **Bilingual Evaluation Understudy
    Score** (**BLEU**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will use the **Natural Language Toolkit** (**NLTK**) to
    implement BLEU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu)'
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with geometric evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BLEU method compares the parts of a candidate sentence to a reference sentence
    or several reference sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Open `BLEU.py`, which is in the chapter directory of the GitHub repository of
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program imports the `nltk` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It then simulates a comparison between a candidate translation produced by the
    machine translation model and the actual translation(s) references in the dataset.
    Remember that a sentence could have been repeated several times and translated
    by different translators in different ways, making it challenging to find efficient
    evaluation strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program can evaluate one or more references:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The score for both examples is `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A straightforward evaluation *P* of the candidate *C*, the reference *R*, and
    the number of correct tokens found in *C (N)* can be represented as a geometric
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This geometric approach is rigid if you are looking for a 3-gram overlap, for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is severe if you are looking for 3-gram overlaps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: A human can see that the score should be `1` and not `0.7`. The hyperparameters
    can be changed, but the approach remains rigid.
  prefs: []
  type: TYPE_NORMAL
- en: The warning in the code above is a good one that announces the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The messages may vary with each version of the program and each run since this
    is a stochastic process.
  prefs: []
  type: TYPE_NORMAL
- en: '*Papineni* et al. (2002) came up with a modified unigram approach. The idea
    was to count the word occurrences in the reference sentence and ensure the word
    was not over-evaluated in the candidate sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example explained by *Papineni* et al. (2002):'
  prefs: []
  type: TYPE_NORMAL
- en: '`Reference 1: The cat is on the mat.`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Reference 2: There is a cat on the mat.`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now consider the following candidate sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Candidate: the the the the the the the`'
  prefs: []
  type: TYPE_NORMAL
- en: We now look for the number of words in the candidate sentence (the `7` occurrences
    of the same word “the”) present in the `Reference 1` sentence (`2` occurrences
    of the word “the”).
  prefs: []
  type: TYPE_NORMAL
- en: A standard unigram precision would be `7/7`.
  prefs: []
  type: TYPE_NORMAL
- en: The modified unigram precision is `2/7`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the BLEU function output warning agrees and suggests using smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s add smoothing techniques to the BLEU toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a smoothing technique
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Chen* and *Cherry* (2014) introduced a smoothing technique that improves standard
    BLEU techniques’ geometric evaluation approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Label smoothing is a very efficient method that improves the performance of
    a transformer model during the training phase. It has a negative impact on perplexity.
    However, it forces the model to be more uncertain. In turn, this has a positive
    effect on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we have to predict what the masked word is in the following
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`The cat [mask] milk`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the output comes out as a softmax vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This would be a brutal approach. Label smoothing can make the system more open-minded
    by introducing epsilon = ![](img/B17948_06_001.png).
  prefs: []
  type: TYPE_NORMAL
- en: The number of elements of `candidate_softmax` is *k=4*.
  prefs: []
  type: TYPE_NORMAL
- en: For label smoothing, we can set ![](img/B17948_06_001.png) to `0.25`, for example.
  prefs: []
  type: TYPE_NORMAL
- en: One of the several approaches to label smoothing can be a straightforward function.
  prefs: []
  type: TYPE_NORMAL
- en: First, reduce the value of `candidate_one_hot` by ![](img/B17948_06_003.png).
  prefs: []
  type: TYPE_NORMAL
- en: Increase the `0` values by ![](img/B17948_06_004.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We obtain the following result if we apply this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '`candidate_smoothed=[0.75,0.083,0.083,0.083]`, making the output open to future
    transformations and changes.'
  prefs: []
  type: TYPE_NORMAL
- en: The transformer uses variants of label smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: A variant of BLEU is chencherry smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: Chencherry smoothing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Chen* and *Cherry* (2014) introduced an interesting way of smoothing candidate
    evaluations by adding ![](img/B17948_06_001.png) to otherwise `0` values. There
    are several chencherry (Boxing Chen + Colin Cherry) methods: [https://www.nltk.org/api/nltk.translate.html](https://www.nltk.org/api/nltk.translate.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first evaluate a French-English example with smoothing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Although a human could accept the candidate, the output score is weak:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s add some openminded smoothing to the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The score does not reach human acceptability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We have now seen how a dataset is preprocessed and how BLEU evaluates machine
    translations.
  prefs: []
  type: TYPE_NORMAL
- en: Translation with Google Translate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Translate, [https://translate.google.com/](https://translate.google.com/),
    provides a ready-to-use interface for translations. Google is progressively introducing
    a transformer encoder into its translation algorithms. In the following section,
    we will implement a transformer model for a translation task with Google Trax.
  prefs: []
  type: TYPE_NORMAL
- en: However, an AI specialist may not be required at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we enter the sentence analyzed in the previous section in Google Translate,
    `Levez-vous svp pour cette minute de silence`, we obtain an English translation
    in real time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Google Translate'
  prefs: []
  type: TYPE_NORMAL
- en: The translation is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Does Industry 4.0 still require AI specialists for translation tasks or simply
    a web interface developer?
  prefs: []
  type: TYPE_NORMAL
- en: 'Google provides every service required for translations on their Google Translate
    platform: [https://cloud.google.com/translate](https://cloud.google.com/translate):'
  prefs: []
  type: TYPE_NORMAL
- en: 'A translation API: A web developer can create an interface for a customer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A media translation API that can translate your streaming content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AutoML translation service that will train a custom model for a specific
    domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Google translate project requires a web developer for the interfaces, a **Subject
    Matter Expert** (**SME**), and perhaps a linguist. However, an AI specialist is
    not a prerequisite.
  prefs: []
  type: TYPE_NORMAL
- en: 'Industry 4.0 is going toward AI as a service. So why bother studying AI development
    with transformers? There are two important reasons to become an Industry 4.0 AI
    specialist:'
  prefs: []
  type: TYPE_NORMAL
- en: In real life, AI projects often run into unexpected problems. For example, Google
    Translate might not fit a specific need no matter how much goodwill was put into
    the project. In that case, Google Trax will come in handy!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use Google Trax for AI, you need to be an AI developer!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You never know! The Fourth Industrial Revolution is connecting everything to
    everything. Some AI projects might run smoothly, and some will require AI expertise
    to solve complex problems. For example, in *Chapter 14*, *Interpreting Black Box
    Transformer Models*, we will show how AI development is sometimes required to
    implement Google Translate.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to implement translations with Trax.
  prefs: []
  type: TYPE_NORMAL
- en: Translations with Trax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Brain developed **Tensor2Tensor** (**T2T**) to make deep learning development
    easier. T2T is an extension of TensorFlow and contains a library of deep learning
    models that contains many transformer examploes.
  prefs: []
  type: TYPE_NORMAL
- en: Although T2T was a good start, Google Brain then produced Trax, an end-to-end
    deep learning library. Trax contains a transformer model that can be applied to
    translations. The Google Brain team presently maintains Trax.
  prefs: []
  type: TYPE_NORMAL
- en: This section will focus on the minimum functions to initialize the English-German
    problem described by *Vaswani* et al. (2017) to illustrate the Transformer’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using preprocessed English and German datasets to show that the Transformer
    architecture is language-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: Open `Trax_Translation.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by installing the modules we need.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Trax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Google Brain has made Trax easy to install and run. We will import the basics
    along with Trax, which can be installed in one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Yes, it’s that simple!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create our transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the original Transformer model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create the original Transformer model as described in *Chapter 2*, *Getting
    Started with the Architecture of the Transformer Model*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Trax function will retrieve a pretrained model configuration in a few lines
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The model is the Transformer with an encoder and decoder stack. Each stack contains
    `6` layers and `8` heads. `d_model=512`, as in the architecture of the original
    Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer requires the pretrained weights to run.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the model using pretrained weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The pretrained weights contain the intelligence of the Transformer. The weights
    constitute the Transformer’s representation of language. The weights can be expressed
    as a number of parameters that will produce some form of *machine intelligence
    IQ*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s give life to the model by initializing the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The machine configuration and its *intelligence* are now ready to run. Let’s
    tokenize a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing a sentence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our machine translator is ready to tokenize a sentence. The notebook uses the
    vocabulary preprocessed by `Trax`. The preprocessing method is similar to the
    one described in this chapter’s *Preprocessing a WMT dataset* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence will now be tokenized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The program will now decode the sentence and produce a translation.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding from the Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Transformer encodes the sentence in English and will decode it in German.
    The model and its weights constitute its set of abilities.
  prefs: []
  type: TYPE_NORMAL
- en: '`Trax` has made the decoding function intuitive to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note that higher temperatures will produce diverse results, just as with human
    translators, as explained in this chapter’s *Defining machine translation* section.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the program will de-tokenize and display the translation.
  prefs: []
  type: TYPE_NORMAL
- en: De-tokenizing and displaying the translation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Brain has produced a mainstream, disruptive, and intuitive implementation
    of the Transformer with `Trax`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now de-tokenizes and displays the translation in a few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is quite impressive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The Transformer translated `machine intelligence` into `Maschinenübersicht`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we deconstruct `Maschinenübersicht` into `Maschin (machine)` + `übersicht
    (intelligence)`, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`über` literally means “over”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sicht` means “sight” or “view”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformer tells us that although it is a machine, it has vision. Machine
    intelligence is growing through Transformers, but it is not human intelligence.
    Machines learn languages with an intelligence of their own.
  prefs: []
  type: TYPE_NORMAL
- en: That concludes our experiment with Google Trax.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through three additional essential aspects of the original
    Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: We started by defining machine translation. Human translation sets an extremely
    high baseline for machines to reach. We saw that English-French and English-German
    translations imply numerous problems to solve. The transformer tackled these problems
    and set state-of-the-art BLEU records to beat.
  prefs: []
  type: TYPE_NORMAL
- en: We then preprocessed a WMT French-English dataset from the European Parliament
    that required cleaning. We had to transform the datasets into lines and clean
    the data up. Once that was done, we reduced the dataset’s size by suppressing
    words that occurred below a frequency threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Machine translation NLP models require identical evaluation methods. Training
    a model on a WMT dataset requires BLEU evaluations. We saw that geometric assessments
    are a good basis for scoring translations, but even modified BLEU has its limits.
    We thus added a smoothing technique to enhance BLEU.
  prefs: []
  type: TYPE_NORMAL
- en: We saw that Google Translate provides a standard translation API, a media streaming
    API, and custom AutoML model training services. Implementing Google Translate
    APIs may require no AI development if the project rolls out smoothly. If not,
    we will have to get our hands dirty, like in the old days!
  prefs: []
  type: TYPE_NORMAL
- en: We implemented an English-to-German translation transformer with Trax, Google
    Brain’s end-to-end deep learning library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now covered the main building blocks to construct transformers: architecture,
    pretraining, training, preprocessing datasets, and evaluation methods.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *The Rise of Suprahuman Transformers with GPT-3 Engines*,
    we will discover mind-blowing ways of implementing transformers with the building
    blocks we explored in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine translation has now exceeded human baselines. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Machine translation requires large datasets. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is no need to compare transformer models using the same datasets. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BLEU is the French word for *blue* and is the acronym of an NLP metric (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Smoothing techniques enhance BERT. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: German-English is the same as English-German for machine translation. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original Transformer multi-head attention sub-layer has 2 heads. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original Transformer encoder has 6 layers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original Transformer encoder has 6 layers but only 2 decoder layers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can train transformers without decoders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'English-German BLEU scores with reference papers and code: [https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The 2014 **Workshop on Machine Translation** (**WMT**): [https://www.statmt.org/wmt14/translation-task.html](https://www.statmt.org/wmt14/translation-task.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*European Parliament Proceedings Parallel Corpus 1996-2011*, parallel corpus
    French-English: [https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jason Brownlee*, *Ph.D*., *How to Prepare a French-to-English Dataset for
    Machine Translation*: [https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/](https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kishore Papineni*, *Salim Roukos*, *Todd Ward*, and *Wei-Jing Zhu*, *2002*,
    *BLEU: a Method for Automatic Evaluation of Machine Translation*: [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jason Brownlee*, *Ph.D*., *A Gentle Introduction to Calculating the BLEU Score
    for Text in Python*: [https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Boxing Chen* and *Colin Cherry*, 2014, *A Systematic Comparison of Smoothing
    Techniques for Sentence-Level BLEU*: [http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, and *Illia Polosukhin*, 2017, *Attention
    Is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trax repository: [https://github.com/google/trax](https://github.com/google/trax)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trax tutorial: [https://trax-ml.readthedocs.io/en/latest/](https://trax-ml.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
