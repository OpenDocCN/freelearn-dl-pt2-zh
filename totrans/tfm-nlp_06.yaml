- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Translation with the Transformer
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Humans master sequence transduction, transferring a representation to another
    object. We can easily imagine a mental representation of a sequence. If somebody
    says *The flowers in my garden are beautiful*, we can easily visualize a garden
    with flowers in it. We see images of the garden, although we might never have
    seen that garden. We might even imagine chirping birds and the scent of flowers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: A machine must learn transduction from scratch with numerical representations.
    Recurrent or convolutional approaches have produced interesting results but have
    not reached significant BLEU translation evaluation scores. Translating requires
    the representation of language *A* transposed into language *B*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: The transformer model’s self-attention innovation increases the analytic ability
    of machine intelligence. A sequence in language *A* is adequately represented
    before attempting to translate it into language *B*. Self-attention brings the
    level of intelligence required by a machine to obtain better BLEU scores.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The seminal *Attention Is All You Need* Transformer obtained the best results
    for English-German and English-French translations in 2017\. Since then, the scores
    have been improved by other transformers.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point in the book, we have covered the essential aspects of transformers:
    the *architecture* of the Transformer, *training* a RoBERTa model from scratch,
    *fine-tuning* a BERT, *evaluating* a fine-tuned BERT, and exploring *downstream
    tasks* with some transformer examples.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through machine translation in three additional
    topics. We will first define what machine translation is. We will then preprocess
    a **Workshop on Machine Translation** (**WMT**) dataset. Finally, we will see
    how to implement machine translations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Defining machine translation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human transductions and translations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine transductions and translations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing a WMT dataset
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating machine translation with BLEU
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric evaluations
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chencherry smoothing
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Google Translate’s API
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing the English-German problem with Trax
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will be to define machine translation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Defining machine translation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) tackled one of the most difficult NLP problems when
    designing the Transformer. The human baseline for machine translation seems out
    of reach for us human-machine intelligence designers. This did not stop *Vaswani*
    et al. (2017) from publishing the Transformer’s architecture and achieving state-of-the-art
    BLEU results.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will define machine translation. Machine translation is
    the process of reproducing human translation by machine transductions and outputs:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_06_01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Machine translation process'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The general idea in *Figure 6.1* is for the machine to do the following in
    a few steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Choose a sentence to translate
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how words relate to each other with hundreds of millions of parameters
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习单词如何相互关联以及数以亿计的参数
- en: Learn the many ways in which words refer to each other
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习单词之间相互指代的多种方式
- en: Use machine transduction to transfer the learned parameters to new sequences
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器传导将学习的参数传递给新的序列
- en: Choose a candidate translation for a word or sequence
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为单词或序列选择候选翻译
- en: The process always starts with a sentence to translate from a source language,
    *A*. The process ends with an output containing a translated sentence in language
    *B*. The intermediate calculations involve transductions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程总是以从源语言*A*翻译的句子开始。过程以包含语言*B*中翻译的句子的输出结束。中间计算涉及传导。
- en: Human transductions and translations
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类传导和翻译
- en: A human interpreter at the European Parliament, for instance, will not translate
    a sentence word by word. Word-by-word translations often make no sense because
    they lack the proper *grammatical structure* and cannot produce the right translation
    because the *context* of each word is ignored.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，欧洲议会的人类口译员不会逐字逐句地翻译句子。逐字翻译通常毫无意义，因为它们缺乏适当的*语法结构*，并且无法产生正确的翻译，因为忽略了每个词的*上下文*。
- en: Human transduction takes a sentence in language *A* and builds a cognitive *representation*
    of the sentence’s meaning. An interpreter (oral translations) or a translator
    (written translations) at the European Parliament will only then transform that
    transduction into an interpretation of that sentence in language *B*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人类传导将语言*A*中的一句话构建成一种认知*表征*这句话的含义。欧洲议会的口译员（口头翻译）或翻译员（书面翻译）只有在此之后才会将该传导转化为语言*B*中对该句话的解释。
- en: We will name the translation done by the interpreter or translator in language
    *B* a *reference* sentence.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将口译员或翻译员在语言*B*中完成的翻译称为*参考*句子。
- en: You will notice several references in the *Machine translation process* described
    in *Figure 6.1*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.1*中描述的*机器翻译过程*中，您会注意到几个参考资料。
- en: A human translator will not translate sentence *A* into sentence *B* several
    times but only once in real life. However, more than one translator could translate
    sentence *A* in real life. For example, you can find several French to English
    translations of *Les Essais* by Montaigne. If you take one sentence, *A*, out
    of the original French version, you will thus find several versions of sentence
    *B* noted as references `1` to `n`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人类翻译员在现实生活中不会将句子*A*翻译为句子*B*多次，而只会翻译一次。然而，在现实生活中可能会有不止一个翻译员翻译句子*A*。例如，你可以找到蒙田的《论文》的多个法语到英语的翻译。如果你从原始法语版本中取出句子*A*，你会发现被标注为引用`1`到`n`的句子*B*的几个版本。
- en: 'If you go to the European Parliament one day, you might notice that the interpreters
    only translate for a limited time of two hours, for example. Then another interpreter
    takes over. No two interpreters have the same style, just like writers have different
    styles. Sentence *A* in the source language might be repeated by the same person
    several times in a day but be translated into several reference sentence *B* versions:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一天你去欧洲议会，你可能会注意到口译员只会在有限的时间内翻译两个小时，例如。然后另一个口译员接替。没有两个口译员有相同的风格，就像作家有不同的风格一样。源语言中的句子*A*可能由同一个人在一天中重复几次，但会被翻译成几个参考句子*B*版本：
- en: '*reference* ={*reference 1*, *reference 2*,…*reference n*}'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*参考* = {*参考 1*, *参考 2*,…*参考 n*}'
- en: Machines have to find a way to think the same way as human translators.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 机器必须找到一种以与人类翻译员相同的方式思考的方法。
- en: Machine transductions and translations
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器传导和翻译
- en: The transduction process of the original Transformer architecture uses the encoder
    stack, the decoder stack, and all the model’s parameters to represent a *reference
    sequence*. We will refer to that output sequence as the *reference*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原始Transformer架构的传导过程使用编码器堆栈、解码器堆栈和所有模型参数来表示一个*参考序列*。我们将称之为*参考*的输出序列。
- en: Why not just say “output prediction”? The problem is that there is no single
    output prediction. The Transformer, like humans, will produce a result we can
    refer to, but that can change if we train it differently or use different transformer
    models!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不直接说“输出预测”？问题在于没有单一的输出预测。Transformer，就像人类一样，将产生一个我们可以参考的结果，但如果我们对其进行不同的训练或使用不同的Transformer模型，结果可能会改变！
- en: We immediately realize that the human baseline of human transduction, representations
    of a language sequence, is quite a challenge. However, much progress has been
    made.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: An evaluation of machine translation proves that NLP has progressed. To determine
    that one solution is better than another, each NLP challenger, lab, or organization
    must refer to the same datasets for the comparison to be valid.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore a WMT dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing a WMT dataset
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Vaswani* et al. (2017) present the Transformer’s achievements on the WMT 2014
    English-to-German translation task and the WMT 2014 English-to-French translation
    task. The Transformer achieves a state-of-the-art BLEU score. BLEU will be described
    in the *Evaluating machine translation with BLEU* section of this chapter.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The 2014 WMT contained several European language datasets. One of the datasets
    contained data taken from version 7 of the Europarl corpus. We will be using the
    French-English dataset from the *European Parliament Proceedings Parallel Corpus*,
    1996-2011 ([https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have downloaded the files and have extracted them, we will preprocess
    the two parallel files:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '`europarl-v7.fr-en.en`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`europarl-v7.fr-en.fr`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will load, clear, and reduce the size of the corpus.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start the preprocessing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the raw data
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will preprocess `europarl-v7.fr-en.en` and `europarl-v7.fr-en.fr`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Open `read.py`, which is in this chapter’s GitHub directory. Ensure that the
    two europarl files are in the same directory as `read.py`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'The program begins using standard Python functions and `pickle` to dump the
    serialized output files:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we define the function to load the file into memory:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The loaded document is then split into sentences:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The shortest and the longest lengths are retrieved:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The imported sentence lines must be cleaned to avoid training useless and noisy
    tokens. The lines are normalized, tokenized on white spaces, and converted to
    lowercase. The punctuation is removed from each token, non-printable characters
    are removed, and tokens containing numbers are excluded. The cleaned line is stored
    as a string.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'The program runs the cleaning function and returns clean appended strings:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We have defined the key functions we will call to prepare the datasets. The
    English data is loaded and cleaned first:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The dataset is now clean, and `pickle` dumps it into a serialized file named
    `English.pkl`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output shows the key statistics and confirms that `English.pkl` is saved:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We now repeat the same process with the French data and dump it into a serialized
    file named `French.pkl`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output shows the key statistics for the French dataset and confirms that
    `French.pkl` is saved.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The main preprocessing is done. But we still need to make sure the datasets
    do not contain noisy and confusing tokens.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Finalizing the preprocessing of the datasets
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, open `read_clean.py` in the same directory as `read.py`. Our process now
    defines the function that will load the datasets that were cleaned up in the previous
    section and then save them once the preprocessing is finalized:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now define a function that will create a vocabulary counter. It is important
    to know how many times a word is used in the sequences we will parse. For example,
    if a word is only used once in a dataset containing two million lines, we will
    waste our energy using precious GPU resources to learn it. Let’s define the counter:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The vocabulary counter will detect words with a frequency that is below `min_occurrence`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In this case, `min_occurrence=5` and the words below or equal to this threshold
    have been removed to avoid wasting the training model’s time analyzing them.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have to deal with **Out-Of-Vocabulary** (**OOV**) words. OOV words can
    be misspelled words, abbreviations, or any word that does not fit standard vocabulary
    representations. We could use automatic spelling, but it would not solve all of
    the problems. For this example, we will simply replace OOV words with the `unk`
    (unknown) token:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will now run the functions for the English dataset, save the output, and
    then display `20` lines:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output functions first show the vocabulary compression obtained:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preprocessed dataset is saved. The output function then displays `20` lines,
    as shown in the following excerpt:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s now run the functions for the French dataset, save the output, and then
    display `20` lines:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output functions first show the vocabulary compression obtained:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preprocessed dataset is saved. The output function then displays `20` lines,
    as shown in the following excerpt:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This section shows how raw data must be processed before training. The datasets
    are now ready to be plugged into a transformer to be trained.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Each line of the French dataset is the *sentence* to translate. Each line of
    the English dataset is the *reference* for a machine translation model. The machine
    translation model must produce an *English candidate translation* that matches
    the *reference*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: BLEU provides a method to evaluate `candidate` translations produced by machine
    translation models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating machine translation with BLEU
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Papineni* et al. (2002) came up with an efficient way to evaluate a human
    translation. The human baseline was difficult to define. However, they realized
    that we could obtain efficient results if we compared human translation with machine
    translation, word for word.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '*Papineni* et al. (2002) named their method the **Bilingual Evaluation Understudy
    Score** (**BLEU**).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will use the **Natural Language Toolkit** (**NLTK**) to
    implement BLEU:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu](http://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: We will begin with geometric evaluations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Geometric evaluations
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The BLEU method compares the parts of a candidate sentence to a reference sentence
    or several reference sentences.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: BLEU 方法将候选句子的部分与参考句子或多个参考句子进行比较。
- en: Open `BLEU.py`, which is in the chapter directory of the GitHub repository of
    this book.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 打开这本书的 GitHub 存储库的章节目录中的`BLEU.py`。
- en: 'The program imports the `nltk` module:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 程序导入了`nltk`模块：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It then simulates a comparison between a candidate translation produced by the
    machine translation model and the actual translation(s) references in the dataset.
    Remember that a sentence could have been repeated several times and translated
    by different translators in different ways, making it challenging to find efficient
    evaluation strategies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它模拟了机器翻译模型生成的候选翻译与数据集中实际翻译参考之间的比较。 请记住，一句话可能已经多次重复，并且以不同的方式被不同的译者翻译，这使得找到有效的评估策略变得具有挑战性。
- en: 'The program can evaluate one or more references:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序可以评估一个或多个参考：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The score for both examples is `1`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个例子的得分都是`1`：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A straightforward evaluation *P* of the candidate *C*, the reference *R*, and
    the number of correct tokens found in *C (N)* can be represented as a geometric
    function:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 候选*C*、参考*R*以及在*C(N)*中找到的正确令牌数量的简单评估*P*可以表示为几何函数：
- en: '![](img/B17948_06_03.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17948_06_03.png)'
- en: 'This geometric approach is rigid if you are looking for a 3-gram overlap, for
    example:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找 3 克重叠，例如：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is severe if you are looking for 3-gram overlaps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在寻找 3 克重叠，则输出会很严重：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: A human can see that the score should be `1` and not `0.7`. The hyperparameters
    can be changed, but the approach remains rigid.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人可以看出得分应该是`1`而不是`0.7`。 超参数可以更改，但方法仍然保持不变。
- en: The warning in the code above is a good one that announces the next section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码中的警告是一个很好的警告，它预告了下一节的内容。
- en: The messages may vary with each version of the program and each run since this
    is a stochastic process.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个程序版本和每次运行的消息可能会有所不同，因为这是一个随机过程。
- en: '*Papineni* et al. (2002) came up with a modified unigram approach. The idea
    was to count the word occurrences in the reference sentence and ensure the word
    was not over-evaluated in the candidate sentence.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*Papineni*等人（2002年）提出了一种修改过的一元方法。 思路是统计参考句子中的词出现次数，并确保在候选句子中不会过度评估该词。'
- en: 'Consider the following example explained by *Papineni* et al. (2002):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑由*Papineni*等人（2002年）解释的以下示例：
- en: '`Reference 1: The cat is on the mat.`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`参考 1：地板上有一只猫。`'
- en: '`Reference 2: There is a cat on the mat.`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`参考 2：垫子上有一只猫。`'
- en: 'Now consider the following candidate sequence:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑以下候选序列：
- en: '`Candidate: the the the the the the the`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`候选：the the the the the the the`'
- en: We now look for the number of words in the candidate sentence (the `7` occurrences
    of the same word “the”) present in the `Reference 1` sentence (`2` occurrences
    of the word “the”).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来找出候选句子中的单词数（相同单词“the”的`7`次出现）在`参考 1`句子中（单词“the”的`2`次出现）的情况。
- en: A standard unigram precision would be `7/7`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的一元精确度将是`7/7`。
- en: The modified unigram precision is `2/7`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的一元精确度为`2/7`。
- en: Note that the BLEU function output warning agrees and suggests using smoothing.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，BLEU 函数的输出警告表示同意并建议使用平滑。
- en: Let’s add smoothing techniques to the BLEU toolkit.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将平滑技术添加到 BLEU 工具包中。
- en: Applying a smoothing technique
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用平滑技术
- en: '*Chen* and *Cherry* (2014) introduced a smoothing technique that improves standard
    BLEU techniques’ geometric evaluation approach.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*Chen*和*Cherry*（2014年）引入了一种平滑技术，可以改进标准 BLEU 技术的几何评估方法。'
- en: Label smoothing is a very efficient method that improves the performance of
    a transformer model during the training phase. It has a negative impact on perplexity.
    However, it forces the model to be more uncertain. In turn, this has a positive
    effect on accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 标签平滑是一种非常有效的方法，可以在训练阶段改进变压器模型的性能。 这对困惑度产生了负面影响。 然而，它迫使模型更加不确定。 这反过来对准确性产生积极影响。
- en: 'For example, suppose we have to predict what the masked word is in the following
    sequence:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们必须预测以下序列中的掩码单词是什么：
- en: '`The cat [mask] milk`.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`猫[mask]了牛奶。`'
- en: 'Imagine the output comes out as a softmax vector:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下输出以 softmax 向量的形式出现：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This would be a brutal approach. Label smoothing can make the system more open-minded
    by introducing epsilon = ![](img/B17948_06_001.png).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个残酷的方法。 标签平滑可以通过引入 epsilon = ![](img/B17948_06_001.png) 使系统更开放。
- en: The number of elements of `candidate_softmax` is *k=4*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`candidate_softmax`的元素数量为*k=4*。'
- en: For label smoothing, we can set ![](img/B17948_06_001.png) to `0.25`, for example.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: One of the several approaches to label smoothing can be a straightforward function.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: First, reduce the value of `candidate_one_hot` by ![](img/B17948_06_003.png).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Increase the `0` values by ![](img/B17948_06_004.png).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'We obtain the following result if we apply this approach:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '`candidate_smoothed=[0.75,0.083,0.083,0.083]`, making the output open to future
    transformations and changes.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: The transformer uses variants of label smoothing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: A variant of BLEU is chencherry smoothing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Chencherry smoothing
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Chen* and *Cherry* (2014) introduced an interesting way of smoothing candidate
    evaluations by adding ![](img/B17948_06_001.png) to otherwise `0` values. There
    are several chencherry (Boxing Chen + Colin Cherry) methods: [https://www.nltk.org/api/nltk.translate.html](https://www.nltk.org/api/nltk.translate.html).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first evaluate a French-English example with smoothing:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Although a human could accept the candidate, the output score is weak:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let’s add some openminded smoothing to the evaluation:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The score does not reach human acceptability:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have now seen how a dataset is preprocessed and how BLEU evaluates machine
    translations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Translation with Google Translate
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google Translate, [https://translate.google.com/](https://translate.google.com/),
    provides a ready-to-use interface for translations. Google is progressively introducing
    a transformer encoder into its translation algorithms. In the following section,
    we will implement a transformer model for a translation task with Google Trax.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: However, an AI specialist may not be required at all.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'If we enter the sentence analyzed in the previous section in Google Translate,
    `Levez-vous svp pour cette minute de silence`, we obtain an English translation
    in real time:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_06_02.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Google Translate'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The translation is correct.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Does Industry 4.0 still require AI specialists for translation tasks or simply
    a web interface developer?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Google provides every service required for translations on their Google Translate
    platform: [https://cloud.google.com/translate](https://cloud.google.com/translate):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'A translation API: A web developer can create an interface for a customer'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A media translation API that can translate your streaming content
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AutoML translation service that will train a custom model for a specific
    domain
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Google translate project requires a web developer for the interfaces, a **Subject
    Matter Expert** (**SME**), and perhaps a linguist. However, an AI specialist is
    not a prerequisite.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Industry 4.0 is going toward AI as a service. So why bother studying AI development
    with transformers? There are two important reasons to become an Industry 4.0 AI
    specialist:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: In real life, AI projects often run into unexpected problems. For example, Google
    Translate might not fit a specific need no matter how much goodwill was put into
    the project. In that case, Google Trax will come in handy!
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To use Google Trax for AI, you need to be an AI developer!
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用谷歌Trax进行AI，您需要是一个AI开发者！
- en: You never know! The Fourth Industrial Revolution is connecting everything to
    everything. Some AI projects might run smoothly, and some will require AI expertise
    to solve complex problems. For example, in *Chapter 14*, *Interpreting Black Box
    Transformer Models*, we will show how AI development is sometimes required to
    implement Google Translate.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你永远不知道！第四次工业革命正在将一切与一切连接起来。一些AI项目可能会顺利运行，而一些则需要AI专业知识来解决复杂问题。例如，在*第14章*中，*解释黑匣子变压器模型*，我们将展示有时需要AI开发来实现Google翻译。
- en: We are now ready to implement translations with Trax.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好使用Trax进行翻译。
- en: Translations with Trax
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Trax进行翻译
- en: Google Brain developed **Tensor2Tensor** (**T2T**) to make deep learning development
    easier. T2T is an extension of TensorFlow and contains a library of deep learning
    models that contains many transformer examploes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌大脑开发了**Tensor2Tensor**（**T2T**）来使深度学习开发更容易。T2T是TensorFlow的一个扩展，包含了一个深度学习模型库，其中包含许多变压器示例。
- en: Although T2T was a good start, Google Brain then produced Trax, an end-to-end
    deep learning library. Trax contains a transformer model that can be applied to
    translations. The Google Brain team presently maintains Trax.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然T2T是一个很好的开始，但谷歌大脑随后推出了Trax，一个端到端的深度学习库。Trax包含一个可以应用于翻译的变压器模型。谷歌大脑团队目前负责维护Trax。
- en: This section will focus on the minimum functions to initialize the English-German
    problem described by *Vaswani* et al. (2017) to illustrate the Transformer’s performance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分将重点介绍初始化由*瓦斯瓦尼*等人（2017年）描述的英德问题所需的最小功能，以说明变压器的性能。
- en: We will be using preprocessed English and German datasets to show that the Transformer
    architecture is language-agnostic.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用预处理的英语和德语数据集来表明变压器架构是语言无关的。
- en: Open `Trax_Translation.ipynb`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`Trax_Translation.ipynb`。
- en: We will begin by installing the modules we need.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始安装我们需要的模块。
- en: Installing Trax
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Trax
- en: 'Google Brain has made Trax easy to install and run. We will import the basics
    along with Trax, which can be installed in one line:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌大脑已经使Trax易于安装和运行。我们将导入基础知识和Trax，可以在一行中安装：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Yes, it’s that simple!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，就是这么简单！
- en: Now, let’s create our transformer model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的变压器模型。
- en: Creating the original Transformer model
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建原始的变压器模型
- en: We will create the original Transformer model as described in *Chapter 2*, *Getting
    Started with the Architecture of the Transformer Model*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建原始的变压器模型，就像*第2章*，*开始使用变压器模型架构*中描述的那样。
- en: 'Our Trax function will retrieve a pretrained model configuration in a few lines
    of code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Trax函数将在几行代码中检索预训练模型配置：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The model is the Transformer with an encoder and decoder stack. Each stack contains
    `6` layers and `8` heads. `d_model=512`, as in the architecture of the original
    Transformer.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是一个具有编码器和解码器堆栈的变压器。每个堆栈包含`6`层和`8`个头。`d_model=512`，与原始变压器的架构相同。
- en: The Transformer requires the pretrained weights to run.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器需要预训练的权重才能运行。
- en: Initializing the model using pretrained weights
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练权重初始化模型
- en: The pretrained weights contain the intelligence of the Transformer. The weights
    constitute the Transformer’s representation of language. The weights can be expressed
    as a number of parameters that will produce some form of *machine intelligence
    IQ*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练权重包含变压器的智能。权重构成了变压器对语言的表示。权重可以表示为将产生某种形式的*机器智能IQ*的参数数量。
- en: 'Let’s give life to the model by initializing the weights:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过初始化权重来给模型赋予生命：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The machine configuration and its *intelligence* are now ready to run. Let’s
    tokenize a sentence.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 机器配置及其*智能*现已准备就绪。让我们对一个句子进行分词。
- en: Tokenizing a sentence
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对句子进行分词
- en: Our machine translator is ready to tokenize a sentence. The notebook uses the
    vocabulary preprocessed by `Trax`. The preprocessing method is similar to the
    one described in this chapter’s *Preprocessing a WMT dataset* section.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的机器翻译器已准备好对句子进行分词。笔记本使用由`Trax`预处理的词汇表。预处理方法类似于本章中描述的*预处理WMT数据集*部分。
- en: 'The sentence will now be tokenized:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个句子将被分词：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The program will now decode the sentence and produce a translation.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将解码句子并产生一个翻译。
- en: Decoding from the Transformer
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从变压器解码
- en: The Transformer encodes the sentence in English and will decode it in German.
    The model and its weights constitute its set of abilities.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器将句子编码为英语，然后将其解码为德语。模型及其权重构成了其能力集。
- en: '`Trax` has made the decoding function intuitive to use:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`Trax`已经使解码函数变得直观易用：'
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that higher temperatures will produce diverse results, just as with human
    translators, as explained in this chapter’s *Defining machine translation* section.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，较高的温度会产生多样化的结果，就像人类翻译者一样，在本章的*定义机器翻译*部分有详细解释。
- en: Finally, the program will de-tokenize and display the translation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，程序将解密并显示翻译。
- en: De-tokenizing and displaying the translation
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解密并显示翻译
- en: Google Brain has produced a mainstream, disruptive, and intuitive implementation
    of the Transformer with `Trax`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Google Brain用`Trax`开发了一个主流、颠覆性和直观的Transformer实现。
- en: 'The program now de-tokenizes and displays the translation in a few lines:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在将解密并显示翻译的结果：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is quite impressive:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 输出相当令人印象深刻：
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The Transformer translated `machine intelligence` into `Maschinenübersicht`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器将`machine intelligence`翻译为`Maschinenübersicht`。
- en: 'If we deconstruct `Maschinenübersicht` into `Maschin (machine)` + `übersicht
    (intelligence)`, we can see that:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`Maschinenübersicht`拆分为`Maschin（机器）`+ `übersicht（智能）`，我们可以看到：
- en: '`über` literally means “over”'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`über`的字面意思是“over”'
- en: '`sicht` means “sight” or “view”'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sicht`的意思是“视野”或“视图”'
- en: The transformer tells us that although it is a machine, it has vision. Machine
    intelligence is growing through Transformers, but it is not human intelligence.
    Machines learn languages with an intelligence of their own.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器告诉我们，尽管它是一台机器，它能够有视觉。机器智能通过Transformer不断增长，但它不是人类智能。机器用自身的智能学习语言。
- en: That concludes our experiment with Google Trax.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对Google Trax的实验。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through three additional essential aspects of the original
    Transformer.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了原始Transformer的三个重要方面。
- en: We started by defining machine translation. Human translation sets an extremely
    high baseline for machines to reach. We saw that English-French and English-German
    translations imply numerous problems to solve. The transformer tackled these problems
    and set state-of-the-art BLEU records to beat.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义了机器翻译。人工翻译为机器设立了一个极高的基准。我们看到，英语-法语和英语-德语翻译意味着需要解决大量问题。Transformer解决了这些问题，并创造了最先进的BLEU记录。
- en: We then preprocessed a WMT French-English dataset from the European Parliament
    that required cleaning. We had to transform the datasets into lines and clean
    the data up. Once that was done, we reduced the dataset’s size by suppressing
    words that occurred below a frequency threshold.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对来自欧洲议会的WMT法语-英语数据集进行了预处理，并进行了数据清理。我们必须将数据集转换成行并清理数据。做完这些之后，我们通过消除低于频率阈值的单词来减小数据集的大小。
- en: Machine translation NLP models require identical evaluation methods. Training
    a model on a WMT dataset requires BLEU evaluations. We saw that geometric assessments
    are a good basis for scoring translations, but even modified BLEU has its limits.
    We thus added a smoothing technique to enhance BLEU.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译NLP模型需要相同的评估方法。在WMT数据集上训练模型需要进行BLEU评估。我们看到了几何评估是打分翻译的一个很好的基础，但即使是修改后的BLEU也有其局限性。因此，我们添加了一种平滑技术来增强BLEU。
- en: We saw that Google Translate provides a standard translation API, a media streaming
    API, and custom AutoML model training services. Implementing Google Translate
    APIs may require no AI development if the project rolls out smoothly. If not,
    we will have to get our hands dirty, like in the old days!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，Google翻译提供了标准的翻译API、媒体流API和自定义的自动机器学习模型训练服务。如果项目顺利进行，实施Google翻译API可能不需要进行AI开发。如果不顺利，我们将不得不像从前一样动手做！
- en: We implemented an English-to-German translation transformer with Trax, Google
    Brain’s end-to-end deep learning library.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Trax，Google Brain的端到端深度学习库，实现了英语到德语的翻译转换器。
- en: 'We have now covered the main building blocks to construct transformers: architecture,
    pretraining, training, preprocessing datasets, and evaluation methods.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了构建转换器的主要构件：架构、预训练、训练、数据预处理和评估方法。
- en: In the next chapter, *The Rise of Suprahuman Transformers with GPT-3 Engines*,
    we will discover mind-blowing ways of implementing transformers with the building
    blocks we explored in the previous chapters.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，*GPT-3引擎崛起的超人类Transformer*中，我们将探索使用我们在前几章中探讨的构件实现Transformer的令人惊叹的方式。
- en: Questions
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Machine translation has now exceeded human baselines. (True/False)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译现在已经超过了人类基准。（是/否）
- en: Machine translation requires large datasets. (True/False)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译需要大规模的数据集。（是/否）
- en: There is no need to compare transformer models using the same datasets. (True/False)
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BLEU is the French word for *blue* and is the acronym of an NLP metric (True/False)
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Smoothing techniques enhance BERT. (True/False)
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: German-English is the same as English-German for machine translation. (True/False)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original Transformer multi-head attention sub-layer has 2 heads. (True/False)
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original Transformer encoder has 6 layers. (True/False)
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The original Transformer encoder has 6 layers but only 2 decoder layers. (True/False)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can train transformers without decoders. (True/False)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'English-German BLEU scores with reference papers and code: [https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-german)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The 2014 **Workshop on Machine Translation** (**WMT**): [https://www.statmt.org/wmt14/translation-task.html](https://www.statmt.org/wmt14/translation-task.html)'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*European Parliament Proceedings Parallel Corpus 1996-2011*, parallel corpus
    French-English: [https://www.statmt.org/europarl/v7/fr-en.tgz](https://www.statmt.org/europarl/v7/fr-en.tgz)'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jason Brownlee*, *Ph.D*., *How to Prepare a French-to-English Dataset for
    Machine Translation*: [https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/](https://machinelearningmastery.com/prepare-french-english-dataset-machine-translation/)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kishore Papineni*, *Salim Roukos*, *Todd Ward*, and *Wei-Jing Zhu*, *2002*,
    *BLEU: a Method for Automatic Evaluation of Machine Translation*: [https://aclanthology.org/P02-1040.pdf](https://aclanthology.org/P02-1040.pdf)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jason Brownlee*, *Ph.D*., *A Gentle Introduction to Calculating the BLEU Score
    for Text in Python*: [https://machinelearningmastery.com/calculate-bleu-score-for-text-python/](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Boxing Chen* and *Colin Cherry*, 2014, *A Systematic Comparison of Smoothing
    Techniques for Sentence-Level BLEU*: [http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, *Jakob Uszkoreit*, *Llion
    Jones*, *Aidan N. Gomez*, *Lukasz Kaiser*, and *Illia Polosukhin*, 2017, *Attention
    Is All You Need*: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trax repository: [https://github.com/google/trax](https://github.com/google/trax)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trax tutorial: [https://trax-ml.readthedocs.io/en/latest/](https://trax-ml.readthedocs.io/en/latest/)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
