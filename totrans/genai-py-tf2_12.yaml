- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: 'Play Video Games with Generative AI: GAIL'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用生成式人工智能玩视频游戏：GAIL
- en: In the preceding chapters, we have seen how we can use generative AI to produce both
    simple (restricted Boltzmann machines) and sophisticated (variational autoencoders,
    generative adversarial models) images, musical notes (MuseGAN), and novel text
    (BERT, GPT-3).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到如何使用生成式人工智能来生成简单的（受限玻尔兹曼机器）和复杂的（变分自动编码器，生成式对抗模型）图像，音符（MuseGAN）和新颖文本（BERT，GPT-3）。
- en: In all these prior examples, we have focused on generating complex data using
    deep neural networks. However, neural networks can also be used to learn *rules*
    for how an entity (such as a video game character or a vehicle) should respond
    to an environment to optimize a reward; as we will describe in this chapter, this
    field is known as **reinforcement learning** (**RL**). While RL is not intrinsically
    tied to either deep learning or generative AI, the union of these fields has created
    a powerful set of techniques for optimizing complex behavioral functions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有先前的示例中，我们着重于使用深度神经网络生成复杂数据。然而，神经网络也可以用于学习实体（如视频游戏角色或车辆）如何响应环境以优化奖励的*规则*；正如我们将在本章描述的那样，这个领域被称为**强化学习**（**RL**）。虽然
    RL 并不是与深度学习或生成式人工智能有内在联系，但这些领域的结合已经创造出了一套优化复杂行为函数的强大技术。
- en: 'In this chapter, we will show you how to apply GANs to learn optimal policies
    for different figures to navigate within the OpenAI simulation environment. To
    understand the powerful combination of these methods with traditional approaches
    in RL, we will first review the more general problem that RL is trying to solve:
    how to determine the right *action* for an entity given a *state*, yielding a
    new *state* and a *reward*. The rules that optimize such rewards are known as
    a *policy*. We will discuss the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您展示如何将 GANs 应用于学习不同角色在 OpenAI 模拟环境中导航的最佳策略。为了理解这些方法与 RL 传统方法的强大组合，我们将首先回顾
    RL 要解决的更一般问题：如何确定给定 *状态* 的实体的正确 *动作*，产生一个新的 *状态* 和一个 *奖励*。优化这些奖励的规则称为 *策略*。我们将讨论以下主题：
- en: How deep neural networks were used to learn complex policies for high dimensional
    data such as the raw pixels from Atari video games.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络如何用于学习高维数据（如Atari视频游戏的原始像素）的复杂策略。
- en: The problem of **inverse reinforcement learning** (**IRL**) – how to learn the
    reward function from observing examples of the policy as given by an "expert"
    agent that makes optimal decisions – this kind of algorithm, as we will describe
    in more detail, is therefore also known as *imitation learning*.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆强化学习**（**IRL**）的问题 – 如何从观察“专家”智能体给出的策略示例中学习奖励函数 – 这种算法因此也被称为*模仿学习*，我们将更详细地描述。'
- en: How we can use the GAN training function to distinguish between expert and non-expert
    behavior (just as we distinguished between simulated and natural data in prior
    examples) to optimize a reward function.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用 GAN 训练函数来区分专家和非专家行为（就像我们在以前的示例中区分模拟数据和自然数据）来优化奖励函数。
- en: 'Reinforcement learning: Actions, agents, spaces, policies, and rewards'
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习：动作，智能体，空间，策略和奖励
- en: 'Recall from *Chapter 1*, *An Introduction to Generative AI: "Drawing" Data
    from Models*, that most *discriminative AI* examples involve applying a continuous
    or discrete label to a piece of data. In the image examples we have discussed
    in this book, this could be applying a deep neural network to determine the digit
    represented by one of the MNIST images, or whether a CIFAR-10 image contains a
    horse. In these cases, the model produces a single output, a prediction with minimal
    error. In reinforcement learning, we also want to make such point predictions,
    but over many steps, and to optimize the total error over repeated uses.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *第1章* 《生成式人工智能导论：“从模型中“绘制”数据》》中回忆起，多数*判别式人工智能*示例都涉及将连续或离散标签应用于一条数据。在本书中我们讨论的图像示例中，这可以是应用深度神经网络来确定MNIST图像代表的数字，或者CIFAR-10图像是否包含一匹马。在这些情况下，模型产生了一个单一输出，一个最小误差的预测。在强化学习中，我们也希望进行这种点预测，但要经过多个步骤，并且在重复使用中优化总误差。
- en: '![](img/B16176_12_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_01.png)'
- en: 'Figure 12.1: Atari video game examples¹'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：Atari视频游戏示例¹
- en: As a concrete example, consider a video game with a player controlling a spaceship
    to shoot down alien vessels. The spaceship navigated by the player in this example
    is the *agent*; the set of pixels on the screen at any point in the game is the
    *state*. Based on this environment, the player needs to take the right *action*
    (for example, moving right, pressing fire) which will maximize the *reward* –
    here quite literally the score for the game. It is not just the next immediate
    action which we need to consider though, but all actions until the end of the
    game, since we can accumulate additional points as long as we do not run out of
    lives. Expert video game players learn how to react in different situations, a
    *policy* to follow when confronted with diverse scenarios during gameplay. The
    problem of RL is to determine a machine learning algorithm that can replicate
    the behavior of such a human expert, by taking a set of inputs (the current state
    of the game) and outputting the optimal action to increase the probability of winning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个具体的例子来说明，考虑一个玩家控制太空船击落外星飞船的视频游戏。在这个例子中，玩家操纵的太空船是*代理*；游戏中任意时间点屏幕上的像素集合就是*状态*。基于这个环境，玩家需要做出正确的*动作*（例如向右移动，按下开火），这将最大化*奖励*
    – 这里确实是游戏的得分。我们不只需要考虑接下来的即时动作，而是所有动作直到游戏结束，因为只要我们不用完生命值，我们就可以累积额外的分数。专业的视频游戏玩家学会如何在不同情况下做出反应，一个*策略*，用以面对游戏过程中不同的情况。RL的问题是确定一个机器学习算法，能够通过接收一组输入（游戏的当前状态）并输出最优动作来增加胜率，来复制这样一个人类专家的行为。
- en: To formalize this description with some mathematical notation, we can denote
    the *environment*, such as the video game, in which an agent acts as ![](img/B16176_12_001.png),
    which outputs a set of data (pixels) at each point in time as the *state*, *x*[t].
    For each point of time in the game, the player (algorithm) selects an *action*,
    *a*[t], from a set of *n* actions *A* = {1, 2, …. *N*}; this is also known as
    the "action set" or "action space." While for clarity we limit our discussion
    in the chapter to discrete action spaces, there is theoretically no such restriction,
    and the theory works just as well for continuous actions (though the resulting
    RL problem is consequently more complex with an infinite space of possibilities).
    For each of these actions, the agent receives a reward, *r*[t], that can change
    the game score.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用数学符号正式描述这一情况，我们可以将*环境*，比如视频游戏，在其中代理作为 ![](img/B16176_12_001.png) 来表示，这个代理在每个时间点输出一组数据（像素）作为*状态*
    *x*[t]。在游戏的每个时间点，玩家（算法）从*动作*集合中选择 *a*[t]，该集合包括*n*个动作 *A* = {1, 2, …. *N*}；这也被称为“动作集”或“动作空间”。虽然为了清晰起见，我们在本章中限制了对离散动作空间的讨论，但理论上并没有这样的限制，这个理论同样适用于连续动作（尽管由此产生的RL问题会更加复杂，因为有无穷无尽的可能性）。对于这些动作中的每一个，代理会获得奖励
    *r*[t]，这可能会改变游戏得分。
- en: If we were to only consider the current screen, *x*[t], as the "state" of the
    system, then our decision relies only on the present, and the RL problem becomes
    a **Markov Decision Process** (**MDP**), as the choice for the next action depends
    only on immediately available data and not on history (*Figure 12.2*). However,
    for the video game example given above, the current screen only is probably not
    enough information to determine the optimal action, because it is only *partially
    observable* – we don't know cases where an enemy starcraft may have moved off
    the screen (and thus where it might re-emerge). We also don't know what direction
    our ship is moving without comparing to prior examples, which might affect whether
    we need to change direction or not. If the current state of the environment contained
    all the information we need to know about the game – such as a game of cards in
    which all players show their hands – then we say that the environment is *fully
    observable*.²
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只考虑当前屏幕 *x*[t] 作为系统的“状态”，那么我们的决策只依赖于当前，并且RL问题就成为了**马尔可夫决策过程**（**MDP**），因为下一步动作的选择只依赖于即时可用的数据，而不依赖于历史（*图12.2*）。然而，在上述给定的视频游戏例子中，仅仅考虑当前屏幕可能不足以确定最优动作，因为它只是*局部可观测*
    – 我们不知道敌方星际飞船是否已经移出屏幕（因此它可能会重新出现）。我们也不知道我们的飞船移动的方向，而不经过比较之前的例子我们就无法得知，这可能会影响我们是否需要改变方向。如果环境的当前状态包含了我们需要知道的游戏的所有信息
    – 比如一场扑克游戏，所有玩家都展示自己的牌 – 那么我们就说这个环境是*完全可观测*的。²
- en: '![](img/B16176_12_02.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_02.png)'
- en: 'Figure 12.2: A Markov Decision Process (MDP)³. The transition (black arrows)
    between states (green circles) via actions with certain probabilities (orange
    circles) yields rewards (orange arrows).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：马尔可夫决策过程（MDP）³。通过具有一定概率的行动（橙色圆圈）之间的状态（绿色圆圈）的转移（黑色箭头）得到奖励（橙色箭头）。
- en: Indeed, a human video game player does not rely only on the immediate state
    of the game to determine what to do next; they also rely on cues from prior points
    in the game, such as the point at which enemies went offscreen to anticipate them
    re-emerging.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，人类的视频游戏玩家并不仅仅依赖游戏的当前状态来确定下一步该做什么；他们还依赖游戏之前的提示，比如敌人离开屏幕的时间点，以便预料它们再次出现。
- en: Similarly, our algorithm will benefit from using a *sequence* of states and
    actions leading to the current state, *s* = {*x*[1]…, *x*[t]; *a*[1]...*a*[t-1]},
    from the game as input to the current decision. In other words, the "state" becomes
    this sequence of prior states and actions, and we can still use the methods developed
    for MDPs to solve this RL problem.¹
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们的算法将受益于使用导致当前状态*s*={*x*[1]…，*x*[t];*a*[1]...*a*[t-1]}的一系列状态和行动作为当前决策的输入。换句话说，“状态”变成了以前状态和行动的这个序列，并且我们仍然可以使用为MDP开发的方法来解决这个RL问题。
- en: 'At each point in time, based on this state (history), we want to make the decision
    that will maximize *future reward*, *R*, at the end of the game. Intuitively,
    we are usually better at estimating the outcome of our immediate actions versus
    their effect far in the future, so we apply a discount to estimating the impact
    of actions taken near-term versus long-term using a *discount* term, ![](img/B16176_12_002.png),
    between 0 and 1 in our computation of expected reward:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间点，基于这个状态（历史），我们希望做出决定，以最大化游戏结束时的*未来奖励* *R*。直觉上，我们通常更擅长估计我们即将采取的行动的结果，而不是它们对未来的长期影响，因此我们需要使用一个*贴现*项，![](img/B16176_12_002.png)，在计算预期奖励时对近期和长期采取行动的影响进行估计：
- en: '![](img/B16176_12_003.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_003.png)'
- en: 'Where *t''* is a timepoint between the current timepoint, *t*, and the end
    of the game, *T*. We can see that there are three potential interpretations of
    this future rewards function based on the value of ![](img/B16176_12_004.png):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*t'*是当前时间点*t*和游戏结束*T*之间的时间点。我们可以看到基于![](img/B16176_12_004.png)的值，未来奖励函数有三种潜在的解释：
- en: '![](img/B16176_12_005.png). Future rewards have no impact on our decision,
    and we care only about the current rewards.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B16176_12_005.png). 未来的奖励对我们的决策没有影响，我们只关心当前的奖励。'
- en: '![](img/B16176_12_006.png). As we increase the distance between *t''* and *t*,
    the exponent of this multiplicative factor becomes greater and the discount term
    smaller, shrinking the future reward to 0 at infinity. In this case, we weight
    nearer-term goals more highly than longer-term goals.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B16176_12_006.png). 随着*t''*和*t*之间的距离增加，这个乘法因子的指数会变得更大，贴现项变得更小，将未来奖励缩小到无限远处为0。在这种情况下，我们更加重视近期目标而不是远期目标。'
- en: '![](img/B16176_12_007.png). In this case, our environment could be deterministic,
    such that we don''t need to discount future reward because there is no uncertainty
    about outcomes, but even in stochastic environments the use of ![](img/B16176_12_008.png)
    or ![](img/B16176_12_009.png) can be seen as a choice between regularizing the
    reward calculation (or not). When the discount factor is < 1, the algorithm is
    less affected by (potentially sparse) data over a longer horizon, so can be helpful
    in cases where the action space is very large and training on many future steps
    without this discounting could lead to overfitting on individual paths in that
    space.⁴'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B16176_12_007.png). 在这种情况下，我们的环境可能是确定性的，因此我们不需要贴现未来的奖励，因为结果没有不确定性，但即使在随机环境中，使用![](img/B16176_12_008.png)或![](img/B16176_12_009.png)可以看作是在奖励计算之间进行正则化的选择（或者选择不进行正则化）。当贴现因子<1时，该算法受到（潜在稀疏的）长期数据的影响较小，因此可以帮助解决行动空间非常大且在没有进行贴现的情况下训练很多未来步骤可能导致在该空间中单个路径上过拟合的情况。'
- en: 'The goal of the RL problem we have described here is then to learn a *value
    function*, Q, which maximizes reward given a sequence and a *policy*, ![](img/B16176_12_010.png),
    that associates sequences to actions – the convention of writing this as the "Q"
    function comes from the fact that the function evaluates the "quality" of decisions
    made by the RL algorithm⁵:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里描述的RL问题的目标是学习一个*值函数*Q，它在给定一个序列和一个*策略*![](img/B16176_12_010.png)的情况下最大化奖励-将这写成"Q"函数的约定来自于这个函数评估RL算法做出的决策的“质量”。
- en: '![](img/B16176_12_011.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_011.png)'
- en: 'In other words, given a state (or, as in our case, a sequence of states) and
    a proposed action, the Q-function scores the proposed action based on the maximum
    total future reward R. This Q-function is generally not known, and we need to
    solve for it – however, the optimal Q-function obeys an important rule based on
    *dynamic programming* called the *principle of optimality*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，给定一个状态（或者，在我们的情况下，一系列状态）和一个提议的动作，Q函数根据最大的总未来奖励R对提议的动作进行评分。这个Q函数通常是未知的，我们需要解决它
    - 然而，最优的Q函数遵循一条基于*动态规划*的重要规则，称为*最优性原则*：
- en: '"An optimal policy has the property that whatever the initial state and initial
    decision are, the remaining decisions must constitute an optimal policy with regard to
    the state resulting from the first decision."⁶'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “最优策略具有这样的性质：无论初始状态和初始决策是什么，剩余的决策必须构成一个对于第一个决策产生的状态而言的最优策略。”⁶
- en: 'In others words, regardless of the starting state, once we transition to a
    new state as a result of the first action, all subsequent actions must be optimal
    under an optimal policy. If we write out this principle in mathematical form,
    it splits the value function into two parts in a recursive expression known as
    the *Bellman equation*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，无论起始状态如何，一旦我们由于第一个动作而转移到新状态，所有后续动作都必须在最优策略下是最优的。如果我们将这个原则以数学形式写出，它将值函数分成两部分，形成一个递归表达式，称为*贝尔曼方程*：
- en: '![](img/B16176_12_012.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_012.png)'
- en: 'Where *Q** is the "optimal" Q-function, *E* is the expected value, and ![](img/B16176_12_013.png)
    is an environment from which future states *s''* are sampled. This expression
    says that the optimal Q-function should give the expected value for the current
    sequence and proposed action as the expectation of the sum of current reward r
    and the discounted future value of the next sequence of actions, *Q**(*s''*, *a''*).
    This expression is also known as a functional expression because the solution
    is the function *Q**, and thus this general class of problems is known as "Q-learning."
    ⁷ One approach to solving for *Q** in Q-learning is through *value iteration*:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Q** 是“最优”的Q函数，*E* 是期望值，而 ![](img/B16176_12_013.png) 是从中采样未来状态 *s'* 的环境。这个表达式表示最优的Q函数应该给出当前序列和提议动作的期望值，作为当前奖励
    *r* 和下一系列动作的折现未来值 *Q**(*s'*, *a'*) 之和的期望值。这个表达式也被称为函数表达式，因为解决方案是函数 *Q**，因此这个一般类问题称为“Q学习”。⁷
    在Q学习中解决 *Q** 的一种方法是通过*值迭代*：
- en: '![](img/B16176_12_014.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_014.png)'
- en: 'Where ![](img/B16176_12_015.png) is a *learning rate* that determines how quickly
    to modify the function with new information. If we run this update for a sufficiently
    large number of *i* and *s*, we could converge to the optimal value for *Q*, *Q**.
    ^(8, 9) For simpler problems where we have only a small number of actions and
    states, we could create a table that gives the value of *Q* for any potential
    action. The entries in this table are randomly initialized for every *Q*(*s*,
    *a*) and updated using the value iteration formula:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B16176_12_015.png) 是一个*学习速率*，决定了如何快速地根据新信息修改函数。如果我们对足够大的*i*和*s*运行此更新，我们可以收敛到*Q*的最优值，*Q**。
    ^(8, 9) 对于简单问题，我们只有少量的动作和状态，我们可以创建一个表格，给出任何潜在动作的*Q*值。该表中的条目对于每个*Q*(*s*, *a*)都是随机初始化的，并使用值迭代公式进行更新：
- en: '| State/Action | A1 | A2 | A3 | A4 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 状态/动作 | A1 | A2 | A3 | A4 |'
- en: '| S1 | Q(S1, A1) |  |  |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| S1 | Q(S1, A1) |  |  |  |'
- en: '| S2 |  |  |  |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| S2 |  |  |  |  |'
- en: '| S3 |  |  |  |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| S3 |  |  |  |  |'
- en: 'Table 12.1: A lookup table for Q-learning. Each cell contains a Q value for
    a particular state (S) and action (A) pair. Values are randomly initialized and
    updated using value iteration.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 表12.1：Q学习的查找表。每个单元格包含特定状态（S）和动作（A）对的Q值。值是随机初始化的，并使用值迭代进行更新。
- en: However, as pointed out by prior authors,¹ for problems such as the video game
    scenario we have described, it is almost impossible to enumerate all potential
    sequences and actions. Also, because the likelihood of any particular sequence-action
    pair is very low, we would need to sample a huge number of examples to be able
    to accurately estimate the *Q* value using value iteration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如之前的作者所指出的¹，对于我们所描述的视频游戏场景这样的问题，几乎不可能枚举所有潜在的序列和动作。此外，由于任何特定序列-动作对的可能性非常低，我们需要采样大量示例才能准确地估计使用值迭代来估计
    *Q* 值。
- en: 'To make this problem more computationally feasible, we need to turn *Q* from
    a lookup table to a function that can *generalize* to estimate the value of state-action
    pairs that it hasn''t seen based on similar examples to which it has been exposed.
    To do this, *Q* can be represented by a parameterized function with some parameters
    ![](img/B16176_12_016.png): the function can be linear, a tree model, or in modern
    applications even the deep neural networks that we have been studying in other
    parts of this book. In this case, our objective resembles more a classical learning
    algorithm, where we try to minimize:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使这个问题更具有计算可行性，我们需要将 *Q* 从查找表转变为一个可以对它没有见过的状态-动作对的值进行推广估计的函数。为了做到这一点， *Q*
    可以用一些参数 ![](img/B16176_12_016.png) 表示：该函数可以是线性的，树模型的，或者在现代应用程序中甚至是我们在本书的其他部分中学习的深度神经网络。在这种情况下，我们的目标更像是一个经典的学习算法，我们尝试最小化:'
- en: '![](img/B16176_12_017.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_017.png)'
- en: With ![](img/B16176_12_018.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ![](img/B16176_12_018.png)
- en: 'and *p* is a distribution of actions and sequences from a potentially very
    large space performed by an agent in an environment ![](img/B16176_12_019.png).
    Since L is differentiable, we can optimize it using stochastic gradient descent,
    just like the deep learning algorithms we discussed in *Chapter 3*, *Building
    Blocks of Deep Neural Networks*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '而 *p* 是代理在环境中执行的潜在非常大空间的行动和序列的分布 ![](img/B16176_12_019.png)。由于 L 是可微分的，我们可以像
    *第 3 章* 中讨论过的深度学习算法一样使用随机梯度下降来优化它:'
- en: '![](img/B16176_12_020.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_020.png)'
- en: By holding the parameters fixed and optimizing *L*, followed by constructing
    new samples from *p* and *s*, we have an algorithm that resembles the iterative
    updates for the value iteration Q-learning as given above.^(10)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过固定参数并优化 *L*，然后从 *p* 和 *s* 构建新的样本，我们得到了一个类似于上面给出的值迭代 Q-学习的迭代更新算法。^(10)
- en: 'Where does this approach fit among the hierarchy of RL methods? Generally,
    RL methods are classified according to:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在RL方法的层次结构中属于哪个位置？一般来说，RL方法根据以下几点进行分类：
- en: Whether they use a static (offline) dataset or are trained on new data continuously
    delivered to the system (online).^(11)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否使用静态（离线）数据集，还是持续交付给系统的新数据进行训练（在线）。^(11)
- en: Whether the policy function is updated with *Q* (on-policy) or independently
    (off-policy).^(12)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略函数是否使用 *Q* 进行更新（on-policy）或独立更新（off-policy）。^(12)
- en: Whether the transition equations are explicitly modeled (model-based) or not
    (model-free).^(12)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过渡方程是否明确建模（基于模型）或不是（无模型）。^(12)
- en: Among these possible RL algorithm variants, the version of Q-learning described
    above is classified as *off-policy*, *online, and model-free*. While we could
    use the values of *Q* as a policy for selecting the next action, our samples from
    *p*(*.*) do not need to hold to this policy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有可能的 RL 算法变体中，上面描述的 Q-学习版本被分类为*off-policy*, *online, and model-free*。虽然我们可以使用
    *Q* 的值作为选择下一个行动的策略，我们从 *p*(*]*) 的样本不需要遵循这个策略。
- en: In practice, an epsilon greedy distribution is used for problems where we want
    to introduce some randomness into the algorithm to prevent it from getting stuck
    in local minima, such as deep Q-learning (introduced in a short while), which
    selects the "greedy" (maximizing Q) action with probability *e*, and a random
    action with probability *1-e*. Thus the policy we are learning (Q) is not strictly
    used to select actions (a) given this randomness. This approach is model-free
    because the neural network approximates the transition model, and is online because
    it is learned on a dynamically generated dataset, though could be trained using
    a static offline history of video game sessions as well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，一个 epsilon-贪婪分布通常用于想要为算法引入一些随机性以防止其陷入局部最小值的问题，比如深度 Q-学习（稍后介绍），它选择“贪婪”（最大化
    Q）的行动概率为 *e*，以及随机行动概率为 *1-e*。因此，我们正在学习的策略（Q）不严格用于选择具有这种随机性的行动（a）。这种方法是无模型的，因为神经网络近似了过渡模型，并且是在线的，因为它是在动态生成的数据集上学习的，尽管也可以使用静态离线视频游戏会话历史进行训练。
- en: In contrast, in *on-policy* Q-learning algorithms, such as **State-Action-Reward-State-Action**
    (**SARSA**) ^(13), we use direct updates for the Q-function as given for the value
    iteration steps described above. Unlike the off-policy example, we are not calculating
    the optimal *Q* based on a sample or exploratory selection of actions generated
    from the distribution *p* (sometimes known as the *behavioral policy*); instead,
    we are selecting actions based on a policy and potentially updating that policy
    as we learn Q.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在*on-policy*的 Q 学习算法中，比如**状态-动作-奖励-状态-动作**（**SARSA**）^(13)，我们使用直接更新 Q 函数的方法，就像上面描述的值迭代步骤那样。与离策略示例不同，我们不是根据从分布*p*（有时称为*行为策略*）生成的样本或探索性选择的动作来计算最优*Q*；相反，我们是根据策略选择动作，并可能在学习
    Q 的过程中更新该策略。
- en: Deep Q-learning
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度 Q 学习
- en: While the field of deep learning is independent of reinforcement learning methods
    such as the Q-learning algorithm, a powerful combination of these two approaches
    was applied in training algorithms to play arcade games at near-human level.¹
    A major insight in this research was to apply a deep neural network to generate
    vector representations from the raw pixels of the video game, rather than trying
    to explicitly represent some features of the "state of the game"; this neural
    network is the Q-function for this RL algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习领域独立于强化学习方法，如 Q 学习算法，但这两种方法的强大组合被应用于训练算法以接近人类水平地玩街机游戏。¹ 在这项研究中的一个重要见解是将深度神经网络应用于从视频游戏的原始像素生成向量表示，而不是试图显式地表示游戏状态的一些特征；这个神经网络是这个
    RL 算法的 Q 函数。
- en: Another key development was a technique called *experience replay*, wherein
    the history of states (here, pixels from video frames in a game), actions, and
    rewards is stored in a fixed-length list and re-sampled at random repeatedly,
    with some stochastic possibility to choose a non-optimal outcome using the epsilon-greedy
    approach described above. The result is that the value function updates are averaged
    over many samples of the same data, and correlations between consecutive samples
    (which could make the algorithm explore only a limited set of the solution space)
    are broken. Further, this "deep" Q-learning algorithm is implemented *off-policy*
    to avoid the potential circular feedback of generating optimal samples with a
    jointly optimized policy function.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键的发展是一种称为*经验回放*的技术，其中状态的历史（在这里，是游戏中的视频帧像素）、行动和奖励被存储在一个固定长度的列表中，并且反复随机重新采样，通过使用上述的ε-贪心方法来以一定的随机可能性选择一个非最优结果。结果是，对于相同数据的许多样本，值函数的更新被平均化，并且连续样本之间的相关性（可能使算法仅探索解空间的有限集合）被打破。此外，这种“深度”Q学习算法是*离策略*实现的，以避免使用联合优化策略函数生成最优样本的潜在循环反馈。
- en: Deep Q-learning is also *model-free*, in the sense we have no representation
    or model (such as a generative model that can simulate new frames of the game)
    of the environment *E*. In fact, as with the video game example, it could just
    be samples of historical data that represent the "internal state" of a game that
    is observed by a player.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 学习也是*无模型*的，即我们没有环境*E*的表示或模型（比如一个可以模拟游戏新帧的生成模型）。事实上，就像视频游戏的例子一样，它可能只是表示被玩家观察到的游戏的“内部状态”的历史数据样本。
- en: 'Putting these pieces together, the deep Q-learning algorithm uses the following
    steps to learn to play Atari games^(14):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些部分组合在一起，深度Q学习算法使用以下步骤来学习玩 Atari 游戏^(14)：
- en: Create a list to store samples of (current state, action, reward, next state)
    as a "replay memory."
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个列表，以存储（当前状态、动作、奖励、下一个状态）的样本作为“回放内存”。
- en: Randomly initialize the weights in the neural network representing the Q-function.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化表示 Q 函数的神经网络中的权重。
- en: For a certain number of gameplay sequences, initialize a starting game screen (pixels)
    and a transformation of this input (such as the last four screens). This "window"
    of fixed-length history is important because otherwise the Q-network would need
    to accommodate arbitrarily sized input (very long or very short sequences of game
    screens), and this restriction makes it easier to apply a convolutional neural
    network to the problem.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一定数量的游戏序列，初始化一个起始游戏屏幕（像素）和对这个输入的转换（比如最后四个屏幕）。这个固定长度历史的“窗口”很重要，因为否则 Q 网络将需要容纳任意大小的输入（非常长或非常短的游戏屏幕序列），这个限制使得将卷积神经网络应用于这个问题变得更容易。
- en: For a certain number of steps (screens) in the game, use epsilon greedy sampling
    to choose the next action given the current screen and reward function computed
    through Q.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After updating the state, save this transition of (current state, action, reward,
    action, next state) into the replay memory.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose random sets of (current state, action, reward, next state) transitions
    from the replay memory, and compute their reward using the Q-function. Use stochastic
    gradient descent to update Q based on these transitions.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue *steps 3-6* for many games and gameplay steps until the weights in the
    Q-network converge.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While other applications of Q-learning have nuances tied to their specific
    domain, the general approach of using a deep neural network to approximate the
    Q-function on a large space of possible outcomes (rather than a small set of states
    and actions that can be represented in a table) has proved effective in many cases.
    Other examples in which deep Q-learning has been applied include:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Processing the positions on a Go (an East Asian game resembling chess) gameboard
    using a CNN and applying Q-learning to determine the next best move in the game
    based on historical examples from human players; a model named "AlphaGo" was published
    in 2015.^(15)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An innovation on the AlphaGo release in 2017, named "AlphaGo Zero," where the
    program learns entirely from synthetic games (two RL agents playing each other)
    rather than historical examples from human players.^(16)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more general form of AlphaGo Zero, "AlphaZero," which also mastered the games
    of Chess and Shogi using self-play.^(17)
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlphaStar, an RL algorithm that can beat human masters in the multi-player real-time
    strategy game StarCraft.^(18)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model called "AlphaFold," which can predict the 3D structure of proteins from
    their 2D sequence – we will describe AlphaFold in more detail in *Chapter 13*,
    *Emerging Applications of Generative AI*.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've thus far described Q-learning and its variants, and how deep learning
    augments Q-learning through deep Q-learning for complex environments and datasets.
    However, the problems we've described all share the common feature that we are
    able to express the reward function in defined mathematical terms (such as the
    score in a game). In many real-world scenarios – such as training an RL agent
    to drive a car – this reward function is not so easy to define. In these cases,
    rather than writing down a reward function, we might instead use examples of human
    drivers as an implicit representation of the reward – this is an approach known
    as inverse reinforcement learning, which we describe in more detail below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Inverse reinforcement learning: Learning from experts'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example of deep Q-learning above relies upon an explicit reward function
    –the score in the game. We don't always have access to an explicit reward function
    though, including in important real-world scenarios such as self-driving cars.
    What "reward" value would we assign to a driver choosing to navigate one way or
    the other given the surrounding environment on the road? While we have an intuitive
    sense of what the "right" decision is, quantifying exhaustive rules for how we
    should score such a reward function would be challenging.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的深度 Q 学习示例依赖于一个明确的奖励函数——游戏中的分数。然而，我们并不总是能够访问明确的奖励函数，包括在重要的现实场景中，比如自动驾驶汽车。在驾驶员根据道路上的环境选择导航的情况下，我们会给予什么“奖励”值呢？虽然我们对“正确”决定有直观的感觉，但为这样的奖励函数量化详尽的规则将是具有挑战性的。
- en: 'Instead of trying to determine the reward function, we could instead observe
    an expert driver perform complex maneuvers such as merging in multi-lane traffic
    and optimize an agent whose behavior mimics that of the expert. This is a more
    general problem known as *imitation learning*. One form of imitation learning
    is *behavioral cloning*,^(19) which follows the following algorithm:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不必尝试确定奖励函数，而是观察专业驾驶员执行复杂的操作，比如在多车道交通中合并，并优化一个行为模仿专家的代理。这是一个更一般的问题，被称为*模仿学习*。模仿学习的一种形式是*行为克隆*，^(19)它遵循以下算法：
- en: Collect a set of state, action pairs ![](img/B16176_12_066.png) from expert
    behavior.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从专家行为中收集一组状态、动作对！[](img/B16176_12_066.png)。
- en: Learn a policy function ![](img/B16176_12_021.png), where ![](img/B16176_12_022.png)
    is a supervised classification algorithm.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习一个策略函数！[](img/B16176_12_021.png)，其中![](img/B16176_12_022.png)是一种监督分类算法。
- en: While this approach is straightforward, it requires a large amount of data in
    order to create a classifier that generalizes to unseen environments, particularly
    when the number of potential environments (as is the case for self-driving cars)
    is very large.^(20)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法很直接，但需要大量数据才能创建一个能够推广到未知环境的分类器，特别是当潜在环境的数量很大时（例如自动驾驶汽车的情况）。^(20)
- en: In particular, the success of behavioral cloning depends on the future distribution
    of the environment being similar to the training data, which is particularly difficult
    when the results of the model can influence the distribution of later observations.
    For example, the choices made by a self-driving car on the road become further
    data for re-training the model, leading to a feedback loop of potentially compounding
    errors and data drift.^(21)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，行为克隆的成功取决于未来环境的分布与训练数据的相似性，当模型的结果能够影响后续观察结果的分布时，这一点尤其困难。例如，自动驾驶汽车在道路上的选择成为重新训练模型的进一步数据，导致潜在的复合错误和数据漂移的反馈循环。^(21)
- en: An alternative is to learn an agent that tries to match the outcomes of an expert's
    *entire trajectory* rather than individual actions, as in behavioral cloning.
    The output of this algorithm is then a function that scores "expert behavior"
    on a trajectory higher than novice behavior. This approach is known as **inverse
    reinforcement learning** (**IRL**) since it reverses the common pattern (*Figure
    12.3*) – on the left we see a typical feedback loop for an RL like we've described
    for the Atari-playing deep Q-network, where an agent (blue) observes a state (*s*)
    and using a reward function (*R*) chooses an action (*a*) that yields a transition
    (*T*) to a new state and a reward (*r*). In contrast, on the right, the rewards
    resulting from these states, actions, and transitions are represented implicitly
    by examples from an expert (*E*), and the agent (blue) instead learns to replicate
    this sequence through a learned reward function (*R*[E]) rather than being explicitly
    "in the loop" of the algorithm. In other words, instead of learning policy from
    an explicit reward function, we observe an expert's behavior and infer a reward
    function that would lead to their observed actions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是学习一个代理，试图匹配专家的*整个轨迹*的结果，而不是单个动作，就像行为克隆中一样。然后，该算法的输出是一个在轨迹上将"专家行为"评分高于新手行为的函数。这种方法被称为**逆强化学习**（**IRL**），因为它颠倒了常见的模式（*图12.3*）-在左侧，我们看到了一个典型的反馈循环，如我们为
    Atari 玩深度 Q 网络描述的那样，其中一个代理（蓝色）观察到一个状态（*s*）并使用奖励函数（*R*）选择一个动作（*a*）产生一个转移（*T*）到一个新状态和一个奖励（*r*）。相比之下，在右侧，从这些状态、动作和转移中产生的奖励隐含地由来自专家（*E*）的例子表示，代理（蓝色）学习复制这个序列通过一个学习到的奖励函数（*R*[E]），而不是显式地"在算法的循环中"。换句话说，我们不是从显式奖励函数中学习策略，而是观察专家的行为并推断出一个会导致他们观察到的动作的奖励函数。
- en: '![](img/B16176_12_03.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_03.png)'
- en: 'Figure 12.3: Reinforcement learning (a) and inverse reinforcement learning
    (b)^(22)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：强化学习（a）和逆强化学习（b）^(22)
- en: 'How could we quantify the behavior of an "expert" through learning a reward
    function from scratch? If we re-examine our prior example of video game playing,
    we could examine sequences of pixel screens *x* and actions *a* (*x*[1], *a*[1],
    *x*[2], *a*[2]…) from expert human play that form a complete session, and try
    to find a function *f* that would give us the total reward for a given game:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如何通过从头开始学习奖励函数来量化"专家"的行为呢？如果我们重新审视我们之前的视频游戏示例，我们可以检查专家人类游戏的像素屏幕 *x* 和动作 *a*（*x*[1]，*a*[1]，*x*[2]，*a*[2]...）序列，这些序列构成了一个完整的会话，并尝试找到一个函数
    *f*，该函数会为给定的游戏给出总奖励：
- en: '![](img/B16176_12_023.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_023.png)'
- en: We could then ask whether the given function *f* tends to replicate the behavior
    of an expert player versus other alternatives. However, there are inherent problems
    – multiple *f*'s might give the same reward result, making it unclear which (of
    many possible solutions) would generalize best to new data.^(23)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以询问给定的函数 *f* 是否倾向于复制专家玩家的行为，而不是其他替代方案。然而，存在固有的问题-多个 *f* 可能会给出相同的奖励结果，使得不清楚哪一个（可能性很多）最能泛化到新数据。^(23)
- en: 'Alternatively, we could instead try to optimize an agent which will result
    in observed trajectories with the same probability as an expert; in other words,
    we would see the same distribution of sequences from following this agent as from
    drawing randomly from expert behavior, and the optimization algorithm is based
    on minimizing the difference between the proposed and the observed empirical distribution
    of sequences from the expert.^(24) This expected distribution (either observed
    or generated by the agent) can be represented by:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以尝试优化一个代理，使其产生的观察轨迹与专家的概率相同；换句话说，我们从跟随该代理产生的序列中看到的分布与从专家行为中随机抽取的序列相同，而优化算法基于最小化提出的和从专家处观察到的序列的经验分布之间的差异。^(24)
    这个预期分布（无论是观察到的还是由代理生成的）可以用以下方式表示：
- en: '![](img/B16176_12_024.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_024.png)'
- en: Where *P* is the probability of trajectory (![](img/B16176_12_025.png)), and
    *f* are the features of a state, such as the observed pixels in the video game
    example. We've removed the problem of solving for an ambiguous reward function,
    but we still are faced with the possibility that many agents could lead to the
    same behavior. We might even need a mixture of different policy or reward functions
    in order to mimic a given expert behavior, and it is unclear how to select these.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里*P*是轨迹(![](img/B16176_12_025.png))的概率，*f*是状态的特征，例如视频游戏示例中的观察像素。我们已经解决了解决模糊奖励函数的问题，但我们仍然面临着许多代理可能导致相同行为的可能性。我们甚至可能需要混合不同的策略或奖励函数来模拟给定的专家行为，并且目前尚不清楚如何选择这些。
- en: '![](img/B16176_12_04.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_04.png)'
- en: 'Figure 12.4: Path trajectories from a (non) deterministic MDP. (b) is a particular
    path (trajectory) from the deterministic MDP (a), while (d) is a sample path from
    a non-deterministic MDP (c), where there is ambiguity as to which state action
    a[4] might lead to.^(23)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：来自（非）确定性MDP的路径轨迹。(*b*)是确定性MDP(a)的特定路径(轨迹)，而(*d*)是来自非确定性MDP(c)的样本路径，其中对于状态动作*a*[4]可能导致的情况存在歧义。(23)
- en: We can appeal to the partition function and Boltzmann distribution that we saw
    in *Chapter 4*, *Teaching Networks to Generate Digits*, when studying restricted
    Boltzmann machines. If we take the MDP represented by an RL agent and "unroll"
    the trajectories from following a particular set of actions in response to a set
    of states, we get a set of variable length paths in a tree diagram as depicted
    in *Figure 12.4*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当研究受限玻尔兹曼机时，我们可以引用我们在*第4章*，*教网络生成数字*中看到的分区函数和玻尔兹曼分布。如果我们将由RL代理表示的MDP并“展开”沿着对一组状态作出反应的一组特定动作的轨迹，我们将得到一组树形图中的变长路径，如*图12.4*所示。
- en: 'Many different distributions over the occurrence of these paths could be obtained
    by following different policies, and even with the same distribution of pixel
    features some of these policies might prefer one path over another based on a
    particular reward function. To resolve this ambiguity, we could optimize a reward
    function with parameters ![](img/B16176_12_016.png) such that paths with the same
    reward function value based on these features receive the same preference, but
    we exponentially prefer a higher reward:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循不同策略，可以获得对这些路径发生的不同分布，即使具有相同的像素特征分布，某些策略可能也会根据特定的奖励函数更喜欢一条路径而不是另一条路径。为了解决这种歧义，我们可以优化具有参数![](img/B16176_12_016.png)的奖励函数，以使基于这些特征的相同奖励函数值的路径获得相同的偏好，但我们指数地偏爱更高的奖励：
- en: '![](img/B16176_12_027.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_027.png)'
- en: This approach is known as *maximum entropy*; recall from *Chapter 4*, *Teaching
    Networks to Generate Digits*, that the *Z* is the partition function, which normalizes
    *P* to be a probability density over average trajectories *f* composed of steps
    *s*[j] evaluated using a (here, linear) reward function ![](img/B16176_12_028.png).^(25)
    Even in the case of non-deterministic outcomes (*Figure 12.4*, (*b*) and (*d*)),
    which could be the case, for example, in a video game where some of the behavior
    of enemy spacecraft is randomly generated by the computer, we could parameterize
    this equation in terms of the transition distribution *T*:^(23)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为*最大熵*;从*第4章*，*教网络生成数字*中回忆，*Z*是分区函数，将*P*归一化为关于由线性奖励函数![](img/B16176_12_028.png)评估的步骤*s*[j]组成的平均轨迹*f*的概率密度。(25)即使在非确定性结果(*图12.4*，(*b*)和(*d*))的情况下，这也可能是可能的，例如，在计算机随机生成敌方太空船行为的视频游戏中，我们可以将此方程参数化为过渡分布*T*的条件。(23)
- en: '![](img/B16176_12_029.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_029.png)'
- en: 'Here, the distribution is similar to before, but we''ve added the probability
    *P*[T] of observing outcome *o* based on actions *a* and states *s* using the
    transition model *T*. ![](img/B16176_12_030.png) denotes the indicator function,
    which evaluates to 1 when ![](img/B16176_12_031.png) and 0 otherwise. We could
    then optimize this equation to find the parameters of the reward function that
    maximize the likelihood of this function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，分布与以前相似，但我们增加了基于行动*a*和状态*s*使用过渡模型*T*观察结果*o*的概率*P*[T]。 ![](img/B16176_12_030.png)
    表示指示函数，当![](img/B16176_12_031.png) 时评估为1，否则为0。然后，我们可以优化此方程以找到最大化此函数概率的奖励函数参数：
- en: '![](img/B16176_12_032.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_032.png)'
- en: 'The gradients of this likelihood are just the difference between the expected
    feature counts (for example, distribution of pixels) from expert play and those
    obtained by following the proposed agent, based on the frequency of visiting a
    given state *D*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此似然的梯度仅是期望的特征计数（例如，像素分布）来自专家游戏和通过遵循提出的代理人获得的特征计数之间的差异，根据访问给定状态*D*的频率：
- en: '![](img/B16176_12_033.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_033.png)'
- en: 'This gives us an objective to optimize, but there are options for what kind
    of *P* we would like to use. The one that we will study is the *maximal causal
    entropy*, where the probability of a given action (and thus, state and path distribution)
    is conditioned on the prior set of states and actions, just as in our prior video
    game example ^(26 27):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们一个优化的目标，但是我们可以选择使用什么样的*P*。 我们将研究的是*最大因果熵*，在其中给定行动的概率（因此，状态和路径分布）条件于先前的状态和行动集，就像我们先前的视频游戏示例中一样^(26
    27)：
- en: '![](img/B16176_12_034.png)![](img/B16176_12_035.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_034.png)![](img/B16176_12_035.png)'
- en: 'Because this entropy value could potentially be infinite if the path never
    terminates, we could apply a discount factor to make it finite^(26):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因为如果路径永远不终止，这个熵值可能是无限的，所以我们可以应用一个折现因子使其有限^(26)：
- en: '![](img/B16176_12_036.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_036.png)'
- en: Where the discount factor *B* is applied to each time step *t*, with expectation
    over the distribution of initial states *P*[0] and subsequent states *P* subject
    to a policy ![](img/B16176_12_037.png); ![](img/B16176_12_038.png) denotes the
    causal entropy ![](img/B16176_12_039.png); and ![](img/B16176_12_040.png) is the
    causally conditioned probability = ![](img/B16176_12_041.png)(the probability
    conditioned on the prior actions in the sequence).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中折现因子*B*应用于每个时间步*t*，期望值为初始状态*P*[0]的分布和随后状态*P*的分布，受到策略![](img/B16176_12_037.png)的影响；![](img/B16176_12_038.png)表示因果熵![](img/B16176_12_039.png)；而![](img/B16176_12_040.png)是因果条件概率=
    ![](img/B16176_12_041.png)（在序列中先前行动的条件概率）。
- en: So at this point, we have an objective to optimize (discounted causal entropy),
    and the notion of computing a reward function for a particular agent that would
    make its behavior similar to an expert. What is the connection between this objective
    and generative AI? The answer is in how we can draw parallels between *discriminating*
    between an expert and learner, and generating samples from the learner – not unlike
    the GANs we studied in *Chapters 6*, *7*, and *8*!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这一点上，我们有一个优化目标（折现因果熵），以及计算为特定代理人制定奖励函数的概念，使其行为类似于专家。 这个目标与生成式人工智能有什么联系呢？
    答案在于我们如何在*区分*专家和学习者之间进行类比，并从学习者中生成样本——与我们在*第6*、*第7*和*第8*章中研究过的 GAN 类似！
- en: Adversarial learning and imitation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗学习和模仿
- en: Given a set of expert observations (of a driver, or a champion video game player),
    we would like to find a reward function that assigns high reward to an agent that
    matches expert behavior, and low reward to agents that do not. At the same time,
    we want to choose such an agent's policy, ![](img/B16176_12_042.png), under this
    reward function such that it is as informative as possible by maximizing entropy
    and preferring expert over non-expert choices. We'll show how both are achieved
    through an algorithm known as **Generative Adversarial Imitation Learning** (**GAIL**)
    published in 2016.^(20)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组专家观察（驾驶员或冠军电子游戏玩家），我们希望找到一个奖励函数，为与专家行为匹配的代理人分配高奖励，并为不匹配的代理人分配低奖励。 同时，我们希望在此奖励函数下选择这样一个代理人的策略，![](img/B16176_12_042.png)，使其尽可能地信息丰富，通过最大化熵和偏好专家而不是非专家的选择。
    我们将展示如何通过一种名为**生成对抗性模仿学习**（**GAIL**）的算法在2016年发表的方法来实现这两点。^(20)
- en: In the following, instead of a "reward" function, we use a "cost" function to
    match the conventions used in the referenced literature on this topic, but it
    is just the negative of the reward function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们使用“成本”函数而不是“奖励”函数来匹配此主题的参考文献中使用的约定，但它只是奖励函数的负值。
- en: 'Putting these constraints together, we get^(20):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些约束放在一起，我们得到^(20)：
- en: '![](img/B16176_12_043.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_043.png)'
- en: 'In the inner term, we are trying to find a policy that maximizes the discounted
    entropy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部项中，我们试图找到一种最大化折现熵的策略：
- en: '![](img/B16176_12_044.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_044.png)'
- en: 'Thus leading to a large negative value, and minimizing the cost term. We want
    to maximize this whole expression over different potential cost functions by selecting
    one that not only satisfies the inner constraint but gives low cost expert-like
    behavior, thus maximizing the overall expression. Note that this inner term is
    also equivalent to an RL problem that seeks an agent whose behavior optimizes
    the objective:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此导致一个大的负值，并最小化成本项。我们希望通过选择一个不仅满足内部约束而且提供低成本专家样式行为的成本函数来最大化不同潜在成本函数的整体表达式。请注意，该内部项也等价于一个寻求其行为优化目标的
    RL 问题：
- en: '![](img/B16176_12_045.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_045.png)'
- en: 'over the space of possible policies ![](img/B16176_12_046.png), denoted ![](img/B16176_12_047.png).
    However, in order to limit the space of possible choices of *c*, we apply a regularization
    function ![](img/B16176_12_048.png), which seeks a low-complexity *c* among many
    possible choices, and adds this constraint into the overall objective:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能策略空间上 ![](img/B16176_12_046.png) 上，记为 ![](img/B16176_12_047.png)。然而，为了限制
    *c* 的可能选择空间，我们应用一个正则化函数 ![](img/B16176_12_048.png)，它在许多可能选择中寻找低复杂度的 *c*，并将此约束添加到整体目标中：
- en: '![](img/B16176_12_049.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_049.png)'
- en: 'We can see that we could alternate between optimizing *c* under a given policy
    (IRL) and optimizing a policy under that *c* (RL) – and that is exactly the approach
    we will follow. We can express the optimal policy, using the approach described
    before, as a measure of **occupancy** (![](img/B16176_12_050.png)) of different
    states under that policy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们可以在给定策略（IRL）下交替优化 *c*，并在该 *c* 下优化策略（RL） - 这正是我们将要采取的方法。我们可以用前面描述的方法表达最优策略，作为对在该策略下不同状态的
    **占用度** (![](img/B16176_12_050.png)) 的度量：
- en: '![](img/B16176_12_051.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_051.png)'
- en: 'Then the optimal point for the RL and IRL problem is equivalent to minimizing
    the difference between the learned and expert occupancy:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，RL 和 IRL 问题的最优点等价于最小化学习和专家占用之间的差异：
- en: '![](img/B16176_12_052.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_052.png)'
- en: 'where ![](img/B16176_12_053.png) denotes a functional composition, ![](img/B16176_12_054.png),
    here representing that the RL agent uses the IRL output as one of its arguments.
    However, the examples of the expert behavior that we receive are often limited,
    so we don''t want to exactly mimic their distribution since that may cause the
    agent to overfit. Instead, we want to use a distance measure, such as the KL divergence
    or JS divergence we saw in *Chapter 6*, *Image Generation with GANs*, to quantify
    differences between the observed distribution of expert behavior and the distribution
    of actions taken by the IRL agent which tries to approximate that behavior:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B16176_12_053.png) 表示函数合成，![](img/B16176_12_054.png)，这里表示 RL 代理将
    IRL 输出作为其参数之一。然而，我们收到的专家行为示例通常是有限的，因此我们不希望完全模仿它们的分布，因为那可能导致代理过度拟合。相反，我们希望使用距离度量，例如我们在
    *第 6 章* 中看到的 KL 散度或 JS 散度，来量化观察到的专家行为分布与试图近似该行为的 IRL 代理采取的动作分布之间的差异：
- en: '![](img/B16176_12_055.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_055.png)'
- en: 'What regularizer function, ![](img/B16176_12_056.png), should be used in this
    algorithm? The authors of the GAIL algorithm present a function that assigns low
    penalty if the cost is low, and high penalty otherwise. A regularizer function
    with this property, for which the full derivation is presented in the paper "Generative
    Adversarial Imitation Learning," is:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个算法中应该使用什么样的正则化函数，![](img/B16176_12_056.png)？GAIL 算法的作者提出了一个函数，如果成本低则分配低惩罚，反之则分配高惩罚。在论文《生成对抗模仿学习》中给出了满足此属性的正则化函数的完整推导：
- en: '![](img/B16176_12_057.png) where ![](img/B16176_12_058.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_057.png) 其中 ![](img/B16176_12_058.png)'
- en: 'The intuition for this function is that it minimizes the difference between
    the causal entropy of the policy *H* with a penalty ![](img/B16176_12_059.png)
    (for example, the likelihood of the learned policy) and the difference between
    the expert and learned policy state occupancies ![](img/B16176_12_060.png):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的直观解释是，它最小化了策略 *H* 的因果熵与惩罚项 ![](img/B16176_12_059.png)（例如，学习策略的可能性）之间的差异，以及专家和学习策略状态的占用差异
    ![](img/B16176_12_060.png)：
- en: '![](img/B16176_12_061.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_061.png)'
- en: 'This ![](img/B16176_12_062.png) or ![](img/B16176_12_063.png) can be written
    as the negative log loss of a binary classifier that distinguishes between expert
    (0) and learned (1) action-states pairs:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 ![](img/B16176_12_062.png) 或 ![](img/B16176_12_063.png) 可以写成一个二元分类器的负对数损失，用于区分专家（0）和学习（1）的行动状态对：
- en: '![](img/B16176_12_064.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_064.png)'
- en: 'The expression probably looks quite familiar from *Chapter 6*, *Image Generation
    with GANs*, as it resembles the objective function of a GAN as well for discriminating
    between generated and real data! Putting these terms together (the regularizer
    and the causal entropy), we see that the complete objective function is:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式可能看起来非常熟悉，因为它类似于 *第 6 章* *使用 GAN 生成图像* 的目标函数，也用于区分生成和真实数据！将这些术语组合在一起（正则化器和因果熵），我们可以看到完整的目标函数是：
- en: '![](img/B16176_12_065.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_065.png)'
- en: 'To optimize this objective function, the GAIL algorithm utilizes the following
    steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要优化这个目标函数，GAIL 算法利用以下步骤：
- en: Prepare a set of expert trajectories and randomly initialize the discriminator
    and policy parameters.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一组专家轨迹并随机初始化鉴别器和策略参数。
- en: Generate a set of trajectories for the RL agent under the current policy.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 RL 代理生成一组轨迹。
- en: Update the discriminator parameters with a stochastic gradient descent step.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机梯度下降步骤更新鉴别器参数。
- en: Update the policy parameters with gradient-based updates using an algorithm
    called **Trust Region Policy Optimization** (**TRPO**) published in 2015 – please
    consult the original article "Trust Region Policy Optimization" for more details
    on the form of this gradient update.^(28)
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 2015 年发布的名为**Trust Region Policy Optimization**（**TRPO**）的算法，使用基于梯度的更新来更新策略参数
    - 请参阅原始文章"Trust Region Policy Optimization"了解有关此梯度更新形式的更多详细信息。^(28)
- en: Repeat *steps 2-4* of the algorithm until the values of the parameters of the
    policy and the discriminator converge.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复算法的 *步骤 2-4* 直到策略和鉴别器参数的值收敛。
- en: Now that we've explained the intuition behind using generative AI in reinforcement
    learning, let's dive into a practical example of training a walking humanoid in
    a virtual environment.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了使用生成式 AI 进行强化学习的直觉，让我们深入到在虚拟环境中训练步行人形的一个实际示例中。
- en: Running GAIL on PyBullet Gym
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyBullet Gym 上运行 GAIL
- en: For our code example in this chapter, we will train a virtual agent to navigate
    a simulated environment – in many RL papers, this environment is simulated using
    the Mujoco framework ([http://www.mujoco.org/](http://www.mujoco.org/)). **Mujoco**
    stands for **Multi joint dynamics with contacts** – it is a physics "engine" that
    allows you to create an artificial agent (such as a pendulum or bipedal humanoid),
    where a "reward" might be an ability to move through the simulated environment.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的代码示例中，我们将训练一个虚拟代理在一个模拟环境中导航 - 在许多 RL 论文中，这个环境是使用 Mujoco 框架 ([http://www.mujoco.org/](http://www.mujoco.org/))
    进行模拟的。**Mujoco** 代表 **Multi joint dynamics with contacts** - 这是一个物理“引擎”，可以让您创建一个人工代理（如摆锤或双足人形），其中“奖励”可能是通过模拟环境移动的能力。
- en: While it is a popular framework used for developing reinforcement learning benchmarks,
    such as by the research group OpenAI (see [https://github.com/openai/baselines](https://github.com/openai/baselines)
    for some of these implementations), it is also closed source and requires a license
    for use. For our experiments, we will use PyBullet Gymperium ([https://github.com/benelot/pybullet-gym](https://github.com/benelot/pybullet-gym)),
    a drop-in replacement for Mujoco that allows us to run a physics simulator and
    import agents trained in Mujoco environments.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它是用于开发强化学习基准的流行框架，例如由研究小组 OpenAI 使用（请参阅[https://github.com/openai/baselines](https://github.com/openai/baselines)以了解其中一些实现），但它也是闭源的，并且需要许可证才能使用。对于我们的实验，我们将使用
    PyBullet Gymperium ([https://github.com/benelot/pybullet-gym](https://github.com/benelot/pybullet-gym))，这是一个用于模拟
    Mujoco 环境中的代理的物理模拟器和导入代理的替代品。
- en: '![](img/B16176_12_05.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_05.png)'
- en: 'Figure 12.5: Examples from the Mujoco simulated environments (http://www.mujoco.org/)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.5: 来自 Mujoco 模拟环境的示例（http://www.mujoco.org/）'
- en: 'To install Pybullet-Gym, you need to first install OpenAI Gym, using the following
    commands:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Pybullet-Gym，您需要先安装 OpenAI Gym，使用以下命令：
- en: '[PRE0]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then install Pybullet:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然后安装 Pybullet：
- en: '[PRE1]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To show how this simulated environment works, let''s create a "hopper," one
    of the many virtual agents you can instantiate with the library:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这个模拟环境是如何工作的，让我们创建一个“hopper”，这是您可以使用库实例化的许多虚拟代理之一：
- en: '[PRE2]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If the commands execute correctly, we will see the following output, an array
    giving the current observation (11-dimensional vector) vector of the walker.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_06.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Observation vector of the walker'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The call to `render("human")` will create a window showing the "hopper," a
    simple single-footed figure which moves in a simulated 3D environment (*Figure
    12.7*):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_07.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: PyGym hopper'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run a few iterations of the hopper in its raw, "untrained" form, to
    get a sense of how it looks. In this simulation, we take up to 1,000 steps and
    visualize it using a pop-up window:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We first clear the environment with `reset()`. Then, for up to 1,000 timesteps,
    we sample the action space (for example, the *x*, *y*, and *z* coordinates representing
    movements of the walking figure within the virtual environment). Then, we use
    that action to get an updated reward and observation, and render the result, until
    the movement completes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'This demonstration comes from a completely untrained hopper. For our GAIL implementation,
    we will need a hopper that has been successfully trained to walk as a sample of
    "expert" trajectories for the algorithm. For this purpose, we''ll download a set
    of hopper data from the OpenAI site, at:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[https://drive.google.com/drive/folders/1h3H4AY_ZBx08hz-Ct0Nxxus-V1melu1U](https://drive.google.com/drive/folders/1h3H4AY_ZBx08hz-Ct0Nxxus-V1melu1U)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: These contain a set of NumPy files, such as `deterministic.trpo.Hopper.0.00.npz`,
    that contain samples of data from reinforcement learning agents trained using
    the Trust Region Policy Optimization algorithm used in *Step 4* of the GAIL algorithm
    we discussed earlier.^(28)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'If we load this data, we can also visualize it using the Pybullet simulator,
    but this time we will see steps from the expert, rather than the random baseline
    agent:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code loads the pre-trained hopper, initializes the virtual environment,
    and takes a maximum of 1,000 steps in which an action (the next move of the hopper)
    is determined using the hopper''s trained policy function, and the environment
    state (position of the hopper) is updated based on that action. Note that here
    the policy function is deterministic, leading to the same outcome for any given
    action at time *t*. You can see the hopper now taking many steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_08.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: The hopper moving with expert policy trained by TRPO'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at what kinds of data are in this NumPy object we''ve
    loaded. You''ll notice that the format, `.npz`, is a gzipped archive of compressed
    files. We can see the names of these archives by using the files parameter of
    the object `mujoco_hopper_np`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'which gives:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The observations are 11-dimensional objects, which you can verify by looking
    at the dimension of `obs`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This array has 1,500 examples, each of 1,000 timesteps, which each have 11 dimensions
    representing different physical quantities (position of hinges, torque, and so
    on). The objective of the hopper task is to move the hopper forward as fast as possible,
    so the reward function is higher when the agent learns to move forward. If we
    examine the `acs` data, we see that it has three dimensions, corresponding to
    points in 3D space. This is a continuous action space, unlike the discrete examples
    we have discussed previously.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该数组有1,500个示例，每个1,000个时间步长，每个时间步长有11个表示不同物理量（铰链位置、扭矩等）的维度。蹦跶者任务的目标是尽可能快地向前移动，所以当代理学会向前移动时，奖励函数会更高。如果我们检查`acs`数据，我们会发现它有三个维度，对应于三维空间中的点。这是一个连续的动作空间，不同于我们之前讨论的离散示例。
- en: '[PRE8]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `ep_rets` corresponds to the predicted reward in the future for an action
    at time t, and the rewards `rews` are the output of the reward function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`ep_rets`对应于在时间t执行动作的未来预测奖励，而奖励`rews`是奖励函数的输出。'
- en: 'The agent: Actor-Critic network'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理：演员-评论家网络
- en: 'To create our GAIL implementation, we''ll first need to specify an agent.^(29)
    This is an Actor-Critic architecture, consisting of two networks: one which learns
    the "value" of an observation (Critic), and another (Actor) which samples actions
    based on observations. These networks could be independent or share parameters;
    for our experiment, we''ll have them share hidden and input layers but have separate
    output layers (*Figure 12.9*).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建我们的GAIL实现，首先我们需要指定一个代理。（29）这是演员-评论家架构，由两个网络组成：一个学习观察结果的“值”（评论家），另一个（演员）基于观察结果进行动作采样。这些网络可以是独立的，也可以共享参数；对于我们的实验，我们让它们共享隐藏层和输入层，但有单独的输出层（*图12.9*）。
- en: '![](img/B16176_12_09.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_12_09.png)'
- en: 'Figure 12.9: Actor-Critic architecture'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：演员-评论家架构
- en: Note the code for the GAIL implementation in this chapter is based on [https://github.com/fangihsiao/GAIL-Tensorflow/blob/master/tf_gail.ipynb](https://github.com/fangihsiao/GAIL-Tensorflow/blob/master/tf_gail.ipynb).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章GAIL实现的代码基于[https://github.com/fangihsiao/GAIL-Tensorflow/blob/master/tf_gail.ipynb](https://github.com/fangihsiao/GAIL-Tensorflow/blob/master/tf_gail.ipynb)。
- en: 'Below we define the `ActorCritic` class:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们定义`ActorCritic`类：
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This class initializes a network that accepts input states and action pairs
    and generates two outputs – one generates new actions (3D coordinates representing
    the next move of the hopper in the virtual space) – the Actor – and the other
    generates values (how successful the movement is in navigating the hopper farther
    in the virtual space) – the Critic. The value output is a single scalar that increases
    with the quality of the hopper's movement, while the action is a 3-unit vector
    representing a move in 3D space with a mean and standard deviation per coordinate.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 该类初始化一个接受输入状态和动作对并生成两个输出的网络-一个生成新动作（代表虚拟空间中蹦跶者的下一步移动的3D坐标）-演员-另一个生成值（表示蹦跶者在虚拟空间中移动的成功程度）-评论家。值输出是一个单一标量，随着蹦跶者运动质量的提高而增加，而动作是一个3单元向量，表示在3D空间中移动的每个坐标的均值和标准偏差。
- en: Because our network has multiple outputs, we need to be careful about setting
    the input layers and initializing them. Notice that we explicitly call build on
    the two output layers, rather than letting them be automatically instantiated
    in the forward pass, as this will cause errors in compiling the model. We also
    instantiate a variable, `_action_dist_std`, to contain the standard deviation
    of the action dimensions, which we'll use to sample new coordinates in the model.
    We've also included `BatchNormalization` layers to prevent the gradients from
    exploding or vanishing in our network.^(29)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的网络有多个输出，所以在设置输入层和初始化它们时需要小心。请注意，我们明确调用了两个输出层的`build`，而不是让它们在前向传递中自动实例化，因为这将导致模型编译错误。我们还实例化了一个变量`_action_dist_std`，包含动作维度的标准偏差，我们将在模型中用它来采样新坐标。我们还包括了`BatchNormalization`层，以防止我们网络中的梯度爆炸或消失。（29）
- en: 'We also need to be able to return the trainable parameters in the model for
    our gradient calculation, using the `get_params` method of the Actor-Critic network:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要能够返回模型中可训练的参数，以便进行梯度计算，使用`Actor-Critic`网络的`get_params`方法：
- en: '[PRE10]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In our forward pass, we calculate the output of the Critic:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的前向传递中，我们计算了评论家的输出：
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To sample new actions (3D moves) from the Actor, we run the sample function
    with the argument `''action''` – if we supply `''entropy''` instead, we return
    the entropy of the action distribution:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we need to be able to return the log probability of the action distribution
    (for our loss function) for the PPO network, described below:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our IRL agent – for which we will use **Proximal Policy Optimization** (**PPO**)
    policy updates, an improvement on TRPO published in 2017 ^(29) – makes use of
    this Actor-Critic network as the "policy" function.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This class initializes a neural network (`_policy`) and provides a placeholder
    for updates to that network (`_new_policy`), so that with each step of the algorithm
    we can update the new policy with reference to its improvement over the last policy.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function inside the `train_policy` loop is optimized using a gradient
    descent algorithm where the magnitude of the gradients is constrained to a fixed
    range ("clipped") so that large gradients don''t lead the loss function (and weights)
    to change drastically between rounds of training:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this loss function, we first take a ratio between the old policy (current
    parameters of the Actor-Critic network) and a potential update (new policy) –
    the exponential of the difference of their log probabilities (which is the likelihood
    of the observed actions under the action distribution of each network) gives a
    ratio. If the new, proposed network is an improvement (its parameters fit the
    observed action sequence better), the ratio is greater than 1\. Otherwise, the
    proposal is unchanged in quality (a ratio of 1) or worse (a ratio less than 1)
    than the current parameters of the Actor-Critic.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'We multiply this ratio by the "advantages," which are the difference between
    returns (the Q-function we described earlier) and the current values of the Actor-Critic''s
    existing state. In this GAIL implementation, we compute advantages through Generalized
    Advantage Estimation^(29), which uses an exponentially smoothed estimate of the
    Q-function, where `gamma` (coefficient) and `tau` (exponents) control how much
    the estimate of future reward decays in the future relative to no future information
    (`tau` = 0) or no decay of importance of future data versus present (`tau` = 1):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The loss function above then uses the advantages multiplied by `surr` (the "surrogate"
    term) and computes two values – the first being the actor loss, which constrains
    the loss term given by the advantages to be within a given range, denoted by the
    `clip_by_value` function. This prevents extremes (much less than 1 or greater
    than 1) of the probability ratio of the new and old policy from making the loss
    function unstable. To this, we add the critic loss, the squared difference between
    the critic value and the advantage function we computed above. Taking a weighted
    sum of the actor and critic functions and the entropy of the action probability
    distribution (whether it assigns high value to a subset of positions, and thus
    contains "information" about the distribution of potential actions) gives the
    overall quality of the policy as an objective for the loss function.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的损失函数然后使用优势乘以`surr`（"替代"项），并计算两个值——第一个是演员损失，它将由优势给出的损失项约束在给定范围内，由`clip_by_value`函数表示。这可以防止新旧策略的概率比率的极端值（远小于1或大于1）使损失函数不稳定。除此之外，我们还加上评论者的损失，评论者值与我们上面计算的优势函数之间的平方差。对演员和评论者函数以及行动概率分布的熵（它是否为位置子集分配高值，从而包含关于潜在行动分布的“信息”）进行加权求和，得到损失函数的总体策略质量作为目标。
- en: Note that the `actor_loss` is multiplied by negative 1 (because it is the ratio
    between the old and new policy probability – thus if it improves it is greater
    than 1, but the loss function should be minimized, thus a larger negative value).
    Similarly, the entropy term has more information if it is larger, and we take
    the negative of this as well as we minimize the loss function. The critic loss
    becomes better the closer to 0 it is, so we leave this term as positive.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`actor_loss`乘以负1（因为它是旧策略概率与新策略概率之间的比率——因此如果它改善了，它大于1，但损失函数应该最小化，因此一个更大的负值）。同样，熵项如果更大，它含有更多信息，我们也要取它的负值，因为我们要最小化损失函数。评论者的损失越接近0，它就变得更好，所以我们保留这个项为正。
- en: 'To use this loss function, we define a custom train function called `train_policy`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个损失函数，我们定义了一个名为`train_policy`的自定义训练函数：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We use the `get_params()` function to extract the trainable parameters of the
    PPO policy network, "watch" them using `GradientTape`, and compute the loss using
    the loss function above. Also, since the Actor-Critic has two outputs (action
    and value), only one of which (value) is affected by a reward update, we could
    have nonexistent gradients, which is why we replace any empty gradients with a
    0.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`get_params()`函数提取PPO策略网络的可训练参数，使用`GradientTape`对它们进行“监视”，并使用上面的损失函数计算损失。此外，由于演员-评论者有两个输出（动作和值），只有一个输出（值）受到奖励更新的影响，我们可能会有不存在的梯度，所以我们用0替换任何空的梯度。
- en: 'In each step of the GAIL inner loop which we described above, we also need
    to be able to replace the old policy with the new, using a deep copy (a copy that
    creates a new variable with the same values, rather than a pointer to the original
    variable):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面描述的GAIL内循环的每一步中，我们还需要能够用深拷贝（创建一个具有相同值的新变量，而不是指向原始变量的指针）来替换旧策略为新策略：
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we can use calls to the Actor-Critic policy network to get the value
    (reward) estimate and sample new actions:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用对演员-评论者策略网络的调用来获得价值（奖励）估计和采样新的行动：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The discriminator
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鉴别器
- en: With our PPO algorithm, we have the "agent" which we are trying to teach to
    behave like an expert. We can sample from our TRPO trained examples of the hopper
    that we downloaded as a "generator."
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的PPO算法，我们有了需要教会表现得像一个专家的“代理人”。我们可以从我们以“生成器”形式下载的TRPO训练示例的弹跳者中抽样。
- en: 'So we now just need a discriminator network, which seeks to distinguish the
    expert behavior from the agent we are training:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要一个鉴别器网络，它旨在区分专家行为和我们正在训练的代理者：
- en: '[PRE20]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Like the Actor-Critic, this is a 3-layer neural network with `BatchNormalization`
    applied between layers. Its single output indicates the quality of the input (like
    the value function of the Actor-Critic), and should be lower when the network
    is more "expert-like." Notice that to get the "reward" output to match the sign
    of the Actor-Critic value output, we reverse the sign of the discriminator because
    it predicts closer to 0 for expert observations:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 就像演员-评论者一样，这是一个3层神经网络，层与层之间应用了`BatchNormalization`。它的单个输出指示输入的质量（就像演员-评论者的价值函数），当网络更像“专家”时，输出应该更低。请注意，为了使“奖励”输出与演员-评论者值输出的符号匹配，我们反转了鉴别器的符号，因为它对专家观察预测得更接近0：
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This transformation is applied to the same `call` function we saw earlier for
    the Actor-Critic network.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Training and results
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train the network, we apply a loss function that tries to classify expert
    (observation, action) pairs as 0s and agent (observation, action) pairs as 1s.
    When the agent learns to generate high quality (observation, action) pairs that
    resemble the expert, the discriminator will have increasing difficulty distinguishing
    between samples from the agent and expert, and will assign agent samples a label
    of 0 as well:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As before, we extract the parameters of the network through `get_params()`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We then apply our loss function to these parameters using `train_discriminator`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we need an update function for our PPO minibatch step, where we randomly
    sample observations from the agent in each inner loop of the GAIL algorithm:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We also want to be able to plot the progress of our agent as we train it, for
    which we''ll take samples from the environment using a model and plot the average
    reward and the performance of the discriminator (how closely the agent and expert discriminator
    reward matches):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This function makes two plots, shown in *Figure 12.10*. On the left is the
    reward for a set of test observations for the agent, which should increase as
    the agent becomes better at moving the hopper. On the right is plotted, for each
    of n-steps of a sample agent and expert movement of the hopper, how well the discriminator
    can tell the difference between the two (whether the orange and blue lines overlap,
    which is optimal, or are far apart, in which case the GAIL algorithm hasn''t converged):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_10.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Agent reward for a series of test observations (left), discriminator
    reward (right). Plots generated by plot().'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'The test samples are generated using the `test_env` function, which resembles
    the Pybullet simulations we saw above – it uses the current agent and computes
    n-steps of the simulation under the current policy, returning the average reward:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For our main function, we''ll set a number of maximum simulation steps and
    the hyperparameters of the algorithm, including the learning rate of the ADAM
    optimizer (`lr`), the number of units in the hidden layer of each network, the
    number of steps to run each hopper simulation for the agent (`num_steps`), the
    number of samples to choose in each Agent update (minibatch size), the number
    of steps to run each inner loop updating the Agent (`ppo_epochs`), the overall
    max number of steps in the algorithm (`max_frames`), and a container to hold the
    out-of-sample rewards estimates that we showed how to plot above (`test_rewards`):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'First we initialize the discriminator and PPO networks, set a counter for the
    inner loop for Agent update (`i_update`), and set up the Pybullet environment:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At each step, we''ll compute a number of timesteps using our current policy,
    and create a list of these observations, actions, and rewards. At regular intervals,
    we will plot the results using the functions we described above:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice that if our simulation is terminated, we "mask" any observations after
    completion so that they don't affect our gradient calculation. Also note that
    we are converting the NumPy data from the sample into tensors using the `tf.convert_to_tensor`
    and `tf.reshape` functions. We use the `compute_gae` function described above
    to smooth out the estimated advantage function.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: We then periodically (here, every 3 cycles) compute an update to the PPO policy
    using minibatch updates.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we sample an expert trajectory from our previously loaded data in
    the TRPO policy, concatenate the observations and actions from the expert and
    new policy, and make a gradient update of the discriminator:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note that you can observe the hopper being trained in the simulation window
    while the algorithm is running – the difficulty of finding a mathematical definition
    of "reward" is shown by the fact that during training, we can see the hopper making
    creative movements (such as crawling, *Figure 12.11*) that allow it to move in
    the simulation space (and thus receive reward) but are not like the expert "hopping"
    motion.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_11.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Hopper "crawling" during GAIL training'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored another application of generative models in reinforcement
    learning. First, we described how RL allows us to learn the behavior of an agent
    in an environment, and how deep neural networks allowed Q-learning to scale to
    complex environments with extremely large observation and action spaces.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed inverse reinforcement learning, and how it varies from RL
    by "inverting" the problem and attempting to "learn by example." We discussed
    how the problem of trying to compare a proposed and expert network can be scored
    using entropy, and how a particular, regularized version of this entropy loss
    has a similar form as the GAN problem we studied in *Chapter 6*, called GAIL (Generative
    Adversarial Imitation Learning). We saw how GAIL is but one of many possible formulations
    of this general idea, using different loss functions. Finally, we implemented
    GAIL using the bullet-gym physics simulator and OpenAI Gym.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we will conclude by exploring recent research in generative
    AI in diverse problem domains including bioinformatics and fluid mechanics, providing
    references for further reading as you continue your discovery of this developing field.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). *Playing Atari with Deep Reinforcement Learning*.
    arXiv. [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bareketain, P. (2019, March 10). Understanding *Stabilising Experience Replay
    for Deep Multi-Agent Reinforcement Learning*. Medium. [https://medium.com/@parnianbrk/understanding-stabilising-experience-replay-for-deep-multi-agent-reinforcement-learning-84b4c04886b5](https://medium.com/@parnianbrk/understanding-stabilising-experience-replay-for-deep-multi-agent-rein)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wikipedia user waldoalverez, under a CC BY-SA 4.0 license ([https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)).
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amit, R., Meir, R., & Ciosek, K. (2020). *Discount Factor as a Regularizer in
    Reinforcement Learning*. Proceedings of the 37th International Conference on Machine
    Learning, Vienna, Austria, PMLR 119, 2020\. [http://proceedings.mlr.press/v119/amit20a/amit20a.pdf](http://proceedings.mlr.press/v119/amit20a/amit20a.pdf)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matiisen, T. (2015, December 19). *Demystifying Deep Reinforcement Learning*.
    Computational Neuroscience Lab. [https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/](https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bellman, R.E. (2003) [1957]. *Dynamic Programming*. Dover.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Watkins, C.J.C.H. (1989), *Learning from Delayed Rewards* (PDF) (Ph.D. thesis),
    Cambridge University, [http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sutton, R., & Barto, A. (1998). *Reinforcement Learning: An Introduction*.
    MIT Press.'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Melo, F.S. *Convergence of Q-Learning: A simple proof*. [http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf](http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf)'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Watkins, C.J.C.H., & Dayan, P. (1992). *Q-learning*. Machine learning, 8(3-4):279-292
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nair, A., Dalal, M., Gupta, A., & Levine, S. (2020). *Accelerating Online Reinforcement
    Learning with Offline Datasets*. arXiv. [https://arxiv.org/abs/2006.09359](https://arxiv.org/abs/2006.09359)
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sutton, R.S. & Barto A.G. (2018). *Reinforcement Learning: An Introduction*
    (2nd ed.). MIT Press.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rummery, G.A., & Niranjan, M. (1994). *On-line Q-learning using Connectionist
    Systems*. Cambridge University Engineering Department.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). *Playing Atari with Deep Reinforcement Learning*.
    arXiv. [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche,
    G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,
    S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach,
    M., Kavukcuoglu, K., Graepel, T., & Hassabis, D. (2016). *Mastering the game of
    Go with deep neural networks and tree search*. *Nature* 529, 484–489\. [https://www.nature.com/articles/nature16961](https://www.nature.com/articles/nature16961)
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
    A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui,
    F., Sifre, L., van den Driessche, G., Graepel, T., & Hassabis, D. (2017). *Mastering
    the game of Go without human knowledge*. *Nature* 550, 354-359\. [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
    Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K.,
    & Hassabis, D. (2017). *Mastering Chess and Shogi by Self-Play with a General
    Reinforcement Learning Algorithm*. arXiv. [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vinyals, O., Babuschkin, I., Silver. D. et al. (2019). *Grandmaster level in
    StarCraft II using multi-agent reinforcement learning*. *Nature* 575, 350-354\.
    [https://www.nature.com/articles/s41586-019-1724-z](https://www.nature.com/articles/s41586-019-1724-z)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pomerleau, D.A. (1991). *Efficient training of artificial neural networks for
    autonomous navigation*. Neural Computation, 3(1):88-97\.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ho, J., & Ermon, S. (2016). *Generative Adversarial Imitation Learning*. arXiv.
    [https://arxiv.org/abs/1606.03476](https://arxiv.org/abs/1606.03476)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ross, S., & Bagnell, J.A. (2010). *Efficient Reductions for Imitation Learning*.
    Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics, PMLR 9:661-668
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on a figure from Arora, S., & Doshi, P. (2018). *A Survey of Inverse
    Reinforcement Learning: Challenges, Methods and Progress*. arXiv. [https://arxiv.org/abs/1806.06877](https://arxiv.org/abs/1806.06877)'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on a figure from Ziebart, B.D., Maas, A., Bagnell, J.A., & Dey, A.K. (2008).
    *Maximum Entropy Inverse Reinforcement Learning*. Proceedings of the Twenty-Third
    AAAI Conference on Artificial Intelligence (2008)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abbeel, P., & Ng, A. Y. (2004). *Apprenticeship learning via inverse reinforcement
    learning*. In Proc. ICML, 1–8.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jaynes, E. T. (1957). *Information theory and statistical mechanics*. Physical
    Review 106:620–630.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Bloem and N. Bambos, *Infinite time horizon maximum causal entropy inverse
    reinforcement learning*, 53rd IEEE Conference on Decision and Control, 2014, pp.
    4911-4916, doi: 10.1109/CDC.2014.7040156\. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.720.5621&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.720.5621&rep=rep1&type=pdf)'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kramer, G. (1998). *Directed information for channels with feedback*. PhD dissertation,
    Swiss Federal Institute of Technology, Zurich. [http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=91AB2ACCC7B3CD1372C2BC2EA267ECEF?doi=10.1.1.728.3888&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=91AB2ACCC7B3CD1372C2BC2EA267ECEF?doi=10.1.1)
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schulman, J., Levine, S., Moritz, P., Jordan, M.I., & Abbeel, P. (2015). *Trust
    Region Policy Optimization*. arXiv. [https://arxiv.org/abs/1502.05477](https://arxiv.org/abs/1502.05477)
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ioffe, S., & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*. arXiv. [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). *Proximal
    Policy Optimization Algorithms*. arXiv. [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). *High-Dimensional
    Continuous Control Using Generalized Advantage Estimation*. arXiv. [https://arxiv.org/abs/1506.02438](https://arxiv.org/abs/1506.02438)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
