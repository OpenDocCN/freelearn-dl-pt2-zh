- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Play Video Games with Generative AI: GAIL'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we have seen how we can use generative AI to produce both
    simple (restricted Boltzmann machines) and sophisticated (variational autoencoders,
    generative adversarial models) images, musical notes (MuseGAN), and novel text
    (BERT, GPT-3).
  prefs: []
  type: TYPE_NORMAL
- en: In all these prior examples, we have focused on generating complex data using
    deep neural networks. However, neural networks can also be used to learn *rules*
    for how an entity (such as a video game character or a vehicle) should respond
    to an environment to optimize a reward; as we will describe in this chapter, this
    field is known as **reinforcement learning** (**RL**). While RL is not intrinsically
    tied to either deep learning or generative AI, the union of these fields has created
    a powerful set of techniques for optimizing complex behavioral functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will show you how to apply GANs to learn optimal policies
    for different figures to navigate within the OpenAI simulation environment. To
    understand the powerful combination of these methods with traditional approaches
    in RL, we will first review the more general problem that RL is trying to solve:
    how to determine the right *action* for an entity given a *state*, yielding a
    new *state* and a *reward*. The rules that optimize such rewards are known as
    a *policy*. We will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How deep neural networks were used to learn complex policies for high dimensional
    data such as the raw pixels from Atari video games.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The problem of **inverse reinforcement learning** (**IRL**) – how to learn the
    reward function from observing examples of the policy as given by an "expert"
    agent that makes optimal decisions – this kind of algorithm, as we will describe
    in more detail, is therefore also known as *imitation learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we can use the GAN training function to distinguish between expert and non-expert
    behavior (just as we distinguished between simulated and natural data in prior
    examples) to optimize a reward function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reinforcement learning: Actions, agents, spaces, policies, and rewards'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall from *Chapter 1*, *An Introduction to Generative AI: "Drawing" Data
    from Models*, that most *discriminative AI* examples involve applying a continuous
    or discrete label to a piece of data. In the image examples we have discussed
    in this book, this could be applying a deep neural network to determine the digit
    represented by one of the MNIST images, or whether a CIFAR-10 image contains a
    horse. In these cases, the model produces a single output, a prediction with minimal
    error. In reinforcement learning, we also want to make such point predictions,
    but over many steps, and to optimize the total error over repeated uses.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Atari video game examples¹'
  prefs: []
  type: TYPE_NORMAL
- en: As a concrete example, consider a video game with a player controlling a spaceship
    to shoot down alien vessels. The spaceship navigated by the player in this example
    is the *agent*; the set of pixels on the screen at any point in the game is the
    *state*. Based on this environment, the player needs to take the right *action*
    (for example, moving right, pressing fire) which will maximize the *reward* –
    here quite literally the score for the game. It is not just the next immediate
    action which we need to consider though, but all actions until the end of the
    game, since we can accumulate additional points as long as we do not run out of
    lives. Expert video game players learn how to react in different situations, a
    *policy* to follow when confronted with diverse scenarios during gameplay. The
    problem of RL is to determine a machine learning algorithm that can replicate
    the behavior of such a human expert, by taking a set of inputs (the current state
    of the game) and outputting the optimal action to increase the probability of winning.
  prefs: []
  type: TYPE_NORMAL
- en: To formalize this description with some mathematical notation, we can denote
    the *environment*, such as the video game, in which an agent acts as ![](img/B16176_12_001.png),
    which outputs a set of data (pixels) at each point in time as the *state*, *x*[t].
    For each point of time in the game, the player (algorithm) selects an *action*,
    *a*[t], from a set of *n* actions *A* = {1, 2, …. *N*}; this is also known as
    the "action set" or "action space." While for clarity we limit our discussion
    in the chapter to discrete action spaces, there is theoretically no such restriction,
    and the theory works just as well for continuous actions (though the resulting
    RL problem is consequently more complex with an infinite space of possibilities).
    For each of these actions, the agent receives a reward, *r*[t], that can change
    the game score.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to only consider the current screen, *x*[t], as the "state" of the
    system, then our decision relies only on the present, and the RL problem becomes
    a **Markov Decision Process** (**MDP**), as the choice for the next action depends
    only on immediately available data and not on history (*Figure 12.2*). However,
    for the video game example given above, the current screen only is probably not
    enough information to determine the optimal action, because it is only *partially
    observable* – we don't know cases where an enemy starcraft may have moved off
    the screen (and thus where it might re-emerge). We also don't know what direction
    our ship is moving without comparing to prior examples, which might affect whether
    we need to change direction or not. If the current state of the environment contained
    all the information we need to know about the game – such as a game of cards in
    which all players show their hands – then we say that the environment is *fully
    observable*.²
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: A Markov Decision Process (MDP)³. The transition (black arrows)
    between states (green circles) via actions with certain probabilities (orange
    circles) yields rewards (orange arrows).'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, a human video game player does not rely only on the immediate state
    of the game to determine what to do next; they also rely on cues from prior points
    in the game, such as the point at which enemies went offscreen to anticipate them
    re-emerging.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, our algorithm will benefit from using a *sequence* of states and
    actions leading to the current state, *s* = {*x*[1]…, *x*[t]; *a*[1]...*a*[t-1]},
    from the game as input to the current decision. In other words, the "state" becomes
    this sequence of prior states and actions, and we can still use the methods developed
    for MDPs to solve this RL problem.¹
  prefs: []
  type: TYPE_NORMAL
- en: 'At each point in time, based on this state (history), we want to make the decision
    that will maximize *future reward*, *R*, at the end of the game. Intuitively,
    we are usually better at estimating the outcome of our immediate actions versus
    their effect far in the future, so we apply a discount to estimating the impact
    of actions taken near-term versus long-term using a *discount* term, ![](img/B16176_12_002.png),
    between 0 and 1 in our computation of expected reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *t''* is a timepoint between the current timepoint, *t*, and the end
    of the game, *T*. We can see that there are three potential interpretations of
    this future rewards function based on the value of ![](img/B16176_12_004.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_005.png). Future rewards have no impact on our decision,
    and we care only about the current rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B16176_12_006.png). As we increase the distance between *t''* and *t*,
    the exponent of this multiplicative factor becomes greater and the discount term
    smaller, shrinking the future reward to 0 at infinity. In this case, we weight
    nearer-term goals more highly than longer-term goals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B16176_12_007.png). In this case, our environment could be deterministic,
    such that we don''t need to discount future reward because there is no uncertainty
    about outcomes, but even in stochastic environments the use of ![](img/B16176_12_008.png)
    or ![](img/B16176_12_009.png) can be seen as a choice between regularizing the
    reward calculation (or not). When the discount factor is < 1, the algorithm is
    less affected by (potentially sparse) data over a longer horizon, so can be helpful
    in cases where the action space is very large and training on many future steps
    without this discounting could lead to overfitting on individual paths in that
    space.⁴'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of the RL problem we have described here is then to learn a *value
    function*, Q, which maximizes reward given a sequence and a *policy*, ![](img/B16176_12_010.png),
    that associates sequences to actions – the convention of writing this as the "Q"
    function comes from the fact that the function evaluates the "quality" of decisions
    made by the RL algorithm⁵:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, given a state (or, as in our case, a sequence of states) and
    a proposed action, the Q-function scores the proposed action based on the maximum
    total future reward R. This Q-function is generally not known, and we need to
    solve for it – however, the optimal Q-function obeys an important rule based on
    *dynamic programming* called the *principle of optimality*:'
  prefs: []
  type: TYPE_NORMAL
- en: '"An optimal policy has the property that whatever the initial state and initial
    decision are, the remaining decisions must constitute an optimal policy with regard to
    the state resulting from the first decision."⁶'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In others words, regardless of the starting state, once we transition to a
    new state as a result of the first action, all subsequent actions must be optimal
    under an optimal policy. If we write out this principle in mathematical form,
    it splits the value function into two parts in a recursive expression known as
    the *Bellman equation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *Q** is the "optimal" Q-function, *E* is the expected value, and ![](img/B16176_12_013.png)
    is an environment from which future states *s''* are sampled. This expression
    says that the optimal Q-function should give the expected value for the current
    sequence and proposed action as the expectation of the sum of current reward r
    and the discounted future value of the next sequence of actions, *Q**(*s''*, *a''*).
    This expression is also known as a functional expression because the solution
    is the function *Q**, and thus this general class of problems is known as "Q-learning."
    ⁷ One approach to solving for *Q** in Q-learning is through *value iteration*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/B16176_12_015.png) is a *learning rate* that determines how quickly
    to modify the function with new information. If we run this update for a sufficiently
    large number of *i* and *s*, we could converge to the optimal value for *Q*, *Q**.
    ^(8, 9) For simpler problems where we have only a small number of actions and
    states, we could create a table that gives the value of *Q* for any potential
    action. The entries in this table are randomly initialized for every *Q*(*s*,
    *a*) and updated using the value iteration formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '| State/Action | A1 | A2 | A3 | A4 |'
  prefs: []
  type: TYPE_TB
- en: '| S1 | Q(S1, A1) |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| S2 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| S3 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12.1: A lookup table for Q-learning. Each cell contains a Q value for
    a particular state (S) and action (A) pair. Values are randomly initialized and
    updated using value iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: However, as pointed out by prior authors,¹ for problems such as the video game
    scenario we have described, it is almost impossible to enumerate all potential
    sequences and actions. Also, because the likelihood of any particular sequence-action
    pair is very low, we would need to sample a huge number of examples to be able
    to accurately estimate the *Q* value using value iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this problem more computationally feasible, we need to turn *Q* from
    a lookup table to a function that can *generalize* to estimate the value of state-action
    pairs that it hasn''t seen based on similar examples to which it has been exposed.
    To do this, *Q* can be represented by a parameterized function with some parameters
    ![](img/B16176_12_016.png): the function can be linear, a tree model, or in modern
    applications even the deep neural networks that we have been studying in other
    parts of this book. In this case, our objective resembles more a classical learning
    algorithm, where we try to minimize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_017.png)'
  prefs: []
  type: TYPE_IMG
- en: With ![](img/B16176_12_018.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'and *p* is a distribution of actions and sequences from a potentially very
    large space performed by an agent in an environment ![](img/B16176_12_019.png).
    Since L is differentiable, we can optimize it using stochastic gradient descent,
    just like the deep learning algorithms we discussed in *Chapter 3*, *Building
    Blocks of Deep Neural Networks*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_020.png)'
  prefs: []
  type: TYPE_IMG
- en: By holding the parameters fixed and optimizing *L*, followed by constructing
    new samples from *p* and *s*, we have an algorithm that resembles the iterative
    updates for the value iteration Q-learning as given above.^(10)
  prefs: []
  type: TYPE_NORMAL
- en: 'Where does this approach fit among the hierarchy of RL methods? Generally,
    RL methods are classified according to:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether they use a static (offline) dataset or are trained on new data continuously
    delivered to the system (online).^(11)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the policy function is updated with *Q* (on-policy) or independently
    (off-policy).^(12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the transition equations are explicitly modeled (model-based) or not
    (model-free).^(12)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among these possible RL algorithm variants, the version of Q-learning described
    above is classified as *off-policy*, *online, and model-free*. While we could
    use the values of *Q* as a policy for selecting the next action, our samples from
    *p*(*.*) do not need to hold to this policy.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, an epsilon greedy distribution is used for problems where we want
    to introduce some randomness into the algorithm to prevent it from getting stuck
    in local minima, such as deep Q-learning (introduced in a short while), which
    selects the "greedy" (maximizing Q) action with probability *e*, and a random
    action with probability *1-e*. Thus the policy we are learning (Q) is not strictly
    used to select actions (a) given this randomness. This approach is model-free
    because the neural network approximates the transition model, and is online because
    it is learned on a dynamically generated dataset, though could be trained using
    a static offline history of video game sessions as well.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, in *on-policy* Q-learning algorithms, such as **State-Action-Reward-State-Action**
    (**SARSA**) ^(13), we use direct updates for the Q-function as given for the value
    iteration steps described above. Unlike the off-policy example, we are not calculating
    the optimal *Q* based on a sample or exploratory selection of actions generated
    from the distribution *p* (sometimes known as the *behavioral policy*); instead,
    we are selecting actions based on a policy and potentially updating that policy
    as we learn Q.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the field of deep learning is independent of reinforcement learning methods
    such as the Q-learning algorithm, a powerful combination of these two approaches
    was applied in training algorithms to play arcade games at near-human level.¹
    A major insight in this research was to apply a deep neural network to generate
    vector representations from the raw pixels of the video game, rather than trying
    to explicitly represent some features of the "state of the game"; this neural
    network is the Q-function for this RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Another key development was a technique called *experience replay*, wherein
    the history of states (here, pixels from video frames in a game), actions, and
    rewards is stored in a fixed-length list and re-sampled at random repeatedly,
    with some stochastic possibility to choose a non-optimal outcome using the epsilon-greedy
    approach described above. The result is that the value function updates are averaged
    over many samples of the same data, and correlations between consecutive samples
    (which could make the algorithm explore only a limited set of the solution space)
    are broken. Further, this "deep" Q-learning algorithm is implemented *off-policy*
    to avoid the potential circular feedback of generating optimal samples with a
    jointly optimized policy function.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Q-learning is also *model-free*, in the sense we have no representation
    or model (such as a generative model that can simulate new frames of the game)
    of the environment *E*. In fact, as with the video game example, it could just
    be samples of historical data that represent the "internal state" of a game that
    is observed by a player.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting these pieces together, the deep Q-learning algorithm uses the following
    steps to learn to play Atari games^(14):'
  prefs: []
  type: TYPE_NORMAL
- en: Create a list to store samples of (current state, action, reward, next state)
    as a "replay memory."
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly initialize the weights in the neural network representing the Q-function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a certain number of gameplay sequences, initialize a starting game screen (pixels)
    and a transformation of this input (such as the last four screens). This "window"
    of fixed-length history is important because otherwise the Q-network would need
    to accommodate arbitrarily sized input (very long or very short sequences of game
    screens), and this restriction makes it easier to apply a convolutional neural
    network to the problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a certain number of steps (screens) in the game, use epsilon greedy sampling
    to choose the next action given the current screen and reward function computed
    through Q.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After updating the state, save this transition of (current state, action, reward,
    action, next state) into the replay memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose random sets of (current state, action, reward, next state) transitions
    from the replay memory, and compute their reward using the Q-function. Use stochastic
    gradient descent to update Q based on these transitions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue *steps 3-6* for many games and gameplay steps until the weights in the
    Q-network converge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While other applications of Q-learning have nuances tied to their specific
    domain, the general approach of using a deep neural network to approximate the
    Q-function on a large space of possible outcomes (rather than a small set of states
    and actions that can be represented in a table) has proved effective in many cases.
    Other examples in which deep Q-learning has been applied include:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing the positions on a Go (an East Asian game resembling chess) gameboard
    using a CNN and applying Q-learning to determine the next best move in the game
    based on historical examples from human players; a model named "AlphaGo" was published
    in 2015.^(15)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An innovation on the AlphaGo release in 2017, named "AlphaGo Zero," where the
    program learns entirely from synthetic games (two RL agents playing each other)
    rather than historical examples from human players.^(16)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A more general form of AlphaGo Zero, "AlphaZero," which also mastered the games
    of Chess and Shogi using self-play.^(17)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AlphaStar, an RL algorithm that can beat human masters in the multi-player real-time
    strategy game StarCraft.^(18)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model called "AlphaFold," which can predict the 3D structure of proteins from
    their 2D sequence – we will describe AlphaFold in more detail in *Chapter 13*,
    *Emerging Applications of Generative AI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We've thus far described Q-learning and its variants, and how deep learning
    augments Q-learning through deep Q-learning for complex environments and datasets.
    However, the problems we've described all share the common feature that we are
    able to express the reward function in defined mathematical terms (such as the
    score in a game). In many real-world scenarios – such as training an RL agent
    to drive a car – this reward function is not so easy to define. In these cases,
    rather than writing down a reward function, we might instead use examples of human
    drivers as an implicit representation of the reward – this is an approach known
    as inverse reinforcement learning, which we describe in more detail below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inverse reinforcement learning: Learning from experts'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example of deep Q-learning above relies upon an explicit reward function
    –the score in the game. We don't always have access to an explicit reward function
    though, including in important real-world scenarios such as self-driving cars.
    What "reward" value would we assign to a driver choosing to navigate one way or
    the other given the surrounding environment on the road? While we have an intuitive
    sense of what the "right" decision is, quantifying exhaustive rules for how we
    should score such a reward function would be challenging.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of trying to determine the reward function, we could instead observe
    an expert driver perform complex maneuvers such as merging in multi-lane traffic
    and optimize an agent whose behavior mimics that of the expert. This is a more
    general problem known as *imitation learning*. One form of imitation learning
    is *behavioral cloning*,^(19) which follows the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect a set of state, action pairs ![](img/B16176_12_066.png) from expert
    behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn a policy function ![](img/B16176_12_021.png), where ![](img/B16176_12_022.png)
    is a supervised classification algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While this approach is straightforward, it requires a large amount of data in
    order to create a classifier that generalizes to unseen environments, particularly
    when the number of potential environments (as is the case for self-driving cars)
    is very large.^(20)
  prefs: []
  type: TYPE_NORMAL
- en: In particular, the success of behavioral cloning depends on the future distribution
    of the environment being similar to the training data, which is particularly difficult
    when the results of the model can influence the distribution of later observations.
    For example, the choices made by a self-driving car on the road become further
    data for re-training the model, leading to a feedback loop of potentially compounding
    errors and data drift.^(21)
  prefs: []
  type: TYPE_NORMAL
- en: An alternative is to learn an agent that tries to match the outcomes of an expert's
    *entire trajectory* rather than individual actions, as in behavioral cloning.
    The output of this algorithm is then a function that scores "expert behavior"
    on a trajectory higher than novice behavior. This approach is known as **inverse
    reinforcement learning** (**IRL**) since it reverses the common pattern (*Figure
    12.3*) – on the left we see a typical feedback loop for an RL like we've described
    for the Atari-playing deep Q-network, where an agent (blue) observes a state (*s*)
    and using a reward function (*R*) chooses an action (*a*) that yields a transition
    (*T*) to a new state and a reward (*r*). In contrast, on the right, the rewards
    resulting from these states, actions, and transitions are represented implicitly
    by examples from an expert (*E*), and the agent (blue) instead learns to replicate
    this sequence through a learned reward function (*R*[E]) rather than being explicitly
    "in the loop" of the algorithm. In other words, instead of learning policy from
    an explicit reward function, we observe an expert's behavior and infer a reward
    function that would lead to their observed actions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Reinforcement learning (a) and inverse reinforcement learning
    (b)^(22)'
  prefs: []
  type: TYPE_NORMAL
- en: 'How could we quantify the behavior of an "expert" through learning a reward
    function from scratch? If we re-examine our prior example of video game playing,
    we could examine sequences of pixel screens *x* and actions *a* (*x*[1], *a*[1],
    *x*[2], *a*[2]…) from expert human play that form a complete session, and try
    to find a function *f* that would give us the total reward for a given game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_023.png)'
  prefs: []
  type: TYPE_IMG
- en: We could then ask whether the given function *f* tends to replicate the behavior
    of an expert player versus other alternatives. However, there are inherent problems
    – multiple *f*'s might give the same reward result, making it unclear which (of
    many possible solutions) would generalize best to new data.^(23)
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we could instead try to optimize an agent which will result
    in observed trajectories with the same probability as an expert; in other words,
    we would see the same distribution of sequences from following this agent as from
    drawing randomly from expert behavior, and the optimization algorithm is based
    on minimizing the difference between the proposed and the observed empirical distribution
    of sequences from the expert.^(24) This expected distribution (either observed
    or generated by the agent) can be represented by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_024.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *P* is the probability of trajectory (![](img/B16176_12_025.png)), and
    *f* are the features of a state, such as the observed pixels in the video game
    example. We've removed the problem of solving for an ambiguous reward function,
    but we still are faced with the possibility that many agents could lead to the
    same behavior. We might even need a mixture of different policy or reward functions
    in order to mimic a given expert behavior, and it is unclear how to select these.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Path trajectories from a (non) deterministic MDP. (b) is a particular
    path (trajectory) from the deterministic MDP (a), while (d) is a sample path from
    a non-deterministic MDP (c), where there is ambiguity as to which state action
    a[4] might lead to.^(23)'
  prefs: []
  type: TYPE_NORMAL
- en: We can appeal to the partition function and Boltzmann distribution that we saw
    in *Chapter 4*, *Teaching Networks to Generate Digits*, when studying restricted
    Boltzmann machines. If we take the MDP represented by an RL agent and "unroll"
    the trajectories from following a particular set of actions in response to a set
    of states, we get a set of variable length paths in a tree diagram as depicted
    in *Figure 12.4*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many different distributions over the occurrence of these paths could be obtained
    by following different policies, and even with the same distribution of pixel
    features some of these policies might prefer one path over another based on a
    particular reward function. To resolve this ambiguity, we could optimize a reward
    function with parameters ![](img/B16176_12_016.png) such that paths with the same
    reward function value based on these features receive the same preference, but
    we exponentially prefer a higher reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_027.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach is known as *maximum entropy*; recall from *Chapter 4*, *Teaching
    Networks to Generate Digits*, that the *Z* is the partition function, which normalizes
    *P* to be a probability density over average trajectories *f* composed of steps
    *s*[j] evaluated using a (here, linear) reward function ![](img/B16176_12_028.png).^(25)
    Even in the case of non-deterministic outcomes (*Figure 12.4*, (*b*) and (*d*)),
    which could be the case, for example, in a video game where some of the behavior
    of enemy spacecraft is randomly generated by the computer, we could parameterize
    this equation in terms of the transition distribution *T*:^(23)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the distribution is similar to before, but we''ve added the probability
    *P*[T] of observing outcome *o* based on actions *a* and states *s* using the
    transition model *T*. ![](img/B16176_12_030.png) denotes the indicator function,
    which evaluates to 1 when ![](img/B16176_12_031.png) and 0 otherwise. We could
    then optimize this equation to find the parameters of the reward function that
    maximize the likelihood of this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The gradients of this likelihood are just the difference between the expected
    feature counts (for example, distribution of pixels) from expert play and those
    obtained by following the proposed agent, based on the frequency of visiting a
    given state *D*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This gives us an objective to optimize, but there are options for what kind
    of *P* we would like to use. The one that we will study is the *maximal causal
    entropy*, where the probability of a given action (and thus, state and path distribution)
    is conditioned on the prior set of states and actions, just as in our prior video
    game example ^(26 27):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_034.png)![](img/B16176_12_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Because this entropy value could potentially be infinite if the path never
    terminates, we could apply a discount factor to make it finite^(26):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_036.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the discount factor *B* is applied to each time step *t*, with expectation
    over the distribution of initial states *P*[0] and subsequent states *P* subject
    to a policy ![](img/B16176_12_037.png); ![](img/B16176_12_038.png) denotes the
    causal entropy ![](img/B16176_12_039.png); and ![](img/B16176_12_040.png) is the
    causally conditioned probability = ![](img/B16176_12_041.png)(the probability
    conditioned on the prior actions in the sequence).
  prefs: []
  type: TYPE_NORMAL
- en: So at this point, we have an objective to optimize (discounted causal entropy),
    and the notion of computing a reward function for a particular agent that would
    make its behavior similar to an expert. What is the connection between this objective
    and generative AI? The answer is in how we can draw parallels between *discriminating*
    between an expert and learner, and generating samples from the learner – not unlike
    the GANs we studied in *Chapters 6*, *7*, and *8*!
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial learning and imitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a set of expert observations (of a driver, or a champion video game player),
    we would like to find a reward function that assigns high reward to an agent that
    matches expert behavior, and low reward to agents that do not. At the same time,
    we want to choose such an agent's policy, ![](img/B16176_12_042.png), under this
    reward function such that it is as informative as possible by maximizing entropy
    and preferring expert over non-expert choices. We'll show how both are achieved
    through an algorithm known as **Generative Adversarial Imitation Learning** (**GAIL**)
    published in 2016.^(20)
  prefs: []
  type: TYPE_NORMAL
- en: In the following, instead of a "reward" function, we use a "cost" function to
    match the conventions used in the referenced literature on this topic, but it
    is just the negative of the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting these constraints together, we get^(20):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the inner term, we are trying to find a policy that maximizes the discounted
    entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus leading to a large negative value, and minimizing the cost term. We want
    to maximize this whole expression over different potential cost functions by selecting
    one that not only satisfies the inner constraint but gives low cost expert-like
    behavior, thus maximizing the overall expression. Note that this inner term is
    also equivalent to an RL problem that seeks an agent whose behavior optimizes
    the objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'over the space of possible policies ![](img/B16176_12_046.png), denoted ![](img/B16176_12_047.png).
    However, in order to limit the space of possible choices of *c*, we apply a regularization
    function ![](img/B16176_12_048.png), which seeks a low-complexity *c* among many
    possible choices, and adds this constraint into the overall objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that we could alternate between optimizing *c* under a given policy
    (IRL) and optimizing a policy under that *c* (RL) – and that is exactly the approach
    we will follow. We can express the optimal policy, using the approach described
    before, as a measure of **occupancy** (![](img/B16176_12_050.png)) of different
    states under that policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the optimal point for the RL and IRL problem is equivalent to minimizing
    the difference between the learned and expert occupancy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_052.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/B16176_12_053.png) denotes a functional composition, ![](img/B16176_12_054.png),
    here representing that the RL agent uses the IRL output as one of its arguments.
    However, the examples of the expert behavior that we receive are often limited,
    so we don''t want to exactly mimic their distribution since that may cause the
    agent to overfit. Instead, we want to use a distance measure, such as the KL divergence
    or JS divergence we saw in *Chapter 6*, *Image Generation with GANs*, to quantify
    differences between the observed distribution of expert behavior and the distribution
    of actions taken by the IRL agent which tries to approximate that behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_055.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What regularizer function, ![](img/B16176_12_056.png), should be used in this
    algorithm? The authors of the GAIL algorithm present a function that assigns low
    penalty if the cost is low, and high penalty otherwise. A regularizer function
    with this property, for which the full derivation is presented in the paper "Generative
    Adversarial Imitation Learning," is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_057.png) where ![](img/B16176_12_058.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The intuition for this function is that it minimizes the difference between
    the causal entropy of the policy *H* with a penalty ![](img/B16176_12_059.png)
    (for example, the likelihood of the learned policy) and the difference between
    the expert and learned policy state occupancies ![](img/B16176_12_060.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_061.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This ![](img/B16176_12_062.png) or ![](img/B16176_12_063.png) can be written
    as the negative log loss of a binary classifier that distinguishes between expert
    (0) and learned (1) action-states pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expression probably looks quite familiar from *Chapter 6*, *Image Generation
    with GANs*, as it resembles the objective function of a GAN as well for discriminating
    between generated and real data! Putting these terms together (the regularizer
    and the causal entropy), we see that the complete objective function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_065.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To optimize this objective function, the GAIL algorithm utilizes the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare a set of expert trajectories and randomly initialize the discriminator
    and policy parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a set of trajectories for the RL agent under the current policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the discriminator parameters with a stochastic gradient descent step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy parameters with gradient-based updates using an algorithm
    called **Trust Region Policy Optimization** (**TRPO**) published in 2015 – please
    consult the original article "Trust Region Policy Optimization" for more details
    on the form of this gradient update.^(28)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2-4* of the algorithm until the values of the parameters of the
    policy and the discriminator converge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we've explained the intuition behind using generative AI in reinforcement
    learning, let's dive into a practical example of training a walking humanoid in
    a virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: Running GAIL on PyBullet Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our code example in this chapter, we will train a virtual agent to navigate
    a simulated environment – in many RL papers, this environment is simulated using
    the Mujoco framework ([http://www.mujoco.org/](http://www.mujoco.org/)). **Mujoco**
    stands for **Multi joint dynamics with contacts** – it is a physics "engine" that
    allows you to create an artificial agent (such as a pendulum or bipedal humanoid),
    where a "reward" might be an ability to move through the simulated environment.
  prefs: []
  type: TYPE_NORMAL
- en: While it is a popular framework used for developing reinforcement learning benchmarks,
    such as by the research group OpenAI (see [https://github.com/openai/baselines](https://github.com/openai/baselines)
    for some of these implementations), it is also closed source and requires a license
    for use. For our experiments, we will use PyBullet Gymperium ([https://github.com/benelot/pybullet-gym](https://github.com/benelot/pybullet-gym)),
    a drop-in replacement for Mujoco that allows us to run a physics simulator and
    import agents trained in Mujoco environments.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Examples from the Mujoco simulated environments (http://www.mujoco.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Pybullet-Gym, you need to first install OpenAI Gym, using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then install Pybullet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To show how this simulated environment works, let''s create a "hopper," one
    of the many virtual agents you can instantiate with the library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If the commands execute correctly, we will see the following output, an array
    giving the current observation (11-dimensional vector) vector of the walker.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Observation vector of the walker'
  prefs: []
  type: TYPE_NORMAL
- en: 'The call to `render("human")` will create a window showing the "hopper," a
    simple single-footed figure which moves in a simulated 3D environment (*Figure
    12.7*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: PyGym hopper'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can run a few iterations of the hopper in its raw, "untrained" form, to
    get a sense of how it looks. In this simulation, we take up to 1,000 steps and
    visualize it using a pop-up window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We first clear the environment with `reset()`. Then, for up to 1,000 timesteps,
    we sample the action space (for example, the *x*, *y*, and *z* coordinates representing
    movements of the walking figure within the virtual environment). Then, we use
    that action to get an updated reward and observation, and render the result, until
    the movement completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This demonstration comes from a completely untrained hopper. For our GAIL implementation,
    we will need a hopper that has been successfully trained to walk as a sample of
    "expert" trajectories for the algorithm. For this purpose, we''ll download a set
    of hopper data from the OpenAI site, at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://drive.google.com/drive/folders/1h3H4AY_ZBx08hz-Ct0Nxxus-V1melu1U](https://drive.google.com/drive/folders/1h3H4AY_ZBx08hz-Ct0Nxxus-V1melu1U)'
  prefs: []
  type: TYPE_NORMAL
- en: These contain a set of NumPy files, such as `deterministic.trpo.Hopper.0.00.npz`,
    that contain samples of data from reinforcement learning agents trained using
    the Trust Region Policy Optimization algorithm used in *Step 4* of the GAIL algorithm
    we discussed earlier.^(28)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we load this data, we can also visualize it using the Pybullet simulator,
    but this time we will see steps from the expert, rather than the random baseline
    agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code loads the pre-trained hopper, initializes the virtual environment,
    and takes a maximum of 1,000 steps in which an action (the next move of the hopper)
    is determined using the hopper''s trained policy function, and the environment
    state (position of the hopper) is updated based on that action. Note that here
    the policy function is deterministic, leading to the same outcome for any given
    action at time *t*. You can see the hopper now taking many steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: The hopper moving with expert policy trained by TRPO'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a closer look at what kinds of data are in this NumPy object we''ve
    loaded. You''ll notice that the format, `.npz`, is a gzipped archive of compressed
    files. We can see the names of these archives by using the files parameter of
    the object `mujoco_hopper_np`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'which gives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The observations are 11-dimensional objects, which you can verify by looking
    at the dimension of `obs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This array has 1,500 examples, each of 1,000 timesteps, which each have 11 dimensions
    representing different physical quantities (position of hinges, torque, and so
    on). The objective of the hopper task is to move the hopper forward as fast as possible,
    so the reward function is higher when the agent learns to move forward. If we
    examine the `acs` data, we see that it has three dimensions, corresponding to
    points in 3D space. This is a continuous action space, unlike the discrete examples
    we have discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `ep_rets` corresponds to the predicted reward in the future for an action
    at time t, and the rewards `rews` are the output of the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent: Actor-Critic network'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create our GAIL implementation, we''ll first need to specify an agent.^(29)
    This is an Actor-Critic architecture, consisting of two networks: one which learns
    the "value" of an observation (Critic), and another (Actor) which samples actions
    based on observations. These networks could be independent or share parameters;
    for our experiment, we''ll have them share hidden and input layers but have separate
    output layers (*Figure 12.9*).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Actor-Critic architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Note the code for the GAIL implementation in this chapter is based on [https://github.com/fangihsiao/GAIL-Tensorflow/blob/master/tf_gail.ipynb](https://github.com/fangihsiao/GAIL-Tensorflow/blob/master/tf_gail.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Below we define the `ActorCritic` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This class initializes a network that accepts input states and action pairs
    and generates two outputs – one generates new actions (3D coordinates representing
    the next move of the hopper in the virtual space) – the Actor – and the other
    generates values (how successful the movement is in navigating the hopper farther
    in the virtual space) – the Critic. The value output is a single scalar that increases
    with the quality of the hopper's movement, while the action is a 3-unit vector
    representing a move in 3D space with a mean and standard deviation per coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Because our network has multiple outputs, we need to be careful about setting
    the input layers and initializing them. Notice that we explicitly call build on
    the two output layers, rather than letting them be automatically instantiated
    in the forward pass, as this will cause errors in compiling the model. We also
    instantiate a variable, `_action_dist_std`, to contain the standard deviation
    of the action dimensions, which we'll use to sample new coordinates in the model.
    We've also included `BatchNormalization` layers to prevent the gradients from
    exploding or vanishing in our network.^(29)
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to be able to return the trainable parameters in the model for
    our gradient calculation, using the `get_params` method of the Actor-Critic network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In our forward pass, we calculate the output of the Critic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To sample new actions (3D moves) from the Actor, we run the sample function
    with the argument `''action''` – if we supply `''entropy''` instead, we return
    the entropy of the action distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need to be able to return the log probability of the action distribution
    (for our loss function) for the PPO network, described below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our IRL agent – for which we will use **Proximal Policy Optimization** (**PPO**)
    policy updates, an improvement on TRPO published in 2017 ^(29) – makes use of
    this Actor-Critic network as the "policy" function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This class initializes a neural network (`_policy`) and provides a placeholder
    for updates to that network (`_new_policy`), so that with each step of the algorithm
    we can update the new policy with reference to its improvement over the last policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function inside the `train_policy` loop is optimized using a gradient
    descent algorithm where the magnitude of the gradients is constrained to a fixed
    range ("clipped") so that large gradients don''t lead the loss function (and weights)
    to change drastically between rounds of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this loss function, we first take a ratio between the old policy (current
    parameters of the Actor-Critic network) and a potential update (new policy) –
    the exponential of the difference of their log probabilities (which is the likelihood
    of the observed actions under the action distribution of each network) gives a
    ratio. If the new, proposed network is an improvement (its parameters fit the
    observed action sequence better), the ratio is greater than 1\. Otherwise, the
    proposal is unchanged in quality (a ratio of 1) or worse (a ratio less than 1)
    than the current parameters of the Actor-Critic.
  prefs: []
  type: TYPE_NORMAL
- en: 'We multiply this ratio by the "advantages," which are the difference between
    returns (the Q-function we described earlier) and the current values of the Actor-Critic''s
    existing state. In this GAIL implementation, we compute advantages through Generalized
    Advantage Estimation^(29), which uses an exponentially smoothed estimate of the
    Q-function, where `gamma` (coefficient) and `tau` (exponents) control how much
    the estimate of future reward decays in the future relative to no future information
    (`tau` = 0) or no decay of importance of future data versus present (`tau` = 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The loss function above then uses the advantages multiplied by `surr` (the "surrogate"
    term) and computes two values – the first being the actor loss, which constrains
    the loss term given by the advantages to be within a given range, denoted by the
    `clip_by_value` function. This prevents extremes (much less than 1 or greater
    than 1) of the probability ratio of the new and old policy from making the loss
    function unstable. To this, we add the critic loss, the squared difference between
    the critic value and the advantage function we computed above. Taking a weighted
    sum of the actor and critic functions and the entropy of the action probability
    distribution (whether it assigns high value to a subset of positions, and thus
    contains "information" about the distribution of potential actions) gives the
    overall quality of the policy as an objective for the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `actor_loss` is multiplied by negative 1 (because it is the ratio
    between the old and new policy probability – thus if it improves it is greater
    than 1, but the loss function should be minimized, thus a larger negative value).
    Similarly, the entropy term has more information if it is larger, and we take
    the negative of this as well as we minimize the loss function. The critic loss
    becomes better the closer to 0 it is, so we leave this term as positive.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use this loss function, we define a custom train function called `train_policy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We use the `get_params()` function to extract the trainable parameters of the
    PPO policy network, "watch" them using `GradientTape`, and compute the loss using
    the loss function above. Also, since the Actor-Critic has two outputs (action
    and value), only one of which (value) is affected by a reward update, we could
    have nonexistent gradients, which is why we replace any empty gradients with a
    0.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each step of the GAIL inner loop which we described above, we also need
    to be able to replace the old policy with the new, using a deep copy (a copy that
    creates a new variable with the same values, rather than a pointer to the original
    variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can use calls to the Actor-Critic policy network to get the value
    (reward) estimate and sample new actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The discriminator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With our PPO algorithm, we have the "agent" which we are trying to teach to
    behave like an expert. We can sample from our TRPO trained examples of the hopper
    that we downloaded as a "generator."
  prefs: []
  type: TYPE_NORMAL
- en: 'So we now just need a discriminator network, which seeks to distinguish the
    expert behavior from the agent we are training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Like the Actor-Critic, this is a 3-layer neural network with `BatchNormalization`
    applied between layers. Its single output indicates the quality of the input (like
    the value function of the Actor-Critic), and should be lower when the network
    is more "expert-like." Notice that to get the "reward" output to match the sign
    of the Actor-Critic value output, we reverse the sign of the discriminator because
    it predicts closer to 0 for expert observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This transformation is applied to the same `call` function we saw earlier for
    the Actor-Critic network.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Training and results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train the network, we apply a loss function that tries to classify expert
    (observation, action) pairs as 0s and agent (observation, action) pairs as 1s.
    When the agent learns to generate high quality (observation, action) pairs that
    resemble the expert, the discriminator will have increasing difficulty distinguishing
    between samples from the agent and expert, and will assign agent samples a label
    of 0 as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we extract the parameters of the network through `get_params()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We then apply our loss function to these parameters using `train_discriminator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we need an update function for our PPO minibatch step, where we randomly
    sample observations from the agent in each inner loop of the GAIL algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We also want to be able to plot the progress of our agent as we train it, for
    which we''ll take samples from the environment using a model and plot the average
    reward and the performance of the discriminator (how closely the agent and expert discriminator
    reward matches):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This function makes two plots, shown in *Figure 12.10*. On the left is the
    reward for a set of test observations for the agent, which should increase as
    the agent becomes better at moving the hopper. On the right is plotted, for each
    of n-steps of a sample agent and expert movement of the hopper, how well the discriminator
    can tell the difference between the two (whether the orange and blue lines overlap,
    which is optimal, or are far apart, in which case the GAIL algorithm hasn''t converged):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: Agent reward for a series of test observations (left), discriminator
    reward (right). Plots generated by plot().'
  prefs: []
  type: TYPE_NORMAL
- en: 'The test samples are generated using the `test_env` function, which resembles
    the Pybullet simulations we saw above – it uses the current agent and computes
    n-steps of the simulation under the current policy, returning the average reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For our main function, we''ll set a number of maximum simulation steps and
    the hyperparameters of the algorithm, including the learning rate of the ADAM
    optimizer (`lr`), the number of units in the hidden layer of each network, the
    number of steps to run each hopper simulation for the agent (`num_steps`), the
    number of samples to choose in each Agent update (minibatch size), the number
    of steps to run each inner loop updating the Agent (`ppo_epochs`), the overall
    max number of steps in the algorithm (`max_frames`), and a container to hold the
    out-of-sample rewards estimates that we showed how to plot above (`test_rewards`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'First we initialize the discriminator and PPO networks, set a counter for the
    inner loop for Agent update (`i_update`), and set up the Pybullet environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'At each step, we''ll compute a number of timesteps using our current policy,
    and create a list of these observations, actions, and rewards. At regular intervals,
    we will plot the results using the functions we described above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice that if our simulation is terminated, we "mask" any observations after
    completion so that they don't affect our gradient calculation. Also note that
    we are converting the NumPy data from the sample into tensors using the `tf.convert_to_tensor`
    and `tf.reshape` functions. We use the `compute_gae` function described above
    to smooth out the estimated advantage function.
  prefs: []
  type: TYPE_NORMAL
- en: We then periodically (here, every 3 cycles) compute an update to the PPO policy
    using minibatch updates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we sample an expert trajectory from our previously loaded data in
    the TRPO policy, concatenate the observations and actions from the expert and
    new policy, and make a gradient update of the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that you can observe the hopper being trained in the simulation window
    while the algorithm is running – the difficulty of finding a mathematical definition
    of "reward" is shown by the fact that during training, we can see the hopper making
    creative movements (such as crawling, *Figure 12.11*) that allow it to move in
    the simulation space (and thus receive reward) but are not like the expert "hopping"
    motion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: Hopper "crawling" during GAIL training'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored another application of generative models in reinforcement
    learning. First, we described how RL allows us to learn the behavior of an agent
    in an environment, and how deep neural networks allowed Q-learning to scale to
    complex environments with extremely large observation and action spaces.
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed inverse reinforcement learning, and how it varies from RL
    by "inverting" the problem and attempting to "learn by example." We discussed
    how the problem of trying to compare a proposed and expert network can be scored
    using entropy, and how a particular, regularized version of this entropy loss
    has a similar form as the GAN problem we studied in *Chapter 6*, called GAIL (Generative
    Adversarial Imitation Learning). We saw how GAIL is but one of many possible formulations
    of this general idea, using different loss functions. Finally, we implemented
    GAIL using the bullet-gym physics simulator and OpenAI Gym.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we will conclude by exploring recent research in generative
    AI in diverse problem domains including bioinformatics and fluid mechanics, providing
    references for further reading as you continue your discovery of this developing field.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). *Playing Atari with Deep Reinforcement Learning*.
    arXiv. [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bareketain, P. (2019, March 10). Understanding *Stabilising Experience Replay
    for Deep Multi-Agent Reinforcement Learning*. Medium. [https://medium.com/@parnianbrk/understanding-stabilising-experience-replay-for-deep-multi-agent-reinforcement-learning-84b4c04886b5](https://medium.com/@parnianbrk/understanding-stabilising-experience-replay-for-deep-multi-agent-rein)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wikipedia user waldoalverez, under a CC BY-SA 4.0 license ([https://creativecommons.org/licenses/by-sa/4.0/](https://creativecommons.org/licenses/by-sa/4.0/)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Amit, R., Meir, R., & Ciosek, K. (2020). *Discount Factor as a Regularizer in
    Reinforcement Learning*. Proceedings of the 37th International Conference on Machine
    Learning, Vienna, Austria, PMLR 119, 2020\. [http://proceedings.mlr.press/v119/amit20a/amit20a.pdf](http://proceedings.mlr.press/v119/amit20a/amit20a.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matiisen, T. (2015, December 19). *Demystifying Deep Reinforcement Learning*.
    Computational Neuroscience Lab. [https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/](https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bellman, R.E. (2003) [1957]. *Dynamic Programming*. Dover.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Watkins, C.J.C.H. (1989), *Learning from Delayed Rewards* (PDF) (Ph.D. thesis),
    Cambridge University, [http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sutton, R., & Barto, A. (1998). *Reinforcement Learning: An Introduction*.
    MIT Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Melo, F.S. *Convergence of Q-Learning: A simple proof*. [http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf](http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Watkins, C.J.C.H., & Dayan, P. (1992). *Q-learning*. Machine learning, 8(3-4):279-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nair, A., Dalal, M., Gupta, A., & Levine, S. (2020). *Accelerating Online Reinforcement
    Learning with Offline Datasets*. arXiv. [https://arxiv.org/abs/2006.09359](https://arxiv.org/abs/2006.09359)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sutton, R.S. & Barto A.G. (2018). *Reinforcement Learning: An Introduction*
    (2nd ed.). MIT Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rummery, G.A., & Niranjan, M. (1994). *On-line Q-learning using Connectionist
    Systems*. Cambridge University Engineering Department.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
    D., & Riedmiller, M. (2013). *Playing Atari with Deep Reinforcement Learning*.
    arXiv. [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van den Driessche,
    G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,
    S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach,
    M., Kavukcuoglu, K., Graepel, T., & Hassabis, D. (2016). *Mastering the game of
    Go with deep neural networks and tree search*. *Nature* 529, 484–489\. [https://www.nature.com/articles/nature16961](https://www.nature.com/articles/nature16961)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
    A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui,
    F., Sifre, L., van den Driessche, G., Graepel, T., & Hassabis, D. (2017). *Mastering
    the game of Go without human knowledge*. *Nature* 550, 354-359\. [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
    Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simonyan, K.,
    & Hassabis, D. (2017). *Mastering Chess and Shogi by Self-Play with a General
    Reinforcement Learning Algorithm*. arXiv. [https://arxiv.org/abs/1712.01815](https://arxiv.org/abs/1712.01815)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vinyals, O., Babuschkin, I., Silver. D. et al. (2019). *Grandmaster level in
    StarCraft II using multi-agent reinforcement learning*. *Nature* 575, 350-354\.
    [https://www.nature.com/articles/s41586-019-1724-z](https://www.nature.com/articles/s41586-019-1724-z)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pomerleau, D.A. (1991). *Efficient training of artificial neural networks for
    autonomous navigation*. Neural Computation, 3(1):88-97\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ho, J., & Ermon, S. (2016). *Generative Adversarial Imitation Learning*. arXiv.
    [https://arxiv.org/abs/1606.03476](https://arxiv.org/abs/1606.03476)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ross, S., & Bagnell, J.A. (2010). *Efficient Reductions for Imitation Learning*.
    Proceedings of the Thirteenth International Conference on Artificial Intelligence
    and Statistics, PMLR 9:661-668
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Based on a figure from Arora, S., & Doshi, P. (2018). *A Survey of Inverse
    Reinforcement Learning: Challenges, Methods and Progress*. arXiv. [https://arxiv.org/abs/1806.06877](https://arxiv.org/abs/1806.06877)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on a figure from Ziebart, B.D., Maas, A., Bagnell, J.A., & Dey, A.K. (2008).
    *Maximum Entropy Inverse Reinforcement Learning*. Proceedings of the Twenty-Third
    AAAI Conference on Artificial Intelligence (2008)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abbeel, P., & Ng, A. Y. (2004). *Apprenticeship learning via inverse reinforcement
    learning*. In Proc. ICML, 1–8.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jaynes, E. T. (1957). *Information theory and statistical mechanics*. Physical
    Review 106:620–630.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Bloem and N. Bambos, *Infinite time horizon maximum causal entropy inverse
    reinforcement learning*, 53rd IEEE Conference on Decision and Control, 2014, pp.
    4911-4916, doi: 10.1109/CDC.2014.7040156\. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.720.5621&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.720.5621&rep=rep1&type=pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kramer, G. (1998). *Directed information for channels with feedback*. PhD dissertation,
    Swiss Federal Institute of Technology, Zurich. [http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=91AB2ACCC7B3CD1372C2BC2EA267ECEF?doi=10.1.1.728.3888&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=91AB2ACCC7B3CD1372C2BC2EA267ECEF?doi=10.1.1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schulman, J., Levine, S., Moritz, P., Jordan, M.I., & Abbeel, P. (2015). *Trust
    Region Policy Optimization*. arXiv. [https://arxiv.org/abs/1502.05477](https://arxiv.org/abs/1502.05477)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ioffe, S., & Szegedy, C. (2015). *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*. arXiv. [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). *Proximal
    Policy Optimization Algorithms*. arXiv. [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schulman, J., Moritz, P., Levine, S., Jordan, M., & Abbeel, P. (2015). *High-Dimensional
    Continuous Control Using Generalized Advantage Estimation*. arXiv. [https://arxiv.org/abs/1506.02438](https://arxiv.org/abs/1506.02438)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
