["```py\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport random\nimport math\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n```", "```py\ndata = np.random.randn(500, 2)\n```", "```py\nprint data[0]\n\narray([-0.08575873,  0.45157591])\n```", "```py\nprint data[0,0]\n\n-0.08575873243708057\n```", "```py\nprint data[0,1]\n\n0.4515759149158441\n```", "```py\ntheta = np.zeros(2)\n```", "```py\nprint theta\n\narray([0., 0.])\n```", "```py\ndef loss_function(data,theta):\n```", "```py\n    m = theta[0]\n    b = theta[1]\n\n    loss = 0\n```", "```py\n    for i in range(0, len(data)):\n```", "```py\n        x = data[i, 0]\n        y = data[i, 1]\n```", "```py\n        y_hat = (m*x + b)\n```", "```py\n        loss = loss + ((y - (y_hat)) ** 2)\n```", "```py\n    mse = loss / float(len(data))\n\n    return mse\n```", "```py\nloss_function(data, theta)\n\n1.0253548008165727\n```", "```py\ndef compute_gradients(data, theta):\n```", "```py\n    gradients = np.zeros(2)\n```", "```py\n    N = float(len(data))  \n```", "```py\n    m = theta[0]\n    b = theta[1]\n```", "```py\n    for i in range(0, len(data)):\n```", "```py\n        x = data[i, 0]\n        y = data[i, 1]\n```", "```py\n        gradients[0] += - (2 / N) * x * (y - (( m* x) + b))\n```", "```py\n        gradients[1] += - (2 / N) * (y - ((theta[0] * x) + b))\n```", "```py\n    epsilon = 1e-6 \n    gradients = np.divide(gradients, N + epsilon)\n\n    return gradients\n```", "```py\ncompute_gradients(data,theta)\n\narray([-9.08423989e-05,  1.05174511e-04])\n```", "```py\nnum_iterations = 50000\n```", "```py\nlr = 1e-2\n```", "```py\nloss = []\n```", "```py\ntheta = np.zeros(2)\n\nfor t in range(num_iterations):\n\n    #compute gradients\n    gradients = compute_gradients(data, theta)\n\n    #update parameter\n    theta = theta - (lr*gradients)\n\n    #store the loss\n    loss.append(loss_function(data,theta))\n```", "```py\nplt.plot(loss)\nplt.grid()\nplt.xlabel('Training Iterations')\nplt.ylabel('Cost')\nplt.title('Gradient Descent')\n```", "```py\ndef minibatch(data, theta, lr = 1e-2, minibatch_ratio = 0.01, num_iterations = 1000):\n```", "```py\n    minibatch_size = int(math.ceil(len(data) * minibatch_ratio))\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        sample_size = random.sample(range(len(data)), minibatch_size)\n        np.random.shuffle(data)\n```", "```py\n        sample_data = data[0:sample_size[0], :]\n```", "```py\n        grad = compute_gradients(sample_data, theta)\n```", "```py\n        theta = theta - (lr * grad)\n\n return theta\n```", "```py\ndef momentum(data, theta, lr = 1e-2, gamma = 0.9, num_iterations = 1000):\n```", "```py\n    vt = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta)\n```", "```py\n        vt = gamma * vt + lr * gradients\n```", "```py\n        theta = theta - vt\n\n return theta\n```", "```py\ndef NAG(data, theta, lr = 1e-2, gamma = 0.9, num_iterations = 1000):\n```", "```py\n    vt = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta - gamma * vt)\n```", "```py\n        vt = gamma * vt + lr * gradients\n```", "```py\n        theta = theta - vt\n\n    return theta\n```", "```py\ndef AdaGrad(data, theta, lr = 1e-2, epsilon = 1e-8, num_iterations = 10000):\n```", "```py\n    gradients_sum = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta) \n```", "```py\n        gradients_sum += gradients ** 2\n```", "```py\n        gradient_update = gradients / (np.sqrt(gradients_sum + epsilon))\n```", "```py\n        theta = theta - (lr * gradient_update)\n\n    return theta\n```", "```py\ndef AdaDelta(data, theta, gamma = 0.9, epsilon = 1e-5, num_iterations = 1000):\n```", "```py\n    # running average of gradients\n    E_grad2 = np.zeros(theta.shape[0])\n\n    #running average of parameter update\n    E_delta_theta2 = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta) \n```", "```py\n        E_grad2 = (gamma * E_grad2) + ((1\\. - gamma) * (gradients ** 2))\n```", "```py\n        delta_theta = - (np.sqrt(E_delta_theta2 + epsilon)) / (np.sqrt(E_grad2 + epsilon)) * gradients\n```", "```py\n        E_delta_theta2 = (gamma * E_delta_theta2) + ((1\\. - gamma) * (delta_theta ** 2))\n```", "```py\n        theta = theta + delta_theta\n\n    return theta\n```", "```py\ndef RMSProp(data, theta, lr = 1e-2, gamma = 0.9, epsilon = 1e-6, num_iterations = 1000):\n```", "```py\n    E_grad2 = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta) \n```", "```py\n        E_grad2 = (gamma * E_grad2) + ((1\\. - gamma) * (gradients ** 2))\n```", "```py\n        theta = theta - (lr / (np.sqrt(E_grad2 + epsilon)) * gradients)\n    return theta\n```", "```py\ndef Adam(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.9, epsilon = 1e-6, num_iterations = 1000):\n```", "```py\n    mt = np.zeros(theta.shape[0])\n    vt = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta) \n```", "```py\n        mt = beta1 * mt + (1\\. - beta1) * gradients\n```", "```py\n        vt = beta2 * vt + (1\\. - beta2) * gradients ** 2\n```", "```py\n        mt_hat = mt / (1\\. - beta1 ** (t+1))\n```", "```py\n        vt_hat = vt / (1\\. - beta2 ** (t+1))\n```", "```py\n        theta = theta - (lr / (np.sqrt(vt_hat) + epsilon)) * mt_hat\n\n    return theta\n```", "```py\ndef Adamax(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-6, num_iterations = 1000):\n```", "```py\n    mt = np.zeros(theta.shape[0])\n    vt = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta) \n```", "```py\n        mt = beta1 * mt + (1\\. - beta1) * gradients\n```", "```py\n        vt = np.maximum(beta2 * vt, np.abs(gradients))\n```", "```py\n        mt_hat = mt / (1\\. - beta1 ** (t+1))\n```", "```py\n        theta = theta - ((lr / (vt + epsilon)) * mt_hat)\n\n    return theta\n```", "```py\ndef AMSGrad(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.9, epsilon = 1e-6, num_iterations = 1000):\n```", "```py\n    mt = np.zeros(theta.shape[0])\n    vt = np.zeros(theta.shape[0])\n    vt_hat = np.zeros(theta.shape[0])\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n        gradients = compute_gradients(data, theta) \n```", "```py\n        mt = beta1 * mt + (1\\. - beta1) * gradients\n```", "```py\n       vt = beta2 * vt + (1\\. - beta2) * gradients ** 2\n```", "```py\n        vt_hat = np.maximum(vt_hat,vt)\n```", "```py\n        mt_hat = mt / (1\\. - beta1 ** (t+1))\n```", "```py\n          theta = theta - (lr / (np.sqrt(vt_hat) + epsilon)) * mt_hat\n\n    return theta\n```", "```py\ndef nadam(data, theta, lr = 1e-2, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-6, num_iterations = 500):\n```", "```py\n    mt = np.zeros(theta.shape[0])\n    vt = np.zeros(theta.shape[0])\n```", "```py\n    beta_prod = 1\n```", "```py\n    for t in range(num_iterations):\n```", "```py\n\n        gradients = compute_gradients(data, theta)\n```", "```py\n        mt = beta1 * mt + (1\\. - beta1) * gradients\n```", "```py\n       vt = beta2 * vt + (1\\. - beta2) * gradients ** 2\n```", "```py\n        beta_prod = beta_prod * (beta1)\n```", "```py\n        mt_hat = mt / (1\\. - beta_prod)\n```", "```py\n        g_hat = grad / (1\\. - beta_prod)\n```", "```py\n        vt_hat = vt / (1\\. - beta2 ** (t))\n```", "```py\n        mt_tilde = (1-beta1**t+1) * mt_hat + ((beta1**t)* g_hat)\n```", "```py\n        theta = theta - (lr / (np.sqrt(vt_hat) + epsilon)) * mt_hat\n\n return theta\n```"]