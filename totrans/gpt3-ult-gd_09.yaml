- en: 'Chapter 4: GPT-3 as Enabler for Next-Gen Startups'
  prefs: []
  type: TYPE_NORMAL
- en: Before the release of GPT-3, most people’s interaction with AI was limited to
    certain specific tasks, like asking Alexa to play your favorite song or using
    Google Translate to converse in different languages. Researchers have successfully
    developed AIs capable of performing mundane tasks, but so far, AIs have yet to
    match humans’ creative potential in performing abstract tasks without clear, well-defined
    instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the era of LLMs around the corner, we are looking at a significant paradigm
    shift. LLMs have shown us that by increasing the size of models, they can perform
    creative and complex tasks similar to humans. Now the biggest question: Is AI
    capable of performing creative activities?'
  prefs: []
  type: TYPE_NORMAL
- en: The creative potential of AI has always been an exciting area of research, though
    mostly hidden behind the tight R&D walls of companies like Google and Facebook
    of the world. GPT-3 is changing how we interact with AI and empowering people
    to build the next generation of applications that seemed like a far-fetched idea
    before its release.
  prefs: []
  type: TYPE_NORMAL
- en: Model-as-a-Service
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will show you how GPT-3 is powering the next wave of startups
    by fueling the imaginations of creative entrepreneurs with the right technology.
    We will also look at how AI research is progressing into commercialization in
    several domains. We’ll also speak with one of the venture capitalists backing
    these initiatives to understand the financial aspects of the burgeoning GPT-3
    economy.
  prefs: []
  type: TYPE_NORMAL
- en: The story of how OpenAI API was created resembles many of the stories of startups
    and companies in this chapter. We interviewed Peter Welinder, VP of Product and
    Partnerships at OpenAI. What he told us was a story of bold experimentation, rapid
    iteration, and leveraging smart design to achieve economies of scale (delivering
    powerful models on a large scale for as little cost as possible).
  prefs: []
  type: TYPE_NORMAL
- en: 'Welinder summarizes OpenAI’s mission in three key points: “Develop AGI (artificial
    general intelligence), make sure it''s safe, and then lastly deploy it into the
    world to make it maximize the benefit to all of humanity.” Thus the company is
    focusing on developing AI that can be applied to a more and more general range
    of needs.'
  prefs: []
  type: TYPE_NORMAL
- en: Hoping to achieve AGI as quickly and safely as possible, one of the technologies
    on which OpenAI decided to gamble was large language models, specifically GPT-3\.
    Welinder says of trying GPT-3, “That was the first time where we had something
    that we felt like, ‘Actually, this seems to be fairly useful, it's getting state-of-the-art
    results on a number of tasks in academic benchmarks and so on.’”
  prefs: []
  type: TYPE_NORMAL
- en: 'Excited at the possibilities, Welinder and four colleagues debated how best
    to use the algorithm: Building a translation engine? A writing assistant? A customer-service
    application? Then it hit them. Welinder says:  “Why not instead just provide this
    technology as an API and let any developers build their own business on top of
    it?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'The API approach aligned with OpenAI’s goals and mission by maximizing the
    technology’s adoption and impact, empowering community members to invent applications
    that the OpenAI team could not have predicted. This also leaves product development
    to skilled developers worldwide, freeing up the OpenAI team to focus on what they
    are truly good at: developing robust, groundbreaking models.'
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, the researchers had focused on designing scalable, efficient
    training systems to squeeze maximum efficiency out of the GPUs. But there had
    been little focus on actually running these models on actual data and getting
    something out of them for real-world applications. So the OpenAI team decided
    to double down on the core API experience, focusing on aspects like fast inference
    and low latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Six months before they planned to launch the beta version of the API, they
    had, according to Welinder, reduced latency to around ten times and increased
    throughput by hundreds of times: “We spent a ton of engineering to really take
    these models, make sure that their GPUs are as efficient as possible, make calls
    to them with really low latency, and make it scalable.” Using the model via API
    instead of needing your own GPUs makes it cost-effective and accessible for ordinary
    developers to play with use cases and try new things. Very low latency is important
    as well, to make it easy to iterate. “You don''t want to put something in and
    then wait for minutes to get the response back, which was the case in the very
    earliest days of the API. And now you can see the model output stuff in real-time,”
    Welinder says.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI believed that the models would grow, making it difficult for developers
    to deploy them; the team wanted to remove this barrier. “It's just going to cost
    you too much because you need so many GPUs and CPUs to play with a use case. It's
    not going to make economic sense for you to deploy this model by yourself,” Welinder
    says. Instead, the company decided to share the model with developers via the
    API. “Thousands of developers are using the same models, and that's the way you
    can reach economies of scale,” Welinder adds. “And that lowers the prices for
    everybody to access these models and further widens the distribution, so more
    people can try out these models.”
  prefs: []
  type: TYPE_NORMAL
- en: Releasing the OpenAI API in a private beta brought quite a few surprises. Their
    previous marquee model, GPT-2, had brought very few real-world use cases to life,
    so the team hoped GPT-3 would prove more useful. It did, and very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another surprise, Welinder says, was that “a lot of people on our platform
    weren''t programmers. They were authors, creatives of various kinds, they were
    designers and product managers, and so on.” GPT-3 in a way, changed what it means
    to be a developer: suddenly, it turns out that to build an AI application, you
    don’t need to know how to program. You just need to be good at describing what
    you want the AI to do using prompts (as discussed in Chapter 2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Welinder and his team found that  “oftentimes people that were really good
    at it had no machine learning background”-- and those who did had to unlearn how
    they thought about a lot of problems to use GPT-3\. Many users built GPT-3-based
    applications without code. The OpenAI team had, without really intending to, lowered
    the barriers to creating applications: a first step towards democratizing AI.
    “The core strategy is to make API usable for as many people as possible, Welinder
    says: "It''s core to our mission to make sure that the barrier to use our technology
    is low. That''s why we built this API.” Another unexpected use case of GPT-3 has
    been coding. Early signs of the model’s coding potential led OpenAI to double
    down on designing for coding use cases. Their efforts resulted in Codex, released
    in mid-2021.[[11]](xhtml-0-12.xhtml#aid_72)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with a stunning variety of use cases, the API gave birth to a whole new
    ecosystem of startups: “Within a few months of launching the API, there were several
    companies that were being built entirely on top of the OpenAI API. Many of them
    have now raised VC funding at fairly high valuations,” Welinder says.'
  prefs: []
  type: TYPE_NORMAL
- en: One of OpenAI’s core principles is working closely with customers. Welinder
    says, "Whenever we have new product features, we try to find customers that we
    know would find those features useful, and we create direct communication channels
    where we give them early access.” For example,  they worked with several customers
    on fine-tuning search functionality before publishing that feature more broadly
    in the API.
  prefs: []
  type: TYPE_NORMAL
- en: Open AI is primarily concerned with ensuring the safe and responsible use of
    AI. In addition to the many positive outcomes, they see growing potential for
    misuse as AI becomes more accessible to the general public. One of the main reasons
    they chose to launch the API in private beta was to understand how people would
    use the models and check their potential for abuse. They examine as many instances
    of undesirable model behavior as possible, using what they learn to inform their
    research and model training.
  prefs: []
  type: TYPE_NORMAL
- en: Welinder finds inspiration in the breadth and creativity of the projects driven
    by the API.  “The coming decade is going to be so exciting in terms of all the
    things that people will build on top of this technology. And I think by working
    together, we can create some really good guard rails to ensure that these technologies,
    these applications that are going to be built, are going to be really, really
    positive for our society.”
  prefs: []
  type: TYPE_NORMAL
- en: 'A Closer Look at the New Startup Environment: Case Studies'
  prefs: []
  type: TYPE_NORMAL
- en: Soon after OpenAI released the API, the startup landscape filled with companies
    using it to solve problems. These entrepreneurs are pioneers in state-of-the-art
    NLP products, and their journeys are informative, particularly for anyone planning
    future business applications based on OpenAI API. The rest of this chapter portrays
    this dynamic landscape through interviews with leaders of some of the top-performing
    startups using GPT-3 at the core of their product architecture about what they’ve
    learned so far in areas such as the creative arts, data analysis, chatbots, copywriting,
    and developer tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Creative Applications of GPT-3: Fable Studio'
  prefs: []
  type: TYPE_NORMAL
- en: One of GPT-3’s most exciting capabilities is storytelling. You can give the
    model a topic and ask it to write a story in a zero-shot setting.
  prefs: []
  type: TYPE_NORMAL
- en: The possibilities have writers expanding their imaginations and coming up with
    extraordinary work. For instance, the play [AI](https://www.youngvic.org/whats-on/ai),
    directed by Jennifer Tang and developed with Chinonyerem Odimba and Nina Segal,
    depicts a unique collaboration between human and computer minds with the help
    of GPT-3.  And author K. Allado McDowell treated GPT-3 as a coauthor in writing
    his book [PHARMAKO-AI](https://www.goodreads.com/book/show/56247773-pharmako-ai),
    which McDowell says “reimagines cybernetics for a world facing multiple crises,
    with profound implications for how we see ourselves, nature and technology in
    the 21st century.”
  prefs: []
  type: TYPE_NORMAL
- en: 'We sat down with Edward Saatchi, co-founder, and CEO of Fable Studio, and Frank
    Carey, Fable Studio’s CTO, to learn about their journey of creating a new genre
    of interactive stories using GPT-3\. Fable adapted Neil Gaiman and Dave McKean’s
    children’s book The Wolves in the Walls into an Emmy Award-winning VR film experience.
    Lucy, the film’s protagonist, can have natural conversations with people thanks
    to the dialogue generated by GPT-3\. Lucy appeared as a guest at Sundance Film
    Festival 2021 and presented her movie, Dracula: Blood Gazpacho.[[12]](xhtml-0-12.xhtml#aid_84)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Saatchi and Carey noticed their audience developing emotional connections to
    Lucy. That led them to focus on using AI to create virtual beings and, with them,
    a new category of storytelling and entertainment that weaves together AI and storytelling.
    As Awan puts it, “We will have new kinds of movies and genres altogether: we will
    have interactive, integrated experiences.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Carey explains that audiences usually think of AI taking up the role of a character,
    as an actor would: one AI corresponds to one character. Instead, Fable’s AI is
    a storyteller, with all sorts of characters in its repertoire. Carey believes
    it is possible to develop an AI storyteller as skilled and creative as the best
    human writers.'
  prefs: []
  type: TYPE_NORMAL
- en: While Lucy’s conversations mostly take place over text and video chat, Fable
    is also experimenting with GPT-3 in 3D simulated worlds for an immersive VR experience.
    The team uses AI to generate audio and gestures and to sync lip movement. They
    use GPT-3 to generate a significant amount of the content for characters’ audience
    interactions. Some of that content can be pre-authored, but much of it has to
    be created on the fly. Lucy's collaborators used GPT-3 extensively, both impromptu
    during her Sundance appearance both and during the creation of the film. As with
    Lucy’s Twitch appearances, Carey says, “more than 80% of the content was generated
    using GPT-3.”
  prefs: []
  type: TYPE_NORMAL
- en: This is a striking change from the team’s earlier text-only experiments, which
    were authored to a greater degree and followed a more linear narrative. The Fable
    Studio team generally didn't use GPT-3 live to handle audience members’ unpredictable
    responses; their techniques for that predated GPT-3\. They did, however, sometimes
    use GPT-3 as a writing partner or a stand-in for the audience when considering
    their potential responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Carey explains that GPT-3 is also a useful tool for human authors: “For the
    impromptu content, we’re using GPT-3 to play tests against, so you can treat GPT
    as the human and you''re sort of playing the character. Going back and forth with
    GPT-3 helps you come up with, like, what would someone ask in this situation?
    What would the follow-up be?” This helps the writers cover as many conversation
    outcomes as possible. “Sometimes it''s been a writing partner, sometimes it’s
    been something that can fill in the gaps around what''s happening,” Saatchi says.
    “So we might think: this is what''s going to happen to the character this week.
    What''s going to happen to the character next week? And GPT-3 [is] filling in
    some of those gaps.”'
  prefs: []
  type: TYPE_NORMAL
- en: The Fable team used GPT-3 to its fullest extent in an experiment at the 2021
    Sundance Film Festival, where Lucy collaborated live with festival participants
    to create her own short film, while Fable Studio and participants were curating
    the ideas she generated, bouncing them off participants, and feeding the audience’s
    ideas back into GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Powering one consistent character with GPT-3 was a special challenge. GPT-3
    is very good for use cases that redirect from the character to the participant,
    like therapy sessions, as well as for characters that have “a very large base
    of knowledge about them, like a celebrity or like a character that’s archetypical
    like Jesus, Santa Claus, or Dracula. But obviously, that caps out around whatever
    information has already been written,” Saatchi explains, noting that anyone who
    interacts extensively with a GPT-3-powered character will reach GPT-3’s limits
    fairly quickly. “It’s trying to get a good answer to the story you’re proposing.
    But if you tell a fantastical story in your prompts, it will come up with fantastical
    answers as well. Right? So it’s not a truth-teller. I would say it’s a storyteller
    by its nature; it’s just trying to find patterns in language.” What many people
    don’t realize about GPT-3 is that its bottom-line task is to tell a story, not
    the “truth,” Carey says.
  prefs: []
  type: TYPE_NORMAL
- en: “It's one thing just to generate a bunch of random scenarios using GPT-3, but
    it's a whole other thing to make sure it's in the voice of that character,” Carey
    adds. “So we have techniques that we're using to create those prompts so that
    the character is well defined for GPT3.” He admits that the team puts extra effort
    into making sure GPT-3 understands the voice of the character and remains within
    its range of possible responses. They also had to avoid allowing participants
    to influence the character, because GPT-3 can pick up on subtle signals. Carey
    explains that if Lucy interacts with an adult, “it'll just play along with the
    vibe, but [if] Lucy's an eight-year-old, it might be picking up a more of an adult
    vibe from the participant and feeding that back to them. But we actually want
    [Lucy] to be speaking in the eight-year-old child-like voice”.
  prefs: []
  type: TYPE_NORMAL
- en: Convincing OpenAI to allow them to create virtual beings with GPT-3 took some
    care.  “We were very interested in having our characters talk to people as characters,"
    says Carey. “You can imagine that can be one of their problematic areas, right?
    [It] could definitely have a potential for being nefariously used [by] someone
    pretending to be human.” The Fable Studio and Open AI teams spent some time working
    out the differences between creating lifelike characters and impersonating humans
    before OpenAI approved Fable’s use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI had another requirement: the Fable team at Fable Studio had to keep
    a human in the loop during any narrative experiments where a virtual being pretended
    to be "real" in front of an audience. It was challenging to make GPT-3 work with
    any experience intended for thousands of people, according to Carey. That said,
    he still thinks large language models are going to be a boon, “even if it''s for
    pre-authoring content or, in more forgiving areas, if used ‘live’ and without
    the restrictions.”'
  prefs: []
  type: TYPE_NORMAL
- en: Carey believes GPT-3 authoring works best as a collaborative tool in the hands
    of a person who knows the art of storytelling and would like to get better results,
    rather than expecting it to do all the work.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to price, the challenge he sees for the storytelling use case
    is that with every API request to keep GPT-3 consistent with the unraveling story,
    one has to “give it all the details and generate something that adds to it. So
    just to generate a few lines, you're charged the entire set of tokens. That could
    be a challenge.”
  prefs: []
  type: TYPE_NORMAL
- en: How did Fable Studio tackle the question of pricing? They managed to largely
    avoid it,  thanks to mainly experimenting with pre-generation, in which “you pre-generate
    a bunch of options and then can use search to find the right option to respond
    back with,” Carey says.
  prefs: []
  type: TYPE_NORMAL
- en: 'They also found a way to lower the number of API users: rather than having
    a large audience interacting with Lucy through their AI, “we''ve kind of pivoted
    to a model where Lucy is actually having a one-to-one conversation, but in a Twitch
    stream.” The audience watches via Twitch rather than making API calls, which alleviates
    the bandwidth issue, limit the number of people Lucy is interacting with at any
    given time, and broadens the audience.'
  prefs: []
  type: TYPE_NORMAL
- en: Saatchi mentions a rumor that GPT-4 is exploring the spatial understanding of
    virtual spaces, which he sees as having more potential than language-only chatbots.
    He advises people exploring this use case to focus on creating characters in virtual
    worlds. Saatchi notes that [Replika](https://replika.ai/), a company that has
    created a virtual AI friend character, is now exploring extending into a metaverse,4
    where virtual beings will have their own apartments and can meet and interact
    with each other as well as, eventually, with human users. “The point is to make
    a character that feels alive, and GPT-3 is one of many tools. Potentially giving
    virtual beings genuine understanding of the spaces that they’re navigating could
    unlock learning for these characters.”
  prefs: []
  type: TYPE_NORMAL
- en: What lies ahead? Carey sees a place for future iterations of GPT-3 in building
    the metaverse, a parallel digital reality where humans can interact and perform
    activities as freely as in the real world. He envisions it generating ideas and
    having a human in the loop to curate them.
  prefs: []
  type: TYPE_NORMAL
- en: Saatchi believes that deemphasizing language as the only mode of interaction
    has the potential to create more interesting and sophisticated interactions with
    AI. “I do think that 3D spaces give us the opportunity to give AI spatial understanding,”
    he continues. The metaverse Saatchi envisions gives AI the ability to walk around
    and explore and gives humans the opportunity to become part of the loop and help
    train virtual beings. He concludes we need radical new thinking, and that the
    metaverse offers significant opportunities to put AIs in 3D spaces and “allow
    them to live simulated lives with humans helping the characters grow.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Analysis Applications of GPT-3: Viable'
  prefs: []
  type: TYPE_NORMAL
- en: The story of the startup [Viable](https://www.askviable.com/) is an example
    of how much things can change from the moment you start working on a business
    idea to actually finding a product-market fit and a customer base. Viable helps
    companies better understand their customers by using GPT-3 to summarize customer
    feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'Viable aggregates feedback like surveys, help desk tickets, live chat logs,
    and customer reviews. It then identifies themes, emotions, and sentiments, pulls
    insights from those results,  and provides a summary in a matter of seconds. For
    example, if asked, “What’s frustrating our customers about the checkout experience?”
    Viable might respond: “Customers are frustrated with the checkout flow because
    it takes too long to load. They also want a way to edit their address in checkout
    and save multiple payment methods.”'
  prefs: []
  type: TYPE_NORMAL
- en: Viable’s original business model involved helping early-stage startup companies
    find product-market fit using surveys and product roadmaps. Requests started coming
    in from bigger companies, asking for support in analyzing huge volumes of text,
    such as “support tickets, social media, app store reviews and survey responses”
    that changed everything, says Daniel Erickson. Erickson is the founder and CEO
    of Viable–and an early adopter of the OpenAI API.  He explains, “I spent actually
    about a month just experimenting, literally just taking our data, putting it into
    the playground, figuring out different prompts and things like that. And eventually,
    I came to the conclusion that [GPT-3] could power a very powerful question and
    answer system.”
  prefs: []
  type: TYPE_NORMAL
- en: Erickson and his colleagues began using the OpenAI API to interact with and
    generate insights from the large datasets they were working with. They initially
    used another NLP model, achieving mediocre results, but when they began working
    with GPT-3, the team saw “at least a 10% increase across the board. When we're
    talking about going from 80% to 90%, that’s a hell of an increase for us”.
  prefs: []
  type: TYPE_NORMAL
- en: Building on that success, they used GPT-3 in combination with other models and
    systems to create a Q&A feature that allows users to ask a question in plain English
    and get an answer.  Viable converts the question to a complex query that can pull
    all the relevant feedback from the database. It then runs the data through another
    series of summarization and analysis models to generate a refined answer.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Viable’s system provides customers with “a 12-paragraph summary
    every week  that outlines things like their top complaints, their top compliments,
    their top requests, and top questions." As you might expect from customer-feedback
    specialists, Viable has thumbs up and thumbs down buttons next to every answer
    the software generates. They use this feedback in retraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Humans are part of the process, too: Viable has an annotation team whose members
    are responsible for building training datasets, both for internal models and GPT-3
    fine-tuning. They use the current iteration of that fine-tuned model to generate
    output, which humans then assess for quality. If the output doesn’t make sense
    or isn’t accurate, they rewrite it. And once they have a list of outputs they
    are satisfied with, they feed that list back into the next iteration of the training
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Erickson notes that the API is a huge advantage since it leaves the hosting,
    debugging, scaling, and optimization to OpenAI: “I would much rather buy than
    build for almost anything that isn’t super core to our tech. And even if it is
    core to our tech, it still makes sense for us to do it with GPT-3.” Therefore,
    their ideal solution would be to use GPT-3 for all the elements of their process.
    But they had to optimize their usage due to cost: “We have companies that are
    giving us hundreds of thousands of data points that are anywhere from five to
    a thousand words each.” Using GPT-3 for everything could get expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Viable mainly uses internal models to structure data, which they developed
    on top of BERT and ALBERT and trained using GPT-3 output. These models are now
    meeting or exceeding the GPT-3’s capabilities for topic extraction, emotion and
    sentiment analysis, and many other tasks. Viable also switched to a usage-based
    pricing model built on top of OpenAI’s API pricing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Erickson maintains that GPT-3 gives Viable an edge over its competition in
    two ways: accuracy and usability.  We have touched upon the impressive 10% accuracy
    boost for Viable. But what about usability? Most of Viable’s competitors build
    tools specifically designed for professional data analysts. Viable felt that was
    too narrow an audience: “We didn''t want to build a piece of software that only
    analysts could use because we feel like that limits the value. What we want to
    do is help teams make better decisions using qualitative data.”'
  prefs: []
  type: TYPE_NORMAL
- en: Instead, Viable’s software itself is the “analyst.” And users can iterate faster,
    thanks to a feedback loop that allows them to answer questions about their data
    in natural language and get a fast and accurate response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Erickson shared some of Viable’s next steps: they will soon introduce quantitative
    data and crunching product analytics. Ultimately, Erickson wants to give users
    the ability to perform a full customer insight analysis and ask questions such
    as  “How many customers are using feature X?” and “Of the customers who use feature
    X, how do they think it should be improved?”'
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, Erickson concludes, “What we sell is generated insights. And so
    the more in-depth and the more powerful that we make those insights, and the more
    quickly we deliver those insights, the more value we create.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Chatbot Applications of GPT-3: Quickchat'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPT-3, being very proficient at language interactions, is an obvious choice
    to enhance the existing chatbot experience. While many apps entertain users with 
    AI chatbot personas, such as [PhilosopherAI](https://philosopherai.com/) and [TalkToKanye](https://talktokanye.com/), 
    two companies specifically leverage this capability in their business applications:
    Quickchat and Replika. Quickchat is well-known for its AI chatbot persona: Emerson
    AI, accessible via Telegram and the Quickchat mobile application. Emerson AI has
    broad general world knowledge, including access to more recent information, even
    after GPT-3’s training time; supports multiple languages; can handle a coherent
    conversation, and is fun to talk to.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Piotr Grudzień and Dominik Posmyk, co-founders of Quickchat, were excited about
    GPT-3 from the start and full of ideas for leveraging it in a new product. During
    their early experiments with the OpenAI API, they kept coming back to the notion
    of “evolving interfaces between machines and people.”  Grudzień explains that
    since the interactions between humans and computers are constantly evolving, natural
    language would be the logical next step: after all, humans prefer to communicate
    via conversation. GPT-3, they concluded, seemed to have the potential to enable
    human-like chat experiences with computers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grudzień says neither of the founders had built any conventional chatbot application
    before. Approaching the task with a “beginner’s mind”  helped them stay fresh
    and open about solving the problem. Unlike other chatbot companies, they didn’t
    start with the ambition of becoming the best customer support or marketing tool.
    What they started with was: “How do I get a human being to talk to a machine in
    such a way that is awe-inspiring and the best thing that they''ve ever tried?”
    They wanted to make a chatbot that doesn’t just complete simple functions such
    as collecting customer data and providing answers but is also ready to answer
    unscripted customer questions or make pleasant small talk. “Instead of saying:
    ‘I don''t know,’” Grudzień adds, it can “fall back on the conversational API and
    keep the conversation going.”'
  prefs: []
  type: TYPE_NORMAL
- en: Posmyk adds, “Our mission is to empower people with Artificial Intelligence,
    not replace them. We believe that over the next decade, AI will accelerate the
    digitization of crucial industries such as education, legal, and healthcare and
    increase our productivity at work and in everyday life.” To provide a glimpse
    of this far-fetched mission, they created Emerson AI, an intelligent general-purpose
    chatbot application powered by GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: Although Emerson AI has a growing community of users, its true purpose is to
    showcase the capabilities of GPT-3 powered chatbots and encourage users to work
    with Quickchat on implementing such a persona for their companies. Quickchat’s
    product offering is a general-purpose conversational AI that can talk about any
    subject. Customers, mostly established companies, can customize the chatbot by
    adding extra information specific to their product (or any topic they want). Quickchat
    has seen diverse applications, such as automating typical FAQ customer-support
    problem-solving and implementing an AI persona to help users search an internal
    company knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike typical chatbot service providers, Quickchat does not build any conversation
    trees or rigid scenarios, nor does it need to teach the chatbot to answer questions
    in a given way. Instead, Grudzień explains, customers follow a simple process:
    “You copy-paste text that contains all the information that you want your AI to
    be using [and] click on the retrain button, which takes a few seconds to absorb
    the knowledge, and that''s it.”  Now trained on your data, the chatbot is ready
    to have a test conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: Asked about the tradeoffs between open-source models and OpenAI API, Grudzień
    shares that “OpenAI API is nice and easy to use because you don't need to worry
    about infrastructure, about latency or model training. It's just calling an API
    and getting an answer. It's super reliable.” However, he believes you pay quite
    a high price for the quality. In comparison, open-source models seem to be a great
    free alternative. In practice, “you do need to pay the cost of cloud computing.
    It requires GPUs and setting up GPUs to work with these models to be fast, then
    to do fine-tuning of your own,” which Grudzień admits, is not a trivial process.
  prefs: []
  type: TYPE_NORMAL
- en: Like Viable’s Ericksen, Grudzień and Posmyk strive to deliver value with every
    API call. But they also hope that as more and more competitive models get released,
    OpenAI’s API pricing will “go down or will plateau to some level, just because
    of the pressure of competition.”
  prefs: []
  type: TYPE_NORMAL
- en: So what has Quickchat learned? First, it takes more than hype to build a profitable
    business. A big media sensation, like the one that launched GPT-3, can provide
    an initial influx of excited enthusiasts, “but then people get bored and wait
    for the next big thing. The only products that survive are ones that actually
    solve some problems that people care about,” Grudzień says. “No one's going to
    use your product just because it's GPT-3\. It needs to deliver some value, either
    useful or fun, or solve some problem. GPT-3 is not going to do that for you. So
    you need to just treat it as yet another tool.”
  prefs: []
  type: TYPE_NORMAL
- en: Another key lesson was to develop solid performance metrics. “Whenever you're
    building a machine learning product, it's always tricky to evaluate,” says Grudzień.
    In his view, because GPT-3 is robust and operates in the difficult-to-quantify
    domain of natural language, evaluating the quality of its output is complex and
    cumbersome. As exciting as a breakthrough can be, he says, “users are going to
    probably judge you on your worst-case performance, at best on your average performance.”
    Quickchat, therefore, optimizes user satisfaction. It was crucial for them to
    design a metric to capture variables correlated with happy users and high retention,
    both of which directly translate to higher revenue.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge, perhaps surprisingly, is GPT’s knack for creativity. “Even
    if you set a temperature very low, whatever prompt you give it, it's still going
    to use that prompt that is tiny and then generates something based on this vast
    knowledge that it has,” Grudzień explains. This makes it easy to generate creative
    text such as poetry, marketing copy, or fantasy stories. But most chatbots are
    for solving customer problems. “It needs to have predictable, repetitive performance,
    while still being conversational and to some extent creative, but not pushing
    it too far.”
  prefs: []
  type: TYPE_NORMAL
- en: Large language models sometimes output text that’s  “weird,” “empty,” or just
    “not that great,” so humans do need to intervene. “If you start measuring whether
    it managed to satisfy some condition or fulfill the task, then it's going to turn
    out that it's really creative, but out of 10 tries, it only succeeded six times—which
    is as good as zero when it comes to real business with paying customers.” Therefore,
    for a successful business application, you need a lot of internal systems and
    models that restrain that creativity and bolster reliability. “To create this
    tool for our customers that works 99% of the time, we developed a number of defense
    mechanisms,” Grudzień says.
  prefs: []
  type: TYPE_NORMAL
- en: 'These days, Quickchat is focused on working deeply with customers to make sure
    their API performance lets them succeed in their use case. What excites Grudzień
    the most is seeing what customers build: “We really, really want to see our chat
    engine being used in thousands of different ways in different products.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Marketing Applications of GPT-3: Copysmith'
  prefs: []
  type: TYPE_NORMAL
- en: 'Can GPT-3 eliminate writer’s block? Kilcher thinks so:  “If you have writer''s
    block, you just ask a model and it gives you a thousand ideas out of a model like
    this, simply as a creative assistance tool.” Let’s look at one such tool: Copysmith.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular applications of GPT-3 is to generate creative content
    on the fly. Copysmith is one of the leading content-creation tools on the market.
    “Copysmith is something that enables users to create and deploy content anywhere
    on the web a hundred times faster through powerful AI,” says co-founder and CTO
    Anna Wang. It uses GPT-3 for copywriting in e-commerce and marketing to generate,
    collaborate and launch quality content at lightning speed. Wang and CEO Shegun
    Otulana shared how two sisters transitioned their small, struggling e-commerce
    store into a successful technology company–and GPT-3’s pivotal role in making
    it possible.
  prefs: []
  type: TYPE_NORMAL
- en: In June 2019, Anna Wang and her sister Jasmine Wang co-founded a Shopify-based
    boutique. But they lacked marketing experience, and “the business utterly collapsed,”
    Anna Wang says.  When the sisters learned about the OpenAI API in 2020, Wang says,
    “we started exploring it for creative pursuits like writing poetry, trying to
    emulate characters from books and movies. One day we realized that if we had this
    tool while we were trying to build the e-commerce store, we would have been able
    to write better calls-to-action, and product descriptions and leveled up our marketing
    game to get it off the ground.”
  prefs: []
  type: TYPE_NORMAL
- en: Inspired, they launched Copysmith in October 2020 to a warm reception. In Anna
    Wang’s words, “That's where everything began. We started talking to users and
    iterating the product based on the feedback.”  GPT-3, she notes, allows you to
    iterate very fast without any prior knowledge, whereas other open source models,
    like BERT and RoBERTa, require a significant level of fine-tuning for every downstream
    task. “It is extremely flexible in terms of the tasks it can perform,” she adds,
    and “it is the most powerful model out there.” What’s more, GPT-3 is “super-friendly
    for developers and users, with its simple text-in text-out interface that allows
    you to perform all kinds of tasks using a simple API.” Its other advantage is
    the simplicity of the API call, compared to the efforts that go into hosting a
    proprietary model.
  prefs: []
  type: TYPE_NORMAL
- en: As for the challenges of building a product based on GPT-3, Otulana says, “You
    are generally bound by the limitations of OpenAI. So, to overcome that, you have
    to give your own entrepreneurial touch to the API for creating something that
    stands out. Another limitation is a slight loss of control, where your progress
    is in essence limited by OpenAI’s progress.”
  prefs: []
  type: TYPE_NORMAL
- en: Anna Wang has two pieces of advice for would-be product designers who want to
    use GPT-3\. First, she says,  “Make sure you are solving a real problem. . .think
    about your user, because one of the easy things with GPT-3 is to fall into the
    mindset of building things within the limit of safety guidelines without allowing
    yourself to be creative.”
  prefs: []
  type: TYPE_NORMAL
- en: Second, Wang advises, “Keep a very close eye on what you are feeding to the
    model. Be careful with the punctuation, grammar, and wording of the prompt. I
    guarantee you'll have a much better experience with the model output.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Coding Applications of GPT-3: Stenography'
  prefs: []
  type: TYPE_NORMAL
- en: As GPT-3 and its descendant model Codex continue to show more ability to interact
    with programming and natural languages, new potential use cases are piling up.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bram Adams, an OpenAI Community Ambassador known for his creative experiments
    with GPT-3 and Codex algorithms, launched one in late 2021: Stenography, which
    leverages both GPT-3 and Codex to automate the annoying task of writing code documentation.
    Stenography found instant success, launching as the number one product of the
    day on the popular product launch portal Product Hunt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adams tried several potential use cases with the API before, narrowing his
    ideas down to the one that has become his new business. “I think a lot of those
    experiments were about me unconsciously edge-testing what a language model like
    GPT-3 can handle.” Adams’s search began with the idea: ‘What would I do if I could
    ask a computer to do anything?’” He began exploring, “poking at the corners of
    the OpenAI API and seeing how far it could go.” He came up with a bot that generates
    Instagram poetry; tried a self-podcasting journaling project in which users would
    speak to digital versions of themselves; worked on a music playlist-building project
    on Spotify based on users’ preferences; and created many more projects in service
    of his curiosity. Thanks to that curiosity, “I got really good early on at understanding
    the different engines of GPT-3.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'So why Stenography? “I got a pretty good signal from the external world that
    this could be very helpful to a lot of people.” While Adams enjoys the elegance
    of well-written code, most GitHub users just download published code and use it:
    “No one''s really gonna admire the beauty that you put into your codebase.” He
    also noticed that great programs on GitHub that aren’t well documented often don’t
    get the visibility they deserve: “The readme [file] is the first thing that everybody
    sees. They immediately scroll down to it.” Stenography was an attempt to think
    about how documentation could evolve to become less annoying for developers: “It''s
    hard because, with documentation in particular, you have to justify what you did.
    So you say, ‘I used this library to do this thing. And then I decided to use this
    thing, and then I added this function to do this thing.’”'
  prefs: []
  type: TYPE_NORMAL
- en: Adams sees documentation as a way for people to reach out to other people on
    their teams, their future selves, or just interested people who stumble across
    the project. Its goal is to make a project understandable to others.” I was interested
    in the idea if GPT-3 could create understandable comments.”  He tried both GPT-3
    and Codex and was impressed with the level of explanation from both models. The
    next question he asked was, “How do I make this really easy and enjoyable for
    developers?”
  prefs: []
  type: TYPE_NORMAL
- en: So how does Stenography work, and how do its components leverage the OpenAI
    API? At a high level, Adams says, there are two main processes, parsing, and explanation,
    and each requires a different strategy. “For the parsing process, I spent a lot
    of time understanding the complexity of code because not all lines in your code
    are even worth documenting.” Some code might have an obvious purpose, have no
    operational value, or no longer be useful.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, “big” code blocks, reaching over 800 lines, are too tricky for
    the model to understand in one go. “You'd have to break down that logic to many
    different kinds of steps to be able to say accurately that this is what this thing
    does. Once I understood that I started thinking, ‘How can I leverage parsing to
    find blocks that are sufficiently complex, but not too complex?’”  Since everyone
    writes code differently, it's a matter of trying to attach to the abstract syntax
    tree and work with the best of what you have. That became the main architectural
    challenge of the parsing layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the explanation layer, “that''s more of a feature of getting GPT-3 and
    Codex to say what you want them to say,” Adams explains. The way to go about it
    is to find creative ways to understand your code’s audience and get GPT-3 to speak
    to it. This layer “can attempt to solve any question, but it might not solve it
    at a hundred percent accuracy like you would get with something like a calculator.
    If you type two plus two equals four, occasionally you get five, but you don''t
    need to write all the functions for multiplication, division, and subtraction.
    Those come for free.” That’s the trade-off with probabilistic systems: sometimes
    they work and sometimes they don''t, but they always return an answer. Adams advises
    remaining fluid enough to be able to pivot your strategy if necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adams stresses the importance of really understanding the problem before you
    start using the OpenAI API. “During my office hours, people will come, and they''ll
    have these huge problems. They''ll be like, ‘How do I build a rocket ship from
    scratch using a prompt?’ And I''m like, ‘Well, there''s a lot of components of
    a rocket ship. GPT-3 isn''t a panacea. It''s a very powerful machine, but only
    if you know what you''re using it for.’” He compares GPT-3 to programming languages
    like JavaScript, Python, and C: “They''re compelling, but only if you understand
    recursion and for loops and while loops, and what tools will help you solve your
    particular problem” For Adams, that has meant asking lots of “technical meta-questions,”
    such as “What is the thing that is being helped by having AI documentation?” and
    “What even is documentation in the first place?” Dealing with these questions
    was the biggest challenge for him.'
  prefs: []
  type: TYPE_NORMAL
- en: “I think a lot of people just immediately rushed to Davinci to solve their problems.
    But if you can solve something on a smaller engine, like an Ada, Babbage, or Curie,
    you actually get to know the problem a lot more deeply than you would if you were
    just trying to throw the whole AI at it with Davinci,” he claims.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to building and scaling a product with the OpenAI API, he advises
    using “small engines or low temperatures, because you can't predict what your
    final prompt will be like (or if it will continue to evolve over time), what you're
    trying to do, and who your end-user is, but using smaller engines and lower temperatures,
    you’ll find answers to the really hard questions faster.” he tells us.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge has been moving from his own standalone experiments to account
    for all the different conditions and ways of working that users might face. Now
    he is working on “finding all the different edge cases” to better understand how
    fast the design layer of the API has to be, how frequently it has to respond with
    a particular request, and how it interacts with different languages.
  prefs: []
  type: TYPE_NORMAL
- en: What’s next for Stenography? Now that Adams has built a product that he’s very
    happy with, in 2022 he plans to focus on sales and talking to the user base. “Stenography
    isn't going to be as much about building as really perfecting the product and
    getting it in front of people.”
  prefs: []
  type: TYPE_NORMAL
- en: An Investor’s Outlook on the GPT-3 Startup Ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: To understand the perspective of investors backing GPT-3-based companies, we
    spoke with Jake Flomenberg of Wing VC, a renowned global venture-capital firm
    and lead investor in several GPT-3-powered startups, including Copy.AI and Simplified.
  prefs: []
  type: TYPE_NORMAL
- en: 'As any market watcher might imagine, venture capitalists are watching nascent
    AI technologies like GPT-3\. Flomenberg summarizes the appeal: GPT-3  is “unlike
    any other NLP model that we have ever seen before. It is a substantial step in
    the direction of building more generalized AI.” The untapped potential is enormous,
    he argues, and the business world still “underestimates and therefore underutilizes
    the capabilities of LLMs.”'
  prefs: []
  type: TYPE_NORMAL
- en: But how should potential investors evaluate something so new and different?
    “We value startups with a deep understanding of the problem, the domain, and the
    technology” as well as a good fit between product and market, Flomenberg says.
    “The nuance in assessing something built on GPT-3 is asking, what’s the secret
    sauce? What is it that the company has built a technologically deep knowledge
    on? Is the company solving a real problem using GPT-3, or just leveraging the
    hype to get their product out in the market? Why now? Why is this team the best
    fit to execute this idea? Is this idea defensible in the real world?” If a startup
    can’t defend its existence, that’s a huge red flag for investors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Investors also keep a close eye on OpenAI and its API, since GPT-3-based businesses
    rely completely on its capabilities. Flomenberg cites OpenAI’s due-diligence review
    process as a major factor in this trust-based relationship: “The startups that
    pass the production review and are a subject of interest by OpenAI automatically
    become hot for investment.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Investors usually dig into the background and expertise of founders while making
    investment decisions. GPT-3 is unusual, though, in that it allows people from
    any background, not just programmers, to build cutting-edge NLP products. Flomenberg
    stresses the importance of the market here: “Generally with a deep tech startup,
    we look for founders with a great understanding of technical and AI domain. But
    with GPT-3-based startups, we are more focused on whether the market resonates
    with the founders’ vision and whether they’re able to identify and address the
    needs of the end-users”. He cites Copy.AI as  “a classic example of a product-led-growth
    model built on top of GPT-3\. They found an extraordinary resonance with their
    users and developed a deep understanding of the technology, bringing depth and
    value to the table.” Successful startups, he says, “keep the AI inside,” focusing
    more on solving users’ problems and meeting their needs by using the right tool
    for the job.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs: []
  type: TYPE_NORMAL
- en: It’s mind-blowing to see these use cases, and many more, built on top of GPT-3
    so quickly and with such success. By late 2021, when this chapter was written,
    several startups in the OpenAI community had already raised hefty rounds of funding
    and were looking at rapid expansion plans. This market tide seems to have awakened
    the appetites of bigger businesses as well. More and more enterprises are starting
    to consider implementing experimental GPT-3 projects within their organizations.
    In Chapter 5, we will look at this market segment consisting of large-scale products
    like GitHub Copilot and particularly the new Microsoft Azure OpenAI Service, which
    is designed to meet the needs of large-scale organizations.
  prefs: []
  type: TYPE_NORMAL
