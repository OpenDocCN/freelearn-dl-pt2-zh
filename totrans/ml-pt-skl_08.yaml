- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying Machine Learning to Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the modern internet and social media age, people’s opinions, reviews, and
    recommendations have become a valuable resource for political science and businesses.
    Thanks to modern technologies, we are now able to collect and analyze such data
    most efficiently. In this chapter, we will delve into a subfield of **natural
    language processing** (**NLP**) called **sentiment analysis** and learn how to
    use machine learning algorithms to classify documents based on their sentiment:
    the attitude of the writer. In particular, we are going to work with a dataset
    of 50,000 movie reviews from the **Internet Movie Database** (**IMDb**) and build
    a predictor that can distinguish between positive and negative reviews.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that we will cover in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and preparing text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building feature vectors from text documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a machine learning model to classify positive and negative movie reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with large text datasets using out-of-core learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferring topics from document collections for categorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the IMDb movie review data for text processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, sentiment analysis, sometimes also called **opinion mining**,
    is a popular subdiscipline of the broader field of NLP; it is concerned with analyzing
    the sentiment of documents. A popular task in sentiment analysis is the classification
    of documents based on the expressed opinions or emotions of the authors with regard
    to a particular topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be working with a large dataset of movie reviews from
    IMDb that has been collected by Andrew Maas and others (*Learning Word Vectors
    for Sentiment Analysis* by *A. L. Maas*, *R. E. Daly*, *P. T. Pham*, *D. Huang*,
    *A. Y. Ng*, and *C. Potts*, *Proceedings of the 49th Annual Meeting of the Association
    for Computational Linguistics: Human Language Technologies*, pages 142–150, Portland,
    Oregon, USA, Association for Computational Linguistics, June 2011). The movie
    review dataset consists of 50,000 polar movie reviews that are labeled as either
    positive or negative; here, positive means that a movie was rated with more than
    six stars on IMDb, and negative means that a movie was rated with fewer than five
    stars on IMDb. In the following sections, we will download the dataset, preprocess
    it into a useable format for machine learning tools, and extract meaningful information
    from a subset of these movie reviews to build a machine learning model that can
    predict whether a certain reviewer liked or disliked a movie.'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining the movie review dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A compressed archive of the movie review dataset (84.1 MB) can be downloaded
    from [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    as a gzip-compressed tarball archive:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are working with Linux or macOS, you can open a new terminal window,
    `cd` into the download directory, and execute `tar -zxf aclImdb_v1.tar.gz` to
    decompress the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are working with Windows, you can download a free archiver, such as 7-Zip
    ([http://www.7-zip.org](http://www.7-zip.org)), to extract the files from the
    download archive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, you can unpack the gzip-compressed tarball archive directly
    in Python as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Preprocessing the movie dataset into a more convenient format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having successfully extracted the dataset, we will now assemble the individual
    text documents from the decompressed download archive into a single CSV file.
    In the following code section, we will be reading the movie reviews into a pandas
    `DataFrame` object, which can take up to 10 minutes on a standard desktop computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the progress and estimated time until completion, we will use
    the **Python Progress Indicator** (**PyPrind**, [https://pypi.python.org/pypi/PyPrind/](https://pypi.python.org/pypi/PyPrind/))
    package, which was developed several years ago for such purposes. PyPrind can
    be installed by executing the `pip install pyprind` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first initialized a new progress bar object, `pbar`,
    with 50,000 iterations, which was the number of documents we were going to read
    in. Using the nested `for` loops, we iterated over the `train` and `test` subdirectories
    in the main `aclImdb` directory and read the individual text files from the `pos`
    and `neg` subdirectories that we eventually appended to the `df` pandas `DataFrame`,
    together with an integer class label (1 = positive and 0 = negative).
  prefs: []
  type: TYPE_NORMAL
- en: Since the class labels in the assembled dataset are sorted, we will now shuffle
    the `DataFrame` using the `permutation` function from the `np.random` submodule—this
    will be useful for splitting the dataset into training and test datasets in later
    sections, when we will stream the data from our local drive directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our own convenience, we will also store the assembled and shuffled movie
    review dataset as a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are going to use this dataset later in this chapter, let’s quickly
    confirm that we have successfully saved the data in the right format by reading
    in the CSV and printing an excerpt of the first three examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are running the code examples in a Jupyter notebook, you should now
    see the first three examples of the dataset, as shown in *Figure 8.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: The first three rows of the movie review dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a sanity check, before we proceed to the next section, let’s make sure that
    the `DataFrame` contains all 50,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Introducing the bag-of-words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may remember from *Chapter 4*, *Building Good Training Datasets – Data
    Preprocessing*, that we have to convert categorical data, such as text or words,
    into a numerical form before we can pass it on to a machine learning algorithm.
    In this section, we will introduce the **bag-of-words** model, which allows us
    to represent text as numerical feature vectors. The idea behind bag-of-words is
    quite simple and can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a vocabulary of unique tokens—for example, words—from the entire set
    of documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We construct a feature vector from each document that contains the counts of
    how often each word occurs in the particular document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the unique words in each document represent only a small subset of all
    the words in the bag-of-words vocabulary, the feature vectors will mostly consist
    of zeros, which is why we call them **sparse**. Do not worry if this sounds too
    abstract; in the following subsections, we will walk through the process of creating
    a simple bag-of-words model step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming words into feature vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To construct a bag-of-words model based on the word counts in the respective
    documents, we can use the `CountVectorizer` class implemented in scikit-learn.
    As you will see in the following code section, `CountVectorizer` takes an array
    of text data, which can be documents or sentences, and constructs the bag-of-words
    model for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'By calling the `fit_transform` method on `CountVectorizer`, we constructed
    the vocabulary of the bag-of-words model and transformed the following three sentences
    into sparse feature vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''The sun is shining''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''The weather is sweet''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''The sun is shining, the weather is sweet, and one and one is two''`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s print the contents of the vocabulary to get a better understanding
    of the underlying concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from executing the preceding command, the vocabulary is stored
    in a Python dictionary that maps the unique words to integer indices. Next, let’s
    print the feature vectors that we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Each index position in the feature vectors shown here corresponds to the integer
    values that are stored as dictionary items in the `CountVectorizer` vocabulary.
    For example, the first feature at index position `0` resembles the count of the
    word `''and''`, which only occurs in the last document, and the word `''is''`,
    at index position `1` (the second feature in the document vectors), occurs in
    all three sentences. These values in the feature vectors are also called the **raw
    term frequencies**: *tf*(*t*, *d*)—the number of times a term, *t*, occurs in
    a document, *d*. It should be noted that, in the bag-of-words model, the word
    or term order in a sentence or document does not matter. The order in which the
    term frequencies appear in the feature vector is derived from the vocabulary indices,
    which are usually assigned alphabetically.'
  prefs: []
  type: TYPE_NORMAL
- en: '**N-gram models**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of items in the bag-of-words model that we just created is also
    called the 1-gram or unigram model—each item or token in the vocabulary represents
    a single word. More generally, the contiguous sequences of items in NLP—words,
    letters, or symbols—are also called *n-grams*. The choice of the number, *n*,
    in the n-gram model depends on the particular application; for example, a study
    by Ioannis Kanaris and others revealed that n-grams of size 3 and 4 yield good
    performances in the anti-spam filtering of email messages (*Words versus character
    n-grams for anti-spam filtering* by *Ioannis Kanaris*, *Konstantinos Kanaris*,
    *Ioannis Houvardas*, and *Efstathios Stamatatos*, *International Journal on Artificial
    Intelligence Tools*, *World Scientific Publishing Company*, 16(06): 1047-1067,
    2007).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize the concept of the n-gram representation, the 1-gram and 2-gram
    representations of our first document, “the sun is shining”, would be constructed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1-gram: “the”, “sun”, “is”, “shining”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2-gram: “the sun”, “sun is”, “is shining”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `CountVectorizer` class in scikit-learn allows us to use different n-gram
    models via its `ngram_range` parameter. While a 1-gram representation is used
    by default, we could switch to a 2-gram representation by initializing a new `CountVectorizer`
    instance with `ngram_range=(2,2)`.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing word relevancy via term frequency-inverse document frequency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we are analyzing text data, we often encounter words that occur across
    multiple documents from both classes. These frequently occurring words typically
    don’t contain useful or discriminatory information. In this subsection, you will
    learn about a useful technique called the **term frequency-inverse document frequency**
    (**tf-idf**), which can be used to downweight these frequently occurring words
    in the feature vectors. The tf-idf can be defined as the product of the term frequency
    and the inverse document frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tf-idf*(*t*, *d*) = *tf*(*t*, *d*) × *idf*(*t*, *d*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *tf*(*t*, *d*) is the term frequency that we introduced in the previous
    section, and *idf*(*t*, *d*) is the inverse document frequency, which can be calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n*[d] is the total number of documents, and *df*(*d*, *t*) is the number
    of documents, *d*, that contain the term *t*. Note that adding the constant 1
    to the denominator is optional and serves the purpose of assigning a non-zero
    value to terms that occur in none of the training examples; the *log* is used
    to ensure that low document frequencies are not given too much weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn library implements yet another transformer, the `TfidfTransformer`
    class, which takes the raw term frequencies from the `CountVectorizer` class as
    input and transforms them into tf-idfs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you saw in the previous subsection, the word `'is'` had the largest term
    frequency in the third document, being the most frequently occurring word. However,
    after transforming the same feature vector into tf-idfs, the word `'``is'` is
    now associated with a relatively small tf-idf (0.45) in the third document, since
    it is also present in the first and second document and thus is unlikely to contain
    any useful discriminatory information.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if we’d manually calculated the tf-idfs of the individual terms in
    our feature vectors, we would have noticed that `TfidfTransformer` calculates
    the tf-idfs slightly differently compared to the standard textbook equations that
    we defined previously. The equation for the inverse document frequency implemented
    in scikit-learn is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the tf-idf computed in scikit-learn deviates slightly from the default
    equation we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '*tf-idf*(*t*, *d*) = *tf*(*t*, *d*) × (*idf*(*t*, *d*) + 1)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the “+1” in the previous idf equation is due to setting `smooth_idf=True`
    in the previous code example, which is helpful for assigning zero weight (that
    is, *idf*(*t*, *d*) = log(1) = 0) to terms that occur in all documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it is also more typical to normalize the raw term frequencies before
    calculating the tf-idfs, the `TfidfTransformer` class normalizes the tf-idfs directly.
    By default (`norm=''l2''`), scikit-learn’s `TfidfTransformer` applies the L2-normalization,
    which returns a vector of length 1 by dividing an unnormalized feature vector,
    *v*, by its L2-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To make sure that we understand how `TfidfTransformer` works, let’s walk through
    an example and calculate the tf-idf of the word `''is''` in the third document.
    The word `''is''` has a term frequency of 3 (*tf* = 3) in the third document,
    and the document frequency of this term is 3 since the term `''is''` occurs in
    all three documents (*df* = 3). Thus, we can calculate the inverse document frequency
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, in order to calculate the tf-idf, we simply need to add 1 to the inverse
    document frequency and multiply it by the term frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we repeated this calculation for all terms in the third document, we’d obtain
    the following tf-idf vectors: `[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69,
    1.29]`. However, notice that the values in this feature vector are different from
    the values that we obtained from `TfidfTransformer` that we used previously. The
    final step that we are missing in this tf-idf calculation is the L2-normalization,
    which can be applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_08_006.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the results now match the results returned by scikit-learn’s
    `TfidfTransformer`, and since you now understand how tf-idfs are calculated, let’s
    proceed to the next section and apply those concepts to the movie review dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning text data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous subsections, we learned about the bag-of-words model, term frequencies,
    and tf-idfs. However, the first important step—before we build our bag-of-words
    model—is to clean the text data by stripping it of all unwanted characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate why this is important, let’s display the last 50 characters from
    the first document in the reshuffled movie review dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see here, the text contains HTML markup as well as punctuation and
    other non-letter characters. While HTML markup does not contain many useful semantics,
    punctuation marks can represent useful, additional information in certain NLP
    contexts. However, for simplicity, we will now remove all punctuation marks except
    for emoticon characters, such as :), since those are certainly useful for sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish this task, we will use Python’s **regular expression** (**regex**)
    library, `re`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Via the first regex, `<[^>]*>`, in the preceding code section, we tried to remove
    all of the HTML markup from the movie reviews. Although many programmers generally
    advise against the use of regex to parse HTML, this regex should be sufficient
    to *clean* this particular dataset. Since we are only interested in removing HTML
    markup and do not plan to use the HTML markup further, using regex to do the job
    should be acceptable. However, if you prefer to use sophisticated tools for removing
    HTML markup from text, you can take a look at Python’s HTML parser module, which
    is described at [https://docs.python.org/3/library/html.parser.html](https://docs.python.org/3/library/html.parser.html).
    After we removed the HTML markup, we used a slightly more complex regex to find
    emoticons, which we temporarily stored as emoticons. Next, we removed all non-word
    characters from the text via the regex `[\W]+` and converted the text into lowercase
    characters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dealing with word capitalization**'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of this analysis, we assume that the capitalization of a word—for
    example, whether it appears at the beginning of a sentence—does not contain semantically
    relevant information. However, note that there are exceptions; for instance, we
    remove the notation of proper names. But again, in the context of this analysis,
    it is a simplifying assumption that the letter case does not contain information
    that is relevant for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we added the temporarily stored emoticons to the end of the processed
    document string. Additionally, we removed the *nose* character (- in :-)) from
    the emoticons for consistency.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular expressions**'
  prefs: []
  type: TYPE_NORMAL
- en: Although regular expressions offer an efficient and convenient approach to searching
    for characters in a string, they also come with a steep learning curve. Unfortunately,
    an in-depth discussion of regular expressions is beyond the scope of this book.
    However, you can find a great tutorial on the Google Developers portal at [https://developers.google.com/edu/python/regular-expressions](https://developers.google.com/edu/python/regular-expressions)
    or you can check out the official documentation of Python’s `re` module at [https://docs.python.org/3.9/library/re.html](https://docs.python.org/3.9/library/re.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the addition of the emoticon characters to the end of the cleaned
    document strings may not look like the most elegant approach, we must note that
    the order of the words doesn’t matter in our bag-of-words model if our vocabulary
    consists of only one-word tokens. But before we talk more about the splitting
    of documents into individual terms, words, or tokens, let’s confirm that our `preprocessor`
    function works correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, since we will make use of the *cleaned* text data over and over again
    during the next sections, let’s now apply our `preprocessor` function to all the
    movie reviews in our `DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Processing documents into tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After successfully preparing the movie review dataset, we now need to think
    about how to split the text corpora into individual elements. One way to *tokenize*
    documents is to split them into individual words by splitting the cleaned documents
    at their whitespace characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the context of tokenization, another useful technique is **word stemming**,
    which is the process of transforming a word into its root form. It allows us to
    map related words to the same stem. The original stemming algorithm was developed
    by Martin F. Porter in 1979 and is hence known as the **Porter stemmer** algorithm
    (*An algorithm for suffix stripping* by *Martin F. Porter*, *Program: Electronic
    Library and Information Systems*, 14(3): 130–137, 1980). The **Natural Language
    Toolkit** (**NLTK**, [http://www.nltk.org](http://www.nltk.org)) for Python implements
    the Porter stemming algorithm, which we will use in the following code section.
    To install the NLTK, you can simply execute `conda install nltk` or `pip install
    nltk`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NLTK online book**'
  prefs: []
  type: TYPE_NORMAL
- en: Although the NLTK is not the focus of this chapter, I highly recommend that
    you visit the NLTK website as well as read the official NLTK book, which is freely
    available at [http://www.nltk.org/book/](http://www.nltk.org/book/), if you are
    interested in more advanced applications in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to use the Porter stemming algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Using the `PorterStemmer` from the `nltk` package, we modified our `tokenizer`
    function to reduce words to their root form, which was illustrated by the simple
    preceding example where the word `'running'` was *stemmed* to its root form `'run'`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming algorithms**'
  prefs: []
  type: TYPE_NORMAL
- en: The Porter stemming algorithm is probably the oldest and simplest stemming algorithm.
    Other popular stemming algorithms include the newer Snowball stemmer (Porter2
    or English stemmer) and the Lancaster stemmer (Paice/Husk stemmer). While both
    the Snowball and Lancaster stemmers are faster than the original Porter stemmer,
    the Lancaster stemmer is also notorious for being more aggressive than the Porter
    stemmer, which means that it will produce shorter and more obscure words. These
    alternative stemming algorithms are also available through the NLTK package ([http://www.nltk.org/api/nltk.stem.html](http://www.nltk.org/api/nltk.stem.html)).
  prefs: []
  type: TYPE_NORMAL
- en: While stemming can create non-real words, such as `'thu'` (from `'thus'`), as
    shown in the previous example, a technique called *lemmatization* aims to obtain
    the canonical (grammatically correct) forms of individual words—the so-called
    *lemmas*. However, lemmatization is computationally more difficult and expensive
    compared to stemming and, in practice, it has been observed that stemming and
    lemmatization have little impact on the performance of text classification (*Influence
    of Word Normalization on Text Classification*, by *Michal Toman*, *Roman Tesar*,
    and *Karel Jezek*, *Proceedings of InSciT*, pages 354–358, 2006).
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into the next section, where we will train a machine learning
    model using the bag-of-words model, let’s briefly talk about another useful topic
    called **stop word removal**. Stop words are simply those words that are extremely
    common in all sorts of texts and probably bear no (or only a little) useful information
    that can be used to distinguish between different classes of documents. Examples
    of stop words are *is*, *and*, *has*, and *like*. Removing stop words can be useful
    if we are working with raw or normalized term frequencies rather than tf-idfs,
    which already downweight the frequently occurring words.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove stop words from the movie reviews, we will use the set of 127 English
    stop words that is available from the NLTK library, which can be obtained by calling
    the `nltk.download` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After we download the stop words set, we can load and apply the English stop
    word set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Training a logistic regression model for document classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will train a logistic regression model to classify the
    movie reviews into *positive* and *negative* reviews based on the bag-of-words
    model. First, we will divide the `DataFrame` of cleaned text documents into 25,000
    documents for training and 25,000 documents for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use a `GridSearchCV` object to find the optimal set of parameters
    for our logistic regression model using 5-fold stratified cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that for the logistic regression classifier, we are using the LIBLINEAR
    solver as it can perform better than the default choice (`'lbfgs'`) for relatively
    large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multiprocessing via the n_jobs parameter**'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we highly recommend setting `n_jobs=-1` (instead of `n_jobs=1`,
    as in the previous code example) to utilize all available cores on your machine
    and speed up the grid search. However, some Windows users reported issues when
    running the previous code with the `n_jobs=-1` setting related to pickling the
    `tokenizer` and `tokenizer_porter` functions for multiprocessing on Windows. Another
    workaround would be to replace those two functions, `[tokenizer, tokenizer_porter]`,
    with `[str.split]`. However, note that replacement by the simple `str.split` would
    not support stemming.
  prefs: []
  type: TYPE_NORMAL
- en: When we initialized the `GridSearchCV` object and its parameter grid using the
    preceding code, we restricted ourselves to a limited number of parameter combinations,
    since the number of feature vectors, as well as the large vocabulary, can make
    the grid search computationally quite expensive. Using a standard desktop computer,
    our grid search may take 5-10 minutes to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous code example, we replaced `CountVectorizer` and `TfidfTransformer`
    from the previous subsection with `TfidfVectorizer`, which combines `CountVectorizer`
    with the `TfidfTransformer`. Our `param_grid` consisted of two parameter dictionaries.
    In the first dictionary, we used `TfidfVectorizer` with its default settings (`use_idf=True`,
    `smooth_idf=True`, and `norm=''l2''`) to calculate the tf-idfs; in the second
    dictionary, we set those parameters to `use_idf=False`, `smooth_idf=False`, and
    `norm=None` in order to train a model based on raw term frequencies. Furthermore,
    for the logistic regression classifier itself, we trained models using L2 regularization
    via the penalty parameter and compared different regularization strengths by defining
    a range of values for the inverse-regularization parameter `C`. As an optional
    exercise, you are also encouraged to add L1 regularization to the parameter grid
    by changing `''clf__penalty'': [''l2'']` to `''clf__penalty'': [''l2'', ''l1'']`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the grid search has finished, we can print the best parameter set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding output, we obtained the best grid search results
    using the regular `tokenizer` without Porter stemming, no stop word library, and
    tf-idfs in combination with a logistic regression classifier that uses L2-regularization
    with the regularization strength `C` of `10.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the best model from this grid search, let’s print the average 5-fold
    cross-validation accuracy scores on the training dataset and the classification
    accuracy on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The results reveal that our machine learning model can predict whether a movie
    review is positive or negative with 90 percent accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '**The naïve Bayes classifier**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A still very popular classifier for text classification is the naïve Bayes
    classifier, which gained popularity in applications of email spam filtering. Naïve
    Bayes classifiers are easy to implement, computationally efficient, and tend to
    perform particularly well on relatively small datasets compared to other algorithms.
    Although we don’t discuss naïve Bayes classifiers in this book, the interested
    reader can find an article about naïve Bayes text classification that is freely
    available on arXiv (*Naive Bayes and Text Classification I – Introduction and
    Theory* by *S. Raschka*, *Computing Research Repository* (*CoRR*), abs/1410.5329,
    2014, [http://arxiv.org/pdf/1410.5329v3.pdf](http://arxiv.org/pdf/1410.5329v3.pdf)).
    Different versions of na**ï**ve Bayes classifiers referenced in this article are
    implemented in scikit-learn. You can find an overview page with links to the respective
    code classes here: [https://scikit-learn.org/stable/modules/naive_bayes.html](https://scikit-learn.org/stable/modules/naive_bayes.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Working with bigger data – online algorithms and out-of-core learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you executed the code examples in the previous section, you may have noticed
    that it could be computationally quite expensive to construct the feature vectors
    for the 50,000-movie review dataset during a grid search. In many real-world applications,
    it is not uncommon to work with even larger datasets that can exceed our computer’s
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Since not everyone has access to supercomputer facilities, we will now apply
    a technique called **out-of-core learning**, which allows us to work with such
    large datasets by fitting the classifier incrementally on smaller batches of a
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Text classification with recurrent neural networks**'
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 15*, *Modeling Sequential Data Using Recurrent Neural Networks*,
    we will revisit this dataset and train a deep learning-based classifier (a recurrent
    neural network) to classify the reviews in the IMDb movie review dataset. This
    neural network-based classifier follows the same out-of-core principle using the
    stochastic gradient descent optimization algorithm, but does not require the construction
    of a bag-of-words model.
  prefs: []
  type: TYPE_NORMAL
- en: Back in *Chapter 2*, *Training Simple Machine Learning Algorithms for Classification*,
    the concept of **stochastic gradient descent** was introduced; it is an optimization
    algorithm that updates the model’s weights using one example at a time. In this
    section, we will make use of the `partial_fit` function of `SGDClassifier` in
    scikit-learn to stream the documents directly from our local drive and train a
    logistic regression model using small mini-batches of documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define a `tokenizer` function that cleans the unprocessed text
    data from the `movie_data.csv` file that we constructed at the beginning of this
    chapter and separates it into word tokens while removing stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a generator function, `stream_docs`, that reads in and
    returns one document at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that our `stream_docs` function works correctly, let’s read in the
    first document from the `movie_data.csv` file, which should return a tuple consisting
    of the review text as well as the corresponding class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define a function, `get_minibatch`, that will take a document stream
    from the `stream_docs` function and return a particular number of documents specified
    by the `size` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, we can’t use `CountVectorizer` for out-of-core learning since
    it requires holding the complete vocabulary in memory. Also, `TfidfVectorizer`
    needs to keep all the feature vectors of the training dataset in memory to calculate
    the inverse document frequencies. However, another useful vectorizer for text
    processing implemented in scikit-learn is `HashingVectorizer`. `HashingVectorizer`
    is data-independent and makes use of the hashing trick via the 32-bit `MurmurHash3`
    function by Austin Appleby (you can find more information about MurmurHash at
    [https://en.wikipedia.org/wiki/MurmurHash](https://en.wikipedia.org/wiki/MurmurHash)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding code, we initialized `HashingVectorizer` with our `tokenizer`
    function and set the number of features to `2**21`. Furthermore, we reinitialized
    a logistic regression classifier by setting the `loss` parameter of `SGDClassifier`
    to `'log'`. Note that by choosing a large number of features in `HashingVectorizer`,
    we reduce the chance of causing hash collisions, but we also increase the number
    of coefficients in our logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now comes the really interesting part—having set up all the complementary functions,
    we can start the out-of-core learning using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we made use of the PyPrind package to estimate the progress of our learning
    algorithm. We initialized the progress bar object with 45 iterations and, in the
    following `for` loop, we iterated over 45 mini-batches of documents where each
    mini-batch consists of 1,000 documents. Having completed the incremental learning
    process, we will use the last 5,000 documents to evaluate the performance of our
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**NoneType error**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that if you encounter a `NoneType` error, you may have executed
    the `X_test, y_test = get_minibatch(...)` code twice. Via the previous loop, we
    have 45 iterations where we fetch 1,000 documents each. Hence, there are exactly
    5,000 documents left for testing, which we assign via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If we execute this code twice, then there are not enough documents left in the
    generator, and `X_test` returns `None`. Hence, if you encounter the `NoneType`
    error, you have to start at the previous `stream_docs(...)` code again.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the accuracy of the model is approximately 87 percent, slightly
    below the accuracy that we achieved in the previous section using the grid search
    for hyperparameter tuning. However, out-of-core learning is very memory efficient,
    and it took less than a minute to complete.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can use the last 5,000 documents to update our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**The word2vec model**'
  prefs: []
  type: TYPE_NORMAL
- en: A more modern alternative to the bag-of-words model is word2vec, an algorithm
    that Google released in 2013 (*Efficient Estimation of Word Representations in
    Vector Space* by *T. Mikolov*, *K. Chen*, *G. Corrado*, and *J. Dean*, [https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)).
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec algorithm is an unsupervised learning algorithm based on neural
    networks that attempts to automatically learn the relationship between words.
    The idea behind word2vec is to put words that have similar meanings into similar
    clusters, and via clever vector spacing, the model can reproduce certain words
    using simple vector math, for example, *king* – *man* + *woman* = *queen*.
  prefs: []
  type: TYPE_NORMAL
- en: The original C-implementation with useful links to the relevant papers and alternative
    implementations can be found at [https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/).
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with latent Dirichlet allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Topic modeling** describes the broad task of assigning topics to unlabeled
    text documents. For example, a typical application is the categorization of documents
    in a large text corpus of newspaper articles. In applications of topic modeling,
    we then aim to assign category labels to those articles, for example, sports,
    finance, world news, politics, and local news. Thus, in the context of the broad
    categories of machine learning that we discussed in *Chapter 1*, *Giving Computers
    the Ability to Learn from Data*, we can consider topic modeling as a clustering
    task, a subcategory of unsupervised learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss a popular technique for topic modeling called
    **latent Dirichlet allocation** (**LDA**). However, note that while latent Dirichlet
    allocation is often abbreviated as LDA, it is not to be confused with *linear
    discriminant analysis*, a supervised dimensionality reduction technique that was
    introduced in *Chapter 5*, *Compressing Data via Dimensionality Reduction*.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposing text documents with LDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the mathematics behind LDA is quite involved and requires knowledge of
    Bayesian inference, we will approach this topic from a practitioner’s perspective
    and interpret LDA using layman’s terms. However, the interested reader can read
    more about LDA in the following research paper: *Latent Dirichlet Allocation*,
    by *David M. Blei*, *Andrew Y. Ng*, and *Michael I. Jordan*, *Journal of Machine
    Learning Research 3*, pages: 993-1022, Jan 2003, [https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: LDA is a generative probabilistic model that tries to find groups of words that
    appear frequently together across different documents. These frequently appearing
    words represent our topics, assuming that each document is a mixture of different
    words. The input to an LDA is the bag-of-words model that we discussed earlier
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: A document-to-topic matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word-to-topic matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA decomposes the bag-of-words matrix in such a way that if we multiply those
    two matrices together, we will be able to reproduce the input, the bag-of-words
    matrix, with the lowest possible error. In practice, we are interested in those
    topics that LDA found in the bag-of-words matrix. The only downside may be that
    we must define the number of topics beforehand—the number of topics is a hyperparameter
    of LDA that has to be specified manually.
  prefs: []
  type: TYPE_NORMAL
- en: LDA with scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will use the `LatentDirichletAllocation` class implemented
    in scikit-learn to decompose the movie review dataset and categorize it into different
    topics. In the following example, we will restrict the analysis to 10 different
    topics, but readers are encouraged to experiment with the hyperparameters of the
    algorithm to further explore the topics that can be found in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we are going to load the dataset into a pandas `DataFrame` using the
    local `movie_data.csv` file of the movie reviews that we created at the beginning
    of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Next, we are going to use the already familiar `CountVectorizer` to create the
    bag-of-words matrix as input to the LDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we will use scikit-learn’s built-in English stop word library
    via `stop_words=''english''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we set the maximum document frequency of words to be considered
    to 10 percent (`max_df=.1`) to exclude words that occur too frequently across
    documents. The rationale behind the removal of frequently occurring words is that
    these might be common words appearing across all documents that are, therefore,
    less likely to be associated with a specific topic category of a given document.
    Also, we limited the number of words to be considered to the most frequently occurring
    5,000 words (`max_features=5000`), to limit the dimensionality of this dataset
    to improve the inference performed by LDA. However, both `max_df=.1` and `max_features=5000`
    are hyperparameter values chosen arbitrarily, and readers are encouraged to tune
    them while comparing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example demonstrates how to fit a `LatentDirichletAllocation`
    estimator to the bag-of-words matrix and infer the 10 different topics from the
    documents (note that the model fitting can take up to 5 minutes or more on a laptop
    or standard desktop computer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: By setting `learning_method='batch'`, we let the `lda` estimator do its estimation
    based on all available training data (the bag-of-words matrix) in one iteration,
    which is slower than the alternative `'online'` learning method, but can lead
    to more accurate results (setting `learning_method='online'` is analogous to online
    or mini-batch learning, which we discussed in *Chapter 2*, *Training Simple Machine
    Learning Algorithms for Classification*, and previously in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: '**Expectation-maximization**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit-learn library’s implementation of LDA uses the **expectation-maximization**
    (**EM**) algorithm to update its parameter estimates iteratively. We haven’t discussed
    the EM algorithm in this chapter, but if you are curious to learn more, please
    see the excellent overview on Wikipedia ([https://en.wikipedia.org/wiki/Expectation–maximization_algorithm](https://en.wikipedia.org/wiki/Expectation–maximization_algorithm))
    and the detailed tutorial on how it is used in LDA in Colorado Reed’s tutorial,
    *Latent Dirichlet Allocation: Towards a Deeper Understanding*, which is freely
    available at [http://obphio.us/pdfs/lda_tutorial.pdf](http://obphio.us/pdfs/lda_tutorial.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After fitting the LDA, we now have access to the `components_` attribute of
    the `lda` instance, which stores a matrix containing the word importance (here,
    `5000`) for each of the 10 topics in increasing order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'To analyze the results, let’s print the five most important words for each
    of the 10 topics. Note that the word importance values are ranked in increasing
    order. Thus, to print the top five words, we need to sort the topic array in reverse
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on reading the five most important words for each topic, you may guess
    that the LDA identified the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Generally bad movies (not really a topic category)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Movies about families
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: War movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Art movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Crime movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Horror movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comedy movie reviews
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Movies somehow related to TV shows
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Movies based on books
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Action movies
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To confirm that the categories make sense based on the reviews, let’s plot
    three movies from the horror movie category (horror movies belong to category
    6 at index position `5`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the preceding code example, we printed the first 300 characters from
    the top three horror movies. The reviews—even though we don’t know which exact
    movie they belong to—sound like reviews of horror movies (however, one might argue
    that `Horror movie #2` could also be a good fit for topic category 1: *Generally
    bad movies*).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use machine learning algorithms to classify
    text documents based on their polarity, which is a basic task in sentiment analysis
    in the field of NLP. Not only did you learn how to encode a document as a feature
    vector using the bag-of-words model, but you also learned how to weight the term
    frequency by relevance using tf-idf.
  prefs: []
  type: TYPE_NORMAL
- en: Working with text data can be computationally quite expensive due to the large
    feature vectors that are created during this process; in the last section, we
    covered how to utilize out-of-core or incremental learning to train a machine
    learning algorithm without loading the whole dataset into a computer’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you were introduced to the concept of topic modeling using LDA to categorize
    the movie reviews into different categories in an unsupervised fashion.
  prefs: []
  type: TYPE_NORMAL
- en: So far, in this book, we have covered many machine learning concepts, best practices,
    and supervised models for classification. In the next chapter, we will look at
    another subcategory of supervised learning, *regression analysis*, which lets
    us predict outcome variables on a continuous scale, in contrast to the categorical
    class labels of the classification models that we have been working with so far.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
