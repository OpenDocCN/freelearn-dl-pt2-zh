- en: Getting Started with Reinforcement Learning and PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用强化学习和PyTorch
- en: We kick off our journey of practical reinforcement learning and PyTorch with
    the basic, yet important, reinforcement learning algorithms, including random
    search, hill climbing, and policy gradient. We will start by setting up the working
    environment and OpenAI Gym, and you will become familiar with reinforcement learning
    environments through the Atari and CartPole playgrounds. We will also demonstrate
    how to develop algorithms to solve the CartPole problem step by step. Also, we
    will review the essentials of PyTorch and prepare for the upcoming learning examples
    and projects.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始了实用强化学习和PyTorch之旅，使用基本但重要的强化学习算法，包括随机搜索、爬山和策略梯度。我们将从设置工作环境和OpenAI Gym开始，通过Atari和CartPole游乐场熟悉强化学习环境。我们还将逐步演示如何开发算法来解决CartPole问题。此外，我们将回顾PyTorch的基础知识，并为即将进行的学习示例和项目做准备。
- en: 'This chapter contains the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下操作：
- en: Setting up the working environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: Installing OpenAI Gym
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装OpenAI Gym
- en: Simulating Atari environments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟Atari环境
- en: Simulating the CartPole environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模拟CartPole环境
- en: Reviewing the fundamentals of PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾PyTorch的基本原理
- en: Implementing and evaluating a random search policy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和评估随机搜索策略
- en: Developing the hill-climbing algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发爬山算法
- en: Developing a policy gradient algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发策略梯度算法
- en: Setting up the working environment
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: Let's get started with setting up the working environment, including the correct
    versions of Python and Anaconda, and PyTorch as the main framework that is used
    throughout the book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始设置工作环境，包括正确版本的Python和Anaconda，以及作为本书主要框架的PyTorch。
- en: Python is the language we use to implement all reinforcement learning algorithms
    and techniques throughout the book. In this book, we will be using Python 3, or
    more specifically, 3.6 or above. If you are a Python 2 user, now is the best time
    for you to switch to Python 3, as Python 2 will no longer be supported after 2020\.
    The transition is very smooth, though, so don't panic.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Python是本书中实现所有强化学习算法和技术的语言。在本书中，我们将使用Python 3，具体来说是3.6或以上版本。如果您仍在使用Python 2，现在是切换到Python
    3的最佳时机，因为Python 2将在2020年后不再受支持。不过过渡非常顺利，不必担心。
- en: '**Anaconda** is an open source Python distribution ([www.anaconda.com/distribution/](http://www.anaconda.com/distribution/))
    for data science and machine learning. We will be using Anaconda''s package manager,
    `conda`, to install Python packages, along with `pip`.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**Anaconda**是一个开源的Python发行版（[www.anaconda.com/distribution/](http://www.anaconda.com/distribution/)），用于数据科学和机器学习。我们将使用Anaconda的包管理器`conda`来安装Python包，以及`pip`。'
- en: '**PyTorch** ([https://pytorch.org/](https://pytorch.org/)), primarily developed
    by the Facebook AI Research (FAIR) Group, is a trendy machine learning library
    based on Torch ([http://torch.ch/](http://torch.ch/)). Tensors in PyTorch replace
    NumPy''s `ndarrays`, which provides more flexibility and compatibility with GPUs.
    Because of the powerful computational graphs and the simple and friendly interface,
    the PyTorch community is expanding on a daily basis, and it has seen heavy adoption
    by more and more tech giants.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyTorch**（[https://pytorch.org/](https://pytorch.org/)），主要由Facebook AI Research（FAIR）组开发，是基于Torch（[http://torch.ch/](http://torch.ch/)）的流行机器学习库。PyTorch中的张量取代了NumPy的`ndarrays`，提供了更多的灵活性和与GPU的兼容性。由于强大的计算图和简单友好的接口，PyTorch社区每天都在扩展，越来越多的技术巨头也在采用它。'
- en: Let's see how to properly set up all of these components.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何正确设置所有这些组件。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will begin by installing Anaconda. You can skip this if you already have
    Anaconda for Python 3.6 or 3.7 running on your system. Otherwise, you can follow
    the instructions at [https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/)
    for your operating system, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从安装Anaconda开始。如果您的系统已经运行Python 3.6或3.7的Anaconda，则可以跳过此步骤。否则，您可以按照以下操作系统的说明安装，链接如下：
- en: '![](img/be97f9c3-7d19-46a4-a21e-257e230c53df.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be97f9c3-7d19-46a4-a21e-257e230c53df.png)'
- en: 'Feel free to play around with PyTorch once the setup is done. To verify that
    you have the right setup of Anaconda and Python, you can enter the following line
    in your Terminal in Linux/Mac or Command Prompt in Windows (from now on, we will
    just call it Terminal):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，可以随意使用 PyTorch 进行实验。要验证你是否正确设置了 Anaconda 和 Python，请在 Linux/Mac 的终端或 Windows
    的命令提示符中输入以下命令（从现在起，我们将简称为终端）：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It will display your Python Anaconda environment. You should see something
    similar to the following screenshot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 它将显示你的 Python Anaconda 环境。你应该看到类似以下截图：
- en: '![](img/af958adb-6047-463f-a401-7ad60b524f18.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af958adb-6047-463f-a401-7ad60b524f18.png)'
- en: If Anaconda and Python 3.x are not mentioned, please check the system path or
    the path Python is running from.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未提到 Anaconda 和 Python 3.x，请检查系统路径或 Python 运行路径。
- en: 'The next thing to do is to install PyTorch. First, go to [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    and pick the description of your environment from the following table:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步要做的是安装 PyTorch。首先，前往 [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)，然后从以下表格中选择你的环境描述：
- en: '![](img/dd781986-8e61-4116-afc7-b7821cfda18f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd781986-8e61-4116-afc7-b7821cfda18f.png)'
- en: 'Here, we use **Mac**, **Conda**, **Python 3.7**, and running locally (no CUDA)
    as an example, and enter the resulting command line in the Terminal:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们以 **Mac**，**Conda**，**Python 3.7** 以及在本地运行（没有 CUDA）作为示例，并在终端中输入生成的命令行：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To confirm PyTorch is installed correctly, run the following lines of code
    in Python:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要确认 PyTorch 是否正确安装，请在 Python 中运行以下代码：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If a 3 x 4 matrix is displayed, that means PyTorch is installed correctly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果显示了一个 3 x 4 的矩阵，则表示 PyTorch 安装正确。
- en: Now we have successfully set up the working environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功设置好工作环境。
- en: How it works...
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: We have just created a tensor of size 3 x 4 in PyTorch. It is an empty matrix.
    By saying `empty`, this doesn't mean all elements are of the value `Null`. Instead,
    they are a bunch of meaningless floats that are considered placeholders. Users
    are required to set all the values later. This is very similar to NumPy's empty
    array.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在 PyTorch 中创建了一个大小为 3 x 4 的张量。它是一个空矩阵。所谓的 `empty` 并不意味着所有元素都是 `Null` 的值。相反，它们是一堆没有意义的浮点数，被视为占位符。用户需要稍后设置所有的值。这与
    NumPy 的空数组非常相似。
- en: There's more...
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Some of you may question the necessity of installing Anaconda and using `conda`
    to manage packages since it is easy to install packages with `pip`. In fact, `conda`
    is a better packaging tool than `pip`. We mainly use `conda` for the following
    four reasons:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会质疑安装 Anaconda 和使用 `conda` 管理包的必要性，因为使用 `pip` 安装包很容易。事实上，`conda` 是比 `pip`
    更好的打包工具。我们主要使用 `conda` 有以下四个理由：
- en: '**It handles library dependencies nicely**: Installing a package with `conda`
    will automatically download all of its dependencies. However, doing so with `pip`
    will lead to a warning, and installation will be aborted.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它能很好地处理库依赖关系**：使用 `conda` 安装包会自动下载其所有依赖项。但是，使用 `pip` 则会导致警告并中止安装。'
- en: '**It solves conflicts of packages gracefully**: If installing a package requires
    another package of a specific version (let''s say 2.3 or after, for example),
    `conda` will update the version of the other package automatically.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它能优雅地解决包的冲突**：如果安装一个包需要另一个特定版本的包（比如说 2.3 或之后的版本），`conda` 将自动更新另一个包的版本。'
- en: '**It creates a virtual environment easily**: A virtual environment is a self-contained
    package directory tree. Different applications or projects can use different virtual
    environments. All virtual environments are isolated from each other. It is recommended
    to use virtual environments so that whatever we do for one application doesn''t
    affect our system environment or any other environment.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它能轻松创建虚拟环境**：虚拟环境是一个自包含的包目录树。不同的应用程序或项目可以使用不同的虚拟环境。所有虚拟环境彼此隔离。建议使用虚拟环境，这样我们为一个应用程序所做的任何操作都不会影响我们的系统环境或任何其他环境。'
- en: '**It is also compatible with pip**: We can still use `pip` in `conda` with
    the following command:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**它也与 pip 兼容**：我们仍然可以在 `conda` 中使用 `pip`，使用以下命令：'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See also
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'If you are interested in learning more about `conda`, feel free to check out
    the following resources:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对学习使用 `conda` 感兴趣，请随意查看以下资源：
- en: '**Conda user guide**: [https://conda.io/projects/conda/en/latest/user-guide/index.html](https://conda.io/projects/conda/en/latest/user-guide/index.html)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Conda 用户指南**：[https://conda.io/projects/conda/en/latest/user-guide/index.html](https://conda.io/projects/conda/en/latest/user-guide/index.html)'
- en: '**Creating and managing virtual environments with conda**: [https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 conda 创建和管理虚拟环境**：[https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)'
- en: 'If you want to get more familiar with PyTorch, you can go through the *Getting
    Started* section in the official tutorial at [https://pytorch.org/tutorials/#getting-started](https://pytorch.org/tutorials/#getting-started).
    We recommend you at least finish the following:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更加熟悉 PyTorch，可以查看官方教程中的*入门*部分，位于[https://pytorch.org/tutorials/#getting-started](https://pytorch.org/tutorials/#getting-started)。我们建议至少完成以下内容：
- en: '**What is PyTorch**: [https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**什么是 PyTorch**：[https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)'
- en: '**Learning PyTorch with examples**: [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习 PyTorch 的示例**：[https://pytorch.org/tutorials/beginner/pytorch_with_examples.html](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)'
- en: Installing OpenAI Gym
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 OpenAI Gym
- en: After setting up the working environment, we can now install OpenAI Gym. You
    can't work on reinforcement learning without using OpenAI Gym, which gives you
    a variety of environments in which to develop your learning algorithms.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 设置工作环境后，我们现在可以安装 OpenAI Gym。在不使用 OpenAI Gym 的情况下，您无法进行强化学习，该工具为您提供了多种环境，用于开发学习算法。
- en: '**OpenAI** ([https://openai.com/](https://openai.com/)) is a non-profit research
    company that is focused on building safe **artificial general intelligence** (**AGI**)
    and ensuring that it benefits humans. **OpenAI Gym** is a powerful and open source
    toolkit for developing and comparing reinforcement learning algorithms. It provides
    an interface to varieties of reinforcement learning simulations and tasks, from
    walking to moon landing, from car racing to playing Atari games. See [https://gym.openai.com/envs/](https://gym.openai.com/envs/)
    for the full list of environments.We can write **agents** to interact with OpenAI
    Gym environments using any numerical computation library, such as PyTorch, TensorFlow,
    or Keras.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI** ([https://openai.com/](https://openai.com/)) 是一家致力于构建安全的**人工通用智能**（**AGI**）并确保其造福于人类的非营利性研究公司。**OpenAI
    Gym** 是一个强大且开源的工具包，用于开发和比较强化学习算法。它为多种强化学习仿真和任务提供接口，涵盖从步行到登月，从汽车赛车到玩 Atari 游戏的各种场景。查看[https://gym.openai.com/envs/](https://gym.openai.com/envs/)获取完整的环境列表。我们可以使用诸如
    PyTorch、TensorFlow 或 Keras 等任何数值计算库编写**代理**，与 OpenAI Gym 环境进行交互。'
- en: How to do it...
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'There are two ways to install Gym. The first one is to use `pip`, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以安装 Gym。第一种是使用`pip`，如下所示：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For `conda` users, remember to install `pip` first in `conda` using the following
    command before installing Gym using `pip`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`conda`用户，请记住在使用`pip`安装 Gym 之前，首先在`conda`中安装`pip`，使用以下命令：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is because Gym is not officially available in `conda` as of early 2019.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为截至2019年初，Gym 尚未正式在`conda`中提供。
- en: 'Another approach is to build from source:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是从源代码构建：
- en: 'First, clone the package directly from its Git repository:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，直接从其 Git 仓库克隆该包：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Go to the downloaded folder and install Gym from there:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到下载的文件夹，并从那里安装 Gym：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: And now you are good to go. Feel free to play around with `gym`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以开始了。随意尝试使用`gym`玩耍。
- en: 'You can also check the available `gym` environment by typing the following
    lines of code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以通过输入以下代码行来检查可用的`gym`环境：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will give you a long list of environments if you installed Gym properly.
    We will play around with some of them in the next recipe, *Simulating Atari environments*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正确安装了 Gym，这将为您提供一个环境的长列表。我们将在下一个示例*模拟 Atari 环境*中尝试其中的一些。
- en: How it works...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何运行的...
- en: Compared to the simple `pip` approach for installing Gym, the second approach
    provides more flexibility if you want to add new environments and modify Gym itself.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用简单的`pip`方法安装 Gym 相比，第二种方法在您想要添加新环境和修改 Gym 本身时提供更大的灵活性。
- en: There's more...
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: You may wonder why we need to test reinforcement learning algorithms on Gym's
    environments since the actual environments we work in can be a lot different.
    You will recall that reinforcement learning doesn't make many assumptions about
    the environment, but it gets to know more about the environment by interacting
    with it. Also, when comparing the performance of different algorithms, we need
    to apply them to standardized environments. Gym is a perfect benchmark, covering
    many versatile and easy-to-use environments. This is similar to the datasets that
    we often use as benchmarks in supervised and unsupervised learning, such as MNIST,
    Imagenet, MovieLens, and Thomson Reuters News.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想为什么我们需要在Gym的环境中测试强化学习算法，因为我们实际工作中的环境可能会大不相同。你会想起强化学习并不对环境做出太多假设，但通过与环境的交互来更多地了解它。此外，在比较不同算法的性能时，我们需要将它们应用于标准化的环境中。Gym是一个完美的基准测试，涵盖了许多多功能和易于使用的环境。这与我们在监督和无监督学习中经常使用的数据集类似，如MNIST、Imagenet、MovieLens和Thomson
    Reuters News。
- en: See also
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: Take a look at the official Gym documentation at [https://gym.openai.com/docs/](https://gym.openai.com/docs/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 查看官方Gym文档，请访问[https://gym.openai.com/docs/](https://gym.openai.com/docs/)。
- en: Simulating Atari environments
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟Atari环境
- en: To get started with Gym, let's play some Atari games with it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用Gym，让我们玩一些Atari游戏。
- en: The Atari environments ([https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari))
    are a variety of **Atari 2600** video games, such as Alien, AirRaid, Pong, and
    Space Race. If you have ever played Atari games, this recipe should be fun for
    you, as you will play an Atari game, Space Invaders. However, an agent will act
    on your behalf.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Atari环境（[https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)）是各种Atari
    2600视频游戏，如Alien、AirRaid、Pong和Space Race。如果你曾经玩过Atari游戏，这个步骤应该很有趣，因为你将玩一个Atari游戏，Space
    Invaders。然而，一个代理将代表你行动。
- en: How to do it...
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s simulate the Atari environments by following these steps:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤模拟Atari环境：
- en: 'To run any `atari` environment for the first time, we need to install the `atari`
    dependencies by running this command in the Terminal:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一次运行任何`atari`环境时，我们需要通过在终端中运行以下命令安装`atari`依赖项：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, if you used the second approach in the previous recipe to `install
    gym`, you can run the following command instead:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你在上一个步骤中使用了第二种方法来`安装gym`，你可以运行以下命令代替：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After installing the Atari dependencies, we import the `gym` library in Python:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完Atari依赖项后，我们在Python中导入`gym`库：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create an instance of the `SpaceInvaders` environment:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SpaceInvaders`环境的实例：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reset the environment:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, this also returns the initial state of the environment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，这也会返回环境的初始状态。
- en: 'Render the environment:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You will see a small window popping up, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到一个小窗口弹出，如下所示：
- en: '![](img/8a0f5de0-ef4a-4ca8-a069-84c9a1a27948.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a0f5de0-ef4a-4ca8-a069-84c9a1a27948.png)'
- en: As you can see from the game window, the spaceship starts with three lives (the
    red spaceships).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从游戏窗口看到的，飞船从三条生命（红色飞船）开始。
- en: 'Randomly pick one possible move and execute the action:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个可能的动作并执行它：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `step()` method returns what happens after an action is taken, including
    the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()`方法返回在执行动作后发生的事情，包括以下内容：'
- en: '**New state**: The new observation.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新状态**：新的观察。'
- en: '**Reward**: The reward associated with that action in that state.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：与该动作在该状态下相关联的奖励。'
- en: '**Is done**: A flag indicating whether the game ends. In a `SpaceInvaders`
    environment, this will be `True` if the spaceship has no more lives left or all
    the aliens are gone; otherwise, it will remain `False`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**是否完成**：指示游戏是否结束的标志。在`SpaceInvaders`环境中，如果飞船没有更多生命或者所有外星人都消失了，这将为`True`；否则，它将保持为`False`。'
- en: '**Info**: Extra information pertaining to the environment. This is about the
    number of lives left in this case. This is useful for debugging.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息**：与环境相关的额外信息。这是关于当前剩余生命的数量。这在调试时非常有用。'
- en: 'Let’s take a look at the `is_done` flag and `info`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`is_done`标志和`info`：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we render the environment:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们渲染环境：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The game window becomes the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏窗口变成了以下样子：
- en: '![](img/f9a47f76-d9e2-49a6-96cd-c7272298495d.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9a47f76-d9e2-49a6-96cd-c7272298495d.png)'
- en: You won't notice much difference in the game window, because the spaceship just
    made a move.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏窗口中你不会注意到太大的差异，因为飞船只是移动了一下。
- en: 'Now, let''s make a `while` loop and let the agent perform as many actions as
    it can:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个`while`循环，让代理尽可能执行多个动作：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Meanwhile, you will see that the game is running, and the spaceship keeps moving
    and shooting, and so do the aliens. And it is pretty fun to watch, too. At the
    end, when the game ends, the window looks like the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，您会看到游戏正在运行，飞船不断移动和射击，外星人也是如此。观看起来也挺有趣的。最后，当游戏结束时，窗口如下所示：
- en: '![](img/7238068c-e6ed-43be-ae91-246820a14162.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7238068c-e6ed-43be-ae91-246820a14162.png)'
- en: As you can see, we scored 150 points in this game. You may get a higher or lower
    score than this because the actions the agent performs are all randomly selected.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，我们在这个游戏中得了150分。您可能会得到比这更高或更低的分数，因为代理执行的动作是随机选择的。
- en: 'We also confirm that no lives are left with the last piece of info:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还确认最后一条信息中没有剩余的生命：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Using Gym, we can easily create an environment instance by calling the `make()`
    method with the name of the environment as the parameter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gym，我们可以通过调用`make()`方法并以环境名称作为参数轻松创建一个环境实例。
- en: As you may have noticed, the actions that the agent performs are randomly chosen
    using the `sample()` method.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能已经注意到的，代理执行的动作是使用`sample()`方法随机选择的。
- en: Note that, normally, we would have a more sophisticated agent guided by reinforcement
    learning algorithms. Here, we just demonstrated how to simulate an environment,
    and how an agent takes actions regardless of the outcome.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通常情况下，我们会有一个更复杂的由强化学习算法引导的代理。在这里，我们只是演示了如何模拟一个环境以及代理如何无视结果而采取行动。
- en: 'Run this a few times and see what we get:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多次运行这个程序，看看我们能得到什么：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'There are six possible actions in total. We can also see this by running the
    following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有六个可能的动作。我们还可以通过运行以下命令来查看：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Actions from 0 to 5 stand for No Operation, Fire, Up, Right, Left, and Down,
    respectively, which are all the moves the spaceship in the game can do.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从0到5的动作分别代表无操作、开火、向上、向右、向左和向下，这些是游戏中飞船可以执行的所有移动。
- en: The `step()` method will let the agent take the action that is specified as
    its parameter. The `render()` method will update the display window based on the
    latest observation of the environment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`step()`方法将让代理执行指定为其参数的动作。`render()`方法将根据环境的最新观察更新显示窗口。'
- en: 'The observation of the environment, `new_state`, is represented by a 210 x
    160 x 3 matrix, as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的观察值`new_state`由一个210 x 160 x 3的矩阵表示，如下所示：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This means that each frame of the display screen is an RGB image of size 210
    x 160.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着显示屏的每一帧都是一个大小为210 x 160的RGB图像。
- en: There's more...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这还不是全部...
- en: You may wonder why we need to install Atari dependencies. In fact, there are
    a few more environments that do not accompany the installation of `gym`, such
    as Box2d, Classic control, MuJoCo, and Robotics.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想为什么我们需要安装Atari的依赖项。事实上，还有一些环境并没有随`gym`一起安装，比如Box2d、经典控制、MuJoCo和机器人学。
- en: 'Take the `Box2d` environments, for example; we need to install the `Box2d`
    dependencies before we first run the environments. Again, two installation approaches
    are as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以`Box2d`环境为例；在首次运行环境之前，我们需要安装`Box2d`依赖项。再次，以下是两种安装方法：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After that, we can play around with the `LunarLander` environment, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可以尝试使用`LunarLander`环境，如下所示：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'A game window will pop up:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一个游戏窗口将弹出：
- en: '![](img/5cff3554-8472-452d-8eb3-8b60457a851f.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cff3554-8472-452d-8eb3-8b60457a851f.png)'
- en: See also
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: If you are looking to simulate an environment but are not sure of the name you
    should use in the `make()` method, you can find it in the table of environments
    at [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    Besides the name used to call an environment, the table also shows the size of
    the observation matrix and the number of possible actions. Have fun playing around
    with the environments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想模拟一个环境但不确定在`make()`方法中应该使用的名称，您可以在[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)的环境表中找到它。除了调用环境时使用的名称外，表还显示了观察矩阵的大小和可能动作的数量。玩转这些环境时尽情享乐吧。
- en: Simulating the CartPole environment
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟CartPole环境
- en: In this recipe, we will work on simulating one more environment in order to
    get more familiar with Gym. The CartPole environment is a classic one in reinforcement
    learning research.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将模拟一个额外的环境，以便更加熟悉Gym。CartPole环境是强化学习研究中的经典环境之一。
- en: 'CartPole is a traditional reinforcement learning task in which a pole is placed
    upright on top of a cart. The agent moves the cart either to the left or to the
    right by 1 unit in a timestep. The goal is to balance the pole and prevent it
    from falling over. The pole is considered to have fallen if it is more than 12
    degrees from the vertical, or the cart moves 2.4 units away from the origin. An
    episode terminates when any of the following occurs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole 是传统的强化学习任务，其中一个杆子直立放在购物车顶部。代理人在每个时间步长内将购物车向左或向右移动 1 单位。目标是平衡杆子，防止其倒下。如果杆子与垂直方向超过
    12 度，或者购物车离原点移动超过 2.4 单位，则认为杆子已倒下。当发生以下任何一种情况时，一个 episode 终止：
- en: The pole falls over
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆子倒下了
- en: The number of timesteps reaches 200
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间步数达到了 200
- en: How to do it...
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做…
- en: 'Let''s simulate the `CartPole` environment by following these steps:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤模拟 `CartPole` 环境：
- en: To run the CartPole environment, let's first search for its name in the table
    of environments at [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    We get `'CartPole-v0'` and also learn that the observation space is represented
    in a 4-dimensional array, and that there are two possible actions (which makes
    sense).
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行 CartPole 环境，让我们首先在[https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments)的环境表中搜索其名称。我们得到了
    `'CartPole-v0'`，并且还了解到观测空间由一个四维数组表示，有两种可能的动作（这是有道理的）。
- en: 'We import the Gym library and create an instance of the `CartPole` environment:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入 Gym 库，并创建一个 `CartPole` 环境的实例：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Reset the environment:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, this also returns the initial state represented by an array
    of four floats.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这也返回了由四个浮点数组成的初始状态。
- en: 'Render the environment:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE27]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'You will see a small window popping up, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到一个小窗口弹出，如下所示：
- en: '![](img/399dacfb-5fb5-4e3f-b3f1-23fcf51d02a7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/399dacfb-5fb5-4e3f-b3f1-23fcf51d02a7.png)'
- en: 'Now, let''s make a `while` loop and let the agent perform as many random actions
    as it can:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们制作一个 `while` 循环，并让代理尽可能执行多个随机动作：
- en: '[PRE28]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Meanwhile, you will see that the cart and pole are moving. At the end, you
    will see they both stop. The window looks like the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，您将看到购物车和杆子在移动。最后，您将看到它们停止。窗口看起来像这样：
- en: '![](img/a1890bf2-76c8-4e3e-a86a-240b0d6a98ac.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1890bf2-76c8-4e3e-a86a-240b0d6a98ac.png)'
- en: The episode only lasts several steps because the left or right actions are chosen
    randomly. Can we record the whole process so we can replay it afterward? We can
    do so with just two lines of code in Gym, as shown in *Step 7*. If you are using
    a Mac or Linux system, you need to complete *Step 6* first; otherwise, you can
    jump to *Step 7*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机选择左或右动作，每个 episode 只持续几个步骤。我们能记录整个过程以便之后回放吗？我们可以在 Gym 中仅用两行代码实现，如 *Step
    7* 所示。如果您使用的是 Mac 或 Linux 系统，则需要先完成 *Step 6*；否则，您可以直接跳转到 *Step 7*。
- en: 'To record video, we need to install the `ffmpeg` package. For Mac, it can be
    installed via the following command:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要记录视频，我们需要安装 `ffmpeg` 包。对于 Mac，可以通过以下命令安装：
- en: '[PRE29]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For Linux, the following command should do it:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Linux，以下命令应该可以完成：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After creating the `CartPole` instance, add these two lines:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `CartPole` 实例后，添加以下两行：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will record what is displayed in the window and store it in the specified
    directory.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将记录窗口中显示的内容，并存储在指定的目录中。
- en: Now re-run the codes from *Step 3* to *Step 5*. After an episode terminates,
    we can see that an `.mp4` file is created in the `video_dir` folder. The video
    is quite short; it may last 1 second or so.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在重新运行从 *Step 3* 到 *Step 5* 的代码。在一个 episode 结束后，我们可以看到在 `video_dir` 文件夹中创建了一个
    `.mp4` 文件。视频非常短暂，可能只有 1 秒左右。
- en: How it works...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理…
- en: 'In this recipe, we print out the state array for every step. But what does
    each float in the array mean? We can find more information about CartPole on Gym''s
    GitHub wiki page: [https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0).
    It turns out that those four floats represent the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们每一步都打印出状态数组。但是数组中的每个浮点数代表什么？我们可以在 Gym 的 GitHub wiki 页面上找到有关 CartPole
    的更多信息：[https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0)。原来这四个浮点数分别表示以下内容：
- en: 'Cart position: This ranges from -2.4 to 2.4, and any position beyond this range
    will trigger episode termination.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购物车位置：其范围从 -2.4 到 2.4，超出此范围的任何位置都将触发 episode 终止。
- en: Cart velocity.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购物车速度。
- en: 'Pole angle: Any value less than -0.209 (-12 degrees) or greater than 0.209
    (12 degrees) will trigger episode termination.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆角度：任何小于 -0.209（-12 度）或大于 0.209（12 度）的值将触发 episode 终止。
- en: Pole velocity at the tip.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆末端的极点速度。
- en: In terms of the action, it is either 0 or 1, which corresponds to pushing the
    cart to the left and to the right, respectively.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在动作方面，要么是 0，要么是 1，分别对应将车推向左侧和右侧。
- en: The **reward** in this environment is +1 for every timestep before the episode
    terminates. We can also verify this by printing out the reward for every step.
    And the total reward is simply the number of timesteps.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，**奖励**是在每个时间步之前 +1。我们还可以通过打印每一步的奖励来验证这一点。而总奖励就是时间步数。
- en: There's more...
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容…
- en: So far, we've run only one episode. In order to assess how well the agent performs,
    we can simulate many episodes and then average the total rewards for an individual
    episode. The average total reward will tell us about the performance of the agent
    that takes random actions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只运行了一个 episode。为了评估代理的表现，我们可以模拟许多 episode，然后对每个 episode 的总奖励取平均值。平均总奖励将告诉我们采取随机行动的代理的表现如何。
- en: 'Let’s set 10,000 episodes:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置 10,000 个 episodes：
- en: '[PRE32]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In each episode, we compute the total reward by accumulating the reward in
    every step:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个 episode 中，我们通过累积每一步的奖励来计算总奖励：
- en: '[PRE33]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we calculate the average total reward:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算平均总奖励：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: On average, taking a random action scores 22.25\.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，随机采取一个动作可以得到 22.25 分。
- en: We all know that taking random actions is not sophisticated enough, and we will
    implement an advanced policy in upcoming recipes. But for the next recipe, let's
    take a break and review the basics of PyTorch.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道，随机采取行动并不够复杂，我们将在接下来的示例中实施一个高级策略。但是在下一个示例中，让我们休息一下，回顾一下 PyTorch 的基础知识。
- en: Reviewing the fundamentals of PyTorch
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾 PyTorch 的基础知识
- en: As we’ve already mentioned, PyTorch is the numerical computation library we
    use to implement reinforcement learning algorithms in this book.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，PyTorch 是本书中用来实现强化学习算法的数值计算库。
- en: PyTorch is a trendy scientific computing and machine learning (including deep
    learning) library developed by Facebook. Tensor is the core data structure in
    PyTorch, which is similar to NumPy's `ndarrays`. PyTorch and NumPy are comparable
    in scientific computing. However, PyTorch is faster than NumPy in array operations
    and array traversing. This is mainly due to the fact that array element access
    is faster in PyTorch. Hence, more and more people believe PyTorch will replace
    NumPy.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是由 Facebook 开发的时髦科学计算和机器学习（包括深度学习）库。张量是 PyTorch 的核心数据结构，类似于 NumPy 的`ndarrays`。在科学计算中，PyTorch
    和 NumPy 是可以比较的。然而，在数组操作和遍历中，PyTorch 比 NumPy 更快。这主要是因为 PyTorch 中的数组元素访问速度更快。因此，越来越多的人认为
    PyTorch 将取代 NumPy。
- en: How to do it...
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Let''s do a quick review of the basic programming in PyTorch to get more familiar
    with it:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下 PyTorch 的基本编程，以便更加熟悉它：
- en: 'We created an uninitialized matrix in an earlier recipe. How about a randomly
    initialized one? See the following commands:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在之前的一个示例中，我们创建了一个未初始化的矩阵。那么随机初始化一个矩阵怎么样呢？请看以下命令：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Random floats from a uniform distribution in the interval (0, 1) are generated.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在区间 (0, 1) 内生成随机浮点数。
- en: 'We can specify the desired data type of the returned tensor. For example, a
    tensor of the double type (`float64`) is returned as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以指定返回张量的所需数据类型。例如，返回双精度类型（`float64`）的张量如下所示：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: By default, `float` is the returned data type.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，返回的数据类型是`float`。
- en: 'Next, let''s create a matrix full of zeros and a matrix full of ones:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个全零矩阵和一个全一矩阵：
- en: '[PRE37]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To get the size of a tensor, use this code:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取张量的大小，使用以下代码：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`torch.Size` is actually a tuple.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.Size` 实际上是一个元组。'
- en: 'To reshape a tensor, we can use the `view()` method:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要重新塑造张量，我们可以使用`view()`方法：
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can create a tensor directly from data, including a single value, a list,
    and a nested list:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以直接从数据创建张量，包括单个值、列表和嵌套列表：
- en: '[PRE40]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To access the elements in a tensor of more than one element, we can use indexing
    in a similar way to NumPy:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要访问多个元素的张量中的元素，我们可以类似于 NumPy 使用索引：
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As with a one-element tensor, we do so by using the `item()` method:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与单元素张量一样，我们使用`item()`方法：
- en: '[PRE42]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Tensor and NumPy arrays are mutually convertible. Convert a tensor to a NumPy
    array using the `numpy()` method:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 张量和 NumPy 数组可以相互转换。使用`numpy()`方法将张量转换为 NumPy 数组：
- en: '[PRE43]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Convert a NumPy array to a tensor with `from_numpy()`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`from_numpy()`将 NumPy 数组转换为张量：
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that if the input NumPy array is of the float data type, the output tensor
    will be of the double type. Typecasting may occasionally be needed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果输入的NumPy数组是浮点数据类型，则输出张量将是双类型。偶尔可能需要类型转换。
- en: 'Take a look at the following example, where a tensor of the double type is
    converted to a `float`:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下示例，其中将双类型的张量转换为`float`：
- en: '[PRE45]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Operations in PyTorch are similar to NumPy as well. Take addition as an example;
    we can simply do the following:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch中的操作与NumPy类似。以加法为例，我们可以简单地执行以下操作：
- en: '[PRE46]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Or we can use the `add()` method as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以使用`add()`方法如下所示：
- en: '[PRE47]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'PyTorch supports in-place operations, which mutate the tensor object. For example,
    let''s run this command:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch支持原地操作，这些操作会改变张量对象。例如，让我们运行这个命令：
- en: '[PRE48]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You will see that `x3` is changed to the result of the original `x3`plus `x4`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到`x3`被更改为原始的`x3`加上`x4`的结果：
- en: '[PRE49]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: There's more...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Any method with **_** indicates that it is an in-place operation, which updates
    the tensor with the resulting value.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 任何带有**_**的方法表示它是一个原地操作，它更新张量并返回结果值。
- en: See also
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For the full list of tensor operations in PyTorch, please go to the official
    docs at [https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html).
    This is the best place to search for information if you get stuck on a PyTorch
    programming problem.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 欲查看PyTorch中的所有张量操作，请访问官方文档[https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html)。这是在PyTorch编程问题上遇到困难时搜索信息的最佳位置。
- en: Implementing and evaluating a random search policy
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施和评估随机搜索策略
- en: After some practice with PyTorch programming, starting from this recipe, we
    will be working on more sophisticated policies to solve the CartPole problem than
    purely random actions. We start with the random search policy in this recipe.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用PyTorch编程进行一些实践后，从这个示例开始，我们将致力于比纯粹的随机动作更复杂的策略来解决CartPole问题。我们从这个配方开始使用随机搜索策略。
- en: A simple, yet effective, approach is to map an observation to a vector of two
    numbers representing two actions. The action with the higher value will be picked.
    The linear mapping is depicted by a weight matrix whose size is 4 x 2 since the
    observations are 4-dimensional in this case. In each episode, the weight is randomly
    generated and is used to compute the action for every step in this episode. The
    total reward is then calculated. This process repeats for many episodes and, in
    the end, the weight that enables the highest total reward will become the learned
    policy. This approach is called **random search** because the weight is randomly
    picked in each trial with the hope that the best weight will be found with a large
    number of trials.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单但有效的方法是将观测映射到表示两个动作的两个数字的向量中。将选择具有较高值的动作。线性映射由一个大小为4 x 2的权重矩阵表示，因为在这种情况下，观测是4维的。在每个episode中，权重是随机生成的，并且用于计算该episode中每一步的动作。然后计算总奖励。这个过程重复多个episode，并且最终能够提供最高总奖励的权重将成为学习策略。这种方法被称为**随机搜索**，因为在每个试验中权重都是随机选择的，希望通过大量的试验找到最佳权重。
- en: How to do it...
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Let''s go ahead and implement a random search algorithm with PyTorch:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用PyTorch实现一个随机搜索算法：
- en: 'Import the Gym and PyTorch packages and create an environment instance:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入Gym和PyTorch包，并创建一个环境实例：
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Obtain the dimensions of the observation and action space:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取观测空间和动作空间的维度：
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: These will be used when we define the tensor for the weight matrix, which is
    size 4 x 2 in size.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为权重矩阵定义张量时，将使用这些内容，该矩阵的大小为4 x 2。
- en: 'Define a function that simulates an episode given the input weight and returns
    the total reward:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，模拟给定输入权重的一个episode，并返回总奖励：
- en: '[PRE52]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Here, we convert the state array to a tensor of the float type because we need
    to compute the multiplication of the state and weight tensor, `torch.matmul(state,
    weight)`, for linear mapping. The action with the higher value is selected using
    the `torch.argmax()` operation. And don't forget to take the value of the resulting
    action tensor using `.item()` because it is a one-element tensor.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将状态数组转换为浮点类型的张量，因为我们需要计算状态和权重张量的乘积`torch.matmul(state, weight)`以进行线性映射。使用`torch.argmax()`操作选择具有更高值的动作。不要忘记使用`.item()`获取结果动作张量的值，因为它是一个单元素张量。
- en: 'Specify the number of episodes:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定episode的数量：
- en: '[PRE53]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We need to keep track of the best total reward on the fly, as well as the corresponding
    weight. So, we specify their starting values:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要实时跟踪最佳总奖励，以及相应的权重。因此，我们指定它们的起始值：
- en: '[PRE54]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We will also record the total reward for every episode:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将记录每个 episode 的总奖励：
- en: '[PRE55]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we can run `n_episode`. For each episode, we do the following:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行 `n_episode`。对于每个 episode，我们执行以下操作：
- en: Randomly pick the weight
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择权重
- en: Let the agent take actions according to the linear mapping
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让代理根据线性映射采取行动
- en: An episode terminates and returns the total reward
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 episode 结束并返回总奖励
- en: Update the best total reward and the best weight if necessary
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据需要更新最佳总奖励和最佳权重
- en: Also, keep a record of the total reward
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，保留总奖励的记录
- en: 'Put this into code as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 将其放入代码中如下：
- en: '[PRE56]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: We have obtained the best policy through 1,000 random searches. The best policy
    is parameterized by `best_weight`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 1,000 次随机搜索，我们已经得到了最佳策略。最佳策略由 `best_weight` 参数化。
- en: 'Before we test out the best policy in the testing episodes, we can calculate
    the average total reward achieved by random linear mapping:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们在测试 episode 上测试最佳策略之前，我们可以计算通过随机线性映射获得的平均总奖励：
- en: '[PRE57]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: This is more than twice what we got from the random action policy (22.25).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们从随机动作策略（22.25）获得的要多两倍。
- en: 'Now, let''s see how the learned policy performs on 100 new episodes:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看学习到的策略在 100 个新 episode 上的表现：
- en: '[PRE58]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Surprisingly, the average reward for the testing episodes is close to the maximum
    of 200 steps with the learned policy. Be aware that this value may vary a lot.
    It could be anywhere from 160 to 200.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，在测试 episode 中，学习到的策略的平均奖励接近最大的 200 步。请注意，这个值可能会有很大的变化，从 160 到 200 不等。
- en: How it works...
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理如下...
- en: The random search algorithm works so well mainly because of the simplicity of
    our CartPole environment. Its observation state is composed of only four variables.
    You will recall that the observation in the Atari Space Invaders game is more
    than 100,000 (which is 210 * 160 * 3) . The number of dimensions of the action
    state in CartPole is a third of that in Space Invaders. In general, simple algorithms
    work well for simple problems. In our case, we simply search for the best linear
    mapping from the observation to the action from a random pool.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 随机搜索算法之所以如此有效，主要是因为我们的 CartPole 环境简单。它的观测状态仅由四个变量组成。你可能还记得，在阿塔利 Space Invaders
    游戏中，观测超过 100,000（即 210 * 160 * 3）。CartPole 中动作状态的维度是 Space Invaders 的三分之一。总的来说，简单的算法对简单的问题效果很好。在我们的情况下，我们只需从随机池中搜索最佳的从观测到动作的线性映射。
- en: Another interesting thing we've noticed is that before we select and deploy
    the best policy (the best linear mapping), random search also outperforms random
    action. This is because random linear mapping does take the observations into
    consideration. With more information from the environment, the decisions made
    in the random search policy are more intelligent than completely random ones.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到的另一件有趣的事情是，在我们选择和部署最佳策略（最佳线性映射）之前，随机搜索也优于随机动作。这是因为随机线性映射确实考虑了观测值。随着从环境中获得的更多信息，随机搜索策略中做出的决策比完全随机的决策更为智能。
- en: There's more...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'We can also plot the total reward for every episode in the training phase:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制训练阶段每个 episode 的总奖励：
- en: '[PRE59]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This will generate the following plot:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表：
- en: '![](img/aef3787b-e607-4042-91bf-57b4f042aa56.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aef3787b-e607-4042-91bf-57b4f042aa56.png)'
- en: 'If you have not installed matplotlib, you can do so via the following command:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尚未安装 matplotlib，则可以通过以下命令安装：
- en: '[PRE60]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can see that the reward for each episode is pretty random, and that there
    is no trend of improvement as we go through the episodes. This is basically what
    we expected.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个 episode 的奖励相当随机，并且在逐个 episode 过程中没有改进的趋势。这基本上是我们预期的。
- en: 'In the plot of reward versus episodes, we can see that there are some episodes
    in which the reward reaches 200\. We can end the training phase whenever this
    occurs since there is no room to improve. Incorporating this change, we now have
    the following for the training phase:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在奖励与 episode 的绘图中，我们可以看到有些 episode 的奖励达到了 200。一旦出现这种情况，我们可以结束训练阶段，因为没有改进的余地了。经过这一变化，我们现在的训练阶段如下：
- en: '[PRE61]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The policy achieving the maximal reward is found in episode 17\. Again, this
    may vary a lot because the weights are generated randomly for each episode. To
    compute the expectation of training episodes needed, we can repeat the preceding
    training process 1,000 times and take the average of the training episodes:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 17 个回合找到了达到最大奖励的策略。再次提醒，由于每个回合的权重是随机生成的，这可能会有很大的变化。为了计算所需的训练回合的期望，我们可以重复前述的训练过程
    1,000 次，并取训练回合的平均值：
- en: '[PRE62]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: On average, we expect that it takes around 13 episodes to find the best policy.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 平均来看，我们预计需要大约 13 个回合来找到最佳策略。
- en: Developing the hill-climbing algorithm
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发爬坡算法
- en: As we can see in the random search policy, each episode is independent. In fact,
    all episodes in random search can be run in parallel, and the weight that achieves
    the best performance will eventually be selected. We've also verified this with
    the plot of reward versus episode, where there is no upward trend. In this recipe,
    we will develop a different algorithm, a hill-climbing algorithm, to transfer
    the knowledge acquired in one episode to the next episode.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在随机搜索策略中看到的，每个回合都是独立的。事实上，随机搜索中的所有回合可以并行运行，并最终选择达到最佳性能的权重。我们还通过奖励与回合的图表验证了这一点，在那里没有上升趋势。在本篇中，我们将开发一种不同的算法，即爬坡算法，以将一个回合中获得的知识转移到下一个回合中。
- en: In the hill-climbing algorithm, we also start with a randomly chosen weight.
    But here, for every episode, we add some noise to the weight. If the total reward
    improves, we update the weight with the new one; otherwise, we keep the old weight.
    In this approach, the weight is gradually improved as we progress through the
    episodes, instead of jumping around in each episode.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在爬坡算法中，我们同样从一个随机选择的权重开始。但是在这里，对于每个回合，我们会给权重添加一些噪声。如果总奖励有所改善，我们就用新的权重更新它；否则，我们保留旧的权重。在这种方法中，权重随着回合的进行逐渐改进，而不是在每个回合中跳动。
- en: How to do it...
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s go ahead and implement the hill-climbing algorithm with PyTorch:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用 PyTorch 实现爬坡算法：
- en: 'As before, import the necessary packages, create an environment instance, and
    obtain the dimensions of the observation and action space:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，导入必要的包，创建环境实例，并获取观测空间和动作空间的维度：
- en: '[PRE63]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: We will reuse the `run_episode` function we defined in the previous recipe,
    so we will not repeat it here. Again, given the input weight, it simulates an
    episode and returns the total reward.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将重复使用在前一篇中定义的 `run_episode` 函数，因此在此不再赘述。同样，给定输入的权重，它模拟一个回合并返回总奖励。
- en: 'Let''s make it 1,000 episodes for now:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们先做 1,000 个回合：
- en: '[PRE64]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We need to keep track of the best total reward on the fly, as well as the corresponding
    weight. So, let''s specify their starting values:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要实时跟踪最佳总奖励，以及相应的权重。因此，让我们指定它们的起始值：
- en: '[PRE65]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We will also record the total reward for every episode:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将记录每个回合的总奖励：
- en: '[PRE66]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'As we mentioned, we will add some noise to the weight for each episode. In
    fact, we will apply a scale to the noise so that the noise won''t overwhelm the
    weight. Here, we will choose 0.01 as the noise scale:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，我们将在每个回合为权重添加一些噪声。事实上，我们将为噪声应用一个比例，以防止噪声过多影响权重。在这里，我们将选择 0.01 作为噪声比例：
- en: '[PRE67]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Now, we can run the `n_episode` function. After we randomly pick an initial
    weight, for each episode, we do the following:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以运行 `n_episode` 函数。在我们随机选择了初始权重之后，每个回合，我们都会执行以下操作：
- en: Add random noise to the weight
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向权重添加随机噪声
- en: Let the agent take actions according to the linear mapping
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让代理根据线性映射采取行动
- en: An episode terminates and returns the total reward
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个回合终止并返回总奖励
- en: If the current reward is greater than the best one obtained so far, update the
    best reward and the weight
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前奖励大于迄今为止获得的最佳奖励，则更新最佳奖励和权重
- en: Otherwise, the best reward and the weight remain unchanged
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，最佳奖励和权重保持不变
- en: Also, keep a record of the total reward
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，记下总奖励
- en: 'Put this into code as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 将其转换为代码如下：
- en: '[PRE68]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We also calculate the average total reward achieved by the hill-climbing version
    of linear mapping:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还计算了线性映射的爬坡版本所达到的平均总奖励：
- en: '[PRE69]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'To assess the training using the hill-climbing algorithm, we repeat the training
    process multiple times (by running the code from *Step 4* to *Step 6* multiple
    times). We observe that the average total reward fluctuates a lot. The following
    are the results we got when running it 10 times:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估使用爬坡算法的训练，我们多次重复训练过程（运行从*第四步*到*第六步*的代码多次）。我们观察到平均总奖励波动很大。以下是我们运行10次时得到的结果：
- en: '[PRE70]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: What could cause such variance? It turns out that if the initial weight is bad,
    adding noise at a small scale will have little effect on improving the performance.
    This will cause poor convergence. On the other hand, if the initial weight is
    good, adding noise at a big scale might move the weight away from the optimal
    weight and jeopardize the performance. How can we make the training of the hill-climbing
    model more stable and reliable? We can actually make the noise scale adaptive
    to the performance, just like the adaptive learning rate in gradient descent.
    Let's see *Step 8* for more details.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 什么会导致这样的差异？事实证明，如果初始权重不好，以小比例添加噪声将对改善性能影响甚微。这将导致收敛不良。另一方面，如果初始权重良好，以大比例添加噪声可能会使权重远离最优权重并危及性能。我们如何使爬坡模型的训练更稳定可靠？实际上，我们可以使噪声比例适应性地根据性能调整，就像梯度下降中的自适应学习率一样。更多详情请参见*第八步*。
- en: 'To make the noise adaptive, we do the following:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使噪声适应性，我们采取以下措施：
- en: Specify a starting noise scale.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定一个起始噪声比例。
- en: If the performance in an episode improves, decrease the noise scale. In our
    case, we take half of the scale, but set `0.0001` as the lower bound.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一集的表现提高，减少噪声比例。在我们的情况下，我们取比例的一半，但将`0.0001`设为下限。
- en: If the performance in an episode drops, increase the noise scale. In our case,
    we double the scale, but set `2` as the upper bound.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一集的表现下降，增加噪声比例。在我们的情况下，我们将比例加倍，但将`2`设为上限。
- en: 'Put this into code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 将其编写成代码：
- en: '[PRE71]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The reward is increasing as the episodes progress. It reaches the maximum of
    200 within the first 100 episodes and stays there. The average total reward also
    looks promising:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励随着集数的增加而增加。在前100集内达到最高值200并保持不变。平均总奖励看起来也很有前景：
- en: '[PRE72]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We also plot the total reward for every episode as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还绘制了每一集的总奖励如下：
- en: '[PRE73]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'In the resulting plot, we can see a clear upward trend before it plateaus at
    the maximum value:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果图中，我们可以看到一个明显的上升趋势，在达到最大值后趋于平稳：
- en: '![](img/1c200cf7-f129-476b-b722-3ce1730a4c80.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c200cf7-f129-476b-b722-3ce1730a4c80.png)'
- en: Feel free to run the new training process a few times. The results are very
    stable compared to learning with a constant noise scale.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 随时运行新的训练过程几次。与使用固定噪声比例进行学习相比，结果非常稳定。
- en: 'Now, let''s see how the learned policy performs on 100 new episodes:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看学习策略在100个新集数上的表现：
- en: '[PRE74]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Let''s see the average performance:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看平均表现：
- en: '[PRE75]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The average reward for the testing episodes is close to the maximum of 200 that
    we obtained with the learned policy. You can re-run the evaluation multiple times.
    The results are pretty consistent.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集合的平均奖励接近我们通过学习策略获得的最高值200。你可以多次重新运行评估。结果非常一致。
- en: How it works...
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行原理如下...
- en: We are able to achieve much better performance with the hill-climbing algorithm
    than with random search by simply adding adaptive noise to each episode. We can
    think of it as a special kind of gradient descent without a target variable. The
    additional noise is the gradient, albeit in a random way. The noise scale is the
    learning rate, and it is adaptive to the reward from the previous episode. The
    target variable in hill climbing becomes achieving the highest reward. In summary,
    rather than isolating each episode, the agent in the hill-climbing algorithm makes
    use of the knowledge learned from each episode and performs a more reliable action
    in the next episode. As the name hill climbing implies, the reward moves upwards
    through the episodes as the weight gradually moves towards the optimum value.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简单地在每集中添加自适应噪声，使用爬坡算法能够实现比随机搜索更好的性能。我们可以将其视为一种没有目标变量的特殊梯度下降。额外的噪声就是梯度，尽管是以随机的方式。噪声比例是学习率，并且根据上一集的奖励进行自适应。在爬坡中，目标变量成为达到最高奖励。总之，爬坡算法中的智能体不是将每集孤立开来，而是利用从每集中学到的知识，并在下一集中执行更可靠的操作。正如其名称所示，奖励通过集数向上移动，权重逐渐朝向最优值。
- en: There's more...
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: We can observe that the reward can reach the maximum value within the first
    100 episodes. Can we just stop training when the reward reaches 200, as we did
    with the random search policy? That might not be a good idea. Remember that the
    agent is making continuous improvements in hill climbing. Even if it finds a weight
    that generates the maximum reward, it can still search around this weight for
    the optimal point. Here, we define the optimal policy as the one that can solve
    the CartPole problem. According to the following wiki page, [https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0),
    "solved" means the average reward over 100 consecutive episodes is no less than
    195.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，在前100个回合内奖励可以达到最大值。当奖励达到200时，我们是否可以停止训练，就像我们在随机搜索策略中所做的那样？这可能不是一个好主意。记住，代理在爬坡时在持续改进。即使它找到了生成最大奖励的权重，它仍然可以在这个权重周围搜索最优点。在这里，我们将最优策略定义为能够解决CartPole问题的策略。根据以下维基页面，[https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0)，"解决"意味着连续100个回合的平均奖励不低于195。
- en: 'We refine the stopping criterion accordingly:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相应地完善了停止标准：
- en: '[PRE76]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: At episode 137, the problem is considered solved.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在第137回合，问题被认为已解决。
- en: See also
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you are interested in learning more about the hill-climbing algorithm, the
    following resources are useful:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣了解更多关于爬坡算法的信息，以下资源是有用的：
- en: '[https://en.wikipedia.org/wiki/Hill_climbing](https://en.wikipedia.org/wiki/Hill_climbing)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Hill_climbing](https://en.wikipedia.org/wiki/Hill_climbing)'
- en: '[https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/](https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/](https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/)'
- en: Developing a policy gradient algorithm
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发策略梯度算法
- en: The last recipe of the first chapter is about solving the CartPole environment
    with a policy gradient algorithm. This may be more complicated than we need for
    this simple problem, in which the random search and hill-climbing algorithms suffice.
    However, it is a great algorithm to learn, and we will use it in more complicated
    environments later in the book.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 第一章的最后一个配方是使用策略梯度算法解决CartPole环境的问题。对于这个简单的问题，这可能比我们需要的更复杂，随机搜索和爬山算法已经足够了。然而，这是一个很棒的学习算法，我们将在本书后面更复杂的环境中使用它。
- en: In the policy gradient algorithm, the model weight moves in the direction of
    the gradient at the end of each episode. We will explain the computation of gradients
    in the next section. Also, in each step, it **samples** an action from the policy
    based on the probabilities computed using the state and weight. It no longer takes
    an action with certainty, in contrast with random search and hill climbing (by
    taking the action achieving the higher score). Hence, the policy switches from
    deterministic to **stochastic**.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略梯度算法中，模型权重在每个回合结束时朝着梯度的方向移动。我们将在下一节中解释梯度的计算。此外，在每个步骤中，它根据使用状态和权重计算的概率**随机**采样一个动作。与随机搜索和爬坡算法（通过执行获得更高分数的动作）相反，策略从确定性切换到**随机**。
- en: How to do it...
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Now, it is time to implement the policy gradient algorithm with PyTorch:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候用PyTorch实现策略梯度算法了：
- en: 'As before, import the necessary packages, create an environment instance, and
    obtain the dimensions of the observation and action space:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，导入必要的包，创建环境实例，并获取观察和动作空间的维度：
- en: '[PRE77]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'We define the `run_episode` function, which simulates an episode given the
    input weight and returns the total reward and the gradients computed. More specifically,
    it does the following tasks in each step:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了`run_episode`函数，该函数模拟了给定输入权重的一个回合，并返回总奖励和计算的梯度。具体来说，它在每个步骤中执行以下任务：
- en: Calculates the probabilities, `probs`, for both actions based on the current
    state and input weight
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算基于当前状态和输入权重的两个动作的概率 `probs`
- en: Samples an action, `action`, based on the resulting probabilities
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据得出的概率样本一个动作 `action`
- en: Computes the derivatives, `d_softmax`, of the `softmax` function with the probabilities
    as input
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算`softmax`函数的导数 `d_softmax`，其中概率作为输入
- en: Divides the resulting derivatives, `d_softmax`, by the probabilities, probs,
    to get the derivatives, `d_log`, of the log term with respect to the policy
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将得出的导数 `d_softmax` 除以概率 `probs`，得到与策略相关的对数项的导数 `d_log`
- en: Applies the chain rule to compute the gradient, `grad`, of the weights
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用链式法则计算权重`grad`的梯度
- en: Records the resulting gradient, grad
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录结果梯度，grad
- en: Performs the action, accumulates the reward, and updates the state
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行动作，累积奖励，并更新状态
- en: 'Putting all of this into code, we have the following:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放入代码中，我们得到以下内容：
- en: '[PRE78]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: After an episode finishes, it returns the total reward obtained in this episode
    and the gradients computed for the individual steps. These two outputs will be
    used to update the weight.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 当一集结束后，它返回本集获得的总奖励和个别步骤计算的梯度。这两个输出将用于使用随机梯度上升法更新权重。
- en: 'Let''s make it 1,000 episodes for now:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 暂时让它运行1,000集：
- en: '[PRE79]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This means we will run `run_episode` and `n_episode`times.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将运行`run_episode`和`n_episode`次。
- en: 'Initiate the weight:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重：
- en: '[PRE80]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'We will also record the total reward for every episode:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会记录每一集的总奖励：
- en: '[PRE81]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'At the end of each episode, we need to update the weight using the computed
    gradients. For every step of the episode, the weight moves by *learning rate *
    gradient* calculated in this *step * total* reward in the remaining steps. Here,
    we choose `0.001` as the learning rate:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每集结束时，我们需要使用计算出的梯度更新权重。对于每一集的每一步，权重根据在剩余步骤中计算的*学习率 * 梯度* * 总奖励*的策略梯度移动。在这里，我们选择`0.001`作为学习率：
- en: '[PRE82]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, we can run `n_episode`episodes:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以运行`n_episode`集：
- en: '[PRE83]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now, we calculate the average total reward achieved by the policy gradient
    algorithm:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们计算策略梯度算法达到的平均总奖励：
- en: '[PRE84]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We also plot the total reward for every episode as follows:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还会绘制每集的总奖励如下：
- en: '[PRE85]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'In the resulting plot, we can see a clear upward trend before it stays at the
    maximum value:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的图表中，我们可以看到在保持最大值之前有一个明显的上升趋势：
- en: '![](img/fae7b840-847f-4d56-a1b4-a19920b27520.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fae7b840-847f-4d56-a1b4-a19920b27520.png)'
- en: We can also see that the rewards oscillate even after it converges. This is
    because the policy gradient algorithm is a stochastic policy.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，即使在收敛后，奖励仍然会波动。这是因为策略梯度算法是一种随机策略。
- en: 'Now, let''s see how the learned policy performs on 100 new episodes:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看学习策略在100个新集上的表现：
- en: '[PRE86]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Let''s see the average performance:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看平均表现：
- en: '[PRE87]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The average reward for the testing episodes is close to the maximum value of
    200 for the learned policy. You can re-run the evaluation multiple times. The
    results are pretty consistent.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的平均奖励接近于学习策略的最大值200。您可以多次重新运行评估。结果非常一致。
- en: How it works...
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The policy gradient algorithm trains an agent by taking small steps and updating
    the weight based on the rewards associated with those steps at the end of an episode.
    The technique of having the agent run through an entire episode and then updating
    the policy based on the rewards obtained is called **Monte Carlo** policy gradient.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度算法通过采取小步骤并根据这些步骤在一集结束时获得的奖励更新权重来训练代理程序。在整个一集结束后根据获得的奖励更新策略的技术被称为**蒙特卡洛**策略梯度。
- en: The action is selected based on the probability distribution computed based
    on the current state and the model’s weight. For example, if the probabilities
    for the left and right actions are [0.6, 0.4], this means the left action is selected
    60% of the time; it doesn't mean the left action is chosen, as in the random search
    and hill-climbing algorithms.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 根据当前状态和模型权重计算的概率分布选择动作。例如，如果左右动作的概率为[0.6, 0.4]，这意味着左动作被选择的概率为60%；这并不意味着左动作被选择，如在随机搜索和爬山算法中一样。
- en: We know that the reward is 1 for each step before an episode terminates. Hence,
    the future reward we use to calculate the policy gradient at each step is the
    number of steps remaining. After each episode, we feed the gradient history multiplied
    by the future rewards to update the weight using the stochastic gradient ascent
    method. In this way, the longer an episode is, the bigger the update of the weight.
    This will eventually increase the chance of getting a larger total reward.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在一集终止之前，每一步的奖励为1。因此，我们用于在每一步计算策略梯度的未来奖励是剩余步数。在每集结束后，我们通过将梯度历史乘以未来奖励来使用随机梯度上升法更新权重。这样，一集越长，权重更新就越大。这最终会增加获得更大总奖励的机会。
- en: As we mentioned at the start of this section, the policy gradient algorithm
    might be overkill for a simple environment such as CartPole, but it should get
    us ready for more complicated problems.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节开头提到的，对于像CartPole这样简单的环境来说，策略梯度算法可能有点过头了，但它应该能够让我们准备好处理更复杂的问题。
- en: There's more...
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'If we examine the reward/episode plot, it seems that we can also stop early
    during training when it has been solved – the average reward over 100 consecutive
    episodes is no less than 195\. We just add the following lines of code to the
    training session:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查奖励/每轮的图表，似乎在训练过程中当解决问题时也可以提前停止 - 连续 100 轮的平均奖励不少于 195。我们只需在训练会话中添加以下几行代码：
- en: '[PRE88]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Re-run the training session. You should get something similar to the following,
    which stops after several hundred episodes:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 重新运行训练会话。您应该会得到类似以下的结果，几百轮后停止：
- en: '[PRE89]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: See also
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: Check out [http://www.scholarpedia.org/article/Policy_gradient_methods](http://www.scholarpedia.org/article/Policy_gradient_methods)
    for more information about policy gradient methods.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 查看有关策略梯度方法的更多信息，请访问 [http://www.scholarpedia.org/article/Policy_gradient_methods](http://www.scholarpedia.org/article/Policy_gradient_methods)。
