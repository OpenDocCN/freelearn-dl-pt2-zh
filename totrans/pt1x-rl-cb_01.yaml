- en: Getting Started with Reinforcement Learning and PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We kick off our journey of practical reinforcement learning and PyTorch with
    the basic, yet important, reinforcement learning algorithms, including random
    search, hill climbing, and policy gradient. We will start by setting up the working
    environment and OpenAI Gym, and you will become familiar with reinforcement learning
    environments through the Atari and CartPole playgrounds. We will also demonstrate
    how to develop algorithms to solve the CartPole problem step by step. Also, we
    will review the essentials of PyTorch and prepare for the upcoming learning examples
    and projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter contains the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the working environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing OpenAI Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating Atari environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulating the CartPole environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing the fundamentals of PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing and evaluating a random search policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the hill-climbing algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a policy gradient algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the working environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get started with setting up the working environment, including the correct
    versions of Python and Anaconda, and PyTorch as the main framework that is used
    throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Python is the language we use to implement all reinforcement learning algorithms
    and techniques throughout the book. In this book, we will be using Python 3, or
    more specifically, 3.6 or above. If you are a Python 2 user, now is the best time
    for you to switch to Python 3, as Python 2 will no longer be supported after 2020\.
    The transition is very smooth, though, so don't panic.
  prefs: []
  type: TYPE_NORMAL
- en: '**Anaconda** is an open source Python distribution ([www.anaconda.com/distribution/](http://www.anaconda.com/distribution/))
    for data science and machine learning. We will be using Anaconda''s package manager,
    `conda`, to install Python packages, along with `pip`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**PyTorch** ([https://pytorch.org/](https://pytorch.org/)), primarily developed
    by the Facebook AI Research (FAIR) Group, is a trendy machine learning library
    based on Torch ([http://torch.ch/](http://torch.ch/)). Tensors in PyTorch replace
    NumPy''s `ndarrays`, which provides more flexibility and compatibility with GPUs.
    Because of the powerful computational graphs and the simple and friendly interface,
    the PyTorch community is expanding on a daily basis, and it has seen heavy adoption
    by more and more tech giants.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to properly set up all of these components.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by installing Anaconda. You can skip this if you already have
    Anaconda for Python 3.6 or 3.7 running on your system. Otherwise, you can follow
    the instructions at [https://docs.anaconda.com/anaconda/install/](https://docs.anaconda.com/anaconda/install/)
    for your operating system, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/be97f9c3-7d19-46a4-a21e-257e230c53df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Feel free to play around with PyTorch once the setup is done. To verify that
    you have the right setup of Anaconda and Python, you can enter the following line
    in your Terminal in Linux/Mac or Command Prompt in Windows (from now on, we will
    just call it Terminal):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'It will display your Python Anaconda environment. You should see something
    similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af958adb-6047-463f-a401-7ad60b524f18.png)'
  prefs: []
  type: TYPE_IMG
- en: If Anaconda and Python 3.x are not mentioned, please check the system path or
    the path Python is running from.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next thing to do is to install PyTorch. First, go to [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)
    and pick the description of your environment from the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd781986-8e61-4116-afc7-b7821cfda18f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we use **Mac**, **Conda**, **Python 3.7**, and running locally (no CUDA)
    as an example, and enter the resulting command line in the Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'To confirm PyTorch is installed correctly, run the following lines of code
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If a 3 x 4 matrix is displayed, that means PyTorch is installed correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have successfully set up the working environment.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just created a tensor of size 3 x 4 in PyTorch. It is an empty matrix.
    By saying `empty`, this doesn't mean all elements are of the value `Null`. Instead,
    they are a bunch of meaningless floats that are considered placeholders. Users
    are required to set all the values later. This is very similar to NumPy's empty
    array.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of you may question the necessity of installing Anaconda and using `conda`
    to manage packages since it is easy to install packages with `pip`. In fact, `conda`
    is a better packaging tool than `pip`. We mainly use `conda` for the following
    four reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It handles library dependencies nicely**: Installing a package with `conda`
    will automatically download all of its dependencies. However, doing so with `pip`
    will lead to a warning, and installation will be aborted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It solves conflicts of packages gracefully**: If installing a package requires
    another package of a specific version (let''s say 2.3 or after, for example),
    `conda` will update the version of the other package automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It creates a virtual environment easily**: A virtual environment is a self-contained
    package directory tree. Different applications or projects can use different virtual
    environments. All virtual environments are isolated from each other. It is recommended
    to use virtual environments so that whatever we do for one application doesn''t
    affect our system environment or any other environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It is also compatible with pip**: We can still use `pip` in `conda` with
    the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are interested in learning more about `conda`, feel free to check out
    the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Conda user guide**: [https://conda.io/projects/conda/en/latest/user-guide/index.html](https://conda.io/projects/conda/en/latest/user-guide/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creating and managing virtual environments with conda**: [https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to get more familiar with PyTorch, you can go through the *Getting
    Started* section in the official tutorial at [https://pytorch.org/tutorials/#getting-started](https://pytorch.org/tutorials/#getting-started).
    We recommend you at least finish the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is PyTorch**: [https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning PyTorch with examples**: [https://pytorch.org/tutorials/beginner/pytorch_with_examples.html](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After setting up the working environment, we can now install OpenAI Gym. You
    can't work on reinforcement learning without using OpenAI Gym, which gives you
    a variety of environments in which to develop your learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI** ([https://openai.com/](https://openai.com/)) is a non-profit research
    company that is focused on building safe **artificial general intelligence** (**AGI**)
    and ensuring that it benefits humans. **OpenAI Gym** is a powerful and open source
    toolkit for developing and comparing reinforcement learning algorithms. It provides
    an interface to varieties of reinforcement learning simulations and tasks, from
    walking to moon landing, from car racing to playing Atari games. See [https://gym.openai.com/envs/](https://gym.openai.com/envs/)
    for the full list of environments.We can write **agents** to interact with OpenAI
    Gym environments using any numerical computation library, such as PyTorch, TensorFlow,
    or Keras.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two ways to install Gym. The first one is to use `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'For `conda` users, remember to install `pip` first in `conda` using the following
    command before installing Gym using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is because Gym is not officially available in `conda` as of early 2019.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach is to build from source:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, clone the package directly from its Git repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Go to the downloaded folder and install Gym from there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And now you are good to go. Feel free to play around with `gym`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also check the available `gym` environment by typing the following
    lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will give you a long list of environments if you installed Gym properly.
    We will play around with some of them in the next recipe, *Simulating Atari environments*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to the simple `pip` approach for installing Gym, the second approach
    provides more flexibility if you want to add new environments and modify Gym itself.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may wonder why we need to test reinforcement learning algorithms on Gym's
    environments since the actual environments we work in can be a lot different.
    You will recall that reinforcement learning doesn't make many assumptions about
    the environment, but it gets to know more about the environment by interacting
    with it. Also, when comparing the performance of different algorithms, we need
    to apply them to standardized environments. Gym is a perfect benchmark, covering
    many versatile and easy-to-use environments. This is similar to the datasets that
    we often use as benchmarks in supervised and unsupervised learning, such as MNIST,
    Imagenet, MovieLens, and Thomson Reuters News.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a look at the official Gym documentation at [https://gym.openai.com/docs/](https://gym.openai.com/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: Simulating Atari environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with Gym, let's play some Atari games with it.
  prefs: []
  type: TYPE_NORMAL
- en: The Atari environments ([https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari))
    are a variety of **Atari 2600** video games, such as Alien, AirRaid, Pong, and
    Space Race. If you have ever played Atari games, this recipe should be fun for
    you, as you will play an Atari game, Space Invaders. However, an agent will act
    on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s simulate the Atari environments by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To run any `atari` environment for the first time, we need to install the `atari`
    dependencies by running this command in the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, if you used the second approach in the previous recipe to `install
    gym`, you can run the following command instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing the Atari dependencies, we import the `gym` library in Python:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an instance of the `SpaceInvaders` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this also returns the initial state of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Render the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see a small window popping up, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a0f5de0-ef4a-4ca8-a069-84c9a1a27948.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the game window, the spaceship starts with three lives (the
    red spaceships).
  prefs: []
  type: TYPE_NORMAL
- en: 'Randomly pick one possible move and execute the action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `step()` method returns what happens after an action is taken, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**New state**: The new observation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward**: The reward associated with that action in that state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Is done**: A flag indicating whether the game ends. In a `SpaceInvaders`
    environment, this will be `True` if the spaceship has no more lives left or all
    the aliens are gone; otherwise, it will remain `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Info**: Extra information pertaining to the environment. This is about the
    number of lives left in this case. This is useful for debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at the `is_done` flag and `info`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The game window becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9a47f76-d9e2-49a6-96cd-c7272298495d.png)'
  prefs: []
  type: TYPE_IMG
- en: You won't notice much difference in the game window, because the spaceship just
    made a move.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s make a `while` loop and let the agent perform as many actions as
    it can:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Meanwhile, you will see that the game is running, and the spaceship keeps moving
    and shooting, and so do the aliens. And it is pretty fun to watch, too. At the
    end, when the game ends, the window looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7238068c-e6ed-43be-ae91-246820a14162.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we scored 150 points in this game. You may get a higher or lower
    score than this because the actions the agent performs are all randomly selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also confirm that no lives are left with the last piece of info:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Gym, we can easily create an environment instance by calling the `make()`
    method with the name of the environment as the parameter.
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noticed, the actions that the agent performs are randomly chosen
    using the `sample()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, normally, we would have a more sophisticated agent guided by reinforcement
    learning algorithms. Here, we just demonstrated how to simulate an environment,
    and how an agent takes actions regardless of the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run this a few times and see what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'There are six possible actions in total. We can also see this by running the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Actions from 0 to 5 stand for No Operation, Fire, Up, Right, Left, and Down,
    respectively, which are all the moves the spaceship in the game can do.
  prefs: []
  type: TYPE_NORMAL
- en: The `step()` method will let the agent take the action that is specified as
    its parameter. The `render()` method will update the display window based on the
    latest observation of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The observation of the environment, `new_state`, is represented by a 210 x
    160 x 3 matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This means that each frame of the display screen is an RGB image of size 210
    x 160.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may wonder why we need to install Atari dependencies. In fact, there are
    a few more environments that do not accompany the installation of `gym`, such
    as Box2d, Classic control, MuJoCo, and Robotics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the `Box2d` environments, for example; we need to install the `Box2d`
    dependencies before we first run the environments. Again, two installation approaches
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can play around with the `LunarLander` environment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'A game window will pop up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cff3554-8472-452d-8eb3-8b60457a851f.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are looking to simulate an environment but are not sure of the name you
    should use in the `make()` method, you can find it in the table of environments
    at [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    Besides the name used to call an environment, the table also shows the size of
    the observation matrix and the number of possible actions. Have fun playing around
    with the environments.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating the CartPole environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will work on simulating one more environment in order to
    get more familiar with Gym. The CartPole environment is a classic one in reinforcement
    learning research.
  prefs: []
  type: TYPE_NORMAL
- en: 'CartPole is a traditional reinforcement learning task in which a pole is placed
    upright on top of a cart. The agent moves the cart either to the left or to the
    right by 1 unit in a timestep. The goal is to balance the pole and prevent it
    from falling over. The pole is considered to have fallen if it is more than 12
    degrees from the vertical, or the cart moves 2.4 units away from the origin. An
    episode terminates when any of the following occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: The pole falls over
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of timesteps reaches 200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s simulate the `CartPole` environment by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: To run the CartPole environment, let's first search for its name in the table
    of environments at [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).
    We get `'CartPole-v0'` and also learn that the observation space is represented
    in a 4-dimensional array, and that there are two possible actions (which makes
    sense).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We import the Gym library and create an instance of the `CartPole` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Reset the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this also returns the initial state represented by an array
    of four floats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Render the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see a small window popping up, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/399dacfb-5fb5-4e3f-b3f1-23fcf51d02a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s make a `while` loop and let the agent perform as many random actions
    as it can:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Meanwhile, you will see that the cart and pole are moving. At the end, you
    will see they both stop. The window looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a1890bf2-76c8-4e3e-a86a-240b0d6a98ac.png)'
  prefs: []
  type: TYPE_IMG
- en: The episode only lasts several steps because the left or right actions are chosen
    randomly. Can we record the whole process so we can replay it afterward? We can
    do so with just two lines of code in Gym, as shown in *Step 7*. If you are using
    a Mac or Linux system, you need to complete *Step 6* first; otherwise, you can
    jump to *Step 7*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To record video, we need to install the `ffmpeg` package. For Mac, it can be
    installed via the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For Linux, the following command should do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the `CartPole` instance, add these two lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This will record what is displayed in the window and store it in the specified
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: Now re-run the codes from *Step 3* to *Step 5*. After an episode terminates,
    we can see that an `.mp4` file is created in the `video_dir` folder. The video
    is quite short; it may last 1 second or so.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we print out the state array for every step. But what does
    each float in the array mean? We can find more information about CartPole on Gym''s
    GitHub wiki page: [https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0).
    It turns out that those four floats represent the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cart position: This ranges from -2.4 to 2.4, and any position beyond this range
    will trigger episode termination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cart velocity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pole angle: Any value less than -0.209 (-12 degrees) or greater than 0.209
    (12 degrees) will trigger episode termination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pole velocity at the tip.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of the action, it is either 0 or 1, which corresponds to pushing the
    cart to the left and to the right, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The **reward** in this environment is +1 for every timestep before the episode
    terminates. We can also verify this by printing out the reward for every step.
    And the total reward is simply the number of timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've run only one episode. In order to assess how well the agent performs,
    we can simulate many episodes and then average the total rewards for an individual
    episode. The average total reward will tell us about the performance of the agent
    that takes random actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set 10,000 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'In each episode, we compute the total reward by accumulating the reward in
    every step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we calculate the average total reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: On average, taking a random action scores 22.25\.
  prefs: []
  type: TYPE_NORMAL
- en: We all know that taking random actions is not sophisticated enough, and we will
    implement an advanced policy in upcoming recipes. But for the next recipe, let's
    take a break and review the basics of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the fundamentals of PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we’ve already mentioned, PyTorch is the numerical computation library we
    use to implement reinforcement learning algorithms in this book.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is a trendy scientific computing and machine learning (including deep
    learning) library developed by Facebook. Tensor is the core data structure in
    PyTorch, which is similar to NumPy's `ndarrays`. PyTorch and NumPy are comparable
    in scientific computing. However, PyTorch is faster than NumPy in array operations
    and array traversing. This is mainly due to the fact that array element access
    is faster in PyTorch. Hence, more and more people believe PyTorch will replace
    NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s do a quick review of the basic programming in PyTorch to get more familiar
    with it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We created an uninitialized matrix in an earlier recipe. How about a randomly
    initialized one? See the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Random floats from a uniform distribution in the interval (0, 1) are generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can specify the desired data type of the returned tensor. For example, a
    tensor of the double type (`float64`) is returned as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: By default, `float` is the returned data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s create a matrix full of zeros and a matrix full of ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the size of a tensor, use this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`torch.Size` is actually a tuple.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To reshape a tensor, we can use the `view()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a tensor directly from data, including a single value, a list,
    and a nested list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the elements in a tensor of more than one element, we can use indexing
    in a similar way to NumPy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'As with a one-element tensor, we do so by using the `item()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Tensor and NumPy arrays are mutually convertible. Convert a tensor to a NumPy
    array using the `numpy()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert a NumPy array to a tensor with `from_numpy()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the input NumPy array is of the float data type, the output tensor
    will be of the double type. Typecasting may occasionally be needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following example, where a tensor of the double type is
    converted to a `float`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Operations in PyTorch are similar to NumPy as well. Take addition as an example;
    we can simply do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we can use the `add()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'PyTorch supports in-place operations, which mutate the tensor object. For example,
    let''s run this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see that `x3` is changed to the result of the original `x3`plus `x4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any method with **_** indicates that it is an in-place operation, which updates
    the tensor with the resulting value.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the full list of tensor operations in PyTorch, please go to the official
    docs at [https://pytorch.org/docs/stable/torch.html](https://pytorch.org/docs/stable/torch.html).
    This is the best place to search for information if you get stuck on a PyTorch
    programming problem.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and evaluating a random search policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After some practice with PyTorch programming, starting from this recipe, we
    will be working on more sophisticated policies to solve the CartPole problem than
    purely random actions. We start with the random search policy in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: A simple, yet effective, approach is to map an observation to a vector of two
    numbers representing two actions. The action with the higher value will be picked.
    The linear mapping is depicted by a weight matrix whose size is 4 x 2 since the
    observations are 4-dimensional in this case. In each episode, the weight is randomly
    generated and is used to compute the action for every step in this episode. The
    total reward is then calculated. This process repeats for many episodes and, in
    the end, the weight that enables the highest total reward will become the learned
    policy. This approach is called **random search** because the weight is randomly
    picked in each trial with the hope that the best weight will be found with a large
    number of trials.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and implement a random search algorithm with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the Gym and PyTorch packages and create an environment instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain the dimensions of the observation and action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: These will be used when we define the tensor for the weight matrix, which is
    size 4 x 2 in size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that simulates an episode given the input weight and returns
    the total reward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Here, we convert the state array to a tensor of the float type because we need
    to compute the multiplication of the state and weight tensor, `torch.matmul(state,
    weight)`, for linear mapping. The action with the higher value is selected using
    the `torch.argmax()` operation. And don't forget to take the value of the resulting
    action tensor using `.item()` because it is a one-element tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to keep track of the best total reward on the fly, as well as the corresponding
    weight. So, we specify their starting values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also record the total reward for every episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run `n_episode`. For each episode, we do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly pick the weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let the agent take actions according to the linear mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An episode terminates and returns the total reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the best total reward and the best weight if necessary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, keep a record of the total reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Put this into code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We have obtained the best policy through 1,000 random searches. The best policy
    is parameterized by `best_weight`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we test out the best policy in the testing episodes, we can calculate
    the average total reward achieved by random linear mapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: This is more than twice what we got from the random action policy (22.25).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how the learned policy performs on 100 new episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Surprisingly, the average reward for the testing episodes is close to the maximum
    of 200 steps with the learned policy. Be aware that this value may vary a lot.
    It could be anywhere from 160 to 200.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The random search algorithm works so well mainly because of the simplicity of
    our CartPole environment. Its observation state is composed of only four variables.
    You will recall that the observation in the Atari Space Invaders game is more
    than 100,000 (which is 210 * 160 * 3) . The number of dimensions of the action
    state in CartPole is a third of that in Space Invaders. In general, simple algorithms
    work well for simple problems. In our case, we simply search for the best linear
    mapping from the observation to the action from a random pool.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting thing we've noticed is that before we select and deploy
    the best policy (the best linear mapping), random search also outperforms random
    action. This is because random linear mapping does take the observations into
    consideration. With more information from the environment, the decisions made
    in the random search policy are more intelligent than completely random ones.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also plot the total reward for every episode in the training phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aef3787b-e607-4042-91bf-57b4f042aa56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you have not installed matplotlib, you can do so via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the reward for each episode is pretty random, and that there
    is no trend of improvement as we go through the episodes. This is basically what
    we expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the plot of reward versus episodes, we can see that there are some episodes
    in which the reward reaches 200\. We can end the training phase whenever this
    occurs since there is no room to improve. Incorporating this change, we now have
    the following for the training phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The policy achieving the maximal reward is found in episode 17\. Again, this
    may vary a lot because the weights are generated randomly for each episode. To
    compute the expectation of training episodes needed, we can repeat the preceding
    training process 1,000 times and take the average of the training episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: On average, we expect that it takes around 13 episodes to find the best policy.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the hill-climbing algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we can see in the random search policy, each episode is independent. In fact,
    all episodes in random search can be run in parallel, and the weight that achieves
    the best performance will eventually be selected. We've also verified this with
    the plot of reward versus episode, where there is no upward trend. In this recipe,
    we will develop a different algorithm, a hill-climbing algorithm, to transfer
    the knowledge acquired in one episode to the next episode.
  prefs: []
  type: TYPE_NORMAL
- en: In the hill-climbing algorithm, we also start with a randomly chosen weight.
    But here, for every episode, we add some noise to the weight. If the total reward
    improves, we update the weight with the new one; otherwise, we keep the old weight.
    In this approach, the weight is gradually improved as we progress through the
    episodes, instead of jumping around in each episode.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and implement the hill-climbing algorithm with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, import the necessary packages, create an environment instance, and
    obtain the dimensions of the observation and action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We will reuse the `run_episode` function we defined in the previous recipe,
    so we will not repeat it here. Again, given the input weight, it simulates an
    episode and returns the total reward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s make it 1,000 episodes for now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to keep track of the best total reward on the fly, as well as the corresponding
    weight. So, let''s specify their starting values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also record the total reward for every episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned, we will add some noise to the weight for each episode. In
    fact, we will apply a scale to the noise so that the noise won''t overwhelm the
    weight. Here, we will choose 0.01 as the noise scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run the `n_episode` function. After we randomly pick an initial
    weight, for each episode, we do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add random noise to the weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let the agent take actions according to the linear mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An episode terminates and returns the total reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the current reward is greater than the best one obtained so far, update the
    best reward and the weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, the best reward and the weight remain unchanged
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, keep a record of the total reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Put this into code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We also calculate the average total reward achieved by the hill-climbing version
    of linear mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'To assess the training using the hill-climbing algorithm, we repeat the training
    process multiple times (by running the code from *Step 4* to *Step 6* multiple
    times). We observe that the average total reward fluctuates a lot. The following
    are the results we got when running it 10 times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: What could cause such variance? It turns out that if the initial weight is bad,
    adding noise at a small scale will have little effect on improving the performance.
    This will cause poor convergence. On the other hand, if the initial weight is
    good, adding noise at a big scale might move the weight away from the optimal
    weight and jeopardize the performance. How can we make the training of the hill-climbing
    model more stable and reliable? We can actually make the noise scale adaptive
    to the performance, just like the adaptive learning rate in gradient descent.
    Let's see *Step 8* for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the noise adaptive, we do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify a starting noise scale.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the performance in an episode improves, decrease the noise scale. In our
    case, we take half of the scale, but set `0.0001` as the lower bound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the performance in an episode drops, increase the noise scale. In our case,
    we double the scale, but set `2` as the upper bound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Put this into code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The reward is increasing as the episodes progress. It reaches the maximum of
    200 within the first 100 episodes and stays there. The average total reward also
    looks promising:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We also plot the total reward for every episode as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'In the resulting plot, we can see a clear upward trend before it plateaus at
    the maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c200cf7-f129-476b-b722-3ce1730a4c80.png)'
  prefs: []
  type: TYPE_IMG
- en: Feel free to run the new training process a few times. The results are very
    stable compared to learning with a constant noise scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how the learned policy performs on 100 new episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the average performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: The average reward for the testing episodes is close to the maximum of 200 that
    we obtained with the learned policy. You can re-run the evaluation multiple times.
    The results are pretty consistent.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are able to achieve much better performance with the hill-climbing algorithm
    than with random search by simply adding adaptive noise to each episode. We can
    think of it as a special kind of gradient descent without a target variable. The
    additional noise is the gradient, albeit in a random way. The noise scale is the
    learning rate, and it is adaptive to the reward from the previous episode. The
    target variable in hill climbing becomes achieving the highest reward. In summary,
    rather than isolating each episode, the agent in the hill-climbing algorithm makes
    use of the knowledge learned from each episode and performs a more reliable action
    in the next episode. As the name hill climbing implies, the reward moves upwards
    through the episodes as the weight gradually moves towards the optimum value.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can observe that the reward can reach the maximum value within the first
    100 episodes. Can we just stop training when the reward reaches 200, as we did
    with the random search policy? That might not be a good idea. Remember that the
    agent is making continuous improvements in hill climbing. Even if it finds a weight
    that generates the maximum reward, it can still search around this weight for
    the optimal point. Here, we define the optimal policy as the one that can solve
    the CartPole problem. According to the following wiki page, [https://github.com/openai/gym/wiki/CartPole-v0](https://github.com/openai/gym/wiki/CartPole-v0),
    "solved" means the average reward over 100 consecutive episodes is no less than
    195.
  prefs: []
  type: TYPE_NORMAL
- en: 'We refine the stopping criterion accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: At episode 137, the problem is considered solved.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are interested in learning more about the hill-climbing algorithm, the
    following resources are useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Hill_climbing](https://en.wikipedia.org/wiki/Hill_climbing)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/](https://www.geeksforgeeks.org/introduction-hill-climbing-artificial-intelligence/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a policy gradient algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last recipe of the first chapter is about solving the CartPole environment
    with a policy gradient algorithm. This may be more complicated than we need for
    this simple problem, in which the random search and hill-climbing algorithms suffice.
    However, it is a great algorithm to learn, and we will use it in more complicated
    environments later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: In the policy gradient algorithm, the model weight moves in the direction of
    the gradient at the end of each episode. We will explain the computation of gradients
    in the next section. Also, in each step, it **samples** an action from the policy
    based on the probabilities computed using the state and weight. It no longer takes
    an action with certainty, in contrast with random search and hill climbing (by
    taking the action achieving the higher score). Hence, the policy switches from
    deterministic to **stochastic**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, it is time to implement the policy gradient algorithm with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, import the necessary packages, create an environment instance, and
    obtain the dimensions of the observation and action space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the `run_episode` function, which simulates an episode given the
    input weight and returns the total reward and the gradients computed. More specifically,
    it does the following tasks in each step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the probabilities, `probs`, for both actions based on the current
    state and input weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samples an action, `action`, based on the resulting probabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computes the derivatives, `d_softmax`, of the `softmax` function with the probabilities
    as input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divides the resulting derivatives, `d_softmax`, by the probabilities, probs,
    to get the derivatives, `d_log`, of the log term with respect to the policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applies the chain rule to compute the gradient, `grad`, of the weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records the resulting gradient, grad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performs the action, accumulates the reward, and updates the state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putting all of this into code, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: After an episode finishes, it returns the total reward obtained in this episode
    and the gradients computed for the individual steps. These two outputs will be
    used to update the weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make it 1,000 episodes for now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: This means we will run `run_episode` and `n_episode`times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initiate the weight:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also record the total reward for every episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of each episode, we need to update the weight using the computed
    gradients. For every step of the episode, the weight moves by *learning rate *
    gradient* calculated in this *step * total* reward in the remaining steps. Here,
    we choose `0.001` as the learning rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run `n_episode`episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we calculate the average total reward achieved by the policy gradient
    algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'We also plot the total reward for every episode as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'In the resulting plot, we can see a clear upward trend before it stays at the
    maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fae7b840-847f-4d56-a1b4-a19920b27520.png)'
  prefs: []
  type: TYPE_IMG
- en: We can also see that the rewards oscillate even after it converges. This is
    because the policy gradient algorithm is a stochastic policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how the learned policy performs on 100 new episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see the average performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The average reward for the testing episodes is close to the maximum value of
    200 for the learned policy. You can re-run the evaluation multiple times. The
    results are pretty consistent.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The policy gradient algorithm trains an agent by taking small steps and updating
    the weight based on the rewards associated with those steps at the end of an episode.
    The technique of having the agent run through an entire episode and then updating
    the policy based on the rewards obtained is called **Monte Carlo** policy gradient.
  prefs: []
  type: TYPE_NORMAL
- en: The action is selected based on the probability distribution computed based
    on the current state and the model’s weight. For example, if the probabilities
    for the left and right actions are [0.6, 0.4], this means the left action is selected
    60% of the time; it doesn't mean the left action is chosen, as in the random search
    and hill-climbing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the reward is 1 for each step before an episode terminates. Hence,
    the future reward we use to calculate the policy gradient at each step is the
    number of steps remaining. After each episode, we feed the gradient history multiplied
    by the future rewards to update the weight using the stochastic gradient ascent
    method. In this way, the longer an episode is, the bigger the update of the weight.
    This will eventually increase the chance of getting a larger total reward.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned at the start of this section, the policy gradient algorithm
    might be overkill for a simple environment such as CartPole, but it should get
    us ready for more complicated problems.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we examine the reward/episode plot, it seems that we can also stop early
    during training when it has been solved – the average reward over 100 consecutive
    episodes is no less than 195\. We just add the following lines of code to the
    training session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Re-run the training session. You should get something similar to the following,
    which stops after several hundred episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out [http://www.scholarpedia.org/article/Policy_gradient_methods](http://www.scholarpedia.org/article/Policy_gradient_methods)
    for more information about policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
