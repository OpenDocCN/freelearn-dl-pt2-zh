- en: Maze Solving with Deep Q-Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度Q网络解决迷宫问题
- en: Imagine for a moment that your data is not a discrete body of text or a carefully
    cleaned set of records from your organization's data warehouse. Perhaps you would
    like to train an agent to navigate an environment. How would you begin to solve
    this problem? None of the techniques that we have covered so far are suitable
    for such a task. We need to think about how we can train our model in quite a
    different way to make this problem tractable. Additionally, with use cases where
    the problem can be framed as an agent exploring and attaining a reward from an
    environment, from game playing to personalized news recommendations, **Deep Q-Networks**
    (**DQNs**) are useful tools in our arsenal of deep learning techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你的数据不是离散的文本体或者来自你组织数据仓库的精心清理的记录集合。也许你想训练一个代理去导航一个环境。你将如何开始解决这个问题？到目前为止，我们涵盖的技术都不适合这样的任务。我们需要考虑如何以一种完全不同的方式训练我们的模型，使得这个问题可解决。此外，在使用案例中，问题可以被定义为一个代理探索并从环境中获得奖励，从游戏玩法到个性化新闻推荐，**深度Q网络**
    (**DQNs**) 是我们深度学习技术武器库中有用的工具。
- en: '**Reinforcement learning** (**RL**) has been described by Yann LeCun (who was
    instrumental in the development of **Convolutional Neural Networks** (**CNNs**)
    and, at the time of writing, the director of Facebook AI Research) as the cherry
    on the cake of machine learning methods. In this analogy, unsupervised learning
    is the cake and supervised learning is the icing. What''s important for us to
    understand here is that RL only solves a very specific case of problems, despite
    offering the promise of model-free learning, where you simply offer some scalar
    reward as your model optimizes successfully toward the goal you have specified.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习** (**RL**) 被Yann LeCun描述为机器学习方法的“蛋糕上的樱桃”（他在**卷积神经网络** (**CNNs**) 的发展中起了重要作用，并且在撰写本文时是Facebook
    AI Research的主任）。在这个类比中，无监督学习是蛋糕，监督学习是糖霜。这里我们需要理解的重点是，尽管RL提供了无模型学习的承诺，你只需提供一些标量奖励作为你的模型朝着指定的目标成功优化的过程中。'
- en: 'This chapter will offer a brief background on why this is, and how RL fits
    into the picture more generally. Specifically, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将简要介绍为什么会这样，以及RL如何更普遍地融入图景中。具体而言，我们将涵盖以下主题：
- en: What is a DQN?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是DQN？
- en: Learning about the Q-learning algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Q-learning算法
- en: Learning about how to train a DQN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何训练一个**DQN**
- en: Building a DQN for solving mazes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个用于解决迷宫的DQN
- en: What is a DQN?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是DQN？
- en: 'As you will learn, a DQN is not that different from the standard feedforward
    and convolutional networks that we have covered so far. Indeed, all the standard
    ingredients are present:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将会学到的，一个DQN与我们迄今为止涵盖的标准前馈和卷积网络并没有太大的区别。事实上，所有标准的要素都存在：
- en: A representation of our data (in this example, the state of our maze and the
    agent trying to navigate through it)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们数据的表示（在这个例子中，是我们迷宫的状态以及试图通过它导航的代理的状态）
- en: Standard layers to process a representation of our maze, which also includes
    standard operations between these layers, such as the `Tanh` activation function
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理迷宫表示的标准层，其中包括这些层之间的标准操作，例如`Tanh`激活函数
- en: An output layer with a linear activation, which gives you predictions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有线性激活的输出层，这给出了预测结果
- en: Here, our predictions represent possible moves affecting the state of our input.
    In the case of maze solving, we are trying to predict moves that produce the maximum
    (and cumulative) expected reward for our player, which ultimately leads to the
    maze's exit. These predictions occur as part of a training loop, where the learning
    algorithm uses a *Gamma* variable as a decaying-over-time variable that balances
    the exploration of the environment's state space and the exploitation of knowledge
    gleaned by building up a map of actions, states, or rewards.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们的预测代表着可能影响输入状态的移动。在迷宫解决的情况下，我们试图预测产生最大（和累积）期望奖励的移动，最终导致迷宫的出口。这些预测是作为训练循环的一部分出现的，学习算法使用一个作为随时间衰减的变量的*Gamma*来平衡环境状态空间的探索和通过建立行动、状态或奖励地图获取的知识的利用。
- en: Let's introduce a number of new concepts. First, we need an *m* x *n* matrix
    that represents the rewards, *R*, for a given *state* (that is, a row) and *action*
    (that is, a column). We also need a *Q* table. This is a matrix (initialized with
    zero values) that represents the memory of the agent (that is, our player trying
    to find its way through the maze), or a history of states, actions taken, and
    their rewards.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们介绍一些新概念。首先，我们需要一个*m* x *n*矩阵，表示给定*状态*（即行）和*动作*（即列）的奖励*R*。我们还需要一个*Q*表。这是一个矩阵（初始化为零值），表示代理的记忆（即我们的玩家试图找到迷宫的方式）或状态历史、采取的行动及其奖励。
- en: 'These two matrices relate to each other. We can determine the memory (*Q* table)
    of our agent with respect to the table of known rewards with the following formula:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个矩阵相互关联。我们可以通过以下公式确定我们的代理的记忆*Q*表与已知奖励表的关系：
- en: '*Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q(状态, 动作) = R(状态, 动作) + Gamma * Max[Q(下一个状态, 所有动作)]*'
- en: Here, our epoch is an **episode**. Our agent performs an *action* and receives
    updates or rewards from the environment until the state of the system is terminal.
    In our example, this means getting stuck in the maze.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的时代是**一个回合**。我们的代理执行一个*动作*并从环境中获取更新或奖励，直到系统状态终止。在我们的例子中，这意味着迷宫中卡住了。
- en: The thing we are trying to learn is a policy. This policy is a function or a
    map of states to actions. It is a giant *n*-dimensional table of optimal actions
    given every possible state in our system.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图学习的东西是一个策略。这个策略是一个将状态映射到动作的函数或映射。它是一个关于我们系统中每个可能状态的最优动作的*n*维巨大表。
- en: Our ability to assess a state, *S*, is dependent on the assumption that it is
    a **Markov Decision Process** (**MDP**). As we've pointed out previously, this
    book is more concerned with implementation rather than theory; however, MDPs are
    fundamental to any real understanding of RL, so it's worth going over them in
    a bit of detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估状态*S*的能力取决于假设它是一个**马尔可夫决策过程**（**MDP**）。正如我们之前指出的，这本书更关注实现而非理论；然而，MDP对于真正理解RL至关重要，因此稍微详细地讨论它们是值得的。
- en: We use a capital *S* to denote all the possible states of our system. In the
    case of a maze, this is every possible location of an agent within the boundaries
    of the maze.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用大写*S*来表示系统的所有可能状态。在迷宫的情况下，这是迷宫边界内代理位置的所有可能位置。
- en: We use a lowercase *s* to denote a single state. The same applies to all actions, *A*,
    and an individual action, *a*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用小写*s*表示单个状态。对所有动作*A*和一个单独的动作*a*也是如此。
- en: Each pair *(s**, a)* produces a distribution of the rewards, *R*. It also produces
    *P*, which is referred to as the transition probability, where for a given *(s,
    a)*, the distribution of possible next states is *s(t + 1)*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每对*(s**, a)*生成奖励分布*R*。它还生成*P*，称为转移概率，对于给定的*(s, a)*，可能的下一个状态分布是*s(t + 1)*。
- en: 'We also have a hyperparameter, which is the discount factor (*gamma*). In the
    vein of hyperparameters generally, this is something we set ourselves. This is
    the relative value assigned to the predicted reward for a given time step. For
    example, let''s say we want to assign a greater value to the predicted rewards
    in the next time step, rather than after three time steps. We can represent this
    in the context of our objective in order to learn an optimal policy; the pseudocode
    looks like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个超参数，即折现因子(*gamma*)。一般来说，这是我们自己设置的超参数。这是为了预测奖励在给定时间步长时的相对价值。例如，假设我们希望为下一个时间步骤的预测奖励分配更大的价值，而不是三个时间步骤之后的奖励。我们可以在学习最优策略的目标的上下文中表示它；伪代码如下：
- en: '*OptimalPolicy = max(sum(gamma x reward) for timestep t*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*OptimalPolicy = max(sum(gamma x reward) for timestep t*'
- en: Breaking down the conceptual components of our DQN further, we can now talk
    about the value function. This function indicates the cumulative reward for a
    given state. For example, early on in our maze exploration, the cumulative expected
    reward is low. This is because of the number of possible actions or states our
    agent could take or occupy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分解我们的DQN的概念组件，我们现在可以讨论价值函数。这个函数表示给定状态的累积奖励。例如，在我们的迷宫探索早期，累积预期奖励较低。这是因为我们的代理可以采取或占据的可能动作或状态数量。
- en: Q-learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习
- en: 'Now, we come to the real meat of our system: the Q-value function. This includes
    the cumulative expected reward for actions *a1*, *a2*, and a given state, *s*.
    We are, of course, interested in finding the optimal Q-value function. This means
    that not only do we have a given *(s, a)*, but we have trainable parameters (the
    sum of the product) of the weights and biases in our DQN that we modify or update
    as we train our network. These parameters allow us to define an optimal policy,
    that is, a function to apply to any given states and actions available to the
    agent. This yields an optimal Q-value function, one that theoretically tells our
    agent what the best course of action is at any step. A bad football analogy might
    be the Q-value function as the coach yelling instructions into the rookie agent''s
    ear.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来到我们系统的真正核心：Q值函数。这包括对于给定状态*s*和动作*a1*、*a2*的累积预期奖励。当然，我们对找到最优Q值函数很感兴趣。这意味着我们不仅有一个给定的*(s,
    a)*，而且我们有可训练参数（权重和偏置在我们的DQN中的乘积的总和），我们在训练网络时修改或更新这些参数。这些参数允许我们定义一个最优策略，即适用于任何给定状态和代理可用动作的函数。这产生了一个最优Q值函数，理论上告诉我们的代理在任何步骤中最佳的行动是什么。一个不好的足球类比可能是Q值函数就像教练在新秀代理的耳边大喊指令。
- en: 'So, when written in pseudocode, our quest for an optimal policy looks like
    this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当以伪代码书写时，我们对最优策略的追求如下所示：
- en: '*OptimalPolicy = (state, action, theta)*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*最优策略 = （状态，动作，theta）*'
- en: Here, *theta* refers to the trainable parameters of our DQN.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*theta*指的是我们DQN的可训练参数。
- en: So, what is a DQN? Let's now examine the structure of our network in detail and,
    more importantly, how it is used. Here, we will bring in our Q-value functions
    and use our neural network to calculate the expected reward for a given state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是DQN？现在让我们详细检查我们网络的结构，更重要的是，它如何被使用。在这里，我们将引入我们的Q值函数，并使用我们的神经网络计算给定状态的预期奖励。
- en: 'Like the networks we have covered so far, there are a number of hyperparameters
    we set upfront:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们迄今为止涵盖的网络一样，我们提前设置了许多超参数：
- en: Gamma (the discount factor of future rewards, for example, 0.95)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma（未来奖励的折现因子，例如，0.95）
- en: Epsilon (exploration or exploitation, 1.0, skewed to exploration)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon（探索或利用，1.0，偏向探索）
- en: Epsilon decay (the shift to the exploitation of learned knowledge over time,
    for example, 0.995)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon衰减（随着时间的推移，从学习知识到利用知识的转变，例如，0.995）
- en: Epsilon decay minimum (for example, 0.01)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon衰减最小值（例如，0.01）
- en: Learning rate (this is still set by default despite using the **Adaptive Moment
    Estimation** (**Adam**))
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率（尽管使用**自适应矩估计**（**Adam**）仍然是默认设置）
- en: State size
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态大小
- en: Action size
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作大小
- en: Batch size (in powers of two; start with 32 and tune your way from there)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小（以2的幂为单位；从32开始，逐步调整）
- en: Number of episodes
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节目数
- en: We also need a fixed sequential memory for the experience replay feature, sizing
    it at 2,000 entries.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个固定的顺序记忆来进行经验重播功能，将其大小设置为2,000条目。
- en: Optimization and network architecture
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化和网络架构
- en: As for our optimization method, we use Adam. You may recall from [Chapter 2](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml),
    *What is a Neural Network and How Do I Train One?*, that the Adam solver belongs
    to the class of solvers that use a dynamic learning rate. In vanilla SGD, we fix
    the learning rate. Here, the learning rate is set per parameter, giving us more
    control in cases where sparsity of data (vectors) is a problem. Additionally,
    we use the root MSE propagation versus the previous gradient, understanding the
    rate of change in the shape of our optimization surface and, by doing so, improving
    how our network handles noise in the data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我们的优化方法，我们使用Adam。您可能还记得来自[第2章](d80f3d0b-0a4e-4695-923c-4feef972214a.xhtml)的内容，*什么是神经网络，我如何训练一个？*，Adam求解器属于使用动态学习率的求解器类别。在传统的SGD中，我们固定学习率。在这里，学习率针对每个参数进行设置，使我们在数据（向量）稀疏的情况下更具控制力。此外，我们使用根均方误差传播与先前梯度相比，理解我们优化表面形状的变化速率，并通过这样做改进我们的网络如何处理数据中的噪声。
- en: 'Now, let''s talk about the layers of our neural network. Our first two layers
    are standard feedforward networks with **Rectified Linear Unit** (**ReLU**) activation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈我们神经网络的层次。我们的前两层是标准的前馈网络，采用**整流线性单元**（**ReLU**）激活：
- en: '*output = activation(dotp(input, weights) + bias)*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出 = 激活（点积（输入，权重） + 偏置）*'
- en: The first is sized according to the state size (that is, a vector representation
    of all the possible states in the system).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个按状态大小进行调整（即系统中所有可能状态的向量表示）。
- en: Our output layer is restricted to the number of possible actions. These are
    achieved by applying a linear activation to our second hidden dimension's output.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出层限制为可能动作的数量。这些通过将线性激活应用于我们第二隐藏维度的输出来实现。
- en: Our loss function depends on the task and data we have; in general, we will
    use MSE or cross-entropy loss.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的损失函数取决于任务和我们拥有的数据；通常我们会使用MSE或交叉熵损失。
- en: Remember, act, and replay!
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记住，行动，然后重放！
- en: 'Beyond the usual suspects involved in our neural network, we need to define
    additional functions for our agent''s memory. The remember function takes a number
    of inputs, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们神经网络中通常涉及的对象，我们需要为代理的记忆定义额外的函数。`remember`函数接受多个输入，如下所示：
- en: State
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态
- en: Action
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动
- en: Reward
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励
- en: Next state
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个状态
- en: Is done
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否完成
- en: It appends these values to the memory (that is, a sequentially ordered list).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它将这些值附加到内存中（即，一个按顺序排列的列表）。
- en: 'We now define how an agent takes an action in an act function. This is where
    we manage the balance between the exploration of the state space and the exploitation
    of learned knowledge. These are the steps to follow:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义代理如何在`act`函数中采取行动。这是我们管理探索状态空间和利用学习知识之间平衡的地方。遵循以下步骤：
- en: It takes in one value, that is, the `state`.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它接收一个值，即`state`。
- en: From there, it applies `epsilon`; that is, if a random value between 0 and 1
    is less than `epsilon`, then take a random action. Over time, our epsilon decays,
    reducing the randomness of the action!
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那里，应用`epsilon`；也就是说，如果介于0到1之间的随机值小于`epsilon`，则采取随机动作。随着时间的推移，我们的epsilon会衰减，减少动作的随机性！
- en: We then feed the state into our model to make a prediction about what action
    to take.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将状态输入到我们的模型中，以预测应采取的行动。
- en: From this function, we return `max(a)`.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个函数中，我们返回`max(a)`。
- en: 'The additional function we need is for the experience replay. The steps that
    this function take are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的额外函数是用于经验回放的。此函数的步骤如下：
- en: Create a random sample (of `batch_size`) selected from our 2,000-unit memory,
    which was defined and added to by the preceding remember function
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机样本（`batch_size`）从我们的2000单位内存中选择，这是由前面的`remember`函数定义并添加的。
- en: 'Iterate over the `state`, `action`, `reward`, `next_state`, and `isdone` inputs,
    as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历`state`，`action`，`reward`，`next_state`和`isdone`输入，如下所示：
- en: Set `target` = `reward`
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`target` = `reward`。
- en: 'If not done, then use the following formula:'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果未完成，则使用以下公式：
- en: '*Estimated future reward = current reward + (discounting factor (gamma) * call
    to model(predicted max expected reward) of next_state)*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*估计的未来奖励 = 当前奖励 + （折现因子（gamma）* 模型预测的下一个状态的预期最大奖励的调用）*'
- en: Map the future `reward` input to the model (that is, the predicted future `reward`
    input from the current state)
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将未来的`reward`输入映射到模型（即从当前状态预测的未来`reward`输入）。
- en: Finally, `replay` the memory by passing the current state and the targeted future
    reward for a single epoch of training
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过传递当前状态和单个训练时期的目标未来奖励来`重放`记忆。
- en: Decrement `epsilon` by using `epsilon_decay`
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`epsilon_decay`递减`epsilon`。
- en: This covers the theory of DQNs and Q-learning more generally; now, it's time
    to write some code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分涵盖了DQNs和Q-learning的理论，现在是写一些代码的时候了。
- en: Solving a maze using a DQN in Gorgonia
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Gorgonia中使用DQN解决迷宫问题。
- en: Now, it's time to build our maze solver!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候建立我们的迷宫求解器了！
- en: 'Using a DQN to solve a little ASCII maze is a bit like bringing a bulldozer
    to the beach to make sandcastles for your kids: it''s completely unnecessary,
    but you get to play with a big machine. However, as a tool for learning about
    DQNs, mazes are invaluable. This is because the number of states or actions in
    the game is limited, and the representation of constraints is also simple (such
    as the *walls* of our maze that our agent cannot move through). This means that
    we can step through our program and easily inspect what our network is doing.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DQN解决一个小ASCII迷宫有点像带着推土机去沙滩为你的孩子做沙堡：完全不必要，但你可以玩一个大机器。然而，作为学习DQN的工具，迷宫是无价的。这是因为游戏中的状态或动作数量有限，约束的表示也很简单（例如我们的迷宫的*墙壁*代表了我们的代理无法通过的障碍）。这意味着我们可以逐步执行我们的程序并轻松检查我们的网络在做什么。
- en: 'We will follow these steps:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤进行：
- en: Create a `maze.go` file for this bit of code
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这段代码创建一个`maze.go`文件。
- en: Import our libraries and set our data type
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们的库并设置我们的数据类型。
- en: Define our `Maze{}`
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们的`Maze{}`
- en: Write a `NewMaze()` function to instantiate this `struct`
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个`NewMaze()`函数来实例化这个`struct`。
- en: 'We also need to define our `Maze{}` helper functions. These include the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义我们的`Maze{}`辅助函数。这些包括以下内容：
- en: '`CanMoveTo()`: Check whether a move is valid'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CanMoveTo()`: 检查移动是否有效'
- en: '`Move()`: Move our player to a co-ordinate in the maze'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Move()`: 将我们的玩家移动到迷宫中的一个坐标'
- en: '`Value()`: Return the reward for a given action'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Value()`: 返回给定动作的奖励'
- en: '`Reset()`: Set player to start co-ordinates'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reset()`: 将玩家设置到起始坐标'
- en: 'Let''s take a look at the start of the code for our maze generator. This is
    an excerpt, and the remainder of the code can be found in the book''s GitHub repository:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们迷宫生成器代码的开头。这是一个摘录，其余的代码可以在书的GitHub仓库中找到：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we''ve got the code that we need to generate and interact with a maze,
    we need to define the simple feedforward, fully connected network. This code should
    be familiar to us by now. Let''s create `nn.go`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了我们需要生成和与迷宫交互的代码，我们需要定义简单的前馈全连接网络。到现在为止，这段代码应该对我们来说已经很熟悉了。让我们创建`nn.go`：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now begin to define the DQN that will make use of this neural network.
    First, let''s create a `memory.go` file with the basic `struct` type that captures
    information about a given episode:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始定义将利用这个神经网络的DQN了。首先，让我们创建一个`memory.go`文件，其中包含捕获给定情节信息的基本`struct`类型：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will make a memory of `[]Memories` and use it to store the per-play X/Y state
    co-ordinates, move vectors, expected reward, next states/possible moves, and whether
    the maze is solved.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个`[]Memories`的记忆，并用它来存储每次游戏的X/Y状态坐标、移动向量、预期奖励、下一个状态/可能的移动以及迷宫是否已解决。
- en: 'Now we can edit our `main.go` and pull everything together. First, we define
    our possible moves across the *m x n* matrix that represents our maze:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编辑我们的`main.go`，把一切整合在一起。首先，我们定义跨*m x n*矩阵的可能移动：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we need our main `DQN{}` structure to which we attach the neural network
    we defined earlier, our VM/Solver, and our DQN-specific hyper parameters. We also
    need an `init()` function to build the embedded feedforward network as well as
    the `DQN` object itself:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要我们的主`DQN{}`结构，我们在其中附加了之前定义的神经网络、我们的VM/Solver以及我们DQN特定的超参数。我们还需要一个`init()`函数来构建嵌入的前馈网络以及`DQN`对象本身：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next is our experience `replay()` function. Here, we first create batches of
    memory from which we retrain and update our network, gradually updating our epsilon:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们的经验`replay()`函数。在这里，我们首先从记忆中创建批次，然后重新训练和更新我们的网络，逐步更新我们的epsilon：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `predict()` function—called when we are determining the best possible move
    (or move with the greatest predicted reward)—is next. It takes a player''s position
    in the maze and a single move, and returns our neural network''s projected reward
    for that move:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`predict()`函数，在确定最佳移动（或具有最大预测奖励的移动）时调用。它接受迷宫中玩家的位置和一个单一移动，并返回我们神经网络对该移动的预期奖励：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then define our main training loop for `n` episodes, moving around the maze
    and building our DQN''s memory:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为`n`个情节定义我们的主训练循环，围绕迷宫移动并构建我们的DQN的记忆：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We also need a `bestAction()` function that selects the best possible move
    to take, given a slice of options and an instance of our maze:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个`bestAction()`函数，根据选项切片和我们迷宫的实例选择最佳移动：
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we define a `getPossibleActions()` function to produce a slice of
    possible moves, given our maze and our little `max()` helper function for finding
    the maximum value in a slice of `float32s`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义一个`getPossibleActions()`函数来生成可能移动的切片，考虑到我们的迷宫和我们的小`max()`辅助函数，用于找到`float32s`切片中的最大值：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With all those pieces in place, we can write our `main()` function to complete
    our DQN. We begin by setting `vars`, which includes our epsilon. Then, we initialize `DQN{}`
    and instantiate `Maze`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些部分齐全后，我们可以编写我们的`main()`函数完成我们的DQN。我们从设置`vars`开始，其中包括我们的epsilon。然后，我们初始化`DQN{}`并实例化`Maze`：
- en: 'We then kick off our training loop and, once complete, try to solve our maze:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们启动我们的训练循环，一旦完成，尝试解决我们的迷宫：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, let''s execute our program and observe the outputs:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行我们的程序并观察输出：
- en: '![](img/ec332bbe-2012-41eb-9c88-4328f20b0ace.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec332bbe-2012-41eb-9c88-4328f20b0ace.png)'
- en: We can see the dimensions of the maze, as well as a simple representation of
    walls (`1`), a clear path (`o`), our player (`2`), and our maze exit (`3`). The
    next line, `{1 0} {9 20}`, tells us the exact *(X, Y)* co-ordinates of our player's
    starting point and the maze's exit, respectively. We then loop through the movement
    vectors as a sanity check and begin our training run across `n` episodes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到迷宫的尺寸，以及墙壁（`1`）、明显路径（`o`）、我们的玩家（`2`）和迷宫出口（`3`）的简单表示。接下来的一行，`{1 0} {9 20}`，告诉我们玩家起点和迷宫出口的确切*(X,
    Y)*坐标。然后我们通过移动向量进行一次健全性检查，并开始我们的训练运行跨过`n`剧集。
- en: 'Our agent now moves through the maze:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的智能体现在通过迷宫移动：
- en: '![](img/4413ba38-a2c7-4478-bd05-3597ff7882e7.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4413ba38-a2c7-4478-bd05-3597ff7882e7.png)'
- en: You can experiment with different numbers of episodes (and episode lengths),
    and generate larger and more complex mazes!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试不同数量的剧集（和剧集长度），并生成更大更复杂的迷宫！
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we had look into the background of RL and what a DQN is, including
    the Q-learning algorithm. We have seen how DQNs offer a unique (relative to the
    other architectures that we've discussed so far) approach to solving problems.
    We are not supplying *output labels* in the traditional sense as with, say, our
    CNN from [Chapter 5](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml), *Next Word Prediction
    with Recurrent Neural Networks*, which processed CIFAR image data. Indeed, our
    output label was a cumulative reward for a given action relative to an environment's
    state, so you may now see that we have dynamically created output labels. But
    instead of them being an end goal for our network, these labels help a virtual
    agent make intelligent decisions within a discrete space of possibilities. We
    also looked into what types of predictions we can make around rewards or actions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入了解了强化学习的背景以及什么是DQN，包括Q-learning算法。我们看到了DQN相对于我们迄今讨论的其他架构提供了一种独特的解决问题的方法。我们没有像传统意义上的输出标签那样为CNN提供输出标签，例如我们在[第5章](b22a0573-9e14-46a4-9eec-e3f2713cb5f8.xhtml)中处理CIFAR图像数据时的情况。事实上，我们的输出标签是相对于环境状态的给定动作的累积奖励，因此你现在可以看到我们已经动态创建了输出标签。但是，这些标签不是网络的最终目标，而是帮助虚拟智能体在离散的可能性空间内做出智能决策。我们还探讨了我们可以在奖励或行动周围做出何种类型的预测。
- en: Now you can think about other possible applications for a DQN and, more generally,
    for problems where you have a simple reward of some kind but no labels for your
    data—the canonical example being an agent in some sort of environment. The *agent*
    and *environment* should be defined in the most general way possible, as you are
    not limited to a bit of math playing Atari games or trying to solve a maze. For
    example, a user of your website can be considered an agent, and an environment
    is a space in which you have some kind of feature-based representation of your
    content. You could use this approach to build a recommendation engine for news.
    You can refer to the *Further reading* section for a link to a paper that you
    may want to implement as an exercise.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以考虑使用DQN（Deep Q-Network）的其他可能应用，更普遍地应用于一些问题，其中你有某种简单的奖励但没有数据的标签——典型的例子是在某种环境中的智能体。*智能体*和*环境*应该以尽可能通用的方式定义，因为你不仅仅局限于数学玩Atari游戏或尝试解决迷宫问题。例如，你网站的用户可以被视为一个智能体，而环境则是一个具有基于特征表示的内容空间。你可以使用这种方法来构建一个新闻推荐引擎。你可以参考*进一步阅读*部分的一篇论文链接，这可能是你想要作为练习实现的内容。
- en: In the next chapter, we will look into building a **Variational Autoencoder**
    (**VAE**) and learn about the advantages that a VAE has over a standard autoencoder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨构建**变分自编码器**（**VAE**）以及VAE相对于标准自编码器的优势。
- en: Further reading
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Playing Atari with Deep Reinforcement Learning*, available at [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用深度强化学习玩Atari游戏*，可在[https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)获取。'
- en: '*DRN: A Deep Reinforcement Learning Framework for News Recommendation*, available
    at [http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DRN：用于新闻推荐的深度强化学习框架*，可在[http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf](http://www.personal.psu.edu/~gjz5038/paper/www2018_reinforceRec/www2018_reinforceRec.pdf)获取。'
