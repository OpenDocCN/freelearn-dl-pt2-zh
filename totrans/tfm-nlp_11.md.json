["```py\n!pip install -q transformers \n```", "```py\nfrom transformers import pipeline \n```", "```py\n    pipeline(\"<task-name>\") \n    ```", "```py\n    pipeline(\"<task-name>\", model=\"<model_name>\") \n    ```", "```py\n    pipeline('<taskname>', model='<model name>', tokenizer='<tokenizer_name>') \n    ```", "```py\nnlp_qa = pipeline('question-answering') \n```", "```py\nsequence = \"The traffic began to slow down on Pioneer Boulevard in Los Angeles, making it difficult to get out of the city. However, WBGO was playing some cool jazz, and the weather was cool, making it rather pleasant to be making it out of the city on this Friday afternoon. Nat King Cole was singing as Jo, and Maria slowly made their way out of LA and drove toward Barstow. They planned to get to Las Vegas early enough in the evening to have a nice dinner and go see a show.\" \n```", "```py\nnlp_qa(context=sequence, question='Where is Pioneer Boulevard ?') \n```", "```py\n{'answer': 'Los Angeles,', 'end': 66, 'score': 0.988201259751591, 'start': 55} \n```", "```py\nnlp_ner = pipeline(\"ner\") \n```", "```py\nsequence = \"The traffic began to slow down on Pioneer Boulevard in Los Angeles, making it difficult to get out of the city. However, WBGO was playing some cool jazz, and the weather was cool, making it rather pleasant to be making it out of the city on this Friday afternoon. Nat King Cole was singing as Jo and Maria slowly made their way out of LA and drove toward Barstow. They planned to get to Las Vegas early enough in the evening to have a nice dinner and go see a show.\" \n```", "```py\nprint(nlp_ner(sequence)) \n```", "```py\n[{'word': 'Pioneer', 'score': 0.97, 'entity': 'I-LOC', 'index': 8}, \n{'word': 'Boulevard', 'score': 0.99, 'entity': 'I-LOC', 'index': 9}, \n{'word': 'Los', 'score': 0.99, 'entity': 'I-LOC', 'index': 11}, \n{'word': 'Angeles', 'score': 0.99, 'entity': 'I-LOC', 'index': 12}, \n{'word': 'W', 'score': 0.99, 'entity': 'I-ORG', 'index': 26}, \n{'word': '##B', 'score': 0.99, 'entity': 'I-ORG', 'index': 27}, \n{'word': '##G', 'score': 0.98, 'entity': 'I-ORG', 'index': 28}, \n{'word': '##O', 'score': 0.97, 'entity': 'I-ORG', 'index': 29}, \n{'word': 'Nat', 'score': 0.99, 'entity': 'I-PER', 'index': 59}, \n{'word': 'King', 'score': 0.99, 'entity': 'I-PER', 'index': 60}, \n{'word': 'Cole', 'score': 0.99, 'entity': 'I-PER', 'index': 61}, \n{'word': 'Jo', 'score': 0.99, 'entity': 'I-PER', 'index': 65}, \n{'word': 'Maria', 'score': 0.99, 'entity': 'I-PER', 'index': 67},\n{'word': 'LA', 'score': 0.99, 'entity': 'I-LOC', 'index': 74}, \n{'word': 'Bar', 'score': 0.99, 'entity': 'I-LOC', 'index': 78}, \n{'word': '##sto', 'score': 0.85, 'entity': 'I-LOC', 'index': 79}, \n{'word': '##w', 'score': 0.99, 'entity': 'I-LOC', 'index': 80}, \n{'word': 'Las', 'score': 0.99 'entity': 'I-LOC', 'index': 87}, \n{'word': 'Vegas', 'score': 0.9989519715309143, 'entity': 'I-LOC', 'index': 88}] \n```", "```py\n[{'word': 'Pioneer', 'score': 0.97, 'entity': 'I-LOC', 'index': 8}, \n{'word': 'Boulevard', 'score': 0.99, 'entity': 'I-LOC', 'index': 9}, \n{'word': 'Los', 'score': 0.99, 'entity': 'I-LOC', 'index': 11}, \n{'word': 'Angeles', 'score': 0.99, 'entity': 'I-LOC', 'index': 12}, \n{'word': 'LA', 'score': 0.99, 'entity': 'I-LOC', 'index': 74}, \n{'word': 'Bar', 'score': 0.99, 'entity': 'I-LOC', 'index': 78}, \n{'word': '##sto', 'score': 0.85, 'entity': 'I-LOC', 'index': 79}, \n{'word': '##w', 'score': 0.99, 'entity': 'I-LOC', 'index': 80}, \n{'word': 'Las', 'score': 0.99 'entity': 'I-LOC', 'index': 87}, \n{'word': 'Vegas', 'score': 0.9989519715309143, 'entity': 'I-LOC', 'index': 88}] \n```", "```py\nfor i in range beginning of output to end of the output:\n    filter records containing I-LOC\n    merge the I-LOCs that fit together\n    save the merged I-LOCs for questions-answering \n```", "```py\nfrom the first location to the last location:\n    choose randomly:\n        Template 1: Where is [I-LOC]?\n        Template 2: Where is [I-LOC] located? \n```", "```py\nWhere is Pioneer Boulevard?\nWhere is Los Angeles located?\nWhere is LA?\nWhere is Barstow?\nWhere is Las Vegas located? \n```", "```py\nnlp_qa = pipeline('question-answering')\nprint(\"Question 1.\",nlp_qa(context=sequence, question='Where is Pioneer Boulevard ?'))\nprint(\"Question 2.\",nlp_qa(context=sequence, question='Where is Los Angeles located?'))\nprint(\"Question 3.\",nlp_qa(context=sequence, question='Where is LA ?'))\nprint(\"Question 4.\",nlp_qa(context=sequence, question='Where is Barstow ?'))\nprint(\"Question 5.\",nlp_qa(context=sequence, question='Where is Las Vegas located ?')) \n```", "```py\nQuestion 1\\. {'score': 0.9879662851935791, 'start': 55, 'end': 67, 'answer': 'Los Angeles,'}\nQuestion 2\\. {'score': 0.9875189033668121, 'start': 34, 'end': 51, 'answer': 'Pioneer Boulevard'}\nQuestion 3\\. {'score': 0.5090435442006118, 'start': 55, 'end': 67, 'answer': 'Los Angeles,'}\nQuestion 4\\. {'score': 0.3695214621538554, 'start': 387, 'end': 396, 'answer': 'Las Vegas'}\nQuestion 5\\. {'score': 0.21833994202792262, 'start': 355, 'end': 363, 'answer': 'Barstow.'} \n```", "```py\nnlp_qa = pipeline('question-answering')\nnlp_qa(context=sequence, question='Who was singing ?') \n```", "```py\n{'answer': 'Nat King Cole,'\n 'end': 277,\n 'score': 0.9653632081862433,\n 'start': 264} \n```", "```py\nnlp_qa(context=sequence, question='Who was going to Las Vegas ?') \n```", "```py\n{'answer': 'Nat King Cole,'\n 'end': 277,\n 'score': 0.3568152742800521,\n 'start': 264} \n```", "```py\nnlp_qa(context=sequence, question='Who are they?') \n```", "```py\n{'answer': 'Jo and Maria',\n 'end': 305,\n 'score': 0.8486017557290779,\n 'start': 293} \n```", "```py\nnlp_qa(context=sequence, question='Who drove to Las Vegas?') \n```", "```py\n{'answer': 'Nat King Cole was singing as Jo and Maria',\n 'end': 305,\n 'score': 0.35940926070820467,\n 'start': 264} \n```", "```py\nprint(nlp_qa.model) \n```", "```py\nDistilBertForQuestionAnswering((distilbert): DistilBertModel( \n```", "```py\n(5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True) \n```", "```py\nnlp_qa = pipeline('question-answering', model='google/electra-small-generator', tokenizer='google/electra-small-generator')\nnlp_qa(context=sequence, question='Who drove to Las Vegas ?') \n```", "```py\n{'answer': 'to slow down on Pioneer Boulevard in Los Angeles, making it difficult to',\n 'end': 90,\n 'score': 2.5295573154019736e-05,\n start': 18} \n```", "```py\n- This IS expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture..\n- This IS NOT expected if you are initializing ElectraForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical.. \n```", "```py\nThe traffic began to slow down on Pioneer Boulevard in Los Angeles, making it difficult to get out of the city. However, WBGO was playing some cool jazz, and the weather was cool, making it rather pleasant to be making it out of the city on this Friday afternoon. Nat King Cole was singing as Jo and Maria slowly made their way out of LA and drove toward Barstow. They planned to get to Las Vegas early enough in the evening to have a nice dinner and go see a show. \n```", "```py\nverbs={\"began,\" \"slow,\" \"making\"(1), \"playing,\" \"making\"(2), \"making\"(3), \"singing,\",â€¦, \"made,\" \"drove,\" \"planned,\" go,\" see\"} \n```", "```py\ndef maxcount:\nfor in range first verb to last verb:\n    for each verb\n       counter +=1\n       if counter>max_count, filter verb \n```", "```py\nverbs={\"began,\" \"slow,\" \"playing,\" \"singing,\" \"drove,\" \"planned,\" go,\" see\"} \n```", "```py\nbegan: The traffic [V: began] [ARG1: to slow down on Pioneer Boulevard in Los Angeles , making it difficult to get out of the city] . However , WBGO was playing some cool jazz] , and the weather was cool , making it rather pleasant to be making it out of the city on this Friday afternoon . Nat King Cole was singing as Jo and Maria slowly made their way out of LA and drove toward Barstow . They planned to get to Las Vegas early enough in the evening to have a nice dinner and go see a show . \n```", "```py\ndef maxlength:\nfor in range first verb to last verb:\n    for each verb\n       if length(argument of verb)>max_length, filter verb \n```", "```py\nverbs={ \"slow\", \"playing\", \"singing\", \"drove\",   \"planned\",\" go\",\" see\"} \n```", "```py\nslow: [ARG1: The traffic] began to [V: slow] down [ARG1: on] [ARGM-ADV: Pioneer Boulevard] [ARGM-LOC: in Los Angeles] , [ARGM-ADV: making it difficult to get out of the city] . \n```", "```py\ndef whowhat:\n   if NER(ARGi)==I-PER, then:\n        template=Who is [VERB]  \n   if NER(ARGi)!=I-PER, then:\n     template=What is [VERB] \n```", "```py\nWhat is slow? \n```", "```py\nnlp_qa = pipeline ('question-answering')\nnlp_qa(context= sequence, question='What was slow?') \n```", "```py\n{'answer': 'The traffic',\n'end': 11, \n'score': 0.4652545872921081, \n'start': 0} \n```", "```py\nplaying: The traffic began to slow down on Pioneer Boulevard in Los Angeles , making it difficult to get out of the city . [ARGM-DIS: However] , [ARG0: WBGO] was [V: playing] [ARG1: some cool jazz] \n```", "```py\nWhat is playing? \n```", "```py\nnlp_qa = pipeline('question-answering')\nnlp_qa(context=sequence, question='What was playing') \n```", "```py\n{'answer': 'cool jazz,,'\n 'end': 153,\n 'score': 0.35047012837950753,\n 'start': 143} \n```", "```py\nWho is singing? \n```", "```py\nnlp_qa = pipeline('question-answering')\nnlp_qa(context=sequence, question='Who sees a show?') \n```", "```py\n{'answer': 'Nat King Cole,'\n 'end': 277,\n 'score': 0.5587267250683112,\n 'start': 264} \n```", "```py\nSet0={'Los Angeles', 'the city,' 'LA'}\nSet1=[Jo and Maria, their, they} \n```", "```py\n# Install Haystack\n!pip install farm-haystack==0.6.0\n# Install specific versions of urllib and torch to avoid conflicts with preinstalled versions on Colab\n!pip install urllib3==1.25.4\n!pip install torch==1.6.0+cu101-f https://download.pytorch.org/whl/torch_stable.html \n```", "```py\n# Load a  local model or any of the QA models on Hugging Face's model hub (https://huggingface.co/models)\nfrom haystack.reader.farm import FARMReader\nreader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True, no_ans_boost=0, return_no_answer=False) \n```", "```py\ntext = \"The traffic began to slow down on Pioneer Boulevard inâ€¦/â€¦ have a nice dinner and go see a show.\" \n```"]