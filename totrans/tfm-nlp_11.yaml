- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let Your Data Do the Talking: Story, Questions, and Answers'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading comprehension requires many skills. When we read a text, we notice the
    keywords and the main events and create mental representations of the content.
    We can then answer questions using our knowledge of the content and our representations.
    We also examine each question to avoid traps and making mistakes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: No matter how powerful they have become, transformers cannot answer open questions
    easily. An open environment means that somebody can ask any question on any topic,
    and a transformer would answer correctly. That is difficult but possible to some
    extent with GPT-3, as we will see in this chapter. However, transformers often
    use general domain training datasets in a closed question-and-answer environment.
    For example, critical answers in medical care and law interpretation will often
    require additional NLP functionality.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: However, transformers cannot answer any question correctly regardless of whether
    the training environment is closed with preprocessed question-answer sequences.
    A transformer model can sometimes make wrong predictions if a sequence contains
    more than one subject and compound propositions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will focus on methods to build a question generator that finds
    unambiguous content in a text with the help of other NLP tasks. The question generator
    will show some of the ideas applied to implement question-answering.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by showing how difficult it is to ask random questions and expect
    the transformer to respond well every time.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: We will help a `DistilBERT` model answer questions by introducing **Named Entity
    Recognition** (**NER**) functions that suggest reasonable questions. In addition,
    we will lay the ground for a question generator for transformers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: We will add an ELECTRA model that was pretrained as a discriminator to our question-answering
    toolbox.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: We will continue by adding **Semantic Role Labeling** (**SRL**) functions to
    the blueprint of the text generator.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Then, the *Next steps* section will provide additional ideas to build a reliable
    question-answering solution, including implementing the `Haystack` framework.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will go straight to the GPT-3 Davinci engine interface online to
    explore question-answering tasks in an open environment. Again, no development,
    no training, and no preparation are required!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will see how to build your own multi-task NLP
    helpers or use Cloud AI for question-answering.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The limits of random question-answering
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using NER to create meaningful questions based on entity identification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beginning to design the blueprint of a question generator for transformers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the questions found with NER
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing an ELECTRA encoder pretrained as a discriminator
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the ELECTRA model with standard questions
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SRL to create meaningful questions based on predicate identification
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Project management guidelines to implement question-answering transformers
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施问答变压器的项目管理指南
- en: Analyzing how to create a question generated using SRL
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析如何利用SRL生成问题
- en: Using the output of NER and SRL to define the blueprint of a question generator
    for transformers
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用NER和SRL的输出来定义变压器问题生成器的蓝图
- en: Exploring Haystack’s question-answering framework with RoBERTa
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RoBERTa探索Haystack的问答框架
- en: Using GPT-3’s interface requires no development or preparation
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GPT-3的界面无需开发或准备
- en: Let’s begin by going through the methodology we will apply to analyze the generation
    of questions for question-answering tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过我们将应用的方法来分析问答任务的问题生成过程。
- en: Methodology
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: Question-answering is mainly presented as an NLP exercise involving a transformer
    and a dataset containing the ready-to-ask questions and answering those questions.
    The transformer is trained to answer the questions asked in this closed environment.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 问答主要是作为一个涉及变压器和包含准备提问和回答这些问题的数据集的NLP练习。变压器被训练来回答在这个封闭环境中提出的问题。
- en: However, in more complex situations, reliable transformer model implementations
    require customized methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在更复杂的情况下，可靠的变压器模型实现需要定制方法。
- en: Transformers and methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器和方法
- en: A perfect and efficient universal transformer model for question-answering or
    any other NLP task does not exist. The best model for a project is the one that
    produces the best outputs for a specific dataset and task.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 完美和高效的通用变压器模型，无论是用于问答还是其他NLP任务，都不存在。对于一个特定的数据集和任务来说，最适合的模型是产生最佳输出的模型。
- en: '*The method outperforms models* in many cases. For example, a suitable method
    with an average model often will produce more efficient results than a flawed
    method with an excellent model.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*该方法在许多情况下优于模型*。例如，一个适当的方法通常会比一个优秀模型但有缺陷的方法产生更高效的结果。'
- en: In this chapter, we will run `DistilBERT`, `ELECTRA`, and `RoBERTa` models.
    Some produce better *performances* than others.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将运行`DistilBERT`、`ELECTRA`和`RoBERTa`模型。有些比其他模型有更好的*性能*。
- en: However, *performance* does not guarantee a result in a critical domain.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在关键领域中，*性能*并不保证结果。
- en: For example, in a space rocket and spacecraft production project, asking an
    NLP bot a question means obtaining one exact answer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在太空火箭和航天器生产项目中，向NLP机器人提问意味着获得一个确切的答案。
- en: Suppose the user needs to ask a question on a hundred-page report on the status
    of a regeneratively cooled nozzle and combustion chamber of a rocket. The question
    could be specific, such as `Is the cooling status reliable or not?` That is the
    bottom-line information the user wants from the NLP bot.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户需要在一个关于火箭再生式冷却喷嘴和燃烧室状态的一百页报告中提出问题。问题可能很具体，比如`冷却状态可靠吗？`这是用户从NLP机器人那里想要得到的底线信息。
- en: To cut a long story short, letting the NLP bot, transformer model or not, make
    a literal statistical answer with no quality and cognitive control is too risky
    and would not happen. A trustworthy NLP bot would be connected to a knowledge
    base containing data and rules to run a rule-based expert system in the background
    to check the NLP bot’s answer. The NLP transformer model bot would produce a smooth,
    reliable natural language answer, possibly with a human voice.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，让NLP机器人、变压器模型或其他任何东西做一个没有质量和认知控制的统计答案是太冒险了，也不会发生。可信赖的NLP机器人将连接到一个包含数据和规则的知识库，以在后台运行基于规则的专家系统来检查NLP机器人的答案。NLP变压器模型机器人将产生一个流畅、可靠的自然语言答案，可能还是人声。
- en: A universal transformer *model* and *method* that will fit all needs does not
    exist. Each project requires specific functions and a customized approach and
    will vary tremendously depending on the users’ expectations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于所有需求的通用变压器*模型*和*方法*并不存在。每个项目都需要特定的功能和定制的方法，这取决于用户的期望值，变化将是巨大的。
- en: This chapter will focus on the general constraints of question-answering beyond
    a specific transformer model choice. This chapter is not a question-answering
    project guide but an introduction to how transformers can be used for question-answering.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将着眼于超出特定变压器模型选择范围的问答的一般约束。本章不是一个问答项目指南，而是介绍了变压器如何用于问答。
- en: 'We will focus on using question-answering in an open environment where the
    questions were not prepared beforehand. Transformer models require help from other
    NLP tasks and classical programs. We will explore some methods to give an idea
    of how to combine tasks to reach the goal of a project:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于在没有事先准备的情况下使用问答在开放环境中。变压器模型需要其他NLP任务和经典程序的帮助。我们将探索一些方法，以提供如何组合任务以达到项目目标的想法：
- en: '*Method 0* explores a trial and error approach of asking questions randomly.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Method 0* 探索了随机提问的试错方法。'
- en: '*Method 1* introduces NER to help prepare the question-answering tasks.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Method 1* 引入NER来帮助准备问答任务。'
- en: '*Method 2* tries to help the default transformer with an ELECTRA transformer
    model. It also introduces SRL to help the transformer prepare questions.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Method 2* 尝试使用ELECTRA变压器模型来帮助默认变压器。它还引入了SRL来帮助变压器准备问题。'
- en: The introduction to these three methods shows that a single question-answering
    method will not work for high-profile corporate projects. Adding NER and SRL will
    improve the linguistic intelligence of a transformer agent solution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法的介绍表明，单一的问答方法对于高调的公司项目不起作用。添加NER和SRL将提高变压器代理解决方案的语言智能。
- en: For example, in one of my first AI NLP projects implementing question-answering
    for a defense project in a tactical situation for an aerospace corporation, I
    combined different NLP methods to ensure that the answer provided was 100% reliable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我最早的一个AI NLP项目中，为航空航天公司的防御项目实施问答功能，我结合了不同的NLP方法，以确保提供的答案是`100%`可靠的。
- en: You can design a multi-method solution for each project you implement.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以为您实施的每个项目设计一个多方法解决方案。
- en: Let’s start with the trial and error approach.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从试错方法开始。
- en: 'Method 0: Trial and error'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Method 0：试错
- en: Question-answering seems very easy. Is that true? Let’s find out.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 问答似乎非常简单。这是真的吗？让我们来看看。
- en: Open `QA.ipynb`, the Google Colab notebook we will be using in this chapter.
    We will run the notebook cell by cell.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 打开本章将使用的Google Colab笔记本`QA.ipynb`。我们将逐个单元格地运行笔记本。
- en: 'Run the first cell to install Hugging Face’s transformers, the framework we
    will be implementing in this chapter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行第一个单元格来安装Hugging Face的transformers，这是我们将在本章中实施的框架：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note: Hugging Face transformers continually evolve, updating libraries and
    modules to adapt to the market. If the default version doesn’t work, you might
    have to pin one with `!pip install transformers==[version that runs with the other
    functions in the notebook]`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Hugging Face transformers不断发展，更新库和模块以适应市场。如果默认版本不起作用，您可能需要使用`!pip install
    transformers==[与笔记本中的其他函数兼容的版本]`来固定一个版本。
- en: We will now import Hugging Face’s pipeline, which contains many ready-to-use
    transformer resources. They provide high-level abstraction functions for the Hugging
    Face library resources to perform a wide range of tasks. We can access those NLP
    tasks through a simple API. The program was created on Google Colab. It recommended
    to run it on Google Colab VM using a free Gmail account.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将导入Hugging Face的pipeline，其中包含许多即用型变压器资源。它们为Hugging Face库资源提供高级抽象函数，以执行各种任务。我们可以通过一个简单的API访问这些NLP任务。该程序是在Google
    Colab上创建的。建议使用免费的Gmail帐户在Google Colab VM上运行它。
- en: 'The `pipeline` is imported with one line of code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一行代码导入`pipeline`：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once that is done, we have one-line options to instantiate transformer models
    and tasks:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们有一行选项来实例化变压器模型和任务：
- en: 'Perform an NLP task with the default `model` and default `tokenizer`:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认的`model`和默认的`tokenizer`执行一个NLP任务：
- en: '[PRE2]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Perform an NLP task using a custom `model`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义`model`执行一个NLP任务：
- en: '[PRE3]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Perform NLP tasks using a custom `model` and a custom `tokenizer`:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自定义`model`和自定义`tokenizer`执行NLP任务：
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s begin with the default model and tokenizer:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从默认模型和分词器开始：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, all we have to do is provide a text that we will then use to submit questions
    to the transformer:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要提供一段文本，然后我们将使用它来向变压器提交问题：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The sequence is deceptively simple, and all we need to do is plug one line
    of code into the API to ask a question and obtain an answer:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个序列看起来非常简单，我们所需要做的就是将一行代码插入API中来提问并获得答案：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is a perfect answer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个完美的答案：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have just implemented a question-answering transformer NLP task in a few
    lines of code! You could now download a ready-to-use dataset that contains texts,
    questions, and answers.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚用几行代码实现了一个问答变压器NLP任务！您现在可以下载一个包含文本、问题和答案的即用数据集。
- en: 'In fact, the chapter could end right here, and you would be all set for question-answering
    tasks. However, things are never simple in real-life implementations. Suppose
    we have to implement a question-answering transformer model for users to ask questions
    on many documents stored in the database. We have two significant constraints:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，本章可以在此结束，您将准备好进行问答任务了。然而，在实际实施中事情从来都不简单。假设我们必须为用户实现一个问答变换模型，让用户在数据库中存储的许多文档上提问。我们有两个重要的约束：
- en: We first need to run the transformer through a set of key documents and create
    questions that show that the system works
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先需要运行变换器通过一组关键文档，并创建显示系统工作的问题
- en: We must show how we can guarantee that the transformer answers the questions
    correctly
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须展示如何保证变换器正确回答问题
- en: 'Several questions immediately arise:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 几个问题立即出现：
- en: Who is going to find the questions to ask to test the system?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁将找到要询问测试系统的问题？
- en: Even if an expert agrees to do the job, what will happen if many of the questions
    produce erroneous results?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使专家同意做这项工作，如果许多问题产生错误结果会怎么样？
- en: Will we keep training the model if the results are not satisfactory?
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果结果不令人满意，我们是否会继续训练模型？
- en: What happens if some of the questions cannot be answered, no matter which model
    we use or train?
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一些问题无论我们使用或训练哪个模型都无法回答，会发生什么？
- en: What if this works on a limited sample but the process takes too long and cannot
    be scaled up because it costs too much?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果这在有限样本上有效，但过程耗时且无法扩展，因为成本过高，会怎样？
- en: If we just try questions that come to us with an expert’s help and see which
    ones work and which ones don’t, it could take forever. Trial and error is not
    the solution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只是试试专家帮助我们提出的问题，看看哪些有效，哪些无效，那将永远不会完成。试错不是解决方案。
- en: This chapter aims to provide some methods and tools that will reduce the cost
    of implementing a question-answering transformer model. *Finding good questions
    for question-answering is quite a challenge when implementing new datasets for
    a customer*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在提供一些方法和工具，以降低实施问答变换模型的成本。*在为客户实施新数据集时，为问答问题找到好问题是一项相当大的挑战*。
- en: We can think of a transformer as a LEGO^® set of building blocks we can assemble
    as we see fit using encoder-only or decoder-only stacks. We can use a set of small,
    large, or **extra-large** (**XL**) transformer models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将变换器视为我们可以根据需要组装的一套乐高®（LEGO®）堆积积木，使用编码器堆栈或解码器堆栈。我们可以使用一组小型、大型或**超大**（**XL**）变换器模型。
- en: We can also think of the NLP tasks we have explored in this book as a LEGO^®
    set of solutions in a project we must implement. We can assemble two or more NLP
    tasks to reach our goals, just like any other software implementation. We can
    go from a trial and error search for questions to a methodical approach.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将我们在本书中探讨过的 NLP 任务看作是我们必须实施的项目中的解决方案的一套乐高®（LEGO®）集。我们可以组装两个或更多 NLP 任务来达到我们的目标，就像任何其他软件实现一样。我们可以从试错搜索问题转向系统方法。
- en: 'In this chapter:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中：
- en: We will continue to run `QA.ipynb` cell by cell to explore the methods described
    in each section.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将继续逐个单元格运行`QA.ipynb`来探索每个部分描述的方法。
- en: We will also use the `AllenNLP` NER interface to obtain a visual representation
    of the NER and SRL results. You can enter the sentence in the interface by going
    to [https://demo.allennlp.org/reading-comprehension](https://demo.allennlp.org/reading-comprehension),
    then select **Named Entity Recognition** or **Semantic Role Labeling,** and enter
    the sequence. In this chapter, we will take the `AllenNLP` model used into account.
    We just want to obtain visual representations.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将使用`AllenNLP` NER 接口获取 NER 和 SRL 结果的可视化表示。您可以通过访问[https://demo.allennlp.org/reading-comprehension](https://demo.allennlp.org/reading-comprehension)，然后选择**命名实体识别**或**语义角色标注**，并输入序列，在界面中输入句子。在本章中，我们将考虑使用的`AllenNLP`模型。我们只想获得可视化表示。
- en: Let’s start by trying to find the right XL transformer model questions for question-answering
    with a NER-first method.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从尝试使用 NER-first 方法为问题回答找到合适的 XL 变换器模型问题开始。
- en: 'Method 1: NER first'
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法 1：NER 先
- en: This section will use NER to help us find ideas for good questions. Transformer
    models are continuously trained and updated. Also, the datasets used for training
    might change. Finally, these are not rule-based algorithms that produce the same
    result each time. The outputs might change from one run to another. NER can detect
    people, locations, organizations, and other entities in a sequence. We will first
    run a NER task that will give us some of the main parts of the paragraph we can
    focus on to ask questions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将使用NER来帮助我们找到好问题的想法。变压器模型是持续训练和更新的。此外，用于训练的数据集可能会发生变化。最后，这些不是基于规则的算法，每次可能产生相同的结果。输出可能会在一个运行到另一个运行之间发生变化。NER可以检测序列中的人、位置、组织和其他实体。我们首先运行一个NER任务，它将为我们提供段落的一些主要部分，以便我们专注于提问。
- en: Using NER to find questions
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NER寻找问题
- en: 'We will continue to run `QA.ipynb` cell by cell. The program now initializes
    the pipeline with the NER task to perform with the default model and tokenizer:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续逐步运行`QA.ipynb`。现在程序使用NER任务以默认模型和分词器初始化管道：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will continue to use the deceptively simple sequence we ran in the *Method
    0: Trial and error* section of this chapter:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用本章*方法0：试和错误*部分中运行的看似简单的序列：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We run the `nlp_ner` cell in `QA.ipynb`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`QA.ipynb`中运行了`nlp_ner`单元格：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output generates the result of the NLP tasks. The scores were rounded up
    to two decimal places to fit the width of the page:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出生成了自然语言处理任务的结果。分数被四舍五入到两位小数以适应页面的宽度：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The documentation of Hugging Face describes the labels used. In our case, the
    main ones are:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的文档描述了在我们的案例中使用的标签。主要的标签包括：
- en: '`I-PER`, the name of a person'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-PER`，一个人的名字'
- en: '`I-ORG`, the name of an organization'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-ORG`，一个组织名称'
- en: '`I-LOC`, the name of a location'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`，一个位置名称'
- en: The result is correct. Note that `Barstow` was split into three tokens.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是正确的。注意`Barstow`被分成了三个标记。
- en: 'Let’s run the same sequence on `AllenNLP` in the **Named Entity Recognition**
    section ([https://demo.allennlp.org/named-entity-recognition](https://demo.allennlp.org/named-entity-recognition))
    to obtain a visual representation of our sequence:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在**命名实体识别**部分的`AllenNLP`上运行相同的序列（[https://demo.allennlp.org/named-entity-recognition](https://demo.allennlp.org/named-entity-recognition)）以获得我们序列的可视化表示：
- en: '![Text  Description automatically generated](img/B17948_11_01.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![文本说明自动生成](img/B17948_11_01.png)'
- en: 'Figure 11.1: NER'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：NER
- en: We can see that NER has highlighted the key entities we will use to create questions
    for question-answering.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到NER已经突出显示了我们将用于创建问答问题的关键实体。
- en: 'Let’s ask our transformer two types of questions:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们询问我们的变压器两种类型的问题：
- en: Questions related to locations
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与位置相关的问题
- en: Questions related to persons
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与人有关的问题
- en: Let’s begin with location questions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从位置问题开始。
- en: Location entity questions
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置实体问题
- en: '`QA.ipynb` produced nearly `20` entities. The location entities are particularly
    interesting:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`QA.ipynb`生成了近20个实体。位置实体特别有趣：'
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Applying heuristics
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用启发式方法
- en: 'We can apply heuristics, a method, to create questions with the output `QA.ipynb`
    generated:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以应用启发式方法，使用`QA.ipynb`生成的输出创建问题：
- en: Merge the locations back into their original form with a parser
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将位置合并回其原始形式
- en: Apply a template to the locations
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模板应用到位置
- en: 'It is beyond the scope of this book to write classical code for a project.
    We could write a function that would do the work for us, as shown in this pseudocode:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围已经超出了为项目写经典代码的范围。我们可以编写一个能为我们完成工作的函数，如下伪代码所示：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The NER output would become:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: NER的输出将变为：
- en: '`I-LOC`, `Pioneer Boulevard`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`，`Pioneer Boulevard`'
- en: '`I-LOC`, `Los Angeles`'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`，`Los Angeles`'
- en: '`I-LOC`, `LA`'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`，`LA`'
- en: '`I-LOC`, `Barstow`'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`，`Barstow`'
- en: '`I-LOC`, `Las Vegas`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`I-LOC`，`Las Vegas`'
- en: 'We could then generate questions automatically with two templates. For example,
    we could apply a random function. We could write a function that would do the
    job for us, as shown in the following pseudocode:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两个模板自动生成问题。例如，我们可以应用一个随机函数。我们可以编写一个能为我们完成工作的函数，如下伪代码所示：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We would obtain five questions automatically. For example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将自动生成五个问题。例如：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We know that some of these questions cannot be directly answered with the sequence
    we created. But we can also manage that automatically. Suppose the questions were
    created automatically with our method:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，有些问题不能直接用我们创建的序列回答。但我们也可以自动处理这个问题。假设问题是用我们的方法自动生成的：
- en: Enter a sequence
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一个序列
- en: Run NER
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行NER
- en: Create the questions automatically
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动创建问题
- en: 'Let’s suppose the questions were created automatically and let’s run them:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 假设问题是自动生成的，然后让我们运行它们：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output shows that only `Question 1` was answered correctly:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示只有 `问题 1` 被正确回答了：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output displays the `score`, the `start` and `end` position of the answer,
    and the `answer` itself. The `score` of `Question 2` is `0.98` in this run, although
    it wrongly states that `Los Angeles` in `Pioneer Boulevard`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了 `分数`、答案的 `起始` 和 `结束` 位置，以及 `答案` 本身。在这次运行中，`问题 2` 的 `分数` 是 `0.98`，尽管它错误地说明了
    `Pioneer Boulevard` 在 `Los Angeles` 中。
- en: What do we do now?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们该怎么办？
- en: It’s time to control transformers with project management to add quality and
    decision-making functions.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是控制变压器的时间了，通过项目管理来增加质量和决策功能。
- en: Project management
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 项目管理
- en: 'We will examine four examples, among many others, of how to manage the transformer
    and the hard-coded functions that manage it automatically. We will classify these
    four project management examples into four project levels: easy, intermediate,
    difficult, and very difficult. Project management is not in the scope of this
    book, so we will briefly go through these four categories:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查四个例子，其中包括管理变压器和管理它的硬编码函数的方法。我们将这四个项目管理示例分类为四个项目级别：简单、中级、困难和非常困难。项目管理不在本书的范围之内，因此我们将简要介绍这四个类别：
- en: '**An easy project** could be a website for an elementary school. A teacher
    might be delighted by what we have seen. The text could be displayed on an HTML
    page. The five answers to the questions we obtained automatically could be merged
    with some development into five assertions in a fixed format: `I-LOC is in I-LOC`
    (for example, `Barstow is in California`). We then add `(True, False)` under each
    assertion. All the teacher would have to do would be to have an administrator
    interface that allows the teacher to click on the right answers to finalize a
    multiple-choice questionnaire!'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个简单的项目** 可以是一个小学的网站。老师可能会对我们所见到的感到高兴。文本可以显示在 HTML 页面上。我们自动获得的五个问题的答案可以与一些开发合并为五个断言，并以固定格式显示：`I-LOC
    在 I-LOC 中`（例如，`Barstow 在 California 中`）。然后在每个断言下添加 `(True, False)`。老师所需要做的就是有一个管理员界面，允许老师点击正确答案来完成多项选择问卷！'
- en: '**An intermediate project** could be to encapsulate the transformer’s automatic
    questions and answers in a program that uses an API to check the answers and correct
    them automatically. The user would see nothing. The process is seamless. The wrong
    answers the transformer made would be stored for further analysis.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个中级项目** 可以是封装变压器的自动问题和答案，使用 API 检查答案并自动纠正的程序。用户看不到任何东西。这个过程是无缝的。变压器产生的错误答案将被存储以供进一步分析。'
- en: '**A difficult project** would be implementing an intermediate project in a
    chatbot with follow-up questions. For example, the transformer correctly places
    `Pioneer Boulevard` in `Los Angeles`. A chatbot user might ask a natural follow-up
    question such as `near where in LA?`. This requires more development.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个困难的项目** 将是在具有后续问题的聊天机器人中实现一个中级项目。例如，变压器将 `Pioneer Boulevard` 正确放置在 `Los
    Angeles` 中。聊天机器人用户可能会问一个自然的后续问题，比如 `在 LA 的哪里附近？` 这需要更多的开发。'
- en: '**A very difficult project** would be a research project that would train the
    transformer to recognize `I-LOC` entities over millions of records in datasets
    and output results of real-time streaming of map software APIs.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一个非常困难的项目** 将是一个研究项目，培训变压器识别数据集中数百万记录中的 `I-LOC` 实体，并输出地图软件 API 的实时流结果。'
- en: The good news is that we can also find a way to use what we find.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是我们也可以找到一种利用我们所发现的方法。
- en: The bad news is that implemented transformers or any AI in real-life projects
    require powerful machines and a tremendous amount of teamwork between project
    managers, **Subject Matter Experts** (**SMEs**), developers, and end users.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 坏消息是，实现的变压器或任何现实项目中的 AI 需要强大的机器和项目经理、**主题专家**（**SMEs**）、开发人员和最终用户之间大量的团队合作。
- en: Let’s now try person entity questions.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试人物实体问题。
- en: Person entity questions
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人物实体问题
- en: 'Let’s start with an easy question for the transformer:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从变压器的一个简单问题开始：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The answer is correct. It states who in the sequence was singing:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是正确的。它说明了序列中谁在唱歌：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will now ask the transformer a question that requires some thinking because
    it is not clearly stated:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将向变压器提出一个需要一些思考的问题，因为它没有明确陈述：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It is impossible to answer that question without taking the sentence apart.
    The transformer makes a big mistake:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The transformer is honest enough to display a `score` of only `0.35`. This `score`
    might vary from one calculation to another or from one transformer model to another.
    We can see that the transformer faced a semantic labeling problem. Let’s try to
    do better with person entity questions applying an SRL-first method.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: SRL first'
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The transformer could not find who was driving to go to `Las Vegas` and thought
    it was from `Nat King Cole` instead of `Jo` and `Maria`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: What went wrong? Can we see what the transformers think and obtain an explanation?
    To find out, let’s go back to semantic role modeling. If necessary, take a few
    minutes to review *Chapter 10*, *Semantic Role Labeling with BERT-Based Transformers*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run the same sequence on `AllenNLP` in the **Semantic Role Labeling**
    section, [https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling),
    to obtain a visual representation of the verb `drove` in our sequence by running
    the SRL BERT model we used in the previous chapter:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_11_02.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: SRL run on the text'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: SRL BERT found 19 frames. In this section, we focus on `drove`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The results may vary from one run to another or when AllenNLP updates
    the model versions.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: We can see the problem. The argument of the verb `drove` is `Jo and Maria`.
    It seems that the inference could be made.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that transformer models keep evolving. The output might vary; however,
    the concepts remain the same.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Is that true? Let’s ask the question in `QA.ipynb`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is correct:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Could we find a way to ask the question to obtain the right answer? We will
    try by paraphrasing the question:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We obtain a somewhat better result:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The transformer now understands that `Nat King Cole` was `singing` and that
    `Jo and Maria` were doing something in the meantime.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: We still need to go further and find a way to ask better questions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try another model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Question-answering with ELECTRA
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before switching models, we need to know which one we are using:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output first shows that the model is a DistilBERT model trained on question-answering:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The model has `6` layers and `768` features, as shown in layer `6` (the layers
    are numbered from `0` to `n`):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We will now try the `ELECTRA` transformer model. *Clark* et al. (2020) designed
    a transformer model that improved the **Masked Language Modeling** (**MLM**) pretraining
    method.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Fine-Tuning BERT Models*, in the *Masked language modeling*
    subsection, we saw that the BERT model inserts random masked tokens with `[MASK]`
    during the training process.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '*Clark* et al. (2020) introduced plausible alternatives with a generator network
    rather than simply using random tokens. BERT models are trained to predict the
    identities of the (masked) corrupted tokens. *Clark* et al. (2020) trained an
    ELECTRA model as a discriminator to predict whether the masked token was a generated
    token or not. *Figure 11.3* shows how ELECTRA is trained:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*Clark*等人（2020）提出了一种可行的替代方案，使用了生成器网络而不仅仅是使用随机令牌。BERT模型被训练以预测（屏蔽的）损坏令牌的标识。*Clark*等人（2020）训练了一个ELECTRA模型作为鉴别器，以预测屏蔽的令牌是否是生成的令牌。*图11.3*显示了ELECTRA的训练方式：'
- en: '![Diagram  Description automatically generated](img/B17948_11_03.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示描述](img/B17948_11_03.png)'
- en: 'Figure 11.3: ELECTRA is trained as a discriminator'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：ELECTRA作为鉴别器进行训练
- en: '*Figure 11.3* shows that the original sequence is masked before going through
    the generator. The generator inserts *acceptable* tokens and not random tokens.
    The ELECTRA transformer model is trained to predict whether a token comes from
    the original sequence or has been replaced.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.3*显示，在送入生成器之前，原始序列会被屏蔽。生成器插入*可接受*的令牌而不是随机的令牌。ELECTRA变压器模型训练以预测一个令牌是否来自原始序列还是已被替换。'
- en: The architecture of an ELECTRA transformer model and most of its hyperparameters
    are the same as BERT transformer models.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ELECTRA变压器模型的架构和大多数超参数与BERT变压器模型相同。
- en: 'We now want to see if we can obtain a better result. The next cell to run in
    `QA.ipynb` is the question-answering cell with an `ELECTRA-small-generator`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在希望看看是否能获得更好的结果。在*QA.ipynb*中要运行的下一个单元是使用*ELECTRA-small-generator*的问答单元：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is not what we expect:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出不是我们所期望的：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output might change from one run or transformer model to another; however,
    the idea remains the same.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 输出可能会从一个运行或变压器模型到另一个变化；然而，想法仍然是一样的。
- en: 'The output also sends training messages:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出也发送了训练消息：
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You might not like these warning messages and might conclude that this is a
    bad model. But always explore every avenue that is offered to you. ELECTRA might
    require more training, of course. But *experiment* as much as possible to find
    new ideas! Then you can decide to train a model further or move on to another
    one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能不喜欢这些警告消息，并可能得出这是一个糟糕的模型的结论。但一定要尽可能地探索给你提供的每一个途径。当然，ELECTRA可能需要更多的训练。但*尝试*尽可能多地找到新的想法!
    然后你可以决定是否进一步训练模型或转移到另一个模型。
- en: We must now think of the next steps to take.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须考虑下一步要采取的步骤。
- en: Project management constraints
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目管理约束
- en: We have not obtained the results we expected with the default DistilBERT and
    ELECTRA transformer models.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有获得我们所期望的默认DistilBERT和ELECTRA变压器模型的结果。
- en: 'There are three main options among other solutions:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他解决方案中，有三个主要选项：
- en: Train DistilBERT and ELECTRA or other models with additional datasets. Training
    datasets is a costly process in real-life projects. The training could last months
    if new datasets need to be implemented and hyperparameters changed. The hardware
    cost needs to be taken into account as well. Furthermore, if the result is unsatisfactory,
    a project manager might shut the project down.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用额外的数据集训练DistilBERT和ELECTRA或其他模型。在现实项目中，训练数据集是一个昂贵的过程。如果需要实施新的数据集并改变超参数，训练可能会持续几个月。硬件成本也需要考虑在内。此外，如果结果不尽人意，项目经理可能会关闭项目。
- en: 'You can also try ready-to-use transformers, although they might not fit your
    needs, such as the Hugging Face model: [https://huggingface.co/transformers/usage.html#extractive-question-answering](https://huggingface.co/transformers/usage.html#extractive-question-answering).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你也可以尝试使用现成的变压器，尽管它们可能不符合你的需求，比如Hugging Face模型：[https://huggingface.co/transformers/usage.html#extractive-question-answering](https://huggingface.co/transformers/usage.html#extractive-question-answering)。
- en: Find a way to obtain better results by using additional NLP tasks to help the
    question-answering model.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用额外的NLP任务来帮助问答模型，找到获得更好结果的方法。
- en: In this chapter, we will focus on finding additional NLP tasks to help the default
    DistilBERT model.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注寻找额外的NLP任务，以帮助默认的DistilBERT模型。
- en: Let’s use SRL to extract the predicates and their arguments.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用SRL来提取谓词及其论证。
- en: Using SRL to find questions
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SRL来找到问题
- en: AllenNLP uses the BERT-based model we implemented in the `SRL.ipynb` notebook
    in *Chapter 10*, *Semantic Role Labeling with BERT-Based Transformers*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: AllenNLP使用我们在*SRL.ipynb*笔记本中实现的基于BERT的模型，*第10章*，*使用基于BERT的变压器进行语义角色标注*。
- en: Let’s rerun the sequence on AllenNLP in the **Semantic Role Labeling** section,
    [https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling),
    to obtain a visual representation of the predicates in the sequence.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在**语义角色标注**部分重新运行AllenNLP上的序列，[https://demo.allennlp.org/semantic-role-labeling](https://demo.allennlp.org/semantic-role-labeling)，以获取句子中谓词的可视化表示。
- en: 'We will enter the sequence we have been working on:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进入我们一直在处理的序列：
- en: '[PRE33]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The BERT-based model found several predicates. Our goal is to find the properties
    of SRL outputs that could automatically generate questions based on the verbs
    in a sentence.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BERT模型找到了几个谓词。我们的目标是找到SRL输出的特性，可以根据句子中的动词自动生成问题。
- en: 'We will first list the predicate candidates produced by the BERT model:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先列出BERT模型产生的谓词候选者：
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If we had to write a program, we could start by introducing a verb counter,
    as shown in the following pseudocode:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们必须编写一个程序，我们可以先引入一个动词计数器，如下面的伪代码所示：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: If the counter exceeds the number of acceptable occurrences (`max_count`), the
    verb will be excluded in this experiment. Without further development, it will
    be too difficult to disambiguate multiple semantic roles of the verb’s arguments.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果计数器超出可接受发生的次数（`max_count`），则该动词将在此实验中被排除。如果没有进一步的开发，要消除动词参数的多重语义角色将会太困难。
- en: Let’s take `made`, which is the past tense of `make`, out of the list as well.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也把`made`（`make`的过去式）从列表中移除。
- en: 'Our list is now limited to:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列表现在被限制在：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we continued to write a function to filter the verbs, we could look for
    verbs with lengthy arguments. The verb `began` has a very long argument:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续编写一个函数来过滤动词，我们可以寻找参数很长的动词。动词`began`有一个非常长的参数：
- en: '![Graphical user interface  Description automatically generated with medium
    confidence](img/B17948_11_04.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  描述自动生成并具有中等信心](img/B17948_11_04.png)'
- en: 'Figure 11.4: SRL applied to the verb “began”'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：SRL应用于动词“began”
- en: 'The argument of `began` is so long it doesn’t fit in the screenshot. The text
    version shows how difficult it would be to interpret the argument of `began`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`began`的参数太长，无法在截图中显示。文本版本显示了解释`began`参数有多困难：'
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We could add a function to filter verbs that contain arguments that exceed
    a maximum length:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加一个函数来过滤包含超出最大长度的参数的动词：
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If the length of one verb’s arguments exceeds a maximum length (`max_length`),
    the verb will be excluded in this experiment. For the moment, let’s just take
    `began` out of the list:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个动词的参数长度超过了最大长度（`max_length`），则该动词将在此实验中被排除。暂时让我们把`began`从列表中移出：
- en: 'Our list is now limited to:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的列表现在被限制在：
- en: '[PRE39]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We could add more exclusion rules depending on the project we are working on.
    We can also call the `maxlength` function again with a very restrictive `max_length`
    value to extract potentially interesting candidates for our automatic question
    generator. The verb candidates with the shortest arguments could be transformed
    into questions. The verb `slow` fits the three rules we set: it appears only once
    in the sequence, the arguments are not too long, and it contains some of the shortest
    arguments in the sequence. The AllenNLP visual representation confirms our choice:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据我们正在处理的项目添加更多排除规则。我们还可以再次使用非常严格的`max_length`值调用`maxlength`函数，以提取可能对我们的自动问题生成器感兴趣的候选动词。参数最短的动词候选者可以转换为问题。动词`slow`符合我们设置的三条规则：它在序列中只出现一次，参数不太长，并且包含序列中一些最短的参数。AllenNLP的可视化表示确认了我们的选择：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B17948_11_05.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![一台计算机的截图  描述自动生成并具有中等信心](img/B17948_11_05.png)'
- en: 'Figure 11.5: SRL applied to the verb “slow”'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：SRL应用于动词“slow”
- en: 'The text output can be easily parsed:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 文本输出可以很容易地被解析：
- en: '[PRE40]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This result and the following outputs may vary with the ever-evolving transformer
    models, but the idea remains the same. The verb `slow` is identified and this
    is the key aspect of this SRL output.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果和接下来的输出可能会随着不断发展的转换模型而有所变化，但是想法仍然是一样的。动词`slow`被识别出来，而这是SRL输出的关键方面。
- en: 'We could automatically generate the `what` template. We will not generate a
    `who` template because none of the arguments were labeled `I-PER` (person). We
    could write a function that manages these two possibilities, as shown in the following
    pseudocode:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以自动生成`what`模板。我们不会生成`who`模板，因为没有一个参数标记为`I-PER`（人物）。我们可以编写一个函数来管理这两种可能性，如下面的伪代码所示：
- en: '[PRE41]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This function would require more work to deal with verb forms and modifiers.
    However, in this experiment, we will just apply the function and generate the
    following question:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数需要更多的工作来处理动词形式和修饰词。然而，在这个实验中，我们将只应用该函数并生成以下问题：
- en: '[PRE42]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s run the default `pipeline` with the following cell:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用下面的单元格来运行默认的`pipeline`：
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The result is satisfactory:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是令人满意的：
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The default model, in this case, `DistilBERT`, correctly answered the question.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 默认模型，在这种情况下是`DistilBERT`，正确回答了问题。
- en: 'Our automatic question generator can do the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自动问答生成器可以做到以下几点：
- en: Run NER automatically
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动运行NER
- en: Parse the results with classical code
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用经典代码解析结果
- en: Generate entity-only questions
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成仅实体问题
- en: Run SRL automatically
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动运行SRL
- en: Filter the results with rules
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用规则过滤结果
- en: Generate SRL-only questions using the NER results to determine which template
    to use
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NER结果生成仅SRL问题，以确定使用哪个模板
- en: This solution is by no means complete. More work needs to be done and probably
    requires additional NLP tasks and code. However, it gives an idea of the hard
    work implementing AI, in any form, requires.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案绝对并不完整。还需要更多的工作，可能需要额外的自然语言处理任务和代码。然而，这给出了实现任何形式的AI所需的艰苦工作的一个概念。
- en: 'Let’s try our approach with the next filter verb: `playing`. The visual representation
    shows that the arguments are `WBGO` and `some cool jazz`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试我们的方法与下一个过滤动词：`playing`。可视化表示显示参数是`WBGO`和`一些很酷的爵士乐`：
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_11_06.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序自动生成描述](img/B17948_11_06.png)'
- en: 'Figure 11.6: SRL applied to the verb “playing”'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：应用于动词“playing”的SRL
- en: 'The text version is easy to parse:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 文本版本易于解析：
- en: '[PRE45]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This result and the following outputs may vary with the ever-evolving transformer
    models, but the idea remains the same: identifying the verb and its arguments.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果和以下输出可能会随着不断发展的变压器模型而有所不同，但思想仍然是一样的：识别动词及其参数。
- en: 'If we ran the `whowhat` function, it would show that there is no `I-PER` in
    the arguments. The template chosen will be the `what` template, and the following
    question could be generated automatically:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行`whowhat`函数，它会显示参数中没有`I-PER`。所选模板将是`what`模板，并且以下问题可能会自动生成：
- en: '[PRE46]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s run the default pipeline with this question in the following cell:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下单元格中使用这个问题来运行默认的pipeline：
- en: '[PRE47]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is also satisfactory:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 输出也是令人满意的：
- en: '[PRE48]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`singing` is a good candidate, and the `whowhat` function would find the `I-PER`
    template and automatically generate the following question:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`唱歌`是一个很好的选择，`whowhat`函数会找到`I-PER`模板并自动生成以下问题：'
- en: '[PRE49]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We have already successfully tested this question in this chapter.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在本章中成功测试了这个问题。
- en: The next verb is `drove`, which we have already tagged as a problem. The transformer
    cannot solve this problem.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个动词是`drove`，我们已经标记为一个问题。变压器无法解决这个问题。
- en: 'The verb `go` is a good candidate:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 动词`go`是一个很好的选择：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_11_07.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序自动生成描述](img/B17948_11_07.png)'
- en: 'Figure 11.7: SRL applied to the verb “go”'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：应用于动词“go”的SRL
- en: 'It would take additional development to produce a template with the correct
    verb form. Let’s suppose the work was done and ask the model the following question:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 需要进一步开发才能生成带有正确动词形式的模板。假设工作已完成并问模型以下问题：
- en: '[PRE50]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is the wrong argument:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是错误的参数：
- en: '[PRE51]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We can see that the presence of `Nat King Cole` and `Jo` and `Maria` in the
    same sequence in a complex sequence creates disambiguation problems for transformer
    models and any NLP model. More project management and research would be required.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`Nat King Cole`、`Jo`和`Maria`在一个复杂序列中的出现会为变压器模型和任何NLP模型造成歧义问题。需要更多的项目管理和研究。
- en: Next steps
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: There is no easy way to implement question-answering or shortcuts. We began
    to implement methods that could generate questions automatically. Automatic question
    generation is a critical aspect of NLP.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 实现问答或捷径的方法并不容易。我们开始实施可以自动生成问题的方法。自动生成问题是NLP的一个关键方面。
- en: More transformer models need to be pretrained with multi-task datasets containing
    NER, SRL, and question-answering problems to solve. Project managers also need
    to learn how to combine several NLP tasks to help solve a specific task, such
    as question-answering.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 需要对更多 Transformer 模型进行预训练，包含命名实体识别（NER）、语义角色标注（SRL）和问答问题，以解决问题。项目经理还需要学习如何组合几个
    NLP 任务来帮助解决特定任务，例如问答。
- en: 'Coreference resolution, [https://demo.allennlp.org/coreference-resolution](https://demo.allennlp.org/coreference-resolution),
    could have helped our model identify the main subjects in the sequence we worked
    on. This result produced with `AllenNLP` shows an interesting analysis:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 指代消解，[https://demo.allennlp.org/coreference-resolution](https://demo.allennlp.org/coreference-resolution)，可以帮助我们的模型识别我们工作的序列中的主要主语。这个由
    `AllenNLP` 生成的结果显示了一个有趣的分析：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B17948_11_08.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面、文本、应用程序 自动生成的描述](img/B17948_11_08.png)'
- en: 'Figure 11.8: Coreference resolution of a sequence'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：序列的指代消解
- en: 'We could continue to develop our program by adding the output of coreference
    resolution:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加指代消解的输出来继续开发我们的程序：
- en: '[PRE52]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We could add coreference resolution as a pretraining task or add it as a post-processing
    task in the question generator. In any case, question generators that simulate
    human behavior can considerably enhance the performance of question-answering
    tasks. We will include more customized additional NLP tasks in the pretraining
    process of question-answering models.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将指代消解添加为一个预训练任务，或者将其作为问题生成器的后处理任务。无论哪种方式，模拟人类行为的问题生成器都可以显著提升问答任务的性能。我们将在问题回答模型的预训练过程中包含更多定制的额外
    NLP 任务。
- en: 'Of course, we can decide to use new strategies to pretrain the models we ran
    in this chapter, such as DistilBERT and ELECTRA, and then let users ask the questions
    they wish. I recommend both approaches:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以决定使用新的策略来预训练我们在本章中运行的模型，例如 DistilBERT 和 ELECTRA，然后让用户提出他们希望的问题。我推荐两种方法都使用：
- en: Work on question generators for question-answering tasks. These questions can
    be used for educational purposes, train transformers, or even provide ideas for
    real-time users.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为问答任务开发问题生成器。这些问题可以用于教育目的，训练 Transformer，甚至为实时用户提供思路。
- en: Work on pretraining transformer models by including specific NLP tasks, which
    will improve their question-answering performance. Use the question generator
    to train it further.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过包含特定的 NLP 任务来对 Transformer 模型进行预训练，这将提高它们的问答性能。使用问题生成器进一步训练它。
- en: Exploring Haystack with a RoBERTa model
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RoBERTa 模型探索 Haystack
- en: '`Haystack` is a question-answering framework with interesting functionality.
    It is worth exploring to see if it might fit your needs for a given project.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`Haystack` 是一个具有有趣功能的问答框架。值得探索一下，看看它是否适合您的项目需求。'
- en: In this section, we will run question-answering on the sentence we experimented
    with using other models and methods in this chapter.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对我们在本章中使用其他模型和方法进行实验的句子进行问答。
- en: Open `Haystack_QA_Pipeline.ipynb`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 `Haystack_QA_Pipeline.ipynb`。
- en: 'The first cell installs the modules necessary to run `Haystack`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个单元格安装运行 `Haystack` 所需的模块：
- en: '[PRE53]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The notebook uses a RoBERTa model:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这个笔记本使用了一个 RoBERTa 模型：
- en: '[PRE54]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: You can go back to *Chapter 4*, *Pretraining a RoBERTa Model from Scratch*,
    for a general description of a RoBERTa model.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以回到*第四章*，*从头开始预训练 RoBERTa 模型*，了解 RoBERTa 模型的一般描述。
- en: 'The remaining cells of the notebook will answer questions on the text we have
    been exploring in detail in this chapter:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的其余部分将回答我们在本章中详细探讨的文本的问题：
- en: '[PRE55]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: You can compare the answers obtained with the previous sections’ outputs and
    decide which transformer model you would like to implement.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以比较之前章节的输出得到的答案，并决定您想要实现哪种 Transformer 模型。
- en: Exploring Q&A with a GTP-3 engine
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GTP-3 引擎探索问答
- en: This section will try to avoid training, fine-tuning, loading a program on a
    server, or even using a dataset. Instead, a user can simply connect to their OpenAI
    account and use the interactive educational interface.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将尝试避免训练、微调、在服务器上加载程序，甚至使用数据集。相反，用户只需连接到他们的 OpenAI 账户并使用交互式教育界面即可。
- en: 'A GPT-3 engine online educational interface will provide sufficiently good
    answers by providing *E* (explanation) and *T* (text) as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 GPT-3 引擎在线教育界面将通过提供 *E*（解释）和 *T*（文本）来提供足够好的答案，如下所示：
- en: '*E* = Answer questions from this text'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*E* = 回答这段文本的问题'
- en: '*T* = `The traffic began to slow down on Pioneer Boulevard in…/… have a nice
    dinner and go see a show.`'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some questions asked and answers obtained in the form of question-answer:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '`Who is going to Las Vegas?`: `Jo and Maria`'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Who was singing?`: `Nat King Cole`'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What kind of music was playing?`: `jazz`'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`What was the plan for the evening?`: `to have a nice dinner and go see a show`'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it! That’s all you need to do to run a wide range of educational NLP
    tasks online with an interactive interface even without an API with GPT-3 engines.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change *S* (showing GPT-3 what is expected) and *E* and create endless
    interactions. The next generation of NLP is born! An Industry 4.0 developer, consultant,
    or project manager will need to acquire a new set of skills: cognitive approaches,
    linguistics, psychology, and other cross-disciplinary dimensions. If necessary,
    you can take your time and go back to *Chapter 7*, *The Rise of Suprahuman Transformers
    with GPT-3 Engines*.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: We have explored some critical aspects of the use of question-answering with
    transformers. Let’s sum up the work we have done.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we found that question-answering isn’t as easy as it seems.
    Implementing a transformer model only takes a few minutes. However, getting it
    to work can take a few hours or several months!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: We first asked the default transformer in the Hugging Face pipeline to answer
    some simple questions. `DistilBERT`, the default transformer, answered the simple
    questions quite well. However, we chose easy questions. In real life, users ask
    all kinds of questions. The transformer can get confused and produce erroneous
    output.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: We then decided to continue to ask random questions and get random answers,
    or we could begin to design the blueprint of a question generator, which is a
    more productive solution.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: We started by using NER to find useful content. We designed a function that
    could automatically create questions based on NER output. The quality was promising
    but required more work.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: We tried an ELECTRA model that did not produce the results we expected. We stopped
    for a few minutes to decide if we would spend costly resources to train transformer
    models or design a question generator.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: We added SRL to the blueprint of the question generator and tested the questions
    it could produce. We also added NER to the analysis and generated several meaningful
    questions. The `Haystack` framework was also introduced to discover other ways
    of addressing question-answering with RoBERTa.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we ran an example using a GPT-3 engine directly in the OpenAI educational
    interactive interface without an API. Cloud AI platforms are increasing in power
    and accessibility.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'Our experiments led to one conclusion: multi-task transformers will provide
    better performance on complex NLP tasks than a transformer trained on a specific
    task. Implementing transformers requires well-prepared multi-task training, heuristics
    in classical code, and a question generator. The question generator can be used
    to train the model further by using the questions as training input data or as
    a standalone solution.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Detecting Customer Emotions to Make Predictions*, we will
    explore how to implement sentiment analysis on social media feedback.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A trained transformer model can answer any question. (True/False)
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Question-answering requires no further research. It is perfect as it is. (True/False)
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Named Entity Recognition** (**NER**) can provide useful information when
    looking for meaningful questions. (True/False)'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Semantic Role Labeling** (**SRL**) is useless when preparing questions. (True/False)'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A question generator is an excellent way to produce questions. (True/False)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing question answering requires careful project management. (True/False)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ELECTRA models have the same architecture as GPT-2\. (True/False)
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ELECTRA models have the same architecture as BERT but are trained as discriminators.
    (True/False)
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NER can recognize a location and label it as `I-LOC`. (True/False)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NER can recognize a person and label that person as `I-PER`. (True/False)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The Allen Institute* for AI: [https://allennlp.org/](https://allennlp.org/)'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Allen Institute* for reading comprehension resources: [https://demo.allennlp.org/reading-comprehension](https://demo.allennlp.org/reading-comprehension)'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kevin Clark*, *Minh-Thang Luong*, *Quoc V. Le*, *Christopher D. Manning*,
    2020, *ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators*:
    [https://arxiv.org/abs/2003.10555](https://arxiv.org/abs/2003.10555)'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face pipelines: [https://huggingface.co/transformers/main_classes/pipelines.html](https://huggingface.co/transformers/main_classes/pipelines.html)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub Haystack framework repository: [https://github.com/deepset-ai/haystack/](https://github.com/deepset-ai/haystack/)'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
