["```py\ndetector = dlib.get_frontal_face_detector() \npredictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\nimage = cv2.imread('nicolas_ref.png')\n# convert to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) \nfaces = detector(gray)\n# identify and mark features\nfor face in faces:  \n    x1 = face.left() \n    y1 = face.top() \n    x2 = face.right() \n    y2 = face.bottom() \n    landmarks = predictor(gray, face) \n    for n in range(0, 68): \n        x = landmarks.part(n).x \n        y = landmarks.part(n).y \n        cv2.circle(image, (x, y), 2, (255, 0, 0), -1) \n```", "```py\ndef detect_faces(image, face_list):\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    ax = plt.gca()\n    for face in face_list:\n        # mark faces\n        x, y, width, height = face['box']\n        rect = Rectangle((x, y), width, height, fill=False, \n                                                color='orange')\n        ax.add_patch(rect)\n        # mark landmark features\n        for key, value in face['keypoints'].items():\n            dot = Circle(value, radius=12, color='red')\n            ax.add_patch(dot)\n    plt.show()\n# instantiate the detector\ndetector = MTCNN()\n# load sample image\nimage = cv2.imread('trump_ref.png')\n# detect face and facial landmarks\nfaces = detector.detect_faces(image)\n# visualize results\ndetect_faces(image, faces) \n```", "```py\n# Entity class\nclass DetectedFace(object):\n    def __init__(self, image, x, w, y, h, landmarks):\n        self.image = image\n        self.x = x\n        self.w = w\n        self.y = y\n        self.h = h\n        self.landmarks = landmarks\n    def landmarksAsXY(self):\n        return [(p.x, p.y) for p in self.landmarks.parts()] \n```", "```py\npredictor_68_point_model = face_recognition_models.pose_predictor_model_location()\npose_predictor = dlib.shape_predictor(predictor_68_point_model) \ndetect_faces utility method:\n```", "```py\ndef detect_faces(frame):\n    face_locations = face_recognition.face_locations(frame)\n    landmarks = _raw_face_landmarks(frame, face_locations)\n    for ((y, right, bottom, x), landmarks) in zip(face_locations, landmarks):\n        yield DetectedFace(frame[y: bottom, x: right], \n                           x, right - x, y, bottom - y, landmarks) \n```", "```py\ndetect_faces function to extract all the faces in the input image:\n```", "```py\n# Load Sample Image\nimage = cv2.imread('sample_image.jpg')\nplt.imshow(cv2.cvtColor(image , cv2.COLOR_BGR2RGB))\nplt.axis('off');\n# Detect faces and visualize them all\ndetected_faces = [face for face in detect_faces(image)] \n```", "```py\nfor face in detected_faces:\n  plt.imshow(cv2.cvtColor(face.image , cv2.COLOR_BGR2RGB))\n  plt.axis('off');\n  plt.show() \n```", "```py\nFaceFilter, which helps us to do so:\n```", "```py\nclass FaceFilter():\n    def __init__(self, reference_file_path, threshold=0.65):\n        \"\"\"\n        Works only for single face images\n        \"\"\"\n        image = face_recognition.load_image_file(reference_file_path)\n        self.encoding = face_recognition.face_encodings(image)[0]\n        self.threshold = threshold\n    def check(self, detected_face):\n        encodings = face_recognition.face_encodings(detected_face.image)\n        if len(encodings) > 0:\n            encodings = encodings[0]\n            score = face_recognition.face_distance([self.encoding],                                                    encodings)\n        else:\n            print(\"No faces found in the image!\")\n            score = 0.8\n        return score <= self.threshold \n```", "```py\nFaceFilter using a reference image. We then iterate through the list of detected_faces to see which faces actually belong to Donald Trump:\n```", "```py\nface_filter = FaceFilter('trump_ref.png')\nfor face in detected_faces:\n  if face_filter.check(face):\n    plt.title(\"Matching Face\")\n    plt.imshow(cv2.cvtColor(face.image , cv2.COLOR_BGR2RGB))\n    plt.axis('off');\n    plt.show() \n```", "```py\nExtract class, which takes in the extracted face as input and generates an aligned output; in other words, we align the orientation of the cropped/extracted face with that of the reference image:\n```", "```py\nclass Extract(object):\n    def extract(self, image, face, size):\n        if face.landmarks is None:\n            print(\"Warning! landmarks not found. Switching to crop!\")\n            return cv2.resize(face.image, (size, size))\n        alignment = get_align_mat(face)\n        return self.transform(image, alignment, size, padding=48)\n    def transform(self, image, mat, size, padding=0):\n        mat = mat * (size - 2 * padding)\n        mat[:, 2] += padding\n        return cv2.warpAffine(image, mat, (size, size)) \n```", "```py\ndef get_faces(reference_image,image,extractor,debug=False):\n    faces_count = 0\n    facefilter = FaceFilter(reference_image)\n    for face in detect_faces(image):\n        if not facefilter.check(face):\n            print('Skipping not recognized face!')\n            continue\n        resized_image = extractor.extract(image, face, 256)\n        if debug:\n          imgplot = plt.imshow(cv2.cvtColor(resized_image,                                             cv2.COLOR_BGR2RGB))\n          plt.show()\n        yield faces_count, face\n        faces_count +=1 \n```", "```py\ndef create_face_dataset(reference_face_filepath,\n                        input_dir,\n                        output_dir,\n                        extractor,\n                        included_extensions=included_extensions):\n  image_list = [fn for fn in glob.glob(input_dir+\"/*.*\") \\\n              if any(fn.endswith(ext) for ext in included_extensions)]\n  print(\"Total Images to Scan={}\".format(len(image_list)))\n  positive_ctr = 0\n  try:\n    for filename in image_list:\n        image = cv2.imread(filename)\n        for idx, face in get_faces(reference_face_filepath,image,extractor):\n            resized_image = extractor.extract(image, face, 256)\n            output_file = output_dir+\"/\"+str(filename).split(\"/\")[-1]\n            cv2.imwrite(output_file, resized_image)\n            positive_ctr += 1\n  except Exception as e:\n      print('Failed to extract from image: {}. Reason: {}'.format(filename, e))\n  print(\"Images with reference face={}\".format(positive_ctr)) \n```", "```py\ndef conv(x, filters):\n    x = Conv2D(filters, kernel_size=5, strides=2, padding='same')(x)\n    x = LeakyReLU(0.1)(x)\n    return x \n```", "```py\ndef upscale(x, filters):\n    x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)\n    x = LeakyReLU(0.1)(x)\n    x = UpSampling2D()(x)\n    return x \n```", "```py\ndef Encoder(input_shape, encoder_dim):\n    input_ = Input(shape=input_shape)\n    x = input_\n    x = conv(x, 128)\n    x = conv(x, 256)\n    x = conv(x, 512)\n    x = conv(x, 1024)\n    x = Dense(encoder_dim)(Flatten()(x))\n    x = Dense(4 * 4 * 1024)(x)\n    # Passed flattened X input into 2 dense layers, 1024 and 1024*4*4\n    x = Reshape((4, 4, 1024))(x)\n    # Reshapes X into 4,4,1024\n    x = upscale(x, 128)\n    return Model(input_, x) \n```", "```py\ndef Decoder(input_shape=(8, 8, 512)):\n    input_ = Input(shape=input_shape)\n    x = input_\n    x = upscale(x, 256)\n    x = upscale(x, 128)\n    x = upscale(x, 64)\n    x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)\n    return Model(input_, x) \n```", "```py\nENCODER_DIM = 1024\nIMAGE_SHAPE = (64, 64, 3)\nencoder = Encoder(IMAGE_SHAPE,ENCODER_DIM)\ndecoder_A = Decoder()\ndecoder_B = Decoder()\noptimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999) #orig adam 5e-5\nx = Input(shape=IMAGE_SHAPE)\nautoencoder_A = Model(x, decoder_A(encoder(x)))\nautoencoder_B = Model(x, decoder_B(encoder(x)))\nautoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')\nautoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error') \n```", "```py\ndef random_transform(image,\n                     rotation_range,\n                     zoom_range,\n                     shift_range,\n                     random_flip):\n    h, w = image.shape[0:2]\n    rotation = np.random.uniform(-rotation_range, rotation_range)\n    scale = np.random.uniform(1 - zoom_range, 1 + zoom_range)\n    tx = np.random.uniform(-shift_range, shift_range) * w\n    ty = np.random.uniform(-shift_range, shift_range) * h\n    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n    mat[:, 2] += (tx, ty)\n    result = cv2.warpAffine(image, mat, (w, h), \n                            borderMode=cv2.BORDER_REPLICATE)\n    if np.random.random() < random_flip:\n        result = result[:, ::-1]\n    return result \n```", "```py\ndef minibatch(image_list, batchsize):\n    length = len(image_list)\n    epoch = i = 0\n    shuffle(image_list)\n    while True:\n        size = batchsize\n        if i + size > length:\n            shuffle(image_list)\n            i = 0\n            epoch += 1\n        images = np.float32([read_image(image_list[j])\n                             for j in range(i, i + size)])\n        warped_img, target_img = images[:, 0, :, :, :],                                  images[:, 1, :, :, :]\n        i += size\n        yield epoch, warped_img, target_img\ndef minibatchAB(image_list, batchsize):\n    batch = minibatch(image_list, batchsize)\n    for ep1, warped_img, target_img in batch:\n        yield ep1, warped_img, target_img \n```", "```py\ndef train_one_step(iter,batch_genA,batch_genB,autoencoder_A,autoencoder_B):\n    epoch, warped_A, target_A = next(batch_genA)\n    epoch, warped_B, target_B = next(batch_genB)\n    loss_A = autoencoder_A.train_on_batch(warped_A, target_A)\n    loss_B = autoencoder_B.train_on_batch(warped_B, target_B)\n    print(\"[#{0:5d}] loss_A: {1:.5f}, loss_B: {2:.5f}\".format(iter, loss_A, loss_B))\nctr = 10000\nbatchsize = 64\nsave_interval = 100\nmodel_dir = \"models\"\nfn_imgA = get_image_paths('nicolas_face')\nfn_imgB = get_image_paths('trump_face')\nbatch_genA = minibatchAB(fn_imgA, batchsize)\nbatch_genB = minibatchAB(fn_imgB, batchsize)\nfor epoch in range(0, ctr):\n    save_iteration = epoch % save_interval == 0\n    train_one_step(epoch,batch_genA,batch_genB,autoencoder_A,autoencoder_B)\n    if save_iteration:\n        print(\"{}/{}\".format(epoch,ctr))\n        save_weights('models',encoder,decoder_A,decoder_B) \n```", "```py\nclass Convert():\n    def __init__(self, encoder,\n                 blur_size=2,\n                 seamless_clone=False,\n                 mask_type=\"facehullandrect\",\n                 erosion_kernel_size=None,\n                 **kwargs):\n        self.encoder = encoder\n        self.erosion_kernel = None\n        if erosion_kernel_size is not None:\n            self.erosion_kernel = cv2.getStructuringElement(\n                cv2.MORPH_ELLIPSE, (erosion_kernel_size, erosion_kernel_size))\n        self.blur_size = blur_size\n        self.seamless_clone = seamless_clone\n        self.mask_type = mask_type.lower()\n    def patch_image(self, image, face_detected):\n\n        size = 64\n        image_size = image.shape[1], image.shape[0]\n        # get face alignment matrix\n        mat = np.array(get_align_mat(face_detected)).reshape(2, 3) * size\n        # perform affine transformation to \n        # transform face as per alignment matrix\n        new_face = self.get_new_face(image, mat, size)\n        # get face mask matrix\n        image_mask = self.get_image_mask(image, new_face, \n                                         face_detected, mat, \n                                         image_size)\n\n        return self.apply_new_face(image,\n                                   new_face,\n                                   image_mask,\n                                   mat,\n                                   image_size,\n                                   size) \n```", "```py\n def apply_new_face(self,\n                       image,\n                       new_face,\n                       image_mask,\n                       mat,\n                       image_size,\n                       size):\n        base_image = np.copy(image)\n        new_image = np.copy(image)\n        # perform affine transformation for better match\n        cv2.warpAffine(new_face, mat, image_size, new_image,\n                       cv2.WARP_INVERSE_MAP, cv2.BORDER_TRANSPARENT)\n        outimage = None\n        if self.seamless_clone:\n            masky, maskx = cv2.transform(np.array([size / 2, size / 2]).reshape(1, 1, 2), cv2.invertAffineTransform(mat)).reshape(2).astype(int)\n            outimage = cv2.seamlessClone(new_image.astype(np.uint8), base_image.astype(np.uint8), (image_mask * 255).astype(np.uint8), (masky, maskx), cv2.NORMAL_CLONE)\n        else:            \n            # apply source face on the target image's mask\n            foreground = cv2.multiply(image_mask, \n                                      new_image.astype(float))\n            # keep background same \n            background = cv2.multiply(1.0 - image_mask, \n                                      base_image.astype(float))\n            # merge foreground and background components\n            outimage = cv2.add(foreground, background)\n        return outimage\n    def get_new_face(self, image, mat, size):\n        # function to align input image based on\n        # base image face matrix\n        face = cv2.warpAffine(image, mat, (size, size))\n        face = np.expand_dims(face, 0)\n        new_face = self.encoder(face / 255.0)[0]\n        return np.clip(new_face * 255, 0, 255).astype(image.dtype)\n    def get_image_mask(self, image, new_face, face_detected, mat, image_size):\n        # function to get mask/portion of image covered by face\n        face_mask = np.zeros(image.shape, dtype=float)\n        if 'rect' in self.mask_type:\n            face_src = np.ones(new_face.shape, dtype=float)\n            cv2.warpAffine(face_src, mat, image_size, face_mask,\n                           cv2.WARP_INVERSE_MAP, cv2.BORDER_TRANSPARENT)\n        hull_mask = np.zeros(image.shape, dtype=float)\n        if 'hull' in self.mask_type:\n            hull = cv2.convexHull(np.array(face_detected.landmarksAsXY()).reshape(\n                (-1, 2)).astype(int)).flatten().reshape((-1, 2))\n            cv2.fillConvexPoly(hull_mask, hull, (1, 1, 1))\n        if self.mask_type == 'rect':\n            image_mask = face_mask\n        elif self.mask_type == 'faceHull':\n            image_mask = hull_mask\n        else:\n            image_mask = ((face_mask * hull_mask))\n        # erode masked image to improve blending\n        if self.erosion_kernel is not None:\n            image_mask = cv2.erode(image_mask, self.erosion_kernel, \n                                   iterations=1)\n        # blur masked image to improve blending\n        if self.blur_size != 0:\n            image_mask = cv2.blur(image_mask, (self.blur_size, \n                                               self.blur_size))\n        return image_mask \n```", "```py\ndef convert(converter, item,output_dir):\n    try:\n        (filename, image, faces) = item\n        image1 = None\n        for idx, face in faces:\n            image1 = converter.patch_image(image, face)\n        if np.any(image1):\n          output_file = output_dir+\"/\"+str(filename).split(\"/\")[-1]\n          cv2.imwrite(str(output_file), image1)\n    except Exception as e:\n        print('Failed to convert image: {}. Reason: {}'.format(filename, e)) \nConvert class and an inference loop to generate output:\n```", "```py\nconv_name = \"Masked\"\nswap_model = False\nblur_size = 2\nseamless_clone = False\nmask_type = \"facehullandrect\"\nerosion_kernel_size = None\nsmooth_mask = True\navg_color_adjust = True\nfaceswap_converter = Convert(model_swapper(False,autoencoder_A,autoencoder_B),\n    blur_size = blur_size,\n    seamless_clone = seamless_clone,\n    mask_type = mask_type,\n    erosion_kernel_size = erosion_kernel_size,\n    smooth_mask = smooth_mask,\n    avg_color_adjust = avg_color_adjust\n)\nlist_faces=get_list_images_faces('nicolas_face',                                 'nicolas_ref.png',extractor)\nfor item in list_faces:\n    #print(item)\n    convert(faceswap_converter, item,'face_swaps_trump') \n```", "```py\nbuild_generator, that prepares the generator network:\n```", "```py\ndef build_generator(img_shape,channels=3,num_filters=64):\n    # Image input\n    input_layer = Input(shape=img_shape)\n    # Downsampling\n    down_sample_1 = downsample_block(input_layer, \n                                     num_filters, \n                                     batch_normalization=False)\n    # rest of the downsampling blocks have batch_normalization=true\n    down_sample_2 = downsample_block(down_sample_1, num_filters*2)\n    down_sample_3 = downsample_block(down_sample_2, num_filters*4)\n    down_sample_4 = downsample_block(down_sample_3, num_filters*8)\n    down_sample_5 = downsample_block(down_sample_4, num_filters*8)\n    down_sample_6 = downsample_block(down_sample_5, num_filters*8)\n    down_sample_7 = downsample_block(down_sample_6, num_filters*8)\n    # Upsampling blocks with skip connections\n    upsample_1 = upsample_block(down_sample_7, down_sample_6, \n                                               num_filters*8)\n    upsample_2 = upsample_block(upsample_1, down_sample_5, \n                                            num_filters*8)\n    upsample_3 = upsample_block(upsample_2, down_sample_4, \n                                            num_filters*8)\n    upsample_4 = upsample_block(upsample_3, down_sample_3, \n                                            num_filters*8)\n    upsample_5 = upsample_block(upsample_4, down_sample_2, \n                                            num_filters*2)\n    upsample_6 = upsample_block(upsample_5, down_sample_1, num_filters)\n    upsample_7 = UpSampling2D(size=2)(upsample_6)\n    output_img = Conv2D(channels, \n                        kernel_size=4, \n                        strides=1, \n                        padding='same', \n                        activation='tanh')(upsample_7)\n    return Model(input_layer, output_img) \n```", "```py\ndef build_discriminator(img_shape,num_filters=64):\n    input_img = Input(shape=img_shape)\n    cond_img = Input(shape=img_shape)\n    # Concatenate input and conditioning image by channels \n    # as input for discriminator\n    combined_input = Concatenate(axis=-1)([input_img, cond_img])\n    # First discriminator block does not use batch_normalization\n    disc_block_1 = discriminator_block(combined_input, \n                                       num_filters, \n                                       batch_normalization=False)\n    disc_block_2 = discriminator_block(disc_block_1, num_filters*2)\n    disc_block_3 = discriminator_block(disc_block_2, num_filters*4)\n    disc_block_4 = discriminator_block(disc_block_3, num_filters*8)\n    output = Conv2D(1, kernel_size=4, strides=1, padding='same')(disc_block_4)\n    return Model([input_img, cond_img], output) \ndiscriminator network uses repeating blocks consisting of convolutional, LeakyReLU, and batch normalization layers. The output is a *patch-GAN* kind of setup that divides the whole output into several overlapping patches to calculate fake versus real. The patch-GAN ensures high-quality outputs that feel more realistic.\n```", "```py\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\ndiscriminator = build_discriminator(img_shape=(IMG_HEIGHT,IMG_WIDTH,3),\n                                    num_filters=64)\ndiscriminator.compile(loss='mse',\n                      optimizer=Adam(0.0002, 0.5),\n                      metrics=['accuracy'])\ngenerator = build_generator(img_shape=(IMG_HEIGHT,IMG_WIDTH,3),\n                            channels=3,\n                            num_filters=64)\nsource_img = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\ncond_img = Input(shape=(IMG_HEIGHT,IMG_WIDTH,3))\nfake_img = generator(cond_img)\ndiscriminator.trainable = False\noutput = discriminator([fake_img, cond_img])\ngan = Model(inputs=[source_img, cond_img], outputs=[output, fake_img])\ngan.compile(loss=['mse', 'mae'],\n            loss_weights=[1, 100],\n            optimizer=Adam(0.0002, 0.5)) \n```", "```py\ndef train(generator, \n          discriminator, \n          gan, \n          patch_gan_shape, \n          epochs,\n          path='/content/data',\n          batch_size=1, \n          sample_interval=50):\n\n    # Ground truth shape/Patch-GAN outputs\n    real_y = np.ones((batch_size,) + patch_gan_shape)\n    fake_y = np.zeros((batch_size,) + patch_gan_shape)\n    for epoch in range(epochs):\n      print(\"Epoch={}\".format(epoch))\n      for idx, (imgs_source, imgs_cond) in enumerate(batch_generator(path=path,\n          batch_size=batch_size,\n          img_res=[IMG_HEIGHT, IMG_WIDTH])):\n            # train discriminator\n            # generator generates outputs based on \n            # conditioned input images\n            fake_imgs = generator.predict([imgs_cond])\n            # calculate discriminator loss on real samples\n            disc_loss_real = discriminator.train_on_batch([imgs_source, \n                                                           imgs_cond], \n                                                           real_y)\n            # calculate discriminator loss on fake samples\n            disc_loss_fake = discriminator.train_on_batch([fake_imgs, \n                                                           imgs_cond], \n                                                           fake_y)\n            # overall discriminator loss\n            discriminator_loss = 0.5 * np.add(disc_loss_real, disc_loss_fake)\n            # train generator\n            gen_loss = gan.train_on_batch([imgs_source, imgs_cond], [real_y, imgs_source])\n            # training updates every 50 iterations\n            if idx % 50 == 0:\n              print (\"[Epoch {}/{}] [Discriminator loss: {}, accuracy: {}] [Generator loss: {}]\".format(epoch, epochs, \n                                        discriminator_loss[0], \n                                        100*discriminator_loss[1],\n                                        gen_loss[0]))\n            # Plot and Save progress every few iterations\n            if idx % sample_interval == 0:\n              plot_sample_images(generator=generator,\n                                 path=path,\n                                 epoch=epoch,\n                                 batch_num=idx,\n                                 output_dir='images') \nwith paired training examples. Pix2pix is a highly optimized GAN which requires very few resources overall. With only 400 samples and 200 epochs, we trained our landmarks-to-video frame GAN. \n```", "```py\nget_landmarks and get_obama functions:\n```", "```py\nCROP_SIZE = 256\nDOWNSAMPLE_RATIO = 4\ndef get_landmarks(black_image,gray,faces):\n    for face in faces:\n        detected_landmarks = predictor(gray, face).parts()\n        landmarks = [[p.x * DOWNSAMPLE_RATIO, p.y * DOWNSAMPLE_RATIO] for p in detected_landmarks]\n        jaw = reshape_for_polyline(landmarks[0:17])\n        left_eyebrow = reshape_for_polyline(landmarks[22:27])\n        right_eyebrow = reshape_for_polyline(landmarks[17:22])\n        nose_bridge = reshape_for_polyline(landmarks[27:31])\n        lower_nose = reshape_for_polyline(landmarks[30:35])\n        left_eye = reshape_for_polyline(landmarks[42:48])\n        right_eye = reshape_for_polyline(landmarks[36:42])\n        outer_lip = reshape_for_polyline(landmarks[48:60])\n        inner_lip = reshape_for_polyline(landmarks[60:68])\n        color = (255, 255, 255)\n        thickness = 3\n        cv2.polylines(black_image, [jaw], False, color, thickness)\n        cv2.polylines(black_image, [left_eyebrow], False, color, \n                      thickness)\n        cv2.polylines(black_image, [right_eyebrow], False, color, \n                      thickness)\n        cv2.polylines(black_image, [nose_bridge], False, color, \n                      thickness)\n        cv2.polylines(black_image, [lower_nose], True, color, \n                      thickness)\n        cv2.polylines(black_image, [left_eye], True, color, thickness)\n        cv2.polylines(black_image, [right_eye], True, color, thickness)\n        cv2.polylines(black_image, [outer_lip], True, color, thickness)\n        cv2.polylines(black_image, [inner_lip], True, color, thickness)\n    return black_image\ndef get_obama(landmarks):\n    landmarks = (landmarks/127.5)-1\n    landmarks = tf.image.resize(landmarks, [256,256]).numpy()\n    fake_imgs = generator.predict(np.expand_dims(landmarks,axis=0))\n    return fake_imgs \n```", "```py\ncap = cv2.VideoCapture(0)\nfps = video.FPS().start()\nk = 0\ndisplay_plots = True\ndisplay_cv2 = True\nwhile True:\n    k += 1\n    ret, frame = cap.read(0)\n    if np.all(np.array(frame.shape)):\n        frame_resize = cv2.resize(frame, None, fx=1 / DOWNSAMPLE_RATIO, fy=1 / DOWNSAMPLE_RATIO)\n        gray = cv2.cvtColor(frame_resize, cv2.COLOR_BGR2GRAY)\n        faces = detector(gray, 1)\n        black_image = np.zeros(frame.shape, np.uint8)\n        landmarks = get_landmarks(black_image.copy(),gray,faces)\n        img_tgt = (landmarks/127.5)-1\n        img_tgt = tf.image.resize(img_tgt, [256,256]).numpy()\n        obama = generator.predict(np.expand_dims(img_tgt,axis=0))[0]\n        try:\n            obama = 0.5 * obama + 0.5\n            gen_imgs = np.concatenate([np.expand_dims(cv2.cvtColor(rescale_frame(frame_resize), cv2.COLOR_RGB2BGR),axis=0), \n                     np.expand_dims(rescale_frame(obama),axis=0), \n                     np.expand_dims(rescale_frame(landmarks),axis=0)])\n            if display_plots:\n                titles = ['Live', 'Generated', 'Landmarks']\n                rows, cols = 1, 3\n                fig, axs = plt.subplots(rows, cols)\n                for j in range(cols):\n                    if j!=1:\n                        axs[j].imshow(gen_imgs[j].astype(int))\n                    else:\n                        axs[j].imshow(gen_imgs[j])\n                    axs[j].set_title(titles[j])\n                    axs[j].axis('off')\n                plt.show()\n            if display_cv2:\n                cv2.imshow('synthetic obama', cv2.cvtColor(gen_imgs[1], cv2.COLOR_BGR2RGB))\n                #cv2.imshow('landmark', rescale_frame(landmarks))\n        except Exception as ex:\n            print(ex)\n        fps.update()\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\nfps.stop()\nprint('[INFO] elapsed time (total): {:.2f}'.format(fps.elapsed()))\nprint('[INFO] approx. FPS: {:.2f}'.format(fps.fps()))\ncap.release()\ncv2.destroyAllWindows() \nusing the pix2pix GAN. Upon executing the video capture and manipulation loop, we are able to generate some promising results. Some of the re-enactments are depicted in the following figure:\n```"]