- en: Learning Text Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks require inputs only in numbers. So when we have textual data,
    we convert them into numeric or vector representation and feed it to the network.
    There are various methods for converting the input text to numeric form. Some
    of the popular methods include **term frequency-inverse document frequency** (**tf-idf**),
    **bag of words (BOW)**, and so on. However, these methods do not capture the semantics
    of the word. This means that these methods will not understand the meaning of
    the words.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about an algorithm called **word2vec** which
    converts the textual input to a meaningful vector. They learn the semantic vector
    representation for each word in the given input text. We will start off the chapter
    by understanding about word2vec model and two different types of word2vec model
    called **continuous bag-of-words** (**CBOW**) and skip-gram model. Next, we will
    learn how to build word2vec model using gensim library and how to visualize high
    dimensional word embeddings in tensorboard.
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead, we will learn about **doc2vec** model which is used for learning
    the representations for a document. We will understand two different methods in
    doc2vec called **Paragraph Vector -** **Distributed Memory** **Model** (**PV-DM**)
    and **Paragraph Vector -** **Distributed Bag of Words** (**PV-DBOW**). We will
    also see how to perform document classification using doc2vec. At the end of the
    chapter, we will learn about skip-thoughts algorithms and quick thoughts algorithm
    which is used for learning the sentence representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will understand the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The word2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a word2vec model using gensim
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing word embeddings in TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doc2vec model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding similar documents using doc2vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip-thoughts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick-thoughts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the word2vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec is one of the most popular and widely used models for generating the
    word embeddings. What are word embeddings though? Word embeddings are the vector
    representations of words in a vector space. The embedding generated by the word2vec
    model captures the syntactic and semantic meanings of a word. Having a meaningful
    vector representation of a word helps the neural network to understand the word
    better.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s consider the following text: *Archie used to live in New
    York, he then moved to Santa Clara. He loves apples and strawberries.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Word2vec model generates the vector representation for each of the words in
    the preceding text. If we project and visualize the vectors in embedding space,
    we can see how all the similar words are plotted close together. As you can see
    in the following figure, words *apples* and *strawberries* are plotted close together,
    and *New York* and *Santa Clara* are plotted close together. They are plotted
    close together because the word2vec model has learned that *apples* and *strawberries*
    are similar entities that is, fruits and *New York* and *Santa Clara* are similar
    entities, that is *cities*, and so their vectors (embeddings) are similar to each
    other, and which is why the distance between them is less:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30025401-9037-42e2-a7c7-09ebeeb6caa0.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, with word2vec model, we can learn the meaningful vector representation
    of a word which helps the neural networks to understand what the word is about.
    Having a good representation of a word would be useful in various tasks. Since
    our network can understand the contextual and syntactic meaning of words, this
    will branch out to various use cases such as text summarization, sentiment analysis,
    text generation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay. But how do the word2vec model learn the word embeddings? There are two
    types of word2vec models for learning the embeddings of a word:'
  prefs: []
  type: TYPE_NORMAL
- en: CBOW model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Skip-gram model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will go into detail and learn how each of these models learns the vector
    representations of a word.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the CBOW model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say we have a neural network with an input layer, a hidden layer, and
    an output layer. The goal of the network is to predict a word given its surrounding
    words. The word that we are trying to predict is called the **target word** and
    the words surrounding the target word are called the **context words**.
  prefs: []
  type: TYPE_NORMAL
- en: How many context words do we use to predict the target word? We use a window
    of size ![](img/565e5e2c-349b-4dec-8198-86027ebd3b89.png) to choose the context
    word. If the window size is 2, then we use two words before and two words after
    the target word as the context words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the sentence, *The Sun rises in the east* with the word *rises*
    as the target word. If we set the window size as 2, then we take the words *the*
    and *sun,* which are the two words before, and *in* and *the* which are the two
    words after the target word *rises* as context words, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d67de55c-90c2-4457-a746-c9427430741e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So the input to the network is context words and the output is a target word.
    How do we feed these inputs to the network? The neural network accepts only numeric
    input so we cannot feed the raw context words directly as an input to the network.
    Hence, we convert all the words in the given sentence into a numeric form using
    the one-hot encoding technique, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50b341fe-78f6-4142-8858-caec284b7d07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The architecture of the CBOW model is shown in the following figure. As you
    can see, we feed the context words, *the, sun, in*, and *the,* as inputs to the
    network and it predicts the target word *rises* a*s* an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38a8ac22-8330-4477-9692-b04fe3326bf6.png)'
  prefs: []
  type: TYPE_IMG
- en: In the initial iteration, the network cannot predict the target word correctly.
    But over a series of iterations, it learns to predict the correct target word
    using gradient descent. With gradient descent, we update the weights of the network
    and find the optimal weights with which we can predict the correct target word.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have one input, one hidden, and one output layer, as shown in the preceding
    figure, we will have two weights:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer to hidden layer weight, ![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer to output layer weight, ![](img/dfbb80c7-0625-43b1-9d59-c3f84e0fa17e.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the training process, the network will try to find the optimal values
    for these two sets of weights so that it can predict the correct target word.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the optimal weights between the input to a hidden layer ![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
    forms the vector representation of words. They basically constitute the semantic
    meaning of the words. So, after training, we simply remove the output layer and
    take the weights between the input and hidden layers and assign them to the corresponding
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'After training, if we look at the ![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
    matrix, it represents the embeddings for each of the words. So, the embedding
    for the word *sun* is [0.0, 0.3,0.3,0.6,0.1 ]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ae49ea5-39d7-46be-bbfd-b06845fb7491.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the CBOW model learns to predict the target word with the given context
    words. It learns to predict the correct target word using gradient descent. During
    training, it updates the weights of the network through gradient descent and finds
    the optimal weights with which we can predict the correct target word. The optimal
    weights between the input and hidden layers form the vector representations of
    a word. So, after training, we simply take the weights between the input and hidden
    layers and assign them as a vector to the corresponding words.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an intuitive understanding of the CBOW model, we will go into
    detail and learn mathematically how exactly the word embeddings are computed.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that weights between the input and the hidden layers basically form
    the vector representation of the words. But how exactly does the CBOW model predicts
    the target word? How does it learn the optimal weights using backpropagation?
    Let's look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: CBOW with a single context word
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned that, in the CBOW model, we try to predict the target word given
    the context words, so it takes some ![](img/1be066cd-9ae6-4c55-8ea4-bc2e5f280d8e.png)
    number of context words as an input and returns one target word as an output.
    In CBOW model with a single context word, we will have only one context word,
    that is, ![](img/f767c138-9bb5-4bc2-b598-8c19caab3d5b.png). So, the network takes
    only one context word as an input and returns one target word as an output.
  prefs: []
  type: TYPE_NORMAL
- en: Before going ahead, first, let's familiarize ourselves with the notations. All
    the unique words we have in our corpus is called the **vocabulary**. Considering
    the example we saw in the *Understanding the CBOW model* section, we have five
    unique words in the sentenceâ€”*the*, *sun*, *rises*, *in*, and *east*. These five
    words are our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![](img/28e68adf-0cf3-41d7-b313-250b1a777f93.png)denote the size of the
    vocabulary (that is, number of words) and ![](img/5395fd7f-8249-4750-afec-9c901d8a7f3c.png)
    denotes the number of neurons in the hidden layer. We learned that we have one
    input, one hidden, and one output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: The input layer is represented by ![](img/10bb5dbd-fdf0-4699-8297-d12490cf3480.png).
    When we say ![](img/70f5dd90-b264-4e2a-9132-1df2e29c5974.png), it represents the
    ![](img/24d01c97-ea8f-4a0c-8a4c-a1c95e5543fe.png) input word in the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hidden layer is represented by ![](img/d4db72ad-4b63-4e36-95bd-4bed53452038.png).
    When we say ![](img/55fb0590-0038-4c33-a828-f1e509469be6.png), it represents the
    ![](img/0c4beba7-b212-4130-b23c-5891a33120e2.png)neuron in the hidden layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer is represented by ![](img/bf19185d-47ce-48e8-a5b9-8abe0d83f9be.png).
    When we say ![](img/1707ff27-1948-48c5-9986-3760050ca4d2.png) it represents the
    ![](img/c7991abb-c811-4e4f-b9d0-d150baeb0300.png)output word in the vocabulary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dimension of input to hidden layer weight ![](img/16c17c70-d122-4dd9-8efc-180583d17de6.png)
    is ![](img/93436413-de5f-4eed-be42-4804aa6e9c51.png) (which is the *size of our
    vocabulary x the number of neurons in the hidden layer*) and the dimension of
    hidden to output layer weight, ![](img/053566fc-39df-4ac1-be14-0504c8e951f0.png)
    is ![](img/0938958c-85c1-4482-8438-b0c890b67e05.png) (that is, the *number of
    neurons in the hidden layer x the size of the vocabulary*). The representation
    of the elements of the matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88598b9c-17d3-48e2-bf20-f4d173924600.png) represents an element in
    the matrix between node ![](img/4312bbef-3b4c-4994-8b2d-383de32644dc.png) of the
    input layer and node ![](img/cd0d6d00-0870-44d0-8273-d6d21b5bc57a.png) of the
    hidden layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/625fd42e-d7ca-4a5c-9878-aec1f414b10b.png) represents an element in
    the matrix between node ![](img/95948634-5121-4bb3-ac04-87b3ce8a417b.png) of the
    hidden layer and node ![](img/efe5da3d-88d4-4aef-9e6e-cd8787aa0008.png) of the
    output layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure will help us to attain clarity on the notations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8deba7bd-3467-46a7-bcff-38e287a52b68.png)'
  prefs: []
  type: TYPE_IMG
- en: Forward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to predict target words given a context word, we need to perform forward
    propagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we multiply the input ![](img/b59575fa-9574-4b08-847f-c60923c09f48.png)
    with the input to hidden layer weight ![](img/42869efe-315b-4bb8-acd5-cfba8e46acb6.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b08663e6-7e08-4cc9-a0c6-77ce38474012.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know each of the input words is one-hot encoded, so when we multiply ![](img/ddf8720e-6c3e-4ade-9245-d78a0e6714bd.png)
    with ![](img/fd23003b-f971-47e3-8bfb-4cab40fff647.png), we basically obtain the
    ![](img/b1d3087b-3f76-4ff0-ad4a-1b1b76a3e242.png) row of ![](img/6bf8629c-5577-4e5e-a744-09a8328af6c0.png)
    to ![](img/d27e266c-0b00-4cf2-94aa-31dd74ffd685.png). So, we can directly write
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/772add17-ac4f-4bbe-a9fb-ca263fa739eb.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0f435fb3-2560-4c5d-9863-7b4647a5a058.png) basically implies the vector
    representation of the input word. Let''s denote the vector representation for
    the input word ![](img/1e51dde8-5816-498b-8e09-bd5038617a9d.png) by ![](img/4831b672-87ba-497d-8b52-5e80d9098041.png).
    So, the previous equation can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14c7bb66-a44c-40c8-b990-51b6b8bc7fc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we are in the hidden layer ![](img/4acc897c-0c04-4a77-8527-6e382bdb80f8.png)
    and we have another set of weight, which is hidden to output layer weight, ![](img/fe6b3a36-a507-41df-9cc2-a6aeffa627d4.png).
    We know that we have ![](img/64f5782d-47fe-4555-96c5-d5b489ad2acf.png) number
    of words in our vocabulary and we need to compute the probability for each of
    the words in our vocabulary to be the target word.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let ![](img/b2ebffb6-d301-46a8-bfa5-4fcf004cfc00.png) denote the score for
    the ![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)word in our vocabulary to
    be a target word. The score ![](img/86f338d1-6f8e-4058-9df1-30d71bbeb2c4.png)
    is computed by multiplying the value of hidden layer ![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)
    and the hidden to output layer weight ![](img/bea30eb3-8cc3-46aa-9be7-ca17659e4f14.png).
    Since we are calculating the score for a word ![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png),
    we multiply the hidden layer ![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)
    with the ![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png) column of the matrix,
    ![](img/e0101955-901b-4d4a-9027-a00033476af1.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2454457a-0a5e-4451-90ad-2b1909c822ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png) column of the weight
    matrix ![](img/e0101955-901b-4d4a-9027-a00033476af1.png) basically denotes the
    vector representation of the word ![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png).
    Let''s denote the vector representation of the ![](img/9ef9f1db-e40c-410e-ab64-dc8166e5406b.png)
    word by ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png). So, the preceding equation
    can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5984dac-97c6-4384-ab34-eed4b8989cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(1)* in equation *(2)*, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8454782c-a236-49f6-a8be-29b282f72385.png)'
  prefs: []
  type: TYPE_IMG
- en: Can you infer what the preceding equation is trying to say? We are basically
    computing the dot product between the input context word representation, ![](img/b1fc0aac-ccdb-40d1-a876-afe56832946b.png),
    and the representation of ![](img/7c4ef403-fb01-45cb-ad55-474e50a656be.png) word
    in our vocabulary, ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png).
  prefs: []
  type: TYPE_NORMAL
- en: Computing the dot product between any two vectors helps us to understand how
    similar they are. Hence, computing the dot product between ![](img/54377cf3-2f02-4baa-a8a3-b7c4c818d703.png)
    and ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png) tells us how similar the
    ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png) word in our vocabulary is to
    the input context word. Thus, when the score for a ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png)
    word in the vocabulary , ![](img/e1af9677-c6f2-466f-bb02-6dcbce530271.png) is
    high, then it implies that the word ![](img/b7ea99e1-97ed-411e-8593-1eff0dea4578.png)
    is similar to the given input word and it is the target word. Similarly, when
    the score for a ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png) word in the
    vocabulary, ![](img/a00856ab-ad98-4502-8f0a-49340d60e79d.png), is low, then it
    implies that the word ![](img/7d1306a6-05a2-4227-addf-8ed05aec706d.png) is not
    similar to the given input word and it is not the target word.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png) basically gives us the
    score for a word ![](img/facc6cb1-db9d-4ecf-a60e-28a4fd152571.png) to be the target
    word. But instead of having ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)
    as a raw score, we convert them into probabilities. We know that the softmax function
    squashes values between 0 to 1, so we can use the softmax function for converting
    ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png) into the probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write our output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55d7d9f4-6b63-48e6-8891-d6c3ac30236e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/c571759f-1422-406d-9713-80ca2e80320e.png) tells us the probability
    for a word ![](img/bb81e391-269b-4e9d-b719-c6a562784121.png) to the target word
    given an input context word. We compute the probability for all the words in our
    vocabulary and select the word that has a high probability as the target word.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what is our objective function? that is, how do we compute the loss?
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to find the correct target word. Let ![](img/ee7a65a2-78a3-48fb-8473-375153d20017.png)
    denote the probability of the correct target word. So, we need to maximize this
    probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e04e42c-0178-48ac-a12a-611cf58a1ba6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of maximizing the raw probabilities, we maximize the log probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62e56fe1-4a68-4389-aa0a-597fbf1e35ae.png)'
  prefs: []
  type: TYPE_IMG
- en: But why do we want to maximize the log probability instead of the raw probability?
    Because machines have limitations in representing a floating point of a fraction
    and when we multiply many probabilities, it will lead to a value that is infinitely
    small. So, to avoid that, we use log probabilities and it will ensure numerical
    stability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a maximization objective, we need to convert this to a minimization
    objective so that we can apply our favorite gradient descent algorithm for minimizing
    the objective function. How can we change our maximization objective to the minimization
    objective? We can do that by simply adding the negative sign. So our objective
    function becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/802c0bdc-a33b-4eab-9a82-36152c243dd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccf589ba-6418-4f6b-94bd-610879c30b62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(3)* in equation *(4)*, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f75fdf2f-9b33-4340-979b-a69612d2568d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'According to the logarithm quotient rule, *log(a/b) = log(a) - log(b)*, we
    can rewrite the previous equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24907d6f-88c6-4b8d-a094-0f962c167621.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/602521e1-4cca-47e3-b589-217278f67647.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that *log* and *exp* cancel each other, so we can cancel the *log*
    and *exp* in the first term and our final loss function becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15049d3f-2a48-497c-bbf4-e9c1d6b877d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We minimize the loss function using the gradient descent algorithm. So, we
    backpropagate the network, calculate the gradient of the loss function with respect
    to weights, and update the weights. We have two sets of weights, input to hidden
    layer weight ![](img/a97130e6-b8ec-4bf8-b44c-cfb431e50af0.png) and hidden to output
    layer weights ![](img/25402288-f99a-46b3-9409-5fc1ad0130a9.png). We calculate
    gradients of loss with respect to both of these weights and update them according
    to the weight update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b6c6b97-6b29-4494-aaee-2d46ad552afa.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/6fda6f1d-81a1-4361-9f7c-55da60e9de05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to better understand the backpropagation, let''s recollect the steps
    involved in the forward propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5a7d022d-3be1-404c-a4dc-9bb4c72ad987.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we compute the gradients of loss with respect to the hidden to output
    layer ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png). We cannot calculate the
    gradient of loss ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png) with respect
    to ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png) directly from ![](img/6ccca083-758a-4ec5-ae2b-e48ea51a46f7.png),
    as there is no ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png) term in the loss
    function ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png), so we apply the chain
    rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cb9185c-2894-40bd-96fc-9c8b4ad92600.png)'
  prefs: []
  type: TYPE_IMG
- en: Please refer to the equations of forward propagation to understand how derivatives
    are calculated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The derivative of the first term is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/64182944-a208-4c28-957a-246af8a54e18.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/29f4b902-e2cd-4949-88e6-40e4e4755cb2.png) is the error term, which
    is the difference between the actual word and predicted word.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will calculate the derivative of the second term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know ![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/65ed56d8-8fd2-421f-a888-85926a972ec6.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the **gradient of loss ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)
    with respect to ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png)** is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d556d3b2-2bbb-4fba-bfe7-79c8115d3b2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we compute the gradient with respect to the input to hidden layer weight
    ![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png). We cannot calculate the derivative
    directly from ![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png), as there is no
    ![](img/f3a8d544-9c42-4f28-93e3-a3be04ec4cea.png) term in ![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png),
    so we apply the chain rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e103418-91ce-41e4-84bb-481386f8170f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to compute the derivative of the first term in the preceding equation,
    we again apply the chain rule, as we cannot compute the derivative of ![](img/3814d69c-7064-4f02-82b2-75db5faba6cd.png)
    with respect to ![](img/9785cb31-f498-41ff-919a-40eb62a0853f.png) directly from
    ![](img/bc927050-b111-4bb2-a438-bd78614fcd8d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df7ae6d8-1595-46b2-86c4-ae9315cb4965.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From equation *(5)*, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc911a96-65f1-4cba-8891-3906670cf422.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know ![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1b8ba1a-1ea5-4d87-a495-220e5c316ccd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of having the sum, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a700b684-1d54-49b2-9058-1bafb30d9f98.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/be3580f6-a59d-49c0-8ce0-becdb699e0b8.png) denotes the sum of the output
    vector of all words in the vocabulary, weighted by their prediction error.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now calculate the derivative of the second term.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know, ![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad2b53dd-8193-4b02-9366-77fb060d676b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the **gradient of loss ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)
    with respect to ![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png)** is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f33f4252-bd4e-4c03-bca5-6b760b09221f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, our weight update equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ec5dabb-8b62-406f-8345-41e3407e8e15.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/8a80e3b8-52a7-438c-9a8d-d277a7cefc07.png)'
  prefs: []
  type: TYPE_IMG
- en: We update the weights of our network using the preceding equation and obtain
    an optimal weights during training. The optimal input to hidden layer weight,
    ![](img/2bd9fff6-c950-4bba-afa7-2fcb03f4c13e.png), becomes the vector representation
    for the words in our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code for `Single_context_CBOW` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: CBOW with multiple context words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understood how the CBOW model works with a single word as a context,
    we will see how it will work when you have multiple words as context words. The
    architecture of CBOW with multiple input words as a context is shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8067832-9cf4-4d07-951b-6ad2f0720d48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There is not much difference between the multiple words as a context and a
    single word as a context. The difference is that, with multiple contexts words
    as inputs, we take the average of all the input context words. That is, as a first
    step, we forward propagate the network and compute the value of ![](img/ba4894e5-f69c-48e4-ad6b-7deb10ff2e41.png)
    by multiplying input ![](img/239da2e3-155c-48ba-bb32-71d50b821224.png) and weights
    ![](img/52646ca3-7984-4386-b57e-0dd3c053db60.png), as we saw in the *CBOW with
    a single context word* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff073da4-ac16-4bd6-9137-26007776bf56.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But, here, since we have multiple context words, we will have multiple inputs
    (that is ![](img/7f74c8cb-6fd0-4033-b097-374c7277c5e3.png)), where ![](img/b17c0fb1-83ff-4a04-8e14-3ff7cc39688f.png)
    is the number of context words, and we simply take the average of them and multiply
    with the weight matrix, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21dee3f2-48b0-48ad-a685-3cf220a5cea8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/79ec7be5-7da6-485f-9a99-5d69c0ebd372.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to what we learned in the *CBOW with single context word* section, ![](img/37ec8731-2be1-4f09-8eef-07cf1731d4f2.png)
    represents the vector representation of the input context word ![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png).
    ![](img/591d1ae7-16d0-468c-84b9-db6ce8a5a1cd.png) represents the vector representation
    of the input word ![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We denote the representation of the input context word ![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png)
    by ![](img/09ff6572-5c5b-41a9-8116-494b59b704e8.png), the representation of the
    input context word ![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png) by ![](img/bfe95f24-ad89-44b6-bd71-d5d4145a9541.png),
    and so on. So, we can rewrite the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84b27cb4-4d63-4717-a014-15aa30a2cff9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/ee8ad712-1053-455a-a687-887767c82a94.png) represents the number
    of context words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing the value of ![](img/a19f3259-021d-4713-be65-ffb8cfeb8f8a.png) is
    the same as we saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cae8ddb8-09da-423d-b76c-7dcaf67e9e15.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/f2c71772-b8c8-4e08-85a5-81cd43f8e129.png) denotes the vector representation
    of the ![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png) word in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Substituting equation *(6)* in equation *(7)*, we write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26f9683b-8929-4579-be93-0e9b4e3d1b99.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation gives us the similarity between the ![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png)
    word in the vocabulary and the average representations of given input context
    words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function is the same as we saw in the single word context and it is
    given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9a1c6a1-2d8e-49ad-a12a-4e6ac59c4e14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, there is a small difference in backpropagation. We know that in backpropagation
    we compute gradients and update our weights according to the weight update rule.
    Recall that, in the previous section, this is how we update the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b12d13c2-602e-474c-b900-3691b46e703c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/35ac212c-b0b5-4a0e-98e4-320b0e9e2e90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since, here, we have multiple context words as an input, we take an average
    of context words while computing ![](img/ca797f1c-a327-4135-b840-9a76c4a8e94d.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15493e8e-84c3-43f6-84e0-e8d06230a8a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Computing ![](img/c64c8923-7b7f-428f-960e-96b2caa8f052.png)is the same as we
    saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b604533-b018-41f3-ab17-968e367fc5fa.png)'
  prefs: []
  type: TYPE_IMG
- en: So, in a nutshell, in the multi-word context, we just take the average of multiple
    context input words and build the model as we did in the single word context of
    CBOW.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding skip-gram model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s look at another interesting type of the word2vec model, called
    skip-gram. Skip-gram is just the reverse of the CBOW model,. That is in a skip-gram
    model, we try to predict the context words given the target word as an input.
    As shown in the following figure, we can notice that we have the target word as
    *rises* and we need to predict the context words *the, sun, in*, and *the*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/870ba64d-fdef-49ca-890c-1accb0b47366.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the CBOW model, we use the window size to determine how many context
    words we need to predict. The architecture of the skip-gram model is shown in
    the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see that it takes the single target word as input and tries to predict
    the multiple context words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af0c565a-b398-4b1b-bb7a-0a2e5fb818e9.png)'
  prefs: []
  type: TYPE_IMG
- en: In the skip-gram model, we try to predict the context words based on the target
    word. So, it takes one target word as an input and returns ![](img/91936d04-a92a-47b2-808c-3e1389753372.png)
    context words as output, as shown in the above figure. So, after training the
    skip-gram model to predict the context words, the weights between our input to
    hidden layer ![](img/5f024499-4591-477e-9667-e24e23d802fd.png) becomes the vector
    representation of the words, just like we saw in the CBOW model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of the skip-gram model, let us dive into
    detail and learn how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation in skip-gram
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will understand how forward propagation works in the skip-gram model.
    Let''s use the same notations we used in the CBOW model. The architecture of the
    skip-gram model is shown in the following figure. As you can see, we feed only
    one target word ![](img/60664943-7c5b-41c9-b69c-38d707ad0e14.png) as an input
    and it returns the ![](img/7073a847-9879-4511-a235-aab29edb2b44.png) context words
    as an output ![](img/1abce1f4-baae-4202-9e58-c3b08e0c7698.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/552225d9-108c-46a4-9b72-dc8895415c23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to what we saw in CBOW, in the *Forward propagation* section, first
    we multiply our input ![](img/e1fd5da0-c909-4186-b9dd-53dee6356719.png) with the
    input to hidden layer weights ![](img/8e685519-eb49-4b24-9f91-b9ddacc3eae8.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abe3c86e-c276-456d-ae31-f1760fa3416e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can directly rewrite the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7439c251-2328-4c30-8ef3-efa342622a35.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/a79e9b8a-f7f9-497b-b60a-ff4f80021260.png) implies the vector representation
    for the input word ![](img/3c25d777-af88-49fa-a1ba-4de48ad5b12f.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute ![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png), which implies
    a similarity score between the word ![](img/fdb95f62-9fad-4e54-bdec-1a085f9cc023.png)
    word in our vocabulary and the input target word. Similar to what we saw in the
    CBOW model, ![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png) can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60fdc55d-bbab-4355-8cc6-b1d6abfbf7dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can directly rewrite the above equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c57122ec-3d10-48a7-bd65-abb5f5aa4519.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/1ccd2510-7ece-4313-b6dd-dbd119cd2dc2.png) implies the vector representation
    of the word ![](img/040097ca-cb43-4dab-94ef-290e17b125d0.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'But, unlike the CBOW model where we just predicted the one target word, here
    we are predicting the ![](img/1a0af03f-a854-4368-9990-43aeec26fd66.png) number
    of context words. So, we can rewrite the above equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5131630-008b-437d-b8e4-b808d21baebc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, ![](img/e6ff7f32-90bd-453e-8df9-76411762aa7b.png) implies the score for
    the ![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png) word in the vocabulary to
    be the context word ![](img/e61144a0-5581-48d9-a1b0-8fd115c5f4d1.png). That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0cea83ee-9d5b-47ef-b2ef-bf952c60401d.png) implies the score for the
    word ![](img/b28be818-fb99-49f2-9932-dd39f8c7a8b8.png)to be the first context
    word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8edb41b3-3b2f-4c0a-9ec3-1def443cfd5c.png) implies the score for the
    word ![](img/23dade3d-bf24-4335-976a-08795e347b89.png) to be the second context
    word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2dbc1d19-b16b-43a5-923b-9a8695fbbdda.png) implies the score for the
    word ![](img/2049eb6f-1418-41db-a43f-0dc3ea3cb896.png) to be the third context
    word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And since we want to convert our scores to probabilities, we apply the softmax
    function and compute ![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/830ca969-b68f-4f5c-942d-4c7d2ab8eba4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png) implies the probability
    of the ![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png) word in the vocabulary
    to be the context word ![](img/8df6187b-ea84-4630-b2bb-99f3513ec3a1.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us see how to compute the loss function. Let ![](img/85f31ebd-9b63-4b6a-a2bd-074745b99561.png)
    denote the probability of the correct context word. So, we need to maximize this
    probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6abc546-b4ee-4b41-abea-1d9e0e3ae7ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of maximizing raw probabilities, maximize the log probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/749e5fbc-9416-46bb-8e62-aa1836828d68.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to what we saw in the CBOW model, we convert this into the minimization
    objective function by adding the negative sign:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97c83a18-fdcb-408d-87f0-dcd3a45889d0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(8)* in the preceding equation, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f437214e-7a5f-4717-82ae-a94798058816.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we have ![](img/2e76d1e7-1b07-4196-8292-82464decc20f.png) context words,
    we take the product sum of the probabilities as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbe9479d-e864-4e77-a76b-a0161b23dadc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, according to logarithm rules, we can rewrite the above equation and our
    final loss function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a98abd9e-b5bc-4c3f-80ca-ec8b73504bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Look at the loss function of the CBOW and skip-gram models. You'll notice that
    the only difference between the CBOW loss function and skip-gram loss function
    is the addition of the context word ![](img/1ac9a6b6-fc2e-4a8d-b43d-0f5fd7016ceb.png).
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We minimize the loss function using the gradient descent algorithm. So, we backpropagate
    the network, calculate the gradient of the loss function with respect to weights,
    and update the weights according to the weight update rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the gradient of loss with respect to hidden to output layer
    ![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png). We cannot calculate the derivative
    of loss with respect to ![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png) directly
    from ![](img/bc366eee-0a8a-4feb-8986-4d7ee848fd5b.png) as it has no ![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png)
    term in it, so we apply the chain rule as shown below. It is basically the same
    as what we saw in the CBOW model, except that here we sum over all the context
    words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/152f51e3-e4c6-455e-a9a5-5db71a134958.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, let''s compute the first term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/217ea4cc-4208-4abc-858a-ec9639c90c62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We know that ![](img/78f049e9-aa2c-4b6e-be4e-e6d530469692.png) is the error
    term, which is the difference between the actual word and the predicted word.
    For notation simplicity, we can write this sum over all the context words as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/806b7879-6d72-42c7-9ae4-90bf16dcd0a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can say that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4512aed6-0806-43f4-bd04-1bd927e25737.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compute the second term. Since we know ![](img/04ffe61d-f457-440f-b2ee-065d0de515f4.png),
    we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/146d13a2-d5ea-4031-b930-5da86abe06cf.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the **gradient of loss ![](img/50d48bac-c87c-4686-b1c0-b7c9ee29dc1e.png)
    with respect to ![](img/864f4d04-3109-43ff-ba54-2309fe6318d5.png)** is given as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f716372-88a2-4c88-91e8-3d423577e498.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we compute the gradient of loss with respect to the input to hidden layer
    weight ![](img/abb5c13a-1b06-4ce6-9535-8069ca7bad9f.png). It is simple and exactly
    same as we saw in the CBOW model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26ac31ee-0c19-4154-8bc7-2dab5be38e1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the **gradient of loss ![](img/50d48bac-c87c-4686-b1c0-b7c9ee29dc1e.png)
    with respect to ![](img/abb5c13a-1b06-4ce6-9535-8069ca7bad9f.png)** is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73a7b000-52d0-44d6-ae90-2ba997439cb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the gradients, we update our weights *W* and *W''* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77a93b00-0f6a-485b-9213-2f053d348185.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a1a480ec-be9c-4f1f-a546-9ae95f156091.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, while training the network, we update the weights of our network using
    the preceding equation and obtain optimal weights. The optimal weight between
    the input to hidden layer, ![](img/8006cddb-a1a5-414d-bf56-cb77755ba238.png) becomes
    the vector representation for the words in our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Various training strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we will look at different training strategies which can optimize and increase
    the efficiency of our word2vec model.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical softmax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In both the CBOW and skip-gram models, we used the softmax function for computing
    the probability of the occurrence of a word. But computing the probability using
    the softmax function is computationally expensive. Say, we are building a CBOW
    model; we compute the probability of the ![](img/ba9c9d7a-046a-46c2-ad93-3c770306ac88.png)
    word in our vocabulary to be the target word as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9679ecaa-45cf-4bce-8b85-21ed6c323ba1.png)'
  prefs: []
  type: TYPE_IMG
- en: If you look at the preceding equation, we are basically driving the exponent
    of the ![](img/3551b1d9-5ce6-4433-92ef-5a593c9e6068.png) with the exponent of
    all the words ![](img/31854ccf-fede-424d-818f-305ac15843bd.png) in the vocabulary.
    Our complexity would be ![](img/87cf4f3a-baf5-491e-b80b-e45540ec7feb.png), where
    ![](img/d7d624f5-bf6a-4a35-ae3e-059a99240350.png) is the vocabulary size. When
    we train the word2vec model with a vocabulary comprising millions of words, it
    is definitely going to be computationally expensive. So, to combat this problem,
    instead of using the softmax function, we use the hierarchical softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hierarchical softmax function uses a Huffman binary search tree and significantly
    reduces the complexity to ![](img/638cfce0-635f-44e9-be0a-1e0d5d99f46e.png). As
    shown in the following diagram, in hierarchical softmax, we replace the output
    layer with a binary search tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff41b350-161f-4d98-86c1-52519381e659.png)'
  prefs: []
  type: TYPE_IMG
- en: Each leaf node in the tree represents a word in the vocabulary and all the intermediate
    nodes represent the relative probability of their child node.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we compute the probability of a target word given a context word? We
    simply traverse the tree by making a decision whether to turn left or right. As
    shown in the following figure, the probability of the word *flew* to be the target
    word, given some context word ![](img/839fbeee-e568-440f-be47-d080e80da01e.png),
    is computed as a product of the probabilities along the path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6af5744d-76c7-4328-9ecd-fd0fe9b5395f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5b801e9f-5594-41b0-b3ea-791da2013858.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The probability of the target word is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc53fb4a-6996-483f-be03-86885b53298e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But how do we compute these probabilities? Each node ![](img/f85382f5-7066-4de6-bfa9-a1248991e99a.png)
    has an embedding associated with it (say, ![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png)).
    To compute the probability for a node, we multiply the node''s embedding ![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png)
    with hidden layer value ![](img/80974a3f-5565-4985-81de-08a900f927c7.png) and
    apply a sigmoid function. For instance, the probability of a node ![](img/cc7a19fa-e393-4c69-9b3d-6b9a485cd2ef.png)
    to take a right, given a context word ![](img/4d992bd5-1e88-477b-a9fd-31bc407d6b3c.png),
    is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/022eb4f4-f05f-488b-8ad5-092cac240c54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we computed the probability of taking right, we can easily compute the
    probability of taking left by simply subtracting the probability of taking right
    from 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3390104-af49-424d-9cab-9a2f84176099.png)'
  prefs: []
  type: TYPE_IMG
- en: If we sum the probability of all the leaf nodes, then it equals to 1, meaning
    that our tree is already normalized, and to find a probability of a word, we need
    to evaluate only ![](img/ed24f8a7-44e8-4e19-b7a5-97c4566cdd0c.png) nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Negative sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say we are building a CBOW model and we have a sentence *Birds are flying
    in the sky.* Let the context words be *birds*, *are*, *in*, and *the* and the
    target word be *flying.*
  prefs: []
  type: TYPE_NORMAL
- en: We need to update the weights of the network every time it predicts the incorrect
    target word. So, except for the word *flying*, if a different word is predicted
    as a target word, then we update the network.
  prefs: []
  type: TYPE_NORMAL
- en: But this is just a small set of vocabulary. Consider the case where we have
    millions of words in the vocabulary. In that case, we need to perform numerous
    weight updates until the network predict the correct target word. It is time-consuming
    and also not an efficient method. So, instead of doing this, we mark the correct
    target word as a positive class and sample a few words from the vocabulary and
    mark it as a negative class.
  prefs: []
  type: TYPE_NORMAL
- en: What we are essentially doing here is that we are converting our multinomial
    class problem to a binary classification problem (that is, instead of trying to
    predict the target word, the model classifies whether the given word is target
    word or not).
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability that the word is chosen as a negative sample is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eceda56e-380e-4bb8-8910-b96d059c4269.png)'
  prefs: []
  type: TYPE_IMG
- en: Subsampling frequent words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our corpus, there will be certain words that occur very frequently, such
    as *the*, *is*, and so on, and there are certain words that occur infrequently.
    To maintain a balance between these two, we use a subsampling technique. So, we
    remove the words that occur frequently more than a certain threshold with the
    probability ![](img/d0c8b060-74f7-4551-b98b-be70f5cf0e9f.png), and it can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58b3bab2-32ac-4361-98c7-003ca670032e.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/e0a0cb07-0547-4e52-a1ab-f0364c8ee42b.png) is the threshold and
    ![](img/3720374d-c131-4e69-9d11-5bf517a05b92.png) is the frequency of the word
    ![](img/f66887f6-2312-434a-8866-adfc1965a158.png).
  prefs: []
  type: TYPE_NORMAL
- en: Building the word2vec model using gensim
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have understood how the word2vec model works, let''s see how to
    build the word2vec model using the `gensim` library. Gensim is one of the popular
    scientific software packages widely used for building vector space models. It
    can be installed via `pip`. So, we can just type the following command in the
    terminal to install the `gensim` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have installed gensim, we will see how to build the word2vec model
    using that. You can download the dataset used in this section along with complete
    code with step by step explanation from GitHub at [http://bit.ly/2Xjndj4](http://bit.ly/2Xjndj4).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Loading the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what we got in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/120fcfd9-82fd-49ed-b5ec-ebcd744496be.png)'
  prefs: []
  type: TYPE_IMG
- en: Preprocessing and preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define a function for preprocessing the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how the preprocessed text looks like by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The genism library requires input in the form of a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '`*text = [ [word1, word2, word3], [word1, word2, word3] ]*`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that each row in our data contains a set of sentences. So, we split
    them by `''.''` and convert them into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, as shown, now, we have the data in a list. But we need to convert them
    into a list of lists. So, now again we split it by a space `'' ''`. That is, first,
    we split the data by `''.''` and then we split them by `'' ''` so that we can
    get our data in a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that we have our inputs in the form of a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the whole text in our dataset to a list of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown, we successfully converted the whole text in our dataset into a list
    of lists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, the problem we have is that our corpus contains only unigrams and it will
    not give us results when we give a bigram as an input, for example, *san francisco*.
  prefs: []
  type: TYPE_NORMAL
- en: So we use gensim's `Phrases` functions, which collects all the words that occur
    together and adds an underscore between them. So, now *san francisco* becomes
    *san_francisco*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set the `min_count` parameter to `25`, which implies that we ignore all
    the words and bigrams that appear less the `min_count`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, now, an underscore has been added to the bigrams in our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We check one more value from the corpus to see how an underscore is added for
    bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s build our model. Let''s define some of the important hyperparameters
    that our model needs:'
  prefs: []
  type: TYPE_NORMAL
- en: The `size` parameter represents the size of the vector, that is, dimensions
    of our vector, to represent a word. The size can be chosen according to our data
    size. If our data is very small, then we can set the size to a small value, but
    if we have a significantly large dataset, then we can set the size to `300`. In
    our case, we set the size to `100`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `window_size` parameter represents the distance that should be considered
    between the target word and its neighboring word. Words exceeding the window size
    from the target word will not be considered for learning. Typically, a small window
    size is preferred.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `min_count` parameter represents the minimum frequency of words. If the
    particular word's occurrence is less than a `min_count`, then we can simply ignore
    that word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `workers` parameter specifies the number of worker threads we need to train
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting `sg=1` implies that we use the skip-gram model for training, but if
    it is set to `sg=0`, then it implies that we use CBOW model for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define all the hyperparameters using following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the model using the `Word2Vec` function from gensim:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Once, we trained the model successfully, we save them. Saving and loading the
    model is very simple; we can simply use the `save` and `load` functions, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also `load` the already saved `Word2Vec` model by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating the embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now evaluate what our model has learned and how well our model has understood
    the semantics of the text. The `genism` library provides the `most_similar` function,
    which gives us the top similar words related to the given word.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following code, given `san_diego` as an input, we are
    getting all the other related city names that are most similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also apply arithmetic operations on our vectors to check how accurate
    our vectors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also find the words that do not match in the given set of words; for
    instance, in the following list called `text`, other than the word `holiday`,
    all others are city names. Since Word2Vec has understood this difference, it returns
    the word `holiday` as the one that does not match with the other words in the
    list as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing word embeddings in TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned how to build word2vec model for generating
    word embeddings using gensim. Now, we will see how to visualize those embeddings
    using TensorBoard. Visualizing word embeddings help us to understand the projection
    space and also helps us to easily validate the embeddings. TensorBoard provides
    us a built-in visualizer called the **embedding projector** for interactively
    visualizing and analyzing the high-dimensional data like our word embeddings.
    We will learn how can we use the TensorBoard's projector for visualizing the word
    embeddings step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the saved model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the model, we will save the number of words in our model to the
    `max_size` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that the dimension of word vectors will be ![](img/660327a5-a215-420a-a69c-d7f9dc635989.png).
    So, we initialize a matrix named `w2v` with the shape as our `max_size`, which
    is the vocabulary size, and the model''s first layer size, which is the number
    of neurons in the hidden layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a new file called `metadata.tsv`, where we save all the words
    in our model and we store the embedding of each word in the `w2v` matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we initialize the TensorFlow session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the TensorFlow variable called `embedding` that holds the word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an object to the `saver` class, which is actually used for saving and
    restoring variables to and from our checkpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `FileWriter`, we can save our summaries and events to our event file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we initialize the projectors and add the `embeddings`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we specify our `tensor_name` as `embedding` and `metadata_path` to the
    `metadata.tsv` file, where we have the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'And, finally, save the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open the terminal and type the following command to open the `tensorboard`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the TensorBoard is opened, go to the PROJECTOR tab. We can see the output,
    as shown in the following screenshot. As you can notice, when we type the word
    `delighted`, we can see all the related words, such as `pleasant`, `surprise`,
    and many more similar words, adjacent to that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48201f67-190b-45b6-bcbb-f5141142d804.png)'
  prefs: []
  type: TYPE_IMG
- en: Doc2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have seen how to generate embeddings for a word. But how can we generate
    the embeddings for a document? A naive method would be to compute a word vector
    for each word in the document and take an average of it. Mikilow and Le introduced
    a new method for generating the embeddings for documents instead of just taking
    the average of word embeddings. They introduced two new methods, called PV-DM
    and PV-DBOW. Both of these methods just add a new vector, called **paragraph id**.
    Let's see how exactly these two methods work.
  prefs: []
  type: TYPE_NORMAL
- en: Paragraph Vector â€“ Distributed Memory model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PV-DM is similar to the CBOW model, where we try to predict the target word
    given a context word. In PV-DM, along with word vectors, we introduce one more
    vector, called the paragraph vector. As the name suggests, the paragraph vector
    learns the vector representation of the whole paragraph and it captures the subject
    of the paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figure, each paragraph is mapped to a unique vector
    and each word is also mapped to a unique vector. So, in order to predict the target
    word, we combine the word vectors and paragraph vector by either concatenating
    or averaging them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46304231-6efb-4064-a6b3-309982ffa2a8.png)'
  prefs: []
  type: TYPE_IMG
- en: But having said all that, how is the paragraph vector useful in predicting the
    target word? What is really the use of having the paragraph vector? We know that
    we try to predict the target word based on the context words. Context words are
    of a fixed length and they are sampled within a sliding window from a paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Along with context words, we also make use of the paragraph vector for predicting
    the target word. Since the paragraph vector contains information about the subject
    of the paragraph, they contain meanings that the context words do not hold. That
    is, context word contains information about the particular words alone but the
    paragraph vector contains the information about the whole paragraph. So, we can
    think of the paragraph vector as a new word that is used along with context words
    for predicting the target word.
  prefs: []
  type: TYPE_NORMAL
- en: Paragraph vector is the same for all the context words sampled from the same
    paragraph and are not shared across paragraphs. Let's say that we have three paragraphs,
    *p1*, *p2*, and *p3*. If the context is sampled from a paragraph *p1*, then the
    *p1* vector is used to predict the target word. If a context is sampled from paragraph
    *p2,* then the *p2* vector is used. Thus, Paragraph vectors are not shared across
    paragraphs. However, word vectors are shared across all paragraphs. That is, the
    vector for the word *sun* is the same across all the paragraphs. We call our model
    as a distributed memory model of paragraph vectors, as our paragraph vectors serve
    as a memory that holds information that is missing from the current context words.
  prefs: []
  type: TYPE_NORMAL
- en: So, both of the paragraph vectors and word vectors are learned using stochastic
    gradient descent. On each iteration, we sample context words from a random paragraph,
    try to predict the target word, calculate the error, and update the parameters.
    After training, the paragraph vectors capture the embeddings of the paragraphs
    (documents).
  prefs: []
  type: TYPE_NORMAL
- en: Paragraph Vector â€“ Distributed Bag of Words model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PV-DBOW is similar to the skip-gram model, where we try to predict the context
    words based on the target word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e0ff5d7-de13-4d77-ba4c-0f997acb9883.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike previous methods, here we do not try to predict the next words. Instead,
    we use a paragraph vector to classify the words in the document. But how do they
    work? We train the model to understand whether the word belongs to a paragraph
    or not. We sample some set of words and then feed it to a classifier, which tells
    us whether the words belong to a particular paragraph or not, and in such a way
    we learn the paragraph vector.
  prefs: []
  type: TYPE_NORMAL
- en: Finding similar documents using doc2vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will see how to perform document classification using doc2vec. In this
    section, we will use the 20 `news_dataset`. It consists of 20,000 documents over
    20 different news categories. We will use only four categories: `Electronics`,
    `Politics`, `Science`, and `Sports`. So, we have 1,000 documents under each of
    these four categories. We rename the documents with a prefix, `category_`. For
    example, all science documents are renamed as `Science_1`, `Science_2`, and so
    on. After renaming them, we combine all the documents and place them in a single
    folder. The combined data, along with complete code is available at available
    as a Jupyter Notebook on GitHub at [http://bit.ly/2KgBWYv](http://bit.ly/2KgBWYv).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we train our doc2vec model to classify and find similarities between these
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import all the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we load all our documents and save the document names in the `docLabels`
    list and the document content in a list called `data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see in `docLabels` list we have all our documents'' names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a class called `DocIterator`, which acts as an iterator to run over
    all the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an object called `it` to the `DocIterator` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s build the model. Let''s first, define some of the important hyperparameters
    of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The `size` parameter represents our embedding size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `alpha` parameter represents our learning rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `min_alpha` parameter implies that our learning rate, `alpha`, will decay
    to `min_alpha` during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting `dm=1` implies that we use the distributed memory (PV-DM) model and
    if we set `dm=0`, it implies that we use the distributed bag of words (PV-DBOW)
    model for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `min_count` parameter represents the minimum frequency of words. If the
    particular word's occurrence is less than a `min_count`, than we can simply ignore
    that word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These hyperparameters are defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s define the model using `gensim.models.Doc2ec()` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we can save the model, using the `save` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can load the saved model, using the `load` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s evaluate our model''s performance. The following code shows that
    when we feed the `Sports_1.txt` document as an input, it will input all the related
    documents with the corresponding scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Understanding skip-thoughts algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Skip-thoughts is one of the popular unsupervised learning algorithms for learning
    the sentence embedding. We can see skip-thoughts as an analogy to the skip-gram
    model. We learned that in the skip-gram model, we try to predict the context word
    given a target word, whereas in skip-thoughts, we try to predict the context sentence
    given a target sentence. In other words, we can say that skip-gram is used for
    learning word-level vectors and skip-thoughts is used for learning sentence-level
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm of skip-thoughts is very simple. It consists of an encoder-decoder
    architecture. The role of the encoder is to map the sentence to a vector and the
    role of the decoder is to generate the surrounding sentences that is the previous
    and next sentence of the given input sentence. As shown in the following diagram,
    the skip-thoughts vector consists of one encoder and two decoders, called a previous
    decoder and next decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3a66dd3-7fd4-4146-96b1-bfa996767b07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The working of an encoder and decoder is discussed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: An encoder takes the words in a sentence sequentially and generates
    the embeddings. Let''s say we have a list of sentences. ![](img/9ff4d7cc-8397-4f89-97c7-3c5d7abdb882.png).
    ![](img/6fc4dd51-a29f-4f7b-aab1-7186033226df.png)denotes the ![](img/bef8d295-0f98-49f4-a410-1ddb325c8dcb.png)word
    in a sentence ![](img/519ba00b-ad85-4229-86f4-5f445cbcb6d0.png)and ![](img/007bc484-5995-4611-95b4-ce2cdfef0737.png)denotes
    its word embeddings. So the hidden state of an encoder is interpreted as a sentence
    representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: There are two decoders, called a previous decoder and next decoder.
    As the name suggests, the previous decoder is used to generate the previous sentence,
    and the next decoder is used to generate the next sentence. Let''s say we have
    a sentence ![](img/827fbe9b-c496-4d53-8496-661a034c727c.png)and its embeddings
    are ![](img/1eace04e-e015-42b2-8cfc-642b9c74520c.png). Both of the decoders take
    the embeddings ![](img/04276d5d-1738-4c0a-9641-2258494dc849.png)as an input and
    the previous decoder tries to generate the previous sentence, ![](img/b1ce0037-7f17-4bb4-b2d1-26262cfd1ba7.png),
    and the next decoder tries to generate the next sentence, ![](img/e7786bd8-d584-4f33-ae2c-413d6cf2ecb6.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we train our model by minimizing the reconstruction error of both the previous
    and next decoders. Because when the decoders reconstruct/generate correct previous
    and next sentences, it means that we have a meaningful sentence embedding ![](img/04276d5d-1738-4c0a-9641-2258494dc849.png).
    We send the reconstruction error to the encoder, so that encoder can optimize
    the embeddings and send better representations to the decoder. Once we have trained
    our model, we use our encoder to generate the embedding for a new sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Quick-thoughts for sentence embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quick-thoughts is another interesting algorithm for learning the sentence embeddings.
    In skip-thoughts, we saw how we used the encoder-decoder architecture to learn
    the sentence embeddings. In quick-thoughts, we try to learn whether a given sentence
    is related to the candidate sentence. So, instead of using a decoder, we use a
    classifier to learn whether a given input sentence is related to the candidate
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png) be the input sentence
    and ![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png) be the set of candidate
    sentences containing both valid context and invalid context sentences related
    to the given input sentence ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png).
    Let **![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png)** be any candidate sentence
    from the ![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png).
  prefs: []
  type: TYPE_NORMAL
- en: We use two encoding functions, ![](img/5ef11cca-cb04-4487-8662-f8e0b37bffcf.png)
    and ![](img/ccb8587f-cc34-49bb-97fd-a0b718b60350.png). The role of these two functions,
    ![](img/2d8df112-c7bf-4a59-aa08-d8d871e8451b.png) and ![](img/7b85920f-f7e4-434c-bd5e-e42bb0cb8022.png),
    is to learn the embeddings, that is, to learn the vector representations of a
    given sentence ![](img/155d14d5-772b-4d94-8817-d0ab1c63d1d6.png) and candidate
    sentence ![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Once these two functions generate the embeddings, we use a classifier ![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png),
    which returns the probability for each candidate sentence to be related to the
    given input sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figure, the probability of the second candidate sentence
    ![](img/414b1656-46a2-4012-925a-ba38a5b3680d.png) is high, as it is related to
    the given input sentence ![](img/4b7f2269-0f75-43d1-ae97-1ff5953cd9cd.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1111caf-a7dd-49d0-8727-6d5bf29d2906.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the probability that ![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)
    is a correct sentence ,that is, ![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)
    is related to the given input sentence ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png)
    is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c9a1c15-b8bc-46cf-892d-d508edf27ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png) is a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of our classifier is to identify the valid context sentence related
    to a given input sentence ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png). So,
    our cost function is to maximize the probability of finding the correct context
    sentence for the given input sentence ![](img/280b50e3-3e78-4898-98b4-408efed0901d.png).
    If it classifies the sentence correctly, then it means that our encoders learned
    the better representation of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding word embeddings and we looked at
    two different types of Word2Vec model, called CBOW, where we try to predict the
    target word given the context word, and skip-gram, where we try to predict the
    context word given the target word.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned about various training strategies in Word2Vec. We looked at
    hierarchical softmax, where we replace the output layer of the network with a
    Huffman binary tree and reduce the complexity to ![](img/9508f84c-0cd7-4394-a7f5-cd3f9879ccf9.png).
    We also learned about negative sampling and subsampling frequent word methods.
    Then we understood how to build the Word2Vec model using a gensim library and
    how to project the high-dimensional word embeddings to visualize them in TensorBoard.
    Going forward, we studied how the doc2vec model works with two types of doc2vec
    modelsâ€”PV-DM and PV-DBOW. Following this, we learned about the skip-thoughts model,
    where we learn the embedding of a sentence by predicting the previous and next
    sentences of the given sentence and we also explored the quick-thoughts model
    at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about generative models and how generative
    models are used to generate images.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our newly acquired knowledge by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the difference between the skip-gram and CBOW models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the loss function of the CBOW model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the need for negative sampling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define PV-DM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of the encoder and decoder in the skip-thoughts vector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are quick thoughts vector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Explore the following links to gain more insights into learning representation
    of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Distributed Representations of Words and Phrases and their Compositionality*,
    by Tomas Mikolov, et al., [https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distributed Representations of Sentences and Documents*, by Quoc Le and Tomas
    Mikolov, [https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Skip-thought Vectors*, by Ryan Kiros, et al., [https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*An Efficient Framework for Learning Sentence Representations*, by Lajanugen
    Logeswaran and Honglak Lee, [https://arxiv.org/pdf/1803.02893.pdf](https://arxiv.org/pdf/1803.02893.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
