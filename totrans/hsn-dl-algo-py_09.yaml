- en: Learning Text Representations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习文本表示
- en: Neural networks require inputs only in numbers. So when we have textual data,
    we convert them into numeric or vector representation and feed it to the network.
    There are various methods for converting the input text to numeric form. Some
    of the popular methods include **term frequency-inverse document frequency** (**tf-idf**),
    **bag of words (BOW)**, and so on. However, these methods do not capture the semantics
    of the word. This means that these methods will not understand the meaning of
    the words.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络只接受数字输入。因此，当我们有文本数据时，我们将其转换为数值或向量表示并将其馈送给网络。有多种方法可以将输入文本转换为数值形式。一些流行的方法包括**词频-逆文档频率**（**tf-idf**）、**词袋模型（BOW）**等。然而，这些方法不能捕捉单词的语义。这意味着这些方法不会理解单词的含义。
- en: In this chapter, we will learn about an algorithm called **word2vec** which
    converts the textual input to a meaningful vector. They learn the semantic vector
    representation for each word in the given input text. We will start off the chapter
    by understanding about word2vec model and two different types of word2vec model
    called **continuous bag-of-words** (**CBOW**) and skip-gram model. Next, we will
    learn how to build word2vec model using gensim library and how to visualize high
    dimensional word embeddings in tensorboard.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一个称为**word2vec**的算法，它将文本输入转换为有意义的向量。它们学习给定输入文本中每个单词的语义向量表示。我们将从理解word2vec模型开始，并了解两种不同类型的word2vec模型，即**连续词袋模型**（**CBOW**）和skip-gram模型。接下来，我们将学习如何使用gensim库构建word2vec模型以及如何在tensorboard中可视化高维词嵌入。
- en: Going ahead, we will learn about **doc2vec** model which is used for learning
    the representations for a document. We will understand two different methods in
    doc2vec called **Paragraph Vector -** **Distributed Memory** **Model** (**PV-DM**)
    and **Paragraph Vector -** **Distributed Bag of Words** (**PV-DBOW**). We will
    also see how to perform document classification using doc2vec. At the end of the
    chapter, we will learn about skip-thoughts algorithms and quick thoughts algorithm
    which is used for learning the sentence representations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习**doc2vec**模型，它用于学习文档的表示。我们将了解doc2vec中的两种不同方法，称为**段落向量 - 分布式内存模型**（**PV-DM**）和**段落向量
    - 分布式词袋模型**（**PV-DBOW**）。我们还将看到如何使用doc2vec进行文档分类。在本章的最后，我们将学习关于skip-thoughts算法和quick
    thoughts算法，这两者都用于学习句子表示。
- en: 'In this chapter, we will understand the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将了解以下主题：
- en: The word2vec model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec 模型
- en: Building a word2vec model using gensim
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用gensim构建word2vec模型
- en: Visualizing word embeddings in TensorBoard
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorBoard中可视化词嵌入
- en: Doc2vec model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doc2vec 模型
- en: Finding similar documents using doc2vec
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用doc2vec找到相似文档
- en: Skip-thoughts
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-thoughts
- en: Quick-thoughts
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quick-thoughts
- en: Understanding the word2vec model
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解word2vec模型
- en: Word2vec is one of the most popular and widely used models for generating the
    word embeddings. What are word embeddings though? Word embeddings are the vector
    representations of words in a vector space. The embedding generated by the word2vec
    model captures the syntactic and semantic meanings of a word. Having a meaningful
    vector representation of a word helps the neural network to understand the word
    better.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 是生成词嵌入最流行和广泛使用的模型之一。那么什么是词嵌入？词嵌入是词在向量空间中的向量表示。word2vec模型生成的嵌入捕捉了词的句法和语义含义。拥有一个有意义的词向量表示有助于神经网络更好地理解单词。
- en: 'For instance, let''s consider the following text: *Archie used to live in New
    York, he then moved to Santa Clara. He loves apples and strawberries.*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑以下文本：*Archie used to live in New York, he then moved to Santa Clara.
    He loves apples and strawberries.*
- en: 'Word2vec model generates the vector representation for each of the words in
    the preceding text. If we project and visualize the vectors in embedding space,
    we can see how all the similar words are plotted close together. As you can see
    in the following figure, words *apples* and *strawberries* are plotted close together,
    and *New York* and *Santa Clara* are plotted close together. They are plotted
    close together because the word2vec model has learned that *apples* and *strawberries*
    are similar entities that is, fruits and *New York* and *Santa Clara* are similar
    entities, that is *cities*, and so their vectors (embeddings) are similar to each
    other, and which is why the distance between them is less:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 模型为前文中的每个单词生成向量表示。如果我们在嵌入空间中投影和可视化这些向量，我们可以看到所有相似的单词都被放在一起。如下图所示，单词*apples*和*strawberries*被放在一起，*New
    York*和*Santa Clara*也被放在一起。它们被放在一起是因为 word2vec 模型学习到了*apples*和*strawberries*是相似的实体，即水果，以及*New
    York*和*Santa Clara*是相似的实体，即城市，因此它们的向量（嵌入）相似，因此它们之间的距离较小：
- en: '![](img/30025401-9037-42e2-a7c7-09ebeeb6caa0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30025401-9037-42e2-a7c7-09ebeeb6caa0.png)'
- en: Thus, with word2vec model, we can learn the meaningful vector representation
    of a word which helps the neural networks to understand what the word is about.
    Having a good representation of a word would be useful in various tasks. Since
    our network can understand the contextual and syntactic meaning of words, this
    will branch out to various use cases such as text summarization, sentiment analysis,
    text generation, and more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过 word2vec 模型，我们可以学习单词的有意义的向量表示，这有助于神经网络理解单词的含义。拥有一个良好的单词表示在各种任务中都非常有用。由于我们的网络能够理解单词的上下文和语法含义，这将分支到各种用例，如文本摘要、情感分析、文本生成等等。
- en: 'Okay. But how do the word2vec model learn the word embeddings? There are two
    types of word2vec models for learning the embeddings of a word:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。但是 word2vec 模型是如何学习单词嵌入的呢？有两种类型的 word2vec 模型用于学习单词的嵌入：
- en: CBOW model
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CBOW 模型
- en: Skip-gram model
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Skip-gram 模型
- en: We will go into detail and learn how each of these models learns the vector
    representations of a word.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细介绍并学习这些模型如何学习单词的向量表示。
- en: Understanding the CBOW model
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 CBOW 模型
- en: Let's say we have a neural network with an input layer, a hidden layer, and
    an output layer. The goal of the network is to predict a word given its surrounding
    words. The word that we are trying to predict is called the **target word** and
    the words surrounding the target word are called the **context words**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个神经网络，包括一个输入层、一个隐藏层和一个输出层。网络的目标是预测给定其周围单词的一个单词。我们试图预测的单词称为**目标单词**，周围的单词称为**上下文单词**。
- en: How many context words do we use to predict the target word? We use a window
    of size ![](img/565e5e2c-349b-4dec-8198-86027ebd3b89.png) to choose the context
    word. If the window size is 2, then we use two words before and two words after
    the target word as the context words.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用大小为 ![](img/565e5e2c-349b-4dec-8198-86027ebd3b89.png) 的窗口来选择上下文单词，以预测目标单词。如果窗口大小为2，则使用目标单词前两个单词和后两个单词作为上下文单词。
- en: 'Let''s consider the sentence, *The Sun rises in the east* with the word *rises*
    as the target word. If we set the window size as 2, then we take the words *the*
    and *sun,* which are the two words before, and *in* and *the* which are the two
    words after the target word *rises* as context words, as shown in the following
    figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑句子*The Sun rises in the east*，以*rise*作为目标单词。如果我们将窗口大小设为2，则我们取目标单词*rise*之前的两个单词*the*和*sun*，以及之后的两个单词*in*和*the*作为上下文单词，如下图所示：
- en: '![](img/d67de55c-90c2-4457-a746-c9427430741e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d67de55c-90c2-4457-a746-c9427430741e.png)'
- en: 'So the input to the network is context words and the output is a target word.
    How do we feed these inputs to the network? The neural network accepts only numeric
    input so we cannot feed the raw context words directly as an input to the network.
    Hence, we convert all the words in the given sentence into a numeric form using
    the one-hot encoding technique, as shown in the following figure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络的输入是上下文单词，输出是目标单词。我们如何将这些输入馈送到网络中？神经网络只接受数值输入，因此我们不能直接将原始上下文单词作为网络的输入。因此，我们使用一种称为一热编码的技术将给定句子中的所有单词转换为数值形式，如下图所示：
- en: '![](img/50b341fe-78f6-4142-8858-caec284b7d07.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50b341fe-78f6-4142-8858-caec284b7d07.png)'
- en: 'The architecture of the CBOW model is shown in the following figure. As you
    can see, we feed the context words, *the, sun, in*, and *the,* as inputs to the
    network and it predicts the target word *rises* a*s* an output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW模型的结构如下图所示。您可以看到，我们将上下文单词*the, sun, in*和*the,*作为输入送入网络，它预测出*the, sun, in*和*the,*的目标单词*rises*和*an*：
- en: '![](img/38a8ac22-8330-4477-9692-b04fe3326bf6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38a8ac22-8330-4477-9692-b04fe3326bf6.png)'
- en: In the initial iteration, the network cannot predict the target word correctly.
    But over a series of iterations, it learns to predict the correct target word
    using gradient descent. With gradient descent, we update the weights of the network
    and find the optimal weights with which we can predict the correct target word.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始迭代中，网络无法正确预测目标单词。但经过一系列迭代，它学会了使用梯度下降预测正确的目标单词。通过梯度下降，我们更新网络的权重，并找到能够预测正确目标单词的最优权重。
- en: 'As we have one input, one hidden, and one output layer, as shown in the preceding
    figure, we will have two weights:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的图所示，我们有一个输入层，一个隐藏层和一个输出层，因此我们将有两组权重：
- en: Input layer to hidden layer weight, ![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层到隐藏层的权重，![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
- en: Hidden layer to output layer weight, ![](img/dfbb80c7-0625-43b1-9d59-c3f84e0fa17e.png)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层到输出层的权重，![](img/dfbb80c7-0625-43b1-9d59-c3f84e0fa17e.png)
- en: During the training process, the network will try to find the optimal values
    for these two sets of weights so that it can predict the correct target word.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，网络将尝试找到这两组权重的最优值，以便能够预测正确的目标单词。
- en: It turns out that the optimal weights between the input to a hidden layer ![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
    forms the vector representation of words. They basically constitute the semantic
    meaning of the words. So, after training, we simply remove the output layer and
    take the weights between the input and hidden layers and assign them to the corresponding
    words.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，输入到隐藏层的最优权重![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)形成了单词的向量表示。它们基本上构成了单词的语义含义。因此，在训练后，我们只需移除输出层，并取输入层和隐藏层之间的权重，并将其分配给相应的单词。
- en: 'After training, if we look at the ![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)
    matrix, it represents the embeddings for each of the words. So, the embedding
    for the word *sun* is [0.0, 0.3,0.3,0.6,0.1 ]:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，如果我们查看![](img/662d04d5-e524-4deb-b62d-88965f3729d1.png)矩阵，它表示每个单词的嵌入。因此，单词*sun*的嵌入是[0.0,
    0.3, 0.3, 0.6, 0.1]：
- en: '![](img/2ae49ea5-39d7-46be-bbfd-b06845fb7491.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ae49ea5-39d7-46be-bbfd-b06845fb7491.png)'
- en: Thus, the CBOW model learns to predict the target word with the given context
    words. It learns to predict the correct target word using gradient descent. During
    training, it updates the weights of the network through gradient descent and finds
    the optimal weights with which we can predict the correct target word. The optimal
    weights between the input and hidden layers form the vector representations of
    a word. So, after training, we simply take the weights between the input and hidden
    layers and assign them as a vector to the corresponding words.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CBOW模型学会了使用给定的上下文单词预测目标单词。它通过梯度下降学习预测正确的目标单词。在训练过程中，它通过梯度下降更新网络的权重，并找到能够预测正确目标单词的最优权重。输入层和隐藏层之间的最优权重形成了单词的向量表示。因此，在训练后，我们只需取输入层和隐藏层之间的权重，并将其分配为相应单词的向量。
- en: Now that we have an intuitive understanding of the CBOW model, we will go into
    detail and learn mathematically how exactly the word embeddings are computed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对CBOW模型有了直观的理解，我们将详细学习数学上如何计算单词嵌入的过程。
- en: We learned that weights between the input and the hidden layers basically form
    the vector representation of the words. But how exactly does the CBOW model predicts
    the target word? How does it learn the optimal weights using backpropagation?
    Let's look at this in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到了输入层和隐藏层之间的权重基本上形成了单词的向量表示。但CBOW模型如何准确预测目标单词呢？它如何使用反向传播学习最优权重？让我们在下一节中探讨这个问题。
- en: CBOW with a single context word
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CBOW模型使用单个上下文单词
- en: We learned that, in the CBOW model, we try to predict the target word given
    the context words, so it takes some ![](img/1be066cd-9ae6-4c55-8ea4-bc2e5f280d8e.png)
    number of context words as an input and returns one target word as an output.
    In CBOW model with a single context word, we will have only one context word,
    that is, ![](img/f767c138-9bb5-4bc2-b598-8c19caab3d5b.png). So, the network takes
    only one context word as an input and returns one target word as an output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在CBOW模型中，我们试图根据上下文词预测目标词，因此它以一定数量的上下文词 ![](img/1be066cd-9ae6-4c55-8ea4-bc2e5f280d8e.png)
    作为输入，并返回一个目标词作为输出。在只有一个上下文词的CBOW模型中，我们只有一个上下文词，即 ![](img/f767c138-9bb5-4bc2-b598-8c19caab3d5b.png)。因此，网络只接受一个上下文词作为输入，并返回一个目标词作为输出。
- en: Before going ahead, first, let's familiarize ourselves with the notations. All
    the unique words we have in our corpus is called the **vocabulary**. Considering
    the example we saw in the *Understanding the CBOW model* section, we have five
    unique words in the sentence—*the*, *sun*, *rises*, *in*, and *east*. These five
    words are our vocabulary.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，首先让我们熟悉一下符号。我们语料库中的所有唯一词称为**词汇表**。考虑我们在*理解CBOW模型*部分看到的例子，句子中有五个唯一词——*the*、*sun*、*rises*、*in*
    和 *east*。这五个词是我们的词汇表。
- en: 'Let ![](img/28e68adf-0cf3-41d7-b313-250b1a777f93.png)denote the size of the
    vocabulary (that is, number of words) and ![](img/5395fd7f-8249-4750-afec-9c901d8a7f3c.png)
    denotes the number of neurons in the hidden layer. We learned that we have one
    input, one hidden, and one output layer:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![](img/28e68adf-0cf3-41d7-b313-250b1a777f93.png) 表示词汇表的大小（即词的数量），![](img/5395fd7f-8249-4750-afec-9c901d8a7f3c.png)
    表示隐藏层中神经元的数量。我们学到我们有一个输入层、一个隐藏层和一个输出层：
- en: The input layer is represented by ![](img/10bb5dbd-fdf0-4699-8297-d12490cf3480.png).
    When we say ![](img/70f5dd90-b264-4e2a-9132-1df2e29c5974.png), it represents the
    ![](img/24d01c97-ea8f-4a0c-8a4c-a1c95e5543fe.png) input word in the vocabulary.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层由 ![](img/10bb5dbd-fdf0-4699-8297-d12490cf3480.png) 表示。当我们说 ![](img/70f5dd90-b264-4e2a-9132-1df2e29c5974.png)
    时，它表示词汇表中的第 ![](img/24d01c97-ea8f-4a0c-8a4c-a1c95e5543fe.png) 个输入词。
- en: The hidden layer is represented by ![](img/d4db72ad-4b63-4e36-95bd-4bed53452038.png).
    When we say ![](img/55fb0590-0038-4c33-a828-f1e509469be6.png), it represents the
    ![](img/0c4beba7-b212-4130-b23c-5891a33120e2.png)neuron in the hidden layer.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层由 ![](img/d4db72ad-4b63-4e36-95bd-4bed53452038.png) 表示。当我们说 ![](img/55fb0590-0038-4c33-a828-f1e509469be6.png)
    时，表示隐藏层中的第 ![](img/0c4beba7-b212-4130-b23c-5891a33120e2.png) 个神经元。
- en: Output layer is represented by ![](img/bf19185d-47ce-48e8-a5b9-8abe0d83f9be.png).
    When we say ![](img/1707ff27-1948-48c5-9986-3760050ca4d2.png) it represents the
    ![](img/c7991abb-c811-4e4f-b9d0-d150baeb0300.png)output word in the vocabulary.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层由 ![](img/bf19185d-47ce-48e8-a5b9-8abe0d83f9be.png) 表示。当我们说 ![](img/1707ff27-1948-48c5-9986-3760050ca4d2.png)
    时，它表示词汇表中的第 ![](img/c7991abb-c811-4e4f-b9d0-d150baeb0300.png) 个输出词。
- en: 'The dimension of input to hidden layer weight ![](img/16c17c70-d122-4dd9-8efc-180583d17de6.png)
    is ![](img/93436413-de5f-4eed-be42-4804aa6e9c51.png) (which is the *size of our
    vocabulary x the number of neurons in the hidden layer*) and the dimension of
    hidden to output layer weight, ![](img/053566fc-39df-4ac1-be14-0504c8e951f0.png)
    is ![](img/0938958c-85c1-4482-8438-b0c890b67e05.png) (that is, the *number of
    neurons in the hidden layer x the size of the vocabulary*). The representation
    of the elements of the matrix is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到隐藏层权重的维度 ![](img/16c17c70-d122-4dd9-8efc-180583d17de6.png) 是 ![](img/93436413-de5f-4eed-be42-4804aa6e9c51.png)（即*我们词汇表的大小乘以隐藏层神经元的数量*），隐藏到输出层权重的维度
    ![](img/053566fc-39df-4ac1-be14-0504c8e951f0.png) 是 ![](img/0938958c-85c1-4482-8438-b0c890b67e05.png)（即*隐藏层神经元的数量乘以词汇表的大小*）。矩阵元素的表示如下：
- en: '![](img/88598b9c-17d3-48e2-bf20-f4d173924600.png) represents an element in
    the matrix between node ![](img/4312bbef-3b4c-4994-8b2d-383de32644dc.png) of the
    input layer and node ![](img/cd0d6d00-0870-44d0-8273-d6d21b5bc57a.png) of the
    hidden layer'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/88598b9c-17d3-48e2-bf20-f4d173924600.png) 表示输入层节点 ![](img/4312bbef-3b4c-4994-8b2d-383de32644dc.png)
    到隐藏层节点 ![](img/cd0d6d00-0870-44d0-8273-d6d21b5bc57a.png) 之间矩阵中的一个元素。'
- en: '![](img/625fd42e-d7ca-4a5c-9878-aec1f414b10b.png) represents an element in
    the matrix between node ![](img/95948634-5121-4bb3-ac04-87b3ce8a417b.png) of the
    hidden layer and node ![](img/efe5da3d-88d4-4aef-9e6e-cd8787aa0008.png) of the
    output layer'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/625fd42e-d7ca-4a5c-9878-aec1f414b10b.png) 表示隐藏层节点 ![](img/95948634-5121-4bb3-ac04-87b3ce8a417b.png)
    到输出层节点 ![](img/efe5da3d-88d4-4aef-9e6e-cd8787aa0008.png) 之间矩阵中的一个元素。'
- en: 'The following figure will help us to attain clarity on the notations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下图将帮助我们更清楚地理解这些符号：
- en: '![](img/8deba7bd-3467-46a7-bcff-38e287a52b68.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8deba7bd-3467-46a7-bcff-38e287a52b68.png)'
- en: Forward propagation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: In order to predict target words given a context word, we need to perform forward
    propagation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测给定上下文单词的目标单词，我们需要执行前向传播。
- en: 'First, we multiply the input ![](img/b59575fa-9574-4b08-847f-c60923c09f48.png)
    with the input to hidden layer weight ![](img/42869efe-315b-4bb8-acd5-cfba8e46acb6.png):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入![](img/b59575fa-9574-4b08-847f-c60923c09f48.png)与输入到隐藏层权重![](img/42869efe-315b-4bb8-acd5-cfba8e46acb6.png)相乘：
- en: '![](img/b08663e6-7e08-4cc9-a0c6-77ce38474012.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b08663e6-7e08-4cc9-a0c6-77ce38474012.png)'
- en: 'We know each of the input words is one-hot encoded, so when we multiply ![](img/ddf8720e-6c3e-4ade-9245-d78a0e6714bd.png)
    with ![](img/fd23003b-f971-47e3-8bfb-4cab40fff647.png), we basically obtain the
    ![](img/b1d3087b-3f76-4ff0-ad4a-1b1b76a3e242.png) row of ![](img/6bf8629c-5577-4e5e-a744-09a8328af6c0.png)
    to ![](img/d27e266c-0b00-4cf2-94aa-31dd74ffd685.png). So, we can directly write
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道每个输入单词都是独热编码的，因此当我们将![](img/ddf8720e-6c3e-4ade-9245-d78a0e6714bd.png)与![](img/fd23003b-f971-47e3-8bfb-4cab40fff647.png)相乘时，我们基本上得到了![](img/b1d3087b-3f76-4ff0-ad4a-1b1b76a3e242.png)行的![](img/6bf8629c-5577-4e5e-a744-09a8328af6c0.png)到![](img/d27e266c-0b00-4cf2-94aa-31dd74ffd685.png)的向量表示。因此，我们可以直接写成如下形式：
- en: '![](img/772add17-ac4f-4bbe-a9fb-ca263fa739eb.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/772add17-ac4f-4bbe-a9fb-ca263fa739eb.png)'
- en: '![](img/0f435fb3-2560-4c5d-9863-7b4647a5a058.png) basically implies the vector
    representation of the input word. Let''s denote the vector representation for
    the input word ![](img/1e51dde8-5816-498b-8e09-bd5038617a9d.png) by ![](img/4831b672-87ba-497d-8b52-5e80d9098041.png).
    So, the previous equation can be written as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/0f435fb3-2560-4c5d-9863-7b4647a5a058.png)基本上意味着输入单词的向量表示。让我们用![](img/4831b672-87ba-497d-8b52-5e80d9098041.png)表示输入单词![](img/1e51dde8-5816-498b-8e09-bd5038617a9d.png)的向量表示。因此，前述方程可以写成如下形式：'
- en: '![](img/14c7bb66-a44c-40c8-b990-51b6b8bc7fc4.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14c7bb66-a44c-40c8-b990-51b6b8bc7fc4.png)'
- en: Now we are in the hidden layer ![](img/4acc897c-0c04-4a77-8527-6e382bdb80f8.png)
    and we have another set of weight, which is hidden to output layer weight, ![](img/fe6b3a36-a507-41df-9cc2-a6aeffa627d4.png).
    We know that we have ![](img/64f5782d-47fe-4555-96c5-d5b489ad2acf.png) number
    of words in our vocabulary and we need to compute the probability for each of
    the words in our vocabulary to be the target word.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们处于隐藏层![](img/4acc897c-0c04-4a77-8527-6e382bdb80f8.png)，我们有另一组权重，即隐藏到输出层的权重，![](img/fe6b3a36-a507-41df-9cc2-a6aeffa627d4.png)。我们知道词汇表中有![](img/64f5782d-47fe-4555-96c5-d5b489ad2acf.png)个单词，我们需要计算词汇表中每个单词作为目标单词的概率。
- en: 'Let ![](img/b2ebffb6-d301-46a8-bfa5-4fcf004cfc00.png) denote the score for
    the ![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)word in our vocabulary to
    be a target word. The score ![](img/86f338d1-6f8e-4058-9df1-30d71bbeb2c4.png)
    is computed by multiplying the value of hidden layer ![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)
    and the hidden to output layer weight ![](img/bea30eb3-8cc3-46aa-9be7-ca17659e4f14.png).
    Since we are calculating the score for a word ![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png),
    we multiply the hidden layer ![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)
    with the ![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png) column of the matrix,
    ![](img/e0101955-901b-4d4a-9027-a00033476af1.png):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让![](img/b2ebffb6-d301-46a8-bfa5-4fcf004cfc00.png)表示我们词汇表中单词![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)成为目标单词的分数。分数![](img/86f338d1-6f8e-4058-9df1-30d71bbeb2c4.png)通过将隐藏层值![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)与隐藏到输出层权重![](img/bea30eb3-8cc3-46aa-9be7-ca17659e4f14.png)相乘来计算。由于我们正在计算单词![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png)的分数，我们将隐藏层![](img/73775ba1-d2ca-49fe-9681-7d397a7e97a0.png)与矩阵的![](img/e0101955-901b-4d4a-9027-a00033476af1.png)列相乘：
- en: '![](img/2454457a-0a5e-4451-90ad-2b1909c822ab.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2454457a-0a5e-4451-90ad-2b1909c822ab.png)'
- en: 'The ![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png) column of the weight
    matrix ![](img/e0101955-901b-4d4a-9027-a00033476af1.png) basically denotes the
    vector representation of the word ![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png).
    Let''s denote the vector representation of the ![](img/9ef9f1db-e40c-410e-ab64-dc8166e5406b.png)
    word by ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png). So, the preceding equation
    can be written as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵的![](img/4b807e72-e345-453e-a78c-bf14b788ddc2.png)列基本上表示单词![](img/2c437c53-80d9-4115-9efa-4f47205e37ba.png)的向量表示。我们用![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png)来表示该单词的向量表示。因此，前述方程可以写成如下形式：
- en: '![](img/f5984dac-97c6-4384-ab34-eed4b8989cb5.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5984dac-97c6-4384-ab34-eed4b8989cb5.png)'
- en: 'Substituting equation *(1)* in equation *(2)*, we can write the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程*(1)*代入方程*(2)*，我们可以写成如下形式：
- en: '![](img/8454782c-a236-49f6-a8be-29b282f72385.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8454782c-a236-49f6-a8be-29b282f72385.png)'
- en: Can you infer what the preceding equation is trying to say? We are basically
    computing the dot product between the input context word representation, ![](img/b1fc0aac-ccdb-40d1-a876-afe56832946b.png),
    and the representation of ![](img/7c4ef403-fb01-45cb-ad55-474e50a656be.png) word
    in our vocabulary, ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你能推断出前述方程式试图表达什么吗？我们基本上在计算输入上下文词表示 ![](img/b1fc0aac-ccdb-40d1-a876-afe56832946b.png)
    与我们词汇表中词 ![](img/7c4ef403-fb01-45cb-ad55-474e50a656be.png) 表示之间的点积。
- en: Computing the dot product between any two vectors helps us to understand how
    similar they are. Hence, computing the dot product between ![](img/54377cf3-2f02-4baa-a8a3-b7c4c818d703.png)
    and ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png) tells us how similar the
    ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png) word in our vocabulary is to
    the input context word. Thus, when the score for a ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png)
    word in the vocabulary , ![](img/e1af9677-c6f2-466f-bb02-6dcbce530271.png) is
    high, then it implies that the word ![](img/b7ea99e1-97ed-411e-8593-1eff0dea4578.png)
    is similar to the given input word and it is the target word. Similarly, when
    the score for a ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png) word in the
    vocabulary, ![](img/a00856ab-ad98-4502-8f0a-49340d60e79d.png), is low, then it
    implies that the word ![](img/7d1306a6-05a2-4227-addf-8ed05aec706d.png) is not
    similar to the given input word and it is not the target word.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 计算任意两个向量之间的点积有助于我们理解它们有多相似。因此，计算 ![](img/54377cf3-2f02-4baa-a8a3-b7c4c818d703.png)
    和 ![](img/efe6a642-947a-410e-994a-d298b5ab1e70.png) 的点积告诉我们词汇表中的 ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png)
    词与输入上下文词有多相似。因此，当词汇表中 ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png) 词的分数 ![](img/e1af9677-c6f2-466f-bb02-6dcbce530271.png)
    高时，意味着该词 ![](img/b7ea99e1-97ed-411e-8593-1eff0dea4578.png) 与给定的输入词相似且是目标词。同样地，当词汇表中
    ![](img/4de293ce-4181-4416-9548-7d2d3cfced60.png) 词的分数 ![](img/a00856ab-ad98-4502-8f0a-49340d60e79d.png)
    低时，意味着该词 ![](img/7d1306a6-05a2-4227-addf-8ed05aec706d.png) 不与给定的输入词相似且不是目标词。
- en: Thus, ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png) basically gives us the
    score for a word ![](img/facc6cb1-db9d-4ecf-a60e-28a4fd152571.png) to be the target
    word. But instead of having ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)
    as a raw score, we convert them into probabilities. We know that the softmax function
    squashes values between 0 to 1, so we can use the softmax function for converting
    ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png) into the probability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/f1bd5535-dded-486c-b909-b4c06128042d.png) 基本上给出了单词 ![](img/facc6cb1-db9d-4ecf-a60e-28a4fd152571.png)
    成为目标词的分数。但是我们不使用原始分数 ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)，而是将它们转换为概率。我们知道
    softmax 函数将值压缩在 0 到 1 之间，因此我们可以使用 softmax 函数将 ![](img/f1bd5535-dded-486c-b909-b4c06128042d.png)
    转换为概率。
- en: 'We can write our output as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将输出写成如下形式：
- en: '![](img/55d7d9f4-6b63-48e6-8891-d6c3ac30236e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55d7d9f4-6b63-48e6-8891-d6c3ac30236e.png)'
- en: Here, ![](img/c571759f-1422-406d-9713-80ca2e80320e.png) tells us the probability
    for a word ![](img/bb81e391-269b-4e9d-b719-c6a562784121.png) to the target word
    given an input context word. We compute the probability for all the words in our
    vocabulary and select the word that has a high probability as the target word.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/c571759f-1422-406d-9713-80ca2e80320e.png) 告诉我们给定输入上下文词时，单词 ![](img/bb81e391-269b-4e9d-b719-c6a562784121.png)
    是目标词的概率。我们计算我们词汇表中所有单词的概率，并选择具有高概率的单词作为目标词。
- en: Okay, what is our objective function? that is, how do we compute the loss?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们的目标函数是什么？也就是说，我们如何计算损失？
- en: 'Our goal is to find the correct target word. Let ![](img/ee7a65a2-78a3-48fb-8473-375153d20017.png)
    denote the probability of the correct target word. So, we need to maximize this
    probability:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到正确的目标词。让 ![](img/ee7a65a2-78a3-48fb-8473-375153d20017.png) 表示正确目标词的概率。因此，我们需要最大化这个概率：
- en: '![](img/3e04e42c-0178-48ac-a12a-611cf58a1ba6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e04e42c-0178-48ac-a12a-611cf58a1ba6.png)'
- en: 'Instead of maximizing the raw probabilities, we maximize the log probabilities:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 不是最大化原始概率，而是最大化对数概率：
- en: '![](img/62e56fe1-4a68-4389-aa0a-597fbf1e35ae.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62e56fe1-4a68-4389-aa0a-597fbf1e35ae.png)'
- en: But why do we want to maximize the log probability instead of the raw probability?
    Because machines have limitations in representing a floating point of a fraction
    and when we multiply many probabilities, it will lead to a value that is infinitely
    small. So, to avoid that, we use log probabilities and it will ensure numerical
    stability.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么我们要最大化对数概率而不是原始概率？因为机器在表示分数的浮点数时存在限制，当我们乘以许多概率时，会导致一个无限小的值。因此，为了避免这种情况，我们使用对数概率，这将确保数值稳定性。
- en: 'Now we have a maximization objective, we need to convert this to a minimization
    objective so that we can apply our favorite gradient descent algorithm for minimizing
    the objective function. How can we change our maximization objective to the minimization
    objective? We can do that by simply adding the negative sign. So our objective
    function becomes the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个最大化目标，我们需要将其转换为最小化目标，以便应用我们喜爱的梯度下降算法来最小化目标函数。我们如何将我们的最大化目标转换为最小化目标？我们可以通过简单地添加负号来做到这一点。因此，我们的目标函数变为以下形式：
- en: '![](img/802c0bdc-a33b-4eab-9a82-36152c243dd9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/802c0bdc-a33b-4eab-9a82-36152c243dd9.png)'
- en: 'The loss function can be given as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数可以表示如下：
- en: '![](img/ccf589ba-6418-4f6b-94bd-610879c30b62.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccf589ba-6418-4f6b-94bd-610879c30b62.png)'
- en: 'Substituting equation *(3)* in equation *(4)*, we get the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 *(3)* 替换到方程 *(4)* 中，我们得到以下结果：
- en: '![](img/f75fdf2f-9b33-4340-979b-a69612d2568d.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f75fdf2f-9b33-4340-979b-a69612d2568d.png)'
- en: 'According to the logarithm quotient rule, *log(a/b) = log(a) - log(b)*, we
    can rewrite the previous equation as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对数商规则，*log(a/b) = log(a) - log(b)*，我们可以将上一个方程重写如下：
- en: '![](img/24907d6f-88c6-4b8d-a094-0f962c167621.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24907d6f-88c6-4b8d-a094-0f962c167621.png)'
- en: '![](img/602521e1-4cca-47e3-b589-217278f67647.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/602521e1-4cca-47e3-b589-217278f67647.png)'
- en: 'We know that *log* and *exp* cancel each other, so we can cancel the *log*
    and *exp* in the first term and our final loss function becomes the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 *log* 和 *exp* 互相抵消，因此我们可以在第一项中取消 *log* 和 *exp*，因此我们最终的损失函数变为以下形式：
- en: '![](img/15049d3f-2a48-497c-bbf4-e9c1d6b877d0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15049d3f-2a48-497c-bbf4-e9c1d6b877d0.png)'
- en: Backward propagation
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'We minimize the loss function using the gradient descent algorithm. So, we
    backpropagate the network, calculate the gradient of the loss function with respect
    to weights, and update the weights. We have two sets of weights, input to hidden
    layer weight ![](img/a97130e6-b8ec-4bf8-b44c-cfb431e50af0.png) and hidden to output
    layer weights ![](img/25402288-f99a-46b3-9409-5fc1ad0130a9.png). We calculate
    gradients of loss with respect to both of these weights and update them according
    to the weight update rule:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用梯度下降算法来最小化损失函数。因此，我们进行网络反向传播，计算损失函数对权重的梯度，并更新权重。我们有两组权重，从输入到隐藏层的权重 ![](img/a97130e6-b8ec-4bf8-b44c-cfb431e50af0.png)，以及从隐藏到输出层的权重
    ![](img/25402288-f99a-46b3-9409-5fc1ad0130a9.png)。我们计算损失相对于这些权重的梯度，并根据权重更新规则更新它们。
- en: '![](img/1b6c6b97-6b29-4494-aaee-2d46ad552afa.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b6c6b97-6b29-4494-aaee-2d46ad552afa.png)'
- en: '![](img/6fda6f1d-81a1-4361-9f7c-55da60e9de05.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fda6f1d-81a1-4361-9f7c-55da60e9de05.png)'
- en: 'In order to better understand the backpropagation, let''s recollect the steps
    involved in the forward propagation:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解反向传播，让我们回顾一下正向传播中涉及的步骤：
- en: '![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png)'
- en: '![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png)'
- en: '![](img/5a7d022d-3be1-404c-a4dc-9bb4c72ad987.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a7d022d-3be1-404c-a4dc-9bb4c72ad987.png)'
- en: 'First, we compute the gradients of loss with respect to the hidden to output
    layer ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png). We cannot calculate the
    gradient of loss ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png) with respect
    to ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png) directly from ![](img/6ccca083-758a-4ec5-ae2b-e48ea51a46f7.png),
    as there is no ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png) term in the loss
    function ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png), so we apply the chain
    rule as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算损失对隐藏到输出层的权重 ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png) 的梯度。我们无法直接从损失函数
    ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png) 中直接计算损失对 ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png)
    的梯度，因此我们应用链式法则如下：
- en: '![](img/8cb9185c-2894-40bd-96fc-9c8b4ad92600.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cb9185c-2894-40bd-96fc-9c8b4ad92600.png)'
- en: Please refer to the equations of forward propagation to understand how derivatives
    are calculated.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考正向传播的方程以了解如何计算导数。
- en: 'The derivative of the first term is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第一项的导数如下所示：
- en: '![](img/64182944-a208-4c28-957a-246af8a54e18.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64182944-a208-4c28-957a-246af8a54e18.png)'
- en: Here, ![](img/29f4b902-e2cd-4949-88e6-40e4e4755cb2.png) is the error term, which
    is the difference between the actual word and predicted word.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/29f4b902-e2cd-4949-88e6-40e4e4755cb2.png)是误差项，即实际单词与预测单词之间的差异。
- en: Now, we will calculate the derivative of the second term.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算第二项的导数。
- en: 'Since we know ![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png)：
- en: '**![](img/65ed56d8-8fd2-421f-a888-85926a972ec6.png)**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/65ed56d8-8fd2-421f-a888-85926a972ec6.png)**'
- en: 'Thus, the **gradient of loss ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)
    with respect to ![](img/e6a825b7-8282-4168-bbc8-df61ede3413b.png)** is given as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，损失函数关于![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)的**梯度**如下所示：
- en: '![](img/d556d3b2-2bbb-4fba-bfe7-79c8115d3b2b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d556d3b2-2bbb-4fba-bfe7-79c8115d3b2b.png)'
- en: 'Now, we compute the gradient with respect to the input to hidden layer weight
    ![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png). We cannot calculate the derivative
    directly from ![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png), as there is no
    ![](img/f3a8d544-9c42-4f28-93e3-a3be04ec4cea.png) term in ![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png),
    so we apply the chain rule as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算关于隐藏层输入到输出的权重![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png)的梯度。我们不能直接从![](img/43ecfc92-17e8-4610-b1bf-54550923efcc.png)计算![](img/f3a8d544-9c42-4f28-93e3-a3be04ec4cea.png)的导数，因此我们应用链式法则如下：
- en: '![](img/0e103418-91ce-41e4-84bb-481386f8170f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e103418-91ce-41e4-84bb-481386f8170f.png)'
- en: 'In order to compute the derivative of the first term in the preceding equation,
    we again apply the chain rule, as we cannot compute the derivative of ![](img/3814d69c-7064-4f02-82b2-75db5faba6cd.png)
    with respect to ![](img/9785cb31-f498-41ff-919a-40eb62a0853f.png) directly from
    ![](img/bc927050-b111-4bb2-a438-bd78614fcd8d.png):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算上述方程中第一项的导数，我们再次应用链式法则，因为我们不能直接从![](img/bc927050-b111-4bb2-a438-bd78614fcd8d.png)计算![](img/3814d69c-7064-4f02-82b2-75db5faba6cd.png)关于![](img/9785cb31-f498-41ff-919a-40eb62a0853f.png)的导数：
- en: '![](img/df7ae6d8-1595-46b2-86c4-ae9315cb4965.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df7ae6d8-1595-46b2-86c4-ae9315cb4965.png)'
- en: 'From equation *(5)*, we can write:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程*(5)*，我们可以写成：
- en: '![](img/fc911a96-65f1-4cba-8891-3906670cf422.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc911a96-65f1-4cba-8891-3906670cf422.png)'
- en: 'Since we know ![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道![](img/b8e6fb04-ba14-4311-9213-efe537f7d5f0.png)：
- en: '![](img/f1b8ba1a-1ea5-4d87-a495-220e5c316ccd.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1b8ba1a-1ea5-4d87-a495-220e5c316ccd.png)'
- en: 'Instead of having the sum, we can write:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与其对求和，我们可以写成：
- en: '![](img/a700b684-1d54-49b2-9058-1bafb30d9f98.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a700b684-1d54-49b2-9058-1bafb30d9f98.png)'
- en: '![](img/be3580f6-a59d-49c0-8ce0-becdb699e0b8.png) denotes the sum of the output
    vector of all words in the vocabulary, weighted by their prediction error.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/be3580f6-a59d-49c0-8ce0-becdb699e0b8.png)表示加权后所有词汇表中的输出向量之和。'
- en: Let's now calculate the derivative of the second term.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们计算第二项的导数。
- en: 'Since we know, ![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道，![](img/8b5acbdb-48e1-41dd-baf2-c5a65157a656.png)：
- en: '![](img/ad2b53dd-8193-4b02-9366-77fb060d676b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad2b53dd-8193-4b02-9366-77fb060d676b.png)'
- en: 'Thus, the **gradient of loss ![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)
    with respect to ![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png)** is given as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，损失函数关于![](img/a3bce5bb-b087-427e-8922-d0a1959ed411.png)和![](img/1e7596aa-79a8-4cbc-a91a-64a0e80bf9b0.png)的**梯度**如下所示：
- en: '![](img/f33f4252-bd4e-4c03-bca5-6b760b09221f.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f33f4252-bd4e-4c03-bca5-6b760b09221f.png)'
- en: 'So, our weight update equation becomes the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的权重更新方程如下：
- en: '![](img/8ec5dabb-8b62-406f-8345-41e3407e8e15.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ec5dabb-8b62-406f-8345-41e3407e8e15.png)'
- en: '![](img/8a80e3b8-52a7-438c-9a8d-d277a7cefc07.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a80e3b8-52a7-438c-9a8d-d277a7cefc07.png)'
- en: We update the weights of our network using the preceding equation and obtain
    an optimal weights during training. The optimal input to hidden layer weight,
    ![](img/2bd9fff6-c950-4bba-afa7-2fcb03f4c13e.png), becomes the vector representation
    for the words in our vocabulary.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用上述方程更新网络的权重，并在训练期间获得最佳权重。隐藏层输入到输出的最优权重![](img/2bd9fff6-c950-4bba-afa7-2fcb03f4c13e.png)，成为我们词汇表中单词的向量表示。
- en: 'The Python code for `Single_context_CBOW` is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`Single_context_CBOW`的Python代码如下：'
- en: '[PRE0]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: CBOW with multiple context words
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 含有多个上下文词的CBOW
- en: 'Now that we understood how the CBOW model works with a single word as a context,
    we will see how it will work when you have multiple words as context words. The
    architecture of CBOW with multiple input words as a context is shown in the following
    figure:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了 CBOW 模型如何处理单个单词作为上下文时，我们将看看在多个单词作为上下文单词时它是如何工作的。具有多个输入单词作为上下文的 CBOW
    架构显示在以下图中：
- en: '![](img/d8067832-9cf4-4d07-951b-6ad2f0720d48.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8067832-9cf4-4d07-951b-6ad2f0720d48.png)'
- en: 'There is not much difference between the multiple words as a context and a
    single word as a context. The difference is that, with multiple contexts words
    as inputs, we take the average of all the input context words. That is, as a first
    step, we forward propagate the network and compute the value of ![](img/ba4894e5-f69c-48e4-ad6b-7deb10ff2e41.png)
    by multiplying input ![](img/239da2e3-155c-48ba-bb32-71d50b821224.png) and weights
    ![](img/52646ca3-7984-4386-b57e-0dd3c053db60.png), as we saw in the *CBOW with
    a single context word* section:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 多个单词作为上下文与单个单词作为上下文之间没有太大差别。不同之处在于，当有多个上下文单词作为输入时，我们取所有输入上下文单词的平均值。也就是说，作为第一步，我们向前传播网络并通过乘以输入
    ![](img/239da2e3-155c-48ba-bb32-71d50b821224.png) 和权重 ![](img/52646ca3-7984-4386-b57e-0dd3c053db60.png)
    来计算 ![](img/ba4894e5-f69c-48e4-ad6b-7deb10ff2e41.png)，就像我们在 *单个上下文单词的 CBOW* 部分看到的一样：
- en: '![](img/ff073da4-ac16-4bd6-9137-26007776bf56.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff073da4-ac16-4bd6-9137-26007776bf56.png)'
- en: 'But, here, since we have multiple context words, we will have multiple inputs
    (that is ![](img/7f74c8cb-6fd0-4033-b097-374c7277c5e3.png)), where ![](img/b17c0fb1-83ff-4a04-8e14-3ff7cc39688f.png)
    is the number of context words, and we simply take the average of them and multiply
    with the weight matrix, shown as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于我们有多个上下文单词，我们将有多个输入（即 ![](img/7f74c8cb-6fd0-4033-b097-374c7277c5e3.png)），其中
    ![](img/b17c0fb1-83ff-4a04-8e14-3ff7cc39688f.png) 是上下文单词的数量，我们简单地取它们的平均值，并与权重矩阵相乘，如下所示：
- en: '![](img/21dee3f2-48b0-48ad-a685-3cf220a5cea8.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21dee3f2-48b0-48ad-a685-3cf220a5cea8.png)'
- en: '![](img/79ec7be5-7da6-485f-9a99-5d69c0ebd372.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79ec7be5-7da6-485f-9a99-5d69c0ebd372.png)'
- en: Similar to what we learned in the *CBOW with single context word* section, ![](img/37ec8731-2be1-4f09-8eef-07cf1731d4f2.png)
    represents the vector representation of the input context word ![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png).
    ![](img/591d1ae7-16d0-468c-84b9-db6ce8a5a1cd.png) represents the vector representation
    of the input word ![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png), and so on.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在 *单个上下文单词的 CBOW* 部分学到的，![](img/37ec8731-2be1-4f09-8eef-07cf1731d4f2.png)
    表示输入上下文单词 ![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png) 的向量表示。![](img/591d1ae7-16d0-468c-84b9-db6ce8a5a1cd.png)
    表示输入单词 ![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png) 的向量表示，依此类推。
- en: 'We denote the representation of the input context word ![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png)
    by ![](img/09ff6572-5c5b-41a9-8116-494b59b704e8.png), the representation of the
    input context word ![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png) by ![](img/bfe95f24-ad89-44b6-bd71-d5d4145a9541.png),
    and so on. So, we can rewrite the preceding equation as:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入上下文单词 ![](img/b6235131-45be-42ad-be4b-854cc46b4aa1.png) 的表示称为 ![](img/09ff6572-5c5b-41a9-8116-494b59b704e8.png)，输入上下文单词
    ![](img/5f24fc7e-9191-4105-add5-0fc640497cbe.png) 的表示称为 ![](img/bfe95f24-ad89-44b6-bd71-d5d4145a9541.png)，以此类推。因此，我们可以将前述方程重写为：
- en: '![](img/84b27cb4-4d63-4717-a014-15aa30a2cff9.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84b27cb4-4d63-4717-a014-15aa30a2cff9.png)'
- en: Here, ![](img/ee8ad712-1053-455a-a687-887767c82a94.png) represents the number
    of context words.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/ee8ad712-1053-455a-a687-887767c82a94.png) 表示上下文单词的数量。
- en: 'Computing the value of ![](img/a19f3259-021d-4713-be65-ffb8cfeb8f8a.png) is
    the same as we saw in the previous section:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 ![](img/a19f3259-021d-4713-be65-ffb8cfeb8f8a.png) 的值与我们在前一节中看到的相同：
- en: '![](img/cae8ddb8-09da-423d-b76c-7dcaf67e9e15.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cae8ddb8-09da-423d-b76c-7dcaf67e9e15.png)'
- en: Here, ![](img/f2c71772-b8c8-4e08-85a5-81cd43f8e129.png) denotes the vector representation
    of the ![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png) word in the vocabulary.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/f2c71772-b8c8-4e08-85a5-81cd43f8e129.png) 表示词汇中的 ![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png)
    的向量表示。
- en: 'Substituting equation *(6)* in equation *(7)*, we write the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程 *(6)* 替换到方程 *(7)* 中，我们得到如下内容：
- en: '![](img/26f9683b-8929-4579-be93-0e9b4e3d1b99.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26f9683b-8929-4579-be93-0e9b4e3d1b99.png)'
- en: The preceding equation gives us the similarity between the ![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png)
    word in the vocabulary and the average representations of given input context
    words.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程给出了词汇中的 ![](img/debbebe9-f140-4728-99d9-8c20d4e21070.png) 与给定输入上下文单词的平均表示之间的相似度。
- en: 'The loss function is the same as we saw in the single word context and it is
    given as:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数与我们在单词上下文中看到的相同，如下所示：
- en: '![](img/b9a1c6a1-2d8e-49ad-a12a-4e6ac59c4e14.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9a1c6a1-2d8e-49ad-a12a-4e6ac59c4e14.png)'
- en: 'Now, there is a small difference in backpropagation. We know that in backpropagation
    we compute gradients and update our weights according to the weight update rule.
    Recall that, in the previous section, this is how we update the weights:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在反向传播中有一个小小的区别。我们知道，在反向传播中，我们计算梯度并根据权重更新规则更新我们的权重。回想一下，在前面的章节中，这是我们更新权重的方式：
- en: '![](img/b12d13c2-602e-474c-b900-3691b46e703c.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b12d13c2-602e-474c-b900-3691b46e703c.png)'
- en: '![](img/35ac212c-b0b5-4a0e-98e4-320b0e9e2e90.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35ac212c-b0b5-4a0e-98e4-320b0e9e2e90.png)'
- en: 'Since, here, we have multiple context words as an input, we take an average
    of context words while computing ![](img/ca797f1c-a327-4135-b840-9a76c4a8e94d.png):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里我们有多个上下文词作为输入，在计算![](img/ca797f1c-a327-4135-b840-9a76c4a8e94d.png)时，我们取上下文词的平均值：
- en: '![](img/15493e8e-84c3-43f6-84e0-e8d06230a8a4.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15493e8e-84c3-43f6-84e0-e8d06230a8a4.png)'
- en: 'Computing ![](img/c64c8923-7b7f-428f-960e-96b2caa8f052.png)is the same as we
    saw in the previous section:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 计算![](img/c64c8923-7b7f-428f-960e-96b2caa8f052.png)与我们在上一节中看到的方式相同：
- en: '![](img/7b604533-b018-41f3-ab17-968e367fc5fa.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b604533-b018-41f3-ab17-968e367fc5fa.png)'
- en: So, in a nutshell, in the multi-word context, we just take the average of multiple
    context input words and build the model as we did in the single word context of
    CBOW.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在多词上下文中，我们只需取多个上下文输入词的平均值，并像在CBOW的单词上下文中那样构建模型。
- en: Understanding skip-gram model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解跳字模型
- en: 'Now, let''s look at another interesting type of the word2vec model, called
    skip-gram. Skip-gram is just the reverse of the CBOW model,. That is in a skip-gram
    model, we try to predict the context words given the target word as an input.
    As shown in the following figure, we can notice that we have the target word as
    *rises* and we need to predict the context words *the, sun, in*, and *the*:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看另一种有趣的word2vec模型类型，称为跳字模型。跳字模型只是CBOW模型的反向。也就是说，在跳字模型中，我们试图预测给定目标词的上下文词。正如下图所示，我们可以注意到我们有目标词为*rises*，我们需要预测的上下文词为*the,
    sun, in*和*the*：
- en: '![](img/870ba64d-fdef-49ca-890c-1accb0b47366.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/870ba64d-fdef-49ca-890c-1accb0b47366.png)'
- en: Similar to the CBOW model, we use the window size to determine how many context
    words we need to predict. The architecture of the skip-gram model is shown in
    the following figure.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 与CBOW模型类似，我们使用窗口大小来确定需要预测多少上下文单词。跳字模型的架构如下图所示。
- en: 'As we can see that it takes the single target word as input and tries to predict
    the multiple context words:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，它以单个目标词作为输入，并尝试预测多个上下文词：
- en: '![](img/af0c565a-b398-4b1b-bb7a-0a2e5fb818e9.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af0c565a-b398-4b1b-bb7a-0a2e5fb818e9.png)'
- en: In the skip-gram model, we try to predict the context words based on the target
    word. So, it takes one target word as an input and returns ![](img/91936d04-a92a-47b2-808c-3e1389753372.png)
    context words as output, as shown in the above figure. So, after training the
    skip-gram model to predict the context words, the weights between our input to
    hidden layer ![](img/5f024499-4591-477e-9667-e24e23d802fd.png) becomes the vector
    representation of the words, just like we saw in the CBOW model.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳字模型中，我们试图基于目标词预测上下文词。因此，它以一个目标词作为输入，并返回![](img/91936d04-a92a-47b2-808c-3e1389753372.png)上下文词作为输出，如上图所示。因此，在训练跳字模型以预测上下文词之后，我们输入到隐藏层之间的权重![](img/5f024499-4591-477e-9667-e24e23d802fd.png)成为词的向量表示，就像我们在CBOW模型中看到的那样。
- en: Now that we have a basic understanding of the skip-gram model, let us dive into
    detail and learn how they work.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对跳字模型有了基本的理解，让我们深入细节，学习它们是如何工作的。
- en: Forward propagation in skip-gram
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型中的前向传播
- en: 'First, we will understand how forward propagation works in the skip-gram model.
    Let''s use the same notations we used in the CBOW model. The architecture of the
    skip-gram model is shown in the following figure. As you can see, we feed only
    one target word ![](img/60664943-7c5b-41c9-b69c-38d707ad0e14.png) as an input
    and it returns the ![](img/7073a847-9879-4511-a235-aab29edb2b44.png) context words
    as an output ![](img/1abce1f4-baae-4202-9e58-c3b08e0c7698.png):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解跳字模型中的前向传播如何工作。让我们使用我们在CBOW模型中使用的相同符号。跳字模型的架构如下图所示。正如你所看到的，我们只传入一个目标词![](img/60664943-7c5b-41c9-b69c-38d707ad0e14.png)作为输入，并返回![](img/7073a847-9879-4511-a235-aab29edb2b44.png)上下文词作为输出![](img/1abce1f4-baae-4202-9e58-c3b08e0c7698.png)：
- en: '![](img/552225d9-108c-46a4-9b72-dc8895415c23.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/552225d9-108c-46a4-9b72-dc8895415c23.png)'
- en: 'Similar to what we saw in CBOW, in the *Forward propagation* section, first
    we multiply our input ![](img/e1fd5da0-c909-4186-b9dd-53dee6356719.png) with the
    input to hidden layer weights ![](img/8e685519-eb49-4b24-9f91-b9ddacc3eae8.png):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们在CBOW中看到的，在*前向传播*部分，首先我们将输入 ![](img/e1fd5da0-c909-4186-b9dd-53dee6356719.png)
    与输入到隐藏层的权重 ![](img/8e685519-eb49-4b24-9f91-b9ddacc3eae8.png) 相乘：
- en: '![](img/abe3c86e-c276-456d-ae31-f1760fa3416e.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abe3c86e-c276-456d-ae31-f1760fa3416e.png)'
- en: 'We can directly rewrite the preceding equation as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接将上述方程重写为：
- en: '![](img/7439c251-2328-4c30-8ef3-efa342622a35.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7439c251-2328-4c30-8ef3-efa342622a35.png)'
- en: Here, ![](img/a79e9b8a-f7f9-497b-b60a-ff4f80021260.png) implies the vector representation
    for the input word ![](img/3c25d777-af88-49fa-a1ba-4de48ad5b12f.png).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/a79e9b8a-f7f9-497b-b60a-ff4f80021260.png) 暗示着输入单词 ![](img/3c25d777-af88-49fa-a1ba-4de48ad5b12f.png)
    的向量表示。
- en: 'Next, we compute ![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png), which implies
    a similarity score between the word ![](img/fdb95f62-9fad-4e54-bdec-1a085f9cc023.png)
    word in our vocabulary and the input target word. Similar to what we saw in the
    CBOW model, ![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png) can be given as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算 ![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png)，这意味着我们的词汇中单词 ![](img/fdb95f62-9fad-4e54-bdec-1a085f9cc023.png)
    与输入目标单词之间的相似性分数。类似于CBOW模型中看到的那样，![](img/31bc65cb-80f7-453a-81cf-8248e0c8db91.png)
    可以表示为：
- en: '![](img/60fdc55d-bbab-4355-8cc6-b1d6abfbf7dc.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60fdc55d-bbab-4355-8cc6-b1d6abfbf7dc.png)'
- en: 'We can directly rewrite the above equation as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接将上述方程重写为：
- en: '![](img/c57122ec-3d10-48a7-bd65-abb5f5aa4519.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c57122ec-3d10-48a7-bd65-abb5f5aa4519.png)'
- en: Here, ![](img/1ccd2510-7ece-4313-b6dd-dbd119cd2dc2.png) implies the vector representation
    of the word ![](img/040097ca-cb43-4dab-94ef-290e17b125d0.png).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/1ccd2510-7ece-4313-b6dd-dbd119cd2dc2.png) 暗示着词汇中的 ![](img/040097ca-cb43-4dab-94ef-290e17b125d0.png)
    的向量表示。
- en: 'But, unlike the CBOW model where we just predicted the one target word, here
    we are predicting the ![](img/1a0af03f-a854-4368-9990-43aeec26fd66.png) number
    of context words. So, we can rewrite the above equation as:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，与CBOW模型不同，我们不仅预测一个目标单词，而是预测 ![](img/1a0af03f-a854-4368-9990-43aeec26fd66.png)
    个上下文单词。因此，我们可以将上述方程重写为：
- en: '![](img/f5131630-008b-437d-b8e4-b808d21baebc.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5131630-008b-437d-b8e4-b808d21baebc.png)'
- en: 'Thus, ![](img/e6ff7f32-90bd-453e-8df9-76411762aa7b.png) implies the score for
    the ![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png) word in the vocabulary to
    be the context word ![](img/e61144a0-5581-48d9-a1b0-8fd115c5f4d1.png). That is:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/e6ff7f32-90bd-453e-8df9-76411762aa7b.png) 暗示着词汇中的 ![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png)
    单词得分将作为上下文单词 ![](img/e61144a0-5581-48d9-a1b0-8fd115c5f4d1.png)。
- en: '![](img/0cea83ee-9d5b-47ef-b2ef-bf952c60401d.png) implies the score for the
    word ![](img/b28be818-fb99-49f2-9932-dd39f8c7a8b8.png)to be the first context
    word'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/0cea83ee-9d5b-47ef-b2ef-bf952c60401d.png) 暗示着单词 ![](img/b28be818-fb99-49f2-9932-dd39f8c7a8b8.png)
    的得分将作为第一个上下文单词'
- en: '![](img/8edb41b3-3b2f-4c0a-9ec3-1def443cfd5c.png) implies the score for the
    word ![](img/23dade3d-bf24-4335-976a-08795e347b89.png) to be the second context
    word'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8edb41b3-3b2f-4c0a-9ec3-1def443cfd5c.png) 暗示着单词 ![](img/23dade3d-bf24-4335-976a-08795e347b89.png)
    的得分将作为第二个上下文单词'
- en: '![](img/2dbc1d19-b16b-43a5-923b-9a8695fbbdda.png) implies the score for the
    word ![](img/2049eb6f-1418-41db-a43f-0dc3ea3cb896.png) to be the third context
    word'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/2dbc1d19-b16b-43a5-923b-9a8695fbbdda.png) 暗示着单词 ![](img/2049eb6f-1418-41db-a43f-0dc3ea3cb896.png)
    的得分将作为第三个上下文单词'
- en: 'And since we want to convert our scores to probabilities, we apply the softmax
    function and compute ![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望将得分转换为概率，我们应用softmax函数并计算 ![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png)：
- en: '![](img/830ca969-b68f-4f5c-942d-4c7d2ab8eba4.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/830ca969-b68f-4f5c-942d-4c7d2ab8eba4.png)'
- en: Here, ![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png) implies the probability
    of the ![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png) word in the vocabulary
    to be the context word ![](img/8df6187b-ea84-4630-b2bb-99f3513ec3a1.png).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/b9e66e5e-c83e-452d-9ff2-8244fa6d4b3e.png) 暗示着词汇中的 ![](img/6f3eacb9-617d-48be-997b-9222c5697c97.png)
    单词成为上下文单词 ![](img/8df6187b-ea84-4630-b2bb-99f3513ec3a1.png) 的概率。
- en: 'Now, let us see how to compute the loss function. Let ![](img/85f31ebd-9b63-4b6a-a2bd-074745b99561.png)
    denote the probability of the correct context word. So, we need to maximize this
    probability:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何计算损失函数。让 ![](img/85f31ebd-9b63-4b6a-a2bd-074745b99561.png) 表示正确上下文单词的概率。因此，我们需要最大化这个概率：
- en: '![](img/c6abc546-b4ee-4b41-abea-1d9e0e3ae7ef.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6abc546-b4ee-4b41-abea-1d9e0e3ae7ef.png)'
- en: 'Instead of maximizing raw probabilities, maximize the log probabilities:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 不是最大化原始概率，而是最大化对数概率：
- en: '![](img/749e5fbc-9416-46bb-8e62-aa1836828d68.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/749e5fbc-9416-46bb-8e62-aa1836828d68.png)'
- en: 'Similar to what we saw in the CBOW model, we convert this into the minimization
    objective function by adding the negative sign:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97c83a18-fdcb-408d-87f0-dcd3a45889d0.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(8)* in the preceding equation, we can write the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f437214e-7a5f-4717-82ae-a94798058816.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Since we have ![](img/2e76d1e7-1b07-4196-8292-82464decc20f.png) context words,
    we take the product sum of the probabilities as:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbe9479d-e864-4e77-a76b-a0161b23dadc.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: 'So, according to logarithm rules, we can rewrite the above equation and our
    final loss function becomes:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a98abd9e-b5bc-4c3f-80ca-ec8b73504bc4.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Look at the loss function of the CBOW and skip-gram models. You'll notice that
    the only difference between the CBOW loss function and skip-gram loss function
    is the addition of the context word ![](img/1ac9a6b6-fc2e-4a8d-b43d-0f5fd7016ceb.png).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Backward propagation
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We minimize the loss function using the gradient descent algorithm. So, we backpropagate
    the network, calculate the gradient of the loss function with respect to weights,
    and update the weights according to the weight update rule.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the gradient of loss with respect to hidden to output layer
    ![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png). We cannot calculate the derivative
    of loss with respect to ![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png) directly
    from ![](img/bc366eee-0a8a-4feb-8986-4d7ee848fd5b.png) as it has no ![](img/5575ad1c-031f-4b3d-943d-9fe1ff092277.png)
    term in it, so we apply the chain rule as shown below. It is basically the same
    as what we saw in the CBOW model, except that here we sum over all the context
    words:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/152f51e3-e4c6-455e-a9a5-5db71a134958.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: 'First, let''s compute the first term:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/217ea4cc-4208-4abc-858a-ec9639c90c62.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'We know that ![](img/78f049e9-aa2c-4b6e-be4e-e6d530469692.png) is the error
    term, which is the difference between the actual word and the predicted word.
    For notation simplicity, we can write this sum over all the context words as:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/806b7879-6d72-42c7-9ae4-90bf16dcd0a9.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'So, we can say that:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4512aed6-0806-43f4-bd04-1bd927e25737.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compute the second term. Since we know ![](img/04ffe61d-f457-440f-b2ee-065d0de515f4.png),
    we can write:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/146d13a2-d5ea-4031-b930-5da86abe06cf.png)**'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the **gradient of loss ![](img/50d48bac-c87c-4686-b1c0-b7c9ee29dc1e.png)
    with respect to ![](img/864f4d04-3109-43ff-ba54-2309fe6318d5.png)** is given as
    follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f716372-88a2-4c88-91e8-3d423577e498.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
- en: 'Now, we compute the gradient of loss with respect to the input to hidden layer
    weight ![](img/abb5c13a-1b06-4ce6-9535-8069ca7bad9f.png). It is simple and exactly
    same as we saw in the CBOW model:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26ac31ee-0c19-4154-8bc7-2dab5be38e1a.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the **gradient of loss ![](img/50d48bac-c87c-4686-b1c0-b7c9ee29dc1e.png)
    with respect to ![](img/abb5c13a-1b06-4ce6-9535-8069ca7bad9f.png)** is given as:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73a7b000-52d0-44d6-ae90-2ba997439cb3.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73a7b000-52d0-44d6-ae90-2ba997439cb3.png)'
- en: 'After computing the gradients, we update our weights *W* and *W''* as:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度后，我们更新我们的权重*W*和*W'*为：
- en: '![](img/77a93b00-0f6a-485b-9213-2f053d348185.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77a93b00-0f6a-485b-9213-2f053d348185.png)'
- en: '![](img/a1a480ec-be9c-4f1f-a546-9ae95f156091.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1a480ec-be9c-4f1f-a546-9ae95f156091.png)'
- en: Thus, while training the network, we update the weights of our network using
    the preceding equation and obtain optimal weights. The optimal weight between
    the input to hidden layer, ![](img/8006cddb-a1a5-414d-bf56-cb77755ba238.png) becomes
    the vector representation for the words in our vocabulary.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练网络时，我们使用前述方程更新网络的权重，并获得最优权重。输入到隐藏层之间的最优权重，![](img/8006cddb-a1a5-414d-bf56-cb77755ba238.png)，成为我们词汇表中单词的向量表示。
- en: Various training strategies
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 各种训练策略
- en: Now, we will look at different training strategies which can optimize and increase
    the efficiency of our word2vec model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看一些不同的训练策略，这些策略可以优化和提高我们的word2vec模型的效率。
- en: Hierarchical softmax
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层softmax
- en: 'In both the CBOW and skip-gram models, we used the softmax function for computing
    the probability of the occurrence of a word. But computing the probability using
    the softmax function is computationally expensive. Say, we are building a CBOW
    model; we compute the probability of the ![](img/ba9c9d7a-046a-46c2-ad93-3c770306ac88.png)
    word in our vocabulary to be the target word as:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在CBOW和skip-gram模型中，我们使用softmax函数计算单词出现的概率。但使用softmax函数计算概率是计算上昂贵的。比如，在构建CBOW模型时，我们计算目标词![](img/ba9c9d7a-046a-46c2-ad93-3c770306ac88.png)在我们词汇表中作为目标词的概率为：
- en: '![](img/9679ecaa-45cf-4bce-8b85-21ed6c323ba1.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9679ecaa-45cf-4bce-8b85-21ed6c323ba1.png)'
- en: If you look at the preceding equation, we are basically driving the exponent
    of the ![](img/3551b1d9-5ce6-4433-92ef-5a593c9e6068.png) with the exponent of
    all the words ![](img/31854ccf-fede-424d-818f-305ac15843bd.png) in the vocabulary.
    Our complexity would be ![](img/87cf4f3a-baf5-491e-b80b-e45540ec7feb.png), where
    ![](img/d7d624f5-bf6a-4a35-ae3e-059a99240350.png) is the vocabulary size. When
    we train the word2vec model with a vocabulary comprising millions of words, it
    is definitely going to be computationally expensive. So, to combat this problem,
    instead of using the softmax function, we use the hierarchical softmax function.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看前面的方程，我们基本上驱动指数![](img/3551b1d9-5ce6-4433-92ef-5a593c9e6068.png)与词汇表中所有单词![](img/31854ccf-fede-424d-818f-305ac15843bd.png)的指数。我们的复杂度将是![](img/87cf4f3a-baf5-491e-b80b-e45540ec7feb.png)，其中![](img/d7d624f5-bf6a-4a35-ae3e-059a99240350.png)是词汇表的大小。当我们用包含数百万单词的词汇表训练word2vec模型时，这显然会变得计算上昂贵。因此，为了解决这个问题，我们不使用softmax函数，而是使用分层softmax函数。
- en: 'The hierarchical softmax function uses a Huffman binary search tree and significantly
    reduces the complexity to ![](img/638cfce0-635f-44e9-be0a-1e0d5d99f46e.png). As
    shown in the following diagram, in hierarchical softmax, we replace the output
    layer with a binary search tree:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 分层softmax函数使用霍夫曼二叉搜索树，将复杂度显著降低到![](img/638cfce0-635f-44e9-be0a-1e0d5d99f46e.png)。如下图所示，在分层softmax中，我们用二叉搜索树替换输出层：
- en: '![](img/ff41b350-161f-4d98-86c1-52519381e659.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff41b350-161f-4d98-86c1-52519381e659.png)'
- en: Each leaf node in the tree represents a word in the vocabulary and all the intermediate
    nodes represent the relative probability of their child node.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 树中的每个叶子节点代表词汇表中的一个单词，所有中间节点表示它们子节点的相对概率。
- en: 'How do we compute the probability of a target word given a context word? We
    simply traverse the tree by making a decision whether to turn left or right. As
    shown in the following figure, the probability of the word *flew* to be the target
    word, given some context word ![](img/839fbeee-e568-440f-be47-d080e80da01e.png),
    is computed as a product of the probabilities along the path:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如何计算给定上下文单词的目标单词的概率？我们简单地遍历树，根据需要向左或向右转向。如下图所示，给定一些上下文单词![](img/839fbeee-e568-440f-be47-d080e80da01e)，单词*flew*成为目标单词的概率计算为沿路径的概率乘积：
- en: '![](img/6af5744d-76c7-4328-9ecd-fd0fe9b5395f.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6af5744d-76c7-4328-9ecd-fd0fe9b5395f.png)'
- en: '![](img/5b801e9f-5594-41b0-b3ea-791da2013858.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b801e9f-5594-41b0-b3ea-791da2013858.png)'
- en: 'The probability of the target word is shown as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 目标词的概率如下所示：
- en: '![](img/dc53fb4a-6996-483f-be03-86885b53298e.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc53fb4a-6996-483f-be03-86885b53298e.png)'
- en: 'But how do we compute these probabilities? Each node ![](img/f85382f5-7066-4de6-bfa9-a1248991e99a.png)
    has an embedding associated with it (say, ![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png)).
    To compute the probability for a node, we multiply the node''s embedding ![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png)
    with hidden layer value ![](img/80974a3f-5565-4985-81de-08a900f927c7.png) and
    apply a sigmoid function. For instance, the probability of a node ![](img/cc7a19fa-e393-4c69-9b3d-6b9a485cd2ef.png)
    to take a right, given a context word ![](img/4d992bd5-1e88-477b-a9fd-31bc407d6b3c.png),
    is computed as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们如何计算这些概率呢？每个节点 ![](img/f85382f5-7066-4de6-bfa9-a1248991e99a.png) 都与一个嵌入相关联（比如，![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png)）。要计算一个节点的概率，我们将节点的嵌入
    ![](img/87b82004-c9a2-40d0-9a94-3c7bd6541269.png) 与隐藏层的值 ![](img/80974a3f-5565-4985-81de-08a900f927c7.png)
    相乘，并应用 sigmoid 函数。例如，给定上下文词 ![](img/4d992bd5-1e88-477b-a9fd-31bc407d6b3c.png)，节点
    ![](img/cc7a19fa-e393-4c69-9b3d-6b9a485cd2ef.png) 右侧的概率计算如下：
- en: '![](img/022eb4f4-f05f-488b-8ad5-092cac240c54.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/022eb4f4-f05f-488b-8ad5-092cac240c54.png)'
- en: 'Once we computed the probability of taking right, we can easily compute the
    probability of taking left by simply subtracting the probability of taking right
    from 1:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了右侧概率的概率，我们可以通过简单地从1中减去右侧概率来轻松计算左侧概率：
- en: '![](img/d3390104-af49-424d-9cab-9a2f84176099.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3390104-af49-424d-9cab-9a2f84176099.png)'
- en: If we sum the probability of all the leaf nodes, then it equals to 1, meaning
    that our tree is already normalized, and to find a probability of a word, we need
    to evaluate only ![](img/ed24f8a7-44e8-4e19-b7a5-97c4566cdd0c.png) nodes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们把所有叶子节点的概率加起来，那么它等于1，这意味着我们的树已经被归一化了，为了找到一个单词的概率，我们只需要评估 ![](img/ed24f8a7-44e8-4e19-b7a5-97c4566cdd0c.png)
    节点。
- en: Negative sampling
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负采样
- en: Let's say we are building a CBOW model and we have a sentence *Birds are flying
    in the sky.* Let the context words be *birds*, *are*, *in*, and *the* and the
    target word be *flying.*
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设正在构建一个CBOW模型，我们有一个句子*Birds are flying in the sky.* 让上下文词为*birds*, *are*,
    *in*, *the*，目标词为*flying*。
- en: We need to update the weights of the network every time it predicts the incorrect
    target word. So, except for the word *flying*, if a different word is predicted
    as a target word, then we update the network.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 每次预测不正确的目标词时，我们都需要更新网络的权重。因此，除了单词 *flying* 之外，如果预测出现不同的单词作为目标词，我们就更新网络。
- en: But this is just a small set of vocabulary. Consider the case where we have
    millions of words in the vocabulary. In that case, we need to perform numerous
    weight updates until the network predict the correct target word. It is time-consuming
    and also not an efficient method. So, instead of doing this, we mark the correct
    target word as a positive class and sample a few words from the vocabulary and
    mark it as a negative class.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是一个小词汇集。考虑一种情况，词汇表中有数百万个词。在这种情况下，我们需要进行大量的权重更新，直到网络预测出正确的目标词。这是耗时的，而且也不是一个高效的方法。因此，我们不是这样做，我们将正确的目标词标记为正类，并从词汇表中抽样几个词并标记为负类。
- en: What we are essentially doing here is that we are converting our multinomial
    class problem to a binary classification problem (that is, instead of trying to
    predict the target word, the model classifies whether the given word is target
    word or not).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里实际上正在做的是将我们的多项式类问题转换为二元分类问题（即，不再试图预测目标词，而是模型分类给定的词是否是目标词）。
- en: 'The probability that the word is chosen as a negative sample is given as:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 选择负样本作为单词的概率如下：
- en: '![](img/eceda56e-380e-4bb8-8910-b96d059c4269.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eceda56e-380e-4bb8-8910-b96d059c4269.png)'
- en: Subsampling frequent words
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对频繁出现的词进行子采样
- en: 'In our corpus, there will be certain words that occur very frequently, such
    as *the*, *is*, and so on, and there are certain words that occur infrequently.
    To maintain a balance between these two, we use a subsampling technique. So, we
    remove the words that occur frequently more than a certain threshold with the
    probability ![](img/d0c8b060-74f7-4551-b98b-be70f5cf0e9f.png), and it can be represented
    as:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的语料库中，会有一些非常频繁出现的词，比如*the*, *is*等等，还有一些很少出现的词。为了在这两者之间保持平衡，我们使用一种子采样技术。因此，我们以概率
    ![](img/d0c8b060-74f7-4551-b98b-be70f5cf0e9f.png) 删除那些频繁出现超过某个阈值的词，可以表示为：
- en: '![](img/58b3bab2-32ac-4361-98c7-003ca670032e.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58b3bab2-32ac-4361-98c7-003ca670032e.png)'
- en: Here, ![](img/e0a0cb07-0547-4e52-a1ab-f0364c8ee42b.png) is the threshold and
    ![](img/3720374d-c131-4e69-9d11-5bf517a05b92.png) is the frequency of the word
    ![](img/f66887f6-2312-434a-8866-adfc1965a158.png).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/e0a0cb07-0547-4e52-a1ab-f0364c8ee42b.png)是阈值，![](img/3720374d-c131-4e69-9d11-5bf517a05b92.png)是词![](img/f66887f6-2312-434a-8866-adfc1965a158.png)的频率。
- en: Building the word2vec model using gensim
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用gensim构建word2vec模型
- en: 'Now that we have understood how the word2vec model works, let''s see how to
    build the word2vec model using the `gensim` library. Gensim is one of the popular
    scientific software packages widely used for building vector space models. It
    can be installed via `pip`. So, we can just type the following command in the
    terminal to install the `gensim` library:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了word2vec模型的工作原理，让我们看看如何使用`gensim`库构建word2vec模型。Gensim是广泛用于构建向量空间模型的流行科学软件包之一。它可以通过`pip`安装。因此，我们可以在终端中输入以下命令来安装`gensim`库：
- en: '[PRE1]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have installed gensim, we will see how to build the word2vec model
    using that. You can download the dataset used in this section along with complete
    code with step by step explanation from GitHub at [http://bit.ly/2Xjndj4](http://bit.ly/2Xjndj4).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了gensim，我们将看看如何使用它构建word2vec模型。您可以从GitHub下载本节使用的数据集以及包含逐步说明的完整代码：[http://bit.ly/2Xjndj4](http://bit.ly/2Xjndj4)。
- en: 'First, we will import the necessary libraries:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入必要的库：
- en: '[PRE2]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading the dataset
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'Load the dataset:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE3]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s see what we got in our data:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的数据中有什么：
- en: '[PRE4]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code generates the following output:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![](img/120fcfd9-82fd-49ed-b5ec-ebcd744496be.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/120fcfd9-82fd-49ed-b5ec-ebcd744496be.png)'
- en: Preprocessing and preparing the dataset
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理和准备数据集
- en: 'Define a function for preprocessing the dataset:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个用于预处理数据集的函数：
- en: '[PRE5]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see how the preprocessed text looks like by running the following code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码来查看预处理文本的样子：
- en: '[PRE6]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We get the output as:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出如下：
- en: '[PRE7]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Preprocess the whole dataset:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理整个数据集：
- en: '[PRE8]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The genism library requires input in the form of a list of lists:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim库要求输入以列表的列表形式提供：
- en: '`*text = [ [word1, word2, word3], [word1, word2, word3] ]*`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`*text = [ [word1, word2, word3], [word1, word2, word3] ]*`'
- en: 'We know that each row in our data contains a set of sentences. So, we split
    them by `''.''` and convert them into a list:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道我们的数据中的每一行包含一组句子。因此，我们通过 `'.'` 将它们分割并将它们转换为列表：
- en: '[PRE9]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code generates the following output:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '[PRE10]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Thus, as shown, now, we have the data in a list. But we need to convert them
    into a list of lists. So, now again we split it by a space `'' ''`. That is, first,
    we split the data by `''.''` and then we split them by `'' ''` so that we can
    get our data in a list of lists:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如图所示，现在我们的数据是以列表的形式呈现的。但是我们需要将它们转换为列表的列表。所以，我们现在再次使用空格`' '`来分割它们。也就是说，我们首先通过
    `'.'` 分割数据，然后再通过 `' '` 分割它们，这样我们就可以得到我们的数据以列表的列表形式：
- en: '[PRE11]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can see that we have our inputs in the form of a list of lists:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到我们的输入以列表的形式呈现：
- en: '[PRE12]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Convert the whole text in our dataset to a list of lists:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们数据集中的整个文本转换为列表的列表：
- en: '[PRE13]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As shown, we successfully converted the whole text in our dataset into a list
    of lists:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所示，我们成功地将数据集中的整个文本转换为列表的列表：
- en: '[PRE14]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, the problem we have is that our corpus contains only unigrams and it will
    not give us results when we give a bigram as an input, for example, *san francisco*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的问题是，我们的语料库仅包含单个词和它不会在我们给出大量输入时给我们结果，例如，*san francisco*。
- en: So we use gensim's `Phrases` functions, which collects all the words that occur
    together and adds an underscore between them. So, now *san francisco* becomes
    *san_francisco*.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们使用gensim的`Phrases`函数，它收集所有一起出现的单词，并在它们之间添加下划线。所以现在*san francisco*变成了*san_francisco*。
- en: 'We set the `min_count` parameter to `25`, which implies that we ignore all
    the words and bigrams that appear less the `min_count`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`min_count`参数设置为`25`，这意味着我们会忽略出现次数少于`min_count`的所有单词和双词组：
- en: '[PRE15]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, now, an underscore has been added to the bigrams in our corpus:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，现在我们的语料库中的双词组已经添加了下划线：
- en: '[PRE16]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We check one more value from the corpus to see how an underscore is added for
    bigrams:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查语料库中的另一个值，以查看如何为双词组添加下划线：
- en: '[PRE17]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Building the model
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Now let''s build our model. Let''s define some of the important hyperparameters
    that our model needs:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建我们的模型。让我们定义一些模型需要的重要超参数：
- en: The `size` parameter represents the size of the vector, that is, dimensions
    of our vector, to represent a word. The size can be chosen according to our data
    size. If our data is very small, then we can set the size to a small value, but
    if we have a significantly large dataset, then we can set the size to `300`. In
    our case, we set the size to `100`.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `window_size` parameter represents the distance that should be considered
    between the target word and its neighboring word. Words exceeding the window size
    from the target word will not be considered for learning. Typically, a small window
    size is preferred.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `min_count` parameter represents the minimum frequency of words. If the
    particular word's occurrence is less than a `min_count`, then we can simply ignore
    that word.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `workers` parameter specifies the number of worker threads we need to train
    the model.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting `sg=1` implies that we use the skip-gram model for training, but if
    it is set to `sg=0`, then it implies that we use CBOW model for training.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define all the hyperparameters using following code:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s train the model using the `Word2Vec` function from gensim:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once, we trained the model successfully, we save them. Saving and loading the
    model is very simple; we can simply use the `save` and `load` functions, respectively:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can also `load` the already saved `Word2Vec` model by using the following
    code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Evaluating the embeddings
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now evaluate what our model has learned and how well our model has understood
    the semantics of the text. The `genism` library provides the `most_similar` function,
    which gives us the top similar words related to the given word.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following code, given `san_diego` as an input, we are
    getting all the other related city names that are most similar:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can also apply arithmetic operations on our vectors to check how accurate
    our vectors are as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can also find the words that do not match in the given set of words; for
    instance, in the following list called `text`, other than the word `holiday`,
    all others are city names. Since Word2Vec has understood this difference, it returns
    the word `holiday` as the one that does not match with the other words in the
    list as shown:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Visualizing word embeddings in TensorBoard
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned how to build word2vec model for generating
    word embeddings using gensim. Now, we will see how to visualize those embeddings
    using TensorBoard. Visualizing word embeddings help us to understand the projection
    space and also helps us to easily validate the embeddings. TensorBoard provides
    us a built-in visualizer called the **embedding projector** for interactively
    visualizing and analyzing the high-dimensional data like our word embeddings.
    We will learn how can we use the TensorBoard's projector for visualizing the word
    embeddings step by step.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Load the saved model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After loading the model, we will save the number of words in our model to the
    `max_size` variable:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型后，我们将单词数量保存到`max_size`变量中：
- en: '[PRE27]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We know that the dimension of word vectors will be ![](img/660327a5-a215-420a-a69c-d7f9dc635989.png).
    So, we initialize a matrix named `w2v` with the shape as our `max_size`, which
    is the vocabulary size, and the model''s first layer size, which is the number
    of neurons in the hidden layer:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道单词向量的维度将是 ![](img/660327a5-a215-420a-a69c-d7f9dc635989.png)。因此，我们用形状为我们的`max_size`（词汇量大小）和模型第一层大小（隐藏层中的神经元数）的矩阵`w2v`来初始化：
- en: '[PRE28]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we create a new file called `metadata.tsv`, where we save all the words
    in our model and we store the embedding of each word in the `w2v` matrix:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个名为`metadata.tsv`的新文件，在其中保存我们模型中的所有单词，并将每个单词的嵌入存储在`w2v`矩阵中：
- en: '[PRE29]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we initialize the TensorFlow session:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化TensorFlow会话：
- en: '[PRE30]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Initialize the TensorFlow variable called `embedding` that holds the word embeddings:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化名为`embedding`的TensorFlow变量，用于保存单词嵌入：
- en: '[PRE31]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Initialize all the variables:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有变量：
- en: '[PRE32]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create an object to the `saver` class, which is actually used for saving and
    restoring variables to and from our checkpoints:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个到`saver`类的对象，该类实际上用于将变量保存和从检查点恢复：
- en: '[PRE33]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Using `FileWriter`, we can save our summaries and events to our event file:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`FileWriter`，我们可以将摘要和事件保存到我们的事件文件中：
- en: '[PRE34]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, we initialize the projectors and add the `embeddings`:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们初始化投影仪并添加`embeddings`：
- en: '[PRE35]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we specify our `tensor_name` as `embedding` and `metadata_path` to the
    `metadata.tsv` file, where we have the words:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的`tensor_name`指定为`embedding`，并将`metadata_path`设置为`metadata.tsv`文件，其中包含我们的单词：
- en: '[PRE36]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'And, finally, save the model:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，保存模型：
- en: '[PRE37]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, open the terminal and type the following command to open the `tensorboard`:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打开终端并输入以下命令以打开`tensorboard`：
- en: '[PRE38]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Once the TensorBoard is opened, go to the PROJECTOR tab. We can see the output,
    as shown in the following screenshot. As you can notice, when we type the word
    `delighted`, we can see all the related words, such as `pleasant`, `surprise`,
    and many more similar words, adjacent to that:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 打开TensorBoard后，转到PROJECTOR选项卡。我们可以看到输出，如下图所示。您会注意到，当我们键入单词`delighted`时，我们可以看到所有相关的单词，例如`pleasant`、`surprise`等等，都与之相邻：
- en: '![](img/48201f67-190b-45b6-bcbb-f5141142d804.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48201f67-190b-45b6-bcbb-f5141142d804.png)'
- en: Doc2vec
  id: totrans-355
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Doc2vec
- en: So far, we have seen how to generate embeddings for a word. But how can we generate
    the embeddings for a document? A naive method would be to compute a word vector
    for each word in the document and take an average of it. Mikilow and Le introduced
    a new method for generating the embeddings for documents instead of just taking
    the average of word embeddings. They introduced two new methods, called PV-DM
    and PV-DBOW. Both of these methods just add a new vector, called **paragraph id**.
    Let's see how exactly these two methods work.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何为单词生成嵌入。但是如何为文档生成嵌入呢？一种简单的方法是计算文档中每个单词的单词向量并取平均值。Mikilow和Le提出了一种新的方法，用于生成文档的嵌入，而不仅仅是取单词嵌入的平均值。他们引入了两种新方法，称为PV-DM和PV-DBOW。这两种方法都引入了一个称为**段落id**的新向量。让我们看看这两种方法的工作原理。
- en: Paragraph Vector – Distributed Memory model
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 段落向量 - 分布式内存模型
- en: PV-DM is similar to the CBOW model, where we try to predict the target word
    given a context word. In PV-DM, along with word vectors, we introduce one more
    vector, called the paragraph vector. As the name suggests, the paragraph vector
    learns the vector representation of the whole paragraph and it captures the subject
    of the paragraph.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: PV-DM类似于CBOW模型，我们尝试根据上下文单词预测目标单词。在PV-DM中，除了单词向量外，我们还引入了一个称为段落向量的额外向量。顾名思义，段落向量学习整个段落的向量表示，并捕捉段落的主题。
- en: 'As shown in the following figure, each paragraph is mapped to a unique vector
    and each word is also mapped to a unique vector. So, in order to predict the target
    word, we combine the word vectors and paragraph vector by either concatenating
    or averaging them:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，每个段落都映射到一个唯一的向量，每个单词也映射到一个唯一的向量。因此，为了预测目标单词，我们通过连接或平均单词向量和段落向量来组合它们：
- en: '![](img/46304231-6efb-4064-a6b3-309982ffa2a8.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46304231-6efb-4064-a6b3-309982ffa2a8.png)'
- en: But having said all that, how is the paragraph vector useful in predicting the
    target word? What is really the use of having the paragraph vector? We know that
    we try to predict the target word based on the context words. Context words are
    of a fixed length and they are sampled within a sliding window from a paragraph.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 但说了这么多，段落向量在预测目标单词方面有什么用处呢？拥有段落向量真的有什么用呢？我们知道，我们试图基于上下文单词预测目标单词。上下文单词长度固定，并且在一个段落的滑动窗口中抽样。
- en: Along with context words, we also make use of the paragraph vector for predicting
    the target word. Since the paragraph vector contains information about the subject
    of the paragraph, they contain meanings that the context words do not hold. That
    is, context word contains information about the particular words alone but the
    paragraph vector contains the information about the whole paragraph. So, we can
    think of the paragraph vector as a new word that is used along with context words
    for predicting the target word.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上下文单词，我们还利用段落向量来预测目标单词。因为段落向量包含段落主题的信息，它们包含上下文单词不包含的含义。也就是说，上下文单词只包含关于特定单词本身的信息，而段落向量包含整个段落的信息。因此，我们可以将段落向量视为与上下文单词一起用于预测目标单词的新单词。
- en: Paragraph vector is the same for all the context words sampled from the same
    paragraph and are not shared across paragraphs. Let's say that we have three paragraphs,
    *p1*, *p2*, and *p3*. If the context is sampled from a paragraph *p1*, then the
    *p1* vector is used to predict the target word. If a context is sampled from paragraph
    *p2,* then the *p2* vector is used. Thus, Paragraph vectors are not shared across
    paragraphs. However, word vectors are shared across all paragraphs. That is, the
    vector for the word *sun* is the same across all the paragraphs. We call our model
    as a distributed memory model of paragraph vectors, as our paragraph vectors serve
    as a memory that holds information that is missing from the current context words.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 来自同一段落抽样的所有上下文单词共享相同的段落向量，并且跨段落不共享。假设我们有三个段落，*p1*，*p2*和*p3*。如果上下文是从段落*p1*中抽样的，则使用*p1*向量来预测目标单词。如果上下文是从段落*p2*中抽样的，则使用*p2*向量。因此，段落向量在段落之间不共享。然而，单词向量在所有段落中共享。也就是说，*sun*的向量在所有段落中是相同的。我们称我们的模型为分布式记忆模型的段落向量，因为我们的段落向量充当了一种存储信息的记忆，这些信息在当前上下文单词中是缺失的。
- en: So, both of the paragraph vectors and word vectors are learned using stochastic
    gradient descent. On each iteration, we sample context words from a random paragraph,
    try to predict the target word, calculate the error, and update the parameters.
    After training, the paragraph vectors capture the embeddings of the paragraphs
    (documents).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，段落向量和单词向量都是使用随机梯度下降学习的。在每次迭代中，我们从一个随机段落中抽样上下文单词，尝试预测目标单词，计算误差并更新参数。训练结束后，段落向量捕捉了段落（文档）的嵌入。
- en: Paragraph Vector – Distributed Bag of Words model
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 段落向量 - 分布式袋模型
- en: 'PV-DBOW is similar to the skip-gram model, where we try to predict the context
    words based on the target word:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: PV-DBOW类似于skip-gram模型，我们试图基于目标单词预测上下文单词：
- en: '![](img/8e0ff5d7-de13-4d77-ba4c-0f997acb9883.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e0ff5d7-de13-4d77-ba4c-0f997acb9883.png)'
- en: Unlike previous methods, here we do not try to predict the next words. Instead,
    we use a paragraph vector to classify the words in the document. But how do they
    work? We train the model to understand whether the word belongs to a paragraph
    or not. We sample some set of words and then feed it to a classifier, which tells
    us whether the words belong to a particular paragraph or not, and in such a way
    we learn the paragraph vector.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于以前的方法，我们这里不试图预测下一个单词。相反，我们使用段落向量来分类文档中的单词。但是它们是如何工作的呢？我们训练模型以理解单词是否属于某个段落。我们抽样一些单词集，并将其馈送到分类器中，该分类器告诉我们单词是否属于特定段落，通过这种方式我们学习段落向量。
- en: Finding similar documents using doc2vec
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用doc2vec查找相似文档
- en: 'Now, we will see how to perform document classification using doc2vec. In this
    section, we will use the 20 `news_dataset`. It consists of 20,000 documents over
    20 different news categories. We will use only four categories: `Electronics`,
    `Politics`, `Science`, and `Sports`. So, we have 1,000 documents under each of
    these four categories. We rename the documents with a prefix, `category_`. For
    example, all science documents are renamed as `Science_1`, `Science_2`, and so
    on. After renaming them, we combine all the documents and place them in a single
    folder. The combined data, along with complete code is available at available
    as a Jupyter Notebook on GitHub at [http://bit.ly/2KgBWYv](http://bit.ly/2KgBWYv).'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到如何使用doc2vec进行文档分类。在本节中，我们将使用20个`news_dataset`。它包含20个不同新闻类别的20,000篇文档。我们只使用四个类别：`Electronics`、`Politics`、`Science`和`Sports`。因此，每个类别下有1,000篇文档。我们将这些文档重新命名，以`category_`为前缀。例如，所有科学文档都重命名为`Science_1`、`Science_2`等。重命名后，我们将所有文档组合并放置在一个单独的文件夹中。完整的数据以及完整的代码可以在GitHub的Jupyter
    Notebook上找到，网址为[http://bit.ly/2KgBWYv](http://bit.ly/2KgBWYv)。
- en: Now, we train our doc2vec model to classify and find similarities between these
    documents.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们训练我们的doc2vec模型来对这些文档进行分类并找出它们之间的相似性。
- en: 'First, we import all the necessary libraries:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所有必要的库：
- en: '[PRE39]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we load all our documents and save the document names in the `docLabels`
    list and the document content in a list called `data`:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们加载所有文档并将文档名称保存在`docLabels`列表中，将文档内容保存在名为`data`的列表中：
- en: '[PRE40]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can see in `docLabels` list we have all our documents'' names:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`docLabels`列表中看到我们所有文档的名称：
- en: '[PRE41]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define a class called `DocIterator`, which acts as an iterator to run over
    all the documents:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为`DocIterator`的类，作为遍历所有文档的迭代器：
- en: '[PRE42]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Create an object called `it` to the `DocIterator` class:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`it`的`DocIterator`类对象：
- en: '[PRE43]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, let''s build the model. Let''s first, define some of the important hyperparameters
    of the model:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建模型。首先，定义模型的一些重要超参数：
- en: The `size` parameter represents our embedding size.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size`参数表示我们的嵌入大小。'
- en: The `alpha` parameter represents our learning rate.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alpha`参数表示我们的学习率。'
- en: The `min_alpha` parameter implies that our learning rate, `alpha`, will decay
    to `min_alpha` during training.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_alpha`参数意味着我们的学习率`alpha`在训练期间会衰减到`min_alpha`。'
- en: Setting `dm=1` implies that we use the distributed memory (PV-DM) model and
    if we set `dm=0`, it implies that we use the distributed bag of words (PV-DBOW)
    model for training.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置`dm=1`意味着我们使用分布式内存（PV-DM）模型，如果设置`dm=0`，则表示我们使用分布式词袋（PV-DBOW）模型进行训练。
- en: The `min_count` parameter represents the minimum frequency of words. If the
    particular word's occurrence is less than a `min_count`, than we can simply ignore
    that word.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count`参数表示单词的最小出现频率。如果特定单词的出现少于`min_count`，我们可以简单地忽略该单词。'
- en: 'These hyperparameters are defined as:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数被定义为：
- en: '[PRE44]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now let''s define the model using `gensim.models.Doc2ec()` class:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用`gensim.models.Doc2ec()`类定义模型：
- en: '[PRE45]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Train the model:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE46]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'After training, we can save the model, using the `save` function:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以保存模型，使用`save`函数：
- en: '[PRE47]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can load the saved model, using the `load` function:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`load`函数加载保存的模型：
- en: '[PRE48]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, let''s evaluate our model''s performance. The following code shows that
    when we feed the `Sports_1.txt` document as an input, it will input all the related
    documents with the corresponding scores:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型的性能。以下代码显示，当我们将`Sports_1.txt`文档作为输入时，它将输出所有相关文档及其相应的分数：
- en: '[PRE49]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Understanding skip-thoughts algorithm
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解skip-thoughts算法
- en: Skip-thoughts is one of the popular unsupervised learning algorithms for learning
    the sentence embedding. We can see skip-thoughts as an analogy to the skip-gram
    model. We learned that in the skip-gram model, we try to predict the context word
    given a target word, whereas in skip-thoughts, we try to predict the context sentence
    given a target sentence. In other words, we can say that skip-gram is used for
    learning word-level vectors and skip-thoughts is used for learning sentence-level
    vectors.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-thoughts是一种流行的无监督学习算法，用于学习句子嵌入。我们可以将skip-thoughts视为skip-gram模型的类比。我们了解到在skip-gram模型中，我们试图预测给定目标词的上下文词，而在skip-thoughts中，我们试图预测给定目标句子的上下文句子。换句话说，我们可以说skip-gram用于学习单词级别的向量，而skip-thoughts用于学习句子级别的向量。
- en: 'The algorithm of skip-thoughts is very simple. It consists of an encoder-decoder
    architecture. The role of the encoder is to map the sentence to a vector and the
    role of the decoder is to generate the surrounding sentences that is the previous
    and next sentence of the given input sentence. As shown in the following diagram,
    the skip-thoughts vector consists of one encoder and two decoders, called a previous
    decoder and next decoder:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃思想的算法非常简单。它由一个编码器-解码器架构组成。编码器的角色是将句子映射到一个向量，而解码器的角色是生成给定输入句子的前后句子。如下图所示，跳跃思想向量包括一个编码器和两个解码器，称为前一个解码器和后一个解码器：
- en: '![](img/a3a66dd3-7fd4-4146-96b1-bfa996767b07.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3a66dd3-7fd4-4146-96b1-bfa996767b07.png)'
- en: 'The working of an encoder and decoder is discussed next:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 下面讨论编码器和解码器的工作：
- en: '**Encoder**: An encoder takes the words in a sentence sequentially and generates
    the embeddings. Let''s say we have a list of sentences. ![](img/9ff4d7cc-8397-4f89-97c7-3c5d7abdb882.png).
    ![](img/6fc4dd51-a29f-4f7b-aab1-7186033226df.png)denotes the ![](img/bef8d295-0f98-49f4-a410-1ddb325c8dcb.png)word
    in a sentence ![](img/519ba00b-ad85-4229-86f4-5f445cbcb6d0.png)and ![](img/007bc484-5995-4611-95b4-ce2cdfef0737.png)denotes
    its word embeddings. So the hidden state of an encoder is interpreted as a sentence
    representation.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：编码器按顺序处理句子中的单词并生成嵌入。假设我们有一系列句子。 ![](img/9ff4d7cc-8397-4f89-97c7-3c5d7abdb882.png)。
    ![](img/6fc4dd51-a29f-4f7b-aab1-7186033226df.png)表示句子中的第 ![](img/bef8d295-0f98-49f4-a410-1ddb325c8dcb.png)个单词，而
    ![](img/007bc484-5995-4611-95b4-ce2cdfef0737.png)表示其单词嵌入。因此，编码器的隐藏状态被解释为句子的表示。'
- en: '**Decoder**: There are two decoders, called a previous decoder and next decoder.
    As the name suggests, the previous decoder is used to generate the previous sentence,
    and the next decoder is used to generate the next sentence. Let''s say we have
    a sentence ![](img/827fbe9b-c496-4d53-8496-661a034c727c.png)and its embeddings
    are ![](img/1eace04e-e015-42b2-8cfc-642b9c74520c.png). Both of the decoders take
    the embeddings ![](img/04276d5d-1738-4c0a-9641-2258494dc849.png)as an input and
    the previous decoder tries to generate the previous sentence, ![](img/b1ce0037-7f17-4bb4-b2d1-26262cfd1ba7.png),
    and the next decoder tries to generate the next sentence, ![](img/e7786bd8-d584-4f33-ae2c-413d6cf2ecb6.png).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：有两个解码器，称为前一个解码器和后一个解码器。顾名思义，前一个解码器用于生成前一个句子，后一个解码器用于生成下一个句子。假设我们有一个句子
    ![](img/827fbe9b-c496-4d53-8496-661a034c727c.png)及其嵌入为 ![](img/1eace04e-e015-42b2-8cfc-642b9c74520c.png)。这两个解码器都将嵌入
    ![](img/04276d5d-1738-4c0a-9641-2258494dc849.png)作为输入，前一个解码器尝试生成前一个句子， ![](img/b1ce0037-7f17-4bb4-b2d1-26262cfd1ba7.png)，而后一个解码器尝试生成下一个句子，
    ![](img/e7786bd8-d584-4f33-ae2c-413d6cf2ecb6.png)。'
- en: So, we train our model by minimizing the reconstruction error of both the previous
    and next decoders. Because when the decoders reconstruct/generate correct previous
    and next sentences, it means that we have a meaningful sentence embedding ![](img/04276d5d-1738-4c0a-9641-2258494dc849.png).
    We send the reconstruction error to the encoder, so that encoder can optimize
    the embeddings and send better representations to the decoder. Once we have trained
    our model, we use our encoder to generate the embedding for a new sentence.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过最小化前后解码器的重构误差来训练我们的模型。因为当解码器正确重构/生成前后句子时，这意味着我们有一个有意义的句子嵌入 ![](img/04276d5d-1738-4c0a-9641-2258494dc849.png)。我们将重构误差发送到编码器，以便编码器可以优化嵌入并向解码器发送更好的表示。一旦我们训练好我们的模型，我们就可以使用编码器为新句子生成嵌入。
- en: Quick-thoughts for sentence embeddings
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子嵌入的快速思考
- en: Quick-thoughts is another interesting algorithm for learning the sentence embeddings.
    In skip-thoughts, we saw how we used the encoder-decoder architecture to learn
    the sentence embeddings. In quick-thoughts, we try to learn whether a given sentence
    is related to the candidate sentence. So, instead of using a decoder, we use a
    classifier to learn whether a given input sentence is related to the candidate
    sentence.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 快速思考是另一种有趣的学习句子嵌入的算法。在跳跃思想中，我们看到如何使用编码器-解码器架构来学习句子嵌入。在快速思考中，我们尝试学习给定句子是否与候选句子相关联。因此，我们不使用解码器，而是使用分类器来学习给定输入句子是否与候选句子相关。
- en: Let ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png) be the input sentence
    and ![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png) be the set of candidate
    sentences containing both valid context and invalid context sentences related
    to the given input sentence ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png).
    Let **![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png)** be any candidate sentence
    from the ![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 让 ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png) 为输入句子，![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png)
    为包含与给定输入句子 ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png) 相关的有效上下文和无效上下文句子集合。让
    **![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png)** 是来自于 ![](img/b0630a0b-7910-4ef4-920f-f3e4bd309750.png)
    的任意候选句子。
- en: We use two encoding functions, ![](img/5ef11cca-cb04-4487-8662-f8e0b37bffcf.png)
    and ![](img/ccb8587f-cc34-49bb-97fd-a0b718b60350.png). The role of these two functions,
    ![](img/2d8df112-c7bf-4a59-aa08-d8d871e8451b.png) and ![](img/7b85920f-f7e4-434c-bd5e-e42bb0cb8022.png),
    is to learn the embeddings, that is, to learn the vector representations of a
    given sentence ![](img/155d14d5-772b-4d94-8817-d0ab1c63d1d6.png) and candidate
    sentence ![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png), respectively.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个编码函数，![](img/5ef11cca-cb04-4487-8662-f8e0b37bffcf.png) 和 ![](img/ccb8587f-cc34-49bb-97fd-a0b718b60350.png)。这两个函数的作用分别是学习给定句子
    ![](img/155d14d5-772b-4d94-8817-d0ab1c63d1d6.png) 和候选句子 ![](img/c23cd410-c1de-405f-97a3-7ea8fc7f9202.png)
    的嵌入，即学习它们的向量表示。
- en: Once these two functions generate the embeddings, we use a classifier ![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png),
    which returns the probability for each candidate sentence to be related to the
    given input sentence.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这两个函数生成嵌入，我们使用分类器 ![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png)，它返回每个候选句子与给定输入句子相关的概率。
- en: 'As shown in the following figure, the probability of the second candidate sentence
    ![](img/414b1656-46a2-4012-925a-ba38a5b3680d.png) is high, as it is related to
    the given input sentence ![](img/4b7f2269-0f75-43d1-ae97-1ff5953cd9cd.png):'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，第二候选句子的概率 ![](img/414b1656-46a2-4012-925a-ba38a5b3680d.png) 较高，因为它与给定的输入句子
    ![](img/4b7f2269-0f75-43d1-ae97-1ff5953cd9cd.png) 有关：
- en: '![](img/e1111caf-a7dd-49d0-8727-6d5bf29d2906.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1111caf-a7dd-49d0-8727-6d5bf29d2906.png)'
- en: 'Thus, the probability that ![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)
    is a correct sentence ,that is, ![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)
    is related to the given input sentence ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png)
    is computed as:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png) 是正确句子的概率，即 ![](img/a184fd5a-6449-49b3-a30f-c492a7750c80.png)
    与给定输入句子 ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png) 相关的计算公式为：
- en: '![](img/2c9a1c15-b8bc-46cf-892d-d508edf27ee3.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c9a1c15-b8bc-46cf-892d-d508edf27ee3.png)'
- en: Here, ![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png) is a classifier.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/620965c2-10d4-44d1-bb08-856aa2a00aa3.png) 是一个分类器。
- en: The goal of our classifier is to identify the valid context sentence related
    to a given input sentence ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png). So,
    our cost function is to maximize the probability of finding the correct context
    sentence for the given input sentence ![](img/280b50e3-3e78-4898-98b4-408efed0901d.png).
    If it classifies the sentence correctly, then it means that our encoders learned
    the better representation of the sentence.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分类器的目标是识别与给定输入句子 ![](img/ee090958-7d89-4951-9f6d-e0c289474fdc.png) 相关的有效上下文句子。因此，我们的成本函数是最大化找到给定输入句子正确上下文句子的概率
    ![](img/280b50e3-3e78-4898-98b4-408efed0901d.png)。如果它正确分类句子，则表示我们的编码器学习了更好的句子表示。
- en: Summary
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We started off the chapter by understanding word embeddings and we looked at
    two different types of Word2Vec model, called CBOW, where we try to predict the
    target word given the context word, and skip-gram, where we try to predict the
    context word given the target word.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解词嵌入开始本章，看了两种不同类型的Word2Vec模型，称为CBOW，我们试图预测上下文词给定目标词，以及Skip-gram，我们试图预测目标词给定上下文词。
- en: Then, we learned about various training strategies in Word2Vec. We looked at
    hierarchical softmax, where we replace the output layer of the network with a
    Huffman binary tree and reduce the complexity to ![](img/9508f84c-0cd7-4394-a7f5-cd3f9879ccf9.png).
    We also learned about negative sampling and subsampling frequent word methods.
    Then we understood how to build the Word2Vec model using a gensim library and
    how to project the high-dimensional word embeddings to visualize them in TensorBoard.
    Going forward, we studied how the doc2vec model works with two types of doc2vec
    models—PV-DM and PV-DBOW. Following this, we learned about the skip-thoughts model,
    where we learn the embedding of a sentence by predicting the previous and next
    sentences of the given sentence and we also explored the quick-thoughts model
    at the end of the chapter.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了 Word2Vec 中的各种训练策略。我们讨论了分层 softmax，其中我们用哈夫曼二叉树替换网络的输出层，并将复杂度降低到 ![](img/9508f84c-0cd7-4394-a7f5-cd3f9879ccf9.png)。我们还学习了负采样和子采样频繁词汇的方法。然后我们了解了如何使用
    gensim 库构建 Word2Vec 模型，以及如何将高维词嵌入投影到 TensorBoard 中进行可视化。接下来，我们研究了 doc2vec 模型如何使用
    PV-DM 和 PV-DBOW 两种类型的 doc2vec 模型。在此之后，我们学习了 skip-thoughts 模型的工作原理，通过预测给定句子的前后句子来学习句子的嵌入，并在章节末尾探索了
    quick-thoughts 模型。
- en: In the next chapter, we will learn about generative models and how generative
    models are used to generate images.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习生成模型以及生成模型如何用于生成图像。
- en: Questions
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our newly acquired knowledge by answering the following questions:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们新获得的知识：
- en: What is the difference between the skip-gram and CBOW models?
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: skip-gram 和 CBOW 模型之间的区别是什么？
- en: What is the loss function of the CBOW model?
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CBOW 模型的损失函数是什么？
- en: What is the need for negative sampling?
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负采样的必要性是什么？
- en: Define PV-DM.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 PV-DM 是什么？
- en: What is the role of the encoder and decoder in the skip-thoughts vector?
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 skip-thoughts 向量中，编码器和解码器的角色是什么？
- en: What are quick thoughts vector?
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 快速思想向量是什么？
- en: Further reading
  id: totrans-431
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Explore the following links to gain more insights into learning representation
    of text:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 探索以下链接，以深入了解文本表示学习：
- en: '*Distributed Representations of Words and Phrases and their Compositionality*,
    by Tomas Mikolov, et al., [https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*单词和短语的分布式表示及其组成性*, 作者 Tomas Mikolov 等, [https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)'
- en: '*Distributed Representations of Sentences and Documents*, by Quoc Le and Tomas
    Mikolov, [https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*句子和文档的分布式表示*, 作者 Quoc Le 和 Tomas Mikolov, [https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)'
- en: '*Skip-thought Vectors*, by Ryan Kiros, et al., [https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Skip-thought Vectors*, 作者 Ryan Kiros 等, [https://arxiv.org/pdf/1506.06726.pdf](https://arxiv.org/pdf/1506.06726.pdf)'
- en: '*An Efficient Framework for Learning Sentence Representations*, by Lajanugen
    Logeswaran and Honglak Lee, [https://arxiv.org/pdf/1803.02893.pdf](https://arxiv.org/pdf/1803.02893.pdf)'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习句子表示的高效框架*, 作者 Lajanugen Logeswaran 和 Honglak Lee, [https://arxiv.org/pdf/1803.02893.pdf](https://arxiv.org/pdf/1803.02893.pdf)'
