["```py\npip install torch torchvision\n```", "```py\nconda install torch torchvision -c pytorch\n```", "```py\n    import torch\n    x = torch.tensor([1.,2.])\n    print(x)\n    ```", "```py\n    x = torch.tensor([1., 2.])\n    y = torch.tensor([3., 4.])\n    print(x * y)\n    ```", "```py\n    x = torch.tensor([[1., 2.],[5., 3.],[0., 4.]])\n    print(x[0][1])\n    ```", "```py\nprint(x[0][1].item())\n```", "```py\nx.shape\n```", "```py\n    cuda = torch.device('cuda') \n    ```", "```py\n    x = torch.tensor([5., 3.], device=cuda)\n    ```", "```py\n    y = torch.tensor([4., 2.]).cuda()\n    ```", "```py\n    x*y\n    ```", "```py\n    train = pd.read_csv(\"train.csv\")\n    train_labels = train['label'].values\n    train = train.drop(\"label\",axis=1).values.reshape(len(train),1,28,28)\n    ```", "```py\n    X = torch.Tensor(train.astype(float))\n    y = torch.Tensor(train_labels).long()\n    ```", "```py\nclass MNISTClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 392)\n        self.fc2 = nn.Linear(392, 196)\n        self.fc3 = nn.Linear(196, 98)\n        self.fc4 = nn.Linear(98, 10)\n```", "```py\nself.dropout = nn.Dropout(p=0.2)\n```", "```py\n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = self.dropout(F.relu(self.fc1(x)))\n        x = self.dropout(F.relu(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n        x = F.log_softmax(self.fc4(x), dim=1)\n```", "```py\nmodel = MNISTClassifier()\nloss_function = nn.NLLLoss()\nopt = optim.Adam(model.parameters(), lr=0.001)\n```", "```py\n    for epoch in range(50): \n        images = Variable(X)\n        labels = Variable(y)\n    ```", "```py\n    opt.zero_grad()\n    ```", "```py\n    outputs = model(images)\n    ```", "```py\n    loss = loss_function(outputs, labels)\n    loss.backward()\n    opt.step()\n    ```", "```py\n    print ('Epoch [%d/%d] Loss: %.4f' %(epoch+1, 50,         loss.data.item()))\n    ```", "```py\ntest = pd.read_csv(\"test.csv\")\ntest_labels = test['label'].values\ntest = test.drop(\"label\",axis=1).values.reshape(len(test),                  1,28,28)\nX_test = torch.Tensor(test.astype(float))\ny_test = torch.Tensor(test_labels).long()\n```", "```py\npreds = model(X_test)\n```", "```py\nprint(preds[0])\n```", "```py\n_, predictionlabel = torch.max(preds.data, 1)\npredictionlabel = predictionlabel.tolist()\npredictionlabel = pd.Series(predictionlabel)\ntest_labels = pd.Series(test_labels)\npred_table = pd.concat([predictionlabel, test_labels], axis=1)\npred_table.columns =['Predicted Value', 'True Value']\ndisplay(pred_table.head())\n```", "```py\npreds = len(predictionlabel)\ncorrect = len([1 for x,y in zip(predictionlabel, test_labels)               if x==y])\nprint((correct/preds)*100)\n```", "```py\n    (\"This is my favourite chapter\".lower().split(),\\\n     \"English\"),\n    (\"Estoy en la biblioteca\".lower().split(), \"Spanish\")\n    ```", "```py\n    word_dict = {}\n    i = 0\n    for words, language in training_data + test_data:\n        for word in words:\n            if word not in word_dict:\n                word_dict[word] = i\n                i += 1\n    print(word_dict)\n    ```", "```py\n    corpus_size = len(word_dict)\n    languages = 2\n    label_index = {\"Spanish\": 0, \"English\": 1}\n    class BagofWordsClassifier(nn.Module):  \n        def __init__(self, languages, corpus_size):\n            super(BagofWordsClassifier, self).__init__()\n            self.linear = nn.Linear(corpus_size, languages)\n        def forward(self, bow_vec):\n            return F.log_softmax(self.linear(bow_vec), dim=1)\n    ```", "```py\n    def make_bow_vector(sentence, word_index):\n        word_vec = torch.zeros(corpus_size)\n        for word in sentence:\n            word_vec[word_dict[word]] += 1\n        return word_vec.view(1, -1)\n    ```", "```py\n    def make_target(label, label_index):\n        return torch.LongTensor([label_index[label]])\n    ```", "```py\n    model = BagofWordsClassifier(languages, corpus_size)\n    loss_function = nn.NLLLoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.1)\n    ```", "```py\nfor epoch in range(100):\n    for sentence, label in training_data:\n        model.zero_grad()\n        bow_vec = make_bow_vector(sentence, word_dict)\n        target = make_target(label, label_index)\n        log_probs = model(bow_vec)\n        loss = loss_function(log_probs, target)\n        loss.backward()\n        optimizer.step()\n\n    if epoch % 10 == 0:\n        print('Epoch: ',str(epoch+1),', Loss: ' +                         str(loss.item()))\n```", "```py\n    def make_predictions(data):\n        with torch.no_grad():\n            sentence = data[0]\n            label = data[1]\n            bow_vec = make_bow_vector(sentence, word_dict)\n            log_probs = model(bow_vec)\n            print(sentence)\n            print(label + ':')\n            print(np.exp(log_probs))\n\n    make_predictions(test_data[0])\n    make_predictions(test_data[1])\n    ```", "```py\n    def return_params(word): \n        index = word_dict[word]\n        for p in model.parameters():\n            dims = len(p.size())\n            if dims == 2:\n                print(word + ':')\n                print('Spanish Parameter = ' +                    str(p[0][index].item()))\n                print('English Parameter = ' +                    str(p[1][index].item()))\n                print('\\n')\n\n    return_params('estoy')\n    return_params('book')\n    ```", "```py\nnew_sentence = ([\"not\"],\"English\")\nmake_predictions(new_sentence)\n```"]