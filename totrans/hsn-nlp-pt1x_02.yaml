- en: '*Chapter 1*: Fundamentals of Machine Learning and Deep Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*：机器学习和深度学习基础'
- en: Our world is rich with natural language data. Over the past several decades,
    the way we communicate with one another has shifted to the digital realm and,
    as such, this data can be used to build models that can improve our online experience.
    From returning relevant results within a search engine, to autocompleting the
    next word you type in an email, the benefits of being able to extract insights
    from natural language is clear to see.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的世界充满了自然语言数据。在过去的几十年里，我们彼此之间的沟通方式已经转变为数字领域，因此这些数据可以用来构建能够改进我们在线体验的模型。从在搜索引擎中返回相关结果，到在电子邮件中自动完成下一个输入的词语，能够从自然语言中提取洞察力的好处是显而易见的。
- en: While the way we, as humans, understand language differs notably from the way
    a model or *artificial intelligence* may understand it, by shedding light on machine
    learning and what it is used for, we can begin to understand just how these deep
    learning models *understand* language and what fundamentally happens when a model
    learns from data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们人类理解语言的方式与模型或*人工智能*理解的方式有显著区别，但通过揭示机器学习及其用途，我们可以开始理解这些深度学习模型如何*理解*语言，以及模型从数据中学习时发生的基本情况。
- en: Throughout this book, we will explore this application of artificial intelligence
    and deep learning to natural language. Through the use of PyTorch, we will learn,
    step by step, how to build models that allow us to perform sentiment analysis,
    text classification, and sequence translation, which will lead to us building
    a basic chatbot. By covering the theory behind each of these models, as well as
    demonstrating how to implement them practically, we will demystify the field of
    **natural language processing** (**NLP**) and provide you with enough background
    for you to start building your own models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们将探讨人工智能和深度学习在自然语言处理中的应用。通过使用PyTorch，我们将逐步学习如何构建模型，从而进行情感分析、文本分类和序列翻译，这将使我们能够构建基本的聊天机器人。通过涵盖每个模型背后的理论，并演示如何实际实施它们，我们将揭开**自然语言处理**（**NLP**）领域的神秘面纱，并为您提供足够的背景知识，让您可以开始构建自己的模型。
- en: 'In our first chapter, we will explore some of the basic concepts of machine
    learning. We will then take this a step further by examining the fundamentals
    of deep learning, neural networks, and some of the advantages that deep learning
    methods afford over basic machine learning techniques. Finally, we will take a
    more detailed look at deep learning, specifically with regard to NLP-specific
    tasks and how we can use deep learning models to gain insights from natural language. Specifically,
    we''ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一章中，我们将探讨一些机器学习的基本概念。然后，我们将进一步深入研究深度学习、神经网络以及深度学习方法相对于基本机器学习技术的优势。最后，我们将更详细地探讨深度学习，特别是在处理自然语言相关任务时，以及我们如何利用深度学习模型从自然语言中获取洞察力。具体来说，我们将涵盖以下主题：
- en: Overview of machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: Introduction to neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: NLP for machine learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的自然语言处理（NLP）
- en: Overview of machine learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: Fundamentally, machine learning is the algorithmic process used to identify
    patterns and extract trends from data. By training specific machine learning algorithms
    on data, a machine learning model may learn insights that aren't immediately obvious
    to the human eye. A medical imaging model may learn to detect cancer from images
    of the human body, while a sentiment analysis model may learn that a book review
    containing the words *good*, *excellent*, and *entertaining* is more likely to
    be a positive review than one containing the words *bad*, *terrible*, and *boring*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，机器学习是用于从数据中识别模式和提取趋势的算法过程。通过在数据上训练特定的机器学习算法，机器学习模型可能会学习到人眼不容易察觉的洞察力。医学成像模型可能会学习从人体图像中检测癌症，而情感分析模型可能会学习到包含“好”、“优秀”和“有趣”的书评更可能是正面评价，而包含“坏”、“糟糕”和“无聊”的书评更可能是负面评价。
- en: 'Broadly speaking, machine learning algorithms fall into two main categories:
    supervised learning and unsupervised learning.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，机器学习算法可以分为两大类：监督学习和无监督学习。
- en: Supervised learning
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: '**Supervised learning** covers any task where we wish to use an input to predict
    an output. Let''s say we wish to train a model to predict house prices. We know
    that larger houses tend to sell for more money, but we don''t know the exact relationship
    between price and size. A machine learning model can learn this relationship by
    looking at the data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**涵盖任何我们希望使用输入来预测输出的任务。假设我们希望训练一个模型来预测房屋价格。我们知道较大的房屋通常售价更高，但我们不知道价格与大小之间的确切关系。机器学习模型可以通过查看数据来学习这种关系：'
- en: '![Figure 1.1 – Table showing housing data'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 – 显示房屋数据的表格'
- en: '](img/B12365_01_1.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_1.jpg)'
- en: Figure 1.1 – Table showing housing data
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 显示房屋数据的表格
- en: 'Here, we have been given the sizes of four houses that recently sold, as well
    as the prices they sold for. Given the data on these four houses, can we use this
    information to make a prediction about a new house on the market? A simple machine
    learning model known as a **regression** can estimate the relationship between
    these two factors:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经得到了最近售出的四栋房屋的大小，以及它们售出的价格。鉴于这四栋房屋的数据，我们能否利用这些信息对市场上的新房屋进行预测？一个简单的机器学习模型，即**回归**，可以估计这两个因素之间的关系：
- en: '![Figure 1.2 – Output of the housing data'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2 – 房屋数据的输出'
- en: '](img/B12365_01_2.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_2.jpg)'
- en: Figure 1.2 – Output of the housing data
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 房屋数据的输出
- en: 'Given this historic data, we can use this data to estimate a relationship between
    **size** (X) and **price** (Y). Now that we have an estimation of the relationship
    between size and price, if we are given a new house where we just know its size,
    we can use this to predict its price using the learned function:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些历史数据，我们可以利用这些数据来估计**大小**（X）和**价格**（Y）之间的关系。现在我们已经估计出大小和价格之间的关系，如果我们得到一座新房屋的大小信息，我们可以使用这些信息来预测其价格，使用已学到的函数：
- en: '![Figure 1.3 – Predicting house prices'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.3 – 预测房屋价格'
- en: '](img/B12365_01_3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_3.jpg)'
- en: Figure 1.3 – Predicting house prices
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 预测房屋价格
- en: 'Therefore, all supervised learning tasks aim to learn some function of the
    model inputs to predict an output, given many examples of how input relates to
    output:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，所有监督学习任务的目标是学习模型输入的某些函数以预测输出，在给定许多示例的情况下，说明输入如何与输出相关：
- en: '*Given many (X, y), learn:*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定许多（X, y），学习：*'
- en: '*F (X) = y*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*F (X) = y*'
- en: 'The input to your number can consist of any number of features. Our simple
    house price model consisted of just a single feature (**size**), but we may wish
    to add more features to give an even better prediction (for example, number of
    bedrooms, size of the garden, and so on). So, more specifically, our supervised
    model learns a function in order to map a number of inputs to an output. This
    is given by the following equation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您的数字输入可以包含任意数量的特征。我们简单的房价模型仅包含一个特征（**大小**），但我们可能希望添加更多特征以获得更好的预测（例如，卧室数量，花园大小等）。因此，更具体地说，我们的监督模型学习一种函数，以便将多个输入映射到输出。这由以下方程给出：
- en: 'Given many *([X0, X1, X2,…,Xn], y),* learn:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 给定许多*([X0, X1, X2,…,Xn], y)*，学习：
- en: '*f(X**0, X1, X2,…,Xn) = y*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(X**0, X1, X2,…,Xn) = y*'
- en: 'In the preceding example, the function that we learn is as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们学到的函数如下：
- en: '![](img/Formula_01_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_001.jpg)'
- en: Here,![](img/Formula_01_002.png) is the *x* axis intercept and ![](img/Formula_01_003.png)
    is the slope of the line.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_01_002.png)是*x*轴截距，![](img/Formula_01_003.png)是直线的斜率。
- en: 'Models can consist of millions, even billions, of input features (though you
    may find that you run into hardware limitations when the feature space becomes
    too large). The types of inputs to a model may vary as well, with models being
    able to learn from images:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以由数百万，甚至数十亿个输入特征组成（尽管在特征空间过大时可能会遇到硬件限制）。模型的输入类型也可能各不相同，模型可以从图像中学习：
- en: '![Figure 1.4 – Model training'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.4 – 模型训练'
- en: '](img/B12365_01_4.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_4.jpg)'
- en: Figure 1.4 – Model training
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 模型训练
- en: 'As we shall explore in more detail later, they can also learn from text:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们稍后将详细探讨的那样，它们还可以从文本中学习：
- en: '*I loved this film* -> Positive'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢这部电影* -> 正面'
- en: '*This movie was terrible* -> Negative'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*这部电影太糟糕了* -> 负面'
- en: '*The best film I saw this year* -> ?'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*我今年看过的最好的电影* -> ?'
- en: Unsupervised learning
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: '**Unsupervised learning** differs from supervised learning in that unsupervised
    learning doesn''t use pairs of inputs and outputs (*X, y*) to learn. Instead,
    we only provide input data and the model will learn something about the structure
    or representation of the input data. One of the most common methods of unsupervised
    learning is **clustering**.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**无监督学习**与监督学习不同之处在于，无监督学习不使用输入和输出（*X, y*）的配对来学习。相反，我们只提供输入数据，模型将学习输入数据的结构或表示。无监督学习的最常见方法之一是**聚类**。'
- en: 'For example, we take a dataset of readings of temperatures and rainfall measures
    from a set of four different countries but have no labels about where these readings
    were taken. We can use a clustering algorithm to identify the distinct clusters
    (countries) that exist within the data:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们拿到了来自四个不同国家的温度和降雨量测量数据集，但没有标签说明这些测量数据来自哪里。我们可以使用聚类算法识别数据中存在的不同簇（国家）：
- en: '![Figure 1.5 – Output of the clustering algorithm'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.5 – 聚类算法的输出'
- en: '](img/B12365_01_5.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_5.jpg)'
- en: Figure 1.5 – Output of the clustering algorithm
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 聚类算法的输出
- en: Clustering also has uses within the realm of NLP. If we are given a dataset
    of emails and want to determine how many different languages are being spoken
    within these emails, a form of clustering could help us identify this. If English
    words appear frequently with other English words within the same email and Spanish
    words appear frequently with other Spanish words, we would use clustering to determine
    how many distinct clusters of words our dataset has and, thus, the number of languages.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类在自然语言处理中也有应用。如果我们有一个电子邮件数据集，并且想确定这些电子邮件中使用了多少种不同的语言，聚类的形式可以帮助我们确定这一点。如果英语单词在同一封电子邮件中经常与其他英语单词一起出现，并且西班牙语单词也经常与其他西班牙语单词一起出现，我们将使用聚类来确定我们的数据集中有多少个不同的单词簇，从而确定语言的数量。
- en: How do models learn?
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型如何学习？
- en: 'In order for models to learn, we need some way of evaluating how our model
    is performing. To do this, we use a concept called loss. **Loss** is a measure
    of how close our model predictions are from their true values. For a given house
    in our dataset, one measure of loss could be the difference between the true price
    (*y*) and the price predicted by our model (![](img/Formula_01_004.png)). We could
    assess the total loss within our system by taking an average of this loss across
    all houses in the dataset. However, the positive loss could theoretically cancel
    out negative loss, so a more common measure of loss is the **mean squared error**:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型学习，我们需要一些评估模型表现的方法。为此，我们使用了一个称为损失的概念。**损失**是衡量我们的模型预测与实际值有多接近的指标。对于数据集中的某个房屋来说，损失的一种度量可以是真实价格（*y*）与我们模型预测的价格（![](img/Formula_01_004.png)）之间的差异。我们可以通过计算数据集中所有房屋的这种损失的平均值来评估系统内的总损失。然而，正损失理论上可能会抵消负损失，因此更常见的损失度量是**均方误差**：
- en: '![](img/Formula_01_005.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_005.png)'
- en: While other models may use different loss functions, regressions generally use
    mean squared error. Now, we can calculate a measure of loss across our entire
    dataset, but we still need a way of algorithmically arriving at the lowest possible
    loss. This process is known as **gradient descent**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然其他模型可能使用不同的损失函数，但回归通常使用均方误差。现在，我们可以计算整个数据集的损失度量，但我们仍然需要一种算法上达到最低可能损失的方法。这个过程称为**梯度下降**。
- en: Gradient descent
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Here, we have plotted our loss function as it relates to a single learned parameter
    within our house price model, ![](img/Formula_01_006.png). We note that when ![](img/Formula_01_007.png)
    is set too high, the MSE loss is high, and when ![](img/Formula_01_008.png) is
    set too low, the MSE loss is also high. The *sweet spot*, or the point where the
    loss is minimized, lies somewhere in the middle. To calculate this algorithmically,
    we use gradient descent. We will see this in more detail when we begin to train
    our own neural networks:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们绘制了我们的损失函数与我们房价模型中的单个学习参数的关系，![](img/Formula_01_006.png)。我们注意到当![](img/Formula_01_007.png)设置得太高时，均方误差损失也很高，而当![](img/Formula_01_008.png)设置得太低时，均方误差损失同样很高。损失被最小化的“甜点”，或者说损失最小的点，位于中间某处。为了算法地计算这一点，我们使用梯度下降。当我们开始训练自己的神经网络时，我们将更详细地看到这一点：
- en: '![Figure 1.6 – Gradient descent'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.6 – 梯度下降'
- en: '](img/B12365_01_6.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_6.jpg)'
- en: Figure 1.6 – Gradient descent
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 – 梯度下降
- en: 'We first initialize ![](img/Formula_01_009.png) with a random value. To reach
    the point where our loss is minimized, we need to move further *downhill* from
    our loss function, toward the middle of the valley. To do this, we first need
    to know which direction to move in. At our initial point, we use basic calculus
    to calculate the initial gradient of the slope:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先用一个随机值初始化![](img/Formula_01_009.png)。为了达到使损失最小化的点，我们需要从损失函数的下坡处向中间移动。为了做到这一点，我们首先需要知道向哪个方向移动。在我们的初始点，我们使用基本的微积分来计算初始斜坡的梯度：
- en: '![](img/Formula_01_010.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_010.png)'
- en: In our preceding example, the gradient at the initial point is positive. This
    tells us that our value of ![](img/Formula_01_011.png) is larger than the optimal
    value, so we update our value of ![](img/Formula_01_012.png) so that it's lower
    than our previous value. We gradually iterate this process until  ![](img/Formula_01_013.png)
    moves closer and closer to the value where MSE is minimized. This happens at the
    point where the gradient equals zero.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的前述示例中，初始点处的梯度是正的。这告诉我们我们的![](img/Formula_01_011.png)的值大于最优值，所以我们更新我们的![](img/Formula_01_012.png)的值，使其低于先前的值。我们逐步迭代这个过程，直到![](img/Formula_01_013.png)越来越接近使均方误差最小化的值的点。这发生在梯度等于零的点。
- en: Overfitting and underfitting
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: 'Consider the following scenario, where a basic linear model is poorly fitted
    to our data. We can see that our model, denoted by the equation ![](img/Formula_01_014.png),
    does not appear to be a good predictor:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下情况，基本线性模型在我们的数据上拟合得很差。我们可以看到我们的模型，由方程![](img/Formula_01_014.png)表示，似乎不是一个很好的预测器：
- en: '![Figure 1.7 – Example of underfitting and overfitting'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7 – 欠拟合和过拟合示例'
- en: '](img/B12365_01_7.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_7.jpg)'
- en: Figure 1.7 – Example of underfitting and overfitting
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 – 欠拟合和过拟合示例
- en: 'When our model does not fit the data well because of a lack of features, lack
    of data, or model underspecification, we call this **underfitting**. We note the
    increasing gradient of our data and suspect that a model, if using a polynomial,
    may be a better fit; for example, ![](img/Formula_01_015.png). We will see later
    that due to the complex architecture of neural networks, underfitting is rarely
    an issue:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的模型由于特征不足、数据不足或模型规格不足而无法很好地拟合数据时，我们称之为**欠拟合**。我们注意到数据的梯度逐渐增加，并怀疑如果使用多项式，例如![](img/Formula_01_015.png)，模型可能会更好地拟合；我们稍后将看到，由于神经网络的复杂结构，欠拟合很少成为问题：
- en: 'Consider the following example. Here, we''re fitting a function using our house
    price model to not only the size of the house (*X*), but the second and third
    order polynomials too *(X2, X3)*. Here, we can see that our new model fits our
    data points perfectly. However, this does not necessarily result in a good model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例。在这里，我们使用我们的房价模型来拟合一个函数，不仅仅使用房屋大小（*X*），还使用了二次和三次多项式（*X2, X3*）。在这里，我们可以看到我们的新模型完美地拟合了我们的数据点。然而，这并不一定会导致一个好的模型：
- en: '![Figure 1.8 – Sample output of overfitting'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8 – 过拟合的样本输出'
- en: '](img/B12365_01_8.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_8.jpg)'
- en: Figure 1.8 – Sample output of overfitting
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8 – 过拟合的样本输出
- en: We now have a house of size **110 sq m** to predict the price of. Using our
    intuition, as this house is larger than the **100 sq m** house, we would expect
    this house to be more expensive at around **$340,000**. Using our fitted polynomial
    model, we can see that the predicted price is actually lower than the smaller
    house at around **$320,000**. Our model fits the data we have trained it on well,
    but it does not generalize well to a new, unseen datapoint. This is known as **overfitting**.
    Because of overfitting, it is important not to evaluate a model's performance
    on the data it was trained on, so we need to generate a separate set of data to
    evaluate our data on.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一栋**110平方米**的房子来预测价格。根据我们的直觉，因为这栋房子比**100平方米**的房子大，我们预计这栋房子的价格会更高，大约**$340,000**。然而，使用我们拟合的多项式模型，我们发现预测的价格实际上低于较小的房子，大约**$320,000**。我们的模型很好地拟合了训练数据，但对新的、未见过的数据点泛化能力不强。这被称为**过拟合**。因为过拟合的原因，重要的是不要在模型训练的数据上评估模型的性能，因此我们需要生成一个单独的数据集来评估我们的数据。
- en: Train versus test
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练与测试
- en: 'Normally, when training models, we separate our data into two parts: a training
    set of data and a smaller test set of data. We train the model using the training
    set of data and evaluate it on the test set of data. This is done in order to
    measure the model''s performance on an unseen set of data. As mentioned previously,
    for a model to be a good predictor, it must generalize well to a new set of data
    that the model hasn''t seen before, and this is precisely what evaluating on a
    testing set of data measures.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练模型时，我们将数据分为两部分：一个训练数据集和一个较小的测试数据集。我们使用训练数据集训练模型，并在测试数据集上评估其性能。这样做是为了衡量模型在未见过的数据集上的表现。正如前面提到的，要使模型成为一个良好的预测器，它必须很好地推广到模型之前没有见过的新数据集，这正是评估测试数据集的作用。
- en: Evaluating models
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'While we seek to minimize loss in our models, this alone does not give us much
    information about how good our model is at actually making predictions. Consider
    an anti-spam model that predicts whether a received email is spam or not and automatically
    sends spam emails to a junk folder. One simple measure of evaluating performance
    is **accuracy**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们努力在模型中最小化损失，但这本身并不能提供有关我们的模型在实际预测中表现如何的信息。考虑一个反垃圾邮件模型，它预测接收的电子邮件是否为垃圾邮件，并自动将垃圾邮件发送到垃圾文件夹。评估性能的一个简单指标是**准确率**：
- en: '![](img/Formula_01_016.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_016.png)'
- en: 'To calculate accuracy, we simply take the number of emails that were predicted
    correctly as spam/non-spam and divide this by the total number of predictions
    we made. If we correctly predicted 990 emails out of 1,000, we would have an accuracy
    of 99%. However, a high accuracy does not necessarily mean our model is good:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算准确率，我们只需将正确预测为垃圾邮件/非垃圾邮件的电子邮件数量除以我们总共进行的预测数量。如果我们在1,000封邮件中正确预测了990封，那么我们的准确率就是99%。然而，高准确率并不一定意味着我们的模型很好：
- en: '![Figure 1.9 – Table showing data predicted as spam/non-spam'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9 – 显示预测为垃圾邮件/非垃圾邮件的数据表'
- en: '](img/B12365_01_9.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_9.jpg)'
- en: Figure 1.9 – Table showing data predicted as spam/non-spam
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 显示预测为垃圾邮件/非垃圾邮件的数据表
- en: 'Here, we can see that although our model predicted 990 emails as not spam correctly
    (known as true negatives), it also predicted 10 emails that were spam as not spam
    (known as false negatives). Our model just assumes that all emails are not spam,
    which is not a good anti-spam filter at all! Instead of just using accuracy, we
    should also evaluate our model using **precision and recall**. In this scenario,
    the fact that our model would have a recall of zero (meaning no positive results
    were returned) would be an immediate red flag:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，尽管我们的模型正确预测了990封邮件不是垃圾邮件（称为真负），但它还预测了10封垃圾邮件不是垃圾邮件（称为假负）。我们的模型假定所有邮件都不是垃圾邮件，这根本不是一个好的反垃圾邮件过滤器！除了准确率之外，我们还应该使用**精确率和召回率**来评估我们的模型。在这种情况下，我们的模型召回率为零（意味着没有返回任何正结果），这将是一个立即的红旗：
- en: '![](img/Formula_01_017.jpg)![](img/Formula_01_018.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_017.jpg)![](img/Formula_01_018.jpg)'
- en: Neural networks
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: In our previous examples, we have discussed mainly regressions in the form ![](img/Formula_01_019.png).
    We have touched on using polynomials to fit more complex equations such as ![](img/Formula_01_020.png).
    However, as we add more features to our model, when to use a transformation of
    the original feature becomes a case of trial and error. Using **neural networks**,
    we are able to fit a much more complex function, *y = f(X)*, to our data, without
    the need to engineer or transform our existing features.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的示例中，我们主要讨论了形式为 ![](img/Formula_01_019.png) 的回归。我们已经涉及使用多项式来拟合诸如 ![](img/Formula_01_020.png)
    这样更复杂的方程。然而，随着我们向模型添加更多特征，何时使用原始特征的变换成为一个试错的过程。使用**神经网络**，我们能够将更复杂的函数 *y = f(X)*
    拟合到我们的数据中，而无需对现有特征进行工程化或转换。
- en: Structure of neural networks
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络的结构
- en: 'When we were learning the optimal value of ![](img/Formula_01_021.png), which
    minimized loss in our regressions, this is effectively the same as a **one-layer
    neural network**:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在学习 ![](img/Formula_01_021.png) 的最优值时，这实际上等同于一个**单层神经网络**：
- en: '![Figure 1.10 – One-layer neural network'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10 – 单层神经网络'
- en: '](img/B12365_01_10.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_10.jpg)'
- en: Figure 1.10 – One-layer neural network
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10 – 单层神经网络
- en: 'Here, we take each of our features, ![](img/Formula_01_022.png), as an input,
    illustrated here by a **node**. We wish to learn the parameters, ![](img/Formula_01_023.png),
    which are represented as **connections** in this diagram. Our final sum of all
    the products between ![](img/Formula_01_024.png) and ![](img/Formula_01_025.png)
    gives us our final prediction, *y*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将每个特征![](img/Formula_01_022.png)作为输入，这里用**节点**表示。我们希望学习参数![](img/Formula_01_023.png)，这在图中表示为**连接**。我们最终的所有![](img/Formula_01_024.png)和![](img/Formula_01_025.png)之间的乘积的总和给出了我们的最终预测*y*：
- en: '![](img/Formula_01_026.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_026.png)'
- en: 'A neural network simply builds upon this initial concept, adding extra layers
    to the calculation, thus increasing the complexity and the parameters learned,
    giving us something like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经网络简单地建立在这个初始概念之上，向计算中添加额外的层，从而增加复杂性和学习的参数，给我们像这样的东西：
- en: '![Figure 1.11 – Fully connected network'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11 – 全连接网络'
- en: '](img/B12365_01_11.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_11.jpg)'
- en: Figure 1.11 – Fully connected network
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11 – 全连接网络
- en: Every input node is connected to every node in another layer. This is known
    as a **fully connected layer**. The output from the fully connected layer is then
    multiplied by its own additional weights in order to predict *y*. Therefore, our
    predictions are no longer just a function of ![](img/Formula_01_027.png) but now
    include multiple learned weights against each parameter. Feature ![](img/Formula_01_028.png)
    is no longer affected not just by ![](img/Formula_01_029.png). Now, it is also
    affected by the ![](img/Formula_01_030.png). parameters.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入节点都连接到另一层中的每个节点。这被称为**全连接层**。全连接层的输出然后乘以它自己的额外权重，以预测*y*。因此，我们的预测不再只是![](img/Formula_01_027.png)的函数，而是包括每个参数的多个学习权重。特征![](img/Formula_01_028.png)不再仅仅受到![](img/Formula_01_029.png)的影响。现在，它还受到![](img/Formula_01_030.png)的影响。
- en: 'Since each node within the fully connected layer takes all values of *X* as
    input, the neural network is able to learn interaction features between the input
    features. Multiple fully connected layers can be chained together to learn even
    more complex features. In this book, we will see that all of the neural networks
    we build will use this concept; chaining together multiple layers of different
    varieties in order to construct even more complex models. However, there is one
    additional key element to cover before we can fully understand neural networks:
    activation functions.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于全连接层内的每个节点都将*X*的所有值作为输入，神经网络能够学习输入特征之间的交互特征。可以将多个全连接层串联在一起，以学习更复杂的特征。在本书中，我们将看到，我们构建的所有神经网络都将使用这一概念；将不同类型的多层串联在一起，以构建更复杂的模型。然而，在我们完全理解神经网络之前，还有一个额外的关键要素需要覆盖：激活函数。
- en: Activation functions
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'While chaining various weights together allows us to learn more complex parameters,
    ultimately, our final prediction will still be a combination of the linear products
    of weights and features. If we wish our neural networks to learn a truly complex,
    non-linear function, then we must introduce an element of nonlinearity into our
    model. This is done through the use of **activation functions**:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将各种权重串联在一起使我们能够学习更复杂的参数，但最终，我们的最终预测仍然是权重和特征的线性乘积的组合。如果我们希望我们的神经网络学习一个真正复杂的非线性函数，那么我们必须在我们的模型中引入非线性元素。这是通过使用**激活函数**来实现的：
- en: '![Figure 1.12 – Activation functions in neural networks'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12 – 神经网络中的激活函数'
- en: '](img/B12365_01_12.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_12.jpg)'
- en: Figure 1.12 – Activation functions in neural networks
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – 神经网络中的激活函数
- en: 'We apply an activation function to each node within our fully connected layer.
    What this means is that each node in the fully connected layer takes a sum of
    features and weights as input, applies a nonlinear function to the resulting value,
    and outputs the transformed result. While there are many different activation
    functions, the most frequently used in recent times is **ReLU**, or the **Rectified
    Linear Unit**:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每个全连接层的每个节点应用一个激活函数。这意味着全连接层中的每个节点都将特征和权重的和作为输入，将非线性函数应用于结果值，并输出转换后的结果。虽然有许多不同的激活函数，但最近最常用的是**ReLU**，或**修正线性单元**：
- en: '![Figure 1.13 – Representation of ReLU output'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13 – ReLU 输出的表示'
- en: '](img/B12365_01_13.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_13.jpg)'
- en: Figure 1.13 – Representation of ReLU output
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – ReLU 输出的表示
- en: ReLU is a very simple non-linear function that returns *y = 0* when ![](img/Formula_01_031.png)
    and *y = X* when *X > 0*. After introducing these activation functions to our
    model, our final learned function becomes nonlinear, meaning we can create more
    models than we would have been able to using a combination of conventional regression
    and feature engineering alone.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 是一个非常简单的非线性函数，在![](img/Formula_01_031.png)时返回*y = 0*，在*X > 0*时返回*y = X*。在我们的模型中引入这些激活函数后，我们的最终学习函数变得非线性，这意味着我们可以比仅使用传统回归和特征工程组合创建更多的模型。
- en: How do neural networks learn?
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络如何学习？
- en: The act of learning from our data using neural networks is slightly more complicated
    than when we learned using basic regressions. While we still use gradient descent
    as before, the actual loss function we need to differentiate becomes significantly
    more complex. In a one-layered neural network with no activation functions, we
    can easily calculate the derivative of the loss function as it is easy to see
    how the loss function changes as we vary each parameter. However, in a multi-layered
    neural network with activation functions, this is more complex.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络从我们的数据中学习的过程，比使用基本回归时稍微复杂一些。尽管我们仍然使用之前的梯度下降，但我们需要区分的实际损失函数变得显著复杂。在没有激活函数的单层神经网络中，我们可以轻松计算损失函数的导数，因为我们可以清楚地看到损失函数在每个参数变化时的变化。然而，在具有激活函数的多层神经网络中，情况就复杂得多了。
- en: 'We must first perform a **forward-pass**, which is where, using the model''s
    current state, we compute the predicted value of *y* and evaluate this against
    the true value of *y* in order to obtain a measure of loss. Using this loss, we
    move backward through the network, calculating the gradient at each parameter
    within the network. This allows us to know which direction to update our parameter
    in so that we can move closer toward the point where loss is minimized. This is
    known as **backpropagation**. We can calculate the derivative of the loss function
    with respect to each parameter using the **chain rule**:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先执行**前向传播**，这是使用模型的当前状态计算预测值*y*并将其与真实值*y*进行评估以获取损失度量的过程。利用这个损失，我们向网络反向传播，计算网络中每个参数的梯度。这使我们能够知道应该朝哪个方向更新我们的参数，以便我们能够朝着最小化损失的点移动。这就是所谓的**反向传播**。我们可以使用**链式法则**计算损失函数相对于每个参数的导数：
- en: '![](img/Formula_01_032.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_01_032.png)'
- en: 'Here, ![](img/Formula_01_033.png) is the output at each given node within the
    network. So, to summarize, the four main steps we take when performing gradient
    descent on neural networks are as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_01_033.png) 是网络中每个给定节点的输出。因此，总结一下，在神经网络上执行梯度下降时我们采取的四个主要步骤如下：
- en: Perform a forward pass using your data, calculating the total loss of the network.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的数据执行前向传播，计算网络的总损失。
- en: Using backpropagation, calculate the gradients of each parameter with respect
    to loss at each node in the network.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播，计算网络中每个节点处损失相对于每个参数的梯度。
- en: Update the values of these parameters, moving toward the direction where loss
    is minimized.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新这些参数的值，朝着最小化损失的方向移动。
- en: Repeat until convergence.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到收敛为止。
- en: Overfitting in neural networks
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的过拟合
- en: 'We saw that, in the case of our regressions, it was possible to add so many
    features that it was possible to overfit the network. This gets to a point where
    the model fits the training data perfectly but does not generalize well to an
    unseen test set of data. This is a common problem in neural networks as the increased
    complexity of the models means that it is often possible to fit a function to
    the training set of data that doesn''t necessarily generalize. The following is
    a plot of the total loss on the training and test sets of data after each forward
    and backward pass of the dataset (known as an epoch):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在回归的情况下，可以添加很多特征，这样就可能对网络进行过度拟合。这导致模型完全适合训练数据，但对未见过的测试数据集的泛化能力不强。这在神经网络中是一个常见问题，因为模型复杂度增加意味着往往可以将函数拟合到训练数据集，但这不一定具有泛化能力。以下是在每次数据集的前向和后向传播（称为一个
    epoch）之后训练和测试数据集的总损失的图表：
- en: '![Figure 1.14 – Test and training epochs'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14 – 测试和训练轮次'
- en: '](img/B12365_01_14.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B12365_01_14.jpg)'
- en: Figure 1.14 – Test and training epochs
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 – 测试和训练轮次
- en: Here, we can see that as we continue to train the network, the training loss
    gets smaller over time as we move closer to the point where the total loss is
    minimized. While this generalizes well to the test set of data up to a point,
    after a while, the total loss on the test set of data begins to increase as our
    function overfits to the data in the training set. One solution to this is **early
    stopping**. Because we want our model to make good predictions on data it hasn't
    seen before, we can stop training our model at the point where test loss is minimized.
    A fully trained NLP model may be able to easily classify sentences it has seen
    before, but the measure of a model that has truly learned something is its ability
    to make predictions on unseen data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续训练网络的过程中，随着时间的推移，训练损失会逐渐减小，直到我们接近最小化总损失的点。尽管这在一定程度上对测试数据集泛化良好，但是一段时间后，测试数据集上的总损失开始增加，因为我们的函数对训练集中的数据过拟合了。解决这个问题的一个方法是**早停法**。因为我们希望模型能够在未见过的数据上做出良好的预测，我们可以在测试损失最小化的点停止训练模型。一个完全训练好的自然语言处理模型可能能够轻松分类它之前见过的句子，但真正学到东西的模型的衡量标准是它在未见数据上的预测能力。
- en: NLP for machine learning
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的自然语言处理
- en: Unlike humans, computers do not understand text – at least not in the same way
    that we do. In order to create machine learning models that are able to learn
    from data, we must first learn to represent natural language in a way that computers
    are able to process.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类不同，计算机并不以我们理解的方式理解文本。为了创建能够从数据中学习的机器学习模型，我们必须首先学会以计算机能够处理的方式表示自然语言。
- en: When we discussed machine learning fundamentals, you may have noticed that loss
    functions all deal with numerical data so as to be able to minimize loss. Because
    of this, we wish to represent our text in a numerical format that can form the
    basis of our input into a neural network. Here, we will cover a couple of basic
    ways of numerically representing our data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论机器学习的基本原理时，你可能已经注意到损失函数都处理数值数据，以便能够最小化损失。因此，我们希望将文本表示为一个能够成为神经网络输入基础的数值格式。在这里，我们将介绍几种基本的文本数值表示方法。
- en: Bag-of-words
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**词袋模型**'
- en: 'The first and most simple way of representing text is by using a **bag-of-words**
    representation. This method simply counts the words in a given sentence or document
    and counts all the words. These counts are then transformed into a vector where
    each element of the vector is the count of the times each word in the **corpus**
    appears within the sentence. The corpus is simply all the words that appear across
    all the sentences/documents being analyzed. Take the following two sentences:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表示文本最简单也是最简单的方法之一是使用**词袋模型**表示。这种方法简单地计算给定句子或文档中的单词，并计算所有单词的数量。然后，这些计数被转换为向量，向量的每个元素是语料库中每个单词在句子中出现的次数。语料库简单来说就是分析的所有句子/文档中出现的所有单词。看下面的两个句子：
- en: '*The cat sat on the mat*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*猫坐在垫子上*'
- en: '*The dog sat on the cat*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*狗坐在猫上*'
- en: 'We can represent each of these sentences as a count of words:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将每个句子表示为单词计数：
- en: '![Figure 1.15 – Table of word counts'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15 – 单词计数表'
- en: '](img/B12365_01_15.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[img/B12365_01_15.jpg)'
- en: Figure 1.15 – Table of word counts
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15 – 单词计数表
- en: 'Then, we can transform these into individual vectors:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这些转换为单个向量：
- en: '*The cat sat on the mat -> [2,1,0,1,1,1]*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*猫坐在垫子上 -> [2,1,0,1,1,1]*'
- en: '*The dog sat on the cat -> [2,1,1,1,1,0]*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*狗坐在猫上 -> [2,1,1,1,1,0]*'
- en: This numeric representation could then be used as the input features to a machine
    learning model where the feature vector is ![](img/Formula_01_034.png).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这种数值表示可以用作机器学习模型的输入特征向量，其中特征向量是 ![](img/Formula_01_034.png)。
- en: Sequential representation
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序表示法
- en: 'We will see later in this book that more complex neural network models, including
    RNNs and LSTMs, do not just take a single vector as input, but can take a whole
    sequence of vectors in the form of a matrix. Because of this, in order to better
    capture the order of words and thus the meaning of any sentence, we are able to
    represent this in the form of a sequence of one-hot encoded vectors:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书后面看到，更复杂的神经网络模型，包括循环神经网络（RNNs）和长短期记忆网络（LSTMs），不仅仅接受单个向量作为输入，而是可以接受整个向量序列形式的矩阵。因此，为了更好地捕捉单词的顺序和句子的意义，我们可以以一系列独热编码向量的形式表示这些内容：
- en: '![Figure 1.16 – One-hot encoded vectors'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.16 – 独热编码向量'
- en: '](img/B12365_01_16.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[img/B12365_01_16.jpg)'
- en: Figure 1.16 – One-hot encoded vectors
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16 – 独热编码向量
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the fundamentals of machine learning and neural
    networks, as well as a brief overview of transforming text for use within these
    models. In the next chapter, we will provide a brief overview of PyTorch and how
    it can be used to construct some of these models.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习和神经网络的基础知识，以及转换文本以供这些模型使用的简要概述。在下一章中，我们将简要介绍PyTorch及其如何用于构建这些模型。
