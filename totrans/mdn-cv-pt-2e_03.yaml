- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: PyTorch Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 基础知识
- en: In the previous chapter, we learned about the fundamental building blocks of
    a neural network and also implemented forward- and backpropagation from scratch
    in Python.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了神经网络的基本构建模块，并且在 Python 中从头开始实现了前向传播和反向传播。
- en: In this chapter, we will dive into the foundations of building a neural network
    using PyTorch, which we will leverage multiple times in subsequent chapters when
    we learn about various use cases in image analysis. We will start by learning
    about the core data type that PyTorch works on – tensor objects. We will then
    dive deep into the various operations that can be performed on tensor objects
    and how to leverage them when building a neural network model on top of a toy
    dataset (so that we strengthen our understanding before we gradually look at more
    realistic datasets, starting with the next chapter). This will allow us to gain
    an understanding of how to build neural network models using PyTorch to map input
    and output values. Finally, we will learn about implementing custom loss functions
    so that we can customize them based on the use case we are solving.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨使用 PyTorch 构建神经网络的基础知识，在后续章节中，当我们学习图像分析的各种用例时，将多次利用这些知识。我们将首先学习
    PyTorch 工作的核心数据类型——张量对象。然后，我们将深入探讨可以在张量对象上执行的各种操作，以及在构建神经网络模型时如何利用它们，这是在一个玩具数据集上（在我们逐步查看更现实的数据集之前，从下一章开始）加强我们的理解。这将使我们能够理解如何使用
    PyTorch 构建神经网络模型以映射输入和输出值。最后，我们将学习如何实现自定义损失函数，以便根据我们解决的用例定制它们。
- en: 'Specifically, this chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，本章将涵盖以下主题：
- en: Installing PyTorch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: PyTorch tensors
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 张量
- en: Building a neural network using PyTorch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建神经网络
- en: Using a sequential method to build a neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用顺序方法构建神经网络
- en: Saving and loading a PyTorch model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载 PyTorch 模型
- en: 'All code in this chapter is available for reference in the `Chapter02` folder
    of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的 GitHub 仓库的 `Chapter02` 文件夹中提供了本章中的所有代码的参考：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Installing PyTorch
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: PyTorch provides multiple functionalities that aid in building a neural network
    – abstracting the various components using high-level methods, and also providing
    us with tensor objects that leverage GPUs to train a neural network faster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了多个功能，有助于构建神经网络——使用高级方法抽象各种组件，并且还提供了利用 GPU 加速训练神经网络的张量对象。
- en: Before installing PyTorch, we first need to ensure that Python is installed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 PyTorch 之前，我们首先需要确保已安装 Python。
- en: 'Next, we’ll install PyTorch, which is quite simple:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将安装 PyTorch，这相当简单：
- en: 'Visit the **QUICK START LOCALLY** section on the [https://pytorch.org/](https://pytorch.org/)
    website and choose your operating system (**Your OS**), **Conda** for **Package**,
    **Python** for **Language**, and **CPU** for **Compute Platform**. If you have
    CUDA libraries, you may choose the appropriate version:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://pytorch.org/](https://pytorch.org/) 网站上的 **QUICK START LOCALLY**
    部分，并选择您的操作系统 (**Your OS**)，**Conda** 作为 **Package**，**Python** 作为 **Language**，以及
    **CPU** 作为 **Compute Platform**。如果您有 CUDA 库，可以选择适当的版本：
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_02_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图的描述](img/B18457_02_01.png)'
- en: 'Figure 2.1: Installing PyTorch'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：安装 PyTorch
- en: This will prompt you to run a command such as `conda install pytorch torchvision
    torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia` in your terminal.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提示您在终端中运行命令，例如 `conda install pytorch torchvision torchaudio pytorch-cuda=12.1
    -c pytorch -c nvidia`。
- en: Run the command in Command Prompt/Terminal and let Anaconda install PyTorch
    and the necessary dependencies.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符/终端中运行命令，让 Anaconda 安装 PyTorch 和必要的依赖项。
- en: If you own an NVIDIA graphics card as a hardware component, it is highly recommended
    to install CUDA drivers, which accelerate deep learning training by orders of
    magnitude. Refer to the *Appendix* within the GitHub repo for this book for instructions
    on how to install CUDA drivers. Once you have them installed, you can select **12.1**
    as the CUDA version and use that command instead to install PyTorch.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您拥有 NVIDIA 显卡作为硬件组件，强烈建议安装 CUDA 驱动程序，这将大幅加速深度学习训练。有关如何安装 CUDA 驱动程序的说明，请参阅本书的
    GitHub 仓库中的 *附录*。安装完毕后，您可以选择 **12.1** 作为 CUDA 版本，并使用该命令安装 PyTorch。
- en: 'You can execute `python` in Command Prompt/Terminal and then type the following
    to verify that PyTorch is indeed installed:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在命令提示符/终端中执行`python`，然后输入以下内容以验证PyTorch确实已安装：
- en: '[PRE0]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'All the code in this book can be executed in Google Colab: [https://colab.research.google.com/](https://colab.research.google.com/).
    Python and PyTorch are available by default in Google Colab. We highly encourage
    you to execute all code on Colab – which includes access to the GPU too, for free!
    Thanks to Google for providing such an excellent resource!'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本书中的所有代码都可以在Google Colab中执行：[https://colab.research.google.com/](https://colab.research.google.com/)。Python和PyTorch在Google
    Colab中默认可用。我们强烈建议您在Colab中执行所有代码 – 这还包括免费访问GPU！感谢Google提供了这样一个出色的资源！
- en: So, we have successfully installed PyTorch. We will now perform some basic tensor
    operations in Python to help you get the hang of things.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经成功安装了PyTorch。现在我们将在Python中执行一些基本的张量操作，帮助您掌握相关内容。
- en: PyTorch tensors
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch张量
- en: 'Tensors are the fundamental data types of PyTorch. A tensor is a multidimensional
    matrix similar to NumPy’s ndarrays:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是PyTorch的基本数据类型。张量是类似于NumPy的ndarray的多维矩阵：
- en: A scalar can be represented as a zero-dimensional tensor.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标量可以表示为零维张量。
- en: A vector can be represented as a one-dimensional tensor.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向量可以表示为一维张量。
- en: A two-dimensional matrix can be represented as a two-dimensional tensor.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 二维矩阵可以表示为二维张量。
- en: A multi-dimensional matrix can be represented as a multi-dimensional tensor.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多维矩阵可以表示为多维张量。
- en: 'Pictorially, the tensors look as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，张量如下所示：
- en: '![Diagram  Description automatically generated](img/B18457_02_02.png)Figure
    2.2: Tensor representation'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![自动生成的图表描述](img/B18457_02_02.png)图 2.2：张量表示'
- en: For instance, we can consider a color image as a three-dimensional tensor of
    pixel values, since a color image consists of `height x width x 3` pixels – where
    the three channels correspond to the RGB channels. Similarly, a grayscale image
    can be considered a two-dimensional tensor, as it consists of `height x width`
    pixels.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将彩色图像视为像素值的三维张量，因为彩色图像由`height x width x 3`个像素组成 – 其中三个通道对应RGB通道。类似地，灰度图像可以被视为二维张量，因为它由`height
    x width`个像素组成。
- en: By the end of this section, we will have learned why tensors are useful and
    how to initialize them, as well as how to perform various operations on top of
    tensors. This will serve as a base for when we leverage tensors to build a neural
    network model later in this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本节末尾，我们将学习张量为何有用，如何初始化它们，以及如何在张量之上执行各种操作。这将为我们后面在本章中利用张量构建神经网络模型打下基础。
- en: Initializing a tensor
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化张量
- en: 'Tensors are useful in multiple ways. Apart from using them as base data structures
    for images, one more prominent use for them is when tensors are leveraged to initialize
    the weights connecting different layers of a neural network. In this section,
    we will practice the different ways of initializing a tensor object:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 张量在多种情况下都很有用。除了将它们作为图像的基本数据结构使用外，它们还在连接神经网络不同层的权重时发挥了更显著的作用。在本节中，我们将练习初始化张量对象的不同方法：
- en: 'The following code is available can be found in the `Initializing_a_tensor.ipynb`
    file located in the `Chapter02` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的`Chapter02`文件夹中的`Initializing_a_tensor.ipynb`文件中可以找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Import PyTorch and initialize a tensor by calling `torch.tensor` on a list:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入PyTorch并通过在列表上调用`torch.tensor`来初始化一个张量：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, access the tensor object’s shape and data type:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，访问张量对象的形状和数据类型：
- en: '[PRE2]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The data type of all elements within a tensor is the same. That means if a
    tensor contains data of different data types (such as a Boolean, an integer, and
    a float), the entire tensor is coerced to the most generic data type:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 张量中所有元素的数据类型都是相同的。这意味着如果一个张量包含不同数据类型的数据（例如布尔值、整数和浮点数），整个张量将被强制转换为最通用的数据类型：
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see in the output of the preceding code, `False`, which was a Boolean,
    and `1`, which was an integer, were converted into floating-point numbers.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您在前面代码的输出中看到的那样，布尔值`False`和整数`1`被转换为浮点数。
- en: Alternatively, similar to NumPy, we can initialize tensor objects using built-in
    functions. Note that the parallels that we drew between tensors and weights of
    a neural network come to light now – where we are initializing tensors so that
    they represent the weight initialization of a neural network.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，与NumPy类似，我们可以使用内置函数初始化张量对象。请注意，我们现在描绘的张量和神经网络权重之间的类比现在变得明显 - 我们正在初始化张量以表示神经网络的权重初始化。
- en: 'Generate a tensor object that has three rows and four columns filled with zeros:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个由三行四列填充为0的张量对象：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Generate a tensor object that has three rows and four columns filled with ones:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个由三行四列填充为1的张量对象：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Generate three rows and four columns of values between 0 and 10 (including
    the low value but not including the high value):'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成三行四列的数值，其取值范围在0到10之间（包括低值但不包括高值）：
- en: '[PRE6]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Generate random numbers between 0 and 1 with three rows and four columns:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成三行四列的随机数，取值范围在0到1之间：
- en: '[PRE7]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Generate numbers that follow a normal distribution with three rows and four
    columns:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成遵循正态分布的数值，有三行四列：
- en: '[PRE8]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can directly convert a NumPy array into a Torch tensor using `torch.tensor(<numpy-array>)`:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以直接使用`torch.tensor(<numpy-array>)`将NumPy数组转换为Torch张量：
- en: '[PRE9]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that we have learned about initializing tensor objects, we will learn about
    performing various matrix operations on top of them next.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何初始化张量对象，接下来我们将学习如何在其上执行各种矩阵操作。
- en: Operations on tensors
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量的操作
- en: 'Similar to NumPy, you can perform various basic operations on tensor objects.
    Parallels to neural network operations are the matrix multiplication of input
    with weights, the addition of bias terms, and reshaping input or weight values
    when required. Each of these and additional operations are done as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与NumPy类似，您可以对张量对象执行各种基本操作。神经网络操作的类比包括将输入与权重进行矩阵乘法、加上偏差项以及在需要时重新塑造输入或权重值。每个操作以及其他操作如下执行：
- en: The following code can be found in the `Operations_on_tensors.ipynb` file in
    the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在本书的GitHub代码库的`Chapter02`文件夹中的`Operations_on_tensors.ipynb`文件中找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Multiplication of all the elements present in `x` by `10` can be performed
    using the following code:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`x`中所有元素乘以`10`可以通过以下代码执行：
- en: '[PRE10]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Adding `10` to the elements in `x` and storing the resulting tensor in `y`
    can be performed using the following code:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`x`中的元素加`10`并将结果张量存储在`y`中的操作可以通过以下代码实现：
- en: '[PRE11]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Reshaping a tensor can be performed using the following code:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过以下代码执行张量的重新塑形：
- en: '[PRE12]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Another way to reshape a tensor is by using the `squeeze` method, where we
    provide the axis index that we want to remove. Note that this is applicable only
    when the axis we want to remove has only one item in that dimension:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种重新塑形张量的方法是使用`squeeze`方法，我们提供要移除的轴索引。请注意，仅当要移除的轴在该维度中只有一个项目时才适用：
- en: '[PRE13]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The opposite of `squeeze` is `unsqueeze`, which means we add a dimension to
    the matrix, which can be performed using the following code:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`squeeze`的相反操作是`unsqueeze`，意味着我们向矩阵中添加一个维度，可以通过以下代码执行：'
- en: '[PRE14]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Using `None` for indexing is a fancy way of unsqueezing, as shown, and will
    be used often in this book to create fake channel/batch dimensions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引中使用`None`是一种添加维度的巧妙方式，如所示，将经常在本书中用于创建假通道/批量维度。
- en: 'Matrix multiplication of two different tensors can be performed using the following
    code:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过以下代码执行两个不同张量的矩阵乘法：
- en: '[PRE15]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Alternatively, matrix multiplication can also be performed by using the `@`
    operator:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，也可以使用`@`运算符执行矩阵乘法：
- en: '[PRE16]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Similar to `concatenate` in NumPy, we can perform concatenation of tensors
    using the `cat` method:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与NumPy中的`concatenate`类似，我们可以使用`cat`方法对张量进行连接：
- en: '[PRE17]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Extraction of the maximum value in a tensor can be performed using the following
    code:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以下代码可以提取张量中的最大值：
- en: '[PRE18]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can extract the maximum value along with the row index where the maximum
    value is present:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以提取最大值以及最大值所在的行索引：
- en: '[PRE19]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that, in the preceding output, we fetch the maximum values across dimension
    `0`, which is the rows of the tensor. Hence, the maximum values across all rows
    are the values present in the 4th index and, hence, the `indices` output is all
    fours too. Furthermore, `.max` returns both the maximum values and the location
    (`argmax`) of the maximum values.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在上述输出中，我们获取维度`0`（即张量的行）上的最大值。因此，所有行的最大值是第4个索引处的值，因此`indices`输出也全为四。此外，`.max`返回最大值及其位置（`argmax`）。
- en: 'Similarly, the output when fetching the maximum value across columns is as
    follows:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类似地，当获取跨列的最大值时，输出如下：
- en: '[PRE20]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `min` operation is exactly the same as `max` but returns the minimum and
    arg-minimum where applicable.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`min`操作与`max`完全相同，但在适用时返回最小值和最小位置。'
- en: 'Permute the dimensions of a tensor object:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排列张量对象的维度：
- en: '[PRE21]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that the shape of the tensor changes when we perform permute on top of
    the original tensor.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们在原始张量上执行置换时，张量的形状会发生变化。
- en: Never reshape (that is, use `tensor.view` on) a tensor to swap the dimensions.
    Even though Torch will not throw an error, this is wrong and will create unforeseen
    results during training. If you need to swap dimensions, always use permute.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要重新形状（即在`tensor.view`上使用），以交换维度。即使Torch不会报错，这也是错误的，并且会在训练期间产生意想不到的结果。如果需要交换维度，请始终使用permute。
- en: Since it is difficult to cover all the available operations in this book, it
    is important to know that you can do almost all NumPy operations in PyTorch with
    almost the same syntax as NumPy. Standard mathematical operations, such as `abs`,
    `add`, `argsort`, `ceil`, `floor`, `sin`, `cos`, `tan`, `cumsum`, `cumprod`, `diag`,
    `eig`, `exp`, `log`, `log2`, `log10`, `mean`, `median`, `mode`, `resize`, `round`,
    `sigmoid`, `softmax`, `square`, `sqrt`, `svd`, and `transpose`, to name a few,
    can be directly called on any tensor with or without axes where applicable. You
    can always run `dir(torch.Tensor)` to see all the methods possible for a Torch
    tensor and `help(torch.Tensor.<method>)` to go through the official help and documentation
    for that method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在本书中涵盖所有可用操作很困难，重要的是要知道您几乎可以用与NumPy几乎相同的语法在PyTorch中执行几乎所有NumPy操作。标准的数学运算，如`abs`、`add`、`argsort`、`ceil`、`floor`、`sin`、`cos`、`tan`、`cumsum`、`cumprod`、`diag`、`eig`、`exp`、`log`、`log2`、`log10`、`mean`、`median`、`mode`、`resize`、`round`、`sigmoid`、`softmax`、`square`、`sqrt`、`svd`和`transpose`等等，可以直接在具有或不具有适用轴的任何张量上调用。您可以随时运行`dir(torch.Tensor)`来查看Torch张量的所有可能方法，并运行`help(torch.Tensor.<method>)`以查看该方法的官方帮助和文档。
- en: Next, we will learn about leveraging tensors to perform gradient calculations
    on top of data, which is a key aspect of performing backpropagation in neural
    networks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何利用张量在数据上执行梯度计算，这是神经网络中反向传播的关键方面。
- en: Auto gradients of tensor objects
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量对象的自动梯度
- en: As we saw in the previous chapter, differentiation and calculating gradients
    play a critical role in updating the weights of a neural network. PyTorch’s tensor
    objects come with built-in functionality to calculate gradients.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中看到的，微分和计算梯度在更新神经网络的权重中起着至关重要的作用。 PyTorch的张量对象具有内置功能来计算梯度。
- en: The following code can be found in the `Auto_gradient_of_tensors.ipynb` file
    in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书GitHub存储库的`Chapter02`文件夹中的`Auto_gradient_of_tensors.ipynb`文件中找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'In this section, we will understand how to calculate the gradients of a tensor
    object using PyTorch:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解如何使用PyTorch计算张量对象的梯度：
- en: 'Define a tensor object and also specify that it requires a gradient to be calculated:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个张量对象，并指定需要计算梯度：
- en: '[PRE22]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding code, the `requires_grad` parameter specifies that the gradient
    is to be calculated for the tensor object.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`requires_grad`参数指定要为张量对象计算梯度。
- en: Next, define the way to calculate the output, which in this specific case is
    the sum of the squares of all inputs:![](img/B18457_02_001.png)
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义计算输出的方式，这在本例中是所有输入平方和的总和：![](img/B18457_02_001.png)
- en: 'This is represented in code using the following line:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这在代码中用以下行表示：
- en: '[PRE23]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We know that the gradient of the preceding function is *2*x*. Let’s validate
    this using the built-in functions provided by PyTorch.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们知道前述函数的梯度是*2*x*。让我们使用PyTorch提供的内置函数来验证这一点。
- en: 'The gradient of a variable can be calculated by calling the `backward()` method.
    In our case, we calculate the gradient – change in `out` (output) for a small
    change in `x` (input) – as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可通过调用 `backward()` 方法计算变量的梯度。在我们的情况下，我们计算梯度 - 输出 `out` 对输入 `x` 的微小变化的影响 - 如下所示：
- en: '[PRE24]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We are now in a position to obtain the gradient of `out` with respect to `x`,
    as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以获取相对于 `x` 的 `out` 的梯度如下：
- en: '[PRE25]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This results in the following output:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE26]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Notice that the gradients obtained previously match with the intuitive gradient
    values (which are two times the value of *x*).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，先前获得的梯度与直观梯度值（即 *x* 的两倍）匹配。
- en: As an exercise, try recreating the scenario in `Chain rule.ipynb` in *Chapter
    1* with PyTorch. Compute the gradients after making a forward pass and make a
    single update. Verify that the updated weights match what we calculated in the
    notebook.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，在 *第 1 章* 的 `Chain rule.ipynb` 中尝试重新创建此场景，并使用 PyTorch 计算梯度。在进行前向传播后进行单次更新后，验证更新的权重是否与我们在笔记本中计算的相匹配。
- en: So far, we have learned about initializing, manipulating, and calculating gradients
    on top of a tensor object, which together constitute the fundamental building
    blocks of a neural network. Except for calculating auto gradients, initializing
    and manipulating data can also be performed using NumPy arrays. This calls for
    us to understand the reason why you should use tensor objects over NumPy arrays
    when building a neural network, which we will go through in the next section.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了在张量对象的顶部初始化、操作和计算梯度，这些构成神经网络的基本构建模块。除了计算自动梯度外，还可以使用 NumPy 数组进行初始化和操作数据。这要求我们理解为什么在构建神经网络时应使用张量对象而不是
    NumPy 数组的原因，我们将在下一节中详细介绍。
- en: Advantages of PyTorch’s tensors over NumPy’s ndarrays
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 张量相对于 NumPy 的 ndarray 的优势
- en: In the previous chapter, we saw that when calculating the optimal weight values,
    we vary each weight by a small amount and understand its impact on reducing the
    overall loss value. Note that the loss calculation based on the weight update
    of one weight does not impact the loss calculation of the weight update of other
    weights in the same iteration. Thus, this process can be optimized if each weight
    update is made by a different core in parallel instead of updating weights sequentially.
    A GPU comes in handy in this scenario, as it consists of thousands of cores when
    compared to a CPU (which, in general, could have <=64 cores).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们看到在计算最优权重值时，我们通过微小变化每个权重，并理解其对减少总体损失值的影响。请注意，基于一个权重更新的损失计算不会影响同一迭代中其他权重更新的损失计算。因此，如果每个权重更新由不同的核并行进行而不是顺序更新权重，则可以优化此过程。在这种情况下，GPU非常有用，因为它在核心数量上比CPU多得多（一般情况下，CPU可能有
    <=64 核）。
- en: 'A Torch tensor object is optimized to work with a GPU compared to NumPy. To
    understand this further, let’s perform a small experiment, where we perform the
    operation of matrix multiplication using NumPy arrays in one scenario and tensor
    objects in another, comparing the time taken to perform matrix multiplication
    in both scenarios:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 与 NumPy 相比，Torch 张量对象被优化以在 GPU 上工作。为了进一步理解这一点，让我们进行一个小实验，在这个实验中，我们在一个场景中使用 NumPy
    数组进行矩阵乘法操作，而在另一个场景中使用张量对象，并比较执行矩阵乘法所需的时间：
- en: The following code can be found in the `Numpy_Vs_Torch_object_computation_speed_comparison.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可在本书 GitHub 存储库的 `Chapter02` 文件夹中的 `Numpy_Vs_Torch_object_computation_speed_comparison.ipynb`
    文件中找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Generate two different `torch` objects:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成两个不同的 `torch` 对象：
- en: '[PRE27]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define the device to which we will store the tensor objects we created in *step
    1*:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们将在 *步骤 1* 中创建的张量对象存储到的设备：
- en: '[PRE28]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that if you don’t have a GPU device, the device will be `cpu` (furthermore,
    you would not notice the drastic difference in time taken to execute when using
    a CPU).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您没有 GPU 设备，设备将是 `cpu`（此外，当使用 CPU 时，您不会注意到执行所需的时间差异很大）。
- en: 'Register the tensor objects that were created in *step 1* with the device (registering
    tensor objects means storing information in a device):'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册在 *步骤 1* 中创建的张量对象到设备上（注册张量对象意味着将信息存储在设备中）：
- en: '[PRE29]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Perform matrix multiplication of the Torch objects, and also time it so that
    we can compare the speed to a scenario where matrix multiplication is performed
    on NumPy arrays:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Torch对象上执行矩阵乘法，并计时以便我们可以将其与在NumPy数组上执行矩阵乘法的速度进行比较：
- en: '[PRE30]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Perform matrix multiplication of the same tensors on `cpu`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`cpu`上执行相同张量的矩阵乘法：
- en: '[PRE31]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Perform the same matrix multiplication, this time on NumPy arrays:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行相同的矩阵乘法，这次在NumPy数组上进行：
- en: '[PRE32]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You will notice that the matrix multiplication performed on Torch objects on
    a GPU is ~18X faster than Torch objects on a CPU, and ~40X faster than the matrix
    multiplication performed on NumPy arrays. In general, `matmul` with Torch tensors
    on a CPU is still faster than NumPy. Note that you will notice this kind of speed
    increase only if you have a GPU device. If you are working on a CPU device, you
    will not notice the dramatic increase in speed. This is why if you do not own
    a GPU, we recommend using Google Colab notebooks, as the service provides free
    GPUs.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您将注意到，在GPU上执行的Torch对象的矩阵乘法比在CPU上快约18倍，并且比在NumPy数组上执行的矩阵乘法快约40倍。通常情况下，使用Torch张量在CPU上进行的`matmul`仍然比NumPy快。请注意，只有当您拥有GPU设备时，才能注意到这种速度增加。如果您在CPU设备上工作，您将不会注意到这种显著的速度增加。这就是为什么如果您没有GPU，我们建议使用Google
    Colab笔记本，因为该服务提供免费的GPU。
- en: Now that we have learned how tensor objects are leveraged across the various
    individual components/operations of a neural network and how using the GPU can
    speed up computation, in the next section, we will learn about putting this all
    together to build a neural network using PyTorch.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何在神经网络的各个组件/操作中利用张量对象，并且了解了如何使用GPU加速计算，接下来我们将学习将所有这些放在一起使用PyTorch构建神经网络。
- en: Building a neural network using PyTorch
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch构建神经网络
- en: 'In the previous chapter, we learned about building a neural network from scratch,
    where the components of a neural network are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了如何从头开始构建神经网络，其中神经网络的组件如下：
- en: The number of hidden layers
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的数量
- en: The number of units in a hidden layer
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层中的单元数
- en: Activation functions performed at the various layers
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各层执行的激活函数
- en: The loss function that we try to optimize for
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们试图优化的损失函数
- en: The learning rate associated with the neural network
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与神经网络相关联的学习率
- en: The batch size of data leveraged to build the neural network
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于构建神经网络的数据批量大小
- en: The number of epochs of forward- and backpropagation
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向传播和反向传播的时期数量
- en: However, all of these were built from scratch using NumPy arrays in Python.
    In this section, we will learn about implementing all of these using PyTorch on
    a toy dataset. Note that we will leverage our learning so far regarding initializing
    tensor objects, performing various operations on top of them, and calculating
    the gradient values to update weights when building a neural network using PyTorch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有这些都是使用Python中的NumPy数组从头构建的。在本节中，我们将学习如何在玩具数据集上使用PyTorch来实现所有这些。请注意，我们将利用迄今为止关于初始化张量对象、在其上执行各种操作以及计算梯度值以在构建神经网络时更新权重的学习。
- en: To learn how to intuitively perform various operations, we will build a neural
    network on a toy dataset in this chapter. But starting with the next chapter,
    we will deal with solving more realistic problems and datasets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学会直观地执行各种操作，我们将在本章节的玩具数据集上构建一个神经网络。但从下一章开始，我们将处理更现实的问题和数据集。
- en: 'The toy problem we’ll solve to understand the implementation of neural networks
    using PyTorch involves a simple addition of two numbers, where we initialize the
    dataset as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解决一个简单的添加两个数字的玩具问题，以便理解使用PyTorch实现神经网络的方法，其中我们初始化数据集如下：
- en: The following code can be found in the `Building_a_neural_network_using_PyTorch_on_a_toy_dataset.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的GitHub存储库的`Chapter02`文件夹中的`Building_a_neural_network_using_PyTorch_on_a_toy_dataset.ipynb`文件中可以找到以下代码，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Define the input (`x`) and output (`y`) values:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入(`x`)和输出(`y`)的值：
- en: '[PRE33]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Notice that in the preceding input and output variable initialization, the input
    and output are a list of lists where the sum of values in the input list is the
    values in the output list.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的输入和输出变量初始化中，输入和输出都是一个列表的列表，其中输入列表中的值的和是输出列表中的值。
- en: 'Convert the input lists into tensor objects:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入列表转换为张量对象：
- en: '[PRE34]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, we have converted the tensor objects into floating-point objects.
    It is good practice to have tensor objects as floats or long ints, as they will
    be multiplied by decimal values (weights) anyway.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如您所见，我们已将张量对象转换为浮点对象。将张量对象作为浮点数或长整数是一个好习惯，因为它们最终将乘以小数值（权重）。
- en: 'Furthermore, we register the input (`X`) and output (`Y`) data points to the
    device – `cuda` if you have a GPU and `cpu` if you don’t have a GPU:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们将输入（`X`）和输出（`Y`）数据点注册到设备 - 如果您有GPU，则为`cuda`，如果您没有GPU，则为`cpu`：
- en: '[PRE35]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the neural network architecture:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络架构：
- en: 'The `torch.nn` module contains functions that help in building neural network
    models:'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch.nn`模块包含有助于构建神经网络模型的函数：'
- en: '[PRE36]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will create a class (`MyNeuralNet`) that can compose our neural network
    architecture. It is mandatory to inherit from `nn.Module` when creating a model
    architecture, as it is the base class for all neural network modules:'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个类（`MyNeuralNet`），该类可以组合我们的神经网络架构。在创建模型架构时，强制性继承`nn.Module`作为所有神经网络模块的基类：
- en: '[PRE37]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Within the class, we initialize all the components of a neural network using
    the `__init__` method. We should call `super().__init__()` to ensure that the
    class inherits `nn.Module`:'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在类内部，我们使用`__init__`方法初始化了神经网络的所有组件。我们应该调用`super().__init__()`以确保该类继承自`nn.Module`：
- en: '[PRE38]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: With the preceding code, by specifying `super().__init__()`, we are now able
    to take advantage of all the pre-built functionalities that have been written
    for `nn.Module`. The components that are going to be initialized in the `init`
    method will be used across different methods in the `MyNeuralNet` class.
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上述代码中，通过指定`super().__init__()`，我们现在能够利用为`nn.Module`编写的所有预构建功能。将在`init`方法中初始化的组件将在`MyNeuralNet`类的不同方法中使用。
- en: 'Define the layers in the neural network:'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络中的层：
- en: '[PRE39]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In the preceding lines of code, we specified all the layers of neural network
    – a linear layer (`self.input_to_hidden_layer`), followed by ReLU activation (`self.hidden_layer_activation`),
    and finally, a linear layer (`self.hidden_to_output_layer`). For now, the choice
    of the number of layers and activation is arbitrary. We’ll learn about the impact
    of the number of units in layers and layer activations in more detail in the next
    chapter.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的代码行中，我们指定了神经网络的所有层 - 一个线性层（`self.input_to_hidden_layer`），然后是ReLU激活（`self.hidden_layer_activation`），最后是线性层（`self.hidden_to_output_layer`）。目前，层数和激活的选择是任意的。在下一章中，我们将更详细地学习层中单位数量和层激活的影响。
- en: 'Moving on, let’s understand what the functions in the preceding code are doing
    by printing the output of the `nn.Linear` method:'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们通过打印`nn.Linear`方法的输出来理解上述代码中的函数在做什么：
- en: '[PRE40]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the preceding code, the linear method takes two values as input and outputs
    seven values, and it also has a bias parameter associated with it. Furthermore,
    `nn.ReLU()` invokes the ReLU activation, which can then be used in other methods.
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的代码中，线性方法接受两个值作为输入并输出七个值，并且还有一个与之相关的偏差参数。此外，`nn.ReLU()`调用了ReLU激活函数，然后可以在其他方法中使用。
- en: 'Some of the other commonly used activation functions are as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一些其他常用的激活函数：
- en: Sigmoid
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Softmax
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax
- en: Tanh
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: 'Now that we have defined the components of a neural network, let’s connect
    the components together while defining the forward-propagation of the network:'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了神经网络的组件，让我们在定义网络的前向传播时将这些组件连接在一起：
- en: '[PRE41]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It is mandatory to use `forward` as the function name, since PyTorch has reserved
    this function as the method for performing forward-propagation. Using any other
    name in its place will raise an error.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`forward`作为函数名称是强制性的，因为PyTorch已将此函数保留为执行前向传播的方法。使用其他任何名称将引发错误。
- en: So far, we have built the model architecture; let’s inspect the randomly initialized
    weight values in the next step.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经建立了模型架构；让我们在下一步中检查随机初始化的权重值。
- en: 'You can access the initial weights of each of the components by performing
    the following steps:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过执行以下步骤访问每个组件的初始权重：
- en: 'Create an instance of the `MyNeuralNet` class object that we defined earlier
    and register it to `device`:'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`MyNeuralNet`类对象的实例，并将其注册到`device`：
- en: '[PRE42]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The weights and bias of each layer can be accessed by specifying the following:'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过指定以下内容访问每层的权重和偏差：
- en: '[PRE43]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示：
- en: '![Text  Description automatically generated](img/B18457_02_03.png)Figure 2.3:
    Weight values associated with the connections between input layer & hidden layer'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![文本描述自动生成](img/B18457_02_03.png)图2.3：输入层与隐藏层之间连接的权重值'
- en: The values in your output will vary from the preceding, as the neural network
    is initialized with random values every time. If you wanted them to remain the
    same when executing the code over multiple iterations, you would need to specify
    the seed using the `manual_seed` method in Torch as `torch.manual_seed(0)` just
    before creating the instance of the class object.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络每次以随机值初始化，因此输出的值将与前值不同。如果希望在多次迭代执行代码时保持相同值，需要在创建类对象实例之前使用Torch的`manual_seed`方法指定种子，如`torch.manual_seed(0)`。
- en: 'All the parameters of a neural network can be obtained by using the following
    code:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有神经网络的参数可以通过以下代码获取：
- en: '[PRE44]'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The preceding code returns a generator object.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码返回一个生成器对象。
- en: 'Finally, the parameters are obtained by looping through the generator, as follows:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，通过以下方式循环生成器以获取参数：
- en: '[PRE45]'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The preceding code results in the following output:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码将导致以下输出：
- en: '![](img/B18457_02_04.png)Figure 2.4: Weight & bias values'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18457_02_04.png)图2.4：权重和偏置值'
- en: The model has registered these tensors as special objects that are necessary
    to keep track of both forward- and back-propagation. When defining any `nn` layers
    in the `__init__` method, it will automatically create corresponding tensors and
    simultaneously register them. You can also manually register these parameters
    using the `nn.Parameter(<tensor>)` function. Hence, the following code is equivalent
    to the neural network class that we defined previously.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当在`__init__`方法中定义任何`nn`层时，模型已将这些张量注册为特殊对象，必要时用于跟踪前向和反向传播。它将自动创建相应的张量并同时注册它们。您还可以使用`nn.Parameter(<tensor>)`函数手动注册这些参数。因此，以下代码等效于我们之前定义的神经网络类。
- en: 'An alternative way of defining the model using the `nn.Parameter` function
    is as follows:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nn.Parameter`函数定义模型的另一种方式如下：
- en: '[PRE46]'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the loss function that we optimize for. Given that we are predicting
    for a continuous output, we’ll optimize for mean squared error:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们优化的损失函数。考虑到我们在预测连续输出，我们将优化均方误差：
- en: '[PRE47]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The other prominent loss functions are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其他显著的损失函数如下：
- en: '`CrossEntropyLoss` (for multinomial classification)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CrossEntropyLoss`（用于多项分类）'
- en: '`BCELoss` (binary cross-entropy loss for binary classification)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BCELoss`（用于二元分类的二元交叉熵损失）'
- en: 'The loss value of a neural network can be calculated by passing the input values
    through the `neuralnet` object and then calculating `MSELoss` for the given inputs:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将输入值传递给`neuralnet`对象，然后计算给定输入的`MSELoss`，可以计算神经网络的损失值：
- en: '[PRE48]'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code, `mynet(X)` calculates the output values when the input
    is passed through the neural network. Furthermore, the `loss_func` function calculates
    the `MSELoss` value corresponding to the prediction of the neural network (`_Y`)
    and the actual values (`Y`).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`mynet(X)`计算输入通过神经网络时的输出值。此外，`loss_func`函数计算神经网络预测（`_Y`）和实际值（`Y`）之间的`MSELoss`值。
- en: As a convention, in this book, we will use `_<variable>` to associate a prediction
    corresponding to the ground truth `<variable>`. Above this `<variable>` is `Y`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 作为惯例，在本书中，我们将使用`_<variable>`来关联与真实值`<variable>`对应的预测。在此`<variable>`之上是`Y`。
- en: Also, note that when computing the loss, we *always* send the prediction first
    and then the ground truth. This is a PyTorch convention.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在计算损失时，我们始终先发送预测值，然后是真实值。这是PyTorch的惯例。
- en: Now that we have defined the loss function, we will define the optimizer that
    tries to reduce the loss value. The input to the optimizer will be the parameters
    (weights and biases) corresponding to the neural network and the learning rate
    when updating the weights.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已定义了损失函数，接下来我们将定义优化器，以尝试减少损失值。优化器的输入将是与神经网络相对应的参数（权重和偏置）以及在更新权重时的学习率。
- en: For this instance, we will consider the stochastic gradient descent (there will
    be more on different optimizers and the impact of the learning rate in the next
    chapter).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将考虑随机梯度下降（在下一章中将会更多地讨论不同的优化器和学习率的影响）。
- en: 'Import the `SGD` method from the `torch.optim` module, and then pass the neural
    network object (`mynet`) and learning rate (`lr`) as parameters to the `SGD` method:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `torch.optim` 模块中导入 `SGD` 方法，然后将神经网络对象（`mynet`）和学习率（`lr`）作为参数传递给 `SGD` 方法：
- en: '[PRE49]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Perform all the steps to be done in an epoch together:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个 epoch 中要完成的所有步骤一起执行：
- en: Calculate the loss value corresponding to the given input and output.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算给定输入和输出对应的损失值。
- en: Calculate the gradient corresponding to each parameter.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个参数对应的梯度。
- en: Update the parameter values based on the learning rate and gradient of each
    parameter.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据学习率和每个参数的梯度更新参数值。
- en: 'Once the weights are updated, ensure that the gradients that have been calculated
    in the previous step are flushed before calculating the gradients in the next
    epoch:'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重后，请确保在计算下一个 epoch 中的梯度之前刷新在上一步中计算的梯度：
- en: '[PRE50]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Repeat the preceding steps as many times as the number of epochs using a `for`
    loop. In the following example, we perform the weight update process for a total
    of 50 epochs. Furthermore, we store the loss value in each epoch in the list –
    `loss_history`:'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `for` 循环重复前述步骤的次数等于 epoch 的数量。在以下示例中，我们执行了总共 50 个 epoch 的权重更新过程。此外，我们将每个
    epoch 中的损失值存储在列表 `loss_history` 中：
- en: '[PRE51]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s plot the variation in loss over increasing epochs (as we saw in the previous
    chapter, we update weights in such a way that the overall loss value decreases
    with increasing epochs):'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制损失随着 epoch 增加的变化（正如我们在前一章中看到的，我们以使整体损失值随 epoch 增加而减少的方式更新权重）：
- en: '[PRE52]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The preceding code results in the following plot:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码导致以下图表：
- en: '![Chart, histogram  Description automatically generated](img/B18457_02_05.png)Figure
    2.5: Loss variation over increasing epochs'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![图表，直方图 自动生成的描述](img/B18457_02_05.png)图 2.5：随着 epoch 增加的损失变化'
- en: Note that, as expected, the loss value decreases over increasing epochs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如预期的那样，损失值随着 epoch 的增加而减少。
- en: So far, in this section, we have updated the weights of a neural network by
    calculating the loss based on all the data points provided in the input dataset.
    In the next section, we will learn about the advantage of using only a sample
    of input data points per weight update.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本节中，我们通过基于输入数据集中提供的所有数据点计算损失来更新神经网络的权重。在接下来的部分，我们将了解每次权重更新时仅使用部分输入数据点的优势。
- en: Dataset, DataLoader, and batch size
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集、数据加载器和批大小
- en: One hyperparameter in a neural network that we have not considered yet is the
    batch size. The batch size refers to the number of data points considered to calculate
    the loss value or update weights.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中一个尚未考虑的超参数是批大小（batch size）。批大小指的是用于计算损失值或更新权重的数据点数量。
- en: This hyperparameter especially comes in handy in scenarios where there are millions
    of data points, and using all of them for one instance of a weight update is not
    optimal, as memory is not available to hold so much information. In addition,
    a sample can be representative enough of the data. The batch size helps ensure
    that we fetch multiple samples of data that are representative enough, but not
    necessarily 100% representative of the total data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特别的超参数在有数百万数据点的情况下特别有用，在这种情况下，使用所有数据点来更新权重的一个实例并不是最佳选择，因为内存无法容纳如此多的信息。此外，样本可能足够代表数据。批大小有助于确保我们获取足够代表性的多个数据样本，但不一定是总数据的100%代表性。
- en: 'In this section, we will come up with a way to specify the batch size to be
    considered when calculating the gradient of weights and to update the weights,
    which are in turn used to calculate the updated loss value:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提出一种方法来指定在计算权重梯度和更新权重时要考虑的批大小，这些权重进而用于计算更新后的损失值：
- en: 'The following code can be found in the `Specifying_batch_size_while_training_a_model.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书 GitHub 存储库的 `Chapter02` 文件夹中的 `Specifying_batch_size_while_training_a_model.ipynb`
    文件中找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Import the methods that help to load data and deal with datasets:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入帮助加载数据和处理数据集的方法：
- en: '[PRE53]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Import the data, convert it into floating-point numbers, and register them
    to a device:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据，将其转换为浮点数，并将其注册到设备上：
- en: 'Provide the data points to work on:'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供要处理的数据点：
- en: '[PRE54]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Convert the data into floating-point numbers:'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为浮点数：
- en: '[PRE55]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Register data to the device – given that we are working on a GPU, we specify
    that the device is `''cuda''`. If you are working on a CPU, specify the device
    as `''cpu''`:'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据注册到设备上 - 鉴于我们使用的是GPU，我们指定设备为`'cuda'`。如果您使用CPU，请将设备指定为`'cpu'`：
- en: '[PRE56]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Instantiate a class of the dataset – `MyDataset`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化数据集的类 - `MyDataset`：
- en: '[PRE57]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Within the `MyDataset` class, we store the information to fetch one data point
    at a time so that a batch of data points can be bundled together (using `DataLoader`)
    and sent through one forward- and one backpropagation to update the weights:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MyDataset`类内部，我们存储信息以逐个获取数据点，以便将一批数据点捆绑在一起（使用`DataLoader`），然后通过一次前向传播和一次反向传播来更新权重：
- en: 'Define an `__init__` method that takes input and output pairs and converts
    them into Torch float objects:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`__init__`方法，该方法接受输入和输出对，并将它们转换为Torch浮点对象：
- en: '[PRE58]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Specify the length (`__len__`) of the input dataset so that the class is aware
    of the number of datapoints present in the input dataset:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定输入数据集的长度（`__len__`），以便类知道输入数据集中存在的数据点数量：
- en: '[PRE59]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Finally, the `__getitem__` method is used to fetch a specific row:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`__getitem__`方法获取特定的行：
- en: '[PRE60]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In the preceding code, `ix` refers to the index of the row that is to be fetched
    from the dataset, which will be an integer between 0 and the length of the dataset.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`ix`指的是要从数据集中获取的行的索引，该索引将是0到数据集长度之间的整数。
- en: 'Create an instance of the defined class:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建已定义类的实例：
- en: '[PRE61]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Pass the dataset instance defined previously through `DataLoader` to fetch
    the `batch_size` number of data points from the original input and output tensor
    objects:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将之前定义的数据集实例通过`DataLoader`传递，以获取原始输入和输出张量对象中的`batch_size`数量的数据点：
- en: '[PRE62]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: In addition, in the preceding code, we also specify that we fetch a random sample
    (by mentioning that `shuffle=True`) of two data points (by mentioning `batch_size=2`)
    from the original input dataset (`ds`).
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，在前面的代码中，我们还指定从原始输入数据集（`ds`）中获取两个数据点的随机样本（通过设置`shuffle=True`）以及指定批次大小为`2`。
- en: 'To fetch the batches from `dl`, we loop through it:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要从`dl`中获取批次数据，我们需要遍历它：
- en: '[PRE63]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'This results in the following output:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '[PRE64]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The preceding code resulted in two sets of input-output pairs, as there was
    a total of four data points in the original dataset, while the batch size that
    was specified was `2`.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码中，由于原始数据集中有四个数据点，而指定的批次大小为`2`，因此导致了两组输入输出对。
- en: 'Now, we define the neural network class that we defined in the previous section:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义在前一节中定义的神经网络类：
- en: '[PRE65]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we define the model object (`mynet`), loss function (`loss_func`), and
    optimizer (`opt`) too, as defined in the previous section:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型对象（`mynet`），损失函数（`loss_func`）和优化器（`opt`），如前一节所定义的：
- en: '[PRE66]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, loop through the batches of data points to minimize the loss value,
    just like we did in *step 6* in the previous section:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，遍历数据点批次以最小化损失值，就像我们在前一节中的*步骤6*中所做的一样：
- en: '[PRE67]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: While the preceding code seems very similar to the code that we went through
    in the previous section, we are performing 2X the number of weight updates per
    epoch when compared to the number of times the weights were updated in the previous
    section. The batch size in this section is `2`, whereas the batch size in the
    previous section was `4` (the total number of data points).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的代码看起来与我们在前一节中讨论过的代码非常相似，但是在每个epoch中，我们的权重更新次数是前一节中权重更新次数的两倍。本节中的批次大小为`2`，而前一节中的批次大小为`4`（总数据点数）。
- en: Predicting on new data points
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对新数据点进行预测：
- en: 'In the previous section, we learned how to fit a model on known data points.
    In this section, we will learn how to leverage the forward method defined in the
    trained `mynet` model from the previous section to predict on unseen data points.
    We will continue from the code built in the previous section:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了如何在已知数据点上拟合模型。在本节中，我们将学习如何利用前一节中训练的`mynet`模型中定义的前向方法来预测未见数据点。我们将继续使用前一节中构建的代码：
- en: 'Create the data points that we want to test our model on:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建我们想要测试模型的数据点：
- en: '[PRE68]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Note that the new dataset (`val_x`) will also be a list of lists, as the input
    dataset was a list of lists.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，新数据集（`val_x`）也将是一个列表的列表，因为输入数据集是一个列表的列表。
- en: 'Convert the new data points into a tensor float object and register it to the
    device:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新数据点转换为浮点张量对象并注册到设备上：
- en: '[PRE69]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Pass the tensor object through the trained neural network – `mynet` – as if
    it were a Python function. This is the same as performing a forward-propagation
    through the model that was built:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将张量对象通过已训练的神经网络`mynet`传递，就像它是一个Python函数一样。这与通过建立的模型进行前向传播是相同的：
- en: '[PRE70]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The preceding code returns the predicted output values associated with the input
    data points.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回与输入数据点相关联的预测输出值。
- en: So far, we have been able to train our neural network to map an input with output,
    where we updated weight values by performing backpropagation to minimize the loss
    value (which is calculated using a pre-defined loss function).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经能够训练我们的神经网络将输入映射到输出，通过执行反向传播来更新权重值以最小化损失值（使用预定义的损失函数计算）。
- en: In the next section, we will learn about building our own custom loss function
    instead of using a pre-defined loss function.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习如何构建我们自己的自定义损失函数，而不是使用预定义的损失函数。
- en: Implementing a custom loss function
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自定义损失函数
- en: In certain cases, we might have to implement a loss function that is customized
    to the problem we are solving – especially in complex use cases involving **object
    detection/generative adversarial networks** (**GANs**). PyTorch provides the functionalities
    for us to build a custom loss function by writing a function of our own.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能需要实现一个自定义损失函数，该函数根据我们解决的问题进行自定义，特别是涉及**目标检测/生成对抗网络（GANs）**的复杂用例。PyTorch为我们提供了编写自定义函数来构建自定义损失函数的功能。
- en: 'In this section, we will implement a custom loss function that does the same
    job as that of the `MSELoss` function that comes pre-built within `nn.Module`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个自定义损失函数，其功能与`nn.Module`中预先构建的`MSELoss`函数相同：
- en: The following code can be found in the `Implementing_custom_loss_function.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以在本书的GitHub存储库的`Chapter02`文件夹中的`Implementing_custom_loss_function.ipynb`文件中找到，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Import the data, build the dataset and `DataLoader`, and define a neural network,
    as done in the previous section:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据，构建数据集和`DataLoader`，并定义一个神经网络，与上一节相同：
- en: '[PRE71]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Define the custom loss function by taking two tensor objects as input, taking
    their difference, squaring them up, and then returning the mean value of the squared
    difference between the two:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过取两个张量对象作为输入，获取它们的差异，对它们进行平方处理，然后返回两者之间平方差的均值来定义自定义损失函数：
- en: '[PRE72]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'For the same input and output combination that we had in the previous section,
    `nn.MSELoss` is used in fetching the mean squared error loss, as follows:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于在上一节中相同的输入和输出组合，使用`nn.MSELoss`来获取均方误差损失，如下所示：
- en: '[PRE73]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Similarly, the output of the loss value when we use the function that we defined
    in *step 2* is as follows:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，当我们在*步骤2*中定义的函数中使用时，损失值的输出如下：
- en: '[PRE74]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Notice that the results match. We have used the built-in `MSELoss` function
    and compared its result with the custom function that we built. We can define
    a custom function of our choice, depending on the problem we are solving.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意结果匹配。我们使用了内置的`MSELoss`函数，并将其结果与我们构建的自定义函数进行了比较。我们可以根据我们解决的问题定义我们选择的自定义函数。
- en: So far, we have learned about calculating the output at the last layer. The
    intermediate layer values have been a black box so far. In the next section, we
    will learn how to fetch the intermediate layer values of a neural network.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了计算最后一层的输出。迄今为止，中间层的值一直是一个黑盒子。在接下来的部分，我们将学习如何获取神经网络的中间层值。
- en: Fetching the values of intermediate layers
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取中间层的值
- en: In certain scenarios, it is helpful to fetch the intermediate layer values of
    the neural network (there will be more on this when we discuss the style transfer
    and transfer learning use cases in *Chapters 4* and *5*).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，获取神经网络的中间层值非常有帮助（在我们讨论风格转移和迁移学习用例时将会详细介绍*第4章*和*第5章*）。
- en: 'PyTorch provides the functionality to fetch the intermediate values of the
    neural network in two ways:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了两种方式来获取神经网络的中间值：
- en: The following code can be found in the `Fetching_values_of_intermediate_layers.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可以在本书的GitHub存储库的`Chapter02`文件夹中的`Fetching_values_of_intermediate_layers.ipynb`文件中找到，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'One way is by directly calling layers as if they are functions. This can be
    done as follows:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种方法是直接调用层，就像它们是函数一样。可以按如下方式完成：
- en: '[PRE75]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note that we had to call the `input_to_hidden_layer` activation prior to calling
    `hidden_layer_activation` as the output of `input_to_hidden_layer` is the input
    to the `hidden_layer_activation` layer.
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在调用`hidden_layer_activation`层之前，我们必须先调用`input_to_hidden_layer`激活，因为`input_to_hidden_layer`的输出是`hidden_layer_activation`层的输入。
- en: The other way is by specifying the layers that we want to look at in the `forward`
    method.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一种方法是通过指定我们在`forward`方法中想要查看的层来实现。
- en: Let’s look at the hidden layer values after activation for the model we have
    been working on in this chapter.
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们查看本章中我们一直在使用的模型的激活后的隐藏层值。
- en: 'While all the following code remains the same as what we saw in the previous
    section, we have ensured that the `forward` method returns not only the output
    but also the hidden layer values after activation (`hidden2`):'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然以下代码与我们在上一节中看到的代码相同，但我们确保`forward`方法不仅返回输出，还返回激活后的隐藏层值（`hidden2`）：
- en: '[PRE76]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can now access the hidden layer values by specifying the following:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以通过指定以下内容来访问隐藏层值：
- en: '[PRE77]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Note that the 0^(th) index output of `mynet` is as we have defined it – the
    final output of the forward-propagation on the network – while the first index
    output is the hidden layer value post-activation.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`mynet`的第0个索引输出是我们定义的网络前向传播的最终输出，而第一个索引输出是激活后的隐藏层值。
- en: Using a sequential method to build a neural network
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用序列方法构建神经网络
- en: So far, we have learned how to implement a neural network using the class of
    neural networks where we manually built each layer. However, unless we are building
    a complicated network, the steps to build a neural network architecture are straightforward,
    where we specify the layers and the sequence with which layers are to be stacked.
    Let’s move on and learn about a simplified way of defining the neural network
    architecture using the `Sequential` class.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用神经网络类来实现神经网络，其中我们手动构建了每一层。然而，除非我们构建的是一个复杂的网络，否则构建神经网络架构的步骤是直接的，我们只需指定层和层堆叠的顺序。让我们继续学习一种简化定义神经网络架构的方法，使用`Sequential`类。
- en: 'We will perform the same steps that we did in the previous sections, except
    that the class that was used to define the neural network architecture manually
    will be substituted with a `Sequential` class to create a neural network architecture.
    Let’s code up the network for the same toy data that we have worked on in this
    chapter:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行与前几节相同的步骤，不同之处在于用于手动定义神经网络架构的类将替换为`Sequential`类，以创建神经网络架构。让我们为在本章中已经处理过的相同玩具数据编写网络代码：
- en: 'The following code is available as `Sequential_method_to_build_a_neural_network.ipynb`
    in the `Chapter02` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书GitHub存储库的`Chapter02`文件夹中的`Sequential_method_to_build_a_neural_network.ipynb`中提供：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Define the toy dataset:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义玩具数据集：
- en: '[PRE78]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Import the relevant packages and define the device we will work on:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并定义我们将在其上工作的设备：
- en: '[PRE79]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now, we define the dataset class (`MyDataset`):'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义数据集类（`MyDataset`）：
- en: '[PRE80]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Define the dataset (`ds`) and dataloader (`dl`) objects:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集（`ds`）和数据加载器（`dl`）对象：
- en: '[PRE81]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Define the model architecture using the `Sequential` method available in the
    `nn` package:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nn`包中可用的`Sequential`方法定义模型架构：
- en: '[PRE82]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Note that, in the preceding code, we defined the same architecture of the network
    as we defined in previous sections, but we defined it differently. `nn.Linear`
    accepts two-dimensional input and gives an eight-dimensional output for each data
    point. Furthermore, `nn.ReLU` performs ReLU activation on top of the eight-dimensional
    output, and finally, the eight-dimensional input gives a one-dimensional output
    (which, in our case, is the output of the addition of the two inputs) using the
    final `nn.Linear` layer.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述代码中，我们定义了与前几节中相同的网络架构，但我们的定义方式有所不同。`nn.Linear`接受二维输入，并为每个数据点输出八维输出。此外，`nn.ReLU`在八维输出上执行ReLU激活，最后，八维输入通过最终的`nn.Linear`层得到一维输出（在我们的情况下，这是两个输入相加的输出）。
- en: 'Print a summary of the model we defined in *step 5*:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印我们在*步骤5*中定义的模型摘要：
- en: 'Install and import the package that enables us to print the model summary:'
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并导入使我们能够打印模型摘要的包：
- en: '[PRE83]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Print a summary of the model, which expects the name of the model and also
    the input size of the model:'
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型的摘要，该摘要期望模型的名称和模型的输入大小：
- en: '[PRE84]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The preceding code gives the following output:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码给出了以下输出：
- en: '![Table  Description automatically generated](img/B18457_02_06.png)Figure 2.6:
    Summary of model architecture'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![表自动生成的描述](img/B18457_02_06.png)图 2.6：模型架构总结'
- en: The output shape of the first layer is (-1, 8), where -1 represents that there
    can be as many data points as the batch size, and 8 represents that for each data
    point, we have an eight-dimensional output, resulting in an output of the shape
    – (batch size x 8). The interpretation for the next two layers is similar.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层的输出形状是（-1, 8），其中 -1 表示可以有与批处理大小一样多的数据点，8 表示每个数据点我们有一个八维输出，从而导致形状为（批处理大小 x
    8）的输出。对于接下来的两层，解释是类似的。
- en: 'Next, we define the loss function (`loss_func`) and optimizer (`opt`) and train
    the model, just like we did in the previous section. In this case, we don’t need
    to define a model object; a network is not defined within a class in this scenario:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义损失函数（`loss_func`）和优化器（`opt`），并训练模型，就像我们在上一节中所做的那样。在这种情况下，我们不需要定义一个模型对象；在这种情况下，网络没有在类内定义：
- en: '[PRE85]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now that we have trained the model, we can predict values on a validation dataset
    that we define now:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了模型，我们可以在我们现在定义的验证数据集上预测值：
- en: 'Define the validation dataset:'
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义验证数据集：
- en: '[PRE86]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Predict the output of passing the validation list through the model (note that
    the expected value is the summation of the two inputs for each list within the
    list of lists). As defined in the dataset class, we first convert the list of
    lists into a float after converting it into a tensor object and registering it
    to the device:'
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测通过模型传递验证列表的输出（请注意，预期值是列表中每个列表的两个输入的总和）。根据数据集类的定义，我们首先将列表的列表转换为浮点数，然后将其转换为张量对象并注册到设备：
- en: '[PRE87]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The output of the preceding code (mentioned in the comment above) is close to
    what is expected (which is the summation of the input values).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出（如上面的注释所述）接近预期的输出（即输入值的总和）。
- en: Now that we have learned to leverage the sequential method to define and train
    a model, let’s learn about saving and loading a model to make an inference.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了利用顺序方法来定义和训练模型，让我们学习一下如何保存和加载模型以进行推理。
- en: Saving and loading a PyTorch model
  id: totrans-334
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载 PyTorch 模型
- en: One of the important aspects of working on neural network models is to save
    and load back a model after training. Think of a scenario where you have to make
    inferences from an already-trained model. You would load the trained model instead
    of training it again.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理神经网络模型时的一个重要方面是在训练后保存并重新加载模型。想象一下，您必须从已经训练好的模型进行推理的场景。您将加载训练好的模型，而不是再次训练它。
- en: The following code can be found in the `save_and_load_pytorch_model.ipynb` file
    in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码可以在本书 GitHub 仓库的 `Chapter02` 文件夹中的 `save_and_load_pytorch_model.ipynb`
    文件中找到，链接为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Before going through the relevant commands to do that, taking the preceding
    example as our case, let’s understand what the important components that completely
    define a neural network are. We need the following:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论相关命令来完成这一点之前，以我们的案例为例，让我们了解完全定义神经网络的重要组件。我们需要以下内容：
- en: A unique name (key) for each tensor (parameter)
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个张量（参数）都有一个唯一的名称（键）
- en: The logic to connect every tensor in the network with one or the other
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将逻辑连接到网络中的每个张量中的一个或另一个
- en: The values (weight/bias values) of each tensor
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个张量的值（权重/偏差值）
- en: While the first point is taken care of during the `__init__` phase of a definition,
    the second point is taken care of during the `forward` method definition. By default,
    the values in a tensor are randomly initialized during the `__init__` phase. But
    what we want is to load a *specific* set of weights (or values) that were learned
    when training a model and associate each value with a specific name. This is what
    you obtain by calling a special method, described in the following sections.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义期间的 `__init__` 阶段处理第一个点，而在 `forward` 方法定义期间处理第二个点。在 `__init__` 阶段，默认情况下，在张量中随机初始化值。但是我们想要的是加载训练模型时学到的一组特定权重（或值），并将每个值与特定名称关联起来。这是通过调用以下特殊方法获得的。
- en: Using state_dict
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 state_dict
- en: The `model.state_dict()` command is at the root of understanding how saving
    and loading PyTorch models works. The dictionary in `model.state_dict()` corresponds
    to the parameter names (keys) and the values (weight and bias values) corresponding
    to the model. `state` refers to the current snapshot of the model (where the snapshot
    is the set of values at each tensor).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.state_dict()`命令是理解如何保存和加载PyTorch模型工作原理的根本。`model.state_dict()`中的字典对应于参数名称（键）和模型对应的值（权重和偏置值）。`state`指的是模型的当前快照（快照是每个张量的值集）。'
- en: 'It returns a dictionary (`OrderedDict`) of keys and values:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个键和值的字典（`OrderedDict`）：
- en: '![Text  Description automatically generated](img/B18457_02_07.png)Figure 2.7:
    State dictionary with weight & bias values'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '![自动生成的文本描述](img/B18457_02_07.png)图 2.7：带有权重和偏置值的状态字典'
- en: The keys are the names of the model’s layers, and the values correspond to the
    weights of these layers.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 键是模型层的名称，值对应于这些层的权重。
- en: Saving
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存
- en: Running `torch.save(model.state_dict(), 'mymodel.pth')` will save this model
    in a Python serialized format on the disk with the name `mymodel.pth`. A good
    practice is to transfer the model to the CPU before calling `torch.save`, as this
    will save tensors as CPU tensors and not as CUDA tensors. This will help in loading
    the model onto any machine, whether it contains CUDA capabilities or not.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`torch.save(model.state_dict(), 'mymodel.pth')`会将该模型以Python序列化格式保存在名为`mymodel.pth`的磁盘上。一个好的做法是在调用`torch.save`之前将模型转移到CPU，因为这样会将张量保存为CPU张量而不是CUDA张量。这将有助于在加载模型到任何计算机上时，无论其是否具有CUDA功能。
- en: 'We save the model using the following code:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码保存模型：
- en: '[PRE88]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Loading
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载
- en: 'Loading a model would require us to initialize the model with random weights
    first and then load the weights from `state_dict`:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型需要我们首先使用随机权重初始化模型，然后从`state_dict`中加载权重：
- en: 'Create an empty model with the same command that was used originally when training:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与训练时使用的相同命令创建一个空模型：
- en: '[PRE89]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Load the model from disk and unserialize it to create an `orderedDict` value:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从磁盘加载模型并反序列化以创建一个`orderedDict`值：
- en: '[PRE90]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Load `state_dict` onto `model`, register to `device`, and make a prediction:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`state_dict`加载到`model`中，注册到`device`，并进行预测：
- en: '[PRE91]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: If all the weight names are present in the model, then you would get a message
    saying all the keys were matched. This implies we can load our model from disk,
    for all purposes, on any machine in the world. Next, we can register the model
    to the device and perform inference on the new data points.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有权重名称都存在于模型中，则会收到所有键匹配的消息。这意味着我们可以在世界上的任何计算机上从磁盘加载我们的模型，以便所有目的。接下来，我们可以将模型注册到设备，并对新数据点进行推断。
- en: Alternatively, we can use `torch.save(model, '<path>')` to save the model and
    `torch.load('<path>')` to load the model. Even though this looks more convenient
    with fewer steps, it is not advised and is less flexible and prone to errors when
    the neural network version/Python version changes. While `torch.save(model.state_dict())`
    saves only the weights (i.e, a dictionary of tensors), `torch.save(model)` will
    save the Python class also. This creates problems when the PyTorch/Python version
    changes during loading time.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用`torch.save(model, '<path>')`保存模型，并使用`torch.load('<path>')`加载模型。尽管这看起来步骤更少更方便，但不建议，且在神经网络版本/Python版本更改时更不灵活，容易出错。虽然`torch.save(model.state_dict())`仅保存权重（即张量字典），`torch.save(model)`将同时保存Python类。这在加载时可能会导致PyTorch/Python版本不一致的问题。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about the building blocks of PyTorch – tensor objects
    – and performing various operations on top of them. We proceeded further by building
    a neural network on a toy dataset, where we started by building a class that initializes
    the feed-forward architecture, fetching data points from the dataset by specifying
    the batch size, and defining the loss function and the optimizer, looping through
    multiple epochs. Finally, we also learned about defining custom loss functions
    to optimize a metric of choice and leveraging the sequential method to simplify
    the process of defining the network architecture. All these steps form the foundation
    of building a neural network, which will be leveraged multiple times in the various
    use cases that we will build in subsequent chapters.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了PyTorch的基础构建模块——张量对象——以及在其上执行各种操作。我们进一步构建了一个玩具数据集上的神经网络，首先构建了一个初始化前馈架构的类，通过指定批处理大小从数据集中获取数据点，并定义了损失函数和优化器，通过多个时期进行循环。最后，我们还学习了定义自定义损失函数以优化选择的指标，并利用顺序方法简化网络架构定义的过程。所有这些步骤构成了构建神经网络的基础，将在后续章节中多次应用于各种用例中。
- en: With this knowledge of the various components of building a neural network using
    PyTorch, we will proceed to the next chapter, where we will learn about the various
    practical aspects of dealing with the hyperparameters of a neural network on image
    datasets.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握使用PyTorch构建神经网络的各种组件的知识，我们将继续进入下一章节，在该章节中，我们将学习处理图像数据集上神经网络超参数的各种实际方面。
- en: Questions
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why should we convert integer inputs into float values during training?
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练期间为什么要将整数输入转换为浮点数值？
- en: What are the methods used to reshape a tensor object?
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于重新塑形张量对象的方法有哪些？
- en: Why is computation faster with tensor objects than with NumPy arrays?
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用张量对象比使用NumPy数组计算更快的原因是什么？
- en: What constitutes the init magic function in a neural network class?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经网络类中，初始化魔术函数包括哪些内容？
- en: Why do we perform zero gradients before performing backpropagation?
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行反向传播之前为什么要执行零梯度操作？
- en: What magic functions constitute the dataset class?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些魔术函数构成数据集类？
- en: How do we make predictions on new data points?
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在新数据点上进行预测？
- en: How do we fetch the intermediate layer values of a neural network?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何获取神经网络中间层的值？
- en: How does the `Sequential` method help simplify the definition of the architecture
    of a neural network?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Sequential`方法如何帮助简化神经网络架构的定义？'
- en: While updating `loss_history`, we append `loss.item()` instead of `loss`. What
    does this accomplish, and why is it useful to append `loss.item()` instead of
    just `loss`?
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新`loss_history`时，我们附加`loss.item()`而不是`loss`。这样做有什么作用，为什么附加`loss.item()`而不只是`loss`很有用？
- en: What are the advantages of using `torch.save(model.state_dict())`?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`torch.save(model.state_dict())`的优势是什么？
- en: Learn more on Discord
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Discord上了解更多
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
