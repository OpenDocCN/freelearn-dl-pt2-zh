- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the fundamental building blocks of
    a neural network and also implemented forward- and backpropagation from scratch
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive into the foundations of building a neural network
    using PyTorch, which we will leverage multiple times in subsequent chapters when
    we learn about various use cases in image analysis. We will start by learning
    about the core data type that PyTorch works on – tensor objects. We will then
    dive deep into the various operations that can be performed on tensor objects
    and how to leverage them when building a neural network model on top of a toy
    dataset (so that we strengthen our understanding before we gradually look at more
    realistic datasets, starting with the next chapter). This will allow us to gain
    an understanding of how to build neural network models using PyTorch to map input
    and output values. Finally, we will learn about implementing custom loss functions
    so that we can customize them based on the use case we are solving.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network using PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a sequential method to build a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving and loading a PyTorch model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All code in this chapter is available for reference in the `Chapter02` folder
    of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch provides multiple functionalities that aid in building a neural network
    – abstracting the various components using high-level methods, and also providing
    us with tensor objects that leverage GPUs to train a neural network faster.
  prefs: []
  type: TYPE_NORMAL
- en: Before installing PyTorch, we first need to ensure that Python is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll install PyTorch, which is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit the **QUICK START LOCALLY** section on the [https://pytorch.org/](https://pytorch.org/)
    website and choose your operating system (**Your OS**), **Conda** for **Package**,
    **Python** for **Language**, and **CPU** for **Compute Platform**. If you have
    CUDA libraries, you may choose the appropriate version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Installing PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: This will prompt you to run a command such as `conda install pytorch torchvision
    torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia` in your terminal.
  prefs: []
  type: TYPE_NORMAL
- en: Run the command in Command Prompt/Terminal and let Anaconda install PyTorch
    and the necessary dependencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you own an NVIDIA graphics card as a hardware component, it is highly recommended
    to install CUDA drivers, which accelerate deep learning training by orders of
    magnitude. Refer to the *Appendix* within the GitHub repo for this book for instructions
    on how to install CUDA drivers. Once you have them installed, you can select **12.1**
    as the CUDA version and use that command instead to install PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can execute `python` in Command Prompt/Terminal and then type the following
    to verify that PyTorch is indeed installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All the code in this book can be executed in Google Colab: [https://colab.research.google.com/](https://colab.research.google.com/).
    Python and PyTorch are available by default in Google Colab. We highly encourage
    you to execute all code on Colab – which includes access to the GPU too, for free!
    Thanks to Google for providing such an excellent resource!'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, we have successfully installed PyTorch. We will now perform some basic tensor
    operations in Python to help you get the hang of things.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Tensors are the fundamental data types of PyTorch. A tensor is a multidimensional
    matrix similar to NumPy’s ndarrays:'
  prefs: []
  type: TYPE_NORMAL
- en: A scalar can be represented as a zero-dimensional tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A vector can be represented as a one-dimensional tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A two-dimensional matrix can be represented as a two-dimensional tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A multi-dimensional matrix can be represented as a multi-dimensional tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pictorially, the tensors look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_02_02.png)Figure
    2.2: Tensor representation'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, we can consider a color image as a three-dimensional tensor of
    pixel values, since a color image consists of `height x width x 3` pixels – where
    the three channels correspond to the RGB channels. Similarly, a grayscale image
    can be considered a two-dimensional tensor, as it consists of `height x width`
    pixels.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this section, we will have learned why tensors are useful and
    how to initialize them, as well as how to perform various operations on top of
    tensors. This will serve as a base for when we leverage tensors to build a neural
    network model later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing a tensor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tensors are useful in multiple ways. Apart from using them as base data structures
    for images, one more prominent use for them is when tensors are leveraged to initialize
    the weights connecting different layers of a neural network. In this section,
    we will practice the different ways of initializing a tensor object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available can be found in the `Initializing_a_tensor.ipynb`
    file located in the `Chapter02` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import PyTorch and initialize a tensor by calling `torch.tensor` on a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, access the tensor object’s shape and data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data type of all elements within a tensor is the same. That means if a
    tensor contains data of different data types (such as a Boolean, an integer, and
    a float), the entire tensor is coerced to the most generic data type:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see in the output of the preceding code, `False`, which was a Boolean,
    and `1`, which was an integer, were converted into floating-point numbers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Alternatively, similar to NumPy, we can initialize tensor objects using built-in
    functions. Note that the parallels that we drew between tensors and weights of
    a neural network come to light now – where we are initializing tensors so that
    they represent the weight initialization of a neural network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a tensor object that has three rows and four columns filled with zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate a tensor object that has three rows and four columns filled with ones:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate three rows and four columns of values between 0 and 10 (including
    the low value but not including the high value):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate random numbers between 0 and 1 with three rows and four columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate numbers that follow a normal distribution with three rows and four
    columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can directly convert a NumPy array into a Torch tensor using `torch.tensor(<numpy-array>)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have learned about initializing tensor objects, we will learn about
    performing various matrix operations on top of them next.
  prefs: []
  type: TYPE_NORMAL
- en: Operations on tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to NumPy, you can perform various basic operations on tensor objects.
    Parallels to neural network operations are the matrix multiplication of input
    with weights, the addition of bias terms, and reshaping input or weight values
    when required. Each of these and additional operations are done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Operations_on_tensors.ipynb` file in
    the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication of all the elements present in `x` by `10` can be performed
    using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Adding `10` to the elements in `x` and storing the resulting tensor in `y`
    can be performed using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshaping a tensor can be performed using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Another way to reshape a tensor is by using the `squeeze` method, where we
    provide the axis index that we want to remove. Note that this is applicable only
    when the axis we want to remove has only one item in that dimension:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The opposite of `squeeze` is `unsqueeze`, which means we add a dimension to
    the matrix, which can be performed using the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using `None` for indexing is a fancy way of unsqueezing, as shown, and will
    be used often in this book to create fake channel/batch dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix multiplication of two different tensors can be performed using the following
    code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, matrix multiplication can also be performed by using the `@`
    operator:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to `concatenate` in NumPy, we can perform concatenation of tensors
    using the `cat` method:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extraction of the maximum value in a tensor can be performed using the following
    code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can extract the maximum value along with the row index where the maximum
    value is present:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, in the preceding output, we fetch the maximum values across dimension
    `0`, which is the rows of the tensor. Hence, the maximum values across all rows
    are the values present in the 4th index and, hence, the `indices` output is all
    fours too. Furthermore, `.max` returns both the maximum values and the location
    (`argmax`) of the maximum values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Similarly, the output when fetching the maximum value across columns is as
    follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `min` operation is exactly the same as `max` but returns the minimum and
    arg-minimum where applicable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Permute the dimensions of a tensor object:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the shape of the tensor changes when we perform permute on top of
    the original tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Never reshape (that is, use `tensor.view` on) a tensor to swap the dimensions.
    Even though Torch will not throw an error, this is wrong and will create unforeseen
    results during training. If you need to swap dimensions, always use permute.
  prefs: []
  type: TYPE_NORMAL
- en: Since it is difficult to cover all the available operations in this book, it
    is important to know that you can do almost all NumPy operations in PyTorch with
    almost the same syntax as NumPy. Standard mathematical operations, such as `abs`,
    `add`, `argsort`, `ceil`, `floor`, `sin`, `cos`, `tan`, `cumsum`, `cumprod`, `diag`,
    `eig`, `exp`, `log`, `log2`, `log10`, `mean`, `median`, `mode`, `resize`, `round`,
    `sigmoid`, `softmax`, `square`, `sqrt`, `svd`, and `transpose`, to name a few,
    can be directly called on any tensor with or without axes where applicable. You
    can always run `dir(torch.Tensor)` to see all the methods possible for a Torch
    tensor and `help(torch.Tensor.<method>)` to go through the official help and documentation
    for that method.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about leveraging tensors to perform gradient calculations
    on top of data, which is a key aspect of performing backpropagation in neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Auto gradients of tensor objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, differentiation and calculating gradients
    play a critical role in updating the weights of a neural network. PyTorch’s tensor
    objects come with built-in functionality to calculate gradients.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Auto_gradient_of_tensors.ipynb` file
    in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will understand how to calculate the gradients of a tensor
    object using PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a tensor object and also specify that it requires a gradient to be calculated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, the `requires_grad` parameter specifies that the gradient
    is to be calculated for the tensor object.
  prefs: []
  type: TYPE_NORMAL
- en: Next, define the way to calculate the output, which in this specific case is
    the sum of the squares of all inputs:![](img/B18457_02_001.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is represented in code using the following line:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We know that the gradient of the preceding function is *2*x*. Let’s validate
    this using the built-in functions provided by PyTorch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The gradient of a variable can be calculated by calling the `backward()` method.
    In our case, we calculate the gradient – change in `out` (output) for a small
    change in `x` (input) – as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now in a position to obtain the gradient of `out` with respect to `x`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the gradients obtained previously match with the intuitive gradient
    values (which are two times the value of *x*).
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, try recreating the scenario in `Chain rule.ipynb` in *Chapter
    1* with PyTorch. Compute the gradients after making a forward pass and make a
    single update. Verify that the updated weights match what we calculated in the
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about initializing, manipulating, and calculating gradients
    on top of a tensor object, which together constitute the fundamental building
    blocks of a neural network. Except for calculating auto gradients, initializing
    and manipulating data can also be performed using NumPy arrays. This calls for
    us to understand the reason why you should use tensor objects over NumPy arrays
    when building a neural network, which we will go through in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of PyTorch’s tensors over NumPy’s ndarrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous chapter, we saw that when calculating the optimal weight values,
    we vary each weight by a small amount and understand its impact on reducing the
    overall loss value. Note that the loss calculation based on the weight update
    of one weight does not impact the loss calculation of the weight update of other
    weights in the same iteration. Thus, this process can be optimized if each weight
    update is made by a different core in parallel instead of updating weights sequentially.
    A GPU comes in handy in this scenario, as it consists of thousands of cores when
    compared to a CPU (which, in general, could have <=64 cores).
  prefs: []
  type: TYPE_NORMAL
- en: 'A Torch tensor object is optimized to work with a GPU compared to NumPy. To
    understand this further, let’s perform a small experiment, where we perform the
    operation of matrix multiplication using NumPy arrays in one scenario and tensor
    objects in another, comparing the time taken to perform matrix multiplication
    in both scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Numpy_Vs_Torch_object_computation_speed_comparison.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate two different `torch` objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the device to which we will store the tensor objects we created in *step
    1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that if you don’t have a GPU device, the device will be `cpu` (furthermore,
    you would not notice the drastic difference in time taken to execute when using
    a CPU).
  prefs: []
  type: TYPE_NORMAL
- en: 'Register the tensor objects that were created in *step 1* with the device (registering
    tensor objects means storing information in a device):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform matrix multiplication of the Torch objects, and also time it so that
    we can compare the speed to a scenario where matrix multiplication is performed
    on NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform matrix multiplication of the same tensors on `cpu`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform the same matrix multiplication, this time on NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will notice that the matrix multiplication performed on Torch objects on
    a GPU is ~18X faster than Torch objects on a CPU, and ~40X faster than the matrix
    multiplication performed on NumPy arrays. In general, `matmul` with Torch tensors
    on a CPU is still faster than NumPy. Note that you will notice this kind of speed
    increase only if you have a GPU device. If you are working on a CPU device, you
    will not notice the dramatic increase in speed. This is why if you do not own
    a GPU, we recommend using Google Colab notebooks, as the service provides free
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how tensor objects are leveraged across the various
    individual components/operations of a neural network and how using the GPU can
    speed up computation, in the next section, we will learn about putting this all
    together to build a neural network using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network using PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned about building a neural network from scratch,
    where the components of a neural network are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of hidden layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of units in a hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation functions performed at the various layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function that we try to optimize for
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The learning rate associated with the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The batch size of data leveraged to build the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of epochs of forward- and backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, all of these were built from scratch using NumPy arrays in Python.
    In this section, we will learn about implementing all of these using PyTorch on
    a toy dataset. Note that we will leverage our learning so far regarding initializing
    tensor objects, performing various operations on top of them, and calculating
    the gradient values to update weights when building a neural network using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to intuitively perform various operations, we will build a neural
    network on a toy dataset in this chapter. But starting with the next chapter,
    we will deal with solving more realistic problems and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The toy problem we’ll solve to understand the implementation of neural networks
    using PyTorch involves a simple addition of two numbers, where we initialize the
    dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Building_a_neural_network_using_PyTorch_on_a_toy_dataset.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the input (`x`) and output (`y`) values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that in the preceding input and output variable initialization, the input
    and output are a list of lists where the sum of values in the input list is the
    values in the output list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the input lists into tensor objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, we have converted the tensor objects into floating-point objects.
    It is good practice to have tensor objects as floats or long ints, as they will
    be multiplied by decimal values (weights) anyway.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Furthermore, we register the input (`X`) and output (`Y`) data points to the
    device – `cuda` if you have a GPU and `cpu` if you don’t have a GPU:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the neural network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `torch.nn` module contains functions that help in building neural network
    models:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will create a class (`MyNeuralNet`) that can compose our neural network
    architecture. It is mandatory to inherit from `nn.Module` when creating a model
    architecture, as it is the base class for all neural network modules:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within the class, we initialize all the components of a neural network using
    the `__init__` method. We should call `super().__init__()` to ensure that the
    class inherits `nn.Module`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the preceding code, by specifying `super().__init__()`, we are now able
    to take advantage of all the pre-built functionalities that have been written
    for `nn.Module`. The components that are going to be initialized in the `init`
    method will be used across different methods in the `MyNeuralNet` class.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the layers in the neural network:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding lines of code, we specified all the layers of neural network
    – a linear layer (`self.input_to_hidden_layer`), followed by ReLU activation (`self.hidden_layer_activation`),
    and finally, a linear layer (`self.hidden_to_output_layer`). For now, the choice
    of the number of layers and activation is arbitrary. We’ll learn about the impact
    of the number of units in layers and layer activations in more detail in the next
    chapter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Moving on, let’s understand what the functions in the preceding code are doing
    by printing the output of the `nn.Linear` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, the linear method takes two values as input and outputs
    seven values, and it also has a bias parameter associated with it. Furthermore,
    `nn.ReLU()` invokes the ReLU activation, which can then be used in other methods.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some of the other commonly used activation functions are as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have defined the components of a neural network, let’s connect
    the components together while defining the forward-propagation of the network:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is mandatory to use `forward` as the function name, since PyTorch has reserved
    this function as the method for performing forward-propagation. Using any other
    name in its place will raise an error.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have built the model architecture; let’s inspect the randomly initialized
    weight values in the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can access the initial weights of each of the components by performing
    the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create an instance of the `MyNeuralNet` class object that we defined earlier
    and register it to `device`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The weights and bias of each layer can be accessed by specifying the following:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B18457_02_03.png)Figure 2.3:
    Weight values associated with the connections between input layer & hidden layer'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: The values in your output will vary from the preceding, as the neural network
    is initialized with random values every time. If you wanted them to remain the
    same when executing the code over multiple iterations, you would need to specify
    the seed using the `manual_seed` method in Torch as `torch.manual_seed(0)` just
    before creating the instance of the class object.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the parameters of a neural network can be obtained by using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: The preceding code returns a generator object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, the parameters are obtained by looping through the generator, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18457_02_04.png)Figure 2.4: Weight & bias values'
  prefs: []
  type: TYPE_NORMAL
- en: The model has registered these tensors as special objects that are necessary
    to keep track of both forward- and back-propagation. When defining any `nn` layers
    in the `__init__` method, it will automatically create corresponding tensors and
    simultaneously register them. You can also manually register these parameters
    using the `nn.Parameter(<tensor>)` function. Hence, the following code is equivalent
    to the neural network class that we defined previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative way of defining the model using the `nn.Parameter` function
    is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'Define the loss function that we optimize for. Given that we are predicting
    for a continuous output, we’ll optimize for mean squared error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The other prominent loss functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CrossEntropyLoss` (for multinomial classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BCELoss` (binary cross-entropy loss for binary classification)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The loss value of a neural network can be calculated by passing the input values
    through the `neuralnet` object and then calculating `MSELoss` for the given inputs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: In the preceding code, `mynet(X)` calculates the output values when the input
    is passed through the neural network. Furthermore, the `loss_func` function calculates
    the `MSELoss` value corresponding to the prediction of the neural network (`_Y`)
    and the actual values (`Y`).
  prefs: []
  type: TYPE_NORMAL
- en: As a convention, in this book, we will use `_<variable>` to associate a prediction
    corresponding to the ground truth `<variable>`. Above this `<variable>` is `Y`.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that when computing the loss, we *always* send the prediction first
    and then the ground truth. This is a PyTorch convention.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have defined the loss function, we will define the optimizer that
    tries to reduce the loss value. The input to the optimizer will be the parameters
    (weights and biases) corresponding to the neural network and the learning rate
    when updating the weights.
  prefs: []
  type: TYPE_NORMAL
- en: For this instance, we will consider the stochastic gradient descent (there will
    be more on different optimizers and the impact of the learning rate in the next
    chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `SGD` method from the `torch.optim` module, and then pass the neural
    network object (`mynet`) and learning rate (`lr`) as parameters to the `SGD` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform all the steps to be done in an epoch together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the loss value corresponding to the given input and output.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the gradient corresponding to each parameter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameter values based on the learning rate and gradient of each
    parameter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the weights are updated, ensure that the gradients that have been calculated
    in the previous step are flushed before calculating the gradients in the next
    epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Repeat the preceding steps as many times as the number of epochs using a `for`
    loop. In the following example, we perform the weight update process for a total
    of 50 epochs. Furthermore, we store the loss value in each epoch in the list –
    `loss_history`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the variation in loss over increasing epochs (as we saw in the previous
    chapter, we update weights in such a way that the overall loss value decreases
    with increasing epochs):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B18457_02_05.png)Figure
    2.5: Loss variation over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, as expected, the loss value decreases over increasing epochs.
  prefs: []
  type: TYPE_NORMAL
- en: So far, in this section, we have updated the weights of a neural network by
    calculating the loss based on all the data points provided in the input dataset.
    In the next section, we will learn about the advantage of using only a sample
    of input data points per weight update.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset, DataLoader, and batch size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One hyperparameter in a neural network that we have not considered yet is the
    batch size. The batch size refers to the number of data points considered to calculate
    the loss value or update weights.
  prefs: []
  type: TYPE_NORMAL
- en: This hyperparameter especially comes in handy in scenarios where there are millions
    of data points, and using all of them for one instance of a weight update is not
    optimal, as memory is not available to hold so much information. In addition,
    a sample can be representative enough of the data. The batch size helps ensure
    that we fetch multiple samples of data that are representative enough, but not
    necessarily 100% representative of the total data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will come up with a way to specify the batch size to be
    considered when calculating the gradient of weights and to update the weights,
    which are in turn used to calculate the updated loss value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be found in the `Specifying_batch_size_while_training_a_model.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the methods that help to load data and deal with datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the data, convert it into floating-point numbers, and register them
    to a device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Provide the data points to work on:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the data into floating-point numbers:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Register data to the device – given that we are working on a GPU, we specify
    that the device is `''cuda''`. If you are working on a CPU, specify the device
    as `''cpu''`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a class of the dataset – `MyDataset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within the `MyDataset` class, we store the information to fetch one data point
    at a time so that a batch of data points can be bundled together (using `DataLoader`)
    and sent through one forward- and one backpropagation to update the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define an `__init__` method that takes input and output pairs and converts
    them into Torch float objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify the length (`__len__`) of the input dataset so that the class is aware
    of the number of datapoints present in the input dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the `__getitem__` method is used to fetch a specific row:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, `ix` refers to the index of the row that is to be fetched
    from the dataset, which will be an integer between 0 and the length of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an instance of the defined class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the dataset instance defined previously through `DataLoader` to fetch
    the `batch_size` number of data points from the original input and output tensor
    objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In addition, in the preceding code, we also specify that we fetch a random sample
    (by mentioning that `shuffle=True`) of two data points (by mentioning `batch_size=2`)
    from the original input dataset (`ds`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To fetch the batches from `dl`, we loop through it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code resulted in two sets of input-output pairs, as there was
    a total of four data points in the original dataset, while the batch size that
    was specified was `2`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we define the neural network class that we defined in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the model object (`mynet`), loss function (`loss_func`), and
    optimizer (`opt`) too, as defined in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, loop through the batches of data points to minimize the loss value,
    just like we did in *step 6* in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While the preceding code seems very similar to the code that we went through
    in the previous section, we are performing 2X the number of weight updates per
    epoch when compared to the number of times the weights were updated in the previous
    section. The batch size in this section is `2`, whereas the batch size in the
    previous section was `4` (the total number of data points).
  prefs: []
  type: TYPE_NORMAL
- en: Predicting on new data points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we learned how to fit a model on known data points.
    In this section, we will learn how to leverage the forward method defined in the
    trained `mynet` model from the previous section to predict on unseen data points.
    We will continue from the code built in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the data points that we want to test our model on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the new dataset (`val_x`) will also be a list of lists, as the input
    dataset was a list of lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the new data points into a tensor float object and register it to the
    device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the tensor object through the trained neural network – `mynet` – as if
    it were a Python function. This is the same as performing a forward-propagation
    through the model that was built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code returns the predicted output values associated with the input
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have been able to train our neural network to map an input with output,
    where we updated weight values by performing backpropagation to minimize the loss
    value (which is calculated using a pre-defined loss function).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about building our own custom loss function
    instead of using a pre-defined loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a custom loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In certain cases, we might have to implement a loss function that is customized
    to the problem we are solving – especially in complex use cases involving **object
    detection/generative adversarial networks** (**GANs**). PyTorch provides the functionalities
    for us to build a custom loss function by writing a function of our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will implement a custom loss function that does the same
    job as that of the `MSELoss` function that comes pre-built within `nn.Module`:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Implementing_custom_loss_function.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the data, build the dataset and `DataLoader`, and define a neural network,
    as done in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the custom loss function by taking two tensor objects as input, taking
    their difference, squaring them up, and then returning the mean value of the squared
    difference between the two:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the same input and output combination that we had in the previous section,
    `nn.MSELoss` is used in fetching the mean squared error loss, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarly, the output of the loss value when we use the function that we defined
    in *step 2* is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the results match. We have used the built-in `MSELoss` function
    and compared its result with the custom function that we built. We can define
    a custom function of our choice, depending on the problem we are solving.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about calculating the output at the last layer. The
    intermediate layer values have been a black box so far. In the next section, we
    will learn how to fetch the intermediate layer values of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching the values of intermediate layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In certain scenarios, it is helpful to fetch the intermediate layer values of
    the neural network (there will be more on this when we discuss the style transfer
    and transfer learning use cases in *Chapters 4* and *5*).
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch provides the functionality to fetch the intermediate values of the
    neural network in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Fetching_values_of_intermediate_layers.ipynb`
    file in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'One way is by directly calling layers as if they are functions. This can be
    done as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we had to call the `input_to_hidden_layer` activation prior to calling
    `hidden_layer_activation` as the output of `input_to_hidden_layer` is the input
    to the `hidden_layer_activation` layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The other way is by specifying the layers that we want to look at in the `forward`
    method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at the hidden layer values after activation for the model we have
    been working on in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While all the following code remains the same as what we saw in the previous
    section, we have ensured that the `forward` method returns not only the output
    but also the hidden layer values after activation (`hidden2`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now access the hidden layer values by specifying the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the 0^(th) index output of `mynet` is as we have defined it – the
    final output of the forward-propagation on the network – while the first index
    output is the hidden layer value post-activation.
  prefs: []
  type: TYPE_NORMAL
- en: Using a sequential method to build a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how to implement a neural network using the class of
    neural networks where we manually built each layer. However, unless we are building
    a complicated network, the steps to build a neural network architecture are straightforward,
    where we specify the layers and the sequence with which layers are to be stacked.
    Let’s move on and learn about a simplified way of defining the neural network
    architecture using the `Sequential` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will perform the same steps that we did in the previous sections, except
    that the class that was used to define the neural network architecture manually
    will be substituted with a `Sequential` class to create a neural network architecture.
    Let’s code up the network for the same toy data that we have worked on in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is available as `Sequential_method_to_build_a_neural_network.ipynb`
    in the `Chapter02` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the toy dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the relevant packages and define the device we will work on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define the dataset class (`MyDataset`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the dataset (`ds`) and dataloader (`dl`) objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the model architecture using the `Sequential` method available in the
    `nn` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, in the preceding code, we defined the same architecture of the network
    as we defined in previous sections, but we defined it differently. `nn.Linear`
    accepts two-dimensional input and gives an eight-dimensional output for each data
    point. Furthermore, `nn.ReLU` performs ReLU activation on top of the eight-dimensional
    output, and finally, the eight-dimensional input gives a one-dimensional output
    (which, in our case, is the output of the addition of the two inputs) using the
    final `nn.Linear` layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print a summary of the model we defined in *step 5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Install and import the package that enables us to print the model summary:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print a summary of the model, which expects the name of the model and also
    the input size of the model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code gives the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18457_02_06.png)Figure 2.6:
    Summary of model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The output shape of the first layer is (-1, 8), where -1 represents that there
    can be as many data points as the batch size, and 8 represents that for each data
    point, we have an eight-dimensional output, resulting in an output of the shape
    – (batch size x 8). The interpretation for the next two layers is similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define the loss function (`loss_func`) and optimizer (`opt`) and train
    the model, just like we did in the previous section. In this case, we don’t need
    to define a model object; a network is not defined within a class in this scenario:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have trained the model, we can predict values on a validation dataset
    that we define now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the validation dataset:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the output of passing the validation list through the model (note that
    the expected value is the summation of the two inputs for each list within the
    list of lists). As defined in the dataset class, we first convert the list of
    lists into a float after converting it into a tensor object and registering it
    to the device:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding code (mentioned in the comment above) is close to
    what is expected (which is the summation of the input values).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned to leverage the sequential method to define and train
    a model, let’s learn about saving and loading a model to make an inference.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and loading a PyTorch model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the important aspects of working on neural network models is to save
    and load back a model after training. Think of a scenario where you have to make
    inferences from an already-trained model. You would load the trained model instead
    of training it again.
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `save_and_load_pytorch_model.ipynb` file
    in the `Chapter02` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going through the relevant commands to do that, taking the preceding
    example as our case, let’s understand what the important components that completely
    define a neural network are. We need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A unique name (key) for each tensor (parameter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The logic to connect every tensor in the network with one or the other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values (weight/bias values) of each tensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the first point is taken care of during the `__init__` phase of a definition,
    the second point is taken care of during the `forward` method definition. By default,
    the values in a tensor are randomly initialized during the `__init__` phase. But
    what we want is to load a *specific* set of weights (or values) that were learned
    when training a model and associate each value with a specific name. This is what
    you obtain by calling a special method, described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using state_dict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `model.state_dict()` command is at the root of understanding how saving
    and loading PyTorch models works. The dictionary in `model.state_dict()` corresponds
    to the parameter names (keys) and the values (weight and bias values) corresponding
    to the model. `state` refers to the current snapshot of the model (where the snapshot
    is the set of values at each tensor).
  prefs: []
  type: TYPE_NORMAL
- en: 'It returns a dictionary (`OrderedDict`) of keys and values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B18457_02_07.png)Figure 2.7:
    State dictionary with weight & bias values'
  prefs: []
  type: TYPE_NORMAL
- en: The keys are the names of the model’s layers, and the values correspond to the
    weights of these layers.
  prefs: []
  type: TYPE_NORMAL
- en: Saving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Running `torch.save(model.state_dict(), 'mymodel.pth')` will save this model
    in a Python serialized format on the disk with the name `mymodel.pth`. A good
    practice is to transfer the model to the CPU before calling `torch.save`, as this
    will save tensors as CPU tensors and not as CUDA tensors. This will help in loading
    the model onto any machine, whether it contains CUDA capabilities or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We save the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loading a model would require us to initialize the model with random weights
    first and then load the weights from `state_dict`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an empty model with the same command that was used originally when training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the model from disk and unserialize it to create an `orderedDict` value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `state_dict` onto `model`, register to `device`, and make a prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If all the weight names are present in the model, then you would get a message
    saying all the keys were matched. This implies we can load our model from disk,
    for all purposes, on any machine in the world. Next, we can register the model
    to the device and perform inference on the new data points.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can use `torch.save(model, '<path>')` to save the model and
    `torch.load('<path>')` to load the model. Even though this looks more convenient
    with fewer steps, it is not advised and is less flexible and prone to errors when
    the neural network version/Python version changes. While `torch.save(model.state_dict())`
    saves only the weights (i.e, a dictionary of tensors), `torch.save(model)` will
    save the Python class also. This creates problems when the PyTorch/Python version
    changes during loading time.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the building blocks of PyTorch – tensor objects
    – and performing various operations on top of them. We proceeded further by building
    a neural network on a toy dataset, where we started by building a class that initializes
    the feed-forward architecture, fetching data points from the dataset by specifying
    the batch size, and defining the loss function and the optimizer, looping through
    multiple epochs. Finally, we also learned about defining custom loss functions
    to optimize a metric of choice and leveraging the sequential method to simplify
    the process of defining the network architecture. All these steps form the foundation
    of building a neural network, which will be leveraged multiple times in the various
    use cases that we will build in subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge of the various components of building a neural network using
    PyTorch, we will proceed to the next chapter, where we will learn about the various
    practical aspects of dealing with the hyperparameters of a neural network on image
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why should we convert integer inputs into float values during training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the methods used to reshape a tensor object?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is computation faster with tensor objects than with NumPy arrays?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What constitutes the init magic function in a neural network class?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we perform zero gradients before performing backpropagation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What magic functions constitute the dataset class?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we make predictions on new data points?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we fetch the intermediate layer values of a neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the `Sequential` method help simplify the definition of the architecture
    of a neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While updating `loss_history`, we append `loss.item()` instead of `loss`. What
    does this accomplish, and why is it useful to append `loss.item()` instead of
    just `loss`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of using `torch.save(model.state_dict())`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
