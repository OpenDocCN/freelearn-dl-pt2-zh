- en: '*Chaper 6*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing the Sequence of Data with RNNs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Explain the concepts of Recurrent Neural Networks (RNNs)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a simple RNN architecture to solve a forecasting data problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with Long Short-Term Memory (LSTM) architectures, and generate text using
    LSTM networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a data problem using long-term and short-term memory
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a Natural Language Processing (NLP) problem using RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you'll be equipped with the skills required for you to use
    RNNs to solve NLP problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the book of the previous chapters, different network architectures have
    been explained – from traditional Artificial Neural Networks (ANNs), which can
    solve both classification and regression problems, to Convolutional Neural Networks
    (CNNs), which are mainly used to solve computer vision problems by performing
    the tasks of object classification, localization, detection, and segmentation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, we will explore the concept of **Recurrent Neural Networks**
    (**RNNs**) and solve sequential data problems. These network architectures are
    capable of handling sequential data, where context is crucial, thanks to their
    ability to hold information from previous predictions, which is called memory.
    This means that, for instance, when analyzing a sentence, word by word, RNNs have
    the capability of holding information from the first word of the sentence when
    they are handling the last one.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the chapter will explore **Long Short-Term Memory** (**LSTM**) network
    architecture, which is a type of RNN that can hold both long-term and short-term
    memory, which is especially useful for long sequences of data, such as video clips.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the chapter will also explore the concept of **Natural Language Processing**
    (**NLP**). NLP refers to the interaction of computers with human languages, which
    is a popular topic nowadays thanks to the rise of virtual assistants that provide
    customized customer services. Nonetheless, the chapter will use NLP to work on
    sentiment analysis, which consists of analyzing the meaning behind sentences.
    This is useful in order to understand the sentiment of clients with regards to
    a product or a service, based on customer reviews.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a reminder, the GitHub repository containing all code used in this chapter
    can be found at [https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as humans do not reset their thinking every second, neural networks that
    aim to understand human language should not do so either. This means that in order
    to understand each word from a paragraph or even a whole book, you or the model
    are required to understand the previous words, which can help to give context
    to words that may have different meanings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Traditional neural networks, as we have discussed so far, are not capable of
    performing such tasks – hence the creation of the concept and network architecture
    of RNNs. As briefly explained before, these network architectures contain loops
    among the different nodes. This allows information to remain in the model for
    longer periods of time. Due to this, the output from the model becomes both a
    prediction and a memory, which will be used when the next bit of sequenced text
    is passed through the model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This concept goes back to the 1980s, although it has only become recently popular
    thanks to advances in technology that have led to an increase in the computational
    power of machines and have allowed the recollection of data, as well as to the
    development of the concept of LSTM RNNs in the 1990s, which increased their book
    of action. RNNs are one of the most promising network architectures out there
    thanks to their ability to store internal memory, which allows them to efficiently
    handle sequences of data and solve a wide variety of data problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Applications of RNNs
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we have made it very clear that RNNs best work with sequences of data,
    such as text, audio clips, and videos, it is still necessary to explain the different
    applications of RNNs for real-life problems in order to understand why they are
    growing in popularity every day.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a brief explanation of the different tasks that can be performed through
    the use of RNNs:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '**NLP**: This refers to the ability of machines to represent human language.
    Nowadays, this is perhaps one of the most explored areas of deep learning, and
    undoubtedly the preferred data problem when making use of RNNs. The idea is to
    train the network using text as input data, such as poems and books, among others,
    with the objective of creating a model that is capable of generating such texts.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NLP is commonly used for the creation of chatbots (virtual assistants). By
    learning from previous human conversations, NLP models are capable of assisting
    a person to solve frequently asked questions or queries. You will probably have
    experienced this when trying to contact a bank through an online chat system,
    where, typically, you are transferred to a human operator the minute that the
    query falls outside the conventional. Another common example of chatbots in real-life
    are restaurants that take queries through Facebook Messenger:'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11865_06_01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Facebook''s Messenger Chatbot'
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Speech recognition**: Similar to NLP, speech recognition attempts to understand
    and represent human language. However, the difference here is that the former
    (NLP) is trained and produces the output in the form of text, while the latter
    (speech recognition) uses audio clips. With the proliferation of developments
    in this field, and the interest of big companies, these models are capable of
    understanding different languages and even different accents and pronunciation.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A popular example of a speech recognition device is Alexa – the voice-activated
    virtual assistance model from Amazon:'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语音识别设备的一个流行例子是Alexa - 亚马逊的语音激活虚拟助理模型：
- en: '![Figure 6.2: Amazon’s Alexa](img/C11865_06_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2：亚马逊的Alexa](img/C11865_06_02.jpg)'
- en: 'Figure 6.2: Amazon''s Alexa'
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.2：亚马逊的Alexa
- en: '**Machine translation**: This refers to a machine''s ability to translate human
    languages effectively. According to this, the input is the source language (for
    instance, Spanish) and the output is the target language (such as English). The
    main difference between NLP and machine translation is that, in the latter, the
    output is built after the entire input has been fed to the model.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：这指的是机器有效地翻译人类语言的能力。根据这一原理，输入是源语言（例如西班牙语），输出是目标语言（例如英语）。自然语言处理与机器翻译的主要区别在于，后者的输出是在将整个输入馈送到模型之后构建的。'
- en: 'With the rise of globalization and the popularity of leisure traveling, nowadays
    people require access to more than one language. Due to this, a proliferation
    of devices that are capable of translating between different languages have emerged.
    One of these latest creations is Google''s introduction of Pixel Buds, which can
    perform translations in real time:'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着全球化的兴起和休闲旅行的流行，现代人需要访问多种语言。因此，涌现了能够在不同语言之间进行翻译的设备的大量使用。其中最新的创新之一是Google推出的Pixel
    Buds，可以实时进行翻译：
- en: '![Figure 6.3: Google’s Pixel Buds](img/C11865_06_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3：Google Pixel Buds](img/C11865_06_03.jpg)'
- en: 'Figure 6.3: Google''s Pixel Buds'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.3：Google Pixel Buds
- en: '**Time-series forecasting**: A less popular application of an RNN is the prediction
    of a sequence of data points in the future based on historical data. RNNs are
    particularly good at this task due to their ability to retain an internal memory,
    which allows time-series analysis to consider the different timesteps in the past
    to perform a prediction or a series of predictions in the future.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间序列预测**：RNN的一个较少被使用的应用是基于历史数据预测未来数据点序列。由于RNN具有保持内部记忆的能力，因此特别擅长这项任务，使得时间序列分析能够考虑过去不同时间步中的数据来进行未来的预测或一系列预测。'
- en: 'This is often used to foresee future income or demand, which helps a company
    be prepared for different scenarios:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这经常用于预测未来的收入或需求，帮助公司为不同的情况做好准备：
- en: '![Figure 6.4: The forecasting of monthly sales (qty)](img/C11865_06_04.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4：每月销量（数量）的预测](img/C11865_06_04.jpg)'
- en: 'Figure 6.4: The forecasting of monthly sales (qty)'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.4：每月销量（数量）的预测
- en: For example, if by forecasting the demand of several health care products, it
    is determined that there will be an increase in one of the products while the
    other will decrease, the company may decide to produce more of that product and
    less of another.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，通过预测多种健康产品的需求，确定其中一种产品将增加而另一种将减少，公司可以决定生产更多这种产品而减少其他产品的生产量。
- en: '**Image recognition**: Coupled with CNNs, RNNs can give an image a caption
    or a description. This combination of models allows you to detect all the objects
    in the image and, hence, determines what the image is principally made of. The
    output can either be a set of tags of the objects present in the image, a description
    of the image, or a caption of the relevant objects in the image, as shown in the
    following diagram:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别**：结合CNN，RNN可以给图像加上标题或描述。这种模型组合使得您能够检测图像中的所有物体，并因此确定图像的主要构成。输出可以是图像中存在的对象的一组标签，图像的描述，或者是图像中相关对象的标题，如下图所示：'
- en: '![Figure 6.5: Image recognition using RNNs](img/C11865_06_05.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5：使用RNN进行图像识别](img/C11865_06_05.jpg)'
- en: 'Figure 6.5: Image recognition using RNNs'
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.5：使用RNN进行图像识别
- en: How Do RNNs Work?
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN如何工作？
- en: Put simply, RNNs take an input (x) and return an output (y). Here, the output
    is not only influenced by the input but also by the entire history of inputs that
    were fed in the past. This history of inputs is often referred to as the model's
    internal state or memory, which are sequences of data that follow an order and
    are related to one another – such as a time series, which is a sequence of data
    points (for example, sales) that are listed in an order (such as by month).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，RNN接收一个输入（x）并返回一个输出（y）。在这里，输出不仅受输入影响，还受过去输入的整个历史影响。这些输入的历史通常称为模型的内部状态或记忆，这些是按顺序排列并相互关联的数据序列，例如时间序列，即按顺序列出的数据点（例如销售），这些数据点相互关联。
- en: Note
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Bear in mind that the general structure of an RNN may vary depending on the
    problem at hand. For instance, they can be of the one-to-many type or the many-to-one
    type, as mentioned in *Chapter 2*, *Building Blocks of Neural Networks*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In order to better understand the concept of RNNs, it is important to explain
    the difference between RNNs and traditional neural networks. Traditional neural
    networks are often referred to as feedforward neural networks because the information
    only moves in one direction, that is, from the input to the output, without going
    through a node twice to perform a prediction. These networks do not have any memory
    of what has been fed in the past, which is why they are no good at predicting
    what is coming next in a sequence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, in RNNs, information cycles using loops, so that every prediction
    is made considering both the input and the memory from previous predictions. It
    works by copying the output of each prediction and passing it back into the network
    for the subsequent prediction. In this way, RNNs have two inputs: the present
    value and the past information:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: A graphical representation of a network, where A shows a feedforward
    neural network and B shows an RNN](img/C11865_06_06.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: A graphical representation of a network, where A shows a feedforward
    neural network and B shows an RNN'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The internal memory of traditional RNNs is short-term only. However, we will
    later explore an architecture that is capable of storing long-term and short-term
    memory.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'By using information from previous predictions, the network is trained with
    a sequence of ordered data that allows it to predict the following step. This
    is achieved by combining the current information with the output from the previous
    step into a single operation (as shown in *Figure 6.7*). The output from this
    operation will become the prediction as well as part of the input for the subsequent
    prediction:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: An RNN computation for each prediction](img/C11865_06_07.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: An RNN computation for each prediction'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the operation that occurs inside a node is that of any other
    neural network; initially, the data is passed through a linear function. The weights
    and biases are the parameters to be updated during the training process. Next,
    the linearity of this output is broken using an activation function. In this case,
    this is the `tanh` function, as several studies have shown that it achieves better
    results for most data problems:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 6.8: A mathematical computation of traditional RNNs](img/C11865_06_08.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: A mathematical computation of traditional RNNs'
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, Mt-1 refers to the memory that is derived from the previous prediction,
    W and b are the weights and biases, and E refers to the current event.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: With the preceding in mind, let's consider the sales data of a product from
    the last two years. RNNs are capable of predicting the next month's sales because
    by storing the information from the last couple of months, they are able to check
    whether sales have been increasing or decreasing.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个产品过去两年的销售数据。RNNs 能够预测下个月的销售情况，因为它们通过存储过去几个月的信息，可以检查销售是增加还是减少。
- en: Using *Figure 6.7*, the prediction of the next month could be handled by taking
    the last month's sales (that is, the current event) and the short-term memory
    (which is a representation of the data from the last couple of months) and combining
    them together. The output from this operation will contain both the prediction
    of the next month and some relevant information from the last couple of months,
    which will, in turn, become the new short-term memory for the subsequent prediction.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *Figure 6.7*，可以通过使用上个月的销售数据（即当前事件）和短期记忆（这是过去几个月数据的表示）进行下个月的预测。这个操作的输出将包含下个月的预测以及过去几个月的相关信息，这些信息反过来将成为后续预测的新的短期记忆。
- en: Moreover, it is important to mention that some RNN architectures, such as the
    LSTM network, will also be able to consider data from two years ago or even much
    earlier (since it stores long-term memory), which will let the network know whether
    a decrease during a particular month is likely to continue to decrease or start
    to increase. We will explore this topic in more detail later on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还需提到一些 RNN 架构，如 LSTM 网络，也能考虑两年甚至更早的数据（因为它存储了长期记忆），这将帮助网络了解某个月份的减少趋势是否可能继续减少或开始增加。我们稍后会更详细地探讨这个话题。
- en: RNNs in PyTorch
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch 中的 RNN
- en: 'In PyTorch, which is similar to any other layer, the recurrent layers are defined
    in a single line of code. This will then be called inside the forward function
    of the network, as shown in the following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，就像任何其他层一样，递归层在一行代码中定义。然后会在网络的前向函数中调用，如下面的代码所示：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the recurrent layer must be defined as taking arguments for the number
    of expected features in the input (`input_size`); the number of features in the
    hidden state, which is defined by the user (`hidden_size`); and the number of
    recurrent layers (`num_layers`).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，递归层必须定义为接受输入中预期特征的数量 (`input_size`)；由用户定义的隐藏状态中的特征数量 (`hidden_size`)；以及递归层数量
    (`num_layers`)。
- en: Note
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In a similar way to any other neural network, the hidden size refers to the
    number of nodes (neurons) in that layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他神经网络类似，隐藏大小指的是该层中的节点（神经元）数量。
- en: The `batch_first` argument is set to `True` to define that the input and output
    tensors are in the form of batches, sequences, and features.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_first` 参数设置为 `True`，以定义输入和输出张量为批处理、序列和特征的形式。'
- en: In the `forward` function, the input is passed through the recurrent layers
    and flattened out in order to be passed through the fully connected layer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `forward` 函数中，输入通过递归层并展开，以通过完全连接的层传递。
- en: 'Moreover, the training of such network can be handled as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此类网络的训练可以如下处理：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For each epoch, the hidden state is initialized to `none`. This is because,
    in each epoch, the network will try to map the inputs to the targets (given a
    set of parameters). This mapping should occur without any bias (hidden state)
    from the previous runs through the dataset.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 epoch，隐藏状态被初始化为 `none`。这是因为在每个 epoch 中，网络将尝试将输入映射到目标（在给定一组参数的情况下）。这种映射应该在不受来自先前数据集运行的偏置（隐藏状态）的影响下进行。
- en: Next, a `for` loop is performed to go through the different batches of data.
    Inside this loop, a prediction is made, and a hidden state is saved to be used
    as the input for the following batch.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过 `for` 循环遍历不同的数据批次。在此循环内，进行预测，并保存隐藏状态以供下一个批次使用。
- en: Finally, the loss function is calculated, which is used to update the parameters
    of the network. Then, the process starts again, until the desired number of epochs
    has been reached.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，计算损失函数，用于更新网络的参数。然后，这个过程会再次开始，直到达到期望的 epoch 数量。
- en: 'Activity 11: Using a Simple RNN for a Time Series Prediction'
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 11：使用简单的 RNN 进行时间序列预测
- en: 'For the following activity, a simple RNN will be used to solve a time series
    problem. Let''s consider the following scenario: your company wants to be able
    to predict the demand, ahead of time, for all its products. This is because it
    takes quite some time to produce each product and the procedure costs a lot of
    money. Consequently, they do not wish to spend money and time in production unless
    the product is likely to be sold. In order to predict this, they have provided
    you with a dataset containing the weekly demand (in sales transactions) for all
    products from the last year''s sales:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下面的活动，将使用一个简单的 RNN 来解决时间序列问题。考虑以下情景：您的公司希望能够提前预测所有产品的需求。这是因为每个产品的生产需要相当长的时间，并且程序成本高昂。因此，他们不希望在产品可能被销售之前花费时间和金钱。为了预测这一点，他们提供了一个数据集，其中包含去年销售的所有产品的每周需求（销售交易量）：
- en: Note
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The CSV file containing the dataset that will be used for the following activity
    can be found in this book's GitHub repository. The URL of the repository is mentioned
    in the introduction of this chapter. It is also available online at [https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly.](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 包含用于下面活动的数据集的 CSV 文件可以在本书的 GitHub 仓库中找到。该仓库的 URL 在本章的介绍中提到。它也可以在线访问：[https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly.](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly.)
- en: First, import the required libraries.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入所需的库。
- en: 'Then, set the `seed` equal to `0` to reproduce the results in this book, using
    the following line of code:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将 `seed` 设置为 `0`，以在本书中重现结果，使用以下代码行：
- en: '[PRE2]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Load the dataset and slice it so that it contains all the rows but only the
    columns from index 1 to 52.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并对其进行切片，以包含所有行但只包含从索引 1 到 52 的列。
- en: Plot the sales transactions by week of five randomly-chosen products from the
    entire dataset. Use a random seed of `0` when doing the random sampling in order
    to achieve the same results as in the current activity.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制来自整个数据集的五种随机选择产品的每周销售交易。在进行随机抽样时，请使用随机种子 `0`，以获得与当前活动中相同的结果。
- en: Create the `inputs` and `targets` variables that will be fed to the network
    to create the model. These variables should be of the same shape and be converted
    to PyTorch Tensors.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建将输入到网络以创建模型的 `inputs` 和 `targets` 变量。这些变量应该具有相同的形状，并转换为 PyTorch 张量。
- en: The `inputs` variable should contain the data for all products for all weeks,
    except the last week because the idea of the model is to predict this final week.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`inputs` 变量应该包含所有产品在所有周的数据，除了最后一周，因为模型的想法是预测这最后一周。'
- en: The `targets` variable should be one step ahead of the `inputs` variable – that
    is, the first value of the `targets` variable should be the second one of the
    inputs variable, and so on, until the last value of the `targets` variable (which
    should be the last week that was left outside of the `inputs` variable).
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`targets` 变量应该比 `inputs` 变量提前一步 - 也就是说，`targets` 变量的第一个值应该是 `inputs` 变量的第二个值，依此类推，直到
    `targets` 变量的最后一个值（应该是在 `inputs` 变量之外剩下的最后一周）。'
- en: Create a class containing the architecture of the network; note that the output
    size of the fully-connected layer should be 1.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含网络架构的类；请注意全连接层的输出大小应为 1。
- en: Initialize the class function containing the model. Feed the input size, the
    number of neurons in each recurrent layer (10), and the number of recurrent layers
    (1).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化包含模型的类函数。输入大小、每个递归层中的神经元数（10）和递归层的数量（1）。
- en: Define a loss function, an optimization algorithm, and the number of epochs
    to train the network; use the Mean Squared Error loss function, the Adam optimizer,
    and 10,000 epochs.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数、优化算法和训练网络的 epochs 数量；使用均方误差损失函数、Adam 优化器和 10,000 个 epochs。
- en: Use a `for` loop to perform the training process by going through all the epochs.
    In each epoch, a prediction must be made, along with the subsequent calculation
    of the loss function and the optimization of the parameters of the network. Then,
    save the loss of each of the epochs.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `for` 循环执行训练过程，遍历所有 epochs。在每个 epoch 中，必须进行预测，同时计算损失函数并优化网络参数。然后，保存每个 epoch
    的损失。
- en: Plot the losses of all the epochs.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制所有 epochs 的损失。
- en: Using a scatter plot, display the predictions that were obtained in the last
    epoch of the training process against the ground truth values (that is, the sales
    transactions of the last week).
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用散点图显示在训练过程的最后一个时期获得的预测结果与地面真实值（即上周销售交易）的对比。
- en: Note
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 219.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此活动的解决方案可在第219页找到。
- en: Long Short-Term Memory Networks (LSTMs)
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆网络（LSTM）
- en: As mentioned before, RNNs store short-term memory only. This is an issue when
    dealing with long sequences of data, where the network will have trouble carrying
    the information from the earlier steps to the final ones.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，RNN 只存储短期记忆。在处理长序列数据时会出现问题，网络将难以将早期步骤的信息传递到最终步骤。
- en: For instance, take the poem, The Raven, which was written by the famous poet
    Edgar Alan Poe and is over 1,000 words long. Attempting to process it using a
    traditional RNN, with the objective of creating a subsequent related poem, will
    result in the model leaving out crucial information from the first couple of paragraphs.
    This, in turn, may result in an output that is unrelated to the initial subject
    of the poem. For instance, it could ignore that the event occurred at night, and
    so make the new poem not very scary.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以诗人埃德加·爱伦·坡创作的诗《乌鸦》为例，全文超过1000字。试图使用传统的RNN处理它，目的是创建一个相关的后续诗歌，将导致模型忽略第一段落中的关键信息。这反过来可能导致输出与诗歌的初始主题无关。例如，它可能会忽略事件发生在夜晚，从而使新诗歌不够可怕。
- en: This inability to hold long-term memory occurs because traditional RNNs suffer
    from a problem called vanishing gradients. This occurs when the gradients, which
    are used to update the parameters of the network to minimize the loss function,
    become extremely small so that they no longer contribute to the learning process
    of the network. This typically occurs in the first layers of the networks, making
    the network forget what it has seen a while ago.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种无法保持长期记忆的问题是因为传统的RNN遇到了称为梯度消失的问题。当梯度变得极小以至于不再对网络的学习过程有贡献时，用于更新网络参数以最小化损失函数的梯度在网络的早期层次通常会出现这种情况，导致网络忘记了一段时间前看到的信息。
- en: Because of this, **LSTM** networks were developed. LSTM networks can remember
    information over long periods of time as they store their internal memory in a
    similar way to a computer – that is, by having the ability to read, write, and
    delete information as needed, which is achieved through the use of gates.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**LSTM** 网络被开发出来。LSTM 网络能够像计算机一样在长时间内记住信息，通过使用门控的方式来读取、写入和删除信息。
- en: These gates help the network to decide what information to keep and what information
    to delete from the memory (whether to open the gate), based on the importance
    that it assigns to each bit of information. This is extremely useful because it
    not only allows for more information to be stored (as long-term memory), but it
    also helps throw away useless information that may alter the result of a prediction,
    such as the articles in a sentence.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门有助于网络决定保留哪些信息以及删除哪些信息（是否打开门），根据它分配给每个信息位的重要性。这非常有用，因为它不仅允许存储更多信息（作为长期记忆），而且还有助于丢弃可能改变预测结果的无用信息，例如句子中的冠词。
- en: Applications
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用
- en: 'Besides the applications previously explained, the ability of LSTM networks
    to store long-term information has allowed data scientists to work on complex
    data problems that make use of large sequences of data as inputs, some of which
    will be explained next:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了先前解释的应用外，LSTM 网络存储长期信息的能力使数据科学家能够解决复杂的数据问题，这些问题利用大量数据序列作为输入，下面将进一步解释其中一些：
- en: '**Text generation**: Generating any text, such as the one you are reading here,
    can be converted into the task of an LSTM network. This works by selecting each
    letter based on all the previous letters. Networks that perform this task are
    trained with large texts, such as those of famous books. This is because the final
    model will create text that is similar to the style of writing of the one that
    it was trained from. For instance, a model that is trained over a poem will have
    a narrative that is different to the one you would expect in a conversation with
    a neighbor.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Music generation**: Just as a sequence of text can be inputted into the network
    with the objective of generating similar new text, a sequence of notes can also
    be fed into the network to generate new sequences of musical notes. Keeping track
    of the previous notes will help achieve a harmonized melody, rather than just
    a series of random musical notes. For example, feeding an audio file with a popular
    song from The Beatles will result in a sequence of musical notes that resembles
    the harmony of the group.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handwriting generation and recognition**: Here, each letter is also a product
    of all the previous letters, which, in turn, will result in a set of handwritten
    letters that have a meaning. Likewise, LSTM networks can also be used to recognize
    handwritten texts, where the prediction of one letter will depend on all the previously
    predicted letters.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Do LSTM Networks Work?
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, it has been made clear that what differentiates LSTM networks from traditional
    RNNs is their ability to have a long-term memory. However, it is important to
    mention that as time passes, very old information is less likely to influence
    the next output. Considering this, LSTM networks also have the ability to take
    into account the distance between data bits and the underlying context in order
    to also make the decision to forget some piece of information that is no longer
    relevant.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: So, how do LSTM networks decide when to remember and when to forget? Different
    to traditional RNNs, where only one calculation is performed in each node, LSTM
    networks perform four different calculations that allow the interaction between
    the different inputs of the network (that is the current event, the short-term
    memory, and the long-term memory) to arrive at an outcome.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the process behind LSTM networks, let''s consider the four gates
    that are used to manage the information in the network, which are represented
    in the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: LSTM network gates](img/C11865_06_09.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: LSTM network gates'
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The function of each of the gates in *Figure 6.9* can be explained as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '`tanh`). The output from this is multiplied by an ignore factor, which removes
    any irrelevant information. To calculate the ignore factor, the short-term memory
    and the current event are passed through a linear function. Then, they are squeezed
    together by the `sigmoid` activation function:'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Equation 6.10: The mathematical computations that occur in the learn gate](img/C11865_06_10.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.10: 学习门中发生的数学计算](img/C11865_06_10.jpg)'
- en: 'Figure 6.10: The mathematical computations that occur in the learn gate'
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.10: 学习门中发生的数学计算'
- en: Here, STM refers to the short-term memory that is derived from the previous
    prediction, W and b are the weights and biases, and E refers to the current event.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，STM指的是从先前预测中得出的短期记忆，W和b是权重和偏置，E指当前事件。
- en: '`sigmoid`):'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sigmoid`）：'
- en: '![Equation 6.11: The mathematical computations that occur in the forget gate](img/C11865_06_11.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.11: 忘记门中发生的数学计算](img/C11865_06_11.jpg)'
- en: 'Figure 6.11: The mathematical computations that occur in the forget gate'
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.11: 忘记门中发生的数学计算'
- en: Here, STM refers to the short-term memory that is derived from the previous
    prediction, LSM is the long-term memory that is derived from the previous prediction,
    W and b are the weights and biases, and E refers to the current event.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，STM指的是从先前预测中得出的短期记忆，LSM是从先前预测中得出的长期记忆，W和b是权重和偏置，E指当前事件。
- en: '**Remember gates**: The long-term memory that was not forgotten in the forget
    gate and the information that was kept from the learn gate are joined together
    in the remember gate, which will become the new long-term memory. Mathematically,
    this is achieved by summing the output from the learn and forget gates:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆门**：在忘记门中未被遗忘的长期记忆和从学习门中保留的信息在记忆门中合并在一起，成为新的长期记忆。从数学上讲，这通过将来自学习门和忘记门的输出相加来实现：'
- en: '![Equation 6.12: The mathematical computation that occurs in the remember gate](img/C11865_06_12.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.12: 记忆门中发生的数学计算](img/C11865_06_12.jpg)'
- en: 'Figure 6.12: The mathematical computation that occurs in the remember gate'
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.12: 记忆门中发生的数学计算'
- en: Here, L refers to the output from the learn gate, while F is the output from
    the forget gate.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，L指的是来自学习门的输出，而F是来自忘记门的输出。
- en: '`tanh`) over the output from the forget gate. Second, it applies a linear and
    an activation function (`sigmoid`) over both the short-term memory and the current
    event. Third, it multiplies the output from the previous steps. The output from
    the third step will be the new short-term memory and the prediction from the current
    step:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tanh`）对忘记门的输出执行线性和激活函数（`sigmoid`）。其次，它对短期记忆和当前事件的输出进行线性和激活函数（`sigmoid`）运算。第三，它将前述步骤的输出相乘。第三步的输出将成为新的短期记忆和当前步骤的预测：'
- en: '![Equation 6.13: The mathematical computations that occur in the use gate](img/C11865_06_13.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.13: 使用门中发生的数学计算](img/C11865_06_13.jpg)'
- en: 'Figure 6.13: The mathematical computations that occur in the use gate'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.13: 使用门中发生的数学计算'
- en: Here, STM refers to the short-term memory that is derived from the previous
    prediction, W and b are the weights and biases, and E refers to the current event.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，STM指的是从先前预测中得出的短期记忆，W和b是权重和偏置，E指当前事件。
- en: Note
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Although the use of the different activation functions and mathematical operators
    seems arbitrary, it is done this way because it has been proved to work on most
    data problems that deal with large sequences of data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用不同的激活函数和数学运算符似乎是随意的，但之所以这样做是因为它已被证明适用于处理大量数据序列的大多数数据问题。
- en: 'The preceding process is done for every single prediction that is performed
    by the model. For instance, for a model built to create literary pieces, the process
    of learning, forgetting, remembering, and using the information is performed for
    every single letter that will be produced by the model, as shown in the following
    figure:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 模型执行的每一个预测都会进行上述过程。例如，对于一个用于创建文学作品的模型，学习、遗忘、记忆和使用信息的过程将针对每个将由模型生成的字母执行，如下图所示：
- en: '![Figure 6.14: A LSTM network process through time](img/C11865_06_14.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14: LSTM 网络随时间的过程](img/C11865_06_14.jpg)'
- en: 'Figure 6.14: A LSTM network process through time'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.14: LSTM 网络随时间的过程'
- en: LSTM Networks in PyTorch
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch 中的 LSTM 网络
- en: The process of defining the LSTM network architecture in PyTorch is similar
    to that of any other neural network that we have discussed so far. However, it
    is important to note that, when dealing with sequences of data that are different
    to that of numbers, there is some preprocessing required in order to feed the
    network with data that it can understand and process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering this, an explanation of the general steps to train a model to be
    able to take text data as inputs and retrieve a new piece of textual data will
    be made. It is important to mention that not all steps explained here are strictly
    required, but as a group they make a clean and reusable code for using LSTM with
    textual data:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the Input Data
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is to load the text file into the code. This data will go through
    a series of transformations in order to be properly fed into the model. This is
    necessary because neural networks perform a series of mathematical computations
    to arrive at an output, which means that all inputs must be numerical. Additionally,
    it is also a good practice to feed the data in batches to the model, rather than
    all at once, as it helps to reduce the training times, especially for long datasets.
    These transformations are explained as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '**Numbered Labels**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a list of unduplicated characters is obtained from the input data. Each
    of these characters is assigned a number. Then, the input data is encoded by replacing
    each character with the assigned number. For instance, the word "hello" will be
    encoded into 123344, given the following mapping of characters and numbers:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 6.15: The mapping of characters and numbers](img/C11865_06_15.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: The mapping of characters and numbers'
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Generating the Batches**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: For RNNs, batches are created using two variables. First, the number of sequences
    per batch, and second, the length of each sequence. These values are used to divide
    the data into matrices, which will help speed up the calculations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a dataset of 24 integers, with the number of sequences per batch set
    to 2 and the sequence length equal to 4, the division works as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16: Batch generation for RNNs](img/C11865_06_16.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Batch generation for RNNs'
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As can be seen in *Figure 6.16*, 3 batches are created – with each of these
    batches containing 2 sequences, each of length 4.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: This batch generation process should be done for `x` and for `y`, where the
    former is the input to the network, and the latter represents the targets. According
    to this, the idea of the network is to find a way to map the relationship between
    `x` and `y`, considering that `y` will be 1 step ahead of `x`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The batches for `x` are created following the methodology explained in the
    previous diagram (Figure 6.16). Then, the batches of `y` will be created of the
    same length of those of `x`. This is because the first element of `y` will be
    the second element of `x`, and so on, until the last element of `y` (which will
    be the first element of `x)`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`x`的批次是按照前述图表（图 6.16）中解释的方法创建的。然后，`y`的批次将与`x`的长度相同。这是因为`y`的第一个元素将是`x`的第二个元素，依此类推，直到`y`的最后一个元素（它将是`x`的第一个元素）：'
- en: Note
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There are a number of different approaches that you can use to fill in the last
    element of y, and the one that is mentioned here is the most commonly used. The
    choice of approach is often a matter of preference, although some data problems
    may benefit more from an approach than from others.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种不同的方法可以用来填充`y`的最后一个元素，这里提到的方法是最常用的方法。选择方法通常是偏好的问题，尽管某些数据问题可能更适合某种方法而不是其他方法。
- en: '![Figure 6.17: A representation of the batches for X and Y](img/C11865_06_17.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17：X 和 Y 的批次表示](img/C11865_06_17.jpg)'
- en: 'Figure 6.17: A representation of the batches for X and Y'
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.17：X 和 Y 的批次表示
- en: Note
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Although generating batches is considered as part of the preprocessing of the
    data, it is often programmed inside the `for` loop of the training process.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成批次被认为是数据预处理的一部分，但通常在训练过程的`for`循环内编程。
- en: One-Hot Encoding
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单热编码
- en: 'To convert all characters into numbers is not enough to feed them into the
    model. This is because this approximation introduces some bias to your model,
    since the characters that are converted into higher numerical values will be evaluated
    as more important. To avoid this, it is a good practice to encode the different
    batches as one-hot matrices. This consists of creating a three-dimensional matrix
    with zeros and ones, where a zero represents the absence of an event and the one
    refers to the presence of the event. Bearing this in mind, the final shape of
    the matrix should be as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有字符转换为数字并不足以将它们馈送到模型中。这是因为此近似会为您的模型引入一些偏差，因为转换为较高数值的字符将被视为更重要。为了避免这种情况，最好的做法是将不同批次编码为单热矩阵。这包括创建一个由零和一组成的三维矩阵，其中零表示事件的缺失，而一表示事件的存在。请记住，矩阵的最终形状应如下所示：
- en: '![Equation 6.18: One-hot matrix dimensions](img/C11865_06_18.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 6.18：单热矩阵维度](img/C11865_06_18.jpg)'
- en: 'Equation 6.18: One-hot matrix dimensions'
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 方程式 6.18：单热矩阵维度
- en: This means that for every position in the batch, it will create a sequence of
    values of length that is equal to the total number of characters in the entire
    text. For every character, it will place a zero, except for the one that is present
    in that position (where it will place a one).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对于批次中的每个位置，它将创建一个长度等于整个文本中字符总数的值序列。对于每个字符，它将放置一个零，除了在该位置存在的字符（在该位置将放置一个一）。
- en: Note
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find out more about one-hot encoding at [https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f.](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f.)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)找到更多关于单热编码的信息。
- en: Building the Architecture
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建架构
- en: 'Similar to other neural networks, an LSTM layer is easily defined in a single
    line of code. Nevertheless, the class containing the architecture of the network
    must now include a function that allows the initialization of the features of
    the hidden and cell state (that is, both memories of the network). An example
    of an LSTM network architecture is shown as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他神经网络类似，LSTM层可以在一行代码中轻松定义。然而，网络架构的类现在必须包含一个函数，允许初始化隐藏状态和细胞状态的特征（即网络的两个记忆）。以下是LSTM网络架构的示例：
- en: '[PRE3]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Again, the `batch_first` argument is set to `True`, when the input and output
    tensors are in the form of batches, sequences, and features. Otherwise, there
    is no need to define it as its default value is `False`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，当输入和输出张量以批次、序列和特征的形式存在时，`batch_first`参数被设置为`True`。否则，无需定义它，因为其默认值为`False`。
- en: As can be seen, the LSTM layers are defined in a single line, taking as arguments
    the number of features in the input data (that is, the number of non-duplicated
    characters), the number of hidden dimensions (neurons), and the number of LSTM
    layers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: The forward function, as with any other network, defines the way that data is
    moved through the network in a forward pass.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a function is defined to initialize the hidden and cell states to zero
    in every epoch. This is achieved by `next(self.parameters()).data.new()`, which
    grabs the first parameter of the model and creates a new tensor of the same type,
    with the specified dimensions inside the parenthesis, which is then filled with
    zeros. Both the hidden and cell state are fed into the model as a tuple.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the loss function and the optimization algorithm have been defined, it
    is time to train the model. This is achieved by following a very similar approach
    to the one that is used for other neural network architectures, as shown in the
    following code snippet:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As shown in the preceding code, the following steps are followed:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: It is necessary to go through the data several times in order to arrive at a
    better model; hence, the necessity to set a number of epochs.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each epoch, the hidden and cell states must be initialized. This is achieved
    by making a call to the function that was previously created in the class.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data is fed into the model in batches; taking into account that the input data
    should be encoded as a one-hot matrix.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output from the network is obtained by calling the model over a batch of
    data, then the loss function is calculated, and then the parameters are optimized.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing Predictions
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a good practice to provide the first couple of characters to the trained
    model, in order to perform a prediction that has some sort of purpose. This initial
    character should be fed to the model without performing any prediction, but with
    the purpose of generating a memory. Next, each new character is created by feeding
    the previous character and the memory into the network. The output from the model
    is then passed through a `softmax` function in order to obtain the probability
    of the new character of being each of the possible characters. Finally, from the
    characters with higher probabilities, one is randomly chosen.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12: Text Generation with LSTM Networks'
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The text data that will be used for the following activity is accessible for
    free on the internet, although you can also find it in this book's GitHub repository.
    The URL of the repository is mentioned in the introduction to this chapter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following activity, we will train an LSTM network using the book Alice
    in Wonderland to then be able to feed into the model a starting sentence and have
    it complete the sentence. Let''s consider the following scenario: you love things
    that make life easier and have decided to build a model that helps you to complete
    sentences when you are writing an email. To do so, you have decided to train a
    network using a popular children''s book:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下活动，我们将使用《爱丽丝梦游仙境》训练一个LSTM网络，然后能够向模型提供一个起始句子并让它完成句子。让我们考虑以下情景：你喜欢能让生活更轻松的事物，并决定建立一个模型，帮助你在写电子邮件时完成句子。为此，你已经决定使用一本流行的儿童书籍来训练一个网络：
- en: Note
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to mention that while the network in this activity is trained
    for enough iterations to display decent results, it is not trained and configured
    to achieve the best performance. You are encouraged to play with it to improve
    the performance.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，虽然本活动中的网络经过了足够的迭代以显示出不错的结果，但它并未经过训练和配置以达到最佳性能。鼓励您进行调整以改善性能。
- en: Import the required libraries.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: Open and read the text from Alice in Wonderland into the notebook. Print an
    extract of the first 100 characters and the total length of the text file.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开并读取《爱丽丝梦游仙境》的文本到笔记本中。打印前100个字符的摘录和文本文件的总长度。
- en: Create a variable containing a list of the unduplicated characters in your dataset.
    Then, create a dictionary that maps each character to an integer, where the characters
    will be the keys and the integers will be the values.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含数据集中不重复字符的列表变量。然后，创建一个字典，将每个字符映射到一个整数，其中字符将是键，整数将是值。
- en: Encode each letter of your dataset to their paired integer. Print the first
    100 encoded characters and the total length of the encoded version of your dataset.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集中的每个字母编码为它们配对的整数。打印前100个编码字符和编码版本的总长度。
- en: Create a function that takes in a batch and encodes it as a one-hot matrix.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，接受一个批次并将其编码为一个独热矩阵。
- en: Create the class that defines the architecture of the network. The class should
    contain an additional function that initializes the states of the LSTM layers.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建定义网络架构的类。该类应包含一个额外的函数，用于初始化LSTM层的状态。
- en: Determine the number of batches to be created from your dataset, bearing in
    mind that each batch should contain 100 sequences and each should be a length
    of 50\. Next, split the encoded data into 100 sequences.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定要从数据集中创建的批次数，记住每个批次应包含100个序列，每个序列长度为50。接下来，将编码数据拆分为100个序列。
- en: Initialize your model using 256 as the number of hidden units for a total of
    two recurrent layers.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用256作为两个递归层的隐藏单元数来初始化您的模型。
- en: Define the loss function and the optimization algorithms. Use the Adam optimizer
    and the cross-entropy loss.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数和优化算法。使用Adam优化器和交叉熵损失。
- en: Train the network for 20 epochs, bearing in mind that, in each epoch, the data
    must be divided into batches with a sequence length of 50\. This means that each
    epoch will have 100 sequences, each with a length of 50.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络20个时期，记住每个时期数据必须分成具有50个序列长度的批次。这意味着每个时期将有100个序列，每个长度为50。
- en: Note
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Bear in mind that the batches are created both for the inputs and targets, where
    the latter is a copy of the former, but one step ahead.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请牢记，批次不仅适用于输入和目标，其中后者是前者的副本，但向前推进一步。
- en: Plot the progress of the loss function over time.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制损失函数随时间的进展。
- en: 'Use the following sentence starter to feed into the trained model and complete
    the sentence: "So she was considering in her own mind "'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下句子作为训练模型的开头，并完成句子："So she was considering in her own mind "
- en: Note
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 223.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第223页找到。
- en: Natural Language Processing (NLP)
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: Computers are good at analyzing standardized data, such as financial records
    or databases stored in tables. In fact, they are better than humans in doing so
    as they have the capability to analyze hundreds of variables at a time. On the
    other hand, humans are great at analyzing unstructured data, such as language,
    which is something that computers are not great at doing unless they have a set
    of rules at hand to help them to understand it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, the biggest challenge for computers in regard to human language
    is that even though a computer can be good at analyzing human language after being
    trained for a very long time on a very large dataset, they are still unable to
    understand the real meaning behind a sentence as they are not intuitive, nor are
    they capable of reading between lines.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: This means that while humans are able to understand that a sentence that says
    "He was on fire last night. What a great game!" refers to the performance of a
    player of some kind of sport, a computer will understand it in a literal sense,
    meaning that it will interpret it as someone who has actually caught on fire last
    night.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: NLP is a sub-field of Artificial Intelligence (AI), which works by enabling
    computers to understand the human language. While it may be the case that humans
    will always be better at this task, the main objective of NLP is to bring computers
    closer to humans when it comes to understanding the human language.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to create models that focus on particular areas of understanding
    the human language, such as machine translation and text summarization. This specialization
    of tasks helps the computer develop a model that is capable of solving real-life
    data problems, without having to deal with all the complexity of human language
    at once.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: One of these areas of human language understanding, which is highly popular
    these days, is sentiment analysis.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general terms, sentiment analysis consists of understanding the sentiment
    behind input text. It has grown increasingly popular considering that with the
    proliferation of social media platforms, the quantity of messages and comments
    that a company receives each day has grown exponentially. This has made the task
    of manually revising and responding to each message in real time impossible, which
    can be damaging to a company's image.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis focuses on extracting the essential components of a sentence,
    while at the same time, ignoring the details. This helps to solve two primary
    needs:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the key aspects of a product or service that customers care the
    most about.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extracting the feelings behind each of these aspects, in order to determine
    which ones are causing positive and negative reactions, and hence, be able to
    transform them accordingly:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.19: An example of a tweet](img/C11865_06_19.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: An example of a tweet'
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the preceding figure, a model that performs sentiment analysis is likely
    to pick up the following information:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '"Debates" as the main topic of the tweet.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '"Debates" 作为推文的主要话题。'
- en: '"Sad" as the feeling derived from them.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '"Sad" 表示从中产生的情感。'
- en: '"America" as the main location of the sentiment over the topic.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '"America" 作为该话题情感的主要地点。'
- en: As you can see, the concept of sentiment analysis can be key to any company
    that has an online presence, as it will have the ability to respond surprisingly
    fast to those comments that require immediate attention and with a precision that
    is similar to that of a human.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，情感分析的概念对于任何具有在线存在的公司都可能至关重要，因为它将能够对那些需要立即关注的评论作出令人惊讶的快速反应，并且具有与人类相似的精度。
- en: As an example use of sentiment analysis, some companies may choose to perform
    sentiment analysis with the vast number of messages that they receive daily in
    order to prioritize a response for those messages that contain complaints or negative
    feelings. This will not only help to mitigate the negative feelings from those
    particular customers; it will also help the company improve on their mistakes
    rapidly and create a trusting relationship with their customers.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 作为情感分析的示例用途，一些公司可能选择对他们每天接收的大量消息执行情感分析，以便为那些包含投诉或负面情绪的消息优先进行响应。这不仅有助于缓解特定客户的负面情绪；还有助于公司迅速改进他们的错误并与客户建立信任关系。
- en: The process of performing NLP for sentiment analysis will be further explained
    in the following section. We will explain the concept of word embedding and the
    different steps that you can perform to develop such a model in PyTorch, which
    will be the objective of the final activity of this chapter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 关于情感分析的自然语言处理（NLP）过程将在接下来的部分中进一步解释。我们将解释词嵌入的概念以及您可以执行的不同步骤来在PyTorch中开发这样一个模型，这将是本章最后活动的目标。
- en: Sentiment Analysis in PyTorch
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中的情感分析
- en: Building a model to perform sentiment analysis in PyTorch is fairly similar
    to what we have seen so far with RNNs. The difference is that, on this occasion,
    the text data will be processed word by word. The steps that are required to build
    such a model are listed below.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中构建情感分析模型与我们迄今为止看到的RNNs非常相似。不同之处在于，这次文本数据将逐词进行处理。下面列出了构建这样一个模型所需的步骤。
- en: Preprocessing the Input Data
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理输入数据
- en: 'As with any other data problem, the data is first loaded into the code, bearing
    in mind that different methodologies are used for different data types. Besides
    converting the entire set of words to lowercase, the data undergoes some basic
    transformations that will allow you to feed the data into the network. The most
    common transformations are as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他数据问题一样，首先将数据加载到代码中，记住不同数据类型使用不同的方法。除了将整套单词转换为小写之外，数据还经历了一些基本的转换，这将允许您将数据馈送到网络中。最常见的转换如下所示：
- en: '**Eliminating punctuation**: When processing text data word-by-word for natural
    language processing purposes, it is a good practice to remove any punctuation.
    This is to avoid taking the same word as two separate words because one of them
    is followed by a dot, comma, or any other special character. Once this is achieved,
    it is possible to define a list containing the vocabulary (that is, the entire
    set of words) that is present in the input text.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消除标点符号**：在处理文本数据时，逐词进行自然语言处理时，去除任何标点符号是一个良好的实践。这是为了避免将同一个单词视为两个不同的单词，因为其中一个后面跟着句点、逗号或任何其他特殊字符。一旦实现了这一点，就可以定义一个包含词汇表（即输入文本中存在的所有单词集合）的列表。'
- en: '**Numbered labels**: Similar to the process of mapping characters previously
    explained, each word in the vocabulary is mapped to an integer, which will be
    used to replace the words of the input text in order to feed it into the network:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数字标签**：与先前解释的字符映射过程类似，词汇表中的每个单词都映射到一个整数，该整数将用于替换输入文本中的单词以供输入到网络中：'
- en: '![Equation 6.20: The mapping of words and numbers](img/C11865_06_20.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![Equation 6.20: The mapping of words and numbers](img/C11865_06_20.jpg)'
- en: 'Equation 6.20: The mapping of words and numbers'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Equation 6.20: 单词和数字的映射'
- en: Instead of performing one-hot encoding, PyTorch allows you to embed words in
    a single line of code that can be defined inside the class containing the network
    architecture (which will be explained next).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 与执行独热编码不同，PyTorch允许您在包含网络架构的类内部定义一行代码，该代码可以嵌入单词，这将在接下来解释。
- en: Building the Architecture
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建架构
- en: Again, the process of defining the network architecture is fairly similar to
    what we have studied so far. However, as mentioned before, the network should
    also include an embedding layer that will take the input data (that has been converted
    to a numeric representation) and assign a degree of relevance to each word. That
    is to say, values that will be updated during the training process until the most
    relevant words are weighted more highly.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，定义网络架构的过程与我们迄今为止所学的相似。然而，正如前面提到的，网络还应包括一个嵌入层，该层将接收已转换为数值表示的输入数据，并为每个词分配一个相关度。也就是说，在训练过程中，将更新这些值，直到最相关的词被赋予更高的权重。
- en: 'Next, an example of an architecture is displayed:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，显示了一个架构示例：
- en: '[PRE5]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the embedding layer will take as an argument the length of the
    entire vocabulary and an embedding dimension that is set by the user. This embedding
    dimension will be the input size of the LSTM layers, and the rest of the architecture
    will remain the same as before.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，嵌入层将以整个词汇表的长度和由用户设置的嵌入维度作为参数。这个嵌入维度将是 LSTM 层的输入大小，其余的架构将与之前保持一致。
- en: Training the Model
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: Finally, after defining a loss function and an optimization algorithm, the process
    of training the model is the same as other neural networks. Data may be split
    into different sets, depending on the needs and purpose of the study. A number
    of epochs and a methodology is defined in order to split the data into batches.
    The memory of the network is, typically, kept between the batches of data, but
    is then initialized to zero in every epoch. The output from the network is obtained
    by calling the model over a batch of data, then the loss function is calculated,
    and the parameters are optimized.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在定义损失函数和优化算法之后，训练模型的过程与其他神经网络相同。数据可能根据研究的需求和目的分为不同的集合。为了将数据分成批次，定义了一些时期和方法。网络的内存通常在数据的批次之间保持不变，但在每个时期都会初始化为零。通过对数据批次调用模型来获得网络的输出，然后计算损失函数并优化参数。
- en: 'Activity 13: Performing NLP for Sentiment Analysis'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 13：进行情感分析的自然语言处理
- en: Note
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The text file containing the dataset that will be used for the following activity
    can be found in this book's GitHub repository. The URL of the repository is mentioned
    in the introduction to this chapter. It is also available online at [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 包含以下活动将使用的数据集的文本文件可以在本书的 GitHub 仓库中找到。仓库的 URL 在本章的介绍中提到。它也可以在[https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)上找到。
- en: 'For the following activity, an LSTM network will be used to analyze a set of
    reviews to determine the sentiment behind them. Let''s consider the following
    scenario: you work at the public relations department of an internet provider
    and the process of reviewing every inquiry that you get on the company''s social
    media profiles is taking quite some time. The biggest problem is that the customers
    that are having issues with the service have less patience than those who do not,
    so you need to prioritize your responses so you can address them first. As you
    enjoy programming in your free time, you have decided to try to build a neural
    network that is capable of determining whether a message is negative or positive:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下活动中，将使用 LSTM 网络分析一组评论以确定它们背后的情感。假设以下情景：您在互联网提供商的公共关系部门工作，并且审查公司社交媒体个人资料上每个查询的过程需要相当长的时间。最大的问题是那些对服务有问题的客户比没有问题的客户更加缺乏耐心，因此您需要优先处理他们的回应。由于您在空闲时间喜欢编程，您决定尝试构建一个能够确定消息是负面还是正面的神经网络：
- en: Note
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to mention that the data in this activity is not divided into
    the different sets of data that allow the fine-tuning and testing of the model.
    This is because the main focus of the activity is to display the process of creating
    a model that is capable of performing sentiment analysis.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 需要提到的是，本活动中的数据并未分为不同的数据集，这些数据集允许对模型进行微调和测试。这是因为活动的主要重点是展示创建一个能够执行情感分析的模型的过程。
- en: Import the required libraries.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。
- en: 'Load the dataset containing a set of 1,000 product reviews from Amazon, which
    are paired with a label of 0 (for negative reviews) or 1 (for positive reviews).
    Separate the data into two variables: one containing the reviews and the other
    containing the labels.'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the punctuation from the reviews.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable containing the vocabulary of the entire set of reviews. Additionally,
    create a dictionary that maps each word to an integer, where the words will be
    the keys and the integers will be the values.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the reviews data by replacing each word in a review for its paired integer.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a class containing the architecture of the network. Make sure that you
    include an embedding layer.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: As the data, during the training process, won't be fed in batches, there is
    no need to return the states in the forward function. However, this does not mean
    that the model will not have a memory, but rather the memory is used to process
    each review individually, as one review is not dependent on the next one.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Initialize the model using 64 embedding dimensions and 128 neurons for 3 LSTM
    layers.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function, an optimization algorithm, and the number of epochs
    to train for. For example, you can use binary cross-entropy loss as the loss function,
    the Adam optimizer, and train for 10 epochs.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `for` loop that goes through the different epochs and through every
    single review individually. For each review, perform a prediction, calculate the
    loss function, and update the parameters of the network. Additionally, calculate
    the accuracy of the network over that training data.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the progress of the loss function and accuracy over time.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 228.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, RNNs were discussed. This type of neural network was developed
    in order to solve problems relating to data in sequences. This means that a single
    instance does not contain all the relevant information, as it depends on information
    from the previous instances.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: There are several applications that fit this type of description. For example,
    a specific portion of a text (or speech) may not mean much without the context
    of the rest of the text. However, even though NLP has been explored the most with
    RNN, there are other applications where the context of the text is important,
    such as forecasting, video processing, or music-related problems.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: An RNN works in a very clever way; the network not only outputs a result, but
    also one or more values that are often referred to as memory. This memory value
    is used as an input for future predictions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Different types of RNN configurations also exist, which are based on the inputs
    and outputs of the architecture. In a many-to-one configuration, for instance,
    multiple examples (such as words) may lead to a single final output (such as whether
    a comment is rude or not). In a many-to-many configuration, multiple inputs will
    lead to multiple outputs as in a language translation problem, where there are
    different input words and different output words.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: When working with data problems that deal with very large sequences, traditional
    RNNs present a problem called the vanishing gradient, where the gradients become
    extremely small so that they no longer contribute to the learning process of the
    network, which typically occurs in the earlier layers of the network, causing
    the network to be unable to have a long-term memory.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this problem, the LSTM network was developed. This network
    architecture is capable of storing two types of memory, hence its name. Additionally,
    the mathematical calculations that occur in this network allows it to also forget
    information – by only storing the relevant information from the past.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a very trendy NLP problem was explained: sentiment analysis. In this
    problem, it is important to understand the sentiment behind a text extraction.
    This is a very difficult problem for machines, considering that humans can use
    many different words and forms of expressions (for example, sarcasm) to describe
    the sentiment behind an event. However, thanks to the increase of social media
    usage, which has created a need to process text data faster, this problem has
    become very popular among large companies, which have invested time and money
    to create several approximations in order to solve it, as shown in the final activity
    of this chapter.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
