- en: '*Chaper 6*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Analyzing the Sequence of Data with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain the concepts of Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a simple RNN architecture to solve a forecasting data problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with Long Short-Term Memory (LSTM) architectures, and generate text using
    LSTM networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a data problem using long-term and short-term memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solve a Natural Language Processing (NLP) problem using RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you'll be equipped with the skills required for you to use
    RNNs to solve NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the book of the previous chapters, different network architectures have
    been explained – from traditional Artificial Neural Networks (ANNs), which can
    solve both classification and regression problems, to Convolutional Neural Networks
    (CNNs), which are mainly used to solve computer vision problems by performing
    the tasks of object classification, localization, detection, and segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, we will explore the concept of **Recurrent Neural Networks**
    (**RNNs**) and solve sequential data problems. These network architectures are
    capable of handling sequential data, where context is crucial, thanks to their
    ability to hold information from previous predictions, which is called memory.
    This means that, for instance, when analyzing a sentence, word by word, RNNs have
    the capability of holding information from the first word of the sentence when
    they are handling the last one.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the chapter will explore **Long Short-Term Memory** (**LSTM**) network
    architecture, which is a type of RNN that can hold both long-term and short-term
    memory, which is especially useful for long sequences of data, such as video clips.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the chapter will also explore the concept of **Natural Language Processing**
    (**NLP**). NLP refers to the interaction of computers with human languages, which
    is a popular topic nowadays thanks to the rise of virtual assistants that provide
    customized customer services. Nonetheless, the chapter will use NLP to work on
    sentiment analysis, which consists of analyzing the meaning behind sentences.
    This is useful in order to understand the sentiment of clients with regards to
    a product or a service, based on customer reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a reminder, the GitHub repository containing all code used in this chapter
    can be found at [https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch](https://github.com/TrainingByPackt/Applied-Deep-Learning-with-PyTorch).
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as humans do not reset their thinking every second, neural networks that
    aim to understand human language should not do so either. This means that in order
    to understand each word from a paragraph or even a whole book, you or the model
    are required to understand the previous words, which can help to give context
    to words that may have different meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional neural networks, as we have discussed so far, are not capable of
    performing such tasks – hence the creation of the concept and network architecture
    of RNNs. As briefly explained before, these network architectures contain loops
    among the different nodes. This allows information to remain in the model for
    longer periods of time. Due to this, the output from the model becomes both a
    prediction and a memory, which will be used when the next bit of sequenced text
    is passed through the model.
  prefs: []
  type: TYPE_NORMAL
- en: This concept goes back to the 1980s, although it has only become recently popular
    thanks to advances in technology that have led to an increase in the computational
    power of machines and have allowed the recollection of data, as well as to the
    development of the concept of LSTM RNNs in the 1990s, which increased their book
    of action. RNNs are one of the most promising network architectures out there
    thanks to their ability to store internal memory, which allows them to efficiently
    handle sequences of data and solve a wide variety of data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we have made it very clear that RNNs best work with sequences of data,
    such as text, audio clips, and videos, it is still necessary to explain the different
    applications of RNNs for real-life problems in order to understand why they are
    growing in popularity every day.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a brief explanation of the different tasks that can be performed through
    the use of RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NLP**: This refers to the ability of machines to represent human language.
    Nowadays, this is perhaps one of the most explored areas of deep learning, and
    undoubtedly the preferred data problem when making use of RNNs. The idea is to
    train the network using text as input data, such as poems and books, among others,
    with the objective of creating a model that is capable of generating such texts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NLP is commonly used for the creation of chatbots (virtual assistants). By
    learning from previous human conversations, NLP models are capable of assisting
    a person to solve frequently asked questions or queries. You will probably have
    experienced this when trying to contact a bank through an online chat system,
    where, typically, you are transferred to a human operator the minute that the
    query falls outside the conventional. Another common example of chatbots in real-life
    are restaurants that take queries through Facebook Messenger:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11865_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Facebook''s Messenger Chatbot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Speech recognition**: Similar to NLP, speech recognition attempts to understand
    and represent human language. However, the difference here is that the former
    (NLP) is trained and produces the output in the form of text, while the latter
    (speech recognition) uses audio clips. With the proliferation of developments
    in this field, and the interest of big companies, these models are capable of
    understanding different languages and even different accents and pronunciation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A popular example of a speech recognition device is Alexa – the voice-activated
    virtual assistance model from Amazon:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.2: Amazon’s Alexa](img/C11865_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Amazon''s Alexa'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Machine translation**: This refers to a machine''s ability to translate human
    languages effectively. According to this, the input is the source language (for
    instance, Spanish) and the output is the target language (such as English). The
    main difference between NLP and machine translation is that, in the latter, the
    output is built after the entire input has been fed to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the rise of globalization and the popularity of leisure traveling, nowadays
    people require access to more than one language. Due to this, a proliferation
    of devices that are capable of translating between different languages have emerged.
    One of these latest creations is Google''s introduction of Pixel Buds, which can
    perform translations in real time:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.3: Google’s Pixel Buds](img/C11865_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Google''s Pixel Buds'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Time-series forecasting**: A less popular application of an RNN is the prediction
    of a sequence of data points in the future based on historical data. RNNs are
    particularly good at this task due to their ability to retain an internal memory,
    which allows time-series analysis to consider the different timesteps in the past
    to perform a prediction or a series of predictions in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is often used to foresee future income or demand, which helps a company
    be prepared for different scenarios:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.4: The forecasting of monthly sales (qty)](img/C11865_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The forecasting of monthly sales (qty)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For example, if by forecasting the demand of several health care products, it
    is determined that there will be an increase in one of the products while the
    other will decrease, the company may decide to produce more of that product and
    less of another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Image recognition**: Coupled with CNNs, RNNs can give an image a caption
    or a description. This combination of models allows you to detect all the objects
    in the image and, hence, determines what the image is principally made of. The
    output can either be a set of tags of the objects present in the image, a description
    of the image, or a caption of the relevant objects in the image, as shown in the
    following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.5: Image recognition using RNNs](img/C11865_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Image recognition using RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: How Do RNNs Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Put simply, RNNs take an input (x) and return an output (y). Here, the output
    is not only influenced by the input but also by the entire history of inputs that
    were fed in the past. This history of inputs is often referred to as the model's
    internal state or memory, which are sequences of data that follow an order and
    are related to one another – such as a time series, which is a sequence of data
    points (for example, sales) that are listed in an order (such as by month).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bear in mind that the general structure of an RNN may vary depending on the
    problem at hand. For instance, they can be of the one-to-many type or the many-to-one
    type, as mentioned in *Chapter 2*, *Building Blocks of Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: In order to better understand the concept of RNNs, it is important to explain
    the difference between RNNs and traditional neural networks. Traditional neural
    networks are often referred to as feedforward neural networks because the information
    only moves in one direction, that is, from the input to the output, without going
    through a node twice to perform a prediction. These networks do not have any memory
    of what has been fed in the past, which is why they are no good at predicting
    what is coming next in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, in RNNs, information cycles using loops, so that every prediction
    is made considering both the input and the memory from previous predictions. It
    works by copying the output of each prediction and passing it back into the network
    for the subsequent prediction. In this way, RNNs have two inputs: the present
    value and the past information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6: A graphical representation of a network, where A shows a feedforward
    neural network and B shows an RNN](img/C11865_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: A graphical representation of a network, where A shows a feedforward
    neural network and B shows an RNN'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The internal memory of traditional RNNs is short-term only. However, we will
    later explore an architecture that is capable of storing long-term and short-term
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using information from previous predictions, the network is trained with
    a sequence of ordered data that allows it to predict the following step. This
    is achieved by combining the current information with the output from the previous
    step into a single operation (as shown in *Figure 6.7*). The output from this
    operation will become the prediction as well as part of the input for the subsequent
    prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7: An RNN computation for each prediction](img/C11865_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: An RNN computation for each prediction'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, the operation that occurs inside a node is that of any other
    neural network; initially, the data is passed through a linear function. The weights
    and biases are the parameters to be updated during the training process. Next,
    the linearity of this output is broken using an activation function. In this case,
    this is the `tanh` function, as several studies have shown that it achieves better
    results for most data problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 6.8: A mathematical computation of traditional RNNs](img/C11865_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: A mathematical computation of traditional RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, Mt-1 refers to the memory that is derived from the previous prediction,
    W and b are the weights and biases, and E refers to the current event.
  prefs: []
  type: TYPE_NORMAL
- en: With the preceding in mind, let's consider the sales data of a product from
    the last two years. RNNs are capable of predicting the next month's sales because
    by storing the information from the last couple of months, they are able to check
    whether sales have been increasing or decreasing.
  prefs: []
  type: TYPE_NORMAL
- en: Using *Figure 6.7*, the prediction of the next month could be handled by taking
    the last month's sales (that is, the current event) and the short-term memory
    (which is a representation of the data from the last couple of months) and combining
    them together. The output from this operation will contain both the prediction
    of the next month and some relevant information from the last couple of months,
    which will, in turn, become the new short-term memory for the subsequent prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is important to mention that some RNN architectures, such as the
    LSTM network, will also be able to consider data from two years ago or even much
    earlier (since it stores long-term memory), which will let the network know whether
    a decrease during a particular month is likely to continue to decrease or start
    to increase. We will explore this topic in more detail later on.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs in PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In PyTorch, which is similar to any other layer, the recurrent layers are defined
    in a single line of code. This will then be called inside the forward function
    of the network, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the recurrent layer must be defined as taking arguments for the number
    of expected features in the input (`input_size`); the number of features in the
    hidden state, which is defined by the user (`hidden_size`); and the number of
    recurrent layers (`num_layers`).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a similar way to any other neural network, the hidden size refers to the
    number of nodes (neurons) in that layer.
  prefs: []
  type: TYPE_NORMAL
- en: The `batch_first` argument is set to `True` to define that the input and output
    tensors are in the form of batches, sequences, and features.
  prefs: []
  type: TYPE_NORMAL
- en: In the `forward` function, the input is passed through the recurrent layers
    and flattened out in order to be passed through the fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, the training of such network can be handled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For each epoch, the hidden state is initialized to `none`. This is because,
    in each epoch, the network will try to map the inputs to the targets (given a
    set of parameters). This mapping should occur without any bias (hidden state)
    from the previous runs through the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Next, a `for` loop is performed to go through the different batches of data.
    Inside this loop, a prediction is made, and a hidden state is saved to be used
    as the input for the following batch.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the loss function is calculated, which is used to update the parameters
    of the network. Then, the process starts again, until the desired number of epochs
    has been reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11: Using a Simple RNN for a Time Series Prediction'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the following activity, a simple RNN will be used to solve a time series
    problem. Let''s consider the following scenario: your company wants to be able
    to predict the demand, ahead of time, for all its products. This is because it
    takes quite some time to produce each product and the procedure costs a lot of
    money. Consequently, they do not wish to spend money and time in production unless
    the product is likely to be sold. In order to predict this, they have provided
    you with a dataset containing the weekly demand (in sales transactions) for all
    products from the last year''s sales:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CSV file containing the dataset that will be used for the following activity
    can be found in this book's GitHub repository. The URL of the repository is mentioned
    in the introduction of this chapter. It is also available online at [https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly.](https://archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly.)
  prefs: []
  type: TYPE_NORMAL
- en: First, import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, set the `seed` equal to `0` to reproduce the results in this book, using
    the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Load the dataset and slice it so that it contains all the rows but only the
    columns from index 1 to 52.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the sales transactions by week of five randomly-chosen products from the
    entire dataset. Use a random seed of `0` when doing the random sampling in order
    to achieve the same results as in the current activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the `inputs` and `targets` variables that will be fed to the network
    to create the model. These variables should be of the same shape and be converted
    to PyTorch Tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `inputs` variable should contain the data for all products for all weeks,
    except the last week because the idea of the model is to predict this final week.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `targets` variable should be one step ahead of the `inputs` variable – that
    is, the first value of the `targets` variable should be the second one of the
    inputs variable, and so on, until the last value of the `targets` variable (which
    should be the last week that was left outside of the `inputs` variable).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a class containing the architecture of the network; note that the output
    size of the fully-connected layer should be 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the class function containing the model. Feed the input size, the
    number of neurons in each recurrent layer (10), and the number of recurrent layers
    (1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a loss function, an optimization algorithm, and the number of epochs
    to train the network; use the Mean Squared Error loss function, the Adam optimizer,
    and 10,000 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a `for` loop to perform the training process by going through all the epochs.
    In each epoch, a prediction must be made, along with the subsequent calculation
    of the loss function and the optimization of the parameters of the network. Then,
    save the loss of each of the epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the losses of all the epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a scatter plot, display the predictions that were obtained in the last
    epoch of the training process against the ground truth values (that is, the sales
    transactions of the last week).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 219.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Long Short-Term Memory Networks (LSTMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned before, RNNs store short-term memory only. This is an issue when
    dealing with long sequences of data, where the network will have trouble carrying
    the information from the earlier steps to the final ones.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, take the poem, The Raven, which was written by the famous poet
    Edgar Alan Poe and is over 1,000 words long. Attempting to process it using a
    traditional RNN, with the objective of creating a subsequent related poem, will
    result in the model leaving out crucial information from the first couple of paragraphs.
    This, in turn, may result in an output that is unrelated to the initial subject
    of the poem. For instance, it could ignore that the event occurred at night, and
    so make the new poem not very scary.
  prefs: []
  type: TYPE_NORMAL
- en: This inability to hold long-term memory occurs because traditional RNNs suffer
    from a problem called vanishing gradients. This occurs when the gradients, which
    are used to update the parameters of the network to minimize the loss function,
    become extremely small so that they no longer contribute to the learning process
    of the network. This typically occurs in the first layers of the networks, making
    the network forget what it has seen a while ago.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, **LSTM** networks were developed. LSTM networks can remember
    information over long periods of time as they store their internal memory in a
    similar way to a computer – that is, by having the ability to read, write, and
    delete information as needed, which is achieved through the use of gates.
  prefs: []
  type: TYPE_NORMAL
- en: These gates help the network to decide what information to keep and what information
    to delete from the memory (whether to open the gate), based on the importance
    that it assigns to each bit of information. This is extremely useful because it
    not only allows for more information to be stored (as long-term memory), but it
    also helps throw away useless information that may alter the result of a prediction,
    such as the articles in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides the applications previously explained, the ability of LSTM networks
    to store long-term information has allowed data scientists to work on complex
    data problems that make use of large sequences of data as inputs, some of which
    will be explained next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text generation**: Generating any text, such as the one you are reading here,
    can be converted into the task of an LSTM network. This works by selecting each
    letter based on all the previous letters. Networks that perform this task are
    trained with large texts, such as those of famous books. This is because the final
    model will create text that is similar to the style of writing of the one that
    it was trained from. For instance, a model that is trained over a poem will have
    a narrative that is different to the one you would expect in a conversation with
    a neighbor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Music generation**: Just as a sequence of text can be inputted into the network
    with the objective of generating similar new text, a sequence of notes can also
    be fed into the network to generate new sequences of musical notes. Keeping track
    of the previous notes will help achieve a harmonized melody, rather than just
    a series of random musical notes. For example, feeding an audio file with a popular
    song from The Beatles will result in a sequence of musical notes that resembles
    the harmony of the group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handwriting generation and recognition**: Here, each letter is also a product
    of all the previous letters, which, in turn, will result in a set of handwritten
    letters that have a meaning. Likewise, LSTM networks can also be used to recognize
    handwritten texts, where the prediction of one letter will depend on all the previously
    predicted letters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Do LSTM Networks Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, it has been made clear that what differentiates LSTM networks from traditional
    RNNs is their ability to have a long-term memory. However, it is important to
    mention that as time passes, very old information is less likely to influence
    the next output. Considering this, LSTM networks also have the ability to take
    into account the distance between data bits and the underlying context in order
    to also make the decision to forget some piece of information that is no longer
    relevant.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do LSTM networks decide when to remember and when to forget? Different
    to traditional RNNs, where only one calculation is performed in each node, LSTM
    networks perform four different calculations that allow the interaction between
    the different inputs of the network (that is the current event, the short-term
    memory, and the long-term memory) to arrive at an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the process behind LSTM networks, let''s consider the four gates
    that are used to manage the information in the network, which are represented
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: LSTM network gates](img/C11865_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: LSTM network gates'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The function of each of the gates in *Figure 6.9* can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tanh`). The output from this is multiplied by an ignore factor, which removes
    any irrelevant information. To calculate the ignore factor, the short-term memory
    and the current event are passed through a linear function. Then, they are squeezed
    together by the `sigmoid` activation function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Equation 6.10: The mathematical computations that occur in the learn gate](img/C11865_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: The mathematical computations that occur in the learn gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, STM refers to the short-term memory that is derived from the previous
    prediction, W and b are the weights and biases, and E refers to the current event.
  prefs: []
  type: TYPE_NORMAL
- en: '`sigmoid`):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Equation 6.11: The mathematical computations that occur in the forget gate](img/C11865_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: The mathematical computations that occur in the forget gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, STM refers to the short-term memory that is derived from the previous
    prediction, LSM is the long-term memory that is derived from the previous prediction,
    W and b are the weights and biases, and E refers to the current event.
  prefs: []
  type: TYPE_NORMAL
- en: '**Remember gates**: The long-term memory that was not forgotten in the forget
    gate and the information that was kept from the learn gate are joined together
    in the remember gate, which will become the new long-term memory. Mathematically,
    this is achieved by summing the output from the learn and forget gates:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Equation 6.12: The mathematical computation that occurs in the remember gate](img/C11865_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: The mathematical computation that occurs in the remember gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, L refers to the output from the learn gate, while F is the output from
    the forget gate.
  prefs: []
  type: TYPE_NORMAL
- en: '`tanh`) over the output from the forget gate. Second, it applies a linear and
    an activation function (`sigmoid`) over both the short-term memory and the current
    event. Third, it multiplies the output from the previous steps. The output from
    the third step will be the new short-term memory and the prediction from the current
    step:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Equation 6.13: The mathematical computations that occur in the use gate](img/C11865_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: The mathematical computations that occur in the use gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, STM refers to the short-term memory that is derived from the previous
    prediction, W and b are the weights and biases, and E refers to the current event.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the use of the different activation functions and mathematical operators
    seems arbitrary, it is done this way because it has been proved to work on most
    data problems that deal with large sequences of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding process is done for every single prediction that is performed
    by the model. For instance, for a model built to create literary pieces, the process
    of learning, forgetting, remembering, and using the information is performed for
    every single letter that will be produced by the model, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14: A LSTM network process through time](img/C11865_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: A LSTM network process through time'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LSTM Networks in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of defining the LSTM network architecture in PyTorch is similar
    to that of any other neural network that we have discussed so far. However, it
    is important to note that, when dealing with sequences of data that are different
    to that of numbers, there is some preprocessing required in order to feed the
    network with data that it can understand and process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering this, an explanation of the general steps to train a model to be
    able to take text data as inputs and retrieve a new piece of textual data will
    be made. It is important to mention that not all steps explained here are strictly
    required, but as a group they make a clean and reusable code for using LSTM with
    textual data:'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the Input Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step is to load the text file into the code. This data will go through
    a series of transformations in order to be properly fed into the model. This is
    necessary because neural networks perform a series of mathematical computations
    to arrive at an output, which means that all inputs must be numerical. Additionally,
    it is also a good practice to feed the data in batches to the model, rather than
    all at once, as it helps to reduce the training times, especially for long datasets.
    These transformations are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numbered Labels**'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, a list of unduplicated characters is obtained from the input data. Each
    of these characters is assigned a number. Then, the input data is encoded by replacing
    each character with the assigned number. For instance, the word "hello" will be
    encoded into 123344, given the following mapping of characters and numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 6.15: The mapping of characters and numbers](img/C11865_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: The mapping of characters and numbers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Generating the Batches**'
  prefs: []
  type: TYPE_NORMAL
- en: For RNNs, batches are created using two variables. First, the number of sequences
    per batch, and second, the length of each sequence. These values are used to divide
    the data into matrices, which will help speed up the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a dataset of 24 integers, with the number of sequences per batch set
    to 2 and the sequence length equal to 4, the division works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16: Batch generation for RNNs](img/C11865_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Batch generation for RNNs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As can be seen in *Figure 6.16*, 3 batches are created – with each of these
    batches containing 2 sequences, each of length 4.
  prefs: []
  type: TYPE_NORMAL
- en: This batch generation process should be done for `x` and for `y`, where the
    former is the input to the network, and the latter represents the targets. According
    to this, the idea of the network is to find a way to map the relationship between
    `x` and `y`, considering that `y` will be 1 step ahead of `x`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The batches for `x` are created following the methodology explained in the
    previous diagram (Figure 6.16). Then, the batches of `y` will be created of the
    same length of those of `x`. This is because the first element of `y` will be
    the second element of `x`, and so on, until the last element of `y` (which will
    be the first element of `x)`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are a number of different approaches that you can use to fill in the last
    element of y, and the one that is mentioned here is the most commonly used. The
    choice of approach is often a matter of preference, although some data problems
    may benefit more from an approach than from others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: A representation of the batches for X and Y](img/C11865_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.17: A representation of the batches for X and Y'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although generating batches is considered as part of the preprocessing of the
    data, it is often programmed inside the `for` loop of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To convert all characters into numbers is not enough to feed them into the
    model. This is because this approximation introduces some bias to your model,
    since the characters that are converted into higher numerical values will be evaluated
    as more important. To avoid this, it is a good practice to encode the different
    batches as one-hot matrices. This consists of creating a three-dimensional matrix
    with zeros and ones, where a zero represents the absence of an event and the one
    refers to the presence of the event. Bearing this in mind, the final shape of
    the matrix should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Equation 6.18: One-hot matrix dimensions](img/C11865_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 6.18: One-hot matrix dimensions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This means that for every position in the batch, it will create a sequence of
    values of length that is equal to the total number of characters in the entire
    text. For every character, it will place a zero, except for the one that is present
    in that position (where it will place a one).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find out more about one-hot encoding at [https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f.](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f.)
  prefs: []
  type: TYPE_NORMAL
- en: Building the Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to other neural networks, an LSTM layer is easily defined in a single
    line of code. Nevertheless, the class containing the architecture of the network
    must now include a function that allows the initialization of the features of
    the hidden and cell state (that is, both memories of the network). An example
    of an LSTM network architecture is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Again, the `batch_first` argument is set to `True`, when the input and output
    tensors are in the form of batches, sequences, and features. Otherwise, there
    is no need to define it as its default value is `False`.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, the LSTM layers are defined in a single line, taking as arguments
    the number of features in the input data (that is, the number of non-duplicated
    characters), the number of hidden dimensions (neurons), and the number of LSTM
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: The forward function, as with any other network, defines the way that data is
    moved through the network in a forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a function is defined to initialize the hidden and cell states to zero
    in every epoch. This is achieved by `next(self.parameters()).data.new()`, which
    grabs the first parameter of the model and creates a new tensor of the same type,
    with the specified dimensions inside the parenthesis, which is then filled with
    zeros. Both the hidden and cell state are fed into the model as a tuple.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the loss function and the optimization algorithm have been defined, it
    is time to train the model. This is achieved by following a very similar approach
    to the one that is used for other neural network architectures, as shown in the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding code, the following steps are followed:'
  prefs: []
  type: TYPE_NORMAL
- en: It is necessary to go through the data several times in order to arrive at a
    better model; hence, the necessity to set a number of epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each epoch, the hidden and cell states must be initialized. This is achieved
    by making a call to the function that was previously created in the class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data is fed into the model in batches; taking into account that the input data
    should be encoded as a one-hot matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output from the network is obtained by calling the model over a batch of
    data, then the loss function is calculated, and then the parameters are optimized.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Performing Predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a good practice to provide the first couple of characters to the trained
    model, in order to perform a prediction that has some sort of purpose. This initial
    character should be fed to the model without performing any prediction, but with
    the purpose of generating a memory. Next, each new character is created by feeding
    the previous character and the memory into the network. The output from the model
    is then passed through a `softmax` function in order to obtain the probability
    of the new character of being each of the possible characters. Finally, from the
    characters with higher probabilities, one is randomly chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 12: Text Generation with LSTM Networks'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The text data that will be used for the following activity is accessible for
    free on the internet, although you can also find it in this book's GitHub repository.
    The URL of the repository is mentioned in the introduction to this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following activity, we will train an LSTM network using the book Alice
    in Wonderland to then be able to feed into the model a starting sentence and have
    it complete the sentence. Let''s consider the following scenario: you love things
    that make life easier and have decided to build a model that helps you to complete
    sentences when you are writing an email. To do so, you have decided to train a
    network using a popular children''s book:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to mention that while the network in this activity is trained
    for enough iterations to display decent results, it is not trained and configured
    to achieve the best performance. You are encouraged to play with it to improve
    the performance.
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open and read the text from Alice in Wonderland into the notebook. Print an
    extract of the first 100 characters and the total length of the text file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable containing a list of the unduplicated characters in your dataset.
    Then, create a dictionary that maps each character to an integer, where the characters
    will be the keys and the integers will be the values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode each letter of your dataset to their paired integer. Print the first
    100 encoded characters and the total length of the encoded version of your dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function that takes in a batch and encodes it as a one-hot matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the class that defines the architecture of the network. The class should
    contain an additional function that initializes the states of the LSTM layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the number of batches to be created from your dataset, bearing in
    mind that each batch should contain 100 sequences and each should be a length
    of 50\. Next, split the encoded data into 100 sequences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize your model using 256 as the number of hidden units for a total of
    two recurrent layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function and the optimization algorithms. Use the Adam optimizer
    and the cross-entropy loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the network for 20 epochs, bearing in mind that, in each epoch, the data
    must be divided into batches with a sequence length of 50\. This means that each
    epoch will have 100 sequences, each with a length of 50.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Bear in mind that the batches are created both for the inputs and targets, where
    the latter is a copy of the former, but one step ahead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Plot the progress of the loss function over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following sentence starter to feed into the trained model and complete
    the sentence: "So she was considering in her own mind "'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 223.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computers are good at analyzing standardized data, such as financial records
    or databases stored in tables. In fact, they are better than humans in doing so
    as they have the capability to analyze hundreds of variables at a time. On the
    other hand, humans are great at analyzing unstructured data, such as language,
    which is something that computers are not great at doing unless they have a set
    of rules at hand to help them to understand it.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, the biggest challenge for computers in regard to human language
    is that even though a computer can be good at analyzing human language after being
    trained for a very long time on a very large dataset, they are still unable to
    understand the real meaning behind a sentence as they are not intuitive, nor are
    they capable of reading between lines.
  prefs: []
  type: TYPE_NORMAL
- en: This means that while humans are able to understand that a sentence that says
    "He was on fire last night. What a great game!" refers to the performance of a
    player of some kind of sport, a computer will understand it in a literal sense,
    meaning that it will interpret it as someone who has actually caught on fire last
    night.
  prefs: []
  type: TYPE_NORMAL
- en: NLP is a sub-field of Artificial Intelligence (AI), which works by enabling
    computers to understand the human language. While it may be the case that humans
    will always be better at this task, the main objective of NLP is to bring computers
    closer to humans when it comes to understanding the human language.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to create models that focus on particular areas of understanding
    the human language, such as machine translation and text summarization. This specialization
    of tasks helps the computer develop a model that is capable of solving real-life
    data problems, without having to deal with all the complexity of human language
    at once.
  prefs: []
  type: TYPE_NORMAL
- en: One of these areas of human language understanding, which is highly popular
    these days, is sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general terms, sentiment analysis consists of understanding the sentiment
    behind input text. It has grown increasingly popular considering that with the
    proliferation of social media platforms, the quantity of messages and comments
    that a company receives each day has grown exponentially. This has made the task
    of manually revising and responding to each message in real time impossible, which
    can be damaging to a company's image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentiment analysis focuses on extracting the essential components of a sentence,
    while at the same time, ignoring the details. This helps to solve two primary
    needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the key aspects of a product or service that customers care the
    most about.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extracting the feelings behind each of these aspects, in order to determine
    which ones are causing positive and negative reactions, and hence, be able to
    transform them accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.19: An example of a tweet](img/C11865_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19: An example of a tweet'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From the preceding figure, a model that performs sentiment analysis is likely
    to pick up the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Debates" as the main topic of the tweet.'
  prefs: []
  type: TYPE_NORMAL
- en: '"Sad" as the feeling derived from them.'
  prefs: []
  type: TYPE_NORMAL
- en: '"America" as the main location of the sentiment over the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the concept of sentiment analysis can be key to any company
    that has an online presence, as it will have the ability to respond surprisingly
    fast to those comments that require immediate attention and with a precision that
    is similar to that of a human.
  prefs: []
  type: TYPE_NORMAL
- en: As an example use of sentiment analysis, some companies may choose to perform
    sentiment analysis with the vast number of messages that they receive daily in
    order to prioritize a response for those messages that contain complaints or negative
    feelings. This will not only help to mitigate the negative feelings from those
    particular customers; it will also help the company improve on their mistakes
    rapidly and create a trusting relationship with their customers.
  prefs: []
  type: TYPE_NORMAL
- en: The process of performing NLP for sentiment analysis will be further explained
    in the following section. We will explain the concept of word embedding and the
    different steps that you can perform to develop such a model in PyTorch, which
    will be the objective of the final activity of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Sentiment Analysis in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building a model to perform sentiment analysis in PyTorch is fairly similar
    to what we have seen so far with RNNs. The difference is that, on this occasion,
    the text data will be processed word by word. The steps that are required to build
    such a model are listed below.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the Input Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As with any other data problem, the data is first loaded into the code, bearing
    in mind that different methodologies are used for different data types. Besides
    converting the entire set of words to lowercase, the data undergoes some basic
    transformations that will allow you to feed the data into the network. The most
    common transformations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Eliminating punctuation**: When processing text data word-by-word for natural
    language processing purposes, it is a good practice to remove any punctuation.
    This is to avoid taking the same word as two separate words because one of them
    is followed by a dot, comma, or any other special character. Once this is achieved,
    it is possible to define a list containing the vocabulary (that is, the entire
    set of words) that is present in the input text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Numbered labels**: Similar to the process of mapping characters previously
    explained, each word in the vocabulary is mapped to an integer, which will be
    used to replace the words of the input text in order to feed it into the network:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Equation 6.20: The mapping of words and numbers](img/C11865_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation 6.20: The mapping of words and numbers'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of performing one-hot encoding, PyTorch allows you to embed words in
    a single line of code that can be defined inside the class containing the network
    architecture (which will be explained next).
  prefs: []
  type: TYPE_NORMAL
- en: Building the Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Again, the process of defining the network architecture is fairly similar to
    what we have studied so far. However, as mentioned before, the network should
    also include an embedding layer that will take the input data (that has been converted
    to a numeric representation) and assign a degree of relevance to each word. That
    is to say, values that will be updated during the training process until the most
    relevant words are weighted more highly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, an example of an architecture is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the embedding layer will take as an argument the length of the
    entire vocabulary and an embedding dimension that is set by the user. This embedding
    dimension will be the input size of the LSTM layers, and the rest of the architecture
    will remain the same as before.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, after defining a loss function and an optimization algorithm, the process
    of training the model is the same as other neural networks. Data may be split
    into different sets, depending on the needs and purpose of the study. A number
    of epochs and a methodology is defined in order to split the data into batches.
    The memory of the network is, typically, kept between the batches of data, but
    is then initialized to zero in every epoch. The output from the network is obtained
    by calling the model over a batch of data, then the loss function is calculated,
    and the parameters are optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 13: Performing NLP for Sentiment Analysis'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The text file containing the dataset that will be used for the following activity
    can be found in this book's GitHub repository. The URL of the repository is mentioned
    in the introduction to this chapter. It is also available online at [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following activity, an LSTM network will be used to analyze a set of
    reviews to determine the sentiment behind them. Let''s consider the following
    scenario: you work at the public relations department of an internet provider
    and the process of reviewing every inquiry that you get on the company''s social
    media profiles is taking quite some time. The biggest problem is that the customers
    that are having issues with the service have less patience than those who do not,
    so you need to prioritize your responses so you can address them first. As you
    enjoy programming in your free time, you have decided to try to build a neural
    network that is capable of determining whether a message is negative or positive:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important to mention that the data in this activity is not divided into
    the different sets of data that allow the fine-tuning and testing of the model.
    This is because the main focus of the activity is to display the process of creating
    a model that is capable of performing sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the dataset containing a set of 1,000 product reviews from Amazon, which
    are paired with a label of 0 (for negative reviews) or 1 (for positive reviews).
    Separate the data into two variables: one containing the reviews and the other
    containing the labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the punctuation from the reviews.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable containing the vocabulary of the entire set of reviews. Additionally,
    create a dictionary that maps each word to an integer, where the words will be
    the keys and the integers will be the values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode the reviews data by replacing each word in a review for its paired integer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a class containing the architecture of the network. Make sure that you
    include an embedding layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: As the data, during the training process, won't be fed in batches, there is
    no need to return the states in the forward function. However, this does not mean
    that the model will not have a memory, but rather the memory is used to process
    each review individually, as one review is not dependent on the next one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Initialize the model using 64 embedding dimensions and 128 neurons for 3 LSTM
    layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function, an optimization algorithm, and the number of epochs
    to train for. For example, you can use binary cross-entropy loss as the loss function,
    the Adam optimizer, and train for 10 epochs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `for` loop that goes through the different epochs and through every
    single review individually. For each review, perform a prediction, calculate the
    loss function, and update the parameters of the network. Additionally, calculate
    the accuracy of the network over that training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the progress of the loss function and accuracy over time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 228.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, RNNs were discussed. This type of neural network was developed
    in order to solve problems relating to data in sequences. This means that a single
    instance does not contain all the relevant information, as it depends on information
    from the previous instances.
  prefs: []
  type: TYPE_NORMAL
- en: There are several applications that fit this type of description. For example,
    a specific portion of a text (or speech) may not mean much without the context
    of the rest of the text. However, even though NLP has been explored the most with
    RNN, there are other applications where the context of the text is important,
    such as forecasting, video processing, or music-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN works in a very clever way; the network not only outputs a result, but
    also one or more values that are often referred to as memory. This memory value
    is used as an input for future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Different types of RNN configurations also exist, which are based on the inputs
    and outputs of the architecture. In a many-to-one configuration, for instance,
    multiple examples (such as words) may lead to a single final output (such as whether
    a comment is rude or not). In a many-to-many configuration, multiple inputs will
    lead to multiple outputs as in a language translation problem, where there are
    different input words and different output words.
  prefs: []
  type: TYPE_NORMAL
- en: When working with data problems that deal with very large sequences, traditional
    RNNs present a problem called the vanishing gradient, where the gradients become
    extremely small so that they no longer contribute to the learning process of the
    network, which typically occurs in the earlier layers of the network, causing
    the network to be unable to have a long-term memory.
  prefs: []
  type: TYPE_NORMAL
- en: In order to solve this problem, the LSTM network was developed. This network
    architecture is capable of storing two types of memory, hence its name. Additionally,
    the mathematical calculations that occur in this network allows it to also forget
    information – by only storing the relevant information from the past.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a very trendy NLP problem was explained: sentiment analysis. In this
    problem, it is important to understand the sentiment behind a text extraction.
    This is a very difficult problem for machines, considering that humans can use
    many different words and forms of expressions (for example, sarcasm) to describe
    the sentiment behind an event. However, thanks to the increase of social media
    usage, which has created a need to process text data faster, this problem has
    become very popular among large companies, which have invested time and money
    to create several approximations in order to solve it, as shown in the final activity
    of this chapter.'
  prefs: []
  type: TYPE_NORMAL
