["```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Optimizer\nKD_loss = nn.KLDivLoss(reduction='batchmean')\ndef kd_step(teacher: nn.Module,\n            student: nn.Module,\n            temperature: float,\n            inputs: torch.tensor,\n            optimizer: Optimizer):\n    teacher.eval()\n    student.train()\n    with torch.no_grad():\n        logits_t = teacher(inputs=inputs)\n    logits_s = student(inputs=inputs)\n    loss = KD_loss(input=F.log_softmax(\n                            logits_s/temperature, \n                            dim=-1),\n                   target=F.softmax(\n                            logits_t/temperature, \n                            dim=-1))\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```", "```py\n    from sentence_transformers import SentenceTransformer\n    distilroberta = SentenceTransformer('stsb-distilroberta-base-v2')\n    ```", "```py\n    from datasets import load_metric, load_dataset\n    stsb_metric = load_metric('glue', 'stsb')\n    stsb = load_dataset('glue', 'stsb')\n    mrpc_metric = load_metric('glue', 'mrpc')\n    mrpc = load_dataset('glue','mrpc')\n    ```", "```py\n    import math\n    import tensorflow as tf\n    def roberta_sts_benchmark(batch):\n        sts_encode1 = tf.nn.l2_normalize(\n                    distilroberta.encode(batch['sentence1']),\n                    axis=1)\n        sts_encode2 = tf.nn.l2_normalize(\n            distilroberta.encode(batch['sentence2']), axis=1)\n        cosine_similarities = tf.reduce_sum(\n            tf.multiply(sts_encode1, sts_encode2), axis=1)\n        clip_cosine_similarities = tf.clip_by_value(cosine_similarities,-1.0,1.0)\n        scores = 1.0 -\\\n                  tf.acos(clip_cosine_similarities) / math.pi\n    return scores\n    ```", "```py\n    references = stsb['validation'][:]['label']\n    ```", "```py\n    distilroberta_results = roberta_sts_benchmark(stsb['validation'])\n    ```", "```py\n    from torch.nn.utils import prune\n    pruner = prune.L1Unstructured(amount=0.2)\n    ```", "```py\n    state_dict = distilroberta.state_dict()\n    for key in state_dict.keys():\n        if \"weight\" in key:\n            state_dict[key] = pruner.prune(state_dict[key])\n    ```", "```py\n    distilroberta.load_state_dict(state_dict)\n    ```", "```py\n    distilroberta_results_p = roberta_sts_benchmark(stsb['validation'])\n    ```", "```py\n    import pandas as pd\n    pd.DataFrame({\n    \"DistillRoberta\":stsb_metric.compute(predictions=distilroberta_results, references=references),\n    \"DistillRobertaPruned\":stsb_metric.compute(predictions=distilroberta_results_p, references=references)\n    })\n    ```", "```py\n    import torch\n    distilroberta = torch.quantization.quantize_dynamic(\n                model=distilroberta,\n                qconfig_spec = {\n                torch.nn.Linear :\n                torch.quantization.default_dynamic_qconfig,\n                               },\n                dtype=torch.qint8)\n    ```", "```py\n    distilroberta_results_pq = roberta_sts_benchmark(stsb['validation'])\n    ```", "```py\n    pd.DataFrame({\n    \"DistillRoberta\":stsb_metric.compute(predictions=distilroberta_results, references=references),\n    \"DistillRobertaPruned\":stsb_metric.compute(predictions=distilroberta_results_p, references=references),\n    \"DistillRobertaPrunedQINT8\":stsb_metric.compute(predictions=distilroberta_results_pq, references=references)\n    })\n    ```", "```py\n    distilroberta.save(\"model_pq\")\n    ```", "```py\n    ls model_pq/0_Transformer/ -l --block-size=M | grep pytorch_model.bin\n    -rw-r--r-- 1 root 191M May 23 14:53 pytorch_model.bin\n    ```", "```py\n    !pip install py3nvml\n    ```", "```py\n    !nvidia-smi\n    ```", "```py\n    from transformers import LongformerTokenizer, LongformerForSequenceClassification\n    import torch\n    tokenizer = LongformerTokenizer.from_pretrained(\n        'allenai/longformer-base-4096')\n    model=LongformerForSequenceClassification.from_pretrained(\n        'allenai/longformer-base-4096')\n    sequence= \"hello \"*4093\n    inputs = tokenizer(sequence, return_tensors=\"pt\")\n    print(\"input shape: \",inputs.input_ids.shape)\n    outputs = model(**inputs)\n    ```", "```py\n    input shape:  torch.Size([1, 4096])\n    ```", "```py\n    from transformers import LongformerConfig, \\\n    PyTorchBenchmark, PyTorchBenchmarkArguments\n    config_longformer=LongformerConfig.from_pretrained(\n        \"allenai/longformer-base-4096\")\n    config_longformer_window4=LongformerConfig.from_pretrained(\n        \"allenai/longformer-base-4096\", \n         attention_window=4)\n    ```", "```py\n    from transformers import LongformerModel\n    model = LongformerModel(config_longformer)\n    ```", "```py\n    sequence_lengths=[128,256,512,1024,2048,4096]\n    models=[\"config_longformer\",\"config_longformer_window4\"]\n    configs=[eval(m) for m in models]\n    benchmark_args = PyTorchBenchmarkArguments(\n        sequence_lengths= sequence_lengths, \n        batch_sizes=[1], \n        models= models)\n    benchmark = PyTorchBenchmark(\n        configs=configs, \n        args=benchmark_args)\n    results = benchmark.run()\n    ```", "```py\n    import matplotlib.pyplot as plt \n    def plotMe(results,title=\"Time\"):\n        plt.figure(figsize=(8,8))\n        fmts= [\"rs--\",\"go--\",\"b+-\",\"c-o\"]\n        q=results.memory_inference_result\n        if title==\"Time\": \n            q=results.time_inference_result\n        models=list(q.keys())\n        seq=list(q[models[0]]['result'][1].keys())\n        models_perf=[list(q[m]['result'][1].values()) \\\n            for m in models] \n        plt.xlabel('Sequence Length') \n        plt.ylabel(title) \n        plt.title('Inference Result') \n        for perf,fmt in zip(models_perf,fmts):\n            plt.plot(seq, perf,fmt)\n        plt.legend(models) \n        plt.show()\n    ```", "```py\n    plotMe(results)\n    ```", "```py\n    plotMe(results, \"Memory\")\n    ```", "```py\n    from transformers import BigBirdConfig\n    # Default Bird with num_random_blocks=3, block_size=64\n    sparseBird = BigBirdConfig.from_pretrained(\n        \"google/bigbird-roberta-base\")\n    fullBird = BigBirdConfig.from_pretrained(\n        \"google/bigbird-roberta-base\", \n        attention_type=\"original_full\")\n    ```", "```py\n    sequence_lengths=[256,512,1024,2048, 3072, 4096]\n    models=[\"sparseBird\",\"fullBird\"]\n    configs=[eval(m) for m in models]\n    benchmark_args = PyTorchBenchmarkArguments(\n        sequence_lengths=sequence_lengths,\n        batch_sizes=[1],\n        models=models)\n    benchmark = PyTorchBenchmark(\n        configs=configs, \n        args=benchmark_args)\n    results = benchmark.run()\n    ```", "```py\n    plotMe(results)\n    ```", "```py\n    plotMe(results,\"Memory\")\n    ```", "```py\n    from transformers import ReformerConfig\n    fullReformer = ReformerConfig\\\n        .from_pretrained(\"google/reformer-enwik8\",  \n            lsh_attn_chunk_length=16384, \n            local_attn_chunk_length=16384)\n    sparseReformer = ReformerConfig\\\n        .from_pretrained(\"google/reformer-enwik8\")\n    sequence_lengths=[256, 512, 1024, 2048, 4096, 8192, 12000]\n    models=[\"fullReformer\",\"sparseReformer\"]\n    configs=[eval(e) for e in models]\n    ```", "```py\n    benchmark_args = PyTorchBenchmarkArguments(\n        sequence_lengths=sequence_lengths,\n        batch_sizes=[1],\n        models=models)\n    benchmark = PyTorchBenchmark(\n        configs=configs, \n        args=benchmark_args)\n    result = benchmark.run()\n    ```", "```py\n    plotMe(result)\n    ```", "```py\n    plotMe(result,\"Memory Footprint\")\n    ```"]