["```py\n>>> import torch\n>>> import torch.nn as nn\n>>> torch.manual_seed(1)\n>>> rnn_layer = nn.RNN(input_size=5, hidden_size=2,\n...                    num_layers=1, batch_first=True)\n>>> w_xh = rnn_layer.weight_ih_l0\n>>> w_hh = rnn_layer.weight_hh_l0\n>>> b_xh = rnn_layer.bias_ih_l0\n>>> b_hh = rnn_layer.bias_hh_l0\n>>> print('W_xh shape:', w_xh.shape)\n>>> print('W_hh shape:', w_hh.shape)\n>>> print('b_xh shape:', b_xh.shape)\n>>> print('b_hh shape:', b_hh.shape)\nW_xh shape: torch.Size([2, 5])\nW_hh shape: torch.Size([2, 2])\nb_xh shape: torch.Size([2])\nb_hh shape: torch.Size([2]) \n```", "```py\n>>> x_seq = torch.tensor([[1.0]*5, [2.0]*5, [3.0]*5]).float()\n>>> ## output of the simple RNN:\n>>> output, hn = rnn_layer(torch.reshape(x_seq, (1, 3, 5)))\n>>> ## manually computing the output:\n>>> out_man = []\n>>> for t in range(3):\n...     xt = torch.reshape(x_seq[t], (1, 5))\n...     print(f'Time step {t} =>')\n...     print('   Input           :', xt.numpy())\n...     \n...     ht = torch.matmul(xt, torch.transpose(w_xh, 0, 1)) + b_hh\n...     print('   Hidden          :', ht.detach().numpy()\n...     \n...     if t > 0:\n...         prev_h = out_man[t-1]\n...     else:\n...         prev_h = torch.zeros((ht.shape))\n...     ot = ht + torch.matmul(prev_h, torch.transpose(w_hh, 0, 1)) \\\n...             + b_hh\n...     ot = torch.tanh(ot)\n...     out_man.append(ot)\n...     print('   Output (manual) :', ot.detach().numpy())\n...     print('   RNN output      :', output[:, t].detach().numpy())\n...     print()\nTime step 0 =>\n   Input           : [[1\\. 1\\. 1\\. 1\\. 1.]]\n   Hidden          : [[-0.4701929  0.5863904]]\n   Output (manual) : [[-0.3519801   0.52525216]]\n   RNN output      : [[-0.3519801   0.52525216]]\nTime step 1 =>\n   Input           : [[2\\. 2\\. 2\\. 2\\. 2.]]\n   Hidden          : [[-0.88883156  1.2364397 ]]\n   Output (manual) : [[-0.68424344  0.76074266]]\n   RNN output      : [[-0.68424344  0.76074266]]\nTime step 2 =>\n   Input           : [[3\\. 3\\. 3\\. 3\\. 3.]]\n   Hidden          : [[-1.3074701  1.886489 ]]\n   Output (manual) : [[-0.8649416   0.90466356]]\n   RNN output      : [[-0.8649416   0.90466356]] \n```", "```py\n>>> from torchtext.datasets import IMDB\n>>> train_dataset = IMDB(split='train')\n>>> test_dataset = IMDB(split='test') \n```", "```py\n>>> ## Step 1: create the datasets\n>>> from torch.utils.data.dataset import random_split\n>>> torch.manual_seed(1)\n>>> train_dataset, valid_dataset = random_split(\n...     list(train_dataset), [20000, 5000]) \n```", "```py\n>>> ## Step 2: find unique tokens (words)\n>>> import re\n>>> from collections import Counter, OrderedDict\n>>> \n>>> def tokenizer(text):\n...     text = re.sub('<[^>]*>', '', text)\n...     emoticons = re.findall(\n...         '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n...     )\n...     text = re.sub('[\\W]+', ' ', text.lower()) +\\\n...         ' '.join(emoticons).replace('-', '')\n...     tokenized = text.split()\n...     return tokenized\n>>> \n>>> token_counts = Counter()\n>>> for label, line in train_dataset:\n...     tokens = tokenizer(line)\n...     token_counts.update(tokens)\n>>> print('Vocab-size:', len(token_counts))\nVocab-size: 69023 \n```", "```py\n>>> ## Step 3: encoding each unique token into integers\n>>> from torchtext.vocab import vocab\n>>> sorted_by_freq_tuples = sorted(\n...     token_counts.items(), key=lambda x: x[1], reverse=True\n... )\n>>> ordered_dict = OrderedDict(sorted_by_freq_tuples)\n>>> vocab = vocab(ordered_dict)\n>>> vocab.insert_token(\"<pad>\", 0)\n>>> vocab.insert_token(\"<unk>\", 1)\n>>> vocab.set_default_index(1) \n```", "```py\n>>> print([vocab[token] for token in ['this', 'is',\n...     'an', 'example']])\n[11, 7, 35, 457] \n```", "```py\n>>> ## Step 3-A: define the functions for transformation\n>>> text_pipeline =\\\n...      lambda x: [vocab[token] for token in tokenizer(x)]\n>>> label_pipeline = lambda x: 1\\. if x == 'pos' else 0. \n```", "```py\n>>> ## Step 3-B: wrap the encode and transformation function\n... def collate_batch(batch):\n...     label_list, text_list, lengths = [], [], []\n...     for _label, _text in batch:\n...         label_list.append(label_pipeline(_label))\n...         processed_text = torch.tensor(text_pipeline(_text),\n...                                       dtype=torch.int64)\n...         text_list.append(processed_text)\n...         lengths.append(processed_text.size(0))\n...     label_list = torch.tensor(label_list)\n...     lengths = torch.tensor(lengths)\n...     padded_text_list = nn.utils.rnn.pad_sequence(\n...         text_list, batch_first=True)\n...     return padded_text_list, label_list, lengths\n>>> \n>>> ## Take a small batch\n>>> from torch.utils.data import DataLoader\n>>> dataloader = DataLoader(train_dataset, batch_size=4,\n...                         shuffle=False, collate_fn=collate_batch) \n```", "```py\n>>> text_batch, label_batch, length_batch = next(iter(dataloader))\n>>> print(text_batch)\ntensor([[   35,  1742,     7,   449,   723,     6,   302,     4,\n...\n0,     0,     0,     0,     0,     0,     0,     0]],\n>>> print(label_batch)\ntensor([1., 1., 1., 0.])\n>>> print(length_batch)\ntensor([165,  86, 218, 145])\n>>> print(text_batch.shape)\ntorch.Size([4, 218]) \n```", "```py\n>>> batch_size = 32\n>>> train_dl = DataLoader(train_dataset, batch_size=batch_size,\n...                       shuffle=True, collate_fn=collate_batch)\n>>> valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n...                       shuffle=False, collate_fn=collate_batch)\n>>> test_dl = DataLoader(test_dataset, batch_size=batch_size,\n...                      shuffle=False, collate_fn=collate_batch) \n```", "```py\n>>> embedding = nn.Embedding(\n...     num_embeddings=10,\n...     embedding_dim=3,\n...     padding_idx=0)\n>>> # a batch of 2 samples of 4 indices each\n>>> text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]])\n>>> print(embedding(text_encoded_input))\ntensor([[[-0.7027,  0.3684, -0.5512],\n         [-0.4147,  1.7891, -1.0674],\n         [ 1.1400,  0.1595, -1.0167],\n         [ 0.0573, -1.7568,  1.9067]],\n        [[ 1.1400,  0.1595, -1.0167],\n         [-0.8165, -0.0946, -0.1881],\n         [-0.4147,  1.7891, -1.0674],\n         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>) \n```", "```py\n>>> class RNN(nn.Module):\n...     def __init__(self, input_size, hidden_size):\n...         super().__init__()\n...         self.rnn = nn.RNN(input_size, hidden_size, num_layers=2,\n...                           batch_first=True)\n...         # self.rnn = nn.GRU(input_size, hidden_size, num_layers,\n...         #                   batch_first=True)\n...         # self.rnn = nn.LSTM(input_size, hidden_size, num_layers,\n...         #                    batch_first=True)\n...         self.fc = nn.Linear(hidden_size, 1)\n...\n...     def forward(self, x):\n...         _, hidden = self.rnn(x)\n...         out = hidden[-1, :, :] # we use the final hidden state\n...                                # from the last hidden layer as\n...                                # the input to the fully connected\n...                                # layer\n...         out = self.fc(out)\n...         return out\n>>>\n>>> model = RNN(64, 32)\n>>> print(model)\n>>> model(torch.randn(5, 3, 64))\nRNN(\n  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n  (fc): Linear(in_features=32, out_features=1, bias=True)\n)\ntensor([[ 0.0010],\n        [ 0.2478],\n        [ 0.0573],\n        [ 0.1637],\n        [-0.0073]], grad_fn=<AddmmBackward>) \n```", "```py\n>>> class RNN(nn.Module):\n...     def __init__(self, vocab_size, embed_dim, rnn_hidden_size,\n...                  fc_hidden_size):\n...         super().__init__()\n...         self.embedding = nn.Embedding(vocab_size,\n...                                       embed_dim,\n...                                       padding_idx=0)\n...         self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n...                            batch_first=True)\n...         self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n...         self.relu = nn.ReLU()\n...         self.fc2 = nn.Linear(fc_hidden_size, 1)\n...         self.sigmoid = nn.Sigmoid()\n...\n...     def forward(self, text, lengths):\n...         out = self.embedding(text)\n...         out = nn.utils.rnn.pack_padded_sequence(\n...             out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n...         )\n...         out, (hidden, cell) = self.rnn(out)\n...         out = hidden[-1, :, :]\n...         out = self.fc1(out)\n...         out = self.relu(out)\n...         out = self.fc2(out)\n...         out = self.sigmoid(out)\n...         return out\n>>> \n>>> vocab_size = len(vocab)\n>>> embed_dim = 20\n>>> rnn_hidden_size = 64\n>>> fc_hidden_size = 64\n>>> torch.manual_seed(1)\n>>> model = RNN(vocab_size, embed_dim,\n                rnn_hidden_size, fc_hidden_size)\n>>> model\nRNN(\n  (embedding): Embedding(69025, 20, padding_idx=0)\n  (rnn): LSTM(20, 64, batch_first=True)\n  (fc1): Linear(in_features=64, out_features=64, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=64, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n) \n```", "```py\n>>> def train(dataloader):\n...     model.train()\n...     total_acc, total_loss = 0, 0\n...     for text_batch, label_batch, lengths in dataloader:\n...         optimizer.zero_grad()\n...         pred = model(text_batch, lengths)[:, 0]\n...         loss = loss_fn(pred, label_batch)\n...         loss.backward()\n...         optimizer.step()\n...         total_acc += (\n...             (pred >= 0.5).float() == label_batch\n...         ).float().sum().item()\n...         total_loss += loss.item()*label_batch.size(0)\n...     return total_acc/len(dataloader.dataset), \\\n...            total_loss/len(dataloader.dataset) \n```", "```py\n>>> def evaluate(dataloader):\n...     model.eval()\n...     total_acc, total_loss = 0, 0\n...     with torch.no_grad():\n...         for text_batch, label_batch, lengths in dataloader:\n...             pred = model(text_batch, lengths)[:, 0]\n...             loss = loss_fn(pred, label_batch)\n...             total_acc += (\n...                 (pred>=0.5).float() == label_batch\n...             ).float().sum().item()\n...             total_loss += loss.item()*label_batch.size(0)\n...     return total_acc/len(dataloader.dataset), \\\n...            total_loss/len(dataloader.dataset) \n```", "```py\n>>> loss_fn = nn.BCELoss()\n>>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n```", "```py\n>>> num_epochs = 10\n>>> torch.manual_seed(1)\n>>> for epoch in range(num_epochs):\n...     acc_train, loss_train = train(train_dl)\n...     acc_valid, loss_valid = evaluate(valid_dl)\n...     print(f'Epoch {epoch} accuracy: {acc_train:.4f}'\n...           f' val_accuracy: {acc_valid:.4f}')\nEpoch 0 accuracy: 0.5843 val_accuracy: 0.6240\nEpoch 1 accuracy: 0.6364 val_accuracy: 0.6870\nEpoch 2 accuracy: 0.8020 val_accuracy: 0.8194\nEpoch 3 accuracy: 0.8730 val_accuracy: 0.8454\nEpoch 4 accuracy: 0.9092 val_accuracy: 0.8598\nEpoch 5 accuracy: 0.9347 val_accuracy: 0.8630\nEpoch 6 accuracy: 0.9507 val_accuracy: 0.8636\nEpoch 7 accuracy: 0.9655 val_accuracy: 0.8654\nEpoch 8 accuracy: 0.9765 val_accuracy: 0.8528\nEpoch 9 accuracy: 0.9839 val_accuracy: 0.8596 \n```", "```py\n>>> acc_test, _ = evaluate(test_dl)\n>>> print(f'test_accuracy: {acc_test:.4f}')\ntest_accuracy: 0.8512 \n```", "```py\n>>> class RNN(nn.Module):\n...     def __init__(self, vocab_size, embed_dim,\n...                  rnn_hidden_size, fc_hidden_size):\n...         super().__init__()\n...         self.embedding = nn.Embedding(\n...             vocab_size, embed_dim, padding_idx=0\n...         )\n...         self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n...                            batch_first=True, bidirectional=True)\n...         self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n...         self.relu = nn.ReLU()\n...         self.fc2 = nn.Linear(fc_hidden_size, 1)\n...         self.sigmoid = nn.Sigmoid()\n...\n...     def forward(self, text, lengths):\n...         out = self.embedding(text)\n...         out = nn.utils.rnn.pack_padded_sequence(\n...             out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True\n...         )\n...         _, (hidden, cell) = self.rnn(out)\n...         out = torch.cat((hidden[-2, :, :],\n...                          hidden[-1, :, :]), dim=1)\n...         out = self.fc1(out)\n...         out = self.relu(out)\n...         out = self.fc2(out)\n...         out = self.sigmoid(out)\n...         return out\n>>> \n>>> torch.manual_seed(1)\n>>> model = RNN(vocab_size, embed_dim,\n...             rnn_hidden_size, fc_hidden_size)\n>>> model\nRNN(\n  (embedding): Embedding(69025, 20, padding_idx=0)\n  (rnn): LSTM(20, 64, batch_first=True, bidirectional=True)\n  (fc1): Linear(in_features=128, out_features=64, bias=True)\n  (relu): ReLU()\n  (fc2): Linear(in_features=64, out_features=1, bias=True)\n  (sigmoid): Sigmoid()\n) \n```", "```py\ncurl -O https://www.gutenberg.org/files/1268/1268-0.txt \n```", "```py\n>>> import numpy as np\n>>> ## Reading and processing text\n>>> with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n...     text=fp.read()\n>>> start_indx = text.find('THE MYSTERIOUS ISLAND')\n>>> end_indx = text.find('End of the Project Gutenberg')\n>>> text = text[start_indx:end_indx]\n>>> char_set = set(text)\n>>> print('Total Length:', len(text))\nTotal Length: 1112350\n>>> print('Unique Characters:', len(char_set))\nUnique Characters: 80 \n```", "```py\n>>> chars_sorted = sorted(char_set)\n>>> char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n>>> char_array = np.array(chars_sorted)\n>>> text_encoded = np.array(\n...     [char2int[ch] for ch in text],\n...     dtype=np.int32\n... )\n>>> print('Text encoded shape:', text_encoded.shape)\nText encoded shape: (1112350,)\n>>> print(text[:15], '== Encoding ==>', text_encoded[:15])\n>>> print(text_encoded[15:21], '== Reverse ==>',\n...       ''.join(char_array[text_encoded[15:21]]))\nTHE MYSTERIOUS == Encoding ==> [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n[33 43 36 25 38 28] == Reverse ==> ISLAND \n```", "```py\n>>> for ex in text_encoded[:5]:\n...     print('{} -> {}'.format(ex, char_array[ex]))\n44 -> T\n32 -> H\n29 -> E\n1 ->  \n37 -> M \n```", "```py\n>>> import torch\n>>> from torch.utils.data import Dataset\n>>> seq_length = 40\n>>> chunk_size = seq_length + 1\n>>> text_chunks = [text_encoded[i:i+chunk_size]\n...                for i in range(len(text_encoded)-chunk_size)]\n>>> from torch.utils.data import Dataset\n>>> class TextDataset(Dataset):\n...     def __init__(self, text_chunks):\n...         self.text_chunks = text_chunks\n...\n...     def __len__(self):\n...         return len(self.text_chunks)\n...\n...     def __getitem__(self, idx):\n...         text_chunk = self.text_chunks[idx]\n...         return text_chunk[:-1].long(), text_chunk[1:].long()\n>>>\n>>> seq_dataset = TextDataset(torch.tensor(text_chunks)) \n```", "```py\n>>> for i, (seq, target) in enumerate(seq_dataset):\n...     print(' Input (x): ',\n...           repr(''.join(char_array[seq])))\n...     print('Target (y): ',\n...           repr(''.join(char_array[target])))\n...     print()\n...     if i == 1:\n...         break\n Input (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\nTarget (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n Input (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\nTarget (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by ' \n```", "```py\n>>> from torch.utils.data import DataLoader\n>>> batch_size = 64\n>>> torch.manual_seed(1)\n>>> seq_dl = DataLoader(seq_dataset, batch_size=batch_size,\n...                     shuffle=True, drop_last=True) \n```", "```py\n>>> import torch.nn as nn\n>>> class RNN(nn.Module):\n...     def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n...         super().__init__()\n...         self.embedding = nn.Embedding(vocab_size, embed_dim)\n...         self.rnn_hidden_size = rnn_hidden_size\n...         self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n...                            batch_first=True)\n...         self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n...\n...     def forward(self, x, hidden, cell):\n...         out = self.embedding(x).unsqueeze(1)\n...         out, (hidden, cell) = self.rnn(out, (hidden, cell))\n...         out = self.fc(out).reshape(out.size(0), -1)\n...         return out, hidden, cell\n...\n...     def init_hidden(self, batch_size):\n...         hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n...         cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n...         return hidden, cell \n```", "```py\n>>> vocab_size = len(char_array)\n>>> embed_dim = 256\n>>> rnn_hidden_size = 512\n>>> torch.manual_seed(1)\n>>> model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n>>> model\nRNN(\n  (embedding): Embedding(80, 256)\n  (rnn): LSTM(256, 512, batch_first=True)\n  (fc): Linear(in_features=512, out_features=80, bias=True)\n  (softmax): LogSoftmax(dim=1)\n) \n```", "```py\n>>> loss_fn = nn.CrossEntropyLoss()\n>>> optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n```", "```py\n>>> num_epochs = 10000\n>>> torch.manual_seed(1)\n>>> for epoch in range(num_epochs):\n...     hidden, cell = model.init_hidden(batch_size)\n...     seq_batch, target_batch = next(iter(seq_dl))\n...     optimizer.zero_grad()\n...     loss = 0\n...     for c in range(seq_length):\n...         pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n...         loss += loss_fn(pred, target_batch[:, c])\n...     loss.backward()\n...     optimizer.step()\n...     loss = loss.item()/seq_length\n...     if epoch % 500 == 0:\n...         print(f'Epoch {epoch} loss: {loss:.4f}')\nEpoch 0 loss: 1.9689\nEpoch 500 loss: 1.4064\nEpoch 1000 loss: 1.3155\nEpoch 1500 loss: 1.2414\nEpoch 2000 loss: 1.1697\nEpoch 2500 loss: 1.1840\nEpoch 3000 loss: 1.1469\nEpoch 3500 loss: 1.1633\nEpoch 4000 loss: 1.1788\nEpoch 4500 loss: 1.0828\nEpoch 5000 loss: 1.1164\nEpoch 5500 loss: 1.0821\nEpoch 6000 loss: 1.0764\nEpoch 6500 loss: 1.0561\nEpoch 7000 loss: 1.0631\nEpoch 7500 loss: 0.9904\nEpoch 8000 loss: 1.0053\nEpoch 8500 loss: 1.0290\nEpoch 9000 loss: 1.0133\nEpoch 9500 loss: 1.0047 \n```", "```py\n>>> from torch.distributions.categorical import Categorical\n>>> torch.manual_seed(1)\n>>> logits = torch.tensor([[1.0, 1.0, 1.0]])\n>>> print('Probabilities:',\n...       nn.functional.softmax(logits, dim=1).numpy()[0])\nProbabilities: [0.33333334 0.33333334 0.33333334]\n>>> m = Categorical(logits=logits)\n>>> samples = m.sample((10,))\n>>> print(samples.numpy())\n[[0]\n [0]\n [0]\n [0]\n [1]\n [0]\n [1]\n [2]\n [1]\n [1]] \n```", "```py\n>>> torch.manual_seed(1)\n>>> logits = torch.tensor([[1.0, 1.0, 3.0]])\n>>> print('Probabilities:', nn.functional.softmax(logits, dim=1).numpy()[0])\nProbabilities: [0.10650698 0.10650698 0.78698605]\n>>> m = Categorical(logits=logits)\n>>> samples = m.sample((10,))\n>>> print(samples.numpy())\n[[0]\n [2]\n [2]\n [1]\n [2]\n [1]\n [2]\n [2]\n [2]\n [2]] \n```", "```py\n>>> def sample(model, starting_str,\n...            len_generated_text=500,\n...            scale_factor=1.0):\n...     encoded_input = torch.tensor(\n...         [char2int[s] for s in starting_str]\n...     )\n...     encoded_input = torch.reshape(\n...         encoded_input, (1, -1)\n...     )\n...     generated_str = starting_str\n...\n...     model.eval()\n...     hidden, cell = model.init_hidden(1)\n...     for c in range(len(starting_str)-1):\n...         _, hidden, cell = model(\n...             encoded_input[:, c].view(1), hidden, cell\n...         )\n...    \n...     last_char = encoded_input[:, -1]\n...     for i in range(len_generated_text):\n...         logits, hidden, cell = model(\n...             last_char.view(1), hidden, cell\n...         )\n...         logits = torch.squeeze(logits, 0)\n...         scaled_logits = logits * scale_factor\n...         m = Categorical(logits=scaled_logits)\n...         last_char = m.sample()\n...         generated_str += str(char_array[last_char])\n...\n...     return generated_str \n```", "```py\n>>> torch.manual_seed(1)\n>>> print(sample(model, starting_str='The island'))\nThe island had been made\nand ovylore with think, captain?\" asked Neb; \"we do.\"\nIt was found, they full to time to remove. About this neur prowers, perhaps ended? It is might be\nrather rose?\"\n\"Forward!\" exclaimed Pencroft, \"they were it? It seems to me?\"\n\"The dog Top--\"\n\"What can have been struggling sventy.\"\nPencroft calling, themselves in time to try them what proves that the sailor and Neb bounded this tenarvan's feelings, and then\nstill hid head a grand furiously watched to the dorner nor his only \n```", "```py\n>>> logits = torch.tensor([[1.0, 1.0, 3.0]])\n>>> print('Probabilities before scaling:        ',\n...       nn.functional.softmax(logits, dim=1).numpy()[0])\n>>> print('Probabilities after scaling with 0.5:',\n...       nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n>>> print('Probabilities after scaling with 0.1:',\n...       nn.functional.softmax(0.1*logits, dim=1).numpy()[0])\nProbabilities before scaling:         [0.10650698 0.10650698 0.78698604]\nProbabilities after scaling with 0.5: [0.21194156 0.21194156 0.57611688]\nProbabilities after scaling with 0.1: [0.31042377 0.31042377 0.37915245] \n```", "```py\n    >>> torch.manual_seed(1)\n    >>> print(sample(model, starting_str='The island',\n    ...              scale_factor=2.0))\n    The island is one of the colony?\" asked the sailor, \"there is not to be able to come to the shores of the Pacific.\"\n    \"Yes,\" replied the engineer, \"and if it is not the position of the forest, and the marshy way have been said, the dog was not first on the shore, and\n    found themselves to the corral.\n    The settlers had the sailor was still from the surface of the sea, they were not received for the sea. The shore was to be able to inspect the windows of Granite House.\n    The sailor turned the sailor was the hor \n    ```", "```py\n    >>> torch.manual_seed(1)\n    >>> print(sample(model, starting_str='The island',\n    ...              scale_factor=0.5))\n    The island\n    deep incomele.\n    Manyl's', House, won's calcon-sglenderlessly,\" everful ineriorouins., pyra\" into\n    truth. Sometinivabes, iskumar gave-zen.\"\n    Bleshed but what cotch quadrap which little cedass\n    fell oprely\n    by-andonem. Peditivall--\"i dove Gurgeon. What resolt-eartnated to him\n    ran trail.\n    Withinhe)tiny turns returned, after owner plan bushelsion lairs; they were\n    know? Whalerin branch I\n    pites, Dougg!-iteun,\" returnwe aid masses atong thoughts! Dak,\n    Hem-arches yone, Veay wantzer? Woblding,\n    Herbert, omep \n    ```"]