- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Painting Pictures with Neural Networks Using VAEs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 VAEs 用神经网络绘制图片
- en: As you saw in *Chapter 4*, *Teaching Networks to Generate Digits*, deep neural
    networks are a powerful tool for creating generative models for complex data such
    as images, allowing us to develop a network that can generate images from the
    MNIST hand-drawn digit database. In that example, the data is relatively simple;
    images can only come from a limited set of categories (the digits 0 through 9)
    and are low-resolution grayscale data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在 *第 4 章* 中所看到的，*教网络生成数字*，深度神经网络是创建复杂数据的生成模型的强大工具，允许我们开发一个网络，该网络可以从 MNIST
    手写数字数据库生成图像。在那个例子中，数据相对简单；图像只能来自一组有限的类别（数字 0 到 9），并且是低分辨率灰度数据。
- en: What about more complex data, such as color images drawn from the real world?
    One example of such "real world" data is the Canadian Institute for Advanced Research
    10 class dataset, denoted as CIFAR-10.¹ It is a subset of 60,000 examples from
    a larger set of 80 million images, divided into ten classes – airplanes, cars,
    birds, cats, deer, dogs, frogs, horses, ships, and trucks. While still an extremely
    limited set in terms of the diversity of images we would encounter in the real
    world, these classes have some characteristics that make them more complex than
    MNIST. For example, the MNIST digits can vary in width, curvature, and a few other
    properties; the CIFAR-10 classes have a much wider potential range of variation
    for animal or vehicle photos, meaning we may require more complex models in order
    to capture this variation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的数据呢，比如来自现实世界的彩色图像呢？这类“现实世界”的数据的一个例子是加拿大高级研究所 10 类数据集，简称为 CIFAR-10。¹ 它是来自
    8000 万张图像更大数据集的 60,000 个样本的子集，分为十个类别——飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。虽然在真实世界中我们可能会遇到的图像多样性方面仍然是一个极为有限的集合，但是这些类别具有一些特性，使它们比
    MNIST 更复杂。例如，MNIST 数字可以在宽度、曲率和其他几个属性上变化；而 CIFAR-10 类别的动物或车辆照片有着更广泛的潜在变化范围，这意味着我们可能需要更复杂的模型来捕捉这种变化。
- en: In this chapter, we will discuss a class of generative models known as **Variational
    Autoencoders** (**VAEs**), which are designed to make the generation of these
    complex, real-world images more tractable and tunable. They do this by using a
    number of clever simplifications to make it possible to sample over the complex
    probability distribution represented by real-world images in a way that is scalable.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一类称为**变分自动编码器**（**VAEs**）的生成模型，这些模型旨在使生成这些复杂的现实世界图像更易于处理和调节。它们通过使用许多巧妙的简化方法来使得在复杂的概率分布上进行采样成为可能，从而可扩展。
- en: 'We will explore the following topics to reveal how VAEs work:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨以下主题以揭示 VAEs 的工作原理：
- en: How neural networks create low-dimensional representations of data, and some
    desirable properties of those representations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络如何创建数据的低维表示，以及这些表示的一些理想属性
- en: How variational methods allow us to sample from complex data using these representations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变分方法如何允许我们使用这些表示从复杂数据中进行采样
- en: How using the reparameterization trick allows us to stabilize the variance of
    a neural network based on variational sampling—a VAE
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用重新参数化技巧来稳定基于变分采样的神经网络的方差 —— 一个 VAE
- en: How we can use **Inverse Autoregressive Flow** (**IAF**) to tune the output
    of a VAE
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何使用**逆自回归流**（**IAF**）来调整 VAE 的输出
- en: How to implement VAE/IAF in TensorFlow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 TensorFlow 中实现 VAE/IAF
- en: Creating separable encodings of images
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建图像的可分离编码
- en: 'In *Figure 5.1*, you can see an example of images from the CIFAR-10 dataset,
    along with an example of an early VAE algorithm that can generate fuzzy versions
    of these images based on a random number input:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.1* 中，您可以看到 CIFAR-10 数据集的图像示例，以及一个可以根据随机数输入生成这些图像模糊版本的早期 VAE 算法示例：
- en: '![](img/B16176_05_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_01.png)'
- en: 'Figure 5.1: CIFAR-10 sample (left), VAE (right)²'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：CIFAR-10 样本（左），VAE（右）²
- en: More recent work on VAE networks has allowed these models to generate much better
    images, as you will see later in this chapter. To start, let's revisit the problem
    of generating MNIST digits and how we can extend this approach to more complex data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对 VAE 网络的最新工作已经使得这些模型能够生成更好的图像，正如您将在本章后面看到的那样。首先，让我们重新审视生成 MNIST 数字的问题以及我们如何将这种方法扩展到更复杂的数据。
- en: 'Recall from *Chapter 1*, *An Introduction to Generative AI: "Drawing" Data
    from Models* and *Chapter 4*, *Teaching Networks to Generate Digits* that the
    RBM (or DBN) model in essence involves learning the posterior probability distribution
    for images (*x*) given some latent "code" (*z*), represented by the hidden layer(s)
    of the network, the "marginal likelihood" of *x*:³'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从*第1章*，*生成式人工智能简介：“从模型中”绘制数据*和*第4章*，*教网络生成数字*中回想起，RBM（或DBN）模型本质上涉及学习给定一些潜在“代码”（*z*）的图像(*x*)的后验概率分布，由网络的隐藏层表示，*x*的“边际可能性”：³
- en: '![](img/B16176_05_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_001.png)'
- en: We can see *z* as being an "encoding" of the image *x* (for example, the activations
    of the binary hidden units in the RBM), which can be decoded (for example, run
    the RBM in reverse in order to sample an image) to get a reconstruction of *x*.
    If the encoding is "good," the reconstruction will be close to the original image.
    Because these networks encode and decode representations of their input data,
    they are also known as "autoencoders."
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将*z*视为图像*x*的“编码”（例如，RBM中二进制隐藏单元的激活），可以解码（例如，反向运行RBM以对图像进行采样）以获得*x*的重构。如果编码“好”，重构将接近原始图像。因为这些网络对其输入数据的表示进行编码和解码，它们也被称为“自编码器”。
- en: The ability of deep neural networks to capture the underlying structure of complex
    data is one of their most attractive features; as we saw with the DBN model in
    *Chapter 4*, *Teaching Networks to Generate Digits*, it allows us to improve the
    performance of a classifier by creating a better underlying model for the distribution
    of the data. It can also be used to simply create a better way to "compress" the
    complexity of data, in a similar way to **principal component analysis** (**PCA**)
    in classical statistics. In *Figure 5.2*, you can see how the stacked RBM model
    can be used as a way to encode the distribution of faces, for example.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络捕捉复杂数据的基本结构的能力是它们最吸引人的特征之一；正如我们在*第4章*，*教网络生成数字*中所看到的DBN模型一样，它使我们能够通过为数据的分布创建更好的基础模型来提高分类器的性能。它还可以用于简单地创建一种更好的方法来“压缩”数据的复杂性，类似于经典统计学中的**主成分分析**（**PCA**）。在*图5.2*中，您可以看到堆叠的RBM模型如何用作编码面部分布的一种方式，例如。
- en: 'We start with a "pre-training" phase to create a 30-unit encoding vector, which
    we then calibrate by forcing it to reconstruct the input image, before fine-tuning
    with standard backpropagation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从“预训练”阶段开始，创建一个30单位的编码向量，然后通过强制它重构输入图像来校准它，然后使用标准反向传播进行微调：
- en: '![](img/B16176_05_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_02.png)'
- en: 'Figure 5.2: Using a DBN as an autoencoder⁴'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：使用 DBN 作为自编码器⁴
- en: 'As an example of how the stacked RBM model can more effectively represent the
    distribution of images, the authors of the paper *Reducing the Dimensionality
    of Data with Neural Networks*, from which *Figure 5.2* is derived, demonstrated
    using a two-unit code for the MNIST digits compared to PCA:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为堆叠的RBM模型如何更有效地表示图像分布的示例，从*图5.2*派生的论文*用神经网络减少数据的维数*的作者演示了使用两个单位代码来对比MNIST数字的PCA：
- en: '![](img/B16176_05_03.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_03.png)'
- en: 'Figure 5.3: PCA versus RBM autoencoder for MNIST digits⁵'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：MNIST数字的PCA与RBM自编码器对比⁵
- en: 'On the left, we see the digits 0-9 (represented by different shades and shapes)
    encoded using 2-dimensional PCA. Recall that PCA is generated using a low-dimensional
    factorization of the covariance matrix of the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在左边，我们看到使用二维PCA编码的数字0-9（由不同的阴影和形状表示）。回想一下，PCA是使用数据的协方差矩阵的低维分解生成的：
- en: '![](img/B16176_05_002.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_002.png)'
- en: Where *Cov(X)* is the same height/width *M* as the data (for example, 28 by
    28 pixels in MNIST) and *U* and *V* are both lower dimensional (*M* x *k* and
    *k* x *M*), where *k* is much smaller than *M*. Because they have a smaller number
    of rows/columns *k* than the original data in one dimension, *U* and *V* are lower-dimensional
    representations of the data, and we can get an encoding of an individual image
    by projecting it onto these *k* vectors, giving a *k* unit encoding of the data.
    Since the decomposition (and projection) is a linear transformation (multiplying
    two matrices), the ability of the PCA components to distinguish data well depends
    on the data being linearly separable (we can draw a hyperplane through the space
    between groups—that space could be two-dimensional or *N* dimensional, like the
    784 pixels in the MNIST images).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cov(X)* 的高度/宽度与数据相同（例如，在MNIST中为28 x 28像素），而 *U* 和 *V* 都是较低维度的（*M* x *k* 和
    *k* x *M*），其中 *k* 远小于 *M*。由于它们在一个维度上具有较少的行/列数 *k*，*U* 和 *V* 是数据的低维表示，我们可以通过将其投影到这些
    *k* 向量上来获得对单个图像的编码，从而给出了数据的 *k* 单位编码。由于分解（和投影）是线性变换（两个矩阵的乘积），PCA组件有效区分数据的能力取决于数据是否线性可分（我们可以通过组之间的空间绘制一个超平面—该空间可以是二维的或
    *N* 维的，例如MNIST图像中的784个像素）。'
- en: As you can see in *Figure 5.3*, PCA yields overlapping codes for the images,
    showing that it is challenging to represent digits using a two-component linear
    decomposition, in which vectors representing the same digit are close together,
    while those representing different digits are clearly separated. Conceptually,
    the neural network is able to capture more of the variation between images representing
    different digits than PCA, as shown by its ability to separate the representations
    of these digits more clearly in a two-dimensional space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图5.3* 所示，PCA为图像生成了重叠的代码，表明使用二分量线性分解来表示数字是具有挑战性的，其中表示相同数字的向量彼此靠近，而表示不同数字的向量明显分开。从概念上讲，神经网络能够捕捉更多表示不同数字的图像之间的变化，如其在二维空间中更清晰地分离这些数字的表示所示。
- en: 'As an analogy to understand this phenomenon, consider a very simple two-dimensional
    dataset consisting of parallel hyperbolas (2^(nd) power polynomials) (*Figure
    5.4*):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一现象，可以将其类比为一个非常简单的二维数据集，由平行双曲线（二次多项式）组成（*图5.4*）：
- en: '![](img/B16176_05_04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_04.png)'
- en: 'Figure 5.4: Parallel hyperbolas and separability'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：平行双曲线和可分离性
- en: At the top, even though we have two distinct classes, we cannot draw a straight
    line through two-dimensional space to separate the two groups; in a neural network,
    the weight matrix in a single layer before the nonlinear transformation of a sigmoid
    or tanh is, in essence, a linear boundary of this kind. However, if we apply a
    nonlinear transformation to our 2D coordinates, such as taking the square root
    of the hyperbolas, we can create two separable planes (*Figure 5.4*, *bottom*).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，即使我们有两个不同的类别，我们也无法在二维空间中画一条直线将两个组分开；在神经网络中，单个层中的权重矩阵在sigmoid或tanh的非线性转换之前本质上是这种类型的线性边界。然而，如果我们对我们的2D坐标应用非线性变换，比如取超半径的平方根，我们可以创建两个可分离的平面（*图5.4*，*底部*）。
- en: 'A similar phenomenon is at play with our MNIST data: we need a neural network
    in order to place these 784-digit images into distinct, separable regions of space.
    This goal is achieved by performing a non-linear transformation on the original,
    overlapping data, with an objective function that rewards increasing the spatial
    separation among vectors encoding the images of different digits. A separable
    representation thus increases the ability of the neural network to differentiate
    image classes using these representations. Thus, in *Figure 5.3*, we can see on
    the right that applying the DBN model creates the required non-linear transformation
    to separate the different images.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在MNIST数据中存在类似的现象：我们需要一个神经网络来将这些784位数的图像放置到不同的、可分离的空间区域中。这个目标通过对原始的、重叠的数据执行非线性变换来实现，其中的目标函数奖励增加编码不同数字图像的向量之间的空间分离。因此，可分离的表示增加了神经网络利用这些表示区分图像类别的能力。因此，在
    *图5.3* 中，我们可以看到右侧应用DBN模型创建所需的非线性变换以分离不同的图像。
- en: Now that we've covered how neural networks can compress data into numerical
    vectors and what some desirable properties of those vector representations are,
    we'll examine how to optimally compress information in these vectors. To do so,
    each element of the vector should encode distinct information from the others,
    a property we can achieve using a variational objective. This variational objective
    is the building block for creating VAE networks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了神经网络如何将数据压缩为数值向量以及这些向量表示的一些理想特性，我们将探讨如何在这些向量中最佳压缩信息。为此，向量的每个元素应该从其他元素中编码出不同的信息，我们可以使用变分目标这一属性来实现。这个变分目标是创建VAE网络的基础。
- en: The variational objective
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分目标
- en: 'We previously covered several examples of how images can be compressed into
    numerical vectors using neural networks. This section will introduce the elements
    that allow us to create effective encodings to sample new images from a space
    of random numerical vectors, which are principally efficient inference algorithms
    and appropriate objective functions. Let''s start by quantifying more rigorously
    what makes such an encoding "good" and allows us to recreate images well. We will
    need to maximize the posterior:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了几个例子，展示了如何使用神经网络将图像压缩成数值向量。这一部分将介绍能够让我们从随机数值向量空间中采样新图像的要素，主要是有效的推理算法和适当的目标函数。让我们更加严谨地量化什么样的编码才能让其“好”，并且能够很好地重现图像。我们需要最大化后验概率：
- en: '![](img/B16176_05_003.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_003.png)'
- en: A problem occurs when the probability of *x* is extremely high dimensional,
    which, as you saw, can occur in even simple data such as binary MNIST digits,
    where we have *2^* (number of pixels) possible configurations that we would need
    to integrate over (in a mathematical sense of integrating over a probability distribution)
    to get a measure of the probability of an individual image; in other words, the
    density *p*(*x*) is intractable, making the posterior *p*(*z*|*x*), which depends
    on *p*(*x*), likewise intractable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当*x*的概率非常高维时会出现问题，如你所见，在甚至是简单的数据中，如二元MNIST数字中，我们有*2^*（像素数）可能的配置，我们需要对其进行积分（在概率分布意义上进行积分）以得到对单个图像概率的度量；换句话说，密度*p*(*x*)是棘手的，导致了依赖于*p*(*x*)的后验*p*(*z*|*x*)也同样不容易处理。
- en: In some cases, such as you saw in *Chapter 4*, *Teaching Networks to Generate
    Digits*, we can use simple cases such as binary units to compute an approximation
    such as contrastive divergence, which allows us to still compute a gradient even
    if we can't calculate a closed form. However, this might also be challenging for
    very large datasets, where we would need to make many passes over the data to
    compute an average gradient using **Contrastive Divergence** (**CD**), as you
    saw previously in *Chapter 4*, *Teaching Networks to Generate Digits*.⁶
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，正如你在*第4章*中看到的，*训练网络生成数字*，我们可以使用简单的二元单元，例如对比散度来计算近似，这使我们可以计算梯度，即使我们无法计算封闭形式。然而，在处理非常大的数据集时也可能具有挑战性，我们需要对数据进行多次传递以计算使用**对比散度**（**CD**）计算平均梯度，就像之前在*第4章*中看到的那样。
- en: 'If we can''t calculate the distribution of our encoder *p*(*z*|*x*) directly,
    maybe we could optimize an approximation that is "close enough"—let''s called
    this *q*(*z*|*x*). Then, we could use a measure to determine if the distributions
    are close enough. One useful measure of closeness is whether the two distributions
    encode similar information; we can quantify information using the Shannon Information
    equation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们无法直接计算编码器*p*(*z*|*x*)的分布，也许我们可以优化一个足够“接近”的近似——让我们称之为*q*(*z*|*x*)。然后，我们可以使用一种度量来确定这两个分布是否足够接近。一个有用的接近度量是这两个分布是否编码了类似的信息；我们可以使用香农信息方程来量化信息：
- en: '![](img/B16176_05_004.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_004.png)'
- en: 'Consider why this is a good measure: as *p*(*x*) decreases, an event becomes
    rarer, and thus observation of the event communicates more information about the
    system or dataset, leading to a positive value of *-log*(*p*(*x*)). Conversely,
    as the probability of an event nears 1, that event encodes less information about
    the dataset, and the value of *-log*(*p*(*x*)) becomes 0 (*Figure 5.5*):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下这为什么是一个很好的度量：随着*p*(*x*)的减少，事件变得更加罕见，因此事件的观察向系统或数据集传达更多信息，导致*log*(*p*(*x*))的正值。相反，当事件的概率接近1时，该事件对数据集的编码信息变少，而*log*(*p*(*x*))的值变为0（*图5.5*）：
- en: '![](img/B16176_05_05.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_05.png)'
- en: 'Figure 5.5: Shannon information'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：香农信息
- en: 'Thus, if we wanted to measure the difference between the information encoded
    in two distributions, *p* and *q*, we could use the difference in their information:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想要衡量两个分布*p*和*q*中编码的信息之间的差异，我们可以使用它们的信息之差：
- en: '![](img/B16176_05_005.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_005.png)'
- en: 'Finally, if we want to find the expected difference in information between
    the distributions for all elements of *x*, we can take the average over *p*(*x*):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们想要找到分布在*x*的所有元素上的信息差异的期望值，我们可以对*p*(*x*)取平均：
- en: '![](img/B16176_05_006.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_006.png)'
- en: 'This quantity is known as the **Kullback Leibler** (**KL**) **Divergence**.
    It has a few interesting properties:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个量被称为**Kullback Leibler** (**KL**) **散度**。它有一些有趣的特性：
- en: 'It is not symmetric: *KL*(*p*(*x*), *q*(*x*)) does not, in general, equal *KL*(*q*(*x*),
    *p*(*x*)), so the "closeness" is measured by mapping one distribution to another
    in a particular direction.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它不是对称的：*KL*(*p*(*x*), *q*(*x*))一般来说不等于*KL*(*q*(*x*), *p*(*x*))，所以“接近程度”是通过将一个分布映射到另一个分布的特定方向来衡量的。
- en: Whenever *q*(*x*) and *p*(*x*) match, the term is 0, meaning they are a minimum
    distance from one another. Likewise, *KL*(*p*(*x*), *q*(*x*)) is 0 only if *p*
    and *q* are identical.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当*q*(*x*)和*p*(*x*)匹配时，这个项就是0，意味着它们彼此之间的距离是最小的。同样，只有当*p*和*q*是相同的时候，*KL*(*p*(*x*),
    *q*(*x*))才为0。
- en: If *q*(*x*) is 0 or *p*(*x*) is 0, then *KL* is undefined; by definition, it
    only computes relative information over the range of *x* where the two distributions
    match.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*q*(*x*)为0或者*p*(*x*)为0，那么*KL*是未定义的；按照定义，它仅计算两个分布在*x*的范围内匹配的相对信息。
- en: '*KL* is always greater than 0.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*KL*始终大于0。'
- en: 'If we were to use the *KL* divergence to compute how well an approximation
    *q*(*z,x*) is of our intractable *p*(*z*|*x*), we could write:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要使用*KL*散度来计算近似值*q*(*z,x*)对于我们不可计算的*p*(*z*|*x*)的拟合程度，我们可以写成：
- en: '![](img/B16176_05_007.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_007.png)'
- en: 'and:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 和：
- en: '![](img/B16176_05_008.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_008.png)'
- en: '![](img/B16176_05_009.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_009.png)'
- en: 'Now we can write an expression for our intractable *p*(*x*) as well: since
    *log*(*p*(*x*)) does not depend on *q*(*z*|*x*), the expectation with respect
    to *p*(*x*) is simply *log*(*p*(*x*)). Thus, we can represent the objective of
    the VAE, learning the marginal distribution of *p*(*x*), using the *KL* divergence:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也可以写出我们不可计算的*p*(*x*)的表达式了：由于*log*(*p*(*x*))不依赖于*q*(*z*|*x*)，对*p*(*x*)的期望值简单地是*log*(*p*(*x*))。因此，我们可以用*KL*散度表示VAE的目标，学习*p*(*x*)的边际分布：
- en: '![](img/B16176_05_010.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_010.png)'
- en: The second term is also known as the **Variational Lower Bound**, which is also
    referred to as the **Evidence Lower Bound** (**ELBO**); since *KL*(*q,p*) is strictly
    greater than 0, *log*(*p(x*)) is strictly greater than or (if *KL*(*q,p*) is 0)
    equal to this value.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项也被称为**变分下限**，也被称为**证据下界**（**ELBO**）；由于*KL*(*q,p*)严格大于0，*log*(*p(x*))严格大于或者（如果*KL*(*q,p*)为0）等于这个值。
- en: 'To explain what this objective is doing, notice that the expectation introduces
    a difference between *q*(*z*|*x*) (*encoding x*) and *p*(*x*|*z*)*p*(*z*) (the
    joint probability of the data and the encoding); thus we want to minimize a lower
    bound that is essentially the gap between the probability of the encoding and
    the joint probability of the encoding and data, with an error term given by *KL*(*q,p*),
    the difference between a tractable approximation and intractable form of the encoder
    *p*(*z*|*x*). We can imagine the functions *Q*(*z*|*x*) and *P*(*x*|*z*) being
    represented by two deep neural networks; one generates the latent code *z*(*Q*),
    and the other reconstructs *x* from this code (*P*). We can imagine this as an
    autoencoder setup, as above with the stacked RBM models, with an encoder and decoder:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释这个目标在做什么，注意到期望引入了*q*(*z*|*x*)（*编码x*）和*p*(*x*|*z*)*p*(*z*)（数据和编码的联合概率）之间的差异；因此，我们想要最小化一个下界，它实质上是编码的概率和编码与数据的联合概率之间的差距，误差项由*KL*(*q,p*)给出，这是一个可计算的近似和不可计算的编码器*p*(*z*|*x*)形式之间的差异。我们可以想象函数*Q*(*z*|*x*)和*P*(*x*|*z*)由两个深度神经网络表示；一个生成潜在代码*z*（*Q*），另一个从这个代码重建*x*（*P*）。我们可以把这看作是一个自动编码器设置，就像上面的堆叠RBM模型一样，有一个编码器和解码器：
- en: '![](img/B16176_05_06.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_06.png)'
- en: 'Figure 5.6: Autoencoder/Decoder of an un-reparameterized VAE⁷'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：无再参数化VAE的自动编码器/解码器⁷
- en: 'We want to optimize the parameters of the encoder *Q* and the decoder *P* to
    minimize the reconstruction cost. One way to do this is to construct Monte Carlo
    samples to optimize the parameters ![](img/B16176_05_011.png) of *Q* using gradient
    descent:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望优化编码器 *Q* 和解码器 *P* 的参数，以最小化重构成本。其中一种方法是构造蒙特卡洛样本来使用梯度下降优化 *Q* 的参数：
- en: '![](img/B16176_05_012.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_012.png)'
- en: 'Where we sample *z*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从哪里抽样 *z*：
- en: '![](img/B16176_05_013.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_013.png)'
- en: However, it has been found in practice that a large number of samples may be
    required in order for the variance of these gradient updates to stabilize.⁸
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中发现，可能需要大量的样本才能使这些梯度更新的方差稳定下来。
- en: 'We also have a practical problem here: even if we could choose enough samples
    to get a good approximation of the gradients for the encoder, our network contains
    a stochastic, non-differentiable step (sampling *z*) that we can''t backpropagate
    through, in a similar way we couldn''t backpropagate through the stochastic units
    in the RBM in *Chapter 4*, *Teaching Networks to Generate Digits*. Thus, our reconstruction
    error depends on samples from *z*, but we can''t backpropagate through the step
    that generates these samples to tune the network end to end. Is there a way we
    can create a differentiable decoder/encoder architecture while also reducing the
    variance of sample estimates? One of the main insights of the VAE is to enable
    this through the "reparameterization trick."'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里也遇到了一个实际问题：即使我们可以选择足够的样本来获得对编码器的梯度的良好近似，但我们的网络包含一个随机的、不可微分的步骤（抽样 *z*），我们无法通过反向传播来处理，就像我们无法通过反向传播来处理
    *第 4 章* 中 RBN 中的随机单元一样。因此，我们的重构误差取决于 *z* 的样本，但我们无法通过生成这些样本的步骤进行端到端的网络调整。有没有办法我们可以创建一个可微分的解码器/编码器架构，同时减少样本估计的方差？VAE
    的主要见解之一就是通过 "重新参数化技巧" 实现这一点。
- en: The reparameterization trick
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新参数化技巧
- en: 'In order to allow us to backpropagate through our autoencoder, we need to transform
    the stochastic samples of *z* into a deterministic, differentiable transformation.
    We can do this by reparameterizing *z* as a function of a noise variable ![](img/B16176_05_014.png):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们能够通过我们的自编码器进行反向传播，我们需要将 *z* 的随机样本转换为一个确定性的、可微分的变换。我们可以通过将 *z* 重新参数化为一个噪声变量的函数来实现这一点：
- en: '![](img/B16176_05_015.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_015.png)'
- en: Once we have sampled from ![](img/B16176_05_016.png), the randomness in *z*
    no longer depends on the parameters of the variational distribution *Q* (the encoder),
    and we can backpropagate end to end. Our network now looks like *Figure 5.7*,
    and we can optimize our objective using random samples of ![](img/B16176_05_017.png)
    (for example, a standard normal distribution). This reparameterization moves the
    "random" node out of the encoder/decoder framework so we can backpropagate through
    the whole system, but it also has a subtler advantage; it reduces the variance
    of these gradients. Note that in the un-reparameterized network, the distribution
    of *z* depends on the parameters of the encoder distribution *Q*; thus, as we
    are changing the parameters of *Q*, we are also changing the distribution of *z*,
    and we would need to potentially use a large number of samples to get a decent
    estimate.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从 ![](img/B16176_05_016.png) 中抽样，*z* 中的随机性就不再取决于变分分布 *Q*（编码器）的参数，我们可以进行端到端的反向传播。我们的网络现在看起来像
    *图 5.7*，我们可以使用 ![](img/B16176_05_017.png) 的随机样本（例如，标准正态分布）来优化我们的目标。这种重新参数化将 "随机"
    节点移出了编码器/解码器框架，使我们能够通过整个系统进行反向传播，但它还有一个更微妙的优点；它减少了这些梯度的方差。请注意，在未重新参数化的网络中，*z*
    的分布取决于编码器分布 *Q* 的参数；因此，当我们改变 *Q* 的参数时，我们也在改变 *z* 的分布，并且我们可能需要使用大量样本才能得到一个合理的估计。
- en: 'By reparameterizing, *z* now depends only on our simpler function, *g*, with
    randomness introduced through sampling ![](img/B16176_05_018.png) from a standard
    normal (that doesn''t depend on *Q*); hence, we''ve removed a somewhat circular
    dependency, and made the gradients we are estimating more stable:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新参数化，*z* 现在仅取决于我们更简单的函数 *g*，通过从标准正态分布中进行抽样引入随机性（这不依赖于 *Q*）；因此，我们已经消除了一个有些循环的依赖，并使我们正在估计的梯度更加稳定：
- en: '![](img/B16176_05_07.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_07.png)'
- en: 'Figure 5.7: Autoencoder/decoder of a reparameterized VAE⁹'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：重新参数化 VAE 的自编码器/解码器
- en: 'Now that you have seen how the VAE network is constructed, let''s discuss a
    further refinement of this algorithm that allows VAEs to sample from complex distributions:
    **Inverse Autoregressive Flow** (**IAF**).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到VAE网络是如何构建的，让我们讨论一种进一步改进这一算法的方法，使得VAE能够从复杂分布中取样：**逆自回归流**（**IAF**）。
- en: Inverse Autoregressive Flow
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆自回归流
- en: In our discussion earlier, it was noted that we want to use *q*(*z*|*x*) as
    a way to approximate the "true" *p*(*z*|*x*) that would allow us to generate an
    ideal encoding of the data, and thus sample from it to generate new images. So
    far, we've assumed that *q*(*z*|*x*) has a relatively simple distribution, such
    as a vector of Gaussian distribution random variables that are independent (a
    diagonal covariance matrix with 0s on the non-diagonal elements). This sort of
    distribution has many benefits; because it is simple, we have an easy way to generate
    new samples by drawing from random normal distributions, and because it is independent,
    we can separately tune each element of the latent vector *z* to influence parts
    of the output image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的讨论中，我们指出希望使用*q*(*z*|*x*)来近似“真实”的*p*(*z*|*x*)，这会允许我们生成数据的理想编码，并从中取样生成新的图像。到目前为止，我们假设*q*(*z*|*x*)有一个相对简单的分布，比如独立的高斯分布随机变量的向量（对角协方差矩阵上的非对角元素为0）。这种分布有许多好处；因为它很简单，我们可以轻松地从随机正态分布中进行抽样生成新数据，并且因为它是独立的，我们可以分别调整潜在向量*z*的各个元素，以影响输出图像的各个部分。
- en: However, such a simple distribution may not fit the desired output distribution
    of data well, increasing the *KL* divergence between *p*(*z*|*x*) and *q*(*z*|*x*).
    Is there a way we can keep the desirable properties of *q*(*z*|*x*) but "transform"
    *z* so that it captures more of the complexities needed to represent *x*?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这样一个简单的分布可能不能很好地适应数据的期望输出分布，增加了*p*(*z*|*x*)和*q*(*z*|*x*)之间的*KL*散度。我们能不能以某种方式保留*q*(*z*|*x*)的良好特性，但“变换”*z*，以便它更多地捕捉表示*x*所需的复杂性呢？
- en: One approach is to apply a series of autoregressive transformations to *z* to
    turn it from a simple to a complex distribution; by "autoregressive," we mean
    that each transformation utilizes both data from the previous transformation and
    the current data to compute an updated version of *z*. In contrast, the basic
    form of VAE that we introduced above has only a single "transformation:" from
    *z* to the output (though *z* might pass through multiple layers, there is no
    recursive network link to further refine that output). We've seen such transformations
    before, such as the LSTM networks in *Chapter 3*, *Building Blocks of Deep Neural
    Networks*, where the output of the network is a combination of the current input
    and a weighted version of prior time steps.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是对*z*应用一系列自回归变换，将其从一个简单分布转变为一个复杂分布；通过“自回归”，我们指的是每个变换利用了前一次变换和当前数据来计算*z*的更新版本。相比之下，我们上面介绍的基本VAE形式只有一个“变换”：从*z*到输出（虽然*z*可能经过多层，但没有递归网络链接来进一步完善输出）。我们之前已经见过这样的变换，比如*第3章*中的LSTM网络，其中网络的输出是当前输入和先前时间步加权版本的组合。
- en: 'An attractive property of the independent *q*(*z*|*x*) distributions we discussed
    earlier, such as independent normals, is that they have a very tractable expression
    for the log likelihood. This property is important for the VAE model because its
    objective function depends on integrating over the whole likelihood function,
    which would be cumbersome for more complex log likelihood functions. However,
    by constraining a transformed *z* to computation through a series of autoregressive
    transformations, we have the nice property that the log-likelihood of step *t*
    only depends on *t-1*, thus the Jacobian (gradient matrix of the partial derivative
    between *t* and *t-1*) is lower triangular and can be computed as a sum:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过的独立*q*(*z*|*x*)分布的吸引人之处是，例如独立的正态分布，它们在对数似然函数上有一个非常简单的表达式。这一特性对于VAE模型非常重要，因为其目标函数取决于对整个似然函数进行积分，而对于更复杂的对数似然函数来说，这可能是繁琐的。然而，通过约束一个经过一系列自回归变换的*z*，我们得到了一个很好的特性，即第*t*步的对数似然仅取决于*t-1*，因此雅可比矩阵（*t*和*t-1*之间的偏导数的梯度矩阵）是下三角的，可以计算为一个和：
- en: '![](img/B16176_05_019.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_019.png)'
- en: 'What kinds of transformations *f* could be used? Recall that after the parameterization
    trick, *z* is a function of a noise element *e* and the mean and standard deviation
    output by the encoder *Q*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用哪些种类的变换 *f*？记住，在参数化技巧之后，*z* 是编码器 *Q* 输出的均值和标准差以及一个噪声元素 *e* 的函数：
- en: '![](img/B16176_05_020.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_020.png)'
- en: 'If we apply successive layers of transformation, step *t* becomes the sum of
    ![](img/B16176_05_021.png) and the element-wise product of the prior layer *z*
    and the sigmoidal output ![](img/B16176_05_022.png):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们应用连续的转换层，步骤 *t* 就变成了![](img/B16176_05_021.png)和前一层 *z* 与 sigmoid 输出 ![](img/B16176_05_022.png)
    的逐元素乘积之和：
- en: '![](img/B16176_05_023.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_023.png)'
- en: 'In practice, we use a neural network transformation to stabilize the estimate
    of the mean at each step:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们使用神经网络转换来稳定每一步的均值估计：
- en: '![](img/B16176_05_024.png)![](img/B16176_05_025.png)![](img/B16176_05_026.png)![](img/B16176_05_08.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_024.png)![](img/B16176_05_025.png)![](img/B16176_05_026.png)![](img/B16176_05_08.png)'
- en: 'Figure 5.8: IAF networks⁶'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：IAF 网络⁶
- en: Again, note the similarity of this transformation to the LSTM networks discussed
    in *Chapter 3*, *Building Blocks of Deep Neural Networks*. In *Figure 5.8*, there
    is another output (*h*) from the encoder *Q* in addition to the mean and standard
    deviation in order to sample *z*. *H* is, in essence, "accessory data" that is
    passed into each successive transformation and, along with the weighted sum that
    is being calculated at each step, represents the "persistent memory" of the network
    in a way reminiscent of the LSTM.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，这种转换与*第 3 章*，*深度神经网络的基本构件*中讨论的 LSTM 网络的相似性。在*图 5.8*中，除了均值和标准差之外，编码器 *Q*
    还有另一个输出 (*h*)，用于对 *z* 进行采样。*H* 本质上是“辅助数据”，它被传递到每一个连续的转换中，并且与在每一步计算的加权和一起，以一种类似
    LSTM 的方式，表示网络的“持久记忆”。
- en: Importing CIFAR
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入 CIFAR
- en: Now that we've discussed the underlying theory of VAE algorithms, let's start
    building a practical example using a real-world dataset. As we discussed in the
    introduction, for the experiments in this chapter, we'll be working with the Canadian
    Institute for Advanced Research (CIFAR) 10 dataset.^(10) The images in this dataset
    are part of a larger 80 million "small image" dataset^(11), most of which do not
    have class labels like CIFAR-10\. For CIFAR-10, the labels were initially created
    by student volunteers^(12), and the larger tiny images dataset allows researchers
    to submit labels for parts of the data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了 VAE 算法的基本理论，让我们开始使用真实世界的数据集构建一个实际的例子。正如我们在介绍中讨论的，对于本章的实验，我们将使用加拿大高级研究所（CIFAR）10
    数据集。^(10) 这个数据集中的图像是 8000 万个“小图像”数据集^(11)的一部分，其中大多数没有像 CIFAR-10 这样的类标签。对于 CIFAR-10，标签最初是由学生志愿者创建的^(12)，而更大的小图像数据集允许研究人员为数据的部分提交标签。
- en: 'Like the MNIST dataset, CIFAR-10 can be downloaded using the TensorFlow dataset''s
    API:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 像 MNIST 数据集一样，可以使用 TensorFlow 数据集的 API 下载 CIFAR-10：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will download the dataset to disk and make it available for our experiments.
    To split it into training and test sets, we can use the following commands:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把数据集下载到磁盘并使其可用于我们的实验。要将其拆分为训练集和测试集，我们可以使用以下命令：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s inspect one of the images to see what format it is in:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下其中一幅图像，看看它是什么格式：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output tells us that each image in the dataset is of format `<DatasetV1Adapter
    shapes: {image: (32, 32, 3), label: ()}`, `types: {image: tf.uint8, label: tf.int64}>:`
    Unlike the MNIST dataset we used in *Chapter 4*, *Teaching Networks to Generate
    Digits*, the CIFAR images have three color channels, each with 32 x 32 pixels,
    while the label is an integer from 0 to 9 (representing one of the 10 classes).
    We can also plot the images to inspect them visually:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '输出告诉我们数据集中每个图像的格式是 `<DatasetV1Adapter shapes: {image: (32, 32, 3), label: ()}`,
    `types: {image: tf.uint8, label: tf.int64}>:` 不像我们在*第 4 章*，*教网络生成数字*中使用的 MNIST
    数据集，CIFAR 图像有三个颜色通道，每个通道都有 32 x 32 个像素，而标签是一个从 0 到 9 的整数（代表 10 个类别中的一个）。我们也可以绘制图像来进行视觉检查：'
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives the following output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![](img/B16176_05_09.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_09.png)'
- en: 'Figure 5.9: The output'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：输出
- en: 'Like the RBM model, the VAE model we''ll build in this example has an output
    scaled between 1 and 0 and accepts flattened versions of the images, so we''ll
    need to turn each image into a vector and scale it to a maximum of 1:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 像 RBM 模型一样，在这个示例中我们将构建的 VAE 模型的输出被缩放在 1 到 0 之间，并且接受图像的扁平版本，因此我们需要将每个图像转换为一个向量，并将其缩放到最大为
    1：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This results in each image being a vector of length 3072 (32*32*3), which we
    can reshape once we've run the model to examine the generated images.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致每个图像都是长度为 3072 (32*32*3) 的向量，在运行模型后，我们可以重塑它们以检查生成的图像。
- en: Creating the network from TensorFlow 2
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 TensorFlow 2 创建网络
- en: Now that we've downloaded the CIFAR-10 dataset, split it into test and training
    data, and reshaped and rescaled it, we are ready to start building our VAE model.
    We'll use the same Model API from the Keras module in TensorFlow 2\. The TensorFlow
    documentation contains an example of how to implement a VAE using convolutional
    networks ([https://www.tensorflow.org/tutorials/generative/cvae](https://www.tensorflow.org/tutorials/generative/cvae)),
    and we'll build on this code example; however, for our purposes, we will implement
    simpler VAE networks using MLP layers based on the original VAE paper, *Auto-Encoding
    Variational Bayes*^(13), and show how we adapt the TensorFlow example to also
    allow for IAF modules in decoding.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载了 CIFAR-10 数据集，将其拆分为测试和训练数据，并对其进行了重塑和重新缩放，我们已经准备好开始构建我们的 VAE 模型了。我们将使用
    TensorFlow 2 中的 Keras 模块的相同 Model API。TensorFlow 文档中包含了使用卷积网络实现 VAE 的示例（[https://www.tensorflow.org/tutorials/generative/cvae](https://www.tensorflow.org/tutorials/generative/cvae)），我们将在此代码示例的基础上构建；然而，出于我们的目的，我们将使用基于原始
    VAE 论文《自编码变分贝叶斯》(Auto-Encoding Variational Bayes)^(13) 的 MLP 层实现更简单的 VAE 网络，并展示如何将
    TensorFlow 示例改进为也允许解码中的 IAF 模块。
- en: 'In the original article, the authors propose two kinds of models for use in
    the VAE, both MLP feedforward networks: Gaussian and Bernoulli, with these names
    reflecting the probability distribution functions used in the MLP network outputs
    in their finals layers The Bernoulli MLP can be used as the decoder of the network,
    generating the simulated image *x* from the latent vector *z*. The formula for
    the Bernoulli MLP is:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始文章中，作者提出了两种用于 VAE 的模型，都是 MLP 前馈网络：高斯和伯努利，这些名称反映了 MLP 网络输出中使用的概率分布函数在它们的最终层中。伯努利
    MLP 可以用作网络的解码器，从潜在向量 *z* 生成模拟图像 *x*。伯努利 MLP 的公式如下：
- en: '![](img/B16176_05_027.png)![](img/B16176_05_028.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_027.png)![](img/B16176_05_028.png)'
- en: 'Where the first line is the cross-entropy function we use to determine if the
    network generates an approximation of the original image in reconstruction, while
    *y* is a feedforward network with two layers: a tanh transformation followed by
    a sigmoidal function to scale the output between 0 and 1\. Recall that this scaling
    is why we had to normalize the CIFAR-10 pixels from their original values.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行是我们用于确定网络是否生成原始图像近似重建的交叉熵函数，而 *y* 是一个前馈网络，有两层：一个双曲正切变换，然后是一个 sigmoid 函数将输出缩放到
    0 到 1 之间。回想一下，这种缩放是我们不得不将 CIFAR-10 像素从其原始值归一化的原因。
- en: 'We can easily create this Bernoulli MLP network using the Keras API:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地使用 Keras API 创建这个伯努利 MLP 网络：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We just need to specify the dimensions of the single hidden layer and the latent
    output (*z*). We then specify the forward pass as a composition of these two layers.
    Note that in the output, we've returned three values, with the second two set
    as `None`. This is because in our end model, we could use either the BernoulliMLP
    or GaussianMLP as the decoder. If we used the GaussianMLP, we return three values,
    as we will see below; the example in this chapter utilizes a binary output and
    cross-entropy loss so we can use just the single output, but we want the return
    signatures for the two decoders to match.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要指定单隐藏层和潜在输出 (*z*) 的维度。然后，我们将前向传递指定为这两个层的组合。请注意，在输出中，我们返回了三个值，第二和第三个值均设置为
    `None`。这是因为在我们的最终模型中，我们可以使用 BernoulliMLP 或 GaussianMLP 作为解码器。如果我们使用 GaussianMLP，则返回三个值，正如我们将在下文中看到的；本章中的示例利用了二进制输出和交叉熵损失，因此我们可以只使用单个输出，但我们希望两个解码器的返回签名匹配。
- en: 'The second network type proposed by the authors in the original VAE paper was
    a Gaussian MLP, whose formulas are:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 VAE 论文中作者提出的第二种网络类型是高斯 MLP，其公式为：
- en: '![](img/B16176_05_029.png)![](img/B16176_05_030.png)![](img/B16176_05_031.png)![](img/B16176_05_032.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_029.png)![](img/B16176_05_030.png)![](img/B16176_05_031.png)![](img/B16176_05_032.png)'
- en: 'This network can be used as either the encoder (generating the latent vector
    *z*) or the decoder (generating the simulated image *x*) in the network. The equations
    above assume that it is used as the decoder, and for the encoder we just switch
    the *x* and *z* variables. As you can see, this network has two types of layers,
    a hidden layer given by a tanh transformation of the input, and two output layers,
    each given by linear transformations of the hidden layer, which are used as the
    inputs of a lognormal likelihood function. Like the Bernoulli MLP, we can easily
    implement this simple network using the TensorFlow Keras API:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此网络可以在网络中作为编码器（生成潜在向量*z*）或解码器（生成模拟图像*x*）使用。上述方程假定它用作解码器，对于编码器，我们只需交换*x*和*z*变量。如您所见，这个网络有两种类型的层，一个隐藏层由输入的tanh变换给出，并且两个输出层，每个输出层由隐藏层的线性变换给出，这些输出层被用作对数正态似然函数的输入。像Bernoulli
    MLP一样，我们可以轻松使用TensorFlow Keras API实现这个简单网络：
- en: '[PRE6]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, to implement the call function, we must return the two outputs
    of the model (the mean and log variance of the normal distribution we''ll use
    to compute the likelihood of *z* or *x*). However, recall that for the IAE model,
    the encoder has to have an additional output *h*, which is fed into each step
    of the normalizing flow:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，要实现`call`函数，我们必须返回模型的两个输出（我们将用来计算*z*或*x*的正态分布的均值和对数方差）。然而，请注意，对于IAE模型，编码器必须具有额外的输出*h*，它被馈送到每一步的正规流中：
- en: '![](img/B16176_05_033.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_05_033.png)'
- en: To allow for this additional output, we include a third variable in the output,
    which gets set to a linear transformation of the input if we set the IAF options
    to `True`, and is none if `False`, so we can use the GaussianMLP as an encoder
    in networks both with and without IAF.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要允许额外的输出，我们在输出中包括了第三个变量，如果我们将IAF选项设置为`True`，它将被设置为输入的线性变换，如果为`False`，则为`none`，因此我们可以在具有和不具有IAF的网络中使用GaussianMLP作为编码器。
- en: 'Now that we have both of our subnetworks defined, let''s see how we can use
    them to construct a complete VAE network. Like the sub-networks, we can define
    the VAE using the Keras API:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的两个子网络，让我们看看如何使用它们来构建一个完整的VAE网络。像子网络一样，我们可以使用Keras API定义VAE：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see, this model is defined to contain both an encoder and decoder
    network. Additionally, we allow the user to specify whether we are implementing
    IAF as part of the model, in which case we need a stack of autoregressive transforms
    specified by the `iaf_params` variable. Because this IAF network needs to take
    both *z* and *h* as inputs, the input shape is twice the size of the `latent_dim`
    (*z*). We allow the decoder to be either the GaussianMLP or BernoulliMLP network,
    while the encoder is the GaussianMLP.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，此模型被定义为包含编码器和解码器网络。此外，我们允许用户指定我们是否在模型中实现IAF，如果是的话，我们需要一个由`iaf_params`变量指定的自回归变换的堆栈。因为这个IAF网络需要将*z*和*h*作为输入，输入形状是`latent_dim`
    (*z*)的两倍。我们允许解码器是GaussianMLP或BernoulliMLP网络，而编码器是GaussianMLP。
- en: 'There are a few other functions of this model class that we need to cover;
    the first are simply the encoding and decoding functions of the VAE model class:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型类还有一些其他函数需要讨论；首先是VAE模型类的编码和解码函数：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For the encoder, we simply call (run the forward pass for) the encoder network.
    To decode, you will notice that we specify three outputs. The article that introduced
    VAE models, *Autoencoding Variational Bayes*, provided examples of a decoder specified
    as either a **Gaussian Multilayer Perceptron** (**MLP**) or Benoulli output. If
    we used a Gaussian MLP, the decoder would yield the value, mean, and standard
    deviation vectors for the output, and we need to transform that output to a probability
    (0 to 1) using the sigmoidal transform. In the Bernoulli case, the output is already
    in the range 0 to 1, and we don't need this transformation (`apply_sigmoid`=`False`).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编码器，我们只需调用（运行前向传递）编码器网络。解码时，您会注意到我们指定了三个输出。介绍了VAE模型的文章《自编码变分贝叶斯》提供了解码器的例子，指定为**高斯多层感知器**（**MLP**）或Benoulli输出。如果我们使用了高斯MLP，解码器将为输出值、均值和标准差向量，我们需要使用sigmoidal变换将该输出转换为概率（0到1）。在伯努利情况下，输出已经在0到1的范围内，我们不需要这种转换（`apply_sigmoid`=`False`）。
- en: 'Once we''ve trained the VAE network, we''ll want to use sampling in order to
    generate random latent vectors (*z*) and run the decoder to generate new images.
    While we could just run this as a normal function of the class in the Python runtime,
    we''ll decorate this function with the `@tf.function` annotation, which will allow
    it to be executed in the TensorFlow graph runtime (just like any of the `tf` functions,
    such as `reduce_sum` and multiply), making using of GPU and TPU devices if they
    are available. We sample a value from a random normal distribution, for a specified
    number of samples, and then apply the decoder to generate new images:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, recall that the "reparameterization trick" is used to allow us to
    backpropagate through the value of *z* and reduce the variance of the likelihood
    of *z*. We need to implement this transformation, which is given by:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the original paper, *Autoencoding Variational Bayes*, this is given by:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_05_034.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: where *i* is a data point in *x* and l is a sample from the random distribution,
    here, a normal. In our code, we multiply by 0.5 because we are computing the **log
    variance** (or standard deviation squared), and *log*(*s^2*) = *log*(*s*)*2*,
    so the 0.5 cancels the 2, leaving us with *exp*(*log*(*s*)) = *s*, just as we
    require in the formula.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll also include a class property (with the `@property` decorator) so we
    can access the array of normalizing transforms if we implement IAF:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we''ll need a few additional functions to actually run our VAE algorithm.
    The first computes the log normal **probability density function** (**pdf**),
    used in the computation of the variational lower bound, or ELBO:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now need to utilize this function as part of computing the loss with each
    minibatch gradient descent pass in the process of training the VAE. As with the
    sample method, we''ll decorate this function with the `@tf.function` annotation
    so it will be executed on the graph runtime:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let's unpack a bit of what is going on here. First, we can see that we call
    the encoder network on the input (a minibatch of flattened images, in our case)
    to generate the needed mean, `logvariance`, and, if we are using IAF in our network,
    the accessory input `h` that we'll pass along with each step of the normalizing
    flow transform.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: We apply the "reparameterization trick" on the inputs in order to generate the
    latent vector `z`, and apply a lognormal pdf to get the *logq*(*z*|*x*).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: If we are using IAF, we need to iteratively transform `z` using each network,
    and pass in the `h` (accessory input) from the decoder at each step. Then we apply
    the loss from this transform to the initial loss we computed, as per the algorithm
    given in the IAF paper:^(14)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_05_036.png)![](img/B16176_05_037.png)![](img/B16176_05_038.png)![](img/B16176_05_039.png)![](img/B16176_05_040.png)![](img/B16176_05_041.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Once we have the transformed or untransformed *z*, we decode it using the decoder
    network to get the reconstructed data, *x*, from which we calculate a cross-entropy
    loss. We sum these over the minibatch and take the lognormal pdf of *z* evaluated
    at a standard normal distribution (the prior), before computing the expected lower
    bound.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the expression for the variational lower bound, or ELBO, is:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_05_035.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'So, our minibatch estimator is a sample of this value:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have these ingredients, we can run the stochastic gradient descent
    using the `GradientTape` API, just as we did for the DBN in *Chapter 4*, *Teaching
    Networks to Generate Digits* passing in an optimizer, model, and minibatch of data
    (*x*):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To run the training, first we need to specify a model using the class we''ve
    built. If we don''t want to use IAF, we could do this as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we want to use IAF transformations, we need to include some additional arguments:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'With the model created, we need to specify a number of epochs, an optimizer
    (in this instance, Adam, as we described in *Chapter 3*, *Building Blocks of Deep
    Neural Networks*). We split our data into minibatches of 32 elements, and apply
    gradient updates after each minibatch for the number of epochs we''ve specified.
    At regular intervals, we output the estimate of the ELBO to verify that our model
    is getting better:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can verify that the model is improving by looking at updates, which should
    show an increasing ELBO:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_05_11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: 'To examine the output of the model, we can first look at the reconstruction
    error; does the encoding of the input image by the network approximately capture
    the dominant patterns in the input image, allowing it to be reconstructed from
    its vector *z*? We can compare the raw image to its reconstruction formed by passing
    the image through the encoder, applying IAF, and then decoding it:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For the first few CIFAR-10 images, we get the following output, showing that
    we have captured the overall pattern of the image (although it is fuzzy, a general
    downside to VAEs that we''ll address in our discussion of **Generative Adversarial
    Networks** (**GANs**) in future chapters):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_05_12.png)![](img/B16176_05_13.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: The output for the CIFAR-10 images'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we wanted to create entirely new images? Here we can use the "sample"
    function we defined previously in *Creating the network from TensorFlow 2* to
    create batches of new images from randomly generated *z* vectors, rather than
    the encoded product of CIFAR images:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code will produce output like the following, which shows a set of images
    generated from vectors of random numbers:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16176_05_14.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: '![](img/B16176_05_15.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Images generated from vectors of random numbers'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'These are, admittedly, a bit blurry, but you can appreciate that they show
    structure and look comparable to some of the "reconstructed" CIFAR-10 images you
    saw previously. Part of the challenge here, as we''ll discuss more in subsequent
    chapters, is the loss function itself: the cross-entropy function, in essence,
    penalizes each pixel for how much it resembles the input pixel. While this might
    be mathematically correct, it doesn''t capture what we might think of as conceptual
    "similarity" between an input and reconstructed image. For example, an input image
    could have a single pixel set to infinity, which would create a large difference
    between it and a reconstruction that set that pixel to 0; however, a human, looking
    at the image, would perceive both as being identical. The objective functions
    used for GANs, described in *Chapter 6*, *Image Generation with GANs*, capture
    this nuance more accurately.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，这些图像可能有些模糊，但您可以欣赏到它们显示的结构，并且看起来与您之前看到的一些“重建”CIFAR-10图像相当。在这里的部分挑战，就像我们将在随后的章节中讨论的那样，是损失函数本身：交叉熵函数本质上对每个像素惩罚，以衡量其与输入像素的相似程度。虽然这在数学上可能是正确的，但它并不能捕捉到我们所认为的输入和重建图像之间的“相似性”的概念。例如，输入图像可能有一个像素设为无穷大，这将导致它与将该像素设为0的重建图像之间存在很大差异；然而，一个人观看这个图像时，会认为它们两者完全相同。GANs所使用的目标函数，如*第6章*中描述的*用GAN生成图像*，更准确地捕捉到了这种微妙之处。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you saw how deep neural networks can be used to create representations
    of complex data such as images that capture more of their variance than traditional
    dimension reduction techniques, such as PCA. This is demonstrated using the MNIST
    digits, where a neural network can spatially separate the different digits in
    a two-dimensional grid more cleanly than the principal components of those images.
    The chapter showed how deep neural networks can be used to approximate complex
    posterior distributions, such as images, using variational methods to sample from
    an approximation of an intractable distribution, leading to a VAE algorithm based
    on minimizing the variational lower bound between the true and approximate posterior.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您看到了如何使用深度神经网络来创建复杂数据的表示，例如图像，捕捉到比传统的降维技术如PCA更多的变化。这是通过MNIST数字进行演示的，其中神经网络可以在二维网格上更干净地分离不同数字，而不像这些图像的主成分那样。本章展示了如何使用深度神经网络来近似复杂的后验分布，例如图像，使用变分方法从不可约分布的近似中进行采样，形成了一种基于最小化真实和近似后验之间的变分下界的VAE算法。
- en: You also learned how the latent vector from this algorithm can be reparameterized
    to have lower variance, leading to better convergence in stochastic minibatch
    gradient descent. You saw how the latent vectors generated by encoders in these
    models, which are usually independent, can be transformed into more realistic
    correlated distributions using IAF. Finally, we implemented these models on the
    CIFAR-10 dataset and showed how they can be used to reconstruct the images and
    generate new images from random vectors.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 您还学会了如何重新参数化这个算法生成的潜在向量，以降低方差，从而使随机小批量梯度下降收敛更好。您还看到了这些模型中编码器生成的潜在向量通常是相互独立的，可以使用IAF将其转换为更真实的相关分布。最后，我们在CIFAR-10数据集上实现了这些模型，并展示了它们如何用于重建图像和从随机向量生成新图像。
- en: The next chapter will introduce GANs and show how we can use them to add stylistic
    filters to input images, using the StyleGAN model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍GANs，并展示我们如何使用它们为输入图像添加风格滤镜，使用StyleGAN模型。
- en: References
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Eckersley P., Nasser Y. *Measuring the Progress of AI Research*. EFF. Retrieved
    April 26, 2021, [https://www.eff.org/ai/metrics#Measuring-the-Progress-of-AI-Research](https://www.eff.org/ai/metrics#Measuring-the-Progress-of-AI-Research)
    and CIFAR-10 datasets, [https://www.cs.toronto.edu/~kriz/](https://www.cs.toronto.edu/~kriz/)
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Eckersley P., Nasser Y. *衡量AI研究的进展*。EFF。检索日期 2021年4月26日，[https://www.eff.org/ai/metrics#Measuring-the-Progress-of-AI-Research](https://www.eff.org/ai/metrics#Measuring-the-Progress-of-AI-Research)
    和 CIFAR-10 数据集，[https://www.cs.toronto.edu/~kriz/](https://www.cs.toronto.edu/~kriz/)
- en: Malhotra P. (2018). Autoencoder-Implementations. GitHub repository. [https://www.piyushmalhotra.in/Autoencoder-Implementations/VAE/](https://www.piyushmalhotra.in/Autoencoder-Implementations/VAE/)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Malhotra P. (2018). 自编码器实现。GitHub 仓库。[https://www.piyushmalhotra.in/Autoencoder-Implementations/VAE/](https://www.piyushmalhotra.in/Autoencoder-Implementations/VAE/)
- en: Kingma, D P., Welling, M. (2014). *Auto-Encoding Variational Bayes*. arXiv:1312.6114\.
    [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. E., Salakhutdinov R. R. (2006). *Reducing the Dimensionality of Data
    with Neural Networks*. ScienceMag. [https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton G. E., Salakhutdinov R. R. (2006). *Reducing the Dimensionality of Data
    with Neural Networks*. ScienceMag. [https://www.cs.toronto.edu/~hinton/science.pdf](https://www.cs.toronto.edu/~hinton/science.pdf)
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kingma, D P., Welling, M. (2014). *Auto-Encoding Variational Bayes*. arXiv:1312.6114\.
    [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doersch, C. (2016). *Tutorial on Variational Autoencoders*. arXiv:1606.05908\.
    [https://arxiv.org/pdf/1606.05908.pdf](https://arxiv.org/pdf/1606.05908.pdf)
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paisley, J., Blei, D., Jordan, M. (2012). *Variational Bayesian Inference with
    Stochastic Search*. [https://icml.cc/2012/papers/687.pdf](https://icml.cc/2012/papers/687.pdf)
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Doersch, C. (2016). *Tutorial on Variational Autoencoders*. arXiv:1606.05908\.
    [https://arxiv.org/pdf/1606.05908.pdf](https://arxiv.org/pdf/1606.05908.pdf)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Angelov, Plamen; Gegov, Alexander; Jayne, Chrisina; Shen, Qiang (2016-09-06).
    *Advances in Computational Intelligence Systems: Contributions Presented at the
    16th UK Workshop on Computational Intelligence*, September 7–9, 2016, Lancaster,
    UK. Springer International Publishing. pp. 441–. ISBN 9783319465623\. Retrieved
    January 22, 2018.'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'TinyImages: [http://groups.csail.mit.edu/vision/TinyImages/](http://groups.csail.mit.edu/vision/TinyImages/)'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Krizhevsky A. (2009). *Learning Multiple Layers of Features from Tiny Images*.
    [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kingma, D P., Welling, M. (2014). *Auto-Encoding Variational Bayes*. arXiv:1312.6114\.
    [https://arxiv.org/pdf/1312.6114.pdf](https://arxiv.org/pdf/1312.6114.pdf)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kingma, D P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., Welling,
    M. (2016). *Improving Variational Inference with Inverse Autoregressive Flow*.
    arXiv:1606.04934\. [https://arxiv.org/pdf/1606.04934.pdf](https://arxiv.org/pdf/1606.04934.pdf)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
