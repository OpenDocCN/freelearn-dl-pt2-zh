["```py\ndef build_discriminator(input_shape=(28, 28,), verbose=True):\n    \"\"\"\n    Utility method to build a MLP discriminator\n    Parameters:\n        input_shape:\n            type:tuple. Shape of input image for classification.\n                        Default shape is (28,28)->MNIST\n        verbose:\n            type:boolean. Print model summary if set to true.\n                        Default is True\n    Returns:\n        tensorflow.keras.model object\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=input_shape))\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    if verbose:\n        model.summary()\n    return model \n```", "```py\ndef build_generator(z_dim=100, output_shape=(28, 28), verbose=True):\n    \"\"\"\n    Utility method to build a MLP generator\n    Parameters:\n        z_dim:\n            type:int(positive). Size of input noise vector to be\n                        used as model input.\n                        Default value is 100\n        output_shape:   type:tuple. Shape of output image .\n                        Default shape is (28,28)->MNIST\n        verbose:\n            type:boolean. Print model summary if set to true.\n                        Default is True\n    Returns:\n        tensorflow.keras.model object\n    \"\"\"\n    model = Sequential()\n    model.add(Input(shape=(z_dim,)))\n    model.add(Dense(256, input_dim=z_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(512))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(1024))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(np.prod(output_shape), activation='tanh'))\n    model.add(Reshape(output_shape))\n    if verbose:\n        model.summary()\n    return model \n```", "```py\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy',\n                      optimizer=Adam(0.0002, 0.5),\n                      metrics=['accuracy'])\ngenerator=build_generator()\nz_dim = 100 #noise\nz = Input(shape=(z_dim,))\nimg = generator(z)\n# For the combined model we will only train the generator\ndiscriminator.trainable = False\n# The discriminator takes generated images as input \n# and determines validity\nvalidity = discriminator(img)\n# The combined model  (stacked generator and discriminator)\n# Trains the generator to fool the discriminator\ngan_model = Model(z, validity)\ngan_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5)) \n```", "```py\n# Load MNIST train samples\n(X_train, _), (_, _) = datasets.mnist.load_data()\n# Rescale to [-1, 1]\n  X_train = X_train / 127.5 â€“ 1 \n```", "```py\nidx = np.random.randint(0, X_train.shape[0], batch_size)\nreal_imgs = X_train[idx]\n# pick random noise samples (z) from a normal distribution\nnoise = np.random.normal(0, 1, (batch_size, z_dim))\n# use generator model to generate output samples\nfake_imgs = generator.predict(noise)\n# calculate discriminator loss on real samples\ndisc_loss_real = discriminator.train_on_batch(real_imgs, real_y)\n\n# calculate discriminator loss on fake samples\ndisc_loss_fake = discriminator.train_on_batch(fake_imgs, fake_y)\n\n# overall discriminator loss\ndiscriminator_loss = 0.5 * np.add(disc_loss_real, disc_loss_fake) \n```", "```py\n# train generator\n# pick random noise samples (z) from a normal distribution\nnoise = np.random.normal(0, 1, (batch_size, z_dim))\n# use trained discriminator to improve generator\ngen_loss = gan_model.train_on_batch(noise, real_y) \n```", "```py\ndef build_dc_generator(z_dim=100, verbose=True):\n    model = Sequential()\n    model.add(Input(shape=(z_dim,)))\n    model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=z_dim))\n    model.add(Reshape((7, 7, 128)))\n    model.add(UpSampling2D())\n    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    model.add(UpSampling2D())\n    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Activation(\"relu\"))\n    model.add(Conv2D(1, kernel_size=3, padding=\"same\"))\n    model.add(Activation(\"tanh\"))\n    if verbose:\n        model.summary()\n    return model \nFigure 6.11).\n```", "```py\nz and the class label *y*'s embedding output, using the multiply layer. Please note that this is different from the original implementation, which concatenates vectors *z* and *y*. Changes as compared to vanilla GAN's generator have been highlighted for ease of understanding:\n```", "```py\ndef build_conditional_generator(z_dim=100, output_shape=(28, 28),\n                                **num_classes=****10**, verbose=True):\n    \"\"\"\n    Utility method to build a MLP generator\n    Parameters:\n        z_dim:\n            type:int(positive). Size of input noise vector to be\n                        used as model input.\n                        Default value is 100\n        output_shape:   type:tuple. Shape of output image .\n                        Default shape is (28,28)->MNIST\n        num_classes:    type:int. Number of unique class labels.\n                        Default is 10->MNIST digits\n        verbose:\n            type:boolean. Print model summary if set to true.\n                        Default is True\n    Returns:\n        tensorflow.keras.model object\n    \"\"\"\n    **noise = Input(shape=(z_dim,))**\n    **label = Input(shape=(****1****,), dtype=****'int32'****)**\n    **label_embedding = Flatten()(Embedding(num_classes, z_dim)(label))**\n    **model_input = multiply([noise, label_embedding])**\n    mlp = Dense(256, input_dim=z_dim)(model_input)\n    mlp = LeakyReLU(alpha=0.2)(mlp)\n    mlp = BatchNormalization(momentum=0.8)(mlp)\n    mlp = Dense(512)(mlp)\n    mlp = LeakyReLU(alpha=0.2)(mlp)\n    mlp = Dense(1024)(mlp)\n    mlp = LeakyReLU(alpha=0.2)(mlp)\n    mlp = BatchNormalization(momentum=0.8)(mlp)\n    mlp = Dense(np.prod(output_shape), activation='tanh')(mlp)\n    mlp = Reshape(output_shape)(mlp)\n    **model = Model([noise, label], mlp)**\n    if verbose:\n        model.summary()\n    return model \n network. Changes as compared to vanilla GAN's discriminator have been highlighted:\n```", "```py\ndef build_conditional_discriminator(input_shape=(28, 28,),\n                                    **num_classes=****10**, verbose=True):\n    \"\"\"\n    Utility method to build a conditional MLP discriminator\n    Parameters:\n        input_shape:\n            type:tuple. Shape of input image for classification.\n                        Default shape is (28,28)->MNIST\n        num_classes:    type:int. Number of unique class labels.\n                        Default is 10->MNIST digits\n        verbose:\n            type:boolean. Print model summary if set to true.\n                        Default is True\n    Returns:\n        tensorflow.keras.model object\n    \"\"\"\n    **img = Input(shape=input_shape)**\n    **flat_img = Flatten()(img)**\n    **label = Input(shape=(****1****,), dtype=****'int32'****)**\n    **label_embedding = Flatten()(Embedding(num_classes,**\n                                      **np.prod(input_shape))(label))**\n    **model_input = multiply([flat_img, label_embedding])**\n    mlp = Dense(512, input_dim=np.prod(input_shape))(model_input)\n    mlp = LeakyReLU(alpha=0.2)(mlp)\n    mlp = Dense(512)(mlp)\n    mlp = LeakyReLU(alpha=0.2)(mlp)\n    mlp = Dropout(0.4)(mlp)\n    mlp = Dense(512)(mlp)\n    mlp = LeakyReLU(alpha=0.2)(mlp)\n    mlp = Dropout(0.4)(mlp)\n    mlp = Dense(1, activation='sigmoid')(mlp)\n    **model = Model([img, label], mlp)**\n    if verbose:\n        model.summary()\n    return model \ntraining loop:\n```", "```py\ndef train(generator=None,discriminator=None,gan_model=None,\n          epochs=1000, batch_size=128, sample_interval=50,\n          z_dim=100):\n    # Load MNIST train samples\n    **(X_train, y_train), (_, _) = datasets.mnist.load_data()**\n    # Rescale -1 to 1\n    X_train = X_train / 127.5 - 1\n    X_train = np.expand_dims(X_train, axis=3)\n    **y_train = y_train.reshape(****-1****,** **1****)**\n    # Prepare GAN output labels\n    real_y = np.ones((batch_size, 1))\n    fake_y = np.zeros((batch_size, 1))\n    for epoch in range(epochs):\n        # train disriminator\n        # pick random real samples from X_train\n        idx = np.random.randint(0, X_train.shape[0], batch_size)\n        **real_imgs, labels = X_train[idx], y_train[idx]**\n        # pick random noise samples (z) from a normal distribution\n        noise = np.random.normal(0, 1, (batch_size, z_dim))\n        # use generator model to generate output samples\n        **fake_imgs = generator.predict([noise, labels])**\n        # calculate discriminator loss on real samples\n        **disc_loss_real = discriminator.train_on_batch([real_imgs, labels], real_y)**\n\n        # calculate discriminator loss on fake samples\n        **disc_loss_fake = discriminator.train_on_batch([fake_imgs, labels], fake_y)**\n\n        # overall discriminator loss\n        discriminator_loss = 0.5 * np.add(disc_loss_real, disc_loss_fake)\n\n        # train generator\n        # pick random noise samples (z) from a normal distribution\n        noise = np.random.normal(0, 1, (batch_size, z_dim))\n\n        # pick random labels for conditioning\n        **sampled_labels = np.random.randint(****0****,** **10****, batch_size).reshape(****-1****,** **1****)**\n        # use trained discriminator to improve generator\n        **gen_loss = gan_model.train_on_batch([noise, sampled_labels], real_y)**\n        # training updates\n        print (\"%d [Discriminator loss: %f, acc.: %.2f%%] [Generator loss: %f]\" % (epoch, discriminator_loss[0], \n              100*discriminator_loss[1], gen_loss))\n        # If at save interval => save generated image samples\n        if epoch % sample_interval == 0:\n            sample_images(epoch,generator) \n```", "```py\ndef wasserstein_loss(y_true, y_pred):\n    \"\"\"\n    Custom loss function for W-GAN\n    Parameters:\n        y_true: type:np.array. Ground truth\n        y_pred: type:np.array. Predicted score\n    \"\"\"\n    return K.mean(y_true * y_pred) \n```", "```py\n# Clip critic weights\nfor l in discriminator.layers:\n       weights = l.get_weights()\n       weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n       l.set_weights(weights) \n```", "```py\nimport tensorflow_hub as hub \n```", "```py\ntf.random.set_seed(12)\npro_gan = hub.load(\"https://tfhub.dev/google/progan-128/1\").signatures['default'] \n```", "```py\nvector = tf.random.normal([1, 512])\nsample_image = pro_gan(vector)['default'][0]\nnp_img = sample_image.numpy()\nplt.figure(figsize=(6,6))\nplt.imshow(np_img) \n```"]