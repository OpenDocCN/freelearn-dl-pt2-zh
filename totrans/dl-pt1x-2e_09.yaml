- en: Implementing Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter addresses the notion of semi-supervised learning algorithms through
    the introduction of autoencoders, and then moves on to **restricted Boltzmann
    machines** (**RBMs**) and **deep belief networks** (**DBNs**) in order to understand
    the probability distribution of data. The chapter will give you an overview as
    to how these algorithms have been applied to some real-world problems. Coded examples
    implemented in PyTorch will also be provided.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are an unsupervised learning technique. They can take an unlabeled
    dataset and task it with reconstructing the original input by modeling it using
    an unsupervised learning problem as opposed to a supervised one. The goal of the
    autoencoder is for the input to be as similar as possible to the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of autoencoders and their applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottleneck and loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep belief networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders fall under representational learning and are used to find a compressed
    representation of the inputs. They are composed of an encoder and a decoder. The
    following diagram shows the structure of an autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/693d5531-ef4e-45a0-aa19-a9c0854da1ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Examples of applications of autoencoders include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data denoising
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction for data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpolating text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottleneck and loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders impose a bottleneck on the network, enforcing a compressed knowledge
    representation of the original input. The network would simply learn to memorize
    the input values if the bottleneck were not present. As such, this would mean
    that the model wouldn''t generalize well on unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a862aa43-423e-4616-b93e-6c317b509ee9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order for the model to detect a signal, we need it to be sensitive to the
    input but not so much that it simply memorizes them and doesn''t predict well
    on unseen data. In order to determine the optimal trade-off, we need to construct
    a loss/cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e2bfc83-5110-405d-9e0a-74b6c8312912.png)'
  prefs: []
  type: TYPE_IMG
- en: There are some commonly used autoencoder architectures for imposing these two
    constraints and ensuring there is an optimal trade-off between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Coded example ‚Äì standard autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will show you how to¬†compile an autoencoder model in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, import the relevant libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, define the model parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, initiate a function to transform the images in the MNIST dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the autoencoder class in which to feed the data and initiate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function that will output the images from the model after each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the model over each epoch and review the results of the reconstructed
    images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9ecb6f8-ef83-48aa-8e49-e29b3a0f6bca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the following image¬†shows the output of the autoencoder at each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab6c7350-99d9-4cb1-bde0-602292f98856.png)'
  prefs: []
  type: TYPE_IMG
- en: The more epochs that pass, the clearer the images become as the model continues
    to learn.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoencoders can be used with convolutions instead of fully connected layers.
    This can be done using 3D vectors instead of 1D vectors. In the context of images,
    downsampling the image forces the autoencoder to learn a compressed version of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Coded example ‚Äì convolutional autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will show you how to compile a convolutional autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, you obtain the train and test datasets from the MNIST dataset and
    define the model parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, initiate the model for the convolutional autoencoder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, run the model over each epoch while saving the output images for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can look at the saved images after every epoch in the folder that is mentioned
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Denoising encoders deliberately add noise to the input of the network. These
    autoencoders essentially create a corrupted copy of the data. In doing so, this
    helps the encoder to learn the latent representation in the input data, making
    it more generalizable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fb53730-c36b-4900-be3b-39f76e76f65e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This corrupted image is fed into the network in the same way as other standard
    autoencoders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c46e5ba-3ff4-4584-b559-6f772c705114.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, noises were added in the original input and the encoder encodes
    the input and sends it to the decoder, which then decodes the noisy input into
    the cleaned output. Thus, we have looked at various applications that autoencoders
    can be used for. We will now look at a specific type of autoencoder, which is
    a¬†**variational autoencoder** (**VAE**).
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VAEs¬†are different from the standard autoencoders we have considered so far
    as they describe an observation in latent space in a probabilistic manner rather
    than deterministic. A probability distribution for each latent attribute is output,
    rather than a single value.
  prefs: []
  type: TYPE_NORMAL
- en: Standard autoencoders have somewhat limited applications in the real world as
    they are only really useful when you want to replicate the data that has been
    put into it. Since VAEs are generative models, they can be applied to cases where
    you don't want to output data that is the same as the input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider this in a real-world context. When training an autoencoder
    model on a dataset of faces, you would hope that it would learn latent attributes,
    such as whether the person is smiling, their skin tone, whether they are wearing
    glasses, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cc640bb-841f-438d-a6cc-79837d7e85dd.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see in the preceding diagram, standard autoencoders represent these
    latent attributes as discrete values.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we allow each feature to be within a range of possible values rather than
    a single value, we can use VAEs to describe the attributes in probabilistic terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76947b8a-b694-4db9-b32a-498f4972ba45.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram depicts how we can represent whether the person is smiling
    as either a discrete value or as a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution of each latent attribute is sampled from the image in order
    to generate the vector that is used as the input for the decoder model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07851c42-ae8a-4888-a8d8-39704d6948da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Two vectors are output, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6e926bd-3db3-4433-969d-9237af0bf3a0.png)'
  prefs: []
  type: TYPE_IMG
- en: One describes the mean and the other describes the variance of the distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Training VAEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During training, we calculate the relationship of each parameter in the network
    with respect to the overall loss using a process called **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Standard autoencoders use backpropagation in order to reconstruct the loss
    value across the weights of the network. As the sampling operation in VAEs is
    not differentiable, the gradients cannot be propagated from the reconstruction
    error. The following diagram explains this further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12949e89-c9f1-491d-a822-7e82befa9fa2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to overcome this limitation, the reparameterization trick can be used.
    The reparameterization trick samples Œµ from a unit normal distribution, shifts
    it by the mean ùúá of the latent attribute, and, then scales it by the latent attribute''s
    variance ùúé:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6376b4ec-ebd3-4464-adcd-4c39f59e1313.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This removes the sampling process from the flow of gradients as it is now outside
    of the network. As such, the sampling process doesn''t depend on anything in the
    network. We can now optimize the parameters of the distribution while maintaining
    the ability to randomly sample from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0727f2d1-a7f3-4436-a031-7c6a717b2192.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can transform with a mean, ùúá, and covariance matrix ‚àë as per the following,
    as the distribution of each attribute is Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08421e67-3535-434a-9586-d98bf7e661a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, Œµ ~ N(0,1).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now train the model using simple backpropagation with the introduction
    of the reparameterization trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20d8edd5-d1d0-4190-942c-710c7cd089cb.png)'
  prefs: []
  type: TYPE_IMG
- en: As seen in the preceding diagram, we have trained the autoencoder to smooth
    out the image.
  prefs: []
  type: TYPE_NORMAL
- en: Coded example ‚Äì VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to code a VAE¬†in PyTorch, we can load the libraries and dataset like
    we did in the previous examples. From here, we can define the VAE class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define the loss function with the help of KL divergence and initiate
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can run the model over each epoch and save the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have seen the various autoencoders and how to compile them, let
    us learn how to implement them in recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An¬†**RBM**¬†is an algorithm that has been widely used for tasks such as collaborative
    filtering, feature extraction, topic modeling, and dimensionality reduction. They
    can learn patterns in a dataset in an unsupervised fashion.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you watch a movie and say whether you liked it or not, we could
    use an RBM to help us determine the reason why you made this decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of RBM is to minimize energy defined by the following formula, which
    depends on the configurations of visible/input states, hidden states, weights,
    and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2d5c938-993c-4018-8312-747bdca6ea70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'RBMs are two-layer networks that are the fundamental building blocks of a DBN.
    The first layer of an RBM is a visible/input layer of neurons and the second is
    the hidden layer of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6352c09-c581-41b7-858f-b5bf82a3f269.png)'
  prefs: []
  type: TYPE_IMG
- en: The RBM translates the inputs from the visible layer and translates them into
    a set of numbers. Through several forward and backward passes, the number is¬†then
    translated back to reconstruct the inputs. The restriction in the RBM is such
    that nodes in the same layer are not connected.
  prefs: []
  type: TYPE_NORMAL
- en: 'A low-level feature is fed into each node of the visible layer from the training
    dataset. In the case of image classification, each node would receive one pixel
    value for each pixel in an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3310e3c-b99d-4ff3-97bf-aa1ff08491dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Following one pixel through the network, the input *x* is multiplied by the
    weight from the hidden layer and a bias is then added. From here, this is then
    fed into an activation function, which produces the output, which is essentially
    the strength of the signal passing through it given the input *x*, as shown in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02467675-a93e-4d99-81ca-01aa85c3ac72.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At each node in the hidden layer, x from each pixel value is multiplied by
    a separate weight. The products are then summed, and a bias is added. The output
    of this is then passed through an activation function, producing the output at
    that single node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00ee7d87-14b2-4e4a-8776-4c879004bb8b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At each point in time, the RBM is in a certain state, which refers to the values
    of the neurons in the visible *v* and hidden *h* layers. The probability of such
    a state can be given by the following joint distribution function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/125757b1-9b57-487d-9133-726cc7addde4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, Z is the partition function that is the summation over all possible pairs
    of visible and hidden vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Training RBMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two main steps an RBM carries out during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gibbs sampling**: The first step in the training process uses Gibbs sampling,
    which repeats the following process *k* times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Probability of hidden vector given the input vector; prediction of the hidden
    values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability of the input vector given the hidden vector; prediction of the input
    values. From this, we obtain another input vector, which was recreated from the
    original input values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrastive divergence**: RBMs adjust their weights through contrastive divergence.
    During this process, weights for visible nodes are randomly generated and used
    to generate hidden nodes. The hidden nodes then use the same weights to reconstruct
    visible nodes. The weights used to reconstruct the visible nodes are the same
    throughout. However, the generated nodes are not the same because they aren''t
    connected to each other.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once an RBM has been trained, it is essentially able to express two things:'
  prefs: []
  type: TYPE_NORMAL
- en: The interrelationship between the features of the input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which features are the most important when identifying patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Theoretical example ‚Äì RBM recommender system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of movies, we can use RBMs to uncover a set of latent factors
    that represent their genre, and consequently determine which genre of film a person
    likes. For example, if we were to ask someone to tell us which movies they have
    watched and whether they liked them or not, we can then represent them as binary
    inputs (1 or 0) to the RBM. For those movies they haven't seen or haven't told
    us about, we need to assign a value of -1 so that the network can identify those
    during training and ignore their associated weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example where a user likes *Mrs. Doubtfire*, *The Hangover*,
    and *Bridesmaids*, does not like *Scream* or *Psycho*, and has not yet seen *The
    Hobbit*. Given these inputs, the RBM may identify three hidden factors: comedy,
    horror, and fantasy, which correspond to the genres of the films:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d8e29ec-2944-4f90-a8c8-aaab69ca68c5.png)'
  prefs: []
  type: TYPE_IMG
- en: For each hidden neuron, the RBM assigns a probability of the hidden neuron given
    the input neuron. The final binary values of the neurons are obtained by sampling
    from the Bernoulli distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, the only hidden neuron that represents the comedy¬†genre¬†becomes
    active. As such, given the movie ratings input into the RBM, it predicts that
    the user likes comedy films the most.
  prefs: []
  type: TYPE_NORMAL
- en: For the trained RBM to make predictions on movies the user has not yet seen,
    based on their preference, the RBM uses the probability of the visible neurons
    given the hidden neurons. It samples from the Bernoulli distribution to find out
    which one of the visible neurons can then become active.
  prefs: []
  type: TYPE_NORMAL
- en: Coded example ‚Äì RBM recommender system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuing in the context of movies, we will¬†now give an example of how how
    we can build an RBM recommender system using the PyTorch library. The goal of
    the example is to train a model to determine whether a user will like a movie
    or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we use the MovieLens dataset ([https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/))
    with 1 million ratings, which was created by the GroupLens Research Group at the
    University of Minnesota:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, download the datasets. This can be done through terminal commands
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now import the libraries that we will use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then import the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot illustrates the structure of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/272925c4-059f-410c-b3f1-0833472cddf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Prepare the testing and training datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need to prepare a matrix with the users'' ratings. The matrix will have
    users as rows and movies as columns. Zeros are used to represent cases where a
    user didn''t rate a particular movie. We define the `no_users` and `no_movies`
    variables¬†then consider the `max`¬†value in the training and testing datasets as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define a function named `convert_dataset` that converts the dataset
    into a matrix. It does so by creating a loop that will run through the dataset
    and fetch all of the movies rated by a specific user along with the ratings by
    that same user. We firstly create a matrix of zeros since there are movies the
    user didn''t rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we convert the data into Torch tensors by using the `FloatTensor` utility.
    This will convert the dataset into PyTorch arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we want to make a binary classification, which is whether
    the user will like the movie or not. As such, we convert the ratings into zeros
    and ones. First, however, we replace the existing zeros with -1 in order to represent
    movies that a user never rated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to create a class in order to define the architecture of the RBM.
    The class initializes the weight and bias by using a random normal distribution.
    Two types of biases are also defined, where `a` is the probability of the hidden
    nodes given the visible nodes and `b` is the probability of the visible nodes
    given the hidden nodes. The class creates a `sample_hidden_nodes`¬†function¬†that
    takes `x` as an argument and represents the visible neurons. From here, we compute
    the probability of `h` given `v`, where `h` and `v` represent the hidden and visible
    nodes respectively. This represents the sigmoid activation function. It is computed
    as the product of the vector of the weights and `x` plus the bias `a`. Since we
    are considering the model for binary classification, we return Bernoulli samples
    of the hidden neurons. From here, we create a `sample_visible_function` function¬†that
    will sample the visible nodes. Finally, we create the training function. It takes
    the input vector containing the movie ratings, the visible nodes obtained after
    *k* samplings, the vector of probabilities, and the probabilities of the hidden
    nodes after *k* samplings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we define our model parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From here, we can train the model for each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the error across epochs during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c613abe5-4900-4baa-bf32-656a44fdf51a.png)'
  prefs: []
  type: TYPE_IMG
- en: This can help us to determine how many epochs we should run the training for.
    It shows that after six epochs, the improved performance rate drops and hence
    we should consider stopping the training at this stage.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen the coded example of implementing a recommender system in RBM,
    now let's briefly walk through a DBN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: DBN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A DBN¬†is a multilayer belief network where each layer is an RBM stacked against
    one another. Apart from the first and final layers of the DBN, each layer serves
    as both a hidden layer to the nodes before it and as the input layer to the nodes
    that come after it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/476edcd1-4181-4f6b-8111-adf963439fc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Two layers in the DBN are connected by a matrix of weights. The top two layers
    of a DBN are undirected, which gives a symmetric connection between them, forming
    an associative memory. The lower two layers have direct connections to the layers
    above. The sense of direction converts associative memory into observed variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/884509b6-a309-48a5-a398-c1a3f3cde68f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The two most significant properties of DBNs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A DBN learns top-down, generative weights via an efficient, layer-by-layer procedure.
    These weights determine how the variables in one layer depend on the layer above.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once training is complete, the values of the hidden variables in each layer
    can be inferred by a single bottom-up pass. The pass begins with a visible data
    vector in the lower layer and uses its generative weights in the opposite direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The probability of a joint configuration network over both visible and hidden
    layers depends on the joint configuration network''s energy compared with the
    energy of all other joint configuration networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b151fd9-0ff8-4e58-b605-3c292d38b857.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the pretraining phase of the DBN has been completed by the RBM stack, a
    feedforward network can then be used for the fine-tuning phase in order to create
    a classifier or simply help cluster unlabeled data in an unsupervised learning
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning aims to find the optimal values of the weights between the layers.
    It tweaks the original features in order to obtain more precise boundaries of
    the classes. In order to help the model associate patterns and features to the
    datasets, a small labeled dataset is used.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning can be applied as a stochastic bottom-up pass and then used to adjust
    the top-down weights. Once the top is reached, recursion is applied to the top
    layer. In order to fine-tune further, we can do a stochastic top-down pass and
    adjust the bottom-up weights.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explained what autoencoders are and different variations
    of them. Throughout the chapter, we gave some coded examples of how how they can
    be applied to the MNIST dataset. We later introduced RBMs and explained how these
    can be developed into a DBN along with some additional examples.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce generative adversarial networks. We will
    show how they can be used to generate both images and text.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following for further information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tutorial on Variational Autoencoders*: [https://arxiv.org/abs/1606.05908](https://arxiv.org/abs/1606.05908)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CS598LAZ ‚Äì Variational Autoencoders*: [http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf](http://slazebni.cs.illinois.edu/spring17/lec12_vae.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Auto-Encoding Variational Bayes*: [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning Book*: [https://www.deeplearningbook.org/contents/autoencoders.html](https://www.deeplearningbook.org/contents/autoencoders.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Fast Learning Algorithm for Deep Belief Nets*: [http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf](http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training restricted Boltzmann machines: An introduction*: [https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495](https://www.sciencedirect.com/science/article/abs/pii/S0031320313002495)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Boltzmann Machines*: [http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf](http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Practical Guide to Training Restricted Boltzmann Machines*: [https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Belief Networks*: [https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8](https://link.springer.com/chapter/10.1007/978-3-319-06938-8_8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Neural Networks:* [https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/](https://www.amazon.co.uk/Hands-Neural-Networks-neural-network-ebook/dp/B07SKDSGB6/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
