- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Preparation for Deep Learning Projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in every **machine learning** (**ML**) project consists of data
    collection and data preparation. As a subset of ML, **deep learning** (**DL**)
    involves the same data processing processes. We will start this chapter by setting
    up a standard DL Python notebook environment using Anaconda. Then, we will provide
    concrete examples for collecting data in various formats (JSON, CSV, HTML, and
    XML). In many cases, the collected data gets cleaned up and preprocessed as it
    consists of unnecessary information or invalid formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will introduce popular techniques in this domain: filling in missing
    values, dropping unnecessary entries, and normalizing the values. Next, you will
    learn common feature extraction techniques: the bag-of-words model, term frequency-inverse
    document frequency, one-hot encoding, and dimensionality reduction. Additionally,
    we will present `matplotlib` and `seaborn`, which are the most popular data visualization
    libraries. Finally, we will cover Docker images, which are snapshots of a working
    environment that minimizes potential compatibility issues by bundling an application
    and its dependencies together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up notebook environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection, data cleaning, and data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting features from data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The supplemental material for this chapter can be downloaded from GitHub at
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2).
  prefs: []
  type: TYPE_NORMAL
- en: Setting up notebook environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is one of the most popular programming languages that’s widely used for
    data analysis. Its advantage comes from dynamic typing and being compile-free.
    With its flexibility, it has become the language that data scientists use the
    most. In this section, we will introduce how to set up a Python environment for
    a DL project using **Anaconda** and **Preferred Installer Program** (**PIP**).
    These tools allow you to create a distinct environment for every project while
    simplifying package management. Anaconda provides a desktop application with a
    GUI called Anaconda Navigator. We will walk you through how to set up a Python
    environment and install popular Python libraries for DL projects such as **TensorFlow**,
    **PyTorch**, **NumPy**, **pandas**, **scikit-learn**, **Matplotlib**, and **Seaborn**.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Python environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python can be installed from [www.python.org/downloads](http://www.python.org/downloads).
    However, Python versions are often available through package managers that are
    provided by the operating system, such as **Advanced Package Tool** (**APT**)
    on Linux and **Homebrew** on macOS. Setting up a Python environment begins with
    installing the necessary packages using PIP, a package management system that
    allows you to install and manage various Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Anaconda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When multiple Python projects have been set up on a machine, separating the
    environments would be ideal as each project may depend on different versions of
    those packages. Anaconda can help you with environment management as it is designed
    for both Python package management and environment management. It allows you to
    create virtual environments where the installed packages are bound to each environment
    that is currently active. In addition, Anaconda goes beyond the boundaries of
    Python, allowing users to install non-Python library dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'First things first, Anaconda can be installed from its official website: [www.anaconda.com](http://www.anaconda.com).
    For completeness, we have described the installation process with pictures, at
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_graphical_installer.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_graphical_installer.md).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It can also be installed directly from the Terminal. Anaconda provides installation
    scripts for each operating system ([repo.anaconda.com/archive](http://repo.anaconda.com/archive)).
    You can simply download the right version of the script for your system and run
    it to get Anaconda installed on your machine. As an example, we will describe
    how to install Anaconda from one of these scripts for macOS: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_zsh.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_zsh.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a DL project using Anaconda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, you should have an Anaconda environment ready to use. Now, we
    will create our first virtual environment and install the necessary packages for
    a DL project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can list the available `conda` environments using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the `bookenv` environment that we created previously. To activate
    this environment, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, deactivation can be achieved by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing a Python package can be done through either `pip install <package
    name>` or `conda install <package name>`. In the following code snippet, first,
    we download NumPy, the fundamental package for scientific computing, via the `pip`
    command. Then, we will install PyTorch via the `conda` command. When installing
    PyTorch, you must provide a version for CUDA, a parallel computing platform and
    programming model that is used for general computing on GPUs. CUDA can speed up
    the DL model training by allowing GPUs to process the computation in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorFlow is another popular package for DL projects. Like PyTorch, TensorFlow
    provides different packages for each version of CUDA. The full list can be found
    online here: [https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu).
    To get all libraries related to DL to work seamlessly together, there must be
    compatibility between the Python version, TensorFlow version, GCC compiler version,
    CUDA version, and Bazel build tool version, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Compatibility matrix for the TensorFlow, Python, GCC, Bazel,
    cuDNN, and CUDA versions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – Compatibility matrix for the TensorFlow, Python, GCC, Bazel, cuDNN,
    and CUDA versions
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to `pip` commands, instead of typing `install` commands repeatedly,
    you can generate a single text file that consists of the necessary packages and
    install all of them in a single command. To achieve this, you can provide the
    filename with the `--requirement` (`-r`) option to the `pip install` command,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Common packages required are listed in the CPU-only environments are listed
    in the sample `requirements.txt` file: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/requirements.txt](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/requirements.txt).
    The main packages in the list are TensorFlow and PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at some useful Anaconda commands. Just as `pip install` can
    be used with the `requirements.txt` file, you can also create an environment with
    a set of packages using a YAML file. In the following example, we are using an
    `env.yml` file to save the list of libraries from an existing environment. Later,
    `env.yml` can be used to create a new environment with the same packages, as presented
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet describes a sample YAML file generated from `conda
    env export`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The main components of this YAML file are the name of the environment (`name`),
    the source of the libraries (`channels`), and the list of libraries (`dependencies`).
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Python is a standard language for data analysis due to its simple syntax
  prefs: []
  type: TYPE_NORMAL
- en: b. Python doesn’t require explicit compilation
  prefs: []
  type: TYPE_NORMAL
- en: c. PIP is used for installing Python packages
  prefs: []
  type: TYPE_NORMAL
- en: d. Anaconda handles both Python package management and environment management
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain how to collect data from various sources.
    Then, we will clean and preprocess the collected data for the following processes.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection, data cleaning, and data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will introduce you to various tasks involved in the process
    of data collection. We will describe how to collect data from multiple sources
    and convert them into a generic form that data scientists can use regardless of
    the underlying task. This process can be broken down into a few parts: data collection,
    data cleaning, and data preprocessing. It is worth mentioning that task-specific
    transformation is considered feature extraction, which will be discussed in the
    following section.'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we will introduce different data collection methods for composing initial
    datasets. Different techniques are necessary, depending on how the raw data is
    formatted. Most datasets are either available online as an HFML file or as a JSON
    object. Some data is stored in **Comma-Separated Values** (**CSV**) format, which
    can easily be loaded through the pandas library, a popular data analysis and manipulation
    tool. Hence, we will mainly focus on collecting HTML and JSON data and saving
    it in CSV format in this section. Additionally, we will present some popular dataset
    repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Crawling web pages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Considered a fundamental component of the web, **HyperText Markup Language**
    (**HTML**) data is easily accessible and consists of diverse information. Consequently,
    the ability to crawl web pages can help you collect large amounts of interesting
    data. In this section, we will use BeautifulSoup, a Python-based web crawling
    library ([https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)).
    As an example, we will demonstrate how to crawl Google Scholar pages and how to
    save the crawled data as a CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, several functions of BeautifulSoup will be used to extract
    the author’s first name, last name, email, research interests, citation count,
    h-index (high index), co-author, and paper titles. The following table shows the
    data that we wish to collect in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 2.1 – Data that can be collected from Google Scholar pages
  prefs: []
  type: TYPE_NORMAL
- en: 'Crawling a web page is a two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilize the requests library to get the HTML data in a `response` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a `BeautifulSoup` object that parses the HTML tags in the `response`
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These two steps can be summarized in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to get the contents of interest from the `BeautifulSoup` object.
    *Table 2.2* summarizes common `BeautifulSoup` functions that let you extract the
    content of the interest from the parsed HTML data. Since our goal in this example
    is to store the collected data as a CSV file, we will simply generate a comma-separated
    string representation of the page and write it to a file. The complete implementation
    can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/google_scholar/google_scholar.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/google_scholar/google_scholar.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides the list of methods required for processing raw
    data from Google Scholar pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 2.2 – Possible feature extraction techniques
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about JSON, another popular raw data format.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting JSON data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'JSON is a language-independent format that stores data as key-value and/or
    key-array fields. Since most programming languages support key-value data structures
    (for example, a Dictionary in Python or a HashMap in Java), JSON is considered
    interchangeable (program independent). The following code snippet shows some sample
    JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Have a look at the Awesome JSON Datasets GitHub repository ([https://github.com/jdorfman/awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets)),
    which contains a list of useful JSON data sources. Also, Public API’s GitHub repository
    ([https://github.com/public-apis/public-apis](https://github.com/public-apis/public-apis))
    consists of a list of web server endpoints where various JSON data can be retrieved.
    Additionally, we provide a script that collects JSON data from an endpoint and
    stores the necessary fields as a CSV file: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/rest/get_rest_api_data.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/rest/get_rest_api_data.py).
    This example uses the Reddit dataset available at [https://www.reddit.com/r/all.json](https://www.reddit.com/r/all.json).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce popular public datasets in the fields of data science.
  prefs: []
  type: TYPE_NORMAL
- en: Popular dataset repositories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides web pages and JSON data, many public datasets can be used for various
    purposes. For example, you can get datasets from popular data hubs such as *Kaggle*
    ([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)) or *MIT Data
    Hub* ([https://datahub.csail.mit.edu/browse/public](https://datahub.csail.mit.edu/browse/public)).
    These public datasets are often used for a wide range of activities by many research
    institutes as well as businesses. Data from varying domains such as healthcare,
    government, biology, and computer science are collected during research and donated
    to the repositories for the greater good. Like how these organizations manage
    and provide diverse datasets, community efforts exist for managing various public
    datasets: [https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets#government).'
  prefs: []
  type: TYPE_NORMAL
- en: Another popular source of datasets is data analytics libraries such as *sklearn*,
    *Keras*, and *TensorFlow*. The list of datasets provided by each library can be
    found at [https://scikit-learn.org/stable/datasets](https://scikit-learn.org/stable/datasets),
    [https://keras.io/api/datasets/](https://keras.io/api/datasets/), and [https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, government organizations also provide many datasets to the public.
    For example, you can find interesting, curated datasets related to COVID in a
    data lake hosted by AWS: [https://dj2taa9i652rf.cloudfront.net](https://dj2taa9i652rf.cloudfront.net/).
    From this list of datasets, you can easily download data on Moderna vaccination
    distribution among different states in CSV format by navigating to the `cdc-moderna-vaccine-distribution`
    page.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have collected an initial dataset, the next step is to clean it
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data cleaning is the process of polishing raw data to keep the entries consistent.
    Common operations include filling up empty fields with default values, removing
    characters that are not alpha-numeric such as `?` or `!`, removing stop words,
    and removing HTML tags such as `<p></p>`. Data cleaning also focuses on retaining
    relevant information from the collected data. For example, a user profile page
    may have a wide range of information, such as a biography, first name, email,
    and affiliations. During the data collection process, target information is extracted
    as-is so that it can be kept in the original HTML or JSON tags. In other words,
    the biographic information that’s been collected might still have HTML tags for
    new lines (`<br>`) or bold (`<b></b>`), which do not add much value to the following
    analysis task. Throughout data cleaning, these unnecessary components should be
    dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Before we discuss individual data cleaning operations, it would be nice to have
    some understanding of DataFrames, table-like data structures provided by the pandas
    library ([https://pandas.pydata.org/](https://pandas.pydata.org/)). They have
    rows and columns, just like a SQL table or an Excel sheet. One of their fundamental
    functionalities is `pandas.read_csv`, which allows you to load a CSV file into
    a DataFrame, as demonstrated in the following code snippet. The `tabulate` library
    is a good pick for displaying the content on a terminal as the DataFrame structures
    the data in a table format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to read a CSV file and print the data
    using the `tabulate` library (in the proceeding example, `tabulate` will mimic
    the format of the Postgres psql CLI as we are using the `tablefmt="psql"` option):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the content of the DataFrame in the preceding
    code snippet after being displayed on a terminal using the `tabulate` library
    (you can view a similar output without the `tabulate` library by using `df_vacc.head(5)`).
    The following screenshot shows the allocation of vaccine doses for each jurisdiction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Loading a CSV file using pandas and displaying the contents
    using tabulate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – Loading a CSV file using pandas and displaying the contents using
    tabulate
  prefs: []
  type: TYPE_NORMAL
- en: The first data cleaning operation we will discuss is filling in missing fields
    with default values.
  prefs: []
  type: TYPE_NORMAL
- en: Filling empty fields with default values
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the Google Scholar data we crawled earlier in this chapter to demonstrate
    how empty fields are filled with default values. After data inspection, you will
    find a few authors that have left their affiliations empty as they are unspecified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – The affiliation column contains missing values (nan)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – The affiliation column contains missing values (nan)
  prefs: []
  type: TYPE_NORMAL
- en: The default value for each field differs based on the context and data type.
    For example, nine to six would be a typical default value for an operation hour,
    and an empty string would be a good choice for a missing middle name. The phrase,
    not applicable (N/A) is often used to explicitly indicate that the fields are
    empty. In our example, we will fill out the empty fields that contain `na` to
    indicate that the values were missing in the original web pages and not missed
    out due to errors throughout the collection process. The technique we will demonstrate
    in this example involves the `pandas` library; the DataFrame has a `fillna` method
    that fills the empty values in the specified value. The `fillna` method accepts
    a parameter value of `True` for updating the object in place without creating
    a copy of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet explains how to fill the missing values in a DataFrame
    using the `fillna` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we loaded a CSV file into a DataFrame and set
    missing affiliation entries with `na`. This operation will be executed in place
    without creating an additional copy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will describe how to remove stop words.
  prefs: []
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stop words are words that do not convey much value from an information retrieval
    perspective. Common English stop words include *its*, *and*, *the*, *for*, and
    *that*. As an example, entry of the research interest fields in Google Scholar
    data, we see *security and privacy preservation for wireless networks*. Words
    such as *and* and *for* are not useful when we interpret the meaning of this text.
    Therefore, removing these words is recommended in **natural language processing**
    (**NLP**) tasks. One of the most popular stop word removal features is provided
    by **Natural Language Toolkit** (**NLTK**), which is a suite of libraries and
    programs for symbolic and statistical NLP. The following are a few words that
    are considered as stop word tokens by the NLTK library:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[''doesn'', "doesn''t", ''hadn'', "hadn''t", ''hasn'', "hasn''t", ''haven'',
    "haven''t", ''isn'', "isn''t", ''ma'', …]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Word tokenization is the process of breaking down a sentence into word tokens
    (word vectors). In general, it gets applied before stop word removal. The following
    code snippets demonstrate how to tokenize the `research_interest` fields of Google
    Scholar data and remove stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we first download the stop words corpus for NLTK with `stopwords.words('english')`
    and remove word tokens that are not in the corpus. The full version is available
    at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/data_preproessing/bag_of_words_tf_idf.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/data_preproessing/bag_of_words_tf_idf.py).
  prefs: []
  type: TYPE_NORMAL
- en: Like stop words, text that is not alpha-numeric does not add much value either.
    Therefore, we will explain how to remove them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Removing text that is not alpha-numeric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Alpha-numeric characters are characters that are neither English alphabet characters
    nor numbers. For example, in the text “*Hi, How are you?*”, there are two non-alpha-numeric
    characters: , and ?. As in the case of stop words, they can be dropped as they
    don’t convey much information about the context. Once these characters are removed,
    the text will read *Hi How are you*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove a set of specific characters, we can use **regular expressions**
    (**regex**). Regex is a sequence of characters that represents a search pattern.
    The following *Table 2.3* shows a few important regex search patterns and explains
    what each means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 2.3 – Key regex search patterns
  prefs: []
  type: TYPE_NORMAL
- en: You can find other useful patterns at [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Python provides a built-in `regex` library that supports finding and removing
    a set of texts that matches the given regular expression. The following code snippet
    shows how to remove characters that are not alphanumeric. The `\W` pattern matches
    any character that is not a word character. `+` after the pattern indicates that
    we would like to keep the preceding element one or more times. Putting them together,
    we will find one or more alphanumeric characters in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As the last data cleaning operation, we will introduce how to drop newline characters
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Removing newlines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, the collected text data may have unnecessary newline characters. In
    many cases, the trailing newline characters can be dropped without any harm, regardless
    of what the following tasks are. Such characters can be easily replaced by empty
    strings using Python’s built-in `replace` functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to remove a newline in text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, `"abc\n"` will turn into `"abc"`.
  prefs: []
  type: TYPE_NORMAL
- en: The cleaned data often gets processed further so that the data represents the
    underlying data better. This process is called data preprocessing. We will take
    a deeper look into this process in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of data preprocessing is to transform cleaned data into a generic
    form suitable for a wide range of data analytics tasks. There is not a clear distinction
    between data cleaning and data preprocessing. As a result, tasks such as replacing
    a set of texts or filling in missing values can be categorized as data cleaning,
    as well as data preprocessing. In this section, we will focus on techniques that
    were not covered in the previous section: normalization, converting text into
    lowercase, converting text into bag-of-words, and applying stemming to words.'
  prefs: []
  type: TYPE_NORMAL
- en: Complete implementations of the following examples can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing).
  prefs: []
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, the values for a field might be represented differently, even though
    they mean the same thing. In the case of Google Scholar data, entries in research
    interests may be in different words, even though they refer to a similar domain.
    For example, data science, ML, and **artificial intelligence** (**AI**) refer
    to the same domain of AI in larger contexts. During the data preprocessing stage,
    we typically normalize them by converting ML and data science into AI, which represents
    the underlying information better. This helps the data science algorithms leverage
    the feature for the target task.
  prefs: []
  type: TYPE_NORMAL
- en: 'As demonstrated in the `normalize.py` script within the example repository,
    normalization for the preceding case can easily be achieved by keeping a dictionary
    that maps the expected value to the normalized value. In the following code snippet,
    `artificial_intelligence` will be the normalized value for the `data_science`
    and `machine_learning` features for `research_interests`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The numeric values of a field also require normalization. For numeric values,
    normalization would be the process of rescaling each value into a specific range.
    In the following example, we are scaling each mean count of weekly vaccine distributions
    per state between 0 and 1\. First, we calculate the mean counts for each state.
    Then, we compute the normalized mean count by dividing the mean counts by the
    maximum mean count:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of normalization can be seen in the following screenshot. The table
    in this screenshot consists of two columns – the mean vaccine count before normalization
    and after normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Normalized mean vaccine distribution per state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Normalized mean vaccine distribution per state
  prefs: []
  type: TYPE_NORMAL
- en: The next data preprocessing we will introduce is case conversion for text data.
  prefs: []
  type: TYPE_NORMAL
- en: Case conversion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many cases, text data gets converted into lowercase or uppercase as a way
    of normalization. This brings some level of consistency, especially when the following
    tasks involve comparisons. In the stop words removal example, word tokens in the
    `curr_res_int_tok` variable are searched within the standard English stop words
    of the NLTK library. For the comparison to be successful, the case should be consistent.
    In the following code snippet, the tokens get converted into lowercase before
    the stop word search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Another example can be found in `get_rest_api_data.py`, where we have collected
    and processed data from Reddit. In the following code snippet taken from the script,
    every text field gets converted into lowercase upon collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Next, you will learn how stemming can improve the quality of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Stemming is the process of transforming a word into its root word. The benefit
    of stemming comes from keeping the words consistent if their underlying meaning
    is the same. For example, “*information*”, “*informs*”, and “*informed*” have
    the same root word – “*inform”*. The following example shows how to utilize the
    NLTK library for stemming. The NLTK library offers a stemming feature based on
    *Porter stemming algorithm (Porter, Martin F. “An algorithm for suffix stripping.”
    Program (1980))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, we instantiated `PosterStemmer` from the `nltk.stem`
    library and passed the text into the `stem` function.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Data comes in different formats such as JSON, CSV, HTML, and XML. There are
    many data collection tools available for each type of data.
  prefs: []
  type: TYPE_NORMAL
- en: b. Data cleaning is the process of polishing raw data to keep each entry consistent.
    Common operations include filling up empty features with default values, removing
    characters that are not alphanumeric, removing stop words, and removing unnecessary
    tags.
  prefs: []
  type: TYPE_NORMAL
- en: c. The goal of data preprocessing is to apply generic data augmentation to transform
    cleaned data into a form that is generic for any data analytic task.
  prefs: []
  type: TYPE_NORMAL
- en: d. The domain of data cleaning and data preprocessing overlaps, which means
    that some operations can be used for either process.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have discussed the generic processes for data preparation. Next,
    we will discuss the final process: feature extraction. Unlike the other processes
    we have covered, feature extraction involves task-specific operations. Let’s take
    a closer look.'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Feature extraction** (**feature engineering**) is the process of transforming
    data into features that express the underlying information in a specific way for
    the target task. Data preprocessing applies generic techniques that are often
    necessary for most data analytics tasks. However, feature extraction requires
    you to exploit domain knowledge as it is specific to the task. In this section,
    we will introduce popular feature extraction techniques, including bag-of-words
    for text data, term frequency-inverse document frequency, converting color images
    into gray images, ordinal encoding, one-hot encoding, dimensionality reduction,
    and fuzzy match for comparing two strings.'
  prefs: []
  type: TYPE_NORMAL
- en: Complete implementations of these examples can be found online at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing).
  prefs: []
  type: TYPE_NORMAL
- en: First, we will start with the bag-of-words technique.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text using bag-of-words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`CountVectorizer` class from Sklearn helps to create BoW from text. The following
    code demonstrates how to use Sklearn features for BoW:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot summarizes the output of BoW in a table format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Output of BoW on three sample documents'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – Output of BoW on three sample documents
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce **term frequency-inverse document frequency** (**TF-IDF**)
    for text data.
  prefs: []
  type: TYPE_NORMAL
- en: Applying term frequency-inverse document frequency (TF-IDF) transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem with using word frequency is that the documents that have higher
    frequency will dominate the model or analysis. Hence, it is better to rescale
    the frequency based on how often a word occurs in all documents. Such scaling
    helps to penalize those highly frequent words (such as *the* and *have*) in a
    way that the numerical representation of the text expresses the context better.
  prefs: []
  type: TYPE_NORMAL
- en: Before introducing the formula for TF-IDF, we must define some notations. Let
    *n* be the total number of documents and *t* be a word (term). *df(t)* refers
    to the document frequency for word *t* (how many documents contain the word *t*),
    while *tf(t, d)* refers to the word *t* frequency in document *d* (how many times
    *t* appears in document *d*). With these definitions, we can define *idf(t)*,
    the inverse document frequency, as *log [ n / df(t) ] + 1*.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, *tf-idf(t, d)*, *tf-idf* for word *t* and document *d* can be represented
    as *tf(t, d) * idf(t)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the sample code script, `bag_of_words_tf_idf.py`, we are using research
    interest fields of Google Scholar data to calculate TF-IDF. Here, we utilize the
    `TfidfVectorizer` function of Sklearn. The `fit_transform` function takes in a
    set of documents and generates a TF-IDF-weighted document-term matrix. From this
    matrix, we can print out the top *N* research interests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we create a `TfidfVectorizer` instance and trigger
    the `fit_transform` function using the list of research interest texts (`research_interest_list`).
    Then, we call the `todense` method on the output to obtain the dense representation
    of the resulting matrix. The matrix gets converted into a DataFrame and sorted
    to display the top entries. The following screenshot shows the output of `df.head(3)`
    – three words with the highest TF-IDF from research interest fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Three words with the highest TF-IDF from research interest fields'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – Three words with the highest TF-IDF from research interest fields
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how to process categorical data using one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Creating one-hot encoding (one-of-k)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encoding (one-of-k) is the process of converting discrete values into
    a sequence of binary values. Let’s start with a simple example, where a field
    can have categorical values of either `cat` or `dog`. The one-hot encoding will
    be represented by two bits, where one bit refers to `cat` and the other bit refers
    to `dog`. The bit in the encoding with a value of 1 means that the field has the
    corresponding value. So, 1 0 represents a cat, while 0 1 represents a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **breed** | **pet_type** | **dog** | **cat** |'
  prefs: []
  type: TYPE_TB
- en: '| Retrievers | dog | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Maine Coon | cat | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| German Shepherd | dog | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.4 – Converting categorical values in pet_type into one-hot encoding
    (dog and cat)
  prefs: []
  type: TYPE_NORMAL
- en: 'A demonstration of one-hot encoding can be found in `one_hot_encoding.py`.
    In the following code snippet, we are focusing on the core operations, which involve
    `OneHotEncoder` from Sklearn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `is_artificial_intelligence` column used in the previous code snippet consists
    of two distinct values: `"yes"` and `"no"`. The following screenshot summarizes
    the results of one-hot encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – One-hot encoding for the is_artificial_intelligence field'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – One-hot encoding for the is_artificial_intelligence field
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce another type of encoding called ordinal
    encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Creating ordinal encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ordinal encoding is the process of converting a categorical value into a numerical
    value. In *Table 2.5*, there are two types for pets, dog and cat. Dogs are assigned
    a value of 1 and cats are assigned a value of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **breed** | **pet_type** | **ordinal_encoding** |'
  prefs: []
  type: TYPE_TB
- en: '| Retrievers | dog | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Maine Coon | cat | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| German Shepherd | dog | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.5 – The categorical values in pet_type field encoded as ordinal in ordinal_encoding
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we are using the `LabelEncoder` class from Sklearn
    to transform research interest fields into numerical values. A complete example
    of ordinal encoding can be found in `ordinal_encoding.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet is almost self-explanatory – we simply construct
    a `LabelEncoder` instance and pass the target column to the `fit_transform` method.
    The following screenshot shows the first three rows of the resulting DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Results of ordinal encoding on research interest'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Results of ordinal encoding on research interest
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will explain a technique for images: converting a colored image into
    a grayscale image.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting a colored image into a grayscale image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most common techniques in a computer vision task is to convert a
    colored image into a grayscale image. *OpenCV* is a standard library for image
    processing ([https://opencv.org/](https://opencv.org/)). In the following example,
    we are simply importing the OpenCV library (`import cv2`) and using the `cvtColor`
    function to convert a loaded image into grayscale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When analyzing a large volume of data with multiple fields, you often find that
    reducing the number of dimensions is necessary. In the next section, we will look
    at the available options for this process.
  prefs: []
  type: TYPE_NORMAL
- en: Performing dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, there are more features than what the task needs; not all features
    have useful information. In this case, you can use dimensionality reduction techniques
    such as **Principal Component Analysis** (**PCA**), **Singular Value Decomposition**
    (**SVD**), **Linear Discriminant Analysis** (**LDA**), **t-SNE**, **UMAP**, and
    **ISOMAP** to name a few. Another option is to use DL. You can build a custom
    model for dimensionality reduction or use a pre-defined network structure such
    as **AutoEncoder**. In this section, we will describe PCA in detail as it is the
    most popular technique among the ones we mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a set of features, PCA identifies relationships among the features and
    generates a new set of variables that capture the differences in the samples in
    the most efficient way*. These new variables are called principal components and
    are ranked in order of importance; while constructing the first principal component,
    it squeezes the unimportant variables and leaves them for the second principal
    component. Therefore, the first principal component is not correlated to the remaining
    variables. This process gets repeated to construct principal components of the
    following order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to describe the PCA process more formally, we can say that the process
    has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Constructs a covariance matrix that represents the correlations for every pair
    of features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generates a new set of features that captures different amounts of information
    by calculating the eigenvalues of the covariance matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The new set of features is principal components. By sorting the corresponding
    eigenvalues from highest to lowest, you would get the most useful new feature
    at the top.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the details, we will look at the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)).
    This dataset consists of three classes of Iris plant (setosa, versicolor, and
    virginica), along with four features (sepal width, sepal length, petal width,
    and petal length). In the following diagram, we plot each entry using the two
    new features constructed from PCA. Based on this diagram, we can easily conclude
    that we only need the top two principal components to distinguish the three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – The results of PCA on the Iris dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 – The results of PCA on the Iris dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will use human resources data from Kaggle to demonstrate
    PCA. The initial set of data consists of multiple fields such as salary, whether
    there was a promotion within the last five years or not, and whether an employee
    left the company or not. Once principal components are constructed, they can be
    plotted using matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, first, we loaded the data using the `read_csv`
    function of the pandas library, normalized the entries using `StandardScaler`
    from Sklearn, and applied PCA using Sklearn. The complete example can be found
    at `pca.py`.
  prefs: []
  type: TYPE_NORMAL
- en: As the last technique for feature extraction, we will explain how to effectively
    calculate a distance metric between two sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Applying fuzzy matching to handle similarity between strings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Fuzzy matching** ([https://pypi.org/project/fuzzywuzzy/](https://pypi.org/project/fuzzywuzzy/))
    uses a distance metric that measures the differences between two sequences and
    treats them equally if they can be considered similar. In this section, we will
    demonstrate how fuzzy matching can be applied using *Levenshtein Distance* (Levenshtein,
    Vladimir I. (February 1966). "Binary codes capable of correcting deletions, insertions,
    and reversals". Soviet Physics Doklady. 10 (8): 707–710\. Bibcode: 1966SPhD...10..707L).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular library for fuzzy string matching is `fuzzywuzzy`. The `ratio`
    function will provide the Levenshtein distance score, which we can use to decide
    whether we want to consider the two strings the same for the following process.
    The following code snippet describes the usage of the `ratio` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding example, the `ratio` function will output a higher
    score if the two texts are more similar.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. **Feature extraction** is the process of transforming data into features
    that express the underlying information better for the target task.
  prefs: []
  type: TYPE_NORMAL
- en: b. BoW is a representation of a document based on the occurrence of the words.
    TF-IDF can express the context of a document better by penalizing highly frequent
    words.
  prefs: []
  type: TYPE_NORMAL
- en: c. A colored image can be updated to a grayscale image using the OpenCV library.
  prefs: []
  type: TYPE_NORMAL
- en: d. Categorical features can be represented numerically using ordinal encoding
    or one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: e. When a dataset has too many features, dimensionality reduction can reduce
    the number of features that have the most information. PCA constructs new features
    while retaining most of the information.
  prefs: []
  type: TYPE_NORMAL
- en: f. When evaluating the similarity between two texts, you can apply fuzzy matching
    drop.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been transformed into a reasonable format, you will often
    need to visualize the data to understand its characteristics. In the next section,
    we will introduce popular libraries for data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Performing data visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When applying ML techniques to analyze a dataset, the first step must be understanding
    the available data because every algorithm has advantages that are closely related
    to the underlying data. The key aspects of data that data scientists need to understand
    include data formats, distributions, and relationships among the features. When
    the amount of data is small, necessary information can be collected by analyzing
    each entry manually. However, as the amount of data grows, visualization plays
    a critical role in understanding the data.
  prefs: []
  type: TYPE_NORMAL
- en: Many tools for data visualization are available in Python. **Matplotlib** and
    **Seaborn** are the most popular libraries for statistical data visualization.
    We will introduce these two libraries one by one in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Performing basic visualizations using Matplotlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following example, we will demonstrate how to generate bar charts and
    pie charts using Matplotlib. The data we use represents the weekly distribution
    of COVID vaccines. To use the `matplot` functionality, you must import the package
    first (`import matplotlib.pyplot as plt`). The `plt.bar` function takes the list
    of top 10 state names and a list of its mean distribution to generate a bar chart.
    Similarly, the `plt.pie` function is used to generate a pie chart from a dictionary.
    The `plt.figure` function resizes the plot size and allows users to draw multiple
    charts on the same canvas. The complete implementation can be found at `visualize_matplotlib.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Bar and pie charts generated using Matplotlib'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – Bar and pie charts generated using Matplotlib
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce `Seaborn`, another popular data visualization
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing statistical graphs using Seaborn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Seaborn` is a library built on top of `Matplotlib` to provide a high-level
    interface for drawing statistical graphics that `Matplotlib` does not support.
    In this section, we will learn how to generate line graphs and histograms using
    `Seaborn` for the same dataset. First, we need to import the `Seaborn` library
    along with `Matplotlib` (`import seaborn as sns`). The `sns.line_plot` function
    is designed to accept a DataFrame and column names. Therefore, we must provide
    `df_mean_sorted_top10`, which contains the top 10 states of the highest mean values
    of vaccines distributed and two column names, `state_names` and `count_vaccine`,
    for the *X* and *Y* axes. To plot the histogram, you can use the `sns.dist_plot`
    function, which takes in a DataFrame with a column value for the *Y* axis. If
    we are to use the same mean values, it would be `sns.displot(df_mean_sorted_top10[''count_vaccine''],
    kde=False)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting graphs are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Line graph and histogram generated using Seaborn'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Line graph and histogram generated using Seaborn
  prefs: []
  type: TYPE_NORMAL
- en: The complete implementation can be found in this book’s GitHub repository (`visualize_seaborn.py`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Many libraries can be used for enhanced visualizations: **pyROOT**, a data
    analysis framework from CERN that’s commonly used for research projects ([https://root.cern/manual/python](https://root.cern/manual/python/)),
    **Streamlit**, for easy web app creation ([https://streamlit.io](https://streamlit.io/)),
    **Plotly**, a free open source graphing library ([https://plotly.com](https://plotly.com/)),
    and **Bokeh**, for interactive web visualizations ([https://docs.bokeh.org/en/latest](https://docs.bokeh.org/en/latest/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Visualizing data helps you analyze and understand data that is critical for
    selecting the right machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: b. Matplotlib and Seaborn are the most popular data visualization tools. Other
    tools include pyRoot, Streamlit, Plotly, and Bokeh.
  prefs: []
  type: TYPE_NORMAL
- en: The last section of this chapter will describe Docker, which allows you to achieve
    **operating system** (**OS**)-level virtualization for your project.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, *Setting up notebook environments*, you learned how
    to set up a virtual environment with various packages for DL using `conda` and
    `pip` commands. Furthermore, you know how to save an environment into a YAML file
    and recreate the same environment. However, projects based on virtual environments
    may not be sufficient when the environment needs to be replicated on multiple
    machines as there can be issues coming from non-obvious OS-level dependencies.
    In this situation, Docker would be a great solution. Using Docker, you can create
    a snapshot of your working environment, including the underlying version of your
    OS. Altogether, Docker allows you to separate your applications from your infrastructure
    so that you can deliver your software quickly. Installing Docker can be achieved
    by following the instructions at [https://www.docker.com/get-started](https://www.docker.com/get-started).
    In this book, we will use version 3.5.2.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will introduce a Docker image, a representation of a virtual
    environment in the context of Docker, and explain how to create a Dockerfile for
    the target Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Dockerfiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Docker images are created by so-called Dockerfiles. Every Docker image has
    a base (or parent) image. For DL environments, a good choice for the base image
    would be an image developed for Linux Ubuntu OS – one of the following should
    be a good choice: *ubuntu:18.04* ([https://releases.ubuntu.com/18.04](https://releases.ubuntu.com/18.04))
    or *ubuntu:20.04* ([https://releases.ubuntu.com/20.04](https://releases.ubuntu.com/20.04/)).
    Along with an image for the underlying OS, there are images with specific packages
    already installed. For example, the simplest way to set up a TensorFlow-based
    environment is to download images with TensorFlow installed. A base image has
    been created by TensorFlow developers and can be easily downloaded by using `docker
    pull tensorflow/serving` command ([https://hub.docker.com/r/tensorflow/serving](https://hub.docker.com/r/tensorflow/serving)).
    An environment with PyTorch is also available: [https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how to build with a custom Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Building a custom Docker image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a custom image is also straightforward. However, it involves many commands
    for which we will relegate the details to [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles).
    Once you have built the Docker image, you can instantiate it with something known
    as a Docker container. A Docker container is a standalone executable package of
    software that includes everything that you need to run the target application.
    By following the instructions in the `README.md` file, you can create the Docker
    image, which will run a containerized Jupyter notebook service with the standard
    libraries we described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  prefs: []
  type: TYPE_NORMAL
- en: a. Docker creates a snapshot of your working environment, including the underlying
    OS. The created image can be used to recreate the same environment on different
    machines.
  prefs: []
  type: TYPE_NORMAL
- en: b. Docker helps you separate your environment from infrastructure. This allows
    you to move your applications to different cloud service providers (such as AWS
    or Google Cloud) with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should be able to create a Docker image for your DL project.
    By instantiating the Docker image, you should be able to collect the data you
    need and process it as needed on your local machine or various cloud service providers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described how to prepare a dataset for data analytics tasks.
    The first key point was how to achieve environment virtualization using Anaconda
    and Docker, along with Python package management using `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data preparation process can be broken down into four steps: data collection,
    data cleaning, data preprocessing, and feature extraction. First, we have introduced
    various tools available for data collection that support different data types.
    Once the data has been collected, it is cleaned and preprocessed so that it can
    be transformed into a generic form. Depending on the target task, we often apply
    various feature extraction techniques that are task-specific. In addition, we
    have introduced many tools for data visualization that can help you understand
    the characteristics of the prepared data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have learned how to prepare our data for analytics tasks, in the
    next chapter, we will explain DL model development. We will introduce the basic
    concepts and how to use the two most popular DL frameworks: TensorFlow and PyTorch.'
  prefs: []
  type: TYPE_NORMAL
