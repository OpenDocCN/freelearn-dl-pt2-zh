- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Data Preparation for Deep Learning Projects
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备深度学习项目的数据
- en: The first step in every **machine learning** (**ML**) project consists of data
    collection and data preparation. As a subset of ML, **deep learning** (**DL**)
    involves the same data processing processes. We will start this chapter by setting
    up a standard DL Python notebook environment using Anaconda. Then, we will provide
    concrete examples for collecting data in various formats (JSON, CSV, HTML, and
    XML). In many cases, the collected data gets cleaned up and preprocessed as it
    consists of unnecessary information or invalid formats.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）项目的第一步是数据收集和数据准备。作为机器学习的一个子集，**深度学习**（**DL**）涉及相同的数据处理过程。我们将从使用
    Anaconda 设置标准的 DL Python 笔记本环境开始这一章节。然后，我们将提供在各种格式（JSON、CSV、HTML 和 XML）中收集数据的具体示例。在许多情况下，收集到的数据会进行清理和预处理，因为它包含不必要的信息或无效的格式。'
- en: 'The chapter will introduce popular techniques in this domain: filling in missing
    values, dropping unnecessary entries, and normalizing the values. Next, you will
    learn common feature extraction techniques: the bag-of-words model, term frequency-inverse
    document frequency, one-hot encoding, and dimensionality reduction. Additionally,
    we will present `matplotlib` and `seaborn`, which are the most popular data visualization
    libraries. Finally, we will cover Docker images, which are snapshots of a working
    environment that minimizes potential compatibility issues by bundling an application
    and its dependencies together.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍该领域中的流行技术：填补缺失值、删除不必要的条目和归一化值。接下来，您将学习常见的特征提取技术：词袋模型、词频-逆文档频率、独热编码和降维。此外，我们将介绍最流行的数据可视化库
    `matplotlib` 和 `seaborn`。最后，我们将介绍 Docker 镜像，这些镜像是工作环境的快照，通过将应用程序及其依赖项捆绑在一起，最大限度地减少潜在的兼容性问题。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Setting up notebook environments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置笔记本环境
- en: Data collection, data cleaning, and data preprocessing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集、数据清洗和数据预处理
- en: Extracting features from data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据中提取特征
- en: Performing data visualization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行数据可视化
- en: Introduction to Docker
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 简介
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The supplemental material for this chapter can be downloaded from GitHub at
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的补充材料可以从 GitHub 上下载，网址为 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2)。
- en: Setting up notebook environments
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置笔记本环境
- en: Python is one of the most popular programming languages that’s widely used for
    data analysis. Its advantage comes from dynamic typing and being compile-free.
    With its flexibility, it has become the language that data scientists use the
    most. In this section, we will introduce how to set up a Python environment for
    a DL project using **Anaconda** and **Preferred Installer Program** (**PIP**).
    These tools allow you to create a distinct environment for every project while
    simplifying package management. Anaconda provides a desktop application with a
    GUI called Anaconda Navigator. We will walk you through how to set up a Python
    environment and install popular Python libraries for DL projects such as **TensorFlow**,
    **PyTorch**, **NumPy**, **pandas**, **scikit-learn**, **Matplotlib**, and **Seaborn**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是一种最流行的编程语言之一，广泛用于数据分析。其优势在于动态类型和无需编译。凭借其灵活性，它已成为数据科学家最常用的语言。在本节中，我们将介绍如何使用
    **Anaconda** 和 **Preferred Installer Program**（**PIP**）为 DL 项目设置 Python 环境。这些工具允许您为每个项目创建独立的环境，同时简化包管理。Anaconda
    提供了一个名为 Anaconda Navigator 的带有 GUI 的桌面应用程序。我们将指导您如何设置 Python 环境，并安装用于 DL 项目的流行
    Python 库，如 **TensorFlow**、**PyTorch**、**NumPy**、**pandas**、**scikit-learn**、**Matplotlib**
    和 **Seaborn**。
- en: Setting up a Python environment
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Python 环境
- en: Python can be installed from [www.python.org/downloads](http://www.python.org/downloads).
    However, Python versions are often available through package managers that are
    provided by the operating system, such as **Advanced Package Tool** (**APT**)
    on Linux and **Homebrew** on macOS. Setting up a Python environment begins with
    installing the necessary packages using PIP, a package management system that
    allows you to install and manage various Python packages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可以从 [www.python.org/downloads](http://www.python.org/downloads) 安装。但是，Python
    版本通常可以通过操作系统提供的包管理器（例如 Linux 上的 **高级包管理工具** (**APT**) 和 macOS 上的 **Homebrew**）获取。建立
    Python 环境的第一步是使用 PIP 安装必要的软件包，PIP 是一个允许您安装和管理各种 Python 软件包的软件包管理系统。
- en: Installing Anaconda
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Anaconda
- en: When multiple Python projects have been set up on a machine, separating the
    environments would be ideal as each project may depend on different versions of
    those packages. Anaconda can help you with environment management as it is designed
    for both Python package management and environment management. It allows you to
    create virtual environments where the installed packages are bound to each environment
    that is currently active. In addition, Anaconda goes beyond the boundaries of
    Python, allowing users to install non-Python library dependencies.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当在计算机上设置了多个 Python 项目时，将环境分开可能是理想的，因为每个项目可能依赖于这些软件包的不同版本。Anaconda 可以帮助您管理环境，因为它既设计用于
    Python 软件包管理，又设计用于环境管理。它允许您创建虚拟环境，其中安装的软件包绑定到当前活动的每个环境。此外，Anaconda 超越了 Python
    的界限，允许用户安装非 Python 库依赖项。
- en: 'First things first, Anaconda can be installed from its official website: [www.anaconda.com](http://www.anaconda.com).
    For completeness, we have described the installation process with pictures, at
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_graphical_installer.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_graphical_installer.md).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，可以从其官方网站 [www.anaconda.com](http://www.anaconda.com) 安装 Anaconda。为了完整起见，我们已经在
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_graphical_installer.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_graphical_installer.md)
    上用图片描述了安装过程。
- en: 'It can also be installed directly from the Terminal. Anaconda provides installation
    scripts for each operating system ([repo.anaconda.com/archive](http://repo.anaconda.com/archive)).
    You can simply download the right version of the script for your system and run
    it to get Anaconda installed on your machine. As an example, we will describe
    how to install Anaconda from one of these scripts for macOS: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_zsh.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_zsh.md).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以直接从终端安装。Anaconda 为每个操作系统提供安装脚本（[repo.anaconda.com/archive](http://repo.anaconda.com/archive)）。您可以简单地下载适合您系统的脚本的正确版本，并运行它来在您的计算机上安装
    Anaconda。例如，我们将描述如何从这些脚本之一为 macOS 安装 Anaconda：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_zsh.md](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/anaconda_zsh.md)。
- en: Setting up a DL project using Anaconda
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Anaconda 设置 DL 项目
- en: 'At this point, you should have an Anaconda environment ready to use. Now, we
    will create our first virtual environment and install the necessary packages for
    a DL project:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，您应该已经准备好使用 Anaconda 环境。现在，我们将创建我们的第一个虚拟环境，并安装 DL 项目所需的必要软件包：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can list the available `conda` environments using the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令列出可用的 `conda` 环境：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should see the `bookenv` environment that we created previously. To activate
    this environment, you can use the following command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到我们之前创建的 `bookenv` 环境。要激活此环境，您可以使用以下命令：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly, deactivation can be achieved by using the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，通过以下命令可以实现去激活：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Installing a Python package can be done through either `pip install <package
    name>` or `conda install <package name>`. In the following code snippet, first,
    we download NumPy, the fundamental package for scientific computing, via the `pip`
    command. Then, we will install PyTorch via the `conda` command. When installing
    PyTorch, you must provide a version for CUDA, a parallel computing platform and
    programming model that is used for general computing on GPUs. CUDA can speed up
    the DL model training by allowing GPUs to process the computation in parallel:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `pip` 命令可以安装 Python 包。可以通过 `conda install <package name>` 或 `pip install
    <package name>` 安装 Python 包。在下面的代码片段中，首先我们通过 `pip` 命令下载了 NumPy，这是科学计算的基础包。然后，我们将通过
    `conda` 命令安装 PyTorch。在安装 PyTorch 时，你必须为 CUDA 提供版本，CUDA 是一种用于通用计算的并行计算平台和编程模型。CUDA
    可以通过允许 GPU 并行处理来加速 DL 模型训练：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'TensorFlow is another popular package for DL projects. Like PyTorch, TensorFlow
    provides different packages for each version of CUDA. The full list can be found
    online here: [https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu).
    To get all libraries related to DL to work seamlessly together, there must be
    compatibility between the Python version, TensorFlow version, GCC compiler version,
    CUDA version, and Bazel build tool version, as shown in the following screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是另一个用于深度学习项目的流行包。与 PyTorch 类似，TensorFlow 为每个 CUDA 版本提供不同的包。完整列表可以在这里找到：[https://www.tensorflow.org/install/source#gpu](https://www.tensorflow.org/install/source#gpu)。为了使所有与深度学习相关的库能够无缝工作，必须保证
    Python 版本、TensorFlow 版本、GCC 编译器版本、CUDA 版本和 Bazel 构建工具版本之间的兼容性，如下图所示：
- en: '![Figure 2.1 – Compatibility matrix for the TensorFlow, Python, GCC, Bazel,
    cuDNN, and CUDA versions'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1 – TensorFlow、Python、GCC、Bazel、cuDNN 和 CUDA 版本的兼容矩阵'
- en: '](img/B18522_02_1.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_02_1.jpg)'
- en: Figure 2.1 – Compatibility matrix for the TensorFlow, Python, GCC, Bazel, cuDNN,
    and CUDA versions
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 – TensorFlow、Python、GCC、Bazel、cuDNN 和 CUDA 版本的兼容矩阵
- en: 'Going back to `pip` commands, instead of typing `install` commands repeatedly,
    you can generate a single text file that consists of the necessary packages and
    install all of them in a single command. To achieve this, you can provide the
    filename with the `--requirement` (`-r`) option to the `pip install` command,
    as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 `pip` 命令，你可以通过生成一个文本文件来安装所有必需的包，并在单个命令中安装它们，而不是重复输入 `install` 命令。为了实现这一点，可以将
    `--requirement` (`-r`) 选项与 `pip install` 命令一起使用，如下所示：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Common packages required are listed in the CPU-only environments are listed
    in the sample `requirements.txt` file: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/requirements.txt](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/requirements.txt).
    The main packages in the list are TensorFlow and PyTorch.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: CPU-only 环境中列出的常见所需包在示例 `requirements.txt` 文件中列出：[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/requirements.txt](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/anaconda/requirements.txt)。列表中的主要包括
    TensorFlow 和 PyTorch。
- en: 'Now, let’s look at some useful Anaconda commands. Just as `pip install` can
    be used with the `requirements.txt` file, you can also create an environment with
    a set of packages using a YAML file. In the following example, we are using an
    `env.yml` file to save the list of libraries from an existing environment. Later,
    `env.yml` can be used to create a new environment with the same packages, as presented
    in the following code snippet:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一些有用的 Anaconda 命令。就像 `pip install` 可以与 `requirements.txt` 文件一起使用一样，你也可以使用
    YAML 文件创建一个包含一组包的环境。在以下示例中，我们使用了一个 `env.yml` 文件来保存现有环境中的库列表。稍后，可以使用 `env.yml`
    创建一个具有相同包的新环境，如下代码片段所示：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following code snippet describes a sample YAML file generated from `conda
    env export`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段描述了从 `conda env export` 生成的样本 YAML 文件：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The main components of this YAML file are the name of the environment (`name`),
    the source of the libraries (`channels`), and the list of libraries (`dependencies`).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此 YAML 文件的主要组件是环境的名称 (`name`)、库的来源 (`channels`) 和库的列表 (`dependencies`)。
- en: Things to remember
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事情
- en: a. Python is a standard language for data analysis due to its simple syntax
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: a. Python 是由于其简单的语法而成为数据分析的标准语言
- en: b. Python doesn’t require explicit compilation
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: b. Python 不需要显式编译
- en: c. PIP is used for installing Python packages
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: c. PIP 用于安装 Python 包
- en: d. Anaconda handles both Python package management and environment management
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: d. Anaconda 可同时处理 Python 包管理和环境管理
- en: In the next section, we will explain how to collect data from various sources.
    Then, we will clean and preprocess the collected data for the following processes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将解释如何从各种来源收集数据。然后，我们将对收集到的数据进行清理和预处理，以供后续处理使用。
- en: Data collection, data cleaning, and data preprocessing
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集、数据清理和数据预处理
- en: 'In this section, we will introduce you to various tasks involved in the process
    of data collection. We will describe how to collect data from multiple sources
    and convert them into a generic form that data scientists can use regardless of
    the underlying task. This process can be broken down into a few parts: data collection,
    data cleaning, and data preprocessing. It is worth mentioning that task-specific
    transformation is considered feature extraction, which will be discussed in the
    following section.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您介绍数据收集过程中涉及的各种任务。我们将描述如何从多个来源收集数据，并将其转换为数据科学家可以使用的通用形式，而不论底层任务是什么。这个过程可以分解为几个部分：数据收集、数据清理和数据预处理。值得一提的是，任务特定的转换被认为是特征提取，这将在下一节中讨论。
- en: Collecting data
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 收集数据
- en: First, we will introduce different data collection methods for composing initial
    datasets. Different techniques are necessary, depending on how the raw data is
    formatted. Most datasets are either available online as an HFML file or as a JSON
    object. Some data is stored in **Comma-Separated Values** (**CSV**) format, which
    can easily be loaded through the pandas library, a popular data analysis and manipulation
    tool. Hence, we will mainly focus on collecting HTML and JSON data and saving
    it in CSV format in this section. Additionally, we will present some popular dataset
    repositories.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍用于构建初始数据集的不同数据收集方法。根据原始数据的格式，需要使用不同的技术。大多数数据集要么作为 HFML 文件在线可用，要么作为 JSON
    对象存储。一些数据以 **逗号分隔值**（**CSV**）格式存储，可以通过流行的数据分析和操作工具 pandas 轻松加载。因此，我们将主要专注于在本节中收集
    HTML 和 JSON 数据，并将其保存为 CSV 格式。此外，我们还将介绍一些流行的数据集库。
- en: Crawling web pages
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 爬取网页
- en: Considered a fundamental component of the web, **HyperText Markup Language**
    (**HTML**) data is easily accessible and consists of diverse information. Consequently,
    the ability to crawl web pages can help you collect large amounts of interesting
    data. In this section, we will use BeautifulSoup, a Python-based web crawling
    library ([https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)).
    As an example, we will demonstrate how to crawl Google Scholar pages and how to
    save the crawled data as a CSV file.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Web 的基本组成部分，**超文本标记语言**（**HTML**）数据易于访问，包含多样化的信息。因此，通过爬取网页可以帮助您收集大量有趣的数据。在本节中，我们将使用BeautifulSoup，这是一个基于
    Python 的网络爬虫库（[https://www.crummy.com/software/BeautifulSoup/](https://www.crummy.com/software/BeautifulSoup/)）。作为示例，我们将演示如何爬取
    Google 学术页面，并将爬取的数据保存为 CSV 文件。
- en: 'In this example, several functions of BeautifulSoup will be used to extract
    the author’s first name, last name, email, research interests, citation count,
    h-index (high index), co-author, and paper titles. The following table shows the
    data that we wish to collect in this example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，将使用BeautifulSoup的多个函数来提取作者的名字、姓氏、电子邮件、研究兴趣、引用次数、h指数（高指数）、共同作者和论文标题。下表显示了我们希望在此示例中收集的数据：
- en: '![](img/Table_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Table_01.jpg)'
- en: Table 2.1 – Data that can be collected from Google Scholar pages
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 – 可从 Google 学术页面收集的数据
- en: 'Crawling a web page is a two-step process:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 爬取网页是一个两步过程：
- en: Utilize the requests library to get the HTML data in a `response` object.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利用请求库获取 HTML 数据，并存储在 `response` 对象中。
- en: Construct a `BeautifulSoup` object that parses the HTML tags in the `response`
    object.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个 `BeautifulSoup` 对象，用于解析 `response` 对象中的 HTML 标签。
- en: 'These two steps can be summarized in the following code snippet:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤可以用以下代码片段总结：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The next step is to get the contents of interest from the `BeautifulSoup` object.
    *Table 2.2* summarizes common `BeautifulSoup` functions that let you extract the
    content of the interest from the parsed HTML data. Since our goal in this example
    is to store the collected data as a CSV file, we will simply generate a comma-separated
    string representation of the page and write it to a file. The complete implementation
    can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/google_scholar/google_scholar.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/google_scholar/google_scholar.py).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从 `BeautifulSoup` 对象获取感兴趣的内容。 *表 2.2* 总结了常见的 `BeautifulSoup` 函数，允许您从解析的
    HTML 数据中提取感兴趣的内容。 由于我们在这个例子中的目标是将收集的数据存储为 CSV 文件，我们将简单地生成页面的逗号分隔字符串表示，并将其写入文件。
    完整的实现可以在 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/google_scholar/google_scholar.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/google_scholar/google_scholar.py)
    找到。
- en: 'The following table provides the list of methods required for processing raw
    data from Google Scholar pages:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了从 Google Scholar 页面处理原始数据所需的方法列表：
- en: '![](img/Table_02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Table_02.jpg)'
- en: Table 2.2 – Possible feature extraction techniques
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – 可能的特征提取技术
- en: Next, we will learn about JSON, another popular raw data format.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习另一种流行的原始数据格式 JSON。
- en: Collecting JSON data
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集 JSON 数据
- en: 'JSON is a language-independent format that stores data as key-value and/or
    key-array fields. Since most programming languages support key-value data structures
    (for example, a Dictionary in Python or a HashMap in Java), JSON is considered
    interchangeable (program independent). The following code snippet shows some sample
    JSON data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 是一种与语言无关的格式，将数据存储为键-值和/或键-数组字段。 由于大多数编程语言支持键-值数据结构（例如 Python 中的 Dictionary
    或 Java 中的 HashMap），因此 JSON 被认为是可互换的（独立于程序）。 以下代码片段显示了一些示例 JSON 数据：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Have a look at the Awesome JSON Datasets GitHub repository ([https://github.com/jdorfman/awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets)),
    which contains a list of useful JSON data sources. Also, Public API’s GitHub repository
    ([https://github.com/public-apis/public-apis](https://github.com/public-apis/public-apis))
    consists of a list of web server endpoints where various JSON data can be retrieved.
    Additionally, we provide a script that collects JSON data from an endpoint and
    stores the necessary fields as a CSV file: [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/rest/get_rest_api_data.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/rest/get_rest_api_data.py).
    This example uses the Reddit dataset available at [https://www.reddit.com/r/all.json](https://www.reddit.com/r/all.json).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 Awesome JSON Datasets GitHub 仓库 ([https://github.com/jdorfman/awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets))，其中包含一些有用的
    JSON 数据源的列表。 此外，Public API’s GitHub 仓库 ([https://github.com/public-apis/public-apis](https://github.com/public-apis/public-apis))
    包含一些可以检索各种 JSON 数据的 Web 服务器端点的列表。 此外，我们提供一个脚本，从端点收集 JSON 数据，并将必要的字段存储为 CSV 文件：
    [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/rest/get_rest_api_data.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/rest/get_rest_api_data.py)。
    此示例使用位于 [https://www.reddit.com/r/all.json](https://www.reddit.com/r/all.json)
    的 Reddit 数据集。
- en: Next, we will introduce popular public datasets in the fields of data science.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍数据科学领域中流行的公共数据集。
- en: Popular dataset repositories
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流行的数据集存储库
- en: 'Besides web pages and JSON data, many public datasets can be used for various
    purposes. For example, you can get datasets from popular data hubs such as *Kaggle*
    ([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)) or *MIT Data
    Hub* ([https://datahub.csail.mit.edu/browse/public](https://datahub.csail.mit.edu/browse/public)).
    These public datasets are often used for a wide range of activities by many research
    institutes as well as businesses. Data from varying domains such as healthcare,
    government, biology, and computer science are collected during research and donated
    to the repositories for the greater good. Like how these organizations manage
    and provide diverse datasets, community efforts exist for managing various public
    datasets: [https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets#government).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了网页和JSON数据之外，许多公共数据集可以用于各种目的。例如，您可以从流行的数据集中心如*Kaggle* ([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets))或*MIT
    Data Hub* ([https://datahub.csail.mit.edu/browse/public](https://datahub.csail.mit.edu/browse/public))获取数据集。这些公共数据集经常被许多研究机构和企业用于广泛的活动。从各种领域如医疗保健、政府、生物学和计算机科学的数据在研究过程中收集，并捐赠给仓库以造福更多人。就像这些组织如何管理和提供多样化的数据集一样，社区也在努力管理各种公共数据集：[https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets#government)。
- en: Another popular source of datasets is data analytics libraries such as *sklearn*,
    *Keras*, and *TensorFlow*. The list of datasets provided by each library can be
    found at [https://scikit-learn.org/stable/datasets](https://scikit-learn.org/stable/datasets),
    [https://keras.io/api/datasets/](https://keras.io/api/datasets/), and [https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets),
    respectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的数据集来源是数据分析库，如*sklearn*、*Keras*和*TensorFlow*。每个库提供的数据集列表可以在以下链接找到：[https://scikit-learn.org/stable/datasets](https://scikit-learn.org/stable/datasets)，[https://keras.io/api/datasets/](https://keras.io/api/datasets/)和[https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)。
- en: 'Finally, government organizations also provide many datasets to the public.
    For example, you can find interesting, curated datasets related to COVID in a
    data lake hosted by AWS: [https://dj2taa9i652rf.cloudfront.net](https://dj2taa9i652rf.cloudfront.net/).
    From this list of datasets, you can easily download data on Moderna vaccination
    distribution among different states in CSV format by navigating to the `cdc-moderna-vaccine-distribution`
    page.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，政府机构也向公众提供许多数据集。例如，您可以在由AWS托管的数据湖中找到与COVID相关的有趣且经过精心策划的数据集：[https://dj2taa9i652rf.cloudfront.net](https://dj2taa9i652rf.cloudfront.net/)。从这些数据集列表中，您可以轻松下载关于Moderna疫苗在不同州分布的CSV格式数据，方法是导航至`cdc-moderna-vaccine-distribution`页面。
- en: Now that you have collected an initial dataset, the next step is to clean it
    up.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经收集了初始数据集，接下来的步骤是清理它。
- en: Cleaning data
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理数据
- en: Data cleaning is the process of polishing raw data to keep the entries consistent.
    Common operations include filling up empty fields with default values, removing
    characters that are not alpha-numeric such as `?` or `!`, removing stop words,
    and removing HTML tags such as `<p></p>`. Data cleaning also focuses on retaining
    relevant information from the collected data. For example, a user profile page
    may have a wide range of information, such as a biography, first name, email,
    and affiliations. During the data collection process, target information is extracted
    as-is so that it can be kept in the original HTML or JSON tags. In other words,
    the biographic information that’s been collected might still have HTML tags for
    new lines (`<br>`) or bold (`<b></b>`), which do not add much value to the following
    analysis task. Throughout data cleaning, these unnecessary components should be
    dropped.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清理是将原始数据精磨以保持条目一致的过程。常见的操作包括使用默认值填充空字段，移除非字母数字字符如`?`或`!`，去除停用词，以及删除HTML标记如`<p></p>`。数据清理还侧重于保留从收集的数据中提取的相关信息。例如，用户个人资料页面可能包含广泛的信息，如传记、名字、电子邮件和隶属关系。在数据收集过程中，目标信息被按原样提取，以便保留在原始HTML或JSON标记中。换句话说，已收集的传记信息可能仍然带有换行的HTML标签(`<br>`)或加粗(`<b></b>`)，这些对后续分析任务并不增加多少价值。在整个数据清理过程中，这些不必要的组件应该被丢弃。
- en: Before we discuss individual data cleaning operations, it would be nice to have
    some understanding of DataFrames, table-like data structures provided by the pandas
    library ([https://pandas.pydata.org/](https://pandas.pydata.org/)). They have
    rows and columns, just like a SQL table or an Excel sheet. One of their fundamental
    functionalities is `pandas.read_csv`, which allows you to load a CSV file into
    a DataFrame, as demonstrated in the following code snippet. The `tabulate` library
    is a good pick for displaying the content on a terminal as the DataFrame structures
    the data in a table format.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论单个数据清理操作之前，了解一下DataFrames将会很有帮助，DataFrames是由pandas库提供的类似表格的数据结构（[https://pandas.pydata.org/](https://pandas.pydata.org/)）。它们拥有行和列，就像SQL表或Excel表格一样。它们的一个基本功能是`pandas.read_csv`，它允许你将CSV文件加载到一个DataFrame中，就像下面的代码片段所演示的那样。在终端上显示内容时，`tabulate`库是一个不错的选择，因为DataFrame可以将数据结构化成表格格式。
- en: 'The following code snippet shows how to read a CSV file and print the data
    using the `tabulate` library (in the proceeding example, `tabulate` will mimic
    the format of the Postgres psql CLI as we are using the `tablefmt="psql"` option):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段显示了如何读取CSV文件并使用`tabulate`库打印数据（在上面的例子中，`tabulate`将模仿Postgres psql CLI的格式，因为我们使用了`tablefmt="psql"`选项）：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following screenshot shows the content of the DataFrame in the preceding
    code snippet after being displayed on a terminal using the `tabulate` library
    (you can view a similar output without the `tabulate` library by using `df_vacc.head(5)`).
    The following screenshot shows the allocation of vaccine doses for each jurisdiction:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了前述代码片段中DataFrame在终端上使用`tabulate`库显示的内容（你可以通过使用`df_vacc.head(5)`而不使用`tabulate`库来查看类似的输出）。以下截图显示了每个司法管辖区疫苗剂量的分配情况：
- en: '![Figure 2.2 – Loading a CSV file using pandas and displaying the contents
    using tabulate'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 使用pandas加载CSV文件并使用tabulate显示内容'
- en: '](img/B18522_02_2.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_02_2.jpg)'
- en: Figure 2.2 – Loading a CSV file using pandas and displaying the contents using
    tabulate
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 使用pandas加载CSV文件并使用tabulate显示内容
- en: The first data cleaning operation we will discuss is filling in missing fields
    with default values.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个数据清理操作是使用默认值填充缺失字段。
- en: Filling empty fields with default values
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用默认值填充空字段
- en: 'We will use the Google Scholar data we crawled earlier in this chapter to demonstrate
    how empty fields are filled with default values. After data inspection, you will
    find a few authors that have left their affiliations empty as they are unspecified:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用本章早些时候抓取的Google Scholar数据来演示如何使用默认值填充空字段。在数据检查后，您会发现一些作者没有填写他们的机构，因为这些信息没有明确指定：
- en: '![Figure 2.3 – The affiliation column contains missing values (nan)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – 机构列包含缺失值（nan）'
- en: '](img/B18522_02_3.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_02_3.jpg)'
- en: Figure 2.3 – The affiliation column contains missing values (nan)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 机构列包含缺失值（nan）
- en: The default value for each field differs based on the context and data type.
    For example, nine to six would be a typical default value for an operation hour,
    and an empty string would be a good choice for a missing middle name. The phrase,
    not applicable (N/A) is often used to explicitly indicate that the fields are
    empty. In our example, we will fill out the empty fields that contain `na` to
    indicate that the values were missing in the original web pages and not missed
    out due to errors throughout the collection process. The technique we will demonstrate
    in this example involves the `pandas` library; the DataFrame has a `fillna` method
    that fills the empty values in the specified value. The `fillna` method accepts
    a parameter value of `True` for updating the object in place without creating
    a copy of it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每个字段的默认值根据上下文和数据类型而异。例如，将9到6作为操作时间的典型默认值，将空字符串作为缺少中间名的良好选择。短语“不适用（N/A）”经常用于明确指出字段为空。在我们的示例中，我们将填写包含`na`的空字段，以指示这些值在原始网页中缺失，而不是在整个收集过程中由于错误而丢失。我们将在这个例子中演示的技术涉及`pandas`库；DataFrame有一个`fillna`方法，它填充指定值中的空值。`fillna`方法接受一个参数`True`，用于在原地更新对象而不创建副本。
- en: 'The following code snippet explains how to fill the missing values in a DataFrame
    using the `fillna` method:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段解释了如何使用`fillna`方法填充DataFrame中的缺失值：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code snippet, we loaded a CSV file into a DataFrame and set
    missing affiliation entries with `na`. This operation will be executed in place
    without creating an additional copy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，我们将CSV文件加载到DataFrame中，并将缺失的关联条目设置为`na`。此操作将在原地执行，而不会创建额外的副本。
- en: In the next section, we will describe how to remove stop words.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将描述如何移除停用词。
- en: Removing stop words
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除停用词
- en: 'Stop words are words that do not convey much value from an information retrieval
    perspective. Common English stop words include *its*, *and*, *the*, *for*, and
    *that*. As an example, entry of the research interest fields in Google Scholar
    data, we see *security and privacy preservation for wireless networks*. Words
    such as *and* and *for* are not useful when we interpret the meaning of this text.
    Therefore, removing these words is recommended in **natural language processing**
    (**NLP**) tasks. One of the most popular stop word removal features is provided
    by **Natural Language Toolkit** (**NLTK**), which is a suite of libraries and
    programs for symbolic and statistical NLP. The following are a few words that
    are considered as stop word tokens by the NLTK library:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是从信息检索角度来看没有多大价值的词语。常见的英文停用词包括*its*、*and*、*the*、*for*和*that*。例如，在Google Scholar数据的研究兴趣字段中，我们看到*security
    and privacy preservation for wireless networks*。像*and*和*for*这样的词在解释这段文本的含义时并不有用。因此，在**自然语言处理**（**NLP**）任务中建议移除这些词。**自然语言工具包**（**NLTK**）提供了一个流行的停用词移除功能，它是一套用于符号和统计NLP的库和程序。以下是NLTK库认为是停用词标记的一些单词：
- en: '`[''doesn'', "doesn''t", ''hadn'', "hadn''t", ''hasn'', "hasn''t", ''haven'',
    "haven''t", ''isn'', "isn''t", ''ma'', …]`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`[''doesn'', "doesn''t", ''hadn'', "hadn''t", ''hasn'', "hasn''t", ''haven'',
    "haven''t", ''isn'', "isn''t", ''ma'', …]`'
- en: 'Word tokenization is the process of breaking down a sentence into word tokens
    (word vectors). In general, it gets applied before stop word removal. The following
    code snippets demonstrate how to tokenize the `research_interest` fields of Google
    Scholar data and remove stop words:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 单词标记化是将句子分解为单词标记（单词向量）的过程。通常在停用词移除之前应用。下面的代码片段演示了如何标记化Google Scholar数据的`research_interest`字段并移除停用词：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, we first download the stop words corpus for NLTK with `stopwords.words('english')`
    and remove word tokens that are not in the corpus. The full version is available
    at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/data_preproessing/bag_of_words_tf_idf.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/data_preproessing/bag_of_words_tf_idf.py).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，我们首先使用`stopwords.words('english')`下载NLTK的停用词语料库，并移除不在语料库中的单词标记。完整版本位于[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/data_preproessing/bag_of_words_tf_idf.py](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/blob/main/Chapter_2/data_preproessing/bag_of_words_tf_idf.py)。
- en: Like stop words, text that is not alpha-numeric does not add much value either.
    Therefore, we will explain how to remove them in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与停用词类似，非字母数字文本在信息检索角度上也没有多大价值。因此，我们将在下一节中说明如何移除它们。
- en: Removing text that is not alpha-numeric
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除非字母数字文本
- en: 'Alpha-numeric characters are characters that are neither English alphabet characters
    nor numbers. For example, in the text “*Hi, How are you?*”, there are two non-alpha-numeric
    characters: , and ?. As in the case of stop words, they can be dropped as they
    don’t convey much information about the context. Once these characters are removed,
    the text will read *Hi How are you*.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 字母数字字符是既不是英文字母字符也不是数字的字符。例如，在文本“*Hi, How are you?*”中，有两个非字母数字字符：`,`和`?`。与停用词类似，它们可以被去除，因为它们对上下文的信息贡献不大。一旦这些字符被移除，文本将变成*Hi
    How are you*。
- en: 'To remove a set of specific characters, we can use **regular expressions**
    (**regex**). Regex is a sequence of characters that represents a search pattern.
    The following *Table 2.3* shows a few important regex search patterns and explains
    what each means:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除一组特定字符，我们可以使用**正则表达式**（**regex**）。正则表达式是一系列字符，表示搜索模式。下面的*表 2.3*显示了一些重要的正则表达式搜索模式，并解释了每个模式的含义：
- en: '![](img/Table_03.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Table_03.jpg)'
- en: Table 2.3 – Key regex search patterns
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.3 – 关键正则表达式搜索模式
- en: You can find other useful patterns at [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)找到其他有用的模式。
- en: 'Python provides a built-in `regex` library that supports finding and removing
    a set of texts that matches the given regular expression. The following code snippet
    shows how to remove characters that are not alphanumeric. The `\W` pattern matches
    any character that is not a word character. `+` after the pattern indicates that
    we would like to keep the preceding element one or more times. Putting them together,
    we will find one or more alphanumeric characters in the following code snippet:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Python提供了一个内置的`regex`库，支持查找和删除匹配给定正则表达式的一组文本。以下代码片段展示了如何删除非字母数字字符。`\W`模式匹配任何非单词字符。模式后面的`+`表示我们想保留前面的元素一次或多次。将它们组合在一起，我们将在以下代码片段中找到一个或多个字母数字字符：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As the last data cleaning operation, we will introduce how to drop newline characters
    efficiently.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的数据清理操作，我们将介绍如何高效地删除换行符。
- en: Removing newlines
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除换行符
- en: Finally, the collected text data may have unnecessary newline characters. In
    many cases, the trailing newline characters can be dropped without any harm, regardless
    of what the following tasks are. Such characters can be easily replaced by empty
    strings using Python’s built-in `replace` functionality.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，收集的文本数据可能包含不必要的换行符。在许多情况下，即使是结尾的换行符也可以毫无损害地删除，而不管接下来的任务是什么。可以使用Python的内置`replace`功能将这些字符轻松地替换为空字符串。
- en: 'The following code snippet shows how to remove a newline in text:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在文本中删除换行符：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code snippet, `"abc\n"` will turn into `"abc"`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，`"abc\n"`将会变成`"abc"`。
- en: The cleaned data often gets processed further so that the data represents the
    underlying data better. This process is called data preprocessing. We will take
    a deeper look into this process in the next section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后的数据通常会进一步处理，以更好地表示底层数据。这个过程称为数据预处理。我们将在下一节深入研究这个过程。
- en: Data preprocessing
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'The goal of data preprocessing is to transform cleaned data into a generic
    form suitable for a wide range of data analytics tasks. There is not a clear distinction
    between data cleaning and data preprocessing. As a result, tasks such as replacing
    a set of texts or filling in missing values can be categorized as data cleaning,
    as well as data preprocessing. In this section, we will focus on techniques that
    were not covered in the previous section: normalization, converting text into
    lowercase, converting text into bag-of-words, and applying stemming to words.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理的目标是将清洗后的数据转换为适合各种数据分析任务的通用形式。数据清理和数据预处理之间没有明确的区别。因此，像替换一组文本或填充缺失值这样的任务既可以归类为数据清理，也可以归类为数据预处理。在本节中，我们将重点介绍前一节未涵盖的技术：标准化、将文本转换为小写、将文本转换为词袋模型，并对单词应用词干提取。
- en: Complete implementations of the following examples can be found at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的示例实现可以在[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing)找到。
- en: Normalization
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化
- en: Sometimes, the values for a field might be represented differently, even though
    they mean the same thing. In the case of Google Scholar data, entries in research
    interests may be in different words, even though they refer to a similar domain.
    For example, data science, ML, and **artificial intelligence** (**AI**) refer
    to the same domain of AI in larger contexts. During the data preprocessing stage,
    we typically normalize them by converting ML and data science into AI, which represents
    the underlying information better. This helps the data science algorithms leverage
    the feature for the target task.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，字段的值可能以不同的方式表示，尽管它们表示相同的含义。在谷歌学术数据的情况下，研究兴趣的条目可能用不同的词汇，尽管它们指的是类似的领域。例如，数据科学，ML和**人工智能**（**AI**）在更大的上下文中指的是AI的相同领域。在数据预处理阶段，我们通常通过将ML和数据科学转换为AI来标准化它们，这样可以更好地表示底层信息。这有助于数据科学算法利用特征来完成目标任务。
- en: 'As demonstrated in the `normalize.py` script within the example repository,
    normalization for the preceding case can easily be achieved by keeping a dictionary
    that maps the expected value to the normalized value. In the following code snippet,
    `artificial_intelligence` will be the normalized value for the `data_science`
    and `machine_learning` features for `research_interests`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如在示例存储库中的`normalize.py`脚本中所示，可以通过保持一个映射预期值到标准化值的字典来轻松实现前述案例的标准化。在以下代码片段中，`artificial_intelligence`将是`research_interests`中`data_science`和`machine_learning`特征的标准化值：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The numeric values of a field also require normalization. For numeric values,
    normalization would be the process of rescaling each value into a specific range.
    In the following example, we are scaling each mean count of weekly vaccine distributions
    per state between 0 and 1\. First, we calculate the mean counts for each state.
    Then, we compute the normalized mean count by dividing the mean counts by the
    maximum mean count:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 字段的数值也需要标准化。对于数值，标准化将是将每个值重新缩放到特定范围内的过程。在以下示例中，我们将每个州的每周疫苗分配的平均计数缩放到0和1之间。首先，我们计算每个州的平均计数。然后，通过将平均计数除以最大平均计数来计算标准化的平均计数：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result of normalization can be seen in the following screenshot. The table
    in this screenshot consists of two columns – the mean vaccine count before normalization
    and after normalization:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的结果可以在以下屏幕截图中看到。此屏幕截图的表格包含两列 - 标准化之前和之后的平均疫苗计数：
- en: '![Figure 2.4 – Normalized mean vaccine distribution per state'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.4 - 各州标准化平均疫苗分配量'
- en: '](img/B18522_02_4.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_02_4.jpg)'
- en: Figure 2.4 – Normalized mean vaccine distribution per state
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 - 各州标准化平均疫苗分配量
- en: The next data preprocessing we will introduce is case conversion for text data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来介绍的下一个数据预处理是文本数据的大小写转换。
- en: Case conversion
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大小写转换
- en: 'In many cases, text data gets converted into lowercase or uppercase as a way
    of normalization. This brings some level of consistency, especially when the following
    tasks involve comparisons. In the stop words removal example, word tokens in the
    `curr_res_int_tok` variable are searched within the standard English stop words
    of the NLTK library. For the comparison to be successful, the case should be consistent.
    In the following code snippet, the tokens get converted into lowercase before
    the stop word search:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，文本数据会被转换为小写或大写作为标准化的一种方式。这在涉及比较的任务时带来了一定的一致性。在停用词移除示例中，`curr_res_int_tok`变量中的单词标记将在NLTK库的标准英文停用词中搜索。为了成功比较，大小写应保持一致。在以下代码片段中，在停用词搜索之前，标记会转换为小写：
- en: '[PRE17]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Another example can be found in `get_rest_api_data.py`, where we have collected
    and processed data from Reddit. In the following code snippet taken from the script,
    every text field gets converted into lowercase upon collection:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个示例可以在`get_rest_api_data.py`中找到，我们从Reddit收集和处理数据。在从脚本中提取的以下代码片段中，每个文本字段在收集时都转换为小写：
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, you will learn how stemming can improve the quality of the data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将了解如何通过词干化来改善数据质量。
- en: Stemming
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干化
- en: 'Stemming is the process of transforming a word into its root word. The benefit
    of stemming comes from keeping the words consistent if their underlying meaning
    is the same. For example, “*information*”, “*informs*”, and “*informed*” have
    the same root word – “*inform”*. The following example shows how to utilize the
    NLTK library for stemming. The NLTK library offers a stemming feature based on
    *Porter stemming algorithm (Porter, Martin F. “An algorithm for suffix stripping.”
    Program (1980))*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 词干化是将一个词转换为其根词的过程。词干化的好处在于，如果它们的基本含义相同，可以保持单词的一致性。例如，“*信息*”，“*通知*”和“*通知*”有相同的词根
    - “*通知*”。以下示例展示了如何利用NLTK库进行词干化。NLTK库提供了基于*波特词干算法（Porter, Martin F. “An algorithm
    for suffix stripping.” Program (1980))*的词干化功能：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding code snippet, we instantiated `PosterStemmer` from the `nltk.stem`
    library and passed the text into the `stem` function.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码片段中，我们从`nltk.stem`库实例化了`PosterStemmer`，并将文本传递给`stem`函数。
- en: Things to remember
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的事项
- en: a. Data comes in different formats such as JSON, CSV, HTML, and XML. There are
    many data collection tools available for each type of data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: a. 数据以不同格式出现，如JSON，CSV，HTML和XML。每种类型的数据都有许多数据收集工具可用。
- en: b. Data cleaning is the process of polishing raw data to keep each entry consistent.
    Common operations include filling up empty features with default values, removing
    characters that are not alphanumeric, removing stop words, and removing unnecessary
    tags.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: b. 数据清洗是将原始数据整理成一致性条目的过程。常见操作包括用默认值填充空特征、删除非字母数字字符、删除停用词和不必要的标记。
- en: c. The goal of data preprocessing is to apply generic data augmentation to transform
    cleaned data into a form that is generic for any data analytic task.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: c. 数据预处理的目标是应用通用数据增强技术，将清洗后的数据转换为适用于任何数据分析任务的形式。
- en: d. The domain of data cleaning and data preprocessing overlaps, which means
    that some operations can be used for either process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: d. 数据清洗和数据预处理的领域有重叠，这意味着某些操作可用于两个过程之一。
- en: 'So far, we have discussed the generic processes for data preparation. Next,
    we will discuss the final process: feature extraction. Unlike the other processes
    we have covered, feature extraction involves task-specific operations. Let’s take
    a closer look.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已讨论了数据准备的通用过程。接下来，我们将讨论最后一个过程：特征提取。与我们讨论过的其他过程不同，特征提取涉及特定于任务的操作。让我们更详细地看一下。
- en: Extracting features from data
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据中提取特征
- en: '**Feature extraction** (**feature engineering**) is the process of transforming
    data into features that express the underlying information in a specific way for
    the target task. Data preprocessing applies generic techniques that are often
    necessary for most data analytics tasks. However, feature extraction requires
    you to exploit domain knowledge as it is specific to the task. In this section,
    we will introduce popular feature extraction techniques, including bag-of-words
    for text data, term frequency-inverse document frequency, converting color images
    into gray images, ordinal encoding, one-hot encoding, dimensionality reduction,
    and fuzzy match for comparing two strings.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征提取**（**特征工程**）是将数据转换为表达特定任务底层信息的特征的过程。数据预处理应用通用技术，通常对大多数数据分析任务都是必需的。但是，特征提取要求利用领域知识，因为它是特定于任务的。在本节中，我们将介绍流行的特征提取技术，包括文本数据的词袋模型、词频-逆文档频率、将彩色图像转换为灰度图像、序数编码、独热编码、降维和用于比较两个字符串的模糊匹配。'
- en: Complete implementations of these examples can be found online at [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例的完整实现可以在[https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/data_preproessing)在线找到。
- en: First, we will start with the bag-of-words technique.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从词袋模型技术开始。
- en: Converting text using bag-of-words
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用词袋模型转换文本
- en: '`CountVectorizer` class from Sklearn helps to create BoW from text. The following
    code demonstrates how to use Sklearn features for BoW:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Sklearn 的 `CountVectorizer` 类可帮助从文本创建词袋模型。以下代码演示了如何使用 Sklearn 的功能进行词袋模型：
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following screenshot summarizes the output of BoW in a table format:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图总结了词袋模型的输出，采用表格格式：
- en: '![Figure 2.5 – Output of BoW on three sample documents'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5 – 词袋模型在三个样本文档上的输出'
- en: '](img/B18522_02_5.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_02_5.jpg)'
- en: Figure 2.5 – Output of BoW on three sample documents
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 词袋模型在三个样本文档上的输出
- en: Next, we will introduce **term frequency-inverse document frequency** (**TF-IDF**)
    for text data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍**词频-逆文档频率**（**TF-IDF**）用于文本数据。
- en: Applying term frequency-inverse document frequency (TF-IDF) transformation
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用词频-逆文档频率（TF-IDF）转换
- en: The problem with using word frequency is that the documents that have higher
    frequency will dominate the model or analysis. Hence, it is better to rescale
    the frequency based on how often a word occurs in all documents. Such scaling
    helps to penalize those highly frequent words (such as *the* and *have*) in a
    way that the numerical representation of the text expresses the context better.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词频的问题在于具有更高频率的文档将主导模型或分析。因此，更好的方法是根据单词在所有文档中出现的频率重新调整频率。这种缩放有助于惩罚那些高频单词（如*the*和*have*），使文本的数值表示更好地表达上下文。
- en: Before introducing the formula for TF-IDF, we must define some notations. Let
    *n* be the total number of documents and *t* be a word (term). *df(t)* refers
    to the document frequency for word *t* (how many documents contain the word *t*),
    while *tf(t, d)* refers to the word *t* frequency in document *d* (how many times
    *t* appears in document *d*). With these definitions, we can define *idf(t)*,
    the inverse document frequency, as *log [ n / df(t) ] + 1*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Overall, *tf-idf(t, d)*, *tf-idf* for word *t* and document *d* can be represented
    as *tf(t, d) * idf(t)*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'In the sample code script, `bag_of_words_tf_idf.py`, we are using research
    interest fields of Google Scholar data to calculate TF-IDF. Here, we utilize the
    `TfidfVectorizer` function of Sklearn. The `fit_transform` function takes in a
    set of documents and generates a TF-IDF-weighted document-term matrix. From this
    matrix, we can print out the top *N* research interests:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In the preceding example, we create a `TfidfVectorizer` instance and trigger
    the `fit_transform` function using the list of research interest texts (`research_interest_list`).
    Then, we call the `todense` method on the output to obtain the dense representation
    of the resulting matrix. The matrix gets converted into a DataFrame and sorted
    to display the top entries. The following screenshot shows the output of `df.head(3)`
    – three words with the highest TF-IDF from research interest fields:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Three words with the highest TF-IDF from research interest fields'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_6.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – Three words with the highest TF-IDF from research interest fields
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will learn how to process categorical data using one-hot encoding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Creating one-hot encoding (one-of-k)
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One-hot encoding (one-of-k) is the process of converting discrete values into
    a sequence of binary values. Let’s start with a simple example, where a field
    can have categorical values of either `cat` or `dog`. The one-hot encoding will
    be represented by two bits, where one bit refers to `cat` and the other bit refers
    to `dog`. The bit in the encoding with a value of 1 means that the field has the
    corresponding value. So, 1 0 represents a cat, while 0 1 represents a dog:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '| **breed** | **pet_type** | **dog** | **cat** |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| Retrievers | dog | 1 | 0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| Maine Coon | cat | 0 | 1 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| German Shepherd | dog | 1 | 0 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: Table 2.4 – Converting categorical values in pet_type into one-hot encoding
    (dog and cat)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'A demonstration of one-hot encoding can be found in `one_hot_encoding.py`.
    In the following code snippet, we are focusing on the core operations, which involve
    `OneHotEncoder` from Sklearn:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `is_artificial_intelligence` column used in the previous code snippet consists
    of two distinct values: `"yes"` and `"no"`. The following screenshot summarizes
    the results of one-hot encoding:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – One-hot encoding for the is_artificial_intelligence field'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_7.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – One-hot encoding for the is_artificial_intelligence field
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce another type of encoding called ordinal
    encoding.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Creating ordinal encoding
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ordinal encoding is the process of converting a categorical value into a numerical
    value. In *Table 2.5*, there are two types for pets, dog and cat. Dogs are assigned
    a value of 1 and cats are assigned a value of 2:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '| **breed** | **pet_type** | **ordinal_encoding** |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| Retrievers | dog | 1 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| Maine Coon | cat | 2 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| German Shepherd | dog | 1 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: Table 2.5 – The categorical values in pet_type field encoded as ordinal in ordinal_encoding
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we are using the `LabelEncoder` class from Sklearn
    to transform research interest fields into numerical values. A complete example
    of ordinal encoding can be found in `ordinal_encoding.py`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code snippet is almost self-explanatory – we simply construct
    a `LabelEncoder` instance and pass the target column to the `fit_transform` method.
    The following screenshot shows the first three rows of the resulting DataFrame:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Results of ordinal encoding on research interest'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_8.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Results of ordinal encoding on research interest
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will explain a technique for images: converting a colored image into
    a grayscale image.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Converting a colored image into a grayscale image
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the most common techniques in a computer vision task is to convert a
    colored image into a grayscale image. *OpenCV* is a standard library for image
    processing ([https://opencv.org/](https://opencv.org/)). In the following example,
    we are simply importing the OpenCV library (`import cv2`) and using the `cvtColor`
    function to convert a loaded image into grayscale:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When analyzing a large volume of data with multiple fields, you often find that
    reducing the number of dimensions is necessary. In the next section, we will look
    at the available options for this process.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Performing dimensionality reduction
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In many cases, there are more features than what the task needs; not all features
    have useful information. In this case, you can use dimensionality reduction techniques
    such as **Principal Component Analysis** (**PCA**), **Singular Value Decomposition**
    (**SVD**), **Linear Discriminant Analysis** (**LDA**), **t-SNE**, **UMAP**, and
    **ISOMAP** to name a few. Another option is to use DL. You can build a custom
    model for dimensionality reduction or use a pre-defined network structure such
    as **AutoEncoder**. In this section, we will describe PCA in detail as it is the
    most popular technique among the ones we mentioned.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a set of features, PCA identifies relationships among the features and
    generates a new set of variables that capture the differences in the samples in
    the most efficient way*. These new variables are called principal components and
    are ranked in order of importance; while constructing the first principal component,
    it squeezes the unimportant variables and leaves them for the second principal
    component. Therefore, the first principal component is not correlated to the remaining
    variables. This process gets repeated to construct principal components of the
    following order.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*给定一组特征，PCA识别特征之间的关系，并生成一组新的变量，以最有效的方式捕捉样本之间的差异*。这些新变量称为主成分，并按重要性排名；在构建第一个主成分时，它压缩了不重要的变量，并将它们留给第二个主成分。因此，第一个主成分与其余变量不相关。这个过程重复进行，以构建后续顺序的主成分。'
- en: 'If we were to describe the PCA process more formally, we can say that the process
    has two steps:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更正式地描述PCA过程，我们可以说该过程有两个步骤：
- en: Constructs a covariance matrix that represents the correlations for every pair
    of features.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建代表每对特征的相关性的协方差矩阵。
- en: Generates a new set of features that captures different amounts of information
    by calculating the eigenvalues of the covariance matrix.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过计算协方差矩阵的特征值，生成能够捕获不同信息量的新功能集。
- en: The new set of features is principal components. By sorting the corresponding
    eigenvalues from highest to lowest, you would get the most useful new feature
    at the top.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 新功能集是主要组件。通过按从最高到最低排序相应的特征值，您可以在顶部获得最有用的新功能。
- en: 'To understand the details, we will look at the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)).
    This dataset consists of three classes of Iris plant (setosa, versicolor, and
    virginica), along with four features (sepal width, sepal length, petal width,
    and petal length). In the following diagram, we plot each entry using the two
    new features constructed from PCA. Based on this diagram, we can easily conclude
    that we only need the top two principal components to distinguish the three classes:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解细节，我们将查看Iris数据集（[https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris)）。该数据集包括三类鸢尾植物（山鸢尾、变色鸢尾和维吉尼亚鸢尾），以及四个特征（花萼宽度、花萼长度、花瓣宽度和花瓣长度）。在下图中，我们使用从PCA构建的两个新特征绘制每个条目。根据这个图表，我们可以轻松地得出结论，我们只需要前两个主成分来区分这三个类别：
- en: '![Figure 2.9 – The results of PCA on the Iris dataset'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9 - Iris数据集上PCA的结果'
- en: '](img/B18522_02_09.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18522_02_09.jpg)'
- en: Figure 2.9 – The results of PCA on the Iris dataset
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 - Iris数据集上PCA的结果
- en: 'In the following example, we will use human resources data from Kaggle to demonstrate
    PCA. The initial set of data consists of multiple fields such as salary, whether
    there was a promotion within the last five years or not, and whether an employee
    left the company or not. Once principal components are constructed, they can be
    plotted using matplotlib:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们将使用来自Kaggle的人力资源数据来演示PCA。初始数据集包括多个字段，如工资、过去五年内是否晋升以及员工是否离开公司。一旦构建了主成分，就可以使用matplotlib绘制它们：
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding code snippet, first, we loaded the data using the `read_csv`
    function of the pandas library, normalized the entries using `StandardScaler`
    from Sklearn, and applied PCA using Sklearn. The complete example can be found
    at `pca.py`.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，首先使用pandas库的`read_csv`函数加载数据，使用Sklearn的`StandardScaler`对条目进行标准化，并使用Sklearn应用PCA。完整示例可以在`pca.py`中找到。
- en: As the last technique for feature extraction, we will explain how to effectively
    calculate a distance metric between two sequences.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 作为特征提取的最后一种技术，我们将解释如何有效地计算两个序列之间的距离度量。
- en: Applying fuzzy matching to handle similarity between strings
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用模糊匹配来处理字符串之间的相似性
- en: '**Fuzzy matching** ([https://pypi.org/project/fuzzywuzzy/](https://pypi.org/project/fuzzywuzzy/))
    uses a distance metric that measures the differences between two sequences and
    treats them equally if they can be considered similar. In this section, we will
    demonstrate how fuzzy matching can be applied using *Levenshtein Distance* (Levenshtein,
    Vladimir I. (February 1966). "Binary codes capable of correcting deletions, insertions,
    and reversals". Soviet Physics Doklady. 10 (8): 707–710\. Bibcode: 1966SPhD...10..707L).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**模糊匹配** ([https://pypi.org/project/fuzzywuzzy/](https://pypi.org/project/fuzzywuzzy/))
    使用距离度量来衡量两个序列的差异，并且如果它们被认为是相似的，则对待它们相同。在这一部分中，我们将演示如何使用*莱文斯坦距离*（Levenshtein, Vladimir
    I. (February 1966). "Binary codes capable of correcting deletions, insertions,
    and reversals". Soviet Physics Doklady. 10 (8): 707–710\. Bibcode: 1966SPhD...10..707L）来应用模糊匹配。'
- en: 'The most popular library for fuzzy string matching is `fuzzywuzzy`. The `ratio`
    function will provide the Levenshtein distance score, which we can use to decide
    whether we want to consider the two strings the same for the following process.
    The following code snippet describes the usage of the `ratio` function:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊字符串匹配最流行的库是 `fuzzywuzzy`。`ratio` 函数将提供莱文斯坦距离分数，我们可以用它来决定是否要在接下来的过程中将两个字符串视为相同。下面的代码片段描述了
    `ratio` 函数的使用方式：
- en: '[PRE26]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As shown in the preceding example, the `ratio` function will output a higher
    score if the two texts are more similar.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，`ratio` 函数将在两个文本更相似时输出较高的分数。
- en: Things to remember
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 记住的事情
- en: a. **Feature extraction** is the process of transforming data into features
    that express the underlying information better for the target task.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: a. **特征提取** 是将数据转换为能更好地表达目标任务下潜在信息的特征的过程。
- en: b. BoW is a representation of a document based on the occurrence of the words.
    TF-IDF can express the context of a document better by penalizing highly frequent
    words.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: b. BoW 是基于单词出现情况的文档表示。TF-IDF 可以通过惩罚高频词语更好地表达文档的上下文。
- en: c. A colored image can be updated to a grayscale image using the OpenCV library.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: c. 使用OpenCV库，可以将彩色图像更新为灰度图像。
- en: d. Categorical features can be represented numerically using ordinal encoding
    or one-hot encoding.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: d. 分类特征可以使用序数编码或独热编码进行数值表示。
- en: e. When a dataset has too many features, dimensionality reduction can reduce
    the number of features that have the most information. PCA constructs new features
    while retaining most of the information.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: e. 当数据集具有过多特征时，降维可以减少具有最多信息的特征数量。PCA构建新特征同时保留大部分信息。
- en: f. When evaluating the similarity between two texts, you can apply fuzzy matching
    drop.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: f. 在评估两个文本之间的相似性时，可以应用模糊匹配方法。
- en: Once the data has been transformed into a reasonable format, you will often
    need to visualize the data to understand its characteristics. In the next section,
    we will introduce popular libraries for data visualization.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被转换为合理的格式，通常需要可视化数据以理解其特性。在下一节中，我们将介绍用于数据可视化的流行库。
- en: Performing data visualization
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行数据可视化
- en: When applying ML techniques to analyze a dataset, the first step must be understanding
    the available data because every algorithm has advantages that are closely related
    to the underlying data. The key aspects of data that data scientists need to understand
    include data formats, distributions, and relationships among the features. When
    the amount of data is small, necessary information can be collected by analyzing
    each entry manually. However, as the amount of data grows, visualization plays
    a critical role in understanding the data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用机器学习技术分析数据集时，第一步必须是理解可用数据，因为每个算法的优势与底层数据密切相关。数据科学家需要理解的数据关键方面包括数据格式、分布和特征之间的关系。当数据量较小时，可以通过手动分析每个条目来收集必要信息。然而，随着数据量的增长，可视化在理解数据中扮演关键角色。
- en: Many tools for data visualization are available in Python. **Matplotlib** and
    **Seaborn** are the most popular libraries for statistical data visualization.
    We will introduce these two libraries one by one in this section.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Python中有许多用于数据可视化的工具。**Matplotlib** 和 **Seaborn** 是最流行的用于统计数据可视化的库。我们将在本节逐一介绍这两个库。
- en: Performing basic visualizations using Matplotlib
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Matplotlib执行基本可视化
- en: 'In the following example, we will demonstrate how to generate bar charts and
    pie charts using Matplotlib. The data we use represents the weekly distribution
    of COVID vaccines. To use the `matplot` functionality, you must import the package
    first (`import matplotlib.pyplot as plt`). The `plt.bar` function takes the list
    of top 10 state names and a list of its mean distribution to generate a bar chart.
    Similarly, the `plt.pie` function is used to generate a pie chart from a dictionary.
    The `plt.figure` function resizes the plot size and allows users to draw multiple
    charts on the same canvas. The complete implementation can be found at `visualize_matplotlib.py`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The result of the preceding code is as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Bar and pie charts generated using Matplotlib'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_10.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – Bar and pie charts generated using Matplotlib
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will introduce `Seaborn`, another popular data visualization
    library.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Drawing statistical graphs using Seaborn
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Seaborn` is a library built on top of `Matplotlib` to provide a high-level
    interface for drawing statistical graphics that `Matplotlib` does not support.
    In this section, we will learn how to generate line graphs and histograms using
    `Seaborn` for the same dataset. First, we need to import the `Seaborn` library
    along with `Matplotlib` (`import seaborn as sns`). The `sns.line_plot` function
    is designed to accept a DataFrame and column names. Therefore, we must provide
    `df_mean_sorted_top10`, which contains the top 10 states of the highest mean values
    of vaccines distributed and two column names, `state_names` and `count_vaccine`,
    for the *X* and *Y* axes. To plot the histogram, you can use the `sns.dist_plot`
    function, which takes in a DataFrame with a column value for the *Y* axis. If
    we are to use the same mean values, it would be `sns.displot(df_mean_sorted_top10[''count_vaccine''],
    kde=False)`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The resulting graphs are shown in the following figure:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Line graph and histogram generated using Seaborn'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18522_02_11.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Line graph and histogram generated using Seaborn
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The complete implementation can be found in this book’s GitHub repository (`visualize_seaborn.py`).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Many libraries can be used for enhanced visualizations: **pyROOT**, a data
    analysis framework from CERN that’s commonly used for research projects ([https://root.cern/manual/python](https://root.cern/manual/python/)),
    **Streamlit**, for easy web app creation ([https://streamlit.io](https://streamlit.io/)),
    **Plotly**, a free open source graphing library ([https://plotly.com](https://plotly.com/)),
    and **Bokeh**, for interactive web visualizations ([https://docs.bokeh.org/en/latest](https://docs.bokeh.org/en/latest/)).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Things to remember
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: a. Visualizing data helps you analyze and understand data that is critical for
    selecting the right machine learning algorithm.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: b. Matplotlib and Seaborn are the most popular data visualization tools. Other
    tools include pyRoot, Streamlit, Plotly, and Bokeh.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: b. Matplotlib 和 Seaborn 是最流行的数据可视化工具。其他工具包括 pyRoot、Streamlit、Plotly 和 Bokeh。
- en: The last section of this chapter will describe Docker, which allows you to achieve
    **operating system** (**OS**)-level virtualization for your project.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一节将描述 Docker，它允许你为项目实现**操作系统**（**OS**）级别的虚拟化。
- en: Introduction to Docker
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker 简介
- en: In the previous section, *Setting up notebook environments*, you learned how
    to set up a virtual environment with various packages for DL using `conda` and
    `pip` commands. Furthermore, you know how to save an environment into a YAML file
    and recreate the same environment. However, projects based on virtual environments
    may not be sufficient when the environment needs to be replicated on multiple
    machines as there can be issues coming from non-obvious OS-level dependencies.
    In this situation, Docker would be a great solution. Using Docker, you can create
    a snapshot of your working environment, including the underlying version of your
    OS. Altogether, Docker allows you to separate your applications from your infrastructure
    so that you can deliver your software quickly. Installing Docker can be achieved
    by following the instructions at [https://www.docker.com/get-started](https://www.docker.com/get-started).
    In this book, we will use version 3.5.2.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节 *设置笔记本环境* 中，你学习了如何使用`conda`和`pip`命令设置带有各种包的虚拟环境用于深度学习。此外，你还知道如何将环境保存到 YAML
    文件并重新创建相同的环境。然而，基于虚拟环境的项目在需要在多台机器上复制环境时可能不够，因为可能会出现来自非明显的操作系统级别依赖项的问题。在这种情况下，Docker
    将是一个很好的解决方案。使用 Docker，你可以创建一个包含操作系统版本的工作环境快照。总之，Docker 允许你将应用程序与基础设施分开，以便快速交付软件。安装
    Docker 可以按照 [https://www.docker.com/get-started](https://www.docker.com/get-started)
    上的说明进行。在本书中，我们将使用版本 3.5.2。
- en: In this section, we will introduce a Docker image, a representation of a virtual
    environment in the context of Docker, and explain how to create a Dockerfile for
    the target Docker image.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 Docker 镜像，即 Docker 上下文中虚拟环境的表示，并解释如何为目标 Docker 镜像创建 Dockerfile。
- en: Introduction to Dockerfiles
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Dockerfile 简介
- en: 'Docker images are created by so-called Dockerfiles. Every Docker image has
    a base (or parent) image. For DL environments, a good choice for the base image
    would be an image developed for Linux Ubuntu OS – one of the following should
    be a good choice: *ubuntu:18.04* ([https://releases.ubuntu.com/18.04](https://releases.ubuntu.com/18.04))
    or *ubuntu:20.04* ([https://releases.ubuntu.com/20.04](https://releases.ubuntu.com/20.04/)).
    Along with an image for the underlying OS, there are images with specific packages
    already installed. For example, the simplest way to set up a TensorFlow-based
    environment is to download images with TensorFlow installed. A base image has
    been created by TensorFlow developers and can be easily downloaded by using `docker
    pull tensorflow/serving` command ([https://hub.docker.com/r/tensorflow/serving](https://hub.docker.com/r/tensorflow/serving)).
    An environment with PyTorch is also available: [https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 镜像由所谓的 Dockerfile 创建。每个 Docker 镜像都有一个基础（或父）镜像。对于深度学习环境，作为基础镜像的一个很好选择是为
    Linux Ubuntu 操作系统开发的镜像之一：*ubuntu:18.04*（[https://releases.ubuntu.com/18.04](https://releases.ubuntu.com/18.04)）或
    *ubuntu:20.04*（[https://releases.ubuntu.com/20.04](https://releases.ubuntu.com/20.04/)）。除了基础操作系统的镜像，还有安装了特定包的镜像。例如，建立基于
    TensorFlow 的环境的最简单方法是下载已安装 TensorFlow 的镜像。TensorFlow 开发人员已经创建了一个基础镜像，并可以通过使用 `docker
    pull tensorflow/serving` 命令轻松下载（[https://hub.docker.com/r/tensorflow/serving](https://hub.docker.com/r/tensorflow/serving)）。还提供了一个带有
    PyTorch 的环境：[https://github.com/pytorch/serve/blob/master/docker/README.md](https://github.com/pytorch/serve/blob/master/docker/README.md)。
- en: Next, you will learn how to build with a custom Docker image.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将学习如何使用自定义 Docker 镜像进行构建。
- en: Building a custom Docker image
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自定义 Docker 镜像
- en: Creating a custom image is also straightforward. However, it involves many commands
    for which we will relegate the details to [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles).
    Once you have built the Docker image, you can instantiate it with something known
    as a Docker container. A Docker container is a standalone executable package of
    software that includes everything that you need to run the target application.
    By following the instructions in the `README.md` file, you can create the Docker
    image, which will run a containerized Jupyter notebook service with the standard
    libraries we described in this chapter.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义镜像也很简单。但是，它涉及许多命令，我们将详细信息委托给 [https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles](https://github.com/PacktPublishing/Production-Ready-Applied-Deep-Learning/tree/main/Chapter_2/dockerfiles)。一旦构建了
    Docker 镜像，您可以使用称为 Docker 容器的东西来实例化它。Docker 容器是一个独立的可执行软件包，包括运行目标应用程序所需的所有内容。通过按照
    `README.md` 文件中的说明操作，您可以创建一个 Docker 镜像，该镜像将在本章中描述的标准库中运行一个容器化的 Jupyter 笔记本服务。
- en: Things to remember
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要记住的事情
- en: a. Docker creates a snapshot of your working environment, including the underlying
    OS. The created image can be used to recreate the same environment on different
    machines.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: a. Docker 创建了您工作环境的快照，包括底层操作系统。创建的镜像可用于在不同的机器上重新创建相同的环境。
- en: b. Docker helps you separate your environment from infrastructure. This allows
    you to move your applications to different cloud service providers (such as AWS
    or Google Cloud) with minimal effort.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: b. Docker 帮助您将环境与基础设施分离。这使得您可以将应用程序轻松移动到不同的云服务提供商（如 AWS 或 Google Cloud）。
- en: At this point, you should be able to create a Docker image for your DL project.
    By instantiating the Docker image, you should be able to collect the data you
    need and process it as needed on your local machine or various cloud service providers.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该能够为您的深度学习项目创建一个 Docker 镜像。通过实例化 Docker 镜像，您应该能够在本地机器或各种云服务提供商上收集所需的数据并进行处理。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we described how to prepare a dataset for data analytics tasks.
    The first key point was how to achieve environment virtualization using Anaconda
    and Docker, along with Python package management using `pip`.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了如何为数据分析任务准备数据集。第一个关键点是如何使用 Anaconda 和 Docker 实现环境虚拟化，并使用 `pip` 进行
    Python 包管理。
- en: 'The data preparation process can be broken down into four steps: data collection,
    data cleaning, data preprocessing, and feature extraction. First, we have introduced
    various tools available for data collection that support different data types.
    Once the data has been collected, it is cleaned and preprocessed so that it can
    be transformed into a generic form. Depending on the target task, we often apply
    various feature extraction techniques that are task-specific. In addition, we
    have introduced many tools for data visualization that can help you understand
    the characteristics of the prepared data.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备过程可以分解为四个步骤：数据收集、数据清理、数据预处理和特征提取。首先，我们介绍了支持不同数据类型的数据收集工具。一旦数据被收集，它就会被清理和预处理，以便能够转换为通用形式。根据目标任务，我们经常应用各种特定于任务的特征提取技术。此外，我们介绍了许多数据可视化工具，可以帮助您了解准备好的数据的特性。
- en: 'Now that we have learned how to prepare our data for analytics tasks, in the
    next chapter, we will explain DL model development. We will introduce the basic
    concepts and how to use the two most popular DL frameworks: TensorFlow and PyTorch.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何为分析任务准备数据，下一章中，我们将解释深度学习模型开发。我们将介绍基本概念以及如何使用两个最流行的深度学习框架：TensorFlow
    和 PyTorch。
