["```py\npip install gym==0.20 \n```", "```py\n>>> import gym\n>>> env = gym.make('CartPole-v1')\n>>> env.observation_space\nBox(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n>>> env.action_space\nDiscrete(2) \n```", "```py\n>>> env.reset()\narray([-0.03908273, -0.00837535,  0.03277162, -0.0207195 ]) \n```", "```py\n>>> env.step(action=0)\n(array([-0.03925023, -0.20395158,  0.03235723,  0.28212046]), 1.0, False, {})\n>>> env.step(action=1)\n(array([-0.04332927, -0.00930575,  0.03799964, -0.00018409]), 1.0, False, {}) \n```", "```py\n## Script: gridworld_env.py\nimport numpy as np\nfrom gym.envs.toy_text import discrete\nfrom collections import defaultdict\nimport time\nimport pickle\nimport os\nfrom gym.envs.classic_control import rendering\nCELL_SIZE = 100\nMARGIN = 10\ndef get_coords(row, col, loc='center'):\n    xc = (col+1.5) * CELL_SIZE\n    yc = (row+1.5) * CELL_SIZE\n    if loc == 'center':\n        return xc, yc\n    elif loc == 'interior_corners':\n        half_size = CELL_SIZE//2 - MARGIN\n        xl, xr = xc - half_size, xc + half_size\n        yt, yb = xc - half_size, xc + half_size\n        return [(xl, yt), (xr, yt), (xr, yb), (xl, yb)]\n    elif loc == 'interior_triangle':\n        x1, y1 = xc, yc + CELL_SIZE//3\n        x2, y2 = xc + CELL_SIZE//3, yc - CELL_SIZE//3\n        x3, y3 = xc - CELL_SIZE//3, yc - CELL_SIZE//3\n        return [(x1, y1), (x2, y2), (x3, y3)]\ndef draw_object(coords_list):\n    if len(coords_list) == 1: # -> circle\n        obj = rendering.make_circle(int(0.45*CELL_SIZE))\n        obj_transform = rendering.Transform()\n        obj.add_attr(obj_transform)\n        obj_transform.set_translation(*coords_list[0])\n        obj.set_color(0.2, 0.2, 0.2) # -> black\n    elif len(coords_list) == 3: # -> triangle\n        obj = rendering.FilledPolygon(coords_list)\n        obj.set_color(0.9, 0.6, 0.2) # -> yellow\n    elif len(coords_list) > 3: # -> polygon\n        obj = rendering.FilledPolygon(coords_list)\n        obj.set_color(0.4, 0.4, 0.8) # -> blue\n    return obj \n```", "```py\nfrom gym.envs.classic_control import rendering \n```", "```py\nfrom gym.utils import pyglet_rendering \n```", "```py\nclass GridWorldEnv(discrete.DiscreteEnv):\n    def __init__(self, num_rows=4, num_cols=6, delay=0.05):\n        self.num_rows = num_rows\n        self.num_cols = num_cols\n        self.delay = delay\n        move_up = lambda row, col: (max(row-1, 0), col)\n        move_down = lambda row, col: (min(row+1, num_rows-1), col)\n        move_left = lambda row, col: (row, max(col-1, 0))\n        move_right = lambda row, col: (\n            row, min(col+1, num_cols-1))\n        self.action_defs={0: move_up, 1: move_right,\n                          2: move_down, 3: move_left}\n        ## Number of states/actions\n        nS = num_cols*num_rows\n        nA = len(self.action_defs)\n        self.grid2state_dict={(s//num_cols, s%num_cols):s\n                              for s in range(nS)}\n        self.state2grid_dict={s:(s//num_cols, s%num_cols)\n                              for s in range(nS)}\n        ## Gold state\n        gold_cell = (num_rows//2, num_cols-2)\n\n        ## Trap states\n        trap_cells = [((gold_cell[0]+1), gold_cell[1]),\n                       (gold_cell[0], gold_cell[1]-1),\n                       ((gold_cell[0]-1), gold_cell[1])]\n        gold_state = self.grid2state_dict[gold_cell]\n        trap_states = [self.grid2state_dict[(r, c)]\n                       for (r, c) in trap_cells]\n        self.terminal_states = [gold_state] + trap_states\n        print(self.terminal_states)\n        ## Build the transition probability\n        P = defaultdict(dict)\n        for s in range(nS):\n            row, col = self.state2grid_dict[s]\n            P[s] = defaultdict(list)\n            for a in range(nA):\n                action = self.action_defs[a]\n                next_s = self.grid2state_dict[action(row, col)]\n\n                ## Terminal state\n                if self.is_terminal(next_s):\n                    r = (1.0 if next_s == self.terminal_states[0]\n                         else -1.0)\n                else:\n                    r = 0.0\n                if self.is_terminal(s):\n                    done = True\n                    next_s = s\n                else:\n                    done = False\n                P[s][a] = [(1.0, next_s, r, done)]\n        ## Initial state distribution\n        isd = np.zeros(nS)\n        isd[0] = 1.0\n        super().__init__(nS, nA, P, isd)\n        self.viewer = None\n        self._build_display(gold_cell, trap_cells)\n    def is_terminal(self, state):\n        return state in self.terminal_states\n    def _build_display(self, gold_cell, trap_cells):\n        screen_width = (self.num_cols+2) * CELL_SIZE\n        screen_height = (self.num_rows+2) * CELL_SIZE\n        self.viewer = rendering.Viewer(screen_width,\n                                       screen_height)\n        all_objects = []\n        ## List of border points' coordinates\n        bp_list = [\n            (CELL_SIZE-MARGIN, CELL_SIZE-MARGIN),\n            (screen_width-CELL_SIZE+MARGIN, CELL_SIZE-MARGIN),\n            (screen_width-CELL_SIZE+MARGIN,\n             screen_height-CELL_SIZE+MARGIN),\n            (CELL_SIZE-MARGIN, screen_height-CELL_SIZE+MARGIN)\n        ]\n        border = rendering.PolyLine(bp_list, True)\n        border.set_linewidth(5)\n        all_objects.append(border)\n        ## Vertical lines\n        for col in range(self.num_cols+1):\n            x1, y1 = (col+1)*CELL_SIZE, CELL_SIZE\n            x2, y2 = (col+1)*CELL_SIZE,\\\n                     (self.num_rows+1)*CELL_SIZE\n            line = rendering.PolyLine([(x1, y1), (x2, y2)], False)\n            all_objects.append(line)\n\n        ## Horizontal lines\n        for row in range(self.num_rows+1):\n            x1, y1 = CELL_SIZE, (row+1)*CELL_SIZE\n            x2, y2 = (self.num_cols+1)*CELL_SIZE,\\\n                     (row+1)*CELL_SIZE\n            line=rendering.PolyLine([(x1, y1), (x2, y2)], False)\n            all_objects.append(line)\n\n        ## Traps: --> circles\n        for cell in trap_cells:\n            trap_coords = get_coords(*cell, loc='center')\n            all_objects.append(draw_object([trap_coords]))\n\n        ## Gold: --> triangle\n        gold_coords = get_coords(*gold_cell,\n                                 loc='interior_triangle')\n        all_objects.append(draw_object(gold_coords))\n        ## Agent --> square or robot\n        if (os.path.exists('robot-coordinates.pkl') and\n                CELL_SIZE==100):\n            agent_coords = pickle.load(\n                open('robot-coordinates.pkl', 'rb'))\n            starting_coords = get_coords(0, 0, loc='center')\n            agent_coords += np.array(starting_coords)\n        else:\n            agent_coords = get_coords(\n                0, 0, loc='interior_corners')\n        agent = draw_object(agent_coords)\n        self.agent_trans = rendering.Transform()\n        agent.add_attr(self.agent_trans)\n        all_objects.append(agent)\n        for obj in all_objects:\n            self.viewer.add_geom(obj)\n    def render(self, mode='human', done=False):\n        if done:\n            sleep_time = 1\n        else:\n            sleep_time = self.delay\n        x_coord = self.s % self.num_cols\n        y_coord = self.s // self.num_cols\n        x_coord = (x_coord+0) * CELL_SIZE\n        y_coord = (y_coord+0) * CELL_SIZE\n        self.agent_trans.set_translation(x_coord, y_coord)\n        rend = self.viewer.render(\n             return_rgb_array=(mode=='rgb_array'))\n        time.sleep(sleep_time)\n        return rend\n    def close(self):\n        if self.viewer:\n            self.viewer.close()\n            self.viewer = None \n```", "```py\nif __name__ == '__main__':\n    env = GridWorldEnv(5, 6)\n    for i in range(1):\n        s = env.reset()\n        env.render(mode='human', done=False)\n        while True:\n            action = np.random.choice(env.nA)\n            res = env.step(action)\n            print('Action  ', env.s, action, ' -> ', res)\n            env.render(mode='human', done=res[2])\n            if res[2]:\n                break\n    env.close() \n```", "```py\n## Script: agent.py\nfrom collections import defaultdict\nimport numpy as np\nclass Agent:\n    def __init__(\n            self, env,\n            learning_rate=0.01,\n            discount_factor=0.9,\n            epsilon_greedy=0.9,\n            epsilon_min=0.1,\n            epsilon_decay=0.95):\n        self.env = env\n        self.lr = learning_rate\n        self.gamma = discount_factor\n        self.epsilon = epsilon_greedy\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        ## Define the q_table\n        self.q_table = defaultdict(lambda: np.zeros(self.env.nA))\n    def choose_action(self, state):\n        if np.random.uniform() < self.epsilon:\n            action = np.random.choice(self.env.nA)\n        else:\n            q_vals = self.q_table[state]\n            perm_actions = np.random.permutation(self.env.nA)\n            q_vals = [q_vals[a] for a in perm_actions]\n            perm_q_argmax = np.argmax(q_vals)\n            action = perm_actions[perm_q_argmax]\n        return action\n    def _learn(self, transition):\n        s, a, r, next_s, done = transition\n        q_val = self.q_table[s][a]\n        if done:\n            q_target = r\n        else:\n            q_target = r + self.gamma*np.max(self.q_table[next_s])\n        ## Update the q_table\n        self.q_table[s][a] += self.lr * (q_target - q_val)\n        ## Adjust the epsilon\n        self._adjust_epsilon()\n    def _adjust_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay \n```", "```py\n## Script: qlearning.py\nfrom gridworld_env import GridWorldEnv\nfrom agent import Agent\nfrom collections import namedtuple\nimport matplotlib.pyplot as plt\nimport numpy as np\nnp.random.seed(1)\nTransition = namedtuple(\n    'Transition', ('state', 'action', 'reward',\n                   'next_state', 'done'))\ndef run_qlearning(agent, env, num_episodes=50):\n    history = []\n    for episode in range(num_episodes):\n        state = env.reset()\n        env.render(mode='human')\n        final_reward, n_moves = 0.0, 0\n        while True:\n            action = agent.choose_action(state)\n            next_s, reward, done, _ = env.step(action)\n            agent._learn(Transition(state, action, reward,\n                                    next_s, done))\n            env.render(mode='human', done=done)\n            state = next_s\n            n_moves += 1\n            if done:\n                break\n            final_reward = reward\n        history.append((n_moves, final_reward))\n        print(f'Episode {episode}: Reward {final_reward:.2} '\n              f'#Moves {n_moves}')\n    return history\ndef plot_learning_history(history):\n    fig = plt.figure(1, figsize=(14, 10))\n    ax = fig.add_subplot(2, 1, 1)\n    episodes = np.arange(len(history))\n    moves = np.array([h[0] for h in history])\n    plt.plot(episodes, moves, lw=4,\n             marker='o', markersize=10)\n    ax.tick_params(axis='both', which='major', labelsize=15)\n    plt.xlabel('Episodes', size=20)\n    plt.ylabel('# moves', size=20)\n    ax = fig.add_subplot(2, 1, 2)\n    rewards = np.array([h[1] for h in history])\n    plt.step(episodes, rewards, lw=4)\n    ax.tick_params(axis='both', which='major', labelsize=15)\n    plt.xlabel('Episodes', size=20)\n    plt.ylabel('Final rewards', size=20)\n    plt.savefig('q-learning-history.png', dpi=300)\n    plt.show()\nif __name__ == '__main__':\n    env = GridWorldEnv(num_rows=5, num_cols=6)\n    agent = Agent(env)\n    history = run_qlearning(agent, env)\n    env.close()\n    plot_learning_history(history) \n```", "```py\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom collections import deque\nnp.random.seed(1)\ntorch.manual_seed(1)\nTransition = namedtuple(\n            'Transition', ('state', 'action', 'reward',\n                           'next_state', 'done'))\nclass DQNAgent:\n    def __init__(\n            self, env, discount_factor=0.95,\n            epsilon_greedy=1.0, epsilon_min=0.01,\n            epsilon_decay=0.995, learning_rate=1e-3,\n            max_memory_size=2000):\n        self.env = env\n        self.state_size = env.observation_space.shape[0]\n        self.action_size = env.action_space.n\n        self.memory = deque(maxlen=max_memory_size)\n        self.gamma = discount_factor\n        self.epsilon = epsilon_greedy\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.lr = learning_rate\n        self._build_nn_model()\n    def _build_nn_model(self):\n        self.model = nn.Sequential(nn.Linear(self.state_size, 256),\n                          nn.ReLU(),\n                          nn.Linear(256, 128),\n                          nn.ReLU(),\n                          nn.Linear(128, 64),\n                          nn.ReLU(),\n                          nn.Linear(64, self.action_size))\n        self.loss_fn = nn.MSELoss()\n        self.optimizer = torch.optim.Adam(\n                                self.model.parameters(), self.lr)\n    def remember(self, transition):\n        self.memory.append(transition)\n    def choose_action(self, state):\n        if np.random.rand() <= self.epsilon:\n            return np.random.choice(self.action_size)\n        with torch.no_grad():\n            q_values = self.model(torch.tensor(state,\n                              dtype=torch.float32))[0]\n        return torch.argmax(q_values).item()  # returns action\n    def _learn(self, batch_samples):\n        batch_states, batch_targets = [], []\n        for transition in batch_samples:\n            s, a, r, next_s, done = transition\n            with torch.no_grad():\n                if done:\n                    target = r\n                else:\n                    pred = self.model(torch.tensor(next_s,\n                                  dtype=torch.float32))[0]\n                    target = r + self.gamma * pred.max()\n            target_all = self.model(torch.tensor(s,\n                                    dtype=torch.float32))[0]\n            target_all[a] = target\n            batch_states.append(s.flatten())\n            batch_targets.append(target_all)\n            self._adjust_epsilon()\n            self.optimizer.zero_grad()\n            pred = self.model(torch.tensor(batch_states,\n                              dtype=torch.float32))\n            loss = self.loss_fn(pred, torch.stack(batch_targets))\n            loss.backward()\n            self.optimizer.step()\n        return loss.item()\n    def _adjust_epsilon(self):\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n    def replay(self, batch_size):\n        samples = random.sample(self.memory, batch_size)\n        return self._learn(samples) \n```", "```py\ndef plot_learning_history(history):\n    fig = plt.figure(1, figsize=(14, 5))\n    ax = fig.add_subplot(1, 1, 1)\n    episodes = np.arange(len(history))+1\n    plt.plot(episodes, history, lw=4,\n             marker='o', markersize=10)\n    ax.tick_params(axis='both', which='major', labelsize=15)\n    plt.xlabel('Episodes', size=20)\n    plt.ylabel('Total rewards', size=20)\n    plt.show()\n## General settings\nEPISODES = 200\nbatch_size = 32\ninit_replay_memory_size = 500\nif __name__ == '__main__':\n    env = gym.make('CartPole-v1')\n    agent = DQNAgent(env)\n    state = env.reset()\n    state = np.reshape(state, [1, agent.state_size])\n    ## Filling up the replay-memory\n    for i in range(init_replay_memory_size):\n        action = agent.choose_action(state)\n        next_state, reward, done, _ = env.step(action)\n        next_state = np.reshape(next_state, [1, agent.state_size])\n        agent.remember(Transition(state, action, reward,\n                                  next_state, done))\n        if done:\n            state = env.reset()\n            state = np.reshape(state, [1, agent.state_size])\n        else:\n            state = next_state\n    total_rewards, losses = [], []\n    for e in range(EPISODES):\n        state = env.reset()\n        if e % 10 == 0:\n            env.render()\n        state = np.reshape(state, [1, agent.state_size])\n        for i in range(500):\n            action = agent.choose_action(state)\n            next_state, reward, done, _ = env.step(action)\n            next_state = np.reshape(next_state,\n                                    [1, agent.state_size])\n            agent.remember(Transition(state, action, reward,\n                                      next_state, done))\n            state = next_state\n            if e % 10 == 0:\n                env.render()\n            if done:\n                total_rewards.append(i)\n                print(f'Episode: {e}/{EPISODES}, Total reward: {i}')\n                break\n            loss = agent.replay(batch_size)\n            losses.append(loss)\n    plot_learning_history(total_rewards) \n```"]