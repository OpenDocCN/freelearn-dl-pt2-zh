- en: Appendix V — Answers to the Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1, What are Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are still in the Third Industrial Revolution. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False. Eras in history indeed overlap. However, the Third Industrial Revolution
    focused on making the world digital. The Fourth Industrial Revolution has begun
    to connect everything to everything else: systems, machines, bots, robots, algorithms,
    and more.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Fourth Industrial Revolution is connecting everything to everything else.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. This leads to an increasing amount of automated decisions that formerly
    required human intervention.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Industry 4.0 developers will sometimes have no AI development to do. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. In some projects, AI will be an online service that requires no development.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Industry 4.0 developers might have to implement transformers from scratch. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. In some projects, not all, standard online services or APIs might not
    satisfy the needs of a project. There may not be a satisfactory solution for a
    project in some cases. Instead, a developer will have to customize a model significantly
    and work from scratch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It’s not necessary to learn more than one transformer ecosystem, such as Hugging
    Face, for example. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. A corporation’s policy might be to work only with Google Cloud AI or
    Microsoft Azure AI. Hugging Face might be a tool used in another company. You
    can’t know in advance and, in most cases, cannot decide.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A ready-to-use transformer API can satisfy all needs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True if it is effective. False if the transformer model is not well trained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A company will accept the transformer ecosystem a developer knows best. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. A company may or may not accept what a developer suggests. Therefore,
    it’s safer to cover as many bases as possible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Cloud transformers have become mainstream. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer project can be run on a laptop. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True for a prototype, for example. False for a project involving thousands of
    users.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Industry 4.0 AI specialists will have to be more flexible (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 2, Getting Started with the Architecture of the Transformer Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP transduction can encode and decode text representations. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. NLP is transduction that converts sequences (written or oral) into numerical
    representations, processes them, and decodes the results back into text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Natural Language Understanding** (**NLU**) is a subset of **Natural Language
    Processing** (**NLP**). (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Language modeling algorithms generate probable sequences of words based on input
    sequences. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer is a customized LSTM with a CNN layer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. A transformer does not contain an LSTM or a CNN at all.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer does not contain LSTM or CNN layers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Attention examines all the tokens in a sequence, not just the last one. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer does not use positional encoding. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. A transformer uses positional encoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer contains a feedforward network. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The masked multi-headed attention component of the decoder of a transformer
    prevents the algorithm parsing a given position from seeing the rest of a sequence
    that is being processed. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers can analyze long-distance dependencies better than LSTMs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 3, Fine-Tuning BERT Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERT stands for Bidirectional Encoder Representations from Transformers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERT is a two-step framework. *Step 1* is pretraining. *Step 2* is fine-tuning.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model implies training parameters from scratch. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. BERT fine-tuning is initialized with the trained parameters of pretraining.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERT only pretrains using all downstream tasks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERT pretrains on **Masked Language Modeling** (**MLM**). (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERT pretrains on **Next Sentence Prediction** (**NSP**). (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERT pretrains on mathematical functions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A question-answer task is a downstream task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A BERT pretraining model does not require tokenization. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fine-tuning a BERT model takes less time than pretraining. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 4, Pretraining a RoBERTa Model from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RoBERTa uses a byte-level byte-pair encoding tokenizer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A trained Hugging Face tokenizer produces `merges.txt` and `vocab.json`. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RoBERTa does not use token-type IDs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: DistilBERT has 6 layers and 12 heads. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer model with 80 million parameters is enormous. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. 80 million parameters is a small model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We cannot train a tokenizer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. A tokenizer can be trained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A BERT-like model has six decoder layers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. BERT contains six encoder layers, not decoder layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: MLM predicts a word contained in a mask token in a sentence. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A BERT-like model has no self-attention sublayers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. BERT has self-attention layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Data collators are helpful for backpropagation. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 5, Downstream NLP Tasks with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine intelligence uses the same data as humans to make predictions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True and False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: True. In some cases, machine intelligence surpasses humans when processing massive
    amounts of data to extract meaning and perform a range of tasks that would take
    centuries for humans to process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: False. For NLU, humans have access to more information through their senses.
    Machine intelligence relies on what humans provide for all types of media.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: SuperGLUE is more difficult than GLUE for NLP models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BoolQ expects a binary answer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: WiC stands for Words in Context. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Recognizing Textual Entailment** (**RTE**) detects whether one sequence entails
    another sequence. (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A Winograd schema predicts whether a verb is spelled correctly. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Winograd schemas mainly apply to pronoun disambiguation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformer models now occupy the top ranks of GLUE and SuperGLUE. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Human Baseline Standards are not defined once and for all. They were made tougher
    to attain by SuperGLUE. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformer models will never beat SuperGLUE human baseline standards. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True and False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: False. Transformer models beat human baselines for GLUE and will do the same
    for SuperGLUE in the future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: True. We will keep setting higher benchmark standards as we progress in the
    field of NLU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Variants of transformer models have outperformed RNN and CNN models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. But you never know what will happen in the future in AI!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 6, Machine Translation with the Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine translation has now exceeded human baselines. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Machine translation is one of the most challenging NLP ML tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Machine translation requires large datasets. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There is no need to compare transformer models using the same datasets. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. The only way to compare different models is to use the same datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BLEU is the French word for blue and is the acronym of an NLP metric. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. **BLEU** stands for **Bilingual Evaluation Understudy Score**, making
    it easy to remember.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Smoothing techniques enhance BERT. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: German-English is the same as English-German for machine translation. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Representing German and then translating into another language is not
    the same process as representing English and translating into another language.
    The language structures are not the same.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The original Transformer multi-head attention sublayer has two heads. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Each attention sublayer has eight heads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The original Transformer encoder has six layers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The original Transformer encoder has six layers but only two decoder layers.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. There are six decoder layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can train transformers without decoders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. The architecture of BERT only contains encoders.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 7, The Rise of Suprahuman Transformers with GPT-3 Engines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A zero-shot method trains the parameters once. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. No parameters are trained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Gradient updates are performed when running zero-shot models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GPT models only have a decoder stack. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is impossible to train a 117M GPT model on a local machine. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. We trained one in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is impossible to train the GPT-2 model with a specific dataset. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. We trained one in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A GPT-2 model cannot be conditioned to generate text. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. We implemented this in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A GPT-2 model can analyze the context of input and produce completion content.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We cannot interact with a 345M GTP parameter model on a machine with fewer than
    eight GPUs. (True/False).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. We interacted with a model of this size in this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Supercomputers with 285,000 CPUs do not exist. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Supercomputers with thousands of GPUs are game-changers in AI. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. Thanks to this, we will be able to build models with increasing numbers
    of parameters and connections.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 8, Applying Transformers to Legal and Financial Documents for AI Text
    Summarization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: T5 models only have encoder stacks like BERT models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T5 models have both encoder and decoder stacks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T5 models use relative positional encoding, not absolute positional encoding.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Text-to-text models are only designed for summarization. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Text-to-text models apply a prefix to the input sequence that determines the
    NLP task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T5 models require specific hyperparameters for each task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One of the advantages of text-to-text models is that they use the same hyperparameters
    for all NLP tasks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: T5 transformers do not contain a feedforward network. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hugging Face is a framework that makes transformers easier to implement. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: OpenAI’s transformer engines are game-changers. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. OpenAI has produced a wide range of ready-to-use engines such as Codex
    (language to code) or Davinci (a general purpose engine).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 9, Matching Tokenizers and Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A tokenized dictionary contains every word that exists in a language. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Pretrained tokenizers can encode any dataset. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is good practice to check a database before using it. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is good practice to eliminate obscene data from datasets. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is good practice to delete data containing discriminating assertions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Raw datasets might sometimes produce relationships between noisy content and
    useful content. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A standard pretrained tokenizer contains the English vocabulary of the past
    700 years. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Old English can create problems when encoding data with a tokenizer trained
    in modern English. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Medical and other types of jargon can create problems when encoding data with
    a tokenizer trained in modern English. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Controlling the output of the encoded data produced by a pretrained tokenizer
    is good practice. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 10, Semantic Role Labeling with BERT-Based Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Semantic Role Labeling** (**SRL**) is a text generation task. (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A predicate is a noun. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A verb is a predicate. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Arguments can describe who and what is doing something. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A modifier can be an adverb. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A modifier can be a location. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A BERT-based model contains encoder and decoder stacks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A BERT-based SRL model has standard input formats. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers can solve any SRL task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Chapter 11, Let Your Data Do the Talking: Story, Questions, and Answers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A trained transformer model can answer any question. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Question-answering requires no further research. It is perfect as it is. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Named Entity Recognition** (**NER**) can provide useful information when
    looking for meaningful questions. (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Semantic Role Labeling** (**SRL**) is useless when preparing questions. (True/False)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A question generator is an excellent way to produce questions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implementing question-answering requires careful project management. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ELECTRA models have the same architecture as GPT-2\. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ELECTRA models have the same architecture as BERT but are trained as discriminators.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NER can recognize a location and label it as I-LOC. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NER can recognize a person and label that person as I-PER. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 12, Detecting Customer Emotions to Make Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is not necessary to pretrain transformers for sentiment analysis. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A sentence is always positive or negative. It cannot be neutral. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The principle of compositionality signifies that a transformer must grasp every
    part of a sentence to understand it. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RoBERTa-large was designed to improve the pretraining process of transformer
    models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer can provide feedback that informs us of whether a customer is
    satisfied or not. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the sentiment analysis of a product or service is consistently negative,
    it helps us make appropriate decisions to improve our offer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If a model fails to provide a good result on a task, it requires more training
    or fine-tuning before changing models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 13, Analyzing Fake News with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: News labeled as fake news is always fake. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: News that everybody agrees with is always accurate. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers can be used to run sentiment analysis on Tweets. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Key entities can be extracted from Facebook messages with a DistilBERT model
    running NER. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Key verbs can be identified from YouTube chats with BERT-based models running
    SRL. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Emotional reactions are a natural first response to fake news. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A rational approach to fake news can help clarify one’s position. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Connecting transformers to reliable websites can help somebody understand why
    some news is fake. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers can make summaries of reliable websites to help us understand some
    of the topics labeled as fake news. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can change the world if you use AI for the good of us all. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 14, Interpreting Black Box Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BERTViz only shows the output of the last layer of the BERT model. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. BERTViz displays the outputs of all the layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERTViz shows the attention heads of each layer of a BERT model. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: BERTViz shows how the tokens relate to each other. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: LIT shows the inner workings of the attention heads like BERTViz. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. However, LIT makes non-probing predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Probing is a way for an algorithm to predict language representations. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: NER is a probing task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: PCA and UMAP are non-probing tasks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: LIME is model-agnostic. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers deepen the relationships of the tokens layer by layer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Visual transformer model interpretation adds a new dimension to interpretable
    AI. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 15, From NLP to Task-Agnostic Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reformer transformer models don’t contain encoders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Reformer transformer models contain encoders.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reformer transformer models don’t contain decoders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Reformer transformer models contain encoders and decoders.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The inputs are stored layer by layer in Reformer models. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. The inputs are recomputed at each level, thus saving memory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: DeBERTa transformer models disentangle content and positions. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is necessary to test the hundreds of pretrained transformer models before
    choosing one for a project. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True and False. You can try all of the models, or you can choose a very reliable
    model and implement it to fit your needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The latest transformer model is always the best. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True and false. A lot of research is being produced on transformers, but some
    experimental models are short-lived. Sometimes, though, the latest model exceeds
    the performance of preceding models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is better to have one transformer model per NLP task than one multi-task
    transformer model. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True and False. This is a personal decision you will have to make. Risk assessment
    is a critical aspect of a project.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer model always needs to be fine-tuned. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. GPT-3 engines are zero-shot models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: OpenAI GPT-3 engines can perform a wide range of NLP tasks without fine-tuning.
    (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is always better to implement an AI algorithm on a local server. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. It depends on your project. It’s a risk assessment you will have to make.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Chapter 16, The Emergence of Transformer-Driven Copilots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI copilots that can generate code automatically do not exist. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. GitHub Copilot, for example, is now in production.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AI copilots will never replace humans. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True and false. AI will take over many tasks in sales, support, maintenance,
    and other domains. However, many complex tasks will still require human intervention.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GPT-3 engines can only do one task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. GPT-3 engines can do a wide variety of tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers can be trained to be recommenders. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: True. Transformers have gone from language sequences to sequences in many domains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Transformers can only process language. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Once transformers are trained for language sequences, they can analyze
    many other types of sequences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A transformer sequence can only contain words. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Once the language sequences are processed, transformers only work on
    numbers, not words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Vision transformers cannot equal CNNs. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Transformers are deep neural networks that can equal CNNs in computer
    vision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AI robots with computer vision do not exist. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. For example, robots with computer vision have begun to surface in military
    applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It is impossible to produce Python source code automatically. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: False. Microsoft and OpenAI have joined to produce a copilot that can write
    Python code with us or for us.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We might one day become the copilots of robots. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This could be true or false. This remains a challenge for humans, bots, and
    robots in an ever-growing AI ecosystem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
