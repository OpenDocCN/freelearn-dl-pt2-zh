- en: Appendix V — Answers to the Questions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录V — 问题的答案
- en: Chapter 1, What are Transformers?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章，什么是转换器？
- en: We are still in the Third Industrial Revolution. (True/False)
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们仍处于第三次工业革命。 (True/False)
- en: 'False. Eras in history indeed overlap. However, the Third Industrial Revolution
    focused on making the world digital. The Fourth Industrial Revolution has begun
    to connect everything to everything else: systems, machines, bots, robots, algorithms,
    and more.'
  id: totrans-3
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 历史上的时代确实有重叠。然而，第三次工业革命着眼于使世界数字化。第四次工业革命已经开始将所有事物连接到一起：系统、机器、机器人、机器人、算法等等。
- en: The Fourth Industrial Revolution is connecting everything to everything else.
    (True/False)
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四次工业革命正在将所有事物连接在一起。 (True/False)
- en: True. This leads to an increasing amount of automated decisions that formerly
    required human intervention.
  id: totrans-5
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True. 这导致了越来越多的自动决策，以前需要人为干预。
- en: Industry 4.0 developers will sometimes have no AI development to do. (True/False)
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业4.0开发者有时可能没有AI开发要做。 (True/False)
- en: True. In some projects, AI will be an online service that requires no development.
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True. 在某些项目中，AI将是一个不需要开发的在线服务。
- en: Industry 4.0 developers might have to implement transformers from scratch. (True/False)
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业4.0开发者可能需要从头开始实现转换器。 (True/False)
- en: True. In some projects, not all, standard online services or APIs might not
    satisfy the needs of a project. There may not be a satisfactory solution for a
    project in some cases. Instead, a developer will have to customize a model significantly
    and work from scratch.
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True. 在某些项目中，并非所有标准在线服务或API都能满足项目的需求。在某些情况下，项目可能没有一个令人满意的解决方案。相反，开发者将不得不大幅度定制模型并从头开始工作。
- en: It’s not necessary to learn more than one transformer ecosystem, such as Hugging
    Face, for example. (True/False)
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有必要学习超过一个转换器生态系统，例如Hugging Face。 (True/False)
- en: False. A corporation’s policy might be to work only with Google Cloud AI or
    Microsoft Azure AI. Hugging Face might be a tool used in another company. You
    can’t know in advance and, in most cases, cannot decide.
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 一个公司的政策可能是只与Google Cloud AI或Microsoft Azure AI合作。Hugging Face可能是另一家公司使用的工具。你事先不知道，而且在大多数情况下也不能决定。
- en: A ready-to-use transformer API can satisfy all needs. (True/False)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个即用即用的转换器API可以满足所有需求。 (True/False)
- en: True if it is effective. False if the transformer model is not well trained.
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果有效则为True。如果转换器模型训练不良，则为False。
- en: A company will accept the transformer ecosystem a developer knows best. (True/False)
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一家公司将接受开发者最擅长的转换器生态系统。 (True/False)
- en: False. A company may or may not accept what a developer suggests. Therefore,
    it’s safer to cover as many bases as possible.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 一家公司可能接受也可能不接受开发者建议的内容。因此，尽可能多地涵盖各方面是更安全的做法。
- en: Cloud transformers have become mainstream. (True/False)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 云转换器已成为主流。 (True/False)
- en: True.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True.
- en: A transformer project can be run on a laptop. (True/False)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换器项目可以在笔记本电脑上运行。 (True/False)
- en: True for a prototype, for example. False for a project involving thousands of
    users.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如原型为True。对于涉及成千上万用户的项目为False。
- en: Industry 4.0 AI specialists will have to be more flexible (True/False)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工业4.0 AI专家将不得不更加灵活 (True/False)
- en: True.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True.
- en: Chapter 2, Getting Started with the Architecture of the Transformer Model
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章，开始学习转换器模型的架构
- en: NLP transduction can encode and decode text representations. (True/False)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NLP转换可以对文本表示进行编码和解码。 (True/False)
- en: True. NLP is transduction that converts sequences (written or oral) into numerical
    representations, processes them, and decodes the results back into text.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True. NLP是将序列（书面或口头）转换为数字表示形式的转换，对其进行处理，并将结果解码回文本。
- en: '**Natural Language Understanding** (**NLU**) is a subset of **Natural Language
    Processing** (**NLP**). (True/False)'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**自然语言理解**（**NLU**）是**自然语言处理**（**NLP**）的子集。 (True/False)'
- en: True.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True.
- en: Language modeling algorithms generate probable sequences of words based on input
    sequences. (True/False)
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言建模算法根据输入序列生成可能的单词序列。 (True/False)
- en: True.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True.
- en: A transformer is a customized LSTM with a CNN layer. (True/False)
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换器是带有CNN层的定制LSTM。 (True/False)
- en: False. A transformer does not contain an LSTM or a CNN at all.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 转换器根本不包含LSTM或CNN。
- en: A transformer does not contain LSTM or CNN layers. (True/False)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换器不包含LSTM或CNN层。 (True/False)
- en: True.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True.
- en: Attention examines all the tokens in a sequence, not just the last one. (True/False)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意检查序列中的所有标记，而不仅仅是最后一个。 (True/False)
- en: True.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True.
- en: A transformer does not use positional encoding. (True/False)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换器不使用位置编码。 (True/False)
- en: False. A transformer uses positional encoding.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。一个变压器使用位置编码。
- en: A transformer contains a feedforward network. (True/False)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器包含一个前馈网络。（真/假）
- en: True.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: The masked multi-headed attention component of the decoder of a transformer
    prevents the algorithm parsing a given position from seeing the rest of a sequence
    that is being processed. (True/False)
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器解码器的掩码多头注意力组件防止算法分析正在处理的序列的给定位置时看到其余序列的其余部分。（真/假）
- en: True.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Transformers can analyze long-distance dependencies better than LSTMs. (True/False)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器可以更好地分析长距离依赖关系，优于 LSTMs。（真/假）
- en: True.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Chapter 3, Fine-Tuning BERT Models
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 章，《对 BERT 模型进行微调》
- en: BERT stands for Bidirectional Encoder Representations from Transformers. (True/False)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 代表双向编码器来自变压器。（真/假）
- en: True.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: BERT is a two-step framework. *Step 1* is pretraining. *Step 2* is fine-tuning.
    (True/False)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 是一个两步框架。*第一步*是预训练。*第二步*是微调。（真/假）
- en: True.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Fine-tuning a BERT model implies training parameters from scratch. (True/False)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 BERT 模型进行微调意味着从头开始训练参数。（真/假）
- en: False. BERT fine-tuning is initialized with the trained parameters of pretraining.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。BERT 微调是使用预训练的训练参数初始化的。
- en: BERT only pretrains using all downstream tasks. (True/False)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 只通过所有下游任务进行预训练。（真/假）
- en: False.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: BERT pretrains on **Masked Language Modeling** (**MLM**). (True/False)
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 预训练基于**掩码语言模型**（**MLM**）。（真/假）
- en: True.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: BERT pretrains on **Next Sentence Prediction** (**NSP**). (True/False)
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 的预训练基于**下一句预测**（**NSP**）。（真/假）
- en: True.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: BERT pretrains on mathematical functions. (True/False)
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 预训练基于数学函数。（真/假）
- en: False.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: A question-answer task is a downstream task. (True/False)
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个问题-答案任务是下游任务。（真/假）
- en: True.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A BERT pretraining model does not require tokenization. (True/False)
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERT 预训练模型不需要标记化。（真/假）
- en: False.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: Fine-tuning a BERT model takes less time than pretraining. (True/False)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 BERT 模型进行微调所需时间比预训练时间短。（真/假）
- en: True.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Chapter 4, Pretraining a RoBERTa Model from Scratch
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章，《从头开始预训练 RoBERTa 模型》
- en: RoBERTa uses a byte-level byte-pair encoding tokenizer. (True/False)
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoBERTa 使用字节级字节对编码分词器。（真/假）
- en: True.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A trained Hugging Face tokenizer produces `merges.txt` and `vocab.json`. (True/False)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个经过训练的 Hugging Face 分词器会产生 `merges.txt` 和 `vocab.json`。（真/假）
- en: True.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: RoBERTa does not use token-type IDs. (True/False)
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoBERTa 不使用标记类型 ID。（真/假）
- en: True.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: DistilBERT has 6 layers and 12 heads. (True/False)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DistilBERT 有 6 层和 12 个头。（真/假）
- en: True.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A transformer model with 80 million parameters is enormous. (True/False)
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个拥有 8000 万参数的变压器模型是巨大的。（真/假）
- en: False. 80 million parameters is a small model.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。8000 万参数是一个小型模型。
- en: We cannot train a tokenizer. (True/False)
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能训练分词器。（真/假）
- en: False. A tokenizer can be trained.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。分词器可以被训练。
- en: A BERT-like model has six decoder layers. (True/False)
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似 BERT 的模型有六个解码器层。（真/假）
- en: False. BERT contains six encoder layers, not decoder layers.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。BERT 包含六个编码器层，而不是解码器层。
- en: MLM predicts a word contained in a mask token in a sentence. (True/False)
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MLM 预测句子中包含在掩码标记内的单词。（真/假）
- en: True.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A BERT-like model has no self-attention sublayers. (True/False)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似 BERT 的模型没有自注意力子层。（真/假）
- en: False. BERT has self-attention layers.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。BERT 有自注意力层。
- en: Data collators are helpful for backpropagation. (True/False)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据收集器对于反向传播很有帮助。（真/假）
- en: True.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Chapter 5, Downstream NLP Tasks with Transformers
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章，《使用变压器进行下游 NLP 任务》
- en: Machine intelligence uses the same data as humans to make predictions. (True/False)
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器智能使用与人类相同的数据来作出预测。（真/假）
- en: True and False.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的和假的。
- en: True. In some cases, machine intelligence surpasses humans when processing massive
    amounts of data to extract meaning and perform a range of tasks that would take
    centuries for humans to process.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。在某些情况下，机器智能在处理海量数据以提取含义并执行一系列任务时超越人类，而这些任务人类需要花几个世纪的时间来完成。
- en: False. For NLU, humans have access to more information through their senses.
    Machine intelligence relies on what humans provide for all types of media.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。对于 NLU，人类通过他们的感官可以获取更多信息。机器智能依赖于人类提供的各种媒体。
- en: SuperGLUE is more difficult than GLUE for NLP models. (True/False)
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SuperGLUE 对于 NLP 模型来说比 GLUE 更困难。（真/假）
- en: True.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: BoolQ expects a binary answer. (True/False)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BoolQ 期望一个二进制答案。（真/假）
- en: True.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: WiC stands for Words in Context. (True/False)
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WiC 代表的是上下文中的单词。（真/假）
- en: True.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: '**Recognizing Textual Entailment** (**RTE**) detects whether one sequence entails
    another sequence. (True/False)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Recognizing Textual Entailment**（**RTE**）用于检测一个序列是否蕴涵另一个序列。（真/假）'
- en: True.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A Winograd schema predicts whether a verb is spelled correctly. (True/False)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个Winograd模式可以预测动词是否拼写正确。（真/假）
- en: False. Winograd schemas mainly apply to pronoun disambiguation.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。Winograd模式主要适用于代词消歧。
- en: Transformer models now occupy the top ranks of GLUE and SuperGLUE. (True/False)
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer模型目前占据GLUE和SuperGLUE的前几名。（真/假）
- en: True.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Human Baseline Standards are not defined once and for all. They were made tougher
    to attain by SuperGLUE. (True/False)
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 人类基准标准并不是一成不变的。SuperGLUE使其更加难以达到。（真/假）
- en: True.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Transformer models will never beat SuperGLUE human baseline standards. (True/False)
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformer模型永远不会超越SuperGLUE人类基准标准。（真/假）
- en: True and False.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真和假。
- en: False. Transformer models beat human baselines for GLUE and will do the same
    for SuperGLUE in the future.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。Transformer模型击败了GLUE的人类基准，并将来也将击败SuperGLUE。
- en: True. We will keep setting higher benchmark standards as we progress in the
    field of NLU.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。随着我们在NLU领域的进展，我们将继续设定更高的基准标准。
- en: Variants of transformer models have outperformed RNN and CNN models. (True/False)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变种的transformer模型已经超越了RNN和CNN模型。（真/假）
- en: True. But you never know what will happen in the future in AI!
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。但你永远不知道在AI中将会发生什么！
- en: Chapter 6, Machine Translation with the Transformer
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章，《使用Transformer进行机器翻译》
- en: Machine translation has now exceeded human baselines. (True/False)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译现在已经超过了人类基准。（真/假）
- en: False. Machine translation is one of the most challenging NLP ML tasks.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。机器翻译是最具挑战性的NLP ML任务之一。
- en: Machine translation requires large datasets. (True/False)
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器翻译需要大型数据集。（真/假）
- en: True.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: There is no need to compare transformer models using the same datasets. (True/False)
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不需要使用相同的数据集比较Transformer模型。（真/假）
- en: False. The only way to compare different models is to use the same datasets.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。比较不同模型的唯一方法是使用相同的数据集。
- en: BLEU is the French word for blue and is the acronym of an NLP metric. (True/False)
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BLEU是法语单词，意思是蓝色，并且是NLP度量标准的首字母缩写。（真/假）
- en: True. **BLEU** stands for **Bilingual Evaluation Understudy Score**, making
    it easy to remember.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。**BLEU**代表**双语评估辅助分数**，这样很容易记住。
- en: Smoothing techniques enhance BERT. (True/False)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平滑技术增强了BERT。（真/假）
- en: True.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: German-English is the same as English-German for machine translation. (True/False)
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于机器翻译，德语-英语与英语-德语是相同的。（真/假）
- en: False. Representing German and then translating into another language is not
    the same process as representing English and translating into another language.
    The language structures are not the same.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。首先表示德语，然后翻译成其他语言并不同于表示英语再翻译为另一种语言。语言结构不同。
- en: The original Transformer multi-head attention sublayer has two heads. (True/False)
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初的Transformer多头注意力子层有两个头。（真/假）
- en: False. Each attention sublayer has eight heads.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。每个注意力子层有八个头。
- en: The original Transformer encoder has six layers. (True/False)
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初的Transformer编码器有六层。（真/假）
- en: True.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: The original Transformer encoder has six layers but only two decoder layers.
    (True/False)
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最初的Transformer编码器有六层，但只有两个解码层。（真/假）
- en: False. There are six decoder layers.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。有六个解码器层。
- en: You can train transformers without decoders. (True/False)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以训练没有解码器的transformer。（真/假）
- en: True. The architecture of BERT only contains encoders.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。BERT的结构只包含编码器。
- en: Chapter 7, The Rise of Suprahuman Transformers with GPT-3 Engines
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章，《GPT-3引擎的超人类变压器崛起》
- en: A zero-shot method trains the parameters once. (True/False)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零-shot方法只需一次训练参数。（真/假）
- en: False. No parameters are trained.
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。没有训练参数。
- en: Gradient updates are performed when running zero-shot models. (True/False)
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行零-shot模型时执行梯度更新。（真/假）
- en: False.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: GPT models only have a decoder stack. (True/False)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT模型只有一个解码器堆叠。（真/假）
- en: True.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: It is impossible to train a 117M GPT model on a local machine. (True/False)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地机器上训练117M GPT模型是不可能的。（真/假）
- en: False. We trained one in this chapter.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。我们在本章中训练了一个。
- en: It is impossible to train the GPT-2 model with a specific dataset. (True/False)
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不可能使用特定数据集训练GPT-2模型。（真/假）
- en: False. We trained one in this chapter.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。我们在本章中训练了一个。
- en: A GPT-2 model cannot be conditioned to generate text. (True/False)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个GPT-2模型不能被约束来生成文本。（真/假）
- en: False. We implemented this in this chapter.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。我们在本章中实施了这一点。
- en: A GPT-2 model can analyze the context of input and produce completion content.
    (True/False)
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个GPT-2模型可以分析输入的上下文并生成完成的内容。（真/假）
- en: True.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: We cannot interact with a 345M GTP parameter model on a machine with fewer than
    eight GPUs. (True/False).
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能在拥有少于八个GPU的机器上与一个345M GTP参数模型交互。 (True/False).
- en: False. We interacted with a model of this size in this chapter.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。我们在本章中与这个尺寸的模型交互过。
- en: Supercomputers with 285,000 CPUs do not exist. (True/False)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有285,000个CPU的超级计算机不存在。 (True/False)
- en: False.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: Supercomputers with thousands of GPUs are game-changers in AI. (True/False)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有成千上万个GPU的超级计算机是AI中的游戏变革者。 (True/False)
- en: True. Thanks to this, we will be able to build models with increasing numbers
    of parameters and connections.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。由于这一点，我们将能够构建参数和连接数量逐渐增加的模型。
- en: Chapter 8, Applying Transformers to Legal and Financial Documents for AI Text
    Summarization
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八章，将转换器应用于法律和财务文件以进行AI文本摘要
- en: T5 models only have encoder stacks like BERT models. (True/False)
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5模型只有像BERT模型一样的编码器栈。 (True/False)
- en: False.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: T5 models have both encoder and decoder stacks. (True/False)
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5模型既有编码器栈，又有解码器栈。 (True/False)
- en: True.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: T5 models use relative positional encoding, not absolute positional encoding.
    (True/False)
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5模型使用相对位置编码，而不是绝对位置编码。 (True/False)
- en: True.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Text-to-text models are only designed for summarization. (True/False)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本-文本模型只设计用于摘要。 (True/False)
- en: False.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: Text-to-text models apply a prefix to the input sequence that determines the
    NLP task. (True/False)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本-文本模型将一个前缀应用到输入序列，用来确定NLP任务。 (True/False)
- en: True.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: T5 models require specific hyperparameters for each task. (True/False)
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5模型对每个任务需要具体的超参数。 (True/False)
- en: False.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: One of the advantages of text-to-text models is that they use the same hyperparameters
    for all NLP tasks. (True/False)
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本-文本模型的优势之一是它们对所有NLP任务使用相同的超参数。 (True/False)
- en: True.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: T5 transformers do not contain a feedforward network. (True/False)
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T5转换器不包含前馈网络。 (True/False)
- en: False.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: Hugging Face is a framework that makes transformers easier to implement. (True/False)
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face是一个让转换器更容易实现的框架。 (True/False)
- en: True.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: OpenAI’s transformer engines are game-changers. (True/False)
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI的转换器引擎是游戏变革者。 (True/False)
- en: True. OpenAI has produced a wide range of ready-to-use engines such as Codex
    (language to code) or Davinci (a general purpose engine).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。OpenAI已经开发了一系列可供使用的引擎，如Codex（从语言到代码）或Davinci（通用引擎）。
- en: Chapter 9, Matching Tokenizers and Datasets
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章，匹配分词器和数据集
- en: A tokenized dictionary contains every word that exists in a language. (True/False)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个分词的词典包含了一种语言中存在的每个单词。 (True/False)
- en: False.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: Pretrained tokenizers can encode any dataset. (True/False)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预训练分词器可以对任何数据集进行编码。 (True/False)
- en: False.
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: It is good practice to check a database before using it. (True/False)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用数据库之前检查数据库是个好习惯。 (True/False)
- en: True.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: It is good practice to eliminate obscene data from datasets. (True/False)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中消除淫秽数据是个好习惯。 (True/False)
- en: True.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: It is good practice to delete data containing discriminating assertions. (True/False)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除包含歧视性言论的数据是个好习惯。 (True/False)
- en: True.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Raw datasets might sometimes produce relationships between noisy content and
    useful content. (True/False)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始数据集有时可能会产生嘈杂内容和有用内容之间的关系。 (True/False)
- en: True.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A standard pretrained tokenizer contains the English vocabulary of the past
    700 years. (True/False)
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个标准的预训练分词器包含了过去700年的英语词汇。 (True/False)
- en: False.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: Old English can create problems when encoding data with a tokenizer trained
    in modern English. (True/False)
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 古英语在使用现代英语训练的分词器对数据进行编码时可能会产生问题。 (True/False)
- en: True.
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Medical and other types of jargon can create problems when encoding data with
    a tokenizer trained in modern English. (True/False)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 医学和其他类型的行话在使用现代英语训练的分词器对数据进行编码时会产生问题。 (True/False)
- en: True.
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Controlling the output of the encoded data produced by a pretrained tokenizer
    is good practice. (True/False)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制预训练分词器生成数据的输出是个好习惯。 (True/False)
- en: True.
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Chapter 10, Semantic Role Labeling with BERT-Based Transformers
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第十章，基于BERT的转换器进行语义角色标注
- en: '**Semantic Role Labeling** (**SRL**) is a text generation task. (True/False)'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语义角色标注** (**SRL**) 是一个文本生成任务。 (True/False)'
- en: False.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: A predicate is a noun. (True/False)
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 谓语是一个名词。 (True/False)
- en: False.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假的。
- en: A verb is a predicate. (True/False)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动词是谓语。 (True/False)
- en: True.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: Arguments can describe who and what is doing something. (True/False)
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 论点可以描述谁和什么正在做某事。 (True/False)
- en: True.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A modifier can be an adverb. (True/False)
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个修饰语可以是一个副词。 (True/False)
- en: True.
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A modifier can be a location. (True/False)
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个修饰语可以是一个地点。 (True/False)
- en: True.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真的。
- en: A BERT-based model contains encoder and decoder stacks. (True/False)
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于 BERT 的模型包含编码器和解码器堆栈。（真/假）
- en: False.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: A BERT-based SRL model has standard input formats. (True/False)
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于 BERT 的 SRL 模型具有标准的输入格式。（真/假）
- en: True.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Transformers can solve any SRL task. (True/False)
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器可以解决任何 SRL 任务。（真/假）
- en: False.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: 'Chapter 11, Let Your Data Do the Talking: Story, Questions, and Answers'
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 11 章，让你的数据说话：故事，问题和答案
- en: A trained transformer model can answer any question. (True/False)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练过的变压器模型可以回答任何问题。（真/假）
- en: False.
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: Question-answering requires no further research. It is perfect as it is. (True/False)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问答不需要进一步的研究。它已经完美了。（真/假）
- en: False.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: '**Named Entity Recognition** (**NER**) can provide useful information when
    looking for meaningful questions. (True/False)'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）在寻找有意义的问题时可以提供有用的信息。（真/假）'
- en: True.
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: '**Semantic Role Labeling** (**SRL**) is useless when preparing questions. (True/False)'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语义角色标注**（**SRL**）在准备问题时是无用的。（真/假）'
- en: False.
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: A question generator is an excellent way to produce questions. (True/False)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题生成器是产生问题的绝妙方法。（真/假）
- en: True.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Implementing question-answering requires careful project management. (True/False)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实施问答需要谨慎的项目管理。（真/假）
- en: True.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: ELECTRA models have the same architecture as GPT-2\. (True/False)
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ELECTRA 模型与 GPT-2 具有相同的架构。（真/假）
- en: False.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: ELECTRA models have the same architecture as BERT but are trained as discriminators.
    (True/False)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ELECTRA 模型与 BERT 具有相同的架构，但被训练为辨别器。（真/假）
- en: True.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: NER can recognize a location and label it as I-LOC. (True/False)
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NER 可以识别地点并将其标记为 I-LOC。（真/假）
- en: True.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: NER can recognize a person and label that person as I-PER. (True/False)
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NER 可以识别一个人并将该人标记为 I-PER。（真/假）
- en: True.
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Chapter 12, Detecting Customer Emotions to Make Predictions
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章，检测客户情绪以做出预测
- en: It is not necessary to pretrain transformers for sentiment analysis. (True/False)
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不需要为情感分析预训练变压器。（真/假）
- en: False.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: A sentence is always positive or negative. It cannot be neutral. (True/False)
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个句子总是积极或消极的。它不能是中性的。（真/假）
- en: False.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: The principle of compositionality signifies that a transformer must grasp every
    part of a sentence to understand it. (True/False)
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合成性原则意味着变压器必须理解句子的每个部分才能理解它。（真/假）
- en: True.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: RoBERTa-large was designed to improve the pretraining process of transformer
    models. (True/False)
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoBERTa-large 旨在改进变压器模型的预训练过程。（真/假）
- en: True.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: A transformer can provide feedback that informs us of whether a customer is
    satisfied or not. (True/False)
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个变压器可以提供反馈，告诉我们客户是否满意。（真/假）
- en: True.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: If the sentiment analysis of a product or service is consistently negative,
    it helps us make appropriate decisions to improve our offer. (True/False)
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果产品或服务的情感分析始终是负面的，它可以帮助我们做出改进我们提供的建议的适当决定。（真/假）
- en: True.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: If a model fails to provide a good result on a task, it requires more training
    or fine-tuning before changing models. (True/False)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个模型在某项任务上无法提供好的结果，那么在更换模型之前需要更多的训练或微调。（真/假）
- en: True.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Chapter 13, Analyzing Fake News with Transformers
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 13 章，使用变压器分析假新闻
- en: News labeled as fake news is always fake. (True/False)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记为假新闻的新闻总是假的。.（真/假）
- en: False.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: News that everybody agrees with is always accurate. (True/False)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大家都同意的新闻总是准确的。（真/假）
- en: False.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。
- en: Transformers can be used to run sentiment analysis on Tweets. (True/False)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器可以用于对推文进行情感分析。（真/假）
- en: True.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Key entities can be extracted from Facebook messages with a DistilBERT model
    running NER. (True/False)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用 DistilBERT 模型运行 NER 可以从 Facebook 消息中提取关键实体。（真/假）
- en: True.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Key verbs can be identified from YouTube chats with BERT-based models running
    SRL. (True/False)
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从运行 SRL 的基于 BERT 模型的 YouTube 聊天中可以识别关键动词。（真/假）
- en: True.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Emotional reactions are a natural first response to fake news. (True/False)
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 情感反应是对假新闻的自然第一反应。（真/假）
- en: True.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: A rational approach to fake news can help clarify one’s position. (True/False)
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对假新闻的理性处理可以帮助澄清自己的立场。（真/假）
- en: True.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Connecting transformers to reliable websites can help somebody understand why
    some news is fake. (True/False)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接变压器到可靠的网站可以帮助某人理解为什么一些新闻是假的。（真/假）
- en: True.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Transformers can make summaries of reliable websites to help us understand some
    of the topics labeled as fake news. (True/False)
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器可以对可靠网站进行摘要，帮助我们了解一些被标记为假新闻的主题。（真/假）
- en: True.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: You can change the world if you use AI for the good of us all. (True/False)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你为了我们所有人的利益使用人工智能，你可以改变世界。（真/假）
- en: True.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Chapter 14, Interpreting Black Box Transformer Models
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第14章，解释黑匣子变压器模型
- en: BERTViz only shows the output of the last layer of the BERT model. (True/False)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERTViz 只显示了 BERT 模型的最后一层的输出。（真/假）
- en: False. BERTViz displays the outputs of all the layers.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。 BERTViz 显示所有层的输出。
- en: BERTViz shows the attention heads of each layer of a BERT model. (True/False)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERTViz 显示了 BERT 模型每个层的注意力头。（真/假）
- en: True.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: BERTViz shows how the tokens relate to each other. (True/False)
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BERTViz 显示令牌之间的关系。（真/假）
- en: True.
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: LIT shows the inner workings of the attention heads like BERTViz. (True/False)
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LIT 显示了注意力头的内部工作，就像 BERTViz 一样。（真/假）
- en: False. However, LIT makes non-probing predictions.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。然而，LIT 进行非探测性预测。
- en: Probing is a way for an algorithm to predict language representations. (True/False)
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探测是一种算法预测语言表示的方法。（真/假）
- en: True.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: NER is a probing task. (True/False)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NER 是一个探测任务。（真/假）
- en: True.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: PCA and UMAP are non-probing tasks. (True/False)
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA 和 UMAP 是非探测任务。（真/假）
- en: True.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: LIME is model-agnostic. (True/False)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LIME 是模型不可知的。（真/假）
- en: True.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Transformers deepen the relationships of the tokens layer by layer. (True/False)
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器通过逐层加深令牌之间的关系。（真/假）
- en: True.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Visual transformer model interpretation adds a new dimension to interpretable
    AI. (True/False)
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化变压器模型解释为可解释的人工智能增加了一个新维度。（真/假）
- en: True.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: Chapter 15, From NLP to Task-Agnostic Transformer Models
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章，从自然语言处理到任务不可知的变压器模型
- en: Reformer transformer models don’t contain encoders. (True/False)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reformer 变压器模型不包含编码器。（真/假）
- en: False. Reformer transformer models contain encoders.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。Reformer 变压器模型��含编码器。
- en: Reformer transformer models don’t contain decoders. (True/False)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reformer 变压器模型不包含解码器。（真/假）
- en: False. Reformer transformer models contain encoders and decoders.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。Reformer 变压器模型包含编码器和解码器。
- en: The inputs are stored layer by layer in Reformer models. (True/False)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Reformer 模型中，输入按层存储。（真/假）
- en: False. The inputs are recomputed at each level, thus saving memory.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。输入在每个级别重新计算，从而节省内存。
- en: DeBERTa transformer models disentangle content and positions. (True/False)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeBERTa 变压器模型解缠了内容和位置。（真/假）
- en: True.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: It is necessary to test the hundreds of pretrained transformer models before
    choosing one for a project. (True/False)
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择项目的预训练变压器模型之前，有必要测试成百上千个模型。（真/假）
- en: True and False. You can try all of the models, or you can choose a very reliable
    model and implement it to fit your needs.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真和假。你可以尝试所有的模型，或者你可以选择一个非常可靠的模型并实施它以适应你的需求。
- en: The latest transformer model is always the best. (True/False)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最新的变压器模型总是最好的。（真/假）
- en: True and false. A lot of research is being produced on transformers, but some
    experimental models are short-lived. Sometimes, though, the latest model exceeds
    the performance of preceding models.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真和假。关于变压器有很多研究，但是一些实验模型寿命很短。然而，有时最新的模型超出了前期模型的性能。
- en: It is better to have one transformer model per NLP task than one multi-task
    transformer model. (True/False)
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个自然语言处理任务最好有一个变压器模型，而不是一个多任务变压器模型。（真/假）
- en: True and False. This is a personal decision you will have to make. Risk assessment
    is a critical aspect of a project.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真和假。这是你必须做出的个人决定。风险评估是项目的关键方面。
- en: A transformer model always needs to be fine-tuned. (True/False)
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变压器模型总是需要进行微调。（真/假）
- en: False. GPT-3 engines are zero-shot models.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。GPT-3 引擎是零-shot 模型。
- en: OpenAI GPT-3 engines can perform a wide range of NLP tasks without fine-tuning.
    (True/False)
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenAI GPT-3 引擎可以执行各种自然语言处理任务，不需要进行微调。（真/假）
- en: True.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真。
- en: It is always better to implement an AI algorithm on a local server. (True/False)
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地服务器上实施 AI 算法总是更好的。（真/假）
- en: False. It depends on your project. It’s a risk assessment you will have to make.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。这取决于你的项目。这是你必须做出的风险评估。
- en: Chapter 16, The Emergence of Transformer-Driven Copilots
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章，变压器驱动副驾驶的出现
- en: AI copilots that can generate code automatically do not exist. (True/False)
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 能够自动生成代码的 AI 副驾驶不存在。（真/假）
- en: False. GitHub Copilot, for example, is now in production.
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假。比如，GitHub Copilot 现在正在使用中。
- en: AI copilots will never replace humans. (True/False)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 副驾驶永远不会取代人类。（真/假）
- en: True and false. AI will take over many tasks in sales, support, maintenance,
    and other domains. However, many complex tasks will still require human intervention.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 真和假。人工智能将接管许多销售、支持、维护和其他领域的任务。然而，许多复杂的任务仍然需要人类干预。
- en: GPT-3 engines can only do one task. (True/False)
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT-3引擎只能执行一个任务。(True/False)
- en: False. GPT-3 engines can do a wide variety of tasks.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. GPT-3引擎可以执行各种各样的任务。
- en: Transformers can be trained to be recommenders. (True/False)
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformers可以被训练成推荐者。(True/False)
- en: True. Transformers have gone from language sequences to sequences in many domains.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: True. Transformers已经从语言序列转变为许多领域的序列。
- en: Transformers can only process language. (True/False)
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Transformers只能处理语言。(True/False)
- en: False. Once transformers are trained for language sequences, they can analyze
    many other types of sequences.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 一旦Transformer被训练为处理语言序列，它们可以分析许多其他类型的序列。
- en: A transformer sequence can only contain words. (True/False)
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个Transformer序列只能包含单词。(True/False)
- en: False. Once the language sequences are processed, transformers only work on
    numbers, not words.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 一旦语言序列被处理，Transformer只能处理数字，而不是单词。
- en: Vision transformers cannot equal CNNs. (True/False)
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 视觉Transformer不能与CNN相提并论。(True/False)
- en: False. Transformers are deep neural networks that can equal CNNs in computer
    vision.
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. Transformers是深度神经网络，可以与CNN在计算机视觉上相媲美。
- en: AI robots with computer vision do not exist. (True/False)
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 具有计算机视觉的AI机器人不存在。(True/False)
- en: False. For example, robots with computer vision have begun to surface in military
    applications.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 例如，具有计算机视觉的机器人已经开始在军事应用中出现。
- en: It is impossible to produce Python source code automatically. (True/False)
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动产生Python源代码是不可能的。(True/False)
- en: False. Microsoft and OpenAI have joined to produce a copilot that can write
    Python code with us or for us.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: False. 微软和OpenAI联合生产了一个可以与我们一起或为我们编写Python代码的副驾驶。
- en: We might one day become the copilots of robots. (True/False)
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一天可能会成为机器人的副驾驶员。(True/False)
- en: This could be true or false. This remains a challenge for humans, bots, and
    robots in an ever-growing AI ecosystem.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可能是真的也可能是假的。这仍然是人类、机器人和不断发展的AI生态系统中的一个挑战。
- en: Join our book’s Discord space
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间。
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 加入该书的Discord工作空间，每月与作者进行一次*问答*活动：
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
- en: '![](img/QR_Code5134042288713321484.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5134042288713321484.png)'
