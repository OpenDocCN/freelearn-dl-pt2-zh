["```py\n...\ntype Point struct{ X, Y int }\ntype Vector Point\n\ntype Maze struct {\n  // some maze object\n  *mazegen.Maze\n  repr *tensor.Dense\n  iter [][]tile\n  values [][]float32\n\n  player, start, goal Point\n\n  // meta\n\n  r *rand.Rand\n}\n...\n```", "```py\n...\ntype NN struct {\n  g *ExprGraph\n  x *Node\n  y *Node\n  l []FC\n\n  pred *Node\n  predVal Value\n}\n\nfunc NewNN(batchsize int) *NN {\n  g := NewGraph()\n  x := NewMatrix(g, of, WithShape(batchsize, 4), WithName(\"X\"), WithInit(Zeroes()))\n  y := NewVector(g, of, WithShape(batchsize), WithName(\"Y\"), WithInit(Zeroes()))\n...\n```", "```py\npackage main\n\ntype Memory struct {\n  State Point\n  Action Vector\n  Reward float32\n  NextState Point\n  NextMovables []Vector\n  isDone bool\n}\n```", "```py\npackage main\n\nimport (\n  \"fmt\"\n  \"log\"\n  \"math/rand\"\n  \"time\"\n\n  \"gorgonia.org/gorgonia\"\n)\n\nvar cardinals = [4]Vector{\n  Vector{0, 1}, // E\n  Vector{1, 0}, // N\n  Vector{-1, 0}, // S\n  Vector{0, -1}, // W\n}\n```", "```py\ntype DQN struct {\n  *NN\n  gorgonia.VM\n  gorgonia.Solver\n  Memories []Memory // The Q-Table - stores State/Action/Reward/NextState/NextMoves/IsDone - added to each train x times per episode\n\n  gamma float32\n  epsilon float32\n  epsDecayMin float32\n  decay float32\n}\n\nfunc (m *DQN) init() {\n  if _, err := m.NN.cons(); err != nil {\n    panic(err)\n  }\n  m.VM = gorgonia.NewTapeMachine(m.NN.g)\n  m.Solver = gorgonia.NewRMSPropSolver()\n}\n```", "```py\nfunc (m *DQN) replay(batchsize int) error {\n  var N int\n  if batchsize < len(m.Memories) {\n    N = batchsize\n  } else {\n    N = len(m.Memories)\n  }\n  Xs := make([]input, 0, N)\n  Ys := make([]float32, 0, N)\n  mems := make([]Memory, N)\n  copy(mems, m.Memories)\n  rand.Shuffle(len(mems), func(i, j int) {\n    mems[i], mems[j] = mems[j], mems[i]\n  })\n\n  for b := 0; b < batchsize; b++ {\n    mem := mems[b]\n\n    var y float32\n    if mem.isDone {\n      y = mem.Reward\n    } else {\n      var nextRewards []float32\n      for _, next := range mem.NextMovables {\n        nextReward, err := m.predict(mem.NextState, next)\n        if err != nil {\n          return err\n        }\n        nextRewards = append(nextRewards, nextReward)\n      }\n      reward := max(nextRewards)\n      y = mem.Reward + m.gamma*reward\n    }\n    Xs = append(Xs, input{mem.State, mem.Action})\n    Ys = append(Ys, y)\n    if err := m.VM.RunAll(); err != nil {\n      return err\n    }\n    m.VM.Reset()\n    if err := m.Solver.Step(m.model()); err != nil {\n      return err\n    }\n    if m.epsilon > m.epsDecayMin {\n      m.epsilon *= m.decay\n    }\n  }\n  return nil\n}\n```", "```py\nfunc (m *DQN) predict(player Point, action Vector) (float32, error) {\n  x := input{State: player, Action: action}\n  m.Let1(x)\n  if err := m.VM.RunAll(); err != nil {\n    return 0, err\n  }\n  m.VM.Reset()\n  retVal := m.predVal.Data().([]float32)[0]\n  return retVal, nil\n}\n```", "```py\nfunc (m *DQN) train(mz *Maze) (err error) {\n  var episodes = 20000\n  var times = 1000\n  var score float32\n\n  for e := 0; e < episodes; e++ {\n    for t := 0; t < times; t++ {\n      if e%100 == 0 && t%999 == 1 {\n        log.Printf(\"episode %d, %dst loop\", e, t)\n      }\n\n      moves := getPossibleActions(mz)\n      action := m.bestAction(mz, moves)\n      reward, isDone := mz.Value(action)\n      score = score + reward\n      player := mz.player\n      mz.Move(action)\n      nextMoves := getPossibleActions(mz)\n      mem := Memory{State: player, Action: action, Reward: reward, NextState: mz.player, NextMovables: nextMoves, isDone: isDone}\n      m.Memories = append(m.Memories, mem)\n    }\n  }\n  return nil\n}\n```", "```py\nfunc (m *DQN) bestAction(state *Maze, moves []Vector) (bestAction Vector) {\n  var bestActions []Vector\n  var maxActValue float32 = -100\n  for _, a := range moves {\n    actionValue, err := m.predict(state.player, a)\n    if err != nil {\n      // DO SOMETHING\n    }\n    if actionValue > maxActValue {\n      maxActValue = actionValue\n      bestActions = append(bestActions, a)\n    } else if actionValue == maxActValue {\n      bestActions = append(bestActions, a)\n    }\n  }\n  // shuffle bestActions\n  rand.Shuffle(len(bestActions), func(i, j int) {\n    bestActions[i], bestActions[j] = bestActions[j], bestActions[i]\n  })\n  return bestActions[0]\n}\n```", "```py\nfunc getPossibleActions(m *Maze) (retVal []Vector) {\n  for i := range cardinals {\n    if m.CanMoveTo(m.player, cardinals[i]) {\n      retVal = append(retVal, cardinals[i])\n    }\n  }\n  return retVal\n}\n\nfunc max(a []float32) float32 {\n  var m float32 = -999999999\n  for i := range a {\n    if a[i] > m {\n      m = a[i]\n    }\n  }\n  return m\n}\n```", "```py\nfunc main() {\n  // DQN vars\n\n  // var times int = 1000\n  var gamma float32 = 0.95 // discount factor\n  var epsilon float32 = 1.0 // exploration/exploitation bias, set to 1.0/exploration by default\n  var epsilonDecayMin float32 = 0.01\n  var epsilonDecay float32 = 0.995\n\n  rand.Seed(time.Now().UTC().UnixNano())\n  dqn := &DQN{\n    NN: NewNN(32),\n    gamma: gamma,\n    epsilon: epsilon,\n    epsDecayMin: epsilonDecayMin,\n    decay: epsilonDecay,\n  }\n  dqn.init()\n\n  m := NewMaze(5, 10)\n  fmt.Printf(\"%+#v\", m.repr)\n  fmt.Printf(\"%v %v\\n\", m.start, m.goal)\n\n  fmt.Printf(\"%v\\n\", m.CanMoveTo(m.start, Vector{0, 1}))\n  fmt.Printf(\"%v\\n\", m.CanMoveTo(m.start, Vector{1, 0}))\n  fmt.Printf(\"%v\\n\", m.CanMoveTo(m.start, Vector{0, -1}))\n  fmt.Printf(\"%v\\n\", m.CanMoveTo(m.start, Vector{-1, 0}))\n\n  if err := dqn.train(m); err != nil {\n    panic(err)\n  }\n\n  m.Reset()\n  for {\n    moves := getPossibleActions(m)\n    best := dqn.bestAction(m, moves)\n    reward, isDone := m.Value(best)\n    log.Printf(\"\\n%#v\", m.repr)\n    log.Printf(\"player at: %v best: %v\", m.player, best)\n    log.Printf(\"reward %v, done %v\", reward, isDone)\n    m.Move(best)\n  }\n}\n```"]