["```py\n    def g(self, x_i):\n\n        forward_cell = rnn.BasicLSTMCell(32)\n        backward_cell = rnn.BasicLSTMCell(32)\n        outputs, state_forward, state_backward = rnn.static_bidirectional_rnn(forward_cell, backward_cell, x_i, dtype=tf.float32)\n\n        return tf.add(tf.stack(x_i), tf.stack(outputs))\n```", "```py\ncell = rnn.BasicLSTMCell(64)\nprev_state = cell.zero_state(self.batch_size, tf.float32) \n```", "```py\nfor step in xrange(self.processing_steps):\n```", "```py\n    output, state = cell(XHat, prev_state)\n\n    h_k = tf.add(output, XHat)\n```", "```py\n    content_based_attention = tf.nn.softmax(tf.multiply(prev_state[1], g_embedding)) \n    r_k = tf.reduce_sum(tf.multiply(content_based_attention, g_embedding), axis=0) \n```", "```py\nprev_state = rnn.LSTMStateTuple(state[0], tf.add(h_k, r_k))\n```", "```py\n    def f(self, XHat, g_embedding):\n        cell = rnn.BasicLSTMCell(64)\n        prev_state = cell.zero_state(self.batch_size, tf.float32) \n\n        for step in xrange(self.processing_steps):\n            output, state = cell(XHat, prev_state)\n\n            h_k = tf.add(output, XHat) \n\n            content_based_attention = tf.nn.softmax(tf.multiply(prev_state[1], g_embedding)) \n\n            r_k = tf.reduce_sum(tf.multiply(content_based_attention, g_embedding), axis=0) \n\n            prev_state = rnn.LSTMStateTuple(state[0], tf.add(h_k, r_k))\n\n        return output\n```"]