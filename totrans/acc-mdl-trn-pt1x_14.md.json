["```py\nmaicon@packt:~$ mpirun --np 2 my_mpi_program\n```", "```py\nmaicon@packt:~$ mpirun --np 2 --host r1:1,r2:1 my_mpi_program\n```", "```py\nmaicon@packt:~$ mpirun --np 6 --host r1:4,r2:2 my_mpi_program\n```", "```py\nos.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']\ndevice = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n```", "```py\nTRAINING_SCRIPT=$1NPROCS= \"16\"\nHOSTS=\"machine1:8,machine2:8\"\nCOMMAND=\"python $TRAINING_SCRIPT\"\nexport MASTER_ADDR=\"machine1\"\nexport MASTER_PORT= \"12345\"\nmpirun -x MASTER_ADDR -x MASTER_PORT --np $NPROCS --host $HOSTS $COMMAND\n```", "```py\nmaicon@packt:~$ ./launch_multiple_machines.sh distributed-training.py\n```", "```py\n#!/bin/bash#SBATCH -n 16\n#SBATCH --partition=long_job_gpu\n#SBATCH --nodes=2\n#SBATCH --gpus-per-node=8\nexport MASTER_ADDR=$(hostname)\nexport MASTER_PORT= \"12345\"\nmpirun -x MASTER_ADDR -x MASTER_PORT --np 16 python /share/distributed-training.py\n```", "```py\nmaicon@packt:~$ sbatch distributed-training.sbatch\n```"]