["```py\nthor_review = \"the action scenes were top notch in this movie. Thor has never been this epic in the MCU. He does some pretty epic sh*t in this movie and he is definitely not under-powered anymore. Thor in unleashed in this, I love that.\"\n\nprint(list(thor_review))\n\n```", "```py\n\n#Results\n['t', 'h', 'e', ' ', 'a', 'c', 't', 'i', 'o', 'n', ' ', 's', 'c', 'e', 'n', 'e', 's', ' ', 'w', 'e', 'r', 'e', ' ', 't', 'o', 'p', ' ', 'n', 'o', 't', 'c', 'h', ' ', 'i', 'n', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'v', 'i', 'e', '.', ' ', 'T', 'h', 'o', 'r', ' ', 'h', 'a', 's', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 'b', 'e', 'e', 'n', ' ', 't', 'h', 'i', 's', ' ', 'e', 'p', 'i', 'c', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'M', 'C', 'U', '.', ' ', 'H', 'e', ' ', 'd', 'o', 'e', 's', ' ', 's', 'o', 'm', 'e', ' ', 'p', 'r', 'e', 't', 't', 'y', ' ', 'e', 'p', 'i', 'c', ' ', 's', 'h', '*', 't', ' ', 'i', 'n', ' ', 't', 'h', 'i', 's', ' ', 'm', 'o', 'v', 'i', 'e', ' ', 'a', 'n', 'd', ' ', 'h', 'e', ' ', 'i', 's', ' ', 'd', 'e', 'f', 'i', 'n', 'i', 't', 'e', 'l', 'y', ' ', 'n', 'o', 't', ' ', 'u', 'n', 'd', 'e', 'r', '-', 'p', 'o', 'w', 'e', 'r', 'e', 'd', ' ', 'a', 'n', 'y', 'm', 'o', 'r', 'e', '.', ' ', 'T', 'h', 'o', 'r', ' ', 'i', 'n', ' ', 'u', 'n', 'l', 'e', 'a', 's', 'h', 'e', 'd', ' ', 'i', 'n', ' ', 't', 'h', 'i', 's', ',', ' ', 'I', ' ', 'l', 'o', 'v', 'e', ' ', 't', 'h', 'a', 't', '.']\n```", "```py\nprint(Thor_review.split())\n\n#Results\n\n['the', 'action', 'scenes', 'were', 'top', 'notch', 'in', 'this', 'movie.', 'Thor', 'has', 'never', 'been', 'this', 'epic', 'in', 'the', 'MCU.', 'He', 'does', 'some', 'pretty', 'epic', 'sh*t', 'in', 'this', 'movie', 'and', 'he', 'is', 'definitely', 'not', 'under-powered', 'anymore.', 'Thor', 'in', 'unleashed', 'in', 'this,', 'I', 'love', 'that.']\n```", "```py\nfrom nltk import ngrams\n\nprint(list(ngrams(thor_review.split(),2)))\n\n#Results\n[('the', 'action'), ('action', 'scenes'), ('scenes', 'were'), ('were', 'top'), ('top', 'notch'), ('notch', 'in'), ('in', 'this'), ('this', 'movie.'), ('movie.', 'Thor'), ('Thor', 'has'), ('has', 'never'), ('never', 'been'), ('been', 'this'), ('this', 'epic'), ('epic', 'in'), ('in', 'the'), ('the', 'MCU.'), ('MCU.', 'He'), ('He', 'does'), ('does', 'some'), ('some', 'pretty'), ('pretty', 'epic'), ('epic', 'sh*t'), ('sh*t', 'in'), ('in', 'this'), ('this', 'movie'), ('movie', 'and'), ('and', 'he'), ('he', 'is'), ('is', 'definitely'), ('definitely', 'not'), ('not', 'under-powered'), ('under-powered', 'anymore.'), ('anymore.', 'Thor'), ('Thor', 'in'), ('in', 'unleashed'), ('unleashed', 'in'), ('in', 'this,'), ('this,', 'I'), ('I', 'love'), ('love', 'that.')]\n```", "```py\nprint(list(ngrams(thor_review.split(),3)))\n\n#Results\n\n[('the', 'action', 'scenes'), ('action', 'scenes', 'were'), ('scenes', 'were', 'top'), ('were', 'top', 'notch'), ('top', 'notch', 'in'), ('notch', 'in', 'this'), ('in', 'this', 'movie.'), ('this', 'movie.', 'Thor'), ('movie.', 'Thor', 'has'), ('Thor', 'has', 'never'), ('has', 'never', 'been'), ('never', 'been', 'this'), ('been', 'this', 'epic'), ('this', 'epic', 'in'), ('epic', 'in', 'the'), ('in', 'the', 'MCU.'), ('the', 'MCU.', 'He'), ('MCU.', 'He', 'does'), ('He', 'does', 'some'), ('does', 'some', 'pretty'), ('some', 'pretty', 'epic'), ('pretty', 'epic', 'sh*t'), ('epic', 'sh*t', 'in'), ('sh*t', 'in', 'this'), ('in', 'this', 'movie'), ('this', 'movie', 'and'), ('movie', 'and', 'he'), ('and', 'he', 'is'), ('he', 'is', 'definitely'), ('is', 'definitely', 'not'), ('definitely', 'not', 'under-powered'), ('not', 'under-powered', 'anymore.'), ('under-powered', 'anymore.', 'Thor'), ('anymore.', 'Thor', 'in'), ('Thor', 'in', 'unleashed'), ('in', 'unleashed', 'in'), ('unleashed', 'in', 'this,'), ('in', 'this,', 'I'), ('this,', 'I', 'love'), ('I', 'love', 'that.')]\n```", "```py\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n        self.length = 0\n\n    def add_word(self,word):\n        if word not in self.idx2word:\n            self.idx2word.append(word)\n            self.word2idx[word] = self.length + 1\n            self.length += 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n    def onehot_encoded(self,word):\n        vec = np.zeros(self.length)\n        vec[self.word2idx[word]] = 1\n        return vec\n\n```", "```py\ndic = Dictionary()\n\nfor tok in thor_review.split():\n    dic.add_word(tok)\n\nprint(dic.word2idx)\n```", "```py\n# Results of word2idx\n\n{'the': 1, 'action': 2, 'scenes': 3, 'were': 4, 'top': 5, 'notch': 6, 'in': 7, 'this': 8, 'movie.': 9, 'Thor': 10, 'has': 11, 'never': 12, 'been': 13, 'epic': 14, 'MCU.': 15, 'He': 16, 'does': 17, 'some': 18, 'pretty': 19, 'sh*t': 20, 'movie': 21, 'and': 22, 'he': 23, 'is': 24, 'definitely': 25, 'not': 26, 'under-powered': 27, 'anymore.': 28, 'unleashed': 29, 'this,': 30, 'I': 31, 'love': 32, 'that.': 33}\n\n```", "```py\n# One-hot representation of the word 'were'\ndic.onehot_encoded('were')\narray([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n        0.,  0.,  0.,  0.,  0.,  0.,  0.])\n```", "```py\npip install torchtext\n```", "```py\nfrom torchtext import data\nTEXT = data.Field(lower=True, batch_first=True,fix_length=20)\nLABEL = data.Field(sequential=False)\n```", "```py\ntrain, test = datasets.IMDB.splits(TEXT, LABEL)\n```", "```py\nprint('train.fields', train.fields)\n\n#Results \ntrain.fields {'text': <torchtext.data.field.Field object at 0x1129db160>, 'label': <torchtext.data.field.Field object at 0x1129db1d0>}\n\nprint(vars(train[0]))\n\n#Results \n\nvars(train[0]) {'text': ['for', 'a', 'movie', 'that', 'gets', 'no', 'respect', 'there', 'sure', 'are', 'a', 'lot', 'of', 'memorable', 'quotes', 'listed', 'for', 'this', 'gem.', 'imagine', 'a', 'movie', 'where', 'joe', 'piscopo', 'is', 'actually', 'funny!', 'maureen', 'stapleton', 'is', 'a', 'scene', 'stealer.', 'the', 'moroni', 'character', 'is', 'an', 'absolute', 'scream.', 'watch', 'for', 'alan', '\"the', 'skipper\"', 'hale', 'jr.', 'as', 'a', 'police', 'sgt.'], 'label': 'pos'}\n```", "```py\nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\nLABEL.build_vocab(train)\n```", "```py\nprint(TEXT.vocab.freqs)\n\n# A sample result \nCounter({\"i'm\": 4174,\n         'not': 28597,\n         'tired': 328,\n         'to': 133967,\n         'say': 4392,\n         'this': 69714,\n         'is': 104171,\n         'one': 22480,\n         'of': 144462,\n         'the': 322198,\n```", "```py\n\nprint(TEXT.vocab.vectors)\n\n#Results displaying the 300 dimension vector for each word.\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.0466 0.2132 -0.0074 ... 0.0091 -0.2099 0.0539\n      ... ⋱ ... \n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n0.7724 -0.1800 0.2072 ... 0.6736 0.2263 -0.2919\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n[torch.FloatTensor of size 10002x300]\n\nprint(TEXT.vocab.stoi)\n\n# Sample results\ndefaultdict(<function torchtext.vocab._default_unk_index>,\n            {'<unk>': 0,\n             '<pad>': 1,\n             'the': 2,\n             'a': 3,\n             'and': 4,\n             'of': 5,\n             'to': 6,\n             'is': 7,\n             'in': 8,\n             'i': 9,\n             'this': 10,\n             'that': 11,\n             'it': 12,\n```", "```py\ntrain_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=128, device=-1,shuffle=True)\n#device = -1 represents cpu , if u want gpu leave it to None.\n```", "```py\nbatch = next(iter(train_iter))\nbatch.text\n\n#Results\nVariable containing:\n 5128 427 19 ... 1688 0 542\n   58 2 0 ... 2 0 1352\n    0 9 14 ... 2676 96 9\n       ... ⋱ ... \n  129 1181 648 ... 45 0 2\n 6484 0 627 ... 381 5 2\n  748 0 5052 ... 18 6660 9827\n[torch.LongTensor of size 128x20]\n\nbatch.label\n\n#Results\nVariable containing:\n 2\n 1\n 2\n 1\n 2\n 1\n 1\n 1\n[torch.LongTensor of size 128]\n```", "```py\nclass EmbNet(nn.Module):\n    def __init__(self,emb_size,hidden_size1,hidden_size2=400):\n        super().__init__()\n        self.embedding = nn.Embedding(emb_size,hidden_size1)\n        self.fc = nn.Linear(hidden_size2,3)\n\n    def forward(self,x):\n        embeds = self.embedding(x).view(x.size(0),-1)\n        out = self.fc(embeds)\n        return F.log_softmax(out,dim=-1)\n```", "```py\ndef fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , batch in enumerate(data_loader):\n        text , target = batch.text , batch.label\n        if is_cuda:\n            text,target = text.cuda(),target.cuda()\n\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(text)\n        loss = F.nll_loss(output,target)\n\n        running_loss += F.nll_loss(output,target,size_average=False).data[0]\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n\n    loss = running_loss/len(data_loader.dataset)\n    accuracy = 100\\. * running_correct/len(data_loader.dataset)\n\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy\n\ntrain_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\n\ntrain_iter.repeat = False\ntest_iter.repeat = False\n\nfor epoch in range(1,10):\n\n    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n```", "```py\nfrom torchtext.vocab import GloVe\nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\nLABEL.build_vocab(train,)\n```", "```py\nTEXT.vocab.vectors\n\n#Output\n0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n 0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n 0.0466 0.2132 -0.0074 ... 0.0091 -0.2099 0.0539\n          ... ⋱ ... \n 0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n 0.7724 -0.1800 0.2072 ... 0.6736 0.2263 -0.2919\n 0.0000 0.0000 0.0000 ... 0.0000 0.0000 0.0000\n[torch.FloatTensor of size 10002x300]\n```", "```py\nmodel.embedding.weight.data = TEXT.vocab.vectors\n```", "```py\nclass EmbNet(nn.Module):\n    def __init__(self,emb_size,hidden_size1,hidden_size2=400):\n        super().__init__()\n        self.embedding = nn.Embedding(emb_size,hidden_size1)\n        self.fc1 = nn.Linear(hidden_size2,3)\n\n    def forward(self,x):\n        embeds = self.embedding(x).view(x.size(0),-1)\n        out = self.fc1(embeds)\n        return F.log_softmax(out,dim=-1)\n\nmodel = EmbNet(len(TEXT.vocab.stoi),300,12000)\n```", "```py\nmodel.embedding.weight.requires_grad = False\noptimizer = optim.SGD([ param for param in model.parameters() if param.requires_grad == True],lr=0.001)\n```", "```py\nrnn = RNN(input_size, hidden_size,output_size)\nfor i in range(len(Thor_review):\n        output, hidden = rnn(thor_review[i], hidden)\n```", "```py\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return Variable(torch.zeros(1, self.hidden_size))\n```", "```py\nTEXT = data.Field(lower=True,fix_length=200,batch_first=False)\nLABEL = data.Field(sequential=False,)\ntrain, test = IMDB.splits(TEXT, LABEL)\nTEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300),max_size=10000,min_freq=10)\nLABEL.build_vocab(train,)\n```", "```py\ntrain_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=32, device=-1)\ntrain_iter.repeat = False\ntest_iter.repeat = False\n```", "```py\nclass IMDBRnn(nn.Module):\n\n    def __init__(self,vocab,hidden_size,n_cat,bs=1,nl=2):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.bs = bs\n        self.nl = nl\n        self.e = nn.Embedding(n_vocab,hidden_size)\n        self.rnn = nn.LSTM(hidden_size,hidden_size,nl)\n        self.fc2 = nn.Linear(hidden_size,n_cat)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self,inp):\n        bs = inp.size()[1]\n        if bs != self.bs:\n            self.bs = bs\n        e_out = self.e(inp)\n        h0 = c0 = Variable(e_out.data.new(*(self.nl,self.bs,self.hidden_size)).zero_())\n        rnn_o,_ = self.rnn(e_out,(h0,c0)) \n        rnn_o = rnn_o[-1]\n        fc = F.dropout(self.fc2(rnn_o),p=0.8)\n        return self.softmax(fc)\n```", "```py\nmodel = IMDBRnn(n_vocab,n_hidden,3,bs=32)\nmodel = model.cuda()\n\noptimizer = optim.Adam(model.parameters(),lr=1e-3)\n\ndef fit(epoch,model,data_loader,phase='training',volatile=False):\n    if phase == 'training':\n        model.train()\n    if phase == 'validation':\n        model.eval()\n        volatile=True\n    running_loss = 0.0\n    running_correct = 0\n    for batch_idx , batch in enumerate(data_loader):\n        text , target = batch.text , batch.label\n        if is_cuda:\n            text,target = text.cuda(),target.cuda()\n\n        if phase == 'training':\n            optimizer.zero_grad()\n        output = model(text)\n        loss = F.nll_loss(output,target)\n\n        running_loss += F.nll_loss(output,target,size_average=False).data[0]\n        preds = output.data.max(dim=1,keepdim=True)[1]\n        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n        if phase == 'training':\n            loss.backward()\n            optimizer.step()\n\n    loss = running_loss/len(data_loader.dataset)\n    accuracy = 100\\. * running_correct/len(data_loader.dataset)\n\n    print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}')\n    return loss,accuracy\n\ntrain_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\n\nfor epoch in range(1,5):\n\n    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n\n```", "```py\n#Results\n\ntraining loss is   0.7 and training accuracy is 12564/25000     50.26\nvalidation loss is   0.7 and validation accuracy is 12500/25000      50.0\ntraining loss is  0.66 and training accuracy is 14931/25000     59.72\nvalidation loss is  0.57 and validation accuracy is 17766/25000     71.06\ntraining loss is  0.43 and training accuracy is 20229/25000     80.92\nvalidation loss is   0.4 and validation accuracy is 20446/25000     81.78\ntraining loss is   0.3 and training accuracy is 22026/25000      88.1\nvalidation loss is  0.37 and validation accuracy is 21009/25000     84.04\n```", "```py\nclass IMDBCnn(nn.Module):\n\n    def __init__(self,vocab,hidden_size,n_cat,bs=1,kernel_size=3,max_len=200):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.bs = bs\n    self.e = nn.Embedding(n_vocab,hidden_size)\n    self.cnn = nn.Conv1d(max_len,hidden_size,kernel_size)\n    self.avg = nn.AdaptiveAvgPool1d(10)\n        self.fc = nn.Linear(1000,n_cat)\n        self.softmax = nn.LogSoftmax(dim=-1)\n\n    def forward(self,inp):\n        bs = inp.size()[0]\n        if bs != self.bs:\n            self.bs = bs\n        e_out = self.e(inp)\n        cnn_o = self.cnn(e_out) \n        cnn_avg = self.avg(cnn_o)\n        cnn_avg = cnn_avg.view(self.bs,-1)\n        fc = F.dropout(self.fc(cnn_avg),p=0.5)\n        return self.softmax(fc)\n```", "```py\ntrain_losses , train_accuracy = [],[]\nval_losses , val_accuracy = [],[]\n\nfor epoch in range(1,5):\n\n    epoch_loss, epoch_accuracy = fit(epoch,model,train_iter,phase='training')\n    val_epoch_loss , val_epoch_accuracy = fit(epoch,model,test_iter,phase='validation')\n    train_losses.append(epoch_loss)\n    train_accuracy.append(epoch_accuracy)\n    val_losses.append(val_epoch_loss)\n    val_accuracy.append(val_epoch_accuracy)\n```", "```py\ntraining loss is  0.59 and training accuracy is 16724/25000      66.9\nvalidation loss is  0.45 and validation accuracy is 19687/25000     78.75\ntraining loss is  0.38 and training accuracy is 20876/25000      83.5\nvalidation loss is   0.4 and validation accuracy is 20618/25000     82.47\ntraining loss is  0.28 and training accuracy is 22109/25000     88.44\nvalidation loss is  0.41 and validation accuracy is 20713/25000     82.85\ntraining loss is  0.22 and training accuracy is 22820/25000     91.28\nvalidation loss is  0.44 and validation accuracy is 20641/25000     82.56\n```"]