- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Parallelizing Neural Network Training with PyTorch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 并行化神经网络训练
- en: In this chapter, we will move on from the mathematical foundations of machine
    learning and deep learning to focus on PyTorch. PyTorch is one of the most popular
    deep learning libraries currently available, and it lets us implement **neural
    networks** (**NNs**) much more efficiently than any of our previous NumPy implementations.
    In this chapter, we will start using PyTorch and see how it brings significant
    benefits to training performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从机器学习和深度学习的数学基础转向 PyTorch。PyTorch 是目前最流行的深度学习库之一，它让我们比以前的任何 NumPy 实现更高效地实现**神经网络**（**NNs**）。在本章中，我们将开始使用
    PyTorch，看看它如何显著提升训练性能。
- en: 'This chapter will begin the next stage of our journey into machine learning
    and deep learning, and we will explore the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将开始我们进入机器学习和深度学习的下一阶段的旅程，我们将探讨以下主题：
- en: How PyTorch improves training performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 如何提升训练性能
- en: Working with PyTorch’s `Dataset` and `DataLoader` to build input pipelines and
    enable efficient model training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 的 `Dataset` 和 `DataLoader` 构建输入管道，实现高效的模型训练
- en: Working with PyTorch to write optimized machine learning code
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 编写优化的机器学习代码
- en: Using the `torch.nn` module to implement common deep learning architectures
    conveniently
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `torch.nn` 模块方便地实现常见的深度学习架构
- en: Choosing activation functions for artificial NNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择人工神经网络的激活函数
- en: PyTorch and training performance
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 和训练性能
- en: PyTorch can speed up our machine learning tasks significantly. To understand
    how it can do this, let’s begin by discussing some of the performance challenges
    we typically run into when we execute expensive calculations on our hardware.
    Then, we will take a high-level look at what PyTorch is and what our learning
    approach will be in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 可以显著加速我们的机器学习任务。要理解它是如何做到这一点的，请让我们首先讨论我们在执行昂贵计算时通常遇到的一些性能挑战。然后，我们将从高层次来看
    PyTorch 是什么，以及本章中我们的学习方法会是什么样的。
- en: Performance challenges
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能挑战
- en: The performance of computer processors has, of course, been continuously improving
    in recent years. That allows us to train more powerful and complex learning systems,
    which means that we can improve the predictive performance of our machine learning
    models. Even the cheapest desktop computer hardware that’s available right now
    comes with processing units that have multiple cores.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，计算机处理器的性能在近年来一直在不断提升。这使得我们能够训练更强大和复杂的学习系统，这意味着我们可以提高机器学习模型的预测性能。即使是现在最便宜的桌面计算机硬件也配备有具有多个核心的处理单元。
- en: In the previous chapters, we saw that many functions in scikit-learn allow us
    to spread those computations over multiple processing units. However, by default,
    Python is limited to execution on one core due to the **global interpreter lock**
    (**GIL**). So, although we indeed take advantage of Python’s multiprocessing library
    to distribute our computations over multiple cores, we still have to consider
    that the most advanced desktop hardware rarely comes with more than 8 or 16 such
    cores.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们看到 scikit-learn 中的许多函数允许我们将计算分布到多个处理单元上。然而，默认情况下，由于**全局解释器锁**（**GIL**），Python
    只能在一个核心上执行。因此，尽管我们确实利用 Python 的多进程库将计算分布到多个核心上，但我们仍然必须考虑，即使是最先进的桌面硬件也很少配备超过 8
    或 16 个这样的核心。
- en: You will recall from *Chapter 11*, *Implementing a Multilayer Artificial Neural
    Network from Scratch*, that we implemented a very simple **multilayer perceptron**
    (**MLP**) with only one hidden layer consisting of 100 units. We had to optimize
    approximately 80,000 weight parameters ([784*100 + 100] + [100 * 10] + 10 = 79,510)
    for a very simple image classification task. The images in MNIST are rather small
    (28×28), and we can only imagine the explosion in the number of parameters if
    we wanted to add additional hidden layers or work with images that have higher
    pixel densities. Such a task would quickly become unfeasible for a single processing
    unit. The question then becomes, how can we tackle such problems more effectively?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会回忆起*第11章*，*从头开始实现多层人工神经网络*，我们实现了一个非常简单的**多层感知器**（**MLP**），只有一个包含100个单元的隐藏层。我们必须优化大约80,000个权重参数（[784*100
    + 100] + [100 * 10] + 10 = 79,510）来进行一个非常简单的图像分类任务。MNIST数据集中的图像相当小（28×28），如果我们想要添加额外的隐藏层或者处理像素密度更高的图像，我们可以想象参数数量的激增。这样的任务很快就会对单个处理单元变得不可行。因此问题变成了，我们如何更有效地解决这些问题？
- en: 'The obvious solution to this problem is to use **graphics processing units**
    (**GPUs**), which are real workhorses. You can think of a graphics card as a small
    computer cluster inside your machine. Another advantage is that modern GPUs are
    great value compared to the state-of-the-art **central processing units** (**CPUs**),
    as you can see in the following overview:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的显而易见的解决方案是使用**图形处理单元**（**GPUs**），它们是真正的工作马。你可以把显卡想象成你的机器内部的一个小型计算机集群。另一个优势是，与最先进的**中央处理单元**（**CPUs**）相比，现代GPU性价比非常高，如下面的概述所示：
- en: '![](img/B17582_12_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_01.png)'
- en: 'Figure 12.1: Comparison of a state-of-the-art CPU and GPU'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：现代CPU和GPU的比较
- en: 'The sources for the information in *Figure 12.1* are the following websites
    (date accessed: July 2021):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.1*中信息的来源是以下网站（访问日期：2021年7月）:'
- en: '[https://ark.intel.com/content/www/us/en/ark/products/215570/intel-core-i9-11900kb-processor-24m-cache-up-to-4-90-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/215570/intel-core-i9-11900kb-processor-24m-cache-up-to-4-90-ghz.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ark.intel.com/content/www/us/en/ark/products/215570/intel-core-i9-11900kb-processor-24m-cache-up-to-4-90-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/215570/intel-core-i9-11900kb-processor-24m-cache-up-to-4-90-ghz.html)'
- en: '[https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3080-3080ti/](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3080-3080ti/)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3080-3080ti/](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3080-3080ti/)'
- en: At 2.2 times the price of a modern CPU, we can get a GPU that has 640 times
    more cores and is capable of around 46 times more floating-point calculations
    per second. So, what is holding us back from utilizing GPUs for our machine learning
    tasks? The challenge is that writing code to target GPUs is not as simple as executing
    Python code in our interpreter. There are special packages, such as CUDA and OpenCL,
    that allow us to target the GPU. However, writing code in CUDA or OpenCL is probably
    not the most convenient way to implement and run machine learning algorithms.
    The good news is that this is what PyTorch was developed for!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以现代CPU的价格的2.2倍，我们可以获得一个GPU，它拥有640倍的核心数，并且每秒可以进行大约46倍的浮点计算。那么，是什么阻碍了我们利用GPU来进行机器学习任务？挑战在于编写目标为GPU的代码并不像在解释器中执行Python代码那么简单。有一些特殊的包，比如CUDA和OpenCL，可以让我们针对GPU进行编程。然而，用CUDA或OpenCL编写代码可能不是实现和运行机器学习算法的最方便的方式。好消息是，这正是PyTorch开发的目的！
- en: What is PyTorch?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是PyTorch？
- en: PyTorch is a scalable and multiplatform programming interface for implementing
    and running machine learning algorithms, including convenience wrappers for deep
    learning. PyTorch was primarily developed by the researchers and engineers from
    the **Facebook AI Research** (**FAIR**) lab. Its development also involves many
    contributions from the community. PyTorch was initially released in September
    2016 and is free and open source under the modified BSD license. Many machine
    learning researchers and practitioners from academia and industry have adapted
    PyTorch to develop deep learning solutions, such as Tesla Autopilot, Uber’s Pyro,
    and Hugging Face’s Transformers ([https://pytorch.org/ecosystem/](https://pytorch.org/ecosystem/)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个可扩展且多平台的编程接口，用于实现和运行机器学习算法，包括深度学习的便捷包装器。PyTorch主要由来自**Facebook AI
    Research**（**FAIR**）实验室的研究人员和工程师开发。其开发还涉及来自社区的许多贡献。PyTorch最初发布于2016年9月，以修改的BSD许可证免费开源。许多来自学术界和工业界的机器学习研究人员和从业者已经采用PyTorch来开发深度学习解决方案，例如Tesla
    Autopilot、Uber的Pyro和Hugging Face的Transformers（[https://pytorch.org/ecosystem/](https://pytorch.org/ecosystem/)）。
- en: To improve the performance of training machine learning models, PyTorch allows
    execution on CPUs, GPUs, and XLA devices such as TPUs. However, its greatest performance
    capabilities can be discovered when using GPUs and XLA devices. PyTorch supports
    CUDA-enabled and ROCm GPUs officially. PyTorch’s development is based on the Torch
    library ([www.torch.ch](http://torch.ch/)). As its name implies, the Python interface
    is the primary development focus of PyTorch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高训练机器学习模型的性能，PyTorch允许在CPU、GPU和XLA设备（如TPU）上执行。然而，当使用GPU和XLA设备时，PyTorch具有最优的性能能力。PyTorch官方支持CUDA启用和ROCm
    GPU。PyTorch的开发基于Torch库（[www.torch.ch](http://torch.ch/)）。顾名思义，Python接口是PyTorch的主要开发重点。
- en: PyTorch is built around a computation graph composed of a set of nodes. Each
    node represents an operation that may have zero or more inputs or outputs. PyTorch
    provides an imperative programming environment that evaluates operations, executes
    computation, and returns concrete values immediately. Hence, the computation graph
    in PyTorch is defined implicitly, rather than constructed in advance and executed
    after.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch围绕着一个计算图构建，由一组节点组成。每个节点表示一个可能有零个或多个输入或输出的操作。PyTorch提供了一种即时评估操作、执行计算并立即返回具体值的命令式编程环境。因此，PyTorch中的计算图是隐式定义的，而不是事先构建并在执行之后执行。
- en: Mathematically, tensors can be understood as a generalization of scalars, vectors,
    matrices, and so on. More concretely, a scalar can be defined as a rank-0 tensor,
    a vector can be defined as a rank-1 tensor, a matrix can be defined as a rank-2
    tensor, and matrices stacked in a third dimension can be defined as rank-3 tensors.
    Tensors in PyTorch are similar to NumPy’s arrays, except that tensors are optimized
    for automatic differentiation and can run on GPUs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，张量可以理解为标量、向量、矩阵等的一般化。更具体地说，标量可以定义为秩为0的张量，向量可以定义为秩为1的张量，矩阵可以定义为秩为2的张量，而在第三维堆叠的矩阵可以定义为秩为3的张量。PyTorch中的张量类似于NumPy的数组，但张量经过了优化以进行自动微分并能在GPU上运行。
- en: 'To make the concept of a tensor clearer, consider *Figure 12.2*, which represents
    tensors of ranks 0 and 1 in the first row, and tensors of ranks 2 and 3 in the
    second row:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要更清晰地理解张量的概念，请参考*图12.2*，该图展示了第一行中秩为0和1的张量，以及第二行中秩为2和3的张量：
- en: '![](img/B17582_12_02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_02.png)'
- en: 'Figure 12.2: Different types of tensor in PyTorch'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：PyTorch中不同类型的张量
- en: Now that we know what PyTorch is, let’s see how to use it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了PyTorch是什么，让我们看看如何使用它。
- en: How we will learn PyTorch
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们将如何学习PyTorch
- en: First, we are going to cover PyTorch’s programming model, in particular, creating
    and manipulating tensors. Then, we will see how to load data and utilize the `torch.utils.data`
    module, which will allow us to iterate through a dataset efficiently. In addition,
    we will discuss the existing, ready-to-use datasets in the `torch.utils.data.Dataset`
    submodule and learn how to use them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍PyTorch的编程模型，特别是如何创建和操作张量。然后，我们将看看如何加载数据并利用`torch.utils.data`模块，这将允许我们高效地迭代数据集。此外，我们将讨论`torch.utils.data.Dataset`子模块中现有的即用即得数据集，并学习如何使用它们。
- en: After learning about these basics, the PyTorch neural network `torch.nn` module
    will be introduced. Then, we will move forward to building machine learning models,
    learn how to compose and train the models, and learn how to save the trained models
    on disk for future evaluation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了这些基础知识后，PyTorch 神经网络模块 `torch.nn` 将被介绍。然后，我们将继续构建机器学习模型，学习如何组合和训练这些模型，并了解如何将训练好的模型保存在磁盘上以供未来评估使用。
- en: First steps with PyTorch
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 的首次使用步骤
- en: In this section, we will take our first steps in using the low-level PyTorch
    API. After installing PyTorch, we will cover how to create tensors in PyTorch
    and different ways of manipulating them, such as changing their shape, data type,
    and so on.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将初步了解使用低级别的 PyTorch API。在安装 PyTorch 后，我们将介绍如何在 PyTorch 中创建张量以及不同的操作方法，例如更改它们的形状、数据类型等。
- en: Installing PyTorch
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: To install PyTorch, we recommend consulting the latest instructions on the official
    [https://pytorch.org](https://pytorch.org) website. Below, we will outline the
    basic steps that will work on most systems.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 PyTorch，建议参阅官方网站 [https://pytorch.org](https://pytorch.org) 上的最新说明。以下是适用于大多数系统的基本步骤概述。
- en: 'Depending on how your system is set up, you can typically just use Python’s
    `pip` installer and install PyTorch from PyPI by executing the following from
    your terminal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据系统设置的不同，通常您只需使用 Python 的 `pip` 安装程序，并通过终端执行以下命令从 PyPI 安装 PyTorch：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will install the latest *stable* version, which is 1.9.0 at the time of
    writing. To install the 1.9.0 version, which is guaranteed to be compatible with
    the following code examples, you can modify the preceding command as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装最新的 *稳定* 版本，在撰写时是 1.9.0。要安装 1.9.0 版本，该版本确保与以下代码示例兼容，您可以按照以下方式修改前述命令：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to use GPUs (recommended), you need a compatible NVIDIA graphics
    card that supports CUDA and cuDNN. If your machine satisfies these requirements,
    you can install PyTorch with GPU support, as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望使用 GPU（推荐），则需要一台兼容 CUDA 和 cuDNN 的 NVIDIA 显卡。如果您的计算机符合这些要求，您可以按照以下步骤安装支持
    GPU 的 PyTorch：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'for CUDA 11.1 or:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于 CUDA 11.1 或：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: for CUDA 10.2 as of the time of writing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止适用于 CUDA 10.2。
- en: 'As macOS binaries don’t support CUDA, you can install from source: [https://pytorch.org/get-started/locally/#mac-from-source](https://pytorch.org/get-started/locally/#mac-from-source).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 macOS 二进制版本不支持 CUDA，您可以从源代码安装：[https://pytorch.org/get-started/locally/#mac-from-source](https://pytorch.org/get-started/locally/#mac-from-source)。
- en: For more information about the installation and setup process, please see the
    official recommendations at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 关于安装和设置过程的更多信息，请参阅官方建议，网址为[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)。
- en: 'Note that PyTorch is under active development; therefore, every couple of months,
    new versions are released with significant changes. You can verify your PyTorch
    version from your terminal, as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，PyTorch 处于活跃开发阶段，因此每隔几个月就会发布带有重大更改的新版本。您可以通过终端验证您的 PyTorch 版本，方法如下：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Troubleshooting your installation of PyTorch**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决 PyTorch 安装问题**'
- en: 'If you experience problems with the installation procedure, read more about
    system- and platform-specific recommendations that are provided at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
    Note that all the code in this chapter can be run on your CPU; using a GPU is
    entirely optional but recommended if you want to fully enjoy the benefits of PyTorch.
    For example, while training some NN models on a CPU could take a week, the same
    models could be trained in just a few hours on a modern GPU. If you have a graphics
    card, refer to the installation page to set it up appropriately. In addition,
    you may find this setup guide helpful, which explains how to install the NVIDIA
    graphics card drivers, CUDA, and cuDNN on Ubuntu (not required but recommended
    requirements for running PyTorch on a GPU): [https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf).
    Furthermore, as you will see in *Chapter 17*, *Generative Adversarial Networks
    for Synthesizing New Data*, you can also train your models using a GPU for free
    via Google Colab.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在安装过程中遇到问题，请阅读有关特定系统和平台的推荐信息，网址为[https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)。请注意，本章中的所有代码都可以在您的CPU上运行；使用GPU完全是可选的，但如果您想充分享受PyTorch的好处，则建议使用GPU。例如，使用CPU训练某些神经网络模型可能需要一周时间，而在现代GPU上，同样的模型可能只需几小时。如果您有显卡，请参考安装页面适当设置。此外，您可能会发现这篇设置指南有用，其中解释了如何在Ubuntu上安装NVIDIA显卡驱动程序、CUDA和cuDNN（虽然不是运行PyTorch在GPU上所需的必备条件，但推荐要求）：[https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf)。此外，正如您将在*第17章*中看到的，*生成对抗网络用于合成新数据*，您还可以免费使用Google
    Colab通过GPU训练您的模型。
- en: Creating tensors in PyTorch
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在PyTorch中创建张量
- en: 'Now, let’s consider a few different ways of creating tensors, and then see
    some of their properties and how to manipulate them. Firstly, we can simply create
    a tensor from a list or a NumPy array using the `torch.tensor` or the `torch.from_numpy`
    function as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑几种不同的方式来创建张量，然后看看它们的一些属性以及如何操作它们。首先，我们可以使用`torch.tensor`或`torch.from_numpy`函数从列表或NumPy数组创建张量，如下所示：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This resulted in tensors `t_a` and `t_b`, with their properties, `shape=(3,)`
    and `dtype=int32`, adopted from their source. Similar to NumPy arrays, we can
    also see these properties:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了张量`t_a`和`t_b`，它们的属性为，`shape=(3,)` 和 `dtype=int32`，这些属性是从它们的源头继承而来。与NumPy数组类似，我们也可以看到这些属性：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, creating a tensor of random values can be done as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以如下方式创建随机值张量：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Manipulating the data type and shape of a tensor
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作张量的数据类型和形状
- en: Learning ways to manipulate tensors is necessary to make them compatible for
    input to a model or an operation. In this section, you will learn how to manipulate
    tensor data types and shapes via several PyTorch functions that cast, reshape,
    transpose, and squeeze (remove dimensions).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何操作张量以使它们适合模型或操作的输入是必要的。在本节中，您将通过几个PyTorch函数学习如何通过类型转换、重塑、转置和挤压（去除维度）来操作张量的数据类型和形状。
- en: 'The `torch.to()` function can be used to change the data type of a tensor to
    a desired type:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.to()`函数可用于将张量的数据类型更改为所需类型：'
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: See [https://pytorch.org/docs/stable/tensor_attributes.html](https://pytorch.org/docs/stable/tensor_attributes.html)
    for all other data types.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看[https://pytorch.org/docs/stable/tensor_attributes.html](https://pytorch.org/docs/stable/tensor_attributes.html)获取所有其他数据类型的信息。
- en: 'As you will see in upcoming chapters, certain operations require that the input
    tensors have a certain number of dimensions (that is, rank) associated with a
    certain number of elements (shape). Thus, we might need to change the shape of
    a tensor, add a new dimension, or squeeze an unnecessary dimension. PyTorch provides
    useful functions (or operations) to achieve this, such as `torch.transpose()`,
    `torch.reshape()`, and `torch.squeeze()`. Let’s take a look at some examples:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在接下来的章节中看到的，某些操作要求输入张量具有特定数量的维度（即秩），并与一定数量的元素（形状）相关联。因此，我们可能需要改变张量的形状，添加一个新维度或挤压一个不必要的维度。PyTorch提供了一些有用的函数（或操作）来实现这一点，如`torch.transpose()`、`torch.reshape()`和`torch.squeeze()`。让我们看一些例子：
- en: 'Transposing a tensor:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转置张量：
- en: '[PRE9]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Reshaping a tensor (for example, from a 1D vector to a 2D array):'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '重塑张量（例如，从1D向量到2D数组）:'
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Removing the unnecessary dimensions (dimensions that have size 1, which are
    not needed):'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除不必要的维度（即大小为1的维度）：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Applying mathematical operations to tensors
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对张量应用数学操作
- en: Applying mathematical operations, in particular linear algebra operations, is
    necessary for building most machine learning models. In this subsection, we will
    cover some widely used linear algebra operations, such as element-wise product,
    matrix multiplication, and computing the norm of a tensor.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数学运算，特别是线性代数运算，是构建大多数机器学习模型所必需的。在这个子节中，我们将介绍一些广泛使用的线性代数操作，例如逐元素乘积、矩阵乘法和计算张量的范数。
- en: 'First, let’s instantiate two random tensors, one with uniform distribution
    in the range [–1, 1) and the other with a standard normal distribution:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化两个随机张量，一个具有在[–1, 1)范围内均匀分布的值，另一个具有标准正态分布：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that `torch.rand` returns a tensor filled with random numbers from a uniform
    distribution in the range of [0, 1).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`torch.rand`返回一个填充有从[0, 1)范围内均匀分布的随机数的张量。
- en: 'Notice that `t1` and `t2` have the same shape. Now, to compute the element-wise
    product of `t1` and `t2`, we can use the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`t1`和`t2`具有相同的形状。现在，要计算`t1`和`t2`的逐元素乘积，可以使用以下方法：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To compute the mean, sum, and standard deviation along a certain axis (or axes),
    we can use `torch.mean()`, `torch.sum()`, and `torch.std()`. For example, the
    mean of each column in `t1` can be computed as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要沿着某个轴（或轴）计算均值、总和和标准偏差，可以使用`torch.mean()`、`torch.sum()`和`torch.std()`。例如，可以如下计算`t1`中每列的均值：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The matrix-matrix product between `t1` and `t2` (that is, ![](img/B17582_12_001.png),
    where the superscript T is for transpose) can be computed by using the `torch.matmul()`
    function as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torch.matmul()`函数可以计算`t1`和`t2`的矩阵乘积（即，![](img/B17582_12_001.png)，其中上标T表示转置）：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'On the other hand, computing ![](img/B17582_12_002.png) is performed by transposing
    `t1`, resulting in an array of size 2×2:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，通过对`t1`进行转置来计算![](img/B17582_12_002.png)，结果是一个大小为2×2的数组：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, the `torch.linalg.norm()` function is useful for computing the *L*^p
    norm of a tensor. For example, we can calculate the *L*² norm of `t1` as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`torch.linalg.norm()`函数对于计算张量的*L*^p范数非常有用。例如，我们可以如下计算`t1`的*L*²范数：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Split, stack, and concatenate tensors
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割、堆叠和连接张量
- en: 'In this subsection, we will cover PyTorch operations for splitting a tensor
    into multiple tensors, or the reverse: stacking and concatenating multiple tensors
    into a single one.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个子节中，我们将介绍PyTorch操作，用于将一个张量分割成多个张量，或者反过来，将多个张量堆叠和连接成一个单独的张量。
- en: 'Assume that we have a single tensor, and we want to split it into two or more
    tensors. For this, PyTorch provides a convenient `torch.chunk()` function, which
    divides an input tensor into a list of equally sized tensors. We can determine
    the desired number of splits as an integer using the `chunks` argument to split
    a tensor along the desired dimension specified by the `dim` argument. In this
    case, the total size of the input tensor along the specified dimension must be
    divisible by the desired number of splits. Alternatively, we can provide the desired
    sizes in a list using the `torch.split()` function. Let’s have a look at an example
    of both these options:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个单一的张量，并且我们想将它分成两个或更多的张量。为此，PyTorch提供了一个便捷的`torch.chunk()`函数，它将输入张量分割成等大小的张量列表。我们可以使用`chunks`参数作为整数来确定所需的分割数，以`dim`参数指定沿所需维度分割张量。在这种情况下，沿指定维度的输入张量的总大小必须是所需分割数的倍数。另外，我们可以使用`torch.split()`函数在列表中提供所需的大小。让我们看看这两个选项的示例：
- en: 'Providing the number of splits:'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供分割数量：
- en: '[PRE18]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example, a tensor of size 6 was divided into a list of three tensors
    each with size 2\. If the tensor size is not divisible by the `chunks` value,
    the last chunk will be smaller.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，一个大小为6的张量被分割成了一个包含三个大小为2的张量的列表。如果张量大小不能被`chunks`值整除，则最后一个块将更小。
- en: 'Providing the sizes of different splits:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供不同分割的大小：
- en: 'Alternatively, instead of defining the number of splits, we can also specify
    the sizes of the output tensors directly. Here, we are splitting a tensor of size
    `5` into tensors of sizes `3` and `2`:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，可以直接指定输出张量的大小，而不是定义分割的数量。在这里，我们将一个大小为`5`的张量分割为大小为`3`和`2`的张量：
- en: '[PRE19]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Sometimes, we are working with multiple tensors and need to concatenate or
    stack them to create a single tensor. In this case, PyTorch functions such as
    `torch.stack()` and `torch.cat()` come in handy. For example, let’s create a 1D
    tensor, `A`, containing 1s with size `3,` and a 1D tensor, `B`, containing 0s
    with size `2,` and concatenate them into a 1D tensor, `C`, of size `5`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们需要处理多个张量，并需要将它们连接或堆叠以创建一个单一的张量。在这种情况下，PyTorch 的函数如`torch.stack()`和`torch.cat()`非常方便。例如，让我们创建一个包含大小为`3`的1D张量`A`，其元素全为1，并且一个包含大小为`2`的1D张量`B`，其元素全为0，然后将它们连接成一个大小为`5`的1D张量`C`：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we create 1D tensors `A` and `B`, both with size `3`, then we can stack
    them together to form a 2D tensor, `S`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建了大小为`3`的1D张量`A`和`B`，那么我们可以将它们堆叠在一起形成一个2D张量`S`：
- en: '[PRE21]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The PyTorch API has many operations that you can use for building a model, processing
    your data, and more. However, covering every function is outside the scope of
    this book, where we will focus on the most essential ones. For the full list of
    operations and functions, you can refer to the documentation page of PyTorch at
    [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch API具有许多操作，您可以用它们来构建模型、处理数据等。然而，覆盖每个函数超出了本书的范围，我们将专注于最基本的那些。有关所有操作和函数的完整列表，请参阅PyTorch文档页面：[https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)。
- en: Building input pipelines in PyTorch
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中构建输入流水线
- en: When we are training a deep NN model, we usually train the model incrementally
    using an iterative optimization algorithm such as stochastic gradient descent,
    as we have seen in previous chapters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练深度神经网络模型时，通常使用迭代优化算法（例如随机梯度下降）逐步训练模型，正如我们在前几章中所看到的。
- en: As mentioned at the beginning of this chapter, `torch.nn` is a module for building
    NN models. In cases where the training dataset is rather small and can be loaded
    as a tensor into the memory, we can directly use this tensor for training. In
    typical use cases, however, when the dataset is too large to fit into the computer
    memory, we will need to load the data from the main storage device (for example,
    the hard drive or solid-state drive) in chunks, that is, batch by batch. (Note
    the use of the term “batch” instead of “mini-batch” in this chapter to stay close
    to the PyTorch terminology.) In addition, we may need to construct a data-processing
    pipeline to apply certain transformations and preprocessing steps to our data,
    such as mean centering, scaling, or adding noise to augment the training procedure
    and to prevent overfitting.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头所提到的，`torch.nn`是用于构建神经网络模型的模块。在训练数据集相当小并且可以作为张量直接加载到内存中的情况下，我们可以直接使用这个张量进行训练。然而，在典型的使用情况下，当数据集过大以至于无法完全装入计算机内存时，我们将需要以批次的方式从主存储设备（例如硬盘或固态硬盘）加载数据。此外，我们可能需要构建一个数据处理流水线，对数据应用某些转换和预处理步骤，如均值中心化、缩放或添加噪声，以增强训练过程并防止过拟合。
- en: Applying preprocessing functions manually every time can be quite cumbersome.
    Luckily, PyTorch provides a special class for constructing efficient and convenient
    preprocessing pipelines. In this section, we will see an overview of different
    methods for constructing a PyTorch `Dataset` and `DataLoader`, and implementing
    data loading, shuffling, and batching.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 每次手动应用预处理函数可能会相当繁琐。幸运的是，PyTorch 提供了一个特殊的类来构建高效和方便的预处理流水线。在本节中，我们将看到构建 PyTorch
    `Dataset` 和 `DataLoader` 的不同方法的概述，并实现数据加载、洗牌和分批处理。
- en: Creating a PyTorch DataLoader from existing tensors
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从现有张量创建 PyTorch DataLoader
- en: 'If the data already exists in the form of a tensor object, a Python list, or
    a NumPy array, we can easily create a dataset loader using the `torch.utils.data.DataLoader()`
    class. It returns an object of the `DataLoader` class, which we can use to iterate
    through the individual elements in the input dataset. As a simple example, consider
    the following code, which creates a dataset from a list of values from 0 to 5:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据已经以张量对象、Python 列表或NumPy数组的形式存在，我们可以很容易地使用`torch.utils.data.DataLoader()`类创建数据集加载器。它返回一个`DataLoader`类的对象，我们可以用它来迭代输入数据集中的各个元素。作为一个简单的例子，考虑下面的代码，它从值为0到5的列表创建一个数据集：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can easily iterate through a dataset entry by entry as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地逐个遍历数据集的条目，如下所示：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we want to create batches from this dataset, with a desired batch size of
    `3`, we can do this with the `batch_size` argument as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望从该数据集创建批次，批次大小为`3`，我们可以使用`batch_size`参数如下进行操作：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will create two batches from this dataset, where the first three elements
    go into batch #1, and the remaining elements go into batch #2\. The optional `drop_last`
    argument is useful for cases when the number of elements in the tensor is not
    divisible by the desired batch size. We can drop the last non-full batch by setting
    `drop_last` to `True`. The default value for `drop_last` is `False`.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '这将从该数据集创建两个批次，其中前三个元素进入批次 #1，其余元素进入批次 #2。可选的`drop_last`参数在张量中的元素数不能被所需批次大小整除时非常有用。我们可以通过将`drop_last`设置为`True`来丢弃最后一个不完整的批次。`drop_last`的默认值为`False`。'
- en: We can always iterate through a dataset directly, but as you just saw, `DataLoader`
    provides an automatic and customizable batching to a dataset.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接迭代数据集，但正如您刚看到的，`DataLoader`提供了对数据集的自动和可定制的批处理。
- en: Combining two tensors into a joint dataset
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将两个张量合并为联合数据集
- en: Often, we may have the data in two (or possibly more) tensors. For example,
    we could have a tensor for features and a tensor for labels. In such cases, we
    need to build a dataset that combines these tensors, which will allow us to retrieve
    the elements of these tensors in tuples.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们可能有两个（或更多）张量的数据。例如，我们可以有一个特征张量和一个标签张量。在这种情况下，我们需要构建一个结合这些张量的数据集，这将允许我们以元组形式检索这些张量的元素。
- en: 'Assume that we have two tensors, `t_x` and `t_y`. Tensor `t_x` holds our feature
    values, each of size `3`, and `t_y` stores the class labels. For this example,
    we first create these two tensors as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个张量，`t_x`和`t_y`。张量`t_x`保存我们的特征值，每个大小为`3`，而`t_y`存储类标签。对于这个例子，我们首先创建这两个张量如下：
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we want to create a joint dataset from these two tensors. We first need
    to create a `Dataset` class as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望从这两个张量创建一个联合数据集。我们首先需要创建一个`Dataset`类，如下所示：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'A custom `Dataset` class must contain the following methods to be used by the
    data loader later on:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义的`Dataset`类必须包含以下方法，以便稍后由数据加载器使用：
- en: '`__init__()`: This is where the initial logic happens, such as reading existing
    arrays, loading a file, filtering data, and so forth.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__()`: 这是初始逻辑发生的地方，例如读取现有数组、加载文件、过滤数据等。'
- en: '`__getitem__()`: This returns the corresponding sample to the given index.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__getitem__()`: 这将返回给定索引的对应样本。'
- en: 'Then we create a joint dataset of `t_x` and `t_y` with the custom `Dataset`
    class as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用自定义的`Dataset`类从`t_x`和`t_y`创建一个联合数据集，如下所示：
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we can print each example of the joint dataset as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以如下打印联合数据集的每个示例：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can also simply utilize the `torch.utils.data.TensorDataset` class, if the
    second dataset is a labeled dataset in the form of tensors. So, instead of using
    our self-defined `Dataset` class, `JointDataset`, we can create a joint dataset
    as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第二个数据集是张量形式的带标签数据集，我们也可以简单地利用`torch.utils.data.TensorDataset`类。因此，我们可以如下创建一个联合数据集，而不是使用我们自定义的`Dataset`类`JointDataset`：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that a common source of error could be that the element-wise correspondence
    between the original features (*x*) and labels (*y*) might be lost (for example,
    if the two datasets are shuffled separately). However, once they are merged into
    one dataset, it is safe to apply these operations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一个常见的错误来源可能是原始特征（*x*）和标签（*y*）之间的逐元素对应关系可能会丢失（例如，如果两个数据集分别被洗牌）。然而，一旦它们合并成一个数据集，就可以安全地应用这些操作。
- en: If we have a dataset created from the list of image filenames on disk, we can
    define a function to load the images from these filenames. You will see an example
    of applying multiple transformations to a dataset later in this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从磁盘上的图像文件名列表创建了数据集，我们可以定义一个函数来从这些文件名加载图像。您将在本章后面看到将多个转换应用于数据集的示例。
- en: Shuffle, batch, and repeat
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洗牌、批处理和重复
- en: As was mentioned in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*, when training an NN model using stochastic gradient descent
    optimization, it is important to feed training data as randomly shuffled batches.
    You have already seen how to specify the batch size using the `batch_size` argument
    of a data loader object. Now, in addition to creating batches, you will see how
    to shuffle and reiterate over the datasets. We will continue working with the
    previous joint dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*第2章*中提到的，*用于分类的简单机器学习算法的训练*，在使用随机梯度下降优化训练NN模型时，重要的是以随机打乱的批次方式提供训练数据。您已经看到如何使用数据加载器对象的`batch_size`参数指定批次大小。现在，除了创建批次之外，您还将看到如何对数据集进行洗牌和重新迭代。我们将继续使用之前的联合数据集。
- en: 'First, let’s create a shuffled version data loader from the `joint_dataset`
    dataset:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从`joint_dataset`数据集创建一个打乱顺序的数据加载器：
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here, each batch contains two data records (*x*) and the corresponding labels
    (*y*). Now we iterate through the data loader entry by entry as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个批次包含两个数据记录（*x*）和相应的标签（*y*）。现在我们逐条通过数据加载器迭代数据入口如下：
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The rows are shuffled without losing the one-to-one correspondence between the
    entries in `x` and `y`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 行被随机打乱，但不会丢失`x`和`y`条目之间的一一对应关系。
- en: 'In addition, when training a model for multiple epochs, we need to shuffle
    and iterate over the dataset by the desired number of epochs. So, let’s iterate
    over the batched dataset twice:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在训练模型多个epochs时，我们需要按所需的epochs数量对数据集进行洗牌和迭代。因此，让我们对批处理数据集进行两次迭代：
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This results in two different sets of batches. In the first epoch, the first
    batch contains a pair of values `[y=1, y=2]`, and the second batch contains a
    pair of values `[y=3, y=0]`. In the second epoch, two batches contain a pair of
    values, `[y=2, y=0]` and `[y=1, y=3]` respectively. For each iteration, the elements
    within a batch are also shuffled.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了两组不同的批次。在第一个epoch中，第一批次包含一对值`[y=1, y=2]`，第二批次包含一对值`[y=3, y=0]`。在第二个epoch中，两个批次分别包含一对值`[y=2,
    y=0]`和`[y=1, y=3]`。对于每次迭代，批次内的元素也被打乱了。
- en: Creating a dataset from files on your local storage disk
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从本地存储磁盘上的文件创建数据集
- en: In this section, we will build a dataset from image files stored on disk. There
    is an image folder associated with the online content of this chapter. After downloading
    the folder, you should be able to see six images of cats and dogs in JPEG format.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从存储在磁盘上的图像文件构建数据集。本章的在线内容与一个图像文件夹相关联。下载文件夹后，您应该能够看到六张猫和狗的JPEG格式图像。
- en: 'This small dataset will show how building a dataset from stored files generally
    works. To accomplish this, we are going to use two additional modules: `Image`
    in `PIL` to read the image file contents and `transforms` in `torchvision` to
    decode the raw contents and resize the images.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小数据集将展示如何从存储的文件中构建数据集。为此，我们将使用两个额外的模块：`PIL`中的`Image`来读取图像文件内容和`torchvision`中的`transforms`来解码原始内容并调整图像大小。
- en: 'The `PIL.Image` and `torchvision.transforms` modules provide a lot of additional
    and useful functions, which are beyond the scope of the book. You are encouraged
    to browse through the official documentation to learn more about these functions:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`PIL.Image`和`torchvision.transforms`模块提供了许多额外和有用的函数，这超出了本书的范围。建议您浏览官方文档以了解更多有关这些函数的信息：'
- en: '[https://pillow.readthedocs.io/en/stable/reference/Image.html](https://pillow.readthedocs.io/en/stable/reference/Image.html)
    for `PIL.Image`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://pillow.readthedocs.io/en/stable/reference/Image.html](https://pillow.readthedocs.io/en/stable/reference/Image.html)提供了关于`PIL.Image`的参考文档'
- en: '[https://pytorch.org/vision/stable/transforms.html](https://pytorch.org/vision/stable/transforms.html)
    for `torchvision.transforms`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://pytorch.org/vision/stable/transforms.html](https://pytorch.org/vision/stable/transforms.html)提供了关于`torchvision.transforms`的参考文档'
- en: 'Before we start, let’s take a look at the content of these files. We will use
    the `pathlib` library to generate a list of image files:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们看一下这些文件的内容。我们将使用`pathlib`库生成一个图像文件列表：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we will visualize these image examples using Matplotlib:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Matplotlib可视化这些图像示例：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*Figure 12.3* shows the example images:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.3*显示了示例图像：'
- en: '![](img/B17582_12_03.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_03.png)'
- en: 'Figure 12.3: Images of cats and dogs'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：猫和狗的图像
- en: 'Just from this visualization and the printed image shapes, we can already see
    that the images have different aspect ratios. If you print the aspect ratios (or
    data array shapes) of these images, you will see that some images are 900 pixels
    high and 1200 pixels wide (900×1200), some are 800×1200, and one is 900×742\.
    Later, we will preprocess these images to a consistent size. Another point to
    consider is that the labels for these images are provided within their filenames.
    So, we extract these labels from the list of filenames, assigning label `1` to
    dogs and label `0` to cats:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过这个可视化和打印的图像形状，我们就能看到这些图像具有不同的长宽比。如果打印这些图像的长宽比（或数据数组形状），您会看到一些图像高900像素，宽1200像素（900×1200），一些是800×1200，还有一个是900×742。稍后，我们将把这些图像预处理到一个统一的尺寸。另一个需要考虑的问题是这些图像的标签是作为它们的文件名提供的。因此，我们从文件名列表中提取这些标签，将标签`1`分配给狗，标签`0`分配给猫：
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we have two lists: a list of filenames (or paths of each image) and a
    list of their labels. In the previous section, you learned how to create a joint
    dataset from two arrays. Here, we will do the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个列表：一个是文件名列表（或每个图像的路径），另一个是它们的标签列表。在前一节中，您学习了如何从两个数组创建一个联合数据集。在这里，我们将执行以下操作：
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The joint dataset has filenames and labels.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 联合数据集具有文件名和标签。
- en: 'Next, we need to apply transformations to this dataset: load the image content
    from its file path, decode the raw content, and resize it to a desired size, for
    example, 80×120\. As mentioned before, we use the `torchvision.transforms` module
    to resize the images and convert the loaded pixels into tensors as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对这个数据集应用转换：从文件路径加载图像内容，解码原始内容，并将其调整为所需尺寸，例如80×120。如前所述，我们使用`torchvision.transforms`模块将图像调整大小并将加载的像素转换为张量，操作如下：
- en: '[PRE37]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we update the `ImageDataset` class with the `transform` we just defined:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用刚定义的`transform`更新`ImageDataset`类：
- en: '[PRE38]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, we visualize these transformed image examples using Matplotlib:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Matplotlib可视化这些转换后的图像示例：
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This results in the following visualization of the retrieved example images,
    along with their labels:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致检索到的示例图像以及它们的标签的以下可视化：
- en: '![](img/B17582_12_04.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_04.png)'
- en: 'Figure 12.4: Images are labeled'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：图像带有标签
- en: The `__getitem__` method in the `ImageDataset` class wraps all four steps into
    a single function, including the loading of the raw content (images and labels),
    decoding the images into tensors, and resizing the images. The function then returns
    a dataset that we can iterate over and apply other operations that we learned
    about in the previous sections via a data loader, such as shuffling and batching.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`ImageDataset`类中的`__getitem__`方法将所有四个步骤封装到一个函数中，包括加载原始内容（图像和标签），将图像解码为张量并调整图像大小。然后，该函数返回一个数据集，我们可以通过数据加载器迭代，并应用前面章节中学到的其他操作，如随机排列和分批处理。'
- en: Fetching available datasets from the torchvision.datasets library
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从`torchvision.datasets`库获取可用数据集
- en: The `torchvision.datasets` library provides a nice collection of freely available
    image datasets for training or evaluating deep learning models. Similarly, the
    `torchtext.datasets` library provides datasets for natural language. Here, we
    use `torchvision.datasets` as an example.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.datasets`库提供了一组精美的免费图像数据集，用于训练或评估深度学习模型。类似地，`torchtext.datasets`库提供了用于自然语言的数据集。在这里，我们以`torchvision.datasets`为例。'
- en: The `torchvision` datasets ([https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html))
    are nicely formatted and come with informative descriptions, including the format
    of features and labels and their type and dimensionality, as well as the link
    to the original source of the dataset. Another advantage is that these datasets
    are all subclasses of `torch.utils.data.Dataset`, so all the functions we covered
    in the previous sections can be used directly. So, let’s see how to use these
    datasets in action.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`数据集（[https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)）的格式很好，并带有信息性的描述，包括特征和标签的格式及其类型和维度，以及数据集的原始来源的链接。另一个优点是这些数据集都是`torch.utils.data.Dataset`的子类，因此我们在前面章节中涵盖的所有功能都可以直接使用。那么，让我们看看如何在实际中使用这些数据集。'
- en: 'First, if you haven’t already installed `torchvision` together with PyTorch
    earlier, you need to install the `torchvision` library via `pip` from the command
    line:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果您之前没有与PyTorch一起安装`torchvision`，则需要从命令行使用`pip`安装`torchvision`库：
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You can take a look at the list of available datasets at [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看[https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)上的可用数据集列表。
- en: 'In the following paragraphs, we will cover fetching two different datasets:
    CelebA (`celeb_a`) and the MNIST digit dataset.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我们将介绍获取两个不同数据集的方法：CelebA (`celeb_a`)和MNIST数字数据集。
- en: 'Let’s first work with the CelebA dataset ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    with `torchvision.datasets.CelebA` ([https://pytorch.org/vision/stable/datasets.html#celeba](https://pytorch.org/vision/stable/datasets.html#celeba)).
    The description of `torchvision.datasets.CelebA` provides some useful information
    to help us understand the structure of this dataset:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用CelebA数据集 ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))，使用`torchvision.datasets.CelebA`
    ([https://pytorch.org/vision/stable/datasets.html#celeba](https://pytorch.org/vision/stable/datasets.html#celeba))。`torchvision.datasets.CelebA`的描述提供了一些有用的信息，帮助我们理解这个数据集的结构：
- en: The database has three subsets, `'train'`, `'valid'`, and `'test'`. We can select
    a specific subset or load all of them with the `split` parameter.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库有三个子集，分别是`'train'`、`'valid'`和`'test'`。我们可以通过`split`参数选择特定的子集或加载它们全部。
- en: The images are stored in `PIL.Image` format. And we can obtain a transformed
    version using a custom `transform` function, such as `transforms.ToTensor` and
    `transforms.Resize`.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像以`PIL.Image`格式存储。我们可以使用自定义的`transform`函数获得变换后的版本，例如`transforms.ToTensor`和`transforms.Resize`。
- en: There are different types of targets we can use, including `'attributes'`, `'identity'`,
    and `'landmarks'`. `'attributes'` is 40 facial attributes for the person in the
    image, such as facial expression, makeup, hair properties, and so on; `'identity'`
    is the person ID for an image; and `'``landmarks'` refers to the dictionary of
    extracted facial points, such as the position of the eyes, nose, and so on.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用不同类型的目标，包括`'attributes'`、`'identity'`和`'landmarks'`。`'attributes'`是图像中人物的40个面部属性，例如面部表情、化妆、头发属性等；`'identity'`是图像的人物ID；而`'landmarks'`指的是提取的面部点字典，如眼睛、鼻子等位置。
- en: 'Next, we will call the `torchvision.datasets.CelebA` class to download the
    data, store it on disk in a designated folder, and load it into a `torch.utils.data.Dataset`
    object:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调用`torchvision.datasets.CelebA`类来下载数据，将其存储在指定文件夹中，并将其加载到`torch.utils.data.Dataset`对象中：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You may run into a `BadZipFile: File is not a zip file` error, or `RuntimeError:
    The daily quota of the file img_align_celeba.zip is exceeded and it can''t be
    downloaded. This is a limitation of Google Drive and can only be overcome by trying
    again later`; it just means that Google Drive has a daily maximum quota that is
    exceeded by the CelebA files. To work around it, you can manually download the
    files from the source: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
    In the downloaded folder, `celeba/`, you can unzip the `img_align_celeba.zip`
    file. The `image_path` is the root of the downloaded folder, `celeba/`. If you
    have already downloaded the files once, you can simply set `download=False`. For
    additional information and guidance, we highly recommend to see accompanying code
    notebook at [https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part1.ipynb](https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part1.ipynb).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '您可能会遇到`BadZipFile: File is not a zip file`错误，或者`RuntimeError: The daily quota
    of the file img_align_celeba.zip is exceeded and it can''t be downloaded. This
    is a limitation of Google Drive and can only be overcome by trying again later`；这意味着Google
    Drive的每日下载配额已超过CelebA文件的大小限制。为了解决这个问题，您可以从源地址手动下载文件：[http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)。在下载的`celeba/`文件夹中，您可以解压`img_align_celeba.zip`文件。`image_path`是下载文件夹`celeba/`的根目录。如果您已经下载过文件一次，您可以简单地将`download=False`设置为`True`。如需更多信息和指导，请查看附带的代码笔记本：[https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part1.ipynb](https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part1.ipynb)。'
- en: 'Now that we have instantiated the datasets, let’s check if the object is of
    the `torch.utils.data.Dataset` class:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实例化了数据集，让我们检查对象是否是`torch.utils.data.Dataset`类：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As mentioned, the dataset is already split into train, test, and validation
    datasets, and we only load the train set. And we only use the `''attributes''`
    target. In order to see what the data examples look like, we can execute the following
    code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集已经分为训练集、测试集和验证集，我们只加载训练集。我们只使用`'attributes'`目标。为了查看数据示例的外观，我们可以执行以下代码：
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that the sample in this dataset comes in a tuple of `(PIL.Image, attributes)`.
    If we want to pass this dataset to a supervised deep learning model during training,
    we have to reformat it as a tuple of `(features tensor, label)`. For the label,
    we will use the `'Smiling'` category from the attributes as an example, which
    is the 31st element.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此数据集中的样本以 `(PIL.Image, attributes)` 的元组形式出现。如果我们希望在训练过程中将此数据集传递给监督深度学习模型，我们必须将其重新格式化为
    `(features tensor, label)` 的元组形式。例如，我们将使用属性中的 `'Smiling'` 类别作为标签，这是第31个元素。
- en: 'Finally, let’s take the first 18 examples from it to visualize them with their
    `''Smiling''` labels:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们从中获取前18个示例，以其 `'Smiling'` 标签可视化它们：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The examples and their labels that are retrieved from `celeba_dataset` are
    shown in *Figure 12.5*:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `celeba_dataset` 中检索到的示例及其标签显示在 *Figure 12.5* 中：
- en: '![A collage of a person  Description automatically generated with low confidence](img/B17582_12_05.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![一个人的拼贴  自动以低置信度生成的描述](img/B17582_12_05.png)'
- en: 'Figure 12.5: Model predicts smiling celebrities'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 12.5: 模型预测微笑名人'
- en: This was all we needed to do to fetch and use the CelebA image dataset.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要做的一切，以获取并使用CelebA图像数据集。
- en: 'Next, we will proceed with the second dataset from `torchvision.datasets.MNIST`
    ([https://pytorch.org/vision/stable/datasets.html#mnist](https://pytorch.org/vision/stable/datasets.html#mnist)).
    Let’s see how it can be used to fetch the MNIST digit dataset:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续使用 `torchvision.datasets.MNIST` ([https://pytorch.org/vision/stable/datasets.html#mnist](https://pytorch.org/vision/stable/datasets.html#mnist))
    中的第二个数据集。让我们看看如何使用它来获取MNIST手写数字数据集：
- en: The database has two partitions, `'train'` and `'test'`. We need to select a
    specific subset to load.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库有两个分区，分别是 `'train'` 和 `'test'`。我们需要选择特定的子集进行加载。
- en: The images are stored in `PIL.Image` format. And we can obtain a transformed
    version using a custom `transform` function, such as `transforms.ToTensor` and
    `transforms.Resize`.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像以 `PIL.Image` 格式存储。我们可以使用自定义的 `transform` 函数获取其转换版本，例如 `transforms.ToTensor`
    和 `transforms.Resize`。
- en: There are 10 classes for the target, from `0` to `9`.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标有10个类别，从 `0` 到 `9`。
- en: 'Now, we can download the `''train''` partition, convert the elements to tuples,
    and visualize 10 examples:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以下载 `'train'` 分区，将元素转换为元组，并可视化10个示例：
- en: '[PRE45]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The retrieved example handwritten digits from this dataset are shown as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个数据集中检索到的示例手写数字如下所示：
- en: '![](img/B17582_12_06.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_06.png)'
- en: 'Figure 12.6: Correctly identifying handwritten digits'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 12.6: 正确识别手写数字'
- en: This concludes our coverage of building and manipulating datasets and fetching
    datasets from the `torchvision.datasets` library. Next, we will see how to build
    NN models in PyTorch.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们关于构建和操作数据集以及从`torchvision.datasets`库获取数据集的覆盖。接下来，我们将看到如何在PyTorch中构建NN模型。
- en: Building an NN model in PyTorch
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中构建NN模型
- en: So far in this chapter, you have learned about the basic utility components
    of PyTorch for manipulating tensors and organizing data into formats that we can
    iterate over during training. In this section, we will finally implement our first
    predictive model in PyTorch. As PyTorch is a bit more flexible but also more complex
    than machine learning libraries such as scikit-learn, we will start with a simple
    linear regression model.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，到目前为止，您已经了解了PyTorch的基本实用组件，用于操作张量并将数据组织成可以在训练期间迭代的格式。在本节中，我们将最终在PyTorch中实现我们的第一个预测模型。由于PyTorch比scikit-learn等机器学习库更加灵活但也更加复杂，我们将从一个简单的线性回归模型开始。
- en: The PyTorch neural network module (torch.nn)
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch神经网络模块（torch.nn）
- en: '`torch.nn` is an elegantly designed module developed to help create and train
    NNs. It allows easy prototyping and the building of complex models in just a few
    lines of code.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.nn` 是一个设计优雅的模块，旨在帮助创建和训练神经网络。它允许在几行代码中轻松进行原型设计和构建复杂模型。'
- en: To fully utilize the power of the module and customize it for your problem,
    you need to understand what it’s doing. To develop this understanding, we will
    first train a basic linear regression model on a toy dataset without using any
    features from the `torch.nn` module; we will use nothing but the basic PyTorch
    tensor operations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要充分利用该模块的功能，并为您的问题定制它，您需要理解它在做什么。为了发展这种理解，我们将首先在一个玩具数据集上训练一个基本的线性回归模型，而不使用任何来自
    `torch.nn` 模块的特性；我们只会使用基本的PyTorch张量操作。
- en: Then, we will incrementally add features from `torch.nn` and `torch.optim`.
    As you will see in the following subsections, these modules make building an NN
    model extremely easy. We will also take advantage of the dataset pipeline functionalities
    supported in PyTorch, such as `Dataset` and `DataLoader`, which you learned about
    in the previous section. In this book, we will use the `torch.nn` module to build
    NN models.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将逐步添加来自`torch.nn`和`torch.optim`的特性。正如您将在接下来的小节中看到的，这些模块使得构建NN模型变得极其简单。我们还将利用PyTorch中支持的数据集流水线功能，例如`Dataset`和`DataLoader`，这些您在前一节已经了解过。在本书中，我们将使用`torch.nn`模块来构建NN模型。
- en: The most commonly used approach for building an NN in PyTorch is through `nn.Module`,
    which allows layers to be stacked to form a network. This gives us more control
    over the forward pass. We will see examples of building an NN model using the
    `nn.Module` class.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中构建NN的最常用方法是通过`nn.Module`，它允许将层堆叠起来形成网络。这使我们能够更好地控制前向传播。我们将看到使用`nn.Module`类构建NN模型的示例。
- en: Finally, as you will see in the following subsections, a trained model can be
    saved and reloaded for future use.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如您将在接下来的小节中看到的，训练好的模型可以保存并重新加载以供将来使用。
- en: Building a linear regression model
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建线性回归模型
- en: 'In this subsection, we will build a simple model to solve a linear regression
    problem. First, let’s create a toy dataset in NumPy and visualize it:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小节中，我们将构建一个简单的模型来解决线性回归问题。首先，让我们在NumPy中创建一个玩具数据集并可视化它：
- en: '[PRE46]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As a result, the training examples will be shown in a scatterplot as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练样本将如下显示在散点图中：
- en: '![](img/B17582_12_07.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_07.png)'
- en: 'Figure 12.7: A scatterplot of the training examples'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7：训练样本的散点图
- en: 'Next, we will standardize the features (mean centering and dividing by the
    standard deviation) and create a PyTorch `Dataset` for the training set and a
    corresponding `DataLoader`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将标准化特征（平均中心化和除以标准差），并为训练集创建一个PyTorch的`Dataset`及其相应的`DataLoader`：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, we set a batch size of `1` for the `DataLoader`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为`DataLoader`设置了批大小为`1`。
- en: Now, we can define our model for linear regression as *z* = *wx* + *b*. Here,
    we are going to use the `torch.nn` module. It provides predefined layers for building
    complex NN models, but to start, you will learn how to define a model from scratch.
    Later in this chapter, you will see how to use those predefined layers.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们的线性回归模型为 *z* = *wx* + *b*。在这里，我们将使用`torch.nn`模块。它提供了预定义的层用于构建复杂的NN模型，但首先，您将学习如何从头开始定义一个模型。在本章的后面，您将看到如何使用这些预定义层。
- en: 'For this regression problem, we will define a linear regression model from
    scratch. We will define the parameters of our model, `weight` and `bias`, which
    correspond to the weight and the bias parameters, respectively. Finally, we will
    define the `model()` function to determine how this model uses the input data
    to generate its output:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个回归问题，我们将从头开始定义一个线性回归模型。我们将定义我们模型的参数，`weight`和`bias`，它们分别对应于权重和偏置参数。最后，我们将定义`model()`函数来确定这个模型如何使用输入数据生成其输出：
- en: '[PRE48]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'After defining the model, we can define the loss function that we want to minimize
    to find the optimal model weights. Here, we will choose the **mean squared error**
    (**MSE**) as our loss function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型之后，我们可以定义损失函数，以便找到最优模型权重。在这里，我们将选择**均方误差**（**MSE**）作为我们的损失函数：
- en: '[PRE49]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Furthermore, to learn the weight parameters of the model, we will use stochastic
    gradient descent. In this subsection, we will implement this training via the
    stochastic gradient descent procedure by ourselves, but in the next subsection,
    we will use the `SGD` method from the optimization package, `torch.optim`, to
    do the same thing.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了学习模型的权重参数，我们将使用随机梯度下降。在这个小节中，我们将通过自己实现随机梯度下降过程来训练，但在下一个小节中，我们将使用优化包`torch.optim`中的`SGD`方法来做同样的事情。
- en: To implement the stochastic gradient descent algorithm, we need to compute the
    gradients. Rather than manually computing the gradients, we will use PyTorch’s
    `torch.autograd.backward` function. We will cover `torch.autograd` and its different
    classes and functions for implementing automatic differentiation in *Chapter 13*,
    *Going Deeper – The Mechanics of PyTorch*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现随机梯度下降算法，我们需要计算梯度。与手动计算梯度不同，我们将使用PyTorch的`torch.autograd.backward`函数。我们将涵盖`torch.autograd`及其不同的类和函数，用于实现自动微分在*第13章*，*深入探讨
    - PyTorch的机制*。
- en: 'Now, we can set the learning rate and train the model for 200 epochs. The code
    for training the model against the batched version of the dataset is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以设置学习率并训练模型进行200个epochs。训练模型的代码如下，针对数据集的批处理版本：
- en: '[PRE50]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let’s look at the trained model and plot it. For the test data, we will create
    a NumPy array of values evenly spaced between 0 and 9\. Since we trained our model
    with standardized features, we will also apply the same standardization to the
    test data:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看训练好的模型并绘制它。对于测试数据，我们将创建一个在0到9之间均匀分布的值的NumPy数组。由于我们训练模型时使用了标准化特征，我们还将在测试数据上应用相同的标准化：
- en: '[PRE51]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '*Figure 12.8* shows a scatterplot of the training examples and the trained
    linear regression model:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12.8*显示了训练示例的散点图和训练的线性回归模型：'
- en: '![](img/B17582_12_08.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_08.png)'
- en: 'Figure 12.8: The linear regression model fits the data well'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8：线性回归模型很好地拟合了数据
- en: Model training via the torch.nn and torch.optim modules
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过`torch.nn`和`torch.optim`模块进行模型训练
- en: 'In the previous example, we saw how to train a model by writing a custom loss
    function `loss_fn()` and applied stochastic gradient descent optimization. However,
    writing the loss function and gradient updates can be a repeatable task across
    different projects. The `torch.nn` module provides a set of loss functions, and
    `torch.optim` supports most commonly used optimization algorithms that can be
    called to update the parameters based on the computed gradients. To see how they
    work, let’s create a new MSE loss function and a stochastic gradient descent optimizer:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们看到如何通过编写自定义损失函数`loss_fn()`来训练模型，并应用随机梯度下降优化。然而，编写损失函数和梯度更新可能是在不同项目中重复的任务。`torch.nn`模块提供了一组损失函数，而`torch.optim`支持大多数常用的优化算法，可以根据计算出的梯度来更新参数。为了看看它们是如何工作的，让我们创建一个新的均方误差（MSE）损失函数和一个随机梯度下降优化器：
- en: '[PRE52]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note that here we use the `torch.nn.Linear` class for the linear layer instead
    of manually defining it.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们使用`torch.nn.Linear`类来代替手动定义线性层。
- en: 'Now, we can simply call the `step()` method of the `optimizer` to train the
    model. We can pass a batched dataset (such as `train_dl`, which was created in
    the previous example):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地调用`optimizer`的`step()`方法来训练模型。我们可以传递一个批处理的数据集（例如前面例子中创建的`train_dl`）：
- en: '[PRE53]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'After the model is trained, visualize the results and make sure that they are
    similar to the results of the previous method. To obtain the weight and bias parameters,
    we can do the following:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，可视化结果并确保它们与以前方法的结果相似。要获取权重和偏置参数，我们可以执行以下操作：
- en: '[PRE54]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Building a multilayer perceptron for classifying flowers in the Iris dataset
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个用于分类鸢尾花数据集中花朵的多层感知机
- en: 'In the previous example, you saw how to build a model from scratch. We trained
    this model using stochastic gradient descent optimization. While we started our
    journey based on the simplest possible example, you can see that defining the
    model from scratch, even for such a simple case, is neither appealing nor good
    practice. PyTorch instead provides already defined layers through `torch.nn` that
    can be readily used as the building blocks of an NN model. In this section, you
    will learn how to use these layers to solve a classification task using the Iris
    flower dataset (identifying between three species of irises) and build a two-layer
    perceptron using the `torch.nn` module. First, let’s get the data from `sklearn.datasets`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，您看到了如何从头开始构建模型。我们使用随机梯度下降优化来训练这个模型。虽然我们从最简单的可能示例开始我们的旅程，但是你可以看到，即使对于这样一个简单的案例来说，从头定义模型也既不吸引人，也不是良好的实践。相反，PyTorch通过`torch.nn`提供了已定义的层，可以直接用作NN模型的构建块。在本节中，您将学习如何使用这些层来解决使用鸢尾花数据集（识别三种鸢尾花的物种）的分类任务，并使用`torch.nn`模块构建一个两层感知机。首先，让我们从`sklearn.datasets`获取数据：
- en: '[PRE55]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Here, we randomly select 100 samples (2/3) for training and 50 samples (1/3)
    for testing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们随机选择了100个样本（2/3）用于训练，以及50个样本（1/3）用于测试。
- en: 'Next, we standardize the features (mean centering and dividing by the standard
    deviation) and create a PyTorch `Dataset` for the training set and a corresponding
    `DataLoader`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对特征进行标准化（均值中心化并除以标准差），并为训练集创建一个PyTorch `Dataset`及其相应的`DataLoader`：
- en: '[PRE56]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, we set the batch size to `2` for the `DataLoader`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将`DataLoader`的批处理大小设置为`2`。
- en: Now, we are ready to use the `torch.nn` module to build a model efficiently.
    In particular, using the `nn.Module` class, we can stack a few layers and build
    an NN. You can see the list of all the layers that are already available at [https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html).
    For this problem, we are going to use the `Linear` layer, which is also known
    as a fully connected layer or dense layer, and can be best represented by *f*(*w* × *x* + *b*),
    where *x* represents a tensor containing the input features, *w* and *b* are the
    weight matrix and the bias vector, and *f* is the activation function.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备使用`torch.nn`模块来高效地构建模型。特别地，使用`nn.Module`类，我们可以堆叠几层并建立一个神经网络。您可以在[https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html)查看所有已经可用的层列表。对于这个问题，我们将使用`Linear`层，也被称为全连接层或密集层，可以最好地表示为*f*(*w* × *x* + *b*)，其中*x*代表包含输入特征的张量，*w*和*b*是权重矩阵和偏置向量，*f*是激活函数。
- en: 'Each layer in an NN receives its inputs from the preceding layer; therefore,
    its dimensionality (rank and shape) is fixed. Typically, we need to concern ourselves
    with the dimensionality of output only when we design an NN architecture. Here,
    we want to define a model with two hidden layers. The first one receives an input
    of four features and projects them to 16 neurons. The second layer receives the
    output of the previous layer (which has a size of *16*) and projects them to three
    output neurons, since we have three class labels. This can be done as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每一层都从前一层接收其输入，因此其维度（秩和形状）是固定的。通常，我们只需要在设计神经网络架构时关注输出的维度。在这里，我们希望定义一个具有两个隐藏层的模型。第一层接收四个特征的输入，并将它们投影到16个神经元。第二层接收前一层的输出（其大小为*16*），并将其投影到三个输出神经元，因为我们有三个类标签。可以通过以下方式实现：
- en: '[PRE57]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here, we used the sigmoid activation function for the first layer and softmax
    activation for the last (output) layer. Softmax activation in the last layer is
    used to support multiclass classification since we have three class labels here
    (which is why we have three neurons in the output layer). We will discuss the
    different activation functions and their applications later in this chapter.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们在第一层使用了sigmoid激活函数，在最后（输出）层使用了softmax激活函数。由于我们这里有三个类标签，softmax激活函数在最后一层用于支持多类分类（这也是为什么输出层有三个神经元）。我们将在本章后面讨论不同的激活函数及其应用。
- en: 'Next, we specify the loss function as cross-entropy loss and the optimizer
    as Adam:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将损失函数指定为交叉熵损失，并将优化器指定为Adam：
- en: The Adam optimizer is a robust, gradient-based optimization method, which we
    will talk about in detail in *Chapter 14*, *Classifying Images with Deep Convolutional
    Neural Networks*.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Adam优化器是一种强大的基于梯度的优化方法，我们将在第14章《使用深度卷积神经网络对图像进行分类》中详细讨论。
- en: '[PRE58]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now, we can train the model. We will specify the number of epochs to be `100`.
    The code of training the flower classification model is as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练模型了。我们将指定epoch数为`100`。训练花卉分类模型的代码如下：
- en: '[PRE59]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The `loss_hist` and `accuracy_hist` lists keep the training loss and the training
    accuracy after each epoch. We can use this to visualize the learning curves as
    follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss_hist`和`accuracy_hist`列表保存了每个epoch后的训练损失和训练精度。我们可以使用这些来可视化学习曲线，如下所示：'
- en: '[PRE60]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The learning curves (training loss and training accuracy) are as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线（训练损失和训练精度）如下：
- en: '![](img/B17582_12_09.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_09.png)'
- en: 'Figure 12.9: Training loss and accuracy curves'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9：训练损失和准确率曲线
- en: Evaluating the trained model on the test dataset
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试数据集上评估训练好的模型。
- en: 'We can now evaluate the classification accuracy of the trained model on the
    test dataset:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以评估训练好的模型在测试数据集上的分类准确率了：
- en: '[PRE61]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Since we trained our model with standardized features, we also applied the same
    standardization to the test data. The classification accuracy is 0.98 (that is,
    98 percent).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用标准化特征训练了模型，我们也将相同的标准化应用于测试数据。分类准确率为0.98（即98%）。
- en: Saving and reloading the trained model
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和重新加载训练好的模型
- en: 'Trained models can be saved on disk for future use. This can be done as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型可以保存在磁盘上供将来使用。可以通过以下方式实现：
- en: '[PRE62]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Calling `save(model)` will save both the model architecture and all the learned
    parameters. As a common convention, we can save models using a `'pt'` or `'pth'`
    file extension.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`save(model)`会保存模型架构和所有学到的参数。按照一般惯例，我们可以使用`'pt'`或`'pth'`文件扩展名保存模型。
- en: 'Now, let’s reload the saved model. Since we have saved both the model architecture
    and the weights, we can easily rebuild and reload the parameters in just one line:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新加载保存的模型。由于我们已经保存了模型的结构和权重，我们可以只用一行代码轻松重建和重新加载参数：
- en: '[PRE63]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Try to verify the model architecture by calling `model_new.eval()`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试通过调用`model_new.eval()`来验证模型结构：
- en: '[PRE64]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, let’s evaluate this new model that is reloaded on the test dataset
    to verify that the results are the same as before:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在测试数据集上评估这个重新加载的新模型，以验证结果与之前是否相同：
- en: '[PRE65]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'If you want to save only the learned parameters, you can use `save(model.state_dict())`
    as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只想保存已学习的参数，可以像下面这样使用`save(model.state_dict())`：
- en: '[PRE66]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'To reload the saved parameters, we first need to construct the model as we
    did before, then feed the loaded parameters to the model:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要重新加载保存的参数，我们首先需要像之前一样构建模型，然后将加载的参数提供给模型：
- en: '[PRE67]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Choosing activation functions for multilayer neural networks
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择多层神经网络的激活函数
- en: For simplicity, we have only discussed the sigmoid activation function in the
    context of multilayer feedforward NNs so far; we have used it in the hidden layer
    as well as the output layer in the MLP implementation in *Chapter 11*.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 简单起见，到目前为止我们只讨论了在多层前馈神经网络中使用的S型激活函数；我们在MLP实现的隐藏层和输出层都使用了它(*第11章*)。
- en: Note that in this book, the sigmoidal logistic function, ![](img/B17582_12_003.png),
    is referred to as the *sigmoid* function for brevity, which is common in machine
    learning literature. In the following subsections, you will learn more about alternative
    nonlinear functions that are useful for implementing multilayer NNs.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本书中，逻辑函数，![](img/B17582_12_003.png)，因其简洁性常被称为*sigmoid*函数，在机器学习文献中很常见。在接下来的小节中，您将学习更多关于实现多层神经网络时有用的替代非线性函数的内容。
- en: Technically, we can use any function as an activation function in multilayer
    NNs as long as it is differentiable. We can even use linear activation functions,
    such as in Adaline (*Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*). However, in practice, it would not be very useful to use
    linear activation functions for both hidden and output layers, since we want to
    introduce nonlinearity in a typical artificial NN to be able to tackle complex
    problems. The sum of linear functions yields a linear function after all.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们可以在多层神经网络中使用任何可微函数作为激活函数。我们甚至可以使用线性激活函数，例如在Adaline(*第2章*，*用于分类的简单机器学习算法*)中。然而，在实践中，对于隐藏层和输出层都使用线性激活函数并不是很有用，因为我们想要在典型的人工神经网络中引入非线性，以便能够解决复杂问题。多个线性函数的总和毕竟会产生一个线性函数。
- en: The logistic (sigmoid) activation function that we used in *Chapter 11* probably
    mimics the concept of a neuron in a brain most closely—we can think of it as the
    probability of whether a neuron fires. However, the logistic (sigmoid) activation
    function can be problematic if we have highly negative input, since the output
    of the sigmoid function will be close to zero in this case. If the sigmoid function
    returns output that is close to zero, the NN will learn very slowly, and it will
    be more likely to get trapped in the local minima of the loss landscape during
    training. This is why people often prefer a hyperbolic tangent as an activation
    function in hidden layers.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第11章*中使用的逻辑（S型）激活函数可能最接近大脑中神经元的概念——我们可以将其视为神经元是否触发的概率。然而，如果输入非常负，则逻辑（S型）激活函数的输出会接近于零。如果逻辑函数返回接近于零的输出，在训练过程中神经网络将学习速度非常慢，并且更容易陷入损失地形的局部最小值中。这就是为什么人们通常更喜欢在隐藏层中使用双曲正切作为激活函数的原因。
- en: Before we discuss what a hyperbolic tangent looks like, let’s briefly recapitulate
    some of the basics of the logistic function and look at a generalization that
    makes it more useful for multilabel classification problems.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论双曲正切函数的外观之前，让我们简要回顾一下逻辑函数的一些基础知识，并查看一个使其在多标签分类问题中更有用的泛化。
- en: Logistic function recap
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑函数回顾
- en: As was mentioned in the introduction to this section, the logistic function
    is, in fact, a special case of a sigmoid function. You will recall from the section
    on logistic regression in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using Scikit-Learn*, that we can use a logistic function to model the probability
    that sample *x* belongs to the positive class (class `1`) in a binary classification
    task.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在本节的介绍中提到的，逻辑函数实际上是 S 形函数的一种特殊情况。您可以从*第 3 章*《使用 Scikit-Learn 进行机器学习分类器之旅》中的逻辑回归部分回忆起，我们可以使用逻辑函数来建模样本
    *x* 属于正类（类 `1`）的概率。
- en: 'The given net input, *z*, is shown in the following equation:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的净输入，*z*，如下方程所示：
- en: '![](img/B17582_12_004.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_004.png)'
- en: 'The logistic (sigmoid) function will compute the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑（sigmoid）函数将计算如下：
- en: '![](img/B17582_12_005.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_005.png)'
- en: 'Note that *w*[0] is the bias unit (*y*-axis intercept, which means *x*[0] = 1).
    To provide a more concrete example, let’s take a model for a two-dimensional data
    point, *x*, and a model with the following weight coefficients assigned to the
    *w* vector:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 *w*[0] 是偏置单元（*y*-轴截距，这意味着 *x*[0] = 1）。为了提供一个更具体的示例，让我们来看一个二维数据点 *x* 的模型，以及分配给
    *w* 向量的以下权重系数：
- en: '[PRE68]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: If we calculate the net input (*z*) and use it to activate a logistic neuron
    with those particular feature values and weight coefficients, we get a value of
    `0.888`, which we can interpret as an 88.8 percent probability that this particular
    sample, *x*, belongs to the positive class.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算净输入（*z*）并使用它来激活具有特定特征值和权重系数的逻辑神经元，则得到一个值为 `0.888`，我们可以将其解释为这个特定样本 *x*
    属于正类的概率为 88.8%。
- en: 'In *Chapter 11*,we used the one-hot encoding technique to represent multiclass
    ground truth labels and designed the output layer consisting of multiple logistic
    activation units. However, as will be demonstrated by the following code example,
    an output layer consisting of multiple logistic activation units does not produce
    meaningful, interpretable probability values:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 11 章* 中，我们使用一热编码技术来表示多类别的真实标签，并设计了包含多个逻辑激活单元的输出层。然而，正如以下代码示例所示，由多个逻辑激活单元组成的输出层并不产生有意义的可解释概率值：
- en: '[PRE69]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As you can see in the output, the resulting values cannot be interpreted as
    probabilities for a three-class problem. The reason for this is that they do not
    sum to 1\. However, this is, in fact, not a big concern if we use our model to
    predict only the class labels and not the class membership probabilities. One
    way to predict the class label from the output units obtained earlier is to use
    the maximum value:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在输出中所看到的，得到的值不能被解释为三类问题的概率。其原因在于它们不会加总到 1。然而，如果我们仅使用模型来预测类别标签而不是类别成员概率，这实际上并不是一个大问题。从之前获得的输出单元预测类别标签的一种方法是使用最大值：
- en: '[PRE70]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: In certain contexts, it can be useful to compute meaningful class probabilities
    for multiclass predictions. In the next section, we will take a look at a generalization
    of the logistic function, the `softmax` function, which can help us with this
    task.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情境中，计算多类预测的有意义的类别概率可能会有所帮助。在下一节中，我们将看看逻辑函数的一般化，即 `softmax` 函数，它可以帮助我们完成这项任务。
- en: Estimating class probabilities in multiclass classification via the softmax
    function
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 softmax 函数估计多类分类中的类别概率
- en: In the previous section, you saw how we can obtain a class label using the `argmax`
    function. Previously, in the *Building a multilayer perceptron for classifying
    flowers in the Iris dataset* section, we determined `activation='softmax'` in
    the last layer of the MLP model. The `softmax` function is a soft form of the
    `argmax` function; instead of giving a single class index, it provides the probability
    of each class. Therefore, it allows us to compute meaningful class probabilities
    in multiclass settings (multinomial logistic regression).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，您看到我们如何使用 `argmax` 函数获得类标签。在*构建用于在鸢尾花数据集中分类花卉的多层感知机*部分中，我们确定在 MLP 模型的最后一层使用
    `activation='softmax'`。`softmax` 函数是 `argmax` 函数的一种软形式；它不仅提供单一类别索引，还提供每个类别的概率。因此，它允许我们在多类别设置（多项逻辑回归）中计算有意义的类别概率。
- en: 'In `softmax`, the probability of a particular sample with net input *z* belonging
    to the *i*th class can be computed with a normalization term in the denominator,
    that is, the sum of the exponentially weighted linear functions:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在`softmax`中，特定样本的概率，具有净输入*z*属于第*i*类，可以通过分母中的归一化项来计算，即指数加权线性函数的总和：
- en: '![](img/B17582_12_006.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_006.png)'
- en: 'To see `softmax` in action, let’s code it up in Python:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要看`softmax`如何发挥作用，让我们在Python中编码它：
- en: '[PRE71]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: As you can see, the predicted class probabilities now sum to 1, as we would
    expect. It is also notable that the predicted class label is the same as when
    we applied the `argmax` function to the logistic output.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，预测的类别概率现在总和为1，符合我们的预期。值得注意的是，预测的类别标签与我们对逻辑输出应用`argmax`函数时相同。
- en: 'It may help to think of the result of the `softmax` function as a *normalized*
    output that is useful for obtaining meaningful class-membership predictions in
    multiclass settings. Therefore, when we build a multiclass classification model
    in PyTorch, we can use the `torch.softmax()` function to estimate the probabilities
    of each class membership for an input batch of examples. To see how we can use
    the `torch.softmax()` activation function in PyTorch, we will convert `Z` to a
    tensor in the following code, with an additional dimension reserved for the batch
    size:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有助于将`softmax`函数的结果视为在多类别设置中获取有意义的类成员预测的*归一化*输出。因此，当我们在PyTorch中构建多类别分类模型时，我们可以使用`torch.softmax()`函数来估计每个类别成员的概率，以查看我们如何在下面的代码中使用`torch.softmax()`激活函数，我们将`Z`转换为一个张量，并额外保留一个维度用于批处理大小：
- en: '[PRE72]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Broadening the output spectrum using a hyperbolic tangent
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用双曲正切扩展输出光谱
- en: 'Another sigmoidal function that is often used in the hidden layers of artificial
    NNs is the **hyperbolic tangent** (commonly known as **tanh**), which can be interpreted
    as a rescaled version of the logistic function:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络的隐藏层中经常使用的另一个Sigmoid函数是**双曲正切**（通常称为**tanh**），可以解释为逻辑函数的重新缩放版本：
- en: '![](img/B17582_12_007.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_007.png)'
- en: 'The advantage of the hyperbolic tangent over the logistic function is that
    it has a broader output spectrum ranging in the open interval (–1, 1), which can
    improve the convergence of the backpropagation algorithm (*Neural Networks for
    Pattern Recognition*, *C. M. Bishop*, *Oxford University Press*, pages: 500-501,
    *1995*).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数相比逻辑函数的优势在于其具有更广的输出光谱，范围在开区间（–1, 1），这可以提高反向传播算法的收敛性（*《神经网络模式识别》*，*C. M.
    Bishop*，*牛津大学出版社*，页码：500-501，*1995*）。
- en: 'In contrast, the logistic function returns an output signal ranging in the
    open interval (0, 1). For a simple comparison of the logistic function and the
    hyperbolic tangent, let’s plot the two sigmoidal functions:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，逻辑函数返回一个在开区间（0, 1）内的输出信号。为了简单比较逻辑函数和双曲正切函数，让我们绘制这两个Sigmoid函数：
- en: '[PRE73]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'As you can see, the shapes of the two sigmoidal curves look very similar; however,
    the `tanh` function has double the output space of the `logistic` function:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，两个Sigmoid曲线的形状看起来非常相似；然而，`tanh`函数的输出空间是`logistic`函数的两倍：
- en: '![](img/B17582_12_10.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_10.png)'
- en: 'Figure 12.10: A comparison of the tanh and logistic functions'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10：双曲正切和逻辑函数的比较
- en: Note that we previously implemented the `logistic` and `tanh` functions verbosely
    for the purpose of illustration. In practice, we can use NumPy’s `tanh` function.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们之前详细实现了逻辑和双曲正切函数，仅用于说明目的。实际上，我们可以使用NumPy的`tanh`函数。
- en: 'Alternatively, when building an NN model, we can use `torch.tanh(x)` in PyTorch
    to achieve the same results:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在构建一个NN模型时，我们可以在PyTorch中使用`torch.tanh(x)`来实现相同的结果：
- en: '[PRE74]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In addition, the logistic function is available in SciPy’s `special` module:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，逻辑函数在SciPy的`special`模块中可用：
- en: '[PRE75]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Similarly, we can use the `torch.sigmoid()` function in PyTorch to do the same
    computation, as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以在PyTorch中使用`torch.sigmoid()`函数执行相同的计算，如下所示：
- en: '[PRE76]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Note that using `torch.sigmoid(x)` produces results that are equivalent to `torch.nn.Sigmoid()(x)`,
    which we used earlier. `torch.nn.Sigmoid` is a class to which you can pass in
    parameters to construct an object in order to control the behavior. In contrast,
    `torch.sigmoid` is a function.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用`torch.sigmoid(x)`产生的结果等同于`torch.nn.Sigmoid()(x)`，我们之前使用过。`torch.nn.Sigmoid`是一个类，您可以通过传递参数来构建一个对象以控制其行为。相比之下，`torch.sigmoid`是一个函数。
- en: Rectified linear unit activation
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数ReLU（Rectified linear unit activation）
- en: The **rectified linear unit** (**ReLU**) is another activation function that
    is often used in deep NNs. Before we delve into ReLU, we should step back and
    understand the vanishing gradient problem of tanh and logistic activations.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）是另一种经常在深度神经网络中使用的激活函数。在深入研究ReLU之前，我们应该退后一步，了解tanh和逻辑激活函数的梯度消失问题。'
- en: To understand this problem, let’s assume that we initially have the net input
    *z*[1] = 20, which changes to *z*[2] = 25\. Computing the tanh activation, we
    get ![](img/B17582_12_008.png) and ![](img/B17582_12_009.png), which shows no
    change in the output (due to the asymptotic behavior of the tanh function and
    numerical errors).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这个问题，让我们假设我们最初有净输入 *z*[1] = 20，这变成 *z*[2] = 25\. 计算双曲正切激活函数时，我们得到 ![](img/B17582_12_008.png)
    和 ![](img/B17582_12_009.png)，显示输出没有变化（由于双曲正切函数的渐近行为和数值误差）。
- en: 'This means that the derivative of activations with respect to the net input
    diminishes as *z* becomes large. As a result, learning the weights during the
    training phase becomes very slow because the gradient terms may be very close
    to zero. ReLU activation addresses this issue. Mathematically, ReLU is defined
    as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着激活函数对净输入的导数随着 *z* 变大而减小。因此，在训练阶段学习权重变得非常缓慢，因为梯度项可能非常接近零。ReLU激活解决了这个问题。数学上，ReLU定义如下：
- en: '![](img/B17582_12_010.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_12_010.png)'
- en: 'ReLU is still a nonlinear function that is good for learning complex functions
    with NNs. Besides this, the derivative of ReLU, with respect to its input, is
    always 1 for positive input values. Therefore, it solves the problem of vanishing
    gradients, making it suitable for deep NNs. In PyTorch, we can apply the ReLU
    activation `torch.relu()` as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU仍然是一个非线性函数，非常适合用于学习具有复杂功能的神经网络。除此之外，对ReLU的导数，关于其输入，对于正输入值始终为1。因此，它解决了梯度消失的问题，使其非常适合深度神经网络。在PyTorch中，我们可以如下应用ReLU激活
    `torch.relu()`：
- en: '[PRE77]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We will use the ReLU activation function in the next chapter as an activation
    function for multilayer convolutional NNs.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将作为多层卷积神经网络的激活函数使用ReLU激活函数。
- en: 'Now that we know more about the different activation functions that are commonly
    used in artificial NNs, let’s conclude this section with an overview of the different
    activation functions that we have encountered so far in this book:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对人工神经网络中常用的不同激活函数有了更多了解，让我们总结一下本书中迄今为止遇到的不同激活函数：
- en: '![Table  Description automatically generated](img/B17582_12_11.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B17582_12_11.png)'
- en: 'Figure 12.11: The activation functions covered in this book'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11：本书涵盖的激活函数
- en: You can find the list of all activation functions available in the `torch.nn`
    module at [https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)
    找到`torch.nn` 模块中所有可用的激活函数列表。
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, you learned how to use PyTorch, an open source library for
    numerical computations, with a special focus on deep learning. While PyTorch is
    more inconvenient to use than NumPy, due to its additional complexity to support
    GPUs, it allows us to define and train large, multilayer NNs very efficiently.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何使用PyTorch，一个用于数值计算的开源库，专注于深度学习。虽然PyTorch使用起来比NumPy更不方便，因为它增加了支持GPU的复杂性，但它允许我们定义和高效训练大型、多层次的神经网络。
- en: Also, you learned about using the `torch.nn` module to build complex machine
    learning and NN models and run them efficiently. We explored model building in
    PyTorch by defining a model from scratch via the basic PyTorch tensor functionality.
    Implementing models can be tedious when we have to program at the level of matrix-vector
    multiplications and define every detail of each operation. However, the advantage
    is that this allows us, as developers, to combine such basic operations and build
    more complex models. We then explored `torch.nn`, which makes building NN models
    a lot easier than implementing them from scratch.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您学习了如何使用 `torch.nn` 模块来构建复杂的机器学习和神经网络模型，并高效地运行它们。我们通过基本的PyTorch张量功能从零开始定义了一个模型来探索模型构建。当我们必须在矩阵-向量乘法的水平上编程并定义每个操作的每个细节时，实现模型可能会很乏味。然而，优点在于这允许我们作为开发者结合这些基本操作并构建更复杂的模型。然后，我们探索了`torch.nn`，这使得构建神经网络模型比从头开始实现它们要容易得多。
- en: Finally, you learned about different activation functions and understood their
    behaviors and applications. Specifically, in this chapter, we covered tanh, softmax,
    and ReLU.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您了解了不同的激活函数，并理解了它们的行为和应用。特别是在本章中，我们涵盖了tanh、softmax和ReLU。
- en: In the next chapter, we’ll continue our journey and dive deeper into PyTorch,
    where we’ll find ourselves working with PyTorch computation graphs and the automatic
    differentiation package. Along the way, you’ll learn many new concepts, such as
    gradient computations.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续我们的旅程，并深入研究PyTorch，我们将与PyTorch计算图和自动微分包一起工作。沿途您将学习许多新概念，如梯度计算。
- en: Join our book’s Discord space
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作区，每月举行一次*问答环节*，与作者亲密互动：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
