- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parallelizing Neural Network Training with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will move on from the mathematical foundations of machine
    learning and deep learning to focus on PyTorch. PyTorch is one of the most popular
    deep learning libraries currently available, and it lets us implement **neural
    networks** (**NNs**) much more efficiently than any of our previous NumPy implementations.
    In this chapter, we will start using PyTorch and see how it brings significant
    benefits to training performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will begin the next stage of our journey into machine learning
    and deep learning, and we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How PyTorch improves training performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with PyTorch’s `Dataset` and `DataLoader` to build input pipelines and
    enable efficient model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with PyTorch to write optimized machine learning code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `torch.nn` module to implement common deep learning architectures
    conveniently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing activation functions for artificial NNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch and training performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch can speed up our machine learning tasks significantly. To understand
    how it can do this, let’s begin by discussing some of the performance challenges
    we typically run into when we execute expensive calculations on our hardware.
    Then, we will take a high-level look at what PyTorch is and what our learning
    approach will be in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Performance challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The performance of computer processors has, of course, been continuously improving
    in recent years. That allows us to train more powerful and complex learning systems,
    which means that we can improve the predictive performance of our machine learning
    models. Even the cheapest desktop computer hardware that’s available right now
    comes with processing units that have multiple cores.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we saw that many functions in scikit-learn allow us
    to spread those computations over multiple processing units. However, by default,
    Python is limited to execution on one core due to the **global interpreter lock**
    (**GIL**). So, although we indeed take advantage of Python’s multiprocessing library
    to distribute our computations over multiple cores, we still have to consider
    that the most advanced desktop hardware rarely comes with more than 8 or 16 such
    cores.
  prefs: []
  type: TYPE_NORMAL
- en: You will recall from *Chapter 11*, *Implementing a Multilayer Artificial Neural
    Network from Scratch*, that we implemented a very simple **multilayer perceptron**
    (**MLP**) with only one hidden layer consisting of 100 units. We had to optimize
    approximately 80,000 weight parameters ([784*100 + 100] + [100 * 10] + 10 = 79,510)
    for a very simple image classification task. The images in MNIST are rather small
    (28×28), and we can only imagine the explosion in the number of parameters if
    we wanted to add additional hidden layers or work with images that have higher
    pixel densities. Such a task would quickly become unfeasible for a single processing
    unit. The question then becomes, how can we tackle such problems more effectively?
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious solution to this problem is to use **graphics processing units**
    (**GPUs**), which are real workhorses. You can think of a graphics card as a small
    computer cluster inside your machine. Another advantage is that modern GPUs are
    great value compared to the state-of-the-art **central processing units** (**CPUs**),
    as you can see in the following overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: Comparison of a state-of-the-art CPU and GPU'
  prefs: []
  type: TYPE_NORMAL
- en: 'The sources for the information in *Figure 12.1* are the following websites
    (date accessed: July 2021):'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ark.intel.com/content/www/us/en/ark/products/215570/intel-core-i9-11900kb-processor-24m-cache-up-to-4-90-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/215570/intel-core-i9-11900kb-processor-24m-cache-up-to-4-90-ghz.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3080-3080ti/](https://www.nvidia.com/en-us/geforce/graphics-cards/30-series/rtx-3080-3080ti/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At 2.2 times the price of a modern CPU, we can get a GPU that has 640 times
    more cores and is capable of around 46 times more floating-point calculations
    per second. So, what is holding us back from utilizing GPUs for our machine learning
    tasks? The challenge is that writing code to target GPUs is not as simple as executing
    Python code in our interpreter. There are special packages, such as CUDA and OpenCL,
    that allow us to target the GPU. However, writing code in CUDA or OpenCL is probably
    not the most convenient way to implement and run machine learning algorithms.
    The good news is that this is what PyTorch was developed for!
  prefs: []
  type: TYPE_NORMAL
- en: What is PyTorch?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PyTorch is a scalable and multiplatform programming interface for implementing
    and running machine learning algorithms, including convenience wrappers for deep
    learning. PyTorch was primarily developed by the researchers and engineers from
    the **Facebook AI Research** (**FAIR**) lab. Its development also involves many
    contributions from the community. PyTorch was initially released in September
    2016 and is free and open source under the modified BSD license. Many machine
    learning researchers and practitioners from academia and industry have adapted
    PyTorch to develop deep learning solutions, such as Tesla Autopilot, Uber’s Pyro,
    and Hugging Face’s Transformers ([https://pytorch.org/ecosystem/](https://pytorch.org/ecosystem/)).
  prefs: []
  type: TYPE_NORMAL
- en: To improve the performance of training machine learning models, PyTorch allows
    execution on CPUs, GPUs, and XLA devices such as TPUs. However, its greatest performance
    capabilities can be discovered when using GPUs and XLA devices. PyTorch supports
    CUDA-enabled and ROCm GPUs officially. PyTorch’s development is based on the Torch
    library ([www.torch.ch](http://torch.ch/)). As its name implies, the Python interface
    is the primary development focus of PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is built around a computation graph composed of a set of nodes. Each
    node represents an operation that may have zero or more inputs or outputs. PyTorch
    provides an imperative programming environment that evaluates operations, executes
    computation, and returns concrete values immediately. Hence, the computation graph
    in PyTorch is defined implicitly, rather than constructed in advance and executed
    after.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, tensors can be understood as a generalization of scalars, vectors,
    matrices, and so on. More concretely, a scalar can be defined as a rank-0 tensor,
    a vector can be defined as a rank-1 tensor, a matrix can be defined as a rank-2
    tensor, and matrices stacked in a third dimension can be defined as rank-3 tensors.
    Tensors in PyTorch are similar to NumPy’s arrays, except that tensors are optimized
    for automatic differentiation and can run on GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the concept of a tensor clearer, consider *Figure 12.2*, which represents
    tensors of ranks 0 and 1 in the first row, and tensors of ranks 2 and 3 in the
    second row:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Different types of tensor in PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what PyTorch is, let’s see how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: How we will learn PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we are going to cover PyTorch’s programming model, in particular, creating
    and manipulating tensors. Then, we will see how to load data and utilize the `torch.utils.data`
    module, which will allow us to iterate through a dataset efficiently. In addition,
    we will discuss the existing, ready-to-use datasets in the `torch.utils.data.Dataset`
    submodule and learn how to use them.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about these basics, the PyTorch neural network `torch.nn` module
    will be introduced. Then, we will move forward to building machine learning models,
    learn how to compose and train the models, and learn how to save the trained models
    on disk for future evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: First steps with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take our first steps in using the low-level PyTorch
    API. After installing PyTorch, we will cover how to create tensors in PyTorch
    and different ways of manipulating them, such as changing their shape, data type,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Installing PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To install PyTorch, we recommend consulting the latest instructions on the official
    [https://pytorch.org](https://pytorch.org) website. Below, we will outline the
    basic steps that will work on most systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how your system is set up, you can typically just use Python’s
    `pip` installer and install PyTorch from PyPI by executing the following from
    your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will install the latest *stable* version, which is 1.9.0 at the time of
    writing. To install the 1.9.0 version, which is guaranteed to be compatible with
    the following code examples, you can modify the preceding command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use GPUs (recommended), you need a compatible NVIDIA graphics
    card that supports CUDA and cuDNN. If your machine satisfies these requirements,
    you can install PyTorch with GPU support, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'for CUDA 11.1 or:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: for CUDA 10.2 as of the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: 'As macOS binaries don’t support CUDA, you can install from source: [https://pytorch.org/get-started/locally/#mac-from-source](https://pytorch.org/get-started/locally/#mac-from-source).'
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the installation and setup process, please see the
    official recommendations at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that PyTorch is under active development; therefore, every couple of months,
    new versions are released with significant changes. You can verify your PyTorch
    version from your terminal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Troubleshooting your installation of PyTorch**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you experience problems with the installation procedure, read more about
    system- and platform-specific recommendations that are provided at [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/).
    Note that all the code in this chapter can be run on your CPU; using a GPU is
    entirely optional but recommended if you want to fully enjoy the benefits of PyTorch.
    For example, while training some NN models on a CPU could take a week, the same
    models could be trained in just a few hours on a modern GPU. If you have a graphics
    card, refer to the installation page to set it up appropriately. In addition,
    you may find this setup guide helpful, which explains how to install the NVIDIA
    graphics card drivers, CUDA, and cuDNN on Ubuntu (not required but recommended
    requirements for running PyTorch on a GPU): [https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf).
    Furthermore, as you will see in *Chapter 17*, *Generative Adversarial Networks
    for Synthesizing New Data*, you can also train your models using a GPU for free
    via Google Colab.'
  prefs: []
  type: TYPE_NORMAL
- en: Creating tensors in PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s consider a few different ways of creating tensors, and then see
    some of their properties and how to manipulate them. Firstly, we can simply create
    a tensor from a list or a NumPy array using the `torch.tensor` or the `torch.from_numpy`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This resulted in tensors `t_a` and `t_b`, with their properties, `shape=(3,)`
    and `dtype=int32`, adopted from their source. Similar to NumPy arrays, we can
    also see these properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, creating a tensor of random values can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Manipulating the data type and shape of a tensor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning ways to manipulate tensors is necessary to make them compatible for
    input to a model or an operation. In this section, you will learn how to manipulate
    tensor data types and shapes via several PyTorch functions that cast, reshape,
    transpose, and squeeze (remove dimensions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torch.to()` function can be used to change the data type of a tensor to
    a desired type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: See [https://pytorch.org/docs/stable/tensor_attributes.html](https://pytorch.org/docs/stable/tensor_attributes.html)
    for all other data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will see in upcoming chapters, certain operations require that the input
    tensors have a certain number of dimensions (that is, rank) associated with a
    certain number of elements (shape). Thus, we might need to change the shape of
    a tensor, add a new dimension, or squeeze an unnecessary dimension. PyTorch provides
    useful functions (or operations) to achieve this, such as `torch.transpose()`,
    `torch.reshape()`, and `torch.squeeze()`. Let’s take a look at some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transposing a tensor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reshaping a tensor (for example, from a 1D vector to a 2D array):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Removing the unnecessary dimensions (dimensions that have size 1, which are
    not needed):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Applying mathematical operations to tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applying mathematical operations, in particular linear algebra operations, is
    necessary for building most machine learning models. In this subsection, we will
    cover some widely used linear algebra operations, such as element-wise product,
    matrix multiplication, and computing the norm of a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s instantiate two random tensors, one with uniform distribution
    in the range [–1, 1) and the other with a standard normal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that `torch.rand` returns a tensor filled with random numbers from a uniform
    distribution in the range of [0, 1).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that `t1` and `t2` have the same shape. Now, to compute the element-wise
    product of `t1` and `t2`, we can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To compute the mean, sum, and standard deviation along a certain axis (or axes),
    we can use `torch.mean()`, `torch.sum()`, and `torch.std()`. For example, the
    mean of each column in `t1` can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The matrix-matrix product between `t1` and `t2` (that is, ![](img/B17582_12_001.png),
    where the superscript T is for transpose) can be computed by using the `torch.matmul()`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, computing ![](img/B17582_12_002.png) is performed by transposing
    `t1`, resulting in an array of size 2×2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `torch.linalg.norm()` function is useful for computing the *L*^p
    norm of a tensor. For example, we can calculate the *L*² norm of `t1` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Split, stack, and concatenate tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will cover PyTorch operations for splitting a tensor
    into multiple tensors, or the reverse: stacking and concatenating multiple tensors
    into a single one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have a single tensor, and we want to split it into two or more
    tensors. For this, PyTorch provides a convenient `torch.chunk()` function, which
    divides an input tensor into a list of equally sized tensors. We can determine
    the desired number of splits as an integer using the `chunks` argument to split
    a tensor along the desired dimension specified by the `dim` argument. In this
    case, the total size of the input tensor along the specified dimension must be
    divisible by the desired number of splits. Alternatively, we can provide the desired
    sizes in a list using the `torch.split()` function. Let’s have a look at an example
    of both these options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Providing the number of splits:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this example, a tensor of size 6 was divided into a list of three tensors
    each with size 2\. If the tensor size is not divisible by the `chunks` value,
    the last chunk will be smaller.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Providing the sizes of different splits:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alternatively, instead of defining the number of splits, we can also specify
    the sizes of the output tensors directly. Here, we are splitting a tensor of size
    `5` into tensors of sizes `3` and `2`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sometimes, we are working with multiple tensors and need to concatenate or
    stack them to create a single tensor. In this case, PyTorch functions such as
    `torch.stack()` and `torch.cat()` come in handy. For example, let’s create a 1D
    tensor, `A`, containing 1s with size `3,` and a 1D tensor, `B`, containing 0s
    with size `2,` and concatenate them into a 1D tensor, `C`, of size `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If we create 1D tensors `A` and `B`, both with size `3`, then we can stack
    them together to form a 2D tensor, `S`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The PyTorch API has many operations that you can use for building a model, processing
    your data, and more. However, covering every function is outside the scope of
    this book, where we will focus on the most essential ones. For the full list of
    operations and functions, you can refer to the documentation page of PyTorch at
    [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Building input pipelines in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we are training a deep NN model, we usually train the model incrementally
    using an iterative optimization algorithm such as stochastic gradient descent,
    as we have seen in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this chapter, `torch.nn` is a module for building
    NN models. In cases where the training dataset is rather small and can be loaded
    as a tensor into the memory, we can directly use this tensor for training. In
    typical use cases, however, when the dataset is too large to fit into the computer
    memory, we will need to load the data from the main storage device (for example,
    the hard drive or solid-state drive) in chunks, that is, batch by batch. (Note
    the use of the term “batch” instead of “mini-batch” in this chapter to stay close
    to the PyTorch terminology.) In addition, we may need to construct a data-processing
    pipeline to apply certain transformations and preprocessing steps to our data,
    such as mean centering, scaling, or adding noise to augment the training procedure
    and to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Applying preprocessing functions manually every time can be quite cumbersome.
    Luckily, PyTorch provides a special class for constructing efficient and convenient
    preprocessing pipelines. In this section, we will see an overview of different
    methods for constructing a PyTorch `Dataset` and `DataLoader`, and implementing
    data loading, shuffling, and batching.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a PyTorch DataLoader from existing tensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If the data already exists in the form of a tensor object, a Python list, or
    a NumPy array, we can easily create a dataset loader using the `torch.utils.data.DataLoader()`
    class. It returns an object of the `DataLoader` class, which we can use to iterate
    through the individual elements in the input dataset. As a simple example, consider
    the following code, which creates a dataset from a list of values from 0 to 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily iterate through a dataset entry by entry as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to create batches from this dataset, with a desired batch size of
    `3`, we can do this with the `batch_size` argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create two batches from this dataset, where the first three elements
    go into batch #1, and the remaining elements go into batch #2\. The optional `drop_last`
    argument is useful for cases when the number of elements in the tensor is not
    divisible by the desired batch size. We can drop the last non-full batch by setting
    `drop_last` to `True`. The default value for `drop_last` is `False`.'
  prefs: []
  type: TYPE_NORMAL
- en: We can always iterate through a dataset directly, but as you just saw, `DataLoader`
    provides an automatic and customizable batching to a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Combining two tensors into a joint dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, we may have the data in two (or possibly more) tensors. For example,
    we could have a tensor for features and a tensor for labels. In such cases, we
    need to build a dataset that combines these tensors, which will allow us to retrieve
    the elements of these tensors in tuples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have two tensors, `t_x` and `t_y`. Tensor `t_x` holds our feature
    values, each of size `3`, and `t_y` stores the class labels. For this example,
    we first create these two tensors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we want to create a joint dataset from these two tensors. We first need
    to create a `Dataset` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'A custom `Dataset` class must contain the following methods to be used by the
    data loader later on:'
  prefs: []
  type: TYPE_NORMAL
- en: '`__init__()`: This is where the initial logic happens, such as reading existing
    arrays, loading a file, filtering data, and so forth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__()`: This returns the corresponding sample to the given index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then we create a joint dataset of `t_x` and `t_y` with the custom `Dataset`
    class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can print each example of the joint dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also simply utilize the `torch.utils.data.TensorDataset` class, if the
    second dataset is a labeled dataset in the form of tensors. So, instead of using
    our self-defined `Dataset` class, `JointDataset`, we can create a joint dataset
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that a common source of error could be that the element-wise correspondence
    between the original features (*x*) and labels (*y*) might be lost (for example,
    if the two datasets are shuffled separately). However, once they are merged into
    one dataset, it is safe to apply these operations.
  prefs: []
  type: TYPE_NORMAL
- en: If we have a dataset created from the list of image filenames on disk, we can
    define a function to load the images from these filenames. You will see an example
    of applying multiple transformations to a dataset later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle, batch, and repeat
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As was mentioned in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*, when training an NN model using stochastic gradient descent
    optimization, it is important to feed training data as randomly shuffled batches.
    You have already seen how to specify the batch size using the `batch_size` argument
    of a data loader object. Now, in addition to creating batches, you will see how
    to shuffle and reiterate over the datasets. We will continue working with the
    previous joint dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a shuffled version data loader from the `joint_dataset`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, each batch contains two data records (*x*) and the corresponding labels
    (*y*). Now we iterate through the data loader entry by entry as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The rows are shuffled without losing the one-to-one correspondence between the
    entries in `x` and `y`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, when training a model for multiple epochs, we need to shuffle
    and iterate over the dataset by the desired number of epochs. So, let’s iterate
    over the batched dataset twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This results in two different sets of batches. In the first epoch, the first
    batch contains a pair of values `[y=1, y=2]`, and the second batch contains a
    pair of values `[y=3, y=0]`. In the second epoch, two batches contain a pair of
    values, `[y=2, y=0]` and `[y=1, y=3]` respectively. For each iteration, the elements
    within a batch are also shuffled.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset from files on your local storage disk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will build a dataset from image files stored on disk. There
    is an image folder associated with the online content of this chapter. After downloading
    the folder, you should be able to see six images of cats and dogs in JPEG format.
  prefs: []
  type: TYPE_NORMAL
- en: 'This small dataset will show how building a dataset from stored files generally
    works. To accomplish this, we are going to use two additional modules: `Image`
    in `PIL` to read the image file contents and `transforms` in `torchvision` to
    decode the raw contents and resize the images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PIL.Image` and `torchvision.transforms` modules provide a lot of additional
    and useful functions, which are beyond the scope of the book. You are encouraged
    to browse through the official documentation to learn more about these functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pillow.readthedocs.io/en/stable/reference/Image.html](https://pillow.readthedocs.io/en/stable/reference/Image.html)
    for `PIL.Image`'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pytorch.org/vision/stable/transforms.html](https://pytorch.org/vision/stable/transforms.html)
    for `torchvision.transforms`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start, let’s take a look at the content of these files. We will use
    the `pathlib` library to generate a list of image files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will visualize these image examples using Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 12.3* shows the example images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: Images of cats and dogs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just from this visualization and the printed image shapes, we can already see
    that the images have different aspect ratios. If you print the aspect ratios (or
    data array shapes) of these images, you will see that some images are 900 pixels
    high and 1200 pixels wide (900×1200), some are 800×1200, and one is 900×742\.
    Later, we will preprocess these images to a consistent size. Another point to
    consider is that the labels for these images are provided within their filenames.
    So, we extract these labels from the list of filenames, assigning label `1` to
    dogs and label `0` to cats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have two lists: a list of filenames (or paths of each image) and a
    list of their labels. In the previous section, you learned how to create a joint
    dataset from two arrays. Here, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The joint dataset has filenames and labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to apply transformations to this dataset: load the image content
    from its file path, decode the raw content, and resize it to a desired size, for
    example, 80×120\. As mentioned before, we use the `torchvision.transforms` module
    to resize the images and convert the loaded pixels into tensors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we update the `ImageDataset` class with the `transform` we just defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we visualize these transformed image examples using Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following visualization of the retrieved example images,
    along with their labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Images are labeled'
  prefs: []
  type: TYPE_NORMAL
- en: The `__getitem__` method in the `ImageDataset` class wraps all four steps into
    a single function, including the loading of the raw content (images and labels),
    decoding the images into tensors, and resizing the images. The function then returns
    a dataset that we can iterate over and apply other operations that we learned
    about in the previous sections via a data loader, such as shuffling and batching.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching available datasets from the torchvision.datasets library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `torchvision.datasets` library provides a nice collection of freely available
    image datasets for training or evaluating deep learning models. Similarly, the
    `torchtext.datasets` library provides datasets for natural language. Here, we
    use `torchvision.datasets` as an example.
  prefs: []
  type: TYPE_NORMAL
- en: The `torchvision` datasets ([https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html))
    are nicely formatted and come with informative descriptions, including the format
    of features and labels and their type and dimensionality, as well as the link
    to the original source of the dataset. Another advantage is that these datasets
    are all subclasses of `torch.utils.data.Dataset`, so all the functions we covered
    in the previous sections can be used directly. So, let’s see how to use these
    datasets in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, if you haven’t already installed `torchvision` together with PyTorch
    earlier, you need to install the `torchvision` library via `pip` from the command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: You can take a look at the list of available datasets at [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following paragraphs, we will cover fetching two different datasets:
    CelebA (`celeb_a`) and the MNIST digit dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first work with the CelebA dataset ([http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    with `torchvision.datasets.CelebA` ([https://pytorch.org/vision/stable/datasets.html#celeba](https://pytorch.org/vision/stable/datasets.html#celeba)).
    The description of `torchvision.datasets.CelebA` provides some useful information
    to help us understand the structure of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: The database has three subsets, `'train'`, `'valid'`, and `'test'`. We can select
    a specific subset or load all of them with the `split` parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The images are stored in `PIL.Image` format. And we can obtain a transformed
    version using a custom `transform` function, such as `transforms.ToTensor` and
    `transforms.Resize`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are different types of targets we can use, including `'attributes'`, `'identity'`,
    and `'landmarks'`. `'attributes'` is 40 facial attributes for the person in the
    image, such as facial expression, makeup, hair properties, and so on; `'identity'`
    is the person ID for an image; and `'``landmarks'` refers to the dictionary of
    extracted facial points, such as the position of the eyes, nose, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will call the `torchvision.datasets.CelebA` class to download the
    data, store it on disk in a designated folder, and load it into a `torch.utils.data.Dataset`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You may run into a `BadZipFile: File is not a zip file` error, or `RuntimeError:
    The daily quota of the file img_align_celeba.zip is exceeded and it can''t be
    downloaded. This is a limitation of Google Drive and can only be overcome by trying
    again later`; it just means that Google Drive has a daily maximum quota that is
    exceeded by the CelebA files. To work around it, you can manually download the
    files from the source: [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html).
    In the downloaded folder, `celeba/`, you can unzip the `img_align_celeba.zip`
    file. The `image_path` is the root of the downloaded folder, `celeba/`. If you
    have already downloaded the files once, you can simply set `download=False`. For
    additional information and guidance, we highly recommend to see accompanying code
    notebook at [https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part1.ipynb](https://github.com/rasbt/machine-learning-book/blob/main/ch12/ch12_part1.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have instantiated the datasets, let’s check if the object is of
    the `torch.utils.data.Dataset` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned, the dataset is already split into train, test, and validation
    datasets, and we only load the train set. And we only use the `''attributes''`
    target. In order to see what the data examples look like, we can execute the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that the sample in this dataset comes in a tuple of `(PIL.Image, attributes)`.
    If we want to pass this dataset to a supervised deep learning model during training,
    we have to reformat it as a tuple of `(features tensor, label)`. For the label,
    we will use the `'Smiling'` category from the attributes as an example, which
    is the 31st element.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s take the first 18 examples from it to visualize them with their
    `''Smiling''` labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The examples and their labels that are retrieved from `celeba_dataset` are
    shown in *Figure 12.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A collage of a person  Description automatically generated with low confidence](img/B17582_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Model predicts smiling celebrities'
  prefs: []
  type: TYPE_NORMAL
- en: This was all we needed to do to fetch and use the CelebA image dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will proceed with the second dataset from `torchvision.datasets.MNIST`
    ([https://pytorch.org/vision/stable/datasets.html#mnist](https://pytorch.org/vision/stable/datasets.html#mnist)).
    Let’s see how it can be used to fetch the MNIST digit dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: The database has two partitions, `'train'` and `'test'`. We need to select a
    specific subset to load.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The images are stored in `PIL.Image` format. And we can obtain a transformed
    version using a custom `transform` function, such as `transforms.ToTensor` and
    `transforms.Resize`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are 10 classes for the target, from `0` to `9`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we can download the `''train''` partition, convert the elements to tuples,
    and visualize 10 examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The retrieved example handwritten digits from this dataset are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Correctly identifying handwritten digits'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our coverage of building and manipulating datasets and fetching
    datasets from the `torchvision.datasets` library. Next, we will see how to build
    NN models in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Building an NN model in PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this chapter, you have learned about the basic utility components
    of PyTorch for manipulating tensors and organizing data into formats that we can
    iterate over during training. In this section, we will finally implement our first
    predictive model in PyTorch. As PyTorch is a bit more flexible but also more complex
    than machine learning libraries such as scikit-learn, we will start with a simple
    linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch neural network module (torch.nn)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`torch.nn` is an elegantly designed module developed to help create and train
    NNs. It allows easy prototyping and the building of complex models in just a few
    lines of code.'
  prefs: []
  type: TYPE_NORMAL
- en: To fully utilize the power of the module and customize it for your problem,
    you need to understand what it’s doing. To develop this understanding, we will
    first train a basic linear regression model on a toy dataset without using any
    features from the `torch.nn` module; we will use nothing but the basic PyTorch
    tensor operations.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will incrementally add features from `torch.nn` and `torch.optim`.
    As you will see in the following subsections, these modules make building an NN
    model extremely easy. We will also take advantage of the dataset pipeline functionalities
    supported in PyTorch, such as `Dataset` and `DataLoader`, which you learned about
    in the previous section. In this book, we will use the `torch.nn` module to build
    NN models.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used approach for building an NN in PyTorch is through `nn.Module`,
    which allows layers to be stacked to form a network. This gives us more control
    over the forward pass. We will see examples of building an NN model using the
    `nn.Module` class.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as you will see in the following subsections, a trained model can be
    saved and reloaded for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Building a linear regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will build a simple model to solve a linear regression
    problem. First, let’s create a toy dataset in NumPy and visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the training examples will be shown in a scatterplot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: A scatterplot of the training examples'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will standardize the features (mean centering and dividing by the
    standard deviation) and create a PyTorch `Dataset` for the training set and a
    corresponding `DataLoader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set a batch size of `1` for the `DataLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can define our model for linear regression as *z* = *wx* + *b*. Here,
    we are going to use the `torch.nn` module. It provides predefined layers for building
    complex NN models, but to start, you will learn how to define a model from scratch.
    Later in this chapter, you will see how to use those predefined layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this regression problem, we will define a linear regression model from
    scratch. We will define the parameters of our model, `weight` and `bias`, which
    correspond to the weight and the bias parameters, respectively. Finally, we will
    define the `model()` function to determine how this model uses the input data
    to generate its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'After defining the model, we can define the loss function that we want to minimize
    to find the optimal model weights. Here, we will choose the **mean squared error**
    (**MSE**) as our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Furthermore, to learn the weight parameters of the model, we will use stochastic
    gradient descent. In this subsection, we will implement this training via the
    stochastic gradient descent procedure by ourselves, but in the next subsection,
    we will use the `SGD` method from the optimization package, `torch.optim`, to
    do the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the stochastic gradient descent algorithm, we need to compute the
    gradients. Rather than manually computing the gradients, we will use PyTorch’s
    `torch.autograd.backward` function. We will cover `torch.autograd` and its different
    classes and functions for implementing automatic differentiation in *Chapter 13*,
    *Going Deeper – The Mechanics of PyTorch*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can set the learning rate and train the model for 200 epochs. The code
    for training the model against the batched version of the dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the trained model and plot it. For the test data, we will create
    a NumPy array of values evenly spaced between 0 and 9\. Since we trained our model
    with standardized features, we will also apply the same standardization to the
    test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 12.8* shows a scatterplot of the training examples and the trained
    linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: The linear regression model fits the data well'
  prefs: []
  type: TYPE_NORMAL
- en: Model training via the torch.nn and torch.optim modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we saw how to train a model by writing a custom loss
    function `loss_fn()` and applied stochastic gradient descent optimization. However,
    writing the loss function and gradient updates can be a repeatable task across
    different projects. The `torch.nn` module provides a set of loss functions, and
    `torch.optim` supports most commonly used optimization algorithms that can be
    called to update the parameters based on the computed gradients. To see how they
    work, let’s create a new MSE loss function and a stochastic gradient descent optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note that here we use the `torch.nn.Linear` class for the linear layer instead
    of manually defining it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can simply call the `step()` method of the `optimizer` to train the
    model. We can pass a batched dataset (such as `train_dl`, which was created in
    the previous example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After the model is trained, visualize the results and make sure that they are
    similar to the results of the previous method. To obtain the weight and bias parameters,
    we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Building a multilayer perceptron for classifying flowers in the Iris dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, you saw how to build a model from scratch. We trained
    this model using stochastic gradient descent optimization. While we started our
    journey based on the simplest possible example, you can see that defining the
    model from scratch, even for such a simple case, is neither appealing nor good
    practice. PyTorch instead provides already defined layers through `torch.nn` that
    can be readily used as the building blocks of an NN model. In this section, you
    will learn how to use these layers to solve a classification task using the Iris
    flower dataset (identifying between three species of irises) and build a two-layer
    perceptron using the `torch.nn` module. First, let’s get the data from `sklearn.datasets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Here, we randomly select 100 samples (2/3) for training and 50 samples (1/3)
    for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we standardize the features (mean centering and dividing by the standard
    deviation) and create a PyTorch `Dataset` for the training set and a corresponding
    `DataLoader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set the batch size to `2` for the `DataLoader`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to use the `torch.nn` module to build a model efficiently.
    In particular, using the `nn.Module` class, we can stack a few layers and build
    an NN. You can see the list of all the layers that are already available at [https://pytorch.org/docs/stable/nn.html](https://pytorch.org/docs/stable/nn.html).
    For this problem, we are going to use the `Linear` layer, which is also known
    as a fully connected layer or dense layer, and can be best represented by *f*(*w* × *x* + *b*),
    where *x* represents a tensor containing the input features, *w* and *b* are the
    weight matrix and the bias vector, and *f* is the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each layer in an NN receives its inputs from the preceding layer; therefore,
    its dimensionality (rank and shape) is fixed. Typically, we need to concern ourselves
    with the dimensionality of output only when we design an NN architecture. Here,
    we want to define a model with two hidden layers. The first one receives an input
    of four features and projects them to 16 neurons. The second layer receives the
    output of the previous layer (which has a size of *16*) and projects them to three
    output neurons, since we have three class labels. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used the sigmoid activation function for the first layer and softmax
    activation for the last (output) layer. Softmax activation in the last layer is
    used to support multiclass classification since we have three class labels here
    (which is why we have three neurons in the output layer). We will discuss the
    different activation functions and their applications later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we specify the loss function as cross-entropy loss and the optimizer
    as Adam:'
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer is a robust, gradient-based optimization method, which we
    will talk about in detail in *Chapter 14*, *Classifying Images with Deep Convolutional
    Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can train the model. We will specify the number of epochs to be `100`.
    The code of training the flower classification model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The `loss_hist` and `accuracy_hist` lists keep the training loss and the training
    accuracy after each epoch. We can use this to visualize the learning curves as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The learning curves (training loss and training accuracy) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Training loss and accuracy curves'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the trained model on the test dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now evaluate the classification accuracy of the trained model on the
    test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Since we trained our model with standardized features, we also applied the same
    standardization to the test data. The classification accuracy is 0.98 (that is,
    98 percent).
  prefs: []
  type: TYPE_NORMAL
- en: Saving and reloading the trained model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Trained models can be saved on disk for future use. This can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Calling `save(model)` will save both the model architecture and all the learned
    parameters. As a common convention, we can save models using a `'pt'` or `'pth'`
    file extension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s reload the saved model. Since we have saved both the model architecture
    and the weights, we can easily rebuild and reload the parameters in just one line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Try to verify the model architecture by calling `model_new.eval()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s evaluate this new model that is reloaded on the test dataset
    to verify that the results are the same as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to save only the learned parameters, you can use `save(model.state_dict())`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'To reload the saved parameters, we first need to construct the model as we
    did before, then feed the loaded parameters to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Choosing activation functions for multilayer neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For simplicity, we have only discussed the sigmoid activation function in the
    context of multilayer feedforward NNs so far; we have used it in the hidden layer
    as well as the output layer in the MLP implementation in *Chapter 11*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this book, the sigmoidal logistic function, ![](img/B17582_12_003.png),
    is referred to as the *sigmoid* function for brevity, which is common in machine
    learning literature. In the following subsections, you will learn more about alternative
    nonlinear functions that are useful for implementing multilayer NNs.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, we can use any function as an activation function in multilayer
    NNs as long as it is differentiable. We can even use linear activation functions,
    such as in Adaline (*Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*). However, in practice, it would not be very useful to use
    linear activation functions for both hidden and output layers, since we want to
    introduce nonlinearity in a typical artificial NN to be able to tackle complex
    problems. The sum of linear functions yields a linear function after all.
  prefs: []
  type: TYPE_NORMAL
- en: The logistic (sigmoid) activation function that we used in *Chapter 11* probably
    mimics the concept of a neuron in a brain most closely—we can think of it as the
    probability of whether a neuron fires. However, the logistic (sigmoid) activation
    function can be problematic if we have highly negative input, since the output
    of the sigmoid function will be close to zero in this case. If the sigmoid function
    returns output that is close to zero, the NN will learn very slowly, and it will
    be more likely to get trapped in the local minima of the loss landscape during
    training. This is why people often prefer a hyperbolic tangent as an activation
    function in hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Before we discuss what a hyperbolic tangent looks like, let’s briefly recapitulate
    some of the basics of the logistic function and look at a generalization that
    makes it more useful for multilabel classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic function recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As was mentioned in the introduction to this section, the logistic function
    is, in fact, a special case of a sigmoid function. You will recall from the section
    on logistic regression in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using Scikit-Learn*, that we can use a logistic function to model the probability
    that sample *x* belongs to the positive class (class `1`) in a binary classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'The given net input, *z*, is shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The logistic (sigmoid) function will compute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that *w*[0] is the bias unit (*y*-axis intercept, which means *x*[0] = 1).
    To provide a more concrete example, let’s take a model for a two-dimensional data
    point, *x*, and a model with the following weight coefficients assigned to the
    *w* vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: If we calculate the net input (*z*) and use it to activate a logistic neuron
    with those particular feature values and weight coefficients, we get a value of
    `0.888`, which we can interpret as an 88.8 percent probability that this particular
    sample, *x*, belongs to the positive class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 11*,we used the one-hot encoding technique to represent multiclass
    ground truth labels and designed the output layer consisting of multiple logistic
    activation units. However, as will be demonstrated by the following code example,
    an output layer consisting of multiple logistic activation units does not produce
    meaningful, interpretable probability values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the output, the resulting values cannot be interpreted as
    probabilities for a three-class problem. The reason for this is that they do not
    sum to 1\. However, this is, in fact, not a big concern if we use our model to
    predict only the class labels and not the class membership probabilities. One
    way to predict the class label from the output units obtained earlier is to use
    the maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: In certain contexts, it can be useful to compute meaningful class probabilities
    for multiclass predictions. In the next section, we will take a look at a generalization
    of the logistic function, the `softmax` function, which can help us with this
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating class probabilities in multiclass classification via the softmax
    function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you saw how we can obtain a class label using the `argmax`
    function. Previously, in the *Building a multilayer perceptron for classifying
    flowers in the Iris dataset* section, we determined `activation='softmax'` in
    the last layer of the MLP model. The `softmax` function is a soft form of the
    `argmax` function; instead of giving a single class index, it provides the probability
    of each class. Therefore, it allows us to compute meaningful class probabilities
    in multiclass settings (multinomial logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'In `softmax`, the probability of a particular sample with net input *z* belonging
    to the *i*th class can be computed with a normalization term in the denominator,
    that is, the sum of the exponentially weighted linear functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To see `softmax` in action, let’s code it up in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the predicted class probabilities now sum to 1, as we would
    expect. It is also notable that the predicted class label is the same as when
    we applied the `argmax` function to the logistic output.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may help to think of the result of the `softmax` function as a *normalized*
    output that is useful for obtaining meaningful class-membership predictions in
    multiclass settings. Therefore, when we build a multiclass classification model
    in PyTorch, we can use the `torch.softmax()` function to estimate the probabilities
    of each class membership for an input batch of examples. To see how we can use
    the `torch.softmax()` activation function in PyTorch, we will convert `Z` to a
    tensor in the following code, with an additional dimension reserved for the batch
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Broadening the output spectrum using a hyperbolic tangent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another sigmoidal function that is often used in the hidden layers of artificial
    NNs is the **hyperbolic tangent** (commonly known as **tanh**), which can be interpreted
    as a rescaled version of the logistic function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The advantage of the hyperbolic tangent over the logistic function is that
    it has a broader output spectrum ranging in the open interval (–1, 1), which can
    improve the convergence of the backpropagation algorithm (*Neural Networks for
    Pattern Recognition*, *C. M. Bishop*, *Oxford University Press*, pages: 500-501,
    *1995*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the logistic function returns an output signal ranging in the
    open interval (0, 1). For a simple comparison of the logistic function and the
    hyperbolic tangent, let’s plot the two sigmoidal functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the shapes of the two sigmoidal curves look very similar; however,
    the `tanh` function has double the output space of the `logistic` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: A comparison of the tanh and logistic functions'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we previously implemented the `logistic` and `tanh` functions verbosely
    for the purpose of illustration. In practice, we can use NumPy’s `tanh` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, when building an NN model, we can use `torch.tanh(x)` in PyTorch
    to achieve the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, the logistic function is available in SciPy’s `special` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we can use the `torch.sigmoid()` function in PyTorch to do the same
    computation, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Note that using `torch.sigmoid(x)` produces results that are equivalent to `torch.nn.Sigmoid()(x)`,
    which we used earlier. `torch.nn.Sigmoid` is a class to which you can pass in
    parameters to construct an object in order to control the behavior. In contrast,
    `torch.sigmoid` is a function.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear unit activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **rectified linear unit** (**ReLU**) is another activation function that
    is often used in deep NNs. Before we delve into ReLU, we should step back and
    understand the vanishing gradient problem of tanh and logistic activations.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this problem, let’s assume that we initially have the net input
    *z*[1] = 20, which changes to *z*[2] = 25\. Computing the tanh activation, we
    get ![](img/B17582_12_008.png) and ![](img/B17582_12_009.png), which shows no
    change in the output (due to the asymptotic behavior of the tanh function and
    numerical errors).
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the derivative of activations with respect to the net input
    diminishes as *z* becomes large. As a result, learning the weights during the
    training phase becomes very slow because the gradient terms may be very close
    to zero. ReLU activation addresses this issue. Mathematically, ReLU is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_12_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ReLU is still a nonlinear function that is good for learning complex functions
    with NNs. Besides this, the derivative of ReLU, with respect to its input, is
    always 1 for positive input values. Therefore, it solves the problem of vanishing
    gradients, making it suitable for deep NNs. In PyTorch, we can apply the ReLU
    activation `torch.relu()` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: We will use the ReLU activation function in the next chapter as an activation
    function for multilayer convolutional NNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know more about the different activation functions that are commonly
    used in artificial NNs, let’s conclude this section with an overview of the different
    activation functions that we have encountered so far in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B17582_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: The activation functions covered in this book'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the list of all activation functions available in the `torch.nn`
    module at [https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use PyTorch, an open source library for
    numerical computations, with a special focus on deep learning. While PyTorch is
    more inconvenient to use than NumPy, due to its additional complexity to support
    GPUs, it allows us to define and train large, multilayer NNs very efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you learned about using the `torch.nn` module to build complex machine
    learning and NN models and run them efficiently. We explored model building in
    PyTorch by defining a model from scratch via the basic PyTorch tensor functionality.
    Implementing models can be tedious when we have to program at the level of matrix-vector
    multiplications and define every detail of each operation. However, the advantage
    is that this allows us, as developers, to combine such basic operations and build
    more complex models. We then explored `torch.nn`, which makes building NN models
    a lot easier than implementing them from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned about different activation functions and understood their
    behaviors and applications. Specifically, in this chapter, we covered tanh, softmax,
    and ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll continue our journey and dive deeper into PyTorch,
    where we’ll find ourselves working with PyTorch computation graphs and the automatic
    differentiation package. Along the way, you’ll learn many new concepts, such as
    gradient computations.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
