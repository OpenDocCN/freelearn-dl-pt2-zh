- en: Generating Song Lyrics Using RNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 生成歌词
- en: In a normal feedforward neural network, each input is independent of other input.
    But with a sequential dataset, we need to know about the past input to make a
    prediction. A sequence is an ordered set of items. For instance, a sentence is
    a sequence of words. Let's suppose that we want to predict the next word in a
    sentence; to do so, we need to remember the previous words. A normal feedforward
    neural network cannot predict the correct next word, as it will not remember the
    previous words of the sentence. Under such circumstances (in which we need to
    remember the previous input), to make predictions, we use **recurrent neural networks**
    (**RNNs**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通前馈神经网络中，每个输入都是独立的。但是对于序列数据集，我们需要知道过去的输入以进行预测。序列是一组有序的项目。例如，一个句子是一个单词序列。假设我们想要预测句子中的下一个单词；为此，我们需要记住之前的单词。普通的前馈神经网络无法预测出正确的下一个单词，因为它不会记住句子的前面单词。在这种需要记住先前输入的情况下（在这种情况下，我们需要记住先前输入以进行预测），为了进行预测，我们使用**递归神经网络**（**RNNs**）。
- en: In this chapter, we will describe how an RNN is used to model sequential datasets
    and how it remembers the previous input. We will begin by investigating how an
    RNN differs from a feedforward neural network. Then, we will inspect how forward
    propagation works in an RNN.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将描述如何使用 RNN 对序列数据集进行建模以及如何记住先前的输入。我们将首先研究 RNN 与前馈神经网络的区别。然后，我们将检查 RNN
    中的前向传播是如何工作的。
- en: Moving on, we will examine the **backpropagation through time** (**BPTT**) algorithm,
    which is used for training RNNs. Later, we will look at the vanishing and exploding
    gradient problem, which occurs while training recurrent networks. You will also
    learn how to generate song lyrics using an RNN in TensorFlow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 继续，我们将研究**时域反向传播**（**BPTT**）算法，该算法用于训练 RNN。随后，我们将讨论梯度消失和爆炸问题，在训练递归网络时会出现这些问题。您还将学习如何使用
    TensorFlow 中的 RNN 生成歌词。
- en: At the end of the chapter, will we examine the different types of RNN architectures,
    and how they are used for various applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们将研究不同类型的 RNN 架构，以及它们在各种应用中的使用。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Recurrent neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: Forward propagation in RNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 中的前向传播
- en: Backpropagation through time
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时域反向传播
- en: The vanishing and exploding gradient problem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度消失和爆炸问题
- en: Generating song lyrics using RNNs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RNN 生成歌词
- en: Different types of RNN architectures
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的 RNN 架构
- en: Introducing RNNs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 RNNs
- en: '*The Sun rises in the ____.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*太阳在 ____ 中升起。*'
- en: If we were asked to predict the blank term in the preceding sentence, we would
    probably say east. Why would we predict that the word east would be the right
    word here? Because we read the whole sentence, understood the context, and predicted
    that the word east would be an appropriate word to complete the sentence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们被要求预测前面句子中的空白处，我们可能会说东方。为什么我们会预测东方是正确的词汇在这里？因为我们读了整个句子，理解了上下文，并预测东方是一个适合完成句子的合适词汇。
- en: If we use a feedforward neural network to predict the blank, it would not predict
    the right word. This is due to the fact that in feedforward networks, each input
    is independent of other input and they make predictions based only on the current
    input, and they don't remember previous input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用前馈神经网络来预测空白，它将不能预测出正确的单词。这是因为在前馈网络中，每个输入都是独立的，并且它们仅基于当前输入进行预测，它们不记住前面的输入。
- en: Thus, the input to the network will just be the word preceding the blank, which
    is the word *the*. With this word alone as an input, our network cannot predict
    the correct word, because it doesn't know the context of the sentence, which means
    that it doesn't know the previous set of words to understand the context of the
    sentence and to predict an appropriate next word.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，网络的输入将仅仅是前一个空白处的单词，这个单词是 *the*。仅凭这个单词作为输入，我们的网络无法预测出正确的单词，因为它不知道句子的上下文，也就是说它不知道前面一组单词来理解句子的语境并预测合适的下一个单词。
- en: Here is where we use RNNs. They predict output not only based on the current
    input, but also on the previous hidden state. Why do they have to predict the
    output based on the current input and the previous hidden state? Why can't they
    just use the current input and the previous input?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用 RNN 的地方。它们不仅基于当前输入预测输出，还基于先前的隐藏状态。为什么它们必须基于当前输入和先前的隐藏状态来预测输出呢？为什么不能只使用当前输入和先前的输入呢？
- en: This is because the previous input will only store information about the previous
    word, while the previous hidden state will capture the contextual information
    about all the words in the sentence that the network has seen so far. Basically,
    the previous hidden state acts like a memory and it captures the context of the
    sentence. With this context and the current input, we can predict the relevant
    word.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为前一个输入仅存储有关前一个单词的信息，而前一个隐藏状态将捕捉到网络迄今所见的句子中所有单词的上下文信息。基本上，前一个隐藏状态 acts like
    a memory，并捕捉句子的上下文。有了这个上下文和当前输入，我们可以预测相关的单词。
- en: For instance, let's take the same sentence, *The sun rises in the ____.* As
    shown in the following figure, we first pass the word *the* as an input, and then
    we pass the next word, *sun*, as input; but along with this, we also pass the
    previous hidden state, ![](img/458c6f8a-d40d-45d3-8198-eaf4aafa46ac.png). So,
    every time we pass the input word, we also pass a previous hidden state as an
    input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们拿同样的句子 *The sun rises in the ____.* 举例。如下图所示，我们首先将单词 *the* 作为输入传递，然后将下一个单词
    *sun* 作为输入；但与此同时，我们也传递了上一个隐藏状态，![](img/458c6f8a-d40d-45d3-8198-eaf4aafa46ac.png)。因此，每次传递输入单词时，我们也会传递前一个隐藏状态作为输入。
- en: 'In the final step, we pass the word *the*, and also the previous hidden state
    ![](img/c8294095-6268-44e5-873a-4e6c0e90f390.png), which captures the contextual
    information about the sequence of words that the network has seen so far. Thus,
    ![](img/bf6446ea-a525-4fb6-a28e-593405664893.png) acts as the memory and stores
    information about all the previous words that the network has seen. With ![](img/bd898219-78d6-4fd4-a8f3-7dfa95903ae9.png)
    and the current input word (*the*), we can predict the relevant next word:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们传递单词 *the*，同时传递前一个隐藏状态 ![](img/c8294095-6268-44e5-873a-4e6c0e90f390.png)，它捕捉到了网络迄今为止所见的单词序列的上下文信息。因此，![](img/bf6446ea-a525-4fb6-a28e-593405664893.png)
    充当了记忆，存储了网络已经看到的所有先前单词的信息。有了 ![](img/bd898219-78d6-4fd4-a8f3-7dfa95903ae9.png)
    和当前输入单词 (*the*)，我们可以预测出相关的下一个单词：
- en: '![](img/a2e18a6b-2792-47f8-97c9-863377ab4bde.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2e18a6b-2792-47f8-97c9-863377ab4bde.png)'
- en: In a nutshell, an RNN uses the previous hidden state as memory which captures
    and stores the contextual information (input) that the network has seen so far.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，RNN 使用前一个隐藏状态作为记忆，捕捉并存储网络迄今所见的上下文信息（输入）。
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various **natural language processing** (**NLP**) tasks, such
    as language translation, sentiment analysis, text generation, and so on.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 广泛应用于涉及序列数据的用例，如时间序列、文本、音频、语音、视频、天气等等。它们在各种自然语言处理（**NLP**）任务中被广泛使用，如语言翻译、情感分析、文本生成等。
- en: The difference between feedforward networks and RNNs
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈网络和RNN之间的区别
- en: 'A comparison between an RNN and a feedforward network is shown in the following
    diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 和前馈网络的比较如下图所示：
- en: '![](img/4327bdb1-a3c4-4763-8a6d-82fb97682747.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4327bdb1-a3c4-4763-8a6d-82fb97682747.png)'
- en: As you can observe in the preceding diagram, the RNN contains a looped connection
    in the hidden layer, which implies that we use the previous hidden state along
    with the input to predict the output.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可以在前面的图表中观察到的那样，RNN 在隐藏层中包含一个循环连接，这意味着我们使用前一个隐藏状态与输入一起来预测输出。
- en: Still confused? Let's look at the following unrolled version of an RNN. But
    wait; what is the unrolled version of an RNN?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然感到困惑吗？让我们看看下面展开的一个RNN版本。但等等，什么是RNN的展开版本？
- en: 'It means that we roll out the network for a complete sequence. Let''s suppose
    that we have an input sentence with ![](img/04c2ff2f-17e8-47b5-9943-3ef499dcd39b.png)
    words; then, we will have ![](img/ccf0389e-8454-44c0-a831-7a712a1393dc.png) to
    ![](img/dc35c22d-bba3-4a16-b0f2-aa6ffa5857e9.png) layers, one for each word, as
    shown in the following figure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们展开网络以完成一个完整的序列。假设我们有一个包含 ![](img/04c2ff2f-17e8-47b5-9943-3ef499dcd39b.png)
    个单词的输入句子；那么我们将有 ![](img/ccf0389e-8454-44c0-a831-7a712a1393dc.png) 到 ![](img/dc35c22d-bba3-4a16-b0f2-aa6ffa5857e9.png)
    层，每层对应一个单词，如下图所示：
- en: '![](img/d2509d60-cf99-4d3c-9cc4-b368a9e9c212.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2509d60-cf99-4d3c-9cc4-b368a9e9c212.png)'
- en: As you can see in the preceding figure, at the time step ![](img/8872a7d9-968c-41fe-9887-abbc14a24781.png),
    the output ![](img/c188d6dd-d163-4906-89d7-c61a2bfdab8f.png) is predicted based
    on the current input ![](img/c0b5e079-f0e9-4776-9802-69bc26929f72.png) and the
    previous hidden state ![](img/ba821958-371b-41b1-ad62-dfefef803180.png). Similarly,
    at time step ![](img/3872e1e6-85c5-4424-9683-cd204d2cb891.png), ![](img/2bf40a00-a797-4927-8fab-58170517d363.png)
    is predicted using the current input ![](img/d6655da4-2f28-463f-9317-7c187a62e859.png)
    and the previous hidden state ![](img/aeac31f3-91bd-487d-88fb-f61d87394fa4.png).
    This is how an RNN works; it takes the current input and the previous hidden state
    to predict the output.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前图中所见，在时间步 ![](img/8872a7d9-968c-41fe-9887-abbc14a24781.png)，基于当前输入 ![](img/c0b5e079-f0e9-4776-9802-69bc26929f72.png)
    和先前的隐藏状态 ![](img/ba821958-371b-41b1-ad62-dfefef803180.png) 预测输出 ![](img/c188d6dd-d163-4906-89d7-c61a2bfdab8f.png)。同样，在时间步
    ![](img/3872e1e6-85c5-4424-9683-cd204d2cb891.png)，基于当前输入 ![](img/d6655da4-2f28-463f-9317-7c187a62e859.png)
    和先前的隐藏状态 ![](img/aeac31f3-91bd-487d-88fb-f61d87394fa4.png)，预测 ![](img/2bf40a00-a797-4927-8fab-58170517d363.png)。这就是RNN的工作原理；它利用当前输入和先前的隐藏状态来预测输出。
- en: Forward propagation in RNNs
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络的前向传播
- en: 'Let''s look at how an RNN uses forward propagation to predict the output; but
    before we jump right in, let''s get familiar with the notations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看循环神经网络如何使用前向传播来预测输出；但在我们深入探讨之前，让我们熟悉一下符号：
- en: '![](img/67aaa29a-50cc-49fe-a893-ee7d2a1d4872.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67aaa29a-50cc-49fe-a893-ee7d2a1d4872.png)'
- en: 'The preceding figure illustrates the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图示明了以下：
- en: '![](img/e8c14ce3-b877-46b6-9517-401decad5e39.png) represents the input to hidden
    layer weight matrix'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/e8c14ce3-b877-46b6-9517-401decad5e39.png) 表示输入到隐藏层的权重矩阵'
- en: '![](img/b538e5f2-eede-4658-85af-8970a3516600.png) represents the hidden to
    hidden layer weight matrix'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/b538e5f2-eede-4658-85af-8970a3516600.png) 表示隐藏到隐藏层的权重矩阵'
- en: '![](img/a65d52e4-e147-484f-9fa3-eb6279443766.png) represents the hidden to
    output layer weight matrix'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/a65d52e4-e147-484f-9fa3-eb6279443766.png) 表示隐藏到输出层的权重矩阵'
- en: 'The hidden state ![](img/b9242358-ecb8-4726-8326-d92c4a311927.png) at a time
    step ![](img/cb45a853-41f7-484f-9446-d6f3f23bd3dd.png) can be computed as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 ![](img/cb45a853-41f7-484f-9446-d6f3f23bd3dd.png)，隐藏状态 ![](img/b9242358-ecb8-4726-8326-d92c4a311927.png)
    可以计算如下：
- en: '![](img/5b43f70d-e24c-4d7f-b483-59754144e72d.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b43f70d-e24c-4d7f-b483-59754144e72d.png)'
- en: That is, *hidden state at a time step, t = tanh([input to hidden layer weight
    x input] + [hidden to hidden layer weight x previous hidden state])*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*时间步 t 的隐藏状态 = tanh([输入到隐藏层权重 x 输入] + [隐藏到隐藏层权重 x 先前隐藏状态])*。
- en: 'The output at a time step ![](img/c16541d9-f156-4c48-83e3-6dc4f35070cc.png)
    can be computed as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步 ![](img/c16541d9-f156-4c48-83e3-6dc4f35070cc.png)，输出可以如下计算：
- en: '![](img/10b8680f-7613-4b12-b473-4a471403db20.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10b8680f-7613-4b12-b473-4a471403db20.png)'
- en: That is, *output at a time step, t = softmax (hidden to output layer weight*
    x *hidden state at a time t)*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*时间步 t 的输出 = softmax(隐藏到输出层权重 x 时间步 t 的隐藏状态)*。
- en: 'We can also represent RNNs as shown in the following figure. As you can see,
    the hidden layer is represented by an RNN block, which implies that our network
    is an RNN, and previous hidden states are used in predicting the output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以如下图所示表示循环神经网络（RNN）。正如您所看到的，隐藏层由一个RNN块表示，这意味着我们的网络是一个RNN，并且先前的隐藏状态用于预测输出：
- en: '![](img/4ed16907-a459-43dc-9d6c-640e671139a6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ed16907-a459-43dc-9d6c-640e671139a6.png)'
- en: 'The following diagram shows how forward propagation works in an unrolled version
    of an RNN:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了RNN展开版本中前向传播的工作原理：
- en: '![](img/edec3143-35b0-4af7-ba04-b37e14030d2a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edec3143-35b0-4af7-ba04-b37e14030d2a.png)'
- en: 'We initialize the initial hidden state ![](img/6615f685-b929-45b2-b7e0-e45c559381db.png)
    with random values. As you can see in the preceding figure, the output, ![](img/50ebe989-e25d-4826-9c9b-71bba2cf8ffa.png),
    is predicted based on the current input, ![](img/c32660b6-140a-49df-b3bd-f71b2c1c8987.png)
    and the previous hidden state, which is an initial hidden state, ![](img/6615f685-b929-45b2-b7e0-e45c559381db.png),
    using the following formula:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用随机值初始化初始隐藏状态 ![](img/6615f685-b929-45b2-b7e0-e45c559381db.png)。如前图所示，输出 ![](img/50ebe989-e25d-4826-9c9b-71bba2cf8ffa.png)
    基于当前输入 ![](img/c32660b6-140a-49df-b3bd-f71b2c1c8987.png) 和先前的隐藏状态（即初始隐藏状态） ![](img/6615f685-b929-45b2-b7e0-e45c559381db.png)
    使用以下公式预测：
- en: '![](img/1f792fc7-d64f-4a83-902e-2c420fa51d2b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f792fc7-d64f-4a83-902e-2c420fa51d2b.png)'
- en: '![](img/cd00e1ac-010a-4980-8ca5-2aeab71ccf56.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd00e1ac-010a-4980-8ca5-2aeab71ccf56.png)'
- en: 'Similarly, look at how the output, ![](img/f553c432-9a57-444f-97e8-010ee27f60bc.png),
    is computed. It takes the current input, ![](img/f64094cd-9f12-40dd-8916-7d9ecc16810b.png),
    and the previous hidden state, ![](img/33d251b6-7355-4bdb-8fac-2dc7c7a0d7e3.png):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，看看输出 ![](img/f553c432-9a57-444f-97e8-010ee27f60bc.png) 是如何计算的。它使用当前输入 ![](img/f64094cd-9f12-40dd-8916-7d9ecc16810b.png)
    和先前的隐藏状态 ![](img/33d251b6-7355-4bdb-8fac-2dc7c7a0d7e3.png)：
- en: '![](img/235ea93c-2679-453e-b39c-a69e877df36a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/235ea93c-2679-453e-b39c-a69e877df36a.png)'
- en: '![](img/4e7da6bf-ee7a-4508-a685-3deca9ae7691.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e7da6bf-ee7a-4508-a685-3deca9ae7691.png)'
- en: Thus, in forward propagation to predict the output, RNN uses the current input
    and the previous hidden state.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在前向传播中，为了预测输出，RNN 使用当前输入和先前的隐藏状态。
- en: 'To achieve clarity, let''s look at how to implement forward propagation in
    RNN to predict the output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，让我们看看如何在RNN中实现前向传播以预测输出：
- en: 'Initialize all the weights, ![](img/c3efe389-783a-49bc-836e-476412e21b8a.png),
    ![](img/0e801a23-d593-4670-866b-8fd1450bf481.png), and ![](img/bc9a798b-7e2f-4aff-a0e1-065b94718831.png),
    by randomly drawing from the uniform distribution:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从均匀分布中随机抽取，初始化所有权重 ![](img/c3efe389-783a-49bc-836e-476412e21b8a.png)，![](img/0e801a23-d593-4670-866b-8fd1450bf481.png)，和
    ![](img/bc9a798b-7e2f-4aff-a0e1-065b94718831.png)：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the number of time steps, which will be the length of our input sequence,
    ![](img/e3ea8ad6-f6b8-42c1-b3df-aedd6220ef0d.png):'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义时间步长的数量，这将是我们输入序列的长度 ![](img/e3ea8ad6-f6b8-42c1-b3df-aedd6220ef0d.png)：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the hidden state:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义隐藏状态：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Initialize the initial hidden state, ![](img/749e77de-caab-41e5-bde8-52a5ad068ecb.png),
    with zeros:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用零初始化初始隐藏状态 ![](img/749e77de-caab-41e5-bde8-52a5ad068ecb.png)：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize the output:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化输出：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For every time step, we perform the following:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个时间步长，我们执行以下操作：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Backpropagating through time
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过时间反向传播
- en: 'We just learned how forward propagation works in RNNs and how it predicts the
    output. Now, we compute the loss, ![](img/64479a6c-b690-4efc-86a7-517c7dd06b79.png),
    at each time step, ![](img/4caa63fe-0308-4e69-96e7-3049d125d643.png), to determine
    how well the RNN has predicted the output. We use the cross-entropy loss as our
    loss function. The loss ![](img/4d9d85c2-78d6-47e5-ae6d-6cd76138ded6.png) at a
    time step ![](img/4caa63fe-0308-4e69-96e7-3049d125d643.png) can be given as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了RNN中的前向传播如何工作以及如何预测输出。现在，我们计算损失 ![](img/64479a6c-b690-4efc-86a7-517c7dd06b79.png)，在每个时间步
    ![](img/4caa63fe-0308-4e69-96e7-3049d125d643.png) 上，以确定RNN预测输出的效果。我们使用交叉熵损失作为我们的损失函数。时间步
    ![](img/4caa63fe-0308-4e69-96e7-3049d125d643.png) 处的损失 ![](img/4d9d85c2-78d6-47e5-ae6d-6cd76138ded6.png)
    可以如下给出：
- en: '![](img/4f24f775-00f1-4b1f-b74f-2b31ea5c2c95.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f24f775-00f1-4b1f-b74f-2b31ea5c2c95.png)'
- en: Here, ![](img/20982ece-f436-4ca3-be19-332765024b10.png) is the actual output,
    and ![](img/42669ca6-70c1-4d62-b23e-38ba3ecd9cc3.png) is the predicted output
    at a time step ![](img/9c19e742-9400-42f5-b91b-c2b833aea576.png).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/20982ece-f436-4ca3-be19-332765024b10.png) 是实际输出，而 ![](img/42669ca6-70c1-4d62-b23e-38ba3ecd9cc3.png)
    是时间步长为 ![](img/9c19e742-9400-42f5-b91b-c2b833aea576.png) 时的预测输出。
- en: 'The final loss is a sum of the loss at all the time steps. Suppose that we
    have ![](img/81a851a4-6e51-4bec-86de-121b488ed4b1.png) layers; then, the final
    loss can be given as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最终损失是所有时间步长上的损失之和。假设我们有 ![](img/81a851a4-6e51-4bec-86de-121b488ed4b1.png) 层；那么，最终损失可以如下给出：
- en: '![](img/5ca94542-8bb2-4517-8618-9db73674966f.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ca94542-8bb2-4517-8618-9db73674966f.png)'
- en: 'As shown in the following figure, the final loss is obtained by the sum of
    loss at all the time steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，最终损失是所有时间步长上损失的总和：
- en: '![](img/c99e1c4b-b2f2-4885-9c9a-d49b00dd5483.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c99e1c4b-b2f2-4885-9c9a-d49b00dd5483.png)'
- en: 'We computed the loss, now our goal is to minimize the loss. How can we minimize
    the loss? We can minimize the loss by finding the optimal weights of the RNN.
    As we learned, we have three weights in RNNs: input to hidden, ![](img/506de957-8136-4b7a-b727-16decfa5cb39.png),
    hidden to hidden, ![](img/6c47a07a-178e-4963-8d71-412cecc002eb.png), and hidden
    to output, ![](img/d53286da-5f7e-426d-8412-48e443a6d220.png).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了损失，现在我们的目标是最小化损失。我们如何最小化损失？我们可以通过找到RNN的最优权重来最小化损失。正如我们所学的，RNN中有三个权重：输入到隐藏的权重
    ![](img/506de957-8136-4b7a-b727-16decfa5cb39.png)，隐藏到隐藏的权重 ![](img/6c47a07a-178e-4963-8d71-412cecc002eb.png)，以及隐藏到输出的权重
    ![](img/d53286da-5f7e-426d-8412-48e443a6d220.png)。
- en: 'We need to find optimal values for all of these three weights to minimize the
    loss. We can use our favorite gradient descent algorithm to find the optimal weights.
    We begin by calculating the gradients of the loss function with respect to all
    the weights; then, we update the weights according to the weight update rule as
    follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/800d1857-5449-4155-b448-441188297d6c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
- en: '![](img/a0258154-1ac4-4772-bd1b-7507a7f45b01.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: '![](img/814dc1b3-9460-4875-924e-af775624cc0a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
- en: You can skip the upcoming sections if you don't want to understand the math
    behind the gradient calculation. However, it will help you to better understand
    how BPTT works in RNN better.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: First, we calculate the gradients of loss with respect to the final layer ![](img/f451bf26-6e9d-4a0e-8bc3-ee5753ae46e3.png),
    that is ![](img/f6ece5a6-92ec-4c30-a60e-8b739a0f4728.png), so that we can use
    it in the upcoming steps.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have learned, the loss ![](img/4d9d85c2-78d6-47e5-ae6d-6cd76138ded6.png)
    at a time step ![](img/4caa63fe-0308-4e69-96e7-3049d125d643.png) can be given
    as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa612325-7e04-4e72-a3b2-4a1feea9c90f.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Since we know:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/334d43db-f1c5-4dee-892a-a8ff46c60b0b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'We can write:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/001b5467-5c38-4d8c-8cef-e76ec7447d08.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the gradient of the loss ![](img/4d9d85c2-78d6-47e5-ae6d-6cd76138ded6.png)
    with respect to ![](img/f451bf26-6e9d-4a0e-8bc3-ee5753ae46e3.png) becomes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68b43156-0c4a-42cd-9fa0-298cbd640074.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Now, we will learn how to calculate the gradient of the loss with respect to
    all the weights one by one.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Gradients with respect to the hidden to output weight, V
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s recap the steps involved in the forward propagation:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc1a523f-b1b7-4eef-944c-a7cfed5eded2.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: '![](img/14f05246-b094-4bf9-9b08-7b543ecc6b21.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: '![](img/b06b4611-ead7-4a7d-af35-873d64a8da0a.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'Let''s suppose that ![](img/c3f72581-aa05-4ad7-b0f0-fbcc278a1597.png), and
    substituting this into equation *(2)*, we can rewrite the preceding steps as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc1a523f-b1b7-4eef-944c-a7cfed5eded2.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
- en: '![](img/f1d0c5e5-514f-4c19-8a63-f1de866825ae.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: '![](img/855a730d-09e9-4a57-8795-a68202406061.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: After predicting the output ![](img/f072d0a4-7355-47a7-bea9-9e03ce88bffc.png),
    we are in the final layer of the network. Since we are backpropagating, that is,
    going from the output layer to the input layer, our first weight would be ![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png),
    which is the hidden to output layer weight.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen that the final loss is the sum of the loss over all the time steps,
    and similarly, the final gradient is the sum of gradients over all the time steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3403b3f1-f6bb-42ef-b8fa-b236013e5810.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
- en: 'Hence, we can write:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c24ef53-b82f-4f72-81cd-6b7672b1e646.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: Recall our loss function, ![](img/49409524-301d-4a6b-8ec5-f88c6f42d34d.png);
    we cannot calculate the gradient with respect to
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png) directly from ![](img/842e394d-d8d4-4ad5-9176-0f5415c98d08.png),
    as there are no ![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png) terms in it.
    So, we apply the chain rule. Recall the forward propagation equation; there is
    a ![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png) term in ![](img/db5a20a7-db1c-48b2-85c4-c2017bc7d5ea.png):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png)直接来自![](img/842e394d-d8d4-4ad5-9176-0f5415c98d08.png)，因为其中没有![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png)项。因此，我们应用链式法则。回顾前向传播方程；在![](img/db5a20a7-db1c-48b2-85c4-c2017bc7d5ea.png)中有一个![](img/b5c7341a-7af7-4bb6-9501-174fcfed9b05.png)项：'
- en: '![](img/7dbc66a2-0727-403a-b3a0-bfb1b086ac55.png) where ![](img/d59f4d43-7dd0-401e-8635-308ebbe3c09b.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dbc66a2-0727-403a-b3a0-bfb1b086ac55.png)，其中![](img/d59f4d43-7dd0-401e-8635-308ebbe3c09b.png)'
- en: First, we calculate a partial derivative of the loss with respect to ![](img/db5a20a7-db1c-48b2-85c4-c2017bc7d5ea.png),
    and then, from ![](img/db5a20a7-db1c-48b2-85c4-c2017bc7d5ea.png), we will calculate
    the partial derivative with respect to ![](img/29d11f59-f2ee-498b-99e9-c318ff3acb9c.png).
    From ![](img/d64191a9-469b-44de-9a17-cc396859ae4f.png), we can calculate the derivative
    with respect to ![](img/4f1f59f8-4b8e-43fb-9c5c-baed623ee9d0.png).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算损失对![](img/db5a20a7-db1c-48b2-85c4-c2017bc7d5ea.png)的偏导数，然后从![](img/db5a20a7-db1c-48b2-85c4-c2017bc7d5ea.png)中计算对![](img/29d11f59-f2ee-498b-99e9-c318ff3acb9c.png)的偏导数。从![](img/d64191a9-469b-44de-9a17-cc396859ae4f.png)中，我们可以计算对![](img/4f1f59f8-4b8e-43fb-9c5c-baed623ee9d0.png)的导数。
- en: 'Thus, our equation becomes the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的方程如下：
- en: '![](img/4af89994-d2cd-45c6-a152-e46b5f05c784.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4af89994-d2cd-45c6-a152-e46b5f05c784.png)'
- en: 'As we know that, ![](img/002f1a76-e814-460a-8c78-4561df5d3a3c.png), the gradient
    of loss with respect to ![](img/1a5006ba-52b3-49bd-9d8d-476242f5f8bd.png) can
    be calculated as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道![](img/002f1a76-e814-460a-8c78-4561df5d3a3c.png)，损失函数对![](img/1a5006ba-52b3-49bd-9d8d-476242f5f8bd.png)的梯度可以计算如下：
- en: '![](img/4d4684ca-b4ba-4bca-9a04-64bd59d15b02.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d4684ca-b4ba-4bca-9a04-64bd59d15b02.png)'
- en: 'Substituting equation *(4)* in equation *(3)*, we can write the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程*(4)*代入方程*(3)*，我们可以写成以下形式：
- en: '![](img/36da3d10-1767-4597-b24d-c8833abb4e0c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36da3d10-1767-4597-b24d-c8833abb4e0c.png)'
- en: 'For better understanding, let''s take each of the terms from the preceding
    equation and compute them one by one:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们逐个从前述方程中取出每个项并逐个计算：
- en: '![](img/1112b7f2-339b-4ce5-99cf-c6df78d40194.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1112b7f2-339b-4ce5-99cf-c6df78d40194.png)'
- en: 'From equation *(1)*, we can substitute the value of ![](img/e195ec7d-8c38-4f52-ab57-299b622f2e7c.png)
    in the preceding equation *(6)* as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程*(1)*，我们可以将![](img/e195ec7d-8c38-4f52-ab57-299b622f2e7c.png)的值代入前述方程*(6)*中，如下所示：
- en: '![](img/b2ed4f38-4b08-4015-b1c8-87840c3dfecb.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2ed4f38-4b08-4015-b1c8-87840c3dfecb.png)'
- en: 'Now, we will compute the term ![](img/d09b3f31-d03f-4b17-ba13-fca7055c0054.png).
    Since we know, ![](img/002f1a76-e814-460a-8c78-4561df5d3a3c.png), computing ![](img/0bdacbe8-9489-4ef0-bfe8-6ace5c71c78a.png)
    gives us the derivative of the softmax function:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算项![](img/d09b3f31-d03f-4b17-ba13-fca7055c0054.png)。因为我们知道![](img/002f1a76-e814-460a-8c78-4561df5d3a3c.png)，计算![](img/0bdacbe8-9489-4ef0-bfe8-6ace5c71c78a.png)给出softmax函数的导数：
- en: '![](img/4e2582cf-bc25-4eb8-b235-d018c1f34ff5.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e2582cf-bc25-4eb8-b235-d018c1f34ff5.png)'
- en: 'The derivative of the softmax function can be represented as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数的导数可以表示如下：
- en: '![](img/892c67de-449c-4a37-b8f7-75184b88a427.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/892c67de-449c-4a37-b8f7-75184b88a427.png)'
- en: 'Substituting equation *(8)* into equation *(7)*, we can write the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程*(8)*代入方程*(7)*，我们可以写成以下形式：
- en: '![](img/d7f8d2eb-3cf1-4339-8427-b991003ade3b.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7f8d2eb-3cf1-4339-8427-b991003ade3b.png)'
- en: 'Thus, the final equation becomes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终方程如下：
- en: '![](img/32da4bf8-3349-4236-a6a6-43edcdbbe1a8.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32da4bf8-3349-4236-a6a6-43edcdbbe1a8.png)'
- en: 'Now, we can substitute equation *(9)* into equation *(5)*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将方程*(9)*代入方程*(5)*：
- en: '![](img/12399086-bdfa-400a-a02a-5242d04abef7.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12399086-bdfa-400a-a02a-5242d04abef7.png)'
- en: 'Since we know that ![](img/2b93d32d-50de-4e91-ab73-7f19e1b80d5f.png), we can
    write:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们知道![](img/2b93d32d-50de-4e91-ab73-7f19e1b80d5f.png)，我们可以写成：
- en: '![](img/147a2feb-72a0-4df3-9025-8515df5d35f3.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/147a2feb-72a0-4df3-9025-8515df5d35f3.png)'
- en: 'Substituting the preceding equation into equation *(10)*, we get our final
    equation, that is, gradient of the loss function with respect to ![](img/a1d9a457-be7b-4445-b6f8-9faa63a53991.png),
    as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将前述方程代入方程*(10)*，我们得到我们的最终方程，即损失函数对![](img/a1d9a457-be7b-4445-b6f8-9faa63a53991.png)的梯度如下：
- en: '![](img/2792d04c-bb47-4e78-a0db-87b4da96e897.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2792d04c-bb47-4e78-a0db-87b4da96e897.png)'
- en: Gradients with respect to hidden to hidden layer weights, W
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对隐藏到隐藏层权重W的梯度
- en: 'Now, we will compute the gradients of loss with respect to hidden to hidden
    layer weights, ![](img/a7804dd9-7a32-4507-a4fb-7ea08209578a.png). Similar to ![](img/683d0de8-d8cd-4798-a13d-a01c5b5f8c3b.png),
    the final gradient is the sum of the gradients at all time steps:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将计算损失相对于隐藏到隐藏层权重 ![](img/a7804dd9-7a32-4507-a4fb-7ea08209578a.png) 的梯度。与
    ![](img/683d0de8-d8cd-4798-a13d-a01c5b5f8c3b.png) 类似，最终的梯度是所有时间步长上梯度的总和：
- en: '![](img/fa73b527-33cc-4787-b508-e32652748604.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa73b527-33cc-4787-b508-e32652748604.png)'
- en: 'So, we can write:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写成：
- en: '![](img/834ff810-d70a-49ba-91d7-0ac9640734b3.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/834ff810-d70a-49ba-91d7-0ac9640734b3.png)'
- en: First, let's compute gradient of loss, ![](img/8d47d734-7c46-4b0d-a8de-f88bc29b3918.png)
    with respect to ![](img/43e7ae91-5ab2-40e1-be89-018ed8d7d013.png), that is, ![](img/80edec19-56f8-4a43-b0df-f70a1ae6651a.png).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算损失的梯度 ![](img/8d47d734-7c46-4b0d-a8de-f88bc29b3918.png) 对 ![](img/43e7ae91-5ab2-40e1-be89-018ed8d7d013.png)
    的导数，即 ![](img/80edec19-56f8-4a43-b0df-f70a1ae6651a.png)。
- en: 'We cannot compute derivative of ![](img/8d47d734-7c46-4b0d-a8de-f88bc29b3918.png)
    with respect to ![](img/8904cf4f-986d-416e-a712-e41a3596babd.png) directly from
    as there are no ![](img/37e77c54-a6ac-40fc-9b65-71f35645ea44.png) terms in it.
    So, we use the chain rule to compute the gradients of loss with respect to ![](img/bd613dea-48a2-4e9b-a593-44b2f3a1258d.png).
    Let''s recollect the forward propagation equation:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接从中计算 ![](img/8d47d734-7c46-4b0d-a8de-f88bc29b3918.png) 对 ![](img/8904cf4f-986d-416e-a712-e41a3596babd.png)
    的导数，因为其中没有 ![](img/37e77c54-a6ac-40fc-9b65-71f35645ea44.png) 项。因此，我们使用链式法则计算损失对
    ![](img/bd613dea-48a2-4e9b-a593-44b2f3a1258d.png) 的梯度。让我们重新回顾前向传播方程：
- en: '![](img/a282aa7f-7ee2-4af0-9088-657715b4ad39.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a282aa7f-7ee2-4af0-9088-657715b4ad39.png)'
- en: '![](img/388b6d16-c63b-436d-af50-c528d4db225b.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/388b6d16-c63b-436d-af50-c528d4db225b.png)'
- en: '![](img/fd3bceaa-01ef-494b-ae67-7154329611bc.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd3bceaa-01ef-494b-ae67-7154329611bc.png)'
- en: 'First, we calculate the partial derivative of loss ![](img/66a379fd-243b-4699-9944-e06ebc5d56a6.png)
    with respect to ![](img/92490484-5cb1-4184-a19a-9461fa7a5cb5.png); then, from
    ![](img/783c76ba-a63a-436e-a231-8dc1035cdd78.png), we calculate the partial derivative
    with respect to ![](img/7f1ac4cd-8eac-4534-b7e7-d42de1db8c94.png); then, from
    ![](img/cd675325-add1-4e8f-b6b4-8dccdccef57d.png), we can calculate the derivative
    with respect to *W*, as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算损失 ![](img/66a379fd-243b-4699-9944-e06ebc5d56a6.png) 对 ![](img/92490484-5cb1-4184-a19a-9461fa7a5cb5.png)
    的偏导数；然后，从 ![](img/783c76ba-a63a-436e-a231-8dc1035cdd78.png) 开始，计算对 ![](img/7f1ac4cd-8eac-4534-b7e7-d42de1db8c94.png)
    的偏导数；然后，从 ![](img/cd675325-add1-4e8f-b6b4-8dccdccef57d.png) 开始，我们可以计算对 *W* 的导数，如下所示：
- en: '![](img/d0c93e46-3872-4a8e-be11-43fe60a4d504.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0c93e46-3872-4a8e-be11-43fe60a4d504.png)'
- en: 'Now, let''s compute the gradient of loss, ![](img/33bc7f4c-569e-4cf8-8e07-264224669dde.png)
    with respect to ![](img/43e7ae91-5ab2-40e1-be89-018ed8d7d013.png), that is, ![](img/f869ae3e-cf7d-4699-a1cb-2523617d3ce0.png).
    Thus, again, we will apply the chain rule and get the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算损失的梯度 ![](img/33bc7f4c-569e-4cf8-8e07-264224669dde.png) 对 ![](img/43e7ae91-5ab2-40e1-be89-018ed8d7d013.png)
    的导数，即 ![](img/f869ae3e-cf7d-4699-a1cb-2523617d3ce0.png)。因此，我们再次应用链式法则，得到以下结果：
- en: '![](img/ed563d77-3ca1-4e89-86d1-b81dd4a79313.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed563d77-3ca1-4e89-86d1-b81dd4a79313.png)'
- en: 'If you look at the preceding equation, how can we calculate the term ![](img/21ea5fbb-0b85-4e01-82d4-f36846d11bcb.png)?
    Let''s recall the equation of ![](img/0a8a684f-00ec-4748-85fc-fd8e524e9170.png):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看看前述等式，我们如何计算项 ![](img/21ea5fbb-0b85-4e01-82d4-f36846d11bcb.png)？让我们回顾一下
    ![](img/0a8a684f-00ec-4748-85fc-fd8e524e9170.png) 的方程：
- en: '![](img/ce9781a3-0336-4136-8261-384a1297d27f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce9781a3-0336-4136-8261-384a1297d27f.png)'
- en: As you can see in the preceding equation, computing ![](img/b9759f68-c0a4-4f03-8b86-2252753b5919.png)
    depends on ![](img/542ae61a-c8ac-438c-b7a5-bf89b2879073.png) and ![](img/e056a8c4-f39b-41d3-b99b-158cfd4aee25.png),
    but ![](img/eb66b04b-0700-4165-bd54-a620f8a087f2.png) is not a constant; it is
    a function again. So, we need to calculate the derivative with respect to that,
    as well.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前述等式中所看到的那样，计算 ![](img/b9759f68-c0a4-4f03-8b86-2252753b5919.png) 取决于 ![](img/542ae61a-c8ac-438c-b7a5-bf89b2879073.png)
    和 ![](img/e056a8c4-f39b-41d3-b99b-158cfd4aee25.png)，但 ![](img/eb66b04b-0700-4165-bd54-a620f8a087f2.png)
    并非常数；它再次是一个函数。因此，我们需要计算其相对于该函数的导数。
- en: 'The equation then becomes the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后方程变为：
- en: '![](img/825d9ade-e1eb-46cd-9d17-5fcc62897e78.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/825d9ade-e1eb-46cd-9d17-5fcc62897e78.png)'
- en: 'The following figure shows computing ![](img/cfe99387-87de-4d39-a1e3-1c71ee698df0.png);
    we can notice how ![](img/b9759f68-c0a4-4f03-8b86-2252753b5919.png) is dependent
    on ![](img/542ae61a-c8ac-438c-b7a5-bf89b2879073.png):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了计算 ![](img/cfe99387-87de-4d39-a1e3-1c71ee698df0.png)；我们可以注意到 ![](img/b9759f68-c0a4-4f03-8b86-2252753b5919.png)
    如何依赖于 ![](img/542ae61a-c8ac-438c-b7a5-bf89b2879073.png)：
- en: '![](img/4e1bf1cd-aaa4-45b4-bd1f-de34cde53b66.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e1bf1cd-aaa4-45b4-bd1f-de34cde53b66.png)'
- en: 'Now, let''s compute the gradient of loss, ![](img/b29c042b-b5fd-4bca-943e-7a60c587ae3c.png)
    with respect to ![](img/43e7ae91-5ab2-40e1-be89-018ed8d7d013.png), that is,![](img/d0da61ac-1cb8-4c7d-a68b-1a6a7f7cf04d.png).
    Thus, again, we will apply the chain rule and get the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算损失函数的梯度，即![](img/b29c042b-b5fd-4bca-943e-7a60c587ae3c.png)关于![](img/43e7ae91-5ab2-40e1-be89-018ed8d7d013.png)的梯度。因此，我们再次应用链式法则，得到以下结果：
- en: '![](img/57d5989d-7fd4-4a18-bca9-f72f9992dfeb.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57d5989d-7fd4-4a18-bca9-f72f9992dfeb.png)'
- en: 'In the preceding equation, we can''t compute ![](img/13eb2602-1011-485f-aff3-8aa10fb6240b.png)
    directly. Recall the equation of ![](img/58ebb286-48f5-45dd-a933-5ea3149dd2e7.png):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，我们无法直接计算![](img/13eb2602-1011-485f-aff3-8aa10fb6240b.png)。回顾方程![](img/58ebb286-48f5-45dd-a933-5ea3149dd2e7.png)：
- en: '![](img/0754e778-2993-4050-af9e-2faf72fa6c24.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0754e778-2993-4050-af9e-2faf72fa6c24.png)'
- en: 'As you observe, computing ![](img/6bdcabae-12f4-491b-b6b7-567c74347953.png),
    depends on a function ![](img/5bb0190a-05bc-4728-b2fa-5fe015b3f619.png), whereas
    ![](img/6098d4f4-a409-400f-b305-d62ff7843ecd.png) is again a function which depends
    on function ![](img/fbcd1328-a49d-4539-823f-6b8f6cdf2896.png). As shown in the
    following figure, to compute the derivative with respect to ![](img/a4f53c1b-7611-4c78-b885-56165fe7d195.png),
    we need to traverse until ![](img/59e989bb-1031-471d-9268-8d0db34e2fd1.png), as
    each function is dependent on one another:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所观察到的，计算![](img/6bdcabae-12f4-491b-b6b7-567c74347953.png)取决于一个函数![](img/5bb0190a-05bc-4728-b2fa-5fe015b3f619.png)，而![](img/6098d4f4-a409-400f-b305-d62ff7843ecd.png)再次是取决于函数![](img/fbcd1328-a49d-4539-823f-6b8f6cdf2896.png)的函数。如下图所示，为了计算关于![](img/a4f53c1b-7611-4c78-b885-56165fe7d195.png)的导数，我们需要遍历直到![](img/59e989bb-1031-471d-9268-8d0db34e2fd1.png)，因为每个函数彼此依赖：
- en: '![](img/3d415da8-a11d-42ed-8503-b7aabb36ec9f.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d415da8-a11d-42ed-8503-b7aabb36ec9f.png)'
- en: 'This can be pictorially represented as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用下图来形象地表示：
- en: '![](img/ab170b47-574e-412b-8189-1c5d66dc8d2c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab170b47-574e-412b-8189-1c5d66dc8d2c.png)'
- en: 'This applies for the loss at any time step; say, ![](img/9166e372-27b3-4c05-9ab9-3a5ccf45375b.png).
    So, we can say that to compute any loss ![](img/ad3e986a-0ea8-40a6-9494-609395378d07.png),
    we need to traverse all the way to ![](img/a2438430-3683-4d72-b56d-99234f9d1cf5.png),
    as shown in the following figure:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于任何时间步长的损失；比如说，![](img/9166e372-27b3-4c05-9ab9-3a5ccf45375b.png)。因此，我们可以说，要计算任何损失![](img/ad3e986a-0ea8-40a6-9494-609395378d07.png)，我们需要遍历到![](img/a2438430-3683-4d72-b56d-99234f9d1cf5.png)，如下图所示：
- en: '![](img/e1767315-ee84-4ec5-8a85-0374dba241fe.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1767315-ee84-4ec5-8a85-0374dba241fe.png)'
- en: This is because in RNNs, the hidden state at a time ![](img/d586504e-cac1-4947-b860-904898c59ff5.png)
    is dependent on a hidden state at a time ![](img/2883488b-b02f-4f7f-a26a-5a4d4fffa23c.png),
    which implies that the current hidden state is always dependent on the previous
    hidden state.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在循环神经网络中，时间![](img/d586504e-cac1-4947-b860-904898c59ff5.png)的隐藏状态取决于时间![](img/2883488b-b02f-4f7f-a26a-5a4d4fffa23c.png)的隐藏状态，这意味着当前隐藏状态始终依赖于先前的隐藏状态。
- en: 'So, any loss ![](img/6a2f37b3-d174-4890-9382-74e22ebacb22.png) can be computed
    as shown in the following figure:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何损失![](img/6a2f37b3-d174-4890-9382-74e22ebacb22.png)可以如下图所示地计算：
- en: '![](img/7fc6c457-3cba-4e88-b68b-46f92a8cf7fa.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7fc6c457-3cba-4e88-b68b-46f92a8cf7fa.png)'
- en: 'Thus, we can write, gradient of loss ![](img/8881eb0a-2a3a-4b4c-8480-0658def14f7b.png)
    with respect to ![](img/29bb39fe-99a0-4581-84ac-72db22f872b8.png) becomes:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写出损失函数![](img/8881eb0a-2a3a-4b4c-8480-0658def14f7b.png)关于![](img/29bb39fe-99a0-4581-84ac-72db22f872b8.png)的梯度如下：
- en: '![](img/c1a45c32-3769-4569-8c98-5b07a1c05a75.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1a45c32-3769-4569-8c98-5b07a1c05a75.png)'
- en: 'The sum ![](img/110ac4e4-8707-4023-8f35-ab38a73909e0.png) in the previous equation
    implies the sum over all the hidden states ![](img/6152d3ec-d37c-43e7-8931-faa1911f6a97.png).
    In the preceding equation,![](img/6a56cd8e-4322-4688-a2cf-c949c2a3f762.png) can
    be computed using the chain rule. So, we can say:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，前述方程中的总和![](img/110ac4e4-8707-4023-8f35-ab38a73909e0.png)意味着所有隐藏状态![](img/6152d3ec-d37c-43e7-8931-faa1911f6a97.png)的总和。在前述方程中，![](img/6a56cd8e-4322-4688-a2cf-c949c2a3f762.png)可以使用链式法则计算。因此，我们可以说：
- en: '![](img/3a596778-bf70-4139-8c90-425ece0b98c6.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a596778-bf70-4139-8c90-425ece0b98c6.png)'
- en: 'Assume that *j=3* and *k=0*; then, the preceding equation becomes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*j=3*和*k=0*；那么，前述方程变为：
- en: '![](img/a443c6f9-bea7-4bc1-880f-88156a7847e2.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a443c6f9-bea7-4bc1-880f-88156a7847e2.png)'
- en: 'Substituting equation *(12)* into equation *(11)* will give us the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 将方程*(12)*代入方程*(11)*将得到以下结果：
- en: '![](img/65b1d140-81d3-4055-b111-96f88f4cd367.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65b1d140-81d3-4055-b111-96f88f4cd367.png)'
- en: 'We know that final loss is the sum of loss across all the time steps:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/834ff810-d70a-49ba-91d7-0ac9640734b3.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation *(13)* into the preceding equation, we get the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35c4066f-7230-4ff4-9faf-de6020044c45.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'We have two summations in the preceding equation, where:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3e48fee-b0a7-487e-b1b9-f8ce81aa91ab.png) implies the sum of loss across
    all the time steps'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/753ac5f6-1add-4ee5-b5aa-ece8312eeb5c.png) is the summation over hidden
    states'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, our final equation for computing gradient of loss with respect to *W*,
    is given as:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57d629f2-fb47-40df-b50b-a1b58a73e30d.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will look at how to compute each of the terms in the preceding equation,
    one by one. From equation *(4)* and equation *(9)*, we can say that:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3572a974-3b10-4e5b-acb3-57b098b2b847.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the next term:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2a0ae76-ae52-49ef-a000-c44831b7ebad.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: 'We know that the hidden state ![](img/13e1f88c-3dd3-4e1a-9dcf-a4096cc7c42c.png)
    is computed as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dc4fda6-8b66-4eaa-8aee-3839088e1cdc.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'The derivative of ![](img/55d602d9-9548-4859-a99b-dbf12e12ff39.png) is ![](img/5ea21d93-8420-4e6a-bf5d-4449fc4d5f6b.png),
    so we can write:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f8921c9-8713-4737-88bd-5ea19ea5efce.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the final term ![](img/bd407262-7e2f-47b2-a6ef-6cf798693488.png).
    We know that the hidden state ![](img/860d20ad-deda-49ed-a11e-b5917198f42d.png)
    is computed as, ![](img/49b5386f-6fd3-498c-956a-6908a8043de2.png). Thus, the derivation
    of ![](img/359883d1-8cfb-4c60-b23d-3e487cf7ab5e.png) with respect to ![](img/f63926bf-8e02-4bae-b9e2-a3aa83f268b9.png)
    becomes:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f24c981-c8f1-4e1c-9630-63a9ce2f4e2e.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Substituting all of the calculated terms into equation *(15)*, we get our final
    equation for gradient of loss ![](img/0780e980-493b-4b07-9019-e9935310071a.png)
    with respect to ![](img/8df7ed9c-b546-4548-86f1-7d99e85b2e19.png) as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00798e74-09a6-4521-a5cd-871cea5e4fd0.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Gradients with respect to input to the hidden layer weight, U
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computing the gradients of the loss function with respect to ![](img/7fe2825b-472c-47f5-af2e-7421c1daf9e8.png)
    is the same as ![](img/470b648e-ccd4-4697-a8a4-7d8fcc9d9551.png), since here also
    we take the sequential derivative of ![](img/4447db7f-708c-451a-a72b-9ac034b5e669.png).
    Similar to ![](img/470b648e-ccd4-4697-a8a4-7d8fcc9d9551.png), to compute the derivative
    of any loss ![](img/24cccd52-50ce-47c3-b94c-9add1dd31f8d.png) with respect to
    ![](img/7fe2825b-472c-47f5-af2e-7421c1daf9e8.png), we need to traverse all the
    way back to ![](img/a50a20c6-c54c-4945-b035-6c067baade2f.png).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The final equation for computing the gradient of the loss with respect to ![](img/7fe2825b-472c-47f5-af2e-7421c1daf9e8.png)
    is given as follows. As you may notice, it is basically the same as the equation
    *(15)*, except that we have the term![](img/b7e741dd-bdee-4447-8e4a-2261b07b5691.png)
    instead of ![](img/2e46c8c1-938f-4aec-9ec6-c1dadc1000da.png) shown as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3897b39f-55b0-40d4-bcba-3987878caefd.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3897b39f-55b0-40d4-bcba-3987878caefd.png)'
- en: We have already seen how to compute to the first two terms in the previous section.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在前一节中看到如何计算前两项。
- en: 'Let''s look at the final term ![](img/a4717c74-afdd-4e8d-a388-c577f10db618.png).
    We know that the hidden state ![](img/c933d30a-7bf8-4843-b820-d778a186bb1d.png)
    is computed as, ![](img/0481fc80-3cae-4136-ac58-7f1739f18461.png). Thus, the derivation
    of ![](img/d6780a9b-59c9-4e73-953f-fa5345662113.png) with respect to ![](img/95d442e9-23d4-4676-8cc2-0f70c1c63856.png)
    becomes:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最终项 ![](img/a4717c74-afdd-4e8d-a388-c577f10db618.png)。我们知道隐藏状态 ![](img/c933d30a-7bf8-4843-b820-d778a186bb1d.png)
    计算如下，![](img/0481fc80-3cae-4136-ac58-7f1739f18461.png)。因此，![](img/d6780a9b-59c9-4e73-953f-fa5345662113.png)
    对 ![](img/95d442e9-23d4-4676-8cc2-0f70c1c63856.png) 的导数推导如下：
- en: '![](img/48384cfd-e122-42ea-8eea-8c1a35f179bf.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48384cfd-e122-42ea-8eea-8c1a35f179bf.png)'
- en: 'So, our final equation for a gradient of the loss ![](img/f8f95855-69c2-490c-acf9-68e5f8f67b4d.png),
    with respect to ![](img/7fe2825b-472c-47f5-af2e-7421c1daf9e8.png), can be written
    as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们对损失 ![](img/f8f95855-69c2-490c-acf9-68e5f8f67b4d.png) 对 ![](img/7fe2825b-472c-47f5-af2e-7421c1daf9e8.png)
    的最终梯度方程可以写成如下形式：
- en: '![](img/54561de8-c032-4693-a547-54bf6bd4dfa4.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54561de8-c032-4693-a547-54bf6bd4dfa4.png)'
- en: Vanishing and exploding gradients problem
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度消失和梯度爆炸问题
- en: We just learned how BPTT works, and we saw how the gradient of loss can be computed
    with respect to all the weights in RNNs. But here, we will encounter a problem
    called the **vanishing and exploding gradients**.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习了BPTT的工作原理，看到了如何计算RNN中所有权重的损失梯度。但在这里，我们将遇到一个称为**梯度消失和梯度爆炸**的问题。
- en: While computing the derivatives of loss with respect to ![](img/817f9d6a-c803-4d12-90b6-1b84f825008d.png)
    and ![](img/8a942335-a0e7-4678-a66f-d90787932787.png), we saw that we have to
    traverse all the way back to the first hidden state, as each hidden state at a
    time ![](img/5bb76da6-1ea3-4e61-b400-74a19d5a493c.png) is dependent on its previous
    hidden state at a time ![](img/1a3b3b0a-b675-4a09-9c42-713da17889be.png).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失对 ![](img/817f9d6a-c803-4d12-90b6-1b84f825008d.png) 和 ![](img/8a942335-a0e7-4678-a66f-d90787932787.png)
    的导数时，我们看到我们必须遍历直到第一个隐藏状态，因为每个隐藏状态 ![](img/5bb76da6-1ea3-4e61-b400-74a19d5a493c.png)
    都依赖于其前一个隐藏状态 ![](img/1a3b3b0a-b675-4a09-9c42-713da17889be.png)。
- en: 'For instance, the gradient of loss ![](img/81827407-78c5-447c-b5cc-b1617ffb2f48.png)
    with respect to ![](img/6ec3d4eb-b572-43d2-89ae-4922e8af13d5.png) is given as:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，损失 ![](img/81827407-78c5-447c-b5cc-b1617ffb2f48.png) 对 ![](img/6ec3d4eb-b572-43d2-89ae-4922e8af13d5.png)
    的梯度给出如下：
- en: '![](img/4b8e1b81-6963-40d7-b578-e8cacca890a8.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b8e1b81-6963-40d7-b578-e8cacca890a8.png)'
- en: If you look at the term ![](img/9994e50e-4aaf-4b82-9c28-f5b280763ad6.png) from
    the preceding equation, we can't calculate the derivative
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下前述方程中的项 ![](img/9994e50e-4aaf-4b82-9c28-f5b280763ad6.png)，我们无法计算导数
- en: of ![](img/ad5d13a0-b7c2-4735-9d11-82480eee7d92.png) with respect to ![](img/584bca7b-31c7-409e-8dc8-b144d9a6ce32.png)
    directly. As we know, ![](img/4f6cbf48-6e6c-4064-9b2e-452da085e021.png) is a function
    that is dependent on ![](img/6fff19d9-7f93-47d6-a614-3fbb31ae214d.png) and ![](img/584bca7b-31c7-409e-8dc8-b144d9a6ce32.png).
    So, we need to calculate the derivative with respect to ![](img/6fff19d9-7f93-47d6-a614-3fbb31ae214d.png),
    as well. Even ![](img/9e192206-167c-46d1-8f9e-c5b227d6acfc.png) is a function
    that is dependent on ![](img/697ec559-c85c-4034-a304-0e74b1756b6c.png) and ![](img/584bca7b-31c7-409e-8dc8-b144d9a6ce32.png).
    Thus, we need to calculate the derivative with respect to ![](img/eb6488d9-3a99-4b4e-ac4b-06abb1408d52.png),
    as well.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 ![](img/ad5d13a0-b7c2-4735-9d11-82480eee7d92.png) 对 ![](img/584bca7b-31c7-409e-8dc8-b144d9a6ce32.png)
    的直接推导。正如我们所知，![](img/4f6cbf48-6e6c-4064-9b2e-452da085e021.png) 是一个依赖于 ![](img/6fff19d9-7f93-47d6-a614-3fbb31ae214d.png)
    和 ![](img/584bca7b-31c7-409e-8dc8-b144d9a6ce32.png) 的函数。因此，我们也需要计算对 ![](img/6fff19d9-7f93-47d6-a614-3fbb31ae214d.png)
    的导数。即使 ![](img/9e192206-167c-46d1-8f9e-c5b227d6acfc.png) 也是一个依赖于 ![](img/697ec559-c85c-4034-a304-0e74b1756b6c.png)
    和 ![](img/584bca7b-31c7-409e-8dc8-b144d9a6ce32.png) 的函数。因此，我们还需要计算对 ![](img/eb6488d9-3a99-4b4e-ac4b-06abb1408d52.png)
    的导数。
- en: 'As shown in the following figure, to compute the derivative of ![](img/d1d54dae-34a6-47a4-b0a5-6cafa3ea4f7c.png),
    we need to go all the way back to the initial hidden state ![](img/f3cb32d4-6c46-4f3b-ab25-5639d48617d1.png),
    as each hidden state is dependent on its previous hidden state:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，为了计算 ![](img/d1d54dae-34a6-47a4-b0a5-6cafa3ea4f7c.png) 的导数，我们需要一直追溯到初始隐藏状态
    ![](img/f3cb32d4-6c46-4f3b-ab25-5639d48617d1.png)，因为每个隐藏状态都依赖于其前一个隐藏状态：
- en: '![](img/a4b97dab-8a53-4e2e-b775-ac6d0b583384.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4b97dab-8a53-4e2e-b775-ac6d0b583384.png)'
- en: So, to compute any loss ![](img/1c9c6492-be3a-4813-a05d-ec12582130d3.png), we
    need to traverse all the way back to the initial hidden
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了计算任何损失 ![](img/1c9c6492-be3a-4813-a05d-ec12582130d3.png)，我们需要一直回溯到初始隐藏状态。
- en: 'state ![](img/931e2725-bc7b-49f0-a943-42cb9c89b617.png), as each hidden state
    is dependent on its previous hidden state. Suppose that we have a deep recurrent
    network with 50 layers. To compute the loss ![](img/a23cf5fb-48d2-471b-aca5-766399059c5d.png),
    we need to traverse all the way back to ![](img/931e2725-bc7b-49f0-a943-42cb9c89b617.png),
    as shown in the following figure:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 ![](img/931e2725-bc7b-49f0-a943-42cb9c89b617.png)，因为每个隐藏状态都依赖于其前一个隐藏状态。假设我们有一个具有
    50 层的深度递归网络。为了计算损失 ![](img/a23cf5fb-48d2-471b-aca5-766399059c5d.png)，我们需要一直回溯到
    ![](img/931e2725-bc7b-49f0-a943-42cb9c89b617.png)，如下图所示：
- en: '![](img/4dcd0cd0-a0f7-4f67-b283-83cb90d68d5e.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dcd0cd0-a0f7-4f67-b283-83cb90d68d5e.png)'
- en: So, what is the problem here, exactly? While backpropagating towards the initial
    hidden state, we lose information, and the RNN will not backpropagate perfectly.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，问题究竟出在哪里？在向初始隐藏状态反向传播时，我们丢失了信息，RNN 将不能完美地反向传播。
- en: Remember ![](img/d913a9e9-5fee-4e6e-8139-cb8c1e5ce0c3.png)? Every time we move
    backward, we compute the derivative of ![](img/33120d82-b5ac-462f-8bae-9e599cb7aba0.png).
    A derivative of tanh is bounded to 1\. We know that any two values between 0 and
    1, when multiplied with each other will give us a smaller number. We usually initialize
    the weights of the network to a small number. Thus, when we multiply the derivatives
    and weights while backpropagating, we are essentially multiplying smaller numbers.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 记得 ![](img/d913a9e9-5fee-4e6e-8139-cb8c1e5ce0c3.png) 吗？每次向后移动时，我们计算 ![](img/33120d82-b5ac-462f-8bae-9e599cb7aba0.png)
    的导数。tanh 的导数被限制在 1\. 我们知道，当两个介于 0 和 1 之间的值相乘时，结果将会更小。我们通常将网络的权重初始化为一个小数。因此，当我们在反向传播时乘以导数和权重时，实质上是在乘以较小的数。
- en: So, when we multiply smaller numbers at every step while moving backward, our
    gradient becomes infinitesimally small and leads to a number that the computer
    can't handle; this is called the **vanishing gradient problem.**
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在向后移动时，每一步乘以较小的数，我们的梯度变得无限小，导致计算机无法处理的数值；这被称为**梯度消失问题**。
- en: 'Recall the equation of gradient of the loss with respect to the ![](img/ced572d2-9278-4cab-a2eb-b6cdb02ea31e.png)
    that we saw in the *Gradients with respect to hidden to hidden layer weights,
    W* section:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在 *关于隐藏层到隐藏层权重 W 的梯度* 部分看到的关于损失的梯度方程式：
- en: '![](img/cf4137b0-4a11-4b38-ab29-33b702dced2a.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf4137b0-4a11-4b38-ab29-33b702dced2a.png)'
- en: As you can observe, we are multiplying the weights and derivative of the tanh
    function at every time step. Repeated multiplication of these two leads to a small
    number and causes the vanishing gradients problem.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们在每个时间步长上乘以权重和tanh函数的导数。这两者的重复乘法会导致一个很小的数，从而引起梯度消失问题。
- en: The vanishing gradients problem occurs not only in RNN but also in other deep
    networks where we use sigmoid or tanh as the activation function. So, to overcome
    this, we can use ReLU as an activation function instead of tanh.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题不仅出现在 RNN 中，还出现在其他使用 sigmoid 或 tanh 作为激活函数的深层网络中。因此，为了克服这个问题，我们可以使用 ReLU
    作为激活函数，而不是 tanh。
- en: However, we have a variant of the RNN called the **long short-term memory**
    (**LSTM**) network, which can solve the vanishing gradient problem effectively.
    We will look at how it works in [Chapter 5](c8326380-001a-4ece-8a14-b0a1ea0010b5.xhtml),
    *Improvements to the RNN*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们有一种称为**长短期记忆**（**LSTM**）网络的 RNN 变体，它可以有效地解决梯度消失问题。我们将在第五章 *RNN 的改进* 中看看它是如何工作的。
- en: Similarly, when we initialize the weights of the network to a very large number,
    the gradients will become very large at every step. While backpropagating, we
    multiply a large number together at every time step, and it leads to infinity.
    This is called the **exploding gradient problem.**
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，当我们将网络的权重初始化为非常大的数时，在每一步中梯度会变得非常大。在反向传播时，我们在每个时间步长上乘以一个大数，导致梯度爆炸。这被称为**梯度爆炸问题**。
- en: Gradient clipping
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: We can use gradient clipping to bypass the exploding gradient problem. In this
    method, we normalize the gradients according to a vector norm (say, *L2*) and
    clip the gradient value to a certain range. For instance, if we set the threshold
    as 0.7, then we keep the gradients in the -0.7 to +0.7 range. If the gradient
    value exceeds -0.7, then we change it to -0.7, and similarly, if it exceeds 0.7,
    then we change it to +0.7.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用梯度裁剪来避免梯度爆炸问题。在这种方法中，我们根据向量范数（比如 *L2* 范数）来归一化梯度，并将梯度值裁剪到某个范围内。例如，如果我们将阈值设为
    0.7，那么我们保持梯度在 -0.7 到 +0.7 的范围内。如果梯度值超过 -0.7，我们将其更改为 -0.7；同样地，如果超过 0.7，我们将其更改为
    +0.7。
- en: 'Let''s assume ![](img/46772caf-a135-4853-b011-cbfe4346843c.png) is the gradient
    of loss L with respect to W:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/46772caf-a135-4853-b011-cbfe4346843c.png) 是损失函数 L 关于 W 的梯度：
- en: '![](img/70ac4730-be3f-48b6-829c-d4cc6ac84a0e.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70ac4730-be3f-48b6-829c-d4cc6ac84a0e.png)'
- en: 'First, we normalize the gradients using the L2 norm, that is, ![](img/7201c6b6-7ce4-4f30-af35-ee6ea0be0d0f.png).
    If the normalized gradient exceeds the defined threshold, we update the gradient,
    as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 L2 范数对梯度进行归一化，即 ![](img/7201c6b6-7ce4-4f30-af35-ee6ea0be0d0f.png)。如果归一化的梯度超过了定义的阈值，我们更新梯度如下：
- en: '![](img/0cde0a7c-4f3b-45ec-ad0a-ab169da0b938.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cde0a7c-4f3b-45ec-ad0a-ab169da0b938.png)'
- en: Generating song lyrics using RNNs
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 生成歌词
- en: We have learned enough about RNNs; now, we will look at how to generate song
    lyrics using RNNs. To do this, we simply build a character-level RNN, meaning
    that on every time step, we predict a new character.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了关于 RNN 的足够知识；现在，让我们看看如何使用 RNN 生成歌词。为此，我们简单地构建一个字符级 RNN，也就是说，在每个时间步，我们预测一个新字符。
- en: Let's consider a small sentence, *What a beautiful d*.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个小句子，*What a beautiful d*。
- en: At the first time step, the RNN predicts a new character as *a.* The sentence
    will be updated to, *What a beautiful* *d**a**.*
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个时间步，RNN 预测一个新字符 *a*。句子将更新为 *What a beautiful* *d**a**.*。
- en: At the next time step, it predicts a new character as *y*, and the sentence
    becomes, *What a beautiful da**y**.*
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个时间步，它预测一个新字符 *y*，句子变成了 *What a beautiful da**y**.*。
- en: In this manner, we predict a new character at each time step and generate a
    song. Instead of predicting a new character every time, we can also predict a
    new word every time, which is called **word level RNN**. For simplicity, let's
    start with a character level RNN.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们每个时间步预测一个新字符并生成一首歌曲。除了每次预测一个新字符外，我们还可以每次预测一个新单词，这称为**词级 RNN**。为简单起见，让我们从字符级
    RNN 开始。
- en: But how does RNN predicts a new character on each time step? Let's suppose at
    a time step t=0, we feed an input character say *x*. Now the RNN predicts the
    next character based on the given input character *x*. To predict the next character,
    it predicts the probability of all the characters in our vocabulary to be the
    next character. Once we have this probability distribution we randomly select
    the next character based on this probability. Confusing? Let us better understand
    this with an example.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，RNN 如何在每个时间步预测一个新字符呢？假设在时间步 t=0 时，我们输入一个字符 *x*。现在 RNN 根据给定的输入字符 *x* 预测下一个字符。为了预测下一个字符，它会预测我们词汇表中所有字符成为下一个字符的概率。一旦得到这个概率分布，我们根据这个概率随机选择下一个字符。有点糊涂吗？让我们通过一个例子更好地理解这个过程。
- en: 'For instance, as shown in the following figure, let''s suppose that our vocabulary
    contains four characters *L, O, V,* and *E*; when we feed the character *L* as
    an input, RNN computes the probability of all the words in the vocabulary to be
    the next character:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如下图所示，假设我们的词汇表包含四个字符 *L, O, V,* 和 *E*；当我们将字符 *L* 作为输入时，RNN 计算词汇表中所有单词成为下一个字符的概率：
- en: '![](img/79adc628-079c-474e-b687-af043cadc304.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79adc628-079c-474e-b687-af043cadc304.png)'
- en: So, we have the probabilities as **[0.0, 0.9, 0.0, 0.1]**, corresponding to
    the characters in the vocabulary *[L,O,V,E].* With this probability distribution,
    we select *O* as the next character 90% of the time, and *E* as the next character
    10% of the time. Predicting the next character by sampling from this probability
    distribution adds some randomness to the output.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到概率 **[0.0, 0.9, 0.0, 0.1]**，对应词汇表中的字符 *[L,O,V,E]*。通过从这个概率分布中抽样来预测下一个字符，给输出增加了一些随机性。
- en: 'On the next time step, we feed the predicted character from the previous time
    step and the previous hidden state as an input to predict the next character,
    as shown in the following figure:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个时间步上，我们将上一时间步预测的字符和先前的隐藏状态作为输入，预测下一个字符，如下图所示：
- en: '![](img/d44f75dd-af2d-4754-8992-cc3bb20e1793.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d44f75dd-af2d-4754-8992-cc3bb20e1793.png)'
- en: 'So, on each time step, we feed the predicted character from the previous time
    step and the previous hidden state as input and predict the next character shown
    as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每个时间步上，我们将上一时间步预测的字符和先前的隐藏状态作为输入，并预测下一个字符，如下所示：
- en: '![](img/2741fb9b-44f2-4e0f-a7b6-07359373bd1f.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2741fb9b-44f2-4e0f-a7b6-07359373bd1f.png)'
- en: As you can see in the preceding figure, at time step *t=2*, *V* is passed as
    an input, and it predicts the next character as *E*. But this does not mean that
    every time character *V* is sent as an input it should always return *E* as output.
    Since we are passing input along with the previous hidden state, the RNN has the
    memory of all the characters it has seen so far.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的图中所见，在时间步 *t=2*，*V* 作为输入传递，并预测下一个字符为 *E*。但这并不意味着每次将字符 *V* 作为输入发送时都应始终返回
    *E* 作为输出。由于我们将输入与先前的隐藏状态一起传递，RNN 记住了到目前为止看到的所有字符。
- en: So, the previous hidden state captures the essence of the previous input characters,
    which are *L* and *O*. Now, with this previous hidden state and the input *V**,*
    the RNN predicts the next character as *E*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，先前的隐藏状态捕捉了前面输入字符的精髓，即 *L* 和 *O*。现在，使用此前的隐藏状态和输入 *V*，RNN 预测下一个字符为 *E*。
- en: Implementing in TensorFlow
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中实现
- en: Now, we will look at how to build the RNN model in TensorFlow to generate song
    lyrics. The dataset and also the complete code used in this section with step
    by step explanation is available on GitHub at [http://bit.ly/2QJttyp](http://bit.ly/2QJttyp).
    After downloading, unzip the archive, and place the `songdata.csv` in the `data`
    folder.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看如何在 TensorFlow 中构建 RNN 模型来生成歌词。该数据集以及本节中使用的完整代码和逐步说明可以在 GitHub 上的 [http://bit.ly/2QJttyp](http://bit.ly/2QJttyp)
    获取。下载后，解压缩档案，并将 `songdata.csv` 放在 `data` 文件夹中。
- en: 'Import the required libraries:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE6]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Data preparation
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'Read the downloaded input dataset:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 读取下载的输入数据集：
- en: '[PRE7]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s see what we have in our dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的数据集中有什么：
- en: '[PRE8]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code generates the following output:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成如下输出：
- en: '![](img/b141430d-703a-4651-b047-85272150672d.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b141430d-703a-4651-b047-85272150672d.png)'
- en: 'Our dataset consists of about 57,650 song lyrics:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含约 57,650 首歌曲：
- en: '[PRE9]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have song lyrics from about `643` artists:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有约 `643` 位艺术家的歌词：
- en: '[PRE10]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The number of songs from each artist is shown as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 每位艺术家的歌曲数量如下所示：
- en: '[PRE11]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'On average, we have about `89` songs from each artist:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 平均每位艺术家有约 `89` 首歌曲：
- en: '[PRE12]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have song lyrics in the column `text`, so we combine all the rows of that
    column and save it as a `text` in a variable called `data`, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `text` 列中有歌词，因此我们将该列的所有行组合起来，并将其保存为名为 `data` 的变量中的 `text`，如下所示：
- en: '[PRE13]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s see a few lines of a song:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一首歌的几行：
- en: '[PRE14]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Since we are building a char-level RNN, we will store all the unique characters
    in our dataset into a variable called `chars`; this is basically our vocabulary:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在构建字符级 RNN，我们将数据集中所有唯一字符存储在名为 `chars` 的变量中；这基本上就是我们的词汇表：
- en: '[PRE15]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Store the vocabulary size in a variable called `vocab_size`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 将词汇表大小存储在名为 `vocab_size` 的变量中：
- en: '[PRE16]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Since the neural networks only accept the input in numbers, we need to convert
    all the characters in the vocabulary to a number.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络只接受数字输入，因此我们需要将词汇表中的所有字符转换为数字。
- en: 'We map all the characters in the vocabulary to their corresponding index that
    forms a unique number. We define a `char_to_ix` dictionary, which has a mapping
    of all the characters to their index. To get the index by a character, we also
    define the `ix_to_char` dictionary, which has a mapping of all the indices to
    their respective characters:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将词汇表中的所有字符映射到它们的对应索引，形成一个唯一的数字。我们定义了一个 `char_to_ix` 字典，其中包含所有字符到它们索引的映射。为了通过字符获取索引，我们还定义了
    `ix_to_char` 字典，其中包含所有索引到它们相应字符的映射：
- en: '[PRE17]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As you can see in the following code snippet, the character `''s''` is mapped
    to an index `68` in the `char_to_ix` dictionary:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在下面的代码片段中所见，字符 `'s'` 在 `char_to_ix` 字典中映射到索引 `68`：
- en: '[PRE18]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Similarly, if we give `68` as an input to the `ix_to_char`, then we get the
    corresponding character, which is `''s''`:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果我们将 `68` 作为输入给 `ix_to_char`，那么我们得到相应的字符，即 `'s'`：
- en: '[PRE19]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once we obtain the character to integer mapping, we use one-hot encoding to
    represent the input and output in vector form. A **one-hot encoded vector** is
    basically a vector full of 0s, except, 1 at a position corresponding to a character
    index.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得字符到整数的映射，我们使用独热编码将输入和输出表示为向量形式。**独热编码向量** 基本上是一个全为 0 的向量，除了对应字符索引位置为 1。
- en: 'For example, let''s suppose that the `vocabSize` is `7`, and the character
    *z* is in the fourth position in the vocabulary. Then, the one-hot encoded representation
    for the character *z* can be represented as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 `vocabSize` 是 `7`，而字符 *z* 在词汇表中的第四个位置。那么，字符 *z* 的独热编码表示如下所示：
- en: '[PRE20]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, we have a 1 at the corresponding index of the character, and
    the rest of the values are 0s. This is how we convert each character into a one-hot
    encoded vector.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在对应字符的索引位置有一个 1，其余值为 0。这就是我们将每个字符转换为独热编码向量的方式。
- en: 'In the following code, we define a function called `one_hot_encoder`, which
    will return the one-hot encoded vectors, given an index of the character:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们定义了一个名为 `one_hot_encoder` 的函数，该函数将根据字符的索引返回一个独热编码向量：
- en: '[PRE21]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Defining the network parameters
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义网络参数
- en: 'Next, we define all the network parameters:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义所有网络参数：
- en: 'Define the number of units in the hidden layer:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义隐藏层中的单元数：
- en: '[PRE22]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the length of the input and output sequence:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入和输出序列的长度：
- en: '[PRE23]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the learning rate for gradient descent:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义梯度下降的学习率：
- en: '[PRE24]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Set the seed value:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置种子值：
- en: '[PRE25]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Defining placeholders
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义占位符
- en: 'Now, we will define the TensorFlow placeholders:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义 TensorFlow 的占位符：
- en: 'The `placeholders` for the input and output is defined as:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入和输出的 `placeholders` 定义如下：
- en: '[PRE26]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the `placeholder` for the initial hidden state:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义初始隐藏状态的 `placeholder`：
- en: '[PRE27]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define an `initializer` for initializing the weights of the RNN:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于初始化 RNN 权重的 `initializer`：
- en: '[PRE28]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Defining forward propagation
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义前向传播
- en: 'Let''s define the forward propagation involved in the RNN, which is mathematically
    given as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义涉及 RNN 的前向传播，数学表达如下：
- en: '![](img/523f1fa0-bea0-4810-9134-da3e0f971538.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/523f1fa0-bea0-4810-9134-da3e0f971538.png)'
- en: '![](img/5acff446-3893-46e3-b0fb-fe80338957c8.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5acff446-3893-46e3-b0fb-fe80338957c8.png)'
- en: 'The ![](img/c4dd726d-98a7-4cfa-bf7d-1b82f8d98c21.png) and ![](img/f25b6fa6-b0c4-42e0-b737-89dfec3ec7a2.png)
    are the biases of the hidden and output layers, respectively. For simplicity,
    we haven''t added them to our equations in the previous sections. Forward propagation
    can be implemented as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/c4dd726d-98a7-4cfa-bf7d-1b82f8d98c21.png) 和 ![](img/f25b6fa6-b0c4-42e0-b737-89dfec3ec7a2.png)
    是隐藏层和输出层的偏置项，简单起见，在前面的方程中我们没有添加它们。前向传播可以实现如下：'
- en: '[PRE29]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Apply `softmax` on the output and get the probabilities:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对输出应用 `softmax` 并获取概率：
- en: '[PRE30]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Compute the cross-entropy loss:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 计算交叉熵损失：
- en: '[PRE31]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Store the final hidden state of the RNN in `hprev`. We use this final hidden
    state for making predictions:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 将 RNN 的最终隐藏状态存储在 `hprev` 中。我们使用这个最终隐藏状态来进行预测：
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Defining BPTT
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义 BPTT
- en: 'Now, we will perform the BPTT, with Adam as our optimizer. We will also perform
    gradient clipping to avoid the exploding gradients problem:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将执行 BPTT，使用 Adam 作为优化器。我们还将进行梯度裁剪以避免梯度爆炸问题：
- en: 'Initialize the Adam optimizer:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Adam 优化器：
- en: '[PRE33]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Compute the gradients of the loss with the Adam optimizer:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Adam 优化器计算损失的梯度：
- en: '[PRE34]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Set the threshold for the gradient clipping:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置梯度裁剪的阈值：
- en: '[PRE35]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Clip the gradients that exceed the threshold and bring it to the range:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪超过阈值的梯度并将其带入范围内：
- en: '[PRE36]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Update the gradients with the clipped gradients:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用裁剪后的梯度更新梯度：
- en: '[PRE37]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Start generating songs
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始生成歌曲
- en: 'Start the TensorFlow session and initialize all the variables:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 开始 TensorFlow 会话并初始化所有变量：
- en: '[PRE38]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now, we will look at how to generate the song lyrics using an RNN. What should
    the input and output to the RNN? How does it learn? What is the training data?
    Let's understand this an explanation, along with the code, step by step.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看如何使用 RNN 生成歌词。RNN 的输入和输出应该是什么？它是如何学习的？训练数据是什么？让我们逐步理解这一解释，并附上代码。
- en: We know that in RNNs, the output predicted at a time step ![](img/db822ea3-1b38-46ec-bb99-2663c951b137.png)
    will be sent as the input to the next time step; that is, on every time step,
    we need to feed the predicted character from the previous time step as input.
    So, we prepare our dataset in the same way.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道在 RNN 中，在时间步 ![](img/db822ea3-1b38-46ec-bb99-2663c951b137.png) 预测的输出将作为下一个时间步的输入；也就是说，每个时间步，我们需要将前一个时间步预测的字符作为输入。因此，我们以同样的方式准备我们的数据集。
- en: For instance, look at the following table. Let's suppose that each row is a
    different time step; on a time step ![](img/0249a9fb-0d9b-4b2f-93ff-4b22011a9441.png),
    the RNN predicted a new character, *g*, as the output. This will be sent as the
    input to the next time step, ![](img/8556fd17-b301-4584-8a9a-1d06c3082bc6.png).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，请看下表。假设每一行代表一个不同的时间步；在时间步 ![](img/0249a9fb-0d9b-4b2f-93ff-4b22011a9441.png)，RNN
    预测一个新字符 *g* 作为输出。这将作为下一个时间步 ![](img/8556fd17-b301-4584-8a9a-1d06c3082bc6.png)
    的输入发送。
- en: However, if you notice the input in the time step ![](img/8556fd17-b301-4584-8a9a-1d06c3082bc6.png),
    we removed the first character from the input `o` and added the newly predicted
    character *g* at the end of our sequence. Why are we removing the first character
    from the input? Because we need to maintain the sequence length.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你注意到时间步 ![](img/8556fd17-b301-4584-8a9a-1d06c3082bc6.png) 中的输入，我们从输入 `o`
    中删除了第一个字符，并在序列末尾添加了新预测的字符 *g*。为什么要删除输入中的第一个字符？因为我们需要保持序列长度。
- en: Let's suppose that our sequence length is eight; adding a newly predicted character
    to our sequence increases the sequence length to nine. To avoid this, we remove
    the first character from the input, while adding a newly predicted character from
    the previous time step.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的序列长度是八；将新预测的字符添加到序列中会将序列长度增加到九。为了避免这种情况，我们从输入中移除第一个字符，同时将前一个时间步的新预测字符添加进来。
- en: 'Similarly, in the output data, we also remove the first character on each time
    step, because once it predicts the new character, the sequence length increases.
    To avoid this, we remove the first character from the output on each time step,
    as shown in the following table:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在输出数据中，我们也会在每个时间步删除第一个字符，因为一旦预测了新字符，序列长度就会增加。为了避免这种情况，在每个时间步从输出中删除第一个字符，如下表所示：
- en: '![](img/4a4c6500-b69d-4016-a58b-91988ee76b16.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a4c6500-b69d-4016-a58b-91988ee76b16.png)'
- en: Now, we will look at how we can prepare our input and output sequence similarly
    to the preceding table.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看看如何准备我们的输入和输出序列，类似于前面的表格。
- en: 'Define a variable called `pointer`, which points to the character in our dataset.
    We will set our `pointer` to `0`, which means it points to the first character:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为 `pointer` 的变量，它指向数据集中的字符。我们将 `pointer` 设置为 `0`，这意味着它指向第一个字符：
- en: '[PRE39]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the input data:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入数据：
- en: '[PRE40]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'What does this mean? With the pointer and the sequence length, we slice the
    data. Consider that the `seq_length` is `25` and the pointer is `0`. It will return
    the first 25 characters as input. So, `data[pointer:pointer + seq_length]` returns
    the following output:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这是什么意思？使用指针和序列长度切片数据。假设 `seq_length` 是 `25`，指针是 `0`。它将返回前 25 个字符作为输入。因此，`data[pointer:pointer
    + seq_length]` 返回以下输出：
- en: '[PRE41]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the output, as follows:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输出，如下所示：
- en: '[PRE42]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We slice the output data with one character ahead moved from input data. So,
    `data[pointer + 1:pointer + seq_length + 1]` returns the following:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输出数据切片，从输入数据移动一个字符。因此，`data[pointer + 1:pointer + seq_length + 1]` 返回以下内容：
- en: '[PRE43]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As you can see, we added the next character in the preceding sentence and removed
    the first character. So, on every iteration, we increment the pointer and traverse
    the entire dataset. This is how we obtain the input and output sentence for training
    the RNN.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们在前一句中添加了下一个字符并删除了第一个字符。因此，每次迭代时，我们增加指针并遍历整个数据集。这就是我们为训练 RNN 获取输入和输出句子的方法。
- en: 'As we have learned, an RNN accepts only numbers as input. Once we have sliced
    the input and output sequence, we get the indices of the respective characters,
    using the `char_to_ix` dictionary that we defined:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们学到的那样，RNN 仅接受数字作为输入。一旦我们切片了输入和输出序列，我们使用之前定义的 `char_to_ix` 字典获取相应字符的索引：
- en: '[PRE44]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Convert the indices into one-hot encoded vectors by using the `one_hot_encoder`
    function we defined previously:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前定义的 `one_hot_encoder` 函数将索引转换为 one-hot 编码向量：
- en: '[PRE45]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This `input_vector` and `target_vector` become the input and output for training
    the RNN. Now, Let's start training.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这 `input_vector` 和 `target_vector` 成为训练 RNN 的输入和输出。现在，让我们开始训练。
- en: 'The `hprev_val` variable stores the last hidden state of our trained RNN model
    which we use for making predictions, and we store the loss in a variable called
    `loss_val`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`hprev_val`变量存储了我们训练的RNN模型的最后隐藏状态，我们用它来进行预测，并将损失存储在名为`loss_val`的变量中：'
- en: '[PRE46]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We train the model for *n* iterations. After training, we start making predictions.
    Now, we will look at how to make predictions and generate song lyrics using our
    trained RNN. Set the `sample_length`, that is, the length of the sentence (song)
    we want to generate:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练模型进行*n*次迭代。训练后，我们开始进行预测。现在，我们将看看如何进行预测并使用我们训练的RNN生成歌词。设置`sample_length`，即我们想要生成的句子（歌曲）的长度：
- en: '[PRE47]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Randomly select the starting index of the input sequence:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择输入序列的起始索引：
- en: '[PRE48]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Select the input sentence with the randomly selected index:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 选择具有随机选择索引的输入句子：
- en: '[PRE49]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we know, we need to feed the input as numbers; convert the selected input
    sentence to indices:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，我们需要将输入作为数字进行馈送；将所选输入句子转换为索引：
- en: '[PRE50]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Remember, we stored the last hidden state of the RNN in `hprev_val`. We used
    that for making predictions. We create a new variable called `sample_prev_state_val`
    by copying values from `hprev_val`.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们将RNN的最后隐藏状态存储在`hprev_val`中。我们使用它来进行预测。我们通过从`hprev_val`复制值来创建一个名为`sample_prev_state_val`的新变量。
- en: 'The `sample_prev_state_val` is used as an initial hidden state for making predictions:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample_prev_state_val`用作进行预测的初始隐藏状态：'
- en: '[PRE51]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Initialize the list for storing the predicted output indices:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化用于存储预测输出索引的列表：
- en: '[PRE52]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now, for *t* in range of `sample_length`, we perform the following and generate
    the song for the defined `sample_length`.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于`t`在`sample_length`的范围内，我们执行以下操作并为定义的`sample_length`生成歌曲：
- en: 'Convert the `sampled_input_indices` to the one-hot encoded vectors:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sampled_input_indices`转换为one-hot编码向量：
- en: '[PRE53]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Feed the `sample_input_vector`, and also the hidden state `sample_prev_state_val`,
    as the initial hidden state to the RNN, and get the predictions. We store the
    output probability distribution in `probs_dist`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sample_input_vector`和初始隐藏状态`sample_prev_state_val`馈送给RNN，并获得预测。我们将输出概率分布存储在`probs_dist`中：
- en: '[PRE54]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Randomly select the index of the next character with the probability distribution
    generated by the RNN:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由RNN生成的概率分布随机选择下一个字符的索引：
- en: '[PRE55]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Add this newly predicted index, `ix`, to the `sample_input_indices`, and also
    remove the first index from `sample_input_indices` to maintain the sequence length.
    This will form the input for the next time step:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 将新预测的索引`ix`添加到`sample_input_indices`中，并从`sample_input_indices`中删除第一个索引以保持序列长度。这将形成下一个时间步的输入：
- en: '[PRE56]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Store all the predicted `chars` indices in the `predicted_indices` list:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 存储所有预测的`chars`索引在`predicted_indices`列表中：
- en: '[PRE57]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Convert all the `predicted_indices` to their characters:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有的`predicted_indices`转换为它们对应的字符：
- en: '[PRE58]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Combine all the `predicted_chars` and save it as `text`:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有的`predicted_chars`组合起来，并保存为`text`：
- en: '[PRE59]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Print the predicted text on every 50,000^(th) iteration:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在每50,000^(th)次迭代时打印预测文本：
- en: '[PRE60]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Increment the `pointer` and `iteration`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 增加`pointer`和`iteration`：
- en: '[PRE61]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'On the initial iteration, you can see that the RNN has generated the random
    characters. But at the 50,000^(th) iteration, it has started to generate meaningful
    text:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始迭代中，您可以看到RNN生成了随机字符。但是在第50,000^(th)次迭代中，它开始生成有意义的文本：
- en: '[PRE62]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Different types of RNN architectures
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同类型的RNN架构
- en: Now that we have learned how an RNN works, we will look at a different type
    of RNN architecture that's based on numbers of input and output.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了RNN的工作原理，我们将看看基于输入和输出数量的不同类型的RNN架构。
- en: One-to-one architecture
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对一架构
- en: In a **one-to-one architecture**, a single input is mapped to a single output,
    and the output from the time step *t* is fed as an input to the next time step.
    We have already seen this architecture in the last section for generating songs
    using RNNs.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在**一对一架构**中，单个输入映射到单个输出，时间步*t*的输出作为下一个时间步的输入。我们已经在最后一节中看到了这种架构，用于使用RNN生成歌曲。
- en: For instance, for a text generation task, we take the output generated from
    a current time step and feed it as the input to the next time step to generate
    the next word. This architecture is also widely used in stock market predictions.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在文本生成任务中，我们将当前时间步生成的输出作为下一个时间步的输入，以生成下一个单词。这种架构在股票市场预测中也被广泛使用。
- en: 'The following figure shows the one-to-one RNN architecture. As you can see,
    output predicted at the time step *t* is sent as the input to the next time step:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一对一的RNN架构。如您所见，时间步*t*预测的输出被发送为下一个时间步的输入：
- en: '![](img/aa53eae7-a2f7-4149-9de0-06b7916f6e60.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa53eae7-a2f7-4149-9de0-06b7916f6e60.png)'
- en: One-to-many architecture
  id: totrans-402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对多架构
- en: In a **one-to-many architecture**, a single input is mapped to multiple hidden
    states and multiple output values, which means RNN takes a single input and maps
    it to an output sequence. Although we have a single input value, we share the
    hidden states across time steps to predict the output. Unlike the previous one-to-one
    architecture, here, we only share the previous hidden states across time steps
    and not the previous outputs.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在**一对多架构**中，单个输入映射到多个隐藏状态和多个输出值，这意味着RNN接受单个输入并映射到输出序列。尽管我们只有一个输入值，但我们在时间步之间共享隐藏状态以预测输出。与前述一对一架构不同，这里我们只在时间步之间共享上一个隐藏状态，而不是上一个输出。
- en: One such application of this architecture is image caption generation. We pass
    a single image as an input, and the output is the sequence of words constituting
    a caption of the image.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的一个应用是图像标题生成。我们传递单个图像作为输入，输出是构成图像标题的单词序列。
- en: 'As shown in the following figure, a single image is passed as an input to the
    RNN, and at the first time step, ![](img/5b87e5a4-61c8-4f3f-ad26-380f22e2f150.png),
    the word *Horse* is predicted; on the next time step, ![](img/a5426056-19c6-4b7c-9f69-bdc6fb3e6f5e.png),
    the previous hidden state ![](img/0e5cc4c7-c5be-4ff6-8599-dc2d6ade366e.png) is
    used to predict the next word which is *standing*. Similarly, it continues for
    a sequence of steps and predicts the next word until the caption is generated:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，将单个图像作为输入传递给RNN，在第一个时间步，预测单词*Horse*；在下一个时间步，使用上一个隐藏状态预测下一个单词*standing*。类似地，它持续一系列步骤并预测下一个单词，直到生成标题：
- en: '![](img/e898e507-4926-4708-84b4-3fd31fa19b80.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e898e507-4926-4708-84b4-3fd31fa19b80.png)'
- en: Many-to-one architecture
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多对一架构
- en: A **many-to-one architecture,** as the name suggests, takes a sequence of input
    and maps it to a single output value. One such popular example of a many-to-one
    architecture is **sentiment classification**. A sentence is a sequence of words,
    so on each time step, we pass each word as input and predict the output at the
    final time step.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**多对一架构**，顾名思义，接受一个输入序列并将其映射到单个输出值。多对一架构的一个流行示例是**情感分类**。一句话是一个单词序列，因此在每个时间步，我们将每个单词作为输入传递，并在最终时间步预测输出。
- en: 'Let''s suppose that we have a sentence: *Paris is a beautiful city*. As shown
    in the following figure, at each time step, a single word is passed as an input,
    along with the previous hidden state; and, at the final time step, it predicts
    the sentiment of the sentence:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个句子：*Paris is a beautiful city.* 如下图所示，在每个时间步，一个单词作为输入传递，并且在最终时间步预测句子的情感：
- en: '![](img/922ee723-6f74-4265-b9fb-8837d4c7ee16.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/922ee723-6f74-4265-b9fb-8837d4c7ee16.png)'
- en: Many-to-many architecture
  id: totrans-411
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**多对多架构**'
- en: In **many-to-many architectures**, we map a sequence of input of arbitrary length
    to a sequence of output of arbitrary length. This architecture has been used in
    various applications. Some of the popular applications of many-to-many architectures
    include language translation, conversational bots, and audio generation.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在**多对多架构**中，我们将任意长度的输入序列映射到任意长度的输出序列。这种架构已被用于各种应用中。多对多架构的一些流行应用包括语言翻译、对话机器人和音频生成。
- en: 'Let''s suppose that we are converting a sentence from English to French. Consider
    our input sentence: *W**hat are you doing?* It would be mapped to, *Que faites
    vous* as shown in the following figure:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在将一句英语句子转换成法语。考虑我们的输入句子：*What are you doing?* 如下图所示，它将映射为*Que faites vous*：
- en: '![](img/52a6c838-3b48-48e6-a775-46af909f8fb1.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52a6c838-3b48-48e6-a775-46af909f8fb1.png)'
- en: Summary
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by covering what an RNN is and how an RNN differs
    from a feedforward network. We learned that an RNN is a special type of neural
    network that is widely applied over sequential data; it predicts output based
    on not only the current input but also on the previous hidden state, which acts
    as the memory and stores the sequence of information that the network has seen
    so far.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了什么是RNN，以及RNN与前馈网络的区别。我们了解到RNN是一种特殊类型的神经网络，广泛应用于序列数据；它根据当前输入和先前的隐藏状态预测输出，隐藏状态充当记忆，存储到目前为止网络所见到的信息序列。
- en: We learned how forward propagation works in RNNs, and then we explored a detailed
    derivation of the BPTT algorithm, which is used for training RNN. Then, we explored
    RNNs by implementing them in TensorFlow to generate song lyrics. At the end of
    the chapter, we learned about the different architectures of RNNs, such as one-to-one,
    one-to-many, many-to-one, and many-to-many, which are used for various applications.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了RNN中的前向传播工作原理，然后详细推导了用于训练RNN的BPTT算法。接着，我们通过在TensorFlow中实现RNN来生成歌词。在本章末尾，我们了解了RNN的不同架构，如一对一、一对多、多对一和多对多，它们被用于各种应用中。
- en: In the next chapter, we will learn about the LSTM cell, which solves the vanishing
    gradient problem in RNNs. We will also learn about different variants of RNNs.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习关于LSTM单元的内容，它解决了RNN中的梯度消失问题。我们还将学习不同变体的RNN。
- en: Questions
  id: totrans-419
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Try answering the following questions:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试回答以下问题：
- en: What is the difference between an RNN and a feedforward neural network?
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN与前馈神经网络有什么区别？
- en: How is the hidden state computed in an RNN?
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN中的隐藏状态是如何计算的？
- en: What is the use of a recurrent network?
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归网络有什么用途？
- en: How does the vanishing gradient problem occur?
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度消失问题是如何发生的？
- en: What is the exploding gradient problem?
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是爆炸梯度问题？
- en: How gradient clipping mitigates the exploding gradient problem?
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度裁剪如何缓解爆炸梯度问题？
- en: What are the different types of RNN architectures?
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同类型的RNN架构有哪些？
- en: Further reading
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Refer to the following links to learn more about RNNs:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下链接了解更多关于RNN的内容：
- en: '*Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory
    (LSTM) Network*, by Alex Sherstinsky, [https://arxiv.org/pdf/1808.03314.pdf](https://arxiv.org/pdf/1808.03314.pdf)'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*循环神经网络（RNN）和长短期记忆（LSTM）网络基础*，作者Alex Sherstinsky，[https://arxiv.org/pdf/1808.03314.pdf](https://arxiv.org/pdf/1808.03314.pdf)'
- en: Handwriting generation by RNN with TensorFlow, based on *Generating Sequences
    With Recurrent Neural Networks* by Alex Graves, [https://github.com/snowkylin/rnn-handwriting-generation](https://github.com/snowkylin/rnn-handwriting-generation)
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用TensorFlow进行手写生成的RNN，基于Alex Graves的*用循环神经网络生成序列*，[https://github.com/snowkylin/rnn-handwriting-generation](https://github.com/snowkylin/rnn-handwriting-generation)
