["```py\n#define IOT_CONFIG_WIFI_SSID \"IoT_Net\"\n#define IOT_CONFIG_WIFI_PASSWORD \"password1234\"\n```", "```py\n#define DEVICE_CONNECTION_STRING \"HostName=myhub.azure-devices.net;DeviceId=somerandomname;SharedAccessKey=TWnLEcXf/sxZoacZry0akx7knPOa2gSojrkZ7oyafx0=\"\n```", "```py\n#include \"DHT.h\"\n```", "```py\n#define DHTPIN 27\n#define DHTTYPE DHT11\n```", "```py\nDHT dht(DHTPIN, DHTTYPE);\n```", "```py\nvoid setup()\n{\n    Serial.begin(115200);\n    Serial.println(\"DHT11 sensor!\");\n    dht.begin();\n}\n```", "```py\nvoid loop() {\n    float h = dht.readHumidity();\n    float t = dht.readTemperature();\n    printResults(h,t);\n    delay(2000);\n}\n```", "```py\nvoid printResults(float h,float t)\n{\n    if (isnan(h) || isnan(t)) {\n    Serial.println(\"Failed to read from DHT sensor!\");\n    return;\n}\nSerial.print(\"Humidity: \");\nSerial.print(h);\nSerial.print(\" %\\t\");\nSerial.print(\"Temperature: \");\nSerial.print(t);\nSerial.println(\" *C \");\n}\n```", "```py\nimport koalas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n```", "```py\ndf = spark.sql(\"select * from ChemicalSensor where class <> 'banana'\")\npdf = df.toPandas()\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\npdf.rename(columns = {'class':'classification'}, inplace = True) \nX = pdf\ny = pdf['classification']\n\nlabel_encoder = LabelEncoder()\n\ninteger_encoded = \\\nlabel_encoder.fit_transform(pdf['classification'])\nonehot_encoder = OneHotEncoder(sparse=False)\n\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n\nfeature_cols = ['r1', 'r2', 'r4', 'r5', 'r6','r7', 'r8', 'temp',\n                'humidity', 't0', 'td']\nX = pdf[feature_cols]\ny = onehot_encoded\n\nX_train, X_test, y_train, y_test = \\\ntrain_test_split(X, y, test_size=0.3, random_state=40)\n```", "```py\nmodel_params = {\n    'n_estimators': [50, 150, 250],\n    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n    'min_samples_split': [2, 4, 6]\n}\n```", "```py\nrf_model = RandomForestClassifier(random_state=1)\n\n```", "```py\nclf = GridSearchCV(rf_model, model_params, cv=5)\n```", "```py\nmodel = clf.fit(X_train,y_train)\n```", "```py\ny_pred = clf.predict(X_test)\n```", "```py\nfrom pprint import pprint\npprint(model.best_estimator_.get_params())\n```", "```py\ndocker pull mcr.microsoft.com/mssql/server:2017-latest \n```", "```py\ndocker run -e 'ACCEPT_EULA=Y' -e 'MSSQL_AGENT_ENABLED=true' \\\n-e 'MSSQL_PID=Standard' -e 'SA_PASSWORD=Password!' \\ \n-p 1433:1433 --name sqlserver_1 \\ \n-d mcr.microsoft.com/mssql/server:2017-latest \n```", "```py\nCREATE DATABASE MLTracking\nGO\nUSE MLTracking\nGO\nCREATE TABLE Product( \n  productid INTEGER IDENTITY(1,1) NOT NULL PRIMARY KEY, \n  productName VARCHAR(255) NOT NULL, \n  BeginLife Datetime NOT NULL, \nEndLife Datetime NULL, \n ); \nGO\nCREATE TABLE RUL( \n  RULid INTEGER IDENTITY(1,1) NOT NULL PRIMARY KEY, \nProductId int,\nTotalRULDays int, \nDateCalculated datetime not null \n) \nGO\n```", "```py\nimport pyodbc \n```", "```py\n\nconn = pyodbc.connect('Driver={SQL Server};'\n 'Server=localhost;'\n 'Database=MLTracking;'\n 'uid=sa;'\n 'pwd=Password!;')\n```", "```py\ncursor = conn.cursor()\n```", "```py\ncursor.execute('''\n  INSERT INTO MLTracking.dbo.Product (Product,BeginLife)\n  VALUES\n  ('Smoke Detector 9000',GETDATE()),\n  ''')\nconn.commit()\n```", "```py\ncursor.execute('''\n  INSERT INTO MLTracking.dbo.RUL (ProductId,TotalRULDays,DateCalculated )\n  VALUES\n  (1,478,GETDATE()),\n  ''')\nconn.commit()\n```", "```py\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.compose import ColumnTransformer\n```", "```py\ntrain = spark.sql(\"select * from engine\").toPandas()\ntrain.drop(columns=\"label\" , inplace=True)\ntest = spark.sql(\"select * from engine_test2\").toPandas()\n```", "```py\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', \n                              fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n```", "```py\nnumeric_features = \\\ntrain.select_dtypes(include=['int64', 'float64']).columns\ncategorical_features = \\\ntrain.select_dtypes(include=['object']).drop(['cycle'], \n                                             axis=1).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\n```", "```py\nrf = Pipeline(steps=[('preprocessor', preprocessor),\n                     ('classifier', RandomForestClassifier())])\n```", "```py\nrf.fit(X_train, y_train)\n```", "```py\ny_pred = rf.predict(X_test)\n```", "```py\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nclassifiers = [\n    KNeighborsClassifier(3),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\nfor classifier in classifiers:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', classifier)])\n    pipe.fit(X_train, y_train) \n    print(classifier)\n    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n```", "```py\nfrom confluent_kafka import Producer\nfrom datetime import datetime as dt\nimport json\nimport time\n\nproducer = Producer({\n    'bootstrap.servers': \"pkc-lgwgm.eastus2.azure.confluent.cloud:9092\",\n    'security.protocol': 'SASL_SSL',\n    'sasl.mechanism': \"PLAIN\",\n    \"sasl.username\": \"\",\n    \"sasl.password\": \"\",\n    'auto.offset.reset': 'earliest'\n})\n\ndata = json.dumps({'Record_ID':1,'Temperature':'100','Vibration':120,\n                   'age':1000, 'time':time.time()})\nproducer.send('TurboFan', data)\n```", "```py\nfrom pyspark.sql.types import StringType\nimport json \nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n df.readStream.format(\"kafka\") \n.option(\"kafka.bootstrap.servers\", \"...azure.confluent.cloud:9092\") \n.option(\"subscribe\", \"TurboFan\") \n.option(\"startingOffsets\", \"latest\") \n.option(\"kafka.security.protocol\",\"SASL_SSL\") \n.option(\"kafka.sasl.mechanism\", \"PLAIN\") \n.option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Kafka UserName\\\" password=\\\"Kafka Password\\\";\") \n.load() \n.select($\"value\") \n.withColumn(\"Value\", $\"value\".cast(StringType)) \n```", "```py\nval jsDF1 = kafka1.select( get_json_object($\"Value\", \"$.Temperature\").alias(\"Temp\"), \nget_json_object($\"Value\", \"$.Vibration\").alias(\"Vibration\") \n,get_json_object($\"Value\", \"$.age\").alias(\"Age\") \n)\n```", "```py\ndef score(row):\n    d = json.loads(row)\n    p = pd.DataFrame.from_dict(d, orient = \"index\").transpose() \n    pred = model.predict_proba(p.iloc[:,0:10])[0][0]\n    result = {'Record_ID': d['Record_ID'], 'pred': pred }\n    return str(json.dumps(result))\n```", "```py\ndf = df.selectExpr(\"CAST(value AS STRING)\")\nscore_udf = udf(score, StringType()) \ndf = df.select( score_udf(\"value\").alias(\"value\"))\n```", "```py\nfailure_df = df.filter(df.value > 0.9)\n```", "```py\nquery = df.writeStream.format(\"kafka\") \n.option(\"kafka.bootstrap.servers\", \"{external_ip}:9092\") \n.option(\"topic\", \"Turbofan_Failure\") \n.option(\"kafka.security.protocol\",\"SASL_SSL\") \n.option(\"kafka.sasl.mechanism\", \"PLAIN\") \n.option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Kafka UserName\\\" password=\\\"Kafka Password\\\";\") \n .option(\"checkpointLocation\", \"/temp\").start()\n```", "```py\nfrom confluent_kafka import Consumer\n\nconf = {'bootstrap.servers': \"host1:9092,host2:9092\",\n        'group.id': \"foo\",\n        'kafka.security.protocol':'SASL_SSL, \n        'kafka.sasl.mechanism':'PLAIN', \n        'kafka.sasl.jaas.config': 'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"Kafka UserName\\\" password=\\\"Kafka Password\\\";') \n        'auto.offset.reset': 'smallest'}\n\nrunning = True\nconsumer = Consumer(conf)\nconsumer.subscribe('Turbofan_Failure')\n while running:\n    msg = consumer.poll(timeout=1.0)\n    if msg is None: continue\n    msg_process(msg)\n\ndef msg_process(msg):\n    pass\n```", "```py\nCREATE TABLE users (\n     TurboFanNum BIGINT PRIMARY KEY,\n     temperature BIGINT,\n     humidity BIGINT\n   ) WITH (\n     KAFKA_TOPIC = 'weather', \n     VALUE_FORMAT = 'JSON'\n   );\n```", "```py\nCREATE STREAM TurboFan (\n    TurboFanNum BIGINT,\n    HoursLogged BIGINT,\n    VIBRATIONSCORE BIGING\n  ) WITH (\n    KAFKA_TOPIC='TurboFan',\n    VALUE_FORMAT='JSON'\n  );\n```", "```py\nCREATE STREAM TurboFan_Enriched AS\n  SELECT \n     TurnboFan.TurboFanNum, \n     HoursLogged, \n     VIBRATIONSCORE, \n     temperature,\n     humidity \n\n  FROM TurboFan\n    LEFT JOIN Weather ON Weather.TurboFanNum = TurboFan.TurboFanNum\n  EMIT CHANGES;\n```", "```py\nCREATE STREAM TurboFan_ToHot AS\n  SELECT \n     TurnboFan.TurboFanNum, \n     avg(temperature)\n  FROM TurboFan_Enriched\n  WINDOW TUMBLING (SIZE 20 SECONDS)\n  GROUP BY TurboFanNum\n  HAVING avg(temperature) > 100\n  EMIT CHANGES;\n```"]