["```py\n#Fixing the size of the image, reduce it further if you are not using a GPU.\nimsize = 512 \nis_cuda = torch.cuda.is_available()\n\n#Converting image ,making it suitable for training using the VGG model.\n\nprep = transforms.Compose([transforms.Resize(imsize),\n                           transforms.ToTensor(),\n                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n                                                std=[1,1,1]),\n                           transforms.Lambda(lambda x: x.mul_(255)),\n                          ])\n\n#Converting the generated image back to a format which we can visualise. \n\npostpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n                                                std=[1,1,1]),\n                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n                           ])\npostpb = transforms.Compose([transforms.ToPILImage()])\n\n#This method ensures data in the image does not cross the permissible range .\ndef postp(tensor): # to clip results in the range [0,1]\n    t = postpa(tensor)\n    t[t>1] = 1 \n    t[t<0] = 0\n    img = postpb(t)\n    return img\n\n#A utility function to make data loading easier.\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    image = Variable(prep(image))\n    # fake batch dimension required to fit network's input dimensions\n    image = image.unsqueeze(0)\n    return image\n```", "```py\nstyle_img = image_loader(\"Images/vangogh_starry_night.jpg\")\ncontent_img = image_loader(\"Images/Tuebingen_Neckarfront.jpg\")\n```", "```py\nopt_img = Variable(content_img.data.clone(),requires_grad=True)\n```", "```py\n#Creating a pretrained VGG model\nvgg = vgg19(pretrained=True).features\n\n#Freezing the layers as we will not use it for training.\nfor param in vgg.parameters():\n    param.requires_grad = False\n```", "```py\ntarget_layer = dummy_fn(content_img)\nnoise_layer = dummy_fn(noise_img)\ncriterion = nn.MSELoss()\ncontent_loss = criterion(target_layer,noise_layer)\n```", "```py\nclass GramMatrix(nn.Module):\n\n    def forward(self,input):\n        b,c,h,w = input.size()\n        features = input.view(b,c,h*w)\n        gram_matrix = torch.bmm(features,features.transpose(1,2))\n        gram_matrix.div_(h*w)\n        return gram_matrix\n```", "```py\nb,c,h,w = input.size()\n```", "```py\nfeatures = input.view(b,c,h*w)\n```", "```py\ngram_matrix = torch.bmm(features,features.transpose(1,2))\n```", "```py\nclass StyleLoss(nn.Module):\n\n    def forward(self,inputs,targets):\n        out = nn.MSELoss()(GramMatrix()(inputs),targets)\n        return (out)\n```", "```py\nclass LayerActivations():\n    features=[]\n\n    def __init__(self,model,layer_nums):\n\n        self.hooks = []\n        for layer_num in layer_nums:\n            self.hooks.append(model[layer_num].register_forward_hook(self.hook_fn))\n\n    def hook_fn(self,module,input,output):\n        self.features.append(output)\n\n    def remove(self):\n        for hook in self.hooks:\n            hook.remove()\n```", "```py\ndef extract_layers(layers,img,model=None):\n\n    la = LayerActivations(model,layers)\n    #Clearing the cache \n    la.features = []\n    out = model(img)\n    la.remove()\n    return la.features\n```", "```py\ncontent_targets = extract_layers(content_layers,content_img,model=vgg)\nstyle_targets = extract_layers(style_layers,style_img,model=vgg)\n```", "```py\ncontent_targets = [t.detach() for t in content_targets]\nstyle_targets = [GramMatrix()(t).detach() for t in style_targets]\n```", "```py\ntargets = style_targets + content_targets\n```", "```py\nstyle_layers = [1,6,11,20,25]\ncontent_layers = [21]\nloss_layers = style_layers + content_layers\n```", "```py\nstyle_weights = [1e3/n**2 for n in [64,128,256,512,512]]\ncontent_weights = [1e0]\nweights = style_weights + content_weights\n```", "```py\nprint(vgg)\n\n#Results \n\nSequential(\n  (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU(inplace)\n  (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (3): ReLU(inplace)\n  (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  (5): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (6): ReLU(inplace)\n  (7): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (8): ReLU(inplace)\n  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  (10): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (11): ReLU(inplace)\n  (12): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU(inplace)\n  (14): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (15): ReLU(inplace)\n  (16): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (17): ReLU(inplace)\n  (18): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  (19): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU(inplace)\n  (21): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (22): ReLU(inplace)\n  (23): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (24): ReLU(inplace)\n  (25): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (26): ReLU(inplace)\n  (27): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  (28): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (29): ReLU(inplace)\n  (30): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (31): ReLU(inplace)\n  (32): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (33): ReLU(inplace)\n  (34): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (35): ReLU(inplace)\n  (36): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n)\n```", "```py\nloss_fns = [StyleLoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n```", "```py\noptimizer = optim.LBFGS([opt_img]);\n```", "```py\nmax_iter = 500\nshow_iter = 50\nn_iter=[0]\n\nwhile n_iter[0] <= max_iter:\n\n    def closure():\n        optimizer.zero_grad()\n\n        out = extract_layers(loss_layers,opt_img,model=vgg)\n        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n        loss = sum(layer_losses)\n        loss.backward()\n        n_iter[0]+=1\n        #print loss\n        if n_iter[0]%show_iter == (show_iter-1):\n            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.data[0]))\n\n        return loss\n\n    optimizer.step(closure)\n```", "```py\ntransforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n```", "```py\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        output = self.main(input)\n        return output\n\nnetG = Generator()\nnetG.apply(weights_init)\nprint(netG)\n```", "```py\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n```", "```py\nnetG.apply(weights_init)\n```", "```py\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(_netD, self).__init__()\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        output = self.main(input)\n        return output.view(-1, 1).squeeze(1)\n\nnetD = Discriminator()\nnetD.apply(weights_init)\nprint(netD)\n```", "```py\ncriterion = nn.BCELoss()\n\n# setup optimizer\noptimizerD = optim.Adam(netD.parameters(), lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr, betas=(beta1, 0.999))\n```", "```py\noutput = netD(inputv)\nerrD_real = criterion(output, labelv)\nerrD_real.backward()\n```", "```py\nfake = netG(noisev)\noutput = netD(fake.detach())\nerrD_fake = criterion(output, labelv)\nerrD_fake.backward()\noptimizerD.step()\n```", "```py\nnetG.zero_grad()\nlabelv = Variable(label.fill_(real_label)) # fake labels are real for generator cost\noutput = netD(fake)\nerrG = criterion(output, labelv)\nerrG.backward()\noptimizerG.step()\n```", "```py\nfor epoch in range(niter):\n    for i, data in enumerate(dataloader, 0):\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        # train with real\n        netD.zero_grad()\n        real, _ = data\n        batch_size = real.size(0)\n        if torch.cuda.is_available():\n            real = real.cuda()\n        input.resize_as_(real).copy_(real)\n        label.resize_(batch_size).fill_(real_label)\n        inputv = Variable(input)\n        labelv = Variable(label)\n\n        output = netD(inputv)\n        errD_real = criterion(output, labelv)\n        errD_real.backward()\n        D_x = output.data.mean()\n\n        # train with fake\n        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n        noisev = Variable(noise)\n        fake = netG(noisev)\n        labelv = Variable(label.fill_(fake_label))\n        output = netD(fake.detach())\n        errD_fake = criterion(output, labelv)\n        errD_fake.backward()\n        D_G_z1 = output.data.mean()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        labelv = Variable(label.fill_(real_label)) # fake labels are real for generator cost\n        output = netD(fake)\n        errG = criterion(output, labelv)\n        errG.backward()\n        D_G_z2 = output.data.mean()\n        optimizerG.step()\n\n        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n              % (epoch, niter, i, len(dataloader),\n                 errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2))\n        if i % 100 == 0:\n            vutils.save_image(real_cpu,\n                    '%s/real_samples.png' % outf,\n                    normalize=True)\n            fake = netG(fixed_noise)\n            vutils.save_image(fake.data,\n                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n                    normalize=True)\n```", "```py\nTEXT = d.Field(lower=True, batch_first=True)\ntrain, valid, test = datasets.WikiText2.splits(TEXT,root='data')\n```", "```py\nprint(len(train[0].text))\n\n#output\n2088628\n```", "```py\nprint(train[0].text[:100])\n\n#Results of first 100 tokens\n\n'<eos>', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', '<eos>', 'senjō', 'no', 'valkyria', '3', ':', '<unk>', 'chronicles', '(', 'japanese', ':', '3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@-@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media.vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<unk>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@-@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the'\n```", "```py\ntrain_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n    (train, valid, test), batch_size=20, bptt_len=35, device=0)\n```", "```py\nclass RNNModel(nn.Module):\n    def __init__(self,ntoken,ninp,nhid,nlayers,dropout=0.5,tie_weights=False):\n        #ntoken represents the number of words in vocabulary.\n        #ninp Embedding dimension for each word ,which is the input for the LSTM.\n        #nlayer Number of layers required to be used in the LSTM .\n        #Dropout to avoid overfitting.\n        #tie_weights - use the same weights for both encoder and decoder. \n        super().__init__()\n        self.drop = nn.Dropout()\n        self.encoder = nn.Embedding(ntoken,ninp)\n        self.rnn = nn.LSTM(ninp,nhid,nlayers,dropout=dropout)\n        self.decoder = nn.Linear(nhid,ntoken)\n        if tie_weights:\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange,initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange,initrange)\n\n    def forward(self,input,hidden):\n        emb = self.drop(self.encoder(input))\n        output,hidden = self.rnn(emb,hidden)\n        output = self.drop(output)\n        s = output.size()\n        decoded = self.decoder(output.view(s[0]*s[1],s[2]))\n        return decoded.view(s[0],s[1],decoded.size(1)),hidden\n\n    def init_hidden(self,bsz):\n        weight = next(self.parameters()).data\n\n        return (Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()),Variable(weight.new(self.nlayers,bsz,self.nhid).zero_()))\n\n```", "```py\ncriterion = nn.CrossEntropyLoss()\n\ndef trainf():\n    # Turn on training mode which enables dropout.\n    lstm.train()\n    total_loss = 0\n    start_time = time.time()\n    hidden = lstm.init_hidden(batch_size)\n    for i,batch in enumerate(train_iter):\n        data, targets = batch.text,batch.target.view(-1)\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n        hidden = repackage_hidden(hidden)\n        lstm.zero_grad()\n        output, hidden = lstm(data, hidden)\n        loss = criterion(output.view(-1, ntokens), targets)\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm(lstm.parameters(), clip)\n        for p in lstm.parameters():\n            p.data.add_(-lr, p.grad.data)\n\n        total_loss += loss.data\n\n        if i % log_interval == 0 and i > 0:\n            cur_loss = total_loss[0] / log_interval\n            elapsed = time.time() - start_time\n            (print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | loss {:5.2f} | ppl {:8.2f}'.format(epoch, i, len(train_iter), lr,elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss))))\n            total_loss = 0\n            start_time = time.time()\n```", "```py\nlstm.train()\n```", "```py\ndef repackage_hidden(h):\n    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n    if type(h) == Variable:\n        return Variable(h.data)\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n```", "```py\ntorch.nn.utils.clip_grad_norm(lstm.parameters(), clip)\n```", "```py\n  for p in lstm.parameters():\n      p.data.add_(-lr, p.grad.data)\n```", "```py\ndef evaluate(data_source):\n    # Turn on evaluation mode which disables dropout.\n    lstm.eval()\n    total_loss = 0 \n    hidden = lstm.init_hidden(batch_size)\n    for batch in data_source: \n        data, targets = batch.text,batch.target.view(-1)\n        output, hidden = lstm(data, hidden)\n        output_flat = output.view(-1, ntokens)\n        total_loss += len(data) * criterion(output_flat, targets).data\n        hidden = repackage_hidden(hidden)\n    return total_loss[0]/(len(data_source.dataset[0].text)//batch_size)\n```", "```py\n# Loop over epochs.\nbest_val_loss = None\nepochs = 40\n\nfor epoch in range(1, epochs+1):\n    epoch_start_time = time.time()\n    trainf()\n    val_loss = evaluate(valid_iter)\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n        'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                   val_loss, math.exp(val_loss)))\n    print('-' * 89)\n    if not best_val_loss or val_loss < best_val_loss:\n        best_val_loss = val_loss\n    else:\n        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n        lr /= 4.0\n```", "```py\n-----------------------------------------------------------------------------------------\n| end of epoch  39 | time: 34.16s | valid loss  4.70 | valid ppl   110.01\n-----------------------------------------------------------------------------------------\n| epoch  40 |   200/ 3481 batches | lr 0.31 | ms/batch 11.47 | loss  4.77 | ppl   117.40\n| epoch  40 |   400/ 3481 batches | lr 0.31 | ms/batch  9.56 | loss  4.81 | ppl   122.19\n| epoch  40 |   600/ 3481 batches | lr 0.31 | ms/batch  9.43 | loss  4.73 | ppl   113.08\n| epoch  40 |   800/ 3481 batches | lr 0.31 | ms/batch  9.48 | loss  4.65 | ppl   104.77\n| epoch  40 |  1000/ 3481 batches | lr 0.31 | ms/batch  9.42 | loss  4.76 | ppl   116.42\n| epoch  40 |  1200/ 3481 batches | lr 0.31 | ms/batch  9.55 | loss  4.70 | ppl   109.77\n| epoch  40 |  1400/ 3481 batches | lr 0.31 | ms/batch  9.41 | loss  4.74 | ppl   114.61\n| epoch  40 |  1600/ 3481 batches | lr 0.31 | ms/batch  9.47 | loss  4.77 | ppl   117.65\n| epoch  40 |  1800/ 3481 batches | lr 0.31 | ms/batch  9.46 | loss  4.77 | ppl   118.42\n| epoch  40 |  2000/ 3481 batches | lr 0.31 | ms/batch  9.44 | loss  4.76 | ppl   116.31\n| epoch  40 |  2200/ 3481 batches | lr 0.31 | ms/batch  9.46 | loss  4.77 | ppl   117.52\n| epoch  40 |  2400/ 3481 batches | lr 0.31 | ms/batch  9.43 | loss  4.74 | ppl   114.06\n| epoch  40 |  2600/ 3481 batches | lr 0.31 | ms/batch  9.44 | loss  4.62 | ppl   101.72\n| epoch  40 |  2800/ 3481 batches | lr 0.31 | ms/batch  9.44 | loss  4.69 | ppl   109.30\n| epoch  40 |  3000/ 3481 batches | lr 0.31 | ms/batch  9.47 | loss  4.71 | ppl   111.51\n| epoch  40 |  3200/ 3481 batches | lr 0.31 | ms/batch  9.43 | loss  4.70 | ppl   109.65\n| epoch  40 |  3400/ 3481 batches | lr 0.31 | ms/batch  9.51 | loss  4.63 | ppl   102.43\nval loss 4.686332647950745\n-----------------------------------------------------------------------------------------\n| end of epoch  40 | time: 34.50s | valid loss  4.69 | valid ppl   108.45\n-----------------------------------------------------------------------------------------\n```"]