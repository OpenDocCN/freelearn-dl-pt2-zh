- en: Deep Learning with Sequence Data and Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we covered how to handle spatial data using **Convolution
    Neural Networks** (**CNNs**) and also built image classifiers. In this chapter,
    we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Different representations of text data that are useful for building deep learning
    models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding **recurrent neural networks** (**RNNs**) and different implementations
    of RNNs, such as **Long Short-Term Memory** (**LSTM**) and **Gated Recurrent Unit**
    (**GRU**), which power most of the deep learning models for text and sequential
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using one-dimensional convolutions for sequential data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the applications that can be built using RNNs are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document classifiers**: Identifying the sentiment of a tweet or review, classifying
    news articles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence-to-sequence learning**: For tasks such as language translations,
    converting English to French'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-series forecasting**: Predicting the sales of a store given details
    about previous days'' store details'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text is one of the commonly used sequential data types. Text data can be seen
    as either a sequence of characters or a sequence of words. It is common to see
    text as a sequence of words for most problems. Deep learning sequential models
    such as RNN and its variants are able to learn important patterns from text data
    that can solve problems in areas such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Natural language understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These sequential models also act as important building blocks for various systems,
    such as **question and answering** (**QA**) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Though these models are highly useful in building these applications, they do
    not have an understanding of human language, due to its inherent complexities.
    These sequential models are able to successfully find useful patterns that are
    then used for performing different tasks. Applying deep learning to text is a
    fast-growing field, and a lot of new techniques arrive every month. We will cover
    the fundamental components that power most of the modern-day deep learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning models, like any other machine learning model, do not understand
    text, so we need to convert text into numerical representation. The process of
    converting text into numerical representation is called **vectorization** and
    can be done in different ways, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert text into words and represent each word as a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert text into characters and represent each character as a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create *n*-gram of words and represent them as vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text data can be broken down into one of these representations. Each smaller
    unit of text is called a **token**, and the process of breaking text into tokens
    is called **tokenization**. There are a lot of powerful libraries available in
    Python that can help us in tokenization. Once we convert the text data into tokens,
    we then need to map each token to a vector. One-hot encoding and word embedding are
    the two most popular approaches for mapping tokens to vectors. The following diagram
    summarizes the steps for converting text into their vector representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aff887f1-cdb3-42fc-8d15-62760bd50005.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look in more detail at tokenization, *n*-gram representation, and vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a sentence, splitting it into either characters or words is called **tokenization**.
    There are libraries, such as spaCy, that offer complex solutions to tokenization.
    Let's use simple Python functions such as `split` and `list` to convert the text
    into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how tokenization works on characters and words, let''s consider
    a small review of the movie *Thor: Ragnarok*. We will work with the following
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: The action scenes were top notch in this movie. Thor has never been this epic
    in the MCU. He does some pretty epic sh*t in this movie and he is definitely not
    under-powered anymore. Thor in unleashed in this, I love that.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text into characters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Python `list` function takes a string and converts it into a list of individual
    characters. This does the job of converting the text into characters. The following
    code block shows the code used and the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This result shows how our simple Python function has converted text into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Converting text into words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `split` function available in the Python string object to break
    the text into words. The `split` function takes an argument, based on which it
    splits the text into tokens. For our example, we will use spaces as the delimiters.
    The following code block demonstrates how we can convert text into words using
    the Python `split` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we did not use any separator; by default, the `split`
    function splits on white spaces.
  prefs: []
  type: TYPE_NORMAL
- en: N-gram representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen how text can be represented as characters and words. Sometimes
    it is useful to look at two, three, or more words together. *N*-grams are groups
    of words extracted from given text. In an *n*-gram, *n* represents the number
    of words that can be used together. Let''s look at an example of what a bigram (*n=2*)
    looks like. We used the Python `nltk` package to generate a bigram for `thor_review`.
    The following code block shows the result of the bigram and the code used to generate
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ngrams` function accepts a sequence of words as its first argument and
    the number of words to be grouped as the second argument. The following code block
    shows how a trigram representation would look, and the code used for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The only thing that changed in the preceding code is the *n*-value, the second
    argument to the function.
  prefs: []
  type: TYPE_NORMAL
- en: Many supervised machine learning models, for example, Naive Bayes, use *n*-grams
    to improve their feature space. *n*-grams are also used for spelling correction
    and text-summarization tasks.
  prefs: []
  type: TYPE_NORMAL
- en: One challenge with *n*-gram representation is that it loses the sequential nature
    of text. It is often used with shallow machine learning models. This technique
    is rarely used in deep learning, as architectures such as RNN and Conv1D learn
    these representations automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two popular approaches to mapping the generated tokens to vectors
    of numbers, called **one-hot encoding** and **word embedding**. Let's understand
    how tokens can be converted to these vector representations by writing a simple
    Python program. We will also discuss the various pros and cons of each method.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In one-hot encoding, each token is represented by a vector of length N, where
    *N* is the size of the vocabulary. The vocabulary is the total number of unique
    words in the document. Let''s take a simple sentence and observe how each token
    would be represented as one-hot encoded vectors. The following is the sentence
    and its associated token representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*An apple a day keeps doctor away said the doctor*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One-hot encoding for the preceding sentence can be represented into a tabular
    format as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| An | 100000000 |'
  prefs: []
  type: TYPE_TB
- en: '| apple | 010000000 |'
  prefs: []
  type: TYPE_TB
- en: '| a | 001000000 |'
  prefs: []
  type: TYPE_TB
- en: '| day | 000100000 |'
  prefs: []
  type: TYPE_TB
- en: '| keeps | 000010000 |'
  prefs: []
  type: TYPE_TB
- en: '| doctor | 000001000 |'
  prefs: []
  type: TYPE_TB
- en: '| away | 000000100 |'
  prefs: []
  type: TYPE_TB
- en: '| said | 000000010 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 000000001 |'
  prefs: []
  type: TYPE_TB
- en: 'This table describes the tokens and their one-hot encoded representation. The
    vector length is 9, as there are nine unique words in the sentence. A lot of machine
    learning libraries have eased the process of creating one-hot encoding variables.
    We will write our own implementation to make it easier to understand, and we can
    use the same implementation to build other features required for later examples.
    The following code contains a `Dictionary` class, which contains functionality
    to create a dictionary of unique words along with a function to return a one-hot
    encoded vector for a particular word. Let''s take a look at the code and then
    walk through each functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provides three important functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: The initialization function, `__init__`, creates a `word2idx` dictionary, which
    will store all unique words along with the index. The `idx2word` list stores all
    the unique words, and the `length` variable contains the total number of unique
    words in our documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `add_word` function takes a word and adds it to `word2idx` and `idx2word`, and
    increases the length of the vocabulary, provided the word is unique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `onehot_encoded` function takes a word and returns a vector of length N
    with zeros throughout, except at the index of the word. If the index of the passed
    word is two, then the value of the vector at index two will be one, and all the
    remaining values will be zeros.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As we have defined our `Dictionary` class, let''s use it on our `thor_review`
    data. The following code demonstrates how the `word2idx` is built and how we can
    call our `onehot_encoded` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'One-hot encoding for the word `were` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: One of the challenges with one-hot representation is that the data is too sparse,
    and the size of the vector quickly grows as the number of unique words in the
    vocabulary increases, which is considered to be a limitation, and hence it is
    rarely used with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word embedding is a very popular way of representing text data in problems that
    are solved by deep learning algorithms. Word embedding provides a dense representation
    of a word filled with floating numbers. The vector dimension varies according
    to the vocabulary size. It is common to use a word embedding of dimension size
    50, 100, 256, 300, and sometimes 1,000\. The dimension size is a hyper-parameter
    that we need to play with during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: If we are trying to represent a vocabulary of size 20,000 in one-hot representation
    then we will end up with 20,000 x 20,000 numbers, most of which will be zero.
    The same vocabulary can be represented in word embedding as 20,000 x dimension
    size, where the dimension size could be 10, 50, 300, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to create word embeddings is to start with dense vectors for each token
    containing random numbers, and then train a model such as a document classifier
    or sentiment classifier. The floating point numbers, which represent the tokens,
    will get adjusted in a way such that semantically closer words will have similar
    representation. To understand it, let''s look at the following figure, where we
    plotted the word embedding vectors on a two-dimensional plot of five movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2302e41-c762-4566-aa6e-6b176145c527.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image shows how the dense vectors are tuned in order to have smaller
    distances for words that are semantically similar. Since movie titles such as
    **Superman**, **Thor**, and **Batman** are action movies based on comics, the
    embedding for such words is closer, whereas the embedding for the movie **Titanic**
    is far from the action movies and closer to the movie title **Notebook**, since
    they are romantic movies.
  prefs: []
  type: TYPE_NORMAL
- en: Learning word embedding may not be feasible when you have too little data, and
    in such cases we can use word embeddings that are trained by some other machine
    learning algorithm. An embedding generated from another task is called a **pretrained**
    word embedding. We will learn how to build our own word embeddings and use pretrained
    word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Training word embedding by building a sentiment classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, we briefly learned about word embedding without implementing
    it. In this section, we will download a dataset called `IMDB`, which contains
    reviews, and build a sentiment classifier which calculates whether a review''s
    sentiment is positive, negative, or unknown. In the process of building, we will
    also train word embedding for the words present in the `IMDB` dataset. We will
    use a library called `torchtext` that makes a lot of processes such as downloading,
    text vectorization, and batching much easier. Training a sentiment classifier
    will involve the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading IMDB data and performing text tokenization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building a vocabulary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating batches of vectors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a network model with embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloading IMDB data and performing text tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For applications related to computer vision, we used the `torchvision` library,
    which provides us with a lot of utility functions, helping to building computer
    vision applications. In the same way, there is a library called `torchtext`, part
    of PyTorch, which is built to work with PyTorch and eases a lot of activities
    related to **natural language processing** (**NLP**) by providing different data
    loaders and abstractions for text. At the time of writing, `torchtext` does not
    come with PyTorch installation and requires a separate installation. You can run
    the following code in the command line of your machine to get `torchtext` installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Once it is installed, we will be able to use it. Torchtext provides two important
    modules called `torchtext.data` and `torchtext.datasets`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can download the `IMDB Movies` dataset from the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/orgesleka/imdbmovies](https://www.kaggle.com/orgesleka/imdbmovies)'
  prefs: []
  type: TYPE_NORMAL
- en: torchtext.data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext.data` instance defines a class called `Field`, which helps us
    to define how the data has to be read and tokenized. Let''s look at the following
    example, which we will use for preparing our `IMDB` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we define two `Field` objects, one for actual text and
    another for the label data. For actual text, we expect `torchtext` to lowercase
    all the text, tokenize the text, and trim it to a maximum length of `20`. If we
    are building the application for a production environment, we may fix the length
    to a much larger number. But, for the toy example it works well. The `Field` constructor
    also accepts another argument called **tokenize**, which by default uses the `str.split`
    function. We can also specify spaCy as the argument, or any other tokenizer. For
    our example we will stick with `str.split`.
  prefs: []
  type: TYPE_NORMAL
- en: torchtext.datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext.datasets` instance provide wrappers for using different datasets
    like IMDB, TREC (question classification), language modeling (WikiText-2), and
    a few other datasets. We will use `torch.datasets` to download the `IMDB` dataset
    and split it into `train` and `test` datasets. The following code does that, and
    when you run it for the first time it could take several minutes, depending on
    your broadband connection, as it downloads the `IMDB` datasets from the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous dataset''s `IMDB` class abstracts away all the complexity involved
    in downloading, tokenizing, and splitting the database into `train` and `test`
    datasets. `train.fields` contains a dictionary where `TEXT` is the key and the
    value `LABEL`. Let''s look at `train.fields` and each element of `train` contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We can see from these results that a single element contains a field, `text`,
    along with all the tokens representing the `text`, and a `label` field that contains
    the label of the text. Now we have the `IMDB` dataset ready for batching.
  prefs: []
  type: TYPE_NORMAL
- en: Building vocabulary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we created one-hot encoding for `thor_review`, we created a `word2idx`
    dictionary, which is referred to as the vocabulary since it contains all the details
    of the unique words in the documents. The `torchtext` instance makes that easier
    for us. Once the data is loaded, we can call `build_vocab` and pass the necessary
    arguments that will take care of building the vocabulary for the data. The following
    code shows how the vocabulary is built:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we pass in the `train` object on which we need to build
    the vocabulary, and we also ask it to initialize vectors with pretrained embeddings
    of dimensions `300`. The `build_vocab` object just downloads and creates the dimension
    that will be used later, when we train the sentiment classifier using pretrained
    weights. The `max_size` instance limits the number of words in the vocabulary,
    and `min_freq` removes any word which has not occurred more than ten times, where
    `10` is configurable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the vocabulary is built, we can obtain different values such as frequency,
    word index, and the vector representation for each word. The following code demonstrates
    how to access these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code demonstrates how to access the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `stoi` gives access to a dictionary containing words and their indexes.
  prefs: []
  type: TYPE_NORMAL
- en: Generate batches of vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Torchtext provides `BucketIterator`, which helps in batching all the text and
    replacing the words with the index number of the words. The `BucketIterator` instance
    comes with a lot of useful parameters like `batch_size`, `device` (GPU or CPU),
    and `shuffle` (whether data has to be shuffled). The following code demonstrates
    how to create iterators that generate batches for the `train` and `test` datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code gives a `BucketIterator` object for `train` and `test` datasets.
    The following code will show how to create a `batch` and display the results of
    the `batch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: From the results in the preceding code block, we can see how the text data is
    converted into a matrix of size (`batch_size` * `fix_len`), which is (`128x20`)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Creating a network model with embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We discussed word embeddings briefly earlier. In this section, we create word
    embeddings as part of our network architecture and train the entire model to predict
    the sentiment of each review. At the end of the training, we will have a sentiment
    classifier model and also the word embeddings for the `IMDB` datasets. The following
    code demonstrates how to create a network architecture to predict the sentiment
    using word embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `EmbNet` creates the model for sentiment classification.
    Inside the `__init__` function, we initialize an object of the `nn.Embedding`
    class, which takes two arguments, namely, the size of the vocabulary and the dimensions
    that we wish to create for each word. As we have limited the number of unique
    words, the vocabulary size will be 10,000 and we can start with a small embedding
    size of `10`. For running the program quickly, a small embedding size is useful,
    but when you are trying to build applications for production systems, use embeddings
    of a large size. We also have a linear layer that maps the word embeddings to
    the category (positive, negative, or unknown).
  prefs: []
  type: TYPE_NORMAL
- en: The `forward` function determines how the input data is processed. For a batch
    size of 32 and sentences of a maximum length of 20 words, we will have inputs
    of the shape 32 x 20\. The first embedding layer acts as a lookup table, replacing
    each word with the corresponding embedding vector. For an embedding dimension
    of 10, the output becomes 32 x 20 x 10 as each word is replaced with its corresponding
    embedding. The `view()` function will flatten the result from the embedding layer.
    The first argument passed to `view` will keep that dimension intact. In our case,
    we do not want to combine data from different batches, so we preserve the first
    dimension and flatten the rest of the values in the tensor. After the `view` function
    is applied, the tensor shape changes to 32 x 200\. A dense layer maps the flattened
    embeddings to the number of categories. Once the network is defined, we can train
    the network as usual.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in this network, we lose the sequential nature of the text and
    we just use them as a bag of words.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training the model is very similar to what we saw for building image classifiers,
    so we will be using the same functions. We pass batches of data through the model,
    calculate the outputs and losses, and then optimize the model weights, which includes
    the embedding weights. The following code does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we call the `fit` method by passing the `BucketIterator`
    object that we created for batching the data. The iterator, by default, does not
    stop generating batches, so we have to set the `repeat` variable of the `BucketIterator`
    object to `False`. If we don't set the `repeat` variable to `False` then the `fit`
    function will run indefinitely. Training the model for around 10 epochs gives
    a validation accuracy of approximately 70%.
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained word embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pretrained word embeddings would be useful when we are working in specific
    domains, such as medicine and manufacturing, where we have lot of data to train
    the embeddings. When we have little data on which we cannot meaningfully train
    the embeddings, we can use embeddings, which are trained on different data corpuses
    such as Wikipedia, Google News and Twitter tweets. A lot of teams have open source
    word embeddings trained using different approaches. In this section, we will explore
    how `torchtext` makes it easier to use different word embeddings, and how to use
    them in our PyTorch models. It is similar to transfer learning, which we use in
    computer vision applications. Typically, using pretrained embedding would involve
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the embeddings in the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Freezing the embedding layer weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explore in detail how each step is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `torchtext` library abstracts away a lot of complexity involved in downloading
    the embeddings and mapping them to the right word. Torchtext provides three classes,
    namely `GloVe`, `FastText`, `CharNGram`, in the `vocab` module, that ease the
    process of downloading embeddings, and mapping them to our vocabulary. Each of
    these classes provides different embeddings trained on different datasets and
    using different techniques. Let''s look at some of the different embeddings provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '`charngram.100d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fasttext.en.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fasttext.simple.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.42B.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.840B.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.25d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.50d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.100d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.twitter.27B.200d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`` `glove.6B.50d` ``'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.100d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.200d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glove.6B.300d`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `build_vocab` method of the `Field` object takes in an argument for the
    embeddings. The following code explains how we download the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The value to the argument vector denotes what embedding class is to be used.
    The `name` and `dim` arguments determine on what embeddings can be used. We can
    easily access the embeddings from the `vocab` object. The following code demonstrates
    it, along with a view of how the results will look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now we have downloaded and mapped the embeddings to our vocabulary. Let's understand
    how we can use them with a PyTorch model.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the embeddings in the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `vectors` variable returns a torch tensor of shape `vocab_size x dimensions`
    containing the pretrained embeddings. We have to store the embeddings to the weights
    of our embedding layer. We can assign the weights of the embeddings by accessing
    the weights of the embeddings layer as demonstrated by the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`model` represents the object of our network, and `embedding` represents the
    embedding layer. As we are using the embedding layer with new dimensions, there
    will be a small change in the input to the linear layer that comes after the embedding
    layer. The following code has the new architecture, which is similar to the previously-used
    architecture where we trained our embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Once the embeddings are loaded, we have to ensure that, during training, we
    do not change the embedding weights. Let's discuss how to achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: Freeze the embedding layer weights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a two-step process to tell PyTorch not to change the weights of the embedding
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the `requires_grad` attribute to `False`, which instructs PyTorch that it
    does not need gradients for these weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the passing of the embedding layer parameters to the optimizer. If this
    step is not done, then the optimizer throws an error, as it expects all the parameters
    to have gradients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code demonstrates how easy it is to freeze the embedding layer
    weights and instruct the optimizer not to use those parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We generally pass all the model parameters to the optimizer, but in the previous
    code we pass parameters which have `requires_grad` to be `True`.
  prefs: []
  type: TYPE_NORMAL
- en: We can train the model using this exact code and should achieve similar accuracy.
    All these model architectures fail to take advantage of the sequential nature
    of the text. In the next section, we explore two popular techniques, namely RNN
    and Conv1D, that take advantage of the sequential nature of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are among the most powerful models that enable us to take on applications
    such as classification, labeling on sequential data, generating sequences of text
    (such as with the *SwiftKey* *Keyboard* app which predicts the next word), and
    converting one sequence to another such as translating a language, say, from French
    to English. Most of the model architectures such as feedforward neural networks
    do not take advantage of the sequential nature of data. For example, we need the
    data to present the features of each example in a vector, say all the tokens that represent
    a sentence, paragraph, or documents. Feedforward networks are designed just to
    look at all the features once and map them to output. Let's look at a text example
    which shows why the order, or sequential nature, is important of text. *I had
    cleaned my car* and *I had my car cleaned* are two English sentences with the
    same set of words, but they mean different things only when we consider the order
    of the words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Humans make sense of text data by reading words from left to right and building
    a powerful model that kind of understands all the different things the text says.
    RNN works slightly similarly, by looking at one word in text at a time. RNN is
    also a neural network which has a special layer in it, which loops over the data
    instead of processing all at once. As RNNs can process data in sequence, we can
    use vectors of different lengths and generate outputs of different lengths. Some
    of the different representations are provided in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/163554cd-be34-4dd8-911e-465a50c652c5.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Image source: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
  prefs: []
  type: TYPE_NORMAL
- en: The previous image is from one of the famous blogs on RNN ([http://karpathy.github.io/2015/05/21/rnn-effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness))
    , in which the author, Andrej Karpathy, writes about how to build an RNN from
    scratch using Python and use it as sequence generator.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how RNN works with an example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with an assumption that we have an RNN model already built, and
    try to understand what functionality it provides. Once we understand what an RNN
    does, then let's explore what happens inside an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider the Thor review as input to the RNN model. The example text
    we are looking at is *the action scenes were top notch in this movie....* . We
    first start by passing the first word, **the**, to our model; the model generates
    two different things, a **State Vector** and an **Output** vector. The state vector
    is passed to the model when it processes the next word in the review, and a new
    state vector is generated. We just consider the **Output** of the model generated
    during the last sequence. The following figure summarizes it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/428b553f-c7f7-4772-9354-f99487db93e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding figure demonstrates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How RNN works by unfolding it and the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the state is recursively passed to the same model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By now you will have an idea of what RNN does, but not how it works. Before
    we get into how it works, let''s look at a code snippet which showcases in more
    detail what we have learnt. We will still view RNN as a black box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the `hidden` variable represents the state vector, sometimes
    called **hidden state**. By now we should have an idea of how RNN is used. Now,
    let''s look at the code that implements RNN and understand what happens inside
    the RNN. The following code contains the `RNN` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Except for the word `RNN` in the preceding code, everything else would sound
    pretty similar to what we have used in the previous chapters, as PyTorch hides
    a lot of complexity of backpropagation. Let's walk through the `init` function
    and the `forward` function to understand what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` function initializes two linear layers, one for calculating the
    output and the other for calculating the state or hidden vector.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward` function combines the `input` vector and the `hidden` vector and
    passes it through the two linear layers, which generates an output vector and
    a hidden state. For the `output` layer, we apply a `log_softmax` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `initHidden` function helps in creating hidden vectors with no state for
    calling RNN the very first time. Let''s take a visual look into what the `RNN`
    class does in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da427f73-3c11-40e1-8ac7-6b792031c8f2.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding figure shows how an RNN works.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts of RNN are sometimes tricky to understand when you meet them for
    the first time, so I would strongly recommend some of the amazing blogs provided
    in the following links: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    and [http://colah.github.io/posts/2015-08-Understanding-LSTMs/.](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to use a variant of RNN called **LSTM** to
    build a sentiment classifier on the `IMDB` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are quite popular in building real-world applications like language translation,
    text classification and many more sequential problems, but in reality, we rarely
    would use a vanilla version of RNN which we saw in the previous section. The vanilla
    version of RNN has problems like vanishing gradients and gradient explosion when
    dealing with large sequences. In most of the real-world problems, variants of
    RNN such as LSTM or GRU are used, which solve the limitations of plain RNN and
    also have the ability to handle sequential data better. We will try to understand
    what happens in LSTM, and build a network based on LSTM to solve the text classification
    problem on the `IMDB` datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term dependency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs, in theory, should learn all the dependency required from the historical
    data to build a context of what happens next. Say, for example, we are trying
    to predict the last word in the sentence *the clouds are in the sky*. RNN would
    be able to predict it, as the information (clouds) is just a few words behind.
    Let's take another long paragraph where the dependency need not be that close,
    and we want to predict the last word in it. The sentence looks like *I am born
    in Chennai a city in Tamilnadu. Did schooling in different states of India and
    I speak... *. The vanilla version of RNN, in practice, finds it difficult to remember
    the contexts that happened in the earlier parts of sequences. LSTMs and other
    different variants of RNN solve this problem by adding different neural networks
    inside the LSTM which later decides how much, or what data can be remembered.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTMs are a special kind of RNN, capable of learning long-term dependency. They
    were introduced in 1997 and got popular in the last few years with advancements
    in available data and hardware. They work tremendously well on a large variety
    of problems and are widely used.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs are designed to avoid long-term dependency problems by having a design
    by which it is natural to remember information for a long period of time. In RNNs,
    we saw how they repeat themselves over each element of the sequence. In standard
    RNNs, the repeating module will have a simple structure like a single linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows how a simple RNN repeats itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9bb542e-3479-4972-a3f0-c9ae6bb1ea82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inside LSTM, instead of using a simple linear layer we have smaller networks
    inside the LSTM which does an independent job. The following diagram showcases
    what happens inside an LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b1faf38-e33d-487a-9ea3-6d028543434b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (diagram
    by Christopher Olah)
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the small rectangular (yellow) boxes in the second box in the preceding
    diagram represents a PyTorch layer, the circles represent an element matrix or
    vector addition, and the merging lines represent that two vectors are being concatenated.
    The good part is, we need not implement all of this manually. Most of the modern
    deep learning frameworks provide an abstraction which will take care of what happens
    inside an LSTM. PyTorch provides abstraction of all the functionality inside `nn.LSTM`
    layer, which we can use like any other layer. The most important thing in the
    LSTM is the cell state that passes through all the iterations, represented by
    the horizontal line across the cells in the preceding diagram. Multiple networks
    inside LSTM control what information travels across the cell state. The first
    step in LSTM (a small network represented by the symbol **σ**) is to decide what
    information is going to be thrown away from the cell state. This network is called
    **forget gate** and has a sigmoid as an activation function, which outputs values
    between 0 and 1 for each element in the cell state.The network (PyTorch layer)
    is represented using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1a8fd1b-94ec-407b-a9fa-b2ba2ab10a4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The values from the network decide which values are to be held in the cell
    state and which are to be thrown away. The next step is to decide what information
    we are going to add to the cell state. This has two parts; a sigmoid layer, called
    **input gate**, which decides what values to be updated; and a tanh layer, which
    creates new values to be added to the cell state. The mathematical representation
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8c503d6-8b2b-443c-922b-8fb0ed18aa49.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/57590e15-f86c-48a0-99a3-9ef824cbe5d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the next step, we combine the two values generated by the input gate and
    tanh. Now we can update the cell state, by doing an element-wise multiplication
    between the forget gate and the sum of the product of *i*[*t* ]and C[t],as represented
    by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d42a0f3-2a83-4057-ac84-d385c413aa34.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we need to decide on the output, which will be a filtered version of
    the cell state. There are different versions of LSTM available and most of them
    work on similar principles. As developers or data scientists, we rarely need to
    worry about what goes on inside LSTM. If you want to learn more about them, go
    through the following blog links, which cover a lot of theory in a very intuitive
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Look at Christopher Olah's amazing blog on LSTM ([http://colah.github.io/posts/2015-08-Understanding-LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs)),
    and another blog from Brandon Rohrer ([https://brohrer.github.io/how_rnns_lstm_work.html](https://brohrer.github.io/how_rnns_lstm_work.html))
    where he explains LSTM in a nice video.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we understand LSTM, let''s implement a PyTorch network which we can use
    to build a sentiment classifier. As usual, we will follow these steps for creating
    the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the batches
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating the network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use the same torchtext for downloading, tokenizing and building vocabulary
    for the `IMDB` dataset. When creating the `Field` object, we leave the `batch_first`
    argument at `False`. RNN networks expect the data to be in the form of `Sequence_length`,
    `batch_size` and features. The following is used for preparing the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Creating batches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We use the torchtext `BucketIterator` for creating batches, and the size of
    the batches will be sequence length and batches. For our case, the size will be
    [`200`, `32`], where *200* is the sequence length and *32* is the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code used for batching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Creating the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the code and then walk through the code. You may be surprised
    at how similar the code looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `init` method creates an embedding layer of the size of the vocabulary and
    `hidden_size`. It also creates an LSTM and a linear layer. The last layer is a
    `LogSoftmax` layer for converting the results from the linear layer to probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the `forward` function, we pass the input data of size [`200`, `32`], which
    gets passed through the embedding layer and each token in the batch gets replaced
    by embedding and the size turns to [200, 32, 100], where *100* is the embedding
    dimensions. The LSTM layer takes the output of the embedding layer along with
    two hidden variables. The hidden variables should be of the same type of the embeddings
    output, and their size should be [`num_layers`, `batch_size`, `hidden_size`].
    The LSTM processes the data in a sequence and generates the output of the shape
    [`Sequence_length`, `batch_size`, `hidden_size`], where each sequence index represents
    the output of that sequence. In this case, we just take the output of the last
    sequence, which is of shape [`batch_size`, `hidden_dim`], and pass it on to a
    linear layer to map it to the output categories. Since the model tends to overfit,
    add a dropout layer. You can play with the dropout probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the network is created, we can train the model using the same code as
    seen in the previous examples. The following is the code for training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Following is the result of the training model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Training the model for four epochs gave an accuracy of 84%. Training for more
    epochs resulted in an overfitted model, as the loss started increasing. We can
    try some of the techniques that we tried such as decreasing the hidden dimensions,
    increasing sequence length, and training with smaller learning rates to further
    improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We will also explore how we can use one-dimensional convolutions for training
    on sequence data.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional network on sequence data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how CNNs solve problems in computer vision by learning features from
    the images. In images, CNNs work by convolving across height and width. In the
    same way, time can be treated as a convolutional feature. One-dimensional convolutions
    sometimes perform better than RNNs and are computationally cheaper. In the last
    few years, companies like Facebook have shown success in audio generation and
    machine translation. In this section, we will learn how CNNs can be used to build
    a text classification solution.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding one-dimensional convolution for sequence data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep Learning
    for Computer Vision*, we have seen how two-dimensional weights are learned from
    the training data. These weights move across the image to generate different activations.
    In the same way, one-dimensional convolution activations are learned during training
    of our text classifier, where these weights learn patterns by moving across the
    data. The following diagram explains how one-dimensional convolutions will work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1459e19d-8b6f-4b96-94ee-c1b561de8530.png)'
  prefs: []
  type: TYPE_IMG
- en: For training a text classifier on the `IMDB` dataset, we will follow the same
    steps as we followed for building the classifier using LSTM. The only thing that
    changes is that we use `batch_first = True`, unlike in our LSTM network. So, let's
    look at the network, the training code, and its results.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the network architecture and then walk through the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, instead of an LSTM layer we have a `Conv1d` layer and
    an `AdaptiveAvgPool1d` layer. The convolution layer accepts the sequence length
    as its input size, and the output size to the hidden size, as the kernel size
    three. Since we have to change the dimensions of the linear layer, every time
    we try to run it with different lengths we use an `AdaptiveAvgpool1d` which takes
    input of any size and generates an output of the given size. So, we can use a
    linear layer whose size is fixed. The rest of the code is similar to what we have
    seen in most of the network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The training steps for the model are the same as the previous example. Let''s
    just look at the code to call the `fit` method and the results it generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We ran the model for four epochs, which gave approximately 83% accuracy. Here
    are the results of running the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Since the `validation loss` started increasing after three epochs, I stopped
    running the model. A couple of things that we could try to improve the results
    are using pretrained weights, adding another convolution layer, and trying a `MaxPool1d`
    layer between the convolutions. I leave it to you to try this and see if that
    helps improve the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned different techniques to represent text data in deep
    learning. We learned how to use pretrained word embeddings and our own trained
    embeddings when working on a different domain. We built a text classifier using
    LSTMs and one-dimensional convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to train deep learning algorithms to
    generate stylish images, and new images, and to generate text.
  prefs: []
  type: TYPE_NORMAL
