- en: Deep Reinforcement Learning
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is about developing goal-driven agents to automate
    problem-solving by optimizing their actions within an environment. This involves
    predicting and classifying the available data and training agents to execute tasks
    successfully. Generally, an agent is an entity that has the capacity to interact
    with an environment, and the learning is done by applying feedback in terms of
    cumulative rewards from the environment to inform future actions.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Three different types of reinforcement learning can be distinguished:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Value-based—a value function provides an estimate of how good the current state
    of the environment is.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy-based—where a function determines an action based on a state.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-based—a model of the environment including state transitions, rewards,
    and action planning.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll start with a relatively basic use case of reinforcement
    learning for website optimization with multi-armed bandits, where we'll look at
    an agent and an environment, and how they interact. Then we'll move on to a simple
    demonstration of control, where it gets a bit more complex, and we'll get to see
    an agent environment and a policy-based method, REINFORCE. Finally, we'll learn
    how to play blackjack, where we'll use a **deep Q network** (**DQN**), a value-based
    algorithm that was used in the wave-making AI that could play Atari games created
    by DeepMind in 2015.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a website
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling a cartpole
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing blackjack
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full notebooks are available online on GitHub: [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter06](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter06).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a website
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll deal with website optimization. Often, it is necessary
    to try changes (or better, a single change) on a website to see the effect they
    will have. In a typical scenario of what's called an **A/B test**, two versions
    of the website will be compared systematically. An *A/B* test is conducted by showing
    versions *A* and *B* of a web page to a pre-determined number of users. Later,
    statistical significance or a confidence interval is calculated in order to quantify
    the differences in click-through rates, with the goal of deciding which of the
    two web page variants to keep.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Here, we'll look at website optimization from a reinforcement point of view,
    where for each view (or user loading the page), we choose the best version given
    the available data at the time when they load the website. After each piece of
    feedback (click or no click), we update the statistics. In comparison to A/B testing,
    this procedure can yield a more reliable outcome, and additionally we show the
    best web page variant more often over time. Please note that we are not limited
    to two variants, but we can compare many variants.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: This example use case of website optimization will help us to introduce the
    notions of agent and environment, and show us the trade-off between exploration
    and exploitation. We'll explain these concepts in the *How it works...* section.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网站优化的使用案例将帮助我们介绍代理和环境的概念，并展示探索与利用之间的权衡。我们将在*工作原理...*部分解释这些概念。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么做...
- en: 'In order to implement our recipe, we''ll need two components:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实施我们的方案，我们需要两个组件：
- en: Our agent is decides which web page to present to the user.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的代理决定向用户展示哪个网页。
- en: The environment is a test bed that will give our agent feedback (click or no
    click).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是一个测试平台，将给我们的代理提供反馈（点击或不点击）。
- en: 'Since we are only using standard Python, we don''t need to install anything,
    and we can delve right into implementing our recipe:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅使用标准的Python，无需安装任何东西，我们可以直接开始实施我们的方案：
- en: 'We''ll implement our environment first. We are considering this as a multi-armed
    bandit problem, which we''ll be explaining in the *How it works...* section. Consequently,
    we''ll call our environment `Bandit`:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先我们将实现我们的环境。我们将考虑这作为一个多臂老虎机问题，在*工作原理...*部分中会有详细解释。因此，我们将称我们的环境为`Bandit`：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This bandit is initialized with the number of available choices, `K`. This will
    set up a probability for each of these choices to get a click. In practice, the
    environment would be real user feedback; here, we simulate user behavior instead. The
    `play()` method plays the `ith` machine, and returns a reward of `1` or `0`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个老虎机初始化时有可用选择的数量`K`。这将为每个选择设置一个点击的概率。在实践中，环境将是真实用户的反馈；在这里，我们模拟用户行为。`play()`方法会玩第`i`台机器，并返回`1`或`0`的奖励。
- en: 'Now we need to interact with this environment. This is where our agent comes
    in. The agent has to make decisions, and we''ll give it a strategy to make decisions.
    We''ll include metrics collection as well. An abstract agent looks like this:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要与这个环境进行交互。这就是我们的代理要发挥作用的地方。代理需要做出决策，我们将为它提供一个决策策略。我们也会包括指标的收集。一个抽象的代理看起来是这样的：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Any agent will need an environment to interact with. It needs to make a single
    decision (`run_one_step(self)`), and, in order to see how good its decision-making
    is, we'll need to run a simulation (`run(self, n_steps)`).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 任何代理都需要一个环境来进行交互。它需要做出单一的决策（`run_one_step(self)`），为了看到其决策的好坏，我们需要运行一个模拟（`run(self,
    n_steps)`）。
- en: Agents will contain a lookup list of metric functions and also inherit a metric
    collection functionality. We can run the metrics collection through the `run_metrics(self,
    i)` function.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将包含一个指标函数的查找列表，并且还会继承一个指标收集功能。我们可以通过`run_metrics(self, i)`函数来运行指标收集。
- en: 'The strategy that we use here is called `UCB1`. We''ll explain this strategy
    in the *How to do it...* section:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的策略称为`UCB1`。我们将在*如何做...*部分解释这个策略：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Our `UCB1` agent needs an environment (a bandit) to interact with, and a single
    parameter alpha, which weighs the importance of exploring actions (versus exploiting
    the best known action). The agent maintains its history of choices over time,
    and a record of estimates for each possible choice.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`UCB1`代理需要一个环境（即老虎机）进行交互，并且还需要一个单一的参数 alpha，用于权衡探索动作的重要性（与利用已知最佳动作的程度）。代理会随着时间维护其选择的历史记录，以及每个可能选择的估计记录。
- en: What we should look at is the `run_one_step(self)` method, which makes a single
    choice by picking the best optimistic choice. The `run(self, n_step)` method runs
    a series of choices and gets back the feedback from the environment.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看一下`run_one_step(self)`方法，它通过选择最佳的乐观选择来做出单一选择。`run(self, n_step)`方法运行一系列选择，并从环境中获取反馈。
- en: 'Let''s track two metrics: regret, which is the sum of expected losses occurred
    because of suboptimal choices, and—as a measure of convergence of the agent''s
    estimates against the actual configuration of the environment—the Spearman rank
    correlation (`stats.spearmanr()`).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跟踪两个指标：遗憾值，即由于次优选择而导致的预期损失之和，以及作为代理估计值与环境实际配置之间收敛性的衡量标准——斯皮尔曼等级相关系数（`stats.spearmanr()`）。
- en: The **Spearman rank correlation** is equal to the Pearson correlation (often
    briefly called just the correlation or product-moment correlation) of the ranked
    variables.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**斯皮尔曼等级相关系数**等于排名变量的皮尔逊相关系数（通常简称为相关系数或乘积矩法相关系数）。'
- en: 'The Pearson correlation between two variables, ![](img/10c6e91b-16bd-4c97-888b-61a03d42d28d.png)
    and ![](img/89063572-b6e5-4e77-9352-47c3cf415b34.png), can be expressed as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/857a991f-0184-48f7-830c-3cf25c3b0d13.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: where ![](img/fe3ffd1f-afae-4547-863d-976bfb6287b0.png) is the covariance of
    ![](img/ae4fef2a-7e6e-4d50-8b18-a1e0c2e5f225.png) and ![](img/6cf6cdb6-4a76-4c2b-8b9b-9bbd8c22cabb.png), and ![](img/fb5c045d-c5b6-42bd-b5ba-be53ab12c994.png) is
    the standard deviation of ![](img/2ece40ee-148d-44a7-aa65-fafddde4480c.png).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spearman correlation, instead of operating on the raw scores, is calculated
    on the ranked scores. A rank transformation means that a variable is sorted by
    value, and each entry is assigned its order. Given a rank ![](img/e2246e67-fd3c-48de-8dad-6e80a1d6ecc2.png)of
    point *i* in *X*, the Spearman rank correlation is calculated like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39263887-f11f-43a1-a741-a6ef20e92c22.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: This assesses how well the relationship between two variables can be described
    as a monotonic, but not necessarily linear (as in the case of Pearson correlation)
    function. Like the Pearson correlation, the Spearman correlation ranges between
    ![](img/4a6caf8b-67d4-4b56-950d-674d0d04c4d9.png) for perfectly negatively correlated
    and ![](img/5ac1aeff-9205-43a0-bf15-36df665dcbd7.png) for perfectly correlated.
    ![](img/e8592acc-720d-4807-8112-d31a687a20fb.png) means there's no correlation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Our tracking functions are `update_regret()` and `update_rank_corr()`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now track these metrics in order to compare the influence of the `alpha` parameter
    (more or less exploration). We can then observe convergence and cumulative regret
    over time:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So we have 20 different choices of web pages, and we collect `regret` and `corr`
    as defined, and we run for `5000` iterations. If we plot this, we can get an idea
    of how well this agent performed:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b8966ed-fc1d-42fa-938b-3f25b488b80a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'For the second run, we''ll change alpha to `0.5`, so we''ll do less exploration:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/773a06ca-1f83-4b15-a55d-891f1d0f067b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: We can see that the cumulative regret with *alpha=0.5,* less exploration, is
    much lower than with *alpha=2.0*; however, the overall correlation of the estimates
    to the environment parameters is lower.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: So, with less exploration our agent models the true parameters of the environment
    less well. This comes from the fact that with less exploration the ordering of
    the lower ranked features has not converged. Even though they are ranked as suboptimal,
    they haven't been chosen often enough to determine whether they are worst or second
    worst, for example. This is what we see with less exploration, and this could
    be fine since we might only care about knowing which choice is best.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we dealt with the problem of website optimization. We simulated
    user choices to different versions of web pages while live-updating statistics
    about how good each variant is, and how often it should be shown. Furthermore, we
    compared the upsides and downsides of an explorative scenario and a more exploitative
    scenario.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们处理了网站优化问题。我们模拟用户对不同版本网页的选择，同时实时更新每个变体的统计数据，以及应该显示的频率。此外，我们比较了探索性场景和更加利用性场景的优缺点。
- en: We framed user responses to web pages as a multi-armed bandit problem. A **multi-armed
    bandit** (**MABP**)is a slot machine where gamblers insert a coin and pull one
    of several levers, each of which is associated with a different reward distribution
    that is unknown to the gambler. More generally, a MABP; also called the **K-armed
    bandit problem**) is the problem of allocating resources between competing choices
    in a situation, where the results of each choice are only partially known, but
    may become better known over time. When observations of the world are considered
    when making a decision, this is known as the **contextual bandit**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用户对网页的响应框架化为多臂赌博问题。**多臂赌博**（**MABP**）是一种投币并拉动多个杠杆之一的老虎机，每个杠杆与不同的奖励分布相关联，而这对投资者来说是未知的。更普遍地说，**多臂赌博问题**（也称为**K臂赌博问题**）是在资源在竞争选择之间分配的情况下，每个选择的结果仅部分已知，但随着时间的推移可能会更好地了解。当在做出决策时考虑世界的观察结果时，这被称为**上下文赌博**。
- en: We've used the **Upper Confidence Bound version 1** (**UCB1**) algorithm (Auer
    et al., *Finite-time analysis of the multi-armed bandit problem*, 2002), which
    is easy to implement.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了**置信上界版本1**（**UCB1**）算法（Auer等人，*有限时间分析多臂赌博问题*，2002年），这个算法易于实现。
- en: 'It works as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 运行方式如下：
- en: Play each action once in order to get initial estimates for the mean rewards
    (exploration phase).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获取平均奖励的初始估计值（探索阶段），每个动作都要执行一次。
- en: 'For each round *t* update *Q(a)* and *N(a)*, and play the action *a''* according
    to this formula:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每一轮 *t* 更新 *Q(a)* 和 *N(a)*，并根据以下公式执行动作 *a'*：
- en: '![](img/3d6cfe28-5d39-426b-bba6-64da27e46efa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d6cfe28-5d39-426b-bba6-64da27e46efa.png)'
- en: where ![](img/b34651d6-def9-4521-ab65-c9107bdadff6.png) is the lookup table
    for the mean reward and ![](img/e18455df-dace-4623-81ff-ac440e5fc909.png) is the
    number of times action ![](img/781a087f-89c9-4a59-b207-b8424266740d.png) has been
    played. ![](img/7db2992e-104b-42fa-bc09-a47207dd8c67.png) is a parameter.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/b34651d6-def9-4521-ab65-c9107bdadff6.png) 是平均奖励的查找表， ![](img/e18455df-dace-4623-81ff-ac440e5fc909.png) 是动作 ![](img/781a087f-89c9-4a59-b207-b8424266740d.png) 被执行的次数， ![](img/7db2992e-104b-42fa-bc09-a47207dd8c67.png) 是参数。
- en: UCB algorithms follow the so-called optimism in the face of uncertainty principle
    by choosing the arm with the highest UCB on its confidence interval rather than
    the one with the highest estimated reward. It uses a naive mean estimator for
    the action reward.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: UCB算法遵循在不确定性面前保持乐观的原则，通过选择在其置信区间上UCB最高的臂而不是估计奖励最高的臂来执行动作。它使用简单的均值估计器来估算动作奖励。
- en: The second term in the preceding equation quantifies the uncertainty. The lower
    the uncertainty, the more we rely on *Q(a)*. The uncertainty decreases linearly
    with the number of times an action has been played and increases logarithmically
    with the number of rounds.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程式中的第二项量化了不确定性。不确定性越低，我们越依赖 *Q(a)*。不确定性随着动作播放次数的增加而线性减少，并随着轮数的对数增加而对数增加。
- en: Multi-armed bandits are useful in many domains, including online advertising,
    clinical trials, network routing, or switching between two or more versions of
    a machine learning model in production.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博在许多领域中非常有用，包括在线广告、临床试验、网络路由或在生产中两个或多个版本的机器学习模型之间的切换。
- en: There are many variants of the bandit algorithm that address more complex scenarios,
    for example, costs for switching between choices, or choices with finite lifespans
    such as the secretary problem. The basic setting of the secretary problem is that
    you want to hire a secretary from a finite pool of applicants. Each applicant
    is interviewed in turn in random order, and a definite decision (to hire or not)
    is to be made immediately after the interview. The secretary problem is also called
    the marriage problem.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多变体的赌博算法来处理更复杂的场景，例如，选择之间切换的成本，或者具有有限生命周期的选择，例如秘书问题。秘书问题的基本设置是你想从一个有限的申请者池中雇佣一名秘书。每位申请者按随机顺序进行面试，面试后立即做出明确的决定（是否雇佣）。秘书问题也被称为婚姻问题。
- en: See also
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The Ax library implements many bandit algorithms in Python: [https://ax.dev/](https://ax.dev/).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Ax库在Python中实现了许多赌徒算法：[https://ax.dev/](https://ax.dev/)。
- en: Facebook's PlanOut is a library for massive online field experiments: [https://facebook.github.io/planout/index.html](https://facebook.github.io/planout/index.html).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的PlanOut是一个用于大规模在线实验的库：[https://facebook.github.io/planout/index.html](https://facebook.github.io/planout/index.html)。
- en: 'As reading material we''d recommend these:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为阅读材料，我们建议这些书籍：
- en: Russo and others, 2007, *A Tutorial on Thompson Sampling* ([https://arxiv.org/pdf/1707.02038.pdf](https://arxiv.org/pdf/1707.02038.pdf))
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russo等人，2007年，《关于汤普森采样的教程》([https://arxiv.org/pdf/1707.02038.pdf](https://arxiv.org/pdf/1707.02038.pdf))
- en: 'Szepesvari and Lattimore, *Bandit Algorithms*, 2020 (online version available:
    ([https://tor-lattimore.com/downloads/book/book.pdf](https://tor-lattimore.com/downloads/book/book.pdf))'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szepesvari和Lattimore，《赌徒算法》，2020年（在线版本可用：([https://tor-lattimore.com/downloads/book/book.pdf](https://tor-lattimore.com/downloads/book/book.pdf))
- en: Controlling a cartpole
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制一个倒立摆
- en: The cartpole is a control task available in OpenAI Gym, and has been studied
    for many years. Although it is relatively simple compared to others, it contains
    all that we need in order to implement a reinforcement learning algorithm, and
    everything that we develop here can be applied to other, more complex learning
    tasks. It can also serve as an example of robotic manipulation in a simulated
    environment. The advantage of taking one of the less demanding tasks is that training
    and turnaround is quicker.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 倒立摆是OpenAI Gym中的一个控制任务，已经研究了多年。虽然与其他任务相比相对简单，但它包含了我们实施强化学习算法所需的一切，我们在这里开发的一切也可以应用于其他更复杂的学习任务。它还可以作为在模拟环境中进行机器人操作的示例。选择一个不那么苛刻的任务的好处在于训练和反馈更快。
- en: '**OpenAI Gym** is an open source library that can help to develop reinforcement
    algorithms by standardizing a broad range of environments for agents to interact
    with. OpenAI Gym comes with hundreds of environments and integrations ranging
    from robotic control, and walking in 3D to computer games and self-driving cars: [https://gym.openai.com/](https://gym.openai.com/).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenAI Gym**是一个开源库，可以通过为代理与之交互的广泛环境标准化，帮助开发强化学习算法。OpenAI Gym提供了数百个环境和集成，从机器人控制和三维行走到电脑游戏和自动驾驶汽车：[https://gym.openai.com/](https://gym.openai.com/)。'
- en: 'The cartpole task is depicted in the following screenshot of the OpenAI Gym
    environment and consists of moving a cart to the left or right in order to balance
    a pole in an upright position:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI Gym环境的以下截图中展示了倒立摆任务，通过将购物车向左或向右移动来平衡一个立杆：
- en: '![](img/6368e342-881c-435a-bf20-f75c1baa6d5e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6368e342-881c-435a-bf20-f75c1baa6d5e.png)'
- en: In this recipe, we'll implement the REINFORCE policy gradient method in PyTorch
    to solve the cartpole task. Let's get to it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用PyTorch实现REINFORCE策略梯度方法来解决倒立摆任务。让我们开始吧。
- en: Getting ready
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'There are many libraries that provide collections of test problems and environments.
    One of the libraries with the most integrations is OpenAI Gym, which we''ll utilize
    in this recipe:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多库提供了测试问题和环境的集合。其中一个集成最多的库是OpenAI Gym，我们将在这个示例中使用它：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now use OpenAI Gym in our recipe.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在我们的示例中使用OpenAI Gym了。
- en: How to do it...
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施方法...
- en: OpenAI Gym saves us work—we don't have to define the environment ourselves and
    come up with reward signals, encode the environment, or state which actions are
    allowed.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym为我们节省了工作——我们不必自己定义环境，确定奖励信号，编码环境或说明允许哪些动作。
- en: 'We''ll first load the environment, define a deep learning policy for action
    selection, define an agent that uses this policy to select actions to execute,
    and finally we''ll test how the agent performs in our task:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载环境，定义一个深度学习策略用于动作选择，定义一个使用此策略来选择执行动作的代理，最后我们将测试代理在我们的任务中的表现：
- en: 'First, we''ll load the environment. Every move that the pole doesn''t fall
    over, we get a reward. We have two available moves, left or right, and an observation
    space that includes a representation of the cart position and velocity and the
    pole angle and velocity, as in the following table:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载环境。每次杆子不倒下时，我们都会得到一个奖励。我们有两个可用的移动方式，向左或向右，并且观察空间包括购物车位置和速度的表示以及杆角度和速度，如下表所示：
- en: '| **Num** | **Observation** | **Min** | **Max** |'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| **编号** | **观察值** | **最小值** | **最大值** |'
- en: '| 0 | Cart Position | -2.4 | 2.4 |'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 0 | 购物车位置 | -2.4 | 2.4 |'
- en: '| 1 | Cart Velocity | -Inf | -Inf |'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 1 | 购物车速度 | -Inf | -Inf |'
- en: '| 2 | Pole Angle | ~ -41.8° | ~ 41.8° |'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2 | 杆角度 | ~ -41.8° | ~ 41.8° |'
- en: '| 3 | Pole Velocity At Tip | -Inf | -Inf |'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 3 | Pole Velocity At Tip | -Inf | -Inf |'
- en: You can find out more about this environment here: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里了解更多关于此环境的信息：[https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)。
- en: 'We can load the environment and print these parameters as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以加载环境并打印这些参数如下：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So, we confirm we have four inputs and two actions that our agent has to deal
    with. Our agent will be defined similarly from the previous recipe, *Optimizing
    a website*, only this time, we will define our neural network outside of the agent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们确认我们有四个输入和两个动作，我们的代理将类似于前面的示例*优化网站*定义，只是这次我们会在代理外部定义我们的神经网络。
- en: The agent will create a policy network and use it to take decisions until an
    end state is reached; then it will feed the cumulative rewards into the network
    to learn. Let's start with the policy network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 代理将创建一个策略网络，并使用它来做出决策，直到达到结束状态；然后将累积奖励馈送到网络中进行学习。让我们从策略网络开始。
- en: 'Let''s create a policy network. We''ll take a fully connected feed-forward
    neural network that predicts moves based on the observation space. This is based
    partly on a PyTorch implementation available at [https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py):'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个策略网络。我们将采用一个全连接的前馈神经网络，根据观察空间预测动作。这部分基于PyTorch实现，可以在[https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)找到：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is a neural network module that learns a policy, or, in other words, a
    mapping from observations to actions. This sets up a two-layer neural network
    with one hidden layer and one output layer, where each neuron in the output layer
    corresponds to one possible action. We set the following parameters:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个神经网络模块，用于学习策略，换句话说，从观察到动作的映射。它建立了一个具有一层隐藏层和一层输出层的两层神经网络，其中输出层中的每个神经元对应于一个可能的动作。我们设置了以下参数：
- en: '`lr`: Learning rate'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`: 学习率'
- en: '`n_inputs`: Number of inputs'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_inputs`: 输入数量'
- en: '`n_hidden`: Number of hidden neurons'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_hidden`: 隐藏神经元数量'
- en: '`n_actions`: Input dimensionality'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_actions`: 输入维度'
- en: 'We are ready now to define our agent:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以定义我们的**代理人**了：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Agents evaluate policies to take actions and get rewards. `gamma` is the discount
    factor.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 代理人评估策略以执行动作并获得奖励。 `gamma` 是折扣因子。
- en: With the `choose_action(self, observation)` method, our agent chooses an action
    given an observation. The action is sampled according to the categorical distribution
    from our network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `choose_action(self, observation)` 方法，我们的代理根据观察选择动作。动作是根据我们网络的分类分布进行抽样。
- en: 'We''ve omitted the `run()` method, which looks as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了`run()` 方法，其内容如下：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `run(self)` method similar to the previous recipe, *Optimizing a website*,
    runs a full simulation in the environment until the end is reached. This is until
    the pole is about to fall over or 500 steps are reached (which is the default
    value of  `env._max_episode_steps`).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`run(self)` 方法类似于之前的示例，*优化网站*，在环境中运行完整的模拟直到结束。这是直到杆几乎倒下或达到500步（即 `env._max_episode_steps`
    的默认值）为止。'
- en: 'Next, we''ll test our agent. We''ll start running our agent in the environment
    by simulating interactions with the environment. In order to get a cleaner curve
    for our learning rate, we''ll set `env._max_episode_steps` to `10000`. This means
    the simulation stops after 10,000 steps. If we''d left it at `500`, the default
    value, the algorithm would plateau at a certain performance or its performance
    would stagnate once about 500 steps are reached. Instead, we are trying to optimize
    a bit more:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将测试我们的代理人。我们将在环境中运行我们的代理人，通过模拟与环境的交互来开始。为了获得我们学习率的更干净的曲线，我们将`env._max_episode_steps`设置为`10000`。这意味着模拟在10000步后停止。如果我们保持默认值`500`，算法将在达到约500步后停滞或性能达到某个水平。相反，我们试图做更多的优化：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We should see the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到以下输出：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'While the simulations are going on we are seeing updates every 100 iterations
    with average scores since the last update. We stop once a score of 1,000 is reached.
    This is our score over time:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行模拟时，我们每100次迭代看到一次更新的平均分数。一旦达到1000分，我们就会停止。这是我们的分数随时间的变化情况：
- en: '![](img/99f6fdcb-3dc2-4054-aaba-a407e01c9993.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99f6fdcb-3dc2-4054-aaba-a407e01c9993.png)'
- en: We can see that our policy is continuously improving—the network is learning
    successfully to manipulate the cartpole. Please note that your results can vary.
    The network can learn more quickly or more slowly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的策略正在持续改进——网络正在成功学习如何操作杆车。请注意，您的结果可能会有所不同。网络可能学习得更快或更慢。
- en: In the next section, we'll get into how this algorithm actually works.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入了解这个算法的实际工作原理。
- en: How it works...
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we've looked at a policy-based algorithm in a cartpole control
    scenario. Let's look at some of this in more detail.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们看了一个杆车控制场景中的基于策略的算法。让我们更详细地看看其中的一些内容。
- en: Policy gradient methods find a policy with a given gradient ascent that maximizes
    cumulative rewards with respect to the policy parameters. We've implemented a
    model-free policy-based method, the REINFORCE algorithm (R. Williams, *Simple
    statistical gradient-following algorithms for connectionist reinforcement learning*,
    1992).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法通过给定的梯度上升找到一个策略，以最大化相对于策略参数的累积奖励。我们实现了一种无模型的基于策略的方法，即 REINFORCE 算法（R.
    Williams，《简单的统计梯度跟随算法用于连接主义强化学习》，1992年）。
- en: 'At the heart of a policy-based method we have a policy function. The policy
    function is defined over an environment ![](img/e6b6eae1-47c1-478a-b742-0ac3598336cd.png)
    and an action ![](img/c5de2c55-5cb2-46e2-a90b-05143184484f.png), and returns the
    probability of an action given an environment. In the case of ![](img/f6669627-9c78-4ba4-b934-f1369dffd8f9.png)
    discrete choices, we can use the softmax function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，我们有一个策略函数。策略函数定义在一个环境 ![](img/e6b6eae1-47c1-478a-b742-0ac3598336cd.png)
    和一个动作 ![](img/c5de2c55-5cb2-46e2-a90b-05143184484f.png) 上，并返回给定环境下执行动作的概率。在离散选择的情况下，我们可以使用
    softmax 函数：
- en: '![](img/bd1ada1f-8111-43b9-b502-e5887ebbe58e.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd1ada1f-8111-43b9-b502-e5887ebbe58e.png)'
- en: This is what we've done in our policy network, and this helps us to make our
    action choice.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们在策略网络中所做的，这有助于我们做出我们的动作选择。
- en: 'The value function ![](img/589622ea-134f-422e-83fd-dde2b7f8bc4e.png) (sometimes
    denoted by ![](img/080a47ab-3d88-49c8-bdcb-4208df48cb30.png)) returns the reward
    for any action in a given environment. The update to the policy is defined as
    follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数 ![](img/589622ea-134f-422e-83fd-dde2b7f8bc4e.png)（有时用 ![](img/080a47ab-3d88-49c8-bdcb-4208df48cb30.png)
    表示）返回给定环境中任何动作的奖励。策略的更新定义如下：
- en: '![](img/bbb94709-9aa8-43cd-ab49-642190d0fe0c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbb94709-9aa8-43cd-ab49-642190d0fe0c.png)'
- en: where ![](img/c0ca3cce-5a56-4fff-99e0-87274906c240.png) is the learning rate.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/c0ca3cce-5a56-4fff-99e0-87274906c240.png) 是学习率。
- en: After the initialization of the parameters, the REINFORCE algorithm proceeds
    by applying this update function each time an action is executed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数初始化之后，REINFORCE 算法通过每次执行动作时应用此更新函数来进行。
- en: You should be able to run our implementation on any Gym environment with few
    to no changes. We've deliberately put in a few things (for example, reshaping
    observations to a vector) to make it easier to reuse it; however, you should make
    sure that your network architecture corresponds to the nature of your observations.
    For example, you might want to use a 1D convolutional network or a recurrent neural
    network for time series (such as in stock trading or sounds) or 2D convolutions
    if your observations are images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能够在任何 Gym 环境上运行我们的实现，几乎不需要更改。我们故意做了一些事情（例如，将观测数据重塑为向量），以便更容易重用它；但是，请确保您的网络架构与观测数据的性质相对应。例如，如果您的观测数据是时间序列（例如股票交易或声音），您可能希望使用
    1D 卷积网络或递归神经网络，或者如果您的观测数据是图像，则可以使用 2D 卷积。
- en: There's more...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: There are a few more things that we can play around with. For one, we'd like
    to see the agent interacting with the pole, and secondly, instead of implementing
    an agent from scratch, we can use a library.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的事情可以让我们玩得更开心。首先，我们想看到代理与杆互动，其次，我们可以使用库来避免从零开始实施代理。
- en: Watching our agents in the environment
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察我们在环境中的代理
- en: 'We can play many hundreds of games or try different control tasks. If we want
    to actually watch our agent interact with the environment in a Jupyter notebook,
    we can do it:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以玩数百场游戏或尝试不同的控制任务。如果我们真的想在 Jupyter 笔记本中观看我们的代理与环境的互动，我们可以做到：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We should see our agent interacting with the environment now.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该看到我们的代理与环境互动了。
- en: 'If you are on a remote connection (such as running on Google Colab), you might
    have to do some extra work:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在远程连接（例如在 Google Colab 上运行），您可能需要做一些额外的工作：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the next section, we'll use a reinforcement algorithm that's implemented
    in a library, `RLlib`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用一个实现在库中的强化学习算法，`RLlib`。
- en: Using the RLlib library
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用RLlib库
- en: 'Instead of implementing algorithms from scratch, we can make use of implementations
    in Python libraries and packages. For example, we can train the PPO algorithm
    (Schulman et al., *Proximal Policy Optimization Algorithms*, 2017), which comes
    with the `RLlib` package. RLlib is part of the `Ray` library that we encountered
    in the *Getting started with Artificial Intelligence in Python *recipe in [Chapter
    1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting Started with Artificial
    Intelligence in Python*. PPO is a policy gradient method that introduces a surrogate
    objective function that can be optimized with gradient descent:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python库和包中的实现，而不是从头开始实现算法。例如，我们可以训练PPO算法（Schulman等人，《Proximal Policy Optimization
    Algorithms》，2017），该算法包含在`RLlib`包中。 RLlib是我们在《Python人工智能入门》第1章中遇到的`Ray`库的一部分。 PPO是一种政策梯度方法，引入了一个替代目标函数，可以通过梯度下降进行优化：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will run the training. Your agents will be stored in a local directory,
    so you can load them up later. RLlib lets you use PyTorch and TensorFlow with
    the `''torch'': True` option.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '这将开始训练。 您的代理将存储在本地目录中，以便稍后加载它们。 RLlib允许您使用`''torch'': True`选项来使用PyTorch和TensorFlow。'
- en: See also
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Some reinforcement libraries come with lots of implementations of deep reinforcement
    learning algorithms:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一些强化学习库提供了许多深度强化学习算法的实现：
- en: OpenAI's Baselines (needs TensorFlow version < 2): [https://github.com/openai/baselines](https://github.com/openai/baselines)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI的Baselines（需要TensorFlow版本 < 2）：[https://github.com/openai/baselines](https://github.com/openai/baselines)
- en: RLlib (part of `Ray`) is a library for scalable (and distributed) reinforcement
    learning: [https://docs.ray.io/en/master/rllib-algorithms.html](https://docs.ray.io/en/master/rllib-algorithms.html).
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLlib（`Ray`的一部分）是一个可扩展（和分布式）强化学习库：[https://docs.ray.io/en/master/rllib-algorithms.html](https://docs.ray.io/en/master/rllib-algorithms.html)。
- en: Machin is a reinforcement learning library based on PyTorch: [https://github.com/iffiX/machin](https://github.com/iffiX/machin).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Machin是基于PyTorch的强化学习库：[https://github.com/iffiX/machin](https://github.com/iffiX/machin)。
- en: TF-Agents provides many tools and algorithms (and it works with TensorFlow version
    2.0 and later): [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-Agents提供了许多工具和算法（适用于TensorFlow版本2.0及更高版本）：[https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)。
- en: Please note that the installation of these libraries can take a while and might
    take up gigabytes of your hard disk.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，安装这些库可能需要一段时间，并可能占用几个GB的硬盘空间。
- en: 'Finally, OpenAI provides a repository with many educational resources related
    to reinforcement learning: [https://spinningup.openai.com/](https://spinningup.openai.com/).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，OpenAI提供了一个与强化学习相关的教育资源库：[https://spinningup.openai.com/](https://spinningup.openai.com/)。
- en: Playing blackjack
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩21点游戏
- en: One of the benchmarks in reinforcement learning is gaming. Many different environments
    related to gaming have been designed by researchers or aficionados. A few of the
    milestones in gaming have been mentioned in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml),
    *Getting Started with Artificial Intelligence in Python*. The highlights for many
    would certainly be beating the human champions in both chess and Go—chess champion
    Garry Kasparov in 1997 and Go champion Lee Sedol in 2016—and reaching super-human
    performance in Atari games in 2015.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的一个基准是游戏。 研究人员或爱好者设计了许多与游戏相关的不同环境。 有些游戏的里程碑已经在《Python人工智能入门》第1章中提到。 对许多人来说，游戏的亮点肯定是在国际象棋和围棋两方面击败人类冠军——1997年国际象棋冠军加里·卡斯帕罗夫和2016年围棋冠军李世石——并在2015年达到超人类水平的Atari游戏中表现出色。
- en: 'In this recipe, we get started with one of the simplest game environments:
    blackjack. Blackjack has an interesting property that it has in common with the
    real world: indeterminism.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们开始使用最简单的游戏环境之一：21点游戏。 21点游戏与现实世界有一个有趣的共同点：不确定性。
- en: Blackjack is a card game where, in its simplest form, you play against a card
    dealer. You have a deck of cards in front of you, and you can hit, which means
    you get one more card, or stick, where the dealer gets to draw cards. In order
    to win, you want to get as close as possible to a card score of 21, but not surpass
    21.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement a model in Keras of the value of different actions
    given a configuration of the environment, a value function. The variant we'll
    implement is called the DQN, which was used in the 2015 Atari milestone achievement.
    Let's get to it.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to install a dependency if you haven't installed it yet.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using OpenAI Gym, and we need to have it installed:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We'll be using the Gym environment for blackjack.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need an agent that maintains a model of what effects its actions have. These
    actions are played back from its memory for the purpose of learning. We will start
    with a memory that records past experiences for learning:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement this memory. This memory is essentially a FIFO queue. In Python,
    you could use a deque; however, we found the implementation of the replay memory
    in the PyTorch examples very elegant, so this is based on Adam Paszke''s PyTorch design:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We only really need two methods:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: We need to push new memories, overwriting old ones in the process if our capacity
    is reached.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to sample memories for learning.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The latter point is worth stressing: instead of using all the memories for
    learning, we only take a part of them.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: In the `sample()` method, we made a few alterations to get our data in the right
    shape.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our agent:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Please note the action choice at the beginning of the `play()` method. We throw
    a die to determine if we want to choose an action randomly or if we want to follow
    our model's judgment. This is called an **epsilon-greedy action selection**, which
    leads to more exploration and better adaptation to the environment.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent comes with a few hyperparameters to configure:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '`lr`: The learning rate of the network.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: The batch size for the sampling from memory and network training.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon`: This factor, between `0` and `1`, controls how much randomness we
    want in our responses. `1` means random exploration, `0` means no exploration
    at all (exploitation).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that these three parameters can significantly change the trajectory
    of our learning.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve omitted a method from the listing, which defines the neural network
    model:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is a three-layer neural network with two hidden layers, one with 100 neurons
    and the other with 2 neurons, that come with ReLU activations, and an output layer
    with 1 neuron.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the environment and initialize our agent. We initialize our agent
    and the environment as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This loads the **Blackjack** OpenAI Gym environment and our **DQNAgent** as
    implemented in *Step 2* of this recipe.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: The `epsilon` parameter defines the random behavior of the agent. We don't want
    to set this too low. The learning rate is a value we are choosing after experimenting.
    Since we are playing a stochastic card game, if we set it too high, the algorithm
    will unlearn very quickly. The batch size and memory size parameters are important
    for how much we train at every step and how much we remember about the history
    of rewards, respectively.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`epsilon` 参数定义了代理的随机行为。我们不希望将其设置得太低。学习率是我们在实验后选择的值。由于我们在进行随机卡牌游戏，如果设置得太高，算法将会非常快速地遗忘。批处理大小和记忆大小参数分别决定了每一步的训练量以及关于奖励历史的记忆。'
- en: 'We can see the structure of this network (as shown by Keras''s `summary()`
    method):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这个网络的结构（由Keras的`summary()`方法显示）。
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: For the simulation, one of our key questions is the value of the `epsilon` parameter.
    If we set it too low, our agent won't learn anything; if we set it too high, we'd
    lose money because the agent makes random moves.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模拟来说，我们的关键问题之一是`epsilon`参数的值。如果设置得太低，我们的代理将无法学到任何东西；如果设置得太高，我们将会因为代理做出随机动作而亏钱。
- en: 'Now let''s play blackjack. We chose to steadily decrease `epsilon` in a linear
    fashion, and then we exploit for a number of rounds. When `epsilon` reaches `0`,
    we stop learning:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们来玩21点吧。我们选择以线性方式稳定减少`epsilon`，然后在一定数量的回合内进行利用。当`epsilon`达到`0`时，我们停止学习：
- en: '[PRE21]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This is the code that actually starts playing blackjack:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实际开始玩21点的代码：
- en: '[PRE22]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can see we are collecting statistics of the network training loss to monitor
    during simulations, and we collect the the maximum payout over any 100 consecutive
    plays.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟过程中，我们收集了网络训练损失的统计数据，并且收集了连续100次游戏中的最大津贴。
- en: 'In OpenAI Gym, the rewards, or if we want to stay within the terminology of
    blackjack, the payouts, can be either -1 (we lost), 0 (nothing), or 1 (we win).
    The payouts over time with our learned strategy look as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI Gym中，奖励或者如果我们想保持21点的术语，就是津贴，可以是-1（我们输了），0（什么也没有），或者1（我们赢了）。我们使用学习的策略随时间的津贴如下所示：
- en: '![](img/b85f5091-03ed-459e-89a4-74ec5bd1a1a4.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b85f5091-03ed-459e-89a4-74ec5bd1a1a4.png)'
- en: Because of the huge variability, rather than showing the raw data, we've plotted
    this with moving averages of 100 and 1,000, which results in two lines, one that
    is highly variable and another that is smooth, as you can see in the graph.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于巨大的可变性，我们并未展示原始数据，而是绘制了移动平均线，分别为100和1,000，结果呈现两条线：一条高度变化，另一条平滑，如图所示。
- en: We do see an increase in the payouts over time; however, we are still below
    0, which means we lose money on average. This happens even if we stop learning
    in the exploitation phase.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间推移，我们确实看到了津贴的增加；然而，平均来看我们仍然亏损。即使在停止学习的开发阶段也会发生这种情况。
- en: Our blackjack environment does not have a reward threshold at which it's considered
    solved; however, a write-up lists 100 best episodes with an average of 1.0, which
    is what we reach as well: [https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/](https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的21点环境没有奖励阈值，认为达到此阈值即解决问题；但是，一篇报道列出了100个最佳剧集，平均为1.0，这也是我们达到的：[https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/](https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/)。
- en: How it works...
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we've seen a more advanced algorithm in reinforcement learning,
    more specifically, a value-based algorithm. In value-based reinforcement learning,
    algorithms build an estimator of the value function, which, in turn, lets us choose
    a policy.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们看到了强化学习中更高级的算法，更具体地说是一种基于价值的算法。在基于价值的强化学习中，算法构建了价值函数的估计器，进而让我们选择策略。
- en: The agent deserves a few more comments. If you've read the previous recipe,
    *Controlling a cartpole*, you might think there's really not that much going on—there's
    a network, a `play()` method to decide between actions, and a `learn()` method.
    The code is relatively small. A basic threshold strategy (do my cards sum to 17?)
    is already quite successful, but hopefully what we show in this recipe can still
    be instructive and helpful for more complex use cases. As opposed to the policy
    network we've seen before, this time, rather than suggesting the best action directly,
    the network takes the combination of environment and action as an input and outputs
    the expected reward. Our model is a feed-forward model of two layers, where the
    hidden layer with two neurons get summed in the final layer composed of a single
    neuron. The agent plays in an epsilon-greedy fashion—it makes a random move with
    probability `epsilon`; otherwise it makes the best move according to its knowledge.
    The `play` function suggests the action that has the highest utility by comparing
    expected outcomes over all available actions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q-value function, ![](img/d733147e-8d36-4d57-876e-4e5fdf87333e.png), is
    defined as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51024323-ff6d-4422-8ea4-9b27fbeedc79.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: where ![](img/17a83479-e49b-49a6-8e1f-44a678fcda45.png) are the reward, state,
    and action at time ![](img/55e196a8-e2ee-400b-ad0d-7a77a02bd949.png). ![](img/90241e7a-a629-4877-ab21-16b6c7fb5ab6.png) is
    the discount factor; the policy ![](img/e1322055-52a6-4124-b760-ac3c65497713.png) selects
    the actions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest case, ![](img/39e662a3-4545-4d6c-8ad5-096eb82d378e.png) can
    be a lookup table with an entry for every state-action pair.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimal Q-value function is defined as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43e80260-758c-4855-84fc-685a049d5ec6.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'And, consequently, the best policy can be determined according to this formula:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ea8b645-489f-42ab-ade1-0629413ee7ca.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'In **neural fitted Q-learning **(**NFQ**) (Riedmiller, *Neural Fitted Q Iteration
    – First Experiences with a Data Efficient Neural Reinforcement Learning Method*,
    2005), a neural network runs a forward pass given a state with the outputs corresponding
    to available actions. The neural Q-value function can be updated with gradient
    descent according to a squared error:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50720e42-6f66-4174-bfb3-4115291522f8.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: '![](img/dcb166d9-47c6-4b5a-aa89-bd3ba887275c.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: where ![](img/84083e9d-cbc5-435f-bbae-54a060914a89.png) refers to the parameters
    at iteration ![](img/25509c06-94fd-4d01-8fce-70c9bd872a88.png), and ![](img/ff3f98a7-5e15-4a58-9d7c-b14ec97e168f.png) refer
    to action and state at the next time step.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The DQN (Mnih et al., *Playing Atari with Deep Reinforcement Learning*, 2015)
    builds on NFQ, but introduces a few changes. These include updating parameters
    only in mini-batches every few iterations, based on random samples from a replay
    memory. Since, in the original paper, the algorithm learned from pixel values
    on the screen, the first layers of the network are convolutional (we'll introduce
    these in [Chapter 7](f386de9e-b56d-4b39-bf36-803860def385.xhtml), *Advanced Image
    Applications*).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: DQN（Mnih等人，《使用深度强化学习玩Atari游戏》，2015年）基于NFQ进行了一些改进。这些改进包括仅在几次迭代中的小批量更新参数，基于来自重播记忆的随机样本。由于在原始论文中，该算法从屏幕像素值学习，网络的第一层是卷积层（我们将在[第7章](f386de9e-b56d-4b39-bf36-803860def385.xhtml)，*高级图像应用*中介绍）。
- en: See also
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Here is the website for Sutton and Barto''s seminal book *Reinforcement Learning:
    An Introduction*: [http://incompleteideas.net/book/the-book-2nd.html](http://incompleteideas.net/book/the-book-2nd.html).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Sutton和Barto的开创性著作《强化学习导论》的网站：[http://incompleteideas.net/book/the-book-2nd.html](http://incompleteideas.net/book/the-book-2nd.html)。
- en: 'They''ve described a simple agent for blackjack in there. If you are looking
    for other card games, you can have a look at neuron-poker, an OpenAI poker environment;
    they''ve implemented DQN and other algorithms: [https://github.com/dickreuter/neuron_poker](https://github.com/dickreuter/neuron_poker).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在那里描述了一个简单的21点游戏代理。如果您正在寻找其他卡牌游戏，可以查看neuron-poker，这是一个OpenAI扑克环境；他们实现了DQN和其他算法：[https://github.com/dickreuter/neuron_poker](https://github.com/dickreuter/neuron_poker)。
- en: For more details about the DQNs and how to use it, we recommend reading Mnih
    et al.'s article, *Playing Atari with Deep Reinforcement Learning*: [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DQN及其使用的更多细节，我们建议阅读Mnih等人的文章，《使用深度强化学习玩Atari游戏》：[https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)。
- en: Finally, the DQN and its successors, the Double DQN and Dueling DQNs form the
    basis for AlphaGo, which has been published as *Mastering the game of Go without
    human knowledge* (Silver and others, 2017) in Nature: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，DQN及其后继者，双DQN和对决DQN，构成了AlphaGo的基础，该成果发表于《自然》杂志（Silver等人，2017年），题为《无需人类知识掌握围棋》：[https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)。
