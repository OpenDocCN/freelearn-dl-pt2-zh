- en: Deep Reinforcement Learning
  prefs: []
  type: TYPE_NORMAL
- en: '**Reinforcement learning** is about developing goal-driven agents to automate
    problem-solving by optimizing their actions within an environment. This involves
    predicting and classifying the available data and training agents to execute tasks
    successfully. Generally, an agent is an entity that has the capacity to interact
    with an environment, and the learning is done by applying feedback in terms of
    cumulative rewards from the environment to inform future actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three different types of reinforcement learning can be distinguished:'
  prefs: []
  type: TYPE_NORMAL
- en: Value-based—a value function provides an estimate of how good the current state
    of the environment is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy-based—where a function determines an action based on a state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-based—a model of the environment including state transitions, rewards,
    and action planning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we'll start with a relatively basic use case of reinforcement
    learning for website optimization with multi-armed bandits, where we'll look at
    an agent and an environment, and how they interact. Then we'll move on to a simple
    demonstration of control, where it gets a bit more complex, and we'll get to see
    an agent environment and a policy-based method, REINFORCE. Finally, we'll learn
    how to play blackjack, where we'll use a **deep Q network** (**DQN**), a value-based
    algorithm that was used in the wave-making AI that could play Atari games created
    by DeepMind in 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling a cartpole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing blackjack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The full notebooks are available online on GitHub: [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter06](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing a website
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll deal with website optimization. Often, it is necessary
    to try changes (or better, a single change) on a website to see the effect they
    will have. In a typical scenario of what's called an **A/B test**, two versions
    of the website will be compared systematically. An *A/B* test is conducted by showing
    versions *A* and *B* of a web page to a pre-determined number of users. Later,
    statistical significance or a confidence interval is calculated in order to quantify
    the differences in click-through rates, with the goal of deciding which of the
    two web page variants to keep.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we'll look at website optimization from a reinforcement point of view,
    where for each view (or user loading the page), we choose the best version given
    the available data at the time when they load the website. After each piece of
    feedback (click or no click), we update the statistics. In comparison to A/B testing,
    this procedure can yield a more reliable outcome, and additionally we show the
    best web page variant more often over time. Please note that we are not limited
    to two variants, but we can compare many variants.
  prefs: []
  type: TYPE_NORMAL
- en: This example use case of website optimization will help us to introduce the
    notions of agent and environment, and show us the trade-off between exploration
    and exploitation. We'll explain these concepts in the *How it works...* section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement our recipe, we''ll need two components:'
  prefs: []
  type: TYPE_NORMAL
- en: Our agent is decides which web page to present to the user.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment is a test bed that will give our agent feedback (click or no
    click).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since we are only using standard Python, we don''t need to install anything,
    and we can delve right into implementing our recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll implement our environment first. We are considering this as a multi-armed
    bandit problem, which we''ll be explaining in the *How it works...* section. Consequently,
    we''ll call our environment `Bandit`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This bandit is initialized with the number of available choices, `K`. This will
    set up a probability for each of these choices to get a click. In practice, the
    environment would be real user feedback; here, we simulate user behavior instead. The
    `play()` method plays the `ith` machine, and returns a reward of `1` or `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to interact with this environment. This is where our agent comes
    in. The agent has to make decisions, and we''ll give it a strategy to make decisions.
    We''ll include metrics collection as well. An abstract agent looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Any agent will need an environment to interact with. It needs to make a single
    decision (`run_one_step(self)`), and, in order to see how good its decision-making
    is, we'll need to run a simulation (`run(self, n_steps)`).
  prefs: []
  type: TYPE_NORMAL
- en: Agents will contain a lookup list of metric functions and also inherit a metric
    collection functionality. We can run the metrics collection through the `run_metrics(self,
    i)` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we use here is called `UCB1`. We''ll explain this strategy
    in the *How to do it...* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Our `UCB1` agent needs an environment (a bandit) to interact with, and a single
    parameter alpha, which weighs the importance of exploring actions (versus exploiting
    the best known action). The agent maintains its history of choices over time,
    and a record of estimates for each possible choice.
  prefs: []
  type: TYPE_NORMAL
- en: What we should look at is the `run_one_step(self)` method, which makes a single
    choice by picking the best optimistic choice. The `run(self, n_step)` method runs
    a series of choices and gets back the feedback from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s track two metrics: regret, which is the sum of expected losses occurred
    because of suboptimal choices, and—as a measure of convergence of the agent''s
    estimates against the actual configuration of the environment—the Spearman rank
    correlation (`stats.spearmanr()`).'
  prefs: []
  type: TYPE_NORMAL
- en: The **Spearman rank correlation** is equal to the Pearson correlation (often
    briefly called just the correlation or product-moment correlation) of the ranked
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pearson correlation between two variables, ![](img/10c6e91b-16bd-4c97-888b-61a03d42d28d.png)
    and ![](img/89063572-b6e5-4e77-9352-47c3cf415b34.png), can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/857a991f-0184-48f7-830c-3cf25c3b0d13.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/fe3ffd1f-afae-4547-863d-976bfb6287b0.png) is the covariance of
    ![](img/ae4fef2a-7e6e-4d50-8b18-a1e0c2e5f225.png) and ![](img/6cf6cdb6-4a76-4c2b-8b9b-9bbd8c22cabb.png), and ![](img/fb5c045d-c5b6-42bd-b5ba-be53ab12c994.png) is
    the standard deviation of ![](img/2ece40ee-148d-44a7-aa65-fafddde4480c.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spearman correlation, instead of operating on the raw scores, is calculated
    on the ranked scores. A rank transformation means that a variable is sorted by
    value, and each entry is assigned its order. Given a rank ![](img/e2246e67-fd3c-48de-8dad-6e80a1d6ecc2.png)of
    point *i* in *X*, the Spearman rank correlation is calculated like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39263887-f11f-43a1-a741-a6ef20e92c22.png)'
  prefs: []
  type: TYPE_IMG
- en: This assesses how well the relationship between two variables can be described
    as a monotonic, but not necessarily linear (as in the case of Pearson correlation)
    function. Like the Pearson correlation, the Spearman correlation ranges between
    ![](img/4a6caf8b-67d4-4b56-950d-674d0d04c4d9.png) for perfectly negatively correlated
    and ![](img/5ac1aeff-9205-43a0-bf15-36df665dcbd7.png) for perfectly correlated.
    ![](img/e8592acc-720d-4807-8112-d31a687a20fb.png) means there's no correlation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our tracking functions are `update_regret()` and `update_rank_corr()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now track these metrics in order to compare the influence of the `alpha` parameter
    (more or less exploration). We can then observe convergence and cumulative regret
    over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'So we have 20 different choices of web pages, and we collect `regret` and `corr`
    as defined, and we run for `5000` iterations. If we plot this, we can get an idea
    of how well this agent performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b8966ed-fc1d-42fa-938b-3f25b488b80a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the second run, we''ll change alpha to `0.5`, so we''ll do less exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/773a06ca-1f83-4b15-a55d-891f1d0f067b.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the cumulative regret with *alpha=0.5,* less exploration, is
    much lower than with *alpha=2.0*; however, the overall correlation of the estimates
    to the environment parameters is lower.
  prefs: []
  type: TYPE_NORMAL
- en: So, with less exploration our agent models the true parameters of the environment
    less well. This comes from the fact that with less exploration the ordering of
    the lower ranked features has not converged. Even though they are ranked as suboptimal,
    they haven't been chosen often enough to determine whether they are worst or second
    worst, for example. This is what we see with less exploration, and this could
    be fine since we might only care about knowing which choice is best.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we dealt with the problem of website optimization. We simulated
    user choices to different versions of web pages while live-updating statistics
    about how good each variant is, and how often it should be shown. Furthermore, we
    compared the upsides and downsides of an explorative scenario and a more exploitative
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: We framed user responses to web pages as a multi-armed bandit problem. A **multi-armed
    bandit** (**MABP**)is a slot machine where gamblers insert a coin and pull one
    of several levers, each of which is associated with a different reward distribution
    that is unknown to the gambler. More generally, a MABP; also called the **K-armed
    bandit problem**) is the problem of allocating resources between competing choices
    in a situation, where the results of each choice are only partially known, but
    may become better known over time. When observations of the world are considered
    when making a decision, this is known as the **contextual bandit**.
  prefs: []
  type: TYPE_NORMAL
- en: We've used the **Upper Confidence Bound version 1** (**UCB1**) algorithm (Auer
    et al., *Finite-time analysis of the multi-armed bandit problem*, 2002), which
    is easy to implement.
  prefs: []
  type: TYPE_NORMAL
- en: 'It works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Play each action once in order to get initial estimates for the mean rewards
    (exploration phase).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each round *t* update *Q(a)* and *N(a)*, and play the action *a''* according
    to this formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3d6cfe28-5d39-426b-bba6-64da27e46efa.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/b34651d6-def9-4521-ab65-c9107bdadff6.png) is the lookup table
    for the mean reward and ![](img/e18455df-dace-4623-81ff-ac440e5fc909.png) is the
    number of times action ![](img/781a087f-89c9-4a59-b207-b8424266740d.png) has been
    played. ![](img/7db2992e-104b-42fa-bc09-a47207dd8c67.png) is a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: UCB algorithms follow the so-called optimism in the face of uncertainty principle
    by choosing the arm with the highest UCB on its confidence interval rather than
    the one with the highest estimated reward. It uses a naive mean estimator for
    the action reward.
  prefs: []
  type: TYPE_NORMAL
- en: The second term in the preceding equation quantifies the uncertainty. The lower
    the uncertainty, the more we rely on *Q(a)*. The uncertainty decreases linearly
    with the number of times an action has been played and increases logarithmically
    with the number of rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-armed bandits are useful in many domains, including online advertising,
    clinical trials, network routing, or switching between two or more versions of
    a machine learning model in production.
  prefs: []
  type: TYPE_NORMAL
- en: There are many variants of the bandit algorithm that address more complex scenarios,
    for example, costs for switching between choices, or choices with finite lifespans
    such as the secretary problem. The basic setting of the secretary problem is that
    you want to hire a secretary from a finite pool of applicants. Each applicant
    is interviewed in turn in random order, and a definite decision (to hire or not)
    is to be made immediately after the interview. The secretary problem is also called
    the marriage problem.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Ax library implements many bandit algorithms in Python: [https://ax.dev/](https://ax.dev/).
  prefs: []
  type: TYPE_NORMAL
- en: Facebook's PlanOut is a library for massive online field experiments: [https://facebook.github.io/planout/index.html](https://facebook.github.io/planout/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'As reading material we''d recommend these:'
  prefs: []
  type: TYPE_NORMAL
- en: Russo and others, 2007, *A Tutorial on Thompson Sampling* ([https://arxiv.org/pdf/1707.02038.pdf](https://arxiv.org/pdf/1707.02038.pdf))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szepesvari and Lattimore, *Bandit Algorithms*, 2020 (online version available:
    ([https://tor-lattimore.com/downloads/book/book.pdf](https://tor-lattimore.com/downloads/book/book.pdf))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling a cartpole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cartpole is a control task available in OpenAI Gym, and has been studied
    for many years. Although it is relatively simple compared to others, it contains
    all that we need in order to implement a reinforcement learning algorithm, and
    everything that we develop here can be applied to other, more complex learning
    tasks. It can also serve as an example of robotic manipulation in a simulated
    environment. The advantage of taking one of the less demanding tasks is that training
    and turnaround is quicker.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI Gym** is an open source library that can help to develop reinforcement
    algorithms by standardizing a broad range of environments for agents to interact
    with. OpenAI Gym comes with hundreds of environments and integrations ranging
    from robotic control, and walking in 3D to computer games and self-driving cars: [https://gym.openai.com/](https://gym.openai.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cartpole task is depicted in the following screenshot of the OpenAI Gym
    environment and consists of moving a cart to the left or right in order to balance
    a pole in an upright position:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6368e342-881c-435a-bf20-f75c1baa6d5e.png)'
  prefs: []
  type: TYPE_IMG
- en: In this recipe, we'll implement the REINFORCE policy gradient method in PyTorch
    to solve the cartpole task. Let's get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many libraries that provide collections of test problems and environments.
    One of the libraries with the most integrations is OpenAI Gym, which we''ll utilize
    in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now use OpenAI Gym in our recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI Gym saves us work—we don't have to define the environment ourselves and
    come up with reward signals, encode the environment, or state which actions are
    allowed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll first load the environment, define a deep learning policy for action
    selection, define an agent that uses this policy to select actions to execute,
    and finally we''ll test how the agent performs in our task:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll load the environment. Every move that the pole doesn''t fall
    over, we get a reward. We have two available moves, left or right, and an observation
    space that includes a representation of the cart position and velocity and the
    pole angle and velocity, as in the following table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Num** | **Observation** | **Min** | **Max** |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 0 | Cart Position | -2.4 | 2.4 |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 1 | Cart Velocity | -Inf | -Inf |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 2 | Pole Angle | ~ -41.8° | ~ 41.8° |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '| 3 | Pole Velocity At Tip | -Inf | -Inf |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: You can find out more about this environment here: [https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load the environment and print these parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So, we confirm we have four inputs and two actions that our agent has to deal
    with. Our agent will be defined similarly from the previous recipe, *Optimizing
    a website*, only this time, we will define our neural network outside of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: The agent will create a policy network and use it to take decisions until an
    end state is reached; then it will feed the cumulative rewards into the network
    to learn. Let's start with the policy network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a policy network. We''ll take a fully connected feed-forward
    neural network that predicts moves based on the observation space. This is based
    partly on a PyTorch implementation available at [https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a neural network module that learns a policy, or, in other words, a
    mapping from observations to actions. This sets up a two-layer neural network
    with one hidden layer and one output layer, where each neuron in the output layer
    corresponds to one possible action. We set the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lr`: Learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_inputs`: Number of inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_hidden`: Number of hidden neurons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_actions`: Input dimensionality'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are ready now to define our agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Agents evaluate policies to take actions and get rewards. `gamma` is the discount
    factor.
  prefs: []
  type: TYPE_NORMAL
- en: With the `choose_action(self, observation)` method, our agent chooses an action
    given an observation. The action is sampled according to the categorical distribution
    from our network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve omitted the `run()` method, which looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `run(self)` method similar to the previous recipe, *Optimizing a website*,
    runs a full simulation in the environment until the end is reached. This is until
    the pole is about to fall over or 500 steps are reached (which is the default
    value of  `env._max_episode_steps`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll test our agent. We''ll start running our agent in the environment
    by simulating interactions with the environment. In order to get a cleaner curve
    for our learning rate, we''ll set `env._max_episode_steps` to `10000`. This means
    the simulation stops after 10,000 steps. If we''d left it at `500`, the default
    value, the algorithm would plateau at a certain performance or its performance
    would stagnate once about 500 steps are reached. Instead, we are trying to optimize
    a bit more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'While the simulations are going on we are seeing updates every 100 iterations
    with average scores since the last update. We stop once a score of 1,000 is reached.
    This is our score over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/99f6fdcb-3dc2-4054-aaba-a407e01c9993.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that our policy is continuously improving—the network is learning
    successfully to manipulate the cartpole. Please note that your results can vary.
    The network can learn more quickly or more slowly.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll get into how this algorithm actually works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we've looked at a policy-based algorithm in a cartpole control
    scenario. Let's look at some of this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient methods find a policy with a given gradient ascent that maximizes
    cumulative rewards with respect to the policy parameters. We've implemented a
    model-free policy-based method, the REINFORCE algorithm (R. Williams, *Simple
    statistical gradient-following algorithms for connectionist reinforcement learning*,
    1992).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the heart of a policy-based method we have a policy function. The policy
    function is defined over an environment ![](img/e6b6eae1-47c1-478a-b742-0ac3598336cd.png)
    and an action ![](img/c5de2c55-5cb2-46e2-a90b-05143184484f.png), and returns the
    probability of an action given an environment. In the case of ![](img/f6669627-9c78-4ba4-b934-f1369dffd8f9.png)
    discrete choices, we can use the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd1ada1f-8111-43b9-b502-e5887ebbe58e.png)'
  prefs: []
  type: TYPE_IMG
- en: This is what we've done in our policy network, and this helps us to make our
    action choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value function ![](img/589622ea-134f-422e-83fd-dde2b7f8bc4e.png) (sometimes
    denoted by ![](img/080a47ab-3d88-49c8-bdcb-4208df48cb30.png)) returns the reward
    for any action in a given environment. The update to the policy is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbb94709-9aa8-43cd-ab49-642190d0fe0c.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/c0ca3cce-5a56-4fff-99e0-87274906c240.png) is the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: After the initialization of the parameters, the REINFORCE algorithm proceeds
    by applying this update function each time an action is executed.
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to run our implementation on any Gym environment with few
    to no changes. We've deliberately put in a few things (for example, reshaping
    observations to a vector) to make it easier to reuse it; however, you should make
    sure that your network architecture corresponds to the nature of your observations.
    For example, you might want to use a 1D convolutional network or a recurrent neural
    network for time series (such as in stock trading or sounds) or 2D convolutions
    if your observations are images.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few more things that we can play around with. For one, we'd like
    to see the agent interacting with the pole, and secondly, instead of implementing
    an agent from scratch, we can use a library.
  prefs: []
  type: TYPE_NORMAL
- en: Watching our agents in the environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can play many hundreds of games or try different control tasks. If we want
    to actually watch our agent interact with the environment in a Jupyter notebook,
    we can do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We should see our agent interacting with the environment now.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are on a remote connection (such as running on Google Colab), you might
    have to do some extra work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we'll use a reinforcement algorithm that's implemented
    in a library, `RLlib`.
  prefs: []
  type: TYPE_NORMAL
- en: Using the RLlib library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of implementing algorithms from scratch, we can make use of implementations
    in Python libraries and packages. For example, we can train the PPO algorithm
    (Schulman et al., *Proximal Policy Optimization Algorithms*, 2017), which comes
    with the `RLlib` package. RLlib is part of the `Ray` library that we encountered
    in the *Getting started with Artificial Intelligence in Python *recipe in [Chapter
    1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml), *Getting Started with Artificial
    Intelligence in Python*. PPO is a policy gradient method that introduces a surrogate
    objective function that can be optimized with gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run the training. Your agents will be stored in a local directory,
    so you can load them up later. RLlib lets you use PyTorch and TensorFlow with
    the `''torch'': True` option.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some reinforcement libraries come with lots of implementations of deep reinforcement
    learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI's Baselines (needs TensorFlow version < 2): [https://github.com/openai/baselines](https://github.com/openai/baselines)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RLlib (part of `Ray`) is a library for scalable (and distributed) reinforcement
    learning: [https://docs.ray.io/en/master/rllib-algorithms.html](https://docs.ray.io/en/master/rllib-algorithms.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machin is a reinforcement learning library based on PyTorch: [https://github.com/iffiX/machin](https://github.com/iffiX/machin).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-Agents provides many tools and algorithms (and it works with TensorFlow version
    2.0 and later): [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the installation of these libraries can take a while and might
    take up gigabytes of your hard disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, OpenAI provides a repository with many educational resources related
    to reinforcement learning: [https://spinningup.openai.com/](https://spinningup.openai.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Playing blackjack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the benchmarks in reinforcement learning is gaming. Many different environments
    related to gaming have been designed by researchers or aficionados. A few of the
    milestones in gaming have been mentioned in [Chapter 1](87098651-b37f-4b05-b0ee-878193f28b95.xhtml),
    *Getting Started with Artificial Intelligence in Python*. The highlights for many
    would certainly be beating the human champions in both chess and Go—chess champion
    Garry Kasparov in 1997 and Go champion Lee Sedol in 2016—and reaching super-human
    performance in Atari games in 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we get started with one of the simplest game environments:
    blackjack. Blackjack has an interesting property that it has in common with the
    real world: indeterminism.'
  prefs: []
  type: TYPE_NORMAL
- en: Blackjack is a card game where, in its simplest form, you play against a card
    dealer. You have a deck of cards in front of you, and you can hit, which means
    you get one more card, or stick, where the dealer gets to draw cards. In order
    to win, you want to get as close as possible to a card score of 21, but not surpass
    21.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement a model in Keras of the value of different actions
    given a configuration of the environment, a value function. The variant we'll
    implement is called the DQN, which was used in the 2015 Atari milestone achievement.
    Let's get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to install a dependency if you haven't installed it yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll be using OpenAI Gym, and we need to have it installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We'll be using the Gym environment for blackjack.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need an agent that maintains a model of what effects its actions have. These
    actions are played back from its memory for the purpose of learning. We will start
    with a memory that records past experiences for learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement this memory. This memory is essentially a FIFO queue. In Python,
    you could use a deque; however, we found the implementation of the replay memory
    in the PyTorch examples very elegant, so this is based on Adam Paszke''s PyTorch design:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We only really need two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to push new memories, overwriting old ones in the process if our capacity
    is reached.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to sample memories for learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The latter point is worth stressing: instead of using all the memories for
    learning, we only take a part of them.'
  prefs: []
  type: TYPE_NORMAL
- en: In the `sample()` method, we made a few alterations to get our data in the right
    shape.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at our agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Please note the action choice at the beginning of the `play()` method. We throw
    a die to determine if we want to choose an action randomly or if we want to follow
    our model's judgment. This is called an **epsilon-greedy action selection**, which
    leads to more exploration and better adaptation to the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent comes with a few hyperparameters to configure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lr`: The learning rate of the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: The batch size for the sampling from memory and network training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon`: This factor, between `0` and `1`, controls how much randomness we
    want in our responses. `1` means random exploration, `0` means no exploration
    at all (exploitation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that these three parameters can significantly change the trajectory
    of our learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve omitted a method from the listing, which defines the neural network
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is a three-layer neural network with two hidden layers, one with 100 neurons
    and the other with 2 neurons, that come with ReLU activations, and an output layer
    with 1 neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the environment and initialize our agent. We initialize our agent
    and the environment as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This loads the **Blackjack** OpenAI Gym environment and our **DQNAgent** as
    implemented in *Step 2* of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The `epsilon` parameter defines the random behavior of the agent. We don't want
    to set this too low. The learning rate is a value we are choosing after experimenting.
    Since we are playing a stochastic card game, if we set it too high, the algorithm
    will unlearn very quickly. The batch size and memory size parameters are important
    for how much we train at every step and how much we remember about the history
    of rewards, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the structure of this network (as shown by Keras''s `summary()`
    method):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: For the simulation, one of our key questions is the value of the `epsilon` parameter.
    If we set it too low, our agent won't learn anything; if we set it too high, we'd
    lose money because the agent makes random moves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s play blackjack. We chose to steadily decrease `epsilon` in a linear
    fashion, and then we exploit for a number of rounds. When `epsilon` reaches `0`,
    we stop learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the code that actually starts playing blackjack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You can see we are collecting statistics of the network training loss to monitor
    during simulations, and we collect the the maximum payout over any 100 consecutive
    plays.
  prefs: []
  type: TYPE_NORMAL
- en: 'In OpenAI Gym, the rewards, or if we want to stay within the terminology of
    blackjack, the payouts, can be either -1 (we lost), 0 (nothing), or 1 (we win).
    The payouts over time with our learned strategy look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b85f5091-03ed-459e-89a4-74ec5bd1a1a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Because of the huge variability, rather than showing the raw data, we've plotted
    this with moving averages of 100 and 1,000, which results in two lines, one that
    is highly variable and another that is smooth, as you can see in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: We do see an increase in the payouts over time; however, we are still below
    0, which means we lose money on average. This happens even if we stop learning
    in the exploitation phase.
  prefs: []
  type: TYPE_NORMAL
- en: Our blackjack environment does not have a reward threshold at which it's considered
    solved; however, a write-up lists 100 best episodes with an average of 1.0, which
    is what we reach as well: [https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/](https://gym.openai.com/evaluations/eval_21dT2zxJTbKa1TJg9NB8eg/).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we've seen a more advanced algorithm in reinforcement learning,
    more specifically, a value-based algorithm. In value-based reinforcement learning,
    algorithms build an estimator of the value function, which, in turn, lets us choose
    a policy.
  prefs: []
  type: TYPE_NORMAL
- en: The agent deserves a few more comments. If you've read the previous recipe,
    *Controlling a cartpole*, you might think there's really not that much going on—there's
    a network, a `play()` method to decide between actions, and a `learn()` method.
    The code is relatively small. A basic threshold strategy (do my cards sum to 17?)
    is already quite successful, but hopefully what we show in this recipe can still
    be instructive and helpful for more complex use cases. As opposed to the policy
    network we've seen before, this time, rather than suggesting the best action directly,
    the network takes the combination of environment and action as an input and outputs
    the expected reward. Our model is a feed-forward model of two layers, where the
    hidden layer with two neurons get summed in the final layer composed of a single
    neuron. The agent plays in an epsilon-greedy fashion—it makes a random move with
    probability `epsilon`; otherwise it makes the best move according to its knowledge.
    The `play` function suggests the action that has the highest utility by comparing
    expected outcomes over all available actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q-value function, ![](img/d733147e-8d36-4d57-876e-4e5fdf87333e.png), is
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51024323-ff6d-4422-8ea4-9b27fbeedc79.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/17a83479-e49b-49a6-8e1f-44a678fcda45.png) are the reward, state,
    and action at time ![](img/55e196a8-e2ee-400b-ad0d-7a77a02bd949.png). ![](img/90241e7a-a629-4877-ab21-16b6c7fb5ab6.png) is
    the discount factor; the policy ![](img/e1322055-52a6-4124-b760-ac3c65497713.png) selects
    the actions.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest case, ![](img/39e662a3-4545-4d6c-8ad5-096eb82d378e.png) can
    be a lookup table with an entry for every state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimal Q-value function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43e80260-758c-4855-84fc-685a049d5ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, consequently, the best policy can be determined according to this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ea8b645-489f-42ab-ade1-0629413ee7ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In **neural fitted Q-learning **(**NFQ**) (Riedmiller, *Neural Fitted Q Iteration
    – First Experiences with a Data Efficient Neural Reinforcement Learning Method*,
    2005), a neural network runs a forward pass given a state with the outputs corresponding
    to available actions. The neural Q-value function can be updated with gradient
    descent according to a squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50720e42-6f66-4174-bfb3-4115291522f8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/dcb166d9-47c6-4b5a-aa89-bd3ba887275c.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/84083e9d-cbc5-435f-bbae-54a060914a89.png) refers to the parameters
    at iteration ![](img/25509c06-94fd-4d01-8fce-70c9bd872a88.png), and ![](img/ff3f98a7-5e15-4a58-9d7c-b14ec97e168f.png) refer
    to action and state at the next time step.
  prefs: []
  type: TYPE_NORMAL
- en: The DQN (Mnih et al., *Playing Atari with Deep Reinforcement Learning*, 2015)
    builds on NFQ, but introduces a few changes. These include updating parameters
    only in mini-batches every few iterations, based on random samples from a replay
    memory. Since, in the original paper, the algorithm learned from pixel values
    on the screen, the first layers of the network are convolutional (we'll introduce
    these in [Chapter 7](f386de9e-b56d-4b39-bf36-803860def385.xhtml), *Advanced Image
    Applications*).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is the website for Sutton and Barto''s seminal book *Reinforcement Learning:
    An Introduction*: [http://incompleteideas.net/book/the-book-2nd.html](http://incompleteideas.net/book/the-book-2nd.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'They''ve described a simple agent for blackjack in there. If you are looking
    for other card games, you can have a look at neuron-poker, an OpenAI poker environment;
    they''ve implemented DQN and other algorithms: [https://github.com/dickreuter/neuron_poker](https://github.com/dickreuter/neuron_poker).'
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the DQNs and how to use it, we recommend reading Mnih
    et al.'s article, *Playing Atari with Deep Reinforcement Learning*: [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the DQN and its successors, the Double DQN and Dueling DQNs form the
    basis for AlphaGo, which has been published as *Mastering the game of Go without
    human knowledge* (Silver and others, 2017) in Nature: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270).
  prefs: []
  type: TYPE_NORMAL
