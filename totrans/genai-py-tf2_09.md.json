["```py\nFire and Ice by Robert Frost. We'll use these few lines of text to understand how the BoW model works. The following is a step-by-step approach:\n```", "```py\nnltk to clean up this dataset and prepare it for the next steps. The text cleanup process is limited to lowercasing, removing special characters, and stop word removal only:\n```", "```py\n# import statements and code for the function normalize_corpus \n# have been skipped for brevity. See corresponding \n# notebook for details.\ncats = ['alt.atheism', 'sci.space']\nnewsgroups_train = fetch_20newsgroups(subset='train', \n                                      categories=cats,\n                                      remove=('headers', 'footers', \n                                              'quotes'))\nnorm_corpus = normalize_corpus(newsgroups_train.data) \ngensim to train a skip-gram word2vec model:\n```", "```py\n# tokenize corpus\ntokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]\n# Set values for various parameters\nembedding_size = 32  # Word vector dimensionality\ncontext_window = 20  # Context window size\nmin_word_count = 1   # Minimum word count\nsample = 1e-3        # Downsample setting for frequent words\nsg = 1               # skip-gram model\nw2v_model = word2vec.Word2Vec(tokenized_corpus, size=embedding_size, \n                              window=context_window, \n                              min_count =min_word_count,\n                              sg=sg, sample=sample, iter=200) \n```", "```py\n# get word vector\nw2v_model.wv['sun'] \n```", "```py\narray([ 0.607681, 0.2790227, 0.48256198, 0.41311446, 0.9275479,\n       -1.1269532, 0.8191313, 0.03389674, -0.23167856, 0.3170586,\n        0.0094937, 0.1252524, -0.5247988, -0.2794391, -0.62564677,\n       -0.28145587, -0.70590997, -0.636148, -0.6147065, -0.34033248,\n        0.11295943, 0.44503215, -0.37155458, -0.04982868, 0.34405553,\n        0.49197063, 0.25858226, 0.354654, 0.00691116, 0.1671375,\n        0.51912665,  1.0082873 ], dtype=float32) \n```", "```py\n# get similar words\nw2v_model.wv.most_similar(positive=['god']) \n```", "```py\n[('believe', 0.8401427268981934),\n ('existence', 0.8364629149436951),\n ('exists', 0.8211747407913208),\n ('selfcontradictory', 0.8076522946357727),\n ('gods', 0.7966105937957764),\n ('weak', 0.7965559959411621),\n ('belief', 0.7767481803894043),\n ('disbelieving', 0.7757835388183594),\n ('exist', 0.77425217628479),\n ('interestingly', 0.7742466926574707)] \n```", "```py\n# Set values for various parameters\nembedding_size = 32    # Word vector dimensionality\ncontext_window = 20    # Context window size\nmin_word_count = 1   # Minimum word count\nsample = 1e-3        # Downsample setting for frequent words\nsg = 1               # skip-gram model\nft_model = FastText(tokenized_corpus, size=embedding_size, \n                     window=context_window, min_count = min_word_count, sg=sg, sample=sample, iter=100) \n```", "```py\n# out of vocabulary\nft_model.wv['sunny'] \n```", "```py\narray([-0.16000476, 0.3925578, -0.6220364, -0.14427347, -1.308504,\n        0.611941, 1.2834805, 0.5174112, -1.7918613, -0.8964722,\n       -0.23748468, -0.81343293, 1.2371198 , 1.0380564, -0.44239333,\n        0.20864521, -0.9888209, 0.89212966, -1.1963437, 0.738966,\n       -0.60981965, -1.1683533, -0.7930039, 1.0648874, 0.5561004,\n       -0.28057176, -0.37946936, 0.02066167, 1.3181996, 0.8494686,\n       -0.5021836, -1.0629338], dtype=float32) \n```", "```py\ndatafile_path = r'warpeace_2600-0.txt'\n# Load the text file\ntext = open(datafile_path, 'rb').read().decode(encoding='utf-8')\nprint ('Book contains a total of {} characters'.format(len(text))) \n```", "```py\nBook contains a total of 3293673 characters \n```", "```py\nvocab = sorted(set(text))\nprint ('{} unique characters'.format(len(vocab))) \n```", "```py\n108 unique characters \n```", "```py\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ntext_as_int = np.array([char2idx[c] for c in text])\nprint('{')\nfor char,_ in zip(char2idx, range(20)):\n    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\nprint('  ...\\n}') \n```", "```py\n{\n  '\\n':   0,\n  '\\r':   1,\n  ' ' :   2,\n  '!' :   3,\n... \n```", "```py\nseq_length = 100\nexamples_per_epoch = len(text)//(seq_length+1)\n# Create training examples / targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nfor i in char_dataset.take(10):\n    print(idx2char[i.numpy()]) \n```", "```py\nB\nO\nO\nK\n\nO\nN\nE\n... \nsplit_input_target to prepare the target output as a one-position-shifted transformation of the input itself. In this way, we will be able to generate consecutive (input, output) training pairs using just a single shift in position:\n```", "```py\ndef split_input_target(chunk):\n    \"\"\"\n    Utility which takes a chunk of input text and target \n    as one position shifted form of input chunk.\n    Parameters:\n        chunk: input list of words\n    Returns:\n        Tuple-> input_text(i.e. chunk minus \n        last word),target_text(input chunk minus the first word)\n    \"\"\"\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\ndataset = sequences.map(split_input_target)\nfor input_example, target_example in  dataset.take(1):\n    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print ('Target data:', repr(''.join(idx2char[target_example.numpy()]))) \n```", "```py\nInput data:  '\\r\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n\"Well, Prince, so Genoa and Lucca are now just family estat'\nTarget data: '\\nBOOK ONE: 1805\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nCHAPTER I\\r\\n\\r\\n\"Well, Prince, so Genoa and Lucca are now just family estate' \nbuild_model that prepares a single layer LSTM-based language model:\n```", "```py\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n    \"\"\"\n    Utility to create a model object.\n    Parameters:\n        vocab_size: number of unique characters\n        embedding_dim: size of embedding vector. \n        This typically in powers of 2, i.e. 64, 128, 256 and so on\n        rnn_units: number of GRU units to be used\n        batch_size: batch size for training the model\n    Returns:\n        tf.keras model object\n    \"\"\"\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n    return model\n# Length of the vocabulary in chars\nvocab_size = len(vocab)\n# The embedding dimension\nembedding_dim = 256\n# Number of RNN units\nrnn_units = 1024\nmodel = build_model(\n  vocab_size = len(vocab),\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE) \n```", "```py\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\nmodel.compile(optimizer='adam', loss=loss) \n```", "```py\n# Directory where the checkpoints will be saved\ncheckpoint_dir = r'data/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)\nEPOCHS = 10\nhistory = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) \n```", "```py\nEpoch 1/10\n254/254 [==============================] - 38s 149ms/step - loss: 2.4388\nEpoch 2/10\n254/254 [==============================] - 36s 142ms/step - loss: 1.7407\n.\n.\n.\nEpoch 10/10\n254/254 [==============================] - 37s 145ms/step - loss: 1.1530 \n```", "```py\ndef generate_text(model, mode='greedy', context_string='Hello',\n   num_generate=1000, \n                   temperature=1.0):\n    \"\"\"\n    Utility to generate text given a trained model and context\n    Parameters:\n        model: tf.keras object trained on a sufficiently sized corpus\n        mode: decoding mode. Default is greedy. Other mode is\n              sampling (set temperature)\n        context_string: input string which acts as context for the                         model\n        num_generate: number of characters to be generated\n        temperature: parameter to control randomness of outputs\n    Returns:\n        string : context_string+text_generated\n    \"\"\"\n    # vectorizing: convert context string into string indices\n    input_eval = [char2idx[s] for s in context_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n    # String for generated characters\n    text_generated = []\n    model.reset_states()\n    # Loop till required number of characters are generated\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        if mode == 'greedy':\n          predicted_id = np.argmax(predictions[0])\n\n        elif mode == 'sampling':\n          # temperature helps control the character \n          # returned by the model.\n          predictions = predictions / temperature\n          # Sampling over a categorical distribution\n          predicted_id = tf.random.categorical(predictions, \n                                           num_samples=1)[-1,0].numpy()\n        # predicted character acts as input for next step\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n    return (context_string + ''.join(text_generated)) \n```", "```py\n# greedy decoding\nprint(generate_text(model, context_string=u\"It was in July, 1805\n\",num_generate=50,mode=\"greedy\"))\n# sampled decoding with different temperature settings\nprint(generate_text(model, context_string=u\"It was in July, 1805\n\",num_generate=50, mode=\"sampling\", temperature=0.3))\nprint(generate_text(model, context_string=u\"It was in July, 1805\n\",num_generate=50, mode=\"sampling\",temperature=0.9)) \n```", "```py\nbuild_model function to do just that:\n```", "```py\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size,is_bidirectional=False):\n    \"\"\"\n    Utility to create a model object.\n    Parameters:\n        vocab_size: number of unique characters\n        embedding_dim: size of embedding vector. This typically in                        powers of 2, i.e. 64, 128, 256 and so on\n        rnn_units: number of LSTM units to be used\n        batch_size: batch size for training the model\n    Returns:\n        tf.keras model object\n    \"\"\"\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]))\n    if is_bidirectional:\n      model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform')))\n    else:\n      model.add(tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'))\n      model.add(tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'))\n      model.add(tf.keras.layers.Dense(vocab_size))\n    return model \n```"]