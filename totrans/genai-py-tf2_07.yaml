- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Style Transfer with GANs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GAN进行风格转移
- en: Neural networks are improving in a number of tasks involving analytical and
    linguistic skills. Creativity is one sphere where humans have had an upper hand.
    Not only is art subjective and has no defined boundaries, it is also difficult
    to quantify. Yet this has not stopped researchers from exploring the creative
    capabilities of algorithms. There have been several successful attempts at creating,
    understanding, and even copying art or artistic styles over the years, a few examples
    being *Deep Dream*¹ and *Neural Style Transfer*.²
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在涉及分析和语言技能的各种任务中正在取得进步。创造力是人类一直占有优势的领域，艺术不仅是主观的，没有明确定义的边界，而且很难量化。然而，这并没有阻止研究人员探索算法的创造能力。多年来有过几次成功试图创建、理解甚至模仿艺术或艺术风格的尝试，例如*深梦¹*和*神经风格转移*²。
- en: Generative models are well suited to tasks associated with imagining and creating.
    **Generative Adversarial Networks** (**GANs**) in particular have been studied
    and explored in detail for the task of style transfer over the years. One such
    example is presented in *Figure 7.1*, where the CycleGAN architecture has been
    used to successfully transform photographs into paintings using the styles of
    famous artists such as Monet and Van Gogh.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型非常适合于想象和创作相关的任务。特别是，生成对抗网络（GAN）在多年来的风格转移任务中得到了深入研究和探索。一个典型的例子是在*图7.1*中展示的，CycleGAN架构成功地使用莫奈和梵高等著名艺术家的风格将照片转换成绘画。
- en: '![](img/B16176_07_01.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_01.png)'
- en: 'Figure 7.1: Style transfer based on the artistic style of four famous painters
    using CycleGAN³'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：基于CycleGAN³使用四位著名画家艺术风格的风格转移
- en: '*Figure 7.1* gives us a visual sense of how style transfer works. The samples
    show that the CycleGAN model is able to preserve the details and structures of
    the input image, yet transforms it in a way that mimics famous painters'' works.
    In other words, style transfer is a technique that transforms an input image such
    that it adopts the visual style of another/reference image.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1*给我们展示了风格转移的视觉效果。样本表明CycleGAN模型能够保留输入图像的细节和结构，同时以模仿著名画家作品的方式进行转换。换句话说，风格转移是一种技术，它能够改变输入图像，使其采用另一个/参考图像的视觉风格。'
- en: 'In this chapter, we will cover style transfer methods using different GAN architectures.
    We will focus on the following aspects:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍使用不同GAN架构进行风格转移的方法。我们将重点关注以下方面：
- en: Image-to-image paired style transfer techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像对图像配对风格转移技术
- en: Image-to-image unpaired style transfer techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像对图像无配对风格转移技术
- en: Related works
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关工作
- en: We will cover the internal workings of different GAN architectures and key contributions
    that have enabled the style transfer setup. We will also build and train these
    architectures from scratch to get a better understanding of how they work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖不同GAN架构的内部工作原理和使风格转移设置得以实现的重要贡献。我们还将从头开始构建和训练这些架构，以更好地理解它们的工作原理。
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的所有代码片段都可以直接在Google Colab中运行。出于篇幅的考虑，未包含依赖项的导入语句，但读者可以参考GitHub存储库获取完整的代码：[https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)
- en: Let's get started by looking at paired style transfer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始看配对风格转移吧。
- en: Paired style transfer using pix2pix GAN
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pix2pix GAN进行配对风格转移
- en: In *Chapter 6*, *Image Generation with GANs*, we discussed a number of innovations
    related to GAN architectures that led to improved results and better control of
    the output class. One of those innovations was conditional GANs. This simple yet
    powerful addition to the GAN setup enabled us to navigate the latent vector space
    and control the generator to generate specific outputs. We experimented with a
    simple MNIST conditional GAN where we were able to generate the output of our
    choice.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6章*《使用GAN生成图像》中，我们讨论了与GAN架构相关的多项创新，这些创新导致了改进的结果和更好的输出类别控制。其中之一是条件GAN。将这个简单但强大的添加到GAN设置中，使我们能够浏览潜在的向量空间，并控制生成器生成特定的输出。我们尝试了一个简单的MNIST条件GAN，在那里我们能够生成我们选择的输出。
- en: In this section, we will cover a variant of conditional GANs in the context
    of style transfer. We will go through details of the pix2pix architecture, discuss
    the important components and also train a paired style transfer network of our
    own. We will close this section with some amazing and innovative use cases of
    such a capability.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在风格迁移的背景下介绍条件GAN的一种变体。我们将详细讨论pix2pix架构的重要组件，并训练我们自己的成对风格迁移网络。我们将结束本节，介绍一些这种能力的惊人而创新的用例。
- en: Style transfer is an intriguing research area, pushing the boundaries of creativity
    and deep learning together. In their work titled *Image-to-Image Translation with
    Conditional Adversarial Networks*,⁴ Isola and Zhu et al. present a conditional
    GAN network that is able to learn task-specific loss functions and thus work across
    datasets. As the name suggests, this GAN architecture takes a specific type of
    image as input and transforms it into a different domain. It is called pair-wise
    style transfer as the training set needs to have matching samples from both source
    and target domains. This generic approach is shown to effectively synthesize high-quality
    images from label maps and edge maps, and even colorize images. The authors highlight
    the importance of developing an architecture capable of understanding the dataset
    at hand and learning mapping functions without the need for hand-engineering (which
    has been the case typically).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 风格迁移是一个引人入胜的研究领域，将创造力和深度学习推向极限。在他们的作品《具有条件对抗网络的图像到图像翻译》中，Isola和Zhu等人提出了一个有条件的GAN网络，能够学习任务特定的损失函数，因此可以跨数据集工作。正如其名称所示，这个GAN架构接受特定类型的图像作为输入，并将其转换为不同的域。它被称为成对的风格迁移，因为训练集需要同时具有来自源域和目标域的匹配样本。这种通用方法已被证明能够有效地从标签映射和边缘映射中合成高质量的图像，甚至着色图像。作者强调了开发一种能够理解手头数据集并学习映射函数的架构的重要性，而无需手工工程（通常情况下是这样）。
- en: This work presents a number of contributions on top of the conditional GAN architecture.
    Some of these contributions have been used in other works as well, with the authors
    citing the required references in their work. We encourage readers to go through
    these for an in-depth understanding. We will now cover each component of the pix2pix
    GAN setup in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文在条件GAN架构的基础上提出了一些贡献。其中一些贡献也被其他作品采用，并在其作品中引用了必要的参考文献。我们鼓励读者仔细阅读这些内容以深入理解。现在我们将详细介绍pix2pix
    GAN设置的每个组件。
- en: The U-Net generator
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: U-Net生成器
- en: 'Deep convolutional generators were explored as part of the DC-GAN setup in
    *Chapter 6*,*Image Generation with GANs*. Since CNNs are optimized for computer
    vision tasks, using them for generator as well as discriminator architectures
    has a number of advantages. This work focuses on two related architectures for
    the generator setup. The two choices are the vanilla encoder-decoder architecture
    and the encoder-decoder architecture with skip connections. The architecture with
    skip connections has more in common with the U-Net model⁵ than the encoder-decoder
    setup. Hence, the generator in the pix2pix GAN is termed a U-Net generator. See
    *Figure 7.2* for reference:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积生成器是作为DC-GAN设置的一部分在《第6章，使用GAN生成图像》中探索的。由于CNN针对计算机视觉任务进行了优化，因此将它们用于生成器和鉴别器架构具有许多优势。本文专注于生成器设置的两种相关架构。两种选择是基本的编码器-解码器架构和带有跳跃连接的编码器-解码器架构。具有跳跃连接的架构与U-Net模型比基本的编码器-解码器设置更相似。因此，pix2pix
    GAN中的生成器被称为U-Net生成器。参见*图7.2*作为参考：
- en: '![A picture containing clock  Description automatically generated](img/B16176_07_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![包含时钟的图片 自动生成的描述](img/B16176_07_02.png)'
- en: 'Figure 7.2: (left) Encoder-decoder generator. (right) Encoder-decoder with
    skip connections, or a U-Net generator'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：（左）编码器-解码器生成器。 （右）带有跳跃连接的编码器-解码器，或者是U-Net生成器。
- en: A typical encoder (in the encoder-decoder setup) takes an input and passes it
    through a series of downsampling layers to generate a condensed vector form. This
    condensed vector is termed the bottleneck features. The decoder part then upsamples
    the bottleneck features to generate the final output. This setup is extremely
    useful in a number of scenarios, such as language translation and image reconstruction.
    The bottleneck features condense the overall input into a lower-dimensional space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的编码器（在编码-解码设置中）接受输入并通过一系列下采样层生成一个紧凑的向量形式。这个紧凑的向量被称为瓶颈特征。解码器部分然后将瓶颈特征上采样到生成最终输出。这种设置在许多场景中非常有用，比如语言翻译和图像重建。瓶颈特征将整体输入压缩到较低维度的空间。
- en: Theoretically, the bottleneck features capture all the required information,
    but practically this becomes difficult when the input space is large enough.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，瓶颈特征捕获所有所需信息，但当输入空间足够大时，实际上变得困难。
- en: 'Additionally, for our task of image-to-image translation, there are a number
    of important features that need to be consistent between the input and output
    images. For example, if we are training our GAN to generate aerial photos out
    of outline maps, the information associated with roads, water bodies, and other
    low-level information needs to be preserved between inputs and outputs, as shown
    in *Figure 7.3*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于我们的图像到图像翻译任务，存在许多需要在输入和输出图像之间保持一致的重要特征。例如，如果我们正在训练我们的 GAN 从轮廓地图生成航拍照片，与道路、水体和其他低级信息相关的信息需要在输入和输出之间保持一致，如*图
    7.3*所示：
- en: '![A picture containing text  Description automatically generated](img/B16176_07_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![包含文字的图片  自动生成的描述](img/B16176_07_03.png)'
- en: 'Figure 7.3: The U-Net architecture enables the generator to ensure features
    are consistent between the input and the generated output'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：U-Net 架构使生成器能够确保输入和生成的输出之间的特征一致
- en: 'The U-Net architecture uses skip connections to shuttle important features
    between the input and output (see *Figures 7.2* and *7.3*). In the case of the
    pix2pix GAN, skip connections are added between every *i*^(th) down-sampling layer
    and (*n - i*)^(th) over-sampling layer, where n is the total number of layers
    in the generator. The skip connection leads to the concatenation of all channels
    from the *i*^(th) to (*n - i*)^(th) layers, with the *i*^(th) layers being appended
    to the (*n - i*)^(th) layers:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 架构使用跳跃连接在输入和输出之间传递重要特征（见*图 7.2*和*图 7.3*）。在 pix2pix GAN 的情况下，跳跃连接被添加在每个第
    *i* 个下采样层和（*n - i*）个上采样层之间，其中 n 是生成器中的总层数。跳跃连接导致从第 *i* 层到（*n - i*）层的所有通道被串联在一起，*i*
    层被追加到（*n - i*）层：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_07_04.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图  自动生成的描述](img/B16176_07_04.png)'
- en: 'Figure 7.4: The encoder and decoder blocks of the U-Net generator'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：U-Net 生成器的编码器和解码器块
- en: The generator presented in the paper follows a repeating block structure for
    both encoder and decoder parts. Each encoder block consists of a convolutional
    layer followed by a batch normalization layer, dropout layer, and leaky ReLU activation.
    Every such block downsamples by a factor of 2, using a stride of 2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 文中提到的生成器针对编码器和解码器部分采用了重复的块结构。每个编码器块由卷积层、后跟批量归一化层、辍学层和泄漏的 ReLU 激活组成。每个这样的块通过步幅为
    2 下采样 2 倍。
- en: The decoder blocks use a transposed-convolution layer followed by batch normalization
    and leaky ReLU activation. Each block upsamples by a factor of 2\. A simplified
    setup of encoder and decoder blocks is shown in *Figure 7.4*. As mentioned earlier,
    each of these blocks is connected using a skip connection as well. Equipped with
    this knowledge about the generator, let's get onto implementation details.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块使用转置卷积层，后跟批量归一化和泄漏的 ReLU 激活。每个块上采样 2 倍。编码器和解码器块的简化设置如*图 7.4*所示。 如前所述，每个块之间也使用跳跃连接。掌握了生成器的这些知识后，让我们进入实现细节。
- en: Firstly, let us prepare utility methods for downsampling and upsampling blocks
    of the U-Net generator. The downsampling block uses a stack comprised of convolutional
    layers, followed by leaky ReLU activation, and finally an optional batch normalization
    layer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为 U-Net 生成器的下采样和上采样块准备一些实用的方法。下采样块使用由卷积层组成的堆栈，后跟泄漏的 ReLU 激活，最后是可选的批量归一化层。
- en: 'The `downsample_block` helper function below takes input parameters for the
    number of filters required, kernel size, and whether we need batch normalization
    or not:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的`downsample_block`辅助函数接受所需的滤波器数量、内核大小以及是否需要批归一化的输入参数：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The next helper function is the upsampling block. Each upsampling block is a
    stack comprised of an upsampling layer followed by a convolutional 2D layer, an
    optional dropout layer, and finally the batch normalization layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个辅助函数是上采样块。每个上采样块都是由一个上采样层、一个二维卷积层、一个可选的丢失层和最后一个批归一化层组成的堆栈。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The final piece of the puzzle is to prepare the decoder. For this, we stack
    seven decoder blocks using the `upsample_block()` function, with skip connections
    from the encoder layers. The following snippet implements this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分的关键是准备解码器。为此，我们使用`upsample_block()`函数堆叠了七个解码器块，并从编码器层获取跳跃连接。以下代码片段实现了这一点：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This shows the ease with which we can leverage building blocks to form complex
    architectures such as the U-Net generator. Let us now understand the details associated
    with the discriminator for pix2pix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了我们可以利用构建模块轻松形成复杂架构，如U-Net生成器。现在让我们了解与pix2pix的鉴别器相关的详细信息。
- en: The Patch-GAN discriminator
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补丁-GAN鉴别器
- en: A typical discriminator works by taking an input image and classifies it as
    fake or real, that is, generates a single output scalar. In the case of a conditional
    discriminator, there are two inputs, the first being the conditional input and
    the second the generated sample (from the generator) for classification. For our
    image-to-image transfer use case, the discriminator is provided with a source
    image (conditional input) as well as the generated sample, and its aim is to predict
    whether the generated sample is a plausible transformation of the source or not.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的鉴别器通过获取输入图像并将其分类为假或真来工作，即生成单个输出标量。在有条件的鉴别器的情况下，有两个输入，第一个是条件输入，第二个是用于分类的生成样本（来自生成器）。对于我们的图像到图像转换用例，鉴别器提供了源图像（条件输入）以及生成的样本，并且其目的是预测生成的样本是否是源的合理转换。
- en: The authors of pix2pix propose a Patch-GAN setup for the discriminator, which
    takes the two required inputs and generates an output of size *N* x *N*. *Figure
    7.5* illustrates the concept of Patch-GAN in a simplified manner.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: pix2pix的作者提出了一个Patch-GAN设置用于鉴别器，它接受两个所需输入并生成大小为*N* x *N*的输出。*图7.5*以简化的方式说明了Patch-GAN的概念。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![A picture containing showing, photo, different, screen  Description automatically
    generated](img/B16176_07_05.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![展示的图片，照片，不同，屏幕自动生成的描述](img/B16176_07_05.png)'
- en: 'Figure 7.5: Simplified illustration to understand the workings of a Patch-GAN
    discriminator'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.5: 简化的图示，以了解Patch-GAN鉴别器的工作原理'
- en: 'The configuration presented in the paper uses three Patch-GAN layers using
    a kernel size of 4 x 4 and a stride of 2\. The final two layers use a kernel size
    of 4 x 4 with a stride of 1\. This leads to a 70 x 70 Patch-GAN setup, that is,
    each output pixel/cell/element in the *N* x *N* output matrix corresponds to a
    70 x 70 patch of the input image. Each such 70 x 70 patch has high overlaps as
    the input image has a size of 256 x 256\. To understand this better, let''s work
    through the calculation of effective receptive fields using the following snippet:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 文章中提出的配置使用了三个Patch-GAN层，使用了4 x 4的内核大小和2的步幅。最后两层使用了4 x 4的内核大小和1的步幅。这导致了一个70 x
    70的Patch-GAN设置，也就是说，*N* x *N*输出矩阵中的每个输出像素/单元/元素对应于输入图像的70 x 70补丁。每个这样的70 x 70补丁具有高度的重叠，因为输入图像的大小为256
    x 256。为了更好地理解这一点，让我们通过以下代码片段计算有效感受野的计算：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The snippet shows the calculation for understanding how each output pixel corresponds
    to a patch of size 70 x 70 in the initial input image.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段显示了理解每个输出像素如何对应于初始输入图像中大小为70 x 70的补丁的计算。
- en: The intuitive way of understanding this is to assume that the model prepares
    multiple overlapping patches of the input image and tries to classify each patch
    as fake or real, then averages them to prepare the overall result. This is shown
    to improve the overall output quality of the generated images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 直观理解这一点的方式是假设模型准备了输入图像的多个重叠补丁，并试图将每个补丁分类为假或真，然后对其进行平均以准备整体结果。这已被证明能够提高生成图像的整体输出质量。
- en: The authors experiment with different patch sizes ranging from 1 x 1 (Pixel-GAN)
    to 256 x 256 (Image-GAN), but they report best results with the 70 x 70 configuration
    (Patch-GAN) and little to no improvements beyond it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者尝试了不同的补丁大小，范围从1 x 1（Pixel-GAN）到256 x 256（Image-GAN），但他们报告70 x 70配置（Patch-GAN）取得了最佳结果，而且在这之后几乎没有改进。
- en: 'Intuitively, we can perhaps reason why: in style transfer, the goal is to copy
    local characteristics from the source image onto the target image, so the patch
    size needs to best serve this goal; a pixel-level patch size is too narrow and
    loses sight of larger characteristics, while an image-level patch size is insensitive
    to local variation within the image.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，我们或许可以推断：在样式转移中，目标是从源图像复制局部特征到目标图像，因此补丁大小需要最好地为此目标服务；像素级补丁大小太窄，失去了更大特征的视野，而图像级补丁大小对图像内部的局部变化不敏感。
- en: 'Let''s now prepare our Patch-GAN discriminator using TensorFlow 2\. The first
    step is to prepare a utility for defining a discriminator block consisting of
    a convolutional layer, leaky ReLU, and an optional batch normalization layer:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用TensorFlow 2来准备我们的Patch-GAN鉴别器。第一步是准备一个用于定义鉴别器模块的实用程序，包括卷积层、泄漏ReLU和可选的批量标准化层：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will use these blocks to prepare the Patch-GAN discriminator as follows.
    The snippet below prepares a discriminator model that takes in two inputs (the
    generator''s output and the conditioning image) followed by four discriminator
    blocks with an increasing number of filters:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些模块来准备Patch-GAN鉴别器，步骤如下。下面的片段准备了一个鉴别器模型，它接受两个输入（生成器的输出和调节图像），然后是四个鉴别器模块，带有越来越多的滤波器：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Similar to the generator, we now have a function to build the required Patch-GAN
    discriminator. The next step is to understand the objective functions used to
    train the overall setup.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成器类似，我们现在有一个构建所需的Patch-GAN鉴别器的函数。下一步是了解用于训练整体设置的目标函数。
- en: Loss
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失
- en: 'We discussed conditional GANs in detail in *Chapter 6*, *Image Generation with
    GANs*, where we introduced the overall conditional GAN objective function. Here
    it is again:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第6章*《基于GAN的图像生成》中，我们详细讨论了条件GAN，介绍了总体条件GAN目标函数。在这里再次提到：
- en: '![](img/B16176_07_001.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_001.png)'
- en: 'The authors observe that the typical way of utilizing L1 and L2 regularization
    methods to improve output quality works by capturing low frequencies only, that
    is, local structures that contribute to the overall crispness of the generated
    image. L1 regularization helps prevent blurring compared to L2 regularization.
    Therefore, we can formulate L1 regularization as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者观察到，利用L1和L2正则化方法改进输出质量的典型方式仅通过捕捉低频率的局部结构，即对生成图像整体清晰度的贡献。与L2正则化相比，L1正则化有助于防止模糊。因此，我们可以将L1正则化公式化为：
- en: '![](img/B16176_07_002.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_002.png)'
- en: 'where *x* is the source image, *y* is the conditioned input, and *z* is the
    noise vector. Coupling the U-Net setup with L1 regularization leads to the generation
    of sharp output images, where the GAN handles high frequencies while the L1 assists
    with low frequencies. The updated objective function can be stated as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是源图像，*y*是条件输入，*z*是噪音向量。将U-Net设置与L1正则化相结合可以生成清晰的输出图像，其中GAN处理高频率，而L1协助低频率。更新后的目标函数可以表述为：
- en: '![](img/B16176_07_003.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_003.png)'
- en: Similar to improvements suggested in the original GAN paper, pix2pix also maximizes
    *log(D(G(z|y)))* instead of minimizing *log(1 – D(G(z|y)))*. This results in better
    feedback from gradient curves (refer to the section *Training GANs* in *Chapter
    6*, *Image Generation with GANs*).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始GAN论文中建议的改进类似，pix2pix也最大化*log(D(G(z|y)))*，而不是最小化*log(1 – D(G(z|y)))*。这会导致梯度曲线反馈更好（参见*第6章*《基于GAN的图像生成》的*训练GANs*部分）。
- en: Training pix2pix
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练pix2pix
- en: We now have all the required components ready. The final piece of the puzzle
    is to combine the generator and discriminator into a training loop for preparing
    the pix2pix GAN network.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经准备好所有必需的组件。拼图的最后一块就是将生成器和鉴别器组合成一个训练循环，为准备好的pix2pix GAN网络做准备。
- en: 'We attach relevant loss functions to each of the component networks as well:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将相关的损失函数附加到每个组件网络上：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The above function takes the generator, discriminator, and the combined pix2pix
    GAN model object as inputs. Based on the size of the Patch-GAN discriminator,
    we define NumPy arrays for holding fake and real output predictions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数接受生成器、鉴别器和组合pix2pix GAN模型对象作为输入。根据Patch-GAN鉴别器的大小，我们定义用于保存假和真输出预测的NumPy数组。
- en: Similar to the way we trained GANs in the previous chapter, we loop through
    multiple iterations by first using the generator to generate a fake sample and
    then using this to get discriminator output. Finally, these outputs are used to
    calculate the loss and update the corresponding model weights.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在前一章中训练GAN的方式类似，我们通过首先使用生成器生成假样本，然后使用这个样本来获取鉴别器输出来循环多次迭代。最后，这些输出用于计算损失并更新相应的模型权重。
- en: 'Now that we have the training loop, the following snippet prepares the discriminator
    and GAN networks:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练循环，以下片段准备了鉴别器和GAN网络：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The training loop is simple and similar to what we used in the previous chapter:
    for every epoch, we alternate between training the discriminator and the generator.
    The hyperparameters used are as stated in the pix2pix paper. The outputs from
    the model at different stages of training are showcased in *Figure 7.6*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环简单且类似于我们在前一章中使用的内容：每个时代，我们在训练鉴别器和生成器之间交替。使用的超参数如pix2pix论文中所述。在训练的不同阶段模型的输出在*图7.6*中展示：
- en: '![](img/B16176_07_06.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_06.png)'
- en: 'Figure 7.6: Pix2pix-generated outputs at different stages of training'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：不同训练阶段的pix2pix生成输出
- en: Unlike the simpler architectures we trained in *Chapter 6*,*Image Generation
    with GANs*, despite being far more complex, the pix2pix GAN trains faster and
    stabilizes to far better results in fewer iterations. The outputs showcased in
    *Figure 7.6* show the model's ability to learn the mapping and generate high-quality
    outputs right from the first epoch. This can all be attributed to some of the
    innovations discussed in the previous sections.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在*第6章*中训练的更简单的架构不同，尽管更复杂，但pix2pix GAN的训练速度更快，而且在更少的迭代中稳定到更好的结果。在*图7.6*中展示的输出显示了模型学习映射并从第一个时代开始生成高质量输出的能力。所有这些都可以归因于前几节讨论的一些创新。
- en: Now that we've seen how to set up and train a pix2pix GAN for paired style transfer,
    let's look at some of the things it can be used for.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何为配对风格转换设置和训练pix2pix GAN，让我们看看它可以用于哪些事情。
- en: Use cases
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例
- en: The image-to-image translation setup opens up a lot of use cases and applications
    in the real world. The pix2pix setup provides a generic framework that can be
    applied to a number of image-to-image translation use cases without specifically
    engineering the architectures or loss functions. In their work, Isola and Zhu
    et al. present a number of interesting studies to showcase these capabilities.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像翻译设置在现实世界中开启了许多用例和应用。pix2pix设置提供了一个通用框架，可应用于许多图像到图像翻译用例，而无需专门设计架构或损失函数。在他们的工作中，Isola
    和 Zhu 等人展示了许多有趣的研究来展示这些功能。
- en: 'This conditional GAN setup of the pix2pix GAN is capable of performing tasks
    such as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个pix2pix GAN的有条件GAN设置能够执行诸如：
- en: Building façade generation from label inputs
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从标签输入生成建筑立面
- en: Colorization of black and white images
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑白图像的彩色化
- en: Transforming satellite/aerial map input images to Google Maps-like outputs
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将卫星/航拍地图输入图像转换为类似Google地图的输出
- en: Semantic segmentation tasks such as street view to segment labels
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义分割任务，如街景到分割标签
- en: Sketch to image tasks such as sketch to photo, sketch to portrait, sketch to
    cat, sketch to colored Pokémon, and even outline to fashion objects such as shoes,
    bags, and so on
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将草图转换为图像任务，如草图转照片，草图转肖像，草图转猫，草图转彩色宝可梦，甚至轮廓转时尚物品，如鞋子、包等
- en: Background removal
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 背景去除
- en: In-painting or image completion
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修补或图像完成
- en: Thermal to RGB image translation
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热图到RGB图像的转换
- en: Day to night scene and summer to winter scene conversion
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白天到夜晚场景和夏季到冬季场景转换
- en: 'Some of the types of translations performed in the paper are shown in *Figure
    7.7* for reference:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中执行的一些翻译类型在*图7.7*中供参考：
- en: '![A picture containing photo, different  Description automatically generated](img/B16176_07_07.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![包含照片的图片，自动生成不同的描述](img/B16176_07_07.png)'
- en: 'Figure 7.7: A few examples of different image-to-image translation tasks using
    pix2pix'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：使用pix2pix进行不同图像到图像翻译任务的几个示例
- en: 'As mentioned earlier, the pix2pix architecture is highly optimized and trains
    even on very small datasets. This enables many more creative use cases experimented
    with by the community and other researchers; the authors have developed a website
    for the paper where they showcase such use cases. We encourage readers to visit
    the website for more details: [https://phillipi.github.io/pix2pix/](https://phillipi.github.io/pix2pix/).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，pix2pix 架构经过高度优化，即使在非常小的数据集上也能训练。这使社区和其他研究人员尝试了更多创造性的用例；作者为论文开发了一个展示此类用例的网站。我们鼓励读者访问网站了解更多细节：[https://phillipi.github.io/pix2pix/](https://phillipi.github.io/pix2pix/)。
- en: Having discussed paired style transfer, next we're going to look at unpaired
    style transfer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论了配对样式转移后，接下来我们将看看不配对样式转移。
- en: Unpaired style transfer using CycleGAN
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CycleGAN 进行不配对样式转移
- en: Paired style transfer is a powerful setup with a number of use cases, some of
    which we discussed in the previous section. It provides the ability to perform
    cross-domain transfer given a pair of source and target domain datasets. The pix2pix
    setup also showcased the power of GANs to understand and learn the required loss
    functions without the need for manually specifying them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 配对样式转移是一个强大的设置，有许多用例，我们在前一节中讨论了其中一些。它提供了在给定一对源和目标领域数据集的情况下执行跨域转换的能力。pix2pix
    设置还展示了 GAN 理解和学习所需的损失函数的能力，无需手动指定。
- en: While being a huge improvement over hand-crafted loss functions and previous
    works, paired style transfer is limited by the availability of paired datasets.
    Paired style transfer requires the input and output images to be structurally
    the same even though the domains are different (aerial to map, labels to scene,
    and so on). In this section, we will focus on an improved style transfer architecture
    called CycleGAN.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然相对于手工制作的损失函数和以往的作品而言，配对样式转移是一个巨大的改进，但它受到配对数据集的限制。配对样式转移要求输入和输出图像在结构上完全相同，即使领域不同（从航空到地图，从标签到场景等）。在本节中，我们将重点关注一种名为
    CycleGAN 的改进样式转移架构。
- en: CycleGAN improves upon paired style transfer architecture by relaxing the constraints
    on input and output images. CycleGAN explores the unpaired style transfer paradigm
    where the model actually tries to learn the stylistic differences between source
    and target domains without explicit pairing between input and output images.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 通过放宽输入和输出图像的约束来改进配对样式转移架构。CycleGAN 探索了模型尝试学习源和目标域之间风格差异的不配对样式转移范式，而不需要明确地对输入和输出图像进行配对。
- en: 'Zhu and Park et al. describe this unpaired style transfer as similar to our
    ability to imagine how Van Gogh or Monet would have painted a particular scene,
    without having actually seen a side-by-side example. Quoting from the paper³:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 朱和朴等人将这种不配对样式转移描述为类似于我们能够想象梵高或莫奈会如何画一个特定场景，而没有确实看到并列示例。引用自该论文³：
- en: Instead, we have knowledge of the set of Monet paintings and of the set of landscape
    photographs. We can reason about the stylistic differences between these two sets,
    and thereby imagine what a scene might look like if we were to translate it from
    one set into the other.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相反，我们了解到了莫奈绘画集和风景照片集。我们可以推理出这两组之间的风格差异，因此可以想象如果我们将一个场景从一组转换到另一组会是什么样子。
- en: This provides a nice advantage as well as opening up additional use cases where
    an exact pairing of source and target domains is either not available or we do
    not have enough training examples.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅提供了一个很好的优势，也开拓了额外的用例，其中源和目标领域的精确配对要么不可用，要么我们没有足够的训练示例。
- en: Overall setup for CycleGAN
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN 的整体设置
- en: 'In the case of paired style transfer, the training dataset consists of paired
    samples, denoted as {*x*[i], *y*[i]}, where *x*[i] and *y*[i] have correspondence
    between them. This is shown in *Figure 7.8 (a)* for reference:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在配对样式转移的情况下，训练数据集由配对样本组成，表示为{*x*[i]，*y*[i]}，其中*x*[i]和*y*[i]之间存在对应关系。如参考*图7.8（a）*所示：
- en: '![](img/B16176_07_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_08.png)'
- en: 'Figure 7.8: (a) Paired training examples. (b) Unpaired training examples. (Source:
    Zhu and Park et al. Unpaired Image-to-Image Translation using Cycle-Consistent
    Adversarial Networks, Figure 2)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：（a）配对训练示例。（b）不配对训练示例。（来源：朱和朴等人《使用循环一致对抗网络进行不配对图像到图像转换》，图2）
- en: For CycleGAN, the training dataset consists of unpaired samples from the source
    set, denoted as ![](img/B16176_07_004.png) , and target set ![](img/B16176_07_005.png),
    with no specific information regarding which *x*[i] matches which *y*[j]. See
    *Figure 7.8 (b)* for reference.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CycleGAN，训练数据集包括来自源集的不成对样本，表示为![](img/B16176_07_004.png)，以及目标集![](img/B16176_07_005.png)，没有关于哪个*x*[i]匹配哪个*y*[j]的具体信息。
    参考*图7.8 (b)*。
- en: In the previous chapter, we discussed how GANs learn a mapping ![](img/B16176_07_006.png)
    such that the output ![](img/B16176_07_007.png) is indistinguishable from ![](img/B16176_07_008.png).
    While this works well for other scenarios, it is not so good for unpaired image-to-image
    translation tasks. Due to the lack of paired samples, we cannot use the L1 loss
    as before to learn a *G*, so we need to formulate a different loss for unpaired
    style transfer. In general, when we learn the function *G*(*x*), it is one of
    the numerous possibilities for learning *Y*. In other words, for a given *X* and
    *Y*, there are infinitely many *G*s that will have the same distribution over
    ![](img/B16176_07_009.png).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们讨论了GAN是如何学习一个映射![](img/B16176_07_006.png)，使得输出![](img/B16176_07_007.png)与![](img/B16176_07_008.png)无法区分。
    尽管这对其他场景有效，但对于无配对的图像到图像转换任务却不太好。 由于缺乏配对样本，我们无法像以前那样使用L1损失来学习*G*，因此我们需要为无配对风格转换制定不同的损失。
    一般来说，当我们学习函数*G*(*x*)时，这是学习*Y*的众多可能性之一。 换句话说，对于给定的*X*和*Y*，存在无限多个*G*会在![](img/B16176_07_009.png)上具有相同的分布。
- en: In order to reduce the search space and add more constraints in our search for
    the best possible generator *G*, the authors introduced a property called **cycle
    consistency**. Mathematically, assume we have two generators , *G* and *F*, such
    that ![](img/B16176_07_010.png) and ![](img/B16176_07_011.png). In the best possible
    setting, *G* and *F* would be inverses of each other and should be bijections,
    that is, one-to-one. For CycleGAN, the authors train both generators, *G* and
    *F*, simultaneously for adversarial loss along with cycle consistency constraints
    to encourage ![](img/B16176_07_012.png) and ![](img/B16176_07_013.png). This results
    in the successful training of the unpaired style transfer GAN setup.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少搜索空间并在寻找最佳生成器*G*时增加更多约束，作者引入了一种称为**循环一致性**的特性。 从数学上讲，假设我们有两个生成器*G*和*F*，使得![](img/B16176_07_010.png)和![](img/B16176_07_011.png)。
    在最佳设置中，*G*和*F*将相互逆转并且应该是双射，即一对一。 对于CycleGAN，作者同时训练两个生成器*G*和*F*，以促进对抗损失以及循环一致性约束以鼓励![](img/B16176_07_012.png)和![](img/B16176_07_013.png)。
    这导致成功训练无配对风格转换GAN设置。
- en: 'Please note that similar to the generators, we have two sets of discriminators
    as well in this setup, *D*[Y] for *G* and *D*[X] for *F*. The intuition behind
    this setup of having a pair of generators/discriminators is that we can learn
    the best possible translation from the source domain to target only if we are
    able to do the same in reverse order as well. *Figure 7.9* demonstrates the concept
    of cycle consistency pictorially:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与生成器类似，我们在这个设置中也有两组判别器，*D*[Y]用于*G*，*D*[X]用于*F*。 这个设置背后的直觉是，只有在能够以相反顺序执行相同的操作时，我们才能从源域到目标域学习出最佳的翻译。
    *图7.9*生动地演示了循环一致性的概念：
- en: '![A picture containing clock  Description automatically generated](img/B16176_07_09.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![包含时钟的图片，自动生成的描述](img/B16176_07_09.png)'
- en: 'Figure 7.9: High-level schematic for CycleGAN³'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：CycleGAN³的高级示意图
- en: The first section (left-most) of *Figure 7.9* depicts the CycleGAN setup. The
    setup shows two pairs of generators and discriminators, *G* & *D*[Y] and *F* &
    *D*[X].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.9*的第一部分（最左侧）描述了CycleGAN的设置。 这个设置显示了两对生成器和判别器，*G*和*D*[Y]，*F*和*D*[X]。'
- en: The middle section of *Figure 7.9* shows CycleGAN's forward cycle training.
    Input *x* is transformed to ![](img/B16176_07_014.png) using *G*, and then *F*
    tries to regenerate the original input as ![](img/B16176_07_015.png). This pass
    updates *G* and *D*[Y]. The cycle consistency loss helps to reduce the distance
    between *x* and its regenerated form ![](img/B16176_07_015.png).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.9*的中间部分显示了CycleGAN的前向循环训练。 输入*x*通过*G*转换为![](img/B16176_07_014.png)，然后*F*试图再生原始输入为![](img/B16176_07_015.png)。
    这一步更新*G*和*D*[Y]。循环一致性损失有助于减少*x*和其再生形式![](img/B16176_07_015.png)之间的距离。'
- en: Similarly, the third section (right-most) of *Figure 7.9* showcases the backward
    pass where *y* is transformed to *X* and then *G* tries to regenerate the original
    input as ![](img/B16176_07_017.png).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，*图7.9*的第三部分（最右边）展示了向后传递，其中*y*被转换为*X*，然后*G*尝试再生原始输入，如![](img/B16176_07_017.png)。
- en: To better understand how the unpaired training setup works, let's walk through
    a generic example. Assume the task is to translate from English to French. A setup
    where the model has learned the best possible mapping of English to French would
    be the one which when reversed (that is, French to English) results in the original
    sentence.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解无配对训练设置是如何工作的，让我们通过一个通用示例来走一遍。假设任务是从英语到法语的翻译。一个模型已经学会了从英语到法语的最佳映射的设置将是当它被反转（即，法语到英语）时产生原始句子的结果。
- en: Let's now look under the hood and understand each component in detail in the
    coming subsections.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解并详细了解接下来的小节中的每个组件。
- en: Adversarial loss
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗损失
- en: A typical GAN uses adversarial loss to train a generator that is smart enough
    to fool a discriminator. In the case of CycleGAN, as we have two sets of generators
    and discriminators, we need some tweaking of the adversarial loss. Let us take
    it step by step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的GAN使用对抗损失来训练一个足够聪明的生成器以欺骗鉴别器。在CycleGAN的情况下，由于我们有两组生成器和鉴别器，我们需要对对抗损失进行一些调整。让我们一步一步来。
- en: 'For the first generator-discriminator set in our CycleGAN, that is, ![](img/B16176_07_018.png),
    the adversarial loss can be defined as:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的CycleGAN中的第一组生成器-鉴别器，即![](img/B16176_07_018.png)，对抗损失可以定义为：
- en: '![](img/B16176_07_019.png)![](img/B16176_07_020.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_019.png)![](img/B16176_07_020.png)'
- en: 'Similarly, the second-generator-discriminator ![](img/B16176_07_021.png) set
    is given as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，第二个生成器-鉴别器![](img/B16176_07_021.png)集合给出为：
- en: '![](img/B16176_07_022.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_022.png)'
- en: 'Together, these two objectives form the first two terms of the overall objective
    for CycleGAN. One additional change to both sets of generator-discriminators is
    the minimization part. Instead of using the standard negative log likelihood,
    the choice is made in favor of least squares loss. It is denoted as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个目标一起形成了CycleGAN的总体目标的前两项。对于生成器-鉴别器的两组，还有一个额外的变化是最小化部分。选择不是使用标准的负对数似然，而是选择最小二乘损失。表示为：
- en: '![](img/B16176_07_023.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_023.png)'
- en: The least squares loss is observed to be more stable and leads to better quality
    output samples.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘损失被观察到更稳定，并导致更好质量的输出样本。
- en: Cycle loss
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环损失
- en: We introduced the concept of cycle consistency earlier; now we'll see how to
    implement it explicitly. In their paper for CycleGAN, authors Zhu and Park et
    al. highlight that adversarial loss is not enough for the task of unpaired image-to-image
    translation. Not only is the search space too wide, but with enough capacity,
    the generator can fall into mode-collapse without learning about the actual characteristics
    of the source and target domains.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前介绍了循环一致性的概念；现在我们将看到如何明确实现它。在CycleGAN的论文中，作者朱和朴等人强调，对抗损失对于无配对图像到图像的翻译任务是不够的。不仅搜索空间太广，而且有足够的容量，生成器可以陷入模式坍塌而不学习源域和目标域的实际特征。
- en: 'To reduce the search space and ensure the learned mappings are good enough,
    the CycleGAN setup should be able to generate the original input *x* after being
    processed through both *G* and *F*, that is, ![](img/B16176_07_024.png) as well
    as the reverse path of ![](img/B16176_07_025.png). These are termed as forward
    and backward cycle consistencies respectively. The overall cycle consistency loss
    is an L1 loss defined as:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少搜索空间并确保学习到的映射足够好，CycleGAN设置应该能够生成原始输入*x*经过*G*和*F*处理后，即![](img/B16176_07_024.png)以及![](img/B16176_07_025.png)的反向路径。这些分别称为前向和后向循环一致性。总循环一致性损失是定义为L1损失：
- en: '![](img/B16176_07_026.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_026.png)'
- en: This loss ensures that the reconstruction of the original input from the generated
    output is as close as possible.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此损失确保从生成的输出中重建原始输入尽可能接近。
- en: Identity loss
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 身份损失
- en: 'The authors for CycleGAN also observed a specific issue with the overall setup
    with respect to colored objects. Without any constraints specifically for colors,
    the *G* and *F* generators were found to be introducing different tints while
    going through the forward and backward cycles when none was necessary. To reduce
    this unwanted behavior, a regularization term called **identity loss** was introduced.
    See *Figure 7.10* showcasing this particular effect in action:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的作者们还观察到了关于彩色对象的整体设置的特定问题。在没有特定颜色约束的情况下，*G* 和 *F* 生成器在前向和后向循环中会引入不同的色调，而这并不是必要的。为了减少这种不必要的行为，引入了一项称为**身份损失**的正则化项。查看
    *Figure 7.10* 展示了这种特定效果的实际情况：
- en: '![A picture containing photo, different, showing, show  Description automatically
    generated](img/B16176_07_10.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing photo, different, showing, show  Description automatically
    generated](img/B16176_07_10.png)'
- en: 'Figure 7.10 Impact of identity loss on CycleGAN performance. The outputs correspond
    to those of the generator G(x). (Source: Zhu and Park et al. Unpaired Image-to-Image
    Translation using Cycle-Consistent Adversarial Networks, Figure 9)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 7.10 身份损失对 CycleGAN 性能的影响。输出对应于生成器 G(x) 的输出。（来源：Zhu 和 Park 等人。使用循环一致对抗网络进行非配对图像到图像的翻译，图
    9）
- en: 'As is evident from the middle column in *Figure 7.10*, without the additional
    constraint of the identity loss, CycleGAN introduces unnecessary tints in its
    outputs. Thus, the identity loss, defined as ![](img/B16176_07_027.png), can be
    stated as:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在 *Figure 7.10* 中间列所显示的那样，没有身份损失的附加约束，CycleGAN 在其输出中引入了不必要的色调。因此，身份损失，定义为
    ![](img/B16176_07_027.png)，可以表述为：
- en: '![](img/B16176_07_028.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_028.png)'
- en: In simple words, this loss regularizes the generators to be near an identity
    mapping when real samples from the target domain are used as inputs for generation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，这个损失将生成器正则化为当来自目标域的真实样本被用作生成的输入时的近似身份映射。
- en: Overall loss
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整体损失
- en: 'The overall objective of CycleGAN is simply a weighted sum of the different
    losses we discussed in the previous subsections, namely, the adversarial loss,
    the cycle consistency loss, and the identity loss. The overall objective is defined
    as:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的整体目标简单地是前面子节中讨论的不同损失的加权和，即对抗损失、循环一致性损失和身份损失。整体目标定义为：
- en: '![](img/B16176_07_029.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_029.png)'
- en: The paper highlights different values for ![](img/B16176_07_030.png) and ![](img/B16176_07_031.png)
    for different experiments. We will explicitly mention the value used for these
    regularization terms when we prepare our model from scratch.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 论文强调了不同实验中 ![](img/B16176_07_030.png) 和 ![](img/B16176_07_031.png) 的不同值。当我们从头开始准备我们的模型时，我们将明确说明这些正则化项的值。
- en: 'Hands-on: Unpaired style transfer with CycleGAN'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践操作：使用 CycleGAN 进行非配对式风格转换
- en: We discussed the overall setup for CycleGAN and its key innovations in the form
    of cycle consistency loss and identity loss, which enable unpaired style transfer.
    In this section, we will implement it, part by part, and train a couple of CycleGANs
    to convert apples to oranges and photos to Van Gogh paintings.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 CycleGAN 的整体设置及其关键创新，即循环一致性损失和身份损失，这些使得非配对式风格转换成为可能。在本节中，我们将逐部分实施它，并训练一对
    CycleGANs 将苹果转换为橙子，并将照片转换为梵高的绘画作品。
- en: Generator setup
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器设置
- en: Let us begin with the generator. Similar to the pix2pix GAN, CycleGAN also makes
    use of U-Net generators (pay attention, there are two of them in this setup).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从生成器开始。与 pix2pix GAN 类似，CycleGAN 也使用 U-Net 生成器（注意，此设置中有两个生成器）。
- en: The first step is to prepare utility methods for upsampling and downsampling
    blocks. One important difference here is the use of **instance normalization**
    in place of the batch normalization layer. Instance normalization works by normalizing
    each channel in each training sample. This is in contrast to batch normalization,
    where normalization is done across the whole mini-batch and across all input features.
    See *Chapter 6*, *Image Generation with GANs*, for more details on instance normalization.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是准备上采样和下采样块的实用方法。这里的一个重要区别是使用 **实例归一化** 替代批量归一化层。实例归一化的工作方式是对每个训练样本中的每个通道进行归一化。这与批量归一化相反，批量归一化是在整个小批量和所有输入特征上进行归一化的。有关实例归一化的更多详细信息，请参阅
    *第 6 章*，*使用 GAN 生成图像*。
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The U-Net generators used here are shallower compared to the pix2pix setup,
    yet perform equally well (see the section on *Cycle loss*). The following snippet
    demonstrates the method to build the generator:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的 U-Net 生成器与 pix2pix 设置相比要较浅，但表现同样出色（请参阅 *Cycle loss* 部分）。以下代码片段演示了构建生成器的方法：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we can see, the generator consists of four downsampling and four upsampling
    blocks, followed by a Conv2D layer that outputs the target image. Now let's build
    the discriminator.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，生成器由四个下采样和四个上采样块组成，然后是一个输出目标图像的 Conv2D 层。现在让我们构建辨别器。
- en: Discriminator setup
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 辨别器设置
- en: 'Just like the generators, the discriminators used in CycleGAN make use of contributions
    from the pix2pix paper. The discriminators are Patch-GANs and the following code
    listing demonstrates a method for constructing a discriminator block as well as
    a method for building the discriminators:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成器一样，CycleGAN 中使用的辨别器也利用了来自 pix2pix 论文的内容。辨别器是 Patch-GAN，并且以下代码清单展示了构建辨别器块的方法以及构建辨别器的方法：
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We now have the building blocks ready. Let's use them to build the overall CycleGAN
    architecture.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了构建模块。让我们使用它们来建立整体的 CycleGAN 架构。
- en: GAN setup
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GAN 设置
- en: 'We use these methods to prepare two sets of generators and discriminators required
    for mapping from domain *A* to *B* and then back from *B* to *A*. The following
    snippet does exactly this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这些方法准备了两套生成器和辨别器，用于从域 *A* 映射到 *B*，然后再从 *B* 回映射到 *A*。以下代码片段正是如此：
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We just created objects for both pairs of generators and discriminators. We
    combine them in the `gan` object by defining the required inputs and outputs.
    Let's implement the training loop next.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚为两对生成器和辨别器创建了对象。通过定义必须的输入和输出，我们将它们组合在 `gan` 对象中。接下来实现训练循环。
- en: The training loop
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'The final piece of the puzzle is to write a custom training loop. This loop
    first uses both generators to generate fake samples, which are then used to update
    the discriminators in both directions (that is, `A` to `B` and `B` to `A`). We
    finally use the updated discriminators to train the overall CycleGAN. The following
    snippet shows the training loop:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后完整的画面是编写自定义训练循环。该循环首先使用两个生成器生成假样本，然后用它们更新两个方向的辨别器（即，`A` 到 `B` 和 `B` 到 `A`）。最后使用更新后的辨别器来训练整体的
    CycleGAN。以下代码片段展示了训练循环：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The training loop for CycleGAN is mostly similar to that of pix2pix with a few
    additions. As we have two pairs of generators and discriminators, the function
    takes all four models as input along with a combined `gan` object. The training
    loop starts with the generation of fake samples from both generators and then
    uses them to update weights for their corresponding discriminators. These are
    then combined to train the overall GAN model as well.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN 的训练循环与 pix2pix 的大致相似，但有一些补充。由于我们有两对生成器和辨别器，该函数将所有四个模型作为输入，以及一个合并的 `gan`
    对象。训练循环从两个生成器生成的假样本开始，然后使用它们来更新相应的辨别器的权重。然后将它们合并以训练整体的 GAN 模型。
- en: 'Using the components described in this section, we experimented with two sets
    of style transfer datasets, turning apples into oranges and turning photographs
    into Van Gogh paintings. *Figure 7.11* shows the output of the apples to oranges
    experiment through different stages of training:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本节介绍的组件，我们对两组风格转移数据集进行了实验，将苹果转变成橙子并将照片转变为梵高画作。*图 7.11* 展示了苹果转变成橙子实验在不同训练阶段的输出：
- en: '![](img/B16176_07_11.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_11.png)'
- en: 'Figure 7.11: CycleGAN generated outputs at different stages of training for
    the apples to oranges experiment'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：循环生成对抗网络在苹果到橙子实验不同训练阶段生成的输出
- en: 'Similarly, *Figure 7.12* shows how CycleGAN learns to transform photographs
    into Van Gogh style artwork:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，*Figure 7.12* 展示了 CycleGAN 如何学习将照片转换为梵高风格的艺术作品：
- en: '![](img/B16176_07_12.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_12.png)'
- en: 'Figure 7.12: CycleGAN generated outputs at different stages of training for
    the photographs to Van Gogh style paintings experiment'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：循环生成对抗网络在将照片转变为梵高风格画作实验不同训练阶段生成的输出
- en: As is evident from the samples above (*Figures 7.11* and *7.12*), CycleGAN seems
    to have picked up the nuances from both domains without having paired training
    samples. This is a good leap forward in cases where paired samples are hard to
    get.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上述示例所示（*图 7.11* 和 *7.12*），CycleGAN 似乎已经从两个域中学会了一些细微差别，而没有匹配的训练样本。这在难以获得匹配样本的情况下是一个很好的进展。
- en: Another important observation from the two experiments is the amount of training
    required. While both experiments used exactly the same setup and hyperparameters,
    the apples to oranges experiment trained much faster compared to the photograph
    to Van Gogh style painting setup. This could be attributed to the large number
    of modes in the case of the second experiment, along with diversity in the training
    samples.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两个实验中另一个重要的观察是所需的训练量。虽然两个实验都使用了完全相同的设置和超参数，但与将照片转换为梵高风格绘画设置相比，苹果转橘子实验的训练速度要快得多。这可能归因于第二个实验中模式的数量很大，以及训练样本的多样性。
- en: This ends our section on unpaired style transfer. Now we're going to explore
    some work relating to and branching off from both paired and unpaired style transfer.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们关于无配对风格迁移的部分。现在我们将探讨一些与配对和无配对风格迁移相关并有所分支的工作。
- en: Related works
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关工作
- en: Style transfer is an amusing field and a lot of parallel research is going on
    across different research groups to improve the state of the art. The two most
    influential works in the paired and unpaired style transfer space have been discussed
    in this chapter so far. There have been a few more related works in this space
    that are worth discussing.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 风格迁移是一个有趣的领域，不同研究组之间正在进行大量的并行研究，以改进现有技术。迄今为止，在配对和无配对风格迁移领域中，最有影响力的两项工作已在本章中讨论过。在这个空间中还有一些其他相关工作值得讨论。
- en: In this section, we will briefly discuss two more works in the unpaired image-to-image
    translation space that have similar ideas to CycleGAN. Specifically, we will touch
    upon the DiscoGAN and DualGAN setups, as they present similar ideas with minor
    changes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要讨论无配对图像到图像转换领域中另外两个与 CycleGAN 类似的工作。具体而言，我们将涉及 DiscoGAN 和 DualGAN
    设置，因为它们提出了类似的想法，但存在细微的变化。
- en: It is important to note that there are a number of other works in the same space.
    We limit our discussion to only a few of them for the sake of completeness and
    consistency. Readers are encouraged to explore other interesting architectures
    as well.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，同一领域还有许多其他工作。为了完整性和一致性，我们将我们的讨论限制在其中的几个上。鼓励读者也探索其他有趣的架构。
- en: DiscoGAN
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DiscoGAN
- en: Kim and Cha et al. presented a model that discovers cross-domain relations with
    GANs called DiscoGAN.⁶ The task of transforming black and white images to colored
    images, satellite images to map-like images, and so on can also termed *cross-domain
    transfer*, as well as style transfer.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Kim 和 Cha 等人提出了一种使用 GANs 发现跨域关系的模型，称为 DiscoGAN。⁶ 将黑白图像转换为彩色图像、卫星图像转换为地图样式图像等任务也可以称为*跨域转换*，以及风格迁移。
- en: As we've seen already, cross-domain transfer has a number of applications in
    the real world. Domains such as autonomous driving and healthcare have started
    to leverage deep learning techniques, yet many use cases fall short because of
    the unavailability of larger datasets. Unpaired cross-domain transfer works such
    as DiscoGAN (and CycleGAN) can be of great help in such domains.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，跨域转换在现实世界中具有许多应用。诸如自动驾驶和医疗保健等领域已经开始利用深度学习技术，然而许多用例因为缺乏更大的数据集而受阻。无配对跨域转换工作，如
    DiscoGAN（和 CycleGAN），可以在这些领域提供巨大帮助。
- en: '![A picture containing clock  Description automatically generated](img/B16176_07_13.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![包含时钟的图片 自动生成的描述](img/B16176_07_13.png)'
- en: 'Figure 7.13: DiscoGAN setup⁶'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：DiscoGAN 设置⁶
- en: Published about the same time as CycleGAN, DiscoGAN has a number of similarities
    and a few slight differences in performing unpaired style transfer. Like CycleGAN,
    DiscoGAN achieves cross-domain transfer with the help of two pairs of generators
    and discriminators. The generator in the first pair transforms images from domain
    *A* to *B* (denoted as *G*[AB]) and the discriminator (denoted as *D*[B]) classifies
    whether the generated output (denoted as *x*[A][B]) is real or fake.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CycleGAN 几乎同时发布的 DiscoGAN 在执行无配对风格迁移时具有许多相似之处，以及一些轻微的差异。与 CycleGAN 一样，DiscoGAN
    利用两对生成器和判别器来实现跨域转换。第一对生成器将图像从域 *A* 转换为 *B*（表示为 *G*[AB]），而判别器（表示为 *D*[B]）则分类生成的输出（表示为
    *x*[A][B]）是真实的还是伪造的。
- en: The second generator-discriminator pair is the crux of the paper. By enforcing
    that the system regenerates the original input image from the generated output
    (*x*[A][B]), DiscoGAN is able to learn the required cross-domain representations
    without the need for explicit pairs. This reconstruction of original samples is
    achieved using the second generator (denoted as G[B][A]) and its corresponding
    discriminator (*D*[A]). The overall setup is represented in *Figure 7.13* for
    reference.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个生成器-判别器对是论文的关键。通过强制系统从生成的输出（*x*[A][B]）中再生生成原始输入图像，DiscoGAN能够学习所需的跨域表示，而无需明确的配对。使用第二个生成器（标记为G[B][A]）及其对应的判别器（*D*[A]）实现了对原始样本的重构。整体设置如*图7.13*所示，仅供参考。
- en: As shown in *Figure 7.13*, to learn cross-domain representations, DiscoGAN not
    only transforms images from domain *A* to *x*[AB] and then reconstructs them back,
    but also does the same for images in domain *B* as well (that is, *B* to *x*[BA]
    and reconstruct back). This two-way mapping, also called a bijection, along with
    the reconstruction loss and adversarial loss, helps achieve state-of-the-art results.
    The authors note that setups that only rely on reconstruction loss without the
    additional pipeline (*B* to *x*[BA] and reconstruction) still lead to failure
    modes such as mode collapse.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图7.13*所示，为了学习跨域表示，DiscoGAN不仅将图像从域*A*转换为*x*[AB]，然后重构回来，还对域*B*中的图像做同样的操作（即，*B*到*x*[BA]然后重构回来）。这种双向映射，也称为双射，以及重构损失和对抗损失，有助于实现最先进的结果。作者指出，仅依赖重构损失而没有附加管道（*B*到*x*[BA]和重构）的设置仍会导致模式崩溃等故障模式。
- en: 'Unlike CycleGAN, where we noted that the reconstruction loss is an L1 loss
    and a weighted sum of forward and backward reconstruction, DiscoGAN explores and
    uses the reconstruction loss slightly differently. The DiscoGAN paper mentions
    that the reconstruction loss could be any of the distance measures, such as mean
    squared error, cosine distance, or hinge loss. As shown in the following equations,
    the generators then use the reconstruction loss in their training separately:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与CycleGAN不同，我们注意到重构损失是L1损失和前向、后向重构的加权和，DiscoGAN以略微不同的方式探索和使用重构损失。DiscoGAN论文提到，重构损失可以是任何距离度量，例如均方误差、余弦距离或铰链损失。如下方方程所示，生成器然后单独在训练中使用重构损失：
- en: '![](img/B16176_07_032.png)![](img/B16176_07_033.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_07_032.png)![](img/B16176_07_033.png)'
- en: where ![](img/B16176_07_034.png) represents the original adversarial loss and
    ![](img/B16176_07_035.png) is the reconstruction loss for each of the GAN pairs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/B16176_07_034.png)代表原始的对抗损失，![](img/B16176_07_035.png)是每个GAN对的重构损失。
- en: The generators make use of an encoder-decoder setup with convolutional and deconvolutional
    layers (or transposed convolutions) to downsample and upsample intermediate representations/feature
    maps. The decoder, on the other hand, is similar to the encoder part of the generator,
    consisting of convolutional layers with the final layer being a sigmoid for classification.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器使用具有卷积和反卷积层（或转置卷积）的编码器-解码器设置来对中间表示/特征图进行下采样和上采样。另一方面，解码器类似于生成器的编码器部分，由卷积层组成，最后一层是用于分类的sigmoid函数。
- en: The authors of DiscoGAN present a number of well-documented experiments to understand
    how their proposed architecture handles mode-collapse. One such experiment is
    the car-to-car mapping experiment. In this setup, the authors explore how three
    architectures, that is, the original GAN, the GAN with reconstruction loss, and
    DiscoGAN, handle various modes. The mapping experiment transforms the input image
    of a car with specific rotation (azimuth) to a different rotation angle.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DiscoGAN的作者提出了许多经过充分记录的实验，以了解他们提出的架构如何处理模式崩溃。其中一项实验是汽车到汽车的映射实验。在这个设置中，作者探索了原始GAN、带重构损失的GAN和DiscoGAN这三种架构如何处理各种模式。映射实验将具有特定旋转（方位角）的汽车输入图像转换为不同的旋转角度。
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_07_14.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图 自动生成的描述](img/B16176_07_14.png)'
- en: 'Figure 7.14: Car-to-car mapping experiment to understand mode collapse under
    different settings. (a) Original GAN, (b) GAN with reconstruction loss, (c) DiscoGAN
    setup.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：汽车到汽车的映射实验，以了解不同设置下的模式崩溃情况。(a) 原始GAN，(b) 带重构损失的GAN，(c) DiscoGAN设置。
- en: As noted in *Figure 7.14 (a)* and *(b)*, both the GAN and GAN with reconstruction
    loss suffer from mode collapse; the clustered dots in both cases represent the
    fact that these architectures have been able to learn only a few modes, or car
    orientations (the line being the ground truth). Conversely, *Figure 7.14 (c)*
    shows that DiscoGAN learns a better representation of various modes, with the
    dots spread across the line.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图7.14（a）*和*(b)*所示，无论是GAN还是带重建损失的GAN都遭受模式崩溃；在这两种情况下，聚类的点表示这些架构只能学习少数模式，或者是汽车方向（线是真实情况）。相反，*图7.14（c）*显示DiscoGAN学习了各种模式的更好表示，点分布在线附近。
- en: Setting up DiscoGAN is fairly straightforward. Using the utilities described
    in the chapter so far, we trained a DiscoGAN setup to learn an edges-to-shoes
    mapping.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 设置DiscoGAN相当简单。利用本章介绍的工具，我们训练了一个DiscoGAN设置，学习边缘到鞋子的映射。
- en: '![Graphical user interface  Description automatically generated](img/B16176_07_15.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动创建的说明](img/B16176_07_15.png)'
- en: 'Figure 7.15: DiscoGAN during training for an edges-to-shoes experiment'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：DiscoGAN在边缘到鞋子实验中的训练过程
- en: '*Figure 7.15* shows the training progress of our model. It starts with a random
    mapping to understand boundary shapes and a few colors. Training for longer periods
    achieves even better results, as shown by the authors in their paper.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.15*显示了我们模型的训练进展。从随机映射开始，以了解边界形状和一些颜色。训练更长时间可以获得更好的结果，作者在他们的论文中展示了这一点。'
- en: DualGAN
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DualGAN
- en: 'DualGAN is the latest in the family of unpaired image-to-image translation
    architectures. It approaches this task of unpaired image-to-image translation
    from a slightly different perspective compared to DiscoGAN and CycleGAN. Authors
    Yi and Zhang et al. present their work titled *DualGAN: Unsupervised Dual Learning
    for Image-to-Image Translation*⁷ inspired by a seminal paper on *Dual Learning
    for Machine Translation*.⁸ Quoting from the paper, the idea behind looking at
    image-to-image translation as a dual learning task goes as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'DualGAN是非配对图像到图像转换架构家族中的最新成员。与DiscoGAN和CycleGAN相比，它以略微不同的角度处理这个任务。作者Yi和Zhang等人发布了名为*DualGAN:
    图像到图像转换的无监督双学习*⁷的作品，灵感来自一篇关于*机器翻译的双学习*的重要论文。⁸从论文中引用，将图像到图像转换视为双学习任务的想法如下：'
- en: Our approach is inspired by dual learning from natural language processing.
    Dual learning trains two "opposite" language translators (e.g., English-to-French
    and French-to-English) simultaneously by minimizing the re-construction loss resulting
    from a nested application of the two translators. The two translators represent
    a primal-dual pair and the nested application forms a closed loop, allowing the
    application of reinforcement learning. Specifically, the reconstruction loss measured
    over monolingual data (either English or French) would generate informative feed-back
    to train a bilingual translation model.
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的方法受到自然语言处理中的双学习的启发。双学习同时训练两个“相反的”语言翻译器（例如，英语到法语和法语到英语），通过最小化由两个翻译器的嵌套应用产生的重建损失。这两个翻译器代表原始-双对，并且嵌套应用形成一个闭环，允许应用强化学习。具体来说，在单语数据（英语或法语）上测量的重建损失将生成信息反馈，以训练双语翻译模型。
- en: 'Though not explicitly cited in the CycleGAN paper, the ideas seem to have quite
    a bit of overlap. The setup for DualGAN, as expected, also uses two pairs of generator-discriminator.
    The pairs are termed as primal-GAN and dual-GAN. *Figure 7.16* presents the overall
    setup for DualGAN:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CycleGAN论文中没有明确引用，但这些想法似乎有相当多的重叠。如预期的那样，DualGAN的设置也使用了两对生成器-判别器。这两对被称为原始GAN和双GAN。*图7.16*展示了DualGAN的整体设置：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16176_07_16.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![手机屏幕截图 自动创建的说明](img/B16176_07_16.png)'
- en: 'Figure 7.16: DualGAN setup⁷'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：DualGAN设置⁷
- en: The primal GAN (denoted as *G*[A]) takes input u from domain U and learns to
    transform this into *v* from domain *V*, and the dual GAN does the reverse of
    this. The two feedback signals from this setup are termed the reconstruction error
    and the membership score. The membership score is similar to the adversarial loss
    in CycleGAN, where the aim of *G*[A] is to generate outputs *G*[A](*u*, *z*) that
    are good enough to fool *D*[A].
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 原始GAN（表示为*G*[A]）从域U接收输入u并学习将其转换为来自域V的*v*，而双GAN进行相反操作。这个设置的两个反馈信号称为重建错误和成员评分。成员评分类似于CycleGAN中的对抗损失，其中*G*[A]的目标是生成足够优秀的输出*G*[A](*u*,
    *z*)以欺骗*D*[A]。
- en: 'The reconstruction loss represents the learned ability of *G*[B] to reconstruct
    the original input *u* from the generated output *G*[A](*u*, *z*). The reconstruction
    loss is similar to the cycle consistency loss for CycleGAN, apart from differences
    in problem formulation compared to CycleGAN, and a slight difference in training
    setup as well. The DualGAN setup makes use of Wasserstein loss to train. They
    report that using Wasserstein loss helps achieve stable models easily. *Figure
    7.17* shows a few experiments from the DualGAN paper:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 重建损失表示了 *G*[B] 从生成的输出 *G*[A](*u*, *z*) 中重构原始输入 *u* 的学习能力。重建损失类似于 CycleGAN 的循环一致性损失，除了在问题制定方面与
    CycleGAN 的差异之外，训练设置也略有不同。DualGAN 设置使用 Wasserstein 损失进行训练。他们报告说使用 Wasserstein 损失有助于轻松获得稳定的模型。*图
    7.17* 显示了 DualGAN 论文中的一些实验：
- en: '![A picture containing food  Description automatically generated](img/B16176_07_17.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![一个包含食物的图片  自动生成的描述](img/B16176_07_17.png)'
- en: 'Figure 7.17: Unpaired image-to-image translation using DualGAN⁷'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17：使用 DualGAN 进行非配对图像到图像的翻译⁷
- en: The DualGAN setup also makes use of U-Net style generators and Patch-GAN style
    discriminators, and the impact of these tricks is seen in the quality of the output.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: DualGAN 设置还利用了 U-Net 风格的生成器和 Patch-GAN 风格的鉴别器，这些技巧对输出质量产生了影响。
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the creative side of GAN research through the lenses
    of image-to-image translation tasks. While the creative implications are obvious,
    such techniques also open up avenues to improve the research and development of
    computer vision models for domains where datasets are hard to get.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过图像到图像的翻译任务的视角探索了 GAN 研究的创新一面。尽管创新的影响是显而易见的，但这些技术也为改进计算机视觉模型的研究和开发开辟了途径，尤其是在数据集难以获取的领域。
- en: We started off the chapter by understanding the paired image-to-image translation
    task. This task provides training data where the source and destination domains
    have paired training samples. We explored this task using the pix2pix GAN architecture.
    Through this architecture, we explored how the encoder-decoder architecture is
    useful for developing generators that can produce high-fidelity outputs. The pix2pix
    paper took the encoder-decoder architecture one step further by making use of
    skip-connections or a U-Net style generator.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解配对图像到图像的翻译任务开始了本章。这个任务提供了训练数据，其中源域和目标域有配对的训练样本。我们使用 pix2pix GAN 架构探索了这个任务。通过这种架构，我们探索了编码器-解码器架构如何有助于开发可以产生高保真输出的生成器。pix2pix
    论文通过利用跳跃连接或 U-Net 风格生成器将编码器-解码器架构推进了一步。
- en: This setup also presented another powerful concept, called the Patch-GAN discriminator,
    which works elegantly to assist the overall GAN with better feedback signal for
    different style transfer use cases. We used these concepts to build and train
    our own pix2pix GAN from scratch to transfigure satellite images to Google Maps-like
    outputs. Our training results were good-quality outputs using very few training
    samples and training iterations. This faster and stable training was observed
    to be a direct implication of different innovations contributed by the authors
    of this work. We also explored various other use cases that can be enabled using
    pix2pix style architectures.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置还提出了另一个强大的概念，称为 Patch-GAN 鉴别器，它可以优雅地帮助整体 GAN 获得更好的反馈信号，适用于不同的风格转换用例。我们利用这些概念从零开始构建和训练了我们自己的
    pix2pix GAN，将卫星图像转换为类似 Google 地图的输出。我们的训练结果是使用很少的训练样本和训练迭代得到的高质量输出。观察到这种更快速和稳定的训练是这项工作的作者们贡献的不同创新直接影响的结果。我们还探索了使用
    pix2pix 风格架构可以实现的各种其他用例。
- en: In the second part of the chapter, we extended the task of image-to-image translation
    to work in the unpaired setting. The unpaired training setup is no doubt a more
    complex problem to solve, yet it opens up a lot more avenues. The paired setup
    is good for cases where we have explicit pairs of samples in both source and target
    domains, but most real-life scenarios do not have such datasets.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分中，我们将图像到图像的翻译任务扩展到了非配对的设置中。非配对的训练设置无疑是一个更加复杂的问题，但它开辟了更多的可能性。配对设置适用于在源域和目标域中都有明确样本对的情况，但大多数现实场景下并没有这样的数据集。
- en: We explored the unpaired image-to-image translation setup through CycleGAN architecture.
    The authors of CycleGAN presented a number of intuitive yet powerful contributions
    that enable the unpaired setup to work. We discussed the concepts of cycle-consistency
    loss and identity loss as regularization terms for the overall adversarial loss.
    We specifically discussed how identity loss helps improve the overall reconstruction
    of samples and thus the overall quality of output. Using these concepts, we built
    the CycleGAN setup from scratch using TensorFlow-Keras APIs. We experimented with
    two datasets, apples to oranges and photographs to Van Gogh style paintings. The
    results were exceptionally good in both cases with unpaired samples.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 CycleGAN 架构探索了非配对图像到图像的翻译设置。CycleGAN 的作者提出了许多直观但强大的贡献，使得非配对设置能够工作。我们讨论了循环一致性损失和身份损失作为整体对抗损失的正则化项的概念。我们特别讨论了身份损失如何改善样本的整体重构，从而提高输出的整体质量。利用这些概念，我们使用
    TensorFlow-Keras API 从头开始构建了 CycleGAN 设置。我们对两个数据集进行了实验，一个是苹果到橙子，另一个是照片到梵高风格的绘画。在这两种情况下，结果都非常好。
- en: We discussed a couple of related works in the final section of the chapter,
    the DiscoGAN and DualGAN architectures. Along with CycleGAN, these two architectures
    form the overall family of unpaired image-to-image translation GANs. We discussed
    how these architectures present similar ideas from slightly different perspectives.
    We also discussed how slight differences in their problem formulation and overall
    architectures impact the final results.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一节中，我们讨论了一些相关工作，即 DiscoGAN 和 DualGAN 架构。与 CycleGAN 一起，这两种架构构成了整体非配对图像到图像翻译
    GAN 的家族。我们讨论了这些架构如何从略有不同的角度呈现相似的思想。我们还讨论了问题表述和整体架构中细微差异如何影响最终结果。
- en: In this chapter, we built upon the concepts related to GANs and particularly
    conditional GANs that we discussed in *Chapter 6*, *Image Generation with GANs*.
    We discussed a number of innovations and contributions that make use of simple
    building blocks to enable some amazing use cases. The next set of chapters will
    push the boundaries of generative models even further into domains such as text
    and audio. Fasten your seat belts!
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们在 *第 6 章*，*使用 GAN 生成图像* 中讨论的与 GAN 及特别是条件 GAN 相关的概念基础上进行了拓展。我们讨论了许多创新和贡献，利用简单的构建模块实现了一些惊人的用例。接下来的一系列章节将进一步将生成模型的边界推向文本和音频等领域。系好安全带吧！
- en: References
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Mordvintsev, A., McDonald, K., Rudolph, L., Kim, J-S., Li, J., & daviga404\.
    (2015). deepdream. GitHub repository. [https://github.com/google/deepdream](https://github.com/google/deepdream)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mordvintsev，A.，麦克唐纳，K.，鲁道夫，L.，金，J-S.，李，J.，以及 daviga404。 （2015）。deepdream。GitHub
    代码库。[https://github.com/google/deepdream](https://github.com/google/deepdream)
- en: Gatys, L.A., Ecker, A.S., & Bethge, M. (2015). *A Neural Algorithm of Artistic
    Style*. arXiv. [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gatys，L.A.，Ecker，A.S.，以及 Bethge，M.（2015）。*一种艺术风格的神经算法*。arXiv。[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)
- en: Zhu, J-Y., Park, T., Isola, P., & Efros, A.A. (2017). *Unpaired Image-to-Image
    Translation using Cycle-Consistent Adversarial Networks*. arXiv. [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朱，J-Y.，帕克，T.，伊索拉，P.，以及埃弗罗斯，A.A.（2017）。*使用循环一致性对抗网络进行非配对图像到图像的翻译*。arXiv。[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)
- en: Isola, P., Zhu, J-Y., Zhou, T., & Efros, A.A. (2017). *Image-to-Image Translation
    with Conditional Adversarial Networks*. 2017 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), 2017, pp. 5967-5976\. [https://ieeexplore.ieee.org/document/8100115](https://ieeexplore.ieee.org/document/8100115)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 伊索拉，P.，朱，J-Y.，周，T.，以及埃弗罗斯，A.A.（2017）。*带条件对抗网络的图像到图像翻译*。2017 年 IEEE 计算机视觉与模式识别会议（CVPR），2017，pp.
    5967-5976。[https://ieeexplore.ieee.org/document/8100115](https://ieeexplore.ieee.org/document/8100115)
- en: 'Ronneberger, O., Fisher, P., & Brox, T. (2015). *U-net: Convolutional Networks
    for Biomedical Image Segmentation*. MICCAI, 2015\. [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ronneberger，O.，Fisher，P.，以及 Brox，T.（2015）。*U-net：用于生物医学图像分割的卷积网络*。MICCAI，2015。[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)
- en: Kim, T., Cha, M., Kim, H., Lee, J.K., & Kim, J. (2017). *Learning to Discover
    Cross-Domain Relations with Generative Adversarial Networks*. International Conference
    on Machine Learning (ICML) 2017\. [https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192)
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 金，T.，查，M.，金，H.，李，J.K.，以及金，J.（2017）。*用生成对抗网络学习发现跨域关系*。2017 年国际机器学习会议（ICML）。[https://arxiv.org/abs/1703.05192](https://arxiv.org/abs/1703.05192)
- en: 'Yi, Z., Zhang, H., Tan, P., & Gong, M. (2017). *DualGAN: Unsupervised Dual
    Learning for Image-to-Image Translation*. ICCV 2017\. [https://arxiv.org/abs/1704.02510v4](https://arxiv.org/abs/1704.02510v4)'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Yi, Z., Zhang, H., Tan, P., & Gong, M. (2017). *DualGAN: 无监督的图像到图像翻译的双向学习*.
    ICCV 2017\. [https://arxiv.org/abs/1704.02510v4](https://arxiv.org/abs/1704.02510v4)'
- en: Xia, Y., He, D., Qin, T., Wang, L., Yu, N., Liu, T-Y., & Ma, W-Y. (2016). *Dual
    Learning for Machine Translation*. NIPS 2016\. [https://arxiv.org/abs/1611.00179](https://arxiv.org/abs/1611.00179)
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Xia, Y., He, D., Qin, T., Wang, L., Yu, N., Liu, T-Y., & Ma, W-Y. (2016). *机器翻译的双向学习*.
    NIPS 2016\. [https://arxiv.org/abs/1611.00179](https://arxiv.org/abs/1611.00179)
