- en: Advanced Image Applications
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 高级图像应用
- en: The applications of artificial intelligence in computer vision include robotics, self-driving
    cars, facial recognition, recognizing diseases in biomedical images, and quality
    control in manufacturing, among many others.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉中的人工智能应用包括机器人技术、自动驾驶汽车、人脸识别、生物医学图像中的疾病识别以及制造业的质量控制等。
- en: In this chapter, we'll start with image recognition (or image classification),
    where we'll look into basic models and more advanced models. We'll then create
    images using **Generative Adversarial Networks** (**GANs**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从图像识别（或图像分类）开始，我们将探讨基本模型和更高级的模型。然后，我们将使用**生成对抗网络**（**GANs**）创建图像。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Recognizing clothing items
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别服装项目
- en: Generating images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成图像
- en: Encoding images and style
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码图像和样式
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We'll use many standard libraries, such as NumPy, Keras, and PyTorch, but we'll
    also see a few more libraries that we'll mention at the beginning of each recipe
    as they become relevant.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用许多标准库，如 NumPy、Keras 和 PyTorch，但我们也会看到一些在每个配方开始时会提到的更多库，因为它们变得相关。
- en: You can find the notebooks for this chapter's recipes on GitHub at [https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter07](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter07).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到本章配方的笔记本：[https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter07](https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Cookbook/tree/master/chapter07)。
- en: Recognizing clothing items
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别服装项目
- en: A popular example of image classification is the MNIST dataset, which contains
    digits from 0 to 9 in different styles. Here, we'll use a drop-in replacement,
    called Fashion-MNIST, consisting of different pieces of clothing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的一个流行例子是 MNIST 数据集，其中包含不同风格的数字从 0 到 9。在这里，我们将使用一种称为 Fashion-MNIST 的可替换数据集，其中包含不同的服装。
- en: Fashion-MNIST is a dataset of Zalando's article images consisting of a training
    set of 60,000 examples and a test set of 10,000 examples: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion-MNIST 是 Zalando 的服装图片数据集，包括一个由 60,000 个示例组成的训练集和一个由 10,000 个示例组成的测试集：[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)。
- en: 'Here are a few examples from the dataset:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据集中的一些示例：
- en: '![](img/8458cbc3-4fcb-4552-8188-b8a5750a77ed.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8458cbc3-4fcb-4552-8188-b8a5750a77ed.png)'
- en: In this recipe, we'll recognize clothing items with different models – we'll
    start with generic image features (**Difference of Gaussians**, or **DoG**) and
    a support vector machine; then we'll move on to a feedforward **Multilayer Perceptron**
    (**MLP**); then we'll use a **Convolutional Neural Network** (**ConvNet**); and
    finally, we'll look at transfer learning with MobileNet.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用不同的模型识别服装项目——我们将从通用图像特征（**高斯差分**或**DoG**）和支持向量机开始；然后我们将转向前馈**多层感知器**（**MLP**）；接着我们将使用**卷积神经网络**（**ConvNet**）；最后，我们将使用
    MobileNet 进行迁移学习。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before we can start, we have to install a library. In this recipe, we''ll use
    `scikit-image`, a library for image transformations, so we''ll quickly set this
    up:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们必须安装一个库。在这个配方中，我们将使用`scikit-image`，这是一个用于图像变换的库，因此我们将快速设置它：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We are now ready to move on to the recipe!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好进入配方了！
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: We'll first load and prepare the dataset, then we'll learn models for classifying
    clothing items from the Fashion-MNIST dataset using different approaches. Let's
    start by loading the Fashion-MNIST fashion dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载和准备数据集，然后我们将使用不同的方法学习 Fashion-MNIST 数据集中服装项目的分类模型。让我们从加载 Fashion-MNIST
    时尚数据集开始。
- en: 'We can get the dataset directly via a `keras` utility function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`keras`工具函数直接获取数据集：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are also normalizing the images within the range of 0 and 1 by dividing by
    the maximal pixel intensity (`255.0`), and we visualize the first image.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将图像标准化为 0 到 1 的范围，通过除以最大像素强度（`255.0`），并且我们可视化第一张图像。
- en: 'We should see the following image of a sneaker, the first image in the training
    set:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一张运动鞋的图片，这是训练集中的第一张图片：
- en: '![](img/8e0688ac-887e-4084-8015-78372b95ce7d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e0688ac-887e-4084-8015-78372b95ce7d.png)'
- en: 'As we mentioned in the introduction of this recipe, we''ll apply different
    approaches in the upcoming sections:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本配方介绍中提到的，我们将在接下来的几节中应用不同的方法：
- en: DoG features
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DoG 特征
- en: MLP
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLP
- en: LeNet
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeNet
- en: Transfer learning with MobileNet
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MobileNet 进行迁移学习
- en: Let's start with DoG.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 DoG 开始。
- en: Difference of Gaussians
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高斯差分
- en: Before the breakthroughs of deep learning in image recognition, images were
    featurized using filters from the Difference of Laplacians or Gaussians. These
    are implemented in `scikit-image`, so we can take an off-the-shelf implementation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习在图像识别中取得突破之前，图像是使用来自拉普拉斯差分或高斯的滤波器进行特征化的。这些功能在 `scikit-image` 中实现，因此我们可以采用现成的实现。
- en: 'Let''s write a function that extracts image features using a Gaussian pyramid:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个函数，使用高斯金字塔提取图像特征：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `get_pyramid_features()` function applies the Gaussian pyramid and returns
    these features flattened as a vector. We'll explain what a Gaussian pyramid is in
    the *How it works...* section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_pyramid_features()` 函数应用高斯金字塔并将这些特征展平为一个向量返回。我们将在 *它是如何工作...* 部分解释什么是高斯金字塔。'
- en: 'We are nearly ready to start learning. We only need to iterate over all the
    images and extract our Gaussian pyramid features. Let''s create another function
    that does that:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好开始学习了。我们只需迭代所有图像并提取我们的高斯金字塔特征。让我们创建另一个执行此操作的函数：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For training the model, we apply the `featurize()` function on our training
    dataset. We''ll use a linear support vector machine as our model. Then, we''ll
    apply this model to the features extracted from our test dataset - please note
    that this might take a while to run:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们在我们的训练数据集上应用 `featurize()` 函数。我们将使用线性支持向量机作为我们的模型。然后，我们将此模型应用于从我们的测试数据集中提取的特征
    - 请注意，这可能需要一些时间才能运行：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We get 84% accuracy over the validation dataset from a linear support vector
    machine using these features. With some more tuning of the filters, we could have
    achieved higher performance, but that is beyond the scope of this recipe. Before
    the publication of AlexNet in 2012, this method was one of the state-of-the-art
    methods for image classification.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些特征的线性支持向量机在验证数据集上获得了 84% 的准确率。通过调整滤波器，我们可以达到更高的性能，但这超出了本文的范围。在 2012 年 AlexNet
    发布之前，这种方法是图像分类的最先进方法之一。
- en: Another way to train a model is to flatten the images and feed the normalized
    pixel values directly into a classifier, such as an MLP. That is what we'll try
    now.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的另一种方法是将图像展平，并直接将归一化的像素值输入到分类器中，例如 MLP。这是我们现在要尝试的。
- en: Multilayer perceptron
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多层感知器
- en: A relatively simple way to classify images is with an MLP. In this case of a
    two-layer MLP with 10 neurons, you can think of the hidden layer as a feature
    extraction layer of 10 feature detectors.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 用 MLP 对图像进行分类的一个相对简单的方法是。在这种情况下，使用了一个具有 10 个神经元的两层 MLP，你可以将隐藏层看作是 10 个特征检测器的特征提取层。
- en: We have seen examples of MLPs already a few times in this book, so we'll skip
    over the details here; perhaps of interest is that we flatten images from 28x28
    to a vector of 784\. As for the rest, suffice it to say that we train for categorical
    cross-entropy and we'll monitor accuracy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中我们已经多次看到 MLP 的示例，因此我们将跳过此处的细节；可能感兴趣的是，我们将图像从 28x28 展平为 784 的向量。至于其余部分，可以说我们训练分类交叉熵，并监视准确性。
- en: 'You''ll see this in the following code block:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在以下代码块中看到这一点：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This model has 101,770 trainable parameters between the two layers and their
    connections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在两层及其连接之间有 101,770 个可训练参数。
- en: 'We''ll use the following function to wrap our training set. It should be fairly
    self-explanatory:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下函数封装我们的训练集。这应该是相当容易理解的：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After 50 epochs, our accuracy in the validation set is 0.886.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 50 个周期，我们在验证集上的准确率为 0.886。
- en: The next model is the classic ConvNet proposed for MNIST, employing convolutions,
    pooling, and fully connected layers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个模型是经典的 ConvNet，为 MNIST 提出，使用卷积、池化和全连接层。
- en: LeNet5
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LeNet5
- en: 'LeNet5 is a feedforward neural network with convolutional layers and max pooling,
    as well as fully connected feedforward layers for features leading to the read-out.
    Let''s see how it performs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet5 是一个前馈神经网络，具有卷积层和最大池化，以及用于导致输出的特征的全连接前馈层。让我们看看它的表现：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `create_lenet()` function builds our model. We only have to call it and
    run our `train_model()` function with it in order to fit it to the training dataset
    and see our test performance:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_lenet()` 函数构建我们的模型。我们只需调用它，并使用它运行我们的 `train_model()` 函数，以适应训练数据集并查看我们的测试表现：'
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After 50 episodes, our validation accuracy is at 0.891.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 50 集数，我们的验证准确率为 0.891。
- en: 'We can also have a look at the confusion matrix to see how well we distinguish
    particular pieces of clothing from others:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看混淆矩阵，以查看我们如何区分特定的服装类别：
- en: '![](img/8d5c07cf-93cf-4cfc-a2da-19b598b12494.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d5c07cf-93cf-4cfc-a2da-19b598b12494.png)'
- en: Let's move on to our last attempt at classifying the clothing items.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行我们最后一次尝试对服装项进行分类。
- en: MobileNet transfer learning
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MobileNet迁移学习
- en: The MobileNetV2 model was trained on ImageNet, a database of 14 million images
    that have been hand-annotated into categories of the WordNet taxonomy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV2模型是在ImageNet上训练的，这是一个包含1400万张图像的数据库，已手动注释为WordNet分类系统的类别。
- en: 'MobileNet can be downloaded with weights for transfer learning. This means
    that we leave most or all of MobileNet''s weights fixed. In most cases, we would
    only add a new output projection in order to discriminate a new set of classes
    on top of the MobileNet representation:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet可以下载用于迁移学习的权重。这意味着我们保持大部分或所有MobileNet的权重固定。在大多数情况下，我们只需添加一个新的输出投影来区分MobileNet表示之上的新类别集：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: MobileNet comprises 2,257,984 parameters. Downloading MobileNet, we have the
    convenient option of leaving out the output layer (`include_top=False`), which
    saves us work.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet包含2,257,984个参数。下载MobileNet时，我们有方便的选项可以省略输出层（`include_top=False`），这样可以节省工作量。
- en: 'For our transfer model, we have to append a pooling layer, and then we can
    append an output layer just as in the previous two neural networks:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的迁移模型，我们必须附加一个池化层，然后像前两个神经网络一样附加一个输出层：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Please note that we freeze or fix the weights in the MobileNet model, and only
    learn the two layers that we add on top.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在MobileNet模型中，我们会冻结或固定权重，只学习我们添加在顶部的两个层。
- en: 'You might have noticed a detail when we downloaded MobileNet: we specified
    dimensions of 224x224x3\. MobileNet comes with different input shapes, and 224x224x3
    is one of the smallest. This means that we have to rescale our images and convert
    them to RGB (by concatenating the grayscales). You can find the details for that
    in the online notebook on GitHub.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们下载MobileNet时，您可能已经注意到一个细节：我们指定了224x224x3的尺寸。MobileNet有不同的输入形状，224x224x3是最小的之一。这意味着我们必须重新缩放我们的图像，并将它们转换为RGB（通过串联灰度）。您可以在GitHub上的在线笔记本中找到详细信息。
- en: The validation accuracy for MobileNet transfer learning is very similar to LeNet
    and our MLP: 0.893.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet迁移学习的验证准确率与LeNet和我们的MLP非常相似：0.893。
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Image classification consists of assigning a label to an image, and this was
    where the deep learning revolution started.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类包括为图像分配标签，这是深度学习革命开始的地方。
- en: 'The following graph, taken from the preceding URL, illustrates the performance
    increase for the ImageNet benchmark for image classification over time:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述URL获取的以下图表说明了ImageNet图像分类基准随时间的性能提高：
- en: '![](img/03666f38-8879-4237-a09d-82e3161cccbc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03666f38-8879-4237-a09d-82e3161cccbc.png)'
- en: TOP 1 ACCURACY (also more simply called accuracy) on the *y* axis is a metric that measures
    the proportion of correct predictions over all predictions, or in other words,
    the ratio of how often an object was correctly identified. The State-of-the-art line
    on the graph has been continuously improving over time (the *x* axis), until now,
    reaching an 87.4% accuracy rate with the NoisyStudent method (see here for details: [https://paperswithcode.com/paper/self-training-with-noisy-student-improves](https://paperswithcode.com/paper/self-training-with-noisy-student-improves)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图上的TOP 1准确率（也更简单地称为准确率）在*y*轴上是一个度量，用于衡量所有预测中正确预测的比例，或者换句话说，正确识别对象的频率的比率。随着时间的推移（*x*轴），图上的最先进线一直在持续改进，直到现在，使用NoisyStudent方法达到了87.4%的准确率（详细信息请见此处：[https://paperswithcode.com/paper/self-training-with-noisy-student-improves](https://paperswithcode.com/paper/self-training-with-noisy-student-improves)）。
- en: 'In the following graph, you can see a timeline of deep learning in image recognition,
    where you can see the increasing complexity (in terms of the number of layers)
    and the decreasing error rate in the **ImageNet Large-Scale Visual Recognition
    Challenge** (**ILSVRC**):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图表中，您可以看到图像识别中深度学习的时间线，可以看到复杂性（层数）的增加以及ImageNet大规模视觉识别挑战(ILSVRC)中错误率的降低：
- en: '![](img/b2f9c9ac-28ad-467a-bae3-1cacc46b2e4c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2f9c9ac-28ad-467a-bae3-1cacc46b2e4c.png)'
- en: You can find more details about the challenge at [http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://www.image-net.org/challenges/LSVRC/](http://www.image-net.org/challenges/LSVRC/)找到有关挑战的更多详细信息。
- en: Difference of Gaussian
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高斯差分
- en: The Gaussian pyramid is a series of recursively downscaled versions of the original
    image, where the downscaling is performed using a Gaussian filter. You can find
    detailed information on Scholarpedia at [http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform](http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯金字塔是原始图像的一系列经过递归缩小的版本，其中缩小是使用高斯滤波器完成的。您可以在 Scholarpedia 上详细了解 [http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform](http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform)。
- en: We used utility functions from `skimage` to extract the features, then we applied
    a linear support vector machine on top as the classifier. We could have tried
    other classifiers, such as random forest or gradient boosting, instead in order
    to improve the performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `skimage` 的实用函数来提取特征，然后在顶部应用线性支持向量机作为分类器。为了提高性能，我们本可以尝试其他分类器，如随机森林或梯度提升。
- en: LeNet5
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LeNet5
- en: A CNN or ConvNet is a neural network with at least one convolutional layer.
    LeNet, a classic example of a ConvNet, was proposed by Yann LeCun and others in
    its original form in 1989 (*backpropagation applied to handwritten zip code recognition*)
    and in the revised form (called LeNet5) in 1998 (*gradient-based learning applied
    to document recognition*).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 或 ConvNet 是至少包含一个卷积层的神经网络。LeNet 是 ConvNet 的经典示例，最初由 Yann LeCun 等人提出，1989
    年的原始形式是 (*应用于手写邮政编码识别的反向传播*)，1998 年的修订形式（称为 LeNet5）是 (*应用于文档识别的基于梯度的学习*)。
- en: 'You can see the LeNet architecture in the following diagram (created using
    the NN-SVG tool at [http://alexlenail.me/NN-SVG](http://alexlenail.me/NN-SVG)):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图表中看到 LeNet 的架构（使用 NN-SVG 工具在 [http://alexlenail.me/NN-SVG](http://alexlenail.me/NN-SVG)
    创建）：
- en: '![](img/0828b177-fe32-4e10-a3b4-d91d587ad95e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0828b177-fe32-4e10-a3b4-d91d587ad95e.png)'
- en: Convolutions are a very important transformation in image recognition and are
    one of the most important building blocks of very deep neural networks in image
    recognition. Convolutions consist of feedforward connections, called filters or
    kernels, which are applied to rectangular patches of the image (the previous layer).
    Each resulting map is then the sliding window of the kernel over the whole image.
    These convolutional maps are usually followed by subsampling by pooling layers
    (in the case of LeNet, the maximum of each kernel is extracted).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积在图像识别中是非常重要的转换，也是非常深的神经网络中图像识别的重要组成部分。卷积包括前馈连接，称为过滤器或卷积核，应用于图像的矩形补丁（上一层）。然后，每个生成的映射是核在整个图像上滑动的结果。这些卷积映射通常后面跟随由池化层进行的子采样（在
    LeNet 的情况下，是从每个核中提取的最大值）。
- en: MobileNet transfer learning
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MobileNet 迁移学习
- en: 'MobileNets (Howard and others, *MobileNets: Efficient Convolutional Neural
    Networks for Mobile Vision**Applications*; 2017; [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)) are
    a class of models developed by Google for efficiency with mobile and embedded
    applications, keeping in mind explicitly trading off latency versus accuracy.
    MobileNet consists largely of stacked convolutional layers of different shapes.
    All convolutional layers are followed by batch normalization and ReLU activation.
    The final convolution is followed by an average pooling layer, which removes the
    spatial dimension, and a final dense read-out layer with a softmax function.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 'MobileNets（Howard 等人，*MobileNets: 高效卷积神经网络用于移动视觉应用*；2017；[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)）是由
    Google 开发的一类模型，专为移动和嵌入式应用而设计，明确权衡延迟与准确性。MobileNet 主要由不同形状的堆叠卷积层组成。所有卷积层后跟批归一化和
    ReLU 激活。最后一个卷积层后是一个平均池化层，去除空间维度，以及一个带有 softmax 函数的最终密集输出层。'
- en: Loading the model is a matter of a single command in Keras. The `tf.keras.applications`
    package exposes architectures and weights for many models, such as DenseNet, EfficientNet
    Inception-ResNet V2, Inception V3, MobileNetV1, MobileNetV2, NASNet-A, ResNet,
    ResNet v2, VGG16, VGG19, and Xception V1\. In our case, we have a pre-trained
    model, which means it has architecture and weights with the `tf.keras.applications.MobileNetV2()` function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，加载模型只需一条命令。`tf.keras.applications` 包提供了许多模型的架构和权重，例如 DenseNet、EfficientNet、Inception-ResNet
    V2、Inception V3、MobileNetV1、MobileNetV2、NASNet-A、ResNet、ResNet v2、VGG16、VGG19
    和 Xception V1。在我们的案例中，我们有一个预训练模型，这意味着它具有使用 `tf.keras.applications.MobileNetV2()`
    函数的架构和权重。
- en: We can retrain (fine-tune) the model to improve performance for our application,
    or we could use the model as is and put additional layers on top in order to classify
    new classes.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新训练（微调）模型以提高应用性能，或者我们可以使用原始模型，并在其上添加额外的层以对新类进行分类。
- en: 'What we do to load the model is this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载模型的操作是这样的：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As mentioned before, we have the option of specifying different input shapes
    from a selection of choices. `include_top` specifies whether we want to include
    the classification layer. If we wanted to use the original model output for the
    ImageNet dataset on which it has been trained, we would set this to `True`. Since
    we have different classes in our dataset, we want to set this to `False`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前提到的，我们可以从多个选择中指定不同的输入形状。`include_top`指定是否包含分类层。如果我们想要使用在ImageNet数据集上训练过的原始模型输出，我们会将其设置为`True`。由于我们的数据集中有不同的类别，我们想将其设置为`False`。
- en: If we wanted to fine-tune the model (with or without the top), we would leave
    the base model (MobileNetV2) trainable. Obviously, the training could take much
    longer that way since many more layers would need to be trained. That's why we've
    frozen all of MobileNetV2's layers during training, setting its `trainable` attribute
    to `False`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要微调模型（带或不带顶部），我们将保持基础模型（MobileNetV2）可训练。显然，这种训练方式可能需要更长的时间，因为需要训练更多的层。这就是为什么在训练过程中我们冻结了所有MobileNetV2的层，并将其`trainable`属性设置为`False`。
- en: See also
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: You can find a review of ConvNet, from LeNet over AlexNet to more recent architectures,
    in *A Survey of the Recent Architectures of Deep Convolutional Neural Networks*
    by Khan and others (2020), available from arXiv: [https://arxiv.org/pdf/1901.06032.pdf](https://arxiv.org/pdf/1901.06032.pdf).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*Khan等人（2020）在arXiv上发表的《深度卷积神经网络最近架构的综述》中找到对ConvNet的回顾，从LeNet到AlexNet再到更近期的架构，可在此链接获取：[https://arxiv.org/pdf/1901.06032.pdf](https://arxiv.org/pdf/1901.06032.pdf)。
- en: A more recent architecture is EfficientNet (Mingxing Tan and Quoc V. Le, 2019)
    which achieves state-of-the-art performance on ImageNet, while close to a magnitude
    smaller and about five times faster than the best ConvNets: [https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更近期的架构是EfficientNet（Mingxing Tan和Quoc V. Le，2019），在ImageNet上实现了最先进的性能，同时比最佳ConvNet小约一个数量级，并且比最佳ConvNet快大约五倍：[https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)。
- en: Generating images
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成图像
- en: Adversarial learning with GANs, introduced by Ian Goodfellow and others in 2014,
    is a framework for fitting the distributions of a dataset by pairing two networks
    against each other in a way that one model generates examples and the others discriminate,
    whether they are real or not. This can help us to extend our dataset with new
    training examples. Semi-supervised training with GANs can help achieve higher
    performance in supervised tasks while using only small amounts of labeled training
    examples.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年由Ian Goodfellow等人引入的GAN对抗学习，是一种通过两个网络相互对抗的框架来拟合数据集的分布，其中一个模型生成示例，另一个模型区分这些示例是真实的还是虚假的。这可以帮助我们使用新的训练示例扩展我们的数据集。使用GAN的半监督训练可以在使用少量标记训练示例的同时，实现更高的监督任务性能。
- en: The focus of this recipe is implementing a **Deep Convolutional Generative Adversarial
    Network** (**DCGAN**) and a discriminator on the MNIST dataset, one of the best-known
    datasets, consisting of 60,000 digits between 0 and 9\. We'll explain the terminology
    and background in the *How it works...* section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 本篇重点是在MNIST数据集上实现**深度卷积生成对抗网络**（**DCGAN**）和鉴别器，这是最知名的数据集之一，包含60,000个0到9之间的数字。我们将在*工作原理...*部分解释术语和背景。
- en: Getting ready
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We don''t need any special libraries for this recipe. We''ll use TensorFlow
    with Keras, NumPy, and Matplotlib, all of which we''ve seen earlier. For saving
    images, we''ll use the Pillow library, which you can install or upgrade as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个步骤，我们不需要任何特殊的库。我们将使用TensorFlow与Keras，NumPy和Matplotlib，这些都是我们之前见过的。为了保存图像，我们将使用Pillow库，你可以按以下方式安装或升级：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's jump right into it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们马上开始吧。
- en: How to do it...
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: For our approach with a GAN, we need a generator – a network that takes some
    input, which could be noise – and a discriminator, an image classifier, such as
    the one seen in the *Recognizing clothing items* recipe of this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的GAN方法，我们需要一个生成器——一个接受某些输入（可能是噪声）的网络——以及一个鉴别器，一个图像分类器，例如本章中*识别服装项目*食谱中看到的那样。
- en: Both the generator and discriminator are deep neural networks, and the two will
    be paired together for training. After training, we'll see the training loss,
    example images over epochs, and a composite image of the final epoch.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和鉴别器都是深度神经网络，并将一起进行训练。训练后，我们将看到训练损失、各个时期的示例图像以及最后时期的复合图像。
- en: First, we will design the discriminator.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设计鉴别器。
- en: 'This is a classic example of a ConvNet. It''s a series of convolution and pooling
    operations (typically average or max pooling), followed by flattening and an output
    layer. For more details, please see the *Recognizing clothing items* recipe of
    this chapter:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个经典的ConvNet示例。它是一系列卷积和池化操作（通常是平均或最大池化），接着是平坦化和输出层。更多详情，请参见本章的*识别服装项目*示例：
- en: '[PRE13]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is very similar to the LeNet convolution block, which we introduced in
    the *Recognizing clothing items* recipe of this chapter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在本章的*识别服装项目*示例中介绍的LeNet卷积块非常相似。
- en: Next, we design the generator.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设计生成器。
- en: While discriminators downsample their input with convolutions and pooling operations,
    generators upsample. Our generator takes an input vector of 100 dimensions and
    generates an image from this by performing the operations in the opposite direction
    of a ConvNet. For this reason, this type of network is sometimes referred to as
    a DeconvNet.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然鉴别器通过卷积和池化操作对其输入进行下采样，生成器则进行上采样。我们的生成器接受一个100维的输入向量，并通过执行与ConvNet相反方向的操作生成图像。因此，这种类型的网络有时被称为DeconvNet。
- en: 'The output of the generator is normalized back within the range of -1 to 1
    by the Tanh activation. One of the major contributions of the DCGAN paper (Alec
    Radford and others, 2015, *Unsupervised Representation Learning with Deep Convolutional
    Generative Adversarial Networks*) was the introduction of batch normalization
    after the deconvolutional operation. In Keras, there are two ways to achieve deconvolution:
    one is by using `UpSampling2D` (see [https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D)),
    and the other by using `Conv2DTranspose`. Here, we chose `UpSampling2D`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的输出通过Tanh激活函数重新标准化到-1到1的范围内。DCGAN论文（Alec Radford等人，2015年，《无监督学习中的深度卷积生成对抗网络》）的一个主要贡献之一是在反卷积操作后引入批标准化。在Keras中，有两种实现反卷积的方法：一种是使用`UpSampling2D`（参见[https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D)），另一种是使用`Conv2DTranspose`。这里，我们选择了`UpSampling2D`：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Calling this function, we'll get an output of our network architecture by using `summary()`.
    We'll see that we have 6,751,233 trainable parameters. We'd recommend running
    this on a powerful system – for example, Google Colab.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此函数，我们将使用`summary()`获取我们网络架构的输出。我们将看到有6,751,233个可训练参数。我们建议在强大的系统上运行此示例，例如Google
    Colab。
- en: 'For training the network, we load the MNIST dataset and normalize it:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练网络，我们加载并标准化了MNIST数据集：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The images come in grayscale with pixel values of 0–255\. We normalize into
    the range -1 and +1\. We then reshape to give a singleton dimension at the end.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图像以灰度形式呈现，像素值范围在0–255之间。我们将其标准化到-1到+1的范围内，然后重新调整为在末尾具有单例维度。
- en: 'For the error to feed through to the generator, we chain the generator with
    the discriminator, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将误差传递给生成器，我们将生成器与鉴别器链在一起，如下所示：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As our optimizer, we''ll use Keras stochastic gradient descent:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们的优化器，我们将使用Keras的随机梯度下降：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s create and initialize our models:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建并初始化我们的模型：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A single training step consists of three steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 单个训练步骤包括三个步骤：
- en: The generator generates images from noise.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器从噪声中生成图像。
- en: The discriminator learns to distinguish generated images from real images.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器学习区分生成的图像和真实的图像。
- en: The generator learns to create better images given the discriminator feedback.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器通过鉴别器的反馈学习创建更好的图像。
- en: 'Let''s go through these in turn. First is generating images from noise:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们依次进行这些步骤。首先是从噪声生成图像：
- en: '[PRE19]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, the discriminator learns when given fake and real images:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，鉴别器在给定假和真实图像时进行学习：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We concatenate true, `1`, and fake, `0`, images for the input to the discriminator.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将真实的`1`和假的`0`图像串联起来，作为鉴别器的输入。
- en: 'Finally, the generator learns from the discriminator feedback:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，生成器从鉴别器的反馈中学习：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Please note the inversion of the discriminator target in this function. Instead
    of 0 (for fake, as before), we feed 1s. It is important to note as well that parameters
    in the discriminator are fixed during the learning of the generator (otherwise,
    we'd unlearn again).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个函数中，鉴别器目标的反转。与以前的假0不同，我们现在输入1。同样重要的是，在生成器学习期间，鉴别器的参数是固定的（否则我们将再次忘记）。
- en: 'We can incorporate additional commands into our training in order to save images
    so that we can appreciate our generator progress visually:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在训练中加入额外的命令，以保存图像，以便可以通过视觉方式欣赏我们生成器的进展：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Our training works, then, by alternating the preceding steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练通过交替进行以下步骤来进行：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We let it run. The `tqdm` progress bars will show us how much time is left.
    It might take about an hour on Google Colab.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们让它运行。`tqdm` 进度条将显示剩余时间。在Google Colab上可能需要大约一个小时。
- en: 'Over 100 epochs, our training errors look as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在100个epochs中，我们的训练错误看起来如下：
- en: '![](img/e490a7c7-b0cb-40c6-9606-de3a8a827f58.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e490a7c7-b0cb-40c6-9606-de3a8a827f58.png)'
- en: 'We have saved images – so, we can also have a look at the generator output
    over epochs. Here''s a gallery of a single randomly generated digit for each of
    the 100 epochs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经保存了图像，因此我们也可以查看生成器在epochs中的输出。以下是每个100个epochs中单个随机生成的数字的画廊：
- en: '![](img/d71821c3-f9fd-40f4-811c-2f6fb8dd1455.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d71821c3-f9fd-40f4-811c-2f6fb8dd1455.png)'
- en: 'We can see that images generally become sharper and sharper. This is interesting,
    because the training error for the generator seems to stay at the same baseline
    after the first couple of epochs. Here''s an image that shows the 100 generated
    images during the last epoch:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到图像通常变得越来越清晰。有趣的是，生成器的训练误差似乎在前几个epochs之后保持在相同的基线水平。这是一个显示最后一个epoch期间生成的100个图像的图像：
- en: '![](img/aa2dab9c-b996-49e5-b2dc-3319eab236ce.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa2dab9c-b996-49e5-b2dc-3319eab236ce.png)'
- en: The images are not perfect, but most of them are recognizable as digits.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图像并不完美，但大部分可以识别为数字。
- en: How it works...
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何运作...
- en: Generative models can generate new data with the same statistics as the training
    set, and this can be useful for semi-supervised and unsupervised learning. GANs
    were introduced by Ian Goodfellow and others in 2014 (*Generative Adversarial
    Nets*, in NIPS; [https://papers.nips.cc/paper/5423-generative-adversarial-nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets))
    and DCGANs by Alec Radford and others in 2015 (*Unsupervised Representation Learning
    with Deep Convolutional Generative Adversarial Networks*; [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)).
    Since the original papers, many incremental improvements have been proposed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以生成具有与训练集相同统计特性的新数据，这对半监督和无监督学习很有用。GAN由Ian Goodfellow等人在2014年（*Generative
    Adversarial Nets*，NIPS；[https://papers.nips.cc/paper/5423-generative-adversarial-nets](https://papers.nips.cc/paper/5423-generative-adversarial-nets)）引入，而DCGAN由Alec
    Radford等人在2015年（*Unsupervised Representation Learning with Deep Convolutional
    Generative Adversarial Networks*；[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)）引入。自原始论文以来，已提出许多增量改进。
- en: In the GAN technique, the generative network learns to map from a seed – for
    example, randomized input to the target data distribution – while the discriminative
    network evaluates and discriminates data produced by the generator from the true
    data distribution.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN技术中，生成网络学习将一个种子（例如，随机输入）映射到目标数据分布，而鉴别网络评估并区分生成器产生的数据和真实数据分布。
- en: The generative network generates data while the discriminative network evaluates
    them. The two neural networks compete with each other, where the generative network's
    training increases the error rate of the discriminative network and the discriminator
    training increases the error of the generator, thereby engaging in a weapons race,
    forcing each other to become better.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网络生成数据，鉴别网络评估数据。这两个神经网络相互竞争，生成网络的训练增加了鉴别网络的错误率，而鉴别器的训练增加了生成器的错误率，从而进行武器竞赛，迫使彼此变得更好。
- en: In training, we feed random noise into our generator and then let the discriminator
    learn how to classify generator output against genuine images. The generator is
    then trained given the output of the discriminator, or rather the inverse of it.
    The less likely the discriminator judges an image a fake, the better for the generator,
    and vice versa.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练中，我们将随机噪声输入生成器，然后让鉴别器学习如何对生成器输出与真实图像进行分类。然后，给定鉴别器的输出，或者说其反向的生成器进行训练。鉴别器判断图像为假的可能性越小，对生成器越有利，反之亦然。
- en: See also
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: The original GAN paper, *Generative Adversarial Networks* (Ian Goodfellow and
    others; 2014), is available from arXiv: [https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GAN论文，*Generative Adversarial Networks*（Ian Goodfellow等人，2014年），可以在arXiv上找到：[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)。
- en: The DCGAN paper, *Unsupervised Representation Learning with Deep Convolutional
    Generative Adversarial Networks* (Alec Radford and others; 2015), is available
    on arXiv as well: [https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN论文，*Unsupervised Representation Learning with Deep Convolutional Generative
    Adversarial Networks*（Alec Radford等人，2015年），也可以在arXiv上找到：[https://arxiv.org/abs/1511.06434](https://arxiv.org/abs/1511.06434)。
- en: You can find a tutorial on DCGANs on the PyTorch website at [https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在PyTorch网站上找到有关DCGAN的教程：[https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)。
- en: There are many more GAN architectures that are worth exploring. Erik Linder-Norén
    implemented dozens of state-of-the-art architectures in both PyTorch and Keras.
    You can find them in his GitHub repositories ([https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN) and [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN),
    respectively).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多值得探索的GAN架构。Erik Linder-Norén在PyTorch和Keras中实现了数十种最先进的架构。你可以在他的GitHub仓库中找到它们：[https://github.com/eriklindernoren/PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN)和[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)。
- en: Encoding images and style
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像和风格编码
- en: Autoencoders are useful for representing the input efficiently. In their 2016
    paper, Makhazani and others showed that adversarial autoencoders can create clearer
    representations than variational autoencoders, and – similar to the DCGAN that
    we saw in the previous recipe – we get the added benefit of learning to create
    new examples, which can help in semi-supervised or supervised learning scenarios,
    and allow training with less labeled data. Representing in a compressed fashion
    can also help in content-based retrieval.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器在有效地表示输入方面非常有用。在2016年的一篇论文中，Makhazani等人展示了对抗自编码器可以比变分自编码器创建更清晰的表示，并且——与我们在前一示例中看到的DCGAN类似——我们获得了学习创建新示例的额外好处，这对半监督或监督学习场景有帮助，并且允许使用更少的标记数据进行训练。以压缩方式表示还有助于基于内容的检索。
- en: In this recipe, we'll implement an adversarial autoencoder in PyTorch. We'll
    implement both supervised and unsupervised approaches and show the results. There's
    a nice clustering by classes in the unsupervised approach, and in the supervised
    approach, our encoder-decoder architecture can identify styles, which gives us
    the ability to do style transfer. In this recipe, we'll use the *hello world*
    dataset of computer vision, MNIST.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将在PyTorch中实现对抗自编码器。我们将实现监督和无监督两种方法，并展示结果。在无监督方法中，类别之间有很好的聚类效果；在监督方法中，我们的编码器-解码器架构能够识别风格，从而使我们能够进行风格转移。在这个示例中，我们将使用计算机视觉的*hello
    world*数据集MNIST。
- en: Getting ready
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We''ll need `torchvision` for this recipe. This will help us download our dataset.
    We''ll quickly install it:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将需要使用`torchvision`。这将帮助我们下载我们的数据集。我们将快速安装它：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For PyTorch, we''ll need to get a few preliminaries out of the way, such as
    to enable `CUDA` and set `tensor` type and `device`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PyTorch，我们需要先进行一些准备工作，比如启用`CUDA`并设置`tensor`类型和`device`：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In a break from the style in other recipes, we''ll also get the imports out
    of the way:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他示例风格不同，我们还会先导入必要的库：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, let's get to it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧。
- en: How to do it...
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么做…
- en: We'll implement an adversarial autoencoder in this recipe and apply it to the
    MNIST dataset of digits. This code is based on the implementation by Maurits Diephuis
    and Shideh Rezaeifar: [https://github.com/mdiephuis/adversarial-autoencoders](https://github.com/mdiephuis/adversarial-autoencoders).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将实现一个对抗自编码器，并将其应用于MNIST手写数字数据集。这段代码基于Maurits Diephuis和Shideh Rezaeifar的实现：[https://github.com/mdiephuis/adversarial-autoencoders](https://github.com/mdiephuis/adversarial-autoencoders)。
- en: We'll first get the imports out of the way. Then, we'll load our dataset, define
    the model components, including the encoder, decoder, and discriminator, then
    we'll do our training, and finally, we'll visualize the resulting representations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将导入必要的库。然后，我们将加载我们的数据集，定义模型组件，包括编码器、解码器和判别器，然后进行训练，最后我们将可视化生成的表示。
- en: First is loading the dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是加载数据集。
- en: 'We''ll need to set a few global variables that will define training and the
    dataset. Then, we load our dataset:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置一些全局变量，这些变量将定义训练和数据集。然后，我们加载我们的数据集：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The transformation in the normalize corresponds to the mean and standard deviation
    of the MNIST dataset.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 规范化中的转换对应于MNIST数据集的均值和标准差。
- en: Next is defining the autoencoder model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是定义自编码器模型。
- en: The autoencoder consists of an encoder, a decoder, and a discriminator. If you
    are familiar with autoencoders, this is nothing new for you. You'll find an explanation
    in the next section, *How it works...*, where this is broken down.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器由编码器、解码器和判别器组成。如果您熟悉自编码器，这对您来说不是什么新东西。在下一节*工作原理……*中，我们将对其进行详细解释和分解。
- en: 'First, we''ll define our encoder and decoder:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义我们的编码器和解码器：
- en: '[PRE28]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Please note `dim`, the parameter that stands for the size of the representational
    layer. We choose `10` as the size of our encoding layer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`dim`参数，它表示表示层的大小。我们选择`10`作为我们编码层的大小。
- en: 'Then, we''ll define our decoder:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将定义我们的解码器：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'While we are at it, we can also define our discriminator to compete against
    our encoder:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，我们还可以定义我们的判别器来与我们的编码器竞争：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Please note that we squash our outputs in order to stay within the range of
    0 and 1\. This will become important for our loss function.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了保持在0和1的范围内，我们压缩了我们的输出。这对我们的损失函数非常重要。
- en: Then comes training the model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是训练模型。
- en: 'The adversarial autoencoder can be used in a supervised fashion, where labels
    are fed into the decoder in addition to the encoding output. In that case, we''ll
    need one more utility function, which one-hot encodes our variables:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗自编码器可以在有监督的方式下使用，其中标签被输入到解码器中，除了编码输出之外，我们还需要一个实用函数，将变量进行独热编码：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We''ll show how to use the adversarial autoencoder with and without labels:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何在有标签和无标签情况下使用对抗自编码器：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As you can see in the comment, we''ve broken out the training loop. The training
    loop looks as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在评论中所见，我们已经拆分出训练循环。训练循环如下所示：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: For this code segment, we'll discuss in the *How it works...* section how three
    different losses are calculated and back-propagated. Please also note the supervised
    parameter, which defines whether we want to use supervised or unsupervised training.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这段代码，我们将在*工作原理……*部分讨论如何计算和反向传播三种不同的损失。还请注意监督参数，它定义了我们是要使用监督还是无监督训练。
- en: 'Now, let''s initialize our models and optimizers:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化我们的模型和优化器：
- en: '[PRE34]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we can start training:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This does nothing much except for calling the `train_validate()` function defined
    previously, once with the `train=True` option and once with `train=False`. From
    both calls, we collect the errors for training and validation, respectively.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这没什么大不了的，除了之前定义的`train_validate()`函数的调用，一次是`train=True`选项，一次是`train=False`选项。从这两个调用中，我们分别收集用于训练和验证的错误。
- en: 'The training and validation errors go consistently down, as we can see in the
    following graph:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和验证错误持续下降，正如我们在下面的图表中所见：
- en: '![](img/8632423a-edfe-4697-8b33-df183ffd69f4.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8632423a-edfe-4697-8b33-df183ffd69f4.png)'
- en: If you run this, compare the generator and discriminator losses – it's interesting
    to see how the generator and discriminator losses drive each other.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此代码，比较生成器和判别器的损失——看到生成器和判别器的损失如何相互影响是很有趣的。
- en: The next step is visualizing representations.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是可视化表示：
- en: 'In the supervised condition, the projections of the encoder space do not have much
    to do with the classes, as you can see in the following `tsne` plot:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在有监督条件下，编码器空间的投影与类别关系不大，正如您在下面的`tsne`图中所见：
- en: '![](img/9d13d053-0ff3-46d3-8f6f-74112e6fad11.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d13d053-0ff3-46d3-8f6f-74112e6fad11.png)'
- en: This is a 2D visualization of the encoder's representation space of the digits.
    The colors (or shades if you are looking at this in black and white), representing
    different digits, are all grouped together, rather than being separated into clusters.
    The encoder is not distinguishing between different digits at all.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是编码器数字表示空间的二维可视化。颜色（或者如果您在黑白显示器上看的话，阴影）代表不同的数字，它们都被集中在一起，而不是分成群集。编码器根本不区分不同的数字。
- en: 'What is encoded is something else entirely, which is style. In fact, we can
    vary the input into the decoder on each of the two dimensions separately in order
    to show this:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 编码的东西完全是别的，那就是风格。事实上，我们可以分别在两个维度上变化输入到解码器中，以展示这一点：
- en: '![](img/1c68c6ac-0d69-4022-be7c-cd1be83e6532.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c68c6ac-0d69-4022-be7c-cd1be83e6532.png)'
- en: The first five rows correspond to a linear range of the first dimension with
    the second dimension kept constant, then in the next five rows, the first dimension
    is fixed and the second dimension is varied. We can appreciate that the first
    dimension corresponds to the thickness and the second to inclination. This is
    called style transfer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 前五行对应第一个维度的线性范围，第二个维度保持恒定，然后在接下来的五行中，第一个维度固定，第二个维度变化。我们可以看到第一个维度对应厚度，第二个维度对应倾斜度。这被称为风格转移。
- en: 'We can also try the unsupervised training, by setting `supervised=False`. We
    should see a projection like this, where classes are clustered in the 2D space
    of the `tsne` projection:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试无监督训练，通过设置`supervised=False`。我们应该看到这样的投影，其中类别在`tsne`投影的2D空间中聚类：
- en: '![](img/4a54cf9c-19cf-4385-a9d7-5de2a48b0d4f.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a54cf9c-19cf-4385-a9d7-5de2a48b0d4f.png)'
- en: This is a 2D visualization of the encoder's representation space of the digits.
    Each color (or shade) represents a different digit. We can see different clusters
    that group together instances of the same digit. The encoder is distinguishing
    between different digits.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数字编码器表示空间的二维可视化。每种颜色（或阴影）代表不同的数字。我们可以看到不同的聚类将同一数字的实例组合在一起。编码器区分不同的数字。
- en: In the next section, we'll discuss how this works.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论其工作原理。
- en: How it works...
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: An **autoencoder** is a network of two parts – an encoder and a decoder – where
    the encoder maps the input into a latent space and the decoder reconstructs the
    input. Autoencoders can be trained to reconstruct the input by a reconstruction
    loss, which is often the squared error between the original input and the restored
    input.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**是一个由两部分组成的网络 - 编码器和解码器 - 其中编码器将输入映射到潜在空间，解码器重建输入。自编码器可以通过重建损失进行训练，该损失通常是原始输入与恢复输入之间的平方误差。'
- en: Adversarial autoencoders were introduced in 2016 by Makhazani and others (*Adversarial
    Autoencoders*; [https://arxiv.org/pdf/1511.05644.pdf](https://arxiv.org/pdf/1511.05644.pdf)).
    The publication showed how they could be used in clustering and semi-supervised
    training, or how they could decorrelate class representations. An adversarial
    autoencoder is a probabilistic autoencoder that uses a GAN to perform variational
    inference. Instead of matching input to output directly, the aggregated posterior
    of the hidden code vector of the autoencoder is matched to an arbitrary prior
    distribution.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗自编码器于2016年由Makhazani等人引入（*对抗自编码器*; [https://arxiv.org/pdf/1511.05644.pdf](https://arxiv.org/pdf/1511.05644.pdf)）。该出版物展示了它们如何在聚类和半监督训练中使用，或者它们如何解相关类别表示。对抗自编码器是一种使用GAN执行变分推断的概率自编码器。与直接匹配输入和输出不同，自编码器的隐藏编码向量的聚合后验被匹配到任意先验分布。
- en: 'Since adversarial autoencoders are GANs, and therefore rely on the competition
    between generator and discriminator, the training is a bit more involved than
    it would be for a vanilla autoencoder. We calculate three different types of errors:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对抗自编码器是GAN，因此依赖于生成器和鉴别器之间的竞争，训练比普通自编码器更复杂一些。我们计算三种不同类型的错误：
- en: The standard reconstruction error
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准重建误差
- en: An error of the discriminator that quantifies a failure to distinguish between
    real random numbers and encoder output
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器的一个错误，量化了无法区分真实随机数和编码器输出的失败
- en: The encoder error that penalizes the encoder for failing to trick the discriminator
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对编码器错误的惩罚，因为它未能欺骗鉴别器
- en: In our case, we force prior distribution and decoder output within the range
    0 and 1, and we can therefore use cross-entropy as the reconstruction error.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们强制先验分布和解码器输出在0和1之间的范围内，并且因此可以使用交叉熵作为重建误差。
- en: It might be helpful to highlight the code segments responsible for calculating
    the different types of error.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有助于突出显示负责计算不同类型误差的代码段。
- en: 'The reconstruction error looks like this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差如下所示：
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: There's an extra flag for feeding the labels into the decoder as supervised
    training. We found that in the supervised setting, the encoder doesn't represent
    the digits, but rather the style. We argue that this is the case since, in the
    supervised setting, the reconstruction error doesn't depend on the labels anymore.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个额外的标志用于将标签作为监督训练输入到解码器中。我们发现在监督设置中，编码器并不表示数字，而是表示风格。我们认为这是因为在监督设置中，重建误差不再依赖于标签。
- en: 'The discriminator loss is calculated as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器损失计算如下：
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Please note that this is for training the discriminator, not the encoder, hence
    the `encoder.eval()` statement.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是为了训练鉴别器，而不是编码器，因此有`encoder.eval()`语句。
- en: 'The generator loss is calculated as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器损失计算如下：
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the next section, we'll look at more material and background.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看更多的材料和背景。
- en: See also
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: For a more up-to-date and fuller-featured implementation of adversarial autoencoders,
    please refer to a repository maintained by Maurits Diephuis at [https://github.com/mdiephuis/generative-models](https://github.com/mdiephuis/generative-models).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更更新、更全面的对抗自编码器实现，请参考Maurits Diephuis维护的存储库：[https://github.com/mdiephuis/generative-models](https://github.com/mdiephuis/generative-models)。
- en: For more historical and mathematical background on autoencoders and adversarial
    autoencoders, please see Lilian Weng's excellent overview article, *From Autoencoder
    to Beta-VAE*, on her blog: [https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自编码器和对抗自编码器的更多历史和数学背景，请参阅Lilian Weng在她的博客上的优秀概述文章*From Autoencoder to Beta-VAE*：[https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html](https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html)。
- en: 'Finally, please have a look at Maurits Diephuis and others, *Variational Saccading:
    Efficient Inference for Large Resolution Images* (Ramapuram and others, 2019).
    They introduce a probabilistic algorithm for focusing on regions of interest in
    bigger images inspired by the eye''s saccade movements. You can find their code
    on GitHub at [https://github.com/jramapuram/variational_saccading](https://github.com/jramapuram/variational_saccading).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，请查看Maurits Diephuis等人的*Variational Saccading: Efficient Inference for Large
    Resolution Images*（Ramapuram等人，2019）。他们引入了一个概率算法，用于专注于更大图像中感兴趣的区域，灵感来源于眼睛的扫视运动。你可以在GitHub上找到他们的代码：[https://github.com/jramapuram/variational_saccading](https://github.com/jramapuram/variational_saccading)。'
