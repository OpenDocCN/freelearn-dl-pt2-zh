["```py\n# File location and type\nfile_location = \"/FileStore/tables/soilmoisture_dataset.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)\n```", "```py\n# df.write.format(\"parquet\").saveAsTable(permanent_table_name)\n```", "```py\ndf.write.format(\"delta\").saveAsTable(permanent_table_name)\n```", "```py\n%sql\nSELECT * FROM soilmoisture\n```", "```py\n%sql\nOPTIMIZE soilmoisture ZORDER BY (deviceid)\n```", "```py\nimport databricks.koalas as ks \n\ndf = ks.DataFrame(pump_data)\nprint(\"variance: \" + str(df.var()))\nminuite['time'] = pd.to_datetime(minuite['time'])\nminuite.set_index('time')\nminuite['sample'] = minuite['sample'].rolling(window=600,center=False).std() \n```", "```py\nmax = DF.agg({\"averageRating\": \"max\"}).collect()[0]\n```", "```py\nminute['max'] = minute['sample'].rolling(window=600,center=False).max() \n\nminute['sample'] = minute['sample'].rolling(window=600,center=False).min() \n```", "```py\nSELECT EventTime, Count(*) AS Count\nFROM DeviceStream TIMESTAMP BY CreatedAt\nGROUP by EventTime, TumbelingWindow(minuites, 10)\n```", "```py\nfrom pyspark.sql.functions import * \nwindowedDF = eventsDF.groupBy(window(\"eventTime\", \"10 minute\")).count()\n```", "```py\nSELECT EventTime, Count(*) AS Count\nFROM DeviceStream TIMESTAMP BY CreatedAt\nGROUP by EventTime, HopingWindow(minuites, 10, 5)\n```", "```py\nfrom pyspark.sql.functions import * \nwindowedDF = eventsDF.groupBy(window(\"eventTime\", \"10 minute\", \"5 minute\")).count()\n```", "```py\nSELECT EventTime, Count(*) AS Count\nFROM DeviceStream TIMESTAMP BY CreatedAt\nGROUP by EventTime,\nSlidingWindow(minutes, 10)\nWHERE COUNT(*) > 100\n```", "```py\n%sql\nselect * from Telemetry\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns    \n\n# load the sample training data\ntrain = pd.read_csv('/dbfs/FileStore/tables/Bike_train.csv')\n\nfor i in range(50):\n    a = np.random.normal(5,i+1,10)\n    b.append(a)\nc = np.array(b)\ncm =np.corrcoef(c)\n\nplt.imshow(cm,interpolation='nearest')\nplt.colorbar()\n\n#heat map\nplt.figure(figsize=(17,11))\nsns.heatmap(train.iloc[:,1:30].corr(), cmap= 'viridis', annot=True)\ndisplay(plt.show())\n```", "```py\ndisplay(DF.stat.crosstab(\"titleType\", \"genres\"))\n```", "```py\ndf.stat.cov('averageRating', 'numVotes')\n```", "```py\n%sql\nCREATE WIDGET DROPDOWN tytleType DEFAULT \"movie\" CHOICES SELECT DISTINCT titleType FROM imdbTitles\n```", "```py\n%sql\nselect * from imdbTitles where titleType = getArgument(\"tytleType\")\n```", "```py\ndf = spark.read.format(\"mongo\").option(\"uri\",\n\"mongodb://127.0.0.1/products.inventory\").load()\npipeline =  \"{'deviceid':'8ea23889-3677-4ebe-80b6-3fee6e38a42c'}\" \ndf = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).load()\ndf.show() \n\n```", "```py\npipeline = \"[ { '$match': { 'status': 'A' } }, { '$group': { '_id': '$item', 'total': { '$sum': '$qty' } } } ]\"\ndf = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).load()\ndf.show() \n```", "```py\nimport datetime as dt\nimport json\n\nehConf = {}\nehConf['eventhubs.connectionString'] = [\"The connection string you copies\"]\nehConf['eventhubs.consumerGroup'] = \"[The consumer group you created]\"\n\nstartingEventPosition = {\n  \"offset\": -1, \n  \"seqNo\": -1, #not in use\n  \"enqueuedTime\": None, #not in use\n  \"isInclusive\": True\n}\n\nendingEventPosition = {\n  \"offset\": None, #not in use\n  \"seqNo\": -1, #not in use\n  \"enqueuedTime\": endTime,\n  \"isInclusive\": True\n}\nehConf[\"eventhubs.recieverTimeout\"] = 100\n```", "```py\ndf = spark \\\n .readStream \\\n .format(\"eventhubs\") \\\n .options(**ehConf) \\\n .load()\n```", "```py\nfrom pyspark.sql.types import *\nSchema = StructType([StructField(\"deviceEndSessionTime\", StringType()), StructField(\"sensor1\", StringType()),\n StructField(\"sensor2\", StringType()),\n StructField(\"deviceId\", LongType()),\n ])\n```", "```py\nfrom pyspark.sql.functions import *\n\nrawData = df. \\\n  selectExpr(\"cast(Body as string) as json\"). \\\n  select(from_json(\"json\", Schema).alias(\"data\")). \\\n select(\"data.*\")\n```"]