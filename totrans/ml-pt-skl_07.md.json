["```py\n>>> from scipy.special import comb\n>>> import math\n>>> def ensemble_error(n_classifier, error):\n...     k_start = int(math.ceil(n_classifier / 2.))\n...     probs = [comb(n_classifier, k) *\n...              error**k *\n...              (1-error)**(n_classifier - k)\n...              for k in range(k_start, n_classifier + 1)]\n...     return sum(probs)\n>>> ensemble_error(n_classifier=11, error=0.25)\n0.03432750701904297 \n```", "```py\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> error_range = np.arange(0.0, 1.01, 0.01)\n>>> ens_errors = [ensemble_error(n_classifier=11, error=error)\n...               for error in error_range]\n>>> plt.plot(error_range, ens_errors,\n...          label='Ensemble error',\n...          linewidth=2)\n>>> plt.plot(error_range, error_range,\n...          linestyle='--', label='Base error',\n...          linewidth=2)\n>>> plt.xlabel('Base error')\n>>> plt.ylabel('Base/Ensemble error')\n>>> plt.legend(loc='upper left')\n>>> plt.grid(alpha=0.5)\n>>> plt.show() \n```", "```py\n>>> import numpy as np\n>>> np.argmax(np.bincount([0, 0, 1],\n...           weights=[0.2, 0.2, 0.6]))\n1 \n```", "```py\n>>> ex = np.array([[0.9, 0.1],\n...                [0.8, 0.2],\n...                [0.4, 0.6]])\n>>> p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])\n>>> p\narray([0.58, 0.42])\n>>> np.argmax(p)\n0 \n```", "```py\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import ClassifierMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.pipeline import _name_estimators\nimport numpy as np\nimport operator\nclass MajorityVoteClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, classifiers, vote='classlabel', weights=None):\n\n        self.classifiers = classifiers\n        self.named_classifiers = {\n            key: value for key,\n            value in _name_estimators(classifiers)\n        }\n        self.vote = vote\n        self.weights = weights\n\n    def fit(self, X, y):\n        if self.vote not in ('probability', 'classlabel'):\n            raise ValueError(f\"vote must be 'probability' \"\n                             f\"or 'classlabel'\"\n                             f\"; got (vote={self.vote})\")\n        if self.weights and\n        len(self.weights) != len(self.classifiers):\n            raise ValueError(f'Number of classifiers and'\n                             f' weights must be equal'\n                             f'; got {len(self.weights)} weights,'\n                             f' {len(self.classifiers)} classifiers')\n        # Use LabelEncoder to ensure class labels start\n        # with 0, which is important for np.argmax\n        # call in self.predict\n        self.lablenc_ = LabelEncoder()\n        self.lablenc_.fit(y)\n        self.classes_ = self.lablenc_.classes_\n        self.classifiers_ = []\n        for clf in self.classifiers:\n            fitted_clf = clone(clf).fit(X,\n                               self.lablenc_.transform(y))\n            self.classifiers_.append(fitted_clf)\n        return self \n```", "```py\n def predict(self, X):\n        if self.vote == 'probability':\n            maj_vote = np.argmax(self.predict_proba(X), axis=1)\n        else: # 'classlabel' vote\n\n            # Collect results from clf.predict calls\n            predictions = np.asarray([\n                clf.predict(X) for clf in self.classifiers_\n            ]).T\n\n            maj_vote = np.apply_along_axis(\n                lambda x: np.argmax(\n                    np.bincount(x, weights=self.weights)\n                ),\n                axis=1, arr=predictions\n            )\n        maj_vote = self.lablenc_.inverse_transform(maj_vote)\n        return maj_vote\n\n    def predict_proba(self, X):\n        probas = np.asarray([clf.predict_proba(X)\n                             for clf in self.classifiers_])\n        avg_proba = np.average(probas, axis=0,\n                               weights=self.weights)\n        return avg_proba\n\n    def get_params(self, deep=True):\n        if not deep:\n            return super().get_params(deep=False)\n        else:\n            out = self.named_classifiers.copy()\n            for name, step in self.named_classifiers.items():\n                for key, value in step.get_params(\n                        deep=True).items():\n                    out[f'{name}__{key}'] = value\n            return out \n```", "```py\n>>> from sklearn import datasets\n>>> from sklearn.model_selection import train_test_split\n>>> from sklearn.preprocessing import StandardScaler\n>>> from sklearn.preprocessing import LabelEncoder\n>>> iris = datasets.load_iris()\n>>> X, y = iris.data[50:, [1, 2]], iris.target[50:]\n>>> le = LabelEncoder()\n>>> y = le.fit_transform(y) \n```", "```py\n>>> X_train, X_test, y_train, y_test =\\\n...     train_test_split(X, y,\n...                      test_size=0.5,\n...                      random_state=1,\n...                      stratify=y) \n```", "```py\n>>> from sklearn.model_selection import cross_val_score\n>>> from sklearn.linear_model import LogisticRegression\n>>> from sklearn.tree import DecisionTreeClassifier\n>>> from sklearn.neighbors import KNeighborsClassifier\n>>> from sklearn.pipeline import Pipeline\n>>> import numpy as np\n>>> clf1 = LogisticRegression(penalty='l2',\n...                           C=0.001,\n...                           solver='lbfgs',\n...                           random_state=1)\n>>> clf2 = DecisionTreeClassifier(max_depth=1,\n...                               criterion='entropy',\n...                               random_state=0)\n>>> clf3 = KNeighborsClassifier(n_neighbors=1,\n...                             p=2,\n...                             metric='minkowski')\n>>> pipe1 = Pipeline([['sc', StandardScaler()],\n...                   ['clf', clf1]])\n>>> pipe3 = Pipeline([['sc', StandardScaler()],\n...                   ['clf', clf3]])\n>>> clf_labels = ['Logistic regression', 'Decision tree', 'KNN']\n>>> print('10-fold cross validation:\\n')\n>>> for clf, label in zip([pipe1, clf2, pipe3], clf_labels):\n...     scores = cross_val_score(estimator=clf,\n...                              X=X_train,\n...                              y=y_train,\n...                              cv=10,\n...                              scoring='roc_auc')\n...     print(f'ROC AUC: {scores.mean():.2f} '\n...           f'(+/- {scores.std():.2f}) [{label}]') \n```", "```py\n10-fold cross validation:\nROC AUC: 0.92 (+/- 0.15) [Logistic regression]\nROC AUC: 0.87 (+/- 0.18) [Decision tree]\nROC AUC: 0.85 (+/- 0.13) [KNN] \n```", "```py\n>>> mv_clf = MajorityVoteClassifier(\n...     classifiers=[pipe1, clf2, pipe3]\n... )\n>>> clf_labels += ['Majority voting']\n>>> all_clf = [pipe1, clf2, pipe3, mv_clf]\n>>> for clf, label in zip(all_clf, clf_labels):\n...     scores = cross_val_score(estimator=clf,\n...                              X=X_train,\n...                              y=y_train,\n...                              cv=10,\n...                              scoring='roc_auc')\n...     print(f'ROC AUC: {scores.mean():.2f} '\n...           f'(+/- {scores.std():.2f}) [{label}]')\nROC AUC: 0.92 (+/- 0.15) [Logistic regression]\nROC AUC: 0.87 (+/- 0.18) [Decision tree]\nROC AUC: 0.85 (+/- 0.13) [KNN]\nROC AUC: 0.98 (+/- 0.05) [Majority voting] \n```", "```py\n>>> from sklearn.metrics import roc_curve\n>>> from sklearn.metrics import auc\n>>> colors = ['black', 'orange', 'blue', 'green']\n>>> linestyles = [':', '--', '-.', '-']\n>>> for clf, label, clr, ls \\\n...     in zip(all_clf, clf_labels, colors, linestyles):\n...     # assuming the label of the positive class is 1\n...     y_pred = clf.fit(X_train,\n...                      y_train).predict_proba(X_test)[:, 1]\n...     fpr, tpr, thresholds = roc_curve(y_true=y_test,\n...                                      y_score=y_pred)\n...     roc_auc = auc(x=fpr, y=tpr)\n...     plt.plot(fpr, tpr,\n...              color=clr,\n...              linestyle=ls,\n...              label=f'{label} (auc = {roc_auc:.2f})')\n>>> plt.legend(loc='lower right')\n>>> plt.plot([0, 1], [0, 1],\n...          linestyle='--',\n...          color='gray',\n...          linewidth=2)\n>>> plt.xlim([-0.1, 1.1])\n>>> plt.ylim([-0.1, 1.1])\n>>> plt.grid(alpha=0.5)\n>>> plt.xlabel('False positive rate (FPR)')\n>>> plt.ylabel('True positive rate (TPR)')\n>>> plt.show() \n```", "```py\n>>> sc = StandardScaler()\n>>> X_train_std = sc.fit_transform(X_train)\n>>> from itertools import product\n>>> x_min = X_train_std[:, 0].min() - 1\n>>> x_max = X_train_std[:, 0].max() + 1\n>>> y_min = X_train_std[:, 1].min() - 1\n>>>\n>>> y_max = X_train_std[:, 1].max() + 1\n>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n...                      np.arange(y_min, y_max, 0.1))\n>>> f, axarr = plt.subplots(nrows=2, ncols=2,\n...                         sharex='col',\n...                         sharey='row',\n...                         figsize=(7, 5))\n>>> for idx, clf, tt in zip(product([0, 1], [0, 1]),\n...                         all_clf, clf_labels):\n...     clf.fit(X_train_std, y_train)\n...     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n...     Z = Z.reshape(xx.shape)\n...     axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3)\n...     axarr[idx[0], idx[1]].scatter(X_train_std[y_train==0, 0],\n...                                   X_train_std[y_train==0, 1],\n...                                   c='blue',\n...                                   marker='^',\n...                                   s=50)\n...     axarr[idx[0], idx[1]].scatter(X_train_std[y_train==1, 0],\n...                                   X_train_std[y_train==1, 1],\n...                                   c='green',\n...                                   marker='o',\n...                                   s=50)\n...     axarr[idx[0], idx[1]].set_title(tt)\n>>> plt.text(-3.5, -5.,\n...          s='Sepal width [standardized]',\t\n...          ha='center', va='center', fontsize=12)\n>>> plt.text(-12.5, 4.5,\n...          s='Petal length [standardized]',\n...          ha='center', va='center',\n...          fontsize=12, rotation=90)\n>>> plt.show() \n```", "```py\n>>> mv_clf.get_params()\n{'decisiontreeclassifier':\n DecisionTreeClassifier(class_weight=None, criterion='entropy',\n                        max_depth=1, max_features=None,\n                        max_leaf_nodes=None, min_samples_leaf=1,\n                        min_samples_split=2,\n                        min_weight_fraction_leaf=0.0,\n                        random_state=0, splitter='best'),\n 'decisiontreeclassifier__class_weight': None,\n 'decisiontreeclassifier__criterion': 'entropy',\n [...]\n 'decisiontreeclassifier__random_state': 0,\n 'decisiontreeclassifier__splitter': 'best',\n 'pipeline-1':\n Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,\n                                       with_std=True)),\n                 ('clf', LogisticRegression(C=0.001,\n                                            class_weight=None,\n                                            dual=False,\n                                            fit_intercept=True,\n                                            intercept_scaling=1,\n                                            max_iter=100,\n                                            multi_class='ovr',\n                                            penalty='l2',\n                                            random_state=0,\n                                            solver='liblinear',\n                                            tol=0.0001,\n                                            verbose=0))]),\n 'pipeline-1__clf':\n LogisticRegression(C=0.001, class_weight=None, dual=False,\n                    fit_intercept=True, intercept_scaling=1,\n                    max_iter=100, multi_class='ovr',\n                    penalty='l2', random_state=0,\n                    solver='liblinear', tol=0.0001, verbose=0),\n 'pipeline-1__clf__C': 0.001,\n 'pipeline-1__clf__class_weight': None,\n 'pipeline-1__clf__dual': False,\n [...]\n 'pipeline-1__sc__with_std': True,\n 'pipeline-2':\n Pipeline(steps=[('sc', StandardScaler(copy=True, with_mean=True,\n                                       with_std=True)),\n                 ('clf', KNeighborsClassifier(algorithm='auto',\n                                              leaf_size=30,\n                                              metric='minkowski',\n                                              metric_params=None,\n                                              n_neighbors=1,\n                                              p=2,\n                                              weights='uniform'))]),\n 'pipeline-2__clf':\n KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                      metric='minkowski', metric_params=None,\n                      n_neighbors=1, p=2, weights='uniform'),\n 'pipeline-2__clf__algorithm': 'auto',\n [...]\n 'pipeline-2__sc__with_std': True} \n```", "```py\n>>> from sklearn.model_selection import GridSearchCV\n>>> params = {'decisiontreeclassifier__max_depth': [1, 2],\n...           'pipeline-1__clf__C': [0.001, 0.1, 100.0]}\n>>> grid = GridSearchCV(estimator=mv_clf,\n...                     param_grid=params,\n...                     cv=10,\n...                     scoring='roc_auc')\n>>> grid.fit(X_train, y_train) \n```", "```py\n>>> for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n...     mean_score = grid.cv_results_['mean_test_score'][r]\n...     std_dev = grid.cv_results_['std_test_score'][r]\n...     params = grid.cv_results_['params'][r]\n...     print(f'{mean_score:.3f} +/- {std_dev:.2f} {params}')\n0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,\n                'pipeline-1__clf__C': 0.001}\n0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 1,\n                'pipeline-1__clf__C': 0.1}\n0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 1,\n                'pipeline-1__clf__C': 100.0}\n0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,\n                'pipeline-1__clf__C': 0.001}\n0.983 +/- 0.05 {'decisiontreeclassifier__max_depth': 2,\n                'pipeline-1__clf__C': 0.1}\n0.967 +/- 0.10 {'decisiontreeclassifier__max_depth': 2,\n                'pipeline-1__clf__C': 100.0}\n>>> print(f'Best parameters: {grid.best_params_}')\nBest parameters: {'decisiontreeclassifier__max_depth': 1,\n                  'pipeline-1__clf__C': 0.001}\n>>> print(f'ROC AUC : {grid.best_score_:.2f}')\nROC AUC: 0.98 \n```", "```py\n>>> import pandas as pd\n>>> df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'\n...                       'machine-learning-databases/'\n...                       'wine/wine.data',\n...                       header=None)\n>>> df_wine.columns = ['Class label', 'Alcohol',\n...                    'Malic acid', 'Ash',\n...                    'Alcalinity of ash',\n...                    'Magnesium', 'Total phenols',\n...                    'Flavanoids', 'Nonflavanoid phenols',\n...                    'Proanthocyanins',\n...                    'Color intensity', 'Hue',\n...                    'OD280/OD315 of diluted wines',\n...                    'Proline']\n>>> # drop 1 class\n>>> df_wine = df_wine[df_wine['Class label'] != 1]\n>>> y = df_wine['Class label'].values\n>>> X = df_wine[['Alcohol',\n...              'OD280/OD315 of diluted wines']].values \n```", "```py\n>>> from sklearn.preprocessing import LabelEncoder\n>>> from sklearn.model_selection import train_test_split\n>>> le = LabelEncoder()\n>>> y = le.fit_transform(y)\n>>> X_train, X_test, y_train, y_test =\\\n...            train_test_split(X, y,\n...                             test_size=0.2,\n...                             random_state=1,\n...                             stratify=y) \n```", "```py\ndf = pd.read_csv('https://archive.ics.uci.edu/ml/'\n                 'machine-learning-databases'\n                 '/wine/wine.data',\n                 header=None) \n```", "```py\ndf = pd.read_csv('your/local/path/to/wine.data',\n                 header=None) \n```", "```py\n>>> from sklearn.ensemble import BaggingClassifier\n>>> tree = DecisionTreeClassifier(criterion='entropy',\n...                               random_state=1,\n...                               max_depth=None)\n>>> bag = BaggingClassifier(base_estimator=tree,\n...                         n_estimators=500,\n...                         max_samples=1.0,\n...                         max_features=1.0,\n...                         bootstrap=True,\n...                         bootstrap_features=False,\n...                         n_jobs=1,\n...                         random_state=1) \n```", "```py\n>>> from sklearn.metrics import accuracy_score\n>>> tree = tree.fit(X_train, y_train)\n>>> y_train_pred = tree.predict(X_train)\n>>> y_test_pred = tree.predict(X_test)\n>>> tree_train = accuracy_score(y_train, y_train_pred)\n>>> tree_test = accuracy_score(y_test, y_test_pred)\n>>> print(f'Decision tree train/test accuracies '\n...       f'{tree_train:.3f}/{tree_test:.3f}')\nDecision tree train/test accuracies 1.000/0.833 \n```", "```py\n>>> bag = bag.fit(X_train, y_train)\n>>> y_train_pred = bag.predict(X_train)\n>>> y_test_pred = bag.predict(X_test)\n>>> bag_train = accuracy_score(y_train, y_train_pred)\n>>> bag_test = accuracy_score(y_test, y_test_pred)\n>>> print(f'Bagging train/test accuracies '\n...       f'{bag_train:.3f}/{bag_test:.3f}')\nBagging train/test accuracies 1.000/0.917 \n```", "```py\n>>> x_min = X_train[:, 0].min() - 1\n>>> x_max = X_train[:, 0].max() + 1\n>>> y_min = X_train[:, 1].min() - 1\n>>> y_max = X_train[:, 1].max() + 1\n>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n...                      np.arange(y_min, y_max, 0.1))\n>>> f, axarr = plt.subplots(nrows=1, ncols=2,\n...                         sharex='col',\n...                         sharey='row',\n...                         figsize=(8, 3))\n>>> for idx, clf, tt in zip([0, 1],\n...                         [tree, bag],\n...                         ['Decision tree', 'Bagging']):\n...     clf.fit(X_train, y_train)\n...\n...     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n...     Z = Z.reshape(xx.shape)\n...     axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n...     axarr[idx].scatter(X_train[y_train==0, 0],\n...                        X_train[y_train==0, 1],\n...                        c='blue', marker='^')\n...     axarr[idx].scatter(X_train[y_train==1, 0],\n...                        X_train[y_train==1, 1],\n...                        c='green', marker='o')\n...     axarr[idx].set_title(tt)\n>>> axarr[0].set_ylabel('Alcohol', fontsize=12)\n>>> plt.tight_layout()\n>>> plt.text(0, -0.2,\n...          s='OD280/OD315 of diluted wines',\n...          ha='center',\n...          va='center',\n...          fontsize=12,\n...          transform=axarr[1].transAxes)\n>>> plt.show() \n```", "```py\n>>> y = np.array([1, 1, 1, -1, -1, -1,  1,  1,  1, -1])\n>>> yhat = np.array([1, 1, 1, -1, -1, -1, -1, -1, -1, -1])\n>>> correct = (y == yhat)\n>>> weights = np.full(10, 0.1)\n>>> print(weights)\n[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n>>> epsilon = np.mean(~correct)\n>>> print(epsilon)\n0.3 \n```", "```py\n>>> alpha_j = 0.5 * np.log((1-epsilon) / epsilon)\n>>> print(alpha_j)\n0.42364893019360184 \n```", "```py\n>>> update_if_correct = 0.1 * np.exp(-alpha_j * 1 * 1)\n>>> print(update_if_correct)\n0.06546536707079771 \n```", "```py\n>>> update_if_wrong_1 = 0.1 * np.exp(-alpha_j * 1 * -1)\n>>> print(update_if_wrong_1)\n0.1527525231651947 \n```", "```py\n>>> update_if_wrong_2 = 0.1 * np.exp(-alpha_j * 1 * -1)\n>>> print(update_if_wrong_2) \n```", "```py\n>>> weights = np.where(correct == 1,\n...                    update_if_correct,\n...                    update_if_wrong_1)\n>>> print(weights)\narray([0.06546537, 0.06546537, 0.06546537, 0.06546537, 0.06546537,\n       0.06546537, 0.15275252, 0.15275252, 0.15275252, 0.06546537]) \n```", "```py\n>>> normalized_weights = weights / np.sum(weights)\n>>> print(normalized_weights)\n[0.07142857 0.07142857 0.07142857 0.07142857 0.07142857 0.07142857\n 0.16666667 0.16666667 0.16666667 0.07142857] \n```", "```py\n>>> from sklearn.ensemble import AdaBoostClassifier\n>>> tree = DecisionTreeClassifier(criterion='entropy',\n...                               random_state=1,\n...                               max_depth=1)\n>>> ada = AdaBoostClassifier(base_estimator=tree,\n...                          n_estimators=500,\n...                          learning_rate=0.1,\n...                          random_state=1)\n>>> tree = tree.fit(X_train, y_train)\n>>> y_train_pred = tree.predict(X_train)\n>>> y_test_pred = tree.predict(X_test)\n>>> tree_train = accuracy_score(y_train, y_train_pred)\n>>> tree_test = accuracy_score(y_test, y_test_pred)\n>>> print(f'Decision tree train/test accuracies '\n...       f'{tree_train:.3f}/{tree_test:.3f}')\nDecision tree train/test accuracies 0.916/0.875 \n```", "```py\n>>> ada = ada.fit(X_train, y_train)\n>>> y_train_pred = ada.predict(X_train)\n>>> y_test_pred = ada.predict(X_test)\n>>> ada_train = accuracy_score(y_train, y_train_pred)\n>>> ada_test = accuracy_score(y_test, y_test_pred)\n>>> print(f'AdaBoost train/test accuracies '\n...       f'{ada_train:.3f}/{ada_test:.3f}')\nAdaBoost train/test accuracies 1.000/0.917 \n```", "```py\n>>> x_min = X_train[:, 0].min() - 1\n>>> x_max = X_train[:, 0].max() + 1\n>>> y_min = X_train[:, 1].min() - 1\n>>> y_max = X_train[:, 1].max() + 1\n>>> xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n...                      np.arange(y_min, y_max, 0.1))\n>>> f, axarr = plt.subplots(1, 2,\n...                         sharex='col',\n...                         sharey='row',\n...                         figsize=(8, 3))\n>>> for idx, clf, tt in zip([0, 1],\n...                         [tree, ada],\n...                         ['Decision tree', 'AdaBoost']):\n...     clf.fit(X_train, y_train)\n...     Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n...     Z = Z.reshape(xx.shape)\n...     axarr[idx].contourf(xx, yy, Z, alpha=0.3)\n...     axarr[idx].scatter(X_train[y_train==0, 0],\n...                        X_train[y_train==0, 1],\n...                        c='blue',\n...                        marker='^')\n...     axarr[idx].scatter(X_train[y_train==1, 0],\n...                        X_train[y_train==1, 1],\n...                        c='green',\n...                        marker='o')\n...     axarr[idx].set_title(tt)\n...     axarr[0].set_ylabel('Alcohol', fontsize=12)\n>>> plt.tight_layout()\n>>> plt.text(0, -0.2,\n...          s='OD280/OD315 of diluted wines',\n...          ha='center',\n...          va='center',\n...          fontsize=12,\n...          transform=axarr[1].transAxes)\n>>> plt.show() \n```", "```py\npip install xgboost \n```", "```py\npip install XGBoost==1.5.0 \n```", "```py\n>>> import xgboost as xgb\n>>> model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01,\n...                           max_depth=4, random_state=1,\n...                           use_label_encoder=False)\n>>> gbm = model.fit(X_train, y_train)\n>>> y_train_pred = gbm.predict(X_train)\n>>> y_test_pred = gbm.predict(X_test)\n>>> gbm_train = accuracy_score(y_train, y_train_pred)\n>>> gbm_test = accuracy_score(y_test, y_test_pred)\n>>> print(f'XGboost train/test accuracies '\n...       f'{gbm_train:.3f}/{gbm_test:.3f}')\nXGboost train/test accuracies 0.968/0.917 \n```"]