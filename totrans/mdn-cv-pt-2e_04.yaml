- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a Deep Neural Network with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to code a neural network using PyTorch.
    We also learned about the various hyperparameters that are present in a neural
    network, such as its batch size, learning rate, and loss optimizer. In this chapter,
    we will shift gears and learn how to perform image classification using neural
    networks. Essentially, we will learn how to represent images and tweak the hyperparameters
    of a neural network to understand their impact.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of not introducing too much complexity and confusion, we only covered
    the fundamental aspects of neural networks in the previous chapter. However, there
    are many more inputs that we tweak in a network while training it. Typically,
    these inputs are known as **hyperparameters**. In contrast to the *parameters*
    in a neural network (which are learned during training), hyperparameters are provided
    by the person who builds the network. Changing different aspects of each hyperparameter
    is likely to affect the accuracy or speed of training a neural network. Furthermore,
    a few additional techniques such as scaling, batch normalization, and regularization
    help in improving the performance of a neural network. We will learn about these
    concepts throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before we get to that, we will learn about how an image is represented:
    only then will we do a deep dive into the details of hyperparameters. While learning
    about the impact of hyperparameters, we will restrict ourselves to one dataset:
    Fashion MNIST (details about the dataset can be found at [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)),
    so that we can make a comparison of the impact of variations in various hyperparameters.
    Through this dataset, we will also be introduced to training and validation data
    and why it is important to have two separate datasets. Finally, we will learn
    about the concept of overfitting a neural network and then understand how certain
    hyperparameters help us avoid overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Representing an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why leverage neural networks for image analysis?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling a dataset to improve model accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the impact of varying the batch size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the impact of varying the loss optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the impact of varying the learning rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a deeper neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the impact of batch normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: All code in this chapter is available for reference in the `Chapter03` folder
    of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: We have covered the impact of varying the learning rate in the associated code
    in the GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: Representing an image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A digital image file (typically associated with the extension “JPEG” or “PNG”)
    is comprised of an array of pixels. A pixel is the smallest constituting element
    of an image. In a grayscale image, each pixel is a scalar (single) value between
    `0` and `255`: 0 is black, 255 is white, and anything in between is gray (the
    smaller the pixel value, the darker the pixel is). On the other hand, the pixels
    in color images are three-dimensional vectors that correspond to the scalar values
    that can be found in their red, green, and blue channels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An image has *height x width x c* pixels, where *height* is the number of **rows**
    of pixels, *width* is the number of **columns** of pixels, and *c* is the number
    of **channels**. *c* is `3` for color images (one channel each for the *red, green,*
    and *blue* intensities of the image) and `1` for grayscale images. An example
    grayscale image containing 3 x 3 pixels and their corresponding scalar values
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, shoji, crossword puzzle  Description automatically
    generated](img/B18457_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Representing an image'
  prefs: []
  type: TYPE_NORMAL
- en: Again, a pixel value of `0` means that it is pitch black, while `255` means
    it is pure luminance (that is, pure white for grayscale and pure red/green/blue
    in the respective channel for a color image).
  prefs: []
  type: TYPE_NORMAL
- en: Converting images into structured arrays and scalars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python can convert images into structured arrays and scalars as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Inspecting_grayscale_images.ipynb` file
    located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Download a sample image or upload a custom image of your own:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the `cv2` (to read an image from disk) and `matplotlib` (to plot the
    loaded image) libraries, and read the downloaded image into the Python environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding line of code, we leverage the `cv2.imread` method to read the
    image. This converts an image into an array of pixel values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll crop the image between the 50^(th) and 250^(th) rows, as well as the
    40^(th) and 240^(th) columns. Finally, we’ll convert the image into grayscale
    using the following code and plot it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding sequence of steps is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing calendar  Description automatically generated](img/B18457_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Cropped image'
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that the preceding image is represented as a 200 x 200
    array of pixels. Now, let’s reduce the number of pixels that are used to represent
    the image so that we can overlay the pixel values on the image (this would be
    tougher to do if we were to visualize the pixel values over a 200 x 200 array,
    compared to a 25 x 25 array).
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the image into a 25 x 25 array and plot it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Resized image'
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, having fewer pixels to represent the same image results in a blurrier
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the pixel values. Note that in the following output, due to space
    constraints, we have pasted only the first four rows of pixel values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B18457_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Pixel values of input image'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same set of pixel values, when copied and pasted into MS Excel and color-coded
    by pixel value, would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B18457_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Pixel values corresponding to the image'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, the pixels with a scalar value closer to 255 appear
    lighter, while those closer to 0 appear darker.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a structured array for colored images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding steps apply to color images too, which are represented as three-dimensional
    vectors. The brightest red pixel is denoted as `(255,0,0)`. Similarly, a pure
    white pixel in a three-dimensional vector image is represented as `(255,255,255)`.
    With this in mind, let’s create a structured array of pixel values for a colored
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Inspecting_color_images.ipynb` file
    located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Download a color image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the relevant packages and load the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Crop the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding code, we’ve reordered the channels using the `cv2.cvtcolor`
    method. We’ve done this because when we import images using cv2, the channels
    are ordered as blue first, green next, and finally, red; typically, we are used
    to looking at images in RGB channels, where the sequence is red, green, and then
    blue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the image that’s obtained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output (note that if you are reading the print
    book and haven’t downloaded the color image bundle, the following image will appear
    in grayscale):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Image in the RGB format'
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom-right 3 x 3 array of pixels can be obtained as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print and plot the pixel values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: RGB values of a patch of the image'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to represent an image (i.e., a file on your computer)
    as a tensor, we are now in a position to learn various mathematical operations
    and techniques that can leverage these tensors to perform tasks, such as image
    classification, object detection, image segmentation and many more throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: But first, let’s understand why **artifical neural networks** (**ANNs**) are
    useful for image analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Why leverage neural networks for image analysis?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In traditional computer vision, we would create a few features for every image
    before using them as input. Let’s look at a few such features based on the following
    sample image, in order to appreciate the effort we save by training a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text  Description automatically generated](img/B18457_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: A subset of features that can be generated from an image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we will not walk you through how to get these features, as the intention
    here is to help you realize why creating features manually is a suboptimal exercise.
    However, you can familiarize yourself with the different feature extraction methods
    at [https://docs.opencv.org/4.x/d7/da8/tutorial_table_of_content_imgproc.html](https://docs.opencv.org/4.x/d7/da8/tutorial_table_of_content_imgproc.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Histogram feature**: For some tasks, such as auto-brightness or night vision,
    it is important to understand the illumination in the picture: that is, the fraction
    of pixels that are bright or dark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edges and corners feature**: For tasks such as image segmentation, where
    it is important to find the set of pixels corresponding to each person, it makes
    sense to extract the edges first because the border of a person is just a collection
    of edges. In other tasks, such as image registration, it is vital that key landmarks
    are detected. These landmarks will be a subset of all the corners in an image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Color separation feature**: In tasks such as traffic light detection for
    a self-driving car, it is important that the system understands what color is
    displayed on the traffic lights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image gradients** **feature**: Taking the color separation feature a step
    further, it might be important to understand how the colors change at the pixel
    level. Different textures can give us different gradients, which means they can
    be used as texture detectors. In fact, finding gradients is a prerequisite for
    edge detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a handful of such features. There are so many more that it is
    difficult to cover all of them. The main drawback of creating these features is
    that you need to be an expert in image and signal analysis and should fully understand
    what features are best suited to solve a problem. Even if both constraints are
    satisfied, there is no guarantee that such an expert will be able to find the
    right combination of inputs, and even if they do, there is still no guarantee
    that such a combination will work in new, unseen scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Due to these drawbacks, the community has largely shifted to neural network-based
    models. These models not only find the right features automatically but also learn
    how to optimally combine them to get the job done. As we have already seen in
    the first chapter, neural networks act as both feature extractors and classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve had a look at some examples of historical feature extraction
    techniques and their drawbacks, let’s learn how to train a neural network on images.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing our data for image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given that we are covering multiple scenarios in this chapter, in order for
    us to see the advantage of one scenario over the other, we will work on a single
    dataset throughout this chapter: the Fashion MNIST dataset: which contains images
    of 10 different classes of clothing (shirts, trousers, shoes, and so on). Let’s
    prepare this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Preparing_our_data.ipynb` file located
    in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by downloading the dataset and importing the relevant packages. The `torchvision`
    package contains various datasets, one of which is the `FashionMNIST` dataset,
    which we will work on in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we specify the folder (`data_folder`) where we want to
    store the downloaded dataset. Then, we fetch the `fmnist` data from `datasets.FashionMNIST`
    and store it in `data_folder`. Furthermore, we specify that we only want to download
    the training images by specifying `train = True`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must store the images that are available in `fmnist.data` as `tr_images`
    and the labels (targets) that are available in `fmnist.targets` as `tr_targets`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the tensors that we are dealing with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated with low confidence](img/B18457_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Input and output shapes and unique classes'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that there are 60,000 images, each 28 x 28 in size, and with
    10 possible classes across all the images. Note that `tr_targets` contains the
    numeric values for each class, while `fmnist.classes` gives us the names that
    correspond to each numeric value in `tr_targets`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot a random sample of 10 images for all the 10 possible classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the relevant packages in order to plot a grid of images so that you
    can also work on arrays:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a plot where we can show a 10 x 10 grid, where each row of the grid
    corresponds to a class and each column presents an example image belonging to
    the row’s class. Loop through the unique class numbers (`label_class`) and fetch
    the indices of rows (`label_x_rows`) corresponding to the given class number:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding code, we fetch the 0^(th) index as the output of
    the `np.where` condition, as it has a length of 1\. It contains an array of all
    the indices where the target value (`tr_targets`) is equal to `label_class`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Loop through 10 times to fill the columns of a given row. Furthermore, we need
    to select a random value (`ix`) from the indices corresponding to a given class
    that were obtained previously (`label_x_rows`) and plot them:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Sample Fashion MNIST images'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the preceding image, each row represents a sample of 10 different
    images all belonging to the same class.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train a neural network, we must perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the relevant packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a dataset that can fetch data one data point at a time
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wrap the dataloader from the dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a model and then define the loss function and the optimizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define two functions to train and validate a batch of data, respectively
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a function that will calculate the accuracy of the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform weight updates based on each batch of data over increasing epochs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following lines of code, we’ll perform each of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code can be found in the `Steps_to_build_a_neural_network_on_FashionMNIST.ipynb`
    file located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and the `fmnist` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a class that fetches the dataset. Remember that it is derived from a
    `Dataset` class and needs three magic functions, `__init__`, `__getitem__`, and
    `__len__,` to **always** be defined:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the `__init__` method, we convert the input into a floating-point
    number and also flatten each image into 28*28 = 784 numeric values (where each
    numeric value corresponds to a pixel value). We also specify the number of data
    points in the `__len__` method; here, it is the length of `x`. The `__getitem__`
    method contains logic for what should be returned when we ask for the `ix`^(th)
    data point (`ix` will be an integer between `0` and `__len__`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function that generates a training DataLoader, `trn_dl`, from the
    dataset called `FMNISTDataset`. This will sample 32 data points at random for
    the batch size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a model, as well as the loss function and the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model is a network with one hidden layer containing 1,000 neurons. The output
    is a 10-neuron layer, since there are 10 possible classes. Furthermore, we call
    the `CrossEntropyLoss` function, since the output can belong to any of the 10
    classes for each image. Finally, the key aspect to note in this exercise is that
    we have initialized the learning rate, `lr`, to a value of `0.01` and not the
    default of `0.001`, to see how the model will learn for this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are not using “softmax” in the neural network at all. The range
    of outputs is unconstrained, in that values can have an infinite range, whereas
    cross-entropy loss typically expects outputs as probabilities (each row should
    sum to `1`). Unconstrained values in output still work in this setting because
    `nn.CrossEntropyLoss` actually expects us to send the raw logits (that is, unconstrained
    values). It performs softmax internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that will train the dataset on a batch of images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code passes the batch of images through the model in the forward
    pass. It also computes the loss on the batch and then passes the weights through
    backward propagation and updates them. Finally, it flushes the memory of the gradient
    so that it doesn’t influence how the gradient is calculated in the next pass.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve done this, we can extract the loss value as a scalar by fetching
    `batch_loss.item()` on top of `batch_loss`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a function that calculates the accuracy of a given dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we explicitly mention that we don’t need to calculate
    the gradient by providing `@torch.no_grad()` and calculating the `prediction`
    values, by feed-forwarding input through the model. Next, we invoke `prediction.max(-1)`
    to identify the `argmax` index corresponding to each row. We then compare our
    `argmaxes` with the ground truth through `argmaxes == y` so that we can check
    whether each row is predicted correctly. Finally, we return the list of `is_correct`
    objects after moving it to a CPU and converting it into a NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train the neural network using the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the model, loss, optimizer, and DataLoaders:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the lists that will contain the accuracy and loss values at the
    end of each epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the number of epochs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the lists that will contain the accuracy and loss values corresponding
    to each batch within an epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create batches of training data by iterating through the DataLoader:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the batch using the `train_batch` function, and store the loss value
    at the end of training on top of the batch as `batch_loss`. Furthermore, store
    the loss values across batches in the `epoch_losses` list:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We store the mean loss value across all batches within an epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we calculate the accuracy of the prediction at the end of training on
    all batches:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the loss and accuracy values at the end of each epoch in a list:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The variation of the training loss and accuracy over increasing epochs can
    be displayed using the following code:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_03_11.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.11: Training loss and accuracy values over increasing epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our training accuracy is at 12% at the end of the five epochs. Note that the
    loss value did not decrease considerably over an increasing number of epochs.
    In other words, no matter how long we wait, it is unlikely that the model is going
    to provide high accuracy (say, above 80%). This calls for us to understand how
    the various hyperparameters that were used impact the accuracy of our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Note that since we did not specify `torch.random_seed(0)` at the start of code,
    the results might vary when you execute the code provided. However, the results
    you get should also let you reach a similar conclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a complete picture of how to train a neural network, let’s
    study some good practices we should follow to achieve good model performance and
    the reasons behind using them. This can be achieved by fine-tuning various hyperparameters,
    some of which we will look at in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling a dataset to improve model accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Scaling a dataset is the process of ensuring that the variables are confined
    to a finite range. In this section, we will confine the independent variables’
    values to between `0` and `1` by dividing each input value by the maximum possible
    value in the dataset. This is a value of `255`, which corresponds to white pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: For brevity’s sake, we have only provided the modified code (from the previous
    section) in the upcoming code. The full code can be found in the `Scaling_the_dataset.ipynb`
    file located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the dataset, as well as the training images and targets, as we did in
    the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Modify `FMNISTDataset`, which fetches data, so that the input image is divided
    by 255 (the maximum intensity/value of a pixel):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the only change we’ve made here compared to the previous section
    is that we divide the input data by the maximum possible pixel value: `255`.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the pixel values range between `0` to `255`, dividing them by 255
    will result in values that are always between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a model, just like we did in *steps 4*, *5*, *6*, and *7*, of the previous
    section. The variations for the training loss and accuracy values are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A screenshot of a computer screen  Description automatically generated](img/B18457_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.12: Training loss and accuracy values over increasing epochs on a
    scaled dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the training loss consistently reduced and the training accuracy
    consistently increased to an accuracy of ~85%. Contrast the preceding output with
    the scenario where input data is not scaled, where training loss did not reduce
    consistently, and the accuracy of the training dataset at the end of five epochs
    was only 12%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive into the possible reason why scaling helps here. We’ll take the
    example of how a sigmoid value is calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following table, we’ve calculated the **Sigmoid** column based on the
    preceding formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13: Sigmoid value for the different values of input and weight'
  prefs: []
  type: TYPE_NORMAL
- en: In the left-hand table, we can see that when the weight values are more than
    0.1, the sigmoid value does not vary with an increasing (changing) weight value.
    Furthermore, the sigmoid value changed only by a little when the weight was extremely
    small; the only way to vary the sigmoid value is by changing the weight by a very,
    very small amount.
  prefs: []
  type: TYPE_NORMAL
- en: However, the sigmoid value changed considerably in the right-hand table when
    the input value was small.
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that the exponential of a large negative value (resulting
    from multiplying the weight value by a large number) is very close to 0, while
    the exponential value varies when the weight is multiplied by a scaled input,
    as seen in the right-hand table.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand that the sigmoid value does not change considerably unless
    the weight values are very small, we will learn about how weight values can be
    influenced toward an optimal value.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the input dataset so that it contains a much smaller range of values
    generally helps to achieve better model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll learn about the impact of one of the other major hyperparameters
    of any neural network: **batch size.**'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the impact of varying the batch size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, 32 data points were considered per batch in the training
    dataset. This resulted in a greater number of weight updates per epoch, as there
    were 1,875 weight updates per epoch (60,000/32 is nearly equal to 1,875, where
    60,000 is the number of training images).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we did not consider the model’s performance on an unseen dataset
    (validation dataset). We will explore this in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will compare the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy values of the training and validation data when the training
    batch size is 32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss and accuracy values of the training and validation data when the training
    batch size is 10,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have brought validation data into the picture, let’s rerun the code
    provided in the *Building a neural network* section with additional code to generate
    validation data, as well as to calculate the loss and accuracy values of the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For brevity’s sake, we have only provided the modified code (from the previous
    section) in the upcoming section. The full code can be found in the `Varying_batch_size.ipynb`
    file in the `Chapter03` folder in the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: Batch size of 32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Given that we have already built a model that uses a batch size of 32 during
    training in the previous section, we will elaborate on the additional code that
    is used to work on the validation dataset. We’ll skip going through the details
    of training the model, since this is already covered in the *Building a neural
    network* section. Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Download and import the training images and targets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a similar manner to training images, we must download and import the validation
    dataset by specifying `train = False`, while calling the `FashionMNIST` method
    in our datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Import the relevant packages and define `device`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the dataset class (`FashionMNIST`) and the functions that will be used
    to train on a batch of data (`train_batch`), calculate the accuracy (`accuracy`),
    and then define the model architecture, the loss function, and the optimizer (`get_model`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a function that will get data: that is, `get_data`. This function will
    return the training data with a batch size of `32` and the validation dataset
    with a batch size that’s the length of the validation data (we will not use the
    validation data to train the model; we will only use it to understand the model’s
    accuracy on unseen data):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we created an object of the `FMNISTDataset` class named
    `val`, in addition to the `train` object that we saw earlier. Furthermore, the
    DataLoader for validation (`val_dl`) was fetched with a batch size of `len(val_images)`,
    while the batch size of `trn_dl` is `32`. This is because the training data is
    used to train the model while we fetch the accuracy and loss metrics of the validation
    data. In this section and the next, we are trying to understand the impact of
    varying `batch_size`, based on the model’s training time and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function that calculates the loss of the validation data: that is,
    `val_loss`. Note that we are calculating this separately, since the loss of training
    data is calculated while training the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, we apply `torch.no_grad` because we don’t train the model and
    only fetch predictions. Furthermore, we pass our `prediction` through the loss
    function (`loss_fn`) and return the loss value (`val_loss.item()`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Fetch the training and validation DataLoaders. Also, initialize the model,
    loss function, and optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the lists that contain training and validation accuracy, as well
    as loss values over increasing epochs:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through five epochs, and initialize lists that contain accuracy and loss
    across batches of training data within a given epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through batches of training data, and calculate the accuracy (`train_epoch_accuracy`)
    and loss value (`train_epoch_loss`) within an epoch:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the loss value and accuracy within the one batch of validation data
    (since the batch size of the validation data is equal to the length of the validation
    data):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that in the preceding code, the loss value of the validation data is calculated
    using the `val_loss` function and is stored in the `validation_loss` variable.
    Furthermore, the accuracy of all the validation data points is stored in the `val_is_correct`
    list, while the mean of this is stored in the `val_epoch_accuracy` variable.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we append the training and validation datasets’ accuracy and loss
    values to the lists that contain the epoch-level aggregate validation and accuracy
    values. We’re doing this so that we can look at the epoch level’s improvement
    in the next step:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the improvements in the accuracy and loss values in the training
    and validation datasets over increasing epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.14: Training and validation loss and accuracy over increasing epochs
    with a batch size of 32'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the training and validation accuracy is ~85% by the end of five
    epochs when the batch size is 32\. Next, we will vary the `batch_size` parameter
    when training the DataLoader in the `get_data` function to see its impact on accuracy
    at the end of five epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Batch size of 10,000
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll use 10,000 data points per batch so that we can understand
    what impact varying the batch size has.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the code provided in the *Batch size of 32* section remains exactly
    the same here, except for the code in *step 5*, where we will specify a batch
    size of 10,000\. We encourage you to refer to the respective notebook that’s available
    in this book’s GitHub repository while executing the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'By making only this necessary change in *step 5* and after executing all the
    steps until *step 9*, the variation in the training and validation’s accuracy
    and loss over increasing epochs when the batch size is 10,000 is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15: Training and validation loss and accuracy over increasing epochs
    with a batch size of 10,000'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the accuracy and loss values did not reach the same levels
    as that of the previous scenario, where the batch size was 32, because weights
    are updated fewer times per epoch (6 times) when compared to a batch size of 32
    (1,875).
  prefs: []
  type: TYPE_NORMAL
- en: Having a lower batch size generally helps in achieving optimal accuracy when
    you have a small number of epochs, but it should not be so low that training time
    is impacted.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to scale a dataset, as well as the impact of varying
    the batch size on the model’s training time to achieve a certain accuracy. In
    the next section, we will learn about the impact of varying the loss optimizer
    on the same dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the impact of varying the loss optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we have optimized loss based on the Adam optimizer. A loss optimizer
    helps to arrive at optimal weight values to minimize overall loss. There are a
    variety of loss optimizers (different ways of updating weight values to minimize
    loss values) that impact the overall loss and accuracy of a model. In this section,
    we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify the optimizer so that it becomes a **Stochastic Gradient Descent** (**SGD**)
    optimizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Revert to a batch size of 32 while fetching data in the DataLoader
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the number of epochs to 10 (so that we can compare the performance
    of SGD and Adam over a longer number of epochs)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Making these changes means that only one step in the *Batch size of 32* section
    will change (since the batch size is already 32 in that section); that is, we
    will modify the optimizer so that it’s the SGD optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s modify the `get_model` function in *step 4* of the *Batch size of 32*
    section to modify the optimizer so that we use the SGD optimizer instead, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The complete code can be found in the `Varying_loss_optimizer.ipynb` file located
    in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For the sake of brevity, we will not detail every step from the *Batch size of
    32* section; instead, in the following code, we will discuss only those steps
    where changes are introduced. **We encourage you to refer to the respective notebooks
    on GitHub while executing the code.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Modify the optimizer so that you use the SGD optimizer in the `get_model` function
    while ensuring that everything else remains the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let’s increase the number of epochs in *step 8* while keeping every other
    step (except for *steps 4* and *8*) the same as they are in the *Batch size of
    32* section.
  prefs: []
  type: TYPE_NORMAL
- en: Increase the number of epochs we’ll use to train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After making these changes, once we execute all the remaining steps in the
    *Batch size of 32* section in order, the variation in the training and validation
    datasets’ accuracy and loss values over increasing epochs (when trained with the
    SGD and Adam optimizers individually) will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16: Training and validation loss and accuracy over increasing epochs
    with SGD optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.17: Training and validation loss and accuracy over increasing epochs
    with Adam optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, when we used the Adam optimizer, the accuracy was still very
    close to 85%.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Certain optimizers achieve optimal accuracy faster compared to others. Some
    of the other prominent optimizers that are available include Adagrad, Adadelta,
    AdamW, LBFGS, and RMSprop.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have used a learning rate of 0.01 while training our models and maintained
    it across all the epochs while training the model. In *Chapter 1*, *Artificial
    Neural Network Fundamentals*, we learned that the learning rate plays a key role
    in attaining optimal weight values. Here, the weight values gradually move toward
    the optimal value when the learning rate is small, while the weight values oscillate
    at a non-optimal value when the learning rate is large.
  prefs: []
  type: TYPE_NORMAL
- en: However, initially, it would be intuitive for the weights to be updated quickly
    to a near-optimal scenario. From then on, they should be updated very slowly,
    since the amount of loss that gets reduced initially is high and the amount of
    loss that gets reduced in the later epochs would be low.
  prefs: []
  type: TYPE_NORMAL
- en: This calls for having a high learning rate initially and gradually lowering
    it later as the model achieves near-optimal accuracy. This requires us to understand
    when the learning rate must be reduced (learning rate annealing over time). Refer
    to the `Learning_rate_annealing.ipynb` file located in the `Chapter03` folder
    on GitHub to understand the impact of learning rate annealing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the impact of the varying learning rate, the following scenarios
    will help:'
  prefs: []
  type: TYPE_NORMAL
- en: Higher learning rate (0.1) on a scaled dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower learning rate (0.00001) on a scaled dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower learning rate (0.00001) on a non-scaled dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These three scenarios will not be covered in this chapter; however, you can
    access the full code for them in the `Varying_learning_rate_on_scaled_data.ipynb`
    file and `Varying_learning_rate_on_non_scaled_data.ipynb` file in the `Chapter03`
    folder in the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about how the number of layers in a neural
    network impacts its accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Building a deeper neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, our neural network architecture only has one hidden layer. In this section,
    we will contrast the performance of models where there are two hidden layers and
    no hidden layer (with no hidden layer being a logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: 'A model with two layers within a network can be built as follows (note that
    we have kept the number of units in the second hidden layer set to 1,000). The
    modified `get_model` function (from the code in the *Batch size of 32* section),
    where there are two hidden layers, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be found in the `Impact_of_building_a_deeper_neural_network.ipynb`
    file located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For the sake of brevity, we will not detail every step from the *Batch size of
    32* section. Please refer to the notebooks on GitHub while executing the code**.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the `get_model` function, where there are *no hidden layers*, is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding function, we connect the input directly to the output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we train the models as we did in the *Batch size of 32* section, the accuracy
    and loss on the train and validation datasets will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.18: Training and validation loss and accuracy over increasing epochs
    with varying number of hidden layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, take note of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The model was unable to learn well when there were no hidden layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model overfit by a larger amount when there were two hidden layers compared
    to one hidden layer (the validation loss is higher in the model with two layers
    compared to the model with one layer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, across different sections, we have seen that the model was unable to
    be trained well when the input data wasn’t scaled (brought down to a small range).
    Non-scaled data (data with a higher range) can also occur in hidden layers (especially
    when we have deep neural networks with multiple hidden layers) because of the
    matrix multiplication that’s involved in getting the values of nodes in hidden
    layers. Let’s learn about how batch normalization helps to deal with non-scaled
    data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the impact of batch normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we learned that when the input value is large, the variation of
    the sigmoid output doesn’t make much difference when the weight values change
    considerably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider the opposite scenario, where the input values are very
    small:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A table of numbers with numbers  Description automatically generated](img/B18457_03_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.19: Sigmoid value for the different values of input and weight'
  prefs: []
  type: TYPE_NORMAL
- en: When the input value is very small, the sigmoid output changes slightly, requiring
    a big change to the weight value to achieve optimal results.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in the *Scaling the input data* section, we saw that large input
    values have a negative effect on training accuracy. This suggests that we can
    neither have very small nor very big values for our input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with very small or very big values in input, we may also encounter a
    scenario where the value of one of the nodes in the hidden layer could result
    in either a very small number or a very large number, resulting in the same issue
    we saw previously with the weights connecting the hidden layer to the next layer.
    Batch normalization comes to the rescue in such a scenario, since it normalizes
    the values at each node, just like when we scaled our input values. Typically,
    all the input values in a batch are scaled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18457_03_003.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18457_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18457_03_005.png)'
  prefs: []
  type: TYPE_IMG
- en: By subtracting each data point from the batch mean and then dividing it by the
    batch variance, we have normalized all the data points of the batch at a node
    to a fixed range. While this is known as hard normalization, by introducing the
    γ and β parameters, we let the network identify the best normalization parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how the batch normalization process helps, let’s look at the
    loss and accuracy values on the training and validation datasets, as well as the
    distribution of hidden layer values, in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Very small input values without batch normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Very small input values with batch normalization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Very small input values without batch normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, when we had to scale input data, we scaled it at a value between 0 and
    1\. In this section, we will scale it further at a value between 0 and 0.0001
    so that we can understand the impact of scaling data. As we saw at the beginning
    of this section, small input values cannot change the sigmoid value, even with
    a big variation in weight values.
  prefs: []
  type: TYPE_NORMAL
- en: 'To scale the input dataset so that it has a very low value, we’ll change the
    scaling that we typically do in the `FMNISTDataset` class by reducing the range
    of input values from `0` to `0.0001`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be found in the `Batch_normalization.ipynb` file in the `Chapter03`
    folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e). For brevity,
    we will not detail every step from the *Batch size of 32* section. **Refer to
    the notebooks on GitHub while executing the code.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the lines of code we’ve highlighted (`x = x.float()/(255*10000)`),
    we have reduced the range of input pixel values by dividing them by 10,000.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must redefine the `get_model` function so that we can fetch the model’s
    prediction, as well as the values for the hidden layer. We can do this by specifying
    a neural network class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we defined the `neuralnet` class, which returns the output
    layer values (`x2`) and the hidden layer’s activation values (`x1`). Note that
    the architecture of the network hasn’t changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the `get_model` function returns two outputs now, we need to modify
    the `train_batch` and `val_loss` functions, which make predictions, by passing
    input through the model. Here, we’ll only fetch the output layer values, not the
    hidden layer values. Given that the output layer values are in the 0^(th) index
    of what is returned from the model, we’ll modify the functions so that they only
    fetch the 0^(th) index of predictions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Note that the highlighted portions of the preceding code are where we have ensured
    we only fetch the 0^(th) index of the model’s output (since the 0^(th) index contains
    the output layer’s values).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when we run the rest of the code provided in the *Scaling* *the data*
    section, we’ll see that the variation in the accuracy and loss values in the training
    and validation datasets over increasing epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_03_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.20: Training and validation loss and accuracy when network is trained
    with very small input values'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the preceding scenario, the model didn’t train well, even after
    100 epochs (the model was trained to an accuracy of ~90% on the validation dataset
    within 10 epochs in the previous sections, while the current model only has ~85%
    validation accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the reason why the model doesn’t train as well when the input
    values have a very small range by exploring the hidden values’ distribution, as
    well as the parameter distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A blue rectangular object with numbers  Description automatically generated](img/B18457_03_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.21: Distribution of weights and hidden layer node values when network
    is trained with very small input values'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the first distribution indicates the distribution of values in the
    hidden layer (where we can see that the values have a very small range). Furthermore,
    given that both the input and hidden layer values have a very small range, the
    weights had to be varied by a large amount (for both the weights that connect
    the input to the hidden layer and the weights that connect the hidden layer to
    the output layer).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know that the network doesn’t train well when the input values have
    a very small range, let’s understand how batch normalization helps increase the
    range of values within the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Very small input values with batch normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we’ll only make one change to the code from the previous subsection;
    that is, we’ll add batch normalization while defining the model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The modified `get_model` function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we declared a variable (`batch_norm`) that performs batch
    normalization (`nn.BatchNorm1d`). The reason we perform `nn.BatchNorm1d(1000)`
    is because the output dimension is 1,000 for each image (that is, a 1-dimensional
    output for the hidden layer). Furthermore, in the `forward` method, we pass the
    output of the hidden layer values through batch normalization, prior to ReLU activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variation in the training and validation datasets’ accuracy and loss values
    over increasing epochs is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.22: Training and validation loss when network is trained with very
    small input values and batch normalization'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the model was trained in a manner very similar to how
    it was trained when the input values did not have a very small range. Now, let’s
    understand the distribution of hidden layer values and the weight distribution,
    as seen in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B18457_03_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.23: Distribution of weights and hidden layer node values when network
    is trained with very small input values and batch normalization'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the hidden layer values have a wider spread when we have batch
    normalization and that the weights connecting the hidden layer to the output layer
    have a smaller distribution. This results in the model learning as effectively
    as it could in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization helps considerably when training deep neural networks. It
    helps us avoid gradients becoming so small that the weights are barely updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have seen scenarios where training loss and accuracy are much better
    when compared to validation accuracy and loss: indicating a model that overly
    fits on training data but does not generalize well on validation datasets. We
    will look at fixing overfitting issues next.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of overfitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve seen that the accuracy of the training dataset is typically more
    than 95%, while the accuracy of the validation dataset is ~89%. Essentially, this
    indicates that a model does not generalize as much on unseen datasets, since it
    can learn from the training dataset. This also indicates that the model learns
    all the possible edge cases for the training dataset; these can’t be applied to
    the validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Having high accuracy on the training dataset and considerably lower accuracy
    on the validation dataset refers to the scenario of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the typical strategies that are employed to reduce the effect of overfitting
    are dropout and regularization. We will look at what impact they have on training
    and validation losses in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of adding dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have already learned that whenever `loss.backward()` is calculated, a weight
    update happens. Typically, we would have hundreds of thousands of parameters within
    a network and thousands of data points to train our model on. This gives us the
    possibility that while most parameters help to train the model reasonably, certain
    parameters can be fine-tuned for the training images, resulting in their values
    being dictated by only a few images in the training dataset. This, in turn, results
    in the training data having a high accuracy but not necessarily on the validation
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout is a mechanism that randomly chooses a specified percentage of node
    activations and reduces them to 0\. In the next iteration, another random set
    of hidden units is switched off. This way, the neural network does not optimize
    for edge cases, as the network does not get that many opportunities to adjust
    the weight to memorize for edge cases (given that the weight is not updated in
    each iteration).
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that, during prediction, dropout doesn’t need to be applied, since
    this mechanism can only be applied while training a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, there are cases where the layers behave differently during training
    and validation, as you saw in the case of dropout. For this reason, you must specify
    the mode for the model upfront using one of two methods: `model.train()` to let
    the model know it is in training mode and `model.eval()` to let it know that it
    is in evaluation mode. If we don’t do this, we might get unexpected results. For
    example, in the following image, notice how the model (which contains dropout)
    gives us different predictions on the same input when in training mode.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when the same model is in `eval` mode, it will suppress the dropout
    layer and return the same output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18457_03_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.24: Impact of model.eval() on output values'
  prefs: []
  type: TYPE_NORMAL
- en: 'While defining the architecture, `Dropout` is specified in the `get_model`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be found in the `Impact_of_dropout.ipynb` file in the `Chapter03`
    folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e). For brevity,
    we will not detail every step from the *Batch size of 32* section. **Refer to
    the notebooks on GitHub while executing the code.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Note that, in the preceding code, `Dropout` is specified before linear activation.
    This specifies that a fixed percentage of the weights in the linear activation
    layer won’t be updated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model training is completed, as in the *Batch size of 32* section,
    the loss and accuracy values of the training and validation datasets will be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.25: Training and validation loss and accuracy with dropout'
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, the delta between the training and validation datasets’ accuracy
    is not as large as we saw in the previous scenario, thus resulting in a scenario
    that has less overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from the training accuracy being much higher than the validation accuracy,
    one other feature of overfitting is that certain weight values will be much higher
    than the other weight values. Large weight values can be a symptom of a model
    learning very well on training data (essentially, rote learning based on what
    it has seen).
  prefs: []
  type: TYPE_NORMAL
- en: While dropout is a mechanism that’s used so that the weight values aren’t updated
    as frequently, regularization is another mechanism that we can use for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization is a technique in which we penalize the model for having large
    weight values. Hence, it is an objective function that minimizes the loss of training
    data, as well as the weight values. In this section, we will learn about two types
    of regularization: L1 regularization and L2 regularization.'
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be found in the `Impact_of_regularization.ipynb` file in the
    `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For brevity, we will not detail every step from the *Batch size of 32* section.
    **Refer to the notebooks on GitHub while executing the code.**
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'L1 regularization is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_006.png)'
  prefs: []
  type: TYPE_IMG
- en: The first part of the preceding formula refers to the categorical cross-entropy
    loss that we have used for optimization so far, while the second part refers to
    the absolute sum of the weight values of the model. Note that L1 regularization
    ensures that it penalizes for the high absolute values of weights by incorporating
    them in the loss value calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_007.png) refers to the weightage that we associate with the
    regularization (weight minimization) loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'L1 regularization is implemented while training the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have enforced regularization on the weights and biases across
    all the layers by initializing `l1_regularization`.
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.norm(param,1)` provides the absolute value of the weight and bias values
    across layers. Furthermore, we have a very small weightage (`0.0001`) associated
    with the sum of the absolute value of the parameters across layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we execute the remaining code, as in the *Batch size of 32* section, the
    training and validation datasets’ loss and accuracy values over increasing epochs
    will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_03_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: Training and validation loss and accuracy with L1 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the difference between the training and validation datasets’
    accuracy is not as high as it was without L1 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'L2 regularization is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_008.png)'
  prefs: []
  type: TYPE_IMG
- en: The first part of the preceding formula refers to the categorical cross-entropy
    loss obtained, while the second part refers to the squared sum of the weight values
    of the model. Similar to L1 regularization, we penalize for large weight values
    by having the sum of squared values of weights incorporated into the loss value
    calculation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_03_007.png) refers to the weightage that we associate with the
    regularization (weight minimization) loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'L2 regularization is implemented while training the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, the regularization parameter, ![](img/B18457_03_007.png)
    (`0.01`), is slightly higher than in L1 regularization, since the weights are
    generally between -1 and 1, and squaring them would result in even smaller values.
    Multiplying them by an even smaller number, as we did in L1 regularization, would
    result in us having very little weightage for regularization in the overall loss
    calculation. Once we execute the remaining code, as in the *Batch size of 32*
    section, the training and validation datasets’ loss and accuracy values over increasing
    epochs will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A graph of training and validation  Description automatically generated](img/B18457_03_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.27: Training and validation loss and accuracy with L2 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that L2 regularization has also resulted in the validation and training
    datasets’ accuracy and loss values being closer to each other.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by learning about how an image is represented. Then,
    we learned about how scaling, the value of the learning rate, our choice of optimizer,
    and the batch size help improve the accuracy and speed of training. We then learned
    about how batch normalization helps to increase the speed of training and addresses
    the issues of very small or large values in a hidden layer. Then, we learned about
    scheduling the learning rate to increase accuracy further. We then proceeded to
    understand the concept of overfitting and learned about how dropout and L1 and
    L2 regularization help us avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about image classification using a deep neural network,
    as well as the various hyperparameters that help train a model, in the next chapter,
    we will learn about how what we’ve learned in this chapter can fail and how to
    address this, using convolutional neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What happens if the input values are not scaled in the input dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What could happen if the background has a white pixel color while the content
    has a black pixel color when training a neural network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the impact of batch size on a model’s training time and memory?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the impact of the input value range have on weight distribution at the
    end of training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does batch normalization help improve accuracy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do weights behave differently during training and evaluation in the dropout
    layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we know if a model has overfitted on training data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does regularization help in avoiding overfitting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do L1 and L2 regularization differ from each other?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does dropout help in reducing overfitting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
