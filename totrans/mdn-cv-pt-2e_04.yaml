- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Building a Deep Neural Network with PyTorch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 构建深度神经网络
- en: In the previous chapter, we learned how to code a neural network using PyTorch.
    We also learned about the various hyperparameters that are present in a neural
    network, such as its batch size, learning rate, and loss optimizer. In this chapter,
    we will shift gears and learn how to perform image classification using neural
    networks. Essentially, we will learn how to represent images and tweak the hyperparameters
    of a neural network to understand their impact.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了如何使用 PyTorch 编写神经网络。我们还了解了神经网络中存在的各种超参数，如批量大小、学习率和损失优化器。在本章中，我们将转变方向，学习如何使用神经网络进行图像分类。基本上，我们将学习如何表示图像并调整神经网络的超参数以理解它们的影响。
- en: For the sake of not introducing too much complexity and confusion, we only covered
    the fundamental aspects of neural networks in the previous chapter. However, there
    are many more inputs that we tweak in a network while training it. Typically,
    these inputs are known as **hyperparameters**. In contrast to the *parameters*
    in a neural network (which are learned during training), hyperparameters are provided
    by the person who builds the network. Changing different aspects of each hyperparameter
    is likely to affect the accuracy or speed of training a neural network. Furthermore,
    a few additional techniques such as scaling, batch normalization, and regularization
    help in improving the performance of a neural network. We will learn about these
    concepts throughout this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了不引入过多复杂性和混乱，我们仅在上一章中涵盖了神经网络的基本方面。但是，在训练网络时，我们调整的输入还有很多。通常，这些输入称为**超参数**。与神经网络中的*参数*（在训练过程中学习的）相反，超参数是由构建网络的人提供的。更改每个超参数的不同方面可能会影响训练神经网络的准确性或速度。此外，一些额外的技术，如缩放、批量归一化和正则化，有助于提高神经网络的性能。我们将在本章中学习这些概念。
- en: 'However, before we get to that, we will learn about how an image is represented:
    only then will we do a deep dive into the details of hyperparameters. While learning
    about the impact of hyperparameters, we will restrict ourselves to one dataset:
    Fashion MNIST (details about the dataset can be found at [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)),
    so that we can make a comparison of the impact of variations in various hyperparameters.
    Through this dataset, we will also be introduced to training and validation data
    and why it is important to have two separate datasets. Finally, we will learn
    about the concept of overfitting a neural network and then understand how certain
    hyperparameters help us avoid overfitting.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，我们将学习图像的表示方法：只有这样，我们才能深入探讨超参数的细节。在学习超参数影响时，我们将限制自己使用一个数据集：Fashion MNIST（有关数据集的详细信息可以在
    [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)
    找到），以便我们可以比较不同超参数变化对准确性的影响。通过这个数据集，我们还将介绍训练和验证数据的概念，以及为什么有两个单独的数据集是重要的。最后，我们将学习神经网络过拟合的概念，然后了解某些超参数如何帮助我们避免过拟合。
- en: 'In summary, in this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在本章中，我们将涵盖以下主题：
- en: Representing an image
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示图像
- en: Why leverage neural networks for image analysis?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为何利用神经网络进行图像分析？
- en: Preparing data for image classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为图像分类准备数据
- en: Training a neural network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Scaling a dataset to improve model accuracy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放数据集以提高模型准确性
- en: Understanding the impact of varying the batch size
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解批量大小变化的影响
- en: Understanding the impact of varying the loss optimizer
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解损失优化器变化的影响
- en: Understanding the impact of varying the learning rate
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解学习率变化的影响
- en: Building a deeper neural network
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建更深层次的神经网络
- en: Understanding the impact of batch normalization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解批量归一化的影响
- en: The concept of overfitting
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合的概念
- en: Let’s get started!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: All code in this chapter is available for reference in the `Chapter03` folder
    of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码可以在本书 GitHub 仓库的 `Chapter03` 文件夹中查阅，链接为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: We have covered the impact of varying the learning rate in the associated code
    in the GitHub repo.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 GitHub 仓库的相关代码中已经覆盖了学习率变化的影响。
- en: Representing an image
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示图像
- en: 'A digital image file (typically associated with the extension “JPEG” or “PNG”)
    is comprised of an array of pixels. A pixel is the smallest constituting element
    of an image. In a grayscale image, each pixel is a scalar (single) value between
    `0` and `255`: 0 is black, 255 is white, and anything in between is gray (the
    smaller the pixel value, the darker the pixel is). On the other hand, the pixels
    in color images are three-dimensional vectors that correspond to the scalar values
    that can be found in their red, green, and blue channels.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像文件（通常与扩展名“JPEG”或“PNG”相关联）由像素数组组成。 像素是图像的最小构成元素。 在灰度图像中，每个像素是介于`0`和`255`之间的标量（单一）值：0代表黑色，255代表白色，介于两者之间的是灰色（像素值越小，像素越暗）。
    另一方面，彩色图像中的像素是三维向量，对应于其红、绿和蓝通道中的标量值。
- en: 'An image has *height x width x c* pixels, where *height* is the number of **rows**
    of pixels, *width* is the number of **columns** of pixels, and *c* is the number
    of **channels**. *c* is `3` for color images (one channel each for the *red, green,*
    and *blue* intensities of the image) and `1` for grayscale images. An example
    grayscale image containing 3 x 3 pixels and their corresponding scalar values
    is shown here:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图像有 *height x width x c* 个像素，其中 *height* 是像素的**行数**，*width* 是像素的**列数**，*c*
    是**通道数**。 对于彩色图像，*c* 是 `3`（分别对应图像的红色、绿色和蓝色强度的一个通道），而对于灰度图像，*c* 是 `1`。 这里展示了一个包含
    3 x 3 像素及其对应标量值的灰度图像示例：
- en: '![A picture containing text, shoji, crossword puzzle  Description automatically
    generated](img/B18457_03_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、障子、填字游戏的图片](img/B18457_03_01.png)'
- en: 'Figure 3.1: Representing an image'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.1: 图像表示'
- en: Again, a pixel value of `0` means that it is pitch black, while `255` means
    it is pure luminance (that is, pure white for grayscale and pure red/green/blue
    in the respective channel for a color image).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，像素值为 `0` 意味着它是纯黑色，而 `255` 表示纯亮度（即灰度图像的纯白色和彩色图像中相应通道的纯红/绿/蓝色）。
- en: Converting images into structured arrays and scalars
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将图像转换为结构化数组和标量
- en: 'Python can convert images into structured arrays and scalars as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Python 可以将图像转换为结构化数组和标量，具体如下：
- en: The following code can be found in the `Inspecting_grayscale_images.ipynb` file
    located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可在 GitHub 的 `Chapter03` 文件夹中找到 `Inspecting_grayscale_images.ipynb` 文件中的以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Download a sample image or upload a custom image of your own:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一个样本图像或上传您自己的自定义图像：
- en: '[PRE0]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the `cv2` (to read an image from disk) and `matplotlib` (to plot the
    loaded image) libraries, and read the downloaded image into the Python environment:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `cv2`（从磁盘读取图像）和 `matplotlib`（绘制加载的图像）库，并将下载的图像读入 Python 环境：
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding line of code, we leverage the `cv2.imread` method to read the
    image. This converts an image into an array of pixel values.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码行中，我们利用 `cv2.imread` 方法读取图像。 这将图像转换为像素值数组。
- en: 'We’ll crop the image between the 50^(th) and 250^(th) rows, as well as the
    40^(th) and 240^(th) columns. Finally, we’ll convert the image into grayscale
    using the following code and plot it:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将裁剪图像，从第 50 行到第 250 行，以及从第 40 列到第 240 列。 最后，我们将使用以下代码将图像转换为灰度并绘制它：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding sequence of steps is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤的输出如下所示：
- en: '![A picture containing calendar  Description automatically generated](img/B18457_03_02.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![包含日历的图片](img/B18457_03_02.png)'
- en: 'Figure 3.2: Cropped image'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.2: 裁剪后的图像'
- en: You might have noticed that the preceding image is represented as a 200 x 200
    array of pixels. Now, let’s reduce the number of pixels that are used to represent
    the image so that we can overlay the pixel values on the image (this would be
    tougher to do if we were to visualize the pixel values over a 200 x 200 array,
    compared to a 25 x 25 array).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，前述图像被表示为一个 200 x 200 的像素数组。 现在，让我们减少用于表示图像的像素数量，以便我们可以在图像上叠加像素值（与在
    200 x 200 数组上可视化像素值相比，在 25 x 25 数组上更难实现此目标）。
- en: 'Convert the image into a 25 x 25 array and plot it:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为一个 25 x 25 的数组并绘制它：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This results in the following output:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Chart  Description automatically generated](img/B18457_03_03.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18457_03_03.png)'
- en: 'Figure 3.3: Resized image'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.3: 调整大小后的图像'
- en: Naturally, having fewer pixels to represent the same image results in a blurrier
    output.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，使用较少像素来表示相同图像会产生模糊的输出。
- en: 'Let’s inspect the pixel values. Note that in the following output, due to space
    constraints, we have pasted only the first four rows of pixel values:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查像素值。请注意，由于空间限制，以下输出只粘贴了前四行像素值：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This results in the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B18457_03_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图的说明（低置信度自动生成）](img/B18457_03_04.png)'
- en: 'Figure 3.4: Pixel values of input image'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：输入图像的像素值
- en: 'The same set of pixel values, when copied and pasted into MS Excel and color-coded
    by pixel value, would look as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将相同的像素值集合复制并粘贴到 MS Excel 中，并按像素值进行颜色编码，效果如下：
- en: '![Text  Description automatically generated](img/B18457_03_05.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![文本的说明（自动生成）](img/B18457_03_05.png)'
- en: 'Figure 3.5: Pixel values corresponding to the image'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：图像对应的像素值
- en: As we mentioned previously, the pixels with a scalar value closer to 255 appear
    lighter, while those closer to 0 appear darker.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，像素的标量值接近 255 的显示更浅，接近 0 的则显示更暗。
- en: Creating a structured array for colored images
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为彩色图像创建一个结构化数组
- en: 'The preceding steps apply to color images too, which are represented as three-dimensional
    vectors. The brightest red pixel is denoted as `(255,0,0)`. Similarly, a pure
    white pixel in a three-dimensional vector image is represented as `(255,255,255)`.
    With this in mind, let’s create a structured array of pixel values for a colored
    image:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤同样适用于彩色图像，其表示为三维向量。最亮的红色像素表示为`(255,0,0)`。同样，三维向量图像中的纯白色像素表示为`(255,255,255)`。有了这些基础知识，让我们为彩色图像创建一个结构化的像素值数组：
- en: The following code can be found in the `Inspecting_color_images.ipynb` file
    located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在 GitHub 上的 `Chapter03` 文件夹中的 `Inspecting_color_images.ipynb` 文件中找到以下代码：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Download a color image:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载一幅彩色图像：
- en: '[PRE5]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Import the relevant packages and load the image:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并加载图像：
- en: '[PRE6]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Crop the image:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪图像：
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that in the preceding code, we’ve reordered the channels using the `cv2.cvtcolor`
    method. We’ve done this because when we import images using cv2, the channels
    are ordered as blue first, green next, and finally, red; typically, we are used
    to looking at images in RGB channels, where the sequence is red, green, and then
    blue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述代码中，我们使用了 `cv2.cvtcolor` 方法重新排序了通道。我们这样做是因为当使用 cv2 导入图像时，通道的顺序是蓝色首先，然后是绿色，最后是红色；通常，我们习惯于查看
    RGB 通道的图像，其中顺序是红色、绿色，最后是蓝色。
- en: 'Plot the image that’s obtained:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制获得的图像：
- en: '[PRE8]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output (note that if you are reading the print
    book and haven’t downloaded the color image bundle, the following image will appear
    in grayscale):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出（请注意，如果您正在阅读印刷版的书籍，并且尚未下载彩色图像包，则以下图像将以灰度显示）：
- en: '![Chart  Description automatically generated](img/B18457_03_06.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图表的说明（自动生成）](img/B18457_03_06.png)'
- en: 'Figure 3.6: Image in the RGB format'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：RGB 格式的图像
- en: 'The bottom-right 3 x 3 array of pixels can be obtained as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以按以下步骤获取右下角的 3 x 3 像素阵列：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Print and plot the pixel values:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印并绘制像素值：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code results in the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![Diagram  Description automatically generated](img/B18457_03_07.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图表的说明（自动生成）](img/B18457_03_07.png)'
- en: 'Figure 3.7: RGB values of a patch of the image'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：图像一个区域的 RGB 值
- en: Now that we have learned how to represent an image (i.e., a file on your computer)
    as a tensor, we are now in a position to learn various mathematical operations
    and techniques that can leverage these tensors to perform tasks, such as image
    classification, object detection, image segmentation and many more throughout
    this book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会如何将图像（即计算机上的文件）表示为张量，我们现在可以学习各种数学运算和技术，利用这些张量执行任务，如图像分类、目标检测、图像分割等，本书中的许多任务。
- en: But first, let’s understand why **artifical neural networks** (**ANNs**) are
    useful for image analysis.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们了解为何**人工神经网络**（**ANNs**）在图像分析中很有用。
- en: Why leverage neural networks for image analysis?
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为何要利用神经网络进行图像分析？
- en: 'In traditional computer vision, we would create a few features for every image
    before using them as input. Let’s look at a few such features based on the following
    sample image, in order to appreciate the effort we save by training a neural network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的计算机视觉中，我们会在使用图像作为输入之前为每个图像创建一些特征。让我们看一些基于以下示例图像的这些特征，以便体会通过训练神经网络节省的努力：
- en: '![A picture containing text  Description automatically generated](img/B18457_03_08.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图片的描述 自动生成](img/B18457_03_08.png)'
- en: 'Figure 3.8: A subset of features that can be generated from an image'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：从图像中生成的一部分特征
- en: 'Note that we will not walk you through how to get these features, as the intention
    here is to help you realize why creating features manually is a suboptimal exercise.
    However, you can familiarize yourself with the different feature extraction methods
    at [https://docs.opencv.org/4.x/d7/da8/tutorial_table_of_content_imgproc.html](https://docs.opencv.org/4.x/d7/da8/tutorial_table_of_content_imgproc.html):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不会详细介绍如何获取这些特征，因为这里的意图是帮助您意识到手动创建特征是一种次优的练习。但是，您可以在[https://docs.opencv.org/4.x/d7/da8/tutorial_table_of_content_imgproc.html](https://docs.opencv.org/4.x/d7/da8/tutorial_table_of_content_imgproc.html)了解各种特征提取方法：
- en: '**Histogram feature**: For some tasks, such as auto-brightness or night vision,
    it is important to understand the illumination in the picture: that is, the fraction
    of pixels that are bright or dark.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直方图特征**：对于某些任务，如自动亮度或夜视，理解图片中的照明情况是很重要的：即明亮或黑暗像素的比例。'
- en: '**Edges and corners feature**: For tasks such as image segmentation, where
    it is important to find the set of pixels corresponding to each person, it makes
    sense to extract the edges first because the border of a person is just a collection
    of edges. In other tasks, such as image registration, it is vital that key landmarks
    are detected. These landmarks will be a subset of all the corners in an image.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘和角落特征**：对于诸如图像分割的任务，重要的是找到与每个人对应的像素集合，因此首先提取边缘是有意义的，因为人的边界只是边缘的集合。在其他任务中，如图像配准，检测关键地标是至关重要的。这些地标将是图像中所有角落的子集。'
- en: '**Color separation feature**: In tasks such as traffic light detection for
    a self-driving car, it is important that the system understands what color is
    displayed on the traffic lights.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**颜色分离特征**：在自动驾驶汽车的任务中，如交通灯检测，系统理解交通灯显示的颜色是非常重要的。'
- en: '**Image gradients** **feature**: Taking the color separation feature a step
    further, it might be important to understand how the colors change at the pixel
    level. Different textures can give us different gradients, which means they can
    be used as texture detectors. In fact, finding gradients is a prerequisite for
    edge detection.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像梯度特征**：进一步探讨颜色分离特征，理解像素级别的颜色变化可能很重要。不同的纹理可以给我们不同的梯度，这意味着它们可以用作纹理检测器。事实上，找到梯度是边缘检测的先决条件。'
- en: These are just a handful of such features. There are so many more that it is
    difficult to cover all of them. The main drawback of creating these features is
    that you need to be an expert in image and signal analysis and should fully understand
    what features are best suited to solve a problem. Even if both constraints are
    satisfied, there is no guarantee that such an expert will be able to find the
    right combination of inputs, and even if they do, there is still no guarantee
    that such a combination will work in new, unseen scenarios.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是少数这类特征中的一部分。还有许多其他特征，涵盖它们是困难的。创建这些特征的主要缺点是，您需要成为图像和信号分析的专家，并且应该充分理解哪些特征最适合解决问题。即使两个条件都满足，也不能保证这样的专家能够找到正确的输入组合，即使找到了，也不能保证这样的组合在新的未见场景中能够有效工作。
- en: Due to these drawbacks, the community has largely shifted to neural network-based
    models. These models not only find the right features automatically but also learn
    how to optimally combine them to get the job done. As we have already seen in
    the first chapter, neural networks act as both feature extractors and classifiers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些缺点，社区主要转向基于神经网络的模型。这些模型不仅可以自动找到正确的特征，还可以学习如何最优地组合它们来完成工作。正如我们在第一章中已经看到的，神经网络既是特征提取器，也是分类器。
- en: Now that we’ve had a look at some examples of historical feature extraction
    techniques and their drawbacks, let’s learn how to train a neural network on images.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过一些历史特征提取技术及其缺点的示例，让我们学习如何在图像上训练神经网络。
- en: Preparing our data for image classification
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备我们的数据用于图像分类
- en: 'Given that we are covering multiple scenarios in this chapter, in order for
    us to see the advantage of one scenario over the other, we will work on a single
    dataset throughout this chapter: the Fashion MNIST dataset: which contains images
    of 10 different classes of clothing (shirts, trousers, shoes, and so on). Let’s
    prepare this dataset:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于本章涵盖多种场景，为了看到一个场景比另一个场景的优势，我们将在本章中专注于一个数据集：Fashion MNIST 数据集，其中包含 10 种不同类别的服装图片（衬衫、裤子、鞋子等）。让我们准备这个数据集：
- en: The following code can be found in the `Preparing_our_data.ipynb` file located
    in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码可以在 GitHub 上的 `Chapter03` 文件夹中的 `Preparing_our_data.ipynb` 文件中找到，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Start by downloading the dataset and importing the relevant packages. The `torchvision`
    package contains various datasets, one of which is the `FashionMNIST` dataset,
    which we will work on in this chapter:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始下载数据集并导入相关包。`torchvision` 包含各种数据集，其中之一是 `FashionMNIST` 数据集，在本章中我们将对其进行处理：
- en: '[PRE11]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, we specify the folder (`data_folder`) where we want to
    store the downloaded dataset. Then, we fetch the `fmnist` data from `datasets.FashionMNIST`
    and store it in `data_folder`. Furthermore, we specify that we only want to download
    the training images by specifying `train = True`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们指定了要存储下载数据集的文件夹（`data_folder`）。然后，我们从 `datasets.FashionMNIST` 获取 `fmnist`
    数据并将其存储在 `data_folder` 中。此外，我们指定只下载训练图像，通过指定 `train = True`。
- en: 'Next, we must store the images that are available in `fmnist.data` as `tr_images`
    and the labels (targets) that are available in `fmnist.targets` as `tr_targets`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须将 `fmnist.data` 中可用的图像存储为 `tr_images`，将 `fmnist.targets` 中可用的标签（目标）存储为
    `tr_targets`：
- en: '[PRE12]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Inspect the tensors that we are dealing with:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查我们正在处理的张量：
- en: '[PRE13]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![Chart  Description automatically generated with low confidence](img/B18457_03_09.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Chart  Description automatically generated with low confidence](img/B18457_03_09.png)'
- en: 'Figure 3.9: Input and output shapes and unique classes'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：输入和输出形状以及唯一类别
- en: Here, we can see that there are 60,000 images, each 28 x 28 in size, and with
    10 possible classes across all the images. Note that `tr_targets` contains the
    numeric values for each class, while `fmnist.classes` gives us the names that
    correspond to each numeric value in `tr_targets`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到有 60,000 张图片，每张大小为 28 x 28，并且在所有图片中有 10 种可能的类别。请注意，`tr_targets` 包含每个类别的数值，而
    `fmnist.classes` 给出与 `tr_targets` 中每个数值对应的名称。
- en: 'Plot a random sample of 10 images for all the 10 possible classes:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为所有 10 种可能的类别绘制 10 张随机样本的图片：
- en: 'Import the relevant packages in order to plot a grid of images so that you
    can also work on arrays:'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包以绘制图像网格，以便也可以处理数组：
- en: '[PRE14]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a plot where we can show a 10 x 10 grid, where each row of the grid
    corresponds to a class and each column presents an example image belonging to
    the row’s class. Loop through the unique class numbers (`label_class`) and fetch
    the indices of rows (`label_x_rows`) corresponding to the given class number:'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个图表，我们可以展示一个 10 x 10 的网格，其中每行网格对应一个类别，每列呈现属于该行类别的示例图片。循环遍历唯一的类别号码（`label_class`），并获取对应给定类别号码的行索引（`label_x_rows`）：
- en: '[PRE15]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that in the preceding code, we fetch the 0^(th) index as the output of
    the `np.where` condition, as it has a length of 1\. It contains an array of all
    the indices where the target value (`tr_targets`) is equal to `label_class`.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在上述代码中，我们将 `np.where` 条件的第 0 个索引作为输出提取出来，因为其长度为 1。它包含所有目标值（`tr_targets`）等于
    `label_class` 的索引数组。
- en: 'Loop through 10 times to fill the columns of a given row. Furthermore, we need
    to select a random value (`ix`) from the indices corresponding to a given class
    that were obtained previously (`label_x_rows`) and plot them:'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环 10 次以填充给定行的列。此外，我们需要从之前获取的与给定类别对应的索引中选择一个随机值（`ix`）并绘制它们：
- en: '[PRE16]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This results in the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '![](img/B18457_03_10.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_10.png)'
- en: 'Figure 3.10: Sample Fashion MNIST images'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：Fashion MNIST 样本图片
- en: Note that in the preceding image, each row represents a sample of 10 different
    images all belonging to the same class.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述图片中，每一行代表同一类别的 10 张不同图片的样本。
- en: Training a neural network
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'To train a neural network, we must perform the following steps:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练神经网络，我们必须执行以下步骤：
- en: Import the relevant packages
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包
- en: Build a dataset that can fetch data one data point at a time
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个可以逐个数据点获取数据的数据集
- en: Wrap the dataloader from the dataset
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中包装DataLoader
- en: Build a model and then define the loss function and the optimizer
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个模型，然后定义损失函数和优化器
- en: Define two functions to train and validate a batch of data, respectively
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别定义两个函数来训练和验证一批数据
- en: Define a function that will calculate the accuracy of the data
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个计算数据准确率的函数
- en: Perform weight updates based on each batch of data over increasing epochs
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据每个数据批次进行权重更新，逐渐增加epochs
- en: 'In the following lines of code, we’ll perform each of the following steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码行中，我们将执行以下每个步骤：
- en: The following code can be found in the `Steps_to_build_a_neural_network_on_FashionMNIST.ipynb`
    file located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以上代码可以在GitHub的`Chapter03`文件夹中的`Steps_to_build_a_neural_network_on_FashionMNIST.ipynb`文件中找到，位于[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: 'Import the relevant packages and the `fmnist` dataset:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和`fmnist`数据集：
- en: '[PRE17]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Build a class that fetches the dataset. Remember that it is derived from a
    `Dataset` class and needs three magic functions, `__init__`, `__getitem__`, and
    `__len__,` to **always** be defined:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个获取数据集的类。请记住，它是从`Dataset`类派生的，并需要定义三个魔法函数`__init__`、`__getitem__`和`__len__`：
- en: '[PRE18]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that in the `__init__` method, we convert the input into a floating-point
    number and also flatten each image into 28*28 = 784 numeric values (where each
    numeric value corresponds to a pixel value). We also specify the number of data
    points in the `__len__` method; here, it is the length of `x`. The `__getitem__`
    method contains logic for what should be returned when we ask for the `ix`^(th)
    data point (`ix` will be an integer between `0` and `__len__`).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`__init__`方法中，我们将输入转换为浮点数，并将每个图像展平为28*28 = 784个数值（其中每个数值对应一个像素值）。在`__len__`方法中，我们还指定数据点的数量；这里是`x`的长度。`__getitem__`方法包含了当我们请求第`ix`个数据点时应该返回什么的逻辑（`ix`将是一个介于`0`和`__len__`之间的整数）。
- en: 'Create a function that generates a training DataLoader, `trn_dl`, from the
    dataset called `FMNISTDataset`. This will sample 32 data points at random for
    the batch size:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，从名为`FMNISTDataset`的数据集生成一个训练DataLoader，称为`trn_dl`。这将随机抽样32个数据点作为批量大小：
- en: '[PRE19]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define a model, as well as the loss function and the optimizer:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个模型，以及损失函数和优化器：
- en: '[PRE20]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The model is a network with one hidden layer containing 1,000 neurons. The output
    is a 10-neuron layer, since there are 10 possible classes. Furthermore, we call
    the `CrossEntropyLoss` function, since the output can belong to any of the 10
    classes for each image. Finally, the key aspect to note in this exercise is that
    we have initialized the learning rate, `lr`, to a value of `0.01` and not the
    default of `0.001`, to see how the model will learn for this exercise.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个含有1,000个神经元的隐藏层网络。输出是一个10个神经元的层，因为有10个可能的类。此外，我们调用`CrossEntropyLoss`函数，因为输出可以属于每个图像的10个类中的任意一个。最后，本练习中需要注意的关键方面是，我们已将学习率`lr`初始化为`0.01`，而不是默认值`0.001`，以查看模型在此练习中的学习情况。
- en: Note that we are not using “softmax” in the neural network at all. The range
    of outputs is unconstrained, in that values can have an infinite range, whereas
    cross-entropy loss typically expects outputs as probabilities (each row should
    sum to `1`). Unconstrained values in output still work in this setting because
    `nn.CrossEntropyLoss` actually expects us to send the raw logits (that is, unconstrained
    values). It performs softmax internally.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在神经网络中根本不使用“softmax”。输出的范围是不受限制的，即值可以具有无限的范围，而交叉熵损失通常期望输出为概率（每行应该总和为`1`）。在这种情况下，输出中的无约束值仍然可以工作，因为`nn.CrossEntropyLoss`实际上期望我们发送原始logits（即无约束值）。它在内部执行softmax。
- en: 'Define a function that will train the dataset on a batch of images:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将数据集训练到一批图像上：
- en: '[PRE21]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The preceding code passes the batch of images through the model in the forward
    pass. It also computes the loss on the batch and then passes the weights through
    backward propagation and updates them. Finally, it flushes the memory of the gradient
    so that it doesn’t influence how the gradient is calculated in the next pass.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码在前向传播中将图像批量传递给模型。它还计算批次上的损失，然后通过反向传播传递权重并更新它们。最后，它清除梯度的内存，以免影响下一次传播中的梯度计算方式。
- en: Now that we’ve done this, we can extract the loss value as a scalar by fetching
    `batch_loss.item()` on top of `batch_loss`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们完成了这些步骤，我们可以通过获取`batch_loss.item()`上的`batch_loss`来提取损失值作为标量。
- en: 'Build a function that calculates the accuracy of a given dataset:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个计算给定数据集准确率的函数：
- en: '[PRE22]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the preceding code, we explicitly mention that we don’t need to calculate
    the gradient by providing `@torch.no_grad()` and calculating the `prediction`
    values, by feed-forwarding input through the model. Next, we invoke `prediction.max(-1)`
    to identify the `argmax` index corresponding to each row. We then compare our
    `argmaxes` with the ground truth through `argmaxes == y` so that we can check
    whether each row is predicted correctly. Finally, we return the list of `is_correct`
    objects after moving it to a CPU and converting it into a NumPy array.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们明确提到通过使用`@torch.no_grad()`并计算`prediction`值，通过模型进行前向传播，我们不需要计算梯度。接下来，我们调用`prediction.max(-1)`来识别每行对应的`argmax`索引。然后，我们通过`argmaxes
    == y`来将我们的`argmaxes`与基准真实值进行比较，以便检查每行是否被正确预测。最后，我们在将其转移到CPU并转换为NumPy数组后返回`is_correct`对象列表。
- en: 'Train the neural network using the following lines of code:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码行训练神经网络：
- en: 'Initialize the model, loss, optimizer, and DataLoaders:'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型、损失、优化器和DataLoaders：
- en: '[PRE23]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Initialize the lists that will contain the accuracy and loss values at the
    end of each epoch:'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化将在每个epoch结束时包含准确率和损失值的列表：
- en: '[PRE24]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the number of epochs:'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义epoch的数量：
- en: '[PRE25]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Initialize the lists that will contain the accuracy and loss values corresponding
    to each batch within an epoch:'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化将包含每个epoch内每个批次的准确率和损失值的列表：
- en: '[PRE26]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create batches of training data by iterating through the DataLoader:'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过遍历DataLoader来创建训练数据的批次：
- en: '[PRE27]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Train the batch using the `train_batch` function, and store the loss value
    at the end of training on top of the batch as `batch_loss`. Furthermore, store
    the loss values across batches in the `epoch_losses` list:'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_batch`函数训练批次，并将训练结束时的损失值存储在`batch_loss`的顶部。此外，将跨批次的损失值存储在`epoch_losses`列表中：
- en: '[PRE28]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We store the mean loss value across all batches within an epoch:'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们存储每个epoch内所有批次的平均损失值：
- en: '[PRE29]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we calculate the accuracy of the prediction at the end of training on
    all batches:'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在所有批次训练结束时计算预测的准确率：
- en: '[PRE30]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Store the loss and accuracy values at the end of each epoch in a list:'
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表中存储每个epoch结束时的损失和准确率值：
- en: '[PRE31]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The variation of the training loss and accuracy over increasing epochs can
    be displayed using the following code:'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下代码显示随着epoch增加的训练损失和准确率的变化。
- en: '[PRE32]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![A screenshot of a computer  Description automatically generated](img/B18457_03_11.png)'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![计算机截图的描述](img/B18457_03_11.png)'
- en: 'Figure 3.11: Training loss and accuracy values over increasing epochs'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.11：随着epoch增加的训练损失和准确率值
- en: Our training accuracy is at 12% at the end of the five epochs. Note that the
    loss value did not decrease considerably over an increasing number of epochs.
    In other words, no matter how long we wait, it is unlikely that the model is going
    to provide high accuracy (say, above 80%). This calls for us to understand how
    the various hyperparameters that were used impact the accuracy of our neural network.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练准确率在五个epoch结束时为12%。请注意，随着epoch数量的增加，损失值并没有显著减少。换句话说，无论我们等待多长时间，模型都不太可能提供高准确率（比如超过80%）。这要求我们了解所使用的各种超参数如何影响神经网络的准确性。
- en: Note that since we did not specify `torch.random_seed(0)` at the start of code,
    the results might vary when you execute the code provided. However, the results
    you get should also let you reach a similar conclusion.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们在代码开始时没有指定`torch.random_seed(0)`，因此当您执行所提供的代码时，结果可能会有所不同。但是，您得到的结果应该让您得出类似的结论。
- en: Now that you have a complete picture of how to train a neural network, let’s
    study some good practices we should follow to achieve good model performance and
    the reasons behind using them. This can be achieved by fine-tuning various hyperparameters,
    some of which we will look at in the upcoming sections.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经全面了解了如何训练神经网络，让我们研究一些应遵循的良好实践以实现良好的模型性能以及使用它们的原因。可以通过微调各种超参数来实现这一目标，其中一些将在接下来的部分中进行介绍。
- en: Scaling a dataset to improve model accuracy
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放数据集以提高模型准确性
- en: 'Scaling a dataset is the process of ensuring that the variables are confined
    to a finite range. In this section, we will confine the independent variables’
    values to between `0` and `1` by dividing each input value by the maximum possible
    value in the dataset. This is a value of `255`, which corresponds to white pixels:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放数据集是确保变量限制在有限范围内的过程。在本节中，我们将通过将每个输入值除以数据集中可能的最大值255来将独立变量的值限制在0到1之间。这对应于白色像素的值`255`：
- en: For brevity’s sake, we have only provided the modified code (from the previous
    section) in the upcoming code. The full code can be found in the `Scaling_the_dataset.ipynb`
    file located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为简洁起见，我们仅在接下来的代码中提供了修改后的代码（来自前一节）。完整代码可在GitHub上的`Chapter03`文件夹中的`Scaling_the_dataset.ipynb`文件中找到：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Fetch the dataset, as well as the training images and targets, as we did in
    the previous section.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据集以及训练图像和目标，就像我们在前一节中所做的那样。
- en: 'Modify `FMNISTDataset`, which fetches data, so that the input image is divided
    by 255 (the maximum intensity/value of a pixel):'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`FMNISTDataset`，该数据集获取数据，使得输入图像除以255（像素的最大强度/值）：
- en: '[PRE33]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Note that the only change we’ve made here compared to the previous section
    is that we divide the input data by the maximum possible pixel value: `255`.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与上一节相比，我们唯一更改的是将输入数据除以最大可能像素值：`255`。
- en: Given that the pixel values range between `0` to `255`, dividing them by 255
    will result in values that are always between `0` and `1`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于像素值范围在`0`到`255`之间，将它们除以255将导致值始终在`0`到`1`之间。
- en: 'Train a model, just like we did in *steps 4*, *5*, *6*, and *7*, of the previous
    section. The variations for the training loss and accuracy values are as follows:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型，就像我们在前一节的步骤4、5、6和7中所做的那样。训练损失和准确率值的变化如下：
- en: '![A screenshot of a computer screen  Description automatically generated](img/B18457_03_12.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图 自动生成的描述](img/B18457_03_12.png)'
- en: 'Figure 3.12: Training loss and accuracy values over increasing epochs on a
    scaled dataset'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：在经过扩展的数据集上随着周期的增加的训练损失和准确率数值
- en: As we can see, the training loss consistently reduced and the training accuracy
    consistently increased to an accuracy of ~85%. Contrast the preceding output with
    the scenario where input data is not scaled, where training loss did not reduce
    consistently, and the accuracy of the training dataset at the end of five epochs
    was only 12%.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，训练损失持续降低，训练准确率持续增加，达到约85%的准确率。与未对输入数据进行缩放的情况相比，训练损失未能持续降低，而五个周期结束时训练数据集的准确率仅为12%。
- en: 'Let’s dive into the possible reason why scaling helps here. We’ll take the
    example of how a sigmoid value is calculated:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨为什么在这里缩放有帮助的可能原因。我们将以Sigmoid值计算的示例为例：
- en: '![](img/B18457_03_001.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_001.png)'
- en: 'In the following table, we’ve calculated the **Sigmoid** column based on the
    preceding formula:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们根据前述公式计算了**Sigmoid**列。
- en: '![](img/B18457_03_13.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_13.png)'
- en: 'Figure 3.13: Sigmoid value for the different values of input and weight'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：不同输入和权重值的Sigmoid值
- en: In the left-hand table, we can see that when the weight values are more than
    0.1, the sigmoid value does not vary with an increasing (changing) weight value.
    Furthermore, the sigmoid value changed only by a little when the weight was extremely
    small; the only way to vary the sigmoid value is by changing the weight by a very,
    very small amount.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在左表中，我们可以看到当权重值大于0.1时，Sigmoid值不随权重值的增加（变化）而变化。此外，当权重非常小时，Sigmoid值只有少量变化；改变Sigmoid值的唯一方法是通过非常小的数值改变权重。
- en: However, the sigmoid value changed considerably in the right-hand table when
    the input value was small.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当输入值很小时，右表中的Sigmoid值变化很大。
- en: The reason for this is that the exponential of a large negative value (resulting
    from multiplying the weight value by a large number) is very close to 0, while
    the exponential value varies when the weight is multiplied by a scaled input,
    as seen in the right-hand table.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为大的负值的指数（由于将权重值乘以一个大数得出）非常接近于0，而指数值在权重乘以扩展输入时变化，正如右表所示。
- en: Now that we understand that the sigmoid value does not change considerably unless
    the weight values are very small, we will learn about how weight values can be
    influenced toward an optimal value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解到，除非权重值非常小，否则sigmoid值不会有很大变化，我们将学习如何将权重值影响向最优值。
- en: Scaling the input dataset so that it contains a much smaller range of values
    generally helps to achieve better model accuracy.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放输入数据集，使其包含一个范围更小的值通常有助于实现更好的模型准确性。
- en: 'Next, we’ll learn about the impact of one of the other major hyperparameters
    of any neural network: **batch size.**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习神经网络的另一个重要超参数之一的影响：**批大小**。
- en: Understanding the impact of varying the batch size
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解不同批大小的影响
- en: In the previous sections, 32 data points were considered per batch in the training
    dataset. This resulted in a greater number of weight updates per epoch, as there
    were 1,875 weight updates per epoch (60,000/32 is nearly equal to 1,875, where
    60,000 is the number of training images).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，训练数据集中每批有32个数据点。这导致每个epoch有更多的权重更新次数，因为每个epoch有1,875次权重更新（60,000/32接近于1,875，其中60,000是训练图像的数量）。
- en: Furthermore, we did not consider the model’s performance on an unseen dataset
    (validation dataset). We will explore this in this section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们没有考虑模型在未见数据集（验证数据集）上的性能。我们将在本节中探讨这一点。
- en: 'In this section, we will compare the following:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将比较以下内容：
- en: The loss and accuracy values of the training and validation data when the training
    batch size is 32
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练批大小为32时，训练和验证数据的损失值和准确率。
- en: The loss and accuracy values of the training and validation data when the training
    batch size is 10,000
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练批大小为10,000时，训练和验证数据的损失值和准确率。
- en: Now that we have brought validation data into the picture, let’s rerun the code
    provided in the *Building a neural network* section with additional code to generate
    validation data, as well as to calculate the loss and accuracy values of the validation
    dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了验证数据，让我们重新运行*构建神经网络*部分中提供的代码，增加额外的代码以生成验证数据，并计算验证数据集的损失值和准确率。
- en: For brevity’s sake, we have only provided the modified code (from the previous
    section) in the upcoming section. The full code can be found in the `Varying_batch_size.ipynb`
    file in the `Chapter03` folder in the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们只提供了修改后的代码（来自上一节）在接下来的部分。完整的代码可以在GitHub存储库中的`Chapter03`文件夹中的`Varying_batch_size.ipynb`文件中找到，链接为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Batch size of 32
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批大小为32
- en: 'Given that we have already built a model that uses a batch size of 32 during
    training in the previous section, we will elaborate on the additional code that
    is used to work on the validation dataset. We’ll skip going through the details
    of training the model, since this is already covered in the *Building a neural
    network* section. Let’s get started:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经在前一节中使用批大小为32构建了一个模型，现在我们将详细说明用于处理验证数据集的附加代码。我们将跳过训练模型的详细过程，因为这已经在*构建神经网络*部分中涵盖了。让我们开始吧：
- en: Download and import the training images and targets.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并导入训练图像和目标。
- en: 'In a similar manner to training images, we must download and import the validation
    dataset by specifying `train = False`, while calling the `FashionMNIST` method
    in our datasets:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与训练图像类似，我们必须通过在调用数据集中的`FashionMNIST`方法时指定`train = False`来下载和导入验证数据集：
- en: '[PRE34]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Import the relevant packages and define `device`.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包并定义`device`。
- en: Define the dataset class (`FashionMNIST`) and the functions that will be used
    to train on a batch of data (`train_batch`), calculate the accuracy (`accuracy`),
    and then define the model architecture, the loss function, and the optimizer (`get_model`).
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集类（`FashionMNIST`）和将用于批处理数据的函数（`train_batch`）、计算准确率的函数（`accuracy`），然后定义模型架构、损失函数和优化器（`get_model`）。
- en: 'Define a function that will get data: that is, `get_data`. This function will
    return the training data with a batch size of `32` and the validation dataset
    with a batch size that’s the length of the validation data (we will not use the
    validation data to train the model; we will only use it to understand the model’s
    accuracy on unseen data):'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将获取数据，即`get_data`。该函数将返回批大小为`32`的训练数据和与验证数据长度相同的验证数据集（我们将不使用验证数据来训练模型；我们只使用它来了解模型在未见数据上的准确性）：
- en: '[PRE35]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the preceding code, we created an object of the `FMNISTDataset` class named
    `val`, in addition to the `train` object that we saw earlier. Furthermore, the
    DataLoader for validation (`val_dl`) was fetched with a batch size of `len(val_images)`,
    while the batch size of `trn_dl` is `32`. This is because the training data is
    used to train the model while we fetch the accuracy and loss metrics of the validation
    data. In this section and the next, we are trying to understand the impact of
    varying `batch_size`, based on the model’s training time and accuracy.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们创建了 `FMNISTDataset` 类的 `val` 对象，除了之前看到的 `train` 对象。此外，验证数据的 DataLoader
    (`val_dl`) 使用了 `val_images` 的长度作为批大小，而 `trn_dl` 的批大小为 `32`。这是因为训练数据用于训练模型，而我们获取验证数据的准确率和损失指标。在本节和下节中，我们尝试理解基于模型训练时间和准确性变化
    `batch_size` 的影响。
- en: 'Define a function that calculates the loss of the validation data: that is,
    `val_loss`. Note that we are calculating this separately, since the loss of training
    data is calculated while training the model:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来计算验证数据的损失值：即 `val_loss`。请注意，我们单独计算这个，因为在训练模型时计算了训练数据的损失值：
- en: '[PRE36]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, we apply `torch.no_grad` because we don’t train the model and
    only fetch predictions. Furthermore, we pass our `prediction` through the loss
    function (`loss_fn`) and return the loss value (`val_loss.item()`).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们应用 `torch.no_grad` 因为我们不训练模型，只获取预测。此外，我们通过损失函数 (`loss_fn`) 传递我们的 `prediction`
    并返回损失值 (`val_loss.item()`)。
- en: 'Fetch the training and validation DataLoaders. Also, initialize the model,
    loss function, and optimizer:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练和验证 DataLoader。同时，初始化模型、损失函数和优化器：
- en: '[PRE37]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Train the model, as follows:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型如下：
- en: 'Initialize the lists that contain training and validation accuracy, as well
    as loss values over increasing epochs:'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化包含训练和验证准确率以及损失值的列表，随着 epoch 增加逐步记录它们的变化：
- en: '[PRE38]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Loop through five epochs, and initialize lists that contain accuracy and loss
    across batches of training data within a given epoch:'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在五个 epoch 中循环，并初始化包含给定 epoch 内训练数据批次的准确率和损失的列表：
- en: '[PRE39]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Loop through batches of training data, and calculate the accuracy (`train_epoch_accuracy`)
    and loss value (`train_epoch_loss`) within an epoch:'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个 epoch 内，循环遍历训练数据的批次，并计算准确率 (`train_epoch_accuracy`) 和损失值 (`train_epoch_loss`)：
- en: '[PRE40]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Calculate the loss value and accuracy within the one batch of validation data
    (since the batch size of the validation data is equal to the length of the validation
    data):'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算验证数据一批次内的损失值和准确率（因为验证数据的批大小等于验证数据的长度）：
- en: '[PRE41]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that in the preceding code, the loss value of the validation data is calculated
    using the `val_loss` function and is stored in the `validation_loss` variable.
    Furthermore, the accuracy of all the validation data points is stored in the `val_is_correct`
    list, while the mean of this is stored in the `val_epoch_accuracy` variable.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，在上述代码中，验证数据的损失值是使用 `val_loss` 函数计算的，并存储在 `validation_loss` 变量中。此外，所有验证数据点的准确率存储在
    `val_is_correct` 列表中，而其平均值存储在 `val_epoch_accuracy` 变量中。
- en: 'Finally, we append the training and validation datasets’ accuracy and loss
    values to the lists that contain the epoch-level aggregate validation and accuracy
    values. We’re doing this so that we can look at the epoch level’s improvement
    in the next step:'
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将训练和验证数据集的准确率和损失值附加到包含 epoch 级别聚合验证和准确率值的列表中。我们这样做是为了在下一步中查看 epoch 级别的改进：
- en: '[PRE42]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Visualize the improvements in the accuracy and loss values in the training
    and validation datasets over increasing epochs:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化训练和验证数据集在增加的 epoch 中准确率和损失值的改善：
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_14.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图 自动生成的描述](img/B18457_03_14.png)'
- en: 'Figure 3.14: Training and validation loss and accuracy over increasing epochs
    with a batch size of 32'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.14: 随着批大小为 32 增加的情况下的训练和验证损失及准确率'
- en: As you can see, the training and validation accuracy is ~85% by the end of five
    epochs when the batch size is 32\. Next, we will vary the `batch_size` parameter
    when training the DataLoader in the `get_data` function to see its impact on accuracy
    at the end of five epochs.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当批大小为 32 时，训练和验证准确率在五个 epoch 结束时达到约 85%。接下来，我们将在 `get_data` 函数中训练 DataLoader
    时变化 `batch_size` 参数，以查看其对五个 epoch 结束时准确率的影响。
- en: Batch size of 10,000
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批大小为 10,000
- en: In this section, we’ll use 10,000 data points per batch so that we can understand
    what impact varying the batch size has.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将每批使用 10,000 个数据点，以便了解变化 `batch_size` 的影响。
- en: Note that the code provided in the *Batch size of 32* section remains exactly
    the same here, except for the code in *step 5*, where we will specify a batch
    size of 10,000\. We encourage you to refer to the respective notebook that’s available
    in this book’s GitHub repository while executing the code.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“批量大小为 32”的部分提供的代码在此处保持完全相同，除了“步骤 5”中的代码，我们将指定批量大小为 10,000。在执行代码时，请参考此书的
    GitHub 仓库中提供的相应笔记本。
- en: 'By making only this necessary change in *step 5* and after executing all the
    steps until *step 9*, the variation in the training and validation’s accuracy
    and loss over increasing epochs when the batch size is 10,000 is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在“步骤 5”中仅进行这一必要更改，并在执行所有步骤直到“步骤 9”后，当批量大小为 10,000 时，随着增加时期的训练和验证准确率和损失的变化如下：
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_15.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 描述自动生成](img/B18457_03_15.png)'
- en: 'Figure 3.15: Training and validation loss and accuracy over increasing epochs
    with a batch size of 10,000'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：使用批量大小为 10,000 的增加时期的训练和验证损失和准确率
- en: Here, we can see that the accuracy and loss values did not reach the same levels
    as that of the previous scenario, where the batch size was 32, because weights
    are updated fewer times per epoch (6 times) when compared to a batch size of 32
    (1,875).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到准确率和损失值未达到前一个场景的相同水平，其中批量大小为 32，因为每个时期更新权重的次数较少（6 次），而批量大小为 32 为 1,875
    次。
- en: Having a lower batch size generally helps in achieving optimal accuracy when
    you have a small number of epochs, but it should not be so low that training time
    is impacted.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只有少量时期时，较低的批量大小通常有助于实现最佳精度，但不应过低以至于影响训练时间。
- en: So far, we have learned how to scale a dataset, as well as the impact of varying
    the batch size on the model’s training time to achieve a certain accuracy. In
    the next section, we will learn about the impact of varying the loss optimizer
    on the same dataset.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何缩放数据集，以及在模型训练时间内通过改变批量大小来达到特定精度的影响。在接下来的部分中，我们将学习如何在相同数据集上改变损失优化器的影响。
- en: Understanding the impact of varying the loss optimizer
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解不同损失优化器的影响
- en: 'So far, we have optimized loss based on the Adam optimizer. A loss optimizer
    helps to arrive at optimal weight values to minimize overall loss. There are a
    variety of loss optimizers (different ways of updating weight values to minimize
    loss values) that impact the overall loss and accuracy of a model. In this section,
    we will do the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经基于 Adam 优化器优化了损失。损失优化器有助于确定最优权重值以最小化总体损失。有多种损失优化器（不同的更新权重值以最小化损失值的方式）会影响模型的整体损失和准确率。在本节中，我们将执行以下操作：
- en: Modify the optimizer so that it becomes a **Stochastic Gradient Descent** (**SGD**)
    optimizer
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改优化器，使其成为**随机梯度下降**（**SGD**）优化器
- en: Revert to a batch size of 32 while fetching data in the DataLoader
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DataLoader 中获取数据时恢复为批量大小为 32
- en: Increase the number of epochs to 10 (so that we can compare the performance
    of SGD and Adam over a longer number of epochs)
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将时期数增加到 10（这样我们可以比较 SGD 和 Adam 在更长时期内的表现）
- en: Making these changes means that only one step in the *Batch size of 32* section
    will change (since the batch size is already 32 in that section); that is, we
    will modify the optimizer so that it’s the SGD optimizer.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更改意味着只有“批量大小为 32”的部分中的一个步骤会改变（因为该部分的批量大小已经为 32）；也就是说，我们将修改优化器，使其成为 SGD 优化器。
- en: 'Let’s modify the `get_model` function in *step 4* of the *Batch size of 32*
    section to modify the optimizer so that we use the SGD optimizer instead, as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改“批量大小为 32”的“步骤 4”中的`get_model`函数，以修改优化器，使其使用 SGD 优化器，如下所示：
- en: The complete code can be found in the `Varying_loss_optimizer.ipynb` file located
    in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For the sake of brevity, we will not detail every step from the *Batch size of
    32* section; instead, in the following code, we will discuss only those steps
    where changes are introduced. **We encourage you to refer to the respective notebooks
    on GitHub while executing the code.**
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在 GitHub 上的`Chapter03`文件夹中的`Varying_loss_optimizer.ipynb`文件中找到，链接为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。为了简洁起见，我们不会详细说明“批量大小为
    32”的每一个步骤；相反，在接下来的代码中，我们将仅讨论引入更改的步骤。**在执行代码时，建议您参考 GitHub 上的相应笔记本。**
- en: 'Modify the optimizer so that you use the SGD optimizer in the `get_model` function
    while ensuring that everything else remains the same:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改优化器，使其在`get_model`函数中使用 SGD 优化器，同时确保其他所有内容保持不变：
- en: '[PRE43]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now, let’s increase the number of epochs in *step 8* while keeping every other
    step (except for *steps 4* and *8*) the same as they are in the *Batch size of
    32* section.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在*步骤 8* 中增加 epoch 数量，同时保持除*步骤 4* 和*步骤 8* 外的所有其他步骤与*批大小为 32*节相同。
- en: Increase the number of epochs we’ll use to train the model.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加我们将用于训练模型的 epoch 数量。
- en: 'After making these changes, once we execute all the remaining steps in the
    *Batch size of 32* section in order, the variation in the training and validation
    datasets’ accuracy and loss values over increasing epochs (when trained with the
    SGD and Adam optimizers individually) will be as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这些更改后，一旦按顺序执行*批大小为 32*节中的所有其余步骤，使用 SGD 和 Adam 优化器单独训练时，随着 epoch 增加，训练和验证数据集的准确性和损失值变化如下：
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_16.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18457_03_16.png)'
- en: 'Figure 3.16: Training and validation loss and accuracy over increasing epochs
    with SGD optimizer'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16：使用 SGD 优化器随着 epoch 增加的训练和验证损失及准确率
- en: '![](img/B18457_03_17.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_17.png)'
- en: 'Figure 3.17: Training and validation loss and accuracy over increasing epochs
    with Adam optimizer'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17：使用 Adam 优化器随着 epoch 增加的训练和验证损失及准确率
- en: As you can see, when we used the Adam optimizer, the accuracy was still very
    close to 85%.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，当我们使用 Adam 优化器时，准确率仍非常接近 85%。
- en: '**Note**'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Certain optimizers achieve optimal accuracy faster compared to others. Some
    of the other prominent optimizers that are available include Adagrad, Adadelta,
    AdamW, LBFGS, and RMSprop.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 某些优化器比其他优化器更快地达到最佳准确率。其他显著的优化器包括 Adagrad、Adadelta、AdamW、LBFGS 和 RMSprop。
- en: So far, we have used a learning rate of 0.01 while training our models and maintained
    it across all the epochs while training the model. In *Chapter 1*, *Artificial
    Neural Network Fundamentals*, we learned that the learning rate plays a key role
    in attaining optimal weight values. Here, the weight values gradually move toward
    the optimal value when the learning rate is small, while the weight values oscillate
    at a non-optimal value when the learning rate is large.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在训练我们的模型时，我们使用了学习率为 0.01，并在训练过程中的所有 epoch 中保持不变。在*第一章*，*人工神经网络基础*中，我们学到学习率在达到最佳权重值时发挥关键作用。在这里，当学习率较小时，权重值逐渐向最优值移动，而当学习率较大时，权重值在非最优值处振荡。
- en: However, initially, it would be intuitive for the weights to be updated quickly
    to a near-optimal scenario. From then on, they should be updated very slowly,
    since the amount of loss that gets reduced initially is high and the amount of
    loss that gets reduced in the later epochs would be low.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最初权重迅速更新到接近最优状态是直观的。从那时起，它们应该被非常缓慢地更新，因为最初减少的损失量很高，而后续 epoch 中减少的损失量则很低。
- en: This calls for having a high learning rate initially and gradually lowering
    it later as the model achieves near-optimal accuracy. This requires us to understand
    when the learning rate must be reduced (learning rate annealing over time). Refer
    to the `Learning_rate_annealing.ipynb` file located in the `Chapter03` folder
    on GitHub to understand the impact of learning rate annealing.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这要求初始时具有较高的学习率，并随后逐渐降低，直到模型达到接近最优准确率。这要求我们理解何时必须降低学习率（随时间的学习率退火）。请参阅 GitHub
    上`Chapter03`文件夹中的`Learning_rate_annealing.ipynb`文件，以了解学习率退火的影响。
- en: 'To understand the impact of the varying learning rate, the following scenarios
    will help:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解学习率变化的影响，以下几种情况将有所帮助：
- en: Higher learning rate (0.1) on a scaled dataset
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在经过缩放的数据集上使用较高的学习率（0.1）
- en: Lower learning rate (0.00001) on a scaled dataset
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在经过缩放的数据集上使用较低的学习率（0.00001）
- en: Lower learning rate (0.00001) on a non-scaled dataset
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在未经缩放的数据集上使用较低的学习率（0.00001）
- en: These three scenarios will not be covered in this chapter; however, you can
    access the full code for them in the `Varying_learning_rate_on_scaled_data.ipynb`
    file and `Varying_learning_rate_on_non_scaled_data.ipynb` file in the `Chapter03`
    folder in the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种情况不会在本章中讨论；但是，您可以在 GitHub 存储库的`Chapter03`文件夹中的`Varying_learning_rate_on_scaled_data.ipynb`文件和`Varying_learning_rate_on_non_scaled_data.ipynb`文件中获取它们的全部代码，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: In the next section, we will learn about how the number of layers in a neural
    network impacts its accuracy.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习神经网络中层数如何影响其准确性。
- en: Building a deeper neural network
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建更深的神经网络
- en: So far, our neural network architecture only has one hidden layer. In this section,
    we will contrast the performance of models where there are two hidden layers and
    no hidden layer (with no hidden layer being a logistic regression).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的神经网络架构仅有一个隐藏层。在本节中，我们将对比存在两个隐藏层和没有隐藏层（没有隐藏层时为逻辑回归）的模型性能。
- en: 'A model with two layers within a network can be built as follows (note that
    we have kept the number of units in the second hidden layer set to 1,000). The
    modified `get_model` function (from the code in the *Batch size of 32* section),
    where there are two hidden layers, is as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可以构建如下的网络内两个层的模型（请注意，我们将第二个隐藏层的单元数设置为1,000）。修改后的`get_model`函数（来自*批量大小为32*部分中的代码），其中包含两个隐藏层，如下所示：
- en: The full code can be found in the `Impact_of_building_a_deeper_neural_network.ipynb`
    file located in the `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For the sake of brevity, we will not detail every step from the *Batch size of
    32* section. Please refer to the notebooks on GitHub while executing the code**.**
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在GitHub上的`Chapter03`文件夹中的`Impact_of_building_a_deeper_neural_network.ipynb`文件中找到，网址为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。为了简洁起见，我们不会详细描述*批量大小为32*部分的每一步。在执行代码时，请参考GitHub上的笔记本。
- en: '[PRE44]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Similarly, the `get_model` function, where there are *no hidden layers*, is
    as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，没有隐藏层的`get_model`函数如下所示：
- en: '[PRE45]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note that, in the preceding function, we connect the input directly to the output
    layer.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述函数中，请注意，我们直接将输入连接到输出层。
- en: 'Once we train the models as we did in the *Batch size of 32* section, the accuracy
    and loss on the train and validation datasets will be as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们像在*批量大小为32*部分那样训练模型时，训练和验证数据集上的准确率和损失如下：
- en: '![](img/B18457_03_18.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的带有数字说明的数字表格](img/B18457_03_18.png)'
- en: 'Figure 3.18: Training and validation loss and accuracy over increasing epochs
    with varying number of hidden layers'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18：随着不同隐藏层数量的增加，训练和验证损失以及准确率的变化
- en: 'Here, take note of the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，请注意以下内容：
- en: The model was unable to learn well when there were no hidden layers.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当没有隐藏层时，模型无法学习得很好。
- en: The model overfit by a larger amount when there were two hidden layers compared
    to one hidden layer (the validation loss is higher in the model with two layers
    compared to the model with one layer).
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与仅有一个隐藏层的模型相比，当存在两个隐藏层时，模型的过拟合程度更大（验证损失比仅有一个隐藏层的模型更高）。
- en: So far, across different sections, we have seen that the model was unable to
    be trained well when the input data wasn’t scaled (brought down to a small range).
    Non-scaled data (data with a higher range) can also occur in hidden layers (especially
    when we have deep neural networks with multiple hidden layers) because of the
    matrix multiplication that’s involved in getting the values of nodes in hidden
    layers. Let’s learn about how batch normalization helps to deal with non-scaled
    data in the next section.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在不同的部分中，我们发现当输入数据未经过缩放时（未缩小到一个小范围），模型的训练效果并不好。非缩放的数据（具有较大范围的数据）也可能出现在隐藏层中（特别是在具有多个隐藏层的深度神经网络中），这是因为涉及到节点值获取的矩阵乘法。让我们在下一节中了解批量归一化如何帮助处理非缩放数据。
- en: Understanding the impact of batch normalization
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解批量归一化的影响
- en: Previously, we learned that when the input value is large, the variation of
    the sigmoid output doesn’t make much difference when the weight values change
    considerably.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前学到，当输入值较大时，当权重值发生显著变化时，Sigmoid输出的变化不会有太大的差异。
- en: 'Now, let’s consider the opposite scenario, where the input values are very
    small:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑相反的情况，即输入值非常小的情况：
- en: '![A table of numbers with numbers  Description automatically generated](img/B18457_03_19.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的带有数字说明的数字表格](img/B18457_03_19.png)'
- en: 'Figure 3.19: Sigmoid value for the different values of input and weight'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19：不同输入和权重值的Sigmoid值
- en: When the input value is very small, the sigmoid output changes slightly, requiring
    a big change to the weight value to achieve optimal results.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入值非常小时，Sigmoid输出会稍微变化，需要对权重值进行大的改变才能达到最优结果。
- en: Additionally, in the *Scaling the input data* section, we saw that large input
    values have a negative effect on training accuracy. This suggests that we can
    neither have very small nor very big values for our input.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在*缩放输入数据*部分中，我们看到大输入值对训练精度有负面影响。这表明，我们既不能有非常小的输入值，也不能有非常大的输入值。
- en: 'Along with very small or very big values in input, we may also encounter a
    scenario where the value of one of the nodes in the hidden layer could result
    in either a very small number or a very large number, resulting in the same issue
    we saw previously with the weights connecting the hidden layer to the next layer.
    Batch normalization comes to the rescue in such a scenario, since it normalizes
    the values at each node, just like when we scaled our input values. Typically,
    all the input values in a batch are scaled as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 除了输入中的非常小或非常大的值之外，我们还可能遇到一个情况，即隐藏层中某个节点的值可能会导致一个非常小的数字或一个非常大的数字，这会导致与连接隐藏层到下一层的权重之前看到的问题相同。在这种情况下，批次归一化就派上用场了，因为它像我们缩放输入值时那样归一化每个节点的值。通常，批次中的所有输入值都按以下方式缩放：
- en: '![](img/B18457_03_002.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_002.png)'
- en: '![](img/B18457_03_003.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_003.png)'
- en: '![](img/B18457_03_004.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_004.png)'
- en: '![](img/B18457_03_005.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_005.png)'
- en: By subtracting each data point from the batch mean and then dividing it by the
    batch variance, we have normalized all the data points of the batch at a node
    to a fixed range. While this is known as hard normalization, by introducing the
    γ and β parameters, we let the network identify the best normalization parameters.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从批次均值减去每个数据点，然后将其除以批次方差，我们将节点上批次的所有数据点归一化到一个固定范围。虽然这被称为硬归一化，但通过引入γ和β参数，我们让网络确定最佳归一化参数。
- en: 'To understand how the batch normalization process helps, let’s look at the
    loss and accuracy values on the training and validation datasets, as well as the
    distribution of hidden layer values, in the following scenarios:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解批次归一化过程如何帮助，让我们看看在训练和验证数据集上的损失和精度值，以及以下场景中隐藏层值的分布：
- en: Very small input values without batch normalization
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有批次归一化的非常小的输入值
- en: Very small input values with batch normalization
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带有批次归一化的非常小的输入值
- en: Let’s get started!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Very small input values without batch normalization
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有批次归一化的非常小的输入值
- en: So far, when we had to scale input data, we scaled it at a value between 0 and
    1\. In this section, we will scale it further at a value between 0 and 0.0001
    so that we can understand the impact of scaling data. As we saw at the beginning
    of this section, small input values cannot change the sigmoid value, even with
    a big variation in weight values.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，当我们必须缩放输入数据时，我们将其缩放为介于0和1之间的值。在本节中，我们将进一步将其缩放为介于0和0.0001之间的值，以便我们能够理解缩放数据的影响。正如我们在本节开头所看到的，即使权重值变化很大，小输入值也不能改变sigmoid值。
- en: 'To scale the input dataset so that it has a very low value, we’ll change the
    scaling that we typically do in the `FMNISTDataset` class by reducing the range
    of input values from `0` to `0.0001`, as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缩放输入数据集，使其具有非常低的值，我们将通过以下方式更改我们通常在`FMNISTDataset`类中执行的缩放，即通过将输入值的范围从`0`减少到`0.0001`。
- en: The full code can be found in the `Batch_normalization.ipynb` file in the `Chapter03`
    folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e). For brevity,
    we will not detail every step from the *Batch size of 32* section. **Refer to
    the notebooks on GitHub while executing the code.**
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在GitHub上的`Chapter03`文件夹中的`Batch_normalization.ipynb`文件中找到。为简洁起见，我们不会详细说明*批次大小为32*部分的每一个步骤。**在执行代码时，请参考GitHub上的笔记本。**
- en: '[PRE46]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note that in the lines of code we’ve highlighted (`x = x.float()/(255*10000)`),
    we have reduced the range of input pixel values by dividing them by 10,000.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们突出显示的代码行中（`x = x.float()/(255*10000)`），我们通过将其除以10000来减少输入像素值的范围。
- en: 'Next, we must redefine the `get_model` function so that we can fetch the model’s
    prediction, as well as the values for the hidden layer. We can do this by specifying
    a neural network class, as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须重新定义`get_model`函数，以便我们可以获取模型的预测以及隐藏层的值。我们可以通过指定一个神经网络类来做到这一点，如下所示：
- en: '[PRE47]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, we defined the `neuralnet` class, which returns the output
    layer values (`x2`) and the hidden layer’s activation values (`x1`). Note that
    the architecture of the network hasn’t changed.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们定义了`neuralnet`类，该类返回输出层值（`x2`）和隐藏层的激活值（`x1`）。请注意，网络的架构没有改变。
- en: 'Given that the `get_model` function returns two outputs now, we need to modify
    the `train_batch` and `val_loss` functions, which make predictions, by passing
    input through the model. Here, we’ll only fetch the output layer values, not the
    hidden layer values. Given that the output layer values are in the 0^(th) index
    of what is returned from the model, we’ll modify the functions so that they only
    fetch the 0^(th) index of predictions, as follows:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 `get_model` 函数现在返回两个输出，我们需要修改 `train_batch` 和 `val_loss` 函数，这些函数进行预测时，通过模型传递输入。在这里，我们只会获取输出层的数值，而不是隐藏层的数值。考虑到从模型返回的内容中输出层的数值位于第
    0^(th) 索引中，我们将修改这些函数，使它们只获取预测结果的第 0^(th) 索引，如下所示：
- en: '[PRE48]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note that the highlighted portions of the preceding code are where we have ensured
    we only fetch the 0^(th) index of the model’s output (since the 0^(th) index contains
    the output layer’s values).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述代码的突出部分是我们确保只获取模型输出的第 0^(th) 索引处的位置（因为第 0^(th) 索引包含输出层的数值）。
- en: 'Now, when we run the rest of the code provided in the *Scaling* *the data*
    section, we’ll see that the variation in the accuracy and loss values in the training
    and validation datasets over increasing epochs is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们运行 *Scaling* *the data* 部分提供的其余代码时，我们将看到随着 epochs 增加，训练和验证数据集中准确率和损失值的变化如下：
- en: '![Chart  Description automatically generated](img/B18457_03_20.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成的描述](img/B18457_03_20.png)'
- en: 'Figure 3.20: Training and validation loss and accuracy when network is trained
    with very small input values'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.20：当网络使用非常小的输入值训练时的训练和验证损失与准确率
- en: Note that in the preceding scenario, the model didn’t train well, even after
    100 epochs (the model was trained to an accuracy of ~90% on the validation dataset
    within 10 epochs in the previous sections, while the current model only has ~85%
    validation accuracy).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上述情景中，即使经过 100 个 epochs，模型训练效果仍然不佳（在之前的章节中，模型在验证数据集上的准确率在 10 个 epochs 内就能达到约
    90%，而当前模型的验证准确率仅为约 85%）。
- en: 'Let’s understand the reason why the model doesn’t train as well when the input
    values have a very small range by exploring the hidden values’ distribution, as
    well as the parameter distribution:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过探索隐藏值的分布以及参数分布，来理解当输入值具有非常小的范围时，为何模型训练效果不佳：
- en: '![A blue rectangular object with numbers  Description automatically generated](img/B18457_03_21.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![一个带有数字的蓝色矩形对象  自动生成的描述](img/B18457_03_21.png)'
- en: 'Figure 3.21: Distribution of weights and hidden layer node values when network
    is trained with very small input values'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.21：当网络使用非常小的输入值训练时的权重和隐藏层节点数值的分布
- en: Note that the first distribution indicates the distribution of values in the
    hidden layer (where we can see that the values have a very small range). Furthermore,
    given that both the input and hidden layer values have a very small range, the
    weights had to be varied by a large amount (for both the weights that connect
    the input to the hidden layer and the weights that connect the hidden layer to
    the output layer).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一个分布指示了隐藏层数值的分布（我们可以看到这些数值具有非常小的范围）。此外，考虑到输入和隐藏层数值都非常小，权重必须经过大量变化（包括连接输入到隐藏层的权重和连接隐藏层到输出层的权重）。
- en: Now that we know that the network doesn’t train well when the input values have
    a very small range, let’s understand how batch normalization helps increase the
    range of values within the hidden layer.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，当输入值具有非常小的范围时，网络训练效果不佳，让我们来了解批标准化如何帮助增加隐藏层中数值的范围。
- en: Very small input values with batch normalization
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用批标准化的非常小的输入值
- en: In this section, we’ll only make one change to the code from the previous subsection;
    that is, we’ll add batch normalization while defining the model architecture.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将只对前一小节的代码进行一处更改；也就是在定义模型架构时添加批标准化。
- en: 'The modified `get_model` function is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的 `get_model` 函数如下所示：
- en: '[PRE49]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the preceding code, we declared a variable (`batch_norm`) that performs batch
    normalization (`nn.BatchNorm1d`). The reason we perform `nn.BatchNorm1d(1000)`
    is because the output dimension is 1,000 for each image (that is, a 1-dimensional
    output for the hidden layer). Furthermore, in the `forward` method, we pass the
    output of the hidden layer values through batch normalization, prior to ReLU activation.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们声明了一个变量（`batch_norm`），它执行批量归一化（`nn.BatchNorm1d`）。我们执行`nn.BatchNorm1d(1000)`的原因是因为每个图像的输出维度为1,000（即隐藏层的一维输出）。此外，在`forward`方法中，我们将隐藏层的输出值通过批量归一化传递，然后通过ReLU激活。
- en: 'The variation in the training and validation datasets’ accuracy and loss values
    over increasing epochs is as follows:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练和验证数据集在增加的epochs中准确性和损失值的变化如下：
- en: '![](img/B18457_03_22.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_22.png)'
- en: 'Figure 3.22: Training and validation loss when network is trained with very
    small input values and batch normalization'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.22：当网络使用非常小的输入值和批量归一化进行训练时的训练和验证损失
- en: 'Here, we can see that the model was trained in a manner very similar to how
    it was trained when the input values did not have a very small range. Now, let’s
    understand the distribution of hidden layer values and the weight distribution,
    as seen in the previous section:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到该模型的训练方式与输入值范围不太小时的训练方式非常相似。现在，让我们了解隐藏层值的分布和权重分布，正如在前一节中看到的：
- en: '![Chart, histogram  Description automatically generated](img/B18457_03_23.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图 描述自动生成](img/B18457_03_23.png)'
- en: 'Figure 3.23: Distribution of weights and hidden layer node values when network
    is trained with very small input values and batch normalization'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.23：当网络使用非常小的输入值和批量归一化进行训练时，权重和隐藏层节点值的分布
- en: We can see that the hidden layer values have a wider spread when we have batch
    normalization and that the weights connecting the hidden layer to the output layer
    have a smaller distribution. This results in the model learning as effectively
    as it could in the previous sections.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当我们使用批量归一化时，隐藏层的值具有更广泛的分布，连接隐藏层与输出层的权重具有更小的分布。这导致模型学习效果与前面的部分一样有效。
- en: Batch normalization helps considerably when training deep neural networks. It
    helps us avoid gradients becoming so small that the weights are barely updated.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，批量归一化大大有助于我们。它帮助我们避免梯度变得太小，以至于权重几乎不会更新。
- en: 'So far, we have seen scenarios where training loss and accuracy are much better
    when compared to validation accuracy and loss: indicating a model that overly
    fits on training data but does not generalize well on validation datasets. We
    will look at fixing overfitting issues next.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了训练损失和准确率比验证准确率和损失要好得多的情况：这表明模型在训练数据上过拟合，但在验证数据集上泛化能力不强。接下来我们将看看如何解决过拟合问题。
- en: The concept of overfitting
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合的概念
- en: So far, we’ve seen that the accuracy of the training dataset is typically more
    than 95%, while the accuracy of the validation dataset is ~89%. Essentially, this
    indicates that a model does not generalize as much on unseen datasets, since it
    can learn from the training dataset. This also indicates that the model learns
    all the possible edge cases for the training dataset; these can’t be applied to
    the validation dataset.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到训练数据集的准确率通常超过95%，而验证数据集的准确率约为89%。基本上，这表明模型在未见数据上的泛化能力不强，因为它可以从训练数据集中学习。这还表明，模型学习了训练数据集的所有可能边界情况，这些情况不能应用于验证数据集。
- en: Having high accuracy on the training dataset and considerably lower accuracy
    on the validation dataset refers to the scenario of overfitting.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集上有很高的准确率，而在验证数据集上有相对较低的准确率，指的是过拟合的情况。
- en: Some of the typical strategies that are employed to reduce the effect of overfitting
    are dropout and regularization. We will look at what impact they have on training
    and validation losses in the following sections.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的策略用于减少过拟合的影响，包括**dropout**和正则化。接下来我们将看看它们对训练和验证损失的影响。
- en: Impact of adding dropout
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加dropout的影响
- en: We have already learned that whenever `loss.backward()` is calculated, a weight
    update happens. Typically, we would have hundreds of thousands of parameters within
    a network and thousands of data points to train our model on. This gives us the
    possibility that while most parameters help to train the model reasonably, certain
    parameters can be fine-tuned for the training images, resulting in their values
    being dictated by only a few images in the training dataset. This, in turn, results
    in the training data having a high accuracy but not necessarily on the validation
    dataset.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到，每当计算 `loss.backward()` 时，权重更新就会发生。通常情况下，我们会在网络内有数十万个参数，并且有数千个数据点用于训练我们的模型。这给我们可能性，即使大多数参数有助于合理地训练模型，某些参数可能会被微调以训练图像，导致它们的值仅由训练数据集中的少数图像决定。这反过来导致训练数据在高准确性上表现良好，但在验证数据集上并非必然如此。
- en: Dropout is a mechanism that randomly chooses a specified percentage of node
    activations and reduces them to 0\. In the next iteration, another random set
    of hidden units is switched off. This way, the neural network does not optimize
    for edge cases, as the network does not get that many opportunities to adjust
    the weight to memorize for edge cases (given that the weight is not updated in
    each iteration).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种机制，它随机选择指定百分比的节点激活并将其减少为0。在下一次迭代中，另一组随机的隐藏单元将被关闭。这样，神经网络不会针对边缘情况进行优化，因为网络在每次迭代中都没有那么多机会来调整权重以记住边缘情况（鉴于权重不是每次迭代都会更新）。
- en: Keep in mind that, during prediction, dropout doesn’t need to be applied, since
    this mechanism can only be applied while training a model.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在预测过程中，不需要应用dropout，因为这种机制只能在训练模型时应用。
- en: 'Usually, there are cases where the layers behave differently during training
    and validation, as you saw in the case of dropout. For this reason, you must specify
    the mode for the model upfront using one of two methods: `model.train()` to let
    the model know it is in training mode and `model.eval()` to let it know that it
    is in evaluation mode. If we don’t do this, we might get unexpected results. For
    example, in the following image, notice how the model (which contains dropout)
    gives us different predictions on the same input when in training mode.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练和验证过程中，层的行为可能会有所不同，就像在dropout的情况下所见。因此，您必须在模型前端使用以下两种方法之一指定模式：`model.train()`
    用于让模型知道它处于训练模式，以及 `model.eval()` 用于让模型知道它处于评估模式。如果我们不这样做，可能会得到意想不到的结果。例如，在下图中，请注意模型（其中包含dropout）在训练模式下对相同输入给出不同预测的情况。
- en: 'However, when the same model is in `eval` mode, it will suppress the dropout
    layer and return the same output:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当同一模型处于 `eval` 模式时，它将抑制dropout层并返回相同输出：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18457_03_24.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用 自动生成的描述](img/B18457_03_24.png)'
- en: 'Figure 3.24: Impact of model.eval() on output values'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24：model.eval()对输出值的影响
- en: 'While defining the architecture, `Dropout` is specified in the `get_model`
    function as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义架构时，在 `get_model` 函数中如下指定 `Dropout`：
- en: The full code can be found in the `Impact_of_dropout.ipynb` file in the `Chapter03`
    folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e). For brevity,
    we will not detail every step from the *Batch size of 32* section. **Refer to
    the notebooks on GitHub while executing the code.**
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在GitHub上的 `Impact_of_dropout.ipynb` 文件中的 `Chapter03` 文件夹中找到，网址为 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。为简洁起见，我们在执行代码时不会详细介绍每个步骤。**在执行代码时，请参考GitHub上的笔记本。**
- en: '[PRE50]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note that, in the preceding code, `Dropout` is specified before linear activation.
    This specifies that a fixed percentage of the weights in the linear activation
    layer won’t be updated.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述代码中，`Dropout` 在线性激活之前指定。这指定了线性激活层中一定比例的权重不会被更新。
- en: 'Once the model training is completed, as in the *Batch size of 32* section,
    the loss and accuracy values of the training and validation datasets will be as
    follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，就像 *Batch size of 32* 部分一样，训练集和验证集的损失和准确性值如下：
- en: '![Chart, line chart  Description automatically generated](img/B18457_03_25.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图 自动生成的描述](img/B18457_03_25.png)'
- en: 'Figure 3.25: Training and validation loss and accuracy with dropout'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25：使用dropout的训练和验证损失及准确度
- en: In this scenario, the delta between the training and validation datasets’ accuracy
    is not as large as we saw in the previous scenario, thus resulting in a scenario
    that has less overfitting.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，训练集和验证集的准确性之间的差异不像前一情景中那么大，因此导致了一个具有较少过拟合的情况。
- en: Impact of regularization
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化的影响
- en: Apart from the training accuracy being much higher than the validation accuracy,
    one other feature of overfitting is that certain weight values will be much higher
    than the other weight values. Large weight values can be a symptom of a model
    learning very well on training data (essentially, rote learning based on what
    it has seen).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练准确性远高于验证准确性之外，过拟合的另一个特征是某些权重值远高于其他权重值。大权重值可以是模型在训练数据上学习得非常好的迹象（基本上是根据所见的内容进行死记硬背学习）。
- en: While dropout is a mechanism that’s used so that the weight values aren’t updated
    as frequently, regularization is another mechanism that we can use for this purpose.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一种机制，用于使权重值的更新频率降低，而正则化是另一种我们可以用于此目的的机制。
- en: 'Regularization is a technique in which we penalize the model for having large
    weight values. Hence, it is an objective function that minimizes the loss of training
    data, as well as the weight values. In this section, we will learn about two types
    of regularization: L1 regularization and L2 regularization.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是一种技术，通过惩罚模型具有大权重值来实现。因此，它是一种目标函数，旨在最小化训练数据的损失以及权重值。在本节中，我们将学习两种类型的正则化：L1正则化和L2正则化。
- en: The full code can be found in the `Impact_of_regularization.ipynb` file in the
    `Chapter03` folder on GitHub at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    For brevity, we will not detail every step from the *Batch size of 32* section.
    **Refer to the notebooks on GitHub while executing the code.**
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在GitHub的`Chapter03`文件夹中的`Impact_of_regularization.ipynb`文件中找到，链接为[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。为了简洁起见，我们不会详细说明来自*批量大小为32*部分的每一步。**在执行代码时，请参考GitHub上的笔记本。**
- en: Let’s get started!
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: L1 regularization
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L1正则化
- en: 'L1 regularization is calculated as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: L1正则化计算如下：
- en: '![](img/B18457_03_006.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_006.png)'
- en: The first part of the preceding formula refers to the categorical cross-entropy
    loss that we have used for optimization so far, while the second part refers to
    the absolute sum of the weight values of the model. Note that L1 regularization
    ensures that it penalizes for the high absolute values of weights by incorporating
    them in the loss value calculation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 前面公式的第一部分是我们到目前为止用于优化的分类交叉熵损失，而第二部分是指模型权重值的绝对和。请注意，L1正则化通过将权重的绝对值的和纳入损失值计算中来确保对高绝对权重值进行惩罚。
- en: '![](img/B18457_03_007.png) refers to the weightage that we associate with the
    regularization (weight minimization) loss.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18457_03_007.png)指的是我们与正则化（权重最小化）损失相关联的权重。'
- en: 'L1 regularization is implemented while training the model, as follows:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时实施L1正则化，如下所示：
- en: '[PRE51]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: As you can see, we have enforced regularization on the weights and biases across
    all the layers by initializing `l1_regularization`.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们通过初始化`l1_regularization`在所有层上强制实施了权重和偏差的正则化。
- en: '`torch.norm(param,1)` provides the absolute value of the weight and bias values
    across layers. Furthermore, we have a very small weightage (`0.0001`) associated
    with the sum of the absolute value of the parameters across layers.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.norm(param,1)`提供了跨层权重和偏差值的绝对值。此外，我们还有一个与层间参数的绝对值的和相关联的非常小的权重（`0.0001`）。'
- en: 'Once we execute the remaining code, as in the *Batch size of 32* section, the
    training and validation datasets’ loss and accuracy values over increasing epochs
    will be as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们执行剩余代码，如*批量大小为32*部分所示，随着增加的epochs，训练和验证数据集的损失和准确率值将如下：
- en: '![Chart  Description automatically generated](img/B18457_03_26.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图表 自动生成的描述](img/B18457_03_26.png)'
- en: 'Figure 3.26: Training and validation loss and accuracy with L1 regularization'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26：使用L1正则化的训练和验证损失以及准确率
- en: As you can see, the difference between the training and validation datasets’
    accuracy is not as high as it was without L1 regularization.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，训练集和验证集的准确性之间的差异与没有L1正则化时相比并不高。
- en: L2 regularization
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: L2正则化
- en: 'L2 regularization is calculated as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: L2正则化计算如下：
- en: '![](img/B18457_03_008.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_03_008.png)'
- en: The first part of the preceding formula refers to the categorical cross-entropy
    loss obtained, while the second part refers to the squared sum of the weight values
    of the model. Similar to L1 regularization, we penalize for large weight values
    by having the sum of squared values of weights incorporated into the loss value
    calculation.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 前述公式的第一部分指的是获取的分类交叉熵损失，而第二部分指的是模型权重值的平方和。与 L1 正则化类似，通过将权重的平方和纳入损失值计算中，我们惩罚了较大的权重值。
- en: '![](img/B18457_03_007.png) refers to the weightage that we associate with the
    regularization (weight minimization) loss.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18457_03_007.png) 指的是我们与正则化（权重最小化）损失相关联的权重值。'
- en: 'L2 regularization is implemented while training the model, as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时实施 L2 正则化，具体如下：
- en: '[PRE52]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'In the preceding code, the regularization parameter, ![](img/B18457_03_007.png)
    (`0.01`), is slightly higher than in L1 regularization, since the weights are
    generally between -1 and 1, and squaring them would result in even smaller values.
    Multiplying them by an even smaller number, as we did in L1 regularization, would
    result in us having very little weightage for regularization in the overall loss
    calculation. Once we execute the remaining code, as in the *Batch size of 32*
    section, the training and validation datasets’ loss and accuracy values over increasing
    epochs will be as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，正则化参数，![](img/B18457_03_007.png) (`0.01`)，略高于 L1 正则化，因为权重通常在 -1 和 1 之间，平方后会变得更小。将它们乘以更小的数字，正如我们在
    L1 正则化中所做的那样，会导致我们在整体损失计算中对正则化的权重非常小。执行剩余代码后，就像 *批大小为 32* 部分一样，随着 epoch 的增加，训练和验证数据集的损失和准确率值如下：
- en: '![A graph of training and validation  Description automatically generated](img/B18457_03_27.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![训练和验证的图形化描述](img/B18457_03_27.png)'
- en: 'Figure 3.27: Training and validation loss and accuracy with L2 regularization'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27：使用 L2 正则化的训练和验证损失以及准确率
- en: We can see that L2 regularization has also resulted in the validation and training
    datasets’ accuracy and loss values being closer to each other.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，L2 正则化还导致了验证和训练数据集的准确率和损失值更加接近。
- en: Summary
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter by learning about how an image is represented. Then,
    we learned about how scaling, the value of the learning rate, our choice of optimizer,
    and the batch size help improve the accuracy and speed of training. We then learned
    about how batch normalization helps to increase the speed of training and addresses
    the issues of very small or large values in a hidden layer. Then, we learned about
    scheduling the learning rate to increase accuracy further. We then proceeded to
    understand the concept of overfitting and learned about how dropout and L1 and
    L2 regularization help us avoid overfitting.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从学习图像表示开始了解本章内容。然后，我们学习了如何通过缩放、学习率的值、优化器的选择以及批大小来提高训练的准确性和速度。接着，我们了解了批归一化如何提高训练速度并解决隐藏层中的非常小或非常大的值问题。然后，我们学习了如何通过调整学习率来进一步提高准确性。最后，我们深入了解了过拟合的概念，并学习了如何通过
    dropout 以及 L1 和 L2 正则化来避免过拟合。
- en: Now that we have learned about image classification using a deep neural network,
    as well as the various hyperparameters that help train a model, in the next chapter,
    we will learn about how what we’ve learned in this chapter can fail and how to
    address this, using convolutional neural networks.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了使用深度神经网络进行图像分类以及有助于训练模型的各种超参数，在下一章中，我们将学习本章所学内容的失败情况以及如何使用卷积神经网络解决这些问题。
- en: Questions
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What happens if the input values are not scaled in the input dataset?
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果输入数据集中的输入值未经过缩放，会发生什么？
- en: What could happen if the background has a white pixel color while the content
    has a black pixel color when training a neural network?
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练神经网络时，如果背景是白色像素而内容是黑色像素，可能会发生什么？
- en: What is the impact of batch size on a model’s training time and memory?
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批大小对模型训练时间和内存的影响是什么？
- en: What is the impact of the input value range have on weight distribution at the
    end of training?
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入值范围对训练结束时权重分布的影响是什么？
- en: How does batch normalization help improve accuracy?
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批归一化如何帮助提高准确率？
- en: Why do weights behave differently during training and evaluation in the dropout
    layer?
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么权重在 dropout 层的训练和评估过程中表现不同？
- en: How do we know if a model has overfitted on training data?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何知道模型是否在训练数据上过拟合了？
- en: How does regularization help in avoiding overfitting?
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化如何帮助避免过拟合？
- en: How do L1 and L2 regularization differ from each other?
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L1和L2正则化有何不同？
- en: How does dropout help in reducing overfitting?
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 丢弃法如何帮助减少过拟合？
- en: Learn more on Discord
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解更多关于Discord
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
