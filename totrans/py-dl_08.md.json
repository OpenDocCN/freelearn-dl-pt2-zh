["```py\n  state, action => expected reward\n```", "```py\ndef q(state, action, reward_func, apply_action_func, actions_for_state_func, max_actions_look_ahead, discount_factor=0.9):\n    new_state = apply_action_func(state, action)\n    if max_actions_look_ahead > 0:\n        return reward_func(new_state) + discount_factor \\ * max(q(new_state, new_action, reward_func, apply_action_func, actions_for_state_func, max_actions_look_ahead-1) \nfor new_action in actions_for_state_func(new_state))\n    else:\n        return reward_func(new_state)\n```", "```py\nrewards = [0, 0, 0, 0, 1, 0, 0, 0, 0]\n```", "```py\nimport tensorflow as tf\nimport numpy as np\n\nstates = [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\nNUM_STATES = len(states)\n```", "```py\nNUM_ACTIONS = 2\nDISCOUNT_FACTOR = 0.5\n\ndef one_hot_state(index):\n    array = np.zeros(NUM_STATES)\n    array[index] = 1.\n    return array\n```", "```py\nsession = tf.Session()\nstate = tf.placeholder(\"float\", [None, NUM_STATES])\ntargets = tf.placeholder(\"float\", [None, NUM_ACTIONS])\n```", "```py\nweights = tf.Variable(tf.constant(0., shape=[NUM_STATES, NUM_ACTIONS]))\noutput = tf.matmul(state, weights)\n```", "```py\nloss = tf.reduce_mean(tf.square(output - targets))\ntrain_operation = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\nsession.run(tf.initialize_all_variables())\n```", "```py\nfor _ in range(1000):\n    state_batch = []\n    rewards_batch = []\n\n    for state_index in range(NUM_STATES):\n        state_batch.append(one_hot_state(state_index))\n```", "```py\n         minus_action_index = (state_index - 1) % NUM_STATES\n         plus_action_index = (state_index + 1) % NUM_STATES\n```", "```py\n        minus_action_state_reward = session.run(output, feed_dict={state: [one_hot_state(minus_action_index)]})\n        plus_action_state_reward = session.run(output, feed_dict={state: [one_hot_state(plus_action_index)]})\n```", "```py\n        minus_action_q_value = states[minus_action_index] + DISCOUNT_FACTOR * np.max(minus_action_state_reward)\n\n        plus_action_q_value = states[plus_action_index] + DISCOUNT_FACTOR * np.max(plus_action_state_reward)]\n```", "```py\naction_rewards = [minus_action_q_value, plus_action_q_value]\nrewards_batch.append(action_rewards)\n```", "```py\nsession.run(train_operation, feed_dict={\n        state: state_batch,\n        targets: rewards_batch})\n\nprint([states[x] + np.max(session.run(output, feed_dict={state: [one_hot_state(x)]}))\n    for x in range(NUM_STATES)])\n```", "```py\n[0.0, 0.0, 0.0, 0.05, 1.0, 0.05, 0.0, 0.0, 0.0, 0.0]\n\n```", "```py\n[0.0, 0.0, 0.013, 0.172, 1.013, 0.172, 0.013, 0.0, 0.0, 0.0]\n\n```", "```py\n[0.053, 0.131, 0.295, 0.628, 1.295, 0.628, 0.295, 0.131, 0.053, 0.02]\n\n```", "```py\nterminal = [False, False, False, False, True, False, False, False, False, False]\n```", "```py\n        if terminal[minus_action_index]:\n            minus_action_q_value = DISCOUNT_FACTOR * states[minus_action_index]\n        else:\n            minus_action_state_reward = session.run(output, feed_dict={state: [one_hot_state(minus_action_index)]})\n            minus_action_q_value = DISCOUNT_FACTOR *(states[minus_action_index] + np.max(minus_action_state_reward))\n\n        if terminal[plus_action_index]:\n            plus_action_q_value = DISCOUNT_FACTOR * states[plus_action_index]\n        else:\n            plus_action_state_reward = session.run(output,\n            feed_dict={state: [one_hot_state(plus_action_index)]})\n            plus_action_q_value = DISCOUNT_FACTOR * (states[plus_action_index] + np.max(plus_action_state_reward))\n```", "```py\n[0.049, 0.111, 0.242, 0.497, 1.0, 0.497, 0.242, 0.111, 0.0469, 0.018]\n\n```", "```py\npip install gym[all]\n\n```", "```py\nimport gym\n\nenv = gym.make('CartPole-v0')\ncurrent_state = env.reset()\n\n```", "```py\nfeed_forward_weights_1 = tf.Variable(tf.truncated_normal([4,20], stddev=0.01))\nfeed_forward_bias_1 = tf.Variable(tf.constant(0.0, shape=[20]))\n\nfeed_forward_weights_2 = tf.Variable(tf.truncated_normal([20,2], stddev=0.01))\nfeed_forward_bias_2 = tf.Variable(tf.constant(0.0, shape=[2]))\n\ninput_placeholder = tf.placeholder(\"float\", [None, 4])\nhidden_layer = tf.nn.tanh(tf.matmul(input_placeholder, feed_forward_weights_1) + feed_forward_bias_1)\noutput_layer = tf.matmul(hidden_layer, feed_forward_weights_2) + feed_forward_bias_2 \n```", "```py\naction_placeholder = tf.placeholder(\"float\", [None, 2])\ntarget_placeholder = tf.placeholder(\"float\", [None])\n\nq_value_for_state_action = tf.reduce_sum(tf.mul(output_layer, action_placeholder),reduction_indices=1)\n```", "```py\ncost = tf.reduce_mean(tf.square(target_placeholder â€“ \n                        q_value_for_state_action))\ntrain_operation = tf.train.RMSPropOptimizer(0.001).minimize(cost)\n```", "```py\nfrom collections import deque\nobservations = deque(maxlen=20000)\nlast_action = np.zeros(2)\nlast_action[0] = 1\nlast_state = env.reset()\n```", "```py\nwhile True:\n    env.render()\n    last_action = choose_next_action(last_state)\n    current_state, reward, terminal, _ = env.step(np.argmax(last_action))\n```", "```py\n    if terminal:\n        reward = -1\n```", "```py\n    observations.append((last_state, last_action, reward, current_state, terminal))\n    if len(observations) > 10000:\n        train()\n```", "```py\n   if terminal:\n        last_state = env.reset()\n   else:\n        last_state = current_state\n```", "```py\ndef _train():\n    mini_batch = random.sample(observations, 100)\n```", "```py\n    previous_states = [d[0] for d in mini_batch]\n    actions = [d[1] for d in mini_batch]\n    rewards = [d[2] for d in mini_batch]\n    current_states = [d[3] for d in mini_batch]\n```", "```py\n    agents_reward_per_action = session.run(_output_layer, feed_dict={input_layer: current_states})\n```", "```py\n    agents_expected_reward = []\n    for i in range(len(mini_batch)):\n        if mini_batch[i][4]:\n            # this was a terminal frame so there is no future reward...\n            agents_expected_reward.append(rewards[i])\n        else:\n            agents_expected_reward.append(rewards[i] + FUTURE_REWARD_DISCOUNT * np.max(agents_reward_per_action[i]))\n```", "```py\n    session.run(_train_operation, feed_dict={\n        input_layer: previous_states,\n        action: actions,\n        target: agents_expected_reward})\n```", "```py\nif probability_of_random_action > 0.and len(_observations) > OBSERVATION_STEPS:\n probability_of_random_action -= 1\\. / 10000\n```", "```py\ndef choose_next_action():\n    if random.random() <= probability_of_random_action:\n        action_index = random.randrange(2)\n```", "```py\n    else:\n        readout_t = session.run(output_layer, feed_dict={input_layer: [last_state]})[0]\n        action_index = np.argmax(readout_t)\n    new_action = np.zeros([2])\nnew_action[action_index] = 1\nreturn new_action\n```", "```py\nfrom collections import deque\n\nimport random\nimport gym\nimport numpy as np\n\nenv = gym.make(\"Breakout-v0\")\nobservation = env.reset()\nreward_per_game = 0\nscores = dequeu(maxlen=1000)\n\nwhile True:\n    env.render()\n\n    next_action = random.randint(1, 3)\n    observation, reward, terminal, info = env.step(next_action)\n    reward_per_game += reward\n```", "```py\n    if terminal:\n        scores.append(reward_per_game)\n        reward_per_game = 0\n        print(np.mean(scores))\n        env.reset()\n\n```", "```py\ndef pre_process(screen_image):\n```", "```py\n    screen_image = screen_image[32:-10, 8:-8]\n```", "```py\n    screen_image = screen_image[::2, ::2, 0]\n```", "```py\n    screen_image[screen_image != 0] = 1\n```", "```py\n    return screen_image.astype(np.float)\n```", "```py\nSCREEN_HEIGHT = 84\nSCREEN_WIDTH = 74\nSTATE_FRAMES = 2\n\nCONVOLUTIONS_LAYER_1 = 32\nCONVOLUTIONS_LAYER_2 = 64\nCONVOLUTIONS_LAYER_3 = 64\nFLAT_HIDDEN_NODES = 512\n```", "```py\ndef create_network():\n    input_layer = tf.placeholder(\"float\", [None, SCREEN_HEIGHT, SCREEN_WIDTH, STATE_FRAMES])\n```", "```py\nconvolution_weights_1 = tf.Variable(tf.truncated_normal([8, 8, STATE_FRAMES, CONVOLUTIONS_LAYER_1], stddev=0.01))\n    convolution_bias_1 = tf.Variable(tf.constant(0.01, shape=[CONVOLUTIONS_LAYER_1]))\n```", "```py\n    hidden_convolutional_layer_1 = tf.nn.relu(\n        tf.nn.conv2d(input_layer, convolution_weights_1,         strides=[1, 4, 4, 1], padding=\"SAME\") + convolution_bias_1)\n```", "```py\n    convolution_weights_2 = tf.Variable(tf.truncated_normal([4, 4, CONVOLUTIONS_LAYER_1, CONVOLUTIONS_LAYER_2], stddev=0.01))\n    convolution_bias_2 = tf.Variable(tf.constant(0.01, shape=[CONVOLUTIONS_LAYER_2]))\n\n    hidden_convolutional_layer_2 = tf.nn.relu(\n        tf.nn.conv2d(hidden_convolutional_layer_1, convolution_weights_2, strides=[1, 2, 2, 1], padding=\"SAME\") + convolution_bias_2)\n    convolution_weights_3 = tf.Variable(tf.truncated_normal([3, 3, CONVOLUTIONS_LAYER_2, CONVOLUTIONS_LAYER_3], stddev=0.01))\n    convolution_bias_3 = tf.Variable(tf.constant(0.01, shape=[CONVOLUTIONS_LAYER_2]))\n    hidden_convolutional_layer_3 = tf.nn.relu(\n        tf.nn.conv2d(hidden_convolutional_layer_2, convolution_weights_3, strides=[1, 1, 1, 1], padding=\"SAME\") + convolution_bias_3)\n```", "```py\n    hidden_convolutional_layer_3_flat = tf.reshape(hidden_convolutional_layer_3, [-1, 9*11*CONVOLUTIONAL_LAYER_3])\n```", "```py\n    feed_forward_weights_1 = tf.Variable(tf.truncated_normal([FLAT_SIZE, FLAT_HIDDEN_NODES], stddev=0.01))\n    feed_forward_bias_1 = tf.Variable(tf.constant(0.01, shape=[FLAT_HIDDEN_NODES]))\n\n    final_hidden_activations = tf.nn.relu(\n        tf.matmul(hidden_convolutional_layer_3_flat, feed_forward_weights_1) + feed_forward_bias_1)\n\n    feed_forward_weights_2 = tf.Variable(tf.truncated_normal([FLAT_HIDDEN_NODES, ACTIONS_COUNT], stddev=0.01))\n    feed_forward_bias_2 = tf.Variable(tf.constant(0.01, shape=[ACTIONS_COUNT]))\n\n    output_layer = tf.matmul(final_hidden_activations, feed_forward_weights_2) + feed_forward_bias_2\n\n    return input_layer, output_layer\n```", "```py\nscreen_binary = preprocess(observation)\n\nif last_state is None:\nlast_state = np.stack(tuple(screen_binary for _ in range(STATE_FRAMES)), axis=2)\n```", "```py\nelse:\n screen_binary = np.reshape(screen_binary, (SCREEN_HEIGHT, SCREEN_WIDTH, 1))\n current_state = np.append(last_state[:, :, 1:], screen_binary, axis=2)\n```", "```py\nlast_state = current_state\n```", "```py\nimport zlib\nimport pickle\n\nobservations.append(zlib.compress(\npickle.dumps((last_state, last_action, reward, current_state, terminal), 2), 2))\n```", "```py\nmini_batch_compressed = random.sample(_observations, MINI_BATCH_SIZE)\nmini_batch = [pickle.loads(zlib.decompress(comp_item)) for comp_item in mini_batch_compressed]\n```", "```py\nCHECKPOINT_PATH = \"breakout\"\n\nsaver = tf.train.Saver()\n\nif not os.path.exists(CHECKPOINT_PATH):\n    os.mkdir(CHECKPOINT_PATH)\n\ncheckpoint = tf.train.get_checkpoint_state(CHECKPOINT_PATH)\nif checkpoint:\n    saver.restore(session, checkpoint.model_checkpoint_path)\n```", "```py\nsaver.save(_session, CHECKPOINT_PATH + '/network')\n```"]