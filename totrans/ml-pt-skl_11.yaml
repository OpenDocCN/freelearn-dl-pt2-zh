- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a Multilayer Artificial Neural Network from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may know, deep learning is getting a lot of attention from the press
    and is, without doubt, the hottest topic in the machine learning field. Deep learning
    can be understood as a subfield of machine learning that is concerned with training
    artificial **neural networks** (**NNs**) with many layers efficiently. In this
    chapter, you will learn the basic concepts of artificial NNs so that you are well
    equipped for the following chapters, which will introduce advanced Python-based
    deep learning libraries and **deep neural network** (**DNN**) architectures that
    are particularly well suited for image and text analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that we will cover in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaining a conceptual understanding of multilayer NNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the fundamental backpropagation algorithm for NN training from
    scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a basic multilayer NN for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling complex functions with artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this book, we started our journey through machine learning
    algorithms with artificial neurons in *Chapter 2*, *Training Simple Machine Learning
    Algorithms for Classification*. Artificial neurons represent the building blocks
    of the multilayer artificial NNs that we will discuss in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept behind artificial NNs was built upon hypotheses and models
    of how the human brain works to solve complex problem tasks. Although artificial
    NNs have gained a lot of popularity in recent years, early studies of NNs go back
    to the 1940s, when Warren McCulloch and Walter Pitts first described how neurons
    could work. (*A logical calculus of the ideas immanent in nervous activity*, by
    *W. S. McCulloch* and *W. Pitts*, *The Bulletin of Mathematical Biophysics*, 5(4):115–133,
    1943.)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in the decades that followed the first implementation of the **McCulloch-Pitts
    neuron** model—Rosenblatt’s perceptron in the 1950s—many researchers and machine
    learning practitioners slowly began to lose interest in NNs since no one had a
    good solution for training an NN with multiple layers. Eventually, interest in
    NNs was rekindled in 1986 when D.E. Rumelhart, G.E. Hinton, and R.J. Williams
    were involved in the (re)discovery and popularization of the backpropagation algorithm
    to train NNs more efficiently, which we will discuss in more detail later in this
    chapter (*Learning representations by backpropagating errors*, by *D.E. Rumelhart*,
    *G.E. Hinton*, and *R.J. Williams*, *Nature*, 323 (6088): 533–536, 1986). Readers
    who are interested in the history of **artificial intelligence** (**AI**), machine
    learning, and NNs are also encouraged to read the Wikipedia article on the so-called
    *AI winters*, which are the periods of time where a large portion of the research
    community lost interest in the study of NNs ([https://en.wikipedia.org/wiki/AI_winter](https://en.wikipedia.org/wiki/AI_winter)).'
  prefs: []
  type: TYPE_NORMAL
- en: However, NNs are more popular today than ever thanks to the many breakthroughs
    that have been made in the previous decade, which resulted in what we now call
    deep learning algorithms and architectures—NNs that are composed of many layers.
    NNs are a hot topic not only in academic research but also in big technology companies,
    such as Facebook, Microsoft, Amazon, Uber, Google, and many more that invest heavily
    in artificial NNs and deep learning research.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of today, complex NNs powered by deep learning algorithms are considered
    state-of-the-art solutions for complex problem solving such as image and voice
    recognition. Some of the recent applications include:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting COVID-19 resource needs from a series of X-rays ([https://arxiv.org/abs/2101.04909](https://arxiv.org/abs/2101.04909))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling virus mutations ([https://science.sciencemag.org/content/371/6526/284](https://science.sciencemag.org/content/371/6526/284))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging data from social media platforms to manage extreme weather events
    ([https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-5973.12311](https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-5973.12311))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving photo descriptions for people who are blind or visually impaired ([https://tech.fb.com/how-facebook-is-using-ai-to-improve-photo-descriptions-for-people-who-are-blind-or-visually-impaired/](https://tech.fb.com/how-facebook-is-using-ai-to-improve-photo-descriptions-for-people-who-are-blind-or-visually-impaired/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-layer neural network recap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter is all about multilayer NNs, how they work, and how to train them
    to solve complex problems. However, before we dig deeper into a particular multilayer
    NN architecture, let’s briefly reiterate some of the concepts of single-layer
    NNs that we introduced in *Chapter 2*, namely, the **ADAptive LInear NEuron**
    (**Adaline**) algorithm, which is shown in *Figure 11.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The Adaline algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Chapter 2*, we implemented the Adaline algorithm to perform binary classification,
    and we used the gradient descent optimization algorithm to learn the weight coefficients
    of the model. In every epoch (pass over the training dataset), we updated the
    weight vector **w** and bias unit *b* using the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_001.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B17582_11_002.png) and ![](img/B17582_11_003.png) for the bias
    unit and each weight *w*[j] in the weight vector **w**.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we computed the gradient based on the whole training dataset
    and updated the weights of the model by taking a step in the opposite direction
    of the loss gradient ![](img/B17582_11_004.png). (For simplicity, we will focus
    on the weights and omit the bias unit in the following paragraphs; however, as
    you remember from *Chapter 2*, the same concepts apply.) In order to find the
    optimal weights of the model, we optimized an objective function that we defined
    as the **mean of squared errors** (**MSE**) loss function *L*(**w**). Furthermore,
    we multiplied the gradient by a factor, the **learning rate** ![](img/B17582_02_036.png),
    which we had to choose carefully to balance the speed of learning against the
    risk of overshooting the global minimum of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In gradient descent optimization, we updated all weights simultaneously after
    each epoch, and we defined the partial derivative for each weight *w*[j] in the
    weight vector, **w**, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y*^(^i^) is the target class label of a particular sample *x*^(^i^),
    and *a*^(^i^) is the activation of the neuron, which is a linear function in the
    special case of Adaline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we defined the activation function ![](img/B17582_11_007.png)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the net input, *z*, is a linear combination of the weights that are connecting
    the input layer to the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While we used the activation ![](img/B17582_11_007.png) to compute the gradient
    update, we implemented a threshold function to squash the continuous-valued output
    into binary class labels for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_011.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Single-layer naming convention**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that although Adaline consists of two layers, one input layer and one output
    layer, it is called a single-layer network because of its single link between
    the input and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we learned about a certain *trick* to accelerate the model learning, the
    so-called **stochastic gradient descent** (**SGD**) optimization. SGD approximates
    the loss from a single training sample (online learning) or a small subset of
    training examples (mini-batch learning). We will make use of this concept later
    in this chapter when we implement and train a **multilayer perceptron** (**MLP**).
    Apart from faster learning—due to the more frequent weight updates compared to
    gradient descent—its noisy nature is also regarded as beneficial when training
    multilayer NNs with nonlinear activation functions, which do not have a convex
    loss function. Here, the added noise can help to escape local loss minima, but
    we will discuss this topic in more detail later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the multilayer neural network architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn how to connect multiple single neurons to a
    multilayer feedforward NN; this special type of *fully connected* network is also
    called **MLP**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11.2* illustrates the concept of an MLP consisting of two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, engineering drawing  Description automatically generated](img/B17582_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: A two-layer MLP'
  prefs: []
  type: TYPE_NORMAL
- en: Next to the data input, the MLP depicted in *Figure 11.2* has one hidden layer
    and one output layer. The units in the hidden layer are fully connected to the
    input features, and the output layer is fully connected to the hidden layer. If
    such a network has more than one hidden layer, we also call it a **deep NN**.
    (Note that in some contexts, the inputs are also regarded as a layer. However,
    in this case, it would make the Adaline model, which is a single-layer neural
    network, a two-layer neural network, which may be counterintuitive.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding additional hidden layers**'
  prefs: []
  type: TYPE_NORMAL
- en: We can add any number of hidden layers to the MLP to create deeper network architectures.
    Practically, we can think of the number of layers and units in an NN as additional
    hyperparameters that we want to optimize for a given problem task using the cross-validation
    technique, which we discussed in *Chapter 6*, *Learning Best Practices for Model
    Evaluation and Hyperparameter Tuning*.
  prefs: []
  type: TYPE_NORMAL
- en: However, the loss gradients for updating the network’s parameters, which we
    will calculate later via backpropagation, will become increasingly small as more
    layers are added to a network. This vanishing gradient problem makes model learning
    more challenging. Therefore, special algorithms have been developed to help train
    such DNN structures; this is known as **deep learning**, which we will discuss
    in more detail in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 11.2*, we denote the *i*th activation unit in the *l*th
    layer as ![](img/B17582_11_012.png). To make the math and code implementations
    a bit more intuitive, we will not use numerical indices to refer to layers, but
    we will use the *in* superscript for the input features, the *h* superscript for
    the hidden layer, and the *out* superscript for the output layer. For instance,
    ![](img/B17582_11_013.png) refers to the *i*th input feature value, ![](img/B17582_11_014.png)
    refers to the *i*th unit in the hidden layer, and ![](img/B17582_11_015.png) refers
    to the *i*th unit in the output layer. Note that the **b**’s in *Figure 11.2*
    denote the bias units. In fact, **b**^(^h^) and **b**^(^(out)^) are vectors with
    the number of elements being equal to the number of nodes in the layer they correspond
    to. For example, **b**^(^h^) stores *d* bias units, where *d* is the number of
    nodes in the hidden layer. If this sounds confusing, don’t worry. Looking at the
    code implementation later, where we initialize weight matrices and bias unit vectors,
    will help clarify these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Each node in layer *l* is connected to all nodes in layer *l* + 1 via a weight
    coefficient. For example, the connection between the *k*th unit in layer *l* to
    the *j*th unit in layer *l* + 1 will be written as ![](img/B17582_11_016.png).
    Referring back to *Figure 11.2*, we denote the weight matrix that connects the
    input to the hidden layer as **W**^(^h^), and we write the matrix that connects
    the hidden layer to the output layer as **W**^(^(out)^).
  prefs: []
  type: TYPE_NORMAL
- en: While one unit in the output layer would suffice for a binary classification
    task, we saw a more general form of an NN in the preceding figure, which allows
    us to perform multiclass classification via a generalization of the **one-versus-all**
    (**OvA**) technique. To better understand how this works, remember the **one-hot**
    representation of categorical variables that we introduced in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can encode the three class labels in the familiar Iris dataset
    (0=*Setosa*, 1=*Versicolor*, 2=*Virginica*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_017.png)'
  prefs: []
  type: TYPE_IMG
- en: This one-hot vector representation allows us to tackle classification tasks
    with an arbitrary number of unique class labels present in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to NN representations, the indexing notation (subscripts and
    superscripts) may look a little bit confusing at first. What may seem overly complicated
    at first will make much more sense in later sections when we vectorize the NN
    representation. As introduced earlier, we summarize the weights that connect the
    input and hidden layers by a *d*×*m* dimensional matrix **W**^(^h^), where *d*
    is the number of hidden units and *m* is the number of input units.
  prefs: []
  type: TYPE_NORMAL
- en: Activating a neural network via forward propagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will describe the process of **forward propagation** to
    calculate the output of an MLP model. To understand how it fits into the context
    of learning an MLP model, let’s summarize the MLP learning procedure in three
    simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Starting at the input layer, we forward propagate the patterns of the training
    data through the network to generate an output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Based on the network’s output, we calculate the loss that we want to minimize
    using a loss function that we will describe later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We backpropagate the loss, find its derivative with respect to each weight and
    bias unit in the network, and update the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, after we repeat these three steps for multiple epochs and learn the
    weight and bias parameters of the MLP, we use forward propagation to calculate
    the network output and apply a threshold function to obtain the predicted class
    labels in the one-hot representation, which we described in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s walk through the individual steps of forward propagation to generate
    an output from the patterns in the training data. Since each unit in the hidden
    layer is connected to all units in the input layers, we first calculate the activation
    unit of the hidden layer ![](img/B17582_11_018.png) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17582_11_020.png) is the net input and ![](img/B17582_11_007.png)
    is the activation function, which has to be differentiable to learn the weights
    that connect the neurons using a gradient-based approach. To be able to solve
    complex problems such as image classification, we need nonlinear activation functions
    in our MLP model, for example, the sigmoid (logistic) activation function that
    we remember from the section about logistic regression in *Chapter 3*, *A Tour
    of Machine Learning Classifiers Using Scikit-Learn*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As you may recall, the sigmoid function is an *S*-shaped curve that maps the
    net input *z* onto a logistic distribution in the range 0 to 1, which cuts the
    *y* axis at *z* = 0, as shown in *Figure 11.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with low confidence](img/B17582_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: The sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: MLP is a typical example of a feedforward artificial NN. The term **feedforward**
    refers to the fact that each layer serves as the input to the next layer without
    loops, in contrast to recurrent NNs—an architecture that we will discuss later
    in this chapter and discuss in more detail in *Chapter 15*, *Modeling Sequential
    Data Using Recurrent Neural Networks*. The term *multilayer perceptron* may sound
    a little bit confusing since the artificial neurons in this network architecture
    are typically sigmoid units, not perceptrons. We can think of the neurons in the
    MLP as logistic regression units that return values in the continuous range between
    0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'For purposes of code efficiency and readability, we will now write the activation
    in a more compact form using the concepts of basic linear algebra, which will
    allow us to vectorize our code implementation via NumPy rather than writing multiple
    nested and computationally expensive Python `for` loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_023.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **z**^(^h^) is our 1×*m* dimensional feature vector. **W**^(^h^) is a
    *d*×*m* dimensional weight matrix where *d* is the number of units in the hidden
    layer; consequently, the transposed matrix **W**^(^h^)^T is *m*×*d* dimensional.
    The bias vector **b**^(^h^) consists of *d* bias units (one bias unit per hidden
    node).
  prefs: []
  type: TYPE_NORMAL
- en: After matrix-vector multiplication, we obtain the 1×*d* dimensional net input
    vector **z**^(^h^) to calculate the activation **a**^(^h^) (where ![](img/B17582_11_024.png)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we can generalize this computation to all *n* examples in the
    training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Z**^(^h^) = **X**^(^(in)^)**W**^(^h^)^T + **b**^(^h^)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, **X**^(^(in)^) is now an *n*×*m* matrix, and the matrix multiplication
    will result in an *n*×*d* dimensional net input matrix, **Z**^(^h^). Finally,
    we apply the activation function ![](img/B17582_11_007.png) to each value in the
    net input matrix to get the *n*×*d* activation matrix in the next layer (here,
    the output layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can write the activation of the output layer in vectorized form
    for multiple examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Z**^(^(out)^) = **A**^(^h^)**W**^(^(out)^)^T + **b**^(^(out)^)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we multiply the transpose of the *t*×*d* matrix **W**^(^(out)^) (*t* is
    the number of output units) by the *n*×*d* dimensional matrix, **A**^(^h^), and
    add the *t* dimensional bias vector **b**^(^(out)^) to obtain the *n*×*t* dimensional
    matrix, **Z**^(^(out)^). (The columns in this matrix represent the outputs for
    each sample.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we apply the sigmoid activation function to obtain the continuous-valued
    output of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_027.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to **Z**^(^(out)^), **A**^(^(out)^) is an *n*×*t* dimensional matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we covered a lot of the theory around NNs, which can
    be a little bit overwhelming if you are new to this topic. Before we continue
    with the discussion of the algorithm for learning the weights of the MLP model,
    backpropagation, let’s take a short break from the theory and see an NN in action.
  prefs: []
  type: TYPE_NORMAL
- en: '**Additional resources on backpropagation**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The NN theory can be quite complex; thus, we want to provide readers with additional
    resources that cover some of the topics we discuss in this chapter in more detail
    or from a different perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 6*, *Deep Feedforward Networks*, *Deep Learning*, by *I. Goodfellow*,
    *Y. Bengio*, and *A. Courville*, MIT Press, 2016 (manuscripts freely accessible
    at [http://www.deeplearningbook.org](http://www.deeplearningbook.org)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pattern Recognition and Machine Learning*, by *C. M. Bishop*, Springer New
    York, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lecture video slides from Sebastian Raschka’s deep learning course:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression](https://sebastianraschka.com/blog/2021/dl-course.html#l08-multinomial-logistic-regression--softmax-regression)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://sebastianraschka.com/blog/2021/dl-course.html#l09-multilayer-perceptrons-and-backpropration](https://sebastianraschka.com/blog/2021/dl-course.html#l09-multilayer-perceptrons-and-backpropration)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this section, we will implement and train our first multilayer NN to classify
    handwritten digits from the popular **Mixed National Institute of Standards and
    Technology** (**MNIST**) dataset that has been constructed by Yann LeCun and others
    and serves as a popular benchmark dataset for machine learning algorithms (*Gradient-Based
    Learning Applied to Document Recognition* by *Y. LeCun*, *L. Bottou*, *Y. Bengio*,
    and *P. Haffner*, *Proceedings of the IEEE*, 86(11): 2278-2324, 1998).'
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining and preparing the MNIST dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The MNIST dataset is publicly available at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
    and consists of the following four parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dataset images**: `train-images-idx3-ubyte.gz` (9.9 MB, 47 MB unzipped,
    and 60,000 examples)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training dataset labels**: `train-labels-idx1-ubyte.gz` (29 KB, 60 KB unzipped,
    and 60,000 labels)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test dataset images**: `t10k-images-idx3-ubyte.gz` (1.6 MB, 7.8 MB unzipped,
    and 10,000 examples)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test dataset labels**: `t10k-labels-idx1-ubyte.gz` (5 KB, 10 KB unzipped,
    and 10,000 labels)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The MNIST dataset was constructed from two datasets of the US **National Institute
    of Standards and Technology** (**NIST**). The training dataset consists of handwritten
    digits from 250 different people, 50 percent high school students, and 50 percent
    employees from the Census Bureau. Note that the test dataset contains handwritten
    digits from different people following the same split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of downloading the abovementioned dataset files and preprocessing them
    into NumPy arrays ourselves, we will use scikit-learn’s new `fetch_openml` function,
    which allows us to load the MNIST dataset more conveniently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In scikit-learn, the `fetch_openml` function downloads the MNIST dataset from
    OpenML ([https://www.openml.org/d/554](https://www.openml.org/d/554)) as pandas
    `DataFrame` and Series objects, which is why we use the `.values` attribute to
    obtain the underlying NumPy arrays. (If you are using a scikit-learn version older
    than 1.0, `fetch_openml` downloads NumPy arrays directly so you can omit using
    the `.values` attribute.) The *n*×*m* dimensional `X` array consists of 70,000
    images with 784 pixels each, and the `y` array stores the corresponding 70,000
    class labels, which we can confirm by checking the dimensions of the arrays as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The images in the MNIST dataset consist of 28×28 pixels, and each pixel is represented
    by a grayscale intensity value. Here, `fetch_openml` already unrolled the 28×28 pixels
    into one-dimensional row vectors, which represent the rows in our `X` array (784
    per row or image) above. The second array (`y`) returned by the `fetch_openml`
    function contains the corresponding target variable, the class labels (integers
    0-9) of the handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s normalize the pixels values in MNIST to the range –1 to 1 (originally
    0 to 255) via the following code line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The reason behind this is that gradient-based optimization is much more stable
    under these conditions, as discussed in *Chapter 2*. Note that we scaled the images
    on a pixel-by-pixel basis, which is different from the feature-scaling approach
    that we took in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we derived scaling parameters from the training dataset and used
    these to scale each column in the training dataset and test dataset. However,
    when working with image pixels, centering them at zero and rescaling them to a
    [–1, 1] range is also common and usually works well in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an idea of how those images in MNIST look, let’s visualize examples
    of the digits 0-9 after reshaping the 784-pixel vectors from our feature matrix
    into the original 28×28 image that we can plot via Matplotlib’s `imshow` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now see a plot of the 2×5 subfigures showing a representative image
    of each unique digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: A plot showing one randomly chosen handwritten digit from each
    class'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, let’s also plot multiple examples of the same digit to see how
    different the handwriting for each really is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the code, we should now see the first 25 variants of the digit
    7:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing calendar  Description automatically generated](img/B17582_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Different variants of the handwritten digit 7'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s divide the dataset into training, validation, and test subsets.
    The following code will split the dataset such that 55,000 images are used for
    training, 5,000 images for validation, and 10,000 images for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Implementing a multilayer perceptron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this subsection, we will now implement an MLP from scratch to classify the
    images in the MNIST dataset. To keep things simple, we will implement an MLP with
    only one hidden layer. Since the approach may seem a little bit complicated at
    first, you are encouraged to download the sample code for this chapter from the
    Packt Publishing website or from GitHub ([https://github.com/rasbt/machine-learning-book](https://github.com/rasbt/machine-learning-book))
    so that you can view this MLP implementation annotated with comments and syntax
    highlighting for better readability.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not running the code from the accompanying Jupyter Notebook file
    or don’t have access to the internet, copy the `NeuralNetMLP` code from this chapter
    into a Python script file in your current working directory (for example, `neuralnet.py)`,
    which you can then import into your current Python session via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The code will contain parts that we have not talked about yet, such as the backpropagation
    algorithm. Do not worry if not all the code makes immediate sense to you; we will
    follow up on certain parts later in this chapter. However, going over the code
    at this stage can make it easier to follow the theory later.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s look at the following implementation of an MLP, starting with the
    two helper functions to compute the logistic sigmoid activation and to convert
    integer class label arrays to one-hot encoded labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Below, we implement the main class for our MLP, which we call `NeuralNetMLP`.
    There are three class methods, `.__init__()`, `.forward()`, and `.backward()`,
    that we will discuss one by one, starting with the `__init__` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The `__init__` constructor instantiates the weight matrices and bias vectors
    for the hidden and the output layer. Next, let’s see how these are used in the
    `forward` method to make predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `forward` method takes in one or more training examples and returns the
    predictions. In fact, it returns both the activation values from the hidden layer
    and the output layer, `a_h` and `a_out`. While `a_out` represents the class-membership
    probabilities that we can convert to class labels, which we care about, we also
    need the activation values from the hidden layer, `a_h`, to optimize the model
    parameters; that is, the weight and bias units of the hidden and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s talk about the `backward` method, which updates the weight and
    bias parameters of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `backward` method implements the so-called *backpropagation* algorithm,
    which calculates the gradients of the loss with respect to the weight and bias
    parameters. Similar to Adaline, these gradients are then used to update these
    parameters via gradient descent. Note that multilayer NNs are more complex than
    their single-layer siblings, and we will go over the mathematical concepts of
    how to compute the gradients in a later section after discussing the code. For
    now, just consider the `backward` method as a way for computing gradients that
    are used for the gradient descent updates. For simplicity, the loss function this
    derivation is based on is the same MSE loss that we used in Adaline. In later
    chapters, we will look at alternative loss functions, such as multi-category cross-entropy
    loss, which is a generalization of the binary logistic regression loss to multiple
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this code implementation of the `NeuralNetMLP` class, you may have
    noticed that this object-oriented implementation differs from the familiar scikit-learn
    API that is centered around the `.fit()` and `.predict()` methods. Instead, the
    main methods of the `NeuralNetMLP` class are the `.forward()` and `.backward()`
    methods. One of the reasons behind this is that it makes a complex neural network
    a bit easier to understand in terms of how the information flows through the networks.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason is that this implementation is relatively similar to how more
    advanced deep learning libraries such as PyTorch operate, which we will introduce
    and use in the upcoming chapters to implement more complex neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have implemented the `NeuralNetMLP` class, we use the following code
    to instantiate a new `NeuralNetMLP` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `model` accepts MNIST images reshaped into 784-dimensional vectors (in the
    format of `X_train`, `X_valid`, or `X_test`, which we defined previously) for
    the 10 integer classes (digits 0-9). The hidden layer consists of 50 nodes. Also,
    as you may be able to tell from looking at the previously defined `.forward()`
    method, we use a sigmoid activation function after the first hidden layer and
    output layer to keep things simple. In later chapters, we will learn about alternative
    activation functions for both the hidden and output layers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11.6* summarizes the neural network architecture that we instantiated
    above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: The NN architecture for labeling handwritten digits'
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we are going to implement the training function that
    we can use to train the network on mini-batches of the data via backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Coding the neural network training loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have implemented the `NeuralNetMLP` class in the previous subsection
    and initiated a model, the next step is to train the model. We will tackle this
    in multiple steps. First, we will define some helper functions for data loading.
    Second, we will embed these functions into the training loop that iterates over
    the dataset in multiple epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function we are going to define is a mini-batch generator, which
    takes in our dataset and divides it into mini-batches of a desired size for stochastic
    gradient descent training. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we move on to the next functions, let’s confirm that the mini-batch
    generator works as intended and produces mini-batches of the desired size. The
    following code will attempt to iterate through the dataset, and then we will print
    the dimension of the mini-batches. Note that in the following code examples, we
    will remove the `break` statements. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the network returns mini-batches of size 100 as intended.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have to define our loss function and performance metric that we can
    use to monitor the training process and evaluate the model. The MSE loss and accuracy
    function can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s test the preceding function and compute the initial validation set MSE
    and accuracy of the model we instantiated in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this code example, note that `model.forward()` returns the hidden and output
    layer activations. Remember that we have 10 output nodes (one corresponding to
    each unique class label). Hence, when computing the MSE, we first converted the
    class labels into one-hot encoded class labels in the `mse_loss()` function. In
    practice, it does not make a difference whether we average over the row or the
    columns of the squared-difference matrix first, so we simply call `np.mean()`
    without any axis specification so that it returns a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: The output layer activations, since we used the logistic sigmoid function, are
    values in the range [0, 1]. For each input, the output layer produces 10 values
    in the range [0, 1], so we used the `np.argmax()` function to select the index
    position of the largest value, which yields the predicted class label. We then
    compared the true labels with the predicted class labels to compute the accuracy
    via the `accuracy()` function we defined. As we can see from the preceding output,
    the accuracy is not very high. However, given that we have a balanced dataset
    with 10 classes, a prediction accuracy of approximately 10 percent is what we
    would expect for an untrained model producing random predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the previous code, we can compute the performance on, for example, the
    whole training set if we provide `y_train` as input to targets and the predicted
    labels from feeding the model with `X_train`. However, in practice, our computer
    memory is usually a limiting factor for how much data the model can ingest in
    one forward pass (due to the large matrix multiplications). Hence, we are defining
    our MSE and accuracy computation based on our previous mini-batch generator. The
    following function will compute the MSE and accuracy incrementally by iterating
    over the dataset one mini-batch at a time to be more memory-efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we implement the training loop, let’s test the function and compute
    the initial training set MSE and accuracy of the model we instantiated in the
    previous section and make sure it works as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from the results, our generator approach produces the same results
    as the previously defined MSE and accuracy functions, except for a small rounding
    error in the MSE (0.27 versus 0.28), which is negligible for our purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now get to the main part and implement the code to train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'On a high level, the `train()` function iterates over multiple epochs, and
    in each epoch, it used the previously defined `minibatch_generator()` function
    to iterate over the whole training set in mini-batches for stochastic gradient
    descent training. Inside the mini-batch generator `for` loop, we obtain the outputs
    from the model, `a_h` and `a_out`, via its `.forward()` method. Then, we compute
    the loss gradients via the model’s `.backward()` method—the theory will be explained
    in a later section. Using the loss gradients, we update the weights by adding
    the negative gradient multiplied by the learning rate. This is the same concept
    that we discussed earlier for Adaline. For example, to update the model weights
    of the hidden layer, we defined the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For a single weight, *w*[j], this corresponds to the following partial derivative-based
    update:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_028.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, the last portion of the previous code computes the losses and prediction
    accuracies on the training and test sets to track the training progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now execute this function to train our model for 50 epochs, which may
    take a few minutes to finish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'During training, we should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The reason why we print all this output is that, in NN training, it is really
    useful to compare training and validation accuracy. This helps us judge whether
    the network model performs well, given the architecture and hyperparameters. For
    example, if we observe a low training and validation accuracy, there is likely
    an issue with the training dataset, or the hyperparameters’ settings are not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: In general, training (deep) NNs is relatively expensive compared with the other
    models we’ve discussed so far. Thus, we want to stop it early in certain circumstances
    and start over with different hyperparameter settings. On the other hand, if we
    find that it increasingly tends to overfit the training data (noticeable by an
    increasing gap between training and validation dataset performance), we may want
    to stop the training early, as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next subsection, we will discuss the performance of our NN model in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the neural network performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we discuss backpropagation, the training procedure of NNs, in more detail
    in the next section, let’s look at the performance of the model that we trained
    in the previous subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `train()`, we collected the training loss and the training and validation
    accuracy for each epoch so that we can visualize the results using Matplotlib.
    Let’s look at the training MSE loss first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code plots the loss over the 50 epochs, as shown in *Figure 11.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shape, square  Description automatically generated](img/B17582_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: A plot of the MSE by the number of training epochs'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the loss decreased substantially during the first 10 epochs and
    seems to slowly converge in the last 10 epochs. However, the small slope between
    epoch 40 and epoch 50 indicates that the loss would further decrease with training
    over additional epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s take a look at the training and validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code examples plot those accuracy values over the 50 training
    epochs, as shown in *Figure 11.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B17582_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: Classification accuracy by the number of training epochs'
  prefs: []
  type: TYPE_NORMAL
- en: The plot reveals that the gap between training and validation accuracy increases
    as we train for more epochs. At approximately the 25th epoch, the training and
    validation accuracy values are almost equal, and then, the network starts to slightly
    overfit the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Reducing overfitting**'
  prefs: []
  type: TYPE_NORMAL
- en: One way to decrease the effect of overfitting is to increase the regularization
    strength via L2 regularization, which we introduced in *Chapter 3*, *A Tour of
    Machine Learning Classifiers Using Scikit-Learn*. Another useful technique for
    tackling overfitting in NNs is dropout, which will be covered in *Chapter 14*,
    *Classifying Images with Deep Convolutional Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s evaluate the generalization performance of the model by calculating
    the prediction accuracy on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the test accuracy is very close to the validation set accuracy
    corresponding to the last epoch (94.74%), which we reported during the training
    in the last subsection. Moreover, the respective training accuracy is only minimally
    higher at 95.59%, reaffirming that our model only slightly overfits the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: To further fine-tune the model, we could change the number of hidden units,
    the learning rate, or use various other tricks that have been developed over the
    years but are beyond the scope of this book. In *Chapter 14*, *Classifying Images
    with Deep Convolutional Neural Networks*, you will learn about a different NN
    architecture that is known for its good performance on image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the chapter will introduce additional performance-enhancing tricks such
    as adaptive learning rates, more sophisticated SGD-based optimization algorithms,
    batch normalization, and dropout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other common tricks that are beyond the scope of the following chapters include:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding skip-connections, which are the main contribution of residual NNs (*Deep
    residual learning for image recognition* by *K. He*, *X. Zhang*, *S. Ren*, and
    *J. Sun*, *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    pp. 770-778, 2016)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using learning rate schedulers that change the learning rate during training
    (*Cyclical learning rates for training neural networks* by *L.N. Smith*, *2017
    IEEE Winter Conference on Applications of Computer Vision (WACV)*, pp. 464-472,
    2017)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attaching loss functions to earlier layers in the networks as it’s being done
    in the popular Inception v3 architecture (*Rethinking the Inception architecture
    for computer vision by C. Szegedy*, *V. Vanhoucke*, *S. Ioffe*, *J. Shlens*, and
    *Z. Wojna*, *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, pp. 2818-2826, 2016)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, let’s take a look at some of the images that our MLP struggles with
    by extracting and plotting the first 25 misclassified samples from the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now see a 5×5 subplot matrix where the first number in the subtitles
    indicates the plot index, the second number represents the true class label (`True`),
    and the third number stands for the predicted class label (`Predicted`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, electronics, keyboard  Description automatically
    generated](img/B17582_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Handwritten digits that the model fails to classify correctly'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 11.9*, among others, the network finds 7s challenging
    when they include a horizontal line as in examples 19 and 20\. Looking back at
    an earlier figure in this chapter where we plotted different training examples
    of the number 7, we can hypothesize that the handwritten digit 7 with a horizontal
    line is underrepresented in our dataset and is often misclassified.
  prefs: []
  type: TYPE_NORMAL
- en: Training an artificial neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have seen an NN in action and have gained a basic understanding
    of how it works by looking over the code, let’s dig a little bit deeper into some
    of the concepts, such as the loss computation and the backpropagation algorithm
    that we implemented to learn the model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, we used an MSE loss (as in Adaline) to train the multilayer
    NN as it makes the derivation of the gradients a bit easier to follow. In later
    chapters, we will discuss other loss functions, such as the multi-category cross-entropy
    loss (a generalization of the binary logistic regression loss), which is a more
    common choice for training NN classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous section, we implemented an MLP for multiclass classification
    that returns an output vector of *t* elements that we need to compare to the *t*×1
    dimensional target vector in the one-hot encoding representation. If we predict
    the class label of an input image with class label 2, using this MLP, the activation
    of the third layer and the target may look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our MSE loss either has to sum or average over the *t* activation units
    in our network in addition to averaging over the *n* examples in the dataset or
    mini-batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_030.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, again, the superscript [*i*] is the index of a particular example in our
    training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that our goal is to minimize the loss function *L*(**W**); thus, we
    need to calculate the partial derivative of the parameters **W** with respect
    to each weight for every layer in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_031.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next section, we will talk about the backpropagation algorithm, which
    allows us to calculate those partial derivatives to minimize the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that **W** consists of multiple matrices. In an MLP with one hidden layer,
    we have the weight matrix, **W**^(^h^), which connects the input to the hidden
    layer, and **W**^(^(out)^), which connects the hidden layer to the output layer.
    A visualization of the three-dimensional tensor **W** is provided in *Figure 11.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, engineering drawing  Description automatically generated](img/B17582_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: A visualization of a three-dimensional tensor'
  prefs: []
  type: TYPE_NORMAL
- en: In this simplified figure, it may seem that both **W**^(^h^) and **W**^(^(out)^)
    have the same number of rows and columns, which is typically not the case unless
    we initialize an MLP with the same number of hidden units, output units, and input
    features.
  prefs: []
  type: TYPE_NORMAL
- en: If this sounds confusing, stay tuned for the next section, where we will discuss
    the dimensionality of **W**^(^h^) and **W**^(^(out)^) in more detail in the context
    of the backpropagation algorithm. Also, you are encouraged to read through the
    code of `NeuralNetMLP` again, which is annotated with helpful comments about the
    dimensionality of the different matrices and vector transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Developing your understanding of backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although backpropagation was introduced to the neural network community more
    than 30 years ago (*Learning representations by backpropagating errors*, by *D.E.
    Rumelhart*, *G.E. Hinton*, and *R.J. Williams*, *Nature*, 323: 6088, pages 533–536,
    1986), it remains one of the most widely used algorithms for training artificial
    NNs very efficiently. If you are interested in additional references regarding
    the history of backpropagation, Juergen Schmidhuber wrote a nice survey article,
    *Who Invented Backpropagation?*, which you can find online at [http://people.idsia.ch/~juergen/who-invented-backpropagation.html](http://people.idsia.ch/~juergen/who-invented-backpropagation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: This section will provide both a short, clear summary and the bigger picture
    of how this fascinating algorithm works before we dive into more mathematical
    details. In essence, we can think of backpropagation as a very computationally
    efficient approach to compute the partial derivatives of a complex, non-convex
    loss function in multilayer NNs. Here, our goal is to use those derivatives to
    learn the weight coefficients for parameterizing such a multilayer artificial
    NN. The challenge in the parameterization of NNs is that we are typically dealing
    with a very large number of model parameters in a high-dimensional feature space.
    In contrast to loss functions of single-layer NNs such as Adaline or logistic
    regression, which we have seen in previous chapters, the error surface of an NN
    loss function is not convex or smooth with respect to the parameters. There are
    many bumps in this high-dimensional loss surface (local minima) that we have to
    overcome in order to find the global minimum of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may recall the concept of the chain rule from your introductory calculus
    classes. The chain rule is an approach to compute the derivative of a complex,
    nested function, such as *f*(*g*(*x*)), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can use the chain rule for an arbitrarily long function composition.
    For example, let’s assume that we have five different functions, *f*(*x*), *g*(*x*),
    *h*(*x*), *u*(*x*), and *v*(*x*), and let *F* be the function composition: *F*(*x*) = *f*(*g*(*h*(*u*(*v*(*x*))))).
    Applying the chain rule, we can compute the derivative of this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_033.png)'
  prefs: []
  type: TYPE_IMG
- en: In the context of computer algebra, a set of techniques, known as **automatic
    differentiation**, has been developed to solve such problems very efficiently.
    If you are interested in learning more about automatic differentiation in machine
    learning applications, read A.G. Baydin and B.A. Pearlmutter’s article, *Automatic
    Differentiation of Algorithms for Machine Learning*, arXiv preprint arXiv:1404.7456,
    2014, which is freely available on arXiv at [http://arxiv.org/pdf/1404.7456.pdf](http://arxiv.org/pdf/1404.7456.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Automatic differentiation comes with two modes, the forward and reverse modes;
    backpropagation is simply a special case of reverse-mode automatic differentiation.
    The key point is that applying the chain rule in forward mode could be quite expensive
    since we would have to multiply large matrices for each layer (Jacobians) that
    we would eventually multiply by a vector to obtain the output.
  prefs: []
  type: TYPE_NORMAL
- en: The trick of reverse mode is that we traverse the chain rule from right to left.
    We multiply a matrix by a vector, which yields another vector that is multiplied
    by the next matrix, and so on. Matrix-vector multiplication is computationally
    much cheaper than matrix-matrix multiplication, which is why backpropagation is
    one of the most popular algorithms used in NN training.
  prefs: []
  type: TYPE_NORMAL
- en: '**A basic calculus refresher**'
  prefs: []
  type: TYPE_NORMAL
- en: To fully understand backpropagation, we need to borrow certain concepts from
    differential calculus, which is outside the scope of this book. However, you can
    refer to a review chapter of the most fundamental concepts, which you might find
    useful in this context. It discusses function derivatives, partial derivatives,
    gradients, and the Jacobian. This text is freely accessible at [https://sebastianraschka.com/pdf/books/dlb/appendix_d_calculus.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_d_calculus.pdf).
    If you are unfamiliar with calculus or need a brief refresher, consider reading
    this text as an additional supporting resource before reading the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Training neural networks via backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will go through the math of backpropagation to understand
    how you can learn the weights in an NN very efficiently. Depending on how comfortable
    you are with mathematical representations, the following equations may seem relatively
    complicated at first.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a previous section, we saw how to calculate the loss as the difference between
    the activation of the last layer and the target class label. Now, we will see
    how the backpropagation algorithm works to update the weights in our MLP model
    from a mathematical perspective, which we implemented in the `.backward()` method
    of the `NeuralNetMLP()` class. As we recall from the beginning of this chapter,
    we first need to apply forward propagation to obtain the activation of the output
    layer, which we formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Concisely, we just forward-propagate the input features through the connections
    in the network, as shown by the arrows in *Figure 11.11* for a network with two
    input features, three hidden nodes, and two output nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_11_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: Forward-propagating the input features of an NN'
  prefs: []
  type: TYPE_NORMAL
- en: 'In backpropagation, we propagate the error from right to left. We can think
    of this as an application of the chain rule to the computation of the forward
    pass to compute the gradient of the loss with respect to the model weights (and
    bias units). For simplicity, we will illustrate this process for the partial derivative
    used to update the first weight in the weight matrix of the output layer. The
    paths of the computation we backpropagate are highlighted via the bold arrows
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_11_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12: Backpropagating the error of an NN'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we include the net inputs *z* explicitly, the partial derivative computation
    shown in the previous figure expands as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_035.png)'
  prefs: []
  type: TYPE_IMG
- en: To compute this partial derivative, which is used to update ![](img/B17582_11_036.png),
    we can compute the three individual partial derivative terms and multiply the
    results. For simplicity, we will omit averaging over the individual examples in
    the mini-batch, so we drop the ![](img/B17582_11_037.png) averaging term from
    the following equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with ![](img/B17582_11_038.png), which is the partial derivative
    of the MSE loss (which simplifies to the squared error if we omit the mini-batch
    dimension) with respect to the predicted output score of the first output node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next term is the derivative of the logistic sigmoid activation function
    that we used in the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, we compute the derivative of the net input with respect to the weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Putting all of it together, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then use this value to update the weight via the familiar stochastic gradient
    descent update with a learning rate of ![](img/B17582_02_036.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_044.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In our code implementation of `NeuralNetMLP()`, we implemented the computation
    ![](img/B17582_11_045.png) in vectorized form in the `.backward()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17582_11_046.png)'
  prefs: []
  type: TYPE_IMG
- en: This is because ![](img/B17582_11_047.png) terms are involved in computing the
    partial derivatives (or gradients) of the hidden layer weights as well; hence,
    we can reuse ![](img/B17582_11_047.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Speaking of hidden layer weights, *Figure 11.13* illustrates how to compute
    the partial derivative of the loss with respect to the first weight of the hidden
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, clock  Description automatically generated](img/B17582_11_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.13: Computing the partial derivatives of the loss with respect to
    the first hidden layer weight'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to highlight that since the weight ![](img/B17582_11_049.png)
    is connected to both output nodes, we have to use the *multi-variable* chain rule
    to sum the two paths highlighted with bold arrows. As before, we can expand it
    to include the net inputs *z* and then solve the individual terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_050.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that if we reuse ![](img/B17582_11_047.png) computed previously, this
    equation can be simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_11_052.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding terms can be individually solved relatively easily, as we have
    done previously, because there are no new derivatives involved. For example, ![](img/B17582_11_053.png)
    is the derivative of the sigmoid activation, that is, ![](img/B17582_11_054.png),
    and so forth. We’ll leave solving the individual parts as an optional exercise
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: About convergence in neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might be wondering why we did not use regular gradient descent but instead
    used mini-batch learning to train our NN for the handwritten digit classification
    earlier. You may recall our discussion on SGD that we used to implement online
    learning. In online learning, we compute the gradient based on a single training
    example (*k* = 1) at a time to perform the weight update. Although this is a stochastic
    approach, it often leads to very accurate solutions with a much faster convergence
    than regular gradient descent. Mini-batch learning is a special form of SGD where
    we compute the gradient based on a subset *k* of the *n* training examples with
    1 < *k* < *n*. Mini-batch learning has an advantage over online learning in that
    we can make use of our vectorized implementations to improve computational efficiency.
    However, we can update the weights much faster than in regular gradient descent.
    Intuitively, you can think of mini-batch learning as predicting the voter turnout
    of a presidential election from a poll by asking only a representative subset
    of the population rather than asking the entire population (which would be equal
    to running the actual election).
  prefs: []
  type: TYPE_NORMAL
- en: 'Multilayer NNs are much harder to train than simpler algorithms such as Adaline,
    logistic regression, or support vector machines. In multilayer NNs, we typically
    have hundreds, thousands, or even billions of weights that we need to optimize.
    Unfortunately, the output function has a rough surface, and the optimization algorithm
    can easily become trapped in local minima, as shown in *Figure 11.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_11_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.14: Optimization algorithms can become trapped in local minima'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this representation is extremely simplified since our NN has many
    dimensions; it makes it impossible to visualize the actual loss surface for the
    human eye. Here, we only show the loss surface for a single weight on the *x*
    axis. However, the main message is that we do not want our algorithm to get trapped
    in local minima. By increasing the learning rate, we can more readily escape such
    local minima. On the other hand, we also increase the chance of overshooting the
    global optimum if the learning rate is too large. Since we initialize the weights
    randomly, we start with a solution to the optimization problem that is typically
    hopelessly wrong.
  prefs: []
  type: TYPE_NORMAL
- en: A few last words about the neural network implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may be wondering why we went through all of this theory just to implement
    a simple multilayer artificial network that can classify handwritten digits instead
    of using an open source Python machine learning library. In fact, we will introduce
    more complex NN models in the next chapters, which we will train using the open
    source PyTorch library ([https://pytorch.org](https://pytorch.org)).
  prefs: []
  type: TYPE_NORMAL
- en: Although the from-scratch implementation in this chapter seems a bit tedious
    at first, it was a good exercise for understanding the basics behind backpropagation
    and NN training. A basic understanding of algorithms is crucial for applying machine
    learning techniques appropriately and successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how feedforward NNs work, we are ready to explore
    more sophisticated DNNs using PyTorch, which allows us to construct NNs more efficiently,
    as we will see in *Chapter 12*, *Parallelizing Neural Network Training with PyTorch*.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch, which was originally released in September 2016, has gained a lot of
    popularity among machine learning researchers, who use it to construct DNNs because
    of its ability to optimize mathematical expressions for computations on multidimensional
    arrays utilizing **graphics processing units** (**GPUs**).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we should note that scikit-learn also includes a basic MLP implementation,
    `MLPClassifier`, which you can find at [https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).
    While this implementation is great and very convenient for training basic MLPs,
    we strongly recommend specialized deep learning libraries, such as PyTorch, for
    implementing and training multilayer NNs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned the basic concepts behind multilayer artificial
    NNs, which are currently the hottest topic in machine learning research. In *Chapter
    2*, *Training Simple Machine Learning Algorithms for Classification*, we started
    our journey with simple single-layer NN structures and now we have connected multiple
    neurons to a powerful NN architecture to solve complex problems such as handwritten
    digit recognition. We demystified the popular backpropagation algorithm, which
    is one of the building blocks of many NN models that are used in deep learning.
    After learning about the backpropagation algorithm in this chapter, we are well
    equipped for exploring more complex DNN architectures. In the remaining chapters,
    we will cover more advanced deep learning concepts and PyTorch, an open source
    library that allows us to implement and train multilayer NNs more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
