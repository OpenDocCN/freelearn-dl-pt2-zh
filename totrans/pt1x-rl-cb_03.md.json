["```py\n>>> import torch\n>>> import math\n>>> import matplotlib.pyplot as plt\n```", "```py\n>>> n_point = 1000\n>>> points = torch.rand((n_point, 2)) * 2 - 1\n```", "```py\n>>> n_point_circle = 0\n>>> points_circle = []\n```", "```py\n>>> for point in points:\n...     r = torch.sqrt(point[0] ** 2 + point[1] ** 2)\n...     if r <= 1:\n...         points_circle.append(point)\n...         n_point_circle += 1\n```", "```py\n>>> points_circle = torch.stack(points_circle)\n```", "```py\n>>> plt.plot(points[:, 0].numpy(), points[:, 1].numpy(), 'y.')\n>>> plt.plot(points_circle[:, 0].numpy(), points_circle[:, 1].numpy(), 'c.')\n```", "```py\n>>> i = torch.linspace(0, 2 * math.pi)\n>>> plt.plot(torch.cos(i).numpy(), torch.sin(i).numpy())\n>>> plt.axes().set_aspect('equal')\n>>> plt.show()\n```", "```py\n>>> pi_estimated = 4 * (n_point_circle / n_point)\n>>> print('Estimated value of pi is:', pi_estimated)\n```", "```py\nEstimated value of pi is: 3.156 \n```", "```py\n>>> def estimate_pi_mc(n_iteration):\n...     n_point_circle = 0\n...     pi_iteration = []\n...     for i in range(1, n_iteration+1):\n...         point = torch.rand(2) * 2 - 1\n...         r = torch.sqrt(point[0] ** 2 + point[1] ** 2)\n...         if r <= 1:\n...             n_point_circle += 1\n...         pi_iteration.append(4 * (n_point_circle / i))\n...     plt.plot(pi_iteration)\n...     plt.plot([math.pi] * n_iteration, '--')\n...     plt.xlabel('Iteration')\n...     plt.ylabel('Estimated pi')\n...     plt.title('Estimation history')\n...     plt.show()\n...     print('Estimated value of pi is:', pi_iteration[-1]) The estimated value of pi is: 3.1364\n```", "```py\n>>> estimate_pi_mc(10000)\n```", "```py\n>>> import torch\n>>> import gym >>> env = gym.make(\"FrozenLake-v0\")\n```", "```py\n>>> def run_episode(env, policy):\n ...     state = env.reset()\n ...     rewards = []\n ...     states = [state]\n ...     is_done = False\n ...     while not is_done:\n ...         action = policy[state].item()\n ...         state, reward, is_done, info = env.step(action)\n ...         states.append(state)\n ...         rewards.append(reward)\n ...         if is_done:\n ...             break\n ...     states = torch.tensor(states)\n ...     rewards = torch.tensor(rewards)\n ...     return states, rewards\n```", "```py\n>>> def mc_prediction_first_visit(env, policy, gamma, n_episode):\n...     n_state = policy.shape[0]\n...     V = torch.zeros(n_state)\n...     N = torch.zeros(n_state)\n...     for episode in range(n_episode):\n...         states_t, rewards_t = run_episode(env, policy)\n...         return_t = 0\n...         first_visit = torch.zeros(n_state)\n...         G = torch.zeros(n_state)\n...         for state_t, reward_t in zip(reversed(states_t)[1:], \n                                            reversed(rewards_t)):\n...             return_t = gamma * return_t + reward_t\n...             G[state_t] = return_t\n...             first_visit[state_t] = 1\n...         for state in range(n_state):\n...             if first_visit[state] > 0:\n...                 V[state] += G[state]\n...                 N[state] += 1\n...     for state in range(n_state):\n...         if N[state] > 0:\n...             V[state] = V[state] / N[state]\n...     return V\n```", "```py\n>>> gamma = 1\n>>> n_episode = 10000\n```", "```py\n>>> optimal_policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])\n>>> value = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)\n>>> print('The value function calculated by first-visit MC prediction:\\n', value)\nThe value function calculated by first-visit MC prediction:\ntensor([0.7463, 0.5004, 0.4938, 0.4602, 0.7463, 0.0000, 0.3914, 0.0000, 0.7463, 0.7469, 0.6797, 0.0000, 0.0000, 0.8038, 0.8911, 0.0000])\n```", "```py\n>>> def mc_prediction_every_visit(env, policy, gamma, n_episode):\n...     n_state = policy.shape[0]\n...     V = torch.zeros(n_state)\n...     N = torch.zeros(n_state)\n...     G = torch.zeros(n_state)\n...     for episode in range(n_episode):\n...         states_t, rewards_t = run_episode(env, policy)\n...         return_t = 0\n...         for state_t, reward_t in zip(reversed(states_t)[1:],  \n                                            reversed(rewards_t)):\n...             return_t = gamma * return_t + reward_t\n...             G[state_t] += return_t\n...             N[state_t] += 1\n...     for state in range(n_state):\n...         if N[state] > 0:\n...             V[state] = G[state] / N[state]\n...     return V\n```", "```py\n>>> value = mc_prediction_every_visit(env, optimal_policy, gamma, n_episode)\n```", "```py\n>>> print('The value function calculated by every-visit MC prediction:\\n', value)\nThe value function calculated by every-visit MC prediction:\ntensor([0.6221, 0.4322, 0.3903, 0.3578, 0.6246, 0.0000, 0.3520, 0.0000, 0.6428, 0.6759, 0.6323, 0.0000, 0.0000, 0.7624, 0.8801, 0.0000])\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('Blackjack-v0')\n```", "```py\n>>> env.reset()\n>>> env.reset()\n(20, 5, False)\n```", "```py\n>>> env.reset()\n(18, 6, True)\n```", "```py\n>>> env.step(1)\n((20, 6, True), 0, False, {})\n```", "```py\n>>> env.step(0)\n ((20, 6, True), 1, True, {})\n```", "```py\n>>> env.reset()\n(15, 10, False)\n>>> env.step(1)\n((25, 10, False), -1, True, {})\n```", "```py\n>>> def run_episode(env, hold_score):\n ...     state = env.reset()\n ...     rewards = []\n ...     states = [state]\n ...     is_done = False\n ...     while not is_done:\n ...         action = 1 if state[0] < hold_score else 0\n ...         state, reward, is_done, info = env.step(action)\n ...         states.append(state)\n ...         rewards.append(reward)\n ...         if is_done:\n ...             break\n ...     return states, rewards\n```", "```py\n>>> from collections import defaultdict\n>>> def mc_prediction_first_visit(env, hold_score, gamma, \n                                                 n_episode):\n ...     V = defaultdict(float)\n ...     N = defaultdict(int)\n ...     for episode in range(n_episode):\n ...         states_t, rewards_t = run_episode(env, hold_score)\n ...         return_t = 0\n ...         G = {}\n ...         for state_t, reward_t in zip(states_t[1::-1], \n                                             rewards_t[::-1]):\n ...             return_t = gamma * return_t + reward_t\n ...             G[state_t] = return_t\n ...         for state, return_t in G.items():\n ...             if state[0] <= 21:\n ...                 V[state] += return_t\n ...                 N[state] += 1\n ...     for state in V:\n ...         V[state] = V[state] / N[state]\n ...     return V\n```", "```py\n>>> hold_score = 18\n>>> gamma = 1\n>>> n_episode = 500000\n```", "```py\n>>> value = mc_prediction_first_visit(env, hold_score, gamma, n_episode)\n```", "```py\n>>> print('The value function calculated by first-visit MC prediction:\\n', value)\n```", "```py\n>>> print('Number of states:', len(value))\nNumber of states: 280\n```", "```py\nThe value function calculated by first-visit MC prediction:\ndefaultdict(<class 'float'>, {(20, 6, False): 0.6923485653560042, (17, 5, False): -0.24390243902439024, (16, 5, False): -0.19118165784832453, (20, 10, False): 0.4326379146490474, (20, 7, False): 0.7686220540168588, (16, 6, False): -0.19249478804725503,\n ……\n ……\n(5, 9, False): -0.20612244897959184, (12, 7, True): 0.058823529411764705, (6, 4, False): -0.26582278481012656, (4, 8, False): -0.14937759336099585, (4, 3, False): -0.1680327868852459, (4, 9, False): -0.20276497695852536, (4, 4, False): -0.3201754385964912, (12, 8, True): 0.11057692307692307})\n```", "```py\n>>> import matplotlib\n>>> import matplotlib.pyplot as plt\n>>> from mpl_toolkits.mplot3d import Axes3D\n```", "```py\n>>> def plot_surface(X, Y, Z, title):\n...     fig = plt.figure(figsize=(20, 10))\n...     ax = fig.add_subplot(111, projection='3d')\n...     surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n...             cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n...     ax.set_xlabel('Player Sum')\n...     ax.set_ylabel('Dealer Showing')\n...     ax.set_zlabel('Value')\n...     ax.set_title(title)\n...     ax.view_init(ax.elev, -120)\n...     fig.colorbar(surf)\n...     plt.show()\n```", "```py\n>>> def plot_blackjack_value(V):\n...     player_sum_range = range(12, 22)\n...     dealer_show_range = range(1, 11)\n...     X, Y = torch.meshgrid([torch.tensor(player_sum_range), \n                torch.tensor(dealer_show_range)])\n...     values_to_plot = torch.zeros((len(player_sum_range),  \n                                  len(dealer_show_range), 2))\n...     for i, player in enumerate(player_sum_range):\n...         for j, dealer in enumerate(dealer_show_range):\n...             for k, ace in enumerate([False, True]):\n...                 values_to_plot[i, j, k] = \n                               V[(player, dealer, ace)]\n...     plot_surface(X, Y, values_to_plot[:,:,0].numpy(), \n                \"Blackjack Value Function Without Usable Ace\")\n...     plot_surface(X, Y, values_to_plot[:,:,1].numpy(), \n                \"Blackjack Value Function With Usable Ace\")\n```", "```py\n>>> plot_blackjack_value(value)\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('Blackjack-v0')\n```", "```py\n>>> def run_episode(env, Q, n_action):\n ...     \"\"\"\n ...     Run a episode given a Q-function\n ...     @param env: OpenAI Gym environment\n ...     @param Q: Q-function\n ...     @param n_action: action space\n ...     @return: resulting states, actions and rewards for the entire episode\n ...     \"\"\"\n ...     state = env.reset()\n ...     rewards = []\n ...     actions = []\n ...     states = []\n ...     is_done = False\n ...     action = torch.randint(0, n_action, [1]).item()\n ...     while not is_done:\n ...         actions.append(action)\n ...         states.append(state)\n ...         state, reward, is_done, info = env.step(action)\n ...         rewards.append(reward)\n ...         if is_done:\n ...             break\n ...         action = torch.argmax(Q[state]).item()\n ...     return states, actions, rewards\n```", "```py\n >>> from collections import defaultdict\n >>> def mc_control_on_policy(env, gamma, n_episode):\n ...     \"\"\"\n ...     Obtain the optimal policy with on-policy MC control method\n ...     @param env: OpenAI Gym environment\n ...     @param gamma: discount factor\n ...     @param n_episode: number of episodes\n ...     @return: the optimal Q-function, and the optimal policy\n ...     \"\"\" ...     n_action = env.action_space.n\n ...     G_sum = defaultdict(float)\n ...     N = defaultdict(int)\n ...     Q = defaultdict(lambda: torch.empty(env.action_space.n))\n ...     for episode in range(n_episode):\n ...         states_t, actions_t, rewards_t = run_episode(env, Q, n_action) \n ...         return_t = 0\n ...         G = {}\n ...         for state_t, action_t, reward_t in zip(states_t[::-1], \n                     actions_t[::-1], rewards_t[::-1]):\n ...             return_t = gamma * return_t + reward_t\n ...             G[(state_t, action_t)] = return_t\n ...         for state_action, return_t in G.items():\n ...             state, action = state_action\n ...             if state[0] <= 21:\n ...                 G_sum[state_action] += return_t\n ...                 N[state_action] += 1\n ...                 Q[state][action] = G_sum[state_action] \n                                         / N[state_action]\n ...      policy = {}\n ...      for state, actions in Q.items():\n ...          policy[state] = torch.argmax(actions).item()\n ...      return Q, policy\n```", "```py\n>>> gamma = 1\n>>> n_episode = 500000\n```", "```py\n>>> optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode) >>> print(optimal_policy)\n```", "```py\n>>> optimal_value = defaultdict(float)\n>>> for state, action_values in optimal_Q.items():\n ...     optimal_value[state] = torch.max(action_values).item() >>> print(optimal_value)\n```", "```py\n>>> plot_blackjack_value(optimal_value)\n```", "```py\n{(16, 8, True): 1, (11, 2, False): 1, (15, 5, True): 1, (14, 9, False): 1, (11, 6, False): 1, (20, 3, False): 0, (9, 6, False): 0, (12, 9, False): 0, (21, 2, True): 0, (16, 10, False): 1, (17, 5, False): 0, (13, 10, False): 1, (12, 10, False): 1, (14, 10, False): 0, (10, 2, False): 1, (20, 4, False): 0, (11, 4, False): 1, (16, 9, False): 0, (10, 8,\n ……\n ……\n1, (18, 6, True): 0, (12, 2, True): 1, (8, 3, False): 1, (13, 3, True): 0, (4, 7, False): 1, (18, 8, True): 0, (6, 5, False): 1, (17, 6, True): 0, (19, 9, True): 0, (4, 4, False): 0, (14, 5, True): 1, (12, 6, True): 0, (4, 9, False): 1, (13, 4, True): 1, (4, 8, False): 1, (14, 3, True): 1, (12, 4, True): 1, (4, 6, False): 0, (12, 5, True): 0, (4, 2, False): 1, (4, 3, False): 1, (5, 4, False): 1, (4, 1, False): 0}\n```", "```py\n{(21, 8, False): 0.9262458682060242, (11, 8, False): 0.16684606671333313, (16, 10, False): -0.4662476181983948, (16, 10, True): -0.3643564283847809, (14, 8, False): -0.2743947207927704, (13, 10, False): -0.3887477219104767, (12, 9, False): -0.22795115411281586\n ……\n ……\n(4, 3, False): -0.18421052396297455, (4, 8, False): -0.16806723177433014, (13, 2, True): 0.05485232174396515, (5, 5, False): -0.09459459781646729, (5, 8, False): -0.3690987229347229, (20, 2, True): 0.6965699195861816, (17, 2, True): -0.09696969389915466, (12, 2, True): 0.0517241396009922}\n```", "```py\n>>> hold_score = 18\n>>> hold_policy = {}\n>>> player_sum_range = range(2, 22)\n>>> for player in range(2, 22):\n...     for dealer in range(1, 11):\n...         action = 1 if player < hold_score else 0\n...         hold_policy[(player, dealer, False)] = action\n...         hold_policy[(player, dealer, True)] = action\n```", "```py\n>>> def simulate_episode(env, policy):\n...     state = env.reset()\n...     is_done = False\n...     while not is_done:\n...         action = policy[state]\n...         state, reward, is_done, info = env.step(action)\n...         if is_done:\n...             return reward\n```", "```py\n>>> n_episode = 100000\n>>> n_win_optimal = 0\n>>> n_win_simple = 0\n>>> n_lose_optimal = 0\n>>> n_lose_simple = 0\n```", "```py\n>>> for _ in range(n_episode):\n...     reward = simulate_episode(env, optimal_policy)\n...     if reward == 1:\n...         n_win_optimal += 1\n...     elif reward == -1:\n...         n_lose_optimal += 1\n...     reward = simulate_episode(env, hold_policy)\n...     if reward == 1:\n...         n_win_simple += 1\n...     elif reward == -1:\n...         n_lose_simple += 1\n```", "```py\n>>> print('Winning probability under the simple policy: {}'.format(n_win_simple/n_episode))\nWinning probability under the simple policy: 0.39923\n>>> print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))\nWinning probability under the optimal policy: 0.41281\n```", "```py\n>>> print('Losing probability under the simple policy: {}'.format(n_lose_simple/n_episode))\nLosing probability under the simple policy: 0.51024\n>>> print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))\nLosing probability under the optimal policy: 0.493\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('Blackjack-v0')\n```", "```py\n>>> def run_episode(env, Q, epsilon, n_action):\n...     \"\"\"\n...     Run a episode and performs epsilon-greedy policy\n...     @param env: OpenAI Gym environment\n...     @param Q: Q-function\n...     @param epsilon: the trade-off between exploration and exploitation\n...     @param n_action: action space\n...     @return: resulting states, actions and rewards for the entire episode\n...     \"\"\"\n...     state = env.reset()\n...     rewards = []\n...     actions = []\n...     states = []\n...     is_done = False\n...     while not is_done:\n...         probs = torch.ones(n_action) * epsilon / n_action\n...         best_action = torch.argmax(Q[state]).item()\n...         probs[best_action] += 1.0 - epsilon\n...         action = torch.multinomial(probs, 1).item()\n...         actions.append(action)\n...         states.append(state)\n...         state, reward, is_done, info = env.step(action)\n...         rewards.append(reward)\n...         if is_done:\n...             break\n...     return states, actions, rewards\n```", "```py\n>>> from collections import defaultdict\n>>> def mc_control_epsilon_greedy(env, gamma, n_episode, epsilon):\n...     \"\"\"\n...     Obtain the optimal policy with on-policy MC control with epsilon_greedy\n...     @param env: OpenAI Gym environment\n...     @param gamma: discount factor\n...     @param n_episode: number of episodes\n...     @param epsilon: the trade-off between exploration and exploitation\n...     @return: the optimal Q-function, and the optimal policy\n...     \"\"\"\n...     n_action = env.action_space.n\n...     G_sum = defaultdict(float)\n...     N = defaultdict(int)\n...     Q = defaultdict(lambda: torch.empty(n_action))\n...     for episode in range(n_episode):\n...         states_t, actions_t, rewards_t = \n                     run_episode(env, Q, epsilon, n_action)\n...         return_t = 0\n...         G = {}\n...         for state_t, action_t, reward_t in zip(states_t[::-1], \n                                 actions_t[::-1], rewards_t[::-1]):\n...             return_t = gamma * return_t + reward_t\n...             G[(state_t, action_t)] = return_t\n...         for state_action, return_t in G.items():\n...             state, action = state_action\n...             if state[0] <= 21:\n...                 G_sum[state_action] += return_t\n...                 N[state_action] += 1\n...                 Q[state][action] = \n                         G_sum[state_action] / N[state_action]\n...     policy = {}\n...     for state, actions in Q.items():\n...         policy[state] = torch.argmax(actions).item()\n...     return Q, policy\n```", "```py\n>>> gamma = 1\n>>> n_episode = 500000\n>>> epsilon = 0.1\n```", "```py\n>>> optimal_Q, optimal_policy = mc_control_epsilon_greedy(env, gamma, n_episode, epsilon)\n```", "```py\n>>> n_episode = 100000\n>>> n_win_optimal = 0\n>>> n_lose_optimal = 0\n>>> for _ in range(n_episode):\n...     reward = simulate_episode(env, optimal_policy)\n...     if reward == 1:\n...         n_win_optimal += 1\n...     elif reward == -1:\n...         n_lose_optimal += 1\n```", "```py\n>>> print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))\nWinning probability under the optimal policy: 0.42436\n```", "```py\n>>> print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))\nLosing probability under the optimal policy: 0.48048\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('Blackjack-v0')\n```", "```py\n>>> def gen_random_policy(n_action):\n...     probs = torch.ones(n_action) / n_action\n...     def policy_function(state):\n...         return probs\n...     return policy_function\n>>> random_policy = gen_random_policy(env.action_space.n)\n```", "```py\n>>> def run_episode(env, behavior_policy):\n...     \"\"\"\n...     Run a episode given a behavior policy\n...     @param env: OpenAI Gym environment\n...     @param behavior_policy: behavior policy\n...     @return: resulting states, actions and rewards for the entire episode\n...     \"\"\"\n...     state = env.reset()\n...     rewards = [] ...     actions = []\n...     states = []\n...     is_done = False\n...     while not is_done:\n...         probs = behavior_policy(state)\n...         action = torch.multinomial(probs, 1).item()\n...         actions.append(action)\n...         states.append(state)\n...         state, reward, is_done, info = env.step(action)\n...         rewards.append(reward)\n...         if is_done:\n...             break\n...     return states, actions, rewards\n```", "```py\n>>> from collections import defaultdict\n>>> def mc_control_off_policy(env, gamma, n_episode, behavior_policy):\n...     \"\"\"\n...     Obtain the optimal policy with off-policy MC control method\n...     @param env: OpenAI Gym environment\n...     @param gamma: discount factor\n...     @param n_episode: number of episodes\n...     @param behavior_policy: behavior policy\n...     @return: the optimal Q-function, and the optimal policy\n...     \"\"\"\n...     n_action = env.action_space.n\n...     G_sum = defaultdict(float)\n...     N = defaultdict(int)\n...     Q = defaultdict(lambda: torch.empty(n_action))\n...     for episode in range(n_episode):\n...         W = {}\n...         w = 1\n...         states_t, actions_t, rewards_t = \n                     run_episode(env, behavior_policy)\n...         return_t = 0 ...         G = {}\n...         for state_t, action_t, reward_t in zip(states_t[::-1], \n                                 actions_t[::-1], rewards_t[::-1]):\n...             return_t = gamma * return_t + reward_t\n...             G[(state_t, action_t)] = return_t\n...             if action_t != torch.argmax(Q[state_t]).item():\n...                 break\n...             w *= 1./ behavior_policy(state_t)[action_t]\n...         for state_action, return_t in G.items():\n...             state, action = state_action\n...             if state[0] <= 21:\n...                 G_sum[state_action] += \n                                 return_t * W[state_action]\n...                 N[state_action] += 1\n...                 Q[state][action] = \n                             G_sum[state_action] / N[state_action]\n...     policy = {}\n...     for state, actions in Q.items():\n...         policy[state] = torch.argmax(actions).item()\n...     return Q, policy\n```", "```py\n>>> gamma = 1\n>>> n_episode = 500000\n```", "```py\n>>> optimal_Q, optimal_policy = mc_control_off_policy(env, gamma, n_episode, random_policy)\n```", "```py\n>>> def mc_control_off_policy_incremental(env, gamma, n_episode, behavior_policy):\n...     n_action = env.action_space.n\n...     N = defaultdict(int)\n...     Q = defaultdict(lambda: torch.empty(n_action))\n...     for episode in range(n_episode):\n...         W = 1.\n...         states_t, actions_t, rewards_t = \n                             run_episode(env, behavior_policy)\n...         return_t = 0.\n...         for state_t, action_t, reward_t in \n                     zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n...             return_t = gamma * return_t + reward_t\n...             N[(state_t, action_t)] += 1\n...             Q[state_t][action_t] += (W / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])\n...             if action_t != torch.argmax(Q[state_t]).item():\n...                 break\n...             W *= 1./ behavior_policy(state_t)[action_t]\n...     policy = {}\n...     for state, actions in Q.items():\n...         policy[state] = torch.argmax(actions).item()\n...     return Q, policy\n```", "```py\n>>> optimal_Q, optimal_policy = mc_control_off_policy_incremental(env, gamma, n_episode, random_policy)\n```", "```py\n>>> import torch\n>>> import gym\n>>> env = gym.make('Blackjack-v0')\n```", "```py\n>>> random_policy = gen_random_policy(env.action_space.n)\n```", "```py\n>>> from collections import defaultdict\n>>> def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):\n...     \"\"\"\n...     Obtain the optimal policy with off-policy MC control method with weighted importance sampling\n...     @param env: OpenAI Gym environment\n...     @param gamma: discount factor\n...     @param n_episode: number of episodes\n...     @param behavior_policy: behavior policy\n...     @return: the optimal Q-function, and the optimal policy\n...     \"\"\"\n...     n_action = env.action_space.n\n...     N = defaultdict(float)\n...     Q = defaultdict(lambda: torch.empty(n_action))\n...     for episode in range(n_episode):\n...         W = 1.\n...         states_t, actions_t, rewards_t = \n                             run_episode(env, behavior_policy)\n...         return_t = 0.\n...         for state_t, action_t, reward_t in zip(states_t[::-1], \n                                 actions_t[::-1], rewards_t[::-1]):\n...             return_t = gamma * return_t + reward_t\n...             N[(state_t, action_t)] += W\n...             Q[state_t][action_t] += (W / N[(state_t, action_t)]) \n                                 * (return_t - Q[state_t][action_t])\n...             if action_t != torch.argmax(Q[state_t]).item():\n...                 break\n...             W *= 1./ behavior_policy(state_t)[action_t]\n...     policy = {}\n...     for state, actions in Q.items():\n...         policy[state] = torch.argmax(actions).item()\n...     return Q, policy\n```", "```py\n>>> gamma = 1\n>>> n_episode = 500000\n```", "```py\n>>> optimal_Q, optimal_policy = mc_control_off_policy_weighted(env, gamma, n_episode, random_policy)\n```", "```py\n>>> n_episode = 100000\n>>> n_win_optimal = 0\n>>> n_lose_optimal = 0\n>>> for _ in range(n_episode):\n...     reward = simulate_episode(env, optimal_policy)\n...     if reward == 1:\n...         n_win_optimal += 1\n...     elif reward == -1:\n...         n_lose_optimal += 1\n```", "```py\n>>> print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))\nWinning probability under the optimal policy: 0.43072\n>>> print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))\nLosing probability under the optimal policy: 0.47756\n```"]