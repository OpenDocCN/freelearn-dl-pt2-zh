- en: Learning More about GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned what **Generative Adversarial Networks** (**GANs**) are and how different
    types of GANs are used to generate images in [Chapter 8](b71eb1cb-af20-41ea-9e3d-26c7d0b956ba.xhtml),
    *Generating Images Using GANs*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will uncover various interesting different types of GANs.
    We've learned that GANs can be used to generate new images but we do not have
    any control over the images that they generate. For instance, if we want our GAN
    to generate a human face with specific traits how do we tell this information
    to the GAN? We can't because we have no control over the images generated by the
    generator.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve this, we use a new type of GAN called a **Conditional GAN** (**CGAN**)
    where we can condition the generator and discriminator by specifying what we want
    to generate. We will start off the chapter by comprehending how CGANs can be used
    to generate images of our interest and then we learn how to implement CGANs using
    **TensorFlow**.
  prefs: []
  type: TYPE_NORMAL
- en: We then understand about the **InfoGANs** which is an unsupervised version of
    a CGAN. We will understand what InfoGANs are and how they differ from CGANs, and
    how can we implement them using TensorFlow to generate new images.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we shall learn about **CycleGANs**, which are a very intriguing type of
    GAN. They try to learn the mapping from the distribution of images in one domain
    to the distribution of images in another domain. For instance, to convert a grayscale
    image to a colored image, we train the CycleGAN to learn the mapping between grayscale
    and colored images, which means they learn to map from one domain, to another
    and the best part is, unlike other architectures, they even don't require a paired
    dataset. We will investigate how exactly they learn these mappings and their architecture
    in detail. We will explore how to implement CycleGAN to convert real pictures
    to paintings.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we will explore, **StackGAN**, which can convert
    the text description to a photo-realistic image. We will perceive how StackGANs
    do this by gaining a deeper understanding of their architecture in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating specific digits using CGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: InfoGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of InfoGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Constructing InfoGAN using TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CycleGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting pictures to paintings using CycleGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StackGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that the generator generates new images by learning the real data distribution,
    while the discriminator examines whether the image generated by the generator
    is from the real data distribution or fake data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: However, the generator has the capability to generate new and interesting images
    by learning the real data distribution. We have no control or influence over the
    images generated by the generator. For instance, let's say our generator is generating
    human faces; how can we tell the generator to generate a human face with certain
    features, say big eyes and a sharp nose?
  prefs: []
  type: TYPE_NORMAL
- en: We can't! Because we have no control over the images that are being generated
    by the generator.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this, we introduce a small variant of a GAN called a **CGAN**, which
    imposes a condition to both the generator and the discriminator. This condition
    tells the GAN that what image we want our generator to generate. So, both of our
    components—the discriminator and the generator—act upon this condition.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a simple example. Say we are generating handwritten digits using
    CGAN with the MNIST dataset. Let's assume that we are more focused on generating
    digit 7 instead of other digits. Now, we need to impose this condition to both
    of our generators and discriminators. How do we do that?
  prefs: []
  type: TYPE_NORMAL
- en: The generator takes the noise ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)
    as an input and generates an image. But along with ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png),
    we also pass an additional input, ![](img/92bb425b-a16a-4c8a-a295-243ecfdb386b.png).
    This ![](img/9f1aaca1-ad1d-4079-9a27-8f12401d51fe.png) is a one-hot encoded class
    label. As we are interested in generating digit 7, we set the seventh index to
    1 and set all other indices to 0, that is, [0,0,0,0,0,0,0,1,0,0].
  prefs: []
  type: TYPE_NORMAL
- en: We concatenate the latent vector, ![](img/35a24ed1-1817-4456-ae54-3ab000857bcc.png),
    and the one-hot encoded conditional variable, ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png),
    and pass that as an input to the generator. Then, the generator starts generating
    the digit 7.
  prefs: []
  type: TYPE_NORMAL
- en: What about the discriminator? We know that the discriminator takes image ![](img/b867ddbc-f08c-4126-b54a-2141fa745659.png)
    as an input and tells us whether the image is a real or fake image. In CGAN, We
    want the discriminator to discriminate based on the condition, which means it
    has to identify whether the generated image is a real digit 7 or a fake digit
    7\. So, along with passing input ![](img/e7df7672-8219-4515-8446-49fa00b78885.png),
    we also pass the conditional variable ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png)
    to the discriminator by concatenating ![](img/cb3b4dc9-80cf-4374-8cdb-a5e075850686.png)
    and ![](img/3e98a2ce-923e-448d-80f7-76f21f8c6df8.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the following figure, we are passing ![](img/c164d061-dcfa-4c77-abff-441ad9cd92c5.png)
    and ![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png) to the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d46bd418-6cb7-416f-acb2-42e463616a17.png)'
  prefs: []
  type: TYPE_IMG
- en: The generator is conditioned on information imposed on **![](img/cd509d39-205f-4d4d-ad26-bc085d848f0b.png)**.
    Similarly, along with passing real and fake images to the discriminator, we also
    pass ![](img/4fcaa738-2b08-4797-864f-5b47e6446731.png) to the discriminator. So,
    the generator generates digit 7 and the discriminator learns to discriminate between
    the real 7 and the fake 7.
  prefs: []
  type: TYPE_NORMAL
- en: We've just learned how to generate a specific digit using CGAN, and yet the
    applications of CGAN do not end here. Assume we need to generate a digit with
    a specific width and height. We can also impose this condition on ![](img/0dbfa7ae-18cf-4ba0-ae28-ef771eadbecd.png)
    and make the GAN to generate any desired image.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function of CGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have noticed, there is not much difference between our vanilla GAN
    and CGAN except that in CGANs, we are concatenating the additional input, which
    is the conditioning variable ![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png)
    with the inputs of the generator and the discriminator. So, the loss function
    for both generator and discriminator is the same as the vanilla GAN with the exception
    that it is conditioned on ![](img/083cafd3-8748-4a75-9744-7cdf44eee8c4.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the loss function of the discriminator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6a23fb2-a1aa-4d68-a8de-abf2f794ee16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function of the generator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3daefbdb-8cb3-4431-924b-2856bf8d53ad.png)'
  prefs: []
  type: TYPE_IMG
- en: The CGAN learns by minimizing the loss function using gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Generating specific handwritten digits using CGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how CGAN works and the architecture of CGAN. To strengthen our
    understanding, now we will learn how to implement CGAN in TensorFlow for generating
    an image of specific handwritten digit, say digit 7.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, Load the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Defining the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generator, *G,* takes the noise, ![](img/3fba73ea-bd4f-4352-8535-f23286d101b8.png),
    and also the conditional variable, ![](img/2076de9c-04e1-47b2-93f1-8d9cd520e9a0.png),
    as an input and returns an image. We define the generator as a simple two- layer
    feed-forward network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate the noise, ![](img/c017b2d2-3c5d-4fe5-a6de-29cf18af21b0.png), and
    the conditional variable, ![](img/c5a5b62a-728c-4a2b-a938-24c2b58fbb0c.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second layer and compute the output with the `tanh` activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Defining discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We know that the discriminator, ![](img/92dd0d61-61e9-4979-bef8-5fa25b08d846.png),
    returns the probability; that is, it will tell us the probability of the given
    image being real. Along with the input image, ![](img/21e9ebf0-fbab-4938-9890-29594707d2ba.png),
    it also takes the conditional variable, ![](img/c9b15d4e-d63d-4776-bbce-9bf5d1544b81.png),
    as an input. We define the discriminator also as a simple two-layer feed-forward
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Concatenate input, ![](img/6b91c2b8-d0ad-4fdd-93c7-2027143d3a78.png) and the
    conditional variable, ![](img/f16638c8-eebc-417a-8500-983bb5984248.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second layer and compute the output with the `sigmoid` activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholder for the input, ![](img/6b8eef0e-1ec8-4195-8e3b-acb7878fc080.png),
    conditional variable, ![](img/03fe94eb-54c9-4a6c-ac36-6613626a2b9d.png), and the
    noise, ![](img/aa946952-416d-46e4-8f61-b4525a754412.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Start the GAN!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we feed the noise, ![](img/7a9ce465-36be-4928-a62e-93fad76038e5.png)
    and the conditional variable, ![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png),
    to the generator, and it will output the fake image, that is, ![](img/718a88c0-6582-4fe1-bd16-212f9204db72.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we feed the real image ![](img/f8310e0e-b680-4cf7-862f-050b010071da.png)
    along with conditional variable, ![](img/92212dc2-38b2-47bf-b2a7-214b4dd230db.png),
    to the discriminator, ![](img/ce7b8ed9-614f-4ddc-b414-da102cdc0e64.png), and get
    the probability of them being real:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we feed the fake image, `fake_x`, and the conditional variable,
    ![](img/6adb17cf-a37e-4c68-9ecf-c9da309fa5fb.png), to the discriminator, ![](img/c4d9de79-ac6a-45f8-afa6-c46ffd2933ab.png),
    and get the probability of them being real:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Computing the loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see how to compute the loss function. It is essentially the same
    as the Vanilla GAN except that we add a conditional variable.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c621f81-f100-4980-9d7a-7d9012be17e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we will implement the first term, that is, ![](img/c5c53cc8-1af4-4ec4-b1df-779b67a80b69.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will implement the second term, ![](img/4c06f526-38fe-4aeb-9332-e3ace5521540.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The final loss can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Generator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The generator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b294fdb0-97f2-463a-82aa-cf250794a4bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generator loss can be implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing the loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to optimize our generator and discriminator. So, we collect the parameters
    of the discriminator and generator as `theta_D` and `theta_G` respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Start training the CGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start the TensorFlow session and initialize the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Define the `batch_size:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the number of epochs and the number of classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the images and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Generate the handwritten digit, 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We set the digit (`label`) to generate as `7`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample images based on the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample the condition that is, digit we want to generate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the generator and compute the generator loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the discriminator and compute the discriminator loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly sample noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the digit we want to generate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the selected digit into a one-hot encoded vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the noise and one hot encoded condition to the generator and generate
    the fake image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the loss of generator and discriminator and plot the generator image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see following plot, the generator has now learned to generate the
    digit 7 instead of generating other digits randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4816e841-0f8b-4f5c-829a-c9c066a3bca1.png)'
  prefs: []
  type: TYPE_IMG
- en: Understanding InfoGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: InfoGAN is an unsupervised version of CGAN. In CGAN, we learned how to condition
    the generator and discriminator to generate the image we want. But how can we
    do that when we have no labels in the dataset? Assume we have an MNIST dataset
    with no labels; how can we tell the generator to generate the specific image that
    we are interested in? Since the dataset is unlabeled, we do not even know about
    the classes present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We know that generators use noise *z* as an input and generate the image. Generators
    encapsulate all the necessary information about the image in the *z* and it is
    called **entangled representation**. It is basically learning the semantic representation
    of the image in *z*. If we can disentangle this vector, then we can discover interesting
    features of our image.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we will split this *z* into two:'
  prefs: []
  type: TYPE_NORMAL
- en: Usual noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code *c*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the code? The code *c* is basically interpretable disentangled information.
    Assuming we have MNIST data, then, code *c1* implies the digit label, code *c2*
    implies the width, *c3* implies the stroke of the digit, and so on. We collectively
    represent them with the term *c*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have *z* and *c*, how can we learn meaningful code *c*? Can we learn
    meaningful code with the image generated from the generator? Say a generator generates
    the image of 7\. Now we can say code *c1 is 7* as we know c1 implies the digit
    label.
  prefs: []
  type: TYPE_NORMAL
- en: But since code can mean anything, say, a label, a width of the digit, stroke,
    rotation angle, and so on—how can we learn what we want? The code *c* will be
    learned based on the choice of the prior. For instance, if we chose a multinomial
    prior for *c*, then our InfoGAN might assign a digit label for *c*. Say, we assign
    a Gaussian prior, then it might assign a rotation angle, and so on. We can also
    have more than one prior.
  prefs: []
  type: TYPE_NORMAL
- en: The distribution for prior *c* can be anything. InfoGAN assigns different properties
    according to the distribution. In InfoGAN, the code *c* is inferred automatically
    based on the generator output, unlike CGAN, where we explicitly specify the *c*.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, we are inferring ![](img/0c7149c6-3be8-4775-b309-45e3be1b8545.png)
    based on the generator output, ![](img/90798245-af2f-48c9-97b5-4f0e5a14b3e3.png).
    But how exactly we are inferring ![](img/4191b48d-7b3c-4407-88fa-23a8fa0dbbda.png)?
    We use a concept from information theory called **mutual information**.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mutual information between two random variables tells us the amount of information
    we can obtain from one random variable through another. Mutual information between
    two random variables *x* and *y* can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd64a09b-fa5f-407f-b4e1-a0b07db0e49a.png)'
  prefs: []
  type: TYPE_IMG
- en: It is basically the difference between the entropy of *y* and the conditional
    entropy of *y* given *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information between code ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    and the generator output ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) tells
    us how much information we can obtain about ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    through ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) . If the mutual information
    *c* and ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png) is high, then we can
    say knowing the generator output helps us to infer *c*. But if the mutual information
    is low, then we cannot infer *c* from the generator output. Our goal is to maximize
    the mutual information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mutual information between code ![](img/ff8204f0-ea05-46a2-a28b-d7f13f38195e.png)
    and the generator output, ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png), can
    be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at the elements of the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77b4d2e1-3669-4218-9056-fcdd47af1e82.png) is the entropy of the code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png) is the conditional entropy
    of the code c given the generator output ![](img/43fe9912-04e5-4bc0-aac5-ce5262e9fc20.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: 'But the problem is, how do we compute ![](img/1375ab50-62de-4d2f-bffe-3dea4a5678ad.png)?
    Because to compute this value, we need to know the posterior, ![](img/ea74a871-385f-4fd1-a422-29c2dbe4076b.png),
    which we don''t know yet. So, we estimate the posterior with the auxiliary distribution,
    ![](img/8197d00c-71c7-43b0-bb9f-bde673eeb1d2.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2a3dbb0-00cd-41d2-b072-2577c999e0b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say ![](img/7cf8508c-5680-44f2-9436-4809e8b458dd.png), then we can deduce
    mutual information as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77307ccf-295e-499c-8c4f-3413deffc80f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can say:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/504b15ff-8dbe-4e7b-8f33-f85da19dc791.png)'
  prefs: []
  type: TYPE_IMG
- en: Maximizing mutual information, ![](img/04461b06-9679-4dfd-8a03-3a8c0dbce1b7.png)
    basically implies we are maximizing our knowledge about *c* given the generated
    output, that is, knowing about one variable through another.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the InfoGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Okay. What is really going on here? why are we doing this? To put it in simple
    terms, we split the input to the generator into two: *z* and *c*. Since both *z*
    and *c* are used for generating the image, they capture the semantic meaning of
    the image. The code *c* gives us the interpretable disentangled information about
    the image. So we try to find *c* given the generator output. However, we can''t
    do this easily since we don''t know the posterior, ![](img/1a09fb91-ad1e-4640-9ee1-7858444eeb75.png),
    so we use an auxiliary distribution, ![](img/59e766d6-4103-4d50-a928-6a24384de828.png),
    to learn *c*.'
  prefs: []
  type: TYPE_NORMAL
- en: This auxiliary distribution is basically another neural network; let's call
    this network as *Q* network. The role of the *Q* network is to predict the likelihood
    of *c* given a generator image *x* and is given by ![](img/8bdc985a-0401-4abe-88ab-4a6af6ac2097.png).
  prefs: []
  type: TYPE_NORMAL
- en: First, we sample *c* from a prior, *p(c)*. Then we concatenate *c* and *z* and
    feed them to the generator. Next, we feed the generator result given by ![](img/36423d4e-7ade-4f6e-87b3-b71551f5b61c.png)
    to the discriminator. We know that the role of the discriminator is to output
    the probability of the given image being real. So, it takes the image generated
    by the generator and returns the probability. Also, the *Q* network takes the
    generated image and returns the estimates of *c* given the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the discriminator *D* and *Q* networks take the generator image and return
    the output so they both share some layers. Since they both share some layers,
    we attach the *Q* network to the discriminator, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbc47881-40df-4e43-a508-96ffd57dfa4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the discriminator returns two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the image being real
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimates of *c,* which is the probability of *c* given the generator image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add the mutual information term to our loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the loss function of the discriminator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a5568d6-8ba3-495a-a04d-3adf366f9206.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function of the generator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c69ce3b-afea-432e-a4b8-e52ac1ee4a26.png)'
  prefs: []
  type: TYPE_IMG
- en: Both of the preceding equations implies we are minimizing the loss of GAN along
    with maximizing the mutual information. Still confused about InfoGANs? Don't worry!
    We will learn about InfoGANs step by step better by implementing them in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing an InfoGAN in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will better understand InfoGANs by implementing them in TensorFlow step by
    step. We will use the MNIST dataset and learn how the InfoGAN infers the code
    ![](img/285c7e36-2c3b-451b-9d4c-ec3e14821693.png) automatically based on the generator
    output. We build an Info-DCGAN; that is, we use convolutional layers in the generator
    and discriminator instead of a vanilla neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will import all the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the leaky ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Defining generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generator ![](img/be85bd9d-7482-49d4-a48e-ec8024aeea1c.png) which takes the
    noise, ![](img/315bac8e-4528-42d4-b2db-971178e21db7.png), and also a variable,
    ![](img/8562e754-dcfe-4432-b70f-d9825a801f78.png), as an input and returns an
    image. Instead of using a fully connected layer in the generator, we use a deconvolutional
    network, just like when we studied DCGANs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'First, concatenate the noise, *z,* and the variable, ![](img/e6c9289c-bc98-4d2e-9085-3d0dad48a5b8.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first layer, which is a fully connected layer with batch normalization
    and ReLU activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second layer, which is also fully connected with batch normalization
    and ReLU activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Flatten the result of the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The third layer consists of **deconvolution**; that is, a transpose convolution
    operation and it is followed by batch normalization and ReLU activations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The fourth layer is another transpose convolution operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply sigmoid function to the result of the fourth layer and get the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Defining the discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We learned that both the discriminator ![](img/ef5d0704-9421-4960-99c5-7a189b57d943.png)
    and *Q* network take the generator image and return the output so they both share
    some layers. Since they both share some layers, we attach the *Q* network to the
    discriminator, as we learned in the architecture of InfoGAN. Instead of using
    fully connected layers in the discriminator, we use a convolutional network, as
    we learned in the discriminator of DCGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first layer, which performs the convolution operation followed by
    a leaky ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We also perform convolution operation in the second layer, which is followed
    by batch normalization and a leaky ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Flatten the result of the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the flattened result to a fully connected layer which is the third layer
    and it is followed by batch normalization and leaky ReLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the discriminator output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As we learned that we attach the *Q* network to the discriminator. Define the
    first layer of the *Q* network that takes the final layer of the discriminator
    as inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second layer of the *Q* network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Estimate *c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the discriminator `logits` and the estimated *c* value as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Define the input placeholders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we define the placeholder for the input, ![](img/1198ce65-9257-470f-9ebe-fa1dd5af2b41.png),
    the noise, ![](img/47af0b1c-6f7b-4b5e-8963-0f17cbdcca52.png), and the code, ![](img/7f206bce-181b-4732-a826-72a1001e9921.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Start the GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we feed the noise, ![](img/541c0403-7faf-4e90-904f-3fabb2287695.png)
    and the code, ![](img/19dc097f-0ef5-4373-9c71-2c13948ea07a.png) to the generator,
    and it will output the fake image according to the equation ![](img/d4bb1f15-c827-442a-b4a3-2e2706b728a9.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we feed the real image, ![](img/c3d7298c-8e7f-4dec-a5ff-4c36a29078c8.png),
    to the discriminator, ![](img/19be0dee-d6ea-4c21-a901-3acd2320a789.png), and get
    the probability that the image being real. Along with this, we also obtain the
    estimate of ![](img/235ed536-2d0f-48d3-88b9-3013d10a46d1.png) for the real image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we feed the fake image to the discriminator and get the probability
    of the image being real and also the estimate of ![](img/5e52ecb7-3193-45c3-b47f-1c66192759cb.png)
    for the fake image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Computing loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see how to compute the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The discriminator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4835bff6-b8a1-4f22-b460-2011ba5ed03d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the discriminator loss of an InfoGAN is same as with a CGAN, implementing
    the discriminator loss is the same as what we learned in the CGAN section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Generator loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The loss function of the generator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82e62838-ebe0-4eab-9f5b-823ec4f35519.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generator loss is implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Mutual information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We subtract **mutual information** from both the discriminator and the generator
    loss. So, the final loss function of discriminator and generator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60d35e00-184e-445d-9314-985adea697e2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/6969ec81-922a-4d37-a3c4-d395a2bee81a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The mutual information can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/288708fe-7ab4-4eb1-84b2-a7b46b2eb6eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we define a prior for ![](img/3db44084-161b-4594-95d0-a12b9a741445.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The entropy of ![](img/db8e2ca2-24ae-4c99-aeee-2fd273971ec8.png) is represented
    as ![](img/2bbf0f0f-c47b-4f20-aaa0-c2dfcb91e0ce.png). We know that the entropy
    is calculated as ![](img/b02529c2-01a4-4ae3-ad88-d9ac0311d711.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The conditional entropy of ![](img/00a39cdd-ada4-44a6-a583-2dcf3de42643.png)
    when ![](img/84c1b4e1-f67d-4298-8f91-860d6ec8ac2f.png) is given is ![](img/68c0e69b-b302-47f4-bb8d-ba72a7fefaad.png).
    The code for the conditional entropy is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The mutual information is given as ![](img/fd79fef3-96a8-4c96-94f5-3d0b50ce42e7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The final loss of the discriminator and the generator is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Optimizing the loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we need to optimize our generator and discriminator. So, we collect the
    parameters of the discriminator and generator as ![](img/ae3bf55a-b765-41d3-8a7f-2040f62b651d.png)
    and ![](img/00357b65-480a-4f02-8551-88c1d95a4c81.png) respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the loss using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Beginning training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Define the batch size and the number of epochs and initialize all the TensorFlow
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a helper function for visualizing results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Generating handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Start training and generate the image. For every `100` iterations, we print
    the image generated by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample the value of *c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample noise *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the loss of the generator and the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the generator image for every 100^(th) iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how the generator is evolving on each iteration and generating better
    digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/302b0cac-d211-4e5c-b999-7fc25215d834.png)'
  prefs: []
  type: TYPE_IMG
- en: Translating images using a CycleGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned several types of GANs, and the applications of them are endless.
    We have seen how the generator learns the distribution of real data and generates
    new realistic samples. We will now see a really different and very innovative
    type of GAN called the **CycleGAN**.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other GANs, the CycleGAN maps the data from one domain to another domain,
    which implies that here we try to learn the mapping from the distribution of images
    from one domain to the distribution of images in another domain. To put it simply,
    we translate images from one domain to another.
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean? Assume we want to convert a grayscale image to a colored
    image. The grayscale image is one domain and the colored image is another domain.
    A CycleGAN learns the mapping between these two domains and translates between
    them. This means that given a grayscale image, a CycleGAN converts the image into
    a colored one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applications of CycleGANs are numerous, such as converting real photos to artistic
    pictures, season transfer, photo enhancement, and many more. As shown in the following
    figure, you can see how a CycleGAN converts images between different domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24bfb704-2d29-46b4-ad20-2bfbb147825c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But what is so special about CycleGANs? It''s their ability to convert images
    from one domain to another without any paired examples. Let''s say we are converting
    photos (source) to paintings (target). In a normal image-to-image translation,
    how do we that? We prepare the training data by collecting some photos and also
    their corresponding paintings in pairs, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0f516ad-affa-420a-9e51-175f6100a9b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Collecting these paired data points for every use case is an expensive task,
    and we might not have many records or pairs. Here is where the biggest advantage
    of a CycleGAN lies. It doesn't require data in aligned pairs. To convert from
    photos to paintings, we just need a bunch of photos and a bunch of paintings.
    They don't have to map or align with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following figure, we have some photos in one column and some
    paintings in another column; as you can see, they are not paired with each other.
    They are completely different images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caffee85-96ab-46a7-aef3-6779d717a2cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, to convert images from any source domain to the target domain, we just
    need a bunch of images in both of the domains and it does not have to be paired.
    Now let's see how they work and how they learn the mapping between the source
    and the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other GANs, the CycleGAN consists of two generators and two discriminators.
    Let's represent an image in the source domain by ![](img/e6cbd39c-20ee-4b2e-96c5-b068afd8734d.png)
    and target domain by ![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png). We need
    to learn the mapping between ![](img/519f9f4e-65c0-4e32-a83c-9bf20af36f52.png)
    and ![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we are learning to convert a real picture, ![](img/60960d1b-d4e6-4b9c-87e1-8ee02750cd75.png),
    to a painting, ![](img/d61d0722-fdd6-4331-a4c1-2becae65b971.png), as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57884722-f016-4486-adea-5228202949a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Role of generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have two generators, ![](img/f2387305-924d-40a4-9d6a-3a575d4c4a91.png) and
    ![](img/f26d3b48-e48a-47e2-9b52-3b893a2ca07e.png). The role of ![](img/4ae0c2af-662a-4c7c-8d91-3049e76f238e.png)
    is to learn the mapping from ![](img/c74866d7-29b8-4604-b4cc-282c077598f3.png)
    to ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png). As mentioned above, the
    role of ![](img/884f31f8-643c-4b27-845f-0b4d6996a1b5.png) is to learn to translate
    photos to paintings, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/591401f1-c8e0-4d69-ba80-a805d400bbd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It tries to generate a fake target image, which implies it takes a source image,
    ![](img/39408d7b-f87b-4bd0-b3ed-d9f192d9d5cd.png), as input and generates a fake
    target image, ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a62a24b-575c-4bf3-8cb5-5aaecba7e7e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The role of the generator, ![](img/1b2e1c2d-6249-4a04-b264-4ff7d01d97ff.png),
    is to learn the mapping from ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png)
    to ![](img/a4872b2a-894e-4f82-9517-fbf12be37e3a.png) and learn to translate from
    the painting to a real picture, as shown in following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2572f4c-4e50-405a-aff3-7f8b4eeb3799.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It tries to generate a fake source image, which implies it takes a target image,
    ![](img/ef3db299-58f4-4208-ae0b-8c07c12d508f.png), as input and generates a fake
    source image, ![](img/a92caa9a-5f73-4ac8-b8cc-b1d5fa0df87c.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/545b880a-c12b-48db-95a9-66280805e4ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Role of discriminators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the generators, we have two discriminators, ![](img/80d3f113-745d-4b0f-b80f-be4a1819cd4f.png)
    and ![](img/96eaeabb-208c-4166-b365-7573fc3e077f.png). The role of the discriminator ![](img/ff3805fa-fe28-45ba-91b5-e4a966d8e749.png)
    is to discriminate between the real source image, ![](img/155da74d-cd57-4190-b1f6-a24d9214df6a.png),
    and the fake source image ![](img/24043c10-de75-4b46-bba4-dec621e86dc9.png). We
    know that the fake source image is generated by the generator ![](img/d7a67cf6-df09-41fb-8099-9b079109065d.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an image to discriminator ![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png),
    it returns the probability of the image being a real source image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2370055-f57b-4877-9ee3-222b56144bda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows the discriminator ![](img/8b0e71c7-60a7-4086-8990-112cdb195c92.png),
    as you can observe it takes the real source image x and the fake source image
    generated by the generator F as inputs and returns the probability of the image
    being a real source image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61193748-e404-42e8-bc74-7a5fe90a4e30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The role of discriminator ![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png)
    is to discriminate between the real target image, ![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png),
    and the fake target image, ![](img/e9cf6ebe-aea1-4a4f-a472-11f5fd14d49f.png).
    We know that fake target image is generated by the generator, ![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png).
    Given an image to discriminator ![](img/88040175-da0f-4cfa-ba2e-3f20aaa8e923.png),
    it returns the probability of the image being a real target image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/927703eb-3676-40a3-afd4-5c1640434ff7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure shows the discriminator ![](img/37759a1d-ab99-4bfd-a06e-f47a84bbc54e.png),
    as you can observe it takes the real target image, ![](img/0305c069-b0b7-4c4d-9d5a-6af14c84f7a6.png)
    and the fake target image generated by the generator, ![](img/e6b06a64-0b28-4950-8d3f-4fd5a151b5ca.png)
    as inputs and returns the probability of the image being a real target image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c26c935-7df9-4449-9337-db63fb930635.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In CycleGANs, we have two generators and two discriminators. Generators learn
    to translate images from one domain to another and a discriminator tries to discriminate
    between the translated images.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we can say the loss function of discriminator ![](img/df0890f7-556a-4a6b-acde-ba0debf5b68d.png)
    can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/447e0f80-e9ca-4e2a-b5df-31ee82a9250e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the loss function of discriminator ![](img/dbc8900b-9e03-420c-b703-d61ecdabd2cb.png)
    can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d48690d-01f6-42fc-a996-5c8e1f47103e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function of generator ![](img/171a85e6-72fc-44ca-813e-50e907dd78aa.png)
    can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e50dae88-b189-429d-9e31-ef711f415e4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The loss function of generator ![](img/5666a423-e13a-481e-9f41-42277b0ee5b4.png)
    can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b7edf58-4bda-435f-b4e3-87cdfd384ec2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Altogether, the final loss can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cabb8433-6bad-4d41-95ce-c60c6ab50bff.png)'
  prefs: []
  type: TYPE_IMG
- en: Cycle consistency loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The adversarial loss alone does not ensure the proper mapping of the images.
    For instance, a generator can map the images from the source domain to a random
    permutation of images in the target domain which can match the target distribution.
  prefs: []
  type: TYPE_NORMAL
- en: So, to avoid this, we introduce an additional loss called **cycle consistent
    lo****ss**. It enforces both generators *G* and *F* to be cycle-consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recollect the function of the generators:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generator *G***: Converts *x* to *y*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generator *F***: Converts *y* to *x*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We know that generator *G* takes the source image *x* and converts it to a fake
    target image *y*. Now if we feed this generated fake target image *y* to generator
    *F*, it has to return the original source image *x*. Confusing, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following figure; we have a source image, *x*. First, we feed this
    image to generator *G*, and it returns the fake target image. Now we take this
    fake target image, *y*, and feed it to generator *F*, which has to return the
    original source image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2acdf8f9-b887-4af7-a557-2ecf88bb42fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The above equation can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16f10554-ff0c-45c7-bb4e-726914dd8b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is called the **forward consistency loss** and can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3cde238-f5f1-4b51-ab2e-45b3b7e229ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can specify backward consistent loss, as shown in the following
    figure. Let''s say we have an original target image, *y*. We take this *y* and
    feed it to discriminator *F*, and it returns the fake source image *x*. Now we
    feed this fake source image *x* to the generator *G*, and it has to return the
    original target image *y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28cc50f4-c98d-4edb-868b-32c46c7169d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e90a057d-6622-4304-b62f-6544beced8e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The backward consistency loss can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9527c35d-b8c7-48cd-8cb1-b004a7228ca9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, together with a backward and forward consistent loss, we can write the
    cycle consistency loss as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/921caae3-cc6c-4005-8cc1-87399e37c099.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/3fc60b22-7713-48c1-a1a0-9ab1596b4486.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want our generators to cycle consistent so, we multiply their loss with
    the cycle consistent loss. So, the final loss function can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77f55660-3c8a-42e3-a6d8-1eea3b607e54.png)'
  prefs: []
  type: TYPE_IMG
- en: Converting photos to paintings using a CycleGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we will learn how to implement a CycleGAN in TensorFlow. We will see how
    to convert pictures to paintings using a CycleGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c98cbbab-07be-4a1e-ada3-198683726f4a.png)'
  prefs: []
  type: TYPE_IMG
- en: The dataset used in this section can be downloaded from [https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip](https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/monet2photo.zip).
    Once you have downloaded the dataset, unzip the archive; it will consist of four
    folders, `trainA`, `trainB`, `testA`, and `testB`, with training and testing images.
  prefs: []
  type: TYPE_NORMAL
- en: The `trainA` folder consists of paintings (Monet) and the `trainB` folder consists
    of photos. Since we are mapping photos (*x*) to the paintings (*y*), the `trainB`
    folder, which consists of photos, will be our source image, ![](img/1b539a3e-4e00-4e5a-9f26-6f8824318419.png),
    and the `trainA`, which consists of paintings, will be our target image, ![](img/413d8fcb-4439-4bc7-952e-21ec209ba5b7.png).
  prefs: []
  type: TYPE_NORMAL
- en: The complete code for the CycleGAN with a step-by-step explanation is available
    as a Jupyter Notebook at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of looking at the whole code, we will see only how the CycleGAN is implemented
    in TensorFlow and maps the source image to the target domain. You can also check
    the complete code at [https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Hands-On-Deep-Learning-Algorithms-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `CycleGAN` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the placeholders for the input, `X`, and the output, `Y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the generator, ![](img/182c4d15-8ba5-40f8-b249-97c18125d781.png), that
    maps ![](img/56fe4f4b-9eda-4272-8872-10529152edae.png) to ![](img/9c32ba4d-98d2-44d7-aeb6-d768786c4630.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the generator, ![](img/36bef03d-9e31-47c0-82c4-5fca2416aad6.png), that
    maps ![](img/5f3eaf60-fe74-4775-a7d9-3cc407a97c35.png) to ![](img/d45bf48f-aada-4436-ac4f-b125d352e8b1.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the discriminator, ![](img/3a6cab69-81bb-4464-ab61-c35a59115f15.png),
    that discriminates between the real source image and the fake source image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the discriminator, ![](img/356ef7c9-c99f-4961-b689-ac89e4523e90.png),
    that discriminates between the real target image and the fake target image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the fake source image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the fake target image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the `logits`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We know cycle consistency loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b2a7610-0aa3-445a-a9a5-47650edd03e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can implement the cycle consistency loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Define the loss for both of our discriminators, ![](img/3e052ee5-2ec7-44d5-a708-309775749f01.png)
    and ![](img/cc26f199-c253-4dde-8238-3cbe052233ed.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite our loss function of discriminator with Wasserstein distance
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dfe69197-13a3-4fd4-8cc2-6b43783a27f5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/112ce76a-1cec-4244-b827-b6779ad589d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the loss of both the discriminator is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the loss for both of the generators, ![](img/5a278307-2f9e-4d77-997b-5ca957c0ddeb.png)
    and ![](img/89c696f8-ad04-4042-a889-c93eb7505a7d.png). We can rewrite our loss
    function of generators with Wasserstein distance as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/433704ef-29de-4c76-b26c-de00e1561b24.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4a705c83-3217-4ded-8f50-660f415c4073.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the loss of both the generators multiplied with the cycle consistency
    loss, `cycle_loss` is implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Optimize the discriminators and generators using the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we start training the model, we can see how the loss of discriminators
    and generators decreases over the iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: StackGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will see one of the most intriguing and fascinating types of GAN, which
    is called a **StackGAN**. Can you believe if I say StackGANs can generate photo-realistic
    images just based on the textual descriptions? Well, yes. They can do that. Given
    a text description, they can generate a realistic image.
  prefs: []
  type: TYPE_NORMAL
- en: Let's first understand how an artist draws an image. In the first stage, artists
    draw primitive shapes and create a basic outline that forms an initial version
    of the image. In the next stage, they enhance the image by making it more realistic
    and appealing.
  prefs: []
  type: TYPE_NORMAL
- en: StackGANs works in a similar manner. They divide the process of generating images
    into two stages. Just like artists draw pictures, in the first stage, they generate
    a basic outline, primitive shapes, and create a low-resolution version of the
    image, and in the second stage, they enhance the picture generated in the first
    stage by making it more realistic, and then convert them into a high-resolution
    image.
  prefs: []
  type: TYPE_NORMAL
- en: But how do StackGANs do this?
  prefs: []
  type: TYPE_NORMAL
- en: 'They use two GANs, one for each stage. The GAN in the first stage generates
    a basic image and sends it to the GAN in the next stage, which converts basic
    low-resolution image into a proper high-resolution image. The following figure
    shows how StackGANs generate images in each of the stages based on the text description:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d14ffa08-2b92-42b6-9faf-41852ea42806.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://arxiv.org/pdf/1612.03242.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see , in the first stage, we have a low-resolution version of the
    image, but in the second stage, we have good clarity high-resolution image. But,
    still, how StackGAN are doing this? Remember, when we learned with conditional
    GANs that we can make our GAN generate images that we want by conditioning them?
  prefs: []
  type: TYPE_NORMAL
- en: We just use them in both of the stages. In stage one, our network is conditioned
    based on the text description. With this text description, they generate a basic
    version of an image. In stage II, our network is conditioned based on the image
    generated from stage I and also on the text description.
  prefs: []
  type: TYPE_NORMAL
- en: But why do we have to have to condition on the text description again in stage
    II? Because in stage I, we miss some details specified in the text description
    to create a basic version of an image. So, in stage II, we again condition on
    the text description to fix the missing information and also to make our image
    more realistic.
  prefs: []
  type: TYPE_NORMAL
- en: With this ability to generate pictures just based on the text, it is used for
    numerous applications. It is heavily used in the entertainment industry, for instance,
    for creating frames just based on descriptions, and it can also be used for generating
    comics and many more.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of StackGANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how StackGANs work, we will take a
    closer look into their architecture and see how exactly they generate a picture
    from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete architecture of a StackGAN is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75cf1c71-6b88-4b7f-a1e4-f37f5487a4c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: https://arxiv.org/pdf/1612.03242.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: We will look at each component one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have a text description as an input to the GAN. Based on these descriptions,
    it has to generate the images. But how do they understand the meaning of the text
    to generate a picture?
  prefs: []
  type: TYPE_NORMAL
- en: First, we convert the text into an embedding using an encoder. We represent
    this text embedding by ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png). Can
    we create variations of ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png)? By
    creating variations of text embeddings, ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png),
    we can have additional training pairs, and we can also increase the robustness
    to small perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: Let ![](img/3ecca366-0d12-43d0-8ba6-d70d5af45f18.png) be mean and ![](img/00891861-b74f-4d4b-849d-aadc3615125b.png)
    be the diagonal covariance matrix of our text embedding, ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png).
    Now we randomly sample an additional conditioning variable, ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png),
    from the independent Gaussian distribution, ![](img/0cde2c39-1d06-4b38-9ee5-95e83080118f.png).
    It helps us create variations of text descriptions with their meanings. We know
    that same text can be written in various ways, so with the conditioning variable,
    ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png), we can have various versions
    of the text mapping to the image.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, once we have the text description, we will extract their embeddings using
    the encoder, and then we compute their mean and covariance. Then, we sample ![](img/ae68020a-8f9d-4252-9ae7-b7647563f4cb.png)
    from the Gaussian distribution of the text embedding, ![](img/a67e3253-5674-4278-921d-8eefb3a31d98.png).
  prefs: []
  type: TYPE_NORMAL
- en: Stage I
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OK, now we have a text embedding, ![](img/b98b457b-5113-4138-87a1-5ecc312f9ee4.png),
    and also a conditioning variable, ![](img/2d0a756c-3fa1-4604-8921-217706c768a7.png).
    We will see how it is being used to generate the basic version of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that the goal of the generator is to generate a fake image by learning
    the real data distribution. First, we sample noise from a Gaussian distribution
    and create *z*. Then we concatenate *z* with our conditional variable, ![](img/2049be92-cbff-4589-9f92-654889e904da.png),
    and feed this as an input to the generator which outputs a basic version of the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function of the generator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3563799a-e439-4332-81d9-9870c6fb465f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s examine this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8fefbb15-57bb-408f-a6f4-18edae972eec.png) implies we sample z from
    the fake data distribution, that is, the noise prior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/aa262917-373b-423e-b8ff-5295ea45fa41.png)implies we sample the text
    description, ![](img/045cf8e4-5c8b-4dcc-a914-8e2e7bcc15ef.png), from the real
    data distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8a6a0eeb-b335-4e97-b511-4fe7c92f8d02.png) implies that the generator
    takes the noise and the conditioning variable returns the image. We feed this
    generated image to the discriminator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/89856ddf-2b18-4581-a8e3-4d502f407bf8.png) implies the log probability
    of the generated image being fake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with this loss, we also add a regularizer term, ![](img/fa97f5c1-7daa-4454-b679-108d3f13d2c5.png),
    to our loss function, which implies the KL divergence between the standard Gaussian
    distribution and the conditioning Gaussian distribution. It helps us to avoid
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, our final loss function for the generator becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f75101c3-d7b2-4b82-b0fb-027f1e3cf0f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we feed this generated image to the discriminator, which returns the probability
    of the image being real. The discriminator loss is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff6aed90-dd55-40f7-9541-dfd28d64318d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf8d3d28-dce2-4c91-9fb5-d57329b166e1.png) implies the real image, ![](img/79417429-af4b-4492-86f7-dbe46ae3762a.png),
    conditional on the text description, ![](img/788b3eff-5b35-41c9-b2d4-03cecadd3ba2.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/59e7da3c-1310-432d-8855-869dc1caa9bd.png) implies the generated fake
    image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stage II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned how the basic version of the image is generated in stage I.
    Now, in stage II, we fix the defects of the image produced in stage I and generate
    a more realistic version of the image. We condition our network with the image
    generated from the previous stage and also on the text embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of taking noise as an input, the generator in stage II takes the image
    generated from the previous stage as an input and it is conditioned on the text
    description.
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![](img/dfdca6f9-f1c9-4754-bb16-1dd14a2485c4.png) implies that we are
    sampling ![](img/4412e70a-76f9-42cf-a621-441ef7c58ff1.png) from the ![](img/385a1be2-69ec-4ebd-bdd7-eade9473467c.png).
    It basically means that we are sampling the image generated from stage I.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/532013c8-a184-4d77-b833-45398433ea46.png) implies that we are sampling
    text from the given real data distribution, ![](img/5ebebb89-77fd-4d4b-b3e3-7dfcde29d765.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the generator loss can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a59d6ec-a0dc-4b5b-a7cc-0b37e179e788.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Along with the regularizer, our loss function of the generator becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecc0d204-c26a-429a-802d-bca587641079.png)'
  prefs: []
  type: TYPE_IMG
- en: Discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to tell us whether the image is from the real
    distribution or the generator distribution. Thus, the loss function of the discriminator
    is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f51864d9-cdd0-4a47-a689-6c2f3823d189.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by learning about conditional GANs and how they can be
    used to generate our image of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about InfoGANs, where the code *c* is inferred automatically
    based on the generated output, unlike CGAN, where we explicitly specify *c*. To
    infer *c*, we need to find the posterior, ![](img/6681a392-47b2-4fde-b928-ea3e9dd3f0e7.png),
    which we don't have access to. So, we use an auxiliary distribution. We used mutual
    information to maximize the mutual information, ![](img/30eec497-b316-4712-8e24-0cc194eb757e.png),
    to maximize our knowledge about *c* given the generator output.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned about CycleGANs, which map the data from one domain to another
    domain. We tried to learn the mapping from the distribution of images from photos
    domain to the distribution of images in paintings domain. Finally, we understood
    how StackGANs generate photorealistic images from a text description.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about **autoencoders** and their types.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions to gauge how much you have learned from this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: How do conditional GANs differ from vanilla GANs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the code called in InfoGAN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is mutual information?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need auxiliary distribution in InfoGANs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is cycle consistency loss?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the role of generators in a CycleGAN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do StackGANs convert text descriptions into pictures?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following links for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Conditional Generative Adversarial Nets* by Mehdi Mirza and Simon Osindero,
    [https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*InfoGAN: Interpretable Representation Learning by Information Maximizing Generative
    Adversarial Nets* by Xi Chen et al., [https://arxiv.org/pdf/1606.03657.pdf](https://arxiv.org/pdf/1606.03657.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks*
    by Jun-Yan Zhu et al., [https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
