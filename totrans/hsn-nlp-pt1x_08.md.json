["```py\nwith open(\"sentiment labelled sentences/sentiment.txt\") as f:\n    reviews = f.read()\n\ndata = pd.DataFrame([review.split('\\t') for review in                      reviews.split('\\n')])\ndata.columns = ['Review','Sentiment']\ndata = data.sample(frac=1)\n```", "```py\ndef split_words_reviews(data):\n    text = list(data['Review'].values)\n    clean_text = []\n    for t in text:\n        clean_text.append(t.translate(str.maketrans('', '',                   punctuation)).lower().rstrip())\n    tokenized = [word_tokenize(x) for x in clean_text]\n    all_text = []\n    for tokens in tokenized:\n        for t in tokens:\n            all_text.append(t)\n    return tokenized, set(all_text)\nreviews, vocab = split_words_reviews(data)\nreviews[0]\n```", "```py\ndef create_dictionaries(words):\n    word_to_int_dict = {w:i+1 for i, w in enumerate(words)}\n    int_to_word_dict = {i:w for w, i in word_to_int_dict.                            items()}\n    return word_to_int_dict, int_to_word_dict\nword_to_int_dict, int_to_word_dict = create_dictionaries(vocab)\nint_to_word_dict\n```", "```py\nprint(np.max([len(x) for x in reviews]))\nprint(np.mean([len(x) for x in reviews]))\n```", "```py\ndef pad_text(tokenized_reviews, seq_length):\n\n    reviews = []\n\n    for review in tokenized_reviews:\n        if len(review) >= seq_length:\n            reviews.append(review[:seq_length])\n        else:\n            reviews.append(['']*(seq_length-len(review)) +                    review)\n\n    return np.array(reviews)\npadded_sentences = pad_text(reviews, seq_length = 50)\npadded_sentences[0]\n```", "```py\nint_to_word_dict[0] = ''\nword_to_int_dict[''] = 0\n```", "```py\nencoded_sentences = np.array([[word_to_int_dict[word] for word in review] for review in padded_sentences])\nencoded_sentences[0]\n```", "```py\nclass SentimentLSTM(nn.Module):\n\n    def __init__(self, n_vocab, n_embed, n_hidden, n_output,    n_layers, drop_p = 0.8):\n        super().__init__()\n\n        self.n_vocab = n_vocab  \n        self.n_layers = n_layers \n        self.n_hidden = n_hidden \n```", "```py\n       self.embedding = nn.Embedding(n_vocab, n_embed)\n        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers,                        batch_first = True, dropout = drop_p)\n        self.dropout = nn.Dropout(drop_p)\n        self.fc = nn.Linear(n_hidden, n_output)\n        self.sigmoid = nn.Sigmoid()\n```", "```py\n def forward (self, input_words):\n\n        embedded_words = self.embedding(input_words)\n        lstm_out, h = self.lstm(embedded_words) \n        lstm_out = self.dropout(lstm_out)\n        lstm_out = lstm_out.contiguous().view(-1,                             self.n_hidden)\n        fc_out = self.fc(lstm_out)                  \n        sigmoid_out = self.sigmoid(fc_out)              \n        sigmoid_out = sigmoid_out.view(batch_size, -1)  \n\n        sigmoid_last = sigmoid_out[:, -1]\n\n        return sigmoid_last, h\n```", "```py\n    def init_hidden (self, batch_size):\n\n        device = \"cpu\"\n        weights = next(self.parameters()).data\n        h = (weights.new(self.n_layers, batch_size,\\\n                 self.n_hidden).zero_().to(device),\\\n             weights.new(self.n_layers, batch_size,\\\n                 self.n_hidden).zero_().to(device))\n\n        return h\n```", "```py\nn_vocab = len(word_to_int_dict)\nn_embed = 50\nn_hidden = 100\nn_output = 1\nn_layers = 2\nnet = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n```", "```py\nlabels = np.array([int(x) for x in data['Sentiment'].values])\n```", "```py\ntrain_ratio = 0.8\nvalid_ratio = (1 - train_ratio)/2\n```", "```py\ntotal = len(encoded_sentences)\ntrain_cutoff = int(total * train_ratio)\nvalid_cutoff = int(total * (1 - valid_ratio))\ntrain_x, train_y = torch.Tensor(encoded_sentences[:train_cutoff]).long(), torch.Tensor(labels[:train_cutoff]).long()\nvalid_x, valid_y = torch.Tensor(encoded_sentences[train_cutoff : valid_cutoff]).long(), torch.Tensor(labels[train_cutoff : valid_cutoff]).long()\ntest_x, test_y = torch.Tensor(encoded_sentences[valid_cutoff:]).long(), torch.Tensor(labels[valid_cutoff:])\ntrain_data = TensorDataset(train_x, train_y)\nvalid_data = TensorDataset(valid_x, valid_y)\ntest_data = TensorDataset(test_x, test_y)\n```", "```py\nbatch_size = 1\ntrain_loader = DataLoader(train_data, batch_size = batch_size,                          shuffle = True)\nvalid_loader = DataLoader(valid_data, batch_size = batch_size,                          shuffle = True)\ntest_loader = DataLoader(test_data, batch_size = batch_size,                         shuffle = True)\n```", "```py\nprint_every = 2400\nstep = 0\nn_epochs = 3\nclip = 5  \ncriterion = nn.BCELoss()\noptimizer = optim.Adam(net.parameters(), lr = 0.001)\n```", "```py\nfor epoch in range(n_epochs):\n    h = net.init_hidden(batch_size)\n\n    for inputs, labels in train_loader:\n        step += 1  \n        net.zero_grad()\n        output, h = net(inputs)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm(net.parameters(), clip)\n        optimizer.step()\n```", "```py\nif (step % print_every) == 0:            \n            net.eval()\n            valid_losses = []\n            for v_inputs, v_labels in valid_loader:\n\n                v_output, v_h = net(v_inputs)\n                v_loss = criterion(v_output.squeeze(),                                    v_labels.float())\n                valid_losses.append(v_loss.item())\n            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n                  \"Step: {}\".format(step),\n                  \"Training Loss: {:.4f}\".format(loss.item()),\n                  \"Validation Loss: {:.4f}\".format(np.                                     mean(valid_losses)))\n            net.train()\n```", "```py\ntorch.save(net.state_dict(), 'model.pkl')\n```", "```py\nnet.eval()\ntest_losses = []\nnum_correct = 0\nfor inputs, labels in test_loader:\n    test_output, test_h = net(inputs)\n    loss = criterion(test_output, labels)\n    test_losses.append(loss.item())\n\n    preds = torch.round(test_output.squeeze())\n    correct_tensor = preds.eq(labels.float().view_as(preds))\n    correct = np.squeeze(correct_tensor.numpy())\n    num_correct += np.sum(correct)\n\nprint(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\nprint(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))    \n```", "```py\ndef preprocess_review(review):\n    review = review.translate(str.maketrans('', '',                    punctuation)).lower().rstrip()\n    tokenized = word_tokenize(review)\n    if len(tokenized) >= 50:\n        review = tokenized[:50]\n    else:\n        review= ['0']*(50-len(tokenized)) + tokenized\n\n    final = []\n\n    for token in review:\n        try:\n            final.append(word_to_int_dict[token])\n\n        except:\n            final.append(word_to_int_dict[''])\n\n    return final\n```", "```py\ndef predict(review):\n    net.eval()\n    words = np.array([preprocess_review(review)])\n    padded_words = torch.from_numpy(words)\n    pred_loader = DataLoader(padded_words, batch_size = 1,                             shuffle = True)\n    for x in pred_loader:\n        output = net(x)[0].item()\n\n    msg = \"This is a positive review.\" if output >= 0.5 else           \"This is a negative review.\"\n    print(msg)\n    print('Prediction = ' + str(output))\n```", "```py\npredict(\"The film was good\")\n```", "```py\npredict(\"It was not good\")\n```", "```py\nheroku login\n```", "```py\nheroku create sentiment-analysis-flask-api\n```", "```py\nmkdir flaskAPI\ncd flaskAPI\n```", "```py\npython3 -m venv vir_env\n```", "```py\npip install nltk pandas numpy torch flask gunicorn\n```", "```py\npip freeze > requirements.txt\n```", "```py\n**https://download.pytorch.org/whl/cpu/torch-1.3.1%2Bcpu-cp37-cp37m-linux_x86_64.whl**\n```", "```py\ntouch app.py\ntouch Procfile\ntouch wsgi.py\nmkdir models\n```", "```py\n    import flask\n    from flask import Flask, jsonify, request\n    import json\n    import pandas as pd\n    from string import punctuation\n    import numpy as np\n    import torch\n    from nltk.tokenize import word_tokenize\n    from torch.utils.data import TensorDataset, DataLoader\n    from torch import nn\n    from torch import optim\n    app = Flask(__name__)\n    @app.route('/predict', methods=['GET'])\n    ```", "```py\n    with open('models/word_to_int_dict.json') as handle:\n    word_to_int_dict = json.load(handle)\n    ```", "```py\n    model = SentimentLSTM(5401, 50, 100, 1, 2)\n    model.load_state_dict(torch.load(\"models/model_nlp.pkl\"))\n    ```", "```py\n    request_json = request.get_json()\n    i = request_json['input']\n    words = np.array([preprocess_review(review=i)])\n    ```", "```py\n    output = model(x)[0].item()\n    response = json.dumps({'response': output})\n    \treturn response, 200\n    ```", "```py\n    from app import app as application\n    if __name__ == \"__main__\":\n        application.run()\n    ```", "```py\n    web: gunicorn app:app --preload\n    ```", "```py\ngunicorn --bind 0.0.0.0:8080 wsgi:application -w 1\n```", "```py\ncurl -X GET http://0.0.0.0:8080/predict -H \"Content-Type: application/json\" -d '{\"input\":\"the film was good\"}'\n```", "```py\ngit init\n```", "```py\nvir_env\n__pycache__/\n.DS_Store\n```", "```py\ngit add . -A \ngit commit -m 'commit message here'\ngit push heroku master\n```", "```py\ncurl -X GET https://sentiment-analysis-flask-api.herokuapp.com/predict -H \"Content-Type: application/json\" -d '{\"input\":\"the film was good\"}'\n```"]