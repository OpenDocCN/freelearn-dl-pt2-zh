- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: 'NLP 2.0: Using Transformers to Generate Text'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP 2.0：使用 Transformers 生成文本
- en: As we saw in the previous chapter, the NLP domain has seen some remarkable leaps
    in the way we understand, represent, and process textual data. From handling long-range
    dependencies/sequences using LSTMs and GRUs to building dense vector representations
    using word2vec and friends, the field in general has seen drastic improvements.
    With word embeddings becoming almost the de facto representation method and LSTMs
    as the workhorse for NLP tasks, we were hitting some roadblocks in terms of further
    enhancement. This setup of using embeddings with LSTM made the best use of encoder-decoder
    (and related architectures) style models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中看到的，NLP 领域在我们理解、表示和处理文本数据的方式上取得了一些显著的飞跃。从使用 LSTMs 和 GRUs 处理长距离依赖/序列到使用
    word2vec 和相关技术构建密集向量表示，该领域整体上取得了显著的改进。随着词嵌入几乎成为事实上的表示方法，以及 LSTMs 成为 NLP 任务的主力军，我们在进一步增强方面遇到了一些障碍。将嵌入与
    LSTM 结合使用的这种设置充分利用了编码器-解码器（以及相关体系结构）风格模型。
- en: 'We saw briefly in the previous chapter how certain improvements were achieved
    due to the research and application of CNN-based architectures for NLP use cases.
    In this chapter, we will touch upon the next set of enhancements that led to the
    development of current state-of-the-art transformer architectures. We will focus
    on:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中简要地看到了由于基于 CNN 的体系结构在 NLP 用例中的研究和应用而实现的某些改进。在本章中，我们将涉及导致当前最先进的 transformer
    架构开发的下一组增强功能。我们将重点关注：
- en: An overview of attention and how transformers changed the NLP landscape
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力的概述以及 transformers 如何改变 NLP 领域
- en: The GPT series of models, with a step-by-step guide to preparing a text-generation
    pipeline based on GPT-2
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT 系列模型，提供基于 GPT-2 的文本生成流程的逐步指南
- en: We will be covering topics such as attention, self-attention, contextual embeddings,
    and finally transformer architectures.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖注意力、自注意力、上下文嵌入，最后是 transformer 架构等主题。
- en: 'All code snippets presented in this chapter can be run directly in Google Colab.
    For reasons of space, import statements for dependencies have not been included,
    but readers can refer to the GitHub repository for the full code: [https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中呈现的所有代码片段都可以直接在 Google Colab 中运行。由于空间原因，未包含依赖项的导入语句，但读者可以参考 GitHub 存储库获取完整的代码：[https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2](https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2)。
- en: Let's first turn our attention to attention.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先把注意力转向注意力。
- en: Attention
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力
- en: The LSTM-based architecture we used to prepare our first language model for
    text generation had one major limitation. The RNN layer (generally speaking, it
    could be LSTM, or GRU, etc.) takes in a context window of a defined size as input
    and encodes all of it into a single vector. This bottleneck vector needs to capture
    a lot of information in itself before the decoding stage can use it to start generating
    the next token.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于准备第一个文本生成语言模型的基于 LSTM 的架构存在一个主要限制。RNN 层（一般来说，可能是 LSTM 或 GRU 等）以定义大小的上下文窗口作为输入，并将其全部编码为单个向量。在解码阶段可以使用它开始生成下一个标记之前，这个瓶颈向量需要在自身中捕获大量信息。
- en: 'Attention is one of the most powerful concepts in the deep learning space that
    really changed the game. The core idea behind the attention mechanism is to make
    use of all interim hidden states of the RNN to decide which one to focus upon
    before it is used by the decoding stage. A more formal way of presenting attention
    is:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是深度学习空间中最强大的概念之一，真正改变了游戏规则。注意力机制背后的核心思想是在解码阶段使用之前利用 RNN 的所有中间隐藏状态来决定要关注哪个。更正式地表达注意力的方式是：
- en: Given a vector of values (all the hidden states of the RNN) and a query vector
    (this could be the decoder state), attention is a technique to compute a weighted
    sum of the values, dependent on the query.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 给定一组值的向量（所有 RNN 的隐藏状态）和一个查询向量（这可能是解码器的状态），注意力是一种计算值的加权和的技术，依赖于查询。
- en: The weighted sum acts as a selective summary of the information contained in
    the hidden states (value vectors) and the query decides which values to focus
    on. The roots of the attention mechanism can be found in the research associated
    with **Neural Machine Translation** (**NMT**) architectures. NMT models particularly
    struggled with alignment issues and this is where attention greatly helped. For
    instance, translation of a sentence from English to French may not match words
    one-to-one. Attention is not limited to NMT use cases only and is widely used
    across other NLP tasks, such as text generation and classification.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 加权和作为隐藏状态（值向量）中包含的信息的选择性摘要，而查询决定了要关注哪些值。注意机制的根源可以在与**神经机器翻译**（**NMT**）架构相关的研究中找到。NMT模型特别在对齐问题上遇到困难，而注意力在这方面大大帮助了。例如，从英语翻译成法语的句子可能不是单词一对一匹配的。注意力不仅限于NMT用例，而且广泛应用于其他NLP任务，如文本生成和分类。
- en: The idea is pretty straightforward, but how do we implement and use it? *Figure
    10.1* depicts a sample scenario of how an attention mechanism works. The figure
    demonstrates an unrolled RNN at time step *t*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法非常简单，但我们如何实现和使用它呢？*图10.1*展示了注意力机制的工作示例。图表展示了时间步*t*上展开的RNN。
- en: '![Diagram  Description automatically generated](img/B16176_10_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B16176_10_01.png)'
- en: 'Figure 10.1: A simple RNN with an attention mechanism'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：带有注意机制的简单RNN
- en: 'Referring to the figure, let us understand step-by-step how attention is calculated:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提到图表，我们来逐步了解注意力是如何计算的：
- en: Let the RNN encoder hidden states be denoted as *h*[1], *h*[2] …, *h*[N] and
    the current output vector as *s*[t].
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让RNN编码器隐藏状态表示为*h*[1]、*h*[2]…、*h*[N]，当前输出向量为*s*[t]。
- en: We first calculate the *attention score* *e*^t for time step *t* as:![](img/B16176_10_001.png)
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们计算时间步*t*的*注意分数* *e*^t如下：![](img/B16176_10_001.png)
- en: This step is also called the alignment step.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这一步也被称为对齐步骤。
- en: 'We then transform this score into the *attention distribution*: ![](img/B16176_10_002.png).'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将这个分数转换成*注意力分布*：![](img/B16176_10_002.png)。
- en: Using the softmax function helps us to transform the score into a probability
    distribution that sums to 1.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用softmax函数帮助我们将分数转换为总和为1的概率分布。
- en: The final step is to calculate the attention vector, denoted as *a*[t], also
    called a context vector, by taking a weighted sum of encoder hidden states with
    ![](img/B16176_10_003.png):![](img/B16176_10_004.png)
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是计算注意力向量*a*[t]，也称为上下文向量，方法是将编码器隐藏状态加权求和：![](img/B16176_10_003.png):![](img/B16176_10_004.png)
- en: Once we have the attention vector, we can then simply concatenate it with the
    decoder state vector from the previous time step and continue to decode the vector
    as previously.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了注意向量，我们就可以简单地将其与先前时间步的解码器状态向量连接起来，并像以前一样继续解码向量。
- en: 'Different variants of the attention mechanism have been explored by various
    researchers so far. A couple of important points to note are:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，各种研究人员已经探索了注意机制的不同变体。需要注意的一些重要点包括：
- en: The aforementioned steps for the attention calculation are the same across all variants.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意计算的前述步骤在所有变体中都相同。
- en: However, the difference lies in the way the attention score (denoted as *e*^t)
    is calculated.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，区别在于计算注意力分数（表示为*e*^t）的方式。
- en: Widely used attention scoring functions are content-based attention, additive
    attention, dot-product, and scaled dot-product. Readers are encouraged to explore
    these further for better understanding.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的注意力评分函数有基于内容的注意力，加法注意力，点积和缩放的点积。鼓励读者进一步探索以更好地了解这些。
- en: Contextual embeddings
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文嵌入
- en: 'The big leap from BoW-based text representation models to unsupervised dense
    representation techniques such as word2vec, GloVe, fastText, and so on was the
    secret sauce to improve deep learning model performance on NLP tasks. Yet these
    representations had a few limitations, which we''ll remind ourselves of:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 从基于BoW的文本表示模型到无监督的密集表示技术（如word2vec、GloVe、fastText等）的大跨越是改善深度学习模型在NLP任务上表现的秘密武器。然而，这些表示方法也有一些局限性，我们会提醒自己：
- en: Words can have different meanings depending on the context in which they are
    used. These techniques result in the same vector representation regardless of
    the context. This can be rectified a bit by using very strong word-sense disambiguation
    methods (such as the usage of supervised learning algorithms to disambiguate words),
    but inherently, this isn't captured by any of the known techniques.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词的含义取决于使用的上下文。这些技术导致无论上下文如何，都会得到相同的向量表示。虽然可以通过使用非常强大的词义消歧方法（例如使用监督学习算法消除单词歧义）来解决这个问题，但从本质上来讲，这并没有被任何已知技术所捕捉到。
- en: Words can have different uses, semantics, and syntactic behaviors, yet the word
    representation remains the same.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词可以有不同的用法、语义和句法行为，但单词表示保持不变。
- en: 'If we think about this carefully, the architecture that we prepared in the
    previous chapter using LSTMs was trying to solve these issues internally. To elaborate
    further, let us do a quick recap of the architecture we built:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细思考一下，我们在上一章中使用LSTMs准备的架构正试图在内部解决这些问题。为了进一步阐述，让我们快速回顾一下我们建立的架构：
- en: We started off with our input text being transformed into character or word
    embeddings.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们开始将输入文本转换为字符或单词嵌入。
- en: These embeddings were then passed through an LSTM layer (or a stack of LSTM
    layers or even bi-LSTM layers) and the final hidden state was transformed and
    decoded to generate the next token.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，这些嵌入向量通过一个LSTM层（或者一组LSTM层，甚至是双向LSTM层），最终的隐藏状态被转换和解码以生成下一个标记。
- en: While the starting point leverages pre-trained embeddings, which have the same
    representation of any given word in every context, the LSTM layers bring in the
    context. The set of LSTM layers analyzes the sequence of tokens and each layer
    tries to learn concepts related to *language syntax*, *semantics*, and so on.
    This provides the much-required context to each token's (word or character) representation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然起始点利用了预训练嵌入，这些嵌入在每个上下文中具有相同的表示，但LSTM层引入了上下文。一组LSTM层分析令牌序列，每一层都试图学习与*语言句法*、*语义*等相关的概念。这为每个令牌（单词或字符）的表示提供了非常重要的上下文。
- en: The **TagLM** architecture by Peters et al. in 2017¹ was one of the first works
    that provided an insight into how we could combine pre-trained word embeddings
    with a pre-trained neural language model to generate context-aware embeddings
    for downstream NLP tasks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Peters等人在2017年提出的**TagLM**架构是第一批提供见解的工作之一，说明了如何将预训练的词嵌入与预训练的神经语言模型结合起来，为下游NLP任务生成具有上下文意识的嵌入向量。
- en: 'The big breakthrough that changed the NLP landscape came in the form of **ELMo**,
    or **Embeddings from Language Models**. The ELMo architecture was presented by
    Peters et al. in their work titled *Deep Contextualized Word Representations*
    in 2018.² Without going into too much detail, the main highlights of the ELMo
    architecture were:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 改变了NLP领域的巨大突破是**ELMo**，即**来自语言模型的嵌入**。ELMo架构由Peters等人在他们2018年的作品*Deep Contextualized
    Word Representations*中提出。不详细展开，ELMo架构的主要亮点是：
- en: The model used a bi-LSTM-based language model.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型使用基于双向LSTM的语言模型。
- en: Character CNNs were used to generate embeddings, in place of pre-trained word
    vectors, which made use of huge 4096 LSTM units but transformed into smaller 512-sized
    vectors using feedforward layers.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Character CNNs被用来生成嵌入向量，取代了预训练的词向量，这些向量利用了4096个LSTM单元，但通过前向传播层转换成了更小的512大小的向量。
- en: The model made use of residual layers to help carry the gradient between deep
    layers of the architecture. This helped prevent issues like vanishing gradients.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型利用剩余层来帮助在架构的深层之间传递梯度。这有助于防止梯度消失等问题。
- en: The main innovation was to make use of all the hidden bi-LSTM layers for generating
    input representation. Unlike previous works, where only the final LSTM layer was
    used to fetch the representation of the input, this work took a weighted average
    of all the hidden layers' hidden states. This helped the model learn contextual
    word embeddings where each layer contributed to things like syntax and semantics.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要创新之处在于利用所有隐藏的双向LSTM层来生成输入表示。与以前的作品不同，在以前的作品中，只使用最终的LSTM层来获取输入表示，这项工作对所有隐藏层的隐藏状态进行加权平均。这有助于模型学习上下文词嵌入，其中每一层都有助于语法和语义等方面。
- en: The main reason ELMo got so much attention was not the fact that it helped improve
    the performance by a certain factor. The contextual embeddings learned by ELMo
    helped it improve state-of-the-art performance using previous architectures on
    not just a couple of NLP tasks, but *almost all of them* (see paper for details).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ELMo备受关注的原因并不是它帮助提高了性能，而是ELMo学习的上下文嵌入帮助它在以往的架构上改进了最先进的性能，不仅在几个NLP任务上，而且*几乎所有*的任务上（详见论文）。
- en: The **ULMFiT** model by Howard and Ruder in 2018 was based on similar concepts
    and helped introduce, or rather push, widespread application of *transfer learning*
    in the NLP domain.³
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Howard和Ruder在2018年提出的**ULMFiT**模型基于类似的概念，并帮助推广了在NLP领域的*迁移学习*的广泛应用。³
- en: Self-attention
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力
- en: We've already briefly discussed the attention mechanism and its impact on improving
    NLP models in general. In this section, we will talk about a successive improvement
    upon the attention mechanism called self-attention.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经简要讨论了注意力机制及其对改进NLP模型的影响。在本节中，我们将讨论关于注意力机制的后续改进，即自注意力。
- en: Self-attention was proposed by Cheng et al. in their paper titled *Long Short-Term
    Memory Networks for Machine Reading* in 2016.⁴ The concept of self-attention builds
    upon the general idea of attention. Self-attention enables a model to learn the
    correlation between the current token (character or word or sentence, etc.) and
    its context window. In other words, it is an attention mechanism that relates
    different positions of a given sequence so as to generate a representation of
    the same sequence. Imagine this as a way of transforming word embeddings in the
    context of the given sentence/sequence. The concept of self-attention as presented
    in the original paper itself is depicted in *Figure 10.2*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是由程先生等人在他们2016年题为*用于机器阅读的长短期记忆网络*的论文中提出的。⁴ 自注意力概念是建立在注意力的一般思想之上的。自注意力使得模型能够学习当前标记（字符、单词或句子等）与其上下文窗口之间的相关性。换句话说，它是一种注意力机制，相关于给定序列的不同位置，以生成相同序列的表示。可以将其想象为一种将单词嵌入转换为给定句子/序列的方式。原始论文中呈现的自注意力概念如同
    *图10.2* 所示。
- en: '![Table  Description automatically generated](img/B16176_10_02.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![表描述是自动生成的](img/B16176_10_02.png)'
- en: 'Figure 10.2: Self-attention⁴'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：自注意力⁴
- en: Let us try and understand the self-attention output presented in this figure.
    Each row/sentence represents the state of the model at every time step, with the
    current word highlighted in red. Blue represents the attention of the model, with
    the intensity of focus depicted by the shade of blue. Thus, each word in the context
    of the current word gets to contribute to the embeddings of the current word to
    a certain extent.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解这张图中展示的自注意力输出。每一行/句子表示模型在每个时间步的状态，当前单词用红色突出显示。蓝色表示模型的注意力，其集中程度由蓝色的深浅表示。因此，上下文中的每个单词都有一定程度地影响当前单词的嵌入。
- en: 'Interested readers may explore a notebook by the Google Brain team that showcases
    a framework called Tensor2Tensor (now deprecated in favor of JAX). This notebook
    presents an interactive visualization to help understand the concept of self-attention:
    [https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebook).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 感兴趣的读者可以探索Google Brain团队制作的一个展示名为Tensor2Tensor（现已废弃，改用JAX）的框架的 notebook，这个 notebook
    呈现了一个交互式可视化，帮助理解自注意力的概念：[https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebook)。
- en: This concept forms one of the core building blocks of the transformer architecture
    we are about to discuss next.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念是我们即将讨论的Transformer架构的核心构建模块之一。
- en: Transformers
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer
- en: The culmination of concepts such as attention, contextual embeddings, and recurrence-free
    architectures led to what we now call **transformer architectures**. The transformer
    architecture was presented in the seminal paper *Attention is All You Need* by
    Vaswani et al. back in 2017.⁵ This work represented a complete paradigm shift
    in the NLP space; it presented not just a powerful architecture but also a smart
    use of some of the recently developed concepts that helped it beat state-of-the-art
    models by a margin across different benchmarks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意、上下文嵌入和无循环体系结构等概念的结合导致了我们现在所谓的**变压器体系结构**的诞生。变压器体系结构是由瓦斯瓦尼等人于2017年在具有里程碑意义的论文*注意力就是你所需要的*中提出的。⁵
    这项工作代表了自然语言处理领域的完全范式转变；它不仅提出了一个强大的体系结构，还巧妙地利用了一些最近发展的概念，帮助它在不同基准测试中击败了最先进的模型。
- en: 'We will cover the internals of the transformer architecture briefly. For a
    step-by-step explanation, readers may refer to *Illustrated Transformer* by Jay
    Alammar: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍变压器体系结构的内部。欲了解逐步说明，请参阅Jay Alammar的*插图变压器*：[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)。
- en: At the core, a transformer is a recurrence- and convolution-free attention-based
    encoder-decoder architecture. It *solely depends upon the attention mechanism*
    to learn local and global dependencies and thus enables massive parallelization.
    Let us now have a look at the main contributions of this work.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心部分，变压器是一种无循环和无卷积的基于注意力的编码器-解码器架构。它*完全依赖于注意机制*来学习局部和全局依赖关系，从而实现了大规模并行化。现在让我们来看看这项工作的主要贡献。
- en: Overall architecture
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体架构
- en: As mentioned earlier, a transformer is an encoder-decoder architecture at its
    core. Yet unlike known encoder-decoder architectures in the NLP domain, this work
    presented a stacked encoder-decoder setup.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，变压器在其核心是一个编码器-解码器架构。但与NLP领域已知的编码器-解码器体系结构不同，这项工作呈现了一个堆叠的编码器-解码器设置。
- en: '*Figure 10.3* shows the high-level transformer setup.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.3* 展示了高级变压器设置。'
- en: '![Diagram  Description automatically generated](img/B16176_10_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B16176_10_03.png)'
- en: 'Figure 10.3: A high-level schematic of the transformer architecture'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：变压器体系结构的高级示意图
- en: As shown in the figure, the architecture makes use of multiple encoder blocks
    stacked on top of each other. The decoder itself consists of stacked decoding
    blocks, and the final encoder block feeds into each of the decoder blocks. The
    important thing to note here is that neither the encoder nor the decoder blocks
    are comprised of recurrent or convolutional layers. *Figure 10.4 (A)* outlines
    the encoder block and *Figure 10.4 (B)* the decoder block. Dotted lines denote
    residual connections between different sets of layers. The original paper presented
    the transformer architecture with 6 identical encoder blocks and decoder blocks
    each.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，该架构利用多个堆叠在一起的编码器块。解码器本身由堆叠的解码块组成，并且最后一个编码器块馈送到每个解码器块中。这里要注意的重要一点是，编码器和解码器块都不包含循环或卷积层。*图
    10.4 (A)* 概述了编码器块，而 *图 10.4 (B)* 概述了解码器块。虚线表示不同层之间的残差连接。原始论文以相同的6个编码器块和解码器块呈现了变压器体系结构。
- en: '![Diagram  Description automatically generated](img/B16176_10_04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B16176_10_04.png)'
- en: 'Figure 10.4: A) Encoder block, B) Decoder block used in the transformer architecture'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：A) 编码器块，B) 在变压器体系结构中使用的解码器块
- en: The encoder block, as shown in *Figure 10.4 (A)*, consists of a layer for calculating
    self-attention followed by normalization and feed-forward layers. There are skip
    connections between these layers. The decoder block is almost the same as the
    encoder block, with one additional sub-block consisting of self-attention and
    normalization layers. This additional sub-block takes input from the last encoder
    block to ensure that the encoder's attention is propagated to the decoding blocks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器块，如*图 10.4 (A)*所示，包括一个用于计算自注意力的层，然后是归一化和前馈层。这些层之间有跳跃连接。解码器块与编码器块几乎相同，只是多了一个由自注意力和归一化层组成的子块。这个额外的子块从最后一个编码器块获取输入，以确保编码器的注意力传播到解码块。
- en: The first layer in the decoder block carries a slight modification. This multi-head
    self-attention layer is masked for future timesteps/contexts. This ensures the
    model does not attend to future positions of the target while decoding the current
    token. Let's spend a bit more time trying to understand the multi-head self-attention
    component.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器块中的第一层进行了轻微修改。这个多头自注意力层对未来的时间步/上下文进行了屏蔽。这确保了模型在解码当前标记时不会关注目标的未来位置。让我们花点时间来理解多头自注意力组件。
- en: Multi-head self-attention
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多头自注意力
- en: We discussed the concept of self-attention in the previous section. In this
    section, we will discuss how the transformer architecture implements self-attention
    and its related parameters. While presenting the concept of attention, we discussed
    it in terms of the query vector (decoder state, denoted as *q*) and the value
    vectors (the encoder's hidden state, denoted as *v*).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节讨论了自注意力的概念。在本节中，我们将讨论变换器架构如何实现自注意力及其相关参数。在介绍注意力概念时，我们将其描述为查询向量（解码器状态，表示为*q*）和值向量（编码器的隐藏状态，表示为*v*）。
- en: In the case of transformers, this is modified a bit. We make use of encoder
    states or input tokens as both query and value vectors (self-attention) along
    with an additional vector called the *key* vector (denoted as *k*). The key, value,
    and query vectors are of the same dimension in this case.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于变换器而言，这有所修改。我们使用编码器状态或输入标记作为查询和值向量（自注意力），以及一个额外的向量称为*键*向量（表示为*k*）。在这种情况下，键、值和查询向量的维度相同。
- en: 'The transformer architecture makes use of the *scaled dot-product* as its attention
    mechanism. This scoring function is defined as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器架构使用*缩放点积*作为其注意力机制。这个评分函数定义如下：
- en: '![](img/B16176_10_005.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_10_005.png)'
- en: 'where the attention output is calculated first as the dot product *QK*^T between
    the query and key vectors *Q* and *K* (these are actually matrices, but we will
    explain that in a bit). The dot product tries to capture the similarity of the
    query with encoder states, which is then scaled by the square root of the dimension
    *n* of the input vector. This scaling factor is introduced to ensure the gradients
    are propagated properly, since vanishing gradients are observed for large embedding
    vectors. The softmax operation transforms the score into a probability distribution
    summing to 1\. The final step is to calculate the product of the weighted sum
    of the encoder states (the value vector *V* this time) with the output of the
    softmax. This overall operation is depicted in *Figure 10.5*:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中注意力输出首先计算为查询向量*Q*和键向量*K*（实际上这些是矩阵，但我们稍后会解释）的点积*QK*^T。点积试图捕捉查询与编码器状态的相似性，然后乘以输入向量的维度*n*的平方根进行缩放。引入这个缩放因子是为了确保梯度能够正确传播，因为对于大的嵌入向量观察到了梯度消失。Softmax操作将分数转换为总和为1的概率分布。最后一步是计算编码器状态（这次是值向量*V*）的加权和与Softmax输出的乘积。整个操作在*图10.5*中表示：
- en: '![Diagram  Description automatically generated](img/B16176_10_05.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 说明自动生成](img/B16176_10_05.png)'
- en: 'Figure 10.5: (Left) scaled dot product attention, (right) multi-head self-attention
    combining several self-attention layers in parallel.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：（左）缩放点积注意力，（右）多头自注意力，将几个自注意力层并行组合在一起。
- en: In place of using a single attention head per encoder block, the model makes
    use of multiple attention heads in parallel (as depicted in *Figure 10.5 (right)*).
    The authors mention in the paper that "multi-head attention allows the model to
    jointly attend to information from different representation subspaces at different
    positions. With a single attention head, averaging inhibits this." In other words,
    multi-head attention allows the model to learn different aspects of every word
    in the input, that is, one attention head could be capturing the impact of the
    relationships with prepositions, the other one could be focusing on its interactions
    with verbs, and so on. As each attention head would have its own set of *Q*, *K,*
    and *V* vectors, in practice these are implemented as matrices with each row corresponding
    to a specific head.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个编码器块中使用单个注意头的位置，模型使用多个注意头并行进行操作（如 *图 10.5（右）* 所示）。作者在论文中提到，“多头注意力使模型能够同时关注不同位置的不同表示子空间的信息。使用单个注意头，平均会抑制这种情况。”换句话说，多头注意力使模型能够学习输入中每个单词的不同方面，即，一个注意头可能捕捉与介词的关系的影响，另一个可能专注于它与动词的交互作用，依此类推。由于每个注意头都有自己的
    *Q*、*K* 和 *V* 向量集，实际上这些被实现为矩阵，每行对应于一个特定的头。
- en: 'A highly intuitive visual explanation of multi-head self-attention is presented
    here for reference: [https://www.youtube.com/watch?v=-9vVhYEXeyQ&ab_channel=Peltarion](https://www.youtube.com/watch?v=-9vVhYEXeyQ&ab_channel=Peltarion).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了多头自注意的高度直观的可视化解释，供参考：[https://www.youtube.com/watch?v=-9vVhYEXeyQ&ab_channel=Peltarion](https://www.youtube.com/watch?v=-9vVhYEXeyQ&ab_channel=Peltarion)。
- en: One may think that due to the multi-head setup, the number of parameters would
    suddenly blow out of proportion and slow down the training process. To counteract
    this, the authors made use of smaller-dimensional vectors (size 64) by first projecting
    the larger input embeddings into a smaller dimension. They then made use of 8
    heads in the original implementation. This resulted in a final concatenated vector
    (from all attention heads) of the same dimension as it would be with a single
    attention head with a larger input embedding vector. This neat trick helps the
    model capture a lot more semantics in the same space without any impact on the
    overall training speed. The overall transformer architecture uses multiple such
    encoder blocks with each of them containing multi-head attention layers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会认为，由于多头设置，参数数量会突然激增并减慢训练过程。为了抵消这一点，作者们首先将较大的输入嵌入投影到较小的维度向量（大小为 64），然后使用
    8 个头在原始实现中进行操作。这导致最终的连接向量（来自所有注意头）与具有较大输入嵌入向量的单个注意头的维度相同。这个巧妙的技巧帮助模型在相同的空间中捕捉更多的语义，而不会对整体训练速度产生影响。整体变压器架构使用多个这样的编码器块，每个编码器块都包含多头注意层。
- en: Positional encodings
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'The transformer model is devoid of any recurrence or convolutional layers,
    so in order to ensure that the model understands the importance of the sequence
    of the inputs, the concept of *positional embeddings* was used. The authors chose
    to use the following method to generate positional encodings:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器模型不包含任何循环或卷积层，因此为了确保模型理解输入序列的重要性，使用了“位置嵌入”的概念。作者选择使用以下方法生成位置编码：
- en: '![](img/B16176_10_006.png)![](img/B16176_10_007.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16176_10_006.png)![](img/B16176_10_007.png)'
- en: where *pos* is the position of the input token, *i* is the dimension, and *d*[model]
    is the length of the input embedding vector. The authors use sine for even positions
    and cosine for odd ones. The positional encoding vector dimension is kept the
    same as the input vector and both vectors are summed up before they are fed into
    the encoder or decoder blocks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *pos* 是输入令牌的位置，*i* 是维度，*d*[model] 是输入嵌入向量的长度。作者在偶数位置使用正弦，奇数位置使用余弦。位置编码向量的维度与输入向量相同，并且在输入编码器或解码器块之前将两个向量相加。
- en: The combination of multi-head self-attention along with positional encodings
    helps the transformer network build highly contextual representations of input
    sequences. This, along with the completely attention-based architecture, enables
    the transformer to not only beat the state-of-the-art models on a number of benchmarks,
    but also form the basis of a whole family of transformer-based models. In the
    next section, we will briefly touch upon this family of transformers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力机制与位置编码的结合帮助变压器网络构建输入序列的高度上下文表示。这个结合着完全基于注意力机制的架构，使得变压器不仅能够在许多基准测试中超越现有模型，还能够构建一整个基于变压器的模型系列。在下一节中，我们将简要介绍这一系列变压器。
- en: BERT-ology
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT-ology
- en: The transformer architecture ushered in completely unheard-of performance benchmarks
    in the NLP domain. This recurrence-free setup led to research into and the development
    of a whole family of transformer-based architectures. One of the initial and most
    successful ones is the BERT model. **BERT**, or **Bi-Directional Encoder Representations**
    **from Transformers**, was presented by Devlin et al., a team at Google AI in
    2018.⁶
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器架构在自然语言处理领域引入了完全前所未有的性能基准。这种无循环设置导致了对基于变压器的完整系列架构的研究和开发。最初和最成功的其中之一是 BERT
    模型。**BERT**，或者**来自变压器的双向编码器表示**，是由 Google AI 的 Devlin 等人于 2018 年提出的。⁶
- en: The model drastically improved upon the benchmarks set by the transformer model.
    BERT also helped push the transfer-learning envelope in the NLP domain by showcasing
    how a pre-trained model can be fine-tuned for various tasks to provide state-of-the-art
    performance. In computer vision use cases, we can use a large pre-trained network
    such as VGG or ResNet as a feature extractor with a classification layer, or we
    can fine-tune the whole network for the given task. We can do the same thing using
    BERT as well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在变压器模型设定的基准上取得了显著的改进。BERT 还通过展示如何对预训练模型进行微调以提供最先进的性能，推动了自然语言处理领域的迁移学习的发展。在计算机视觉用例中，我们可以使用类似
    VGG 或 ResNet 这样的大型预训练网络作为特征提取器并带有分类层，或者我们可以对给定任务微调整个网络。我们也可以使用 BERT 来实现相同的功能。
- en: The BERT model makes use of a transformer-style encoder with a different number
    of encoder blocks depending on the model size. The authors presented two models,
    BERT-base with 12 blocks and BERT-large with 24 blocks. Both of these models have
    larger feedforward networks (768 and 1024 respectively) and a greater number of
    attention heads (12 and 16 respectively) compared to the original transformer
    setup.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型采用具有不同数量的编码器块的变压器式编码器。作者提出了两个模型，BERT-base 包含 12 个块，BERT-large 包含 24 个块。这两个模型相比原始变压器设置具有更大的前馈网络（分别为
    768 和 1024）和更多的注意力头（分别为 12 和 16）。
- en: 'Another major change from the original transformer implementation is the bi-directional
    masked language model objective. A typical language model ensures causality, that
    is, the decoding process only looks at the past context and not future time steps.
    The authors of BERT tweaked this objective to build context from both directions,
    i.e. the objective of *predicting masked words* along with *next sentence prediction*.
    This is depicted in *Figure 10.6*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始变压器实现的另一个主要变化是双向掩码语言模型目标。典型的语言模型确保因果关系，即解码过程只关注过去的上下文而不是未来的时间步。BERT 的作者调整了这个目标，以从两个方向构建上下文，即
    *预测掩码词* 和 *下一个句子预测*。这在 *图 10.6* 中有所描述：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B16176_10_06.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![Graphical user interface, text, application  Description automatically generated](img/B16176_10_06.png)'
- en: 'Figure 10.6: BERT training objectives of a masked language model and next sentence
    prediction'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：BERT 训练目标，包括掩码语言模型和下一个句子预测
- en: As shown in the figure, the *masked language model* randomly masks out 15% of
    tokens for the training process. They train the model on a huge corpus and then
    fine-tune it for different tasks on GLUE ([https://gluebenchmark.com/](https://gluebenchmark.com/))
    and other related benchmarks. As reported in the paper, the model outperforms
    all previous architectures by a good margin.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，*掩码语言模型*随机掩码了训练过程中的 15% 的标记。他们在大规模语料库上训练模型，然后在 GLUE ([https://gluebenchmark.com/](https://gluebenchmark.com/))
    和其他相关基准上对其进行微调。据报告，该模型在性能上明显优于所有先前的架构。
- en: The success of BERT led to a series of improved models that tweaked certain
    aspects with respect to embeddings, encoder layers, and so on to provide incremental
    performance improvements. Models such as RoBERTa⁷, ALBERT⁸, DistilBERT⁹, XLNet^(10),
    and so on share the core idea and build upon it to provide improvements.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的成功导致一系列改进模型，通过调整嵌入、编码器层等方面的某些方面来提供渐进性能提升。像 RoBERTa⁷、ALBERT⁸、DistilBERT⁹、XLNet^(10)
    等模型共享核心思想并在此基础上提供改进。
- en: As BERT does not conform to causality, it cannot be used for typical language
    modeling tasks such as text generation. In the next section, we will discuss a
    parallel architectural family of transformers from OpenAI.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 BERT 不符合因果关系，因此不能用于典型的语言建模任务，如文本生成。在接下来的章节中，我们将讨论 OpenAI 提出的变压器并行架构系列。
- en: GPT 1, 2, 3…
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT 1, 2, 3…
- en: OpenAI is an AI research group that has been in the spotlight for quite some
    time because of their newsworthy works such as GPT, GPT-2, and the recently released
    GPT-3\. In this section, we will walk through a brief discussion related to these
    architectures and their novel contributions. Toward the end, we will use a pre-trained
    version of GPT-2 for our task of text generation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 是一个人工智能研究团体，由于他们的具有新闻价值的作品，如 GPT、GPT-2 和最近发布的 GPT-3，一直备受关注。在本节中，我们将讨论与这些架构及其新颖贡献相关的简要讨论。最后，我们将使用
    GPT-2 的预训练版本来执行我们的文本生成任务。
- en: 'Generative pre-training: GPT'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '生成式预训练: GPT'
- en: The first model in this series is called **GPT**, or **Generative Pre-Training**.
    It was released in 2018, about the same time as the BERT model. The paper^(11)
    presents a task-agnostic architecture based on the ideas of transformers and unsupervised
    learning. The GPT model was shown to beat several benchmarks such as GLUE and
    SST-2, though the performance was overtaken by BERT, which was released shortly
    after this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系列中的第一个模型被称为**GPT**，或**生成式预训练**。它于 2018 年发布，大约与 BERT 模型同时。该论文^(11) 提出了一个基于变压器和无监督学习思想的任务无关架构。GPT
    模型被证明在 GLUE 和 SST-2 等多个基准测试中取得了胜利，虽然其性能很快被 BERT 超越，后者在此后不久发布。
- en: 'GPT is essentially a language model based on the *transformer-decoder* we presented
    in the previous chapter (see the section on *Transformers*). Since a language
    model can be trained in an unsupervised fashion, the authors of this model made
    use of this unsupervised approach to train on a very large corpus and then fine-tuned
    it for specific tasks. The authors used the **BookCorpus** dataset,^(12) which
    contains over 7,000 unique, unpublished books across different genres. This dataset,
    the authors claim, allows the model to learn long-range information due to the
    presence of long stretches of contiguous text. This is seen to be better than
    the 1B Word Benchmark dataset used by earlier works, which misses out on long-range
    information due to shuffled sentences. The overall GPT setup is depicted in the
    following figure:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 本质上是基于我们在上一章节中提出的*transformer-decoder*的语言模型（参见*Transformers*部分）。由于语言模型可以以无监督方式进行训练，该模型的作者利用了这种无监督方法在非常大的语料库上进行训练，然后针对特定任务进行了微调。作者使用了包含超过
    7,000 本不同流派的独特未发表书籍的**BookCorpus**数据集^(12)。作者声称，该数据集使得模型能够学习到长距离信息，这是由于存在着长串连续文本。这被认为比之前使用的
    1B Word Benchmark 数据集更好，后者由于句子被打乱而丧失了长距离信息。GPT 的整体设置如下图所示：
- en: '![Graphical user interface  Description automatically generated](img/B16176_10_07.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成描述](img/B16176_10_07.png)'
- en: 'Figure 10.7: GPT architecture (left), task-based setup using GPT (right)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.7: GPT 架构（左）, 使用 GPT 的基于任务的设置（右）'
- en: As shown in *Figure 10.7 (left)*, the GPT model is similar to the original transformer-decoder.
    The authors make use of 12 decoder blocks (as opposed to 6 in the original transformer)
    with 768-dimensional states and 12 self-attention heads each. Since the model
    uses masked self-attention, it maintains the causal nature of the language model
    and hence can be used for text generation as well. For the rest of the tasks showcased
    in *Figure 10.7 (right)*, essentially the same pre-trained language model is used
    with minimal task-specific preprocessing of inputs and final task-specific layers/objectives.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 10.7 (左)*所示，GPT 模型与原始变压器-解码器类似。作者使用了 12 个解码器块（而不是原始变压器中的 6 个），每个块具有 768
    维状态和 12 个自注意头。由于该模型使用了掩码自注意力，它保持了语言模型的因果属性，因此还可以用于文本生成。对于*图 10.7 (右)*展示的其余任务，基本上使用相同的预训练语言模型，并对输入进行最小的任务特定预处理和最终任务特定的层/目标。
- en: GPT-2
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-2
- en: GPT was superseded by an even more powerful model called GPT-2\. Radford et
    al. presented the GPT-2 model as part of their work titled *Language Models are
    Unsupervised Multi-task Learners* in 2019.^(13) The largest GPT-2 variant is a
    huge 1.5 billion parameter transformer-based model which was able to perform remarkably
    well on various NLP tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: GPT被一个更加强大的模型GPT-2取代。Radford等人在2019年的论文*Language Models are Unsupervised Multi-task
    Learners*中展示了GPT-2 模型。^(13) 最大的GPT-2变体是一个庞大的15亿参数的基于transformer的模型，在各种NLP任务上表现出色。
- en: The most striking aspect of this work is that the authors showcase how a model
    trained in an unsupervised fashion (language modeling) achieves state-of-the-art
    performance in a *few-shot* setting. This is particularly important because, in
    comparison to GPT and even BERT, GPT-2 does not require any fine-tuning on specific
    tasks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作最引人注目的是，作者展示了一个以无监督方式训练的模型（语言建模）如何在*few-shot*设置中实现了最先进的性能。这一点特别重要，因为与GPT甚至BERT相比，GPT-2在特定任务上不需要进行任何微调。
- en: Similar to GPT, the secret sauce for GPT-2 is its dataset. The authors prepared
    a massive 40 GB dataset by crawling 45 million outbound links from a social networking
    site called Reddit. They performed some heuristic-based cleaning, de-duplication,
    and removal of Wikipedia articles (of course, why not?) to end up with roughly
    8 million documents. This dataset is called the **WebText** dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与GPT类似，GPT-2的秘密武器是它的数据集。作者们通过对一个名为Reddit的社交网站的4500万个外链进行爬取，准备了一个庞大的40 GB数据集。他们进行了一些基于启发式的清理、去重和移除维基百科文章（当然，为什么不呢？）最终得到了大约800万个文档。这个数据集被称为**WebText**数据集。
- en: The overall architecture of GPT-2 remains the same as GPT, with minor changes
    such as the placement of layer normalization at the start of each sub-block and
    an additional layer normalization after the final self-attention block. The four
    variants of the model leveraged 12, 24, 36, and 48 layers respectively. The vocabulary
    was also expanded to cover 50,000 words and the context window was expanded to
    1,024 tokens (as compared to 512 for GPT).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2的总体结构与GPT相同，只是稍微更改了一些地方，比如在每个次块的开头放置层归一化，并在最终的自注意力块之后添加了额外的层归一化。模型的四个变体分别利用了12、24、36和48层。词汇量也扩展到了50000个词，并且上下文窗口扩展到了1024个标记（而GPT为512）。
- en: 'GPT-2 was so performant as a language model that the authors initially decided
    against releasing the pre-trained model for the general good.^(14) They eventually
    did release it, citing the fact that no ill-intentioned use had been found so
    far. It is important to note that it isn''t just an ethical issue. The sheer size
    of the data and the model make it nearly impossible for most people to even think
    about training such a model. *Figure 10.8* depicts the size of some recent NLP
    models and the amount of compute required to train them:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 作为一种语言模型表现非常出色，以至于作者最初决定不释放预训练模型以造福大众^(14)。最终他们还是释放了它，理由是迄今为止还没有发现恶意使用。需注意的是，这不仅仅是道德问题。数据和模型的庞大规模使得大多数人甚至无法想象训练这样的模型的可能性。*Figure
    10.8* 描述了一些最近的NLP模型的规模和训练所需的计算量：
- en: '![Table  Description automatically generated](img/B16176_10_08.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![Table  Description automatically generated](img/B16176_10_08.png)'
- en: 'Figure 10.8: Scale of NLP models^(15)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 10.8: NLP模型的规模^(15)'
- en: TPUs are multiple times faster than typical GPUs and, as shown in the figure,
    GPT-2 requires 2048 TPU days to train on the reported dataset. Compare this against
    256 TPU days for the large BERT model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: TPU的速度比典型的GPU快多次，如图所示，GPT-2 需要在报告的数据集上进行2048 TPU天的训练。与大型BERT模型的256 TPU天相比是相当大的差距。
- en: 'Interested readers may explore the official implementation of GPT-2 here: [https://github.com/openai/gpt-2](https://github.com/openai/gpt-2).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有兴趣的读者可以在这里探索GPT-2的官方实现：[https://github.com/openai/gpt-2](https://github.com/openai/gpt-2).
- en: 'While the official implementation was made for TensorFlow 1.14, an unofficial
    implementation using TensorFlow 2.x is available at this link:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然官方实现是基于 TensorFlow 1.14，但此链接提供了使用 TensorFlow 2.x 的非官方实现：
- en: '[https://akanyaani.github.io/gpt-2-tensorflow2.0/](https://akanyaani.github.io/gpt-2-tensorflow2.0/)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://akanyaani.github.io/gpt-2-tensorflow2.0/](https://akanyaani.github.io/gpt-2-tensorflow2.0/)'
- en: 'Thankfully, as the pre-trained model was released, researchers at Hugging Face
    decided to work toward democratizing the transformer architectures. The `transformer`
    package from Hugging Face is a high-level wrapper that enables us to use these
    massive NLP models with a few lines of code. The library also provides a web app
    for exploring different transformer-based models. *Figure 10.9* is a snapshot
    of the paragraph generated by this web app when provided with a "GPT is" seed:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于预训练模型的发布，Hugging Face 的研究人员决定致力于使变压器架构民主化。来自 Hugging Face 的 `transformer`
    包是一个高级包装器，使我们能够使用几行代码就可以使用这些大规模的自然语言处理模型。该库还提供了一个 Web 应用程序，用于探索不同的基于变压器的模型。*图
    10.9* 是当提供了一个 "GPT is" 种子时，由该 Web 应用程序生成的段落的快照：
- en: '![Text  Description automatically generated](img/B16176_10_09.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本描述](img/B16176_10_09.png)'
- en: 'Figure 10.9: A sample output using GPT-2 based on the Hugging Face transformer
    package^(16)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：使用基于 Hugging Face transformer 包的 GPT-2 的示例输出^(16)
- en: The generated text in the figure shows how impressively GPT-2 works. In the
    first sentence, the model does a remarkable job and even follows a proper convention
    for citing a previous work (although the citation itself isn't correct). The content
    does not make much sense, but syntactically it is on the mark and quite coherent
    with respect to the minimal seed text we provided as input.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图中生成的文本显示了 GPT-2 的惊人效果。在第一句中，模型表现出色，甚至遵循了正确引用先前工作的惯例（尽管引用本身并不正确）。内容并不是很有意义，但在语法上是准确的，相对于我们提供的最小种子文本而言，它是相当连贯的。
- en: We will now leverage the `transformers` package to build a text generation pipeline
    of our own based on GPT-2, and see how well our model can do.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将利用 `transformers` 包来构建一个基于 GPT-2 的文本生成流水线，看看我们的模型能做多好。
- en: Hands-on with GPT-2
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亲身体验 GPT-2
- en: 'Keeping with the theme of some of the previous chapters where we were generating
    fake content using various complex architectures, let''s generate some fake headlines
    using GPT-2\. The million-headlines dataset contains over a million headlines
    from ABC News Australia, collected over a period of 17 years. The dataset is available
    at the following links:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 保持与之前某些章节主题的一致性，在那些章节中，我们使用各种复杂的架构生成了一些虚假内容，让我们使用 GPT-2 生成一些虚假新闻标题。百万条新闻标题数据集包含了澳大利亚广播公司的一百多万条新闻标题，历时
    17 年收集。该数据集可在以下链接处获得：
- en: '[https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SYBGZL)'
- en: '[https://www.kaggle.com/therohk/million-headlines](https://www.kaggle.com/therohk/million-headlines   )'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/therohk/million-headlines](https://www.kaggle.com/therohk/million-headlines)'
- en: We will be using the `transformers` library from Hugging Face to fine-tune GPT-2
    on this dataset. At a high level, this task of fake headline generation is the
    same as the language modeling task we worked on in the initial sections of *Chapter
    9*, *The Rise of Methods for Text Generation*. Since we are using the `transformers`
    package, the steps relating to training dataset creation, tokenization, and finally
    training the model are abstracted with high-level APIs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Hugging Face 的 `transformers` 库来对这个数据集上的 GPT-2 进行微调。从高层次来看，这个假新闻生成任务与我们在
    *第 9 章* 的初始部分中进行的语言建模任务是一样的。由于我们使用的是 `transformers` 包，与训练数据集的创建、标记化和最终训练模型相关的步骤都是使用高级
    API 进行抽象化的。
- en: The `transformers` library works with both TensorFlow and PyTorch backends.
    For this particular case, we are using the default retraining setup based on PyTorch.
    The library is constantly improving and at the time of writing, the stable version
    3.3.1 has issues with fine-tuning GPT-2 using TensorFlow. Since `transformers`
    is a high-level library, readers will not notice much difference in the following
    code snippets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers` 库与 TensorFlow 和 PyTorch 后端都兼容。对于这种特殊情况，我们使用基于 PyTorch 的默认重新训练设置。该库不断改进，在撰写本文时，稳定版本
    3.3.1 在使用 TensorFlow 对 GPT-2 进行微调时存在问题。由于 `transformers` 是一个高级库，读者在以下代码片段中不会注意到太大的差异。'
- en: 'The first step, as always, is to read the dataset at hand and transform it
    into the required format. We need not prepare the word-to-integer and reverse
    mappings on our own. The `Tokenizer` class from the `transformers` library handles
    that for us. The following snippet prepares the dataset and required objects:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，第一步是读取手头的数据集，并将其转换为所需的格式。我们不需要自己准备单词到整数和反向映射。`transformers`库中的`Tokenizer`类会为我们处理这些。以下代码片段准备了数据集和所需的对象：
- en: '[PRE0]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We make use of the class `AutoModelWithLMHead` as a high-level wrapper for GPT-2
    with a language model objective. The `Trainer` class simply iterates through training
    steps based on the parameters set using the `TrainingArguments` class.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`AutoModelWithLMHead`类作为对GPT-2的高级封装，具有语言模型目标。`Trainer`类根据`TrainingArguments`类设置的参数简单地迭代训练步骤。
- en: 'The next step is to simply call the `train` function and let the fine-tuning
    begin. The following snippet shows training steps for GPT-2:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步只需调用`train`函数，让微调开始。以下代码片段展示了GPT-2的训练步骤：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let us now generate some fake headlines to see how good or bad our GPT-2 model
    is. *Figure 10.10* showcases a few fake headlines generated using our model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成一些虚假的新闻标题，看看我们的GPT-2模型的表现好坏。*图10.10*展示了使用我们的模型生成的一些虚假新闻标题：
- en: '![A close up of a flower  Description automatically generated](img/B16176_10_10.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![一朵花的特写  自动生成的说明](img/B16176_10_10.png)'
- en: 'Figure 10.10: Fake headlines using fine-tuned GPT-2\. Text in bold is the seed
    text.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：使用微调GPT-2生成的虚假新闻标题。粗体文本是种子文本。
- en: The generated output showcases the potential of GPT-2 and transformer-based
    architectures in general. Readers should compare this against the LSTM-based variants
    we trained in the initial sections of *Chapter 9*, *The Rise of Methods for Text
    Generation*. The model shown here is able to pick up a few nuances associated
    with news headlines. For instance, it is generating short and crisp sentences,
    picking up words such as *kangaroo*, *indigenous*, and even *Melbourne*, which
    are all relevant in an Australian context, the domain of our training dataset.
    All of this was captured by the model with only a few epochs of training. The
    possibilities are endless.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出展示了GPT-2和基于transformer的架构的潜力。读者应该将此与我们在《第9章 文本生成方法的兴起》的初始部分中训练的基于LSTM的变体进行比较。这里展示的模型能够捕捉到与新闻标题相关的一些细微差别。例如，它生成了简短而精练的句子，捕捉到了像*袋鼠*、*土著*甚至*墨尔本*这样在澳大利亚环境中都相关的词语，这些都存在于我们的训练数据集的领域。所有这些都是模型在仅经过几个epoch的训练后所捕获的。可能性是无穷无尽的。
- en: Mammoth GPT-3
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[猛犸GPT-3](https://wiki.example.org/mammoth_gpt-3)'
- en: The OpenAI group did not just stop after the massive success of GPT-2\. Rather,
    GPT-2 demonstrated how model capacity (parameter size) and larger datasets can
    lead to impressive results. The recently published paper titled *Language Models
    are Few Shot Learners* by Brown et al. was released in May 2020.^(17) This paper
    introduces the mammoth 175 billion-parameter GPT-3 model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI小组并未在GPT-2取得巨大成功后就止步不前。相反，GPT-2展示了模型容量（参数大小）和更大数据集如何可能导致令人印象深刻的结果。Brown等人在2020年5月发布了题为*语言模型是少样本学习者*的论文。^(17)此论文介绍了巨大的1750亿参数GPT-3模型。
- en: GPT-3 is orders of magnitude larger (10x) than any previous language model and
    explores the transformer architecture to its limits. In this work, the authors
    present 8 different variants of the model, starting from a 125 million-parameter,
    12-layer "GPT-3 small" to a 175 billion-parameter, 96-layer GPT-3 model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3比以往任何语言模型都要庞大（大约10倍），并且将transformer架构发挥到了极限。在这项工作中，作者展示了模型的8个不同变体，从一个拥有1.25亿参数，12层的“GPT-3小”到一个拥有1750亿参数，96层的GPT-3模型。
- en: The model architecture is the same as GPT-2, but with one major change (aside
    from the increase in embedding size, attention heads, and layers). This major
    change is the use of alternating dense and locally banded sparse attention patterns
    in transformer blocks. This sparse attention technique is similar to the one presented
    for sparse transformers (see *Generating Long Sequences with Sparse Transformers*,
    Child et al.^(18)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构与GPT-2相同，但有一个主要变化（除了嵌入尺寸、注意头和层数的增加之外）。这个主要变化是在变压器块中使用交替的密集和局部带状稀疏注意力模式。这种稀疏注意技术类似于为稀疏变换器（参见*使用稀疏变换器生成长序列*，Child等人^(18)））提出的技术。
- en: Similar to earlier GPT models, the authors had to prepare an even larger dataset
    for this third iteration. They prepared a 300 billion-token dataset based on existing
    datasets like Common Crawl (filtered for better content), WebText2 (a larger version
    of WebText used for GPT-2), Books1 and Books2, and the Wikipedia dataset. They
    sampled each dataset in proportion to the dataset's quality.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的 GPT 模型类似，作者们不得不为这第三次迭代准备一个更大的数据集。他们基于类似 Common Crawl（过滤了更好的内容）、WebText2（WebText
    的更大版本，用于 GPT-2）、Books1 和 Books2，以及维基百科数据集准备了一个 3000 亿标记的数据集。他们按各自数据集的质量比例抽样。
- en: The authors compare the overall learning paradigm of NLP models, and machine
    learning in general, with the way humans learn. Despite the improved performance
    and capacity of language models over the years, the state-of-the-art models still
    require task-specific fine-tuning. To showcase the capabilities of GPT-3, they
    evaluate the model in *few-shot*, *one-shot*, and *zero-shot modes*. The fine-tuning
    mode is left as a future exercise for the time being.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作者比较了 NLP 模型和总体机器学习的学习范式与人类学习方式。尽管语言模型在这些年来的性能和容量上有所改进，但最先进模型仍需要特定于任务的精细调整。为展示
    GPT-3 的能力，他们评估了该模型的 *少例学习*、*一例学习* 和 *零例学习* 模式。精细调整模式暂时留给未来练习。
- en: 'The three evaluation modes can be summarised as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种评估模式可以总结如下：
- en: '**Zero-shot**: Given only a natural language description of the task, i.e.
    without being shown any examples of correct output, the model predicts the answer.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**零-shot**：仅凭任务的自然语言描述，即在没有展示任何正确输出示例的情况下，模型就能预测答案。'
- en: '**One-shot**: As well as a description of the task, the model is shown one
    example of the task.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一例**：除了任务描述外，模型还展示了一个任务示例。'
- en: '**Few-shot**: As well as a description of the task, the model is shown a few
    examples of the task.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少例学习**：除了任务描述外，模型还展示了一些任务示例。'
- en: In each case, no gradient updates are performed (as we are only evaluating,
    not training, the model in any of these modes). *Figure 10.11* shows sample settings
    for each of the evaluation modes with the task being translation of text from
    English to Spanish.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，都不进行梯度更新（因为我们只是评估模型，在任何这些模式中都不是在训练）。*图 10.11* 显示了每种评估模式的示例设置，任务是将文本从英语翻译成西班牙语。
- en: '![Text  Description automatically generated](img/B16176_10_11.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的文本描述](img/B16176_10_11.png)'
- en: 'Figure 10.11: Evaluation modes for GPT-3'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.11: GPT-3 的评估模式'
- en: As shown in the figure, in zero-shot mode, the model is presented with the task
    description and a prompt for translation. Similarly, for one-shot and few-shot
    modes the model is presented with one and a few examples respectively before presenting
    a prompt for actual translation. The authors observe that GPT-3 achieves promising
    results in zero-shot and one-shot settings. In a few-shot setting, the model is
    mostly competitive and for certain tasks even surpasses the current state-of-the-art.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，在零-shot 模式下，模型展示了任务描述和一个用于翻译的提示。类似地，在一例和少例模式下，模型分别展示了一个和一些示例，然后展示了实际翻译提示。作者观察到
    GPT-3 在零-shot 和一例设置下取得了有希望的结果。在少例设置中，该模型大多数情况下是竞争性的，甚至在某些任务中超过了当前的最先进水平。
- en: Aside from the usual NLP tasks, GPT-3 seems to showcase some extraordinary capabilities
    on tasks which otherwise require rapid adaptation or on-the-fly reasoning. The
    authors observe that GPT-3 is able to perform reasonably well on tasks such as
    unscrambling words, performing three-digit arithmetic, and even using novel words
    in a sentence after seeing them defined just once. The authors also observe that
    the news articles generated by GPT-3 in the few-shot setting are good enough to
    cause difficulties for human evaluators when distinguishing them from human-generated
    articles. It would be interesting to test out GPT-3 against the fine-tuned GPT-2
    we prepared in the previous section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通常的 NLP 任务外，GPT-3 似乎展示了一些在其他情况下需要快速适应或即兴推理的非凡能力。作者观察到 GPT-3 能够在一些任务上表现良好，如拼词、进行三位数的算术，甚至在看到一次定义后就能在句子中使用新词。作者还观察到，在少例学习设置下，GPT-3
    生成的新闻文章足够好，以至于在区分它们与人为生成的文章时会给人类评估者带来困难。有趣的是，在之前部分准备的 GPT-2 上测试一下 GPT-3。
- en: The model is huge enough to require a dedicated high-performance cluster to
    train it as described in the paper. The authors present a discussion on the amount
    of compute and energy required to train this huge model. In its current state,
    the model remains out of bounds for most of us. OpenAI plans to expose the model
    in the form of an API, but details are sketchy at the time of writing this chapter.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型足够庞大，需要一个专门的高性能集群来训练，正如论文中所描述的。作者就训练这个巨大模型所需的计算量和能量进行了讨论。在当前状态下，这个模型对我们大多数人来说还是难以企及的。OpenAI计划以API的形式展示这个模型，但在撰写本章时，细节尚不明朗。
- en: Summary
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced some of the core ideas that have dominated recent
    models for NLP, like the *attention* mechanism, *contextual embeddings*, and *self-attention*.
    We then used this foundation to learn about the *transformer* architecture and
    its internal components. We briefly discussed BERT and its family of architectures.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了近期NLP模型中一些主要的概念，如*attention*机制、*contextual embeddings*和*self-attention*。然后我们使用这个基础来学习*transformer*架构及其内部组件。我们简要地讨论了BERT及其系列架构。
- en: In the next section of the chapter, we presented a discussion on the transformer-based
    language models from OpenAI. We discussed the architectural and dataset-related
    choices for GPT and GPT-2\. We also used the `transformer` package from Hugging
    Face to develop our own GPT-2-based text generation pipeline. We finally closed
    the chapter with a brief discussion on the latest and greatest language model,
    GPT-3\. We discussed various motivations behind developing such a huge model and
    its long list of capabilities, which go beyond the list of traditionally tested
    benchmarks.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的下一节中，我们对OpenAI的基于transformer的语言模型进行了讨论。我们讨论了GPT和GPT-2的架构和数据集相关的选择。我们还使用了Hugging
    Face的`transformer`包来开发我们自己的基于GPT-2的文本生成流水线。最后，我们对最新、最尖端的语言模型GPT-3进行了简要讨论。我们讨论了开发这样一个巨大模型的各种动机以及它超越传统测试基准列表的功能清单。
- en: This chapter, along with *Chapter 9*, *The Rise of Methods for Text Generation*,
    showcased how NLP is a field of study of its own. Yet, concepts from computer
    vision and deep learning/machine learning in general cross-pollinate to push the
    boundaries.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和*第九章* *文本生成方法的兴起* 展示了自然语言处理(NLP)是自己的研究领域。然而，来自计算机视觉和深度学习/机器学习的概念通常相互交叉，以推动技术边界。
- en: In the next chapter, we will shift our focus to understanding the audio landscape
    and how generative models work in the audio domain.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把我们的焦点转移到理解音频领域以及生成模型在音频领域中的工作。
- en: References
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Peters, M.E., Ammar, W., Bhagavatula, C., & Power, R. (2017). *Semi-supervised
    sequence tagging with bidirectional language models*. arXiv. [https://arxiv.org/abs/1705.00108](https://arxiv.org/abs/1705.00108)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Peters, M.E., Ammar, W., Bhagavatula, C., & Power, R. (2017). *Semi-supervised
    sequence tagging with bidirectional language models*. arXiv. [https://arxiv.org/abs/1705.00108](https://arxiv.org/abs/1705.00108)
- en: Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer,
    L. (2018). *Deep contexualized word representations*. arXiv. [https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer,
    L. (2018). *Deep contexualized word representations*. arXiv. [https://arxiv.org/abs/1802.05365](https://arxiv.org/abs/1802.05365)
- en: Howard, J., & Ruder, S. (2018). *Universal Language Model Fine-tuning for Text
    Classification*. arXiv. [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Howard, J., & Ruder, S. (2018). *Universal Language Model Fine-tuning for Text
    Classification*. arXiv. [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)
- en: Cheng, J., Dong, L., & Lapata, M. (2016). *Long Short-Term Memory-Networks for
    Machine Reading*. arXiv. [https://arxiv.org/abs/1601.06733](https://arxiv.org/abs/1601.06733)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cheng, J., Dong, L., & Lapata, M. (2016). *Long Short-Term Memory-Networks for
    Machine Reading*. arXiv. [https://arxiv.org/abs/1601.06733](https://arxiv.org/abs/1601.06733)
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
    Kaiser, L., & Polosukhin, I. (2017). *Attention Is All You Need*. arXiv. [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
    Kaiser, L., & Polosukhin, I. (2017). *Attention Is All You Need*. arXiv. [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: 'Devlin, J., Chang, M-W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding*. arXiv. [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Devlin, J., Chang, M-W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding*. arXiv. [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)'
- en: 'Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., & Stoyanov, V. (2019). *RoBERTa: A Robustly Optimized BERT
    Pretaining Approach*. arXiv. [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., & Stoyanov, V. (2019)。*RoBERTa: A Robustly Optimized BERT
    Pretaining Approach*。arXiv。[https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
- en: 'Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019).
    *ALBERT: A Lite BERT for Self-supervised Learning of Language Representations*.
    arXiv. [https://arxiv.org/abs/1909.11942](https://arxiv.org/abs/1909.11942)'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019)。*ALBERT:
    A Lite BERT for Self-supervised Learning of Language Representations*。arXiv。[https://arxiv.org/abs/1909.11942](https://arxiv.org/abs/1909.11942)'
- en: 'Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019) *DistilBERT, a distilled
    version of BERT: smaller, faster, cheaper and lighter*. arXiv. [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019)。*DistilBERT, a distilled
    version of BERT: smaller, faster, cheaper and lighter*。arXiv。[https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)'
- en: 'Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q.V. (2019).
    *XLNet: Generalized Autoregressive Pretraining for Language Understanding*. arXiv.
    [https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., & Le, Q.V. (2019)。*XLNet:
    Generalized Autoregressive Pretraining for Language Understanding*。arXiv。[https://arxiv.org/abs/1906.08237](https://arxiv.org/abs/1906.08237)'
- en: Radford, A. (2018, June 11). *Improving Language Understanding with Unsupervised
    Learning*. OpenAI. [https://openai.com/blog/language-unsupervised/](https://openai.com/blog/language-unsupervised/)
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A. (2018年6月11日)。*Improving Language Understanding with Unsupervised
    Learning*。OpenAI。[https://openai.com/blog/language-unsupervised/](https://openai.com/blog/language-unsupervised/)
- en: 'Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
    & Fidler, S. (2015). *Aligning Books and Movies: Towards Story-like Visual Explanations
    by Watching Movies and Reading Books*. arXiv. [https://arxiv.org/abs/1506.06724](https://arxiv.org/abs/1506.06724)'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
    & Fidler, S. (2015)。*Aligning Books and Movies: Towards Story-like Visual Explanations
    by Watching Movies and Reading Books*。arXiv。[https://arxiv.org/abs/1506.06724](https://arxiv.org/abs/1506.06724)'
- en: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019).
    *Language Models are Unsupervised Multitask Learners*. OpenAI. [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pd)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019)。*Language
    Models are Unsupervised Multitask Learners*。OpenAI。[https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pd)
- en: Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., & Sutskever,
    I. (2019, February 14). *Better Language Models and Their Implications*. OpenAI.
    [https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Radford, A., Wu, J., Amodei, D., Amodei, D., Clark, J., Brundage, M., & Sutskever,
    I. (2019年2月14日)。*Better Language Models and Their Implications*。OpenAI。[https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/)
- en: 'stanfordonline. (2019, March 21). *Stanford CS224N: NLP with Deep Learning
    | Winter 2019 | Lecture 13 – Contextual Word Embeddings* [Video]. YouTube. [https://www.youtube.com/watch?v=S-CspeZ8FHc&ab_channel=stanfordonline](https://www.youtube.com/watch?v=S-CspeZ8FHc&ab_channel=stanfordonline)'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'stanfordonline. (2019年3月21日)。*斯坦福CS224N: NLP with Deep Learning | Winter 2019
    | Lecture 13 – Contextual Word Embeddings* [视频]。YouTube。[https://www.youtube.com/watch?v=S-CspeZ8FHc&ab_channel=stanfordonline](https://www.youtube.com/watch?v=S-CspeZ8FHc&ab_channel=stanfordonline)'
- en: Hugging Face. (n.d.). *gpt2 abstract*. Retrieved April 22, 2021, from [https://transformer.huggingface.co/doc/arxiv-nlp/ByLHXHhnBJtBLOpRENZmulqc/edit](https://transformer.huggingface.co/doc/arxiv-nlp/ByLHXHhnBJtBLOpRENZmulqc/edit)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face。 (无日期)。*gpt2 abstract*。检索于2021年4月22日，来源：[https://transformer.huggingface.co/doc/arxiv-nlp/ByLHXHhnBJtBLOpRENZmulqc/edit](https://transformer.huggingface.co/doc/arxiv-nlp/ByLHXHhnBJtBLOpRENZmulqc/edit)
- en: Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse,
    C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,
    C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). "*Language
    Models are Few-Shot Learners*". arXiv. [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger,
    G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse,
    C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner,
    C., McCandlish, S., Radford, A., Sutskever, I., & Amodei, D. (2020). "*语言模型是少样本学习器*".
    arXiv. [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
- en: Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). *Generating Long Sequences
    with Sparse Transformers*. arXiv. [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). *使用稀疏Transformer生成长序列*.
    arXiv. [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)
