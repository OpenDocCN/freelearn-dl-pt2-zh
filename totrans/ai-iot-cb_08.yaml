- en: Optimizing with Microcontrollers and Pipelines
  prefs: []
  type: TYPE_NORMAL
- en: Most IoT devices run on **microcontroller units** (**MCUs**), while most machine
    learning happens on CPUs. One of the most cutting-edge innovations in AI is the
    ability to run models on constrained devices. In the past, AI was limited to large
    computers with traditional operating systems such as Windows or Linux. Now, small
    devices can execute machine learning models with technologies such as ONYX and
    TensorFlow Lite. These constrained devices are low cost, can use machine learning
    without an internet connection, and can save dramatically on cloud costs.
  prefs: []
  type: TYPE_NORMAL
- en: Many IoT projects fail due to high cloud costs. IoT devices are often sold for
    a fixed price without a reoccurring subscription model. They then incur high cloud
    costs by performing machine learning or analytics. There is no reason this needs
    to be the case. Even for microcontrollers, the cost can be dramatically reduced
    by pushing machine learning and analytics to the device itself.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to focus on two different development boards.
    The first is the **ESP32**, while the second is the **STM32**. The ESP32 is an
    MCU with Wi-Fi capabilities. They typically cost between $5 - $10 and are great
    for smaller projects where a few sensors need to be added to a device. An example
    of this would be a weather station. In contrast, the **STM32** development boards
    are typically used by electrical engineers to quickly start a project. There are
    dozens of different types of development boards but they use different compute
    modules such as the Cortext M0, M4, and M7\. In terms of the ESP32, electrical
    engineers typically use them as the compute on their IoT devices. Other platforms,
    such as the STM32, are considered starter kits. Electrical engineers use them
    to determine the chipset needed and then design their own boards that specifically
    meet their needs.
  prefs: []
  type: TYPE_NORMAL
- en: Getting these boards running, talking to the cloud, and run ML models are non-trivial.
    This chapter focuses on getting the devices to perform complex computations and
    connecting to the cloud. To do this, we will explore the specific tools that are
    needed. Machine learning is typically done in higher-level languages such as Python,
    while the devices usually use C or C++.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ESP32 with IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an ESP32 environment monitor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with BOM changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building machine learning pipelines with sklearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming machine learning with Spark and Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enriching data using Kafka's KStreams and KTables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ESP32 with IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll use an ESP32 to interface with Azure IoT Hub. Using a
    low-level device, we will code up the network interface. We will also need to
    deploy code to the ESP32 from a computer and then use a serial monitor to view
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are going to use the Arduino framework to program a bare-metal
    IoT solution. On your PC, you will need to install the Arduino **integrated development
    environment** (**IDE**). This will install the supporting software so that we
    can program the ESP32 using the Arduino framework. Next, we will install **Visual
    Studio Code** (**VS Code**). The VS Code IDE has an extension that makes board
    selection and library add-in easy. It also has a serial monitor and several built-in
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have installed the Arduino IDE and VS Code, you need to find the required
    extension tool in VS Code. Then search for `platformIO`, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac9201b6-82fa-4510-bdf8-e0f89ed74475.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you''ve installed **PlatformIO IDE**, connect your ESP32 to your computer
    via USB. Then, find the **PlatformIO** button in the left panel. Next, from the
    **Quick Access** menu, click on **Open**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/516942fe-4852-4c71-a93a-51aa70e78764.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From here, you can find the main PlatformIO window and click on **Open Project**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb90d9dc-2652-4d4b-b679-e7a726f6abf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The startup wizard will take you through choosing a name for your project,
    a framework (**Arduino**, in our case), and a board type. For the pinouts to work
    correctly, you must choose the right board type. Some boards have markings on
    them that will allow you to look up the board types, while others will not. For
    that reason, when purchasing an ESP32, it is important that you can determine
    the board''s type:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b55aee0e-67b9-46fa-9e42-5b13ce2bc505.png)'
  prefs: []
  type: TYPE_IMG
- en: Optionally, you can change where the project is stored.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will need to install the Azure IoT Hub libraries and quickstart code.
    Go back to the **Quick Access** menu and click **Libraries.** Then, type `Azure
    IoT` into the search menu and click on the **AzureIoTHub** library from Microsoft.
    Once you've done this, change the release version to the latest that's available
    and click **Install**. Then, you need to do the same for the **AzureIoTUtility**,
    **WiFi**, and **AzureIoTProtocol_MQTT** libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, go back to the **AzureIoTHub** library. There is some quickstart code
    here that will allow you to quickly connect to the local Wi-Fi and IoT Hub. For
    this recipe, we will use some sample code to test our connection to IoT Hub. In
    the **Examples** section, you will find three code files called `iothub_II_telemetry_sample`,
    `sample_init`, and `iot_configs`, as shown in the following screenshot. Take the
    code from `iothub_II_telemetry_sample` and replace the `main.cpp` code in the
    source code. Next, create two new files called `sample_init.h` and `iot_configs.h` and
    paste the example code from the PlatformIO example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4acf2bca-32e5-4284-b47e-575cdef443e8.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add your Wi-Fi connection string. Change the strings on lines 10 and 11 of
    the `iot_configs.h` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Get a device connection string from Azure IoT Hub and insert it on line 19
    of `iot_configs.h`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With your ESP32 attached to your computer via a USB, click on the **PlatformIO**
    icon in the left panel and then click on **Upload and Monitor**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d9925f9c-470e-4d5b-813c-7d6f20e7829a.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we uploaded the code to the ESP32 and enabled the serial monitor. The
    lower panel in Visual Studio should start displaying text when it has connected
    to a Wi-Fi network and sent messages to IoT Hub. We also created some sample code
    for receiving cloud-to-device messages.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we have only scratched the surface of what the IoT Hub SDK is
    capable of doing. For example, we could even send cloud-to-device messages that
    allow us to queue up a set of messages for the device to digest. We could have
    also sent a direct message. This is like a cloud-to-device message that sends
    a message to a device but does not queue the message. If a device is offline,
    the message never gets sent. Another option would have been to upload to a blob.
    This allows us to upload log or binary files securely and directly to blob storage.
    Finally, we could have used device twins, which allow us to have a configuration
    file set on the device and can be queried across a fleet of devices. This would
    help us find out if an update did not work or a setting did not get set properly.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an ESP32 environment monitor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up a simple environment monitor using hardware is fairly straightforward.
    In this recipe, we are going to take a proof of concept with some simple hardware
    attached to it. In the *There's more...* section, we will talk about how to take
    a design like this and go to production with it, even if you don't have **electrical
    engineers** (**EEs**) on your team. To do this, we are going to introduce **Fritzing**,
    a hardware designer. Although it is not as powerful as **KiCad** or **Altuim Designer**,
    it is a tool that a person who is not an electrical engineer can give to an EE
    or a manufacturing partner and get circuit boards designed and printed.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this recipe is not really to show you how to create a temperature
    and humidity sensor. Temperature and humidity sensors are the *Hello World* of
    IoT. Instead, this recipe focuses on implementing these on constrained devices
    in a rapid way via manufacturing. Not all IoT projects can be done this way. There
    are certainly IoT projects that require EEs to build out complex devices, such
    as one with a video display and sound, or high-speed devices such as those used
    in the medical industry.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are going to build off of the previous recipe. We are going
    to use an ESP32 and must have the Arduino IDE and VS Code installed. In VS Code,
    we are going to add the `PlatformIO` extension. Eventually, we are going to attach
    the ESP32 to the computer we are using via USB but until we attach the sensor,
    leave it unattached. For this recipe, you will need a DHT11 digital humidity and
    temperature sensor, jumper cables, a 10k ohm resistor, and a breadboard. You should
    be able to purchase all of these components for around $20.
  prefs: []
  type: TYPE_NORMAL
- en: From here, we will need to go into VS Code and, using the `PlatformIO` extension,
    create a new project. Then, you will need to install the DHT sensor library from
    the `PlatformIO` library manager. You will then need to download Fritzing. It
    is an open source program. You can contribute to the project on their website
    and receive a copy, but you can also go to GitHub and, under **Releases**, download
    and install the program. The ESP32 comes in numerous hardware versions. Your ESP32
    may have different pins and capabilities. The datasheet for your ESP32 will tell
    you what pins it has. For example, there are pins that can do things with clock
    cycles or voltage measurements. There's also power and ground on various pins.
    These are used to power external sensors. By reviewing the DHT11 and ESP32, you
    can create a mapping from the inputs and outputs of various components.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Fritzing and on the right-hand side panel''s **Parts** section, click
    on the menu and select **Import...**. Then, select the ESP32 and DHT11\. Both
    of these can be found in the source code for this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c3b893d7-3aac-41e0-a58a-acf0eabda783.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Search for a resistor in the Parts list. Once you''ve dragged it onto the screen,
    adjust its properties to **4.7kΩ**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6f437f6f-7737-4cdc-bca3-a8fe17c7d1be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, place the DTH11 on the board and hook up the power rails using 3.3 volts
    and a ground:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8d9806c6-8966-4591-a175-5ed425cb9370.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, attach the power rails to the board. Also, connect the **General-Purpose
    Input/Output** (**GPIO**) pin 27 to the DHT11s data pin. We must also add a 4.7k
    ohm resistor between the 3.3V power rail and the data pin on the DHT11:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d3850df6-0300-42da-a302-d022e29bd697.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, connect the ESP32 to your computer and pull up the `/src/main.cpp` file
    from the `PlatformIO` project we started in the *Getting ready* section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `main.cpp`, include the `DHT.h` library reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create references to your data pin on the ESP32 and the DHT sensor type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `DHT` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the serial port and print a test message. Then, initialize the `dht`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In the main loop, read the temperature and humidity sensors. Then, call out
    to the printing section and wait 2 seconds before continuing the loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a function that checks for errors. If none are found, print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, the temperature and humidity sensors get power and ground from the ESP32\.
    Once we ensured this happens, we specified a data GPIO pin and added a resistor
    to match up the voltages.
  prefs: []
  type: TYPE_NORMAL
- en: When you purchase a DHT11, some come with three pins, while others come with
    four. You can adjust these pins based on the pinout specification for the sensor.
    Similarly, different manufactures of the ESP32 have different pinout specifications.
    Before working with any hardware, it is always important to check the datasheets
    of that particular product.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, we have a working prototype. There are several paths you can
    take to get the boards designed and the product mass-produced in a factory. You
    can hire EEs to do this, but for something this small, you can often go to a company
    that specializes in board design, such as Seeed Studios. Many manufacturing plants
    offer a hardware designing service that can take the Fritzing sketch and turn
    it into a product. These manufacturing plants can often print out prototypes and
    mass-produce the boards when you are ready.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different ways of tuning hyperparameters. If we were to do this
    manually, we could put random variables into our parameters and see which one
    was the best. To do this, we could perform a grid-wise approach, where we map
    the possible options and put in some random tries and keep going down a route
    that seems to produce the best outcomes. We might use statistics or machine learning
    to help us determine what parameters can give us the best results. These different
    approaches have pros and cons, depending on the shape of the loss of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: There are various machine learning libraries that can help us perform these
    types of common tasks easier. `sklearn`, for example, has a `RandomizedSearchCV`
    method that, given a set of parameters, will perform a search for the best model
    with the least loss. In this recipe, we will expand on the *Classifying chemical
    sensors with decision trees* recipe from [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*, and use a random forest. However, we will also add
    a grid search in order to optimize our results.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will be using the MOX sensor dataset from [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*. There, we saved our data in Delta Lake. Due to this,
    we can pull it easily into our Spark Notebook. We will also be using the `koalas`,
    `sklearn`, and `numpy` Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the data from Databricks Delta Lake:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Select and encode the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the parameters you wish to tune:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Create an instance of the random forest classifier algorithm so that we can
    adjust its hyperparameters later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up a grid search estimator so that we can tune our parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the decision tree classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the response for the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the winning set of hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This algorithm is an easy to implement algorithm that we worked with in [Chapter
    3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml), *Machine Learning for IoT*. There,
    we took a random stab at choosing an algorithm. Then, we had it go through one
    run of the code to get the necessary output. In this recipe, however, we caused
    it to run through many more runs to find the best estimator we could find. We
    could have done the same thing using a spreadsheet to keep track of all of the
    runs. However, this allows us to automate the process of performing experiments
    and tracking the results.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with BOM changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bill of Materials** (**BOMs**) are the components that make up the device.
    These can be resistors, chips, and other components. The life cycle of a typical
    IoT product is about 10 years. In that time, things can change with the product.
    A component manufacturer could discontinue a part such as a chip line. Outsourced
    manufacturers typically perform BOM optimization on a board layout, though BOM
    optimization can change the quality of the device. For example, it can change
    the sensitivity of the sensor or the lifetime of a device.'
  prefs: []
  type: TYPE_NORMAL
- en: This can throw off trained models and can have a dramatic effect on any remaining
    useful life calculations and predictive maintenance models. When working with
    IoT and machine learning, tracking changes that have been made to any remaining
    useful life based on BOM and factory changes can help us detect issues with the
    quality and longevity of a device.
  prefs: []
  type: TYPE_NORMAL
- en: This is typically done with a database. When a device is made in a factory,
    that device's serial number, BOM version, and factory details are stored in that
    factory. This is where a total expected lifespan can be applied to the device.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are going to spin up a Docker instance of a SQL Server database.
    To get started, you must install Docker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to build and run a SQL Server database using `docker`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a working SQL Server database, we need to add a database and two
    tables for it. You can connect to the SQL database by installing the `mssql` plugin
    for VS Code and then connecting to the database with the username and password
    we had in the Docker file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3ffca53-2311-4340-8442-589d4d2e57c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you've done this, click on the new SQL Server tool in the left panel. Then,
    click on the plus (**+**) button to be taken through a wizard that will help you
    create a database connection. When the wizard asks you for the **ado.net** connection
    string type in `localhost`, it will ask you for a username and password. Type
    in `sa` for the username and `Password!` for the password.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, run the following SQL statements by clicking on the green arrow in the
    upper right of the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: From here, `pip` install `pyodbc` from `pypi` and create a new Python script
    in VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `pyodbc` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Connect to the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a database connection cursor so that you can run queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert the product and manufacturing date into the `Device` table and commit
    the transaction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve calculated the remaining useful life of that product, add that
    information to the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we showed you how to use a database to track your results over
    time. Databases allow us to insert and update information about our models over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we looked at products. Tracking the end life of devices can
    give our models real-world feedback and lets us know when they should be retrained.
    We can store the predicted error or loss rate and compare it against real-world
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Building machine learning pipelines with sklearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `sklearn pipeline` package makes it easy for us to manage multiple stages
    of feature engineering and modeling. Performing machine learning experiments is
    more just than training models. It is a combination of several factors. First,
    you need to cleanse and transform the data. Then, you must enrich the data with
    feature engineering. These common tasks can be built up into a series of steps
    called a **pipeline**. When we're trying out different variants on our experiments,
    we can use these pipelines to train a series of very complex steps so that they
    become something simple and manageable that can be reused.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we are going to use the data that was previously feature engineered
    in the *Enhancing data using feature engineering* recipe of [Chapter 4](a40f2c07-0e51-46c6-a3cf-66d5c46477c4.xhtml),
    *Deep Learning for Predictive Maintenance*. In that recipe, we put data into Databricks
    and then cleansed that data so that we could use it in additional experiments.
    To retrieve this data, we are simply going to use a `select` statement from Delta
    Lake. For this recipe, you will need `pandas` and `sklearn` installed on your
    Spark cluster.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `pandas` and `sklearn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the data from Delta Lake:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create transformers that convert data into standardized numeric or categorical
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the necessary features and create a processor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a random forest pipeline step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Fit the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building machine learning pipelines is very common in data science. It helps
    simplify complex operations and adds a level of reusability to the software code.
    In this recipe, we used `sklearn` to perform complex operations on a simple pipeline.
    In our pipeline, we created a set of transformers. For the numeric numbers, we
    used a scaler, while for the categorical numbers, we used one-hot encoding. Next,
    we created a processor pipeline. In our case, we used a random forest classifier.
    Note that this pipeline step is an array, so we could pass more classifiers into
    our array. However, for the sake of simplicity, we will save that for the *There's
    more...* section. Finally, we trained and got the predictions from our model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned in the introduction to this recipe, the purpose of pipelines
    is to enable you to tweak your pipeline steps easily. In this section, we are
    going to tweak those steps to help us achieve a higher accuracy rate. In this
    case, we are simply going to extend the preceding code and add a classifier array
    of machine learning algorithms. From there, we will score the model so that we
    can determine which one was the best. The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Streaming machine learning with Spark and Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is a real-time streaming message hub. Combined with Kafka, Databrick's
    ability to ingest streams and perform machine learning on them in real time allows
    you to perform powerful machine learning in near real time. In this recipe, we
    are going to use  Confluent. Confluent is the company that was founded by the
    creators of Kafka. They have a cloud offering on Azure, GCP, and AWS. We are also
    going to use Databricks, which is available on Azure, GCP, and AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the cloud marketplace, spin up Confluent and Databricks. This will give
    you a Kafka and Spark system that is elastically scalable. Once you''ve spinned
    up these systems, go to the Confluent website at [https://confluent.cloud](https://confluent.cloud)
    and enter the username and password you set up in the cloud marketplace. Then,
    click on **Create cluster**. Follow the wizard to create your first cluster. Once
    you are in your cluster, click on **API access** in the menu. Then, find the Create
    key button that will allow you to create an API access key:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf6bfab6-6b2a-40c4-beb8-c8455af0d354.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you've created a key, jot down its username and password; you will need
    these details later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, go to the **Topics** section and, using the **Create topic** button,
    create two topics: one called `Turbofan` and another called `Turbofan_RUL`. Next,
    we will create a Python file so that we can test our new topic. Create a Python
    file with the following code to produce a message for the `TurboFan` topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can go to your topic in the Confluent Cloud UI and watch for messages
    on that topic by selecting the topic (**TurboFan**) and then **Messages**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ce1a793-511d-4af5-8328-65cd3815cf4a.png)'
  prefs: []
  type: TYPE_IMG
- en: If you run the preceding code, you will see a message going to Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stream Kafka into Databricks. In a Databricks notebook, enter the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the fields from the JSON file to serialize them as objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the function that will do the inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform inference using the UDF and save the results in a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Write the devices that are failing to a different DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Stream that DataFrame back to Kafka as a new topic and write the results to
    Kafka:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka is a streaming engine designed to handle large amounts of data. Data is
    ingested into Kafka and then sent to Spark, where the probability can be sent
    back to Kafka. In this example, we used Confluent Cloud and Databricks. These
    managed services can be found on all major cloud marketplaces.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we received real-time data from the engines. Then, we streamed
    that data in Spark and ran an inference on it. Once we'd received the results,
    we streamed them back into a separate Kafka topic. Using a Kafka topic and Kafka
    itself allows us to push that data into a database, data lakes, and microservices,
    all from a single data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to putting all of the data into a topic so that it can be dumped
    into a data store, we can stream the data into an alerting system. To do this,
    we can make a Kafka consumer, as shown in the following code. Here, we will stream
    the code down to a local system and then have a `msg_process` function that we
    can use to write to an alert system such as **Twilio**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Enriching data using Kafka's KStreams and KTables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, in IoT, there are external data sources that must be included. This could
    be weather data that affects the performance of the device or data from other
    nearby devices. An easy way of doing this is to use Kafka KSQL Server. Like we
    did in the previous recipe, we are going to use Confluent Cloud's KSQL Server,
    which you can get if you have a Confluent Cloud subscription.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to get data from a weather service topic and put
    it into a KTable. A KTable is similar to a database table. All data coming into
    Kafka comes in key-value pairs. With KTable, when data comes in with a new key,
    we insert it into our KTable. If it contains a key that already exists in our
    KTable, we update it. We are also going to convert our topic into a KStream. This
    allows us to run standard SQL-like queries over our table and stream. By doing
    this, we can, for example, query the current weather and join it to the engine
    data from our previous recipe. This allows us to enrich the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the **Confluent Cloud ksqlDB** portal, go to the **ksqlDB** tab and add
    an application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32796f45-57f0-4b7f-a097-1757fcd58d63.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you've completed all the setup steps, you will have a KSQL query editor.
    From here, we will edit our queries.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe is a continuation of the previous recipe. You will need the streaming
    `TurboFan` data that we set up running in Kafka. You will also need to run the `weatherstreamer.py` Kafka
    Weather Streamer Python script, which can be found in the `ch10` directory of
    the GitHub repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you will need to go into ksqlDB, which is where you will find the query
    editor. We are going to use that editor to create our streams and tables.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The steps for this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a KTable with our weather topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the `TurboFan` topic into a data stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Join the table and stream to a new topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KSQL Server is a technology built upon the Kafka Streams API. The goal of this
    tool is to allow data enrichment and data transformation to occur in real time.
    In this recipe, we took the streams and converted one into a table of the most
    recent keys. We used these keys to update the values in our table. Next, we took
    a topic and created a stream view on top of it. Finally, we joined our table to
    our stream and created an output as a new stream. This is also a new topic in
    Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With KSQL Server, we can use more of the semantics that SQL provides, such
    as group by, count, and sum. Because Kafka is an unending set of data, we can
    use windowing to grab data by time segments. For example, we may want to know
    if the average temperature was over 100 degrees. We may want to look at this over
    a 20-second period. In KSQL, we can remote this as another stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
