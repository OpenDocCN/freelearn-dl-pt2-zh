- en: Optimizing with Microcontrollers and Pipelines
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Microcontrollers和Pipelines进行优化
- en: Most IoT devices run on **microcontroller units** (**MCUs**), while most machine
    learning happens on CPUs. One of the most cutting-edge innovations in AI is the
    ability to run models on constrained devices. In the past, AI was limited to large
    computers with traditional operating systems such as Windows or Linux. Now, small
    devices can execute machine learning models with technologies such as ONYX and
    TensorFlow Lite. These constrained devices are low cost, can use machine learning
    without an internet connection, and can save dramatically on cloud costs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数物联网设备运行在**微控制器单元**（**MCUs**）上，而大多数机器学习则发生在CPU上。人工智能领域最前沿的创新之一是在受限设备上运行模型。过去，人工智能局限于具有传统操作系统（如Windows或Linux）的大型计算机。现在，小型设备可以使用ONYX和TensorFlow
    Lite等技术执行机器学习模型。这些受限设备成本低廉，可以在没有互联网连接的情况下使用机器学习，并且可以大大节省云成本。
- en: Many IoT projects fail due to high cloud costs. IoT devices are often sold for
    a fixed price without a reoccurring subscription model. They then incur high cloud
    costs by performing machine learning or analytics. There is no reason this needs
    to be the case. Even for microcontrollers, the cost can be dramatically reduced
    by pushing machine learning and analytics to the device itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 许多物联网项目由于高昂的云成本而失败。物联网设备通常以固定价格出售，没有重复订阅模式。然后通过执行机器学习或分析产生高昂的云成本。没有理由这样做。即使对于微控制器，通过将机器学习和分析推送到设备本身，成本也可以大大降低。
- en: In this chapter, we are going to focus on two different development boards.
    The first is the **ESP32**, while the second is the **STM32**. The ESP32 is an
    MCU with Wi-Fi capabilities. They typically cost between $5 - $10 and are great
    for smaller projects where a few sensors need to be added to a device. An example
    of this would be a weather station. In contrast, the **STM32** development boards
    are typically used by electrical engineers to quickly start a project. There are
    dozens of different types of development boards but they use different compute
    modules such as the Cortext M0, M4, and M7\. In terms of the ESP32, electrical
    engineers typically use them as the compute on their IoT devices. Other platforms,
    such as the STM32, are considered starter kits. Electrical engineers use them
    to determine the chipset needed and then design their own boards that specifically
    meet their needs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点介绍两种不同的开发板。第一种是**ESP32**，第二种是**STM32**。ESP32是一种带有Wi-Fi功能的微控制器（MCU）。它们通常的成本在$5
    - $10之间，非常适合需要在设备上添加几个传感器的较小项目，比如天气站。相比之下，**STM32**开发板通常被电气工程师用来快速启动项目。有几十种不同类型的开发板，但它们使用不同的计算模块，如Cortex
    M0、M4和M7。在ESP32方面，电气工程师通常将它们作为其物联网设备上的计算单元。而STM32等其他平台被视为入门套件，电气工程师用它们来确定所需的芯片组，并设计出专门满足其需求的板子。
- en: Getting these boards running, talking to the cloud, and run ML models are non-trivial.
    This chapter focuses on getting the devices to perform complex computations and
    connecting to the cloud. To do this, we will explore the specific tools that are
    needed. Machine learning is typically done in higher-level languages such as Python,
    while the devices usually use C or C++.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让这些板子运行起来，与云通信，并运行机器学习模型是非常不平凡的。本章专注于使设备执行复杂计算并连接到云的过程。为此，我们将探索所需的具体工具。通常使用Python等高级语言进行机器学习，而设备通常使用C或C++。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下配方：
- en: Introduction to ESP32 with IoT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍ESP32与物联网
- en: Implementing an ESP32 environment monitor
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现ESP32环境监测器
- en: Optimizing hyperparameters
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化超参数
- en: Dealing with BOM changes
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理BOM变更
- en: Building machine learning pipelines with sklearn
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用sklearn构建机器学习流水线
- en: Streaming machine learning with Spark and Kafka
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark和Kafka进行流式机器学习
- en: Enriching data using Kafka's KStreams and KTables
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kafka的KStreams和KTables丰富数据
- en: Let's get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Introduction to ESP32 with IoT
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍ESP32与物联网
- en: In this recipe, we'll use an ESP32 to interface with Azure IoT Hub. Using a
    low-level device, we will code up the network interface. We will also need to
    deploy code to the ESP32 from a computer and then use a serial monitor to view
    the results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将使用ESP32与Azure IoT Hub进行接口交互。使用低级设备，我们将编写网络接口的代码。我们还需要从计算机部署代码到ESP32，然后使用串行监视器查看结果。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we are going to use the Arduino framework to program a bare-metal
    IoT solution. On your PC, you will need to install the Arduino **integrated development
    environment** (**IDE**). This will install the supporting software so that we
    can program the ESP32 using the Arduino framework. Next, we will install **Visual
    Studio Code** (**VS Code**). The VS Code IDE has an extension that makes board
    selection and library add-in easy. It also has a serial monitor and several built-in
    tools.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 Arduino 框架来编程裸金属 IoT 解决方案。在您的 PC 上，您需要安装 Arduino **集成开发环境**（**IDE**）。这将安装支持软件，以便我们可以使用
    Arduino 框架来编程 ESP32。接下来，我们将安装 **Visual Studio Code**（**VS Code**）。VS Code IDE
    有一个扩展程序，使得选择板子和添加库变得简单。它还有一个串行监视器和几个内置工具。
- en: 'Once you have installed the Arduino IDE and VS Code, you need to find the required
    extension tool in VS Code. Then search for `platformIO`, as shown in the following
    screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Arduino IDE 和 VS Code 后，您需要在 VS Code 中找到所需的扩展工具。然后，在下面的屏幕截图中搜索 `platformIO`：
- en: '![](img/ac9201b6-82fa-4510-bdf8-e0f89ed74475.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac9201b6-82fa-4510-bdf8-e0f89ed74475.png)'
- en: 'Once you''ve installed **PlatformIO IDE**, connect your ESP32 to your computer
    via USB. Then, find the **PlatformIO** button in the left panel. Next, from the
    **Quick Access** menu, click on **Open**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了 **PlatformIO IDE** 后，通过 USB 将您的 ESP32 连接到计算机。然后，在左侧面板中找到 **PlatformIO**
    按钮。接下来，在 **快速访问** 菜单中，单击 **打开**：
- en: '![](img/516942fe-4852-4c71-a93a-51aa70e78764.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/516942fe-4852-4c71-a93a-51aa70e78764.png)'
- en: 'From here, you can find the main PlatformIO window and click on **Open Project**:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以找到主 PlatformIO 窗口，并单击 **打开项目**：
- en: '![](img/eb90d9dc-2652-4d4b-b679-e7a726f6abf2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb90d9dc-2652-4d4b-b679-e7a726f6abf2.png)'
- en: 'The startup wizard will take you through choosing a name for your project,
    a framework (**Arduino**, in our case), and a board type. For the pinouts to work
    correctly, you must choose the right board type. Some boards have markings on
    them that will allow you to look up the board types, while others will not. For
    that reason, when purchasing an ESP32, it is important that you can determine
    the board''s type:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 启动向导将引导您选择项目名称、框架（在我们的案例中为 Arduino）和板子类型。为了使引脚正确工作，您必须选择正确的板子类型。一些板子上有标记，可以让您查找板子类型，而其他板子则没有。因此，在购买
    ESP32 时，确定板子类型非常重要：
- en: '![](img/b55aee0e-67b9-46fa-9e42-5b13ce2bc505.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b55aee0e-67b9-46fa-9e42-5b13ce2bc505.png)'
- en: Optionally, you can change where the project is stored.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，您可以更改项目存储位置。
- en: Next, you will need to install the Azure IoT Hub libraries and quickstart code.
    Go back to the **Quick Access** menu and click **Libraries.** Then, type `Azure
    IoT` into the search menu and click on the **AzureIoTHub** library from Microsoft.
    Once you've done this, change the release version to the latest that's available
    and click **Install**. Then, you need to do the same for the **AzureIoTUtility**,
    **WiFi**, and **AzureIoTProtocol_MQTT** libraries.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要安装 Azure IoT Hub 库和快速入门代码。返回到 **快速访问** 菜单，单击 **库**。然后，在搜索菜单中键入 `Azure
    IoT`，并点击来自 Microsoft 的 **AzureIoTHub** 库。完成后，将发布版本更改为最新可用版本，然后单击 **安装**。然后，对于
    **AzureIoTUtility**、**WiFi** 和 **AzureIoTProtocol_MQTT** 库，您需要执行相同的操作。
- en: 'Then, go back to the **AzureIoTHub** library. There is some quickstart code
    here that will allow you to quickly connect to the local Wi-Fi and IoT Hub. For
    this recipe, we will use some sample code to test our connection to IoT Hub. In
    the **Examples** section, you will find three code files called `iothub_II_telemetry_sample`,
    `sample_init`, and `iot_configs`, as shown in the following screenshot. Take the
    code from `iothub_II_telemetry_sample` and replace the `main.cpp` code in the
    source code. Next, create two new files called `sample_init.h` and `iot_configs.h` and
    paste the example code from the PlatformIO example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，返回到 **AzureIoTHub** 库。这里有一些快速入门代码，可以让您快速连接到本地 Wi-Fi 和 IoT Hub。对于此示例，我们将使用一些样例代码来测试与
    IoT Hub 的连接。在 **示例** 部分，您将找到三个名为 `iothub_II_telemetry_sample`、`sample_init` 和
    `iot_configs` 的代码文件，如下面的屏幕截图所示。从 `iothub_II_telemetry_sample` 中获取代码，并替换源代码中的 `main.cpp`
    代码。接下来，创建两个新文件分别称为 `sample_init.h` 和 `iot_configs.h`，并将示例代码从 PlatformIO 示例中粘贴进去：
- en: '![](img/4acf2bca-32e5-4284-b47e-575cdef443e8.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4acf2bca-32e5-4284-b47e-575cdef443e8.png)'
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'The steps for this recipe are as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的步骤如下：
- en: 'Add your Wi-Fi connection string. Change the strings on lines 10 and 11 of
    the `iot_configs.h` file:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加您的 Wi-Fi 连接字符串。更改 `iot_configs.h` 文件中第 10 和第 11 行的字符串：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Get a device connection string from Azure IoT Hub and insert it on line 19
    of `iot_configs.h`:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Azure IoT Hub 获取设备连接字符串，并将其插入到 `iot_configs.h` 的第19行：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With your ESP32 attached to your computer via a USB, click on the **PlatformIO**
    icon in the left panel and then click on **Upload and Monitor**:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的 ESP32 通过 USB 连接到计算机，在左侧面板中点击 **PlatformIO** 图标，然后点击 **上传并监控**：
- en: '![](img/d9925f9c-470e-4d5b-813c-7d6f20e7829a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9925f9c-470e-4d5b-813c-7d6f20e7829a.png)'
- en: How it works...
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Here, we uploaded the code to the ESP32 and enabled the serial monitor. The
    lower panel in Visual Studio should start displaying text when it has connected
    to a Wi-Fi network and sent messages to IoT Hub. We also created some sample code
    for receiving cloud-to-device messages.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将代码上传到 ESP32 并启用了串行监视器。在 Visual Studio 的下方面板应该会在连接到 Wi-Fi 网络并向 IoT Hub
    发送消息时开始显示文本。我们还创建了一些用于接收云到设备消息的示例代码。
- en: There's more...
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: In this recipe, we have only scratched the surface of what the IoT Hub SDK is
    capable of doing. For example, we could even send cloud-to-device messages that
    allow us to queue up a set of messages for the device to digest. We could have
    also sent a direct message. This is like a cloud-to-device message that sends
    a message to a device but does not queue the message. If a device is offline,
    the message never gets sent. Another option would have been to upload to a blob.
    This allows us to upload log or binary files securely and directly to blob storage.
    Finally, we could have used device twins, which allow us to have a configuration
    file set on the device and can be queried across a fleet of devices. This would
    help us find out if an update did not work or a setting did not get set properly.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们仅仅触及了 IoT Hub SDK 能做的一小部分。例如，我们甚至可以发送云到设备消息，允许我们为设备排队一组消息进行处理。我们也可以发送直接消息。这类似于云到设备消息，它发送消息到设备，但不会将消息排队。如果设备离线，消息就不会被发送。另一个选择是上传到
    Blob。这允许我们安全地直接上传日志或二进制文件到 Blob 存储。最后，我们还可以使用设备双生对象，允许我们在设备上设置配置文件，并且可以在设备群中查询。这有助于我们找出更新未生效或设置未正确设置的情况。
- en: Implementing an ESP32 environment monitor
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施 ESP32 环境监控
- en: Setting up a simple environment monitor using hardware is fairly straightforward.
    In this recipe, we are going to take a proof of concept with some simple hardware
    attached to it. In the *There's more...* section, we will talk about how to take
    a design like this and go to production with it, even if you don't have **electrical
    engineers** (**EEs**) on your team. To do this, we are going to introduce **Fritzing**,
    a hardware designer. Although it is not as powerful as **KiCad** or **Altuim Designer**,
    it is a tool that a person who is not an electrical engineer can give to an EE
    or a manufacturing partner and get circuit boards designed and printed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用硬件设置简单的环境监控相当简单。在这个示例中，我们将使用一些简单的硬件来进行概念验证。在 *更多内容* 部分，我们将讨论如何进行设计，并进行批量生产，即使您的团队没有**电气工程师**（**EEs**）。为此，我们将介绍**Fritzing**，一个硬件设计工具。虽然它没有**KiCad**或**Altuim
    Designer**那么强大，但非电气工程师也能使用它，并将电路板设计并打印出来。
- en: The goal of this recipe is not really to show you how to create a temperature
    and humidity sensor. Temperature and humidity sensors are the *Hello World* of
    IoT. Instead, this recipe focuses on implementing these on constrained devices
    in a rapid way via manufacturing. Not all IoT projects can be done this way. There
    are certainly IoT projects that require EEs to build out complex devices, such
    as one with a video display and sound, or high-speed devices such as those used
    in the medical industry.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的目标并不是教您如何创建温度和湿度传感器。温度和湿度传感器是 IoT 的 *Hello World*。相反，这个示例侧重于通过制造快速在受限设备上实施这些功能。并非所有的
    IoT 项目都可以这样做。当然也有需要电气工程师构建复杂设备的 IoT 项目，例如具有视频显示和声音的设备，或者医疗行业中使用的高速设备。
- en: Getting ready
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we are going to build off of the previous recipe. We are going
    to use an ESP32 and must have the Arduino IDE and VS Code installed. In VS Code,
    we are going to add the `PlatformIO` extension. Eventually, we are going to attach
    the ESP32 to the computer we are using via USB but until we attach the sensor,
    leave it unattached. For this recipe, you will need a DHT11 digital humidity and
    temperature sensor, jumper cables, a 10k ohm resistor, and a breadboard. You should
    be able to purchase all of these components for around $20.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将基于前一个配方进行构建。我们将使用ESP32，并且必须安装Arduino IDE和VS Code。在VS Code中，我们将添加`PlatformIO`扩展。最终，我们将通过USB将ESP32连接到我们使用的计算机，但在连接传感器之前，请保持未连接状态。对于这个配方，您将需要一个DHT11数字湿度和温度传感器，跳线电缆，一个10k欧姆电阻器和一个面包板。您应该能够以大约20美元购买所有这些组件。
- en: From here, we will need to go into VS Code and, using the `PlatformIO` extension,
    create a new project. Then, you will need to install the DHT sensor library from
    the `PlatformIO` library manager. You will then need to download Fritzing. It
    is an open source program. You can contribute to the project on their website
    and receive a copy, but you can also go to GitHub and, under **Releases**, download
    and install the program. The ESP32 comes in numerous hardware versions. Your ESP32
    may have different pins and capabilities. The datasheet for your ESP32 will tell
    you what pins it has. For example, there are pins that can do things with clock
    cycles or voltage measurements. There's also power and ground on various pins.
    These are used to power external sensors. By reviewing the DHT11 and ESP32, you
    can create a mapping from the inputs and outputs of various components.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们需要进入VS Code，并使用`PlatformIO`扩展创建一个新项目。然后，您需要从`PlatformIO`库管理器安装DHT传感器库。您还需要下载Fritzing。这是一个开源程序。您可以在其网站上为项目做出贡献并获得副本，但您还可以转到GitHub，在**Releases**下下载并安装该程序。ESP32有多个硬件版本。您的ESP32可能具有不同的引脚和功能。您ESP32的数据表将告诉您它具有哪些引脚。例如，有些引脚可以用来进行时钟周期或电压测量。还有各种引脚上的电源和地面。这些用于给外部传感器供电。通过查看DHT11和ESP32，您可以创建各种组件的输入和输出的映射。
- en: How to do it...
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The steps for this recipe are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方的步骤如下：
- en: 'Open Fritzing and on the right-hand side panel''s **Parts** section, click
    on the menu and select **Import...**. Then, select the ESP32 and DHT11\. Both
    of these can be found in the source code for this chapter:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Fritzing，在右侧面板的**Parts**部分，单击菜单，然后选择**Import...**。然后，选择ESP32和DHT11。这两者都可以在本章的源代码中找到：
- en: '![](img/c3b893d7-3aac-41e0-a58a-acf0eabda783.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3b893d7-3aac-41e0-a58a-acf0eabda783.png)'
- en: 'Search for a resistor in the Parts list. Once you''ve dragged it onto the screen,
    adjust its properties to **4.7kΩ**:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在零件列表中搜索电阻器。一旦将其拖到屏幕上，调整其属性为**4.7kΩ**：
- en: '![](img/6f437f6f-7737-4cdc-bca3-a8fe17c7d1be.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f437f6f-7737-4cdc-bca3-a8fe17c7d1be.png)'
- en: 'Now, place the DTH11 on the board and hook up the power rails using 3.3 volts
    and a ground:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在板子上放置DTH11并使用3.3伏特和地面连接电源轨：
- en: '![](img/8d9806c6-8966-4591-a175-5ed425cb9370.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d9806c6-8966-4591-a175-5ed425cb9370.png)'
- en: 'Then, attach the power rails to the board. Also, connect the **General-Purpose
    Input/Output** (**GPIO**) pin 27 to the DHT11s data pin. We must also add a 4.7k
    ohm resistor between the 3.3V power rail and the data pin on the DHT11:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将电源轨连接到板上。同时，将**通用输入/输出**（**GPIO**）引脚27连接到DHT11的数据引脚。我们还必须在3.3V电源轨和DHT11的数据引脚之间添加一个4.7k欧姆电阻器：
- en: '![](img/d3850df6-0300-42da-a302-d022e29bd697.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3850df6-0300-42da-a302-d022e29bd697.png)'
- en: Next, connect the ESP32 to your computer and pull up the `/src/main.cpp` file
    from the `PlatformIO` project we started in the *Getting ready* section.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将ESP32连接到计算机，并从我们在*准备工作*部分开始的`PlatformIO`项目中拉起`/src/main.cpp`文件。
- en: 'In `main.cpp`, include the `DHT.h` library reference:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`main.cpp`中，包含`DHT.h`库的引用：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create references to your data pin on the ESP32 and the DHT sensor type:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建对ESP32上的数据引脚和DHT传感器类型的引用：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Initialize the `DHT` variable:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`DHT`变量：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set up the serial port and print a test message. Then, initialize the `dht`
    object:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置串行端口并打印测试消息。然后初始化`dht`对象：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the main loop, read the temperature and humidity sensors. Then, call out
    to the printing section and wait 2 seconds before continuing the loop:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主循环中，读取温度和湿度传感器的数据。然后，调用打印部分，并在继续循环之前等待2秒：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create a function that checks for errors. If none are found, print the results:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个检查错误的函数。如果没有找到错误，则打印结果：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理...
- en: Here, the temperature and humidity sensors get power and ground from the ESP32\.
    Once we ensured this happens, we specified a data GPIO pin and added a resistor
    to match up the voltages.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，温湿度传感器从ESP32获取电源和地线。一旦我们确保这一点发生，我们指定了一个数据GPIO引脚，并添加了一个电阻来匹配电压。
- en: When you purchase a DHT11, some come with three pins, while others come with
    four. You can adjust these pins based on the pinout specification for the sensor.
    Similarly, different manufactures of the ESP32 have different pinout specifications.
    Before working with any hardware, it is always important to check the datasheets
    of that particular product.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当您购买DHT11时，有些带有三个引脚，而其他带有四个引脚。您可以根据传感器的引脚规格调整这些引脚。同样，ESP32的不同制造商有不同的引脚规格。在处理任何硬件之前，检查该特定产品的数据表格总是很重要的。
- en: There's more...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: At this point, we have a working prototype. There are several paths you can
    take to get the boards designed and the product mass-produced in a factory. You
    can hire EEs to do this, but for something this small, you can often go to a company
    that specializes in board design, such as Seeed Studios. Many manufacturing plants
    offer a hardware designing service that can take the Fritzing sketch and turn
    it into a product. These manufacturing plants can often print out prototypes and
    mass-produce the boards when you are ready.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们有一个工作原型。您可以选择多种路径来设计电路板，并在工厂中批量生产产品。您可以雇用电子工程师来完成此任务，但对于这种小型项目，您通常可以找一家专门从事电路板设计的公司，例如Seeed
    Studios。许多制造工厂提供硬件设计服务，可以将Fritzing草图转化为产品。这些制造工厂通常可以打印出原型并在您准备就绪时批量生产电路板。
- en: Optimizing hyperparameters
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化超参数
- en: There are many different ways of tuning hyperparameters. If we were to do this
    manually, we could put random variables into our parameters and see which one
    was the best. To do this, we could perform a grid-wise approach, where we map
    the possible options and put in some random tries and keep going down a route
    that seems to produce the best outcomes. We might use statistics or machine learning
    to help us determine what parameters can give us the best results. These different
    approaches have pros and cons, depending on the shape of the loss of the experiment.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 调整超参数有许多不同的方法。如果我们手动进行此操作，我们可以将随机变量放入我们的参数中，并查看哪一个是最好的。为了做到这一点，我们可以执行网格式的方法，其中我们映射可能的选项并放入一些随机尝试，并继续进行看起来产生最佳结果的路线。我们可能使用统计或机器学习来帮助我们确定哪些参数可以给我们带来最佳结果。这些不同的方法具有依赖于实验损失形状的优缺点。
- en: There are various machine learning libraries that can help us perform these
    types of common tasks easier. `sklearn`, for example, has a `RandomizedSearchCV`
    method that, given a set of parameters, will perform a search for the best model
    with the least loss. In this recipe, we will expand on the *Classifying chemical
    sensors with decision trees* recipe from [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*, and use a random forest. However, we will also add
    a grid search in order to optimize our results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机器学习库可以帮助我们更轻松地执行这些常见任务。例如，`sklearn`具有一个`RandomizedSearchCV`方法，可以根据一组参数搜索最佳模型以获得最小损失。在本篇文章中，我们将从[第三章](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml)的*物联网机器学习*中扩展*使用决策树分类化学传感器*的食谱，并使用随机森林。然而，我们还将添加网格搜索以优化我们的结果。
- en: Getting ready
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will be using the MOX sensor dataset from [Chapter 3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml),
    *Machine Learning for IoT*. There, we saved our data in Delta Lake. Due to this,
    we can pull it easily into our Spark Notebook. We will also be using the `koalas`,
    `sklearn`, and `numpy` Python packages.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇文章中，我们将使用[第三章](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml)的MOX传感器数据集，*物联网机器学习*中，我们将我们的数据保存在Delta
    Lake中。由于这一点，我们可以轻松地将其拉入我们的Spark Notebook中。我们还将使用Python包`koalas`，`sklearn`和`numpy`。
- en: How to do it...
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps for this recipe are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此食谱的步骤如下：
- en: 'Import the necessary libraries:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Import the data from Databricks Delta Lake:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Databricks Delta Lake导入数据：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Select and encode the data:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择和编码数据：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Select the parameters you wish to tune:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您希望调整的参数：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create an instance of the random forest classifier algorithm so that we can
    adjust its hyperparameters later:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个随机森林分类器算法的实例，以便我们稍后可以调整其超参数：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set up a grid search estimator so that we can tune our parameters:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个网格搜索估计器，以便我们可以调整参数：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Train the decision tree classifier:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练决策树分类器：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Predict the response for the test dataset:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测测试数据集的响应：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Print the winning set of hyperparameters:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印获胜的超参数集：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: This algorithm is an easy to implement algorithm that we worked with in [Chapter
    3](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml), *Machine Learning for IoT*. There,
    we took a random stab at choosing an algorithm. Then, we had it go through one
    run of the code to get the necessary output. In this recipe, however, we caused
    it to run through many more runs to find the best estimator we could find. We
    could have done the same thing using a spreadsheet to keep track of all of the
    runs. However, this allows us to automate the process of performing experiments
    and tracking the results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法是我们在[第3章](4dd8d581-73ef-4698-9a85-85b415c0b2f2.xhtml)中使用的易于实施的算法，*物联网机器学习*。在那里，我们随意选择了一个算法。然后，我们让它运行一次代码以获得必要的输出。然而，在本示例中，我们使其运行更多次，以找到我们能找到的最佳估计器。我们可以使用电子表格做同样的事情来跟踪所有运行的结果。然而，这使我们能够自动化执行实验并跟踪结果的过程。
- en: Dealing with BOM changes
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理BOM更改
- en: '**Bill of Materials** (**BOMs**) are the components that make up the device.
    These can be resistors, chips, and other components. The life cycle of a typical
    IoT product is about 10 years. In that time, things can change with the product.
    A component manufacturer could discontinue a part such as a chip line. Outsourced
    manufacturers typically perform BOM optimization on a board layout, though BOM
    optimization can change the quality of the device. For example, it can change
    the sensitivity of the sensor or the lifetime of a device.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**物料清单**（**BOMs**）是构成设备的组件。这些可以是电阻器、芯片和其他组件。典型物联网产品的生命周期约为10年。在此期间，产品可能发生变化。例如，组件制造商可能会停产某个部件，如芯片系列。外包制造商通常会在电路板布局上进行BOM优化，尽管BOM优化可能会改变设备的质量。例如，它可能会改变传感器的灵敏度或设备的寿命。'
- en: This can throw off trained models and can have a dramatic effect on any remaining
    useful life calculations and predictive maintenance models. When working with
    IoT and machine learning, tracking changes that have been made to any remaining
    useful life based on BOM and factory changes can help us detect issues with the
    quality and longevity of a device.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会影响经过训练的模型，并对剩余有效寿命的计算和预测性维护模型产生显著影响。在处理物联网和机器学习时，跟踪基于BOM和工厂更改的剩余有效寿命的变化可以帮助我们检测设备质量和寿命的问题。
- en: This is typically done with a database. When a device is made in a factory,
    that device's serial number, BOM version, and factory details are stored in that
    factory. This is where a total expected lifespan can be applied to the device.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是通过数据库完成的。当设备在工厂中生产时，该设备的序列号、BOM版本和工厂详细信息存储在该工厂中。这是可以为设备应用总预期寿命的地方。
- en: Getting ready
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we are going to spin up a Docker instance of a SQL Server database.
    To get started, you must install Docker.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将启动一个SQL Server数据库的Docker实例。要开始，请安装Docker。
- en: 'The next step is to build and run a SQL Server database using `docker`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用`docker`构建和运行SQL Server数据库：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, run the Docker container:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行Docker容器：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have a working SQL Server database, we need to add a database and two
    tables for it. You can connect to the SQL database by installing the `mssql` plugin
    for VS Code and then connecting to the database with the username and password
    we had in the Docker file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个工作的SQL Server数据库，我们需要为其添加一个数据库和两个表。你可以通过在VS Code中安装`mssql`插件，然后使用Docker文件中的用户名和密码连接到SQL数据库：
- en: '![](img/e3ffca53-2311-4340-8442-589d4d2e57c0.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3ffca53-2311-4340-8442-589d4d2e57c0.png)'
- en: Once you've done this, click on the new SQL Server tool in the left panel. Then,
    click on the plus (**+**) button to be taken through a wizard that will help you
    create a database connection. When the wizard asks you for the **ado.net** connection
    string type in `localhost`, it will ask you for a username and password. Type
    in `sa` for the username and `Password!` for the password.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，在左侧面板中点击新的SQL Server工具。然后，点击加号（**+**）按钮，跟随向导创建数据库连接。当向导询问**ado.net**连接字符串时输入`localhost`，它会要求你输入用户名和密码。用户名输入`sa`，密码输入`Password!`。
- en: 'Then, run the following SQL statements by clicking on the green arrow in the
    upper right of the screen:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过点击屏幕右上角的绿色箭头运行以下SQL语句：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: From here, `pip` install `pyodbc` from `pypi` and create a new Python script
    in VS Code.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，从`pypi`安装`pyodbc`并在VS Code中创建一个新的Python脚本。
- en: How to do it...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'The steps for this recipe are as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的步骤如下：
- en: 'Import the `pyodbc` library:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pyodbc` 库：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Connect to the database:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接数据库：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create a database connection cursor so that you can run queries:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据库连接光标，以便可以运行查询：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Insert the product and manufacturing date into the `Device` table and commit
    the transaction:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将产品和制造日期插入 `Device` 表并提交事务：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once you''ve calculated the remaining useful life of that product, add that
    information to the database:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦计算出产品的剩余有用寿命，请将该信息添加到数据库中：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works...
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we showed you how to use a database to track your results over
    time. Databases allow us to insert and update information about our models over
    time.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们向您展示了如何使用数据库来跟踪您随时间推移的结果。数据库允许我们随时间插入和更新有关我们模型的信息。
- en: There's more...
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: In this example, we looked at products. Tracking the end life of devices can
    give our models real-world feedback and lets us know when they should be retrained.
    We can store the predicted error or loss rate and compare it against real-world
    devices.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们关注的是产品。跟踪设备的寿命结束可以为我们的模型提供真实世界的反馈，并告诉我们何时应该重新训练它们。我们可以存储预测的错误或损失率，并将其与真实设备进行比较。
- en: Building machine learning pipelines with sklearn
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用sklearn构建机器学习管道
- en: The `sklearn pipeline` package makes it easy for us to manage multiple stages
    of feature engineering and modeling. Performing machine learning experiments is
    more just than training models. It is a combination of several factors. First,
    you need to cleanse and transform the data. Then, you must enrich the data with
    feature engineering. These common tasks can be built up into a series of steps
    called a **pipeline**. When we're trying out different variants on our experiments,
    we can use these pipelines to train a series of very complex steps so that they
    become something simple and manageable that can be reused.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn pipeline` 软件包使我们能够管理特征工程和建模的多个阶段。进行机器学习实验不仅仅是训练模型。它结合了几个因素。首先，您需要清洗和转换数据。然后，您必须通过特征工程丰富数据。这些常见任务可以组合成一系列称为**管道**的步骤。当我们在实验中尝试不同的变体时，我们可以使用这些管道来训练一系列非常复杂的步骤，使它们变成一些简单和可管理的东西，可以重复使用。'
- en: Getting ready
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we are going to use the data that was previously feature engineered
    in the *Enhancing data using feature engineering* recipe of [Chapter 4](a40f2c07-0e51-46c6-a3cf-66d5c46477c4.xhtml),
    *Deep Learning for Predictive Maintenance*. In that recipe, we put data into Databricks
    and then cleansed that data so that we could use it in additional experiments.
    To retrieve this data, we are simply going to use a `select` statement from Delta
    Lake. For this recipe, you will need `pandas` and `sklearn` installed on your
    Spark cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用之前在[第4章](a40f2c07-0e51-46c6-a3cf-66d5c46477c4.xhtml)的*增强数据使用特征工程*示例中进行特征工程的数据，*预测性维护的深度学习*。在那个示例中，我们将数据放入Databricks，然后清洗数据，以便在其他实验中使用。要检索此数据，我们只需使用Delta
    Lake的`select`语句。对于这个示例，您需要在您的Spark集群上安装 `pandas` 和 `sklearn`。
- en: How to do it...
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The steps for this recipe are as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例的步骤如下：
- en: 'Import `pandas` and `sklearn`:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 和 `sklearn`：
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import the data from Delta Lake:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Delta Lake导入数据：
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create transformers that convert data into standardized numeric or categorical
    data:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建转换器将数据转换为标准化的数值或分类数据：
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Extract the necessary features and create a processor:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取必要的特征并创建处理器：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a random forest pipeline step:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建随机森林管道步骤：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Fit the classifier:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合分类器：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Perform the classification:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行分类：
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How it works...
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Building machine learning pipelines is very common in data science. It helps
    simplify complex operations and adds a level of reusability to the software code.
    In this recipe, we used `sklearn` to perform complex operations on a simple pipeline.
    In our pipeline, we created a set of transformers. For the numeric numbers, we
    used a scaler, while for the categorical numbers, we used one-hot encoding. Next,
    we created a processor pipeline. In our case, we used a random forest classifier.
    Note that this pipeline step is an array, so we could pass more classifiers into
    our array. However, for the sake of simplicity, we will save that for the *There's
    more...* section. Finally, we trained and got the predictions from our model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习流水线的构建在数据科学中非常普遍。它有助于简化复杂操作，并为软件代码增加了可重复使用性。在这个示例中，我们使用`sklearn`来在一个简单的流水线上执行复杂操作。在我们的流水线中，我们创建了一组转换器。对于数值数据，我们使用了一个缩放器，而对于分类数据，我们使用了独热编码。接下来，我们创建了一个处理器流水线。在我们的案例中，我们使用了一个随机森林分类器。请注意，这个流水线步骤是一个数组，所以我们可以将更多的分类器传递到我们的数组中。然而，为了简单起见，我们将把这个留到*更多内容*部分。最后，我们训练并从我们的模型中得到了预测结果。
- en: There's more...
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'As we mentioned in the introduction to this recipe, the purpose of pipelines
    is to enable you to tweak your pipeline steps easily. In this section, we are
    going to tweak those steps to help us achieve a higher accuracy rate. In this
    case, we are simply going to extend the preceding code and add a classifier array
    of machine learning algorithms. From there, we will score the model so that we
    can determine which one was the best. The code for this is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这个示例的介绍中提到的，流水线的目的是使您能够轻松调整流水线步骤。在本节中，我们将调整这些步骤以帮助我们实现更高的准确率。在这种情况下，我们只是扩展了前面的代码，并添加了一个机器学习算法分类器数组。从那里，我们将评分模型，以便确定哪一个是最好的。此代码如下所示：
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Streaming machine learning with Spark and Kafka
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark和Kafka进行流式机器学习
- en: Kafka is a real-time streaming message hub. Combined with Kafka, Databrick's
    ability to ingest streams and perform machine learning on them in real time allows
    you to perform powerful machine learning in near real time. In this recipe, we
    are going to use  Confluent. Confluent is the company that was founded by the
    creators of Kafka. They have a cloud offering on Azure, GCP, and AWS. We are also
    going to use Databricks, which is available on Azure, GCP, and AWS.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个实时流消息中心。结合Kafka，Databrick的能力可以实时摄取流并对其进行机器学习，从而使您能够在几乎实时中执行强大的机器学习。在这个示例中，我们将使用Confluent。Confluent是由Kafka的创造者创立的公司。他们在Azure、GCP和AWS上提供云服务。我们还将使用在Azure、GCP和AWS上可用的Databricks。
- en: Getting ready
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'In the cloud marketplace, spin up Confluent and Databricks. This will give
    you a Kafka and Spark system that is elastically scalable. Once you''ve spinned
    up these systems, go to the Confluent website at [https://confluent.cloud](https://confluent.cloud)
    and enter the username and password you set up in the cloud marketplace. Then,
    click on **Create cluster**. Follow the wizard to create your first cluster. Once
    you are in your cluster, click on **API access** in the menu. Then, find the Create
    key button that will allow you to create an API access key:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在云市场中，启动Confluent和Databricks。这将为您提供一个具有弹性可扩展性的Kafka和Spark系统。一旦您启动了这些系统，请访问Confluent网站[https://confluent.cloud](https://confluent.cloud)，并输入您在云市场中设置的用户名和密码。然后，点击**创建集群**。按照向导创建您的第一个集群。一旦您进入集群，请点击菜单中的**API访问**。然后，找到创建密钥按钮，该按钮将允许您创建一个API访问密钥：
- en: '![](img/cf6bfab6-6b2a-40c4-beb8-c8455af0d354.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf6bfab6-6b2a-40c4-beb8-c8455af0d354.png)'
- en: Once you've created a key, jot down its username and password; you will need
    these details later.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您创建了密钥，请记下其用户名和密码；稍后您会需要这些详细信息。
- en: 'Next, go to the **Topics** section and, using the **Create topic** button,
    create two topics: one called `Turbofan` and another called `Turbofan_RUL`. Next,
    we will create a Python file so that we can test our new topic. Create a Python
    file with the following code to produce a message for the `TurboFan` topic:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，转到**主题**部分，并使用**创建主题**按钮创建两个主题：一个名为`Turbofan`，另一个名为`Turbofan_RUL`。接下来，我们将创建一个Python文件，以便我们可以测试我们的新主题。创建一个带有以下代码的Python文件，以生成`TurboFan`主题的消息：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, you can go to your topic in the Confluent Cloud UI and watch for messages
    on that topic by selecting the topic (**TurboFan**) and then **Messages**:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以转到Confluent Cloud UI中的主题，并通过选择主题(**TurboFan**)然后点击**消息**来查看该主题上的消息：
- en: '![](img/5ce1a793-511d-4af5-8328-65cd3815cf4a.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ce1a793-511d-4af5-8328-65cd3815cf4a.png)'
- en: If you run the preceding code, you will see a message going to Kafka.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行上述代码，您将看到一条消息发送到Kafka。
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'The steps for this recipe are as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方的步骤如下：
- en: 'Stream Kafka into Databricks. In a Databricks notebook, enter the following
    code:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Kafka流入Databricks。在Databricks笔记本中，输入以下代码：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Specify the fields from the JSON file to serialize them as objects:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定从JSON文件中的字段序列化为对象：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define the function that will do the inference:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义执行推理的函数：
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Perform inference using the UDF and save the results in a DataFrame:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用UDF执行推理并将结果保存到DataFrame中：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Write the devices that are failing to a different DataFrame:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将失败的设备写入另一个DataFrame：
- en: '[PRE38]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Stream that DataFrame back to Kafka as a new topic and write the results to
    Kafka:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该DataFrame作为一个新主题流回Kafka，并将结果写入Kafka：
- en: '[PRE39]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Kafka is a streaming engine designed to handle large amounts of data. Data is
    ingested into Kafka and then sent to Spark, where the probability can be sent
    back to Kafka. In this example, we used Confluent Cloud and Databricks. These
    managed services can be found on all major cloud marketplaces.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个设计用于处理大量数据的流引擎。数据被摄取到Kafka中，然后发送到Spark，可以将概率发送回Kafka。在这个示例中，我们使用了Confluent
    Cloud和Databricks。这些托管服务可以在所有主要的云市场上找到。
- en: In this recipe, we received real-time data from the engines. Then, we streamed
    that data in Spark and ran an inference on it. Once we'd received the results,
    we streamed them back into a separate Kafka topic. Using a Kafka topic and Kafka
    itself allows us to push that data into a database, data lakes, and microservices,
    all from a single data pipeline.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们从引擎接收了实时数据。然后，我们在Spark中将这些数据流式传输并对其进行推理。一旦我们收到结果，我们将其流回一个单独的Kafka主题。使用Kafka主题和Kafka本身，我们可以将这些数据推送到数据库、数据湖和微服务中，所有这些都来自一个单一的数据管道。
- en: There's more...
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In addition to putting all of the data into a topic so that it can be dumped
    into a data store, we can stream the data into an alerting system. To do this,
    we can make a Kafka consumer, as shown in the following code. Here, we will stream
    the code down to a local system and then have a `msg_process` function that we
    can use to write to an alert system such as **Twilio**:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将所有数据放入主题以便将其转储到数据存储中，我们还可以将数据流式传输到警报系统中。为此，我们可以创建一个Kafka消费者，如下面的代码所示。在这里，我们将数据流式传输到本地系统，然后有一个`msg_process`函数，我们可以用它来写入警报系统，比如**Twilio**：
- en: '[PRE40]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Enriching data using Kafka's KStreams and KTables
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka的KStreams和KTables丰富数据
- en: Often, in IoT, there are external data sources that must be included. This could
    be weather data that affects the performance of the device or data from other
    nearby devices. An easy way of doing this is to use Kafka KSQL Server. Like we
    did in the previous recipe, we are going to use Confluent Cloud's KSQL Server,
    which you can get if you have a Confluent Cloud subscription.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在物联网中，经常有必须包含的外部数据源。这可能是影响设备性能的天气数据或来自其他附近设备的数据。一个简单的方法是使用Kafka KSQL服务器。与之前的示例一样，我们将使用Confluent
    Cloud的KSQL服务器，如果您有Confluent Cloud订阅的话。
- en: In this recipe, we are going to get data from a weather service topic and put
    it into a KTable. A KTable is similar to a database table. All data coming into
    Kafka comes in key-value pairs. With KTable, when data comes in with a new key,
    we insert it into our KTable. If it contains a key that already exists in our
    KTable, we update it. We are also going to convert our topic into a KStream. This
    allows us to run standard SQL-like queries over our table and stream. By doing
    this, we can, for example, query the current weather and join it to the engine
    data from our previous recipe. This allows us to enrich the data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将从天气服务主题获取数据，并将其放入KTable中。KTable类似于数据库表。所有进入Kafka的数据都以键值对的形式出现。使用KTable时，当新的键入数据到来时，我们将其插入到我们的KTable中。如果它包含在我们的KTable中已经存在的键，则我们会对其进行更新。我们还将把我们的主题转换为KStream。这使我们可以在我们的表和流上运行标准类似SQL的查询。通过这样做，例如，我们可以查询当前天气并将其与我们之前的配方中的引擎数据进行连接。这样可以丰富数据。
- en: Getting ready
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In the **Confluent Cloud ksqlDB** portal, go to the **ksqlDB** tab and add
    an application:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Confluent Cloud ksqlDB**门户中，转到**ksqlDB**选项卡并添加一个应用程序：
- en: '![](img/32796f45-57f0-4b7f-a097-1757fcd58d63.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32796f45-57f0-4b7f-a097-1757fcd58d63.png)'
- en: Once you've completed all the setup steps, you will have a KSQL query editor.
    From here, we will edit our queries.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有设置步骤后，您将拥有一个KSQL查询编辑器。从这里，我们将编辑我们的查询。
- en: This recipe is a continuation of the previous recipe. You will need the streaming
    `TurboFan` data that we set up running in Kafka. You will also need to run the `weatherstreamer.py` Kafka
    Weather Streamer Python script, which can be found in the `ch10` directory of
    the GitHub repository for this book.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方是上一个配方的延续。你需要运行在Kafka中设置的流数据`TurboFan`，还需要运行名为`weatherstreamer.py`的Kafka天气流Python脚本，该脚本可以在本书GitHub存储库的`ch10`目录中找到。
- en: Finally, you will need to go into ksqlDB, which is where you will find the query
    editor. We are going to use that editor to create our streams and tables.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要进入ksqlDB，在那里你会找到查询编辑器。我们将使用该编辑器来创建我们的流和表。
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'The steps for this recipe are as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方的步骤如下：
- en: 'Create a KTable with our weather topic:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的天气主题创建一个KTable：
- en: '[PRE41]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Convert the `TurboFan` topic into a data stream:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`TurboFan`主题转换为数据流：
- en: '[PRE42]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Join the table and stream to a new topic:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将表和流连接到一个新主题：
- en: '[PRE43]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: How it works...
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: KSQL Server is a technology built upon the Kafka Streams API. The goal of this
    tool is to allow data enrichment and data transformation to occur in real time.
    In this recipe, we took the streams and converted one into a table of the most
    recent keys. We used these keys to update the values in our table. Next, we took
    a topic and created a stream view on top of it. Finally, we joined our table to
    our stream and created an output as a new stream. This is also a new topic in
    Kafka.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: KSQL Server是建立在Kafka Streams API之上的技术。这个工具的目标是允许实时进行数据增强和数据转换。在这个配方中，我们获取了这些流并将其中一个转换为最近键的表。我们使用这些键来更新我们表中的值。接下来，我们取了一个主题，并在其上创建了一个流视图。最后，我们将我们的表与我们的流进行了连接，并创建了一个输出作为一个新的流。这也是Kafka中的一个新主题。
- en: There's more...
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多的…
- en: 'With KSQL Server, we can use more of the semantics that SQL provides, such
    as group by, count, and sum. Because Kafka is an unending set of data, we can
    use windowing to grab data by time segments. For example, we may want to know
    if the average temperature was over 100 degrees. We may want to look at this over
    a 20-second period. In KSQL, we can remote this as another stream:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KSQL Server，我们可以利用SQL提供的更多语义，例如group by、count和sum。由于Kafka是一个无尽的数据集，我们可以使用窗口来按时间段获取数据。例如，我们可能想知道平均温度是否超过了100度。我们可能想在20秒的时间段内查看这个数据。在KSQL中，我们可以将其远程化为另一个流：
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
