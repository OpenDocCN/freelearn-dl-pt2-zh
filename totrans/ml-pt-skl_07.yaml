- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Combining Different Models for Ensemble Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合不同模型进行集成学习
- en: 'In the previous chapter, we focused on the best practices for tuning and evaluating
    different models for classification. In this chapter, we will build upon those
    techniques and explore different methods for constructing a set of classifiers
    that can often have a better predictive performance than any of its individual
    members. We will learn how to do the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们专注于调整和评估不同分类模型的最佳实践。在本章中，我们将基于这些技术，探讨构建一组分类器的不同方法，这些方法通常比其单个成员具有更好的预测性能。我们将学习如何执行以下操作：
- en: Make predictions based on majority voting
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于多数投票进行预测
- en: Use bagging to reduce overfitting by drawing random combinations of the training
    dataset with repetition
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用装袋（bagging）通过重复随机组合训练数据集来减少过拟合。
- en: Apply boosting to build powerful models from weak learners that learn from their
    mistakes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用提升（boosting）来从学习者的错误中构建强大的模型
- en: Learning with ensembles
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成学习
- en: The goal of **ensemble methods** is to combine different classifiers into a
    meta-classifier that has better generalization performance than each individual
    classifier alone. For example, assuming that we collected predictions from 10
    experts, ensemble methods would allow us to strategically combine those predictions
    by the 10 experts to come up with a prediction that was more accurate and robust
    than the predictions by each individual expert. As you will see later in this
    chapter, there are several different approaches for creating an ensemble of classifiers.
    This section will introduce a basic explanation of how ensembles work and why
    they are typically recognized for yielding a good generalization performance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成方法**的目标是将不同的分类器组合成一个元分类器，其泛化性能比单独的每个分类器都更好。例如，假设我们从10位专家那里收集了预测结果，集成方法将允许我们通过策略性地结合这些预测结果，得出比每个单独专家预测更准确和更稳健的预测结果。正如您将在本章后面看到的，有几种不同的方法可以创建一组分类器的集成。本节将介绍集成如何工作的基本解释，以及为什么它们通常被认为能够产生良好的泛化性能。'
- en: In this chapter, we will focus on the most popular ensemble methods that use
    the **majority voting** principle. Majority voting simply means that we select
    the class label that has been predicted by the majority of classifiers, that is,
    received more than 50 percent of the votes. Strictly speaking, the term “majority
    vote” refers to binary class settings only. However, it is easy to generalize
    the majority voting principle to multiclass settings, which is known as **plurality
    voting**. (In the UK, people distinguish between majority and plurality voting
    via the terms “absolute” and “relative” majority, respectively.)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于使用**多数投票**原则的最流行的集成方法。多数投票简单来说就是选择被多数分类器预测的类标签，即获得超过50%投票的类标签。严格来说，“多数投票”这个术语仅适用于二元分类设置。然而，可以轻松将多数投票原则推广到多类别设置中，这被称为**多数票投票**（在英国，人们通过“绝对多数”和“相对多数”这两个术语来区分多数和多数投票）。
- en: 'Here, we select the class label that received the most votes (the mode). *Figure
    7.1* illustrates the concept of majority and plurality voting for an ensemble
    of 10 classifiers, where each unique symbol (triangle, square, and circle) represents
    a unique class label:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择接收到最多票数的类标签（众数）。*图7.1*说明了对于包含10个分类器的集成，多数投票和多数票投票的概念，其中每个唯一的符号（三角形、正方形和圆形）代表一个唯一的类标签：
- en: '![](img/B17582_07_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_01.png)'
- en: 'Figure 7.1: The different voting concepts'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：不同的投票概念
- en: 'Using the training dataset, we start by training *m* different classifiers
    (*C*[1], ..., *C*[m]). Depending on the technique, the ensemble can be built from
    different classification algorithms, for example, decision trees, support vector
    machines, logistic regression classifiers, and so on. Alternatively, we can also
    use the same base classification algorithm, fitting different subsets of the training
    dataset. One prominent example of this approach is the random forest algorithm
    combining different decision tree classifiers, which we covered in *Chapter 3*,
    *A Tour of Machine Learning Classifiers Using Scikit-Learn*. *Figure 7.2* illustrates
    the concept of a general ensemble approach using majority voting:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据集，我们首先训练*m*个不同的分类器(*C*[1], ..., *C*[m])。根据技术的不同，集成可以由不同的分类算法构建，例如决策树、支持向量机、逻辑回归分类器等。或者，我们也可以使用相同的基础分类算法，拟合训练数据集的不同子集。这种方法的一个显著例子是随机森林算法，它结合了不同的决策树分类器，我们在*第3章*《使用Scikit-Learn的机器学习分类器导览》中有所涵盖。*图7.2*说明了使用多数投票的一般集成方法的概念：
- en: '![](img/B17582_07_02.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_02.png)'
- en: 'Figure 7.2: A general ensemble approach'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：一般集成方法
- en: 'To predict a class label via simple majority or plurality voting, we can combine
    the predicted class labels of each individual classifier, *C*[j], and select the
    class label, ![](img/B17582_04_008.png), that received the most votes:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过简单的多数投票来预测类标签，我们可以结合每个单独分类器*C*[j]的预测类标签，并选择获得最多票数的类标签，![](img/B17582_04_008.png)：
- en: '![](img/B17582_07_002.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_002.png)'
- en: (In statistics, the mode is the most frequent event or result in a set. For
    example, mode{1, 2, 1, 1, 2, 4, 5, 4} = 1.)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (在统计学中，众数是集合中最频繁出现的事件或结果。例如，mode{1, 2, 1, 1, 2, 4, 5, 4} = 1。)
- en: 'For example, in a binary classification task where class1 = –1 and class2 = +1,
    we can write the majority vote prediction as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个二分类任务中，其中 class1 = -1，class2 = +1，我们可以将多数投票预测写成如下形式：
- en: '![](img/B17582_07_003.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_003.png)'
- en: 'To illustrate why ensemble methods can work better than individual classifiers
    alone, let’s apply some concepts of combinatorics. For the following example,
    we will make the assumption that all *n*-base classifiers for a binary classification
    task have an equal error rate, ![](img/B17582_07_004.png). Furthermore, we will
    assume that the classifiers are independent and the error rates are not correlated.
    Under those assumptions, we can simply express the error probability of an ensemble
    of base classifiers as a probability mass function of a binomial distribution:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明为什么集成方法可以比单独的分类器表现更好，让我们应用一些组合数学的概念。对于以下例子，我们假设所有*n*个基分类器对于二分类任务有相等的错误率，![](img/B17582_07_004.png)。此外，我们假设这些分类器是独立的，错误率不相关。在这些假设下，我们可以简单地将基分类器集成的错误概率表示为二项分布的概率质量函数：
- en: '![](img/B17582_07_005.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_005.png)'
- en: 'Here, ![](img/B17582_07_006.png) is the binomial coefficient *n choose k*.
    In other words, we compute the probability that the prediction of the ensemble
    is wrong. Now, let’s take a look at a more concrete example of 11 base classifiers
    (*n* = 11), where each classifier has an error rate of 0.25 (![](img/B17582_07_007.png)):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_07_006.png) 是二项式系数 *n choose k*。换句话说，我们计算集成预测错误的概率。现在，让我们看一个具体的例子，有11个基分类器(*n*
    = 11)，每个分类器的错误率为0.25 (![](img/B17582_07_007.png))：
- en: '![](img/B17582_07_008.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_008.png)'
- en: '**The binomial coefficient**'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**二项式系数**'
- en: 'The binomial coefficient refers to the number of ways we can choose subsets
    of *k* unordered elements from a set of size *n*; thus, it is often called “n
    choose k.” Since the order does not matter here, the binomial coefficient is also
    sometimes referred to as *combination* or *combinatorial number*, and in its unabbreviated
    form, it is written as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 二项式系数是指我们可以从大小为*n*的集合中选择*k*个无序元素的方式数；因此，它通常被称为“n choose k”。由于顺序在这里并不重要，二项式系数有时也被称为*组合数*或*组合数*，在其完整形式中，它的写法如下：
- en: '![](img/B17582_07_009.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_009.png)'
- en: Here, the symbol (!) stands for factorial—for example, 3! = 3×2×1 = 6.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，符号(!)表示阶乘，例如，3! = 3×2×1 = 6。
- en: 'As you can see, the error rate of the ensemble (0.034) is much lower than the
    error rate of each individual classifier (0.25) if all the assumptions are met.
    Note that, in this simplified illustration, a 50-50 split by an even number of
    classifiers, *n*, is treated as an error, whereas this is only true half of the
    time. To compare such an idealistic ensemble classifier to a base classifier over
    a range of different base error rates, let’s implement the probability mass function
    in Python:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，集成的错误率（0.034）远低于每个单独分类器的错误率（0.25），如果所有假设都得到满足的话。请注意，在这个简化的示例中，一个由偶数个分类器进行的50-50分割被视为错误，然而这只有一半的时间是真实的。为了比较这样一个理想的集成分类器与一系列不同基本错误率上的基本分类器，让我们在Python中实现概率质量函数：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After we have implemented the `ensemble_error` function, we can compute the
    ensemble error rates for a range of different base errors from 0.0 to 1.0 to visualize
    the relationship between ensemble and base errors in a line graph:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现了`ensemble_error`函数之后，我们可以计算一系列不同基础错误率的集成错误率，以可视化在一条线图中集成与基础错误之间的关系：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see in the resulting plot, the error probability of an ensemble is
    always better than the error of an individual base classifier, as long as the
    base classifiers perform better than random guessing (![](img/B17582_07_010.png)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在结果图中所见，集成的错误概率总是比单个基础分类器的错误要好，只要基础分类器表现优于随机猜测（![](img/B17582_07_010.png)）。
- en: 'Note that the *y* axis depicts the base error (dotted line) as well as the
    ensemble error (continuous line):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*y* 轴显示了基本错误（虚线）以及集成错误（实线）：
- en: '![](img/B17582_07_03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_03.png)'
- en: 'Figure 7.3: A plot of the ensemble error versus the base error'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：集成错误与基本错误的绘图
- en: Combining classifiers via majority vote
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过多数投票组合分类器
- en: After the short introduction to ensemble learning in the previous section, let’s
    start with a warm-up exercise and implement a simple ensemble classifier for majority
    voting in Python.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节对集成学习的简短介绍之后，让我们开始一个热身练习，并在Python中实现一个简单的多数投票集成分类器。
- en: '**Plurality voting**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**多数投票**'
- en: Although the majority voting algorithm that we will discuss in this section
    also generalizes to multiclass settings via plurality voting, the term “majority
    voting” will be used for simplicity, as is often the case in the literature.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们将在本节讨论的多数投票算法也通过多数投票一般化到多类设置中，但为了简单起见，文献中通常使用“多数投票”这个术语。
- en: Implementing a simple majority vote classifier
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现一个简单的多数投票分类器
- en: 'The algorithm that we are going to implement in this section will allow us
    to combine different classification algorithms associated with individual weights
    for confidence. Our goal is to build a stronger meta-classifier that balances
    out the individual classifiers’ weaknesses on a particular dataset. In more precise
    mathematical terms, we can write the weighted majority vote as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节实现的算法允许我们结合具有置信度的不同分类算法。我们的目标是构建一个更强大的元分类器，以平衡特定数据集上个别分类器的弱点。更精确地说，我们可以将加权多数投票写成如下形式：
- en: '![](img/B17582_07_011.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_011.png)'
- en: 'Here, *w*[j] is a weight associated with a base classifier, *C*[j]; ![](img/B17582_04_008.png)
    is the predicted class label of the ensemble; *A* is the set of unique class labels;
    ![](img/B17582_07_013.png) (Greek chi) is the characteristic function or indicator
    function, which returns 1 if the predicted class of the *j*th classifier matches
    *i* (*C*[j](**x**) = *i*). For equal weights, we can simplify this equation and
    write it as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w*[j] 是与基分类器 *C*[j] 相关联的权重；![](img/B17582_04_008.png) 是集成预测的类标签；*A* 是唯一类标签的集合；![](img/B17582_07_013.png)（希腊字母
    chi）是特征函数或指示函数，如果第 *j* 个分类器的预测类与 *i* 匹配，则返回 1（*C*[j](**x**) = *i*）。对于相等的权重，我们可以简化这个方程并写成如下形式：
- en: '![](img/B17582_07_014.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_014.png)'
- en: 'To better understand the concept of *weighting*, we will now take a look at
    a more concrete example. Let’s assume that we have an ensemble of three base classifiers,
    ![](img/B17582_07_015.png), and we want to predict the class label, ![](img/B17582_07_016.png),
    of a given example, **x**. Two out of three base classifiers predict the class
    label 0, and one, *C*[3], predicts that the example belongs to class 1\. If we
    weight the predictions of each base classifier equally, the majority vote predicts
    that the example belongs to class 0:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解*加权*的概念，我们现在来看一个更具体的例子。假设我们有三个基分类器的集成，![](img/B17582_07_015.png)，我们想要预测给定示例**x**的类标签，![](img/B17582_07_016.png)。其中三个基分类器中的两个预测类标签为0，而一个*C*[3]预测示例属于类1。如果我们对每个基分类器的预测进行平等加权，多数投票预测该示例属于类0：
- en: '![](img/B17582_07_017.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_017.png)'
- en: 'Now, let’s assign a weight of 0.6 to *C*[3], and let’s weight *C*[1] and *C*[2]
    by a coefficient of 0.2:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们给*C*[3]分配权重为0.6，并且让*C*[1]和*C*[2]的权重系数为0.2：
- en: '![](img/B17582_07_018.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_018.png)'
- en: 'More simply, since 3×0.2 = 0.6, we can say that the prediction made by *C*[3]
    has three times more weight than the predictions by *C*[1] or *C*[2], which we
    can write as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，因为3×0.2 = 0.6，我们可以说*C*[3]的预测比*C*[1]或*C*[2]的预测重三倍，可以写成如下形式：
- en: '![](img/B17582_07_019.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_019.png)'
- en: 'To translate the concept of the weighted majority vote into Python code, we
    can use NumPy’s convenient `argmax` and `bincount` functions, where `bincount`
    counts the number of occurrences of each class label. The `argmax` function then
    returns the index position of the highest count, corresponding to the majority
    class label (this assumes that class labels start at 0):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要将加权多数投票的概念翻译成Python代码，我们可以使用NumPy方便的`argmax`和`bincount`函数，其中`bincount`函数统计每个类标签的出现次数。然后`argmax`函数返回最高计数的索引位置，对应于多数类标签（假设类标签从0开始）：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you will remember from the discussion on logistic regression in *Chapter
    3*, certain classifiers in scikit-learn can also return the probability of a predicted
    class label via the `predict_proba` method. Using the predicted class probabilities
    instead of the class labels for majority voting can be useful if the classifiers
    in our ensemble are well calibrated. The modified version of the majority vote
    for predicting class labels from probabilities can be written as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在第3章关于逻辑回归的讨论中所记得的那样，scikit-learn中的某些分类器也可以通过`predict_proba`方法返回预测类标签的概率。如果我们集成中的分类器被很好地校准，使用预测的类概率而不是类标签进行多数投票可能会很有用。从概率中预测类标签的修改版本可以写成如下形式：
- en: '![](img/B17582_07_020.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_020.png)'
- en: Here, *p*[ij] is the predicted probability of the *j*th classifier for class
    label *i*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*p*[ij]是第*j*个分类器对类标签*i*的预测概率。
- en: 'To continue with our previous example, let’s assume that we have a binary classification
    problem with class labels ![](img/B17582_07_021.png) and an ensemble of three
    classifiers, ![](img/B17582_07_015.png). Let’s assume that the classifiers *C*[j]
    return the following class membership probabilities for a particular example,
    **x**:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们之前的例子，假设我们有一个二元分类问题，类标签为![](img/B17582_07_021.png)，并且有三个分类器的集成，![](img/B17582_07_015.png)。假设分类器*C*[j]为特定示例**x**返回以下类成员概率：
- en: '![](img/B17582_07_023.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_023.png)'
- en: 'Using the same weights as previously (0.2, 0.2, and 0.6), we can then calculate
    the individual class probabilities as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与之前相同的权重（0.2、0.2和0.6），我们可以计算各个类别概率如下：
- en: '![](img/B17582_07_024.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_024.png)'
- en: 'To implement the weighted majority vote based on class probabilities, we can
    again make use of NumPy, using `np.average` and `np.argmax`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要基于类别概率实现加权多数投票，我们可以再次利用NumPy，使用`np.average`和`np.argmax`：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Putting everything together, let’s now implement `MajorityVoteClassifier` in
    Python:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 把所有东西放在一起，现在让我们用Python实现`MajorityVoteClassifier`：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’ve added a lot of comments to the code to explain the individual parts. However,
    before we implement the remaining methods, let’s take a quick break and discuss
    some of the code that may look confusing at first. We used the `BaseEstimator`
    and `ClassifierMixin` parent classes to get some base functionality *for free*,
    including the `get_params` and `set_params` methods to set and return the classifier’s
    parameters, as well as the `score` method to calculate the prediction accuracy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在代码中添加了很多注释来解释各个部分。然而，在我们实现剩余的方法之前，让我们先快速休息一下，并讨论一些乍看起来可能令人困惑的代码。我们使用了`BaseEstimator`和`ClassifierMixin`父类来免费获取一些基本功能，包括`get_params`和`set_params`方法用于设置和返回分类器的参数，以及`score`方法用于计算预测准确性。
- en: 'Next, we will add the `predict` method to predict the class label via a majority
    vote based on the class labels if we initialize a new `MajorityVoteClassifier`
    object with `vote=''classlabel''`. Alternatively, we will be able to initialize
    the ensemble classifier with `vote=''probability''` to predict the class label
    based on the class membership probabilities. Furthermore, we will also add a `predict_proba`
    method to return the averaged probabilities, which is useful when computing the
    **receiver operating characteristic area under the curve** (**ROC AUC**):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将添加`predict`方法，根据类标签的多数投票预测类别标签，如果我们使用`vote='classlabel'`初始化一个新的`MajorityVoteClassifier`对象。或者，我们可以初始化集成分类器为`vote='probability'`，以基于类成员概率预测类标签。此外，我们还将添加`predict_proba`方法返回平均概率，这在计算**接收器操作特征曲线下面积**（**ROC
    AUC**）时非常有用：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Also, note that we defined our own modified version of the `get_params` method
    to use the `_name_estimators` function to access the parameters of individual
    classifiers in the ensemble; this may look a little bit complicated at first,
    but it will make perfect sense when we use grid search for hyperparameter tuning
    in later sections.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，我们定义了自己修改版的`get_params`方法来使用`_name_estimators`函数访问集成中各个分类器的参数；这一开始可能看起来有点复杂，但当我们在后续章节中使用网格搜索进行超参数调整时，这将变得非常合理。
- en: '**VotingClassifier in scikit-learn**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**scikit-learn中的VotingClassifier**'
- en: Although the `MajorityVoteClassifier` implementation is very useful for demonstration
    purposes, we implemented a more sophisticated version of this majority vote classifier
    in scikit-learn based on the implementation in the first edition of this book.
    The ensemble classifier is available as `sklearn.ensemble.VotingClassifier` in
    scikit-learn version 0.17 and newer. You can find out more about `VotingClassifier`
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`MajorityVoteClassifier`的实现非常适合演示目的，但我们基于本书第一版的实现在scikit-learn中实现了一个更复杂的多数投票分类器版本。这个集成分类器在scikit-learn版本0.17及更高版本中可用作`sklearn.ensemble.VotingClassifier`。您可以在[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)找到更多关于`VotingClassifier`的信息。
- en: Using the majority voting principle to make predictions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多数投票原则进行预测
- en: 'Now it is time to put the `MajorityVoteClassifier` that we implemented in the
    previous section into action. But first, let’s prepare a dataset that we can test
    it on. Since we are already familiar with techniques to load datasets from CSV
    files, we will take a shortcut and load the Iris dataset from scikit-learn’s `datasets`
    module. Furthermore, we will only select two features, *sepal width* and *petal
    length*, to make the classification task more challenging for illustration purposes.
    Although our `MajorityVoteClassifier` generalizes to multiclass problems, we will
    only classify flower examples from the `Iris-versicolor` and `Iris-virginica`
    classes, with which we will compute the ROC AUC later. The code is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将我们在前一节中实现的`MajorityVoteClassifier`投入使用了。但首先，让我们准备一个可以测试它的数据集。由于我们已经熟悉如何从CSV文件加载数据集的技巧，我们将采取捷径，并从scikit-learn的`datasets`模块加载鸢尾花数据集。此外，我们将仅选择两个特征，即*萼片宽度*和*花瓣长度*，以便更具挑战性地进行分类任务进行说明。尽管我们的`MajorityVoteClassifier`适用于多类问题，但我们只会对来自`Iris-versicolor`和`Iris-virginica`类别的鸢尾花示例进行分类，之后我们将计算ROC
    AUC。代码如下：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Class membership probabilities from decision trees**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**来自决策树的类成员概率**'
- en: Note that scikit-learn uses the `predict_proba` method (if applicable) to compute
    the ROC AUC score. In *Chapter 3*, we saw how the class probabilities are computed
    in logistic regression models. In decision trees, the probabilities are calculated
    from a frequency vector that is created for each node at training time. The vector
    collects the frequency values of each class label computed from the class label
    distribution at that node. Then, the frequencies are normalized so that they sum
    up to 1\. Similarly, the class labels of the k-nearest neighbors are aggregated
    to return the normalized class label frequencies in the k-nearest neighbors algorithm.
    Although the normalized probabilities returned by both the decision tree and k-nearest
    neighbors classifier may look similar to the probabilities obtained from a logistic
    regression model, we have to be aware that they are actually not derived from
    probability mass functions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，scikit-learn使用`predict_proba`方法（如果适用）来计算ROC AUC分数。在*第三章*中，我们看到了逻辑回归模型中如何计算类概率。在决策树中，概率是从训练时为每个节点创建的频率向量中计算出来的。该向量收集每个类标签在该节点处从类标签分布计算出的频率值。然后，对频率进行归一化，使它们总和为1。同样，k最近邻居算法中的k个最近邻居的类标签被聚合以返回k最近邻居算法中的归一化类标签频率。虽然决策树和k最近邻分类器返回的归一化概率看似与逻辑回归模型中获得的概率相似，但我们必须意识到它们实际上并不是从概率质量函数中导出的。
- en: 'Next, we will split the Iris examples into 50 percent training and 50 percent
    test data:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把鸢尾花示例分为50%的训练数据和50%的测试数据：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Using the training dataset, we now will train three different classifiers:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据集，我们现在将训练三种不同的分类器：
- en: Logistic regression classifier
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归分类器
- en: Decision tree classifier
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: k-nearest neighbors classifier
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k最近邻分类器
- en: 'We will then evaluate the model performance of each classifier via 10-fold
    cross-validation on the training dataset before we combine them into an ensemble
    classifier:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在将它们组合成集成分类器之前，我们将在训练数据集上通过10折交叉验证评估每个分类器的模型性能：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output that we receive, as shown in the following snippet, shows that the
    predictive performances of the individual classifiers are almost equal:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收到的输出如下片段所示，显示了各个分类器的预测性能几乎相等：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You may be wondering why we trained the logistic regression and k-nearest neighbors
    classifier as part of a pipeline. The reason behind it is that, as discussed in
    *Chapter 3*, both the logistic regression and k-nearest neighbors algorithms (using
    the Euclidean distance metric) are not scale-invariant, in contrast to decision
    trees. Although the Iris features are all measured on the same scale (cm), it
    is a good habit to work with standardized features.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道为什么我们要将逻辑回归和k最近邻分类器作为管道的一部分进行训练。其背后的原因是，正如在*第三章*中讨论的那样，与决策树不同，逻辑回归和k最近邻算法（使用欧几里得距离度量）都不是尺度不变的。虽然鸢尾花的特征都是在相同的尺度（厘米）上测量的，但习惯上使用标准化特征是一个好习惯。
- en: 'Now, let’s move on to the more exciting part and combine the individual classifiers
    for majority rule voting in our `MajorityVoteClassifier`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行更激动人心的部分，并将单独的分类器组合起来，形成我们的`MajorityVoteClassifier`：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, the performance of `MajorityVotingClassifier` has improved over
    the individual classifiers in the 10-fold cross-validation evaluation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`MajorityVotingClassifier`在10折交叉验证评估中的表现优于单独的分类器。
- en: Evaluating and tuning the ensemble classifier
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估和调优集成分类器
- en: 'In this section, we are going to compute the ROC curves from the test dataset
    to check that `MajorityVoteClassifier` generalizes well with unseen data. We must
    remember that the test dataset is not to be used for model selection; its purpose
    is merely to report an unbiased estimate of the generalization performance of
    a classifier system:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从测试数据集计算ROC曲线，以检查`MajorityVoteClassifier`在未见数据上的泛化能力。我们必须记住，测试数据集不能用于模型选择；它的目的仅仅是报告分类器系统泛化性能的无偏估计：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can see in the resulting ROC, the ensemble classifier also performs
    well on the test dataset (ROC AUC = 0.95). However, you can see that the logistic
    regression classifier performs similarly well on the same dataset, which is probably
    due to the high variance (in this case, the sensitivity of how we split the dataset)
    given the small size of the dataset:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在结果 ROC 图中所见，集成分类器在测试数据集上的表现也很好（ROC AUC = 0.95）。然而，您可以看到逻辑回归分类器在相同数据集上表现同样良好，这可能是由于高方差（在这种情况下，我们拆分数据集的敏感性）造成的，因为数据集的大小较小：
- en: '![](img/B17582_07_04.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_04.png)'
- en: 'Figure 7.4: The ROC curve for the different classifiers'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：不同分类器的 ROC 曲线
- en: Since we only selected two features for the classification examples, it would
    be interesting to see what the decision region of the ensemble classifier actually
    looks like.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只选择了两个特征作为分类示例，看看集成分类器的决策区域实际上是什么样子会很有趣。
- en: 'Although it is not necessary to standardize the training features prior to
    model fitting, because our logistic regression and k-nearest neighbors pipelines
    will automatically take care of it, we will standardize the training dataset so
    that the decision regions of the decision tree will be on the same scale for visual
    purposes. The code is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在模型拟合前无需对训练特征进行标准化，因为我们的逻辑回归和 k 近邻流水线会自动处理，但出于可视化目的，我们将标准化训练数据集，以便决策树的决策区域在相同尺度上。代码如下：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Interestingly, but also as expected, the decision regions of the ensemble classifier
    seem to be a hybrid of the decision regions from the individual classifiers. At
    first glance, the majority vote decision boundary looks a lot like the decision
    of the decision tree stump, which is orthogonal to the *y* axis for *sepal width* ≥ 1\.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，但也是预期的，集成分类器的决策区域似乎是从各个单个分类器的决策区域混合而来的。乍一看，多数投票的决策边界看起来很像决策树桩的决策边界，后者在*sepal
    width* ≥ 1时与 *y* 轴正交。
- en: 'However, you can also notice the nonlinearity from the k-nearest neighbor classifier
    mixed in:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您还可以注意到混入了 k 近邻分类器的非线性：
- en: '![](img/B17582_07_05.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_05.png)'
- en: 'Figure 7.5: The decision boundaries for the different classifiers'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：不同分类器的决策边界
- en: 'Before we tune the individual classifier’s parameters for ensemble classification,
    let’s call the `get_params` method to get a basic idea of how we can access the
    individual parameters inside a `GridSearchCV` object:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在为集成分类调整单个分类器的参数之前，让我们调用`get_params`方法，以基本了解如何访问`GridSearchCV`对象中的单个参数：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Based on the values returned by the `get_params` method, we now know how to
    access the individual classifier’s attributes. Let’s now tune the inverse regularization
    parameter, `C`, of the logistic regression classifier and the decision tree depth
    via a grid search for demonstration purposes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基于`get_params`方法返回的值，我们现在知道如何访问单个分类器的属性。现在让我们通过网格搜索来调整逻辑回归分类器的逆正则化参数`C`和决策树的深度，以进行演示：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the grid search has completed, we can print the different hyperparameter
    value combinations and the average ROC AUC scores computed via 10-fold cross-validation
    as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索完成后，我们可以打印不同的超参数值组合和通过 10 折交叉验证计算的平均 ROC AUC 分数如下：
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, we get the best cross-validation results when we choose a lower
    regularization strength (`C=0.001`), whereas the tree depth does not seem to affect
    the performance at all, suggesting that a decision stump is sufficient to separate
    the data. To remind ourselves that it is a bad practice to use the test dataset
    more than once for model evaluation, we are not going to estimate the generalization
    performance of the tuned hyperparameters in this section. We will move on swiftly
    to an alternative approach for ensemble learning: **bagging**.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当我们选择较低的正则化强度（`C=0.001`）时，我们获得了最佳的交叉验证结果，而树的深度似乎对性能没有影响，这表明决策树桩足以分离数据。为了提醒自己不要在模型评估中多次使用测试数据集是一种不良实践，在本节中我们不会估计调整后的超参数的泛化性能。我们将迅速转向集成学习的另一种方法：**装袋法**。
- en: '**Building ensembles using stacking**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用堆叠构建集成**'
- en: 'The majority vote approach we implemented in this section is not to be confused
    with stacking. The stacking algorithm can be understood as a two-level ensemble,
    where the first level consists of individual classifiers that feed their predictions
    to the second level, where another classifier (typically logistic regression)
    is fit to the level-one classifier predictions to make the final predictions.
    For more information on stacking, see the following resources:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中实施的多数投票方法不应与堆叠混淆。堆叠算法可以理解为一个两级集成，第一级包括个别分类器，它们将其预测传递给第二级，在第二级中，另一个分类器（通常是逻辑回归）适合于一级分类器的预测以进行最终预测。有关堆叠的更多信息，请参阅以下资源：
- en: The stacking algorithm has been described in more detail by David H. Wolpert
    in *Stacked generalization*, *Neural Networks*, 5(2):241–259, 1992 ([https://www.sciencedirect.com/science/article/pii/S0893608005800231](https://www.sciencedirect.com/science/article/pii/S0893608005800231)).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠算法更详细地由David H. Wolpert在《Stacked generalization》中描述，*Neural Networks*，5(2)：241–259，1992年
    ([https://www.sciencedirect.com/science/article/pii/S0893608005800231](https://www.sciencedirect.com/science/article/pii/S0893608005800231))。
- en: Interested readers can find our video tutorial about stacking on YouTube at
    [https://www.youtube.com/watch?v=8T2emza6g80](https://www.youtube.com/watch?v=8T2emza6g80).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于感兴趣的读者，可以在YouTube上找到我们关于堆叠的视频教程，网址为[https://www.youtube.com/watch?v=8T2emza6g80](https://www.youtube.com/watch?v=8T2emza6g80)。
- en: 'A scikit-learn compatible version of a stacking classifier is available from
    mlxtend: [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mlxtend提供了一个与scikit-learn兼容的堆叠分类器版本：[http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/)
- en: Also, a `StackingClassifier` has recently been added to scikit-learn (available
    in version 0.22 and newer); for more information, please see the documentation
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，scikit-learn最近添加了一个`StackingClassifier`（从版本0.22开始提供），有关更多信息，请参阅文档[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html)。
- en: Bagging – building an ensemble of classifiers from bootstrap samples
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging – 从自举样本中构建分类器的集成
- en: Bagging is an ensemble learning technique that is closely related to the `MajorityVoteClassifier`
    that we implemented in the previous section. However, instead of using the same
    training dataset to fit the individual classifiers in the ensemble, we draw bootstrap
    samples (random samples with replacement) from the initial training dataset, which
    is why bagging is also known as *bootstrap aggregating*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging是一种与我们在前一节中实现的`MajorityVoteClassifier`密切相关的集成学习技术。但是，与在集成中使用相同的训练数据集来拟合个别分类器不同，我们从初始训练数据集中抽取自举样本（有替换地随机抽样），这就是为什么bagging也被称为*bootstrap
    aggregating*的原因。
- en: 'The concept of bagging is summarized in *Figure 7.6*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging的概念总结在*Figure 7.6*中：
- en: '![Diagram  Description automatically generated](img/B17582_07_06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表说明](img/B17582_07_06.png)'
- en: 'Figure 7.6: The concept of bagging'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 7.6: Bagging的概念'
- en: In the following subsections, we will work through a simple example of bagging
    by hand and use scikit-learn for classifying wine examples.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将手动进行一个简单的bagging示例，并使用scikit-learn来分类葡萄酒示例。
- en: Bagging in a nutshell
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging简介
- en: 'To provide a more concrete example of how the bootstrap aggregating of a bagging
    classifier works, let’s consider the example shown in *Figure 7.7*. Here, we have
    seven different training instances (denoted as indices 1-7) that are sampled randomly
    with replacement in each round of bagging. Each bootstrap sample is then used
    to fit a classifier, *C*[j], which is most typically an unpruned decision tree:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个更具体的例子来说明一个bagging分类器的自举聚合是如何工作的，让我们考虑*Figure 7.7*中显示的例子。在这里，我们有七个不同的训练实例（标记为索引1-7），在每一轮bagging中随机且有替换地抽样。然后每个自举样本用于拟合分类器*C*[j]，这通常是一个未修剪的决策树：
- en: '![Diagram  Description automatically generated](img/B17582_07_07.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表说明](img/B17582_07_07.png)'
- en: 'Figure 7.7: An example of bagging'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 7.7: Bagging的一个例子'
- en: As you can see from *Figure 7.7*, each classifier receives a random subset of
    examples from the training dataset. We denote these random samples obtained via
    bagging as *Bagging round 1*, *Bagging round 2*, and so on. Each subset contains
    a certain portion of duplicates and some of the original examples don’t appear
    in a resampled dataset at all due to sampling with replacement. Once the individual
    classifiers are fit to the bootstrap samples, the predictions are combined using
    majority voting.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从 *图 7.7* 中所看到的，每个分类器从训练数据集中接收一个随机子集。我们将这些通过 bagging 获得的随机样本标记为 *Bagging
    round 1*、*Bagging round 2* 等等。每个子集包含一定比例的重复项，一些原始样本由于使用替换采样，可能根本不会出现在重新采样的数据集中。一旦个别分类器适合于自举样本，预测结果就会使用多数投票结合起来。
- en: Note that bagging is also related to the random forest classifier that we introduced
    in *Chapter 3*. In fact, random forests are a special case of bagging where we
    also use random feature subsets when fitting the individual decision trees.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，bagging 还与我们在 *第三章* 中介绍的随机森林分类器相关。事实上，随机森林是 bagging 的一种特殊情况，我们在适合个别决策树时也使用随机特征子集。
- en: '**Model ensembles using bagging**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用 bagging 的模型集成**'
- en: Bagging was first proposed by Leo Breiman in a technical report in 1994; he
    also showed that bagging can improve the accuracy of unstable models and decrease
    the degree of overfitting. We highly recommend that you read about his research
    in *Bagging predictors* by *L. Breiman*, *Machine Learning*, 24(2):123–140, 1996,
    which is freely available online, to learn more details about bagging.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 最初由 Leo Breiman 在 1994 年的一份技术报告中提出；他还表明，bagging 可以提高不稳定模型的准确性并降低过拟合的程度。我们强烈建议您阅读
    *L. Breiman* 的 *Bagging predictors* 中的研究，发表于 *Machine Learning*，24(2):123–140,
    1996 年，这篇文章可以在网上免费获取，以了解有关 bagging 的更多细节。
- en: Applying bagging to classify examples in the Wine dataset
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用 bagging 对 Wine 数据集中的示例进行分类
- en: 'To see bagging in action, let’s create a more complex classification problem
    using the Wine dataset that was introduced in *Chapter 4*, *Building Good Training
    Datasets – Data Preprocessing*. Here, we will only consider the Wine classes 2
    and 3, and we will select two features – `Alcohol` and `OD280/OD315 of diluted
    wines`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到 bagging 的实际效果，让我们使用在 *第四章* *构建良好的训练数据集 – 数据预处理* 中介绍的 Wine 数据集创建一个更复杂的分类问题。在这里，我们只考虑
    Wine 类别 2 和 3，并选择两个特征 – `Alcohol` 和 `OD280/OD315 of diluted wines`：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will encode the class labels into binary format and split the dataset
    into 80 percent training and 20 percent test datasets:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把类标签编码成二进制格式，并将数据集拆分为 80% 的训练集和 20% 的测试集：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Obtaining the Wine dataset**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取 Wine 数据集**'
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable. For instance, to load the Wine dataset from a local
    directory, take the following lines:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本书的代码捆绑包中找到 Wine 数据集（以及本书中使用的所有其他数据集），如果您离线工作或 UCI 服务器在 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    暂时不可用时可以使用它。例如，要从本地目录加载 Wine 数据集，请使用以下代码：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'and replace them with these:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 并用以下内容替换它们：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A `BaggingClassifier` algorithm is already implemented in scikit-learn, which
    we can import from the `ensemble` submodule. Here, we will use an unpruned decision
    tree as the base classifier and create an ensemble of 500 decision trees fit on
    different bootstrap samples of the training dataset:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中已经实现了 `BaggingClassifier` 算法，我们可以从 `ensemble` 子模块导入它。在这里，我们将使用一个未修剪的决策树作为基分类器，并创建一个由训练数据集的不同自举样本拟合的
    500 个决策树的集成：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will calculate the accuracy score of the prediction on the training
    and test datasets to compare the performance of the bagging classifier to the
    performance of a single unpruned decision tree:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算训练集和测试集上预测的准确率分数，以比较 bagging 分类器的性能与单个未修剪决策树的性能：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Based on the accuracy values that we printed here, the unpruned decision tree
    predicts all the class labels of the training examples correctly; however, the
    substantially lower test accuracy indicates high variance (overfitting) of the
    model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们在此处打印的准确度值，未修剪的决策树正确预测了所有训练样本的类标签；然而，显著较低的测试准确度表明模型具有高方差（过拟合）：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Although the training accuracies of the decision tree and bagging classifier
    are similar on the training dataset (both 100 percent), we can see that the bagging
    classifier has a slightly better generalization performance, as estimated on the
    test dataset. Next, let’s compare the decision regions between the decision tree
    and the bagging classifier:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管决策树和装袋分类器在训练数据集上的训练准确率相似（均为 100%），我们可以看到装袋分类器在测试数据集上有稍微更好的泛化性能。接下来，让我们比较决策树和装袋分类器之间的决策区域：
- en: '[PRE23]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As we can see in the resulting plot, the piece-wise linear decision boundary
    of the three-node deep decision tree looks smoother in the bagging ensemble:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在得出的绘图中所见，三节点深度决策树的分段线性决策边界在装袋法集成中看起来更加平滑：
- en: '![](img/B17582_07_08.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_08.png)'
- en: 'Figure 7.8: The piece-wise linear decision boundary of a decision tree versus
    bagging'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：决策树的分段线性决策边界与装袋法的对比
- en: We only looked at a very simple bagging example in this section. In practice,
    more complex classification tasks and a dataset’s high dimensionality can easily
    lead to overfitting in single decision trees, and this is where the bagging algorithm
    can really play to its strengths. Finally, we must note that the bagging algorithm
    can be an effective approach to reducing the variance of a model. However, bagging
    is ineffective in reducing model bias, that is, models that are too simple to
    capture the trends in the data well. This is why we want to perform bagging on
    an ensemble of classifiers with low bias, for example, unpruned decision trees.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们只讨论了一个非常简单的装袋示例。在实践中，更复杂的分类任务和数据集的高维度很容易导致单个决策树过拟合，这正是装袋算法可以发挥其优势的地方。最后，我们必须注意，装袋算法可以是降低模型方差的有效方法。然而，装袋在减少模型偏差方面效果不佳，也就是说，模型过于简单无法很好地捕捉数据中的趋势。这就是为什么我们希望在偏差较低的分类器集成上执行装袋，例如未修剪的决策树。
- en: Leveraging weak learners via adaptive boosting
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自适应提升利用弱学习者
- en: 'In this last section about ensemble methods, we will discuss **boosting**,
    with a special focus on its most common implementation: **Adaptive Boosting**
    (**AdaBoost**).'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于集成方法的最后一节中，我们将讨论**提升**，特别关注其最常见的实现方式：**自适应提升**（**AdaBoost**）。
- en: '**AdaBoost recognition**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**AdaBoost 的认知**'
- en: 'The original idea behind AdaBoost was formulated by Robert E. Schapire in 1990
    in *The Strength of Weak Learnability*, *Machine Learning*, 5(2): 197-227, 1990,URL:
    [http://rob.schapire.net/papers/strengthofweak.pdf](http://rob.schapire.net/papers/strengthofweak.pdf).
    After Robert Schapire and Yoav Freund presented the AdaBoost algorithm in the
    *Proceedings of the Thirteenth International Conference* (ICML 1996), AdaBoost
    became one of the most widely used ensemble methods in the years that followed
    (*Experiments with a New Boosting Algorithm* by *Y. Freund*, *R. E. Schapire*,
    and others, *ICML*, volume 96, 148-156, 1996). In 2003, Freund and Schapire received
    the Gödel Prize for their groundbreaking work, which is a prestigious prize for
    the most outstanding publications in the field of computer science.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 的原始理念由 Robert E. Schapire 在 1990 年的《*弱可学习性的力量*》，*机器学习*，5(2)：197-227
    中提出，详细可见于 [http://rob.schapire.net/papers/strengthofweak.pdf](http://rob.schapire.net/papers/strengthofweak.pdf)。在
    Robert Schapire 和 Yoav Freund 于 *第十三届国际会议论文集*（ICML 1996）中展示 AdaBoost 算法后，AdaBoost
    成为随后几年中最广泛使用的集成方法之一（*Experiments with a New Boosting Algorithm* 由 *Y. Freund*、*R.
    E. Schapire* 等人，*ICML*，96 年卷，148-156，1996）。2003 年，Freund 和 Schapire 因其开创性工作获得
    Gödel 奖，这是计算机科学领域中最杰出出版物的一项重要奖项。
- en: In boosting, the ensemble consists of very simple base classifiers, also often
    referred to as **weak learners**, which often only have a slight performance advantage
    over random guessing—a typical example of a weak learner is a decision tree stump.
    The key concept behind boosting is to focus on training examples that are hard
    to classify, that is, to let the weak learners subsequently learn from misclassified
    training examples to improve the performance of the ensemble.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升中，集成由非常简单的基本分类器组成，通常称为**弱学习者**，它们通常仅比随机猜测略有优势—一个典型的弱学习者示例是决策树桩。提升背后的关键概念是专注于难以分类的训练样本，也就是让弱学习者连续从误分类的训练样本中学习，以提高集成的性能。
- en: The following subsections will introduce the algorithmic procedure behind the
    general concept of boosting and AdaBoost. Lastly, we will use scikit-learn for
    a practical classification example.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各小节将介绍增强学习和 AdaBoost 概念背后的算法过程。最后，我们将使用 scikit-learn 进行一个实际的分类示例。
- en: How adaptive boosting works
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应Boosting的工作原理
- en: 'In contrast to bagging, the initial formulation of the boosting algorithm uses
    random subsets of training examples drawn from the training dataset without replacement;
    the original boosting procedure can be summarized in the following four key steps:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与Bagging相反，Boosting算法的初始形式使用从训练数据集中无放回抽取的随机训练例子子集；原始Boosting过程可以总结为以下四个关键步骤：
- en: Draw a random subset (sample) of training examples, *d*[1], without replacement
    from the training dataset, *D*, to train a weak learner, *C*[1].
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集*D*中无放回地抽取一个随机子集（样本），*d*[1]，以训练一个弱学习器，*C*[1]。
- en: Draw a second random training subset, *d*[2], without replacement from the training
    dataset and add 50 percent of the examples that were previously misclassified
    to train a weak learner, *C*[2].
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练数据集中无放回地抽取第二个随机训练子集，*d*[2]，并添加之前错误分类的例子的50%来训练一个弱学习器，*C*[2]。
- en: Find the training examples, *d*[3], in the training dataset, *D*, which *C*[1]
    and *C*[2] disagree upon, to train a third weak learner, *C*[3].
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到训练数据集*D*中*C*[1]和*C*[2]不同意的训练例子*d*[3]，以训练第三个弱学习器*C*[3]。
- en: Combine the weak learners *C*[1], *C*[2], and *C*[3] via majority voting.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过多数投票组合弱学习器*C*[1]、*C*[2]和*C*[3]。
- en: As discussed by Leo Breiman (*Bias, variance, and arcing classifiers*, 1996),
    boosting can lead to a decrease in bias as well as variance compared to bagging
    models. In practice, however, boosting algorithms such as AdaBoost are also known
    for their high variance, that is, the tendency to overfit the training data (*An
    improvement of AdaBoost to avoid overfitting* by *G. Raetsch*, *T. Onoda*, and
    *K. R. Mueller*. *Proceedings of the International Conference on Neural Information
    Processing*, *CiteSeer*, 1998).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如Leo Breiman在1996年所讨论的（*偏差、方差和arcing分类器*），Boosting相对于Bagging模型可以导致偏差和方差的减少。然而，在实践中，Boosting算法如AdaBoost也以其高方差而闻名，即倾向于过拟合训练数据（*通过G.
    Raetsch、T. Onoda和K. R. Mueller的*An improvement of AdaBoost to avoid overfitting*，*Proceedings
    of the International Conference on Neural Information Processing*，*CiteSeer*，1998）。
- en: In contrast to the original boosting procedure described here, AdaBoost uses
    the complete training dataset to train the weak learners, where the training examples
    are reweighted in each iteration to build a strong classifier that learns from
    the mistakes of the previous weak learners in the ensemble.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 与此处描述的原始Boosting过程相反，AdaBoost使用完整的训练数据集来训练弱学习器，在每次迭代中重新加权训练例子，以构建一个从前一组合中的弱学习器错误中学习的强分类器。
- en: 'Before we dive deeper into the specific details of the AdaBoost algorithm,
    let’s take a look at *Figure 7.9* to get a better grasp of the basic concept behind
    AdaBoost:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论AdaBoost算法的具体细节之前，让我们看一下*Figure 7.9*，以更好地理解AdaBoost背后的基本概念：
- en: '![](img/B17582_07_09.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_09.png)'
- en: 'Figure 7.9: The concept of AdaBoost to improve weak learners'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：AdaBoost概念改进弱学习器
- en: To walk through the AdaBoost illustration step by step, we will start with subfigure
    **1**, which represents a training dataset for binary classification where all
    training examples are assigned equal weights. Based on this training dataset,
    we train a decision stump (shown as a dashed line) that tries to classify the
    examples of the two classes (triangles and circles), as well as possibly minimizing
    the loss function (or the impurity score in the special case of decision tree
    ensembles).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了逐步浏览AdaBoost插图，我们将从子图 **1** 开始，该子图代表了一个用于二元分类的训练数据集，所有训练例子被分配相同的权重。基于这个训练数据集，我们训练一个决策树桩（显示为虚线），试图分类两类（三角形和圆圈）的例子，同时可能最小化损失函数（或者在决策树集成的特殊情况下是杂质评分）。
- en: For the next round (subfigure **2**), we assign a larger weight to the two previously
    misclassified examples (circles). Furthermore, we lower the weight of the correctly
    classified examples. The next decision stump will now be more focused on the training
    examples that have the largest weights—the training examples that are supposedly
    hard to classify.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一轮（子图 **2**），我们给之前错误分类的两个例子（圆圈）分配更大的权重。此外，我们降低正确分类例子的权重。接下来的决策树桩现在将更加关注那些具有最大权重的训练例子，即那些据称难以分类的训练例子。
- en: The weak learner shown in subfigure **2** misclassifies three different examples
    from the circle class, which are then assigned a larger weight, as shown in subfigure
    **3**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在子图 **2** 中显示的弱学习器错误分类了三个来自圆圈类的不同例子，然后在子图 **3** 中分配了更大的权重。
- en: Assuming that our AdaBoost ensemble only consists of three rounds of boosting,
    we then combine the three weak learners trained on different reweighted training
    subsets by a weighted majority vote, as shown in subfigure **4**.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的AdaBoost集成仅包括三轮增强，然后通过加权多数投票结合在不同的重新加权训练子集上训练的三个弱学习器，如子图 **4** 所示。
- en: 'Now that we have a better understanding of the basic concept of AdaBoost, let’s
    take a more detailed look at the algorithm using pseudo code. For clarity, we
    will denote element-wise multiplication by the cross symbol (×) and the dot-product
    between two vectors by a dot symbol (⋅):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对AdaBoost的基本概念有了更好的理解，让我们使用伪代码更详细地了解算法。为了清晰起见，我们将用乘号（×）表示元素级乘法，用点号（⋅）表示两个向量的点积：
- en: Set the weight vector, **w**, to uniform weights, where ![](img/B17582_07_026.png).
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重向量 **w** 设置为均匀权重，其中 ![](img/B17582_07_026.png).
- en: 'For *j* in *m* boosting rounds, do the following:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '对于第 *j* 轮的 *m* 次增强，执行以下操作:'
- en: 'Train a weighted weak learner: *C*[j] = train(**X**, **y**, **w**).'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '训练一个加权弱学习器: *C*[j] = train(**X**, **y**, **w**).'
- en: 'Predict class labels: ![](img/B17582_07_027.png).'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '预测类别标签: ![](img/B17582_07_027.png).'
- en: 'Compute the weighted error rate: ![](img/B17582_07_028.png).'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算加权错误率: ![](img/B17582_07_028.png).'
- en: 'Compute the coefficient: ![](img/B17582_07_029.png).'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算系数: ![](img/B17582_07_029.png).'
- en: 'Update the weights: ![](img/B17582_07_030.png).'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '更新权重: ![](img/B17582_07_030.png).'
- en: 'Normalize the weights to sum to 1: ![](img/B17582_07_031.png).'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '将权重归一化为总和为1: ![](img/B17582_07_031.png).'
- en: 'Compute the final prediction: ![](img/B17582_07_032.png).'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算最终预测: ![](img/B17582_07_032.png).'
- en: Note that the expression ![](img/B17582_07_033.png) in *step 2c* refers to a
    binary vector consisting of 1s and 0s, where a 1 is assigned if the prediction
    is incorrect and 0 is assigned otherwise.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 *步骤 2c* 中，表达式 ![](img/B17582_07_033.png) 指的是一个由1和0组成的二进制向量，如果预测不正确则赋值为1，否则赋值为0。
- en: 'Although the AdaBoost algorithm seems to be pretty straightforward, let’s walk
    through a more concrete example using a training dataset consisting of 10 training
    examples, as illustrated in *Figure 7.10*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AdaBoost算法看起来非常直接，让我们通过一个具体的例子来详细介绍一下，该例子使用了包含10个训练样本的训练数据集，如 *图 7.10* 所示：
- en: '![](img/B17582_07_10.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_10.png)'
- en: 'Figure 7.10: Running 10 training examples through the AdaBoost algorithm'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.10: 通过AdaBoost算法运行10个训练样本'
- en: The first column of the table depicts the indices of training examples 1 to
    10\. In the second column, you can see the feature values of the individual samples,
    assuming this is a one-dimensional dataset. The third column shows the true class
    label, *y*[i], for each training sample, *x*[i], where ![](img/B17582_07_034.png).
    The initial weights are shown in the fourth column; we initialize the weights
    uniformly (assigning the same constant value) and normalize them to sum to 1\.
    In the case of the 10-sample training dataset, we therefore assign 0.1 to each
    weight, *w*[i], in the weight vector, **w**. The predicted class labels, ![](img/B17582_07_035.png),
    are shown in the fifth column, assuming that our splitting criterion is ![](img/B17582_07_036.png).
    The last column of the table then shows the updated weights based on the update
    rules that we defined in the pseudo code.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表的第一列显示了训练样本1到10的索引。第二列显示了个别样本的特征值，假设这是一个一维数据集。第三列显示了每个训练样本 *x*[i] 的真实类别标签 *y*[i]，其中
    ![](img/B17582_07_034.png)。第四列显示了初始权重；我们使用均匀初始化权重，并将它们归一化为总和为1。对于这个10个样本的训练数据集，我们因此将每个权重
    *w*[i] 赋值为0.1，放在权重向量 **w** 中。预测的类别标签 ![](img/B17582_07_035.png) 显示在第五列，假设我们的分割标准为
    ![](img/B17582_07_036.png)。表的最后一列根据我们在伪代码中定义的更新规则显示了更新后的权重。
- en: 'Since the computation of the weight updates may look a little bit complicated
    at first, we will now follow the calculation step by step. We will start by computing
    the weighted error rate, ![](img/B17582_07_004.png) (`epsilon`), as described
    in *step 2c*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重更新的计算起初可能看起来有些复杂，我们现在将一步步地跟随计算过程。我们将从计算加权错误率 ![](img/B17582_07_004.png)（`epsilon`）开始，如
    *步骤 2c* 中所述。
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that `correct` is a Boolean array consisting of `True` and `False` values
    where `True` indicates that a prediction is correct. Via `~correct`, we invert
    the array such that `np.mean(~correct)` computes the proportion of incorrect predictions
    (`True` counts as the value 1 and `False` as 0), that is, the classification error.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`correct` 是一个布尔数组，其中包含 `True` 和 `False` 值，其中 `True` 表示预测是正确的。通过 `~correct`，我们反转数组，使
    `np.mean(~correct)` 计算不正确预测的比例（`True` 计为值 1，`False` 计为值 0），即分类错误率。
- en: 'Next, we will compute the coefficient, ![](img/B17582_07_038.png)—shown in
    *step 2d*—which will later be used in *step 2e* to update the weights, as well
    as for the weights in the majority vote prediction (*step 3*):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将计算系数 ![](img/B17582_07_038.png) ——显示在 *步骤 2d* 中 —— 这将稍后用于 *步骤 2e* 中更新权重，以及在多数投票预测
    (*步骤 3*) 中的权重：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After we have computed the coefficient, ![](img/B17582_07_039.png) (`alpha_j`),
    we can now update the weight vector using the following equation:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 计算了系数 ![](img/B17582_07_039.png)（`alpha_j`）之后，我们现在可以使用以下方程更新权重向量：
- en: '![](img/B17582_07_040.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_040.png)'
- en: 'Here, ![](img/B17582_07_041.png) is an element-wise multiplication between
    the vectors of the predicted and true class labels, respectively. Thus, if a prediction,
    ![](img/B17582_07_042.png), is correct, ![](img/B17582_07_043.png) will have a
    positive sign so that we decrease the *i*th weight, since ![](img/B17582_07_039.png)
    is a positive number as well:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_07_041.png) 是预测和真实类标签向量的逐元素乘积。因此，如果预测 ![](img/B17582_07_042.png)
    是正确的，![](img/B17582_07_043.png) 将具有正号，以便我们减少第 *i* 个权重，因为 ![](img/B17582_07_039.png)
    也是一个正数：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Similarly, we will increase the *i*th weight if ![](img/B17582_07_042.png)
    predicted the label incorrectly, like this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果 ![](img/B17582_07_042.png) 预测的标签是错误的，我们将增加第 *i* 个权重，如下所示：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Alternatively, it’s like this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以这样：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can use these values to update the weights as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些值来更新权重，如下所示：
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The code above assigned the `update_if_correct` value to all correct predictions
    and the `update_if_wrong_1` value to all wrong predictions. We omitted using `update_if_wrong_2`
    for simplicity, since it is similar to `update_if_wrong_1` anyway.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将 `update_if_correct` 值分配给所有正确预测，并将 `update_if_wrong_1` 值分配给所有错误预测。为简单起见，我们省略了使用
    `update_if_wrong_2`，因为它与 `update_if_wrong_1` 类似。
- en: 'After we have updated each weight in the weight vector, we normalize the weights
    so that they sum up to 1 (*step 2f*):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新了权重向量中的每个权重后，我们会归一化这些权重，使它们总和为 1 (*步骤 2f*)：
- en: '![](img/B17582_07_046.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_046.png)'
- en: 'In code, we can accomplish that as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以这样实现：
- en: '[PRE30]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Thus, each weight that corresponds to a correctly classified example will be
    reduced from the initial value of 0.1 to 0.0714 for the next round of boosting.
    Similarly, the weights of the incorrectly classified examples will increase from
    0.1 to 0.1667.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个正确分类的示例，其对应的权重将从初始值 0.1 减少到下一轮增强的 0.0714。同样地，错误分类示例的权重将从 0.1 增加到 0.1667。
- en: Applying AdaBoost using scikit-learn
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 应用 AdaBoost
- en: The previous subsection introduced AdaBoost in a nutshell. Skipping to the more
    practical part, let’s now train an AdaBoost ensemble classifier via scikit-learn.
    We will use the same Wine subset that we used in the previous section to train
    the bagging meta-classifier.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节简要介绍了 AdaBoost。跳过更实际的部分，现在让我们通过 scikit-learn 训练 AdaBoost 集成分类器。我们将使用与前一节中训练装袋元分类器相同的
    Wine 子集。
- en: 'Via the `base_estimator` attribute, we will train the `AdaBoostClassifier`
    on 500 decision tree stumps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `base_estimator` 属性，我们将在 500 个决策树桩上训练 `AdaBoostClassifier`：
- en: '[PRE31]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As you can see, the decision tree stump seems to underfit the training data
    in contrast to the unpruned decision tree that we saw in the previous section:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与前一节中看到的未修剪决策树相比，决策树桩似乎对训练数据欠拟合：
- en: '[PRE32]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, you can see that the AdaBoost model predicts all class labels of the training
    dataset correctly and also shows a slightly improved test dataset performance
    compared to the decision tree stump. However, you can also see that we introduced
    additional variance with our attempt to reduce the model bias—a greater gap between
    training and test performance.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到 AdaBoost 模型完全预测了训练数据集的所有类标签，并且与决策树桩相比，测试数据集的表现稍有改善。然而，我们在尝试减少模型偏差时引入了额外的方差，导致训练和测试性能之间的差距增大。
- en: Although we used another simple example for demonstration purposes, we can see
    that the performance of the AdaBoost classifier is slightly improved compared
    to the decision stump and achieved very similar accuracy scores as the bagging
    classifier that we trained in the previous section. However, we must note that
    it is considered bad practice to select a model based on the repeated usage of
    the test dataset. The estimate of the generalization performance may be overoptimistic,
    which we discussed in more detail in *Chapter 6*, *Learning Best Practices for
    Model Evaluation and Hyperparameter Tuning*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用了另一个简单的示例来进行演示，但我们可以看到AdaBoost分类器的性能略有改善，比决策桩实现了非常相似的准确性分数，就像我们在前一节中训练的装袋分类器一样。然而，我们必须注意，基于重复使用测试数据集来选择模型被认为是不良实践。我们在*第6章*
    *，学习模型评估和超参数调整的最佳实践*中更详细地讨论了广义性能估计可能过于乐观的问题。
- en: 'Lastly, let’s check what the decision regions look like:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看决策区域的具体情况：
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'By looking at the decision regions, you can see that the decision boundary
    of the AdaBoost model is substantially more complex than the decision boundary
    of the decision stump. In addition, note that the AdaBoost model separates the
    feature space very similarly to the bagging classifier that we trained in the
    previous section:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察决策区域，您可以看到AdaBoost模型的决策边界比决策桩的决策边界复杂得多。此外，请注意，AdaBoost模型将特征空间分隔得与我们在前一节中训练的装袋分类器非常相似：
- en: '![](img/B17582_07_11.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_11.png)'
- en: 'Figure 7.11: The decision boundaries of the decision tree versus AdaBoost'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：决策树与AdaBoost的决策边界
- en: As concluding remarks about ensemble techniques, it is worth noting that ensemble
    learning increases the computational complexity compared to individual classifiers.
    In practice, we need to think carefully about whether we want to pay the price
    of increased computational costs for an often relatively modest improvement in
    predictive performance.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 作为集成技术的总结，值得注意的是，与单个分类器相比，集成学习增加了计算复杂性。在实践中，我们需要仔细考虑是否愿意为预测性能的相对较小改进支付增加的计算成本。
- en: 'An often-cited example of this tradeoff is the famous $1 million *Netflix Prize*,
    which was won using ensemble techniques. The details about the algorithm were
    published in *The BigChaos Solution to the Netflix Grand Prize* by *A. Toescher*,
    *M. Jahrer*, and *R. M. Bell*, *Netflix Prize documentation*, 2009, which is available
    at [http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf](http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf).
    The winning team received the $1 million grand prize money; however, Netflix never
    implemented their model due to its complexity, which made it infeasible for a
    real-world application:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这种权衡的一个常被引用的例子是著名的**1百万美元 *Netflix Prize* **，它是使用集成技术赢得的。有关算法的详细信息已在*The BigChaos
    Solution to the Netflix Grand Prize*中由*A. Toescher*，*M. Jahrer*和*R. M. Bell*发表，*Netflix
    Prize documentation*，2009年，可在[http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf](http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf)找到。获奖团队获得了100万美元的大奖金；然而，由于其复杂性，Netflix从未实施过他们的模型，这使得它在现实世界中难以应用：
- en: “We evaluated some of the new methods offline but the additional accuracy gains
    that we measured did not seem to justify the engineering effort needed to bring
    them into a production environment.”
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们离线评估了一些新方法，但我们测量到的额外准确度增益似乎不足以证明将它们引入生产环境所需的工程投入是合理的。”
- en: ''
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)'
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)'
- en: Gradient boosting – training an ensemble based on loss gradients
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升 - 基于损失梯度训练集成
- en: Gradient boosting is another variant of the boosting concept introduced in the
    previous section, that is, successively training weak learners to create a strong
    ensemble. Gradient boosting is an extremely important topic because it forms the
    basis of popular machine learning algorithms such as XGBoost, which is well-known
    for winning Kaggle competitions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是前一节介绍的增强概念的另一种变体，即连续训练弱学习器以创建强大的集成。梯度提升是一个极其重要的主题，因为它构成了流行的机器学习算法（如XGBoost）的基础，这些算法以在Kaggle竞赛中获胜而闻名。
- en: The gradient boosting algorithm may appear a bit daunting at first. So, in the
    following subsections, we will cover it step by step, starting with a general
    overview. Then, we will see how gradient boosting is used for classification and
    walk through an example. Finally, after we’ve introduced the fundamental concepts
    of gradient boosting, we will take a brief look at popular implementations, such
    as XGBoost, and we will see how we can use gradient boosting in practice.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法一开始可能显得有点令人畏惧。因此，在接下来的小节中，我们将逐步覆盖它，从一个总体概述开始。然后，我们将看到梯度提升是如何用于分类，并通过一个例子详细介绍。最后，在介绍了梯度提升的基本概念后，我们将简要介绍一些流行的实现，比如XGBoost，并探讨如何在实践中应用梯度提升。
- en: Comparing AdaBoost with gradient boosting
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较AdaBoost和梯度提升
- en: Fundamentally, gradient boosting is very similar to AdaBoost, which we discussed
    previously in this chapter. AdaBoost trains decision tree stumps based on errors
    of the previous decision tree stump. In particular, the errors are used to compute
    sample weights in each round as well as for computing a classifier weight for
    each decision tree stump when combining the individual stumps into an ensemble.
    We stop training once a maximum number of iterations (decision tree stumps) is
    reached. Like AdaBoost, gradient boosting fits decision trees in an iterative
    fashion using prediction errors. However, gradient boosting trees are usually
    deeper than decision tree stumps and have typically a maximum depth of 3 to 6
    (or a maximum number of 8 to 64 leaf nodes). Also, in contrast to AdaBoost, gradient
    boosting does not use the prediction errors for assigning sample weights; they
    are used directly to form the target variable for fitting the next tree. Moreover,
    instead of having an individual weighting term for each tree, like in AdaBoost,
    gradient boosting uses a global learning rate that is the same for each tree.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，梯度提升与AdaBoost非常相似，我们在本章之前已经讨论过AdaBoost。AdaBoost基于前一个决策树桩的错误训练决策树桩。特别是，在每一轮中使用错误来计算样本权重，以及在将各个树桩组合成集合时为每个决策树桩计算分类器权重。一旦达到最大迭代次数（决策树桩数），我们就停止训练。与AdaBoost类似，梯度提升以迭代方式使用预测错误拟合决策树。然而，梯度提升树通常比决策树桩更深，通常具有3到6的最大深度（或8到64个叶节点的最大数量）。此外，与AdaBoost不同的是，梯度提升不使用预测错误来分配样本权重；它们直接用于形成下一个树的目标变量的拟合。此外，与AdaBoost不同的是，梯度提升使用全局学习率，该学习率对每棵树都是相同的，而不是每棵树都有一个单独的加权项。
- en: As you can see, AdaBoost and gradient boosting share several similarities but
    differ in certain key aspects. In the following subsection, we will sketch the
    general outline of the gradient boosting algorithm.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，AdaBoost和梯度提升在某些关键方面有相似之处，但也有所不同。在接下来的小节中，我们将概述梯度提升算法的一般轮廓。
- en: Outlining the general gradient boosting algorithm
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 描绘梯度提升算法的一般轮廓
- en: 'In this section, we will look at gradient boosting for classification. For
    simplicity, we will look at a binary classification example. Interested readers
    can find the generalization to the multi-class setting with logistic loss in *Section
    4.6\. Multiclass logistic regression and classification* of the original gradient
    boosting paper written by Friedman in 2001, *Greedy function approximation: A
    gradient boosting machine*, [https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full).'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究用于分类的梯度提升。为简单起见，我们将看一个二分类的例子。对于感兴趣的读者，可以在2001年Friedman撰写的原始梯度提升论文中找到使用多类别设置和逻辑损失的一般化方法，即*第4.6节，多类逻辑回归和分类*，《贪婪函数逼近：梯度提升机》，[https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full)。
- en: '**Gradient boosting for regression**'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升用于回归**'
- en: 'Note that the procedure behind gradient boosting is a bit more complicated
    than AdaBoost. We omit a simpler regression example, which was given in Friedman’s
    paper, for brevity, but interested readers are encouraged to also consider my
    complementary video tutorial on gradient boosting for regression, which is available
    at: [https://www.youtube.com/watch?v=zblsrxc7XpM](https://www.youtube.com/watch?v=zblsrxc7XpM).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，梯度提升背后的过程比 AdaBoost 稍微复杂一些。为了简洁起见，我们忽略了Friedman在论文中提供的一个更简单的回归示例，但鼓励感兴趣的读者也考虑观看我关于回归梯度提升的补充视频教程，该视频教程可在以下链接找到：[https://www.youtube.com/watch?v=zblsrxc7XpM](https://www.youtube.com/watch?v=zblsrxc7XpM)。
- en: In essence, gradient boosting builds a series of trees, where each tree is fit
    on the error—the difference between the label and the predicted value—of the previous
    tree. In each round, the tree ensemble improves as we are nudging each tree more
    in the right direction via small updates. These updates are based on a loss gradient,
    which is how gradient boosting got its name.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，梯度提升构建了一系列树，每棵树都适应于前一棵树的误差 —— 标签和预测值之间的差异。每一轮，树的集合都会因为我们通过小的更新将每棵树朝正确方向推动而得到改进。这些更新基于损失梯度，这也是梯度提升得名的原因。
- en: The following steps will introduce the general algorithm behind gradient boosting.
    After illustrating the main steps, we will dive into some of its parts in more
    detail and walk through a hands-on example in the next subsections.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤将介绍梯度提升背后的一般算法。在说明主要步骤后，我们将更详细地讨论其部分，并在接下来的小节中通过一个实际例子进行实操演示。
- en: 'Initialize a model to return a constant prediction value. For this, we use
    a decision tree root node; that is, a decision tree with a single leaf node. We
    denote the value returned by the tree as ![](img/B17582_04_008.png), and we find
    this value by minimizing a differentiable loss function *L* that we will define
    later:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个模型以返回一个常数预测值。为此，我们使用决策树的根节点；也就是说，一个只有一个叶节点的决策树。我们将树返回的值表示为 ![](img/B17582_04_008.png)，并通过最小化我们稍后将定义的可微损失函数
    *L* 来找到该值：
- en: '![](img/B17582_07_048.png)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_048.png)'
- en: Here, *n* refers to the *n* training examples in our dataset.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，*n* 指的是数据集中的 *n* 个训练样本。
- en: 'For each tree *m* = 1, ..., *M*, where *M* is a user-specified total number
    of trees, we carry out the following computations outlined in *steps 2a* to *2d*
    below:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每棵树 *m* = 1, ..., *M*，其中 *M* 是用户指定的总树数，我们按下面 2a 到 2d 步骤进行如下计算：
- en: 'Compute the difference between a predicted value ![](img/B17582_07_049.png)
    and the class label *y*[i]. This value is sometimes called the *pseudo-response*
    or *pseudo-residual*. More formally, we can write this pseudo-residual as the
    negative gradient of the loss function with respect to the predicted values:'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测值 ![](img/B17582_07_049.png) 与类标签 *y*[i] 之间的差异。这个值有时被称为伪响应或伪残差。更正式地说，我们可以将这个伪残差写成损失函数对预测值的负梯度：
- en: '![](img/B17582_07_050.png)'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_050.png)'
- en: Note that in the notation above *F*(*x*) is the prediction of the previous tree,
    *F*[m][–1](*x*). So, in the first round, this refers to the constant value from
    the tree (single leaf node) from step 1.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在上述符号中，*F*(*x*) 是前一棵树的预测值，*F*[m][–1](*x*)。因此，在第一轮中，这指的是步骤 1 中来自树（单叶节点）的常数值。
- en: Fit a tree to the pseudo-residuals *r*[im]. We use the notation *R*[jm] to denote
    the *j* = 1 ... *J*[m] leaf nodes of the resulting tree in iteration *m*.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将树适配到伪残差 *r*[im] 上。我们使用符号 *R*[jm] 来表示在第 *m* 次迭代中生成的树的 *j* = 1 ... *J*[m] 叶节点。
- en: 'For each leaf node *R*[jm], we compute the following output value:'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个叶节点 *R*[jm]，我们计算以下输出值：
- en: '![](img/B17582_07_051.png)'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_051.png)'
- en: In the next subsection, we will dive deeper into how this ![](img/B17582_07_052.png)
    is computed by minimizing the loss function. At this point, we can already note
    that leaf nodes *R*[jm] may contain more than one training example, hence the
    summation.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将深入探讨如何通过最小化损失函数计算 ![](img/B17582_07_052.png)。此时，我们已经注意到叶节点 *R*[jm]
    可能包含多个训练样本，因此需要求和。
- en: 'Update the model by adding the output values ![](img/B17582_07_053.png) to
    the previous tree:'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将输出值 ![](img/B17582_07_053.png) 添加到前一棵树来更新模型：
- en: '![](img/B17582_07_054.png)'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_054.png)'
- en: However, instead of adding the full predicted values of the current tree ![](img/B17582_07_053.png)
    to the previous tree ![](img/B17582_07_056.png), we scale ![](img/B17582_07_057.png)
    by a learning rate ![](img/B17582_02_036.png), which is typically a small value
    between 0.01 and 1\. In other words, we update the model incrementally by taking
    small steps, which helps avoid overfitting.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，我们不是将当前树的完整预测值 ![](img/B17582_07_053.png) 添加到上一棵树 ![](img/B17582_07_056.png)，而是通过学习率
    ![](img/B17582_02_036.png) 缩放 ![](img/B17582_07_057.png)，该学习率通常是介于0.01和1之间的小值。换句话说，我们通过小步骤增量更新模型，这有助于避免过拟合。
- en: Now, after looking at the general structure of gradient boosting, we will adopt
    these mechanics to look at gradient boosting for classification.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在查看梯度提升的一般结构后，我们将采用这些机制来研究用于分类的梯度提升。
- en: Explaining the gradient boosting algorithm for classification
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释分类梯度提升算法
- en: 'In this subsection, we will go over the details for implementing the gradient
    boosting algorithm for binary classification. In this context, we will be using
    the logistic loss function that we introduced for logistic regression in *Chapter
    3*, *A Tour of Machine Learning Classifiers Using Scikit-Learn*. For a single
    training example, we can specify the logistic loss as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将详细介绍实施二元分类梯度提升算法的细节。在这种情况下，我们将使用我们在《第3章》，《使用Scikit-Learn的机器学习分类器之旅》中为逻辑回归介绍的逻辑损失函数。对于单个训练示例，我们可以指定逻辑损失如下：
- en: '![](img/B17582_07_059.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_059.png)'
- en: 'In *Chapter 3*, we also introduced the log(odds):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在《第3章》中，我们还介绍了对数几率：
- en: '![](img/B17582_07_060.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_060.png)'
- en: 'For reasons that will make sense later, we will use these log(odds) to rewrite
    the logistic function as follows (omitting intermediate steps here):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 出于后续合理性考虑，我们将使用这些对数几率来重写逻辑函数，如下所示（此处省略中间步骤）：
- en: '![](img/B17582_07_061.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_061.png)'
- en: 'Now, we can define the partial derivative of the loss function with respect
    to these log(odds), ![](img/B17582_07_062.png). The derivative of this loss function
    with respect to the log(odds) is:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义损失函数对这些对数几率的偏导数，![](img/B17582_07_062.png)。该损失函数对对数几率的导数是：
- en: '![](img/B17582_07_063.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_07_063.png)'
- en: After specifying these mathematical definitions, let us now revisit the general
    gradient boosting *steps 1* to *2d* from the previous section and reformulate
    them for this binary classification scenario.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定这些数学定义后，让我们现在重新审视前一节中一般梯度提升的*步骤1*至*2d*，并为此二元分类场景重新制定它们。
- en: Create a root node that minimizes the logistic loss. It turns out that the loss
    is minimized if the root node returns the log(odds), ![](img/B17582_04_008.png).
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个最小化逻辑损失的根节点。结果表明，如果根节点返回对数几率，则损失将最小化，![](img/B17582_04_008.png)。
- en: 'For each tree *m* = 1, ..., *M*, where *M* is a user-specified number of total
    trees, we carry out the following computations outlined in *steps 2a* to *2d*:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每棵树 *m* = 1, ..., *M*，其中 *M* 是用户指定的总树数，我们按*步骤2a*至*2d*进行以下计算：
- en: 'We convert the log(odds) into a probability using the familiar logistic function
    that we used in logistic regression (in *Chapter 3*):'
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用在逻辑回归中使用的熟悉逻辑函数将对数几率转换为概率：
- en: '![](img/B17582_07_065.png)'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_065.png)'
- en: 'Then, we compute the pseudo-residual, which is the negative partial derivative
    of the loss with respect to the log(odds), which turns out to be the difference
    between the class label and the predicted probability:'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们计算伪残差，即损失对对数几率的负偏导数，其实是类标签与预测概率之间的差：
- en: '![](img/B17582_07_066.png)'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_066.png)'
- en: Fit a new tree to the pseudo-residuals.
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新树适应于伪残差。
- en: 'For each leaf node *R*[jm], compute a value ![](img/B17582_07_052.png) that
    minimizes the logistic loss function. This includes a summarization step for dealing
    with leaf nodes that contain multiple training examples:'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个叶子节点 *R*[jm]，计算一个值 ![](img/B17582_07_052.png)，该值最小化逻辑损失函数。这包括一个摘要步骤，用于处理包含多个训练示例的叶子节点：
- en: '![](img/B17582_07_068.png)'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_068.png)'
- en: 'Skipping over intermediate mathematical details, this results in the following:'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跳过中间数学细节，得到以下结果：
- en: '![](img/B17582_07_069.png)'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_069.png)'
- en: Note that the summation here is only over the examples at the node corresponding
    to the leaf node *R*[jm] and not the complete training set.
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，此处的求和仅限于与叶子节点 *R*[jm] 对应的节点示例，而不是完整的训练集。
- en: 'Update the model by adding the gamma value from *step 2c* with learning rate
    ![](img/B17582_02_036.png):'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将学习率为 ![](img/B17582_02_036.png) 的 *step 2c* 中的 gamma 值添加到模型中进行更新：
- en: '![](img/B17582_07_071.png)'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B17582_07_071.png)'
- en: '**Outputting log(odds) vs probabilities**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出对数几率与概率的差异**'
- en: Why do the trees return log(odds) values and not probabilities? This is because
    we cannot just add up probability values and arrive at a meaningful result. (So,
    technically speaking, gradient boosting for classification uses regression trees.)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么树返回的是对数几率而不是概率？这是因为我们不能简单地将概率值相加并得出有意义的结果。（因此，严格来说，用于分类的梯度提升使用回归树。）
- en: In this section, we adopted the general gradient boosting algorithm and specified
    it for binary classification, for instance, by replacing the generic loss function
    with the logistic loss and the predicted values with the log(odds). However, many
    of the individual steps may still seem very abstract, and in the next section,
    we will apply these steps to a concrete example.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们采用了通用的梯度提升算法，并将其具体化为二元分类，例如，通过将通用损失函数替换为逻辑损失函数和预测值替换为对数几率。然而，许多个别步骤可能仍然显得非常抽象，在下一节中，我们将这些步骤应用到一个具体的示例中。
- en: Illustrating gradient boosting for classification
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 展示分类梯度提升
- en: 'The previous two subsections went over the condensed mathematical details of
    the gradient boosting algorithm for binary classification. To make these concepts
    clearer, let’s apply it to a small toy example, that is, a training dataset of
    the following three examples shown in *Figure 7.12*:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个小节介绍了用于二元分类的梯度提升算法的简化数学细节。为了更清楚地解释这些概念，让我们将其应用到一个小型玩具示例中，即以下三个示例的训练数据集，如
    *Figure 7.12* 所示：
- en: '![Table Description automatically generated](img/B17582_07_12.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成表格说明](img/B17582_07_12.png)'
- en: 'Figure 7.12: Toy dataset for explaining gradient boosting'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：用于解释梯度提升的玩具数据集
- en: 'Let’s start with *step 1*, constructing the root node and computing the log(odds),
    and *step 2a*, converting the log(odds) into class-membership probabilities and
    computing the pseudo-residuals. Note that based on what we have learned in *Chapter
    3*, the odds can be computed as the number of successes divided by the number
    of failures. Here, we regard label 1 as success and label 0 as failure, so the
    odds are computed as: odds = 2/1\. Carrying out steps *1* and *2a*, we get the
    following results shown in *Figure 7.13*:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 *step 1* 开始，构建根节点并计算对数几率，以及 *step 2a*，将对数几率转换为类成员概率并计算伪残差。请注意，根据我们在 *Chapter
    3* 中学到的内容，几率可以计算为成功的数量除以失败的数量。在这里，我们将标签 1 视为成功，标签 0 视为失败，因此几率计算为：几率 = 2/1。执行 *1*
    和 *2a* 步骤后，我们得到以下结果，如 *Figure 7.13* 所示：
- en: '![Table Description automatically generated](img/B17582_07_13.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成表格说明](img/B17582_07_13.png)'
- en: 'Figure 7.13: Results from the first round of applying step 1 and step 2a'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：应用步骤 1 和步骤 2a 第一轮的结果
- en: 'Next, in *step 2b*, we fit a new tree on the pseudo-residuals *r*. Then, in
    *step 2c*, we compute the output values, ![](img/B17582_03_056.png), for this
    tree as shown in *Figure 7.14*:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 *step 2b* 中，我们在伪残差 *r* 上拟合一个新的树。然后，在 *step 2c* 中，我们计算该树的输出值，![](img/B17582_03_056.png)，如
    *Figure 7.14* 所示：
- en: '![Diagram Description automatically generated](img/B17582_07_14.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成图示说明](img/B17582_07_14.png)'
- en: 'Figure 7.14: An illustration of steps 2b and 2c, which fits a tree to the residuals
    and computes the output values for each leaf node'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14：展示步骤 2b 和 2c，即将决策树拟合到残差上，并计算每个叶节点的输出值
- en: (Note that we artificially limit the tree to have only two leaf nodes, which
    helps illustrate what happens if a leaf node contains more than one example.)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: （请注意，我们人为地限制了树只有两个叶节点，这有助于说明如果一个叶节点包含多个示例会发生什么。）
- en: 'Then, in the final *step 2d*, we update the previous model and the current
    model. Assuming a learning rate of ![](img/B17582_02_063.png), the resulting prediction
    for the first training example is shown in *Figure 7.15*:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在最后的 *step 2d* 中，我们更新前一模型和当前模型。假设学习率为 ![](img/B17582_02_063.png)，第一个训练示例的预测结果如
    *Figure 7.15* 所示：
- en: '![Diagram Description automatically generated](img/B17582_07_15.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成图示说明](img/B17582_07_15.png)'
- en: 'Figure 7.15: The update of the previous model shown in the context of the first
    training example'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15：在第一个训练示例的背景下展示前一模型的更新
- en: 'Now that we have completed *steps 2a* to *2d* of the first round, *m* = 1,
    we can proceed to execute *steps 2a* to *2d* for the second round, *m* = 2\. In
    the second round, we use the log(odds) returned by the updated model, for example,
    ![](img/B17582_07_074.png), as input to *step* *2A*. The new values we obtain
    in the second round are shown in *Figure 7.16*:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了第一轮的 *步骤 2a* 到 *2d*，*m* = 1，我们可以继续执行第二轮的 *步骤 2a* 到 *2d*，*m* = 2\. 在第二轮中，我们使用更新模型返回的
    log(odds)，例如，![](img/B17582_07_074.png)，作为 *步骤 2A* 的输入。我们在第二轮获得的新值显示在 *图 7.16*
    中：
- en: '![Table Description automatically generated](img/B17582_07_16.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的表描述](img/B17582_07_16.png)'
- en: 'Figure 7.16: Values from the second round next to the values from the first
    round'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：第二轮中的值与第一轮旁边的值
- en: We can already see that the predicted probabilities are higher for the positive
    class and lower for the negative class. Consequently, the residuals are getting
    smaller, too. Note that the process of *steps 2a* to *2d* is repeated until we
    have fit *M* trees or the residuals are smaller than a user-specified threshold
    value. Then, once the gradient boosting algorithm has completed, we can use it
    to predict the class labels by thresholding the probability values of the final
    model, *F*[M](*x*) at 0.5, like logistic regression in *Chapter 3*. However, in
    contrast to logistic regression, gradient boosting consists of multiple trees
    and produces nonlinear decision boundaries. In the next section, we will look
    at how gradient boosting looks in action.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到，对于正类别，预测的概率更高，而对于负类别，预测的概率更低。因此，残差也在变小。请注意，*步骤 2a* 到 *2d* 的过程重复进行，直到我们拟合了
    *M* 棵树或残差小于用户指定的阈值为止。然后，一旦梯度提升算法完成，我们可以使用它通过将最终模型的概率值 *F*[M](*x*) 阈值化为 0.5 来预测类标签，就像第
    3 章中的逻辑回归一样。然而，与逻辑回归不同，梯度提升由多棵树组成，并产生非线性决策边界。在下一节中，我们将看看梯度提升是如何发挥作用的。
- en: Using XGBoost
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 XGBoost
- en: After covering the nitty-gritty details behind gradient boosting, let’s finally
    look at how we can use gradient boosting code implementations.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在讲解了梯度提升背后的细枝末节之后，让我们最终看看如何使用梯度提升的代码实现。
- en: In scikit-learn, gradient boosting is implemented as `sklearn.ensemble.GradientBoostingClassifier`
    (see [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    for more details). It is important to note that gradient boosting is a sequential
    process that can be slow to train. However, in recent years a more popular implementation
    of gradient boosting has emerged, namely, XGBoost.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，梯度提升被实现为 `sklearn.ensemble.GradientBoostingClassifier`（详见 [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    获取更多详细信息）。值得注意的是，梯度提升是一个顺序过程，训练速度可能较慢。然而，在最近几年中，出现了更流行的梯度提升实现，即 XGBoost。
- en: XGBoost proposed several tricks and approximations that speed up the training
    process substantially. Hence, the name XGBoost, which stands for extreme gradient
    boosting. Moreover, these approximations and tricks result in very good predictive
    performances. In fact, XGBoost gained popularity as it has been the winning solution
    for many Kaggle competitions.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 提出了几种技巧和近似方法，显著加快了训练过程。因此，XGBoost 的名称即为 extreme gradient boosting。此外，这些近似和技巧导致了非常好的预测性能。事实上，XGBoost
    因为在许多 Kaggle 竞赛中取得了胜利而变得流行。
- en: Next to XGBoost, there are also other popular implementations of gradient boosting,
    for example, LightGBM and CatBoost. Inspired by LightGBM, scikit-learn now also
    implements a `HistGradientBoostingClassifier`, which is more performant than the
    original gradient boosting classifier (`GradientBoostingClassifier`).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 XGBoost 外，还有其他流行的梯度提升实现，例如 LightGBM 和 CatBoost。受 LightGBM 的启发，scikit-learn
    现在还实现了 `HistGradientBoostingClassifier`，比原始梯度提升分类器（`GradientBoostingClassifier`）性能更好。
- en: 'You can find more details about these methods via the resources below:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下资源找到关于这些方法的更多细节：
- en: '`XGBoost`: [https://xgboost.readthedocs.io/en/stable/](https://xgboost.readthedocs.io/en/stable/)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`XGBoost`: [https://xgboost.readthedocs.io/en/stable/](https://xgboost.readthedocs.io/en/stable/)'
- en: '`LightGBM`: [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LightGBM`: [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)'
- en: '`CatBoost`: [https://catboost.ai](https://catboost.ai)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CatBoost`: [https://catboost.ai](https://catboost.ai)'
- en: '`HistGradientBoostingClassifier`: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HistGradientBoostingClassifier`: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)'
- en: 'However, since XGBoost is still among the most popular gradient boosting implementations,
    we will see how we can use it in practice. First, we need to install it, for example
    via `pip`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于XGBoost仍然是最受欢迎的梯度提升实现之一，我们将看看如何在实践中使用它。首先，我们需要安装它，例如通过 `pip`：
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Installing XGBoost**'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装XGBoost**'
- en: 'For this chapter, we used XGBoost version 1.5.0, which can be installed via:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了 XGBoost 版本 1.5.0，可以通过以下方式安装：
- en: '[PRE35]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: You can find more information about the installation details at [https://xgboost.readthedocs.io/en/stable/install.html](https://xgboost.readthedocs.io/en/stable/install.html)
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 更多有关安装细节的信息可以在 [https://xgboost.readthedocs.io/en/stable/install.html](https://xgboost.readthedocs.io/en/stable/install.html)
    找到。
- en: 'Fortunately, XGBoost’s `XGBClassifier` follows the scikit-learn API. So, using
    it is relatively straightforward:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，XGBoost 的 `XGBClassifier` 遵循了 scikit-learn 的 API。因此，使用它相对比较简单：
- en: '[PRE36]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Here, we fit the gradient boosting classifier with 1,000 trees (rounds) and
    a learning rate of 0.01\. Typically, a learning rate between 0.01 and 0.1 is recommended.
    However, remember that the learning rate is used for scaling the predictions from
    the individual rounds. So, intuitively, the lower the learning rate, the more
    estimators are required to achieve accurate predictions.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 1,000 棵树（回合）和学习率为 0.01 来拟合梯度提升分类器。通常建议学习率在 0.01 到 0.1 之间。但是，请记住，学习率用于缩放来自各个回合的预测。因此，直观地说，学习率越低，需要的估计器数量越多才能获得准确的预测。
- en: Next, we have the `max_depth` for the individual decision trees, which we set
    to 4\. Since we are still boosting weak learners, a value between 2 and 6 is reasonable,
    but larger values may also work well depending on the dataset.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有个体决策树的 `max_depth`，我们将其设置为 4。由于我们仍在增强弱学习器，因此在 2 到 6 之间选择一个值是合理的，但是根据数据集的不同，较大的值也可能表现良好。
- en: Finally, `use_label_encoder=False` disables a warning message which informs
    users that XGBoost is not converting labels by default anymore, and it expects
    users to provide labels in an integer format starting with label 0\. (There is
    nothing to worry about here, since we have been following this format throughout
    this book.)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`use_label_encoder=False` 禁用了一条警告消息，告知用户 XGBoost 不再默认转换标签，并且期望用户以从标签 0 开始的整数格式提供标签。（这里没有什么可担心的，因为我们在本书中一直遵循这种格式。）
- en: There are many more settings available, and a detailed discussion is out of
    the scope of this book. However, interested readers can find more details in the
    original documentation at [https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他可用的设置，详细讨论超出了本书的范围。但是，有兴趣的读者可以在原始文档中找到更多细节，网址为 [https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier)。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at some of the most popular and widely used techniques
    for ensemble learning. Ensemble methods combine different classification models
    to cancel out their individual weaknesses, which often results in stable and well-performing
    models that are very attractive for industrial applications as well as machine
    learning competitions.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了一些最流行和广泛使用的集成学习技术。集成方法结合了不同的分类模型，以消除它们各自的弱点，这通常会产生稳定且性能良好的模型，这对工业应用和机器学习竞赛非常有吸引力。
- en: At the beginning of this chapter, we implemented `MajorityVoteClassifier` in
    Python, which allows us to combine different algorithms for classification. We
    then looked at bagging, a useful technique for reducing the variance of a model
    by drawing random bootstrap samples from the training dataset and combining the
    individually trained classifiers via majority vote. Lastly, we learned about boosting
    in the form of AdaBoost and gradient boosting, which are algorithms based on training
    weak learners that subsequently learn from mistakes.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们用Python实现了`MajorityVoteClassifier`，它允许我们结合不同的分类算法。然后我们看了看bagging，这是一种通过从训练数据集中随机抽取自举样本来减少模型方差的有用技术，并通过多数投票来结合各自训练的分类器。最后，我们学习了关于AdaBoost和梯度提升的
    boosting 技术，这些算法基于训练弱学习器，并从错误中学习。
- en: Throughout the previous chapters, we learned a lot about different learning
    algorithms, tuning, and evaluation techniques. In the next chapter, we will look
    at a particular application of machine learning, sentiment analysis, which has
    become an interesting topic in the internet and social media era.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学到了关于不同学习算法、调优和评估技术的很多知识。在下一章中，我们将研究机器学习的一个特定应用，情感分析，这在互联网和社交媒体时代变得非常有趣。
- en: Join our book’s Discord space
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的Discord工作空间，与作者进行每月的*问我任何事*会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
