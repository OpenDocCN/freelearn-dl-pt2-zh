- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining Different Models for Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we focused on the best practices for tuning and evaluating
    different models for classification. In this chapter, we will build upon those
    techniques and explore different methods for constructing a set of classifiers
    that can often have a better predictive performance than any of its individual
    members. We will learn how to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Make predictions based on majority voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use bagging to reduce overfitting by drawing random combinations of the training
    dataset with repetition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply boosting to build powerful models from weak learners that learn from their
    mistakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning with ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of **ensemble methods** is to combine different classifiers into a
    meta-classifier that has better generalization performance than each individual
    classifier alone. For example, assuming that we collected predictions from 10
    experts, ensemble methods would allow us to strategically combine those predictions
    by the 10 experts to come up with a prediction that was more accurate and robust
    than the predictions by each individual expert. As you will see later in this
    chapter, there are several different approaches for creating an ensemble of classifiers.
    This section will introduce a basic explanation of how ensembles work and why
    they are typically recognized for yielding a good generalization performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the most popular ensemble methods that use
    the **majority voting** principle. Majority voting simply means that we select
    the class label that has been predicted by the majority of classifiers, that is,
    received more than 50 percent of the votes. Strictly speaking, the term “majority
    vote” refers to binary class settings only. However, it is easy to generalize
    the majority voting principle to multiclass settings, which is known as **plurality
    voting**. (In the UK, people distinguish between majority and plurality voting
    via the terms “absolute” and “relative” majority, respectively.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we select the class label that received the most votes (the mode). *Figure
    7.1* illustrates the concept of majority and plurality voting for an ensemble
    of 10 classifiers, where each unique symbol (triangle, square, and circle) represents
    a unique class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: The different voting concepts'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the training dataset, we start by training *m* different classifiers
    (*C*[1], ..., *C*[m]). Depending on the technique, the ensemble can be built from
    different classification algorithms, for example, decision trees, support vector
    machines, logistic regression classifiers, and so on. Alternatively, we can also
    use the same base classification algorithm, fitting different subsets of the training
    dataset. One prominent example of this approach is the random forest algorithm
    combining different decision tree classifiers, which we covered in *Chapter 3*,
    *A Tour of Machine Learning Classifiers Using Scikit-Learn*. *Figure 7.2* illustrates
    the concept of a general ensemble approach using majority voting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: A general ensemble approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'To predict a class label via simple majority or plurality voting, we can combine
    the predicted class labels of each individual classifier, *C*[j], and select the
    class label, ![](img/B17582_04_008.png), that received the most votes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_002.png)'
  prefs: []
  type: TYPE_IMG
- en: (In statistics, the mode is the most frequent event or result in a set. For
    example, mode{1, 2, 1, 1, 2, 4, 5, 4} = 1.)
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in a binary classification task where class1 = –1 and class2 = +1,
    we can write the majority vote prediction as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To illustrate why ensemble methods can work better than individual classifiers
    alone, let’s apply some concepts of combinatorics. For the following example,
    we will make the assumption that all *n*-base classifiers for a binary classification
    task have an equal error rate, ![](img/B17582_07_004.png). Furthermore, we will
    assume that the classifiers are independent and the error rates are not correlated.
    Under those assumptions, we can simply express the error probability of an ensemble
    of base classifiers as a probability mass function of a binomial distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17582_07_006.png) is the binomial coefficient *n choose k*.
    In other words, we compute the probability that the prediction of the ensemble
    is wrong. Now, let’s take a look at a more concrete example of 11 base classifiers
    (*n* = 11), where each classifier has an error rate of 0.25 (![](img/B17582_07_007.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_008.png)'
  prefs: []
  type: TYPE_IMG
- en: '**The binomial coefficient**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The binomial coefficient refers to the number of ways we can choose subsets
    of *k* unordered elements from a set of size *n*; thus, it is often called “n
    choose k.” Since the order does not matter here, the binomial coefficient is also
    sometimes referred to as *combination* or *combinatorial number*, and in its unabbreviated
    form, it is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_009.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the symbol (!) stands for factorial—for example, 3! = 3×2×1 = 6.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the error rate of the ensemble (0.034) is much lower than the
    error rate of each individual classifier (0.25) if all the assumptions are met.
    Note that, in this simplified illustration, a 50-50 split by an even number of
    classifiers, *n*, is treated as an error, whereas this is only true half of the
    time. To compare such an idealistic ensemble classifier to a base classifier over
    a range of different base error rates, let’s implement the probability mass function
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have implemented the `ensemble_error` function, we can compute the
    ensemble error rates for a range of different base errors from 0.0 to 1.0 to visualize
    the relationship between ensemble and base errors in a line graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the resulting plot, the error probability of an ensemble is
    always better than the error of an individual base classifier, as long as the
    base classifiers perform better than random guessing (![](img/B17582_07_010.png)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the *y* axis depicts the base error (dotted line) as well as the
    ensemble error (continuous line):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: A plot of the ensemble error versus the base error'
  prefs: []
  type: TYPE_NORMAL
- en: Combining classifiers via majority vote
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the short introduction to ensemble learning in the previous section, let’s
    start with a warm-up exercise and implement a simple ensemble classifier for majority
    voting in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '**Plurality voting**'
  prefs: []
  type: TYPE_NORMAL
- en: Although the majority voting algorithm that we will discuss in this section
    also generalizes to multiclass settings via plurality voting, the term “majority
    voting” will be used for simplicity, as is often the case in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple majority vote classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm that we are going to implement in this section will allow us
    to combine different classification algorithms associated with individual weights
    for confidence. Our goal is to build a stronger meta-classifier that balances
    out the individual classifiers’ weaknesses on a particular dataset. In more precise
    mathematical terms, we can write the weighted majority vote as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w*[j] is a weight associated with a base classifier, *C*[j]; ![](img/B17582_04_008.png)
    is the predicted class label of the ensemble; *A* is the set of unique class labels;
    ![](img/B17582_07_013.png) (Greek chi) is the characteristic function or indicator
    function, which returns 1 if the predicted class of the *j*th classifier matches
    *i* (*C*[j](**x**) = *i*). For equal weights, we can simplify this equation and
    write it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To better understand the concept of *weighting*, we will now take a look at
    a more concrete example. Let’s assume that we have an ensemble of three base classifiers,
    ![](img/B17582_07_015.png), and we want to predict the class label, ![](img/B17582_07_016.png),
    of a given example, **x**. Two out of three base classifiers predict the class
    label 0, and one, *C*[3], predicts that the example belongs to class 1\. If we
    weight the predictions of each base classifier equally, the majority vote predicts
    that the example belongs to class 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let’s assign a weight of 0.6 to *C*[3], and let’s weight *C*[1] and *C*[2]
    by a coefficient of 0.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'More simply, since 3×0.2 = 0.6, we can say that the prediction made by *C*[3]
    has three times more weight than the predictions by *C*[1] or *C*[2], which we
    can write as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To translate the concept of the weighted majority vote into Python code, we
    can use NumPy’s convenient `argmax` and `bincount` functions, where `bincount`
    counts the number of occurrences of each class label. The `argmax` function then
    returns the index position of the highest count, corresponding to the majority
    class label (this assumes that class labels start at 0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you will remember from the discussion on logistic regression in *Chapter
    3*, certain classifiers in scikit-learn can also return the probability of a predicted
    class label via the `predict_proba` method. Using the predicted class probabilities
    instead of the class labels for majority voting can be useful if the classifiers
    in our ensemble are well calibrated. The modified version of the majority vote
    for predicting class labels from probabilities can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_020.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p*[ij] is the predicted probability of the *j*th classifier for class
    label *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To continue with our previous example, let’s assume that we have a binary classification
    problem with class labels ![](img/B17582_07_021.png) and an ensemble of three
    classifiers, ![](img/B17582_07_015.png). Let’s assume that the classifiers *C*[j]
    return the following class membership probabilities for a particular example,
    **x**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the same weights as previously (0.2, 0.2, and 0.6), we can then calculate
    the individual class probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To implement the weighted majority vote based on class probabilities, we can
    again make use of NumPy, using `np.average` and `np.argmax`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Putting everything together, let’s now implement `MajorityVoteClassifier` in
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We’ve added a lot of comments to the code to explain the individual parts. However,
    before we implement the remaining methods, let’s take a quick break and discuss
    some of the code that may look confusing at first. We used the `BaseEstimator`
    and `ClassifierMixin` parent classes to get some base functionality *for free*,
    including the `get_params` and `set_params` methods to set and return the classifier’s
    parameters, as well as the `score` method to calculate the prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will add the `predict` method to predict the class label via a majority
    vote based on the class labels if we initialize a new `MajorityVoteClassifier`
    object with `vote=''classlabel''`. Alternatively, we will be able to initialize
    the ensemble classifier with `vote=''probability''` to predict the class label
    based on the class membership probabilities. Furthermore, we will also add a `predict_proba`
    method to return the averaged probabilities, which is useful when computing the
    **receiver operating characteristic area under the curve** (**ROC AUC**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Also, note that we defined our own modified version of the `get_params` method
    to use the `_name_estimators` function to access the parameters of individual
    classifiers in the ensemble; this may look a little bit complicated at first,
    but it will make perfect sense when we use grid search for hyperparameter tuning
    in later sections.
  prefs: []
  type: TYPE_NORMAL
- en: '**VotingClassifier in scikit-learn**'
  prefs: []
  type: TYPE_NORMAL
- en: Although the `MajorityVoteClassifier` implementation is very useful for demonstration
    purposes, we implemented a more sophisticated version of this majority vote classifier
    in scikit-learn based on the implementation in the first edition of this book.
    The ensemble classifier is available as `sklearn.ensemble.VotingClassifier` in
    scikit-learn version 0.17 and newer. You can find out more about `VotingClassifier`
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)
  prefs: []
  type: TYPE_NORMAL
- en: Using the majority voting principle to make predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it is time to put the `MajorityVoteClassifier` that we implemented in the
    previous section into action. But first, let’s prepare a dataset that we can test
    it on. Since we are already familiar with techniques to load datasets from CSV
    files, we will take a shortcut and load the Iris dataset from scikit-learn’s `datasets`
    module. Furthermore, we will only select two features, *sepal width* and *petal
    length*, to make the classification task more challenging for illustration purposes.
    Although our `MajorityVoteClassifier` generalizes to multiclass problems, we will
    only classify flower examples from the `Iris-versicolor` and `Iris-virginica`
    classes, with which we will compute the ROC AUC later. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Class membership probabilities from decision trees**'
  prefs: []
  type: TYPE_NORMAL
- en: Note that scikit-learn uses the `predict_proba` method (if applicable) to compute
    the ROC AUC score. In *Chapter 3*, we saw how the class probabilities are computed
    in logistic regression models. In decision trees, the probabilities are calculated
    from a frequency vector that is created for each node at training time. The vector
    collects the frequency values of each class label computed from the class label
    distribution at that node. Then, the frequencies are normalized so that they sum
    up to 1\. Similarly, the class labels of the k-nearest neighbors are aggregated
    to return the normalized class label frequencies in the k-nearest neighbors algorithm.
    Although the normalized probabilities returned by both the decision tree and k-nearest
    neighbors classifier may look similar to the probabilities obtained from a logistic
    regression model, we have to be aware that they are actually not derived from
    probability mass functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will split the Iris examples into 50 percent training and 50 percent
    test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the training dataset, we now will train three different classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbors classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will then evaluate the model performance of each classifier via 10-fold
    cross-validation on the training dataset before we combine them into an ensemble
    classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output that we receive, as shown in the following snippet, shows that the
    predictive performances of the individual classifiers are almost equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You may be wondering why we trained the logistic regression and k-nearest neighbors
    classifier as part of a pipeline. The reason behind it is that, as discussed in
    *Chapter 3*, both the logistic regression and k-nearest neighbors algorithms (using
    the Euclidean distance metric) are not scale-invariant, in contrast to decision
    trees. Although the Iris features are all measured on the same scale (cm), it
    is a good habit to work with standardized features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s move on to the more exciting part and combine the individual classifiers
    for majority rule voting in our `MajorityVoteClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the performance of `MajorityVotingClassifier` has improved over
    the individual classifiers in the 10-fold cross-validation evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating and tuning the ensemble classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are going to compute the ROC curves from the test dataset
    to check that `MajorityVoteClassifier` generalizes well with unseen data. We must
    remember that the test dataset is not to be used for model selection; its purpose
    is merely to report an unbiased estimate of the generalization performance of
    a classifier system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the resulting ROC, the ensemble classifier also performs
    well on the test dataset (ROC AUC = 0.95). However, you can see that the logistic
    regression classifier performs similarly well on the same dataset, which is probably
    due to the high variance (in this case, the sensitivity of how we split the dataset)
    given the small size of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: The ROC curve for the different classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: Since we only selected two features for the classification examples, it would
    be interesting to see what the decision region of the ensemble classifier actually
    looks like.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is not necessary to standardize the training features prior to
    model fitting, because our logistic regression and k-nearest neighbors pipelines
    will automatically take care of it, we will standardize the training dataset so
    that the decision regions of the decision tree will be on the same scale for visual
    purposes. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, but also as expected, the decision regions of the ensemble classifier
    seem to be a hybrid of the decision regions from the individual classifiers. At
    first glance, the majority vote decision boundary looks a lot like the decision
    of the decision tree stump, which is orthogonal to the *y* axis for *sepal width* ≥ 1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, you can also notice the nonlinearity from the k-nearest neighbor classifier
    mixed in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: The decision boundaries for the different classifiers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we tune the individual classifier’s parameters for ensemble classification,
    let’s call the `get_params` method to get a basic idea of how we can access the
    individual parameters inside a `GridSearchCV` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the values returned by the `get_params` method, we now know how to
    access the individual classifier’s attributes. Let’s now tune the inverse regularization
    parameter, `C`, of the logistic regression classifier and the decision tree depth
    via a grid search for demonstration purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After the grid search has completed, we can print the different hyperparameter
    value combinations and the average ROC AUC scores computed via 10-fold cross-validation
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we get the best cross-validation results when we choose a lower
    regularization strength (`C=0.001`), whereas the tree depth does not seem to affect
    the performance at all, suggesting that a decision stump is sufficient to separate
    the data. To remind ourselves that it is a bad practice to use the test dataset
    more than once for model evaluation, we are not going to estimate the generalization
    performance of the tuned hyperparameters in this section. We will move on swiftly
    to an alternative approach for ensemble learning: **bagging**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Building ensembles using stacking**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The majority vote approach we implemented in this section is not to be confused
    with stacking. The stacking algorithm can be understood as a two-level ensemble,
    where the first level consists of individual classifiers that feed their predictions
    to the second level, where another classifier (typically logistic regression)
    is fit to the level-one classifier predictions to make the final predictions.
    For more information on stacking, see the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: The stacking algorithm has been described in more detail by David H. Wolpert
    in *Stacked generalization*, *Neural Networks*, 5(2):241–259, 1992 ([https://www.sciencedirect.com/science/article/pii/S0893608005800231](https://www.sciencedirect.com/science/article/pii/S0893608005800231)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interested readers can find our video tutorial about stacking on YouTube at
    [https://www.youtube.com/watch?v=8T2emza6g80](https://www.youtube.com/watch?v=8T2emza6g80).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A scikit-learn compatible version of a stacking classifier is available from
    mlxtend: [http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, a `StackingClassifier` has recently been added to scikit-learn (available
    in version 0.22 and newer); for more information, please see the documentation
    at [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging – building an ensemble of classifiers from bootstrap samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging is an ensemble learning technique that is closely related to the `MajorityVoteClassifier`
    that we implemented in the previous section. However, instead of using the same
    training dataset to fit the individual classifiers in the ensemble, we draw bootstrap
    samples (random samples with replacement) from the initial training dataset, which
    is why bagging is also known as *bootstrap aggregating*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of bagging is summarized in *Figure 7.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: The concept of bagging'
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will work through a simple example of bagging
    by hand and use scikit-learn for classifying wine examples.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging in a nutshell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To provide a more concrete example of how the bootstrap aggregating of a bagging
    classifier works, let’s consider the example shown in *Figure 7.7*. Here, we have
    seven different training instances (denoted as indices 1-7) that are sampled randomly
    with replacement in each round of bagging. Each bootstrap sample is then used
    to fit a classifier, *C*[j], which is most typically an unpruned decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B17582_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: An example of bagging'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 7.7*, each classifier receives a random subset of
    examples from the training dataset. We denote these random samples obtained via
    bagging as *Bagging round 1*, *Bagging round 2*, and so on. Each subset contains
    a certain portion of duplicates and some of the original examples don’t appear
    in a resampled dataset at all due to sampling with replacement. Once the individual
    classifiers are fit to the bootstrap samples, the predictions are combined using
    majority voting.
  prefs: []
  type: TYPE_NORMAL
- en: Note that bagging is also related to the random forest classifier that we introduced
    in *Chapter 3*. In fact, random forests are a special case of bagging where we
    also use random feature subsets when fitting the individual decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model ensembles using bagging**'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging was first proposed by Leo Breiman in a technical report in 1994; he
    also showed that bagging can improve the accuracy of unstable models and decrease
    the degree of overfitting. We highly recommend that you read about his research
    in *Bagging predictors* by *L. Breiman*, *Machine Learning*, 24(2):123–140, 1996,
    which is freely available online, to learn more details about bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Applying bagging to classify examples in the Wine dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To see bagging in action, let’s create a more complex classification problem
    using the Wine dataset that was introduced in *Chapter 4*, *Building Good Training
    Datasets – Data Preprocessing*. Here, we will only consider the Wine classes 2
    and 3, and we will select two features – `Alcohol` and `OD280/OD315 of diluted
    wines`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will encode the class labels into binary format and split the dataset
    into 80 percent training and 20 percent test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Obtaining the Wine dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable. For instance, to load the Wine dataset from a local
    directory, take the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'and replace them with these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'A `BaggingClassifier` algorithm is already implemented in scikit-learn, which
    we can import from the `ensemble` submodule. Here, we will use an unpruned decision
    tree as the base classifier and create an ensemble of 500 decision trees fit on
    different bootstrap samples of the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will calculate the accuracy score of the prediction on the training
    and test datasets to compare the performance of the bagging classifier to the
    performance of a single unpruned decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the accuracy values that we printed here, the unpruned decision tree
    predicts all the class labels of the training examples correctly; however, the
    substantially lower test accuracy indicates high variance (overfitting) of the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the training accuracies of the decision tree and bagging classifier
    are similar on the training dataset (both 100 percent), we can see that the bagging
    classifier has a slightly better generalization performance, as estimated on the
    test dataset. Next, let’s compare the decision regions between the decision tree
    and the bagging classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see in the resulting plot, the piece-wise linear decision boundary
    of the three-node deep decision tree looks smoother in the bagging ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: The piece-wise linear decision boundary of a decision tree versus
    bagging'
  prefs: []
  type: TYPE_NORMAL
- en: We only looked at a very simple bagging example in this section. In practice,
    more complex classification tasks and a dataset’s high dimensionality can easily
    lead to overfitting in single decision trees, and this is where the bagging algorithm
    can really play to its strengths. Finally, we must note that the bagging algorithm
    can be an effective approach to reducing the variance of a model. However, bagging
    is ineffective in reducing model bias, that is, models that are too simple to
    capture the trends in the data well. This is why we want to perform bagging on
    an ensemble of classifiers with low bias, for example, unpruned decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging weak learners via adaptive boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this last section about ensemble methods, we will discuss **boosting**,
    with a special focus on its most common implementation: **Adaptive Boosting**
    (**AdaBoost**).'
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost recognition**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original idea behind AdaBoost was formulated by Robert E. Schapire in 1990
    in *The Strength of Weak Learnability*, *Machine Learning*, 5(2): 197-227, 1990,URL:
    [http://rob.schapire.net/papers/strengthofweak.pdf](http://rob.schapire.net/papers/strengthofweak.pdf).
    After Robert Schapire and Yoav Freund presented the AdaBoost algorithm in the
    *Proceedings of the Thirteenth International Conference* (ICML 1996), AdaBoost
    became one of the most widely used ensemble methods in the years that followed
    (*Experiments with a New Boosting Algorithm* by *Y. Freund*, *R. E. Schapire*,
    and others, *ICML*, volume 96, 148-156, 1996). In 2003, Freund and Schapire received
    the Gödel Prize for their groundbreaking work, which is a prestigious prize for
    the most outstanding publications in the field of computer science.'
  prefs: []
  type: TYPE_NORMAL
- en: In boosting, the ensemble consists of very simple base classifiers, also often
    referred to as **weak learners**, which often only have a slight performance advantage
    over random guessing—a typical example of a weak learner is a decision tree stump.
    The key concept behind boosting is to focus on training examples that are hard
    to classify, that is, to let the weak learners subsequently learn from misclassified
    training examples to improve the performance of the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections will introduce the algorithmic procedure behind the
    general concept of boosting and AdaBoost. Lastly, we will use scikit-learn for
    a practical classification example.
  prefs: []
  type: TYPE_NORMAL
- en: How adaptive boosting works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In contrast to bagging, the initial formulation of the boosting algorithm uses
    random subsets of training examples drawn from the training dataset without replacement;
    the original boosting procedure can be summarized in the following four key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Draw a random subset (sample) of training examples, *d*[1], without replacement
    from the training dataset, *D*, to train a weak learner, *C*[1].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a second random training subset, *d*[2], without replacement from the training
    dataset and add 50 percent of the examples that were previously misclassified
    to train a weak learner, *C*[2].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the training examples, *d*[3], in the training dataset, *D*, which *C*[1]
    and *C*[2] disagree upon, to train a third weak learner, *C*[3].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the weak learners *C*[1], *C*[2], and *C*[3] via majority voting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As discussed by Leo Breiman (*Bias, variance, and arcing classifiers*, 1996),
    boosting can lead to a decrease in bias as well as variance compared to bagging
    models. In practice, however, boosting algorithms such as AdaBoost are also known
    for their high variance, that is, the tendency to overfit the training data (*An
    improvement of AdaBoost to avoid overfitting* by *G. Raetsch*, *T. Onoda*, and
    *K. R. Mueller*. *Proceedings of the International Conference on Neural Information
    Processing*, *CiteSeer*, 1998).
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the original boosting procedure described here, AdaBoost uses
    the complete training dataset to train the weak learners, where the training examples
    are reweighted in each iteration to build a strong classifier that learns from
    the mistakes of the previous weak learners in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive deeper into the specific details of the AdaBoost algorithm,
    let’s take a look at *Figure 7.9* to get a better grasp of the basic concept behind
    AdaBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: The concept of AdaBoost to improve weak learners'
  prefs: []
  type: TYPE_NORMAL
- en: To walk through the AdaBoost illustration step by step, we will start with subfigure
    **1**, which represents a training dataset for binary classification where all
    training examples are assigned equal weights. Based on this training dataset,
    we train a decision stump (shown as a dashed line) that tries to classify the
    examples of the two classes (triangles and circles), as well as possibly minimizing
    the loss function (or the impurity score in the special case of decision tree
    ensembles).
  prefs: []
  type: TYPE_NORMAL
- en: For the next round (subfigure **2**), we assign a larger weight to the two previously
    misclassified examples (circles). Furthermore, we lower the weight of the correctly
    classified examples. The next decision stump will now be more focused on the training
    examples that have the largest weights—the training examples that are supposedly
    hard to classify.
  prefs: []
  type: TYPE_NORMAL
- en: The weak learner shown in subfigure **2** misclassifies three different examples
    from the circle class, which are then assigned a larger weight, as shown in subfigure
    **3**.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that our AdaBoost ensemble only consists of three rounds of boosting,
    we then combine the three weak learners trained on different reweighted training
    subsets by a weighted majority vote, as shown in subfigure **4**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a better understanding of the basic concept of AdaBoost, let’s
    take a more detailed look at the algorithm using pseudo code. For clarity, we
    will denote element-wise multiplication by the cross symbol (×) and the dot-product
    between two vectors by a dot symbol (⋅):'
  prefs: []
  type: TYPE_NORMAL
- en: Set the weight vector, **w**, to uniform weights, where ![](img/B17582_07_026.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j* in *m* boosting rounds, do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train a weighted weak learner: *C*[j] = train(**X**, **y**, **w**).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Predict class labels: ![](img/B17582_07_027.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the weighted error rate: ![](img/B17582_07_028.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the coefficient: ![](img/B17582_07_029.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the weights: ![](img/B17582_07_030.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Normalize the weights to sum to 1: ![](img/B17582_07_031.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the final prediction: ![](img/B17582_07_032.png).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that the expression ![](img/B17582_07_033.png) in *step 2c* refers to a
    binary vector consisting of 1s and 0s, where a 1 is assigned if the prediction
    is incorrect and 0 is assigned otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the AdaBoost algorithm seems to be pretty straightforward, let’s walk
    through a more concrete example using a training dataset consisting of 10 training
    examples, as illustrated in *Figure 7.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Running 10 training examples through the AdaBoost algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: The first column of the table depicts the indices of training examples 1 to
    10\. In the second column, you can see the feature values of the individual samples,
    assuming this is a one-dimensional dataset. The third column shows the true class
    label, *y*[i], for each training sample, *x*[i], where ![](img/B17582_07_034.png).
    The initial weights are shown in the fourth column; we initialize the weights
    uniformly (assigning the same constant value) and normalize them to sum to 1\.
    In the case of the 10-sample training dataset, we therefore assign 0.1 to each
    weight, *w*[i], in the weight vector, **w**. The predicted class labels, ![](img/B17582_07_035.png),
    are shown in the fifth column, assuming that our splitting criterion is ![](img/B17582_07_036.png).
    The last column of the table then shows the updated weights based on the update
    rules that we defined in the pseudo code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the computation of the weight updates may look a little bit complicated
    at first, we will now follow the calculation step by step. We will start by computing
    the weighted error rate, ![](img/B17582_07_004.png) (`epsilon`), as described
    in *step 2c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note that `correct` is a Boolean array consisting of `True` and `False` values
    where `True` indicates that a prediction is correct. Via `~correct`, we invert
    the array such that `np.mean(~correct)` computes the proportion of incorrect predictions
    (`True` counts as the value 1 and `False` as 0), that is, the classification error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will compute the coefficient, ![](img/B17582_07_038.png)—shown in
    *step 2d*—which will later be used in *step 2e* to update the weights, as well
    as for the weights in the majority vote prediction (*step 3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have computed the coefficient, ![](img/B17582_07_039.png) (`alpha_j`),
    we can now update the weight vector using the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B17582_07_041.png) is an element-wise multiplication between
    the vectors of the predicted and true class labels, respectively. Thus, if a prediction,
    ![](img/B17582_07_042.png), is correct, ![](img/B17582_07_043.png) will have a
    positive sign so that we decrease the *i*th weight, since ![](img/B17582_07_039.png)
    is a positive number as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will increase the *i*th weight if ![](img/B17582_07_042.png)
    predicted the label incorrectly, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, it’s like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use these values to update the weights as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The code above assigned the `update_if_correct` value to all correct predictions
    and the `update_if_wrong_1` value to all wrong predictions. We omitted using `update_if_wrong_2`
    for simplicity, since it is similar to `update_if_wrong_1` anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have updated each weight in the weight vector, we normalize the weights
    so that they sum up to 1 (*step 2f*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In code, we can accomplish that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Thus, each weight that corresponds to a correctly classified example will be
    reduced from the initial value of 0.1 to 0.0714 for the next round of boosting.
    Similarly, the weights of the incorrectly classified examples will increase from
    0.1 to 0.1667.
  prefs: []
  type: TYPE_NORMAL
- en: Applying AdaBoost using scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous subsection introduced AdaBoost in a nutshell. Skipping to the more
    practical part, let’s now train an AdaBoost ensemble classifier via scikit-learn.
    We will use the same Wine subset that we used in the previous section to train
    the bagging meta-classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Via the `base_estimator` attribute, we will train the `AdaBoostClassifier`
    on 500 decision tree stumps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the decision tree stump seems to underfit the training data
    in contrast to the unpruned decision tree that we saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, you can see that the AdaBoost model predicts all class labels of the training
    dataset correctly and also shows a slightly improved test dataset performance
    compared to the decision tree stump. However, you can also see that we introduced
    additional variance with our attempt to reduce the model bias—a greater gap between
    training and test performance.
  prefs: []
  type: TYPE_NORMAL
- en: Although we used another simple example for demonstration purposes, we can see
    that the performance of the AdaBoost classifier is slightly improved compared
    to the decision stump and achieved very similar accuracy scores as the bagging
    classifier that we trained in the previous section. However, we must note that
    it is considered bad practice to select a model based on the repeated usage of
    the test dataset. The estimate of the generalization performance may be overoptimistic,
    which we discussed in more detail in *Chapter 6*, *Learning Best Practices for
    Model Evaluation and Hyperparameter Tuning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let’s check what the decision regions look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'By looking at the decision regions, you can see that the decision boundary
    of the AdaBoost model is substantially more complex than the decision boundary
    of the decision stump. In addition, note that the AdaBoost model separates the
    feature space very similarly to the bagging classifier that we trained in the
    previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: The decision boundaries of the decision tree versus AdaBoost'
  prefs: []
  type: TYPE_NORMAL
- en: As concluding remarks about ensemble techniques, it is worth noting that ensemble
    learning increases the computational complexity compared to individual classifiers.
    In practice, we need to think carefully about whether we want to pay the price
    of increased computational costs for an often relatively modest improvement in
    predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'An often-cited example of this tradeoff is the famous $1 million *Netflix Prize*,
    which was won using ensemble techniques. The details about the algorithm were
    published in *The BigChaos Solution to the Netflix Grand Prize* by *A. Toescher*,
    *M. Jahrer*, and *R. M. Bell*, *Netflix Prize documentation*, 2009, which is available
    at [http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf](http://www.stat.osu.edu/~dmsl/GrandPrize2009_BPC_BigChaos.pdf).
    The winning team received the $1 million grand prize money; however, Netflix never
    implemented their model due to its complexity, which made it infeasible for a
    real-world application:'
  prefs: []
  type: TYPE_NORMAL
- en: “We evaluated some of the new methods offline but the additional accuracy gains
    that we measured did not seem to justify the engineering effort needed to bring
    them into a production environment.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html](http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gradient boosting – training an ensemble based on loss gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting is another variant of the boosting concept introduced in the
    previous section, that is, successively training weak learners to create a strong
    ensemble. Gradient boosting is an extremely important topic because it forms the
    basis of popular machine learning algorithms such as XGBoost, which is well-known
    for winning Kaggle competitions.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient boosting algorithm may appear a bit daunting at first. So, in the
    following subsections, we will cover it step by step, starting with a general
    overview. Then, we will see how gradient boosting is used for classification and
    walk through an example. Finally, after we’ve introduced the fundamental concepts
    of gradient boosting, we will take a brief look at popular implementations, such
    as XGBoost, and we will see how we can use gradient boosting in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing AdaBoost with gradient boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fundamentally, gradient boosting is very similar to AdaBoost, which we discussed
    previously in this chapter. AdaBoost trains decision tree stumps based on errors
    of the previous decision tree stump. In particular, the errors are used to compute
    sample weights in each round as well as for computing a classifier weight for
    each decision tree stump when combining the individual stumps into an ensemble.
    We stop training once a maximum number of iterations (decision tree stumps) is
    reached. Like AdaBoost, gradient boosting fits decision trees in an iterative
    fashion using prediction errors. However, gradient boosting trees are usually
    deeper than decision tree stumps and have typically a maximum depth of 3 to 6
    (or a maximum number of 8 to 64 leaf nodes). Also, in contrast to AdaBoost, gradient
    boosting does not use the prediction errors for assigning sample weights; they
    are used directly to form the target variable for fitting the next tree. Moreover,
    instead of having an individual weighting term for each tree, like in AdaBoost,
    gradient boosting uses a global learning rate that is the same for each tree.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, AdaBoost and gradient boosting share several similarities but
    differ in certain key aspects. In the following subsection, we will sketch the
    general outline of the gradient boosting algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Outlining the general gradient boosting algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at gradient boosting for classification. For
    simplicity, we will look at a binary classification example. Interested readers
    can find the generalization to the multi-class setting with logistic loss in *Section
    4.6\. Multiclass logistic regression and classification* of the original gradient
    boosting paper written by Friedman in 2001, *Greedy function approximation: A
    gradient boosting machine*, [https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boostingmachine/10.1214/aos/1013203451.full).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient boosting for regression**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the procedure behind gradient boosting is a bit more complicated
    than AdaBoost. We omit a simpler regression example, which was given in Friedman’s
    paper, for brevity, but interested readers are encouraged to also consider my
    complementary video tutorial on gradient boosting for regression, which is available
    at: [https://www.youtube.com/watch?v=zblsrxc7XpM](https://www.youtube.com/watch?v=zblsrxc7XpM).'
  prefs: []
  type: TYPE_NORMAL
- en: In essence, gradient boosting builds a series of trees, where each tree is fit
    on the error—the difference between the label and the predicted value—of the previous
    tree. In each round, the tree ensemble improves as we are nudging each tree more
    in the right direction via small updates. These updates are based on a loss gradient,
    which is how gradient boosting got its name.
  prefs: []
  type: TYPE_NORMAL
- en: The following steps will introduce the general algorithm behind gradient boosting.
    After illustrating the main steps, we will dive into some of its parts in more
    detail and walk through a hands-on example in the next subsections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize a model to return a constant prediction value. For this, we use
    a decision tree root node; that is, a decision tree with a single leaf node. We
    denote the value returned by the tree as ![](img/B17582_04_008.png), and we find
    this value by minimizing a differentiable loss function *L* that we will define
    later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_048.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Here, *n* refers to the *n* training examples in our dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For each tree *m* = 1, ..., *M*, where *M* is a user-specified total number
    of trees, we carry out the following computations outlined in *steps 2a* to *2d*
    below:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the difference between a predicted value ![](img/B17582_07_049.png)
    and the class label *y*[i]. This value is sometimes called the *pseudo-response*
    or *pseudo-residual*. More formally, we can write this pseudo-residual as the
    negative gradient of the loss function with respect to the predicted values:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_050.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Note that in the notation above *F*(*x*) is the prediction of the previous tree,
    *F*[m][–1](*x*). So, in the first round, this refers to the constant value from
    the tree (single leaf node) from step 1.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Fit a tree to the pseudo-residuals *r*[im]. We use the notation *R*[jm] to denote
    the *j* = 1 ... *J*[m] leaf nodes of the resulting tree in iteration *m*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each leaf node *R*[jm], we compute the following output value:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_051.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: In the next subsection, we will dive deeper into how this ![](img/B17582_07_052.png)
    is computed by minimizing the loss function. At this point, we can already note
    that leaf nodes *R*[jm] may contain more than one training example, hence the
    summation.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Update the model by adding the output values ![](img/B17582_07_053.png) to
    the previous tree:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_054.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: However, instead of adding the full predicted values of the current tree ![](img/B17582_07_053.png)
    to the previous tree ![](img/B17582_07_056.png), we scale ![](img/B17582_07_057.png)
    by a learning rate ![](img/B17582_02_036.png), which is typically a small value
    between 0.01 and 1\. In other words, we update the model incrementally by taking
    small steps, which helps avoid overfitting.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, after looking at the general structure of gradient boosting, we will adopt
    these mechanics to look at gradient boosting for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the gradient boosting algorithm for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this subsection, we will go over the details for implementing the gradient
    boosting algorithm for binary classification. In this context, we will be using
    the logistic loss function that we introduced for logistic regression in *Chapter
    3*, *A Tour of Machine Learning Classifiers Using Scikit-Learn*. For a single
    training example, we can specify the logistic loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In *Chapter 3*, we also introduced the log(odds):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For reasons that will make sense later, we will use these log(odds) to rewrite
    the logistic function as follows (omitting intermediate steps here):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_061.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can define the partial derivative of the loss function with respect
    to these log(odds), ![](img/B17582_07_062.png). The derivative of this loss function
    with respect to the log(odds) is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17582_07_063.png)'
  prefs: []
  type: TYPE_IMG
- en: After specifying these mathematical definitions, let us now revisit the general
    gradient boosting *steps 1* to *2d* from the previous section and reformulate
    them for this binary classification scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Create a root node that minimizes the logistic loss. It turns out that the loss
    is minimized if the root node returns the log(odds), ![](img/B17582_04_008.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each tree *m* = 1, ..., *M*, where *M* is a user-specified number of total
    trees, we carry out the following computations outlined in *steps 2a* to *2d*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We convert the log(odds) into a probability using the familiar logistic function
    that we used in logistic regression (in *Chapter 3*):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_065.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: 'Then, we compute the pseudo-residual, which is the negative partial derivative
    of the loss with respect to the log(odds), which turns out to be the difference
    between the class label and the predicted probability:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17582_07_066.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Fit a new tree to the pseudo-residuals.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each leaf node *R*[jm], compute a value ![](img/B17582_07_052.png) that
    minimizes the logistic loss function. This includes a summarization step for dealing
    with leaf nodes that contain multiple training examples:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_068.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: 'Skipping over intermediate mathematical details, this results in the following:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B17582_07_069.png)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_IMG
- en: Note that the summation here is only over the examples at the node corresponding
    to the leaf node *R*[jm] and not the complete training set.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Update the model by adding the gamma value from *step 2c* with learning rate
    ![](img/B17582_02_036.png):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17582_07_071.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**Outputting log(odds) vs probabilities**'
  prefs: []
  type: TYPE_NORMAL
- en: Why do the trees return log(odds) values and not probabilities? This is because
    we cannot just add up probability values and arrive at a meaningful result. (So,
    technically speaking, gradient boosting for classification uses regression trees.)
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we adopted the general gradient boosting algorithm and specified
    it for binary classification, for instance, by replacing the generic loss function
    with the logistic loss and the predicted values with the log(odds). However, many
    of the individual steps may still seem very abstract, and in the next section,
    we will apply these steps to a concrete example.
  prefs: []
  type: TYPE_NORMAL
- en: Illustrating gradient boosting for classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous two subsections went over the condensed mathematical details of
    the gradient boosting algorithm for binary classification. To make these concepts
    clearer, let’s apply it to a small toy example, that is, a training dataset of
    the following three examples shown in *Figure 7.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table Description automatically generated](img/B17582_07_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Toy dataset for explaining gradient boosting'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with *step 1*, constructing the root node and computing the log(odds),
    and *step 2a*, converting the log(odds) into class-membership probabilities and
    computing the pseudo-residuals. Note that based on what we have learned in *Chapter
    3*, the odds can be computed as the number of successes divided by the number
    of failures. Here, we regard label 1 as success and label 0 as failure, so the
    odds are computed as: odds = 2/1\. Carrying out steps *1* and *2a*, we get the
    following results shown in *Figure 7.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table Description automatically generated](img/B17582_07_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Results from the first round of applying step 1 and step 2a'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in *step 2b*, we fit a new tree on the pseudo-residuals *r*. Then, in
    *step 2c*, we compute the output values, ![](img/B17582_03_056.png), for this
    tree as shown in *Figure 7.14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram Description automatically generated](img/B17582_07_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: An illustration of steps 2b and 2c, which fits a tree to the residuals
    and computes the output values for each leaf node'
  prefs: []
  type: TYPE_NORMAL
- en: (Note that we artificially limit the tree to have only two leaf nodes, which
    helps illustrate what happens if a leaf node contains more than one example.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the final *step 2d*, we update the previous model and the current
    model. Assuming a learning rate of ![](img/B17582_02_063.png), the resulting prediction
    for the first training example is shown in *Figure 7.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram Description automatically generated](img/B17582_07_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: The update of the previous model shown in the context of the first
    training example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have completed *steps 2a* to *2d* of the first round, *m* = 1,
    we can proceed to execute *steps 2a* to *2d* for the second round, *m* = 2\. In
    the second round, we use the log(odds) returned by the updated model, for example,
    ![](img/B17582_07_074.png), as input to *step* *2A*. The new values we obtain
    in the second round are shown in *Figure 7.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table Description automatically generated](img/B17582_07_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Values from the second round next to the values from the first
    round'
  prefs: []
  type: TYPE_NORMAL
- en: We can already see that the predicted probabilities are higher for the positive
    class and lower for the negative class. Consequently, the residuals are getting
    smaller, too. Note that the process of *steps 2a* to *2d* is repeated until we
    have fit *M* trees or the residuals are smaller than a user-specified threshold
    value. Then, once the gradient boosting algorithm has completed, we can use it
    to predict the class labels by thresholding the probability values of the final
    model, *F*[M](*x*) at 0.5, like logistic regression in *Chapter 3*. However, in
    contrast to logistic regression, gradient boosting consists of multiple trees
    and produces nonlinear decision boundaries. In the next section, we will look
    at how gradient boosting looks in action.
  prefs: []
  type: TYPE_NORMAL
- en: Using XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After covering the nitty-gritty details behind gradient boosting, let’s finally
    look at how we can use gradient boosting code implementations.
  prefs: []
  type: TYPE_NORMAL
- en: In scikit-learn, gradient boosting is implemented as `sklearn.ensemble.GradientBoostingClassifier`
    (see [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
    for more details). It is important to note that gradient boosting is a sequential
    process that can be slow to train. However, in recent years a more popular implementation
    of gradient boosting has emerged, namely, XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost proposed several tricks and approximations that speed up the training
    process substantially. Hence, the name XGBoost, which stands for extreme gradient
    boosting. Moreover, these approximations and tricks result in very good predictive
    performances. In fact, XGBoost gained popularity as it has been the winning solution
    for many Kaggle competitions.
  prefs: []
  type: TYPE_NORMAL
- en: Next to XGBoost, there are also other popular implementations of gradient boosting,
    for example, LightGBM and CatBoost. Inspired by LightGBM, scikit-learn now also
    implements a `HistGradientBoostingClassifier`, which is more performant than the
    original gradient boosting classifier (`GradientBoostingClassifier`).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more details about these methods via the resources below:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XGBoost`: [https://xgboost.readthedocs.io/en/stable/](https://xgboost.readthedocs.io/en/stable/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LightGBM`: [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CatBoost`: [https://catboost.ai](https://catboost.ai)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HistGradientBoostingClassifier`: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, since XGBoost is still among the most popular gradient boosting implementations,
    we will see how we can use it in practice. First, we need to install it, for example
    via `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '**Installing XGBoost**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, we used XGBoost version 1.5.0, which can be installed via:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: You can find more information about the installation details at [https://xgboost.readthedocs.io/en/stable/install.html](https://xgboost.readthedocs.io/en/stable/install.html)
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, XGBoost’s `XGBClassifier` follows the scikit-learn API. So, using
    it is relatively straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Here, we fit the gradient boosting classifier with 1,000 trees (rounds) and
    a learning rate of 0.01\. Typically, a learning rate between 0.01 and 0.1 is recommended.
    However, remember that the learning rate is used for scaling the predictions from
    the individual rounds. So, intuitively, the lower the learning rate, the more
    estimators are required to achieve accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the `max_depth` for the individual decision trees, which we set
    to 4\. Since we are still boosting weak learners, a value between 2 and 6 is reasonable,
    but larger values may also work well depending on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `use_label_encoder=False` disables a warning message which informs
    users that XGBoost is not converting labels by default anymore, and it expects
    users to provide labels in an integer format starting with label 0\. (There is
    nothing to worry about here, since we have been following this format throughout
    this book.)
  prefs: []
  type: TYPE_NORMAL
- en: There are many more settings available, and a detailed discussion is out of
    the scope of this book. However, interested readers can find more details in the
    original documentation at [https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at some of the most popular and widely used techniques
    for ensemble learning. Ensemble methods combine different classification models
    to cancel out their individual weaknesses, which often results in stable and well-performing
    models that are very attractive for industrial applications as well as machine
    learning competitions.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we implemented `MajorityVoteClassifier` in
    Python, which allows us to combine different algorithms for classification. We
    then looked at bagging, a useful technique for reducing the variance of a model
    by drawing random bootstrap samples from the training dataset and combining the
    individually trained classifiers via majority vote. Lastly, we learned about boosting
    in the form of AdaBoost and gradient boosting, which are algorithms based on training
    weak learners that subsequently learn from mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the previous chapters, we learned a lot about different learning
    algorithms, tuning, and evaluation techniques. In the next chapter, we will look
    at a particular application of machine learning, sentiment analysis, which has
    become an interesting topic in the internet and social media era.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code874410888448293359.png)'
  prefs: []
  type: TYPE_IMG
