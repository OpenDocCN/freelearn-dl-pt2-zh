- en: Diving Deep into Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the different modules of deep learning architectures
    that are used to solve real-world problems. In the previous chapter, we used low-level
    operations of PyTorch to build modules such as a network architecture, a loss
    function, and an optimizer. In this chapter, we will explore some of the important
    components of neural networks required to solve real-world problems, along with
    how PyTorch abstracts away a lot of complexity by providing a lot of high-level
    functions. Towards the end of the chapter, we will build algorithms that solve
    real-world problems such as regression, binary classification, and multi-class
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will go through following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive into the various building blocks of neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring higher-level functionalities in PyTorch to build deep learning architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying deep learning to a real-world image classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep dive into the building blocks of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we learned in the previous chapter, training a deep learning algorithm requires
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a data pipeline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Building a network architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the architecture using a loss function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optimizing the network architecture weights using an optimization algorithm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the previous chapter, the network was composed of a simple linear model
    built using PyTorch numerical operations. Though building a neural architecture
    for a toy problem using numerical operations is easier, it quickly becomes complicated
    when we try to build architectures required to solve complex problems in different
    areas, such as computer vision and **natural language processing** (**NLP**).
    Most of the deep learning frameworks, such as PyTorch, TensorFlow, and Apache
    MXNet, provide higher-level functionalities that abstract a lot of this complexity.
    These higher-level functionalities are called **layers** across the deep learning
    frameworks. They accept input data, apply transformations like the ones we have
    seen in the previous chapter, and output the data. To solve real-world problems,
    deep learning architectures constitute of a number of layers ranging from 1 to
    150, or sometimes more than that. Abstracting the low-level operations and training
    deep learning algorithms would look like the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14187b0e-8fa2-415e-a890-6695d3bcae31.png)'
  prefs: []
  type: TYPE_IMG
- en: Summarizing the previous diagram, any deep learning training involves getting
    data, building an architecture that in general is getting a bunch of layers together,
    evaluating the accuracy of the model using a loss function, and then optimizing
    the algorithm by optimizing the weights of our network. Before looking at solving
    some of the real-world problems, we will come to understand higher-level abstractions
    provided by PyTorch for building layers, loss functions, and optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Layers – fundamental blocks of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout the rest of the chapter, we will come across different types of
    layers. To begin, let''s try to understand one of the most important layers, the
    linear layer, which does exactly what our previous network architecture does.
    The linear layer applies a linear transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6333f93-736c-4658-957a-180b30f69600.png)'
  prefs: []
  type: TYPE_IMG
- en: 'What makes it powerful is that fact that the entire function that we wrote
    in the previous chapter can be written in a single line of code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `myLayer` in the preceding code will accept a tensor of size `10` and outputs
    a tensor of size `5` after applying linear transformation. Let''s look at a simple
    example of how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access the trainable parameters of the layer using the `weights` and
    `bias` attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Linear layers are called by different names, such as **dense** or **fully connected
    layers** across different frameworks. Deep learning architectures used for solving
    real-world use cases generally contain more than one layer. In PyTorch, we can
    do it in multiple ways, shown as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple approach is passing the output of one layer to another layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each layer will have its own learnable parameters. The idea behind using multiple
    layers is that each layer will learn some kind of pattern that the later layers
    will build on. There is a problem in adding just linear layers together, as they
    fail to learn anything new beyond a simple representation of a linear layer. Let's
    see through a simple example of why it does not make sense to stack multiple linear
    layers together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we have two linear layers with the following weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Layers** | **Weight1** |'
  prefs: []
  type: TYPE_TB
- en: '| Layer1 | 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Layer2 | 2.0 |'
  prefs: []
  type: TYPE_TB
- en: 'The preceding architecture with two different layers can be simply represented
    as a single layer with a different layer. Hence, just stacking multiple linear
    layers will not help our algorithms to learn anything new. Sometimes, this can
    be unclear, so we can visualize the architecture with the following mathematical
    formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4556ddc4-d0e1-4e0e-a153-c2ec2c779701.png)![](img/7a28a3ad-2d64-4937-9309-5c062885025f.png)'
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we have different non-linearity functions that help in
    learning different relationships, rather than only focusing on linear relationships.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different non-linear functions available in deep learning. PyTorch
    provides these non-linear functionalities as layers and we will be able to use
    them the same way we used the linear layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the popular non-linear functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-linear activations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Non-linear activations are functions that take inputs and then apply a mathematical
    transformation and produce an output. There are several non-linear operations
    that we come across in practice. We will go through some of the popular non-linear
    activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The sigmoid activation function has a simple mathematical form, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aabbaeb0-9f60-44df-940e-9b3874c3ece2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sigmoid function intuitively takes a real-valued number and outputs a number
    in a range between zero and one. For a large negative number, it returns close
    to zero and, for a large positive number, it returns close to one. The following
    plot represents different sigmoid function outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/beb8c9f1-c75f-4e24-b654-5536c74ca248.png)'
  prefs: []
  type: TYPE_IMG
- en: The sigmoid function has been historically used across different architectures,
    but in recent times it has gone out of popularity as it has one major drawback.
    When the output of the sigmoid function is close to zero or one, the gradients
    for the layers before the sigmoid function are close to zero and, hence, the learnable
    parameters of the previous layer get gradients close to zero and the weights do
    not get adjusted often, resulting in dead neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Tanh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The tanh non-linearity function squashes a real-valued number in the range
    of -1 and 1\. The tanh also faces the same issue of saturating gradients when
    tanh outputs extreme values close to -1 and 1\. However, it is preferred to sigmoid,
    as the output of tanh is zero centered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eec87fd4-fe4f-408b-b831-4157206a684f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: http://datareview.info/article/eto-nuzhno-znat-klyuchevyie-rekomendatsii-po-glubokomu-obucheniyu-chast-2/'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ReLU has become more popular in the recent years; we can find either its usage
    or one of its variants'' usages in almost any modern architecture. It has a simple
    mathematical formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x)=max(0,x)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In simple words, ReLU squashes any input that is negative to zero and leaves
    positive numbers as they are. We can visualize the ReLU function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd8dabd2-67a6-41f6-85bd-99575eadf301.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: http://datareview.info/article/eto-nuzhno-znat-klyuchevyie-rekomendatsii-po-glubokomu-obucheniyu-chast-2/'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the pros and cons of using ReLU are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It helps the optimizer in finding the right set of weights sooner. More technically
    it makes the convergence of stochastic gradient descent faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is computationally inexpensive, as we are just thresholding and not calculating
    anything like we did for the sigmoid and tangent functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU has one disadvantage; when a large gradient passes through it during the
    backward propagation, they often become non-responsive; these are called **dead
    neutrons**, which can be controlled by carefully choosing the learning rate. We
    will discuss how to choose learning rates when we discuss the different ways to
    adjust the learning rate in [Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml),
    *Fundamentals of Machine Learning*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leaky ReLU is an attempt to solve a dying problem where, instead of saturating
    to zero, we saturate to a very small number such as 0.001\. For some use cases,
    this activation function provides a superior performance to others, but it is
    not consistent.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch non-linear activations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PyTorch has most of the common non-linear activation functions implemented
    for us already and it can be used like any other layer. Let''s see a quick example
    of how to use the `ReLU` function in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, we take a tensor with two positive values and two
    negative values and apply a `ReLU` on it, which thresholds the negative numbers
    to `0` and retains the positive numbers as they are.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have covered most of the details required for building a network architecture,
    let's build a deep learning architecture that can be used to solve real-world
    problems. In the previous chapter, we used a simple approach so that we could
    focus only on how a deep learning algorithm works. We will not be using that style
    to build our architecture anymore; rather, we will be building the architecture
    in the way it is supposed to be built in PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch way of building deep learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the networks in PyTorch are implemented as classes, subclassing a PyTorch
    class called `nn.Module`, and should implement `__init__` and `forward` methods.
    Inside the `init` function, we initialize any layers, such as the `linear` layer,
    which we covered in the previous section. In the `forward` method, we pass our
    input data into the layers that we initialized in our `init` method and return
    our final output. The non-linear functions are often directly used in the `forward`
    function and some use it in the `init` method too. The following code snippet
    shows how a deep learning architecture is implemented in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you are new to Python, some of the preceding code could be difficult to understand,
    but all it is doing is inheriting a parent class and implementing two methods
    in it. In Python, we subclass by passing the parent class as an argument to the
    class name. The `init` method acts as a constructor in Python and `super` is used
    to pass on arguments of the child class to the parent class, which in our case
    is `nn.Module`.
  prefs: []
  type: TYPE_NORMAL
- en: Model architecture for different machine learning problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The kind of problem we are solving will decide mostly what layers we will use,
    starting from a linear layer to **Long Short-Term Memory** (**LSTM**) for sequential
    data. Based on the type of the problem you are trying to solve, your last layer
    is determined. There are three problems that we generally solve using any machine
    learning or deep learning algorithms. Let''s look at what the last layer would
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: For a regression problem, such as predicting the price of a t-shirt to sell,
    we would use the last layer as a linear layer with an output of one, which outputs
    a continuous value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For classifying a given image as t-shirt or shirt, you would use a sigmoid activation
    function, as it outputs values either closer to one or zero, which is generally
    called a **binary classification problem**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a multi-class classification, where we have to classify whether a given
    image is a t-shirt, jeans, shirt, or dress, we would use a softmax layer at the
    end our network. Let's try to understand intuitively what softmax does without
    going into the math of it. It takes inputs from the previous linear layer, for
    example, and outputs the probabilities for a given number of examples. In our
    example, it would be trained to predict four probabilities for each type of image.
    Remember, all these probabilities always add up to one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have defined our network architecture, we are left with two important
    steps. One is calculating how good our network is at performing a particular task
    of regression, classification, and the next is optimizing the weight.
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer (gradient descent) generally accepts a scalar value, so our `loss`
    function should generate a scalar value that has to be minimized during our training.
    Certain use cases, such as predicting where an obstacle is on the road and classifying
    it to a pedestrian or not, would require two or more loss functions. Even in such
    scenarios, we need to combine the losses to a single scalar for the optimizer
    to minimize. We will discuss examples of combining multiple losses to a single
    scalar in detail with a real-world example in the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we defined our own `loss` function. PyTorch provides
    several implementations of commonly used `loss` functions. Let's take a look at
    the `loss` functions used for regression and classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The commonly used `loss` function for regression problems is **mean square
    error** (**MSE**). It is the same `loss` function we implemented in our previous
    chapter. We can use the `loss` function implemented in PyTorch, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For classification, we use a cross-entropy loss. Before looking at the math
    for cross-entropy, let''s understand what a cross-entropy loss does. It calculates
    the loss of a classification network predicting the probabilities, which should
    sum up to one, like our softmax layer. A cross-entropy loss increases when the
    predicted probability diverges from the correct probability. For example, if our
    classification algorithm predicts 0.1 probability for the following image to be
    a cat, but it is actually a panda, then the cross-entropy loss will be higher.
    If it predicts similar to the actual labels, then the cross-entropy loss will
    be lower:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f8abb0b-31dc-4e78-a06a-1a29631d7706.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look at a sample implementation of how this actually happens in Python
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To use a cross-entropy loss in a classification problem, we really do not need
    to be worried about what happens inside—all we have to remember is that, the loss
    will be high when our predictions are bad and low when predictions are good. PyTorch
    provides us with an implementation of the `loss`, which we can use, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the other `loss` functions that come as part of PyTorch are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| L1 loss | Mostly used as a regularizer. We will discuss it further in [Chapter
    4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals of Machine Learning*.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSE loss | Used as loss function for regression problems. |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-entropy loss | Used for binary and multi-class classification problems.
    |'
  prefs: []
  type: TYPE_TB
- en: '| NLL Loss | Used for classification problems and allows us to use specific
    weights to handle imbalanced datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| NLL Loss2d | Used for pixel-wise classification, mostly for problems related
    to image segmentation. |'
  prefs: []
  type: TYPE_TB
- en: Optimizing network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have calculated the loss of our network, we will optimize the weights
    to reduce the loss and thus improving the accuracy of the algorithm. For the sake
    of simplicity, let''s see these optimizers as black boxes that take loss functions
    and all the learnable parameters and move them slightly to improve our performances.
    PyTorch provides most of the commonly used optimizers required in deep learning.
    If you want to explore what happens inside these optimizers and have a mathematical
    background, I would strongly recommend some of the following blogs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://ruder.io/deep-learning-optimization-2017/](http://ruder.io/deep-learning-optimization-2017/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of the optimizers that PyTorch provides are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: ADADELTA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adagrad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparseAdam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adamax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ASGD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LBFGS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSProp
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rprop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SGD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will get into the details of some of the algorithms in[ Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals
    of Machine Learning*, along with some of the advantages and tradeoffs. Let''s
    walk through some of the important steps in creating any `optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we created an `SGD` optimizer that takes all the
    learnable parameters of your network as the first argument and a learning rate
    that determines what ratio of change can be made to the learnable parameter. In
    [Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals of Machine
    Learning* we will get into more details of learning rates and momentum, which
    is an important parameter of optimizers. Once you create an optimizer object,
    we need to call `zero_grad()` inside our loop, as the parameters will accumulate
    the gradients created during the previous `optimizer` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Once we call `backward` on the `loss` function, which calculates the gradients
    (quantity by which learnable parameters need to change), we call `optimizer.step()`,
    which makes the actual changes to our learnable parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have covered most of the components required to help a computer see/
    recognize images. Let's build a complex deep learning model that can differentiate
    between dogs and cats to put all the theory into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification using deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important step in solving any real-world problem is to get the data.
    Kaggle provides a huge number of competitions on different data science problems.
    We will pick one of the problems that arose in 2014, which we will use to test
    our deep learning algorithms in this chapter and improve it in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep
    Learning for Computer Vision*, which will be on **Convolution Neural Networks**
    (**CNNs**) and some of the advanced techniques that we can use to improve the
    performance of our image recognition models. You can download the data from [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data).
    The dataset contains 25,000 images of dogs and cats. Preprocessing of data and
    the creation of train, validation, and test splits are some of the important steps
    that need to be performed before we can implement an algorithm. Once the data
    is downloaded, taking a look at it, it shows that the folder contains images in
    the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c746e70e-bf9a-40be-8f68-7493de1eb7ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Most of the frameworks make it easier to read the images and tag them to their
    labels when provided in the following format. That means that each class should
    have a separate folder of its images. Here, all cat images should be in the `cat`
    folder and dog images in the `dog` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf800adb-ed3e-4f0e-b644-de91d3789ce5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Python makes it easy to put the data into the right format. Let''s quickly
    take a look at the code and, then, we will go through the important parts of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: All the preceding code does is retrieve all the files and pick 2,000 images
    for creating a validation set. It segregates all the images into the two categories
    of cats and dogs. It is a common and important practice to create a separate validation
    set, as it is not fair to test our algorithms on the same data it is trained on.
    To create a `validation` dataset, we create a list of numbers that are in the range
    of the length of the images in a shuffled order. The shuffled numbers act as an
    index for us to pick a bunch of images for creating our `validation` dataset.
    Let's go through each section of the code in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a file using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `glob` method returns all the files in the particular path. When there are
    a huge number of images, we can also use `iglob`, which returns an iterator, instead
    of loading the names into memory. In our case, we have only 25,000 filenames,
    which can easily fit into memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can shuffle our files using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code returns 25,000 numbers in the range from zero to 25,000 in
    a shuffled order, which we will use as an index for selecting a subset of images
    to create a `validation` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a validation code, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code creates a `validation` folder and creates folders based on
    categories (cats and dogs) inside `train` and `valid` directories.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can shuffle an index with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we use our shuffled index to randomly pick `2000` different
    images for our validation set. We do something similar for the training data to
    segregate the images in the `train` directory.
  prefs: []
  type: TYPE_NORMAL
- en: As we have the data in the format we need, let's quickly look at how to load
    the images as PyTorch tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data into PyTorch tensors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The PyTorch `torchvision.datasets` package provides a utility class called
    `ImageFolder` that can be used to load images along with their associated labels
    when data is presented in the aforementioned format. It is a common practice to
    perform the following preprocessing steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Resize all the images to the same size. Most of the deep learning architectures
    expect the images to be of the same size.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the dataset with the mean and standard deviation of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the image dataset to a PyTorch tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PyTorch makes a lot of these preprocessing steps easier by providing a lot
    of utility functions in the `transforms` module. For our example, let''s apply
    three transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Scale to a 256 x 256 image size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert to a PyTorch tensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalize the data (we will talk about how we arrived at the mean and standard
    deviation in[ Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml), *Deep Learning
    for Computer Vision*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code demonstrates how transformation can be applied and images
    are loaded using the `ImageFolder` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train` object holds all the images and associated labels for the dataset.
    It contains two important attributes: one that gives a mapping between classes
    and the associated index used in the dataset and another one that gives a list
    of classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train.class_to_idx - {''cat'': 0, ''dog'': 1}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train.classes - [''cat'', ''dog'']`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is often a best practice to visualize the data loaded into tensors. To visualize
    the tensors, we have to reshape the tensors and denormalize the values. The following
    function does that for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can pass our tensor to the preceding `imshow` function, which converts
    it into an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6fc5e3ac-369e-4a9f-bcbc-70395c8ed587.png)'
  prefs: []
  type: TYPE_IMG
- en: Loading PyTorch tensors as batches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is a common practice in deep learning or machine learning to batch samples
    of images, as modern **graphics processing units** (**GPUs**) and CPUs are optimized
    to run operations faster on a batch of images. The batch size generally varies
    depending on the kind of GPU we use. Each GPU has its own memory, which can vary
    from 2 GB to 12 GB, and sometimes more for commercial GPUs. PyTorch provides the
    `DataLoader` class, which takes in a dataset and returns us a batch of images.
    It abstracts a lot of complexities in batching, such as the usage of multi-workers
    for applying transformation. The following code converts the previous `train`
    and `valid` datasets into data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `DataLoader` class provides us with a lot of options and some of the most
    commonly used ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`shuffle`: When true, this shuffles the images every time the data loader is
    called.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_workers`: This is responsible for parallelization. It is common practice
    to use a number of workers fewer than the number of cores available in your machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the network architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For most of the real-world use cases, particularly in computer vision, we rarely
    build our own architecture. There are different architectures that can be quickly
    used to solve our real-world problems. For our example, we use a popular deep
    learning algorithm called **ResNet**, which won the first prize in 2015 in different
    competitions, such as ImageNet, related to computer vision. For a simpler understanding,
    let''s assume that this algorithm is a bunch of different PyTorch layers carefully
    tied together and not focus on what happens inside this algorithm. We will see
    some of the key building blocks of the ResNet algorithm in [Chapter 5](3cce1bbe-6d1c-4164-b1ef-8b0688126519.xhtml),
    *Deep Learning for Computer Vision*, when we learn about CNNs. PyTorch makes it
    easier to use a lot of these popular algorithms by providing them off the shelf
    in the `torchvision.models` module. So, for this example, let''s quickly take
    a look at how to use this algorithm and then walk through each line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `models.resnet18(pertrained = True)` object creates an instance of the
    algorithm, which is a collection of PyTorch layers. We can take a quick look at
    what constitutes the ResNet algorithm by printing `model_ft`. A small portion
    of the algorithm looks like the following screenshot. I am not including the full
    algorithm as it could run for several pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09088b50-3dab-4b63-996f-498a5c52bfe2.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the ResNet architecture is a collection of layers, namely `Conv2d`,
    `BatchNorm2d`, and `MaxPool2d`, stitched in a particular way. All these algorithms
    will accept an argument called **pretrained**. When `pretrained` is `True`, the
    weights of the algorithm are already tuned for a particular ImageNet classification
    problem of predicting 1,000 different categories, which include cars, ships, fish,
    cats, and dogs. This algorithm is trained to predict the 1,000 ImageNet categories
    and the weights are adjusted to a certain point where the algorithm achieves state-of-art
    accuracy. These weights are stored and shared with the model that we are using
    for the use case. Algorithms tend to work better when started with fine-tuned
    weights, rather than when started with random weights. So, for our use case, we
    start with pretrained weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ResNet algorithm cannot be used directly, as it is trained to predict one
    of the 1,000 categories. For our use case, we need to predict only one of the
    two categories of dogs and cats. To achieve this, we take the last layer of the
    ResNet model, which is a `linear` layer and change the output features to two,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are running this algorithm on a GPU-based machine, then to make the
    algorithm run on a GPU we call the `cuda` method on the model. It is strongly
    recommended that you run these programs on a GPU-powered machine; it is easy to
    spin a cloud instance with a GPU for less than a dollar. The last line in the
    following code snippet tells PyTorch to run the code on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we have created `DataLoader` instances and algorithms.
    Now, let''s train the model. To do this we need a `loss` function and an `optimizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we created our `loss` function based on `CrossEntropyLoss`
    and the optimizer based on `SGD`. The `StepLR` function helps in dynamically changing
    the learning rate. We will discuss different strategies available to tune the
    learning rate in [Chapter 4](821004b7-d246-4aac-b883-9ab634ae0aea.xhtml), *Fundamentals
    of Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `train_model` function takes in a model and tunes the weights
    of our algorithm by running multiple epochs and reducing the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Passes the images through the model and calculates the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagates during the training phase. For the validation/testing phase,
    it does not adjust the weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The loss is accumulated across batches for each epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best model is stored and validation accuracy is printed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The preceding model, after running for `25` epochs, results in a validation
    accuracy of 87%. The following is the log generated by the preceding `train_model`
    function when run on our `Dogs vs. Cats` dataset; I am just including the result
    of the last few epochs to save space in the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the coming chapters, we will learn more advanced techniques that will help
    us in training more accurate models in a much faster way. The preceding model
    took around 30 minutes to run on a Titan X GPU. We will cover different techniques
    that will help in training the model faster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the complete life cycle of a neural network in
    Pytorch, starting from constituting different types of layers, adding activations,
    calculating cross-entropy loss, and finally optimizing network performance (that
    is, minimizing loss), by adjusting the weights of layers using the SGD optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: We have studied how to apply the popular ResNET architecture to binary or multi-class
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: While doing this, we have tried to solve the real-world image classification
    problem of classifying a cat image as a cat and a dog image as a dog. This knowledge
    can be applied to classify different categories/classes of entities, such as classifying
    species of fish, identifying different kinds of dogs, categorizing plant seedlings,
    grouping together cervical cancer into Type 1, Type 2, and Type 3, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go through the fundamentals of machine learning.
  prefs: []
  type: TYPE_NORMAL
