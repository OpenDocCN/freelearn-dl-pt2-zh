- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining Computer Vision and Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to generate images of interest. In this
    chapter, we will learn how to combine reinforcement learning-based techniques
    (primarily, deep Q-learning) with computer vision-based techniques. This is especially
    useful in scenarios where the learning environment is complex and we cannot gather
    data for all the cases. In such scenarios, we want the model to learn by itself
    in a simulated environment that resembles reality as closely as possible. Such
    models come in handy when used for self-driving cars, robotics, bots in games
    (real as well as digital), and the field of self-supervised learning, in general.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by learning about the basics of reinforcement learning, and then
    about the terminology associated with identifying how to calculate the value (**Q-value**)
    associated with taking an action in a given state. Then, we will learn about filling
    a **Q-table**, which helps to identify the value associated with various actions
    in a given state. We will also learn about identifying the Q-values of various
    actions in scenarios where coming up with a Q-table is infeasible, due to a high
    number of possible states; we’ll do this using a **Deep Q-Network** (**DQN**).
    This is where we will understand how to leverage neural networks in combination
    with reinforcement learning. Then, we will learn about scenarios where the DQN
    model itself does not work, addressing this by using the DQN alongside the **fixed
    targets model**. Here, we will play a video game known as Pong by leveraging CNN
    in conjunction with reinforcement learning. Finally, we will leverage what we’ve
    learned to build an agent that can drive a car autonomously in a simulated environment
    – CARLA.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, in this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the basics of reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing deep Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing deep Q-learning with fixed targets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing an agent to perform autonomous driving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code snippets within this chapter are available in the `Chapter14` folder
    of the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: As the field evolves, we will periodically add valuable supplements to the GitHub
    repository. Do check the `supplementary_sections` folder within each chapter’s
    directory for new and useful content.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the basics of reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is an area of machine learning concerned
    with how software **agents** ought to take **actions** in a given **state** of
    an **environment,** maximizing the notion of cumulative **reward**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how RL helps, let’s consider a simple scenario. Imagine that
    you are playing chess against a computer. Let’s identify the different components
    involved:'
  prefs: []
  type: TYPE_NORMAL
- en: The computer is an **agent** that has learned/is learning how to play chess.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The setup (rules) of the game constitutes the **environment.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we make a move (take an **action**), the **state** of the board (the location
    of various pieces on the chessboard) changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the game, depending on the result, the agent gets a **reward**.
    The objective of the agent is to maximize the reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the machine (*agent1*) is playing against a human, the number of games that
    it can play is finite (depending on the number of games the human can play). This
    might create a bottleneck for the agent to learn well. However, what if `agent1`
    (the agent that is learning the game) can play against *agent2* (`agent2` could
    be another agent that is learning chess, or it could be a piece of chess software
    that has been pre-programmed to play the game well)? Theoretically, the agents
    can play infinite games with each other, which results in maximizing the opportunity
    to learn to play the game well. This way, by playing multiple games, the learning
    agent is likely to learn how to address the different scenarios/states of the
    game well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand the process that the learning agent will follow to learn well:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, the agent takes a random action in a given state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The agent stores the action it has taken in various states within a game in
    **memory**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the agent associates the result of the action in various states with a
    **reward**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After playing multiple games, the agent can correlate the action in a state
    to a potential reward by replaying its **experiences**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next comes the question of quantifying the **value** that corresponds to taking
    an action in a given state. We’ll learn how to calculate this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the state value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand how to quantify the value of a state, let’s use a simple scenario
    where we will define the environment and objective, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing shoji  Description automatically generated](img/B18457_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: Environment'
  prefs: []
  type: TYPE_NORMAL
- en: The environment is a grid with two rows and three columns. The agent starts
    at the **Start** cell, and it achieves its objective (is rewarded with a score
    of +1) if it reaches the bottom-right grid cell. The agent does not get a reward
    if it goes to any other cell. The agent can take an action by going to the right,
    left, bottom, or up, depending on the feasibility of the action (the agent can
    go to the right or the bottom of the start grid cell, for example). The reward
    of reaching any of the remaining cells other than the bottom-right cell is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using this information, let’s calculate the **value** of a cell (the state
    that the agent is in, in a given snapshot). Given that some energy is spent moving
    from one cell to another, we discount the value of reaching a cell by a discount
    factor of γ, where γ takes care of the energy that’s spent in moving from one
    cell to another. Furthermore, the introduction of γ results in the agent learning
    to play well sooner. With this, let’s formalize the widely used Bellman equation,
    which helps to calculate the value of a cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the preceding equation in place, let’s calculate the values of all cells
    (**once the optimal actions in a state have been identified**), with the value
    of γ being 0.9 (the typical value of γ is between 0.9 and 0.99):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the preceding calculations, we can understand how to calculate the values
    in a given state (cell), when given the optimal actions in that state. These values
    are as follows for our simplistic scenario of reaching the terminal state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing shape  Description automatically generated](img/B18457_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Value of each cell'
  prefs: []
  type: TYPE_NORMAL
- en: With the values in place, we expect the agent to follow a path of increasing
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to calculate the state value, in the next section,
    we will understand how to calculate the value associated with a state-action combination.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating the state-action value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we provided a scenario where we already know that the
    agent is taking optimal actions (which is not realistic). In this section, we
    will look at a scenario where we can identify the value that corresponds to a
    state-action combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, each sub-cell within a cell represents the value of
    taking an action in the cell. Initially, the cell values for various actions are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Polygon  Description automatically generated](img/B18457_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: Initial values of different actions in a given state'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the preceding image, cell *b1* (the 1^(st) row and the 2^(nd)
    column) will have a value of 1 if the agent moves right from the cell (as it corresponds
    to the terminal cell); the other actions result in a value of 0\. X indicates
    that the action is not possible, and hence no value is associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over four iterations (steps), the updated cell values for the actions in the
    given state are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Updated cell values after four iterations'
  prefs: []
  type: TYPE_NORMAL
- en: This would then go through multiple iterations to provide the optimal action
    that maximizes value at each cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand how to obtain the cell values in the second table (*Iteration
    2* in the preceding image). Let’s narrow this down to 0.3, which was obtained
    by taking the downward action when present in the 1^(st) row and the 2^(nd) column
    of the second table. When the agent takes the downward action, there is a 1/3
    chance of it taking the optimal action in the next state. Hence, the value of
    taking a downward action is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarly, we can obtain the values of taking different possible actions in
    different cells.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how the values of various actions in a given state are calculated,
    in the next section, we will learn about Q-learning and how we can leverage it,
    along with the Gym environment, so that it can play various games.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Technically, now that we have calculated the various state-action values we
    need, we can identify the action that will be taken in every state. However, in
    the case of a more complex scenario – for example, when playing video games –
    it gets tricky to fetch state information. OpenAI’s **Gym** environment comes
    in handy in this scenario. It contains a pre-defined environment for the game
    we’re playing. Here, it fetches the next state information, given an action that’s
    been taken in the current state. So far, we have considered the scenario of choosing
    the most optimal path. However, there can be scenarios where we are stuck at the
    local minima.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn about Q-learning, which helps to calculate the
    value associated with the action in a state, as well as about leveraging the Gym
    environment so that we can play various games. For now, we’ll take a look at a
    simple game called Frozen Lake that is available within the Gym environment. We’ll
    also take a look at exploration-exploitation, which helps us avoid getting stuck
    at the local minima. However, before we do that, we will learn about the Q-value.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Q-value
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Q in Q-learning or Q-value represents the quality (value) of an action.
    Let’s recap how to calculate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We already know that we must keep **updating** the state-action value of a
    given state until it is saturated. Hence, we’ll modify the preceding formula like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, we replace 1 with the learning rate so that we can
    update the value of the action that’s taken in a state more gradually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_006.png)'
  prefs: []
  type: TYPE_IMG
- en: With this formal definition of Q-value in place, in the next section, we’ll
    learn about the Gym environment and how it helps us fetch the Q-table (which stores
    information about the values of various actions that have been taken at various
    states) and, thus, come up with the optimal actions in a state.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Gym environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will explore the Gym environment and the various functionalities
    present in it while playing the Frozen Lake game:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available as `Understanding_the_Gym_environment.ipynb`
    in the `Chapter14` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the various environments present in the Gym environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code prints a dictionary containing all the games available within
    Gym.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an environment for the chosen game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the created environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Environment state'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, the agent starts at **the top left**. There are four
    holes amid the frozen lake. The agent gets a reward of 0 if they fall in the hole
    and the game is terminated. The objective of the game is for the agent to reach
    **the goal (bottom right)** by taking certain actions (mentioned in step 6).
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the size of the observation space (the number of states) in the game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code gives us an output of `16`. This represents the 16 cells
    that the game has.
  prefs: []
  type: TYPE_NORMAL
- en: 'Print the number of possible actions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code results in a value of `4`, which represents the four possible
    actions that can be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample a random action at a given state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, `.sample()` specifies that we fetch one of the possible four actions
    in a given state. The scalar corresponding to each action can be associated with
    the name of the action. We can do this by inspecting the source code in GitHub:
    [https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reset the environment to its original state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take (`step`) an action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code fetches the next state, the reward, the flag that states
    whether the game was completed, and additional information. We can execute the
    game with `.step`, since the environment readily provides the next state when
    it’s given a step with an action.
  prefs: []
  type: TYPE_NORMAL
- en: These steps form the basis for us to build a Q-table that dictates the optimal
    action to be taken in each state. We’ll do this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Q-table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned how to calculate Q-values for various state-action
    pairs manually. In this section, we will leverage the Gym environment and the
    various modules associated with it to populate the Q-table – where rows represent
    the possible states of an agent and columns represent the actions the agent can
    take. The values of the Q-table represent the Q-values of taking an action in
    a given state.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can populate the values of the Q-table using the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the game environment and the Q-table with zeros.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a random action and fetch the next state, the reward, the flag stating
    whether the game was completed, and additional information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q-value using the Bellman equation we defined earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* and *3* so that there’s a maximum of 50 steps in an episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2*, *3*, and *4* over multiple episodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s code up the preceding strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available as `Building_Q_table.ipynb` in the `Chapter14`
    folder in this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and initialize the game environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the Q-table with zeros:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code checks the possible actions and states that can be used to
    build a Q-table. The Q-table’s dimension should be the number of states multiplied
    by the number of actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Play multiple episodes while taking a random action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we first reset the environment at the end of every episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take a maximum of 50 steps per episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We consider a maximum of 50 steps per episode, as it’s possible for the agent
    to keep oscillating between two states forever (think of left and right actions
    being performed consecutively forever). Thus, we need to specify the maximum number
    of steps an agent can take.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample a random action and take (`step`) the action:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the Q-value that corresponds to the state and the action:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we specified that the learning rate is `0.1` and that
    we’re updating the Q-value of a state-action combination, by taking the maximum
    Q-value of the next state (`np.max(qtable[new_state,:])`) into consideration.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the `state` value to `new_state`, which we obtained previously, and
    accumulate `reward` into `total_rewards`:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Place the rewards in a list (`episode_rewards`), and print the Q-table (`qtable`):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code fetches the Q-values of the various actions across states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated with medium confidence](img/B18457_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Q-values of the various actions across states'
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about how the obtained Q-table is leveraged in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have kept taking a random action every time. However, in a realistic
    scenario, once we have learned that certain actions can’t be taken in certain
    states and vice versa, we don’t need to take a random action anymore. The concept
    of exploration-exploitation comes in handy in such a scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging exploration-exploitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The concept of exploration-exploitation can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploration** is a strategy where we learn what needs to be done (what action
    to take) in a given state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploitation** is a strategy where we leverage what has already been learned
    – that is, which action to take in a given state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During the initial stages, it is ideal to have a high amount of exploration,
    as the agent won’t know what optimal actions to take initially. Through the episodes,
    as the agent learns the Q-values of various state-action combinations over time,
    we must leverage exploitation to perform the action that leads to a high reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this intuition in place, let’s modify the Q-value calculation that we
    built in the previous section so that it includes exploration and exploitation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The bold lines in the preceding code are what’s been added to the code that
    was shown in the previous section. Within this code, we specify that, over increasing
    episodes, we perform more exploitation than exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we’ve obtained the Q-table, we can leverage it to identify the steps that
    the agent needs to take to reach its destination:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we fetch the current `state` that the agent is in, identify
    the `action` that results in a maximum value in the given state-action combination,
    take the action (`step`) to fetch the `new_state` object that the agent would
    be in, and repeat these steps until the game is complete (terminated).
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18457_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Optimal actions that an agent takes'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding figure, the agent is able to take the optimal
    action to reach its goal. Note that this is a simplified example, since the state
    spaces are discrete, resulting in us building a Q-table.
  prefs: []
  type: TYPE_NORMAL
- en: But what if the state spaces are continuous (for example, the state space is
    a snapshot image of a game’s current state)? Building a Q-table becomes very difficult
    (as the number of possible states is very large). Deep Q-learning comes in handy
    in such a scenario. We’ll learn about this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing deep Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned how to build a Q-table, which provides values that correspond
    to a given state-action combination by replaying a game – in this case, the Frozen
    Lake game – over multiple episodes. However, when the state spaces are continuous
    (such as a snapshot of a game of Pong – which is an image), the number of possible
    state spaces becomes huge. We will address this in this section, as well as the
    ones to follow, using deep Q-learning. In this section, we will learn how to estimate
    the Q-value of a state-action combination without a Q-table by using a neural
    network – hence the term **deep** Q-learning. Compared to a Q-table, deep Q-learning
    leverages a neural network to map any given state-action (where the state can
    be continuous or discrete) combination to Q-values.
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we will work on the CartPole environment in Gym. Let’s first
    understand what this is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the CartPole environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our task is to balance a cart pole for as long as possible. The following image
    shows what the CartPole environment looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, diagram  Description automatically generated](img/B18457_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Possible actions in a CartPole environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the pole shifts to the left when the cart moves to the right and
    vice versa. Each state within this environment is defined using four observations,
    whose names and minimum and maximum values are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Observation** | **Minimum Value** | **Maximum Value** |'
  prefs: []
  type: TYPE_TB
- en: '| Cart position | -2.4 | 2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Cart velocity | -inf | inf |'
  prefs: []
  type: TYPE_TB
- en: '| Pole angle | -41.8° | 41.8° |'
  prefs: []
  type: TYPE_TB
- en: '| Pole velocity at the tip | -inf | inf |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14.1: Observations (states) in a CartPole environment'
  prefs: []
  type: TYPE_NORMAL
- en: Note that all the observations that represent a state have continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, deep Q-learning for the game of CartPole balancing works as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch the input values (the image of the game/metadata of the game).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass the input values through a network that has as many outputs as there are
    possible actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output layers predict the action values that correspond to taking an action
    in a given state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A high-level overview of the network architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B18457_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Network architecture to identify the right value of an action
    when given a state'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding image, the network architecture uses the state (four observations)
    as input and the Q-value of taking left and right actions in the current state
    as output. We train the neural network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: During the exploration phase, we perform a random action that has the highest
    value in the output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we store the action, the next state, the reward, and the flag stating
    whether the game was complete in memory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In a given state, if the game is not complete, the Q-value of taking an action
    in a given state will be calculated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B18457_14_007.png)'
  prefs: []
  type: TYPE_IMG
- en: The Q-values of the current state-action combinations remain unchanged except
    for the action that is taken in *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform *steps 1* to *4* multiple times and store the experiences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit a model that takes the state as input and the action values as the expected
    outputs (from memory and replay experience), and minimize the **mean squared error**
    (**MSE**) loss between the target Q-value of the best action in the next state
    and the predicted Q-value of the action in the given state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding steps over multiple episodes while decreasing the exploration
    rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the preceding strategy in place, let’s code up deep Q-learning so that
    we can perform CartPole balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Performing CartPole balancing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform CartPole balancing, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: This code is available as `Deep_Q_Learning_Cart_Pole_balancing.ipynb` in the
    `Chapter14` folder in this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend that you execute the notebook in GitHub to reproduce the results to
    understand the steps you need to perform and the various code components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install and import the relevant packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the network architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the architecture is fairly simple, since it only contains 24 units
    in the 2 hidden layers. The output layer contains as many units as there are possible
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `Agent` class, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `__init__` method with the various parameters, network, and experience
    defined:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `step` function, which fetches data from memory and fits it to the
    model by calling the `learn` function:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we are learning on a random sample of experiences (using `self.memory`)
    instead of a consecutive sequence of experiences, ensuring that the model learns
    what to do based on current inputs only. If we were to give experiences sequentially,
    there would be a risk of the model learning the correlations in the consecutive
    inputs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `act` function, which predicts an action when given a state:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, in the preceding code, we performing exploration-exploitation while
    determining the action to take.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `learn` function, which fits the model so that it predicts action
    values when given a state:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we fetch the sampled experiences and predict the Q-value
    of the action we performed. Furthermore, given that we already know the next state,
    we can predict the best Q-value of the actions in the next state. This way, we
    now know the target value that corresponds to the action that was taken in a given
    state. Finally, we compute the loss between the expected value (`Q_targets`) and
    the predicted value (`Q_expected`) of the Q-value of the action that was taken
    in the current state.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `sample_experiences` function in order to sample experiences from
    memory:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `agent` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Perform deep Q-learning, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the list that will store the score information and also the hyperparameters:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reset the environment in each episode and fetch the state’s shape (number of
    observations). Furthermore, reshape it so that we can pass it to a network:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Loop through `max_t` time steps, identify the action to be performed, and perform
    (`step`) it. Then, reshape it so that the reshaped state is passed to the neural
    network:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model by specifying `agent.step` on top of the current state and resetting
    the state to the next state so that it can be useful in the next iteration:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the score values, print periodically, and stop training if the mean of
    the scores in the previous 10 steps is greater than 450 (which in general is a
    good score and, hence, chosen):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the variation in scores over increasing episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A plot showing the variation of scores over episodes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B18457_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Scores over increasing episodes'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding image, we can see that, after episode 2,000, the model attained
    a high score when balancing the CartPole.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to implement deep Q-learning, in the next section,
    we will learn how to work on a different state space – a video frame in Pong,
    instead of the four state spaces that define the state in the CartPole environment.
    We will also learn how to implement deep Q-learning with the fixed targets model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing deep Q-learning with the fixed targets model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learned how to leverage deep Q-learning to solve
    the CartPole environment in Gym. In this section, we will work on a more complicated
    game of Pong and understand how deep Q-learning, alongside the fixed targets model,
    can solve the game. While working on this use case, you will also learn how to
    leverage a CNN-based model (in place of the vanilla neural network we used in
    the previous section) to solve the problem. The theory from the previous section
    remains largely the same, with one crucial change, a “fixed target model.” Essentially,
    we create a copy of the local model and use that as our guide for our local model
    at every 1,000 steps, along with the local model’s rewards for those 1,000 steps.
    This makes the local model more grounded and updates its weights more smoothly.
    After the 1,000 steps, we update the target model with the local model to update
    the overall learnings.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why two models can be effective is that we reduce the burden on the
    local model to simultaneously select actions and generate the targets to train
    the network – such interdependence can lead to significant oscillations in the
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the use case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of this use case is to build an agent that can play against a
    computer (a pre-trained, non-learning agent) and beat it in a game of Pong, where
    the agent is expected to achieve a score of 21 points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategy that we will adopt to solve the problem of creating a successful
    agent for the game of Pong is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Crop the irrelevant portion of the image to fetch the current frame (state):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: Original image and processed image (frame) in the Pong game'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the preceding image, we have taken the original image and cropped
    the top and bottom pixels of the original image in the processed image.
  prefs: []
  type: TYPE_NORMAL
- en: Stack four consecutive frames – the agent needs the sequence of states to understand
    whether the ball approaches it or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the agent play by taking random actions initially, and keep collecting the
    current state, future state, action taken, and rewards in memory. Only keep information
    about the last 10,000 actions in memory and flush the historical ones beyond 10,000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a network (local network) that takes a sample of states from memory and
    predicts the values of the possible actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define another network (target network) that is a replica of the local network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the target network every 1,000 times the local network is updated. The
    weights of the target network at the end of every 1,000 epochs are the same as
    the weights of the local network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leverage the target network to calculate the Q-value of the best action in the
    next state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the action that the local network suggests, we expect it to predict the
    summation of the immediate reward and the Q-value of the best action in the next
    state.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the MSE loss of the local network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let the agent keep playing until it maximizes its rewards.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the preceding strategy in place, we can now code up the agent so that it
    maximizes its rewards when playing Pong.
  prefs: []
  type: TYPE_NORMAL
- en: Coding up an agent to play Pong
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to code up the agent so that it self-learns how to play
    Pong:'
  prefs: []
  type: TYPE_NORMAL
- en: The following code is available as `Pong_Deep_Q_Learning_with_Fixed_targets.ipynb`
    in the `Chapter14` folder in this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend that you execute the notebook in GitHub to reproduce the results to
    understand the steps to perform and the various code components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the relevant packages and set up the game environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the state size and action size:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will preprocess a frame so that it removes the bottom
    and top pixels that are irrelevant:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will stack four consecutive frames, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function takes `stacked_frames`, the current `state`, and the flag of `is_new_episode`
    as input:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the episode is new (a restart of the game), we will start with a stack of
    initial frames:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the episode is not new, we’ll remove the oldest frame from `stacked_frames`
    and append the latest frame:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the network architecture – that is, `DQNetwork`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Agent` class, as we did in the previous section, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the `__init__` method:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the only addition we’ve made to the `__init__` method in the preceding
    code, compared to the code provided in the previous section, is the `target` network
    and the frequency with which it will be updated (these lines were shown in bold
    in the preceding code).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define the method that will update the weights (`step`), just like we did in
    the previous section:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `act` method, which will fetch the action to be performed in a given
    state:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `learn` function, which will train the local model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, in the preceding code, `Q_targets_next` is predicted using the target
    model instead of the local model that was used in the previous section (we’ve
    highlighted this line in the code). We also update the target network after every
    1,000 steps, where `learn_every_target_counter` is the counter that helps to identify
    whether we should update the target model.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a function (`target_update`) that will update the target model:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will sample experiences from memory:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `Agent` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the parameters that will be used to train the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the agent over increasing episodes, as we did in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following plot shows the variation of scores over increasing episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart  Description automatically generated](img/B18457_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Scores over increasing epochs'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding image, we can see that the agent gradually learned to play
    Pong and that, by the end of 800 episodes, it had learned how to play it while
    receiving a high reward.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve trained an agent to play Pong well, in the next section, we will
    train an agent so that it can drive a car autonomously in a simulated environment.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an agent to perform autonomous driving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have seen RL working in progressively challenging environments,
    we will conclude this chapter by demonstrating that the same concepts can be applied
    to a self-driving car. Since it is impractical to see this working on an actual
    car, we will resort to a simulated environment. This scenario has the following
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: The environment is a full-fledged city of traffic, with cars and additional
    details within the image of a road. The actor (agent) is a car.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs to the car are the various sensory inputs such as a dashcam, **Light
    Detection and Ranging** (**LIDAR**) sensors, and GPS coordinates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The outputs are going to be how fast/slow the car will move, along with the
    level of steering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This simulation will attempt to be an accurate representation of real-world
    physics. Thus, note that the fundamentals will remain the same, whether it is
    a car simulation or a real car.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the environment we are going to install needs a **graphical user interface**
    (**GUI**) to display the simulation. Also, the training will take at least a day,
    if not more. Because of the non-availability of a visual setup and the time usage
    limits of Google Colab, we will not use Google Colab notebooks as we have done
    so far. This is the only section of this book that requires an active Linux operating
    system and, preferably, a GPU to achieve acceptable results in a few days of training.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the CARLA environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned previously, we need an environment that can simulate complex
    interactions to make us believe that we are, in fact, dealing with a realistic
    scenario. CARLA is one such environment. The environment author stated the following
    about CARLA:'
  prefs: []
  type: TYPE_NORMAL
- en: “*CARLA has been developed from the ground up to support development, training,
    and validation of autonomous driving systems. In addition to open source code
    and protocols, CARLA provides open digital assets (urban layouts, buildings, and
    vehicles) that were created for this purpose and can be used freely. The simulation
    platform supports flexible specification of sensor suites, environmental conditions,
    full control of all static and dynamic actors, maps generation, and much more.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two steps we need to follow to set up the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the CARLA binaries for the simulation environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install the Gym version, which provides Python connectivity for the simulation
    environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The steps for this section have been presented as a video walkthrough here:
    [https://tinyurl.com/mcvp-self-driving-agent](https://tinyurl.com/mcvp-self-driving-agent).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Installing the CARLA binaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to install the necessary CARLA binaries:'
  prefs: []
  type: TYPE_NORMAL
- en: Visit [https://github.com/carla-simulator/carla/releases/tag/0.9.6](https://github.com/carla-simulator/carla/releases/tag/0.9.6)
    and download the `CARLA_0.9.6.tar.gz` compiled version file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Move it to a location where you want CARLA to live in your system and unzip
    it. Here, we will demonstrate this by downloading and unzipping CARLA into the
    `Documents` folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add CARLA to `PYTHONPATH` so that any module on your machine can import it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code, we added the directory containing CARLA to a global
    variable called `PYTHONPATH`, which is an environment variable for accessing all
    Python modules. Adding it to `~/.bashrc` will ensure that every time a terminal
    is opened, it can access this new folder. After running the preceding code, restart
    the terminal and run `ipython -c "import carla; carla.__spec__"`. You should get
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B18457_14_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Location of CARLA on your machine'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, provide the necessary permissions and execute CARLA, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a minute or two, you should see a window similar to the following showing
    CARLA running as a simulation, ready to take inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text  Description automatically generated](img/B18457_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: Window showing CARLA running'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve verified that `CARLA` is a simulation environment whose
    binaries work as expected. Let’s move on to installing the Gym environment for
    it. Leave the terminal running as is, since we need the binary to be running in
    the background throughout this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the CARLA Gym environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since there is no official Gym environment, we will take advantage of a user-implemented
    GitHub repository and install the Gym environment for CARLA from there. Follow
    these steps to install CARLA’s Gym environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the Gym repository to a location of your choice and install the library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test your setup by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A window similar to the following should open, showing that we have added a
    fake car to the environment. From here, we can monitor the top view, the LIDAR
    sensor point cloud, and our dashcam:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, light  Description automatically generated](img/B18457_14_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: Overview of the current episode'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first view contains a view that is very similar to what vehicle GPS systems
    show in a car – that is, our vehicle, the various waypoints, and the road lanes.
    However, we shall not use this input for training, as it also shows other cars
    in the view, which is unrealistic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second view is more interesting. Some consider it as the eye of a self-driving
    car. LIDAR emits pulsed light into the surrounding environment (in all directions),
    multiple times every second. It captures the reflected light to determine how
    far the nearest obstacle is in that direction. The onboard computer collates all
    the nearest obstacle information to recreate a 3D point cloud that gives it a
    3D understanding of its environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both the first and second views, we can see that there is a strip ahead of
    the car. This is a waypoint indication of where the car is supposed to go.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third view is a simple dashboard camera.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apart from these three, CARLA provides additional sensor data, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lateral-distance` (a deviation from the lane it should be in)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delta-yaw` (an angle with respect to the road ahead)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`speed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether there’s a hazardous obstacle in front of the vehicle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to use the first four sensors mentioned previously, along with
    LIDAR and our dashcam, to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to understand the components of CARLA and create a DQN model
    for a self-driving car.
  prefs: []
  type: TYPE_NORMAL
- en: Training a self-driving agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create two files before we start the training process in a notebook
    – that is, `model.py` and `actor.py`. These will contain the model architecture
    and the `Agent` class, respectively. The `Agent` class contains the various methods
    we’ll use to train an agent.
  prefs: []
  type: TYPE_NORMAL
- en: The code instructions for this section are present in the `Carla.md` file in
    the `Chapter14` folder of this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  prefs: []
  type: TYPE_NORMAL
- en: Creating model.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is going to be a PyTorch model that will accept the image that’s provided
    to it, as well as other sensor inputs. It will be expected to return the most
    likely action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down this code. As you can see, there are more types of data being
    fed into the `forward` method than in the previous sections, where we were just
    accepting an image as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self.image_branch` expects the image coming from the dashcam of the car.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.lidar_branch` accepts the image that’s generated by the LIDAR sensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.sensor_branch` accepts four sensor inputs in the form of a NumPy array.
    These four items are:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lateral distance (a deviation from the lane it is supposed to be in)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`delta-yaw` (an angle with respect to the road ahead)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The presence of any hazardous obstacles in front of the vehicle
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: See line number 544 in `gym_carla/envs/carla_env.py` (the repository that has
    been Git-cloned) for the same outputs. Using a different branch in the neural
    network will let the module provide different levels of importance for each sensor,
    and the outputs are summed up as the final output. Note that there are nine outputs;
    we will look at these later.
  prefs: []
  type: TYPE_NORMAL
- en: Creating actor.py
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Much like the previous sections, we will use some code to store replay information
    and play it back when training is necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get the imports and hyperparameters in place:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll initialize the target and local networks. No changes have been
    made to the code from the previous section here, except for the module that is
    imported:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since there are more sensors to handle, we’ll transport them as a dictionary
    of state. The state contains the `''image''`, `''lidar''`, and `''sensor''` keys,
    which we introduced in the previous section. We perform preprocessing before sending
    them to the neural network, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to fetch items from replay memory. The following instructions
    are executed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain a batch of current and next states.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the expected reward, `Q_expected`, if a network performs actions in
    the current state.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare it with the target reward, `Q_targets`, that will have been obtained
    when the next state was fed to the network.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Periodically update the target network with the local network.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the code that can be used to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_OL
  type: TYPE_PRE
- en: 'The only major change in the `ReplayBuffer` class is going to be how the data
    is stored. Since we have multiple sensors, each memory state (both the current
    and the next state) is stored as a tuple of data – that is, `states = [images,
    lidars, sensors]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the lines of code in bold fetch the current states, actions, rewards,
    and next states’ information.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the critical components are in place, let’s load the Gym environment
    into a Python notebook and start training.
  prefs: []
  type: TYPE_NORMAL
- en: Training a DQN with fixed targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is no additional theory we need to learn here. The basics remain the
    same; we’ll only make changes to the Gym environment, the architecture of the
    neural network, and the actions our agent needs to take:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the hyperparameters associated with the environment. Refer to each
    comment beside every key-value pair presented in the `params` dictionary in the
    following code. Since we will simulate a complex environment, we need to choose
    the environment’s parameters, such as the number of cars in the city, the number
    of walkers, which town to simulate, the resolution of the dashcam image, and the
    LIDAR sensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding `params` dictionary, the following are important for our simulation
    in terms of the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''discrete'': True`: Our actions lie in a discrete space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''discrete_acc'':[-1,0,1]`: All the possible accelerations that the self-driven
    car is allowed to make during the simulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''discrete_steer'':[-0.3,0,0.3]`: All the possible steering magnitudes that
    the self-driven car is allowed to make during the simulation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, the `discrete_acc` and `discrete_steer` lists contain three
    items each. This means that there are 3 x 3 possible unique actions the car can
    take. So, the network in the `model.py` file has nine discrete states.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to change the parameters once you’ve gone through the official documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we have all the components we need to train the model. Load a pre-trained
    model, if one exists. If we are starting from scratch, keep it as `None`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fix the number of episodes, and define the `dqn` function to train the agent,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reset the state:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Wrap the state into a dictionary (as discussed in the `actor.py:Actor` class)
    and act on it:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the next state that’s obtained from the environment, and then store the
    `state, next_state` pair (along with the rewards and other state information)
    to train the actor using a DQN:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We must repeat the loop until we get a done signal, after which we reset the
    environment and start storing actions once again. After every 100 episodes, store
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Call the `dqn` function to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since this is a more complex environment, training can take a few days, so
    be patient and continue training a few hours at a time using the `load_path` and
    `save_path` arguments. With enough training, the vehicle can maneuver and learn
    how to drive by itself. Here’s a video of the training result we were able to
    achieve after two days of training: [https://tinyurl.com/mcvp-self-driving-agent-result](https://tinyurl.com/mcvp-self-driving-agent-result).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how the values of various actions in a given state
    are calculated. We then learned how the agent updates the Q-table, using the discounted
    value of taking an action in a given state. In the process of doing this, we learned
    how the Q-table is infeasible in a scenario where the number of states is high.
    We also learned how to leverage deep Q-networks to address the scenario where
    the number of possible states is high. Then, we moved on to leveraging CNN-based
    neural networks while building an agent that learned how to play Pong, using a
    DQN based on fixed targets. Finally, we learned how to leverage a DQN with fixed
    targets to perform self-driving, using the CARLA simulator.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen repeatedly in this chapter, you can use deep Q-learning to learn
    very different tasks – such as CartPole balancing, playing Pong, and self-driving
    navigation – with almost the same code. While this is not the end of our journey
    into exploring RL, at this point, we should be able to appreciate how we can use
    CNN-based and reinforcement learning-based algorithms together to solve complex
    problems and build learning agents.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to combine computer vision-based techniques with
    techniques from other prominent areas of research, including meta-learning, natural
    language processing, and reinforcement learning. Apart from this, we’ve also learned
    how to perform object classification, detection, segmentation, and image generation
    using GANs. In the next chapter, we will switch gears and learn how to move a
    deep learning model into production.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does an agent calculate the value of a given state?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is a Q-table populated?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we have a discount factor in a state-action value calculation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need the exploration-exploitation strategy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need to use deep Q-learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How is the value of a given state-action combination calculated using deep Q-learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once an agent has maximized a reward in the CartPole environment, is there a
    chance that it can learn a suboptimal policy later?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn more on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code237402495622324343.png)'
  prefs: []
  type: TYPE_IMG
