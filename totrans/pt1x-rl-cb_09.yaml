- en: Capstone Project – Playing Flappy Bird with DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this very last chapter, we will work on a capstone project—playing Flappy
    Bird using reinforcement learning. We will apply what we have learned throughout
    this book to build an intelligent bot. We will also focus on building **Deep Q-Networks**
    (**DQNs**), fine-tuning model parameters, and deploying the model. Let's see how
    long the bird can stay in the air.
  prefs: []
  type: TYPE_NORMAL
- en: 'The capstone project will be built section by section in the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the game environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Deep Q-Network to play Flappy Bird
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and tuning the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the model and playing the game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a result, the code in each recipe is to be built on top of the previous recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the game environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To play Flappy Bird with a DQN, we first need to set up the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll simulate the Flappy Bird game using Pygame. Pygame ([https://www.pygame.org](https://www.pygame.org/))
    contains a set of Python modules developed for creating video games. It also includes
    graphics and sound libraries needed in games. We can install the `Pygame` package
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Flappy Bird is a famous mobile game originally developed by Dong Nguyen. You
    can try it yourself, using your keyboard, at [https://flappybird.io/](https://flappybird.io/).
    The aim of the game is to remain alive as long as possible. The game ends when
    the bird touches the floor or a pipe. So, the bird needs to flap its wings at
    the right times to get through the random pipes and to avoid falling to the ground.
    Possible actions include flapping and not flapping. In the game environment, the
    reward is +0.1 for every step, with the following two exceptions:'
  prefs: []
  type: TYPE_NORMAL
- en: -1 when a collision occurs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: +1 when the bird gets through the gap between two pipes. The original Flappy
    Bird game is scored based on the number of gaps passed through.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Download the assets of the game environment we need from [https://github.com/yanpanlau/Keras-FlappyBird/tree/master/assets/sprites](https://github.com/yanpanlau/Keras-FlappyBird/tree/master/assets/sprites).
    For simplicity, we''ll just use the images in the `sprites` folder. Specifically,
    we will need the following images:'
  prefs: []
  type: TYPE_NORMAL
- en: '`background-black.png`: The background image of the screen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`base.png`: The image for the floor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pipe-green.png`: The image for the pipes that the bird needs to stay away
    from'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`redbird-downflap.png`: The image for the bird when it''s flapping down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`redbird-midflap.png`: The image for the bird when it''s not flapping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`redbird-upflap.png`: The image for the bird when it''s flapping up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested, you can also use audio files to make the game more fun.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll develop the Flappy Bird game environment using `Pygame` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by developing a utility function that loads images and transforms
    them into the right format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Import all the necessary packages for the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the game and clock and set 30 frames per second as the screen refresh
    frequency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the screen size and create a screen accordingly, then add a caption
    to the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load necessary images (in the `sprites` folder) with the following
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the game variables, including the size of the bird and the pipes, and set
    100 as the vertical gap between two pipes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The flapping movement of the bird rotates through up, middle, down, middle,
    up, and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is just to make the game more fun to watch.
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining all constants, we start with the `__init__method` of the game
    environment''s `FlappyBird` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue by defining the `gen_random_pipe` method, which generates a pair
    of pipes (one upper and one lower) in a given horizontal position and random vertical
    positions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The upper and lower pipes are in the `y` position of `gap_y - pipe_height` and
    `gap_y + pipe_gap_size` respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next method we develop is `check_collision,` which returns `True` if the
    bird collides with the base or a pipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The last and the most import method we need is `next_step`, which performs
    an action and returns the updated image frame of the game, the reward received,
    and whether the episode is over or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: That's all for the Flappy Bird environment.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 8*, we define the velocity of the pipe (to the left by 4 units as time
    goes by), the minimal and maximal vertical velocity of the bird (`-8` and `10`),
    its upward and downward acceleration (`-9` and `1`), its default vertical velocity
    (`0`), the starting index of the bird image (`0`), the initial score, the initial
    horizontal and vertical position of the bird, the position of the base, and the
    coordinates of the pipes that are randomly generated using the `gen_random_pipe`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 11*, by default, the reward for a step is `+0.1`. If the action is
    flap, we increase the bird’s vertical velocity by its upward acceleration. Then,
    we check whether the bird happens to get through a pair of pipes. If it does,
    the game score increases by 1 and the step reward becomes + 1\. We update the
    bird’s position, its image index, as well as the pipes' position. A new pair of
    pipes will be generated if the old pair is about to leave the left-hand side of
    the screen, and the old pair of pipes will be deleted once it goes offscreen.
    If a collision occurs, the episode will end and the reward will be -1; the game
    will also reset. Finally, we’ll display the updated frame on the game screen.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deep Q-Network to play Flappy Bird
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that the Flappy Bird environment is ready, we can start tackling it by building
    a DQN model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, a screen image is returned at each step after an action is
    taken. A CNN is one of the best neural network architectures to deal with image
    inputs. In a CNN, the convolutional layers are able to effectively extract features
    from images, which will be passed on to fully connected layers downstream. In
    our solution, we will use a CNN with three convolutional layers and one fully
    connected hidden layer. An example of CNN architecture is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ecfb19a6-6585-40eb-ade5-8f24dc904ebf.png)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s develop a CNN-based DQN model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We start with the CNN model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now develop a DQN with experience replay using the CNN model we just built:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict` method estimates the output Q-values, given an input state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'An `update` method updates the weights of the neural network, given a training
    sample, and returns the current loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The last part of the `DQN` class is the `replay` method, which performs experience
    replay given a collection of past experiences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: That's it for the DQN class. In the next recipe, we will train the DQN model
    on a number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 2*, we put together the backbone of the CNN-based DQN. It has three
    convolutional layers with various configurations. A ReLU activation function follows
    each convolutional layer. The resulting feature map from the last convolutional
    layer is then flattened and fed to a fully-connected hidden layer with 512 nodes,
    followed by the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we also set a boundary for the initial random value of the weights
    and a zero bias so that the model is more likely to converge faster.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 6* is for step-wise training with experience replay. If we have enough
    experiences, we randomly draw a `replay_size` set of experiences for training.
    We then convert each experience into a training sample composed of the predicted
    values and output target values, given an input state. The target values are computed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the target Q value for the action using the reward and the new Q values,
    as in: [![](img/1a6a21ec-ee29-4f78-881a-3647f1e04b91.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is a terminal state, the target Q value is updated as `r`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And finally, we update the neural network using the selected batch of training
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: Training and tuning the network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will train the DQN model to play Flappy Bird.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each step of the training, we take an action following the epsilon-greedy
    policy: under a certain probability (epsilon), we will take a random action, flapping
    or not flapping in our case; otherwise, we select the action with the highest
    value. We also adjust the value of epsilon for each step as we favor more exploration
    at the beginning and more exploitation when the DQN model is getting more mature.'
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, the observation for each step is a two-dimensional image of
    the screen. We need to transform the observation images into states. Simply using
    one image from a step will not provide enough information to guide the agent as
    to how to react. Hence, we form a state using images from four adjacent steps.
    We will first reshape the image into the expected size, then concatenate the image
    of the current frame with the three previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We train the DQN model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We start by developing the epsilon-greedy policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We specify the size of the preprocessed image, the batch size, the learning
    rate, the gamma , the number of actions, the initial and final epsilon, the number
    of iterations, and the size of the memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We also save the trained model periodically, as it will be a very long process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Don't forget to create a folder named `trained_models`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We specify the random feed for experimental reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a DQN model accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We also create a memory queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: New samples will be appended to the queue, and the old ones will be removed
    as long as there are more than 50,000 samples in the queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize a Flappy Bird environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We then obtain the initial image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned before, we should resize the raw image to `image_size * image_size`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If the `cv2` package is not installed, you can do so with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s preprocess the image accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we construct a state by concatenating four images. Since we only have
    the first frame now, we simply replicate it four times:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We then work on the training loop for `n_iter` steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run that section of code, we''ll see the following logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The training will take a while. Of course, you can speed up training with the
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we save the last trained mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 9*, for each training step, we perform the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Slightly decrease the epsilon, and create an epsilon-greedy policy accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Take the action computed using the epsilon-greedy policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocess the resulting image and construct the new state by appending the
    image to those from the previous three steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Record the experience in this step, including the state, the action, the next
    state, the reward received, and whether it ends or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the model with experience replay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Print out the training status and update the state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the trained model periodically in order to avoid retraining from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying the model and playing the game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've trained the DQN model, let's apply it to play the Flappy Bird
    game.
  prefs: []
  type: TYPE_NORMAL
- en: Playing the game with the trained model is simple. We will just take the action
    associated with the highest value in each step. We will play a few episodes to
    see how it performs. Don’t forget to preprocess the raw screen image and construct
    the state.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We test the DQN model on new episodes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first load the final model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We run 100 episodes, and we perform the following for each episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Hopefully, you will see something like the following image, where the bird
    gets through a series of pipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6864a7d8-989a-4840-9655-8f728aa78bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Step 2*, we perform the following tasks for each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a Flappy Bird environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe the initial image and generate its state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the Q-values, given the state, using the model and taking the action
    with the highest Q-value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observe the new image and whether the episode ends or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the episode continues, compute the state of the next image and assign it
    to the current state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat until the episode ends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
