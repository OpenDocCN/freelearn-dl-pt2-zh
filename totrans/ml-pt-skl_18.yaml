- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Graph Neural Networks for Capturing Dependencies in Graph Structured Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于捕获图结构数据中依赖关系的图神经网络
- en: In this chapter, we will introduce a class of deep learning models that operates
    on graph data, namely, **graph neural networks** (**GNNs**). GNNs have been an
    area of rapid development in recent years. According to the *State of AI* report
    from 2021 ([https://www.stateof.ai/2021-report-launch.html](https://www.stateof.ai/2021-report-launch.html)),
    GNNs have evolved “from niche to one of the hottest fields of AI research.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一类深度学习模型，它们操作的是图数据，即**图神经网络**（**GNNs**）。近年来，GNNs已经迅速发展。根据2021年的*AI现状报告*（[https://www.stateof.ai/2021-report-launch.html](https://www.stateof.ai/2021-report-launch.html)），GNNs已经从一种小众领域发展成为AI研究中最热门的领域之一。
- en: 'GNNs have been applied in a variety of areas, including the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs已被应用于多个领域，包括以下几个方面：
- en: Text classification ([https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903))
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类 ([https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903))
- en: Recommender systems ([https://arxiv.org/abs/1704.06803](https://arxiv.org/abs/1704.06803))
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统 ([https://arxiv.org/abs/1704.06803](https://arxiv.org/abs/1704.06803))
- en: Traffic forecasting ([https://arxiv.org/abs/1707.01926](https://arxiv.org/abs/1707.01926))
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交通预测 ([https://arxiv.org/abs/1707.01926](https://arxiv.org/abs/1707.01926))
- en: Drug discovery ([https://arxiv.org/abs/1806.02473](https://arxiv.org/abs/1806.02473))
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 药物发现 ([https://arxiv.org/abs/1806.02473](https://arxiv.org/abs/1806.02473))
- en: While we can’t cover every new idea in this rapidly developing space, we’ll
    provide a basis to understand how GNNs function and how they can be implemented.
    In addition, we’ll introduce the **PyTorch Geometric** library, which provides
    resources for managing graph data for deep learning as well as implementations
    of many different kinds of graph layers that you can use in your deep learning
    models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们无法涵盖这个快速发展空间中的每一个新想法，但我们将提供一个理解GNNs如何运作及如何实施它们的基础。此外，我们还将介绍**PyTorch Geometric**库，该库提供了管理图数据用于深度学习的资源，以及许多不同种类的图层实现，供您在深度学习模型中使用。
- en: 'The topics that will be covered in this chapter are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖的主题如下：
- en: An introduction to graph data and how it can be represented for use in deep
    neural networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图数据介绍及其如何在深度神经网络中表示和使用
- en: An explanation of graph convolutions, a major building block of common GNNs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图卷积的解释，这是常见图神经网络（**GNNs**）的主要构建模块
- en: A tutorial showing how to implement GNNs for molecular property prediction using
    PyTorch Geometric
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个教程展示如何使用PyTorch Geometric实现用于分子属性预测的GNNs
- en: An overview of methods at the cutting edge of the GNN field
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GNN领域尖端方法概述
- en: Introduction to graph data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图数据介绍
- en: 'Broadly speaking, graphs represent a certain way we describe and capture relationships
    in data. Graphs are a particular kind of data structure that is nonlinear and
    abstract. And since graphs are abstract objects, a concrete representation needs
    to be defined so the graphs can be operated on. Furthermore, graphs can be defined
    to have certain properties that may require different representations. *Figure
    18.1* summarizes the common types of graphs, which we will discuss in more detail
    in the following subsections:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 广义来说，图表达了我们描述和捕捉数据关系的某种方式。图是一种非线性和抽象的数据结构。由于图是抽象对象，因此需要定义具体表示形式，以便对图进行操作。此外，图可以定义具有某些属性，这可能需要不同的表示形式。*图18.1*总结了常见类型的图表，我们将在接下来的小节中详细讨论它们：
- en: '![Diagram  Description automatically generated](img/B17582_18_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_18_01.png)'
- en: 'Figure 18.1: Common types of graphs'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1：常见类型的图表
- en: Undirected graphs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无向图
- en: 'An **undirected graph** consists of **nodes** (in graph theory also often called
    **vertices**) that are connected via edges where the order of the nodes and their
    connection does not matter. *Figure 18.2* sketches two typical examples of undirected
    graphs, a friend graph, and a graph of a chemical molecule consisting of atoms
    connected through chemical bonds (we will be discussing such molecular graphs
    in much more detail in later sections):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**无向图**由通过边连接的**节点**（在图论中通常称为**顶点**）组成，其中节点的顺序及其连接的顺序并不重要。*图18.2*示意了两个典型的无向图示例，一个是朋友关系图，另一个是由化学键连接的原子组成的化学分子图（我们将在后续章节中详细讨论此类分子图）：'
- en: '![Diagram  Description automatically generated](img/B17582_18_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图表描述](img/B17582_18_02.png)'
- en: 'Figure 18.2: Two examples of undirected graphs'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2：两个无向图示例
- en: Other common examples of data that can be represented as undirected graphs include
    images, protein-protein interaction networks, and point clouds.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 可以表示为无向图的其他常见数据示例包括图像、蛋白质相互作用网络和点云。
- en: Mathematically, an undirected graph *G* is a pair (*V*, *E*), where *V* is a
    set of the graph’s nodes, and *E* is the set of edges making up the paired nodes.
    The graph can then be encoded as a |*V*|×|*V*| **adjacency matrix** **A**. Each
    element *x*[ij] in matrix **A** is either a 1 or a 0, with 1 denoting an edge
    between nodes *i* and *j* (vice versa, 0 denotes the absence of an edge). Since
    the graph is undirected, an additional property of **A** is that *x*[ij] = *x*[ji].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，无向图*G*是一个二元组（*V*，*E*），其中*V*是图的节点集合，*E*是构成节点对的边的集合。然后，可以将图编码为|*V*|×|*V*|的**邻接矩阵**
    **A**。矩阵**A**中的每个元素*x*[ij]要么是1，要么是0，其中1表示节点*i*和*j*之间有边（反之，0表示没有边）。由于图是无向的，**A**的另一个特性是*x*[ij]
    = *x*[ji]。
- en: Directed graphs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有向图
- en: '**Directed graphs**, in contrast to undirected graphs discussed in the previous
    section, connect nodes via *directed* edges. Mathematically they are defined in
    the same way as an undirected graph, except that *E*, the set of edges, is a set
    of *ordered* pairs. Therefore, element *x*[ij] of **A** does need not equal *x*[ji].'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**有向图**与前一节讨论的无向图相比，通过*有向*边连接节点。在数学上，它们的定义方式与无向图相同，除了边集*E*是有序对的集合。因此，矩阵**A**的元素*x*[ij]不一定等于*x*[ji]。'
- en: An example of a directed graph is a citation network, where nodes are publications
    and edges from a node are directed toward the nodes of papers that a given paper
    cited.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有向图的一个示例是引用网络，其中节点是出版物，从一个节点到另一个节点的边是指一篇给定论文引用的其他论文的节点。
- en: '![Diagram  Description automatically generated](img/B17582_18_03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示描述](img/B17582_18_03.png)'
- en: 'Figure 18.3: An example of a directed graph'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3：一个有向图的示例
- en: Labeled graphs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记图
- en: Many graphs we are interested in working with have additional information associated
    with each of their nodes and edges. For example, if you consider the caffeine
    molecule shown earlier, molecules can be represented as graphs where each node
    is a chemical element (for example, O, C, N, or H atoms) and each edge is the
    type of bond (for example, single or double bond) between its two nodes. These
    node and edge features need to be encoded in some capacity. Given graph *G*, defined
    by the node set and edge set tuple (*V*, *E*), we define a |*V*|×*f*[V] node feature
    matrix **X**, where *f*[V] is the length of the label vector of each node. For
    edge labels, we define an |*E*|×*f*[E] edge feature matrix **X**[E], where *f*[E]
    is the length of the label vector of each edge.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的许多图形都与每个节点和边相关的附加信息有关。例如，如果考虑前面显示的咖啡因分子，分子可以被表示为图形，其中每个节点都是化学元素（例如，O、C、N或H原子），每条边都是连接其两个节点的键的类型（例如，单键或双键）。这些节点和边的特征需要以某种方式进行编码。给定图形*G*，由节点集和边集元组（*V*，*E*）定义，我们定义一个|*V*|×*f*[V]节点特征矩阵**X**，其中*f*[V]是每个节点标签向量的长度。对于边标签，我们定义一个|*E*|×*f*[E]边特征矩阵**X**[E]，其中*f*[E]是每个边标签向量的长度。
- en: Molecules are an excellent example of data that can be represented as a **labeled
    graph**, and we will be working with molecular data throughout the chapter. As
    such, we will take this opportunity to cover their representation in detail in
    the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 分子是可以表示为**标记图**的数据的一个很好的例子，在本章中我们将一直使用分子数据。因此，我们将利用这个机会在下一节详细讨论它们的表示。
- en: Representing molecules as graphs
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将分子表示为图形
- en: As a chemical overview, molecules can be thought of as groups of atoms held
    together by chemical bonds. There are different atoms corresponding to different
    chemical elements, for example, common elements include carbon (C), oxygen (O),
    nitrogen (N), and hydrogen (H). Also, there are different kinds of bonds that
    form the connection between atoms, for example, single or double bonds.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作为化学概述，分子可以被看作由化学键结合在一起的原子组。有不同的原子对应于不同的化学元素，例如常见的元素包括碳（C）、氧（O）、氮（N）和氢（H）。此外，有不同类型的化学键形成原子之间的连接，例如单键或双键。
- en: 'We can represent a molecule as an undirected graph with a node label matrix,
    where each row is a one-hot encoding of the associated node’s atom type. Additionally,
    there is an edge label matrix where each row is a one-hot encoding of the associated
    edge’s bond type. To simplify this representation, hydrogen atoms are sometimes
    made implicit since their location can be inferred with basic chemical rules.
    Considering the caffeine molecule we saw earlier, an example of a graph representation
    with implicit hydrogen atoms is shown in *Figure 18.4*:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将分子表示为具有节点标签矩阵的无向图，其中每行是相关节点原子类型的 one-hot 编码。此外，还有一条边标签矩阵，其中每行是相关边键类型的 one-hot
    编码。为了简化这种表示，有时会隐含氢原子，因为它们的位置可以通过基本化学规则推断出来。考虑之前看到的咖啡因分子，一个具有隐含氢原子图形表示的示例如 *图 18.4*
    所示：
- en: '![Diagram  Description automatically generated with medium confidence](img/B17582_18_04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![具有中等置信度自动生成的图表描述](img/B17582_18_04.png)'
- en: 'Figure 18.4: Graph representation of a caffeine molecule'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.4：咖啡因分子的图形表示
- en: Understanding graph convolutions
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解图卷积
- en: The previous section showed how graph data can be represented. The next logical
    step is to discuss what tools we have that can effectively utilize those representations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节展示了如何表示图数据。下一个逻辑步骤是讨论我们可以有效利用这些表示的工具。
- en: In the following subsections, we will introduce graph convolutions, which are
    the key component for building GNNs. In this section, we’ll see why we want to
    use convolutions on graphs and discuss what attributes we want those convolutions
    to have. We’ll then introduce graph convolutions through an implementation example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将介绍图卷积，它是构建 GNNs 的关键组成部分。在本节中，我们将看到为什么要在图上使用卷积，并讨论我们希望这些卷积具有哪些属性。然后，我们将通过一个实现示例介绍图卷积。
- en: The motivation behind using graph convolutions
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用图卷积的动机
- en: To help explain graph convolutions, let’s briefly recap how convolutions are
    utilized in convolutional neural networks (CNNs), which we discussed in *Chapter
    14*, *Classifying Images with Deep Convolutional Neural Networks*. In the context
    of images, we can think of a convolution as the process of sliding a convolutional
    filter over an image, where, at each step, a weighted sum is computed between
    the filter and the receptive field (the part of the image it is currently on top
    of).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解释图卷积，让我们简要回顾卷积神经网络（CNNs）中如何使用卷积，我们在 *第 14 章* *使用深度卷积神经网络分类图像* 中讨论过。在图像的上下文中，我们可以将卷积视为将卷积滤波器在图像上滑动的过程，其中每一步都在滤波器和接受域（它当前所在的图像部分）之间计算加权和。
- en: 'As discussed in the CNN chapter, the filter can be viewed as a detector for
    a specific feature. This approach to feature detection is well-suited for images
    for several reasons, for instance, the following priors we can place on image
    data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 CNN 章节中讨论的那样，滤波器可以看作是特定特征的检测器。这种特征检测方法对图像非常适用，原因有几点，例如我们可以对图像数据施加以下先验：
- en: '**Shift-invariance**: We can still recognize a feature in an image regardless
    of where it is located (for example, after translation). A cat can be recognized
    as a cat whether it is in the top left, bottom right, or another part of an image.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**平移不变性**：我们可以在图像中任意位置识别特征（例如平移后）。猫可以被识别为猫，无论它位于图像的左上角、右下角还是其他部位。'
- en: '**Locality**: Nearby pixels are closely related.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**局部性**：附近的像素是密切相关的。'
- en: '**Hierarchy**: Larger parts of an image can often be broken down into combinations
    of associated smaller parts. A cat has a head and legs; the head has eyes and
    a nose; the eyes have pupils and irises.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**层次结构**：图像的较大部分通常可以分解为相关较小部分的组合。猫有头和腿；头部有眼睛和鼻子；眼睛有瞳孔和虹膜。'
- en: Interested readers can find a more formal description of these priors, and priors
    assumed by GNNs, in the 2019 article *Understanding the Representation Power of
    Graph Neural Networks in Learning Graph Topology*, by *N. Dehmamy*, *A.-L. Barabasi*,
    and *R. Yu* ([https://arxiv.org/abs/1907.05008](https://arxiv.org/abs/1907.05008)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对这些先验和 GNNs 所假设的先验有更正式描述感兴趣的读者，可以参考 2019 年文章 *理解图神经网络在学习图拓扑中的表示能力*，作者是 *N.
    Dehmamy*、*A.-L. Barabasi* 和 *R. Yu* ([https://arxiv.org/abs/1907.05008](https://arxiv.org/abs/1907.05008))。
- en: Another reason convolutions are well-suited for processing images is that the
    number of trainable parameters does not depend on the dimensionality of the input.
    You could train a series of 3×3 convolutional filters on, for example, a 256×256
    or a 9×9 image. (However, if the same image is presented in different resolutions,
    the receptive fields and, therefore, the extracted features will differ. And for
    higher-resolution images, we may want to choose larger kernels or add additional
    layers to extract useful features effectively.)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个卷积适合处理图像的原因是可训练参数的数量不依赖于输入的维度。例如，你可以在256×256或9×9的图像上训练一系列3×3的卷积滤波器。（然而，如果同一图像以不同分辨率呈现，则感受野和因此提取的特征将不同。对于更高分辨率的图像，我们可能希望选择更大的核或添加额外的层以有效提取有用的特征。）
- en: Like images, graphs also have natural priors that justify a convolutional approach.
    Both kinds of data, images and graphs, share the locality prior. However, how
    we define locality differs. In images, the prior is on locality in 2D space, while
    with graphs, it is structural locality. Intuitively, this means that a node that
    is one edge away is more likely to be related than a node five edges away. For
    example, in a citation graph, a directly cited publication, which would be one
    edge away, is more likely to have similar subject matter than a publication with
    multiple degrees of separation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 像图像一样，图也有自然的先验，可以证明卷积方法的合理性。图像数据和图数据都共享局部性先验。但是，我们如何定义局部性是不同的。在图像中，局部性是在二维空间中定义的，而在图中，它是结构上的局部性。直觉上，这意味着距离一个边的节点更有可能相关，而距离五个边的节点不太可能相关。例如，在引文图中，直接引用的出版物，即距离一个边的出版物，更有可能与相似主题的出版物相关，而与多度分离的出版物则不太相关。
- en: 'A strict prior for graph data is **permutation invariance**, which means that
    the ordering of the nodes does not affect the output. This is illustrated in *Figure
    18.5*, where changing the ordering of a graph’s nodes does not change the graph’s
    structure:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据的一个严格先验是**置换不变性**，这意味着节点的排序不会影响输出。这在*图 18.5*中有所说明，改变图的节点排序不会改变图的结构：
- en: '![A picture containing chart  Description automatically generated](img/B17582_18_05.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片的描述 自动生成](img/B17582_18_05.png)'
- en: 'Figure 18.5: Different adjacency matrices representing the same graph'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5：表示同一图的不同邻接矩阵
- en: Since the same graph can be represented by multiple adjacency matrices, as illustrated
    in *Figure 18.5*, consequently, any graph convolution needs to be permutation
    invariant.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于同一图可以用多个邻接矩阵表示，正如*图 18.5*所示，因此任何图卷积都需要是置换不变的。
- en: A convolutional approach is also desirable for graphs because it can function
    with a fixed parameter set for graphs of different sizes. This property is arguably
    even more important for graphs than images. For instance, there are many image
    datasets with a fixed resolution where a fully connected approach (for example,
    using a multilayer perceptron) could be possible, as we have seen in *Chapter
    11*, *Implementing a Multilayer Artificial Neural Network from Scratch*. In contrast,
    most graph datasets contain graphs of varying sizes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图来说，卷积方法也是理想的，因为它可以使用固定的参数集处理不同大小的图。这个性质对于图而言可能比图像更为重要。例如，有许多具有固定分辨率的图像数据集，可以使用全连接方法（例如使用多层感知机），正如我们在*第11章*，*从头开始实现多层人工神经网络*中所见。相反，大多数图数据集包含不同大小的图。
- en: While image convolutional operators are standardized, there are many different
    kinds of graph convolutions, and the development of new graph convolutions is
    a very active area of research. Our focus is on providing general ideas so that
    readers can rationalize about the GNNs they wish to utilize. To this end, the
    following subsection will show how to implement a basic graph convolution in PyTorch.
    Then, in the next section, we will construct a simple GNN in PyTorch from the
    ground up.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图像卷积运算符是标准化的，但是图卷积有许多不同种类，并且新图卷积的开发是一个非常活跃的研究领域。我们的重点是提供一般的思路，以便读者可以理解他们希望利用的图神经网络。为此，接下来的小节将展示如何在PyTorch中实现基本的图卷积。然后，在下一节中，我们将从头开始在PyTorch中构建一个简单的GNN。
- en: Implementing a basic graph convolution
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基本的图卷积
- en: 'In this subsection, we will introduce a basic graph convolution function and
    see what happens when it is applied to a graph. Consider the following graph and
    its representation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍一个基本的图卷积函数，并看看当它应用于一个图时会发生什么。考虑以下图及其表示：
- en: '![Chart, line chart  Description automatically generated](img/B17582_18_06.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图表，线图  自动生成描述](img/B17582_18_06.png)'
- en: 'Figure 18.6: A representation of a graph'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18.6: 图的表示'
- en: '*Figure 18.6* depicts an undirected graph with node labels specified by an
    *n*×*n* adjacency matrix **A** and *n*×*f*[in] node feature matrix **X**, where
    the only feature is a one-hot representation of each node’s color—green (G), blue
    (B), or orange (O).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 18.6* 描述了一个无向图，节点标签由 *n*×*n* 邻接矩阵 **A** 和 *n*×*f*[in] 节点特征矩阵 **X** 指定，其中唯一的特征是每个节点的颜色的
    one-hot 表示—绿色（G）、蓝色（B）或橙色（O）。'
- en: One of the most versatile libraries for graph manipulation and visualization
    is NetworkX, which we will be using to illustrate how to construct graphs from
    a label matrix **X** and a node matrix **A**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理和可视化中最多功能的库之一是 NetworkX，我们将使用它来说明如何从标签矩阵 **X** 和节点矩阵 **A** 构建图形。
- en: '**Installing NetworkX**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 NetworkX**'
- en: 'NetworkX is a handy Python library for manipulating and visualizing graphs.
    It can be installed via `pip`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: NetworkX 是一个方便的 Python 库，用于处理和可视化图形。可以通过 `pip` 安装：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We used version 2.6.2 to create the graph visualizations in this chapter. For
    more information, please visit the official website at [https://networkx.org](https://networkx.org).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用版本 2.6.2 来创建本章中的图形可视化。更多信息，请访问官方网站 [https://networkx.org](https://networkx.org)。
- en: 'Using NetworkX, we can construct the graph shown in *Figure 18.6* as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NetworkX，我们可以按如下方式构造图形：
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To draw the graph constructed in the preceding code, we can then use the following
    code:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制前述代码构造的图形，我们可以使用以下代码：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code example, we first initiated a new `Graph` object from
    NetworkX. We then added nodes 1 to 4 together with color specifications for visualization.
    After adding the nodes, we specified their connections (edges). Using the `adjacency_matrix`
    constructor from NetworkX, we create the adjacency matrix **A**, and our custom
    `build_graph_color_label_representation` function creates the node label matrix
    **X** from the information we added to the `Graph` object earlier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码示例中，我们首先从 NetworkX 初始化了一个新的 `Graph` 对象。然后，我们添加了节点 1 到 4，并指定了用于可视化的颜色规范。在添加节点后，我们指定了它们的连接（边）。使用
    NetworkX 的 `adjacency_matrix` 构造函数，我们创建了邻接矩阵 **A**，并且我们的自定义 `build_graph_color_label_representation`
    函数从我们之前添加到 `Graph` 对象的信息创建了节点标签矩阵 **X**。
- en: 'With graph convolutions, we can interpret each row of **X** as being an embedding
    of the information that is stored at the node corresponding to that row. Graph
    convolutions update the embeddings at each node based on the embeddings of their
    neighbors and themselves. For our example implementation, the graph convolution
    will take the following form:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图卷积，我们可以将 **X** 的每一行解释为存储在相应节点上的信息的嵌入。图卷积根据其邻居节点和自身的嵌入更新每个节点的嵌入。对于我们的示例实现，图卷积将采用以下形式：
- en: '![](img/B17582_18_001.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_001.png)'
- en: Here, ![](img/B17582_18_002.png) is the updated embedding for node *i*; **W**[1]
    and **W**[2] are *f*[in]×*f*[out] matrices of learnable filter weights; and *b*
    is a learnable bias vector of length *f*[out].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_18_002.png) 是节点 *i* 的更新嵌入；**W**[1] 和 **W**[2] 是 *f*[in]×*f*[out]
    的可学习滤波器权重矩阵；*b* 是长度为 *f*[out] 的可学习偏置向量。
- en: 'The two weight matrices **W**[1] and **W**[2] can be considered filter banks,
    where each column is an individual filter. Note that this filter design is most
    effective when the locality prior on graph data holds. If a value at a node is
    highly correlated with the value at another node many edges away, a single convolution
    will not capture that relationship. Stacking convolutions will capture more distant
    relationships, as illustrated in *Figure 18.7* (we set the bias to zero for simplicity):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 两个权重矩阵 **W**[1] 和 **W**[2] 可以被视为滤波器组，其中每一列都是一个单独的滤波器。请注意，当图数据上的局部性先验成立时，这种滤波器设计是最有效的。如果一个节点的值与另一个节点的值高度相关，而这两个节点之间存在许多边，单个卷积将无法捕捉到这种关系。堆叠卷积将捕捉更远的关系，如图
    18.7 所示（为简化起见，我们将偏置设为零）：
- en: '![Diagram  Description automatically generated](img/B17582_18_07.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图表  自动生成描述](img/B17582_18_07.png)'
- en: 'Figure 18.7: Capturing relationships from a graph'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18.7: 从图中捕获关系'
- en: 'The design of the graph convolution illustrated in *Figure 18.7* fits our priors
    on graph data, but it may not be clear how to implement the sum over neighbors
    in matrix form. This is where we utilize the adjacency matrix **A**. The matrix
    form of this convolution is **XW**[1] + **AXW**[2]. Here, the adjacency matrix,
    consisting of 1s and 0s, acts as a mask to select nodes and compute the desired
    sums. In NumPy, initializing this layer and computing a forward pass on the previous
    graph could be written as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7中展示的图卷积的设计符合我们对图数据的先验假设，但如何以矩阵形式实现邻居节点之间的求和可能不是很明确。这就是我们利用邻接矩阵 **A** 的地方。这个卷积的矩阵形式是
    **XW**[1] + **AXW**[2]。在NumPy中，初始化这一层并在前一个图上进行前向传播可以写成如下形式：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Computing a forward pass of a graph convolution is that easy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图卷积的前向传播就是这么简单。
- en: Ultimately, we want a graph convolutional layer to update the representation
    of the node information encoded in **X** by utilizing the structural (connectivity)
    information provided by **A**. There are many potential ways to do this, and this
    plays out in the numerous kinds of graph convolutions that have been developed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们希望图卷积层通过利用由 **A** 提供的结构信息（连接性）来更新编码在 **X** 中的节点信息的表示。有许多潜在的方法可以做到这一点，这在已开发的许多类型的图卷积中体现出来。
- en: To talk about different graph convolutions, generally, it would be nice for
    them to have a unifying framework. Thankfully, such a framework was presented
    in *Neural Message Passing for Quantum Chemistry* by *Justin Gilmer* and colleagues,
    2017, [https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要讨论不同的图卷积，通常最好是它们具有一个统一的框架。幸运的是，*Justin Gilmer*和他的同事在2017年的《*神经信息传递用于量子化学*》中提出了这样一个框架，[https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212)。
- en: In this **message-passing** framework, each node in the graph has an associated
    hidden state ![](img/B17582_18_003.png), where *i* is the node’s index at time
    step *t*. The initial value ![](img/B17582_18_004.png) is defined as **X**[i],
    which is the row of **X** associated with node *i*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个**消息传递**框架中，图中的每个节点都有一个关联的隐藏状态 ![](img/B17582_18_003.png)，其中 *i* 是节点在时间步
    *t* 的索引。初始值 ![](img/B17582_18_004.png) 定义为 **X**[i]，即与节点 *i* 相关的 **X** 的行。
- en: 'Each graph convolution can be split into a message-passing phase and a node
    update phase. Let *N*(*i*) be the neighbors of node *i*. For undirected graphs,
    *N*(*i*) is the set of nodes that share an edge with node *i*. For directed graphs,
    *N*(*i*) is the set of nodes that have an edge whose endpoint is node *i*. The
    message-passing phase can be formulated as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图卷积可以分为消息传递阶段和节点更新阶段。设 *N*(*i*) 为节点 *i* 的邻居。对于无向图，*N*(*i*) 是与节点 *i* 共享边的节点集合。对于有向图，*N*(*i*)
    是具有以节点 *i* 为端点的边的节点集合。消息传递阶段可以表述如下：
- en: '![](img/B17582_18_005.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_005.png)'
- en: Here, *M*[t] is a message function. In our example layer, we define this message
    function as ![](img/B17582_18_006.png). The node update phase with the update
    function *U*[t] is ![](img/B17582_18_007.png). In our example layer, this update
    is ![](img/B17582_18_008.png).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*M*[t] 是一个消息函数。在我们的示例层中，我们将此消息函数定义为 ![](img/B17582_18_006.png)。使用更新函数 *U*[t]
    的节点更新阶段为 ![](img/B17582_18_007.png)。在我们的示例层中，此更新为 ![](img/B17582_18_008.png)。
- en: '*Figure 18.8* visualizes the message-passing idea and summarizes the convolution
    we have implemented:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*图18.8* 展示了消息传递的思想并总结了我们实现的卷积：'
- en: '![Diagram  Description automatically generated](img/B17582_18_08.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的图示说明](img/B17582_18_08.png)'
- en: 'Figure 18.8: The convolutions implemented on the graph and the message form'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8：在图上实现的卷积和消息形式
- en: In the next section, we’ll incorporate this graph convolution layer into a GNN
    model implemented in PyTorch.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把这个图卷积层整合到一个在PyTorch中实现的GNN模型中。
- en: Implementing a GNN in PyTorch from scratch
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中从零开始实现GNN
- en: The previous section focused on understanding and implementing a graph convolution
    operation. In this section, we’ll walk you through a basic implementation of a
    graph neural network to illustrate how to apply these methods to graphs if you
    start from scratch. If this approach appears complicated, don’t worry; GNNs are
    relatively complex models to implement. Thus, we’ll introduce PyTorch Geometric
    in a later section, which provides tools to ease the implementation of, and the
    data management for, graph neural networks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节重点介绍了理解和实现图卷积操作。在本节中，我们将通过一个基本的图神经网络实现来演示如何从头开始应用这些方法到图形中。如果这种方法看起来复杂，不用担心；GNN是相对复杂的模型。因此，我们将在后面的章节中介绍PyTorch
    Geometric，它提供了工具来简化图神经网络的实现和数据管理。
- en: Defining the NodeNetwork model
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义NodeNetwork模型
- en: 'We will start this section by showing a PyTorch from-scratch implementation
    of a GNN. We will take a top-down approach, starting with the main neural network
    model, which we call `NodeNetwork`, and then we will fill in the individual details:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从头开始展示一个PyTorch实现的GNN的部分。我们将采取自顶向下的方法，从主神经网络模型开始，我们称之为`NodeNetwork`，然后填充具体细节：
- en: '[PRE4]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `NodeNetwork` model we just defined can be summarized as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚定义的`NodeNetwork`模型可以总结如下：
- en: Perform two graph convolutions (`self.conv_1` and `self.conv_2`)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行两个图卷积（`self.conv_1`和`self.conv_2`）
- en: Pool all the node embeddings via `global_sum_pool`, which we will define later
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`global_sum_pool`汇总所有节点嵌入，稍后我们将定义
- en: Run the pooled embeddings through two fully connected layers (`self.fc_1` and
    `self.out_layer`)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过两个全连接层（`self.fc_1`和`self.out_layer`）运行池化嵌入
- en: Output a class-membership probability via softmax
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过softmax输出类成员概率
- en: 'The structure of the network along with a visualization of what each layer
    is doing is summarized in *Figure 18.9*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构以及每个层所做的可视化总结在*Figure 18.9*中：
- en: '![Diagram  Description automatically generated](img/B17582_18_09.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_18_09.png)'
- en: 'Figure 18.9: A visualization of each neural network layer'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图18.9: 每个神经网络层的可视化'
- en: The individual aspects, such as the graph convolution layers and global pooling,
    will be discussed in the next subsections.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 各个方面，如图卷积层和全局池化，将在接下来的小节中讨论。
- en: Coding the NodeNetwork’s graph convolution layer
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码NodeNetwork的图卷积层
- en: Now, let’s define the graph convolution operation (`BasicGraphConvolutionLayer`)that
    was used inside the previous `NodeNetwork` class*:*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义图卷积操作（`BasicGraphConvolutionLayer`），这在之前的`NodeNetwork`类中使用过*:*
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As with fully connected layers and image convolutional layers, we add a bias
    term so that the intercept of the linear combination of the layer outputs (prior
    to the application of a nonlinearity like ReLU) can vary. The `forward()` method
    implements the matrix form of the forward pass, which we discussed in the previous
    subsection, with the addition of a bias term.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与全连接层和图像卷积层一样，我们添加偏置项，以便调整层输出的线性组合的截距（在应用非线性如ReLU之前）。`forward()`方法实现了前向传播的矩阵形式，我们在前一小节中讨论过，同时加入了一个偏置项。
- en: 'To try out the `BasicGraphConvolutionLayer`, let’s apply it to the graph and
    adjacency matrix that we defined in the section *Implementing a basic graph convolution*
    previously:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要尝试`BasicGraphConvolutionLayer`，让我们将其应用到我们之前在*实现基本图卷积*节中定义的图和邻接矩阵上：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Based on the code example above, we can see that our `BasicGraphConvolutionLayer`
    converted the four-node graph consisting of three features into a representation
    with eight features.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述代码示例，我们可以看到我们的`BasicGraphConvolutionLayer`将由三个特征组成的四节点图转换为具有八个特征的表示形式。
- en: Adding a global pooling layer to deal with varying graph sizes
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加全局池化层以处理不同大小的图
- en: Next, we define the`global_sum_pool()` function that was used in the `NodeNetwork`
    class, where `global_sum_pool()` implements a global pooling layer. Global pooling
    layers aggregate all of a graph’s node embeddings into a fixed-sized output. As
    shown in *Figure 18.9*, `global_sum_pool()`sums all the node embeddings of a graph.
    We note that this global pooling is relatively similar to the global average pooling
    used in CNNs, which is used before the data is run through fully connected layers,
    as we have seen in *Chapter 14*, *Classifying Images with Deep Convolutional Neural
    Networks*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了在`NodeNetwork`类中使用的`global_sum_pool()`函数，其中`global_sum_pool()`实现了一个全局池化层。全局池化层将图的所有节点嵌入聚合为一个固定大小的输出。如图*图
    18.9*所示，`global_sum_pool()`对图的所有节点嵌入求和。我们注意到，这种全局池化与CNN中使用的全局平均池化相对类似，后者在数据通过全连接层之前使用，正如我们在*第14章*中看到的，*用深度卷积神经网络对图像进行分类*。
- en: 'Summing all the node embeddings results in a loss of information, so reshaping
    the data would be preferable, but since graphs can have different sizes, this
    is not feasible. Global pooling can be done with any permutation invariant function,
    for example, `sum`, `max`, and `mean`. Here is the implementation of `global_sum_pool()`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有节点嵌入求和会导致信息丢失，因此更好的做法是重新整形数据，但由于图可以具有不同的大小，这是不可行的。全局池化可以使用任何排列不变的函数，例如`sum`、`max`和`mean`。这里是`global_sum_pool()`的实现：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If data is not batched or the batch size is one, this function just sums over
    the current node embeddings. Otherwise, the embeddings are multiplied with `batch_mat`,
    which has a structure based on how graph data is batched.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据没有批处理或批处理大小为一，此函数只是对当前节点嵌入求和。否则，嵌入将与`batch_mat`相乘，其结构基于图数据的批处理方式。
- en: 'When all data in a dataset has the same dimensionality, batching the data is
    as straightforward as adding a dimension by stacking the data. (Side note: the
    function called in the default batching function in PyTorch is literally called
    `stack`.) Since graph sizes vary, this approach is not feasible with graph data
    unless padding is used. However, padding can be inefficient in cases where graph
    sizes can vary substantially. Usually, the better way to deal with varying graph
    sizes is to treat each batch as a single graph where each graph in the batch is
    a subgraph that is disconnected from the rest. This is illustrated in *Figure
    18.10*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集中的所有数据具有相同的维度时，批处理数据就像通过堆叠数据添加一个维度一样简单。（附注：在PyTorch中默认的批处理函数中调用的函数实际上称为`stack`。）由于图的大小各不相同，除非使用填充，否则这种方法在处理图数据时是不可行的。然而，在图的大小差异显著时，填充可能效率低下。通常，处理不同大小的图的更好方法是将每个批次视为单个图，其中每个批次中的图是与其余图断开连接的子图。这在*图
    18.10*中有所说明：
- en: '![Diagram  Description automatically generated](img/B17582_18_10.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_18_10.png)'
- en: 'Figure 18.10: How to deal with varying graph sizes'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '图 18.10: 处理不同大小的图的方法'
- en: To describe *Figure 18.10* more formally, suppose we are given graphs *G*[1], ..., *G*[k]
    of sizes *n*[1], ..., *n*[k] with *f* features per node. In addition, we are given
    the corresponding adjacency matrices **A**[1], ..., **A**[k] and feature matrices
    **X**[1], ..., **X**[k]. Let *N* be the total number of nodes, ![](img/B17582_18_009.png),
    *s*[1] = 0, and *s*[i] = *s*[i][–1] + *n*[i][–1] for ![](img/B17582_18_010.png).
    As shown in the figure, we define a graph *G*[B] with *N*×*N* adjacency matrix
    **A**[B] and *N*×*f* feature matrix **X**[B]. Using Python index notation, **A**[B][*s*[i]:*s*[i] + *n*[i], *s*[i] + *n*[i]] = **A**[i],
    and all other elements of **A**[B] outside these index sets are 0\. Additionally,
    **X**[B][*s*[i]:*s*[i] + *n*[i], :] = **X**[i].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更正式地描述*图 18.10*，假设我们有大小为*n*[1]，...，*n*[k]的图*G*[1]，...，*G*[k]，每个节点有*f*个特征。此外，我们还有相应的邻接矩阵**A**[1]，...，**A**[k]和特征矩阵**X**[1]，...，**X**[k]。设*N*为节点的总数，![](img/B17582_18_009.png)，*s*[1]
    = 0，且对于![](img/B17582_18_010.png)，*s*[i] = *s*[i-1] + *n*[i-1]。如图所示，我们定义了一个具有*N*×*N*邻接矩阵**A**[B]和*N*×*f*特征矩阵**X**[B]的图*G*[B]。使用Python索引表示，**A**[B][*s*[i]:*s*[i]+*n*[i],
    *s*[i]:*s*[i]+*n*[i]] = **A**[i]，并且**A**[B]的其他元素在这些索引集之外都是0。此外，**X**[B][*s*[i]:*s*[i]+*n*[i],
    :] = **X**[i]。
- en: By design, disconnected nodes will never be in the same receptive field of a
    graph convolution. As a result, when backpropagating gradients of *G*[B] through
    graph convolutions, the gradients attached to each graph in the batch will be
    independent. This means that if we treat a set of graph convolutions as a function
    *f*, if *h*[B] = *f*(*X*[B], *A*[B]) and *h*[i] = *f*(*X*[i], *A*[i]), then *h*[B][*s*[i]:*s*[i] + *n*, :] = *h*[i].
    If the sum global pooling extracts the sums of each *h*[i] from *h*[B] as separate
    vectors, passing that stack of vectors through fully connected layers would keep
    the gradients of each item in the batch separate throughout the entire backpropagation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据设计，断开连接的节点永远不会在图卷积的同一接收场中。因此，当通过图卷积反向传播*G*[B]的梯度时，批次中每个图附加的梯度将是独立的。这意味着，如果我们将一组图卷积视为函数*f*，如果*h*[B]
    = *f*(*X*[B], *A*[B])和*h*[i] = *f*(*X*[i], *A*[i])，那么*h*[B][*s*[i]:*s*[i] + *n*,
    :] = *h*[i]。如果总和全局池从*h*[B]中提取*h*[i]的各自向量的总和，并通过完全连接的层传递该向量堆栈，则在整个反向传播过程中将保持批次中每个项目的梯度独立。
- en: 'This is the purpose of `batch_mat` in `global_sum_pool()`—to serve as a graph
    selection mask that keeps the graphs in the batch separate. We can generate this
    mask for graphs of sizes *n*[1], ..., *n*[k] with the following code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`global_sum_pool()`中`batch_mat`的目的——作为一个图选择掩码，用于保持批次中的图分开。我们可以使用以下代码为大小为*n*[1],
    ..., *n*[k]的图生成此掩码：
- en: '[PRE8]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Thus, given a batch size, *b*, `batch_mat` is a *b*×*N* matrix where batch_mat[*i*–1, *s*[i]:*s*[i] + *n*[i]] = 1
    for ![](img/B17582_18_011.png) and where elements outside these index sets are
    0\. The following is a collate function for constructing a representation of some
    *G*[B] and a corresponding batch matrix:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定批次大小*b*，`batch_mat`是一个*b*×*N*矩阵，其中batch_mat[*i*–1, *s*[i]:*s*[i] + *n*[i]]
    = 1对于![](img/B17582_18_011.png)，并且在这些索引集之外的元素为0。以下是构建某些*G*[B]表示和相应的批次矩阵的整理函数：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Preparing the DataLoader
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备DataLoader
- en: In this section, we will see how the code from the previous subsections all
    comes together. First, we will generate some graphs and put them into a PyTorch
    `Dataset`. Then, we will use our `collate` function in a `DataLoader` for our
    GNN.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到前面各小节中的代码如何结合在一起。首先，我们将生成一些图并将它们放入PyTorch `Dataset`中。然后，我们将在我们的GNN中使用`collate`函数在`DataLoader`中使用它。
- en: 'But before we define the graphs, let’s implement a function that builds a dictionary
    representation that we will use later:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们定义图之前，让我们实现一个函数来构建一个字典表示，稍后我们将使用它：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This function takes a NetworkX graph and returns a dictionary containing its
    adjacency matrix `A`, its node feature matrix `X`, and a binary label `y`. Since
    we won’t actually be training this model on a real-world task, we just set the
    labels arbitrarily. Then, `nx.adjacency_matrix()` takes a NetworkX graph and returns
    a sparse representation that we convert to a dense `np.array` form using `todense()`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受一个NetworkX图，并返回一个包含其邻接矩阵`A`，节点特征矩阵`X`和一个二进制标签`y`的字典。由于我们实际上不会在真实任务中训练这个模型，所以我们只是任意设置标签。然后，`nx.adjacency_matrix()`接受一个NetworkX图并返回一个稀疏表示，我们使用`todense()`将其转换为稠密的`np.array`形式。
- en: 'We’ll now construct graphs and use the `get_graph_dict` function to convert
    NetworkX graphs to a format our network can handle:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将构造图，并使用`get_graph_dict`函数将NetworkX图转换为我们的网络可以处理的格式：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The graphs this code generates are visualized in *Figure 18.11*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成的图在*Figure 18.11*中可视化：
- en: '![](img/B17582_18_11.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_11.png)'
- en: 'Figure 18.11: Four generated graphs'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.11：生成的四个图
- en: This code block constructs four NetworkX graphs and stores them in a list. Here,
    the constructor of `nx.Graph()` initializes an empty graph, and `add_nodes_from()`
    adds nodes to the empty graph from a list of tuples. The first item in each tuple
    is the node’s name, and the second item is a dictionary of that node’s attributes.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码块构造了四个NetworkX图并将它们存储在一个列表中。在这里，`nx.Graph()`的构造函数初始化了一个空图，而`add_nodes_from()`从一个元组列表中将节点添加到空图中。每个元组中的第一个项目是节点的名称，第二个项目是该节点属性的字典。
- en: 'The `add_edges_from()` method of a graph takes a list of tuples where each
    tuple defines an edge between its elements (nodes). Now, we can construct a PyTorch
    `Dataset` for these graphs:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图的`add_edges_from()`方法接受一个元组列表，其中每个元组定义其元素（节点）之间的边。现在，我们可以为这些图构建一个PyTorch `Dataset`：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'While using a custom `Dataset` may seem like unnecessary effort, it allows
    us to exhibit how `collate_graphs()` can be used in a `DataLoader`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用自定义`Dataset`可能看起来是不必要的工作，但它允许我们展示如何在`DataLoader`中使用`collate_graphs()`：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using the NodeNetwork to make predictions
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NodeNetwork进行预测
- en: 'After we have defined all the necessary functions and set up the `DataLoader`,
    we now initialize a new `NodeNetwork` and apply it to our graph data:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义了所有必要的函数并设置了 `DataLoader` 后，我们现在初始化一个新的 `NodeNetwork` 并将其应用于我们的图数据：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that for brevity, we didn’t include a training loop; however, the GNN model
    could be trained in a regular fashion by computing the loss between predicted
    and true class labels, backpropagating the loss via `.backward()`, and updating
    the model weights via a gradient descent-based optimizer. We leave this as an
    optional exercise for the reader. In the next section, we will show how to do
    that with a GNN implementation from PyTorch Geometric, which implements more sophisticated
    GNN code.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为简洁起见，我们没有包括训练循环；然而，可以通过计算预测和真实类标签之间的损失，通过 `.backward()` 反向传播损失，并通过基于梯度下降的优化器更新模型权重来以常规方式训练
    GNN 模型。我们将此留作读者的可选练习。在下一节中，我们将展示如何使用 PyTorch Geometric 实现 GNN，该库实现了更复杂的 GNN 代码。
- en: 'To continue with our previous code, let’s now provide a single input graph
    to the model directly without the `DataLoader`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 要继续我们之前的代码，现在让我们直接向模型提供一个单一的输入图，而不使用 `DataLoader`：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now compare the results from applying the GNN to a single graph (`G1_single`)
    and to the first graph from the `DataLoader` (also the first graph, `G1`, which
    we guaranteed, since we set `shuffle=False`) to double-check that the batch loader
    works correctly. As we can see by using `torch.isclose()` (to account for rounding
    errors), the results are equivalent, as we would have hoped:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以比较将 GNN 应用于单个图 (`G1_single`) 和从 `DataLoader` 中获取的第一个图（也就是第一个图 `G1`，因为我们设置了
    `shuffle=False`），以确保批处理加载器工作正常。通过使用 `torch.isclose()`（以考虑四舍五入误差），我们可以看到结果是等价的，这是我们希望看到的：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Congrats! You now understand how to construct, set up, and run a basic GNN.
    However, from this introduction, you probably realize that managing and manipulating
    graph data can be somewhat laborious. Also, we didn’t even build a graph convolution
    that uses edge labels, which would complicate matters further. Thankfully, there
    is PyTorch Geometric, a package that makes this much easier by providing implementations
    of many GNN layers. We’ll introduce this library with an end-to-end example of
    implementing and training a more complex GNN on molecule data in the next subsection.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在您了解如何构建、设置和运行基本的 GNN。但是，从本介绍中，您可能意识到管理和操作图数据可能有些繁琐。而且，我们甚至没有构建使用边标签的图卷积，这将进一步复杂化事务。幸运的是，PyTorch
    Geometric 提供了许多 GNN 层的实现，使这一切变得更加简单。在下一小节中，我们将通过在分子数据上实现和训练更复杂的 GNN 的端到端示例来介绍这个库。
- en: Implementing a GNN using the PyTorch Geometric library
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch Geometric 库实现 GNN
- en: In this section, we will implement a GNN using the PyTorch Geometric library,
    which simplifies the process of training GNNs. We apply the GNN to QM9, a dataset
    consisting of small molecules, to predict isotropic polarizability, which is a
    measure of a molecule’s tendency to have its charge distorted by an electric field.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 PyTorch Geometric 库实现 GNN，该库简化了训练 GNN 的过程。我们将 GNN 应用于 QM9 数据集，该数据集由小分子组成，以预测各向同性极化率，这是分子在电场中电荷畸变倾向的一种度量。
- en: '**Installing PyTorch Geometric**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**安装 PyTorch Geometric**'
- en: 'PyTorch Geometric can be installed via conda or pip. We recommend you visit
    the official documentation website at [https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)
    to select the installation command recommended for your operating system. For
    this chapter, we used pip to install version 2.0.2 along with its `torch-scatter`
    and `torch-sparse` dependencies:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 conda 或 pip 安装 PyTorch Geometric。我们建议您访问官方文档网站 [https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)
    选择适合您操作系统的安装命令。在本章中，我们使用 pip 安装了版本 2.0.2 以及其 `torch-scatter` 和 `torch-sparse`
    依赖：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s start by loading a dataset of small molecules and look at how PyTorch
    Geometric stores the data:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载小分子数据集开始，并看看 PyTorch Geometric 如何存储数据：
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `Data` object is a convenient, flexible wrapper for graph data. Note that
    many PyTorch Geometric objects require certain keywords in data objects to process
    them correctly. Specifically, `x` should contain node features, `edge_attr` should
    contain edge features, `edge_index` should include an edge list, and `y`should
    contain labels. The QM9 data contains some additional attributes of note: `pos`,
    the position of each of the molecules’ atoms in a 3D grid, and `z`, the atomic
    number of each atom in the molecule. The labels in the QM9 are a bunch of physical
    properties of the molecules, such as dipole moment, free energy, enthalpy, or
    isotropic polarization. We are going to implement a GNN and train it on QM9 to
    predict isotropic polarization.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`Data`对象是图数据的方便、灵活的包装器。请注意，许多PyTorch Geometric对象要求数据对象中包含某些关键字才能正确处理它们。具体来说，`x`应包含节点特征，`edge_attr`应包含边特征，`edge_index`应包括边列表，而`y`应包含标签。QM9数据还包含一些值得注意的附加属性：`pos`，分子中每个原子在3D网格中的位置，以及`z`，分子中每个原子的原子序数。QM9中的标签是分子的一些物理属性，如偶极矩、自由能、焓或各向同性极化率。我们将实现一个GNN，并在QM9上训练它来预测各向同性极化率。'
- en: '**The QM9 dataset**'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**QM9数据集**'
- en: The QM9 dataset contains 133,885 small organic molecules labeled with several
    geometric, energetic, electronic, and thermodynamic properties. QM9 is a common
    benchmark dataset for developing methods for predicting chemical structure-property
    relationships and hybrid quantum mechanic/machine learning methods. More information
    about the dataset can be found at [http://quantum-machine.org/datasets/](http://quantum-machine.org/datasets/).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: QM9数据集包含133,885个小有机分子，标有几何、能量、电子和热力学性质。 QM9是开发预测化学结构-性质关系和混合量子力学/机器学习方法的常见基准数据集。有关数据集的更多信息，请访问[http://quantum-machine.org/datasets/](http://quantum-machine.org/datasets/)。
- en: The bond types of molecules are important; that is, which atoms are connected
    via a certain bond type, for example, single or double bonds, matters. Hence,
    we’ll want to use a graph convolution that can utilize edge features. For this,
    we’ll use the `torch_geometric.nn.NNConv` layer. (If you are interested in the
    implementation details, its source code be found at [https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/nn_conv.html#NNConv](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/nn_conv.html#NNConv).)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 分子的键类型很重要；即通过某种键类型连接的哪些原子，例如单键或双键，都很重要。因此，我们将使用能够利用边特征的图卷积，例如`torch_geometric.nn.NNConv`层。
    （如果您对实现细节感兴趣，可以在[https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/nn_conv.html#NNConv](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/nn_conv.html#NNConv)找到其源代码。）
- en: 'This convolution in the `NNConv` layer takes the following form:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NNConv`层中，这种卷积采用以下形式：
- en: '![](img/B17582_18_012.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_012.png)'
- en: 'Here, *h* is a neural network parameterized by a set of weights ![](img/B17582_18_013.png),
    and **W** is a weight matrix for the node labels. This graph convolution is very
    similar to the one we implemented previously from scratch:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h*是由一组权重![](img/B17582_18_013.png)参数化的神经网络，而**W**是节点标签的权重矩阵。这种图卷积与我们之前从头实现的非常相似：
- en: '![](img/B17582_18_014.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_014.png)'
- en: 'The only real difference is that the **W**[2] equivalent, the neural network
    *h*, is parametrized based on the edge labels, which allows the weights to vary
    for different edge labels. Via the following code, we implement a GNN utilizing
    two such graph convolutional layers (`NNConv`):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的实际区别在于等效的**W**[2]，即神经网络*h*，是基于边标签参数化的，这允许权重因不同的边标签而变化。通过以下代码，我们实现了一个使用两个这样的图卷积层（`NNConv`）的GNN：
- en: '[PRE19]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We’ll train this GNN to predict a molecule’s isotropic polarizability, a measure
    of the relative tendency of a molecule’s charge distribution to be distorted by
    an external electric field. We’ll split the QM9 dataset into training, validation,
    and test sets, and use PyTorch Geometric `DataLoader`. Note that these do not
    require a special collate function, but require a `Data` object with appropriately
    named attributes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练这个GNN来预测分子的各向同性极化率，这是衡量分子电荷分布相对于外部电场扭曲倾向的相对指标。我们将把QM9数据集分为训练、验证和测试集，并使用PyTorch
    Geometric `DataLoader`。请注意，这些不需要特殊的整理函数，但需要一个具有适当命名属性的`Data`对象。
- en: 'Next, let’s split the dataset:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们分割数据集：
- en: '[PRE20]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following code will initialize and train a network on a GPU (if available):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码将在GPU上初始化并训练网络（如果可用）：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The training loop, shown in the following code, follows the familiar pattern
    we have encountered in previous PyTorch chapters, so we can skip the explanation
    details. However, one detail that is worth highlighting is that here we are computing
    the mean squared error (MSE) loss instead of the cross-entropy, since polarizability
    is a continuous target and not a class label:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环如下面的代码所示，遵循我们在前几个PyTorch章节中遇到的熟悉模式，因此我们可以跳过详细的解释。然而，值得强调的一个细节是，这里我们计算的是均方误差（MSE）损失，而不是交叉熵，因为极化率是一个连续的目标，而不是一个类标签。
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Over the first four training epochs, both training and validation loss are decreasing.
    The dataset is large and may take a little while to train on a CPU, so we stop
    training after four epochs. However, if we train the model further, the loss will
    continue to improve. You can train the model for additional epochs to see how
    that changes the performance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前四个训练时期，训练和验证损失都在减少。数据集很大，在CPU上训练可能需要一点时间，因此我们在四个时期后停止训练。但是，如果进一步训练模型，损失将继续改善。您可以继续训练模型以查看如何改进性能。
- en: 'The following code predicts the values on the test data and collects the true
    labels:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码预测了测试数据上的值并收集了真实标签：
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we can make a scatterplot with a subset of the test data. Since the test
    dataset is relatively large (10,000 molecules), the results can be a bit cluttered,
    and for simplicity, we only plot the first 500 predictions and targets:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用测试数据的子集制作散点图。由于测试数据集相对较大（10,000个分子），结果可能有些混乱，为简单起见，我们仅绘制前500个预测和目标：
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The resulting figure is shown here:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的图示如下：
- en: '![Chart, scatter chart  Description automatically generated](img/B17582_18_12.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图 自动生成描述](img/B17582_18_12.png)'
- en: 'Figure 18.12: Predicted isotropic polarizability plotted against the actual
    isotropic polarizability'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.12：预测的各向同性极化率与实际各向同性极化率的图示
- en: Based on the plot, given that the points lie relatively near the diagonal, our
    simple GNN appears to have done a decent job with predicting isotropic polarization
    values, even without hyperparameter tuning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图，考虑到点相对靠近对角线，我们简单的GNN似乎在预测各向同性极化值时表现不错，即使没有超参数调整。
- en: '**TorchDrug – A PyTorch-based library for drug discovery**'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**TorchDrug – 基于PyTorch的药物发现库**'
- en: 'PyTorch Geometric is a comprehensive general-purpose library for working with
    graphs, including molecules, as you have seen in this section. If you are interested
    in more in-depth molecule work and drug discovery, we also recommend considering
    the recently developed TorchDrug library, which offers many convenient utilities
    for working with molecules. You can find out more about TorchDrug here: [https://torchdrug.ai/](https://torchdrug.ai/).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Geometric是一个全面的通用图形库，用于处理图形，包括分子，正如您在本节中看到的。如果您对更深入的分子工作和药物发现感兴趣，我们还建议考虑最近开发的TorchDrug库，该库提供了许多方便的工具来处理分子。您可以在这里了解更多关于TorchDrug的信息：[https://torchdrug.ai/](https://torchdrug.ai/)。
- en: Other GNN layers and recent developments
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他GNN层和最新发展
- en: This section will introduce a selection of additional layers that you can utilize
    in your GNNs, in addition to providing a high-level overview of some recent developments
    in the field. While we will provide background on the intuition behind these layers
    and their implementations, these concepts can become a little complicated mathematically
    speaking, but don’t get discouraged. These are optional topics, and it is not
    necessary to grasp the minutiae of all these implementations. Understanding the
    general ideas behind the layers will be sufficient to experiment with the PyTorch
    Geometric implementations that we reference.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍一些您可以在GNN中使用的额外层次，此外还将提供该领域最新发展的高层次概述。虽然我们将为这些层背后的直觉和实现提供背景，这些概念在数学上可能有些复杂，但不要气馁。这些是可选的主题，不必掌握所有这些实现的细微之处。理解层背后的一般思想将足以使用我们引用的PyTorch
    Geometric实现进行实验。
- en: The following subsections will introduce spectral graph convolution layers,
    graph pooling layers, and normalization layers for graphs. Lastly, the final subsection
    will provide a bird’s eye view of some more advanced kinds of graph neural networks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的小节将介绍谱图卷积层、图池化层和图归一化层。最后，最终的小节将对一些更高级的图神经网络进行总览。
- en: Spectral graph convolutions
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谱图卷积
- en: The graph convolutions we have utilized up to this point have all been spatial
    in nature. This means that they aggregate information based on the topological
    space associated with the graph, which is just a fancy way of saying that spatial
    convolutions operate on local neighborhoods of nodes. As a consequence of this,
    if a GNN that utilizes spatial convolutions needs to capture complex global patterns
    in graph data, then the network will need to stack multiple spatial convolutions.
    In situations where these global patterns are important, but network depth needs
    to be limited, spectral graph convolutions are an alternative kind of convolution
    to consider.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用的图卷积都是空间性质的。这意味着它们根据与图相关的拓扑空间聚合信息，这只是说空间卷积在节点的局部邻域上操作的一种花哨方式。因此，如果利用空间卷积的
    GNN 需要捕捉图数据中复杂的全局模式，那么网络就需要堆叠多个空间卷积。在这些全局模式很重要但需要限制网络深度的情况下，谱图卷积是一种可以考虑的替代卷积类型。
- en: Spectral graph convolutions operate differently than spatial graph convolutions.
    Spectral graph convolutions operate by utilizing the graph’s spectrum—its set
    of eigenvalues—by computing the eigendecomposition of a normalized version of
    the graph’s adjacency matrix called the *graph Laplacian*. That last sentence
    may seem like a doozy, so let’s break it down and go over it step by step.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 谱图卷积的操作方式与空间图卷积不同。谱图卷积通过利用图的频谱—其特征值集合—通过计算称为*图拉普拉斯*的图的归一化版本的特征分解来操作。最后一句可能看起来很复杂，所以让我们逐步分解并讨论它。
- en: For an undirected graph, the Laplacian matrix of a graph is defined as **L** = **D** – **A**,
    where **A** is the adjacency matrix of the graph and **D** is the degree matrix.
    A degree matrix is a diagonal matrix where the element on the diagonal in the
    row with index *i* is the number of edges in and out of the node associated with
    the *i*th row of the adjacency matrix.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无向图，图的拉普拉斯矩阵定义为 **L** = **D** - **A**，其中 **A** 是图的邻接矩阵，**D** 是度矩阵。度矩阵是一个对角矩阵，其中对角线上的元素在与邻接矩阵的第
    *i* 行相关的节点的进出边数。
- en: '**L** is a real-valued symmetric matrix, and it has been proven that real-valued
    symmetric matrices can be decomposed as ![](img/B17582_18_015.png), where **Q**
    is an orthogonal matrix whose columns are the eigenvectors of **L**, and ![](img/B17582_18_016.png)
    is a diagonal matrix whose elements are the eigenvalues of **L**. You can think
    of **Q** as providing an underlying representation of the graph’s structure. Unlike
    spatial convolutions, which use local neighborhoods of the graph that are defined
    by **A**, spectral convolutions utilize the alternative representation of the
    structure from **Q** to update the node embeddings.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**L** 是一个实对称矩阵，已经证明实对称矩阵可以分解为 ![](img/B17582_18_015.png)，其中 **Q** 是正交矩阵，其列是
    **L** 的特征向量，![](img/B17582_18_016.png) 是对角矩阵，其元素是 **L** 的特征值。你可以把 **Q** 看作提供了图结构的底层表示。与使用由
    **A** 定义的图的局部邻域的空间卷积不同，谱卷积利用来自 **Q** 的替代结构表示来更新节点嵌入。'
- en: 'The following example of a spectral convolution utilizes the eigendecomposition
    of the *symmetric normalized graph Laplacian*, which is defined for a graph as
    follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的谱卷积示例利用了 *对称归一化图拉普拉斯* 的特征分解，对于一个图，它定义如下：
- en: '![](img/B17582_18_017.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_017.png)'
- en: Here, **I** is the identity matrix. This is used because the normalization of
    the graph Laplacian can help stabilize the gradient-based training procedure similar
    to feature standardization.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**I** 是单位矩阵。这是因为图拉普拉斯归一化可以帮助稳定基于梯度的训练过程，类似于特征标准化。
- en: 'Given that ![](img/B17582_18_018.png) is the eigendecomposition of **L**[sym],
    the graph convolution is defined as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 ![](img/B17582_18_018.png) 是 **L**[sym] 的特征分解，图卷积定义如下：
- en: '![](img/B17582_18_019.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_019.png)'
- en: Here, **W** is a trainable weight matrix. The inside of the parentheses essentially
    multiplies **X** and **W** by a matrix that encodes structural relationships in
    the graph. The ![](img/B17582_18_020.png) operator here denotes element-wise multiplication
    of the inner terms, while the outside **Q** maps the result back into the original
    basis. This convolution has a few undesirable properties, since computing a graph’s
    eigendecomposition has a computational complexity of *O*(*n*³). This means that
    it is slow, and as it is structured, **W** is dependent on the size of the graph.
    Consequently, the spectral convolution can only be applied to graphs of the same
    size. Furthermore, the receptive field of this convolution is the whole graph,
    and this cannot be tuned in the current formulation. However, various techniques
    and convolutions have been developed to address these issues.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**W** 是一个可训练的权重矩阵。括号里的内容实质上是通过一个编码图结构关系的矩阵来乘以 **X** 和 **W**。这里的 ![](img/B17582_18_020.png)
    运算符表示内部项的逐元素乘法，而外部的 **Q** 将结果映射回原始基础。这种卷积有一些不良特性，因为计算图的特征分解具有 *O*(*n*³) 的计算复杂度。这意味着它速度较慢，并且如其结构所示，**W**
    取决于图的大小。因此，谱卷积只能应用于相同大小的图。此外，该卷积的感受野是整个图，当前的配方不能进行调整。然而，已经开发出各种技术和卷积来解决这些问题。
- en: For example, Bruna and colleagues ([https://arxiv.org/abs/1312.6203](https://arxiv.org/abs/1312.6203))
    introduced a smoothing method that addresses the size dependence of **W** by approximating
    it with a set of functions, each multiplied by their own scalar parameter, ![](img/B17582_18_021.png).
    That is, given the set of functions *f*[1], ..., *f*[n], ![](img/B17582_18_022.png).
    The set of functions is such that the dimensionality can be varied. However, since
    ![](img/B17582_15_030.png) remains scalar, the convolutions parameter space can
    be independent of the graph size.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Bruna 和同事 ([https://arxiv.org/abs/1312.6203](https://arxiv.org/abs/1312.6203))
    引入了一种平滑方法，通过一组函数的逼近来解决 **W** 的大小依赖性，每个函数都乘以它们自己的标量参数，![](img/B17582_18_021.png)。也就是说，给定函数集
    *f*[1], ..., *f*[n]，![](img/B17582_18_022.png)。这组函数的维度可以变化。然而，由于 ![](img/B17582_15_030.png)
    保持标量，卷积参数空间可以独立于图的大小。
- en: Other spectral convolutions worth mentioning include the Chebyshev graph convolution
    ([https://arxiv.org/abs/1606.09375](https://arxiv.org/abs/1606.09375)), which
    can approximate the original spectral convolution at a lower time complexity and
    can have receptive fields with varying sizes. Kipf and Welling ([https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907))
    introduce a convolution with properties similar to the Chebyshev convolutions,
    but with a reduced parameter burden. Implementations of both of these are available
    in PyTorch Geometric as `torch_geometric.nn.ChebConv`and `torch_geometric.nn.GCNConv`
    and are reasonable places to start if you want to play around with spectral convolutions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的其他谱卷积包括 Chebyshev 图卷积 ([https://arxiv.org/abs/1606.09375](https://arxiv.org/abs/1606.09375))，它可以在更低的时间复杂度下近似原始谱卷积，并且可以具有不同大小的感受野。Kipf
    和 Welling ([https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907))
    引入了一个与 Chebyshev 卷积类似的卷积，但减少了参数负担。这两种的实现都可以在 PyTorch Geometric 中找到，分别是 `torch_geometric.nn.ChebConv`
    和 `torch_geometric.nn.GCNConv`，如果你想尝试谱卷积，这是个合理的起点。
- en: Pooling
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化
- en: We will briefly discuss some examples of pooling layers that have been developed
    for graphs. While the downsampling provided by pooling layers has been beneficial
    in CNN architectures, the benefit of downsampling in GNNs has not been realized
    as clearly.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要讨论一些为图形开发的池化层的例子。虽然池化层提供的下采样在 CNN 架构中是有益的，但在 GNN 中下采样的好处并不是很明显。
- en: Pooling layers for image data (ab)use spatial locality, which graphs do not
    have. If a clustering of the nodes in a graph is provided, we can define how a
    graph pooling layer should pool nodes. However, it is unclear how to define optimal
    clustering, and different clustering approaches may be favored for different contexts.
    Even after clustering is determined, if nodes are downsampled, it is unclear how
    the remaining nodes should be connected. While these are still open research questions,
    we’ll look at a few graph pooling layers and point out their approaches to the
    aforementioned issues.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据的池化层滥用了空间局部性，而图形则没有。如果提供图中节点的聚类，我们可以定义图池化层如何对节点进行池化。然而，如何定义最佳聚类仍不明确，并且不同的聚类方法可能适合不同的情境。即使确定了聚类，如果节点被降采样，剩余节点如何连接仍不清楚。尽管这些问题仍然是开放性的研究问题，我们将介绍几种图池化层，并指出它们解决上述问题的方法。
- en: 'As with CNNs, there are mean and max pooling layers that can be applied to
    GNNs. As shown in *Figure 18.13*, given a clustering of nodes, each cluster becomes
    a node in a new graph:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 与CNN类似，可以应用于GNN的均值和最大池化层。如图18.13所示，给定节点的聚类后，每个聚类成为新图中的一个节点：
- en: '![Diagram  Description automatically generated](img/B17582_18_13.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B17582_18_13.png)'
- en: 'Figure 18.13: Applying max pooling to a graph'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.13：将最大池化应用于图形
- en: Each cluster’s embedding is equal to the mean or max of the embeddings of the
    nodes in the cluster. To address connectivity, the cluster is assigned the union
    of all edge indices in the cluster. For example, if nodes *i*, *j*, *k* are assigned
    to cluster *c*[1], any node, or cluster containing a node, that shared an edge
    with *i*, *j*, or *k* will share an edge with *c*[1].
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聚类的嵌入等于该聚类中节点嵌入的均值或最大值。为了解决连接性问题，将为该聚类分配包含该聚类中所有边索引的联合。例如，如果将节点 *i*, *j*,
    *k* 分配给聚类 *c*[1]，那么与 *i*, *j* 或 *k* 共享边的任何节点或包含该节点的聚类，将与 *c*[1] 共享边。
- en: A more complex pooling layer, *DiffPool* ([https://arxiv.org/abs/1806.08804](https://arxiv.org/abs/1806.08804)),
    tries to address both clustering and downsampling simultaneously. This layer learns
    a soft cluster assignment matrix ![](img/B17582_18_024.png), which distributes
    *n* node embeddings into *c* clusters. (For a refresher on soft versus hard clustering,
    refer to the section *Hard versus soft clustering* in *Chapter 10*, *Working with
    Unlabeled Data – Clustering Analysis*.) With this, **X** is updated as **X**′ = **S**^T**X**
    and **A** as **A**′ = **S**^T**A**^T**S**. Notably, **A**′ no longer contains
    discrete values and can instead be viewed as a matrix of edge weightings. Over
    time, *DiffPool* converges to an almost hard clustering assignment with interpretable
    structure.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的池化层 *DiffPool* ([https://arxiv.org/abs/1806.08804](https://arxiv.org/abs/1806.08804))
    试图同时解决聚类和降采样问题。该层学习一个软聚类分配矩阵 ![](img/B17582_18_024.png)，将 *n* 个节点嵌入分配到 *c* 个聚类中。（关于软聚类与硬聚类的区别，请参阅《第10章》，《使用未标记数据
    - 聚类分析》中的章节《硬聚类与软聚类》。）通过这种方式，**X** 更新为 **X**′ = **S**^T**X**，**A** 更新为 **A**′ = **S**^T**A**^T**S**。值得注意的是，**A**′
    不再包含离散值，可以视为一种边权重矩阵。随着时间的推移，*DiffPool* 收敛到几乎硬聚类分配，具有可解释的结构。
- en: 'Another pooling method, top-*k* pooling, drops nodes from the graph instead
    of aggregating them, which circumvents clustering and connectivity issues. While
    this seemingly comes with a loss of the information in the dropped nodes, in the
    context of a network, as long as a convolution occurs before pooling, the network
    can learn to avoid this. The dropped nodes are selected using a projection score
    against a learnable vector *p*. The actual formulation to compute (**X**′, **A**′),
    as stated in *Towards Sparse Hierarchical Graph Classifiers* ([https://arxiv.org/abs/1811.01287](https://arxiv.org/abs/1811.01287)),
    is:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种池化方法，top-*k* 池化，不是聚合图中的节点，而是删除它们，从而避免了聚类和连接性问题。虽然看似会损失删除节点中的信息，但在网络的背景下，只要在池化之前进行卷积，网络就能学会避免这种情况。被删除的节点是根据可学习向量
    *p* 的投影分数来选择的。计算(**X**′, **A**′) 的实际公式如在《向稀疏分层图分类器迈进》([https://arxiv.org/abs/1811.01287](https://arxiv.org/abs/1811.01287))
    中所述：
- en: '![](img/B17582_18_025.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_025.png)'
- en: Here, top-*k* selects the indexes of **y**, with the top *k* values and the
    index vector *i* being used to drop rows of **X** and **A**. Top-*k* pooling is
    implemented in PyTorch Geometric as `torch_geometric.nn.TopKPooling`. Additionally,
    max and mean pooling are implemented as `torch_geometric.nn.max_pool_x` and `torch_geometric.nn.avg_pool_x`,
    respectively.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，top-*k* 选择**y** 的前 *k* 个值的索引，使用索引向量 *i* 来删除**X** 和 **A** 的行。PyTorch Geometric
    实现了 top-*k* 池化为 `torch_geometric.nn.TopKPooling`。此外，最大池化和平均池化分别实现为 `torch_geometric.nn.max_pool_x`
    和 `torch_geometric.nn.avg_pool_x`。
- en: Normalization
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准化
- en: Normalization techniques are utilized in many kinds of neural networks to help
    stabilize and/or speed up the training process. Many approaches, such as batch
    normalization (discussed in *Chapter 17*, *Generative Adversarial Networks for
    Synthesizing New Data*), can be readily applied in GNNs with appropriate bookkeeping.
    In this section, we will briefly describe some of the normalization layers that
    have been designed specifically for graph data.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化技术被广泛应用于许多类型的神经网络中，以帮助稳定和/或加速训练过程。许多方法，如批标准化（在*第17章* *生成对抗网络用于合成新数据*中讨论），可以适用于具有适当记账的GNNs。在本节中，我们将简要描述一些专为图数据设计的标准化层。
- en: As a quick review of normalization, we mean that given a set of feature values
    *x*[1], ..., *x*[n], we update the values with ![](img/B17582_18_026.png), where
    ![](img/B17582_03_001.png) is the mean and ![](img/B17582_03_002.png) the standard
    deviation of the set of values. Typically, most neural network normalization methods
    take the general form ![](img/B17582_18_029.png), where ![](img/B17582_03_064.png)
    and ![](img/B17582_18_031.png) are learnable parameters, and the difference between
    methods has to do with the set of features the normalization is applied over.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 作为标准化的快速回顾，我们认为给定一组特征值 *x*[1], ..., *x*[n]，我们使用 ![](img/B17582_18_026.png) 来更新这些值，其中
    ![](img/B17582_03_001.png) 是均值，![](img/B17582_03_002.png) 是标准差。通常，大多数神经网络标准化方法采用通用形式
    ![](img/B17582_18_029.png)，其中 ![](img/B17582_03_064.png) 和 ![](img/B17582_18_031.png)
    是可学习参数，而标准化方法之间的差异在于应用标准化的特征集。
- en: '*GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training*
    by *Tianle Cai* andcolleagues,2020 ([https://arxiv.org/abs/2009.03294](https://arxiv.org/abs/2009.03294)),
    showed that the mean statistic after aggregation in a graph convolution can contain
    meaningful information, so discarding it completely may not be desirable. To address
    this, they introduced *GraphNorm*.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*GraphNorm: 加速图神经网络训练的原则方法*，由*Tianle Cai*及其同事在2020年提出（[https://arxiv.org/abs/2009.03294](https://arxiv.org/abs/2009.03294)），展示了在图卷积中聚合后的均值统计可能包含有意义的信息，因此完全丢弃它可能不是一个理想的选择。为了解决这个问题，他们引入了*GraphNorm*。'
- en: 'Borrowing notation from the original manuscript, let *h* be the matrix of node
    embeddings. Let *h*[i][, ][j] be the *j*th feature value of node *v*[i], where
    *i* = 1, ..., *n*, and *j* = 1, ..., *d*. *GraphNorm* takes the following form:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 借用原始手稿的符号，设*h* 为节点嵌入矩阵。设*h*[i][, ][j] 为节点*v*[i] 的第 *j* 个特征值，其中 *i* = 1, ..., *n*，*j*
    = 1, ..., *d*。*GraphNorm* 的形式如下：
- en: '![](img/B17582_18_032.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_032.png)'
- en: Here, ![](img/B17582_18_033.png) and ![](img/B17582_18_034.png). The key addition
    is the learnable parameter, ![](img/B17582_15_030.png), which can control how
    much of the mean statistic, ![](img/B17582_02_067.png), to discard.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17582_18_033.png) 和 ![](img/B17582_18_034.png)。关键新增部分是可学习参数 ![](img/B17582_15_030.png)，它可以控制要丢弃的均值统计量
    ![](img/B17582_02_067.png) 的程度。
- en: 'Another graph normalization technique is *MsgNorm,* which was described by
    *Guohao Li* and colleagues in the manuscript *DeeperGCN: All You Need to Train
    Deeper GCNs* in 2020 ([https://arxiv.org/abs/2006.07739](https://arxiv.org/abs/2006.07739)).
    *MsgNorm* corresponds to the message-passing formulation of graph convolutions
    mentioned earlier in the chapter. Using message-passing network nomenclature (defined
    at the end of the subsection *Implementing a basic graph convolution*), after
    a graph convolution has summed over *M*[t] and produced *m*[i] but before updating
    the nodes embedding with *U*[t], *MsgNorm* normalizes *m*[i] with the following
    formula:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种图标准化技术是*MsgNorm*，由*Guohao Li*及其同事在2020年的手稿*DeeperGCN: All You Need to Train
    Deeper GCNs*中描述（[https://arxiv.org/abs/2006.07739](https://arxiv.org/abs/2006.07739)）。*MsgNorm*
    对应于前面章节中提到的图卷积的消息传递形式。使用消息传递网络命名法（在*实施基本图卷积*子节的末尾定义），在图卷积对*M*[t] 求和并产生*m*[i] 但在使用*U*[t]
    更新节点嵌入之前，*MsgNorm* 通过以下公式对*m*[i] 进行标准化：'
- en: '![](img/B17582_18_037.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17582_18_037.png)'
- en: Here, *s* is a learnable scaling factor and the intuition behind this approach
    is to normalize the features of the aggregated messages in a graph convolution.
    While there is no theory to support this normalization approach, it has worked
    well in practice.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*s*是一个可学习的缩放因子，这种方法的直觉是对图卷积中聚合消息的特征进行标准化。虽然没有理论支持这种标准化方法，但在实践中效果显著。
- en: The normalization layers we’ve discussed are all implemented and available via
    PyTorch Geometric as `BatchNorm`, `GroupNorm`, and `MessageNorm`. For more information,
    please visit the PyTorch Geometric documentation at [https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#normalization-layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#normalization-layers).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过的标准化层都通过PyTorch Geometric实现并可用，如`BatchNorm`、`GroupNorm`和`MessageNorm`。更多信息，请访问PyTorch
    Geometric文档：[https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#normalization-layers](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#normalization-layers)。
- en: Unlike graph pooling layers, which may require an additional clustering setup,
    graph normalization layers can be more readily plugged into an existing GNN model.
    Testing a variety of normalization methods during model development and optimization
    is a reasonable and recommended approach.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 与可能需要额外聚类设置的图池化层不同，图归一化层可以更容易地插入现有的GNN模型中。在模型开发和优化过程中测试各种标准化方法是一种合理且推荐的方法。
- en: Pointers to advanced graph neural network literature
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级图神经网络文献的指引
- en: The field of deep learning focused on graphs is developing rapidly, and there
    are many methods that we can’t cover in reasonable detail in this introductory
    chapter. So, before we conclude this chapter, we want to provide interested readers
    with a selection of pointers to noteworthy literature for more in-depth studies
    of this topic.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 着眼于图的深度学习领域正在迅速发展，有许多方法我们在这个入门章节无法详尽介绍。因此，在结束本章之前，我们希望为有兴趣的读者提供一些显著文献的指引，以便深入研究这一主题。
- en: As you might remember from *Chapter 16*, *Transformers – Improving Natural Language
    Processing with Attention Mechanisms*, attention mechanisms can improve the capabilities
    of models by providing additional contexts. In this regard, a variety of attention
    methods for GNNs have been developed. Examples of GNNs augmented with attention
    include *Graph Attention Networks*, by *Petar* *Veličković* and colleagues, 2017
    ([https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)) and *Relational
    Graph Attention Networks* by *Dan Busbridge* and colleagues, 2019 ([https://arxiv.org/abs/1904.05811](https://arxiv.org/abs/1904.05811)).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能还记得的在*第16章*，*变压器——通过注意力机制改进自然语言处理*中，注意力机制可以通过提供额外的上下文来改进模型的能力。在这方面，已开发出多种用于GNN的注意力方法。例如，*Petar
    Veličković*及其同事于2017年提出的*图注意力网络*（[https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)）以及*Dan
    Busbridge*及其同事于2019年提出的*关系图注意力网络*（[https://arxiv.org/abs/1904.05811](https://arxiv.org/abs/1904.05811)）。
- en: Recently, these attention mechanisms have also been utilized in graph transformers
    proposed by *Seongjun Yun* and colleagues, 2020 ([https://arxiv.org/abs/1911.06455](https://arxiv.org/abs/1911.06455))
    and *Heterogeneous Graph Transformer* by *Ziniu Hu* and colleagues, 2020([https://arxiv.org/abs/2003.01332](https://arxiv.org/abs/2003.01332)).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，这些注意力机制还被2020年*Seongjun Yun*及其同事提出的图变换器和2020年*Ziniu Hu*及其同事提出的异构图变换器所利用（[https://arxiv.org/abs/1911.06455](https://arxiv.org/abs/1911.06455)和[https://arxiv.org/abs/2003.01332](https://arxiv.org/abs/2003.01332))。
- en: 'Next to the aforementioned graph transformers, other deep generative models
    have been developed specifically for graphs. There are graph variational autoencoders
    such as those introduced in *Variational Graph Auto-Encoders* by *Kipf* and *Welling*,
    2016 ([https://arxiv.org/abs/1611.07308](https://arxiv.org/abs/1611.07308)), *Constrained
    Graph Variational Autoencoders for Molecule Design* by Qi Liu and colleagues,
    2018([https://arxiv.org/abs/1805.09076](https://arxiv.org/abs/1805.09076)), and
    *GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders*
    by *Simonovsky* and *Komodakis*, 2018 ([https://arxiv.org/abs/1802.03480](https://arxiv.org/abs/1802.03480)).
    Another notable graph variational autoencoder that has been applied to molecule
    generation is the *Junction Tree Variational Autoencoder for Molecular Graph Generation*
    by *Wengong Jin* and colleagues, 2019 ([https://arxiv.org/abs/1802.04364](https://arxiv.org/abs/1802.04364)).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '除了上述的图转换器之外，还开发了其他专门用于图形的深度生成模型。例如，图变分自动编码器，如*Kipf*和*Welling*在2016年提出的《变分图自动编码器》([https://arxiv.org/abs/1611.07308](https://arxiv.org/abs/1611.07308))，以及2018年*刘琦*等人提出的《分子设计的约束图变分自动编码器》([https://arxiv.org/abs/1805.09076](https://arxiv.org/abs/1805.09076))，以及*Simonovsky*和*Komodakis*在2018年提出的《GraphVAE:
    使用变分自动编码器生成小型图形》([https://arxiv.org/abs/1802.03480](https://arxiv.org/abs/1802.03480))。另一个显著的应用于分子生成的图变分自动编码器是*Wengong
    Jin*和同事在2019年提出的《分子图生成的联结树变分自动编码器》([https://arxiv.org/abs/1802.04364](https://arxiv.org/abs/1802.04364))。'
- en: 'Some GANs have been designed to generate graph data, though, as of this writing,
    the performance of GANs on graphs is much less convincing than in the image domain.
    Examples include *GraphGAN: Graph Representation Learning with Generative Adversarial
    Nets* by *Hongwei Wang* and colleagues, 2017 ([https://arxiv.org/abs/1711.08267](https://arxiv.org/abs/1711.08267))
    and *MolGAN: An Implicit Generative Model for Small Molecular Graphs* by *Cao*
    and *Kipf*, 2018 ([https://arxiv.org/abs/1805.11973](https://arxiv.org/abs/1805.11973)).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '一些GAN已被设计用于生成图数据，尽管截至本文撰写时，GAN在图领域的表现远不如在图像领域那般令人信服。例如，*Hongwei Wang*和同事在2017年提出的《GraphGAN:
    使用生成对抗网络进行图表示学习》([https://arxiv.org/abs/1711.08267](https://arxiv.org/abs/1711.08267))，以及*Cao*和*Kipf*在2018年提出的《MolGAN:
    用于小分子图的隐式生成模型》([https://arxiv.org/abs/1805.11973](https://arxiv.org/abs/1805.11973))。'
- en: GNNs have also been incorporated into deep reinforcement learning models—you
    will learn more about reinforcement learning in the next chapter. Examples include
    *Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation*
    by *Jiaxuan You* and colleagues, 2018 ([https://arxiv.org/abs/1806.02473](https://arxiv.org/abs/1806.02473))
    and a deep Q-network proposed in *Optimization of Molecules via Deep Reinforcement
    Learning* by *Zhenpeng Zhou* and colleagues, 2018 ([https://arxiv.org/abs/1810.08678](https://arxiv.org/abs/1810.08678)),
    which utilizes a GNN that was applied to molecule generation tasks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: GNNs也已纳入深度强化学习模型中——你将在下一章节详细学习强化学习。例如，*Jiaxuan You*和同事在2018年提出的《用于目标导向分子图生成的图卷积策略网络》([https://arxiv.org/abs/1806.02473](https://arxiv.org/abs/1806.02473))，以及*Zhenpeng
    Zhou*和同事在2018年提出的《通过深度强化学习优化分子》([https://arxiv.org/abs/1810.08678](https://arxiv.org/abs/1810.08678))，利用了应用于分子生成任务的GNN。
- en: 'Lastly, while not technically graph data, 3D point clouds are sometimes represented
    as such using distance cutoffs to create edges. Applications of graph networks
    in this space include *Point-GNN: Graph Neural Network for 3D Object Detection
    in a Point Cloud* by *Weijing Shi* and colleagues, 2020 ([https://arxiv.org/abs/2003.01251](https://arxiv.org/abs/2003.01251)),
    which detects 3D objects in LiDAR point clouds. In addition, *GAPNet: Graph Attention
    based Point Neural Network for Exploiting Local Feature of Point Cloud* by *Can
    Chen* and colleagues, 2019 ([https://arxiv.org/abs/1905.08705](https://arxiv.org/abs/1905.08705))
    was designed to detect local features in point cloud data, which had been challenging
    for other deep architectures.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，虽然技术上不属于图数据，但有时将3D点云表示为图数据，使用距离截断来创建边。图网络在这一领域的应用包括*Weijing Shi*和同事在2020年提出的《Point-GNN:
    用于LiDAR点云中3D物体检测的图神经网络》([https://arxiv.org/abs/2003.01251](https://arxiv.org/abs/2003.01251))，该网络可以在LiDAR点云中检测3D物体。此外，*Can
    Chen*和同事在2019年设计的《GAPNet: 基于图注意力的点神经网络，用于利用点云的局部特征》([https://arxiv.org/abs/1905.08705](https://arxiv.org/abs/1905.08705))，旨在解决其他深度架构难以处理的点云局部特征检测问题。'
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As the amount of data we have access to continues to increase, so too will our
    need to understand interrelations within the data. While this will be done in
    numerous ways, graphs function as a distilled representation of these relationships,
    so the amount of graph data available will only increase.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们可以访问的数据量不断增加，我们需要理解数据内部的相互关系的需求也会增加。虽然我们会以多种方式来实现这一点，但图表作为这些关系的精炼表示，可用的图表数据量只会增加。
- en: In this chapter, we explained graph neural networks from the ground up by implementing
    a graph convolution layer and a GNN from scratch. We saw that implementing GNNs,
    due to the nature of graph data, is actually quite complex. Thus, to apply GNNs
    to a real-world example, such as predicting molecular polarization, we learned
    how to utilize the PyTorch Geometric library, which provides implementations of
    many of the building blocks we need. Lastly, we went over some of the notable
    literature for diving into the GNN literature more deeply.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过从零开始实现图卷积层和GNN来逐步解释图神经网络。我们看到，由于图数据的性质，实现GNN实际上是非常复杂的。因此，为了将GNN应用于实际示例，例如预测分子极化，我们学习了如何利用PyTorch
    Geometric库，该库提供了我们需要的许多构建模块的实现。最后，我们回顾了一些深入研究GNN领域的重要文献。
- en: Hopefully, this chapter provided an introduction to how deep learning can be
    leveraged to learn on graphs. Methods in this space are currently a hot area of
    research, and many of the ones we have mentioned were published in the last couple
    of years. With this text as a starting point, maybe the next advancement in the
    space can be made by you.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本章介绍了如何利用深度学习来学习图形。这一领域的方法目前是研究的热点领域，我们提到的许多方法都是最近几年发表的。通过这本书作为起点，也许你可以在这个领域取得下一个进展。
- en: In the next chapter, we will look at reinforcement learning, which is a completely
    different category of machine learning compared to what we have covered so far
    in this book.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨强化学习，这是与本书迄今为止涵盖的机器学习完全不同的一类。
- en: Join our book’s Discord space
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的Discord工作空间，参加每月的*问我任何事*与作者的会话：
- en: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/MLwPyTorch](https://packt.link/MLwPyTorch)'
- en: '![](img/QR_Code874410888448293359.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code874410888448293359.png)'
