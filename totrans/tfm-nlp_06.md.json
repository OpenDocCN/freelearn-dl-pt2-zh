["```py\nimport pickle\nfrom pickle import dump \n```", "```py\n# load doc into memory\ndef load_doc(filename):\n        # open the file as read only\n        file = open(filename, mode='rt', encoding='utf-8')\n        # read all text\n        text = file.read()\n        # close the file\n        file.close()\n        return text \n```", "```py\n# split a loaded document into sentences\ndef to_sentences(doc):\n        return doc.strip().split('\\n') \n```", "```py\n# shortest and longest sentence lengths\ndef sentence_lengths(sentences):\n        lengths = [len(s.split()) for s in sentences]\n        return min(lengths), max(lengths) \n```", "```py\n# clean lines\nimport re\nimport string\nimport unicodedata\ndef clean_lines(lines):\n        cleaned = list()\n        # prepare regex for char filtering\n        re_print = re.compile('[^%s]' % re.escape(string.printable))\n        # prepare translation table for removing punctuation\n        table = str.maketrans('', '', string.punctuation)\n        for line in lines:\n                # normalize unicode characters\n                line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n                line = line.decode('UTF-8')\n                # tokenize on white space\n                line = line.split()\n                # convert to lower case\n                line = [word.lower() for word in line]\n                # remove punctuation from each token\n                line = [word.translate(table) for word in line]\n                # remove non-printable chars form each token\n                line = [re_print.sub('', w) for w in line]\n                # remove tokens with numbers in them\n                line = [word for word in line if word.isalpha()]\n                # store as string\n                cleaned.append(' '.join(line))\n        return cleaned \n```", "```py\n# load English data\nfilename = 'europarl-v7.fr-en.en'\ndoc = load_doc(filename)\nsentences = to_sentences(doc)\nminlen, maxlen = sentence_lengths(sentences)\nprint('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\ncleanf=clean_lines(sentences) \n```", "```py\nfilename = 'English.pkl'\noutfile = open(filename,'wb')\npickle.dump(cleanf,outfile)\noutfile.close()\nprint(filename,\" saved\") \n```", "```py\nEnglish data: sentences=2007723, min=0, max=668\nEnglish.pkl  saved \n```", "```py\n# load French data\nfilename = 'europarl-v7.fr-en.fr'\ndoc = load_doc(filename)\nsentences = to_sentences(doc)\nminlen, maxlen = sentence_lengths(sentences)\nprint('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\ncleanf=clean_lines(sentences)\nfilename = 'French.pkl'\noutfile = open(filename,'wb')\npickle.dump(cleanf,outfile)\noutfile.close()\nprint(filename,\" saved\") \n```", "```py\nFrench data: sentences=2007723, min=0, max=693\nFrench.pkl saved \n```", "```py\nfrom pickle import load\nfrom pickle import dump\nfrom collections import Counter\n\n# load a clean dataset\ndef load_clean_sentences(filename):\n        return load(open(filename, 'rb'))\n\n# save a list of clean sentences to file\ndef save_clean_sentences(sentences, filename):\n        dump(sentences, open(filename, 'wb'))\n        print('Saved: %s' % filename) \n```", "```py\n# create a frequency table for all words\ndef to_vocab(lines):\n        vocab = Counter()\n        for line in lines:\n                tokens = line.split()\n                vocab.update(tokens)\n        return vocab \n```", "```py\n# remove all words with a frequency below a threshold\ndef trim_vocab(vocab, min_occurrence):\n        tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n        return set(tokens) \n```", "```py\n# mark all OOV with \"unk\" for all lines\ndef update_dataset(lines, vocab):\n        new_lines = list()\n        for line in lines:\n                new_tokens = list()\n                for token in line.split():\n                        if token in vocab:\n                                new_tokens.append(token)\n                        else:\n                                new_tokens.append('unk')\n                new_line = ' '.join(new_tokens)\n                new_lines.append(new_line)\n        return new_lines \n```", "```py\n# load English dataset\nfilename = 'English.pkl'\nlines = load_clean_sentences(filename)\n# calculate vocabulary\nvocab = to_vocab(lines)\nprint('English Vocabulary: %d' % len(vocab))\n# reduce vocabulary\nvocab = trim_vocab(vocab, 5)\nprint('New English Vocabulary: %d' % len(vocab))\n# mark out of vocabulary words\nlines = update_dataset(lines, vocab)\n# save updated dataset\nfilename = 'english_vocab.pkl'\nsave_clean_sentences(lines, filename)\n# spot check\nfor i in range(20):\n        print(\"line\",i,\":\",lines[i]) \n```", "```py\nEnglish Vocabulary: 105357\nNew English Vocabulary: 41746\nSaved: english_vocab.pkl \n```", "```py\nline 0 : resumption of the session\nline 1 : i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\nline 2 : although, as you will have seen, the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\nline 3 : you have requested a debate on this subject in the course of the next few days during this partsession \n```", "```py\n# load French dataset\nfilename = 'French.pkl'\nlines = load_clean_sentences(filename)\n# calculate vocabulary\nvocab = to_vocab(lines)\nprint('French Vocabulary: %d' % len(vocab))\n# reduce vocabulary\nvocab = trim_vocab(vocab, 5)\nprint('New French Vocabulary: %d' % len(vocab))\n# mark out of vocabulary words\nlines = update_dataset(lines, vocab)\n# save updated dataset\nfilename = 'french_vocab.pkl'\nsave_clean_sentences(lines, filename)\n# spot check\nfor i in range(20):\n        print(\"line\",i,\":\",lines[i]) \n```", "```py\nFrench Vocabulary: 141642\nNew French Vocabulary: 58800\nSaved: french_vocab.pkl \n```", "```py\nline 0 : reprise de la session\nline 1 : je declare reprise la session du parlement europeen qui avait ete interrompue le vendredi decembre dernier et je vous renouvelle tous mes vux en esperant que vous avez passe de bonnes vacances\nline 2 : comme vous avez pu le constater le grand bogue de lan ne sest pas produit en revanche les citoyens dun certain nombre de nos pays ont ete victimes de catastrophes naturelles qui ont vraiment ete terribles\nline 3 : vous avez souhaite un debat a ce sujet dans les prochains jours au cours de cette periode de session \n```", "```py\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction \n```", "```py\n#Example 1\nreference = [['the', 'cat', 'likes', 'milk'], ['cat', 'likes' 'milk']]\ncandidate = ['the', 'cat', 'likes', 'milk']\nscore = sentence_bleu(reference, candidate)\nprint('Example 1', score)\n#Example 2\nreference = [['the', 'cat', 'likes', 'milk']]\ncandidate = ['the', 'cat', 'likes', 'milk']\nscore = sentence_bleu(reference, candidate)\nprint('Example 2', score) \n```", "```py\nExample 1 1.0\nExample 2 1.0 \n```", "```py\n#Example 3\nreference = [['the', 'cat', 'likes', 'milk']]\ncandidate = ['the', 'cat', 'enjoys','milk']\nscore = sentence_bleu(reference, candidate)\nprint('Example 3', score) \n```", "```py\nWarning (from warnings module):\n  File\n\"C:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\nltk\\translate\\bleu_score.py\", line 490\n    warnings.warn(_msg)\nUserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\nExample 3 0.7071067811865475 \n```", "```py\ncandidate_words=[drinks, likes, enjoys, appreciates]\ncandidate_softmax=[0.7, 0.1, 0.1,0.1]\ncandidate_one_hot=[1,0,0,0] \n```", "```py\n#Example 4\nreference = [['je','vous','invite', 'a', 'vous', 'lever','pour', 'cette', 'minute', 'de', 'silence']]\ncandidate = ['levez','vous','svp','pour', 'cette', 'minute', 'de', 'silence']\nscore = sentence_bleu(reference, candidate)\nprint(\"without soothing score\", score) \n```", "```py\nwithout smoothing score 0.37188004246466494 \n```", "```py\nchencherry = SmoothingFunction()\nr1=list('je vous invite a vous lever pour cette minute de silence')\ncandidate=list('levez vous svp pour cette minute de silence')\n\n#sentence_bleu([reference1, reference2, reference3], hypothesis2,smoothing_function=chencherry.method1)\nprint(\"with smoothing score\",sentence_bleu([r1], candidate,smoothing_function=chencherry.method1)) \n```", "```py\nwith smoothing score 0.6194291765462159 \n```", "```py\n#@title Installing Trax\nimport os\nimport numpy as np\n!pip install -q -U trax\nimport trax \n```", "```py\n#@title Creating a Transformer model.\n# Pre-trained model config in gs://trax-ml/models/translation/ende_wmt32k.gin\nmodel = trax.models.Transformer(\n    input_vocab_size=33300,\n    d_model=512, d_ff=2048,\n    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,\n    max_len=2048, mode='predict') \n```", "```py\n#@title Initializing the model using pre-trained weights\nmodel.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',\n                     weights_only=True) \n```", "```py\n#@title Tokenizing a sentence.\nsentence = 'I am only a machine but I have machine intelligence.'\ntokenized = list(trax.data.tokenize(iter([sentence]), # Operates on streams.\n                                   vocab_dir='gs://trax-ml/vocabs/',\n                                   vocab_file='ende_32k.subword'))[0] \n```", "```py\n#@title Decoding from the Transformer\ntokenized = tokenized[None, :]  # Add batch dimension.\ntokenized_translation = trax.supervised.decoding.autoregressive_sample(\n    model, tokenized, temperature=0.0)  # Higher temperature: more diverse results. \n```", "```py\n#@title De-tokenizing and Displaying the Translation\ntokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.\ntranslation = trax.data.detokenize(tokenized_translation,\n                                   vocab_dir='gs://trax-ml/vocabs/',\n                                   vocab_file='ende_32k.subword')\nprint(\"The sentence:\",sentence)\nprint(\"The translation:\",translation) \n```", "```py\nThe sentence: I am only a machine but I have machine intelligence.\nThe translation: Ich bin nur eine Maschine, aber ich habe Maschinenübersicht. \n```"]