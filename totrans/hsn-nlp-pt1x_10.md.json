["```py\n    spacy_german = spacy.load(‘de’)\n    spacy_english = spacy.load(‘en’)\n    ```", "```py\n    def tokenize_german(text):\n        return [token.text for token in spacy_german.            tokenizer(text)]\n    def tokenize_english(text):\n        return [token.text for token in spacy_english.            tokenizer(text)][::-1]\n    ```", "```py\n    SOURCE = Field(tokenize = tokenize_english, \n                init_token = ‘<sos>’, \n                eos_token = ‘<eos>’, \n                lower = True)\n    TARGET = Field(tokenize = tokenize_german, \n                init_token = ‘<sos>’, \n                eos_token = ‘<eos>’, \n                lower = True)\n    ```", "```py\n    train_data, valid_data, test_data = Multi30k.splits(exts = (‘.en’, ‘.de’), fields = (SOURCE, TARGET))\n    ```", "```py\n    print(train_data.examples[0].src)\n    print(train_data.examples[0].trg)\n    ```", "```py\n    print(“Training dataset size: “ + str(len(train_data.       examples)))\n    print(“Validation dataset size: “ + str(len(valid_data.       examples)))\n    print(“Test dataset size: “ + str(len(test_data.       examples)))\n    ```", "```py\n    SOURCE.build_vocab(train_data, min_freq = 2)\n    TARGET.build_vocab(train_data, min_freq = 2)\n    print(“English (Source) Vocabulary Size: “ +        str(len(SOURCE.vocab)))\n    print(“German (Target) Vocabulary Size: “ +        str(len(TARGET.vocab)))\n    ```", "```py\n    device = torch.device(‘cuda’ if torch.cuda.is_available()                       else ‘cpu’)\n    batch_size = 32\n    train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n        (train_data, valid_data, test_data), \n        batch_size = batch_size, \n        device = device)\n    ```", "```py\n    class Encoder(nn.Module):\n        def __init__(self, input_dims, emb_dims, hid_dims,     n_layers, dropout):\n            super().__init__()   \n            self.hid_dims = hid_dims\n            self.n_layers = n_layers\n    ```", "```py\n    self.embedding = nn.Embedding(input_dims, emb_dims)\n    ```", "```py\n    self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers, dropout                    = dropout)\n    self.dropout = nn.Dropout(dropout)\n    ```", "```py\n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (h, cell) = self.rnn(embedded)\n        return h, cell\n    ```", "```py\nclass Decoder(nn.Module):\n    def __init__(self, output_dims, emb_dims, hid_dims,     n_layers, dropout):\n        super().__init__()\n\n        self.output_dims = output_dims\n        self.hid_dims = hid_dims\n        self.n_layers = n_layers\n\n        self.embedding = nn.Embedding(output_dims, emb_dims)\n\n        self.rnn = nn.LSTM(emb_dims, hid_dims, n_layers,                           dropout = dropout)\n\n        self.fc_out = nn.Linear(hid_dims, output_dims)\n\n        self.dropout = nn.Dropout(dropout)\n```", "```py\ndef forward(self, input, h, cell):\n\n    input = input.unsqueeze(0)\n\n    embedded = self.dropout(self.embedding(input))\n\n    output, (h, cell) = self.rnn(embedded, (h, cell))\n\n    pred = self.fc_out(output.squeeze(0))\n\n    return pred, h, cell\n```", "```py\n    class Seq2Seq(nn.Module):\n        def __init__(self, encoder, decoder, device):\n            super().__init__()\n\n            self.encoder = encoder\n            self.decoder = decoder\n            self.device = device\n    ```", "```py\n    def forward(self, src, trg, teacher_forcing_rate = 0.5):\n        batch_size = trg.shape[1]\n        target_length = trg.shape[0]\n        target_vocab_size = self.decoder.output_dims\n\n         outputs = torch.zeros(target_length, batch_size,                     target_vocab_size).to(self.device)\n    ```", "```py\n    h, cell = self.encoder(src)\n    ```", "```py\n    input = trg[0,:]\n    ```", "```py\n    for t in range(1, target_length):\n    output, h, cell = self.decoder(input, h, cell)\n\n    outputs[t] = output\n\n    top = output.argmax(1) \n\n    input = trg[t] if (random.random() < teacher_forcing_                   rate) else top\n\n    return outputs\n    ```", "```py\n    input_dimensions = len(SOURCE.vocab)\n    output_dimensions = len(TARGET.vocab)\n    encoder_embedding_dimensions = 256\n    decoder_embedding_dimensions = 256\n    hidden_layer_dimensions = 512\n    number_of_layers = 2\n    encoder_dropout = 0.5\n    decoder_dropout = 0.5\n    ```", "```py\n    encod = Encoder(input_dimensions,\\\n                    encoder_embedding_dimensions,\\\n                    hidden_layer_dimensions,\\\n                    number_of_layers, encoder_dropout)\n    decod = Decoder(output_dimensions,\\\n                    decoder_embedding_dimensions,\\\n                    hidden_layer_dimensions,\\\n                    number_of_layers, decoder_dropout)\n    model = Seq2Seq(encod, decod, device).to(device)\n    ```", "```py\n    def initialize_weights(m):\n        for name, param in m.named_parameters():\n            nn.init.uniform_(param.data, -0.1, 0.1)\n\n    model.apply(initialize_weights)\n    ```", "```py\n    optimizer = optim.Adam(model.parameters())\n    criterion = nn.CrossEntropyLoss(ignore_index = TARGET.               vocab.stoi[TARGET.pad_token])\n    ```", "```py\n    def train(model, iterator, optimizer, criterion, clip):\n        model.train()\n        epoch_loss = 0\n    ```", "```py\n    for i, batch in enumerate(iterator):\n    src = batch.src\n    trg = batch.trg\n    optimizer.zero_grad()\n    output = model(src, trg)\n    ```", "```py\n    output_dims = output.shape[-1]\n    output = output[1:].view(-1, output_dims)\n    trg = trg[1:].view(-1)\n\n    loss = criterion(output, trg)\n\n    loss.backward()\n    ```", "```py\n    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n\n    optimizer.step()\n\n    epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n    ```", "```py\n    model.eval()\n    ```", "```py\n    with torch.no_grad():\n    ```", "```py\n    output = model(src, trg, 0)\n    ```", "```py\n    epochs = 10\n    grad_clip = 1\n    lowest_validation_loss = float(‘inf’)\n    ```", "```py\n    for epoch in range(epochs):\n\n        start_time = time.time()\n\n        train_loss = train(model, train_iterator, optimizer,                       criterion, grad_clip)\n        valid_loss = evaluate(model, valid_iterator,                          criterion)\n\n        end_time = time.time()\n    ```", "```py\n    if valid_loss < lowest_validation_loss:\n    lowest_validation_loss = valid_loss\n    torch.save(model.state_dict(), ‘seq2seq.pt’) \n    ```", "```py\n    print(f’Epoch: {epoch+1:02} | Time: {np.round(end_time-start_time,0)}s’)\n    print(f’\\tTrain Loss: {train_loss:.4f}’)\n    print(f’\\t Val. Loss: {valid_loss:.4f}’)\n    ```", "```py\n    output = model(src, trg, 0)\n    preds = torch.tensor([[torch.argmax(x).item()] for x         in output])\n    ```", "```py\n    print(‘English Input: ‘ + str([SOURCE.vocab.itos[x] for x        in src][1:-1][::-1]))\n    print(‘Correct German Output: ‘ + str([TARGET.vocab.       itos[x] for x in trg][1:-1]))\n    print(‘Predicted German Output: ‘ + str([TARGET.vocab.       itos[x] for x in preds][1:-1]))\n    ```"]