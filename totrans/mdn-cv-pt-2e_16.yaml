- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Advanced GANs to Manipulate Images
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级GAN用于图像操作
- en: In the previous chapter, we learned about leveraging **Generative Adversarial
    Networks** (**GANs**) to generate realistic images. In this chapter, we will learn
    about leveraging GANs to manipulate images. We will learn about two variations
    of generating images using GANs – paired and unpaired methods. With the paired
    method, we will provide the input and output pair combinations to generate novel
    images based on an input image, which we will learn about in the **Pix2Pix GAN**.
    With the unpaired method, we will specify the input and output; however, we will
    not provide one-to-one correspondence between the input and output, but expect
    the GAN to learn the structure of the two classes, and convert an image from one
    class to another, which we will learn about when we discuss **CycleGAN**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们学习了如何利用**生成对抗网络**（**GANs**）生成逼真图像。在本章中，我们将学习如何利用GANs来操作图像。我们将学习使用GANs生成图像的两种变体方法——配对方法和非配对方法。在配对方法中，我们将提供输入和输出的组合对，以生成基于输入图像的新图像，我们将在**Pix2Pix
    GAN**中学习到。在非配对方法中，我们将指定输入和输出；然而，我们不会提供输入和输出之间的一对一对应关系，而是期望GAN学习两类的结构，并将一个类别的图像转换为另一个类别的图像，这是我们讨论**CycleGAN**时将要学习的内容。
- en: Another class of unpaired image manipulation involves generating images from
    a latent space of random vectors and seeing how images change as the latent vector
    values change, which we will learn about in the *Leveraging StyleGAN on custom
    images* section. Finally, we will learn about leveraging a pre-trained GAN – **Super
    Resolution Generative Adversarial Network** (**SRGAN**), with which we can turn
    a low-resolution image into an image with high resolution.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非配对图像操作的类别涉及从随机向量的潜在空间生成图像，并查看随着潜在向量值的变化图像如何改变，这是我们在*利用StyleGAN在自定义图像上*部分将要学习的内容。最后，我们将学习如何利用预训练的GAN——**超分辨率生成对抗网络**（**SRGAN**），将低分辨率图像转换为高分辨率图像。
- en: 'Specifically, we will learn about the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将学习以下主题：
- en: Leveraging the Pix2Pix GAN to convert a sketch/ picture of edges to a picture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用Pix2Pix GAN将草图/边缘图片转换为图片
- en: Leveraging CycleGAN to convert apples to oranges and vice versa
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用CycleGAN将苹果转换为橙子，反之亦然
- en: Leveraging StyleGAN on custom images to change the expression of images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用StyleGAN在自定义图像上改变图像表情
- en: Super-resolution of an image using SRGAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SRGAN对图像进行超分辨率处理
- en: All code snippets within this chapter are available in the `Chapter13` folder
    of the GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本章中的所有代码片段都可以在GitHub仓库的`Chapter13`文件夹中找到，网址是[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。
- en: Leveraging the Pix2Pix GAN
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用Pix2Pix GAN
- en: Imagine a scenario where we have pairs of images that are related to each other
    (for example, an image of the edges of an object as input and an actual image
    of the object as output). The challenge given is that we want to generate an image
    given the input image of the edges of an object. In a traditional setting, this
    would have been a simple mapping of input to output and hence a supervised learning
    problem. However, imagine that you are working with a creative team that is trying
    to come up with a fresh look for products. In this scenario, supervised learning
    does not help much as it only learns from history. A GAN would come in handy here
    because it would ensure that the generated image would look realistic and would
    leave room for experimentation (as we are interested in checking whether the generated
    image is similar to the images that we want to generate). Specifically, Pix2Pix
    GAN comes in handy in scenarios in which it is trained to generate an image from
    another image that only contains its edges (or contours).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情景，我们有一对彼此相关的图像（例如，一个对象边缘的图像作为输入，实际对象图像作为输出）。面临的挑战是，我们希望根据对象边缘的输入图像生成一张图像。在传统设置中，这将是一个简单的输入到输出的映射，因此是一个监督学习问题。然而，假设你正在与一个试图为产品设计新外观的创意团队合作。在这种情况下，监督学习并没有太多帮助，因为它只从历史中学习。这里GAN会很有用，因为它确保生成的图像看起来逼真，并为实验留下空间（因为我们希望检查生成的图像是否类似于我们想要生成的图像）。具体来说，Pix2Pix
    GAN在需要从另一个仅包含边缘（或轮廓）图像生成图像的情景中非常有用。
- en: 'In this section, we will learn about the architecture used to generate an image
    of a shoe from a hand-drawn doodle of the edges of a shoe. The strategy that we
    will adopt to generate a realistic image from the doodle is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解用于从鞋类图像的手绘边缘生成鞋类图像的架构。我们将采用以下策略来从手绘的草图生成逼真图像：
- en: Fetch a lot of actual images and create the corresponding edges using standard
    `cv2` edge detection techniques.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从大量实际图像中获取，并使用标准的`cv2`边缘检测技术创建相应的边缘。
- en: Sample colors from patches of the original image so that the generator knows
    which colors to generate.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始图像的补丁中采样颜色，以使生成器知道生成哪些颜色。
- en: Build a UNet architecture that takes the edges with the sample patch colors
    as input and predicts the corresponding image – this is our generator.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个UNet架构，该架构以带有样本补丁颜色的边缘作为输入，并预测相应的图像 - 这是我们的生成器。
- en: Build a discriminator architecture that takes an image and predicts whether
    it is real or fake.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个鉴别器架构，它接受图像并预测其真实性。
- en: Train the generator and discriminator together to a point where the generator
    can generate images that fool the discriminator.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练生成器和鉴别器，使之达到生成可以欺骗鉴别器的图像的能力。
- en: 'Let’s code the strategy:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写策略：
- en: 'The following code is available as `Pix2Pix_GAN.ipynb` in the `Chapter13` folder
    of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce the results while you
    follow the steps to perform and read the explanations of the various code components
    in the text.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可作为本书GitHub存储库中“Chapter13”文件夹中的`Pix2Pix_GAN.ipynb`获取：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)
    该代码包含从中下载数据的URL，长度适中。我们强烈建议您在GitHub上执行笔记本以在按照步骤执行并阅读文本中各种代码部分的解释时重现结果。
- en: 'Import the dataset (available here: [https://sketchx.eecs.qmul.ac.uk/downloads/](https://sketchx.eecs.qmul.ac.uk/downloads/))
    and install the relevant packages:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集（在此处可用：[https://sketchx.eecs.qmul.ac.uk/downloads/](https://sketchx.eecs.qmul.ac.uk/downloads/)），并安装相关的包：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code downloads images of shoes. A sample of the downloaded images
    is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码下载了鞋类图像。下载的图像样本如下所示：
- en: '![A picture containing footwear, clothing  Description automatically generated](img/B18457_13_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![包含鞋类服装图像的图片  自动生成的描述](img/B18457_13_01.png)'
- en: 'Figure 13.1: Sample images'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：样本图像
- en: For our problem, we want to draw a shoe given the edges and some sample patch
    colors of the shoe. In the next step, we will fetch the edges from an image of
    a shoe. This way, we can train a model that reconstructs an image of a shoe from
    the edges and sample patch colors of the shoe.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的问题，我们希望根据鞋子的边缘和一些样本补丁颜色绘制鞋子。在下一步中，我们将从鞋类图像中获取边缘。这样，我们可以训练一个模型，从鞋子的边缘和样本补丁颜色中重建出鞋子的图像。
- en: 'Define a function to fetch the edges from the downloaded images:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，从下载的图像中获取边缘：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we are leveraging the various methods available in the
    OpenCV package to fetch the edges from an image (there are more details on how
    the OpenCV methods work in the OpenCV chapter within the `Extra chapters from
    first edition` folder in the GitHub repository).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们利用OpenCV包中提供的各种方法从图像中获取边缘（有关OpenCV方法的详细信息，请参见GitHub存储库中“第一版额外章节”文件夹中的OpenCV章节）。
- en: 'Define the image transformation pipeline to pre-process and normalize the dataset
    (`preprocess` and `normalize`):'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义图像转换流水线，对数据集进行预处理和标准化（`preprocess`和`normalize`）：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Define the dataset class (`ShoesData`). This dataset class returns the original
    image and the image with edges. One additional detail we will pass to the network
    is some patches of color that are present in randomly chosen regions. This way,
    we are enabling the user to take a hand-drawn contour image, sprinkle the required
    colors in different parts of the image, and generate a new image.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集类（`ShoesData`）。此数据集类返回原始图像和带有边缘的图像。我们将向网络传递的一个额外细节是一些颜色补丁，这些补丁出现在随机选择的区域中。这样，我们使用户能够从手绘的轮廓图像中取出所需颜色并在图像的不同部分中添加，从而生成新的图像。
- en: 'An example input (the first image) and output (the third image) are shown here
    (you can view them in color in the digital version of this book):'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此处显示了一个示例输入（第一幅图像）和输出（第三幅图像）（您可以在本书的数字版中以彩色查看）：
- en: '![A picture containing footwear  Description automatically generated](img/B18457_13_02.png)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![包含鞋类的图片](img/B18457_13_02.png)'
- en: 'Figure 13.2: (Left) Original image; (middle) contours of the original image;
    (right) contour image with color information'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 13.2：（左）原始图像；（中）原始图像的轮廓；（右）带有颜色信息的轮廓图像
- en: 'However, the input image we have is just of the shoe (the leftmost image),
    which we will use to extract the edges of the shoe (the middle image). Further,
    we will sprinkle color in the next step to fetch the color information of the
    original image (the rightmost image). The output of the rightmost image, when
    passed through our network, should be the leftmost image. In the following code,
    we will build the class that takes the contour images, sprinkles colors, and returns
    the pair of color-sprinkled images and the original shoe image (the image that
    generated the image with contours):'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，我们拥有的输入图像只是鞋子的图像（最左边的图像），我们将用它来提取鞋子的边缘（中间的图像）。接下来，我们将在下一步中撒上颜色，以获取原始图像的颜色信息（最右边的图像）。当右边的图像通过我们的网络时，应该生成左边的图像。在下面的代码中，我们将构建一个类，该类接受轮廓图像，撒上颜色，并返回撒上颜色的图像对和原始鞋子图像（生成包含轮廓图像的图像）：
- en: 'Define the `ShoesData` class, the `__init__` method, and the `__len__` method:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `ShoesData` 类、`__init__` 方法和 `__len__` 方法：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Define the `__getitem__` method. In this method, we will process the input
    image to fetch an image with edges and then sprinkle the image with the colors
    present in the original image. Here, we are fetching the edges of a given image:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `__getitem__` 方法。在这个方法中，我们将处理输入图像以获取带有边缘的图像，然后在原始图像中撒上颜色。在这里，我们获取给定图像的边缘：
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once we have fetched the edges in the image, we resize and normalize the image:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们获取了图像中的边缘，我们会调整大小并对图像进行归一化处理：
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Sprinkle color on the `edges` image and preprocess the original and `edges`
    images:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `edges` 图像上撒上颜色，并预处理原始图像和 `edges` 图像：
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the functions to sprinkle color:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义撒上颜色的函数：
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the training and validation data’s corresponding datasets and data loaders:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和验证数据的对应数据集和数据加载器：
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the generator and discriminator architectures, which use the weight
    initialization (`weights_init_normal`), `UNetDown`, and `UNetUp` architectures,
    just as we did in *Chapter 9*, *Image Segmentation*, and *Chapter 10*, *Applications
    of Object Detection and Segmentation*, to define the `GeneratorUNet` and `Discriminator`
    architectures:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器和鉴别器架构，这些架构使用权重初始化 (`weights_init_normal`)、`UNetDown` 和 `UNetUp` 架构，正如我们在
    *第 9 章*、“图像分割” 和 *第 10 章*、“目标检测和分割的应用” 中定义 `GeneratorUNet` 和 `Discriminator` 架构：
- en: 'Initialize weights so that they follow a normal distribution:'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重，使其遵循正态分布：
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the `UnetDown` and `UNetUp` classes:'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `UnetDown` 和 `UNetUp` 类：
- en: '[PRE10]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the `GeneratorUNet` class:'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `GeneratorUNet` 类：
- en: '[PRE11]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the `Discriminator` class:'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `Discriminator` 类：
- en: '[PRE12]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the `generator` and `discriminator` model objects and fetch summaries:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器和鉴别器模型对象并获取概要：
- en: '[PRE13]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The generator architecture summary is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器架构概述如下：
- en: '![A close-up of a document  Description automatically generated with medium
    confidence](img/B18457_13_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![一个文件的特写，由中等置信度自动生成的描述](img/B18457_13_03.png)'
- en: 'Figure 13.3: Summary of the generator architecture'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：生成器架构概述
- en: 'The discriminator architecture summary is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器架构概述如下：
- en: '![A picture containing text, receipt, screenshot  Description automatically
    generated](img/B18457_13_04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、收据、截图的图片](img/B18457_13_04.png)'
- en: 'Figure 13.4: Summary of the discriminator architecture'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：鉴别器架构概述
- en: 'Define the function to train the discriminator (`discriminator_train_step`):'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练鉴别器的函数 (`discriminator_train_step`)：
- en: 'The discriminator function takes the source image (`real_src`), real target
    (`real_trg`), and fake target (`fake_trg`) as input:'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴别器函数接受源图像 (`real_src`)、真实目标 (`real_trg`) 和伪造目标 (`fake_trg`) 作为输入：
- en: '[PRE14]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Calculate the loss (`error_real`) by comparing the real target (`real_trg`)
    and the predicted values (`real_src`) of the target; the expectation is that the
    discriminator will predict the images as real (indicated by `torch.ones`) and
    then perform backpropagation:'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过比较真实目标（`real_trg`）和目标的预测值（`real_src`）来计算损失（`error_real`），期望鉴别器将图像预测为真实的（由
    `torch.ones` 表示），然后进行反向传播：
- en: '[PRE15]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Calculate the discriminator loss (`error_fake`) corresponding to the fake images
    (`fake_trg`); the expectation is that the discriminator classifies the fake targets
    as fake images (indicated by `torch.zeros`) and then performs backpropagation:'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算与假图像（`fake_trg`）对应的鉴别器损失（`error_fake`）；预期鉴别器将假目标分类为假图像（由`torch.zeros`指示），然后进行反向传播：
- en: '[PRE16]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Perform the optimizer step and return the overall error and loss values on
    the predicted real and fake targets:'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行优化器步骤，并返回预测的真实和假目标的总体错误和损失值：
- en: '[PRE17]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the function to train the generator (`generator_train_step`), which
    takes a fake target (`fake_trg`) and trains it so that it has a low chance of
    being identified as fake when passed through the discriminator:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练生成器的函数（`generator_train_step`），该函数接受一个假的目标（`fake_trg`）并训练它，使其在通过鉴别器时被识别为假的概率很低：
- en: '[PRE18]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that in the preceding code, in addition to generator loss, we are also
    fetching the pixel loss (`loss_pixel`) corresponding to the difference between
    the generated and the real image of a given contour.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述代码中，除了生成器损失外，我们还获取了对应于给定轮廓的生成图像和真实图像之间差异的像素损失（`loss_pixel`）。
- en: 'Define a function to fetch a sample of predictions:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来获取预测的样本：
- en: '[PRE19]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Apply weight initialization (`weights_init_normal`) to the generator and discriminator
    model objects:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对生成器和鉴别器模型对象应用权重初始化（`weights_init_normal`）：
- en: '[PRE20]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Specify the loss criterion and optimization methods (`criterion_GAN` and `criterion_pixelwise`):'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定损失标准和优化方法（`criterion_GAN`和`criterion_pixelwise`）：
- en: '[PRE21]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Train the model over 100 epochs:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型超过100个时期：
- en: '[PRE22]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Generate the output (image) on a sample hand-drawn contour:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在样本手绘轮廓上生成输出（图像）：
- en: '[PRE23]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code generates the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成以下输出：
- en: '![](img/B18457_13_05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_13_05.png)'
- en: 'Figure 13.5: (Left) Input image; (middle) generated image; (right) original
    image'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：（左）输入图像；（中）生成的图像；（右）原始图像
- en: Note that in the preceding output, we have generated images that have similar
    colors as those of the original image. As an exercise, we encourage you to train
    the model for more epochs and see the improvement in generated images.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述输出中，我们生成了具有与原始图像相似颜色的图像。作为练习，我们鼓励您训练更多时期的模型，并查看生成图像的改进。
- en: In this section, we have learned about using the contours of an image to generate
    an image. However, this required us to provide the input and output as pairs,
    which can be a tedious process. In the next section, we will learn about unpaired
    image translation. This is a process by which the network figures out the translation
    without needing us to specify the input and output mappings of images.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用图像的轮廓来生成图像。然而，这要求我们提供输入和输出作为一对，这可能是一个繁琐的过程。在下一节中，我们将学习无配对图像转换的方法。这是通过网络自动学习图像转换而无需指定输入和输出映射的过程。
- en: Leveraging CycleGAN
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用CycleGAN
- en: Imagine a scenario where we ask you to perform image translation from one class
    to another, but without using the input and the corresponding output images to
    train the model. For example, change the actor present in the current scene of
    a movie from one actor another. However, we give you the images of both classes/actors
    in two distinct folders. CycleGAN comes in handy in such a scenario.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，我们要求您执行从一个类别到另一个类别的图像转换，但是不使用输入和相应的输出图像来训练模型。例如，将电影当前场景中的一个演员更换为另一个演员。然而，我们会给您两个不同文件夹中的两个类别/演员的图像。在这种情况下，CycleGAN非常有用。
- en: In this section, we will learn how to train CycleGAN ([https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593))
    to convert an image of an apple into an image of an orange and vice versa. But
    first, let’s understand how CycleGAN works.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何训练CycleGAN（[https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)）将苹果的图像转换为橙子的图像，反之亦然。但首先，让我们了解CycleGAN的工作原理。
- en: How CycleGAN works
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CycleGAN的工作原理
- en: 'The **Cycle** in CycleGAN refers to the fact that we are translating (converting)
    an image from one class to another and back to the original class. At a high level,
    we will have three separate loss values in this architecture. More details about
    the loss calculations in the next pages:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN中的“Cycle”指的是我们将图像从一个类别转换为另一个类别，然后再转换回原始类别的过程。在高层次上，此架构中将有三个单独的损失值。关于损失计算的更多细节请参阅下一页：
- en: '**Adversarial loss**: This ensures that the both the domain generators accurately
    create objects in their respective domains using the other domain images as inputs.
    The only difference from a standard GAN, in this case, is that the generators
    accept images instead of noise.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗损失**：确保领域生成器使用另一个领域的图像作为输入准确地创建各自领域中的对象。与标准GAN的唯一区别在于，生成器接受图像而不是噪声。'
- en: '**Cycle loss**: The loss of recycling an image from the generated image to
    the original to ensure that the surrounding pixels are not changed.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环损失**：再循环生成图像到原始图像的损失，以确保周围像素不被改变。'
- en: '**Identity loss**: The loss when an input image of one class is passed through
    a generator that is expected to convert an image of another class into the class
    of the input image.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份损失**：当一个输入图像属于一个类别时，通过预期将另一个类别的图像转换为输入图像类别的生成器时的损失。'
- en: 'Here, we will go through the high-level steps of building CycleGAN:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讲解构建CycleGAN的高级步骤：
- en: Import and preprocess the dataset.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入并预处理数据集。
- en: Build the generator and discriminator network UNet architectures.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建生成器和鉴别器网络UNet架构。
- en: 'Define two generators:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义两个生成器：
- en: '**G_AB**: A generator that converts an image of class A to an image of class
    B'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G_AB**：将类A图像转换为类B图像的生成器'
- en: '**G_BA**: A generator that converts an image of class B to an image of class
    A'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**G_BA**：将类B图像转换为类A图像的生成器'
- en: 'Define the **identity loss**:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义**身份损失：**
- en: If you were to send an orange image to an orange generator, ideally if the generator
    has understood everything about oranges, it should not change the image and should
    “generate” the exact same image. We thus create an identity using this knowledge.
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你将一个橙子图像发送给一个橙子生成器，理想情况下，如果生成器已经完全理解了橙子的所有内容，它不应该改变图像，而是应该“生成”完全相同的图像。因此，我们利用这个知识创建一个身份。
- en: Identity loss should be minimal when an image of class A (real_A) is passed
    through G_BA and compared with real_A.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当通过G_BA传递类A的图像（real_A）并与real_A进行比较时，身份损失应该是最小的。
- en: Identity loss should be minimal when an image of class B (real_B) is passed
    through G_AB and compared with real_B.
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当通过G_AB传递类B的图像（real_B）并与real_B进行比较时，身份损失应该是最小的。
- en: Define the **GAN loss:**
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义**GAN损失：**
- en: The discriminator and generator loss for real_A and fake_A (fake_A is obtained
    when real_B image is passed through G_BA)
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于real_A和fake_A（当real_B图像通过G_BA时获得的fake_A），鉴别器和生成器损失。
- en: The discriminator and generator loss for real_B and fake_B (fake_B is obtained
    when the real_A image is passed through G_AB)
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于real_B和fake_B（当real_A图像通过G_AB时获得的fake_B），鉴别器和生成器损失。
- en: Define the **re-cycle** **loss:**
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义**再循环损失：**
- en: Consider a scenario where an image of an apple is to be transformed by an orange
    generator to generate a fake orange, and the fake orange is to be transformed
    back into an apple by the apple generator.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑这样一个场景：将一个苹果图像通过橙子生成器转换为生成假橙子，然后通过苹果生成器将假橙子转换回苹果。
- en: fake_B, which is the output when real_A is passed through G_AB, should regenerate
    real_A when fake_B is passed through G_BA.
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当real_A通过G_AB传递时获得的fake_B应该在fake_B通过G_BA传递时再生出real_A。
- en: fake_A, which is the output when real_B is passed through G_BA, should regenerate
    real_B when fake_A is passed through G_AB.
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当real_B通过G_BA传递时获得的fake_A应该在fake_A通过G_AB传递时再生出real_B。
- en: Optimize for the weighted loss of the three losses.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化三个损失的加权损失。
- en: Now that we understand the steps, let’s code them to convert apples to oranges
    and vice versa.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了步骤，让我们编写代码来将苹果转换成橙子，反之亦然。
- en: Implementing CycleGAN
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施CycleGAN
- en: 'To implement the steps that we just discussed, you can use the following code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现我们刚刚讨论的步骤，可以使用以下代码：
- en: 'This code is available as `CycleGAN.ipynb` in the `Chapter13` folder of this
    book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e). The
    code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce results while you read
    the steps to perform and the explanations of the code components in the text.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码作为本书GitHub存储库中`Chapter13`文件夹下的`CycleGAN.ipynb`可用：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。代码包含从中下载数据的URL，并且长度适中。我们强烈建议您在GitHub上执行笔记本以在阅读代码组件的步骤和解释时再现结果。
- en: 'Download and extract the datasets that contain the folders that have the images
    of apples and oranges:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并提取包含苹果和橙子图像的数据集文件夹：
- en: '[PRE24]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here’s a sample of the images we will be working on:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将要处理的图像的一个示例：
- en: '![A red apple and an orange  Description automatically generated with low confidence](img/B18457_13_06.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![一张红苹果和一个橙色图像  描述自动生成，低置信度](img/B18457_13_06.png)'
- en: 'Figure 13.6: Sample images'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：样本图像
- en: Note that there is no one-to-one correspondence between the apple and orange
    images (unlike the contour-to-shoe generation use case that we learned about in
    the *Leveraging the Pix2Pix GAN* section).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，苹果和橙子图像之间没有一对一的对应关系（不像我们在*Leveraging the Pix2Pix GAN*一节中学到的轮廓到鞋子生成的用例）。
- en: 'Import the required packages:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE25]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define the image transformation pipeline (`transform`):'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义图像转换流水线（`transform`）：
- en: '[PRE26]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the dataset class (`CycleGANDataset`), which takes the `apple` and `orange`
    folders (which are obtained after unzipping the downloaded dataset) as input and
    provides a batch of apple and orange images:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集类（`CycleGANDataset`），接受苹果和橙子文件夹（在解压下载的数据集后获取）作为输入，并提供一批苹果和橙子图像：
- en: '[PRE27]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define the training and validation datasets and data loaders:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和验证数据集以及数据加载器：
- en: '[PRE28]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the weight initialization method of the network (`weights_init_normal`)
    as defined in previous sections:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义网络的权重初始化方法（`weights_init_normal`），如前几节所定义：
- en: '[PRE29]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the residual block network (`ResidualBlock`) because in this instance,
    we will use ResNet:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义残差块网络（`ResidualBlock`），因为在这个例子中，我们将使用ResNet：
- en: '[PRE30]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the generator network (`GeneratorResNet`):'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器网络（`GeneratorResNet`）：
- en: '[PRE31]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define the discriminator network (`Discriminator`):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义鉴别器网络（`Discriminator`）：
- en: '[PRE32]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define the function to generate a sample of images – `generate_sample`:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成图像样本的函数 – `generate_sample`：
- en: '[PRE33]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the function to train the generator (`generator_train_step`):'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练生成器的函数（`generator_train_step`）：
- en: 'The function takes the two generator models (G_AB and G_BA) as `Gs` and `optimizer`,
    and real images of the two classes, `real_A` and `real_B`, as input:'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数将两个生成器模型（G_AB 和 G_BA）作为`Gs`和`optimizer`，以及两类真实图像`real_A`和`real_B`作为输入：
- en: '[PRE34]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Specify the generators:'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定生成器：
- en: '[PRE35]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Set the gradients to zero for the optimizer:'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器的梯度设置为零：
- en: '[PRE36]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If you were to send an orange image to an orange generator, ideally, if the
    generator has understood everything about oranges, it should not make any changes
    to the image and should “generate” the exact image. We thus create an identity
    using this knowledge. The loss function corresponding to `criterion_identity`
    will be defined just prior to training the model. Calculate the identity loss
    (`loss_identity`) for images of type A (apples) and type B (oranges):'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果将橙色图像发送到橙色生成器，理想情况下，如果生成器完全理解了橙子的所有内容，它不应对图像做任何更改，并且应该“生成”出完全相同的图像。因此，我们利用这一知识创建了一个身份。对应于`criterion_identity`的损失函数将在模型训练之前定义。计算类型为A（苹果）和B（橙子）的身份损失（`loss_identity`）：
- en: '[PRE37]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Calculate the GAN loss when the image is passed through the generator and the
    generated image is expected to be as close to the other class as possible (we
    have `np.ones` in this case when training the generator, as we are passing the
    fake images of a class to the discriminator of the same class):'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当图像通过生成器传递，并且生成的图像预期尽可能接近另一类时，计算GAN损失（在这种情况下，当训练生成器时，我们对同一类别的伪造图像传递给鉴别器时，使用`np.ones`）：
- en: '[PRE38]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Calculate the cycle loss. Consider a scenario in which an image of an apple
    is to be transformed by an orange generator to generate a fake orange, and this
    fake orange is to be transformed back into an apple by the apple generator. If
    the generators were perfect, this process should give back the original image,
    which means the following cycle losses should be zero:'
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算循环损失。考虑这样一个场景：一个苹果图像经由橙色生成器转换成一个伪橙色图像，然后这个伪橙色图像再经由苹果生成器转换回苹果。如果生成器是完美的，这个过程应该返回原始图像，这意味着以下循环损失应该为零：
- en: '[PRE39]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Calculate the overall weighted loss (where `lambda_cyc` and `lambda_id` are
    the weights associated with cycle loss and identity loss respectively) and perform
    backpropagation before returning the calculated values:'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算总加权损失（其中`lambda_cyc`和`lambda_id`分别是循环损失和身份损失的权重），在返回计算值之前进行反向传播：
- en: '[PRE40]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define the function to train the discriminator (`discriminator_train_step`):'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练鉴别器的函数（`discriminator_train_step`）：
- en: '[PRE41]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the generator, discriminator objects, optimizers, and loss functions:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器、鉴别器对象、优化器和损失函数：
- en: '[PRE42]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Train the networks over increasing epochs:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在增加的周期内训练网络：
- en: '[PRE43]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Generate the images once we have trained the models:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们训练了模型之后生成图像：
- en: '[PRE44]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code generates the following output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下输出：
- en: '![](img/B18457_13_07.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_13_07.png)'
- en: 'Figure 13.7: Original and reconstructed apples to oranges and vice versa'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：原始和重建的苹果和橙子以及反之亦然
- en: In the preceding image, we can see that we are successfully able to convert
    apples into oranges (the first two rows) and oranges into apples (the last two
    rows).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，我们可以看到我们成功地将苹果转换成橙子（前两行），以及将橙子转换成苹果（最后两行）。
- en: So far, we have learned about paired image-to-image translation through the
    Pix2Pix GAN and unpaired image-to-image translation through CycleGAN. In the next
    section, we will learn about leveraging StyleGAN to convert an image of one style
    into an image of another style.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了通过Pix2Pix GAN进行成对图像转换和通过CycleGAN进行非成对图像转换。在接下来的章节中，我们将学习如何利用StyleGAN将一种风格的图像转换为另一种风格的图像。
- en: Leveraging StyleGAN on custom images
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用StyleGAN处理自定义图像
- en: In *Chapter 11*, we learned about neural style transfer. We generated an image
    by blending the style of one image with the content of another image. However,
    what if we want to create a younger version of a person in a picture or add certain
    attributes to an image, such as glasses? StyleGAN can do this. Let’s learn how
    in the following sections.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11章中，我们学习了神经风格转移。我们通过将一幅图像的风格与另一幅图像的内容混合来生成一幅图像。然而，如果我们想要在图片中创建一个人的年轻版本或者给图像增加眼镜等特定属性，StyleGAN可以做到这一点。让我们在接下来的几节中学习如何做到这一点。
- en: The evolution of StyleGAN
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StyleGAN的演变
- en: 'Let’s first look at a few developments prior to the invention of StyleGAN.
    As we know, generating fake faces (as we saw in the previous chapter) involves
    the usage of GANs. The biggest problem that research faced was that the images
    that could be generated were small (typically 64 x 64). Any effort to generate
    larger images caused the generators or discriminators to fall into local minima,
    which would stop training and generate gibberish. One of the major leaps in generating
    high-quality images appeared in a research paper that proposed **Progressive GAN**
    (**ProGAN** – [https://arxiv.org/abs/1710.10196](https://arxiv.org/abs/1710.10196)),
    which used a clever trick. The size of both the generator and discriminator is
    progressively increased:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一些在StyleGAN发明之前的发展。众所周知，生成假面孔（正如我们在上一章中看到的）涉及到使用GAN。研究面临的最大问题是，可以生成的图像很小（通常为64
    x 64）。任何尝试生成更大图像的努力都会导致生成器或鉴别器陷入局部最小值，停止训练并生成胡言乱语。在一篇提出Progressive GAN（ProGAN）的研究论文中出现了生成高质量图像的重大进展，它使用了一个巧妙的技巧。逐步增加生成器和鉴别器的大小：
- en: First, you create a generator and discriminator to generate 4 x 4 images from
    a latent vector.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您创建一个生成器和鉴别器，以从潜在向量生成4 x 4的图像。
- en: Additional convolution (and upscaling) layers are then added to the trained
    generator and discriminator, which will be responsible for accepting the 4 x 4
    images (which are generated from latent vectors in step 1) and generating/discriminating
    8 x 8 images.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，额外的卷积（和放大）层被添加到训练好的生成器和鉴别器中，这些层负责接受4 x 4像素的图像（这些图像是从步骤1中的潜在向量生成的），并生成/鉴别8
    x 8像素的图像。
- en: Next, new layers are created in the generator and discriminator once again so
    they can be trained to generate larger images. Step by step, the image size is
    increased in this way, the logic being that it is easier to add a new layer to
    an already well-functioning network than trying to learn all the layers from scratch.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在生成器和鉴别器中再次创建新的层，以便它们可以被训练生成更大的图像。逐步增加图像大小的逻辑是，向已经正常运行的网络添加新层比尝试从头学习所有层更容易。
- en: 'In this manner, images are upscaled to a resolution of 1,024 x 1,024 pixels:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，图像被放大到1,024 x 1,024像素的分辨率：
- en: '![Diagram  Description automatically generated](img/B18457_13_08.png)Figure
    13.8: Image upscaling process(source: [https://arxiv.org/pdf/1710.10196v3.pdf](https://arxiv.org/pdf/1710.10196v3.pdf))'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图13.8：图像放大过程（来源：[https://arxiv.org/pdf/1710.10196v3.pdf](https://arxiv.org/pdf/1710.10196v3.pdf)）](img/B18457_13_08.png)'
- en: 'As much as it succeeded, it was fairly difficult to control individual aspects
    of the generated image (such as gender and age), primarily because the network
    gets exactly one input (in the preceding image, this is **Latent** at the top
    of the network). StyleGAN addresses this problem. It uses a similar training scheme
    where images are progressively generated, but with an added set of latent inputs
    every time the network grows. This means the network now accepts multiple latent
    vectors at regular intervals (as shown in block *(a)* in *Figure 13.9*). Every
    latent given at a stage of generation dictates the features that are going to
    be generated at that stage of that network. Let’s discuss the working details
    of StyleGAN in more detail here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它取得了成功，但要控制生成图像的各个方面（如性别和年龄）却相当困难，主要因为网络只接收一个输入（在前述图像中，这是网络顶部的**潜在**）。StyleGAN
    解决了这个问题。它使用类似的训练方案逐步生成图像，但每次网络增长时都添加了一组额外的潜在输入。这意味着网络现在在固定间隔接受多个潜在向量（如图13.9中 *(a)*
    区块所示）。在生成的阶段，每个给定的潜在向量决定了在网络的该阶段生成的特征。让我们在这里更详细地讨论StyleGAN的工作细节：
- en: '![Diagram  Description automatically generated](img/B18457_13_09.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18457_13_09.png)'
- en: 'Figure 13.9: StyleGAN working details'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：StyleGAN工作细节
- en: '(Source: [https://arxiv.org/pdf/1812.04948](https://arxiv.org/pdf/1812.04948)*)*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[https://arxiv.org/pdf/1812.04948](https://arxiv.org/pdf/1812.04948)*）*
- en: 'In the preceding diagram, we can contrast the traditional way of generating
    images and the style-based generator. In a traditional generator, there is only
    one input. However, there is a mechanism in place within a style-based generator.
    Let’s look at the details:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图中，我们可以对比传统生成图像的方式和基于风格的生成器。在传统生成器中，只有一个输入。然而，在基于风格的生成器内部有一种机制。让我们看看细节：
- en: Create a random noise vector *z* of size 1 x 512.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个大小为 1 x 512 的随机噪声向量 *z*。
- en: Feed this to an auxiliary network called the style network (or mapping network),
    which creates a tensor *w* of size 18 x 512.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其提供给称为风格网络（或映射网络）的辅助网络，该网络创建了一个大小为 18 x 512 的张量 *w*。
- en: 'The generator (synthesis) network contains 18 convolution layers. Each layer
    will accept the following as inputs:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成器（合成）网络包含 18 个卷积层。每一层将接受以下输入：
- en: The corresponding row of *w* (A)
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w* 的相应行（A）'
- en: A random noise vector (B)
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个随机噪声向量（B）
- en: The output from the previous layer
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前一层的输出
- en: Note that noise (B) is given only for regularization purposes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，噪声（B）仅用于正则化目的。
- en: The preceding three combined will create a pipeline that takes in a 1 x 512
    vector and creates a 1,024 x 1,024 image.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 前述三者的结合将创建一个管道，接受一个大小为 1 x 512 的向量，并生成一个 1,024 x 1,024 的图像。
- en: 'Let’s now discuss how each of the 18 1 x 512 vectors within the 18 x 512 vector
    that is generated from the mapping network contributes to the generation of an
    image:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细讨论从映射网络生成的大小为 18 x 512 向量中的每个 1 x 512 向量如何对图像的生成贡献：
- en: The 1 x 512 vector that is added in the first few layers of the synthesis network
    contributes to the large-scale features present in the image such as pose and
    face shape (as they are responsible for generating the 4 x 4, 8 x 8 images, and
    so on – which are the first few images that will be further enhanced in the later
    layers).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在合成网络的前几层添加的 1 x 512 向量有助于生成图像的大尺度特征，例如姿势和面部形状（因为它们负责生成 4 x 4、8 x 8 图像等，这些是稍后层次中将进一步增强的第一批图像）。
- en: The vectors added in the middle layers correspond to small-scale features such
    as hairstyle and whether the eyes are open or closed (as they are responsible
    for generating the 16 x 16, 32 x 32, and 64 x 64 images).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在中间层添加的向量对应于小尺度特征，如发型和眼睛是否睁开或闭合（因为它们负责生成 16 x 16、32 x 32 和 64 x 64 图像）。
- en: The vectors added in the last few layers correspond to the color scheme and
    other microstructures of the image. By the time we reach the last few layers,
    the image structure is preserved, and the facial features are preserved but only
    image-level details such as lighting conditions are changed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后几层添加的向量对应于图像的颜色方案和其他微观结构。当我们达到最后几层时，图像结构被保留，面部特征也被保留，但仅改变像素级别的细节，例如光照条件。
- en: 'In the next section, we will leverage a pre-trained StyleGAN2 model to customize
    our image of interest to have different styles. For our objective, we will perform
    style transfer using the StyleGAN2 model. At a high level, here’s how style transfer
    on faces works (the following will be clearer as you go through the results of
    the code):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将利用预训练的 StyleGAN2 模型来自定义我们感兴趣的图像，使其具有不同的风格。对于我们的目标，我们将使用 StyleGAN2
    模型执行样式迁移。在高层次上，面部样式迁移的工作方式如下（随着您通过代码结果进行查看，以下内容将更加清晰）：
- en: Say the `w1` style vector is used to generate `face-1` and the `w2` style vector
    is used to generate `face-2`. Both vectors have a shape of 18 x 512.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设 `w1` 样式向量用于生成 `face-1`，`w2` 样式向量用于生成 `face-2`。两个向量的形状均为 18 x 512。
- en: The first few of the 18 vectors in `w2` (which are responsible for generating
    images from 4 x 4 to 8 x 8 resolutions) are replaced with the corresponding vectors
    from `w1`. Then, we transfer very coarse features such as the pose from `face-1`
    to `face-2`.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `w2` 的前几个 18 个向量（负责从 4 x 4 到 8 x 8 分辨率生成图像）中，将其与 `w1` 中对应的向量替换。然后，我们将非常粗略的特征，如从
    `face-1` 到 `face-2` 的姿势，进行转移。
- en: If the later style vectors (say the third to the fifteenth of the 18 x 512 vectors
    – which are responsible for generating 64 x 64 to 256 x 256 resolution images)
    are replaced in `w2` with those from `w1`, then we transfer features such as eyes,
    nose, and other mid-level facial features.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果后续样式向量（例如 18 x 512 向量中的第三到第十五个，负责生成 64 x 64 到 256 x 256 分辨率的图像）在 `w2` 中被来自
    `w1` 的向量替换，则我们转移眼睛、鼻子和其他中级面部特征。
- en: If the last few style vectors (which are responsible for generating 512 x 512
    to 1,024 x 1,024 resolution images) are replaced, fine-level features such as
    complexion and background (which don’t affect the overall face in a significant
    manner) are transferred.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果替换了最后几个样式向量（负责生成 512 x 512 到 1,024 x 1,024 分辨率图像），则会转移细节特征，如肤色和背景（这些特征对整体面部影响不大）。
- en: With an understanding of how style transfer is done, let’s now see how to perform
    style transfer using StyleGAN2.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 理解了样式迁移的实现方式，现在让我们看看如何使用 StyleGAN2 进行样式迁移。
- en: Implementing StyleGAN
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 StyleGAN
- en: 'To achieve style transfer on custom images using StyleGAN2, we follow these
    broad steps:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 StyleGAN2 在自定义图像上实现样式迁移，我们遵循以下广泛步骤：
- en: Take a custom image.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采用自定义图像。
- en: Align the custom image so that only the face region of the image is stored.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自定义图像对齐，以便仅存储图像的面部区域。
- en: Fetch the latent vector that is likely to generate the custom aligned image
    when passed through StyleGAN2.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取通过 StyleGAN2 传递时可能生成自定义对齐图像的潜在向量。
- en: Generate an image by passing a random noise/latent vector (1 x 512) to the mapping
    network.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将随机噪声/潜在向量（1 x 512）传递给映射网络生成图像。
- en: 'By this step, we have two images – our custom aligned image and the image generated
    by the StyleGAN2 network. We now want to transfer some of the features of the
    custom image to the generated image and vice versa. Let’s code up the preceding
    strategy (note that we are leveraging a pre-trained network fetched from a GitHub
    repository, as training such a network takes days if not weeks):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一步，我们有两幅图像 —— 我们的自定义对齐图像和通过 StyleGAN2 网络生成的图像。现在，我们希望将自定义图像的一些特征转移到生成的图像中，反之亦然。让我们编写前述策略的代码（请注意，我们正在利用从
    GitHub 存储库获取的预训练网络，因为训练这样的网络需要数天甚至数周时间）。
- en: You need a CUDA-enabled environment to run the following code. The following
    code is available as `Customizing_StyleGAN2.ipynb` in the `Chapter13` folder of
    this book’s GitHub repository at [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e).
    The code contains URLs to download data from and is moderately lengthy. We strongly
    recommend you execute the notebook in GitHub to reproduce results while you read
    the steps to perform and the explanations of the various code components in the
    text.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一个支持 CUDA 的环境来运行以下代码。以下代码可以在本书的 GitHub 仓库的 `Chapter13` 文件夹中的 `Customizing_StyleGAN2.ipynb`
    中找到 [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)。该代码包含了从中下载数据的 URL，并且代码长度适中。我们强烈建议您在
    GitHub 上执行此笔记本，以便在阅读步骤和文本中的各种代码组件的说明时重现结果。
- en: 'Clone the repository, install the requirements, and fetch the pre-trained weights:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆存储库，安装要求并获取预训练权重：
- en: '[PRE45]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Load the pre-trained generator and the synthesis network, mapping the network’s
    weights:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练生成器和合成网络，映射网络的权重：
- en: '[PRE46]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the function to generate an image from a random vector:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个从随机向量生成图像的函数：
- en: '[PRE47]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Generate a random vector:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机向量：
- en: '[PRE48]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code, we are passing the random 1 x 512-dimensional vector
    through mapping and truncation networks to generate a vector that is 1 x 18 x
    512\. These 18 x 512 vectors are the ones that dictate the style of the generated
    image.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们将随机的1 x 512维向量通过映射和截断网络生成一个1 x 18 x 512的向量。这些18 x 512向量决定了生成图像的风格。
- en: 'Generate an image from the random vector:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机向量生成一个图像：
- en: '[PRE49]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding code generates the following output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成以下输出：
- en: '![A close-up of a person smiling  Description automatically generated](img/B18457_13_10.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![一位微笑的人的特写 自动生成描述](img/B18457_13_10.png)'
- en: 'Figure 13.10: Image from random latents'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10：来自随机潜在的图像
- en: So far, we have generated an image. In the next few lines of code, you will
    learn about performing style transfer between the preceding generated image and
    an image of your choice.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经生成了一个图像。在接下来的几行代码中，您将学习如何在前面生成的图像与您选择的图像之间执行风格转移。
- en: 'Fetch a custom image (`MyImage.jpg`) and align it. Alignment is important to
    generate proper latent vectors because all generated images in StyleGAN have the
    face centered and features prominently visible:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取自定义图像（`MyImage.jpg`）并对齐它。对齐是生成正确潜在向量的重要步骤，因为StyleGAN中生成的所有图像都有面部居中和特征明显可见：
- en: '[PRE50]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Align the custom image:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对齐自定义图像：
- en: '[PRE51]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Use the aligned image to generate latents that can reproduce the aligned image
    perfectly. This is the process of identifying the latent vector combination that
    minimizes the difference between the aligned image and the image generated from
    the latent vector:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对齐图像生成完美重现对齐图像的潜在变量。这是识别最小化对齐图像与从潜在向量生成的图像之间差异的潜在向量组合的过程：
- en: '[PRE52]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The preceding code generates the following output:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成以下输出：
- en: '![Graphical user interface, text  Description automatically generated](img/B18457_13_11.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本 自动生成描述](img/B18457_13_11.png)'
- en: 'Figure 13.11: Original image and the synthesized image from the corresponding
    latents'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11：原始图像和相应潜在图像的合成图像
- en: 'The `encode_image.py` Python script, at a high level, does the following (for
    a thorough understanding of each step, we encourage you to go through the script
    in the GitHub repo):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Python脚本`encode_image.py`在高层次上执行以下操作（为了彻底了解每个步骤，请查看GitHub存储库中的脚本）：
- en: Creates a random vector in latent space. Alternatively, we can get a set of
    initial latents (vector) that require lesser number of optimizations by passing
    the original image through a network initialized with the weights `image_to_latent.pt`
    and architecture from the file `models/image_to_latent.py` in the same repo.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在潜在空间中创建一个随机向量。或者，我们可以通过将原始图像通过网络传递初始化为权重`image_to_latent.pt`和文件`models/image_to_latent.py`中的架构，以获得需要较少优化的初始潜在（向量）集合。
- en: Synthesizes an image with this vector.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用这个向量合成一个图像。
- en: Compares the synthesized image with the original input image using VGG’s perceptual
    loss.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用VGG的感知损失比较合成图像与原始输入图像。
- en: Performs backpropagation on the `w` random vector to reduce this loss for a
    fixed number of iterations.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`w`随机向量进行反向传播，以在固定迭代次数内减少此损失。
- en: The optimized latent vector will now synthesize an image for which VGG gives
    near-identical features as the input image, and hence the synthesized image will
    look similar to the input image. We now have the latent vectors that correspond
    to the image of interest.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化的潜在向量现在将合成一个图像，该图像在VGG给出的特征几乎与输入图像相同，因此合成图像将类似于输入图像。我们现在有了对应于感兴趣图像的潜在向量。
- en: 'Perform style transfer between images. As discussed, the core logic behind
    style transfer is actually the transfer of parts of style tensors, that is, a
    subset of 18 of the 18 x 512 style tensors. Here, we will be transferring the
    first two rows (of the 18 x 512 tensors) in one case, 3-15 rows in one case, and
    15-18 rows in one case. Since each set of vectors is responsible for generating
    different aspects of the image, each set of swapped vectors swaps different features
    in the image:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像之间执行风格转移。如讨论的那样，风格转移背后的核心逻辑实际上是风格张量的部分转移，即18 x 512风格张量的18个子集之一。在这里，我们将在一个案例中传输前两行（18
    x 512张量的两行），在一个案例中传输第3-15行，以及在一个案例中传输第15-18行。由于每组向量负责生成图像的不同方面，每组交换的向量集合交换图像中的不同特征：
- en: '[PRE53]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code generates the following output:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码生成以下输出：
- en: '![A collage of a child and child  Description automatically generated with
    medium confidence](img/B18457_13_12.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![一个儿童和儿童的拼贴，自动生成的描述，中等置信度](img/B18457_13_12.png)'
- en: 'Figure 13.12: Original images (left side) and the corresponding style transfer
    images (right side)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.12：原始图像（左侧）和对应的风格转移图像（右侧）
- en: In *Figure 13.12*, since we are swapping very early in the pipeline, the most
    high-level features, such as age, are swapped. While swapping the next level of
    features (4 to 15), we will see that the next level of features, such as color
    palette and background, get swapped. Finally, the layers (15,18) don’t seem to
    change the images at all since these features are very subtle and affect very
    fine details in the pictures, such as lighting. Here’s the output with `idxs_to_swap`
    as `slice(4,15)` and `slice (15,18)` respectively.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 13.12*中，由于我们在流水线的早期阶段进行了交换，因此最高级别的特征，如年龄，被交换了。在交换下一个层级的特征（4到15），我们将看到下一个层级的特征，如颜色调色板和背景，被交换了。最后，层次（15,18）似乎根本没有改变图像，因为这些特征非常微妙，影响图片中非常细微的细节，如光照。这里的输出带有`idxs_to_swap`分别为`slice(4,15)`和`slice
    (15,18)`。
- en: '![A collage of two people  Description automatically generated with low confidence](img/B18457_13_13.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![两个人的拼贴，自动生成的描述，低置信度](img/B18457_13_13.png)'
- en: 'Figure 13.13: Style transfer with layer swaps at different levels'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：不同层级上的层交换风格转移
- en: 'Next, we extrapolate a style vector so that the new vectors will only change
    the expression of our custom image. To do this, you need to compute the right
    direction to move the latent vector in. We can achieve this by first creating
    a lot of fake images. An SVM classifier is then used to identify whether the people
    within images are smiling or not. This SVM hence creates a hyperplane that separates
    smiling from non-smiling faces. The required direction to move is going to be
    normal to this hyperplane, which is presented as `stylegan_ffhq_smile_w_boundary.npy`.
    Implementation details can be found in the `InterfaceGAN/edit.py` code:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们推断出一个风格向量，使得新向量只会改变我们自定义图像的表情。为此，您需要计算将潜在向量移动的正确方向。我们可以通过首先创建大量的假图像来实现这一点。然后使用
    SVM 分类器来识别图像中的人是否微笑。因此，这个 SVM 创建了一个分隔笑脸和非笑脸的超平面。移动的所需方向将是垂直于这个超平面，这被呈现为`stylegan_ffhq_smile_w_boundary.npy`。实现细节可以在`InterfaceGAN/edit.py`代码中找到：
- en: '[PRE54]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Here’s how the generated images look:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了生成的图像的外观：
- en: '![A collage of a person''s face  Description automatically generated with medium
    confidence](img/B18457_13_14.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![一个人脸拼贴，自动生成的描述，中等置信度](img/B18457_13_14.png)'
- en: 'Figure 13.14: Progression of emotion from frown to smile'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14：从皱眉到微笑的情感变化进展
- en: In summary, we have learned how research has progressed in generating very high-resolution
    images of faces using GANs. The trick is to increase the complexity of both the
    generator and discriminator in steps of increasing resolution so that at each
    step, both the models are decent at their tasks. We learned how you can manipulate
    the style of a generated image by ensuring that the features at every resolution
    are dictated by an independent input called a style vector. We also learned how
    to manipulate the styles of different images by swapping styles from one image
    to another.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们学习了如何利用 GAN 生成非常高分辨率的人脸图像的研究进展。关键是通过逐步增加生成器和鉴别器的复杂性，以增加分辨率的步骤，使得在每一步中，这两个模型在其任务上都表现得不错。我们学习了如何通过确保每个分辨率上的特征由独立输入（称为风格向量）来操纵生成图像的风格。我们还学习了如何通过从一幅图像中交换风格来操纵不同图像的风格。
- en: '**VToonify** can be used to generate high-quality artistic variations from
    an input video. The paper and associated code can be found here: [https://github.com/williamyang1991/VToonify](https://github.com/williamyang1991/VToonify).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**VToonify** 可用于从输入视频生成高质量的艺术变化。有关论文和相关代码，请访问这里：[https://github.com/williamyang1991/VToonify](https://github.com/williamyang1991/VToonify)。'
- en: Now that we have learned about leveraging the pre-trained StyleGAN2 model to
    perform style transfer, in the next section, we will leverage the pre-trained
    SRGAN model to generate images in high resolution.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会如何利用预训练的 StyleGAN2 模型执行风格转移，在接下来的部分，我们将利用预训练的 SRGAN 模型生成高分辨率的图像。
- en: Introducing SRGAN
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入 SRGAN
- en: In the previous section, we saw a scenario in which we used a pre-trained StyleGAN
    to generate images in a given style. In this section, we will take it a step further
    and learn about using pre-trained models to perform image super-resolution. We
    will gain an understanding of the architecture of the SRGAN model before implementing
    it on images.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到了一个场景，我们在其中使用预训练的StyleGAN来生成给定风格的图像。在本节中，我们将进一步学习如何使用预训练模型来执行图像超分辨率。在实施之前，我们将了解SRGAN模型的架构。
- en: 'First, we will explain why a GAN is a good solution for the task of super-resolution.
    Imagine a scenario in which you are given an image and asked to increase its resolution.
    Intuitively, you would consider various interpolation techniques to perform super-resolution.
    Here’s a sample low-resolution image along with the outputs of various techniques:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将解释为什么GAN是超分辨率任务的一个好解决方案。想象一种情况，你拿到一张图像，并被要求提高其分辨率。直觉上，你会考虑各种插值技术来进行超分辨率。以下是一个低分辨率图像示例，以及各种技术的输出：
- en: '![](img/B18457_13_15.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_13_15.png)'
- en: 'Figure 13.15: The performance of different techniques of image super-resolution'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.15：不同图像超分辨率技术的性能
- en: '(source: [https://arxiv.org/pdf/1609.04802.pdf](https://arxiv.org/pdf/1609.04802.pdf))'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[https://arxiv.org/pdf/1609.04802.pdf](https://arxiv.org/pdf/1609.04802.pdf)）
- en: In the preceding image, we can see that traditional interpolation techniques
    such as bicubic interpolation do not help as much when reconstructing an image
    from a low resolution (in this case, a 4X down-scaled image of the original image).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图像中，我们可以看到传统的插值技术，如双三次插值，在从低分辨率重建图像（在本例中为原始图像的4倍降低缩放图像）方面并没有太大帮助。
- en: While a super-resolution ResNet-based UNet could be useful in this scenario,
    GANs can be more useful as they simulate human perception. The discriminator,
    given that it knows what a typical super-resolution image looks like, can detect
    a scenario where the generated image has properties that do not necessarily look
    like an image with high resolution.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于超分辨率ResNet的UNet在这种情况下可能很有用，但是GAN可以更有用，因为它们模拟了人类的感知。鉴别器知道典型的超分辨率图像是什么样子，可以检测到生成的图像具有不像高分辨率图像的特性的情况。
- en: With the usefulness of GANs for super-resolution established, let’s understand
    and leverage the pre-trained model.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过验证GAN在超分辨率中的实用性，让我们来了解并利用预训练模型。
- en: Architecture
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: While it is possible to code and train an SRGAN from scratch, we will use pre-trained
    models where we can. Hence, for this section, we will leverage the model developed
    by Christian Ledig and his team and published in the paper titled *Photo-Realistic
    Single Image Super-Resolution Using a Generative Adversarial Network*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以从头开始编码和训练SRGAN，但我们将在可能的情况下使用预训练模型。因此，在本节中，我们将利用Christian Ledig及其团队开发并发表在标题为《使用生成对抗网络的逼真单图像超分辨率》的论文中的模型。
- en: 'The architecture of an SRGAN is as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: SRGAN的架构如下：
- en: '![Chart, bar chart  Description automatically generated](img/B18457_13_16.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![图表，条形图 自动生成的描述](img/B18457_13_16.png)'
- en: 'Figure 13.16: SRGAN architecture'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.16：SRGAN架构
- en: '(source: [https://arxiv.org/pdf/1609.04802.pdf](https://arxiv.org/pdf/1609.04802.pdf))'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[https://arxiv.org/pdf/1609.04802.pdf](https://arxiv.org/pdf/1609.04802.pdf)）
- en: In the preceding image, we can see that the discriminator takes high-resolution
    images as input to train a model that predicts whether an image is a high-resolution
    or a low-resolution image. The generator network takes a low-resolution image
    as input and comes up with a high-resolution image. While training the model,
    both content loss and adversarial loss are minimized. For a detailed explanation
    of the details of model training and a comparison of the results obtained from
    the various techniques used to come up with high-resolution images, we recommend
    that you go through Ledig’s paper.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图像中，我们可以看到鉴别器将高分辨率图像作为输入来训练一个模型，预测一个图像是高分辨率还是低分辨率图像。生成器网络以低分辨率图像作为输入，并生成高分辨率图像。在训练模型时，同时最小化内容损失和对抗损失。如果你希望详细了解模型训练的细节以及各种技术用于生成高分辨率图像的比较结果，我们建议你阅读Ledig的论文。
- en: With a high-level understanding of how the model is built, we will now code
    the way to leverage a pre-trained SRGAN model to convert a low-resolution image
    into a high-resolution image.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了模型构建的高层次之后，我们现在将编写代码来利用预训练的SRGAN模型将低分辨率图像转换为高分辨率图像。
- en: Coding SRGAN
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码SRGAN
- en: 'Here are the steps for loading the pre-trained SRGAN and making our predictions:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载预训练的 SRGAN 并进行预测的步骤：
- en: 'The following code is available as `Image super resolution using SRGAN.ipynb`
    in the `Chapter13` folder of this book’s GitHub repository: [https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e)
    The code contains URLs to download data from. We strongly recommend you execute
    the notebook in GitHub to reproduce results while you go through the steps to
    perform and the explanations of the various code components in the text.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在本书 GitHub 仓库的 `Chapter13` 文件夹中可作为 `Image super resolution using SRGAN.ipynb`
    使用：[https://bit.ly/mcvp-2e](https://bit.ly/mcvp-2e) 代码包含了从中下载数据的 URL。我们强烈建议您在
    GitHub 上执行笔记本，以便在您进行步骤执行和文本中各种代码组件的解释时重现结果。
- en: 'Import the relevant packages and the pre-trained model:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包和预训练模型：
- en: '[PRE55]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Load the model:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型：
- en: '[PRE56]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Fetch the image to convert to a higher resolution:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取要转换为更高分辨率的图像：
- en: '[PRE57]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Define the functions to preprocess and postprocess the image:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义预处理和后处理图像的函数：
- en: '[PRE58]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Load the image and preprocess it:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并预处理图像：
- en: '[PRE59]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note that, in the preceding code, we have performed an additional resize on
    the original image to further blur the image, but this is done only for illustration
    because the improvement is more visible when we use a down-scaled image.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述代码中，我们对原始图像执行了额外的调整大小，以进一步模糊图像，但这仅用于说明，因为当我们使用缩小的图像时，改进效果更为明显。
- en: 'Pass the preprocessed image through the loaded `model` and postprocess the
    output of the model:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过加载的 `model` 对预处理过的图像进行处理，并对模型输出进行后处理：
- en: '[PRE60]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Plot the original and the high-resolution images:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像和高分辨率图像：
- en: '[PRE61]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The preceding code results in the following output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/B18457_13_17.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18457_13_17.png)'
- en: 'Figure 13.17: Original image and the corresponding SRGAN output'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.17：原始图像及其对应的 SRGAN 输出
- en: In the preceding image, we can see that the high-resolution image captured details
    that were blurred in the original image.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图像中，我们可以看到高分辨率图像捕捉到了原始图像中模糊的细节。
- en: Note that the contrast between the original and the high-resolution image will
    be high if the original image is blurred or low resolution. However, if the original
    image is not blurred, the contrast will not be that high. We encourage you to
    work with images of varying resolutions.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果原始图像模糊或低分辨率，原始图像和高分辨率图像之间的对比度将会很高。然而，如果原始图像不模糊，则对比度不会那么高。我们鼓励您使用不同分辨率的图像进行工作。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have learned about generating images from the contours of
    an image using the Pix2Pix GAN. Further, we learned about the various loss functions
    in CycleGAN to convert images of one class to another. Next, we learned about
    how StyleGAN can be used to generate realistic faces and also copy the style from
    one image to another, depending on how the generator is trained. Finally, we learned
    about using the pre-trained SRGAN model to generate high-resolution images. All
    of these techniques lay a strong foundation as we advance to learn about more
    modern ways of transferring image attributes in *Chapters 16* and *17*.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用 Pix2Pix GAN 从图像的轮廓生成图像。此外，我们还学习了 CycleGAN 中的各种损失函数，将一类图像转换为另一类图像。接下来，我们学习了如何使用
    StyleGAN 生成逼真的人脸，并根据生成器的训练方式复制图像之间的风格。最后，我们学习了如何使用预训练的 SRGAN 模型生成高分辨率图像。所有这些技术为我们进一步学习在
    *第16* 和 *第17* 章中更现代的图像属性转移方法打下了坚实基础。
- en: In the next chapter, we will switch gears and learn about combining computer
    vision techniques with other prominent techniques in reinforcement learning.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将改变方向，学习如何将计算机视觉技术与强化学习中的其他突出技术结合使用。
- en: Questions
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why do we need a Pix2Pix GAN if a supervised learning algorithm such as UNet
    could have worked to generate images from contours?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要 Pix2Pix GAN，如果像 UNet 这样的监督学习算法可以从轮廓生成图像？
- en: Why do we need to optimize for three different loss functions in CycleGAN?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要针对 CycleGAN 中的三种不同损失函数进行优化？
- en: How do the tricks used by ProgressiveGAN help in building a StyleGAN model?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ProgressiveGAN 使用的技巧如何帮助构建 StyleGAN 模型？
- en: How do we identify the latent vectors that correspond to a given custom image?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何识别与给定自定义图像相对应的潜在向量？
- en: Learn more on Discord
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者讨论：
- en: '[https://packt.link/modcv](https://packt.link/modcv)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/modcv](https://packt.link/modcv)'
- en: '![](img/QR_Code237402495622324343.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code237402495622324343.png)'
