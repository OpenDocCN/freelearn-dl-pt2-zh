- en: Chapter 4. Unsupervised Feature Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。无监督特征学习
- en: One of the reasons why deep neural networks can succeed where other traditional
    machine learning techniques struggle is the capability of learning the right representations
    of entities in the data (features) without needing (much) human and domain knowledge.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络能够成功的一个原因是能够学习数据中实体（特征）的正确表示，而不需要（太多）人类和领域知识。
- en: Theoretically, neural networks are able to consume raw data directly as it is
    and map the input layers to the desired output via the hidden intermediate representations.
    Traditional machine learning techniques focus mainly on the final mapping and
    assume the task of "feature engineering" to have already been done.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，神经网络能够直接消耗原始数据，并通过隐藏的中间表示将输入层映射到所需的输出。传统的机器学习技术主要专注于最终映射，假定“特征工程”的任务已经完成。
- en: Feature engineering is the process that uses the available domain knowledge
    to create smart representations of the data, so that it can be processed by the
    machine learning algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是利用现有的领域知识创建智能数据表示的过程，以便它可以被机器学习算法处理。
- en: Andrew Yan-Tak Ng is a professor at Stanford University and one of the most
    renowned researchers in the field of machine learning and artificial intelligence.
    In his publications and talks, he describes the limitations of traditional machine
    learning when applied to solving real-world problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Yan-Tak Ng是斯坦福大学的教授，也是机器学习和人工智能领域最著名的研究者之一。他在出版物和讲话中描述了传统机器学习在解决实际问题时的局限性。
- en: 'The hardest part of making a machine learning system work is to find the right
    feature representations:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使机器学习系统正常工作最困难的部分是找到正确的特征表示：
- en: '*Coming up with features is difficult, time-consuming, requires expert knowledge.
    When working applications of learning, we spend a lot of time tuning features.*'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提出特征是困难的，耗时的，需要专业知识。在应用学习应用程序时，我们花费了大量时间调整特征。*'
- en: ''
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Anrew Ng, Machine Learning and AI via Brain simulations, Stanford University*'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*安德鲁·吴，机器学习和人工智能通过大脑模拟，斯坦福大学*'
- en: 'Let''s assume we are classifying pictures into a few categories, such as animals
    versus vehicles. The raw data is a matrix of the pixels in the image. If we used
    those pixels directly in a logistic regression or a decision tree, we would create
    rules (or associating weights) for every single picture that might work for the
    given training samples, but that would be very hard to generalize enough to small
    variations of the same pictures. In other words, let''s suppose that my decision
    tree finds that there are five important pixels whose brightness (supposing we
    are displaying only black and white tones) can determine where most of the training
    data get grouped into the two classes--animals and vehicles. The same pictures,
    if cropped, shifted, rotated, or re-colored, would not follow the same rules as
    before. Thus, the model would probably randomly classify them. The main reason
    is that the features we are considering are too weak and unstable. However, we
    could instead first preprocess the data such that we could extract features like
    these:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们正在将图片分类为几个类别，例如动物与车辆。原始数据是图像中的像素矩阵。如果我们直接在逻辑回归或决策树中使用这些像素，我们将为可能适用于给定的训练样本的每一张图片创建规则（或关联权重），但这将非常难以概括到相同图片的轻微变化。换句话说，假设我的决策树发现有五个重要的像素，它们的亮度（假设我们只显示黑白色调）可以确定大多数训练数据被分成两类--动物和车辆。相同的照片，如果裁剪、移位、旋转或重新着色，将不再遵循以前的那些规则。因此，模型可能会对它们进行随机分类。主要原因是我们正在考虑的特征太弱而不稳定。然而，我们可以首先预处理数据，以便提取这样的特征：
- en: Does the picture contain symmetric centric, shapes like wheels?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图片是否包含对称的，像车轮一样的形状？
- en: Does it contain handlebars or a steering wheel?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否包含把手或方向盘？
- en: Does it contain legs or heads?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否包含腿或头？
- en: Does it have a face with two eyes?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否有两只眼睛的脸？
- en: 'In such cases, the decision rules would be quite easy and robust, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，决策规则会非常容易和强大，如下所示：
- en: '![Unsupervised Feature Learning](img/00118.jpeg)![Unsupervised Feature Learning](img/00119.jpeg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![无监督特征学习](img/00118.jpeg)![无监督特征学习](img/00119.jpeg)'
- en: How much effort is needed in order to extract those relevant features?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 需要多少努力才能提取这些相关特征？
- en: Since we don't have handlebar detectors, we could try to hand-design features
    to capture some statistical properties of the picture, for example, finding edges
    in different orientations in different picture quadrants. We need to find a better
    way to represent images than pixels.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有把手检测器，我们可以尝试手动设计特征来捕捉图片的一些统计特性，例如，在不同的图片象限中找到不同方向的边缘。我们需要找到比像素更好的图像表示方法。
- en: 'Moreover, robust and significant features are generally made out of hierarchies
    of previously extracted features. We could start extracting edges in the first
    step, then take the generated "edges vector", and combine them to recognize object
    parts, such as an eye, a nose, a mouth, rather than a light, a mirror, or a spoiler.
    The resulting object parts can again be combined into object models; for example,
    two eyes, one nose, and one mouth form a face, or two wheels, a seat, and a handlebar
    form a motorcycle. The whole detection algorithm could be simplified in the following
    way:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，强大和显著的特征通常是由先前提取的特征层次结构制成的。我们可以在第一步开始提取边缘，然后取得生成的“边缘向量”，并将它们组合起来识别物体部分，比如眼睛、鼻子、嘴巴，而不是光、镜子或者扰流板。最终的物体部分可以再次组合成对象模型；例如，两只眼睛，一只鼻子和一张嘴巴形成一张脸，或者两个车轮、一个座椅和一个把手形成一辆摩托车。整个检测算法可以以以下方式简化：
- en: '![Unsupervised Feature Learning](img/00120.jpeg)![Unsupervised Feature Learning](img/00121.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![无监督特征学习](img/00120.jpeg)![无监督特征学习](img/00121.jpeg)'
- en: By recursively applying sparse features, we manage to get higher-level features.
    This is why you need deeper neural network architectures as opposed to the shallow
    algorithms. The single network can learn how to move from one representation to
    the following, but stacking them together will enable the whole end-to-end workflow.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过递归应用稀疏特征，我们设法获得更高级的特征。这就是为什么你需要比浅层算法更深的神经网络架构。单个网络可以学习如何从一个表示转移到另一个，但是将它们堆叠在一起将使整个端到端的工作流能够实现。
- en: The real power is not just in the hierarchical structures though. It is important
    to note that we have only used unlabeled data so far. We are learning the hidden
    structures by reverse-engineering the data itself instead of relying on manually
    labeled samples. The supervised learning represents only the final classification
    steps, where we need to assign to either the vehicle class or the animal class.
    All of the previous steps are performed in an unsupervised fashion.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，真正的威力并不仅在于层次结构。重要的是要注意到，到目前为止我们只使用了无标签数据。我们通过对数据本身进行逆向工程来学习隐藏的结构，而不是依赖于手动标记的样本。监督学习仅表示最终的分类步骤，我们需要将其分配到车辆类别或动物类别。所有先前的步骤都是以无监督的方式执行的。
- en: We will see how the specific feature extraction for pictures is done in the
    following [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*. In this chapter, we will
    focus on the general approach of learning feature representations for any type
    of data (for example, time signals, text, or general attribute vectors).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下[第5章](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f "第5章
    图像识别")中看到如何为图片执行特定的特征提取，*图像识别*。在本章中，我们将着重介绍学习任何类型数据（例如时间信号、文本或一般的属性向量）的特征表示的一般方法。
- en: 'For that purpose, we will cover two of the most powerful and quite used architectures
    for unsupervised feature learning: autoencoders and restricted Boltzmann machines.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将介绍两种最强大且广泛使用的无监督特征学习架构：自动编码器和受限波尔兹曼机。
- en: Autoencoders
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器
- en: 'Autoencoders are symmetric networks used for unsupervised learning, where output
    units are connected back to input units:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是用于无监督学习的对称网络，其中输出单元连接回输入单元：
- en: '![Autoencoders](img/00122.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![自动编码器](img/00122.jpeg)'
- en: Autoencoder simple representation from H2O training book (https://github.com/h2oai/h2o-training-book/blob/master/hands-on_training/images/autoencoder.png)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 训练手册中的自动编码器简单表示 (https://github.com/h2oai/h2o-training-book/blob/master/hands-on_training/images/autoencoder.png)
- en: The output layer has the same size of the input layer because its purpose is
    to reconstruct its own inputs rather than predicting a dependent target value.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的大小与输入层相同，因为它的目的是重构自己的输入，而不是预测一个依赖目标值。
- en: 'The goal of those networks is to act as a compression filter via an encoding
    layer, Φ that fits the input vector *X* into a smaller latent representation (the
    code) *c*, and then a decoding layer, Φ tries to reconstruct it back to *X''*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络的目标是通过编码层 Φ 充当压缩滤波器，将输入向量 *X* 适合到较小的潜在表示（编码） *c*，然后解码层 Φ 试图将其重构回 *X'*：
- en: '![Autoencoders](img/00123.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器](img/00123.jpeg)'
- en: 'The loss function is the reconstruction error, which will force the network
    to find the most efficient compact representation of the training data with minimum
    information loss. For numerical input, the loss function can be the mean squared
    error:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是重构误差，它将迫使网络找到训练数据的最有效的紧凑表示，同时最小化信息损失。对于数值输入，损失函数可以是均方误差：
- en: '![Autoencoders](img/00124.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器](img/00124.jpeg)'
- en: 'If the input data is not numerical but is represented as a vector of bits or
    multinomial distributions, we can use the cross-entropy of the reconstruction:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入数据不是数值型，而是表示为比特向量或多项分布的向量，我们可以使用重构的交叉熵：
- en: '![Autoencoders](img/00125.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器](img/00125.jpeg)'
- en: Here, *d* is the dimensionality of the input vectors.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d* 是输入向量的维度。
- en: The central layer (the code) of the network is the compressed representation
    of the data. We are effectively translating an n-dimensional array into a smaller
    m-dimensional array, where *m < n*. This process is very similar to dimensionality
    reduction using **Principal Component Analysis** (**PCA**). PCA divides the input
    matrix into orthogonal axes (called components) in such, way that you can reconstruct
    an approximation of the original matrix by projecting the original points on those
    axes. By sorting them by their importance, we can extract the top *m* components
    that can be though as high-level features of the original data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的中央层（编码）是数据的压缩表示。我们实际上将一个 n 维数组转换为一个较小的 m 维数组，其中 *m < n*。这个过程与使用**主成分分析**（**PCA**）进行降维非常相似。PCA
    将输入矩阵分成正交轴（称为分量），以便您可以通过在这些轴上投影原始点来重构原始矩阵的近似值。通过按重要性对它们进行排序，我们可以提取出前 *m* 个组件，这些组件可以被视为原始数据的高级特征。
- en: 'For example, in a multivariate Gaussian distribution, we could represent each
    point as a coordinate over the two orthogonal components that would describe the
    largest possible variance in the data:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在多元高斯分布中，我们可以将每个点表示为两个正交分量上的坐标，这两个分量描述了数据中可能的最大方差：
- en: '![Autoencoders](img/00126.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器](img/00126.jpeg)'
- en: A scatter plot of samples that are distributed according a multivariate (bivariate)
    Gaussian distribution centered at (1,3) with a standard deviation of 3 in the
    (0.866, 0.5) direction and of 1 in the orthogonal direction. The directions represent
    the principal components (PC) associated with the sample. By Nicoguaro (own work)
    CC BY 4.0 (http://creativecommons.org/licenses/by/4.0), via Wikimedia Commons.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个样本散点图，按照以(1,3)为中心，(0.866, 0.5)方向上标准差为3，在正交方向上标准差为1的多元（双变量）高斯分布进行分布。这些方向表示与样本相关联的主成分（PC）。由
    Nicoguaro（自己的作品）CC BY 4.0 (http://creativecommons.org/licenses/by/4.0)，通过维基媒体公共领域。
- en: The limitation of PCA is that it allows only linear transformation of the data,
    which is not always enough.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的局限性在于它只允许对数据进行线性变换，这并不总是足够的。
- en: Autoencoders have the advantage of being able to represent even non-linear representations
    using a non-linear activation function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的优势在于可以使用非线性激活函数表示非线性表示。
- en: 'One famous example of an autoencoder was given by MITCHELL, T. M. in his book
    *Machine Learning*, wcb, 1997\. In that example, we have a dataset with eight
    categorical objects encoded in binary with eight mutually exclusive labels with
    bits. The network will learn a compact representation with just three hidden nodes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的一个著名示例是 MITCHELL 在他的书 *机器学习* 中给出的。在这个例子中，我们有一个数据集，其中包含八个分类对象，用八个相互排斥的比特标记的二进制编码。网络将学习一个仅具有三个隐藏节点的紧凑表示：
- en: '![Autoencoders](img/00127.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器](img/00127.jpeg)'
- en: Tom Mitchell's example of an autoencoder.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Tom Mitchell 的自编码器示例。
- en: By applying the right activation function, the learn-compact representation
    corresponds exactly with the binary representation with three bits.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用正确的激活函数，学习到的紧凑表示与三比特二进制表示完全对应。
- en: There are situations though where just the single hidden layer is not enough
    to represent the whole complexity and variance of the data. Deeper architecture
    can learn more complicated relationships between the input and hidden layers.
    The network is then able to learn latent features and use those to best represent
    the non-trivial informative components in the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，仅仅单个隐藏层不足以表示数据的整个复杂性和变异性。更深的架构可以学习输入和隐藏层之间更复杂的关系。然后，网络能够学习潜在特征并利用这些特征来最好地表示数据中的非平凡信息组成部分。
- en: 'A deep autoencoder is obtained by concatenating two symmetrical networks typically
    made of up to five shallow layers:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过连接两个对称网络获得深度自动编码器，通常由最多五个浅层组成：
- en: '![Autoencoders](img/00128.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![自动编码器](img/00128.jpeg)'
- en: Schematic structure of an autoencoder with 3 fully-connected hidden layers (https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的示意结构，具有3个完全连接的隐藏层（https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png）
- en: Deep autoencoders can learn new latent representations, combining the previously
    learned ones so that each hidden level can be seen as some compressed hierarchical
    representation of the original data. We could then use the code or any other hidden
    layer of the encoding network as valid features describing the input vector.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 深度自动编码器可以学习新的潜在表示，将先前学到的表示组合起来，以便每个隐藏级别可以被视为原始数据的某种压缩层次表示。然后，我们可以使用编码网络的代码或任何其他隐藏层作为描述输入向量的有效特征。
- en: Network design
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络设计
- en: 'Probably the most common question when building a deep neural network is: how
    do we choose the number of hidden layers and the number of neurons for each layer?
    Furthermore, which activation and loss functions do we use?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建深度神经网络时，最常见的问题可能是：我们如何选择隐藏层的数量和每个层的神经元数量？此外，我们使用哪种激活和损失函数？
- en: There is no closed answer. The empirical approach consists of running a sequence
    of trial and error or a standard grid search, where the depth and the size of
    each layer are simply defined as tuning hyperparameters. We will look at a few
    design guidelines.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 没有确定的答案。经验方法包括运行一系列试验和错误或标准网格搜索，其中深度和每个层的大小简单地被定义为调整超参数。我们将看一些设计准则。
- en: 'For autoencoders, the problem is slightly simplified. Since there are many
    variants of autoencoders, we will define the guidelines for the general use case.
    Please keep in mind that each variation will have its own rules to be considered.
    We can suggest the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动编码器，问题略有简化。由于自动编码器有许多变体，我们将定义通用用例的指南。请记住，每个变体都将有其自己的规则需要考虑。我们可以建议以下内容：
- en: The output layer consists of exactly the same size of the input.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的大小与输入完全相同。
- en: The network is symmetric most of the time. Having an asymmetric network would
    mean having different complexities of the encoder and decoder functions. Unless
    you have a particular reason for doing so, there is generally no advantage in
    having asymmetric networks. However, you could decide to share the same weights
    or decide to have different weights in the encoding and decoding networks.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络大多数情况下是对称的。拥有不对称网络意味着编码器和解码器函数的不同复杂性。除非有特殊原因，通常没有对称网络的优势。但是，您可以决定共享相同的权重或者决定在编码和解码网络中具有不同的权重。
- en: During the encoding phase, the hidden layers are smaller than the input, in
    which case, we are talking about "undercomplete autoencoders". A multilayer encoder
    gradually decreases the representation size. The size of the hidden layer, generally,
    is at most half the size of the previous one. If the data input layer has 100
    nodes, then a plausible architecture could be 100-40-20-40-100\. Having bigger
    layers than the input would lead to no compression at all, which means no interesting
    patterns are learned. We will see in the *Regularization* section how this constraint
    is not necessary in case of sparse autoencoders.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在编码阶段，隐藏层比输入小，这种情况下，我们称之为“欠完备自动编码器”。多层编码器逐渐减小表示大小。隐藏层的大小通常最多是前一个的一半。如果数据输入层有100个节点，那么一个合理的架构可能是100-40-20-40-100。比输入更大的层将导致没有任何压缩，这意味着不会学习到有趣的模式。我们将在*正则化*部分看到，这种约束在稀疏自动编码器的情况下并非必要。
- en: The middle layer (the code) covers an important role. In the case of feature
    reduction, we could keep it small and equal to 2, 3, or 4 in order to allow efficient
    data visualizations. In the case of stacked autoencoders, we should set it to
    be larger because it will represent the input layer of the next encoder.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间层（代码）起着重要作用。在特征减少的情况下，我们可以将其保持较小，并且等于2、3或4，以便允许高效的数据可视化。在堆叠的自编码器的情况下，我们应该将其设置得更大，因为它将代表下一个编码器的输入层。
- en: In the case of binary inputs, we want to use sigmoid as the output activation
    function and cross-entropy, or more precisely, the sum of Bernoulli cross-entropies,
    as the loss function.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二进制输入的情况下，我们希望使用sigmoid作为输出激活函数，使用交叉熵，更确切地说，使用伯努利交叉熵的总和，作为损失函数。
- en: For real values, we can use a linear activation function (ReLU or softmax) as
    the output and the **mean squared error** (**MSE**) as the loss function.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于实值，我们可以使用线性激活函数（ReLU或softmax）作为输出，并且使用**均方误差**（**MSE**）作为损失函数。
- en: 'For different types of input data(*x*)and output *u*, you can follow the general
    approach, which consists of the following steps:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于不同类型的输入数据（*x*）和输出*u*，您可以遵循一般方法，其中包括以下步骤：
- en: Finding the probability distribution of observing x, given *u*, P(x/u)
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到观察到x的概率分布，给定*u*，P(x/u)
- en: Finding the relationship between *u* and the hidden layer h(x)
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到*u*和隐藏层h(x)之间的关系
- en: Using ![Network design](img/00123.jpeg)
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ![网络设计](img/00123.jpeg)
- en: In the case of deep networks (with more than one hidden layer), use the same
    activation function for all of them in order to not unbalance the complexity of
    the encoder and decoder.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深层网络（具有多个隐藏层）的情况下，为了不使编码器和解码器的复杂性失衡，使用相同的激活函数。
- en: If we use a linear activation function throughout the whole network, we will
    approximate the behavior of PCA.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在整个网络中使用线性激活函数，我们将近似于PCA的行为。
- en: It is convenient to Gaussian scale (0 mean and unit standard deviation) your
    data unless it is binary, and it is better to leave the input values to be either
    0 or 1\. Categorical data can be represented using one-hot-encoding with dummy
    variables.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非是二进制的，否则最好对您的数据进行高斯缩放（0均值和单位标准差），并且最好将输入值保留为0或1。分类数据可以使用带有虚拟变量的独热编码来表示。
- en: 'Activation functions are as follows:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数如下：
- en: ReLU is generally the default choice for majority of neural networks. Autoencoders,
    given their topology, may benefit from a symmetric activation function. Since
    ReLU tends to overfit more, it is preferred when combined with regularization
    techniques (such as dropout).
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU通常是大多数神经网络的默认选择。由于其拓扑结构，自编码器可能会受益于对称激活函数。由于ReLU往往过拟合，因此在与正则化技术（如dropout）结合时更受欢迎。
- en: If your data is binary or can be scaled in the range of [0, 1], then you would
    probably use a sigmoid activation function. If you used one-hot-encoding for the
    input categorical data, then it's better use ReLU.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的数据是二进制的或者可以缩放到[0,1]的范围内，则可能会使用sigmoid激活函数。如果您对输入分类数据使用了独热编码，则最好使用ReLU。
- en: Hyperbolic tangent (*tanh*) is a good choice for computation optimization in
    case of gradient descent. Since data will be centered around 0, the derivatives
    will be higher. Another effect is reducing bias in the gradients as is well explained
    in the "Efficient BackProp" paper ([http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)).
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切（*tanh*）是在梯度下降情况下进行计算优化的不错选择。由于数据将围绕0中心化，导数将更高。另一个效果是减少梯度中的偏差，正如《高效的反向传播》一文中所解释的那样（[http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)）。
- en: '![Network design](img/00129.jpeg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![网络设计](img/00129.jpeg)'
- en: Different activation functions commonly used for deep neural networks
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度神经网络常用的不同激活函数
- en: Regularization techniques for autoencoders
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器的正则化技术
- en: In the previous chapters, we already saw different forms of regularizations,
    such as L1, L2, early stopping, and dropout. In this section, we will describe
    a few popular techniques specifically tailored for autoencoders.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到了不同形式的正则化，例如L1，L2，提前停止和dropout。在本节中，我们将描述一些专门为自编码器量身定制的几种流行技术。
- en: So far, we have always described autoencoders as "undercomplete", which means
    the hidden layers are smaller than the input layer. This is because having a bigger
    layer would have no compression at all. The hidden units may just copy exactly
    the input and return an exact copy as the output.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直把自动编码器描述为"欠完备"，这意味着隐藏层比输入层小。这是因为拥有更大的层根本没有任何压缩。隐藏单元可能只是精确复制输入并将精确复制作为输出返回。
- en: On the other hand, having more hidden units would allow us to have more freedom
    on learning smarter representations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，拥有更多的隐藏单元将使我们有更多的自由学习智能表示。
- en: 'We will see how we can address this problem with three approaches: denoising
    autoencoders, contractive autoencoders, and sparse autoencoders.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何用三种方法解决这个问题：去噪自动编码器，压缩自动编码器和稀疏自动编码器。
- en: Denoising autoencoders
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Denoising自动编码器
- en: The idea is that we want to train our model to learn how to reconstruct a noisy
    version of the input data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 想法是我们想训练我们的模型学习如何重建输入数据的嘈杂版本。
- en: We will use x to represent the original input, ![Denoising autoencoders](img/00130.jpeg),
    the noisy input, and ![Denoising autoencoders](img/00131.jpeg), the reconstructed
    output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用x表示原始输入，![Denoising autoencoders](img/00130.jpeg)表示带有噪音的输入，![Denoising
    autoencoders](img/00131.jpeg)表示重建的输出。
- en: The noisy input, ![Denoising autoencoders](img/00130.jpeg), is generated by
    randomly assigning a subset of the input ![Denoising autoencoders](img/00130.jpeg)
    to 0, with a given probability 𝑝, plus an additive isotropic Gaussian noise, with
    variance *v* for numerical inputs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 带有噪声的输入，![Denoising autoencoders](img/00130.jpeg)，是通过随机分配输入![Denoising autoencoders](img/00130.jpeg)的子集为0，概率为𝑝，再加上具有方差*v*的加性各向同性高斯噪声而生成的数值输入。
- en: We would then have two new hyper-parameters to tune ?? and ![Denoising autoencoders](img/00132.jpeg),
    which represent the noise level.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将有两个新的超参数要调整??和![Denoising autoencoders](img/00132.jpeg)，它们代表噪音水平。
- en: 'We will use the noisy variant, ![Denoising autoencoders](img/00130.jpeg), as
    the input of the network, but the loss function will still be the error between
    the output ![Denoising autoencoders](img/00131.jpeg) and the original noiseless
    input ![Denoising autoencoders](img/00130.jpeg). If the input dimensionality is
    *d*, the encoding function *f*, and the decoding function *g*, we will write the
    loss function *j* as this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用带噪声的变体，![Denoising autoencoders](img/00130.jpeg)，作为网络的输入，但损失函数仍然是输出![Denoising
    autoencoders](img/00131.jpeg)与原始无噪声输入![Denoising autoencoders](img/00130.jpeg)之间的误差。如果输入维度是*d*，编码函数*f*，解码函数*g*，我们将把损失函数*j*写成这样：
- en: '![Denoising autoencoders](img/00133.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Denoising autoencoders](img/00133.jpeg)'
- en: Here, *L* is the reconstruction error, typically either the MSE or the cross-entropy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*L*是重构误差，通常是均方误差或交叉熵。
- en: With this variant, if a hidden unit tries to exactly copy the input values,
    then the output layer cannot trust 100% because it knows that it could be the
    noise and not the original input. We are forcing the model to reconstruct based
    on the interrelationships between other input units, aka the meaningful structures
    of the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种变体，如果一个隐藏单元试图精确复制输入值，那么输出层就无法完全信任，因为它知道这可能是噪音而不是原始输入。我们正在强迫模型基于其他输入单元之间的相互关系来重建数据的有意义结构。
- en: What we would expect is that the higher the added noise, the bigger the filters
    applied at each hidden unit. By filter, we mean the portion of the original input
    that is activated for that particular feature to be extracted. In case of no noise,
    hidden units tend to extract a tiny subset of the input data and propose it at
    the most untouched version to the next layer. By adding noise to the units, the
    error penalty on badly reconstructing ![Denoising autoencoders](img/00134.jpeg)
    will force the network to keep more information in order to contextualize the
    features regardless of the possible presence of noise.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望的是添加的噪声越大，在每个隐藏单元上应用的滤波器就越大。所谓的滤波器是指针对提取特定特征而激活的原始输入的部分。如果没有噪音，隐藏单元倾向于提取输入数据的一个小子集，并将其作为最不触及的版本提供给下一层。通过向单元添加噪声，对坏重构![Denoising
    autoencoders](img/00134.jpeg)的错误惩罚将迫使网络保留更多信息，以便在可能存在噪音的情况下对特征进行上下文化。
- en: Please pay attention that just adding a small white noise could be equivalent
    to using weight decay regularization. Weight decay is a technique that consists
    of multiplying to a factor less than 1 the weights at each training epoch in order
    to limit the free parameters in our model. Although this is a popular technique
    to regularize neural networks, by setting inputs to 0 with probability *p*, we
    are effectively achieving a totally different result.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只需添加一个小的白噪声就相当于使用权重衰减正则化。权重衰减是一种技术，它在每个训练时期将权重乘以小于1的因子，以便限制模型中的自由参数。虽然这是一种常用的神经网络正则化技术，但通过将输入设置为0的概率*p*，我们实际上实现了完全不同的结果。
- en: We don't want to obtain high-frequency filters that when put together give us
    a more generalized model. Our denoising approach generates filters that do represent
    unique features of the underlying data structures and have individual meanings.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望获得高频滤波器，这些滤波器组合在一起会给出更广义的模型。我们的去噪方法生成代表潜在数据结构的独特特征并具有独立含义的滤波器。
- en: Contractive autoencoders
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收缩自编码器
- en: Contractive autoencoders aim to achieve a similar goal to that of the denoising
    approach by explicitly adding a term that penalizes when the model tries to learn
    uninteresting variations and promote only those variations that are observed in
    the training set.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩自编码器旨在通过明确添加一个惩罚项来实现类似于去噪方法的目标，当模型试图学习无趣的变化并且仅促进在训练集中观察到的那些变化时，它就会受到惩罚。
- en: In other words, the model may try to approximate the identity function by coming
    out with filters representing variations that are not necessarily present in the
    training data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，模型可能会试图通过产生代表训练数据中并非必然存在的变化的滤波器来逼近恒等函数。
- en: We can express this sensitivity as the sum of squares of all partial derivatives
    of the extracted features with respect to the input dimensions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这种敏感性表示为所提取特征对输入维度的所有偏导数的平方和。
- en: 'For an input *x* of dimensionality ![Contractive autoencoders](img/00135.jpeg)
    mapped by the encoding function *f* to the hidden representation *h* of size *d*[*h*],
    the following quantity corresponds to the L2 norm (Frobenius) of the Jacobian
    matrix ![Contractive autoencoders](img/00136.jpeg) of the encoder activations:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由编码函数*f*映射到大小为*d*[h]的隐藏表示*h*的维度为*x*的输入，以下数量对应于编码器激活的雅可比矩阵的L2范数（Frobenius）：
- en: '![Contractive autoencoders](img/00137.jpeg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![收缩自编码器](img/00137.jpeg)'
- en: 'The loss function will be modified as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数将修改如下：
- en: '![Contractive autoencoders](img/00138.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![收缩自编码器](img/00138.jpeg)'
- en: Here, λ is the regularization factor. It is easy to see that the Frobenius norm
    of the Jacobian corresponds to L2 weight decay in the case of a linear encoder.
    The main difference is that for the linear case, the only way of achieving contraction
    would be by keeping weights very small. In case of a sigmoid non-linearity, we
    could also push the hidden units to their saturated regime.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，λ是正则化因子。很容易看出，雅可比的Frobenius范数在线性编码器的情况下对应于L2权重衰减。主要的区别在于对于线性情况，实现收缩的唯一方法是保持权重非常小。在sigmoid非线性的情况下，我们还可以推动隐藏单元进入饱和状态。
- en: Let's analyze the two terms.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这两个术语。
- en: The error *J* (the MSE or cross-entropy) pushes toward keeping the most possible
    information to perfectly reconstruct the original value.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 误差*J*（MSE或交叉熵）推动保留尽可能多的信息以完美重建原始值。
- en: The penalty pushes toward getting rid of all of that information such that the
    derivatives of the hidden units with respect to *X* are minimized. A large value
    means that the learned representation is too unstable with respect to input variations.
    We obtain a small value when we observe very little change to the hidden representations
    as we change the input values. In case of the limit of these derivatives being
    0, we would only keep the information that is invariant with respect to the input
    *X*. We are effectively getting rid of all of the hidden features that are not
    stable enough and too sensitive to small perturbations.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 处罚推动了摆脱所有这些信息，使得隐藏单元对*X*的导数最小化。大值意味着所学到的表示对于输入变化太不稳定。当我们观察到输入值变化时，所观察到的隐藏表示几乎没有变化时，我们得到一个小的值。在这些导数限制为0的情况下，我们只保留了相对于输入*X*不变的信息。我们实际上摆脱了所有不够稳定且对微小扰动过于敏感的隐藏特征。
- en: Let's suppose we have as input a lot of variations of the same data. In the
    case of images, they could be small rotations or different exposures of the same
    subject. In case of network traffic, they could be an increase/decrease of the
    packet header of the same type of traffic, maybe because of a packing/unpacking
    protocol.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的输入是同一数据的许多变化。在图像的情况下，它们可能是同一主题的小旋转或不同曝光。在网络流量的情况下，它们可能是同一类型流量的数据包头部的增加/减少，可能是由于包装/解包协议。
- en: If we only look at this dimension, the model is likely to be very sensitive.
    The Jacobian term would penalize the high sensitivity, but it is compensated by
    the low reconstruction error.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只看这个维度，模型很可能会非常敏感。雅可比项将惩罚高敏感性，但它会被低重构误差所抵消。
- en: In this scenario, we would have one unit that is very sensitive on the variation
    direction but not very useful for all other directions. For example, in the case
    of pictures, we still have the same subject; thus, all of the remaining input
    values are constant. If we don't observe variations on a given direction in the
    training data, we want to discard the feature.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们会有一个单位，对变化方向非常敏感，但对所有其他方向并不是很有用。例如，在图片的情况下，我们仍然拥有相同的主题；因此，所有其余的输入值都是常数。如果我们在训练数据中没有观察到给定方向的变化，我们希望丢弃该特征。
- en: H2O currently does not support contractive autoencoders; however, an open issue
    can be found at [https://0xdata.atlassian.net/browse/PUBDEV-1265](https://0xdata.atlassian.net/browse/PUBDEV-1265).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: H2O目前不支持收缩自编码器；但是，可以在[https://0xdata.atlassian.net/browse/PUBDEV-1265](https://0xdata.atlassian.net/browse/PUBDEV-1265)找到一个未解决的问题。
- en: Sparse autoencoders
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: Autoencoders, as we have seen them so far, always have the hidden layers smaller
    than the input.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器，截至目前我们所见的，隐藏层始终小于输入。
- en: The major reason is that otherwise, the network would have enough capability
    to just memorize exactly the input and reconstruct it perfectly as it is. Adding
    extra capacity to the network would just be redundant.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 主要原因是否则，网络将具有足够的能力只需记忆输入并完美地重构它。向网络添加额外的容量只会是多余的。
- en: Reducing the capacity of the network forces to learn based on a compression
    version of the input. The algorithm will have to pick the most relevant features
    that help better reconstruct the training data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 减少网络的容量会迫使基于输入的压缩版本进行学习。算法将不得不选择最相关的特征，以帮助更好地重构训练数据。
- en: There are situations though where compressing is not feasible. Let's consider
    the case where each input node is formed by independent random variables. If the
    variables are not correlated with each other, the only way of achieving compression
    is to get rid of some of them entirely. We are effectively emulating the behavior
    of PCA.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有些情况下压缩是不可行的。让我们考虑每个输入节点由独立随机变量形成的情况。如果变量彼此不相关，则实现压缩的唯一方法是完全摆脱其中一些。我们实际上正在模拟PCA的行为。
- en: In order to solve this problem, we can set a **sparsity** constraint on the
    hidden units. We will try to push each neuron to be inactive most of the time
    that corresponds to having the output of the activation function close to 0 for
    sigmoid and ReLU, -1 for tanh.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以在隐藏单元上设置一个**稀疏**约束。我们将尝试推动每个神经元大部分时间处于不活跃状态，这对于sigmoid和ReLU来说意味着激活函数的输出接近于0，对于tanh来说是-1。
- en: 'If we call ![Sparse autoencoders](img/00139.jpeg) the activation of hidden
    unit ![Sparse autoencoders](img/00140.jpeg) at layer ![Sparse autoencoders](img/00141.jpeg)
    when input is ![Sparse autoencoders](img/00142.jpeg), we can define the average
    activation of hidden unit ![Sparse autoencoders](img/00140.jpeg) as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们称呼隐藏单元![Sparse autoencoders](img/00140.jpeg)在输入为![Sparse autoencoders](img/00142.jpeg)时的激活为![Sparse
    autoencoders](img/00139.jpeg)，我们可以如下定义隐藏单元![Sparse autoencoders](img/00140.jpeg)的平均激活：
- en: '![Sparse autoencoders](img/00143.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![Sparse autoencoders](img/00143.jpeg)'
- en: Here, ![Sparse autoencoders](img/00144.jpeg) is the size of our training dataset
    (or batch of training data).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![Sparse autoencoders](img/00144.jpeg)是我们的训练数据集（或训练数据批次）的大小。
- en: The sparsity constraint consists of forcing ![Sparse autoencoders](img/00145.jpeg),
    where ![Sparse autoencoders](img/00146.jpeg) is the **sparsity parameter** bounded
    in the interval [1,0] and ideally close enough to 0.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏性约束包括强制![Sparse autoencoders](img/00145.jpeg)，其中![Sparse autoencoders](img/00146.jpeg)是**稀疏参数**，在区间[1,0]内且理想情况下足够接近0。
- en: The original paper ([http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf](http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf))
    recommends values near 0.05.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文([http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf](http://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf))建议值接近0.05。
- en: We model the average activation of each hidden unit as a Bernoulli random variable
    with mean ![Sparse autoencoders](img/00147.jpeg), and we want to force all of
    them to converge to a Bernoulli distribution with mean ![Sparse autoencoders](img/00146.jpeg)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个隐藏单元的平均激活值建模为具有均值![稀疏自编码器](img/00147.jpeg)的伯努利随机变量，并且我们希望所有这些都趋向于具有均值![稀疏自编码器](img/00146.jpeg)的伯努利分布。
- en: In order to do so, we need to add an extra penalty that quantifies the divergence
    of those two distributions. We can define this penalty based on the **Kullback-Leibler**
    (**KL**) divergence between the real distribution ![Sparse autoencoders](img/00148.jpeg)
    and the theoretical one ![Sparse autoencoders](img/00149.jpeg) we would like to
    achieve.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们需要添加一个额外的惩罚项，用于量化这两个分布之间的差异。我们可以根据我们希望实现的实际分布![稀疏自编码器](img/00148.jpeg)和理论分布![稀疏自编码器](img/00149.jpeg)之间的**Kullback-Leibler**（**KL**）散度来定义这个惩罚。
- en: 'In general, for discrete probability distributions *P* and *Q*, the *KL* divergence
    when information is measured in bits is defined as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，对于离散概率分布*P*和*Q*，以比特为单位测量信息时，*KL*散度定义如下：
- en: '![Sparse autoencoders](img/00150.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/00150.jpeg)'
- en: One requirement is that *P* is absolutely continuous with respect to *Q*, that
    is, ![Sparse autoencoders](img/00151.jpeg) for any measurable value of *x*. This
    is also written as ![Sparse autoencoders](img/00152.jpeg). Whenever ![Sparse autoencoders](img/00153.jpeg),
    the contribution of that term will be ![Sparse autoencoders](img/00154.jpeg) since
    that ![Sparse autoencoders](img/00155.jpeg).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个要求是*P*对*Q*绝对连续，即对于任意可测的值*P*都满足![稀疏自编码器](img/00151.jpeg)。这也可以写成![稀疏自编码器](img/00152.jpeg)。每当![稀疏自编码器](img/00153.jpeg)时，该项的贡献将是![稀疏自编码器](img/00154.jpeg)，因为那时的![稀疏自编码器](img/00155.jpeg)。
- en: 'In our case, the ![Sparse autoencoders](img/00156.jpeg) divergence of unit
    *j* would be as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，单元*j*的稀疏自编码器散度如下：
- en: '![Sparse autoencoders](img/00157.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/00157.jpeg)'
- en: This function has the property to be ![Sparse autoencoders](img/00154.jpeg)
    when the two means are equal and increase monotonically, otherwise until approaching
    8 when ![Sparse autoencoders](img/00158.jpeg) is close to
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个平均值相等且单调递增时，此函数的性质是![稀疏自编码器](img/00154.jpeg)，否则直到![稀疏自编码器](img/00158.jpeg)接近8时，![稀疏自编码器](img/00154.jpeg)会像这样增加。
- en: '![Sparse autoencoders](img/00154.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/00154.jpeg)'
- en: or 1.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 或1。
- en: 'The final loss function with extra penalty term added will be this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最终带有额外惩罚项的损失函数如下：
- en: '![Sparse autoencoders](img/00159.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏自编码器](img/00159.jpeg)'
- en: Here, *J* is the standard loss function (the RMSE), ![Sparse autoencoders](img/00160.jpeg)
    is the number of hidden units, and ß is a weight of the sparsity term.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*J*是标准损失函数（均方根误差），![稀疏自编码器](img/00160.jpeg)是隐藏单元的数量，ß是稀疏项的权重。
- en: This extra penalty will cause a small inefficiency to the backpropagation algorithm.
    In particular, the preceding formula will require an additional forward step over
    the whole training set to precompute the average activations ![Sparse autoencoders](img/00158.jpeg)
    before computing the backpropagation on each example.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个额外的惩罚将导致反向传播算法出现一些小的低效。特别是，前述公式在计算每个示例的反向传播之前，将需要经过整个训练集进行额外的前向步骤来预先计算平均激活值![稀疏自编码器](img/00158.jpeg)。
- en: Summary of autoencoders
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器总结
- en: Autoencoders are powerful unsupervised learning algorithms, which are getting
    popularity in fields such as anomaly detection or feature engineering, using the
    output of intermediate layers as features to train a supervised model instead
    of the raw input data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是强大的无监督学习算法，在异常检测或特征工程等领域越来越受欢迎，使用中间层的输出作为特征来训练监督模型，而不是使用原始输入数据。
- en: Unsupervised means they do not require labels or ground truth to be specified
    during training. They just work with whatever data you put as input as long as
    the network has enough capability to learn and represent the intrinsic existing
    relationships. That means that we can set both the size of the code layer (the
    reduced dimensionality *m*) but obtain different results depending on the number
    and size of the hidden layers, if any.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督意味着在训练过程中不需要指定标签或地面真相。只要网络有足够的能力学习和表示内在的存在关系，它们就可以处理输入的任何数据。这意味着我们可以设定编码层的大小（减少的维度*m*），但根据隐藏层的数量和大小来获得不同的结果。
- en: If we are building an autoencoder network, we want to achieve robustness in
    order to avoid wrong representations but at the same time not limit the capacity
    of the network by compressing the information through smaller sequential layers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在构建一个自动编码器网络，我们希望在避免错误表示的同时实现稳健性，但同时不要通过较小的顺序层压缩信息来限制网络的容量。
- en: Denoising, contractive, and autoencoders are all great techniques for solving
    those problems.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 除噪声、收缩和自动编码器都是解决这些问题的很好的技术。
- en: Adding noise is generally simpler and doesn't add complexity in the loss function,
    which results in less computation. On the other hand, the noisy input makes the
    gradient to be sampled and also discard part of the information in exchange for
    better features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪声通常更简单，而且不会在损失函数中增加复杂性，这会导致更少的计算。另一方面，嘈杂的输入使梯度变得不稳定，并且为了获得更好的特征而丢弃部分信息。
- en: Contractive autoencoders are very good at making the model more stable to small
    deviations from the training distribution. Thus, it is a very good candidate for
    reducing false alarms. The drawback is a sort of countereffect that increases
    the reconstruction error in order to reduce the sensibility.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩自动编码器非常擅长使模型对训练分布的小偏差更加稳定。因此，它是减少误报的一个很好的选择。缺点是一种反效果，它会增加重构误差以减少敏感性。
- en: Sparse autoencoders are probably the most complete around solution. It is the
    most expensive to compute for large datasets, but since the gradient is deterministic,
    it can be useful in case of second-order optimizers and, in general, to provide
    a good trade-off between stability and low reconstruction error.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自动编码器可能是最完整的解决方案。对于大型数据集来说，它计算成本最高，但由于梯度是确定的，它可以在二阶优化器的情况下提供很好的稳定性和低重构误差。
- en: Regardless of what choice you make, adopting a regularization technique is strongly
    recommended. They both come with hyper-parameters to tune, which we will see how
    to optimize in the corresponding *Tuning* section.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 不管你做出什么选择，采用正则化技术都是强烈推荐的。它们都带有超参数需调整，我们将在相应的*Tuning*部分中看到如何优化。
- en: In addition to the techniques described so far, it is worth mentioning variational
    autoencoders, which seem to be the ultimate solution for regularizing autoencoders.
    Variational autoencoders belong to the class of generative models. They don't
    just learn the structures that better describe the training data, they learn the
    parameters of a latent unit Gaussian distribution that can best regenerate the
    input data. The final loss function will be the sum of the reconstruction error
    and the KL divergence between the reconstructed latent variable and the Gaussian
    distribution. The encoder phase will generate a code consisting of a vector of
    means and a vector of standard deviations. From the code, we can characterize
    the latent distribution parameters and reconstruct the original input by sampling
    from that distribution.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 除了迄今为止描述的技术外，值得一提的是变分自动编码器，它似乎是正则化自动编码器的最终解决方案。变分自动编码器属于生成模型类别。它不仅学习了最好地描述训练数据的结构，还学习了潜在单位高斯分布的参数，这些参数可以最好地再现输入数据。最终的损失函数将是重构误差和重构的潜在变量之间的KL散度的总和。编码器阶段将生成由均值和标准差向量组成的代码。从代码中，我们可以表征潜在分布参数，并通过从该分布中采样重构原始输入。
- en: Restricted Boltzmann machines
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: In the early 90s, neural networks had largely gone out of fashion. The bulk
    of machine learning research was around other techniques, such as random forests
    and support vector machines. Neural networks with only a single hidden layer were
    less performant than these other techniques, and it was thought that deeper neural
    networks were too difficult to train.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在90年代初，神经网络基本上已经过时。机器学习研究的大部分内容是关于其他技术，如随机森林和支持向量机。只有一个隐藏层的神经网络表现不如这些其他技术，而且人们认为训练更深的神经网络太困难。
- en: The resurgence of interest in neural networks was spearheaded by Geoffrey Hinton,
    who, in 2004, led a team of researchers who proceeded to make a series of breakthroughs
    using restricted Boltzmann machines (RBM) and creating neural networks with many
    layers; they called this approach deep learning. Within 10 years, deep learning
    would go from being a niche technique to dominating every single AI competition.
    RBMs were part of the big breakthrough, allowing Hinton and others to get world
    record scores on a variety of image and speech recognition problems.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兴趣再次高涨于神经网络，由2004年由**Geoffrey Hinton**领导的研究团队率先使用受限玻尔兹曼机（RBM）取得一系列突破，创造了具有多层的神经网络；他们将这种方法称为深度学习。在10年内，深度学习从一种小众技术发展到主导每一个人工智能竞赛。RBM是这一巨大突破的一部分，使得Hinton和其他人在多种图像和语音识别问题上取得世界纪录成绩。
- en: In this section, we will look at the theory of how RBMs work, how to implement
    them, and how they can be combined into deep belief networks.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将研究RBM的工作原理，如何实现它们以及如何将它们结合成深度信念网络。
- en: 'A restricted Boltzmann machine looks a lot like a single layer of a neural
    network. There are a set of input nodes that have connections to another set of
    output nodes:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 一台受限玻尔兹曼机看起来很像是神经网络的一个单层。有一组输入节点与另一组输出节点相连：
- en: '![Restricted Boltzmann machines](img/00161.jpeg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/00161.jpeg)'
- en: Figure 1\. Restricted Boltzmann machine
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图1。受限玻尔兹曼机
- en: The way these output nodes are activated is also identical to an autoencoder.
    There is a weight between each input node and output node, the activation of each
    input nodes multiplied by this matrix of weight mappings, and a bias vector is
    then applied, and the sum for each output node is then put through a sigmoid function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输出节点被激活的方式也与自编码器完全相同。每个输入节点和输出节点之间有一个权重，每个输入节点的激活乘以这个权重映射矩阵，然后应用偏置向量，并且每个输出节点的总和将通过一个sigmoid函数。
- en: What makes a restricted Boltzmann machine different is what the activations
    represent, how we think about them, and the way in which they are trained. To
    begin with, when talking about RBMs rather than talking about input and output
    layers, we refer to the layers as visible and hidden. This is because, when training,
    the visible nodes represent the known information we have. The hidden nodes will
    aim to represent some variables that generated the visible data. This contrasts
    with an autoencoder, where the output layer doesn't explicitly represent anything,
    is just a constrained space through which the information is passed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使得受限玻尔兹曼机与众不同的是激活代表的内容、我们对它们的思考方式以及它们的训练方式。首先，当谈论RBM时，我们不是谈论输入和输出层，而是将层称为可见层和隐藏层。这是因为在训练时，可见节点代表我们已知的信息，而隐藏节点将旨在代表生成可见数据的一些变量。这与自编码器形成对比，自编码器的输出层不再明确地代表任何东西，只是通过信息传递的一种受限空间。
- en: The basis of learning the weights of a restricted Boltzmann machine comes from
    statistical physics and uses an **energy-based model** (**EBM**). In these, every
    state is put through an energy function, which relates to the probability of a
    state occurring. If an energy function returns a high value, we expect this state
    to be unlikely, rarely occurring. Conversely, a low result from an energy function
    means a state that is more stable and will occur more frequently.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 学习受限玻尔兹曼机的权重基础于统计物理学，并使用基于能量的模型（EBM）。在这些模型中，每个状态都经历一个能量函数，它与状态发生概率相关。如果能量函数返回一个高值，我们期望这种状态不太可能发生，很少发生。相反，能量函数的低结果意味着一个更稳定的状态，会更频繁发生。
- en: A good intuitive way of thinking about an energy function is to imagine a huge
    number of bouncy balls being thrown into a box. At first, all the balls have high
    energy and so will be bouncing very high. A state here would be a single snapshot
    in time of all the balls' positions and their associated velocities. These states,
    when the balls are bouncing, are going to be very transitory; they will only exist
    for moments and because of the range of movement of the balls, are very unlikely
    to reoccur. But as the balls start to settle, as the energy leaves the system,
    some of the balls will start to be increasingly stationary. These states are stable
    once it occurs once it never stops occurring. Eventually, once the balls have
    stopped bouncing and all become stationary, we have a completely stable state,
    which has high probability.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的直观思考能量函数的方式是想象将大量的弹跳球扔进一个箱子中。起初，所有的球都具有很高的能量，因此会弹得很高。这里的状态是所有球的位置和它们关联速度的一个时间点快照。当球在弹跳时，这些状态将会非常短暂；它们只会存在片刻，因为球的移动范围很大，很不可能再次出现。但是当球开始平静下来，当能量离开系统时，一些球将开始越来越静止。这些状态一旦发生一次就稳定了，一旦发生就不会停止。最终，当球停止弹跳并且都变成静止时，我们有一个完全稳定的状态，具有很高的概率。
- en: To give an example that applies to restricted Boltzmann machines, consider the
    task of learning a group of images of butterflies. We train our RBM on these images,
    and we want it to assign a low energy value to any image of a butterflies. But
    when given an image from a different set, say cars, it will give it a high energy
    value. Related objects, such as moths, bats, or birds, may have a more medium
    energy value.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以应用于受限波尔兹曼机的例子，考虑学习一组蝴蝶图像的任务。我们在这些图像上训练我们的RBM，并且希望它对任何蝴蝶图像分配低能量值。但是当给出来自不同集合的图像，比如汽车时，它会给它分配一个高能量值。相关的对象，如蛾子、蝙蝠或鸟，可能具有中等能量值。
- en: If we have an energy function defined, the probability of a given state is then
    given as follows:![Restricted Boltzmann machines](img/00162.jpeg)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们定义了一个能量函数，那么给定状态的概率就如下所示：![受限波尔兹曼机](img/00162.jpeg)
- en: Here, v is our state, E is our energy function, and Z is the partition function;
    the sum of all possible configurations of v is defined as follows:![Restricted
    Boltzmann machines](img/00163.jpeg)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，v是我们的状态，E是我们的能量函数，Z是分区函数；v的所有可能配置的总和定义如下：![受限波尔兹曼机](img/00163.jpeg)
- en: Hopfield networks and Boltzmann machines
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 霍普菲尔德网络和波尔兹曼机
- en: Before we go further into restricted Boltzmann machines, let's briefly talk
    about Hopfield networks; this should help in giving us a bit more of an understanding
    about how we get to restricted Boltzmann machines. Hopfield networks are also
    energy-based models, but unlike a restricted Boltzmann machine, it has only visible
    nodes, and they are all interconnected. The activation of each node will always
    be either -1 or +1.![Hopfield networks and Boltzmann machines](img/00164.jpeg)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们进一步讨论受限波尔兹曼机之前，让我们简要谈谈霍普菲尔德网络；这应该有助于我们对如何到达受限波尔兹曼机有更多的理解。霍普菲尔德网络也是基于能量的模型，但与受限波尔兹曼机不同，它只有可见节点，并且它们都是相互连接的。每个节点的激活始终为-1或+1。![霍普菲尔德网络和波尔兹曼机](img/00164.jpeg)
- en: Figure 2\. Hopfield network, all input nodes are interconnected
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2. 霍普菲尔德网络，所有输入节点都相互连接。
- en: When running a Hopfield network (or RBM), you have two options. The first option
    is that you can set the value of every visible node to the corresponding value
    of your data item you are triggering it on. Then you can trigger successive activations,
    where, at each activation, every node has its value updated based on the value
    of the other visible nodes it is connected to. The other option is to just initialize
    the visible nodes randomly and then trigger successive activations so it produces
    random examples of the data it has been trained on. This is often referred to
    as the network daydreaming.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行霍普菲尔德网络（或RBM）时，您有两个选项。第一个选项是您可以将每个可见节点的值设置为您正在触发的数据项的相应值。然后，您可以触发连续的激活，在每次激活时，每个节点的值都根据其连接到的其他可见节点的值进行更新。另一个选项是仅随机初始化可见节点，然后触发连续的激活，以产生其已经训练过的数据的随机示例。这通常被称为网络做白日梦。
- en: The activation of each of these visible nodes at the next time step is defined
    as follows:![Hopfield networks and Boltzmann machines](img/00165.jpeg)
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个时间步的每个可见节点的激活定义如下：![霍普菲尔德网络和波尔兹曼机](img/00165.jpeg)
- en: Here, W is a matrix defining the connection strength between each node v at
    time step t. A thresholding rule is then applied to a to get a new state for v:![Hopfield
    networks and Boltzmann machines](img/00166.jpeg)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，W是一个矩阵，定义了时间步骤t时每个节点v之间的连接强度。然后对a应用阈值规则，得到v的新状态：![霍普菲尔德网络和波尔兹曼机器](img/00166.jpeg)
- en: The weights W between nodes can be either positive or negative, which will lead
    nodes to attract or repel the other nodes in the network, when active. The Hopfield
    network also has a continuous variant, which simply involves replacing the thresholding
    function with the tanh function.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点之间的权重W可以是正的也可以是负的，在激活时会导致节点相互吸引或排斥。霍普菲尔德网络还有一个连续变体，它只是用tanh函数替换了阈值函数。
- en: The energy function for this network is as follows:![Hopfield networks and Boltzmann
    machines](img/00167.jpeg)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该网络的能量函数如下：![霍普菲尔德网络和波尔兹曼机器](img/00167.jpeg)
- en: In matrix notation, it is as follows:![Hopfield networks and Boltzmann machines](img/00168.jpeg)
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用矩阵表示，如下所示：![霍普菲尔德网络和波尔兹曼机器](img/00168.jpeg)
- en: The ![Hopfield networks and Boltzmann machines](img/00169.jpeg) in the equation
    is because we are going through every pair of i and j and so double-counting each
    connection (once when i=1 and j=2 then again when i=2 and j=1).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程中的![霍普菲尔德网络和波尔兹曼机器](img/00169.jpeg)是因为我们要遍历每对i和j，因此重复计算每个连接（当i=1且j=2时，然后当i=2且j=1时又计算一次）。
- en: 'The question that might arise here is: why have a model with only visible nodes?
    I will activate it with data I give it, then trigger some state updates. But what
    useful information does this new state give me? This is where the properties of
    energy-based models become interesting. Different configurations of W will vary
    the energy function associated with the states v. If we set the state of the network
    to something with a high energy function, that is an unstable state (think of
    the many bouncing balls); the network will over successive iterations move to
    a stable state.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里可能出现的问题是：为什么只有可见节点的模型？我会给它激活，然后触发一些状态更新。但是这个新状态给我提供了什么有用的信息呢？能量基模型的特性在这里变得有趣。不同的W配置将改变与状态v相关的能量函数。如果我们将网络状态设置为具有高能量函数的东西，即不稳定状态（想象一下许多弹跳的球）；网络会在连续的迭代中移动到一个稳定状态。
- en: If we train a Hopefield network on a dataset to learn a W with low energy for
    each item in the dataset, we can then make a corrupted sample from the data, say,
    by randomly swapping a few of the inputs between their minus one and plus one
    states. The corrupted samples may be in a high-energy state now because the corruption
    has made it unlikely to be a member of the original dataset. If we activate the
    visible nodes of the network on the corrupted sample, run a few more iterations
    of the network until it has reached a low-energy state; there is a good chance
    that it will have reconstructed the original uncorrupted pattern.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们对数据集训练霍普菲尔德网络，学习得到一个对数据集中每个条目都有低能量的W，然后我们可以从数据中创建一个损坏的样本，比如，通过随机交换一些输入的正负状态。因为损坏使得这些样本不太可能是原数据集的成员，所以这些损坏的样本可能现在处于高能量状态。如果我们激活网络的可见节点上的损坏样本，运行网络的更多迭代直到达到低能量状态；那么网络有很大的可能性已经重构了原始未损坏的模式。
- en: This leads to one use of the Hopfield networks being spelling correction; you
    can train it on a library of words, with the letters used in the words as inputs.
    Then if it is given a misspelled word, it may be able to find the correct original
    word. Another use of the Hopfield networks is as a content-addressable memory.
    One of the big differences between the computer memory and the human memory is
    that with computers, memories are stored with addresses. If a computer wants to
    retrieve a memory, it must know the exact place it stored it in. Human memory,
    on the other hand, can be given a partial section of that memory, the content
    of which can be used to recover the rest of it. For example, if I need to remember
    my pin number, I know the content I'm looking for and the properties of that content,
    a four digit number; my brain uses that to return the values.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这导致 Hopfield 网络的一个用途是拼写纠正；你可以在单词库上对其进行训练，其中包含单词中使用的字母作为输入。然后，如果给出一个拼写错误的单词，它可能能够找到正确的原始单词。Hopfield
    网络的另一个用途是作为内容寻址内存。计算机内存和人类内存之间的一个重要区别是，计算机内存是用地址存储的。如果计算机想要检索内存，它必须知道存储它的确切位置。另一方面，人类记忆可以给出该记忆的部分内容，该内容的特性可以用来恢复其余部分。例如，如果我需要记住我的密码，我知道我正在寻找的内容以及该内容的属性，一个四位数；我的大脑利用这一点返回值。
- en: Hopfield networks allow you to store content-addressable memories, which have
    led some people to suggest (speculatively) that the human memory system may function
    like a Hopfield network, with human dreaming being the attempt to learn the weights.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield 网络允许您存储内容寻址内存，这导致一些人推测人类记忆系统可能像 Hopfield 网络一样运作，人类的梦境是学习权重的尝试。
- en: The last use of a Hopfield network is that it can be used to solve optimization
    tasks, such as the traveling salesman task. The energy function can be defined
    to represent the cost of the task to be optimized, and the nodes of the network
    to represent the choices being optimized. Again, all that needs to be done is
    to minimize the energy function with respect to the weights of the network.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield 网络的最后一个用途是，它可以用于解决优化任务，例如旅行推销员任务。可以定义能量函数来表示要优化的任务的成本，网络的节点表示要优化的选择。同样，只需最小化网络权重的能量函数即可。
- en: Boltzmann machine
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Boltzmann 机器
- en: A Boltzmann machine is also known as a stochastic Hopfield network. In a Hopfield
    network, node activations are set based on the threshold; but in a Boltzmann machine,
    activation is stochastic. The value of a node in a Boltzmann machine is always
    set to either +1 or -1\. The probability of the node being in the state +1 is
    defined as follows:![Boltzmann machine](img/00170.jpeg)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Boltzmann 机器也被称为随机 Hopfield 网络。在 Hopfield 网络中，节点激活是基于阈值设置的；但在 Boltzmann 机器中，激活是随机的。Boltzmann
    机器中的节点值始终设置为 +1 或 -1。节点处于状态 +1 的概率定义如下: ![Boltzmann 机器](img/00170.jpeg)'
- en: Here, *a*[*i*] is the activation for that node as defined for the Hopfield network.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，*a*[*i*] 是针对 Hopfield 网络定义的该节点的激活。
- en: 'To learn the weights of our Boltzmann machine or Hopfield network, we want
    to maximize the likelihood of the dataset, given the W, which is simply the product
    of the likelihood for each data item:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习我们的 Boltzmann 机器或 Hopfield 网络的权重，我们希望最大化给定 W 的数据集的可能性，这简单地是每个数据项的可能性的乘积：
- en: '![Boltzmann machine](img/00171.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![Boltzmann 机器](img/00171.jpeg)'
- en: 'Here, W is the weights matrix, and *x**^((n))* is the nth sample from the dataset
    x of size N. Let''s now replace ![Boltzmann machine](img/00172.jpeg) with the
    actual likelihood from our Boltzmann machine:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，W 是权重矩阵，*x**^((n))* 是大小为 N 的数据集 x 的第 n 个样本。现在让我们用来自我们的 Boltzmann 机器的实际可能性替换
    ![Boltzmann 机器](img/00172.jpeg)：
- en: '![Boltzmann machine](img/00173.jpeg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![Boltzmann 机器](img/00173.jpeg)'
- en: 'Here, *Z* is as shown in the following equation:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Z* 如下方程所示：
- en: '![Boltzmann machine](img/00174.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![Boltzmann 机器](img/00174.jpeg)'
- en: If you look at the original definition of our energy function and Z, then *x'*
    should be every possible configuration of *x* based on the probability distribution
    *p(x)*. We now have W as part of our model, so the distribution will change to
    ![Boltzmann machine](img/00175.jpeg). Unfortunately, ![Boltzmann machine](img/00176.jpeg)
    is, if not completely intractable, at the very least, far too computationally
    expensive to compute. We would need to take every possible configuration of x
    across all possible W.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看我们能量函数和Z的原始定义，那么*x'*应该是基于概率分布*p(x)*的每个可能配置的*x*。我们现在的模型中有W的一部分，因此分布将更改为![玻尔兹曼机](img/00175.jpeg)。不幸的是，![玻尔兹曼机](img/00176.jpeg)如果不是完全棘手的，至少是计算成本太高，无法计算的。我们需要在所有可能的W的所有可能的x的配置中进行计算。
- en: One approach to computing an intractable probability distribution such as this
    is what's called Monte Carlo sampling. This involves taking lots of samples from
    the distribution and using the average of these samples to approximate the true
    value. The more samples we take from the distribution, the more accurate it will
    tend to be. A hypothetical infinite number of samples would be exactly the quantity
    we want, while 1 would be a very poor approximation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这种难以处理的概率分布的一种方法是所谓的蒙特卡罗采样。这涉及从分布中取大量样本，并使用这些样本的平均值来近似真实值。我们从分布中取的样本越多，它的准确性就越高。假设无限数量的样本将完全符合我们想要的数量，而1将是一个非常差的近似值。
- en: 'Since the products of probabilities can get very small, we will instead use
    the log probability; also, let''s also include the definition of *Z*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于概率的乘积可能变得非常小，因此我们将使用对数概率；另外，让我们也包括*Z*的定义：
- en: '![Boltzmann machine](img/00177.jpeg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![玻尔兹曼机](img/00177.jpeg)'
- en: 'Here, x'' is a sample of state of the network, as taken from the probability
    distribution ![Boltzmann machine](img/00175.jpeg) learned by the network. If we
    take the gradient of this with respect to a single weight between nodes i and
    j, it looks like this:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，x'是从网络学习的概率分布![玻尔兹曼机](img/00175.jpeg)中获取的网络状态样本。如果我们对节点i和j之间的单个权重取这个梯度，它看起来像这样：
- en: '![Boltzmann machine](img/00178.jpeg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![玻尔兹曼机](img/00178.jpeg)'
- en: 'Here, ![Boltzmann machine](img/00179.jpeg) across all N samples is simply the
    correlation between the nodes i and j. Another way to write this across all N
    samples, for each weight *i* and *j*, would be this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![玻尔兹曼机](img/00179.jpeg)在所有N个样本中只是节点i和j之间的相关性。另一种写法是对所有N个样本，对于每个权重*i*和*j*，可以写成这样：
- en: '![Boltzmann machine](img/00180.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![玻尔兹曼机](img/00180.jpeg)'
- en: This equation can be understood as being two phases of learning, known as positive
    and negative or, more poetically, waking and sleeping. In the positive phase,
    ![Boltzmann machine](img/00181.jpeg) increases the weights based on the data we
    are given. In the negative phase, ![Boltzmann machine](img/00182.jpeg), we draw
    samples from the model as per the weights we currently have, then move the weights
    away from that distribution. This can be thought of as reducing the probability
    of items generated by the model. We want our model to reflect the data as closely
    as possible, so we want to reduce the selection generated by our model. If our
    model was producing images exactly like the data, then the two terms would cancel
    each other out, and equilibrium would be reached.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程可以理解为学习的两个阶段，被称为正相和负相或者，更具诗意地说，醒和睡眠。在正相中，![玻尔兹曼机](img/00181.jpeg)根据我们所给的数据增加权重。在负相中，![玻尔兹曼机](img/00182.jpeg)，我们从模型中根据当前权重抽取样本，然后将权重远离该分布。这可以被认为是减少模型生成的项目的概率。我们希望我们的模型尽可能地反映数据，因此我们希望减少模型生成的选择。如果我们的模型产生的图像与数据完全相同，那么这两个术语将互相抵消，达到平衡。
- en: Boltzmann machines and Hopfield networks can be useful for tasks such as optimization
    and recommendation systems. They are very computationally expensive. Correlation
    must be measured between every single node, and then a range of Monte Carlo samples
    must be generated from the model for every training step. Also, the kinds of patterns
    it can learn are limited. If we are training on images to learn shapes, it cannot
    learn position invariant information. A butterfly on the left-hand side of an
    image is a completely different beast to a butterfly on the right-hand side of
    an image. In [Chapter 5](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "Chapter 5. Image Recognition"), *Image Recognition*, we will take a look at convolutional
    neural networks, which offers a solution to this problem.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼机和霍普菲尔德网络可用于优化和推荐系统等任务。它们需要大量的计算资源。必须测量每个节点之间的相关性，然后对模型进行每一步训练时的蒙特卡洛样本的范围。此外，它可以学习的模式种类有限。如果我们在图像上训练以学习形状，它无法学习位置不变的信息。图像左侧的蝴蝶与图像右侧的蝴蝶完全不同。在[第5章](part0030_split_000.html#SJGS1-c1ed1b54ca0b4e9fbb9fe2b2431d634f
    "第5章 图像识别")*图像识别*中，我们将看一下卷积神经网络，它提供了这个问题的解决方案。
- en: Restricted Boltzmann machine
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: 'Restricted Boltzmann machines make two changes from the Boltzmann machines:
    the first is that we add in hidden nodes, each of which is connected to every
    visible node, but not to each other. The second is that we remove all connections
    between visible nodes. This has the effect of making each node in the visible
    layer conditionally independent of each other if we are given the hidden layer.
    Nodes in the hidden layer are also conditionally independent, given the visible
    layer. We will also now add bias terms to both the visible and hidden nodes. A
    Boltzmann machine can also be trained with a bias term for each node, but this
    was left out of the equations for ease of notation.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机与玻尔兹曼机相比进行了两项改变：第一是添加了隐藏节点，每个节点都连接到每个可见节点，但彼此不连接。 第二是删除了可见节点之间的所有连接。这导致在给定隐藏层的情况下，可见层中的每个节点都是条件独立的。给定可见层后，隐藏层中的节点也是条件独立的。我们现在还将向可见和隐藏节点添加偏置项。玻尔兹曼机也可以在每个节点上训练有偏置项，但这在等式中被忽略了以便简化符号。
- en: Given that the data we have is only for the visible units, what we aim to do
    through training is find configurations of hidden units that lead to low-energy
    states when combined with the visible units. In our restricted Boltzmann machine,
    the state *x* is now the full configuration of both visible and hidden nodes.
    So, we will parameterize our energy function as E(v, h). It now looks like this:![Restricted
    Boltzmann machine](img/00183.jpeg)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们拥有的数据只针对可见单元，我们的目标是通过训练找到隐藏单元的配置，当与可见单元结合时，可以导致低能态。在我们的受限玻尔兹曼机中，状态*x*现在是可见和隐藏节点的完整配置。因此，我们将能量函数参数化为E(v,
    h)。它现在看起来像这样：![受限玻尔兹曼机](img/00183.jpeg)
- en: Here, a is the bias vector for the visible nodes, b is the bias vector for the
    hidden nodes, and W is the matrix of weights between the visible and hidden nodes.
    Here, ![Restricted Boltzmann machine](img/00184.jpeg) is the dot product of the
    two vectors, equivalent to ![Restricted Boltzmann machine](img/00185.jpeg). Now
    we need to take the gradients of our biases and weights with respect to this new
    energy function.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，a是可见节点的偏置向量，b是隐藏节点的偏置向量，W是可见和隐藏节点之间的权重矩阵。此处，![受限玻尔兹曼机](img/00184.jpeg) 是这两个向量的点积，等价于![受限玻尔兹曼机](img/00185.jpeg)。现在我们需要对新能量函数计算出的偏置和权重进行梯度下降。
- en: 'Because of the conditional independence between layers, we now have this:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于层之间的条件独立性，我们现在有这个：
- en: '![Restricted Boltzmann machine](img/00186.jpeg)'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/00186.jpeg)'
- en: '![Restricted Boltzmann machine](img/00187.jpeg)'
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/00187.jpeg)'
- en: These two definitions will be used in the normalization constant, Z. Since we
    longer have connections between visible nodes, our ![Restricted Boltzmann machine](img/00188.jpeg)
    has changed a lot:![Restricted Boltzmann machine](img/00189.jpeg)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个定义将用于归一化常数Z。由于我们不再有可见节点之间的连接，我们的![受限玻尔兹曼机](img/00188.jpeg) 发生了很大的变化：![受限玻尔兹曼机](img/00189.jpeg)
- en: Here, i is going through each visible node and j through each hidden node. If
    we take the gradient with respect to the different parameters, then what you eventually
    end up with is this:![Restricted Boltzmann machine](img/00190.jpeg)![Restricted
    Boltzmann machine](img/00191.jpeg)![Restricted Boltzmann machine](img/00192.jpeg)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，i 遍历每个可见节点，j 遍历每个隐藏节点。如果我们对不同参数取梯度，那么最终你会得到这个：![受限玻尔兹曼机](img/00190.jpeg)![受限玻尔兹曼机](img/00191.jpeg)![受限玻尔兹曼机](img/00192.jpeg)
- en: As before, ![Restricted Boltzmann machine](img/00193.jpeg) is approximated by
    taking the Monte Carlo samples from the distribution. These final three equations
    give us the complete way to iteratively train all the parameters for a given dataset.
    Training will be a case of updating our parameters by some learning rate by these
    gradients.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 与以前一样，![受限玻尔兹曼机](img/00193.jpeg)是通过从分布中取蒙特卡洛样本来近似的。这最后三个方程给出了我们迭代地训练给定数据集的所有参数的完整方法。训练将是通过这些梯度以某个学习速率更新我们的参数的情况。
- en: It is worth restating on a conceptual level what is going on here. v denotes
    the visible variables, the data from the world on which we are learning. h denotes
    the hidden variables, the variables we will train to generate visible variables.
    The hidden variables do not explicitly represent anything, but through training
    and minimizing the energy in the system, they should eventually find important
    components of the distribution we are looking at. For example, if the visible
    variables are a list of movies, with a value of 1 if a person likes the movie
    and 0 if they do not, the hidden variables may come to represent genres of movie,
    such as horror or comedy, because people may have genre preferences, so this is
    an efficient way to encode people's tastes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念层面上再次说明这里发生了什么是值得的。v 表示可见变量，即我们正在学习的来自世界的数据。h 表示隐藏变量，即我们将训练以生成可见变量的变量。隐藏变量并不明确地代表任何东西，但通过训练和最小化系统中的能量，它们最终应该找到我们正在查看的分布的重要组成部分。例如，如果可见变量是一系列电影，如果一个人喜欢这部电影，则其值为
    1，如果不喜欢，则为 0，那么隐藏变量可能会表示电影的流派，如恐怖片或喜剧片，因为人们可能有流派偏好，所以这是一种编码人们口味的有效方式。
- en: If we generate random samples of hidden variables and then activate the visible
    variables based on this, it should give us a plausible looking set of human tastes
    in movies. Likewise, if we set the visible variables to a random selection of
    movies over successive activations of the hidden and visible nodes, it should
    move us to find a more plausible selection.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们随机生成隐藏变量的样本，然后基于此激活可见变量，那么它应该给我们一个看起来合理的电影口味集。同样，如果我们将可见变量设置为在隐藏和可见节点的连续激活过程中的随机电影选择，那么它应该使我们找到一个更合理的选择。
- en: Implementation in TensorFlow
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中的实现
- en: Now that we have gone through the math, let's see what an implementation of
    it looks like. For this, we will use TensorFlow. TensorFlow is a Google open source
    mathematical graph library that is popular for deep learning. It does not have
    built-in neural network concepts, such as network layers and nodes, which a higher-level
    library such as Keres does; it is closer to a library such as Theano. It has been
    chosen here because being able to work directly on the mathematical symbols underlying
    the network allows the user to get a better understanding of what they are doing.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过了数学，让我们看看它的实现是什么样子的。为此，我们将使用 TensorFlow。TensorFlow 是一个谷歌开源数学图形库，用于深度学习很受欢迎。它没有内置的神经网络概念，比如网络层和节点，这是一个更高级别的库，比如
    Keras 才有；它更接近于像 Theano 这样的库。之所以选择它，是因为能够直接处理网络底层的数学符号，使用户能够更好地理解他们在做什么。
- en: TensorFlow can be installed directly via `pip` using the command `pip install
    tensorflow` for the CPU version or `pip install tensorflow-gpu` if you have NVidea
    GPU-enabled machine.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 可以直接通过 `pip` 安装，使用命令 `pip install tensorflow` 安装 CPU 版本，或者如果您有 NVidea
    GPU 启用的机器，则使用命令 `pip install tensorflow-gpu` 安装 GPU 版本。
- en: We will build a small restricted Boltzmann machine and train it on the MNIST
    collection of handwritten digits. We will have a smaller number of hidden nodes
    than visible nodes, which will force the RBM to learn patterns in the input. The
    success of the training will be measured in the network's ability to reconstruct
    the image after putting it through the hidden layer; for this, we will use the
    mean squared error between the original and our reconstruction. The full code
    sample is in the GitHub repo [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)
    in the `restricted_boltzmann_machine.py` file.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个小型的受限玻尔兹曼机，并对其进行 MNIST 手写数字集的训练。我们将比可见节点少的隐藏节点数，这将迫使 RBM 学习输入中的模式。训练的成功将通过网络在经过隐藏层后重构图像的能力来衡量；为此，我们将使用原始图像与我们的重构之间的均方误差。完整的代码示例在
    GitHub 仓库 [https://github.com/DanielSlater/PythonDeepLearningSamples](https://github.com/DanielSlater/PythonDeepLearningSamples)
    的 `restricted_boltzmann_machine.py` 文件中。
- en: 'Since the MNIST dataset is used so ubiquitously, TensorFlow has a nice built-in
    way to download and cache the MNIST dataset. It can be done by simply calling
    the following code:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 MNIST 数据集被如此广泛地使用，TensorFlow 有一种很好的内置方式来下载和缓存 MNIST 数据集。只需简单地调用以下代码即可完成：
- en: '[PRE0]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will download all the MNIST data into `MNIST_data` into the `"MNIST_data/"`
    directory, if it is not already there. The `mnist` object has properties, `train
    and test`, which allow you to access the data in NumPy arrays. The `MNIST` images
    are all sized 28 by 28, which means 784 pixels per image. We will need one visible
    node in our RBM for each pixel:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把所有 MNIST 数据下载到 `"MNIST_data/"` 目录下的 `MNIST_data` 文件夹中，如果还没有。`mnist` 对象有 `train`
    和 `test` 属性，允许您访问 NumPy 数组中的数据。MNIST 图像都是 28x28 大小的，即每个图像有 784 个像素。我们将为我们的 RBM
    需要每个像素一个可见节点：
- en: '[PRE1]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A placeholder object in TensorFlow represents values that will be passed in
    to the computational graph during usage. In this case, the `input_placeholder`
    object will hold the values of the `MNIST` images we give it. The `"float"` specifies
    the type of value we will be passing in, and the `shape` defines the dimensions.
    In this case, we want 784 values, one for each pixel, and the `None` dimension
    is for batching. Having a None dimension means that it can be of any size; so,
    this will allow us to send variable-sized batches of 784-length arrays:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的占位符对象表示在使用期间将传递到计算图中的值。在这种情况下，`input_placeholder` 对象将保存我们给它的 `MNIST`
    图像的值。`"float"` 指定了我们将传递的值的类型，`shape` 定义了维度。在这种情况下，我们想要 784 个值，每个像素一个，`None` 维度用于批处理。有一个
    None 维度意味着它可以是任何大小；因此，这将允许我们发送可变大小的 784 长度的数组的批次：
- en: '[PRE2]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`tf.variable` represents a variable on the computational graph. This is the
    *W* from our preceding equations. The argument passed to it is how the variable
    values should first be initialized. Here, we are initializing it from a normal
    distribution of size 784 by 300, the number of visible nodes to hidden nodes:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.variable` 表示计算图上的变量。这是前述方程中的 *W*。传递给它的参数是变量值应该如何首先初始化的方式。在这里，我们将其初始化为一个大小为
    784x300 的正态分布，即可见节点到隐藏节点的数量：'
- en: '[PRE3]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These variables will be the `a` and `b` from our preceding equation; they are
    initialised to all start with a value of 0\. Now we will program in the activations
    of our network:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量将是我们前述方程中的 `a` 和 `b`；它们被初始化为全部从值为 0 开始。现在我们将编写网络的激活：
- en: '[PRE4]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This represents the activation of the hidden nodes, ![Implementation in TensorFlow](img/00194.jpeg),
    in the preceding equations. After applying the `sigmoid` function, this activation
    could be put into a binomial distribution so that all values in the hidden layer
    go to 0 or 1, with the probability given; but it turns out an RBM trains just
    as well as the raw probabilities. So, there''s no need to complicate the model
    by doing this:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了在前述方程中隐藏节点的激活，![TensorFlow 实现](img/00194.jpeg)。应用 `sigmoid` 函数后，这个激活可以被放入二项分布中，以便隐藏层中的所有值都变为
    0 或 1，概率由给定；但事实证明，RBM 的训练与原始概率一样好。因此，没有必要通过这种方式复杂化模型：
- en: '[PRE5]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we have the reconstruction of the visible layer, ![Implementation in TensorFlow](img/00195.jpeg).
    As specified by the equation, we give it the `hidden_activation`, and from that,
    we get our sample from the visible layer:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了可见层的重构，![TensorFlow 实现](img/00195.jpeg)。根据方程的规定，我们给它 `hidden_activation`，从中我们得到了可见层的样本：
- en: '[PRE6]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now compute the final sample we need, the activation of the hidden nodes
    from our `visible_reconstruction`. This is equivalent to ![Implementation in TensorFlow](img/00196.jpeg)
    in the equations. We could keep going with successive iterations of hidden and
    visual activation to get a much more unbiased sample from the model. But it doing
    just one rotation works fine for training:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算我们需要的最终样本，来自我们的`visible_reconstruction`的隐藏节点的激活。这相当于方程中的 ![在TensorFlow中的实现](img/00196.jpeg)。我们可以继续使用连续迭代的隐藏和可视激活来从模型中获取一个更加无偏的样本。但只进行一次旋转就足够用于训练：
- en: '[PRE7]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we compute the positive and negative phases. The first phase is the correlation
    across samples from our mini-batch of the `input_placeholder`,![Implementation
    in TensorFlow](img/00197.jpeg) and the first `hidden_activation`, ![Implementation
    in TensorFlow](img/00194.jpeg). Then the negative phase gets the correlation between
    the `visible_reconstruction`, ![Implementation in TensorFlow](img/00195.jpeg)
    and the `final_hidden_activation`, ![Implementation in TensorFlow](img/00196.jpeg):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算正相位和负相位。第一阶段是我们的 `input_placeholder` 中的样本和第一个 `hidden_activation` 之间的相关性，![在TensorFlow中的实现](img/00197.jpeg)。然后，负相位获取
    `visible_reconstruction`，![在TensorFlow中的实现](img/00195.jpeg)和`final_hidden_activation`之间的相关性，![在TensorFlow中的实现](img/00196.jpeg)：
- en: '[PRE8]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Calling `assign_add` on our `weights` variable creates an operation that, when
    run, adds the given quantity to the variable. Here, 0.01 is our learning rate,
    and we scale the positive and negative phases by that:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`weights`变量上调用`assign_add`会创建一个操作，当运行时，会将给定的数量添加到变量中。这里，0.01是我们的学习率，我们通过它来缩放正相位和负相位：
- en: '[PRE9]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we create the operations for scaling the hidden and visible biases. These
    are also scaled by our 0.01 learning rate:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建缩放隐藏和可视偏置的操作。这些也会被我们的0.01学习率缩放：
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Calling `tf.group` creates a new operation than when called executes all the
    operation arguments together. We will always want to update all the weights in
    unison, so it makes sense to create a single operation for them:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`tf.group`创建一个新操作，当调用时会同时执行所有的操作参数。我们总是希望同时更新所有的权重，所以创建一个单独的操作是有意义的：
- en: '[PRE11]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This `loss_op` will give us feedback on how well we are training, using the
    MSE. Note that this is purely used for information; there is no backpropagation
    run against this signal. If we wanted to run this network as a pure autoencoder,
    we would create an optimizer here and activate it to minimize the `loss_op`:?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`loss_op`将为我们提供有关我们的训练效果的反馈，使用MSE。请注意，这仅用于信息；不会针对此信号进行反向传播。如果我们想将这个网络作为一个纯自动编码器来运行，我们将在这里创建一个优化器，并激活它以最小化`loss_op`：？
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we create a session object that will be used for running the computational
    graph. Calling `tf.initialize_all_variables()` is when everything gets initialized
    on to the graph. If you are running TensorFlow on the GPU, this is where the hardware
    is first interfaced with. Now that we have created every step for the RBM, let''s
    put it through a few epochs of running against MNIST and see how well it learns:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个将用于运行计算图的会话对象。调用`tf.initialize_all_variables()`时，所有内容都会初始化到图中。如果你在GPU上运行TensorFlow，这是硬件首先被接口化的地方。现在我们已经为RBM创建了每一步，让我们经历几个时代的MNIST运行，并看看它学到了多少：
- en: '[PRE13]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Every time we call `mnist.train.next_batch(100)`, 100 images are retrieved
    from the `mnist` dataset. At the end of each epoch, the `mnist.train.epochs_completed`
    is incremented by 1, and all the training data is reshuffled. If you run this,
    you may see results something like this:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们调用`mnist.train.next_batch(100)`，就会从`mnist`数据集中检索100张图片。在每个时代结束时，`mnist.train.epochs_completed`会增加1，所有训练数据都会重新洗牌。如果你运行这个，你可能会看到类似这样的结果：
- en: '[PRE14]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now see what an image reconstruction looks like by running the following
    command on the `mnist` data:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过在`mnist`数据上运行以下命令来看看图像重构是什么样的：
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here are some examples of what the reconstructed images with 300 hidden nodes
    look like:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些使用300个隐藏节点的重建图像的示例：
- en: '![Implementation in TensorFlow](img/00198.jpeg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![在TensorFlow中的实现](img/00198.jpeg)'
- en: Figure 3\. Reconstructions of digits using restricted Boltzmann machines with
    different numbers of hidden nodes
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图3. 使用不同数量的隐藏节点对受限玻尔兹曼机进行数字的重构
- en: As you can see, with 300 hidden nodes, less than half the number of pixels,
    it can still do an almost perfect reconstruction of the image, with only a little
    blurring around the edges. But as the number of hidden nodes decreases, so does
    the quality of the reconstruction. Going down to just 10 hidden nodes, the reconstructions
    can produce images that, to the human eye, look like the wrong digit, such as
    the 2 and 3 in Figure 3.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，使用300个隐藏节点，不到像素数量的一半，它仍然可以几乎完美地重建图像，只有边缘周围有一点模糊。但是随着隐藏节点数量的减少，重建的质量也会降低。将隐藏节点减少到只有10个时，重建的图像可能会产生对于人眼来说看起来像是错误的数字，例如图3中的2和3。
- en: Deep belief networks
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深信度网络
- en: 'If we imagine our RBM is learning a set of latent variables that generated
    our visible data and we were feeling inquisitive, we might wonder: can we then
    learn a second layer of latent variables that generated the latent variables for
    the hidden layer? The answer is yes, we can stack RBMs on top of previously trained
    RBMs to be able to learn second, third, fourth, and so on, order information about
    the visible data. These successive layers of RBMs allow the network to learn increasingly
    invariant representations of the underlying structure:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想象我们的RBM正在学习一组生成我们可见数据的潜在变量，并且我们感到好奇，我们可能会想知道：我们是否可以学习第二层生成隐藏层潜在变量的潜在变量？答案是肯定的，我们可以将先前训练的RBM堆叠在一起，以便学习有关可见数据的二阶、三阶、四阶等信息。这些连续的RBM层使网络能够学习越来越不变的表示底层结构：
- en: '![Deep belief networks](img/00199.jpeg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![深信度网络](img/00199.jpeg)'
- en: Figure 4 Deep belief network, containing many chained RBMs
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图4 深信度网络，包含许多链接的RBM
- en: These stacked RBMs are known as deep belief networks and were the deep networks
    used by Geoffrey Hinton in his 2002 paper *Training Products of Experts by Minimizing
    Contrastive Divergence*, to first produce the record-breaking results on MNIST.
    The exact technique he found useful was to train successive RBMs on data with
    only a slight reduction in the size of the layers. Once a layer was trained to
    the point where the reconstruction error was no longer improving, its weights
    were frozen, and a new RBM was stacked on top and again trained until error rate
    convergence. Once the full network was trained, a final supervised layer was put
    at the end in order to map the final RBM's hidden layer to the labels of the data.
    Then the weights of the whole network were used to construct a standard deep feed-forward
    neutral network, allowing those precalculated weights of the deep belief network
    to be updated by backpropagation.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这些堆叠的RBM被称为深度信度网络，是Geoffrey Hinton在他的2002年论文《通过最小化对比散度训练专家产品》中首次在MNIST上取得突破性成果时使用的深度网络。他发现有用的确切技术是在数据上训练连续的RBM，每一层的尺寸都只稍微减小。一旦一个层训练到重建误差不再改善的程度，它的权重就被冻结，然后在其上堆叠一个新的RBM，并再次训练直到误差率收敛。一旦整个网络训练完毕，最后添加一个监督层，以将最终RBM的隐藏层映射到数据的标签。然后使用整个网络的权重构建标准的深度前馈神经网络，使得这些预先计算的深度信度网络的权重能够通过反向传播进行更新。
- en: At first, these had great results, but over time, the techniques for training
    standard feed-forward networks have improved, and RBMs are no longer considered
    the best for image or speech recognition. They also have the problem that because
    of their two-phase nature, they can be a lot slower to train. But they are still
    very popular for things such as recommender systems and pure unsupervised learning.
    Also, from a theoretical point of view, using the energy-based model to learn
    deep representations is a very interesting approach and leaves the door open for
    many extensions that can be built on top of this approach.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，这些方法效果很好，但随着时间的推移，用于训练标准前馈网络的技术已经改进，RBMs不再被认为是图像或语音识别的最佳方法。它们还存在一个问题，就是由于其两阶段性质，它们的训练速度可能会慢得多。但是它们在诸如推荐系统和纯无监督学习等方面仍然非常受欢迎。此外，从理论上讲，使用能量模型来学习深度表示是一种非常有趣的方法，并且为许多可以建立在该方法之上的扩展留下了大门。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We have seen in this chapter two of the most powerful techniques at the core
    of many practical deep learning implementations: autoencoders and restricted Boltzmann
    machines.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了许多实际深度学习实现核心的两种最强大的技术：自动编码器和受限玻尔兹曼机。
- en: For both of them, we started with the shallow example of one hidden layer, and
    we explored how we can stack them together to form a deep neural network able
    to automatically learn high-level and hierarchical features without requiring
    explicit human knowledge.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于它们两个，我们都从一个隐藏层的浅层示例开始，并且探索了如何将它们堆叠在一起形成一个深度神经网络，能够自动学习高层次和分层次的特征，而不需要显式的人类知识。
- en: They both serve similar purposes, but there is a little substantial difference.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 它们都有类似的目的，但有一点小小的实质性差别。
- en: Autoencoders can be seen as a compression filter that we use to compress the
    data in order to preserve only the most informative part of it and be able to
    deterministically reconstruct an approximation of the original data. Autoencoders
    are an elegant solution to dimensionality reduction and non-linear compression
    bypassing the limitations of the principal component analysis (PCA) technique.
    The advantages of autoencoders are that they can be used as preprocessing steps
    for further classification tasks, where the output of each hidden layer is one
    of the possible levels of informative representations of the data, or a denoised
    and recovered version of it. Another great advantage is to exploit the reconstruction
    error as a measure of dissimilarity of a single point from the rest of the group.
    Such a technique is widely used for anomaly detection problems, where the relationships
    from what we observe and the internal representations are constant and deterministic.
    In the case of time-variant relationships or depending upon an observable dimension,
    we could group and train different networks in order to be adaptive, but once
    trained, the network assumes those relationships to not be affected by random
    variations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器可以被看作是我们用来压缩数据的压缩过滤器，以保留其中最具信息量的部分，并能够确定性地重构原始数据的近似。自动编码器是对维度约简和非线性压缩的优雅解决方案，绕过了主成分分析（PCA）技术的限制。自动编码器的优点是，它们可以用作进一步分类任务的预处理步骤，其中每个隐藏层的输出是数据信息表示的可能级别之一，或者是其去噪和恢复版本。另一个巨大的优点是利用重构误差作为一个单点与其余组的不相似性的度量。这样的技术广泛用于异常检测问题，其中我们观察到的内容与内部表示之间的关系是恒定的和确定性的。在时间变化关系或取决于可观察维度的情况下，我们可以分组和训练不同的网络，以便适应，但一旦训练完成，网络就假设这些关系不受随机变化的影响。
- en: 'On the other hand, RBM uses a stochastic approach to sample and adjust weights
    to minimize the reconstruction error. The intuition could be that there might
    exist some visible random variables and some hidden latent attributes, and the
    goal is to find how the two sets are connected to each other. To give an example,
    in the case of movie rating, we can have some hidden attributes, such as film
    genre, and some random observations, such as the rating and/or review. In such
    topology, we can also see the bias term as a way of adjusting the different inherent
    popularities of each movie. If we asked our users to rate which movie they like
    from a set made of *Harry Potter*, *Avatar*, *Lord of The Ring*, *Gladiator*,
    and *Titanic*, we might get a resulting network where two of the latent units
    could represent science fiction movies and Oscar-winning movies:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，RBM 使用随机方法对样本进行采样和调整权重，以最小化重构误差。直觉可能是存在一些可见的随机变量和一些隐藏的潜在属性，目标是找出这两组之间的联系。举个例子，在电影评分的情况下，我们可以有一些隐藏的属性，比如电影类型，以及一些随机的观察，比如评分和/或评论。在这样的拓扑结构中，我们还可以将偏差项看作是调整每部电影不同内在流行度的一种方式。如果我们让用户从由*哈利·波特*、*阿凡达*、*指环王*、*角斗士*和*泰坦尼克号*组成的集合中评价他们喜欢哪部电影，我们可能会得到一个结果网络，其中两个潜在单元可能代表科幻电影和奥斯卡获奖电影：
- en: '![Summary](img/00200.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![概要](img/00200.jpeg)'
- en: Example of possible RBM where only the links with a weight significantly different
    from 0 are drawn.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的 RBM 示例，仅绘制与权重明显不同于 0 的链接。
- en: Although the attributes of SF and Oscar-winning are deterministic (effectively,
    they are attributes of the movie), the ratings of the users are influenced by
    that in a probabilistic way. The learned weights are the parameters that characterize
    the probability distribution of the movie rating (for example, Harry Potter with
    five stars), given that the user likes a particular genre (for example, science
    fiction).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然科幻和奥斯卡获奖的属性是确定性的（实际上，它们是电影的属性），但用户的评级受到概率方式的影响。学习到的权重是表征电影评分的概率分布的参数（例如，哈利·波特获得五星），假设用户喜欢特定的类型（例如，科幻）。
- en: In such a scenario, where the relationships are not deterministic, we want to
    prefer using RBM to using an autoencoder.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种关系不确定的情况下，我们更倾向于使用受限玻尔兹曼机（RBM）而不是自编码器。
- en: In conclusion, unsupervised features learning is a very powerful methodology
    to enrich feature engineering with the minimum required knowledge and human interaction.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，无监督特征学习是一种非常强大的方法，可以用最少的知识和人为干预来丰富特征工程。
- en: Standing to a few benchmarks ([Lee, Pham and Ng, 2009] and [Le, Zhou and Ng,
    2011]) performed in order to measure the accuracy of different feature learning
    techniques, it was proved that unsupervised feature learning improved accuracy
    with respect to the current state of the art.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 根据一些基准测试（[Lee, Pham and Ng, 2009] 和 [Le, Zhou and Ng, 2011]）的结果，用于衡量不同特征学习技术准确性的，已经证明无监督特征学习相对于目前的最新技术有所提高。
- en: There are a few open challenges though. If you do have some knowledge, it is
    always good not to discard it. We could embed that knowledge in the form of priors
    during the initialization step, where we might handcraft the network topology
    and initial state accordingly.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，还存在一些挑战。如果您有一些知识，最好不要将其丢弃。我们可以在初始化阶段以先验的形式嵌入这些知识，在这一阶段我们可以手工设计网络拓扑和初始状态。
- en: Moreover, since neural networks are already hard to explain and are mostly approached
    as black box, having an understanding of at least the input features could help.
    In our unsupervised feature learning, we want to consume raw data directly. Hence,
    understanding how the model works becomes even harder.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于神经网络本身已经很难解释，并且大多数时候被视为黑匣子，因此至少了解输入特征可能有所帮助。在我们的无监督特征学习中，我们希望直接使用原始数据。因此，理解模型的工作原理变得更加困难。
- en: We will not address those issues in this book. We believe that it is too early
    to make some conclusions and that further evolutions of deep learning and the
    way people and businesses approach those applications will converge to a steady
    trustworthiness.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中不会涉及这些问题。我们认为现在下结论还为时过早，深度学习的进一步发展以及人们和企业处理这些应用的方式将会趋于稳定和可靠。
