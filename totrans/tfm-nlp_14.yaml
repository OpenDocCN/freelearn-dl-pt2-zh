- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpreting Black Box Transformer Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Million- to billion-parameter transformer models seem like huge black boxes
    that nobody can interpret. As a result, many developers and users have sometimes
    been discouraged when dealing with these mind-blowing models. However, recent
    research has begun to solve the problem with innovative, cutting-edge tools.
  prefs: []
  type: TYPE_NORMAL
- en: It is beyond the scope of this book to describe all of the explainable AI methods
    and algorithms. So instead, this chapter will focus on ready-to-use visual interfaces
    that provide insights for transformer model developers and users.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter begins by installing and running `BertViz` by *Jesse Vig*. Jesse
    did quite an excellent job of building a visual interface that shows the activity
    in the attention heads of a BERT transformer model. BertViz interacts with the
    BERT models and provides a well-designed interactive interface.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue to focus on visualizing the activity of transformer models
    with the **Language Interpretability Tool** (**LIT**). LIT is a non-probing tool
    that can use PCA or UMAP to represent transformer model predictions. We will go
    through PCA and use UMAP as well.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will visualize a transformer’s journey through the layers of a BERT
    model with dictionary learning. **Local Interpretable Model-agnostic Explanations**
    (**LIME**) provides practical functions to visualize how a transformer learns
    how to understand language. The method shows that transformers often begin by
    learning a word, then the word in the sentence context, and finally long-range
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will be able to interact with users to show visualizations
    of the activity of transformer models. BertViz, LIT, and visualizations through
    dictionary learning still have a long way to go. However, these nascent tools
    will help developers and users understand how transformer models work.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and running BertViz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running BertViz’s interactive interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between probing and non-probing methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Principal Component Analysis** (**PCA**) reminder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running LIT to analyze transformer outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing LIME
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running transformer visualization through dictionary learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-level polysemy disambiguation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing low-level, mid-level, and high-level dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing key transformer factors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first step will begin by installing and using BertViz.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer visualization with BertViz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Jesse Vig*’s article, *A Multiscale Visualization of Attention in the Transformer
    Model*, 2019, recognizes the effectiveness of transformer models. However, *Jesse
    Vig* explains that deciphering the attention mechanism is challenging. The paper
    describes the process of BertViz, a visualization tool.'
  prefs: []
  type: TYPE_NORMAL
- en: BertViz can visualize attention head activity and interpret a transformer model’s
    behavior.
  prefs: []
  type: TYPE_NORMAL
- en: BertViz was first designed to visualize BERT and GPT-3 models. In this section,
    we will visualize the activity of a BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now install and run BertViz.
  prefs: []
  type: TYPE_NORMAL
- en: Running BertViz
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It only takes five steps to visualize transformer attention heads and interact
    with them.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `BertViz.ipynb` notebook in the `Chapter14` directory in the GitHub
    repository of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to install `BertViz` and the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Installing BertViz and importing the modules'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The notebook installs `BertViz`, Hugging Face transformers, and the other basic
    requirements to implement the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The head view and model view libraries are now imported. We will now load the
    BERT model and tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Load the models and retrieve attention'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BertViz supports BERT, GPT-2, RoBERTa, and other models. You can consult BertViz
    on GitHub for more information: [https://github.com/jessevig/BertViz](https://github.com/jessevig/BertViz).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will run a `bert-base-uncased` model and a pretrained tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now enter our two sentences. You can try different sequences to analyze
    the behavior of the model. `sentence_b_start` will be necessary for *Step: 5 Model
    view*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! We are ready to interact with the visualization interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Head view'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We just have one final line to add to activate the visualization of the attention
    heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The words of the first layer (layer 0) are not the actual tokens, but the interface
    is educational. The 12 attention heads of each layer are displayed in different
    colors. The default view is set to layer 0, as shown in *Figure 14.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: The visualization of attention heads'
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to explore attention heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Processing and displaying attention heads'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each color above the two columns of tokens represents an attention head of the
    layer number. Choose a layer number and click on an attention head (color).
  prefs: []
  type: TYPE_NORMAL
- en: The words in the sentences are broken down into tokens in the attention. However,
    in this section, the word `tokens` loosely refers to `words` to help us understand
    how the transformer heads work.
  prefs: []
  type: TYPE_NORMAL
- en: 'I focused on the word `animals` in *Figure 14.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: Selecting a layer, an attention head, and a token'
  prefs: []
  type: TYPE_NORMAL
- en: '`BertViz` shows that the model made a connection between `animals` and several
    words. This is normal since we are only at layer 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Layer 1 begins to isolate words `animals` is related to, as shown in *Figure
    14.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: Visualizing the activity of attention head 11 of layer 1'
  prefs: []
  type: TYPE_NORMAL
- en: Attention head 11 makes a connection between `animals`, `people`, and `adopt`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we click on `cats`, some interesting connections are shown in *Figure 14.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: Visualizing the connections between cats and other tokens'
  prefs: []
  type: TYPE_NORMAL
- en: The word `cats` is now associated with `animals`. This connection shows that
    the model is learning that cats are animals.
  prefs: []
  type: TYPE_NORMAL
- en: You can change the sentences and then click on the layers and attention heads
    to visualize how the transformer makes connections. You will find limits, of course.
    The good and bad connections will show you how transformers work and fail. Both
    cases are valuable for explaining how transformers behave and why they require
    more layers, parameters, and data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how `BertViz` displays the model view.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Model view'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It only takes one line to obtain the model view of a transformer with `BertViz`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`BertViz` displays all of the layers and heads in one view, as shown in the
    view excerpt in *Figure 14.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant bâtiment  Description générée automatiquement](img/B17948_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: Model view mode of BertViz'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you click on one of the heads, you will obtain a head view with word-to-word
    and sentence-to-sentence options. You can then go through the attention heads
    to see how the transformer model makes better representations as it progresses
    through the layers. For example, *Figure 14.6* shows the activity of an attention
    head in the first layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte, noir, laser  Description générée automatiquement](img/B17948_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Activity of an attention head in the lower layers of the model'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the representation makes connections between the separator, `[SEP]`,
    and words, which does not make much sense. However, sometimes tokens are not activated
    in every attention head of every layer. Also, the level of training of a transformer
    model limits the quality of the interpretation.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, `BertViz` remains an interesting educational tool and interpretability
    tool for transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run the intuitive LIT tool.
  prefs: []
  type: TYPE_NORMAL
- en: LIT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LIT’s visual interface will help you find examples that the model processes
    incorrectly, dig into similar examples, see how the model behaves when you change
    a context, and more language issues related to transformer models.
  prefs: []
  type: TYPE_NORMAL
- en: LIT does not display the activities of the attention heads like `BertViz` does.
    However, it’s worth analyzing why things went wrong and trying to find solutions.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose a **Uniform Manifold Approximation and Projection** (**UMAP**)
    visualization or a PCA projector representation. PCA will make more linear projections
    in specific directions and magnitude. UMAP will break its projections down into
    mini-clusters. Both approaches make sense depending on how far you want to go
    when analyzing the output of a model. You can run both and obtain different perspectives
    of the same model and examples.
  prefs: []
  type: TYPE_NORMAL
- en: This section will use PCA to run LIT. Let’s begin with a brief reminder of how
    PCA works.
  prefs: []
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA takes data and represents it at a higher level.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are in your kitchen. Your kitchen is a 3D cartesian coordinate system.
    The objects in your kitchen are all at specific *x*, *y*, *z* coordinates too.
  prefs: []
  type: TYPE_NORMAL
- en: You want to cook a recipe and gather the ingredients on your kitchen table.
    Your kitchen table is a higher-level representation of the recipe in your kitchen.
  prefs: []
  type: TYPE_NORMAL
- en: The kitchen table is using a cartesian coordinate system too. But when you extract
    the *main features* of your kitchen to represent the recipe on your kitchen table,
    you are performing PCA. This is because you have displayed the principal components
    that fit together to make a specific recipe.
  prefs: []
  type: TYPE_NORMAL
- en: The same representation can be applied to NLP. For example, a dictionary is
    a list of words. But the words that mean something together constitute a representation
    of the principal components of a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: The PCA representation of sequences in LIT will help visualize the outputs of
    a transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main steps to obtain an NLP PCA representation are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance**: The numerical variance of a word in a dataset; the frequency
    and frequency of its meaning, for example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Covariance**: The variance of more than one word is related to that of another
    word in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eigenvalues and eigenvectors**: To obtain a representation in the cartesian
    system, we need the vectors and magnitudes representation of the covariances.
    The eigenvectors will provide the direction of the vectors. The eigenvalues will
    provide their magnitudes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deriving the data**: The last step is to apply the feature vectors to the
    original dataset by multiplying the row feature vector by the row data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data to display** = row of feature vector * row of data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA projections provide a clear linear visualization of the data points to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now run LIT.
  prefs: []
  type: TYPE_NORMAL
- en: Running LIT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can run LIT online or open it in a Google Colaboratory notebook. Click
    on the following link to access both options:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pair-code.github.io/lit/](https://pair-code.github.io/lit/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The tutorial page contains several types of NLP tasks to analyze:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pair-code.github.io/lit/tutorials/](https://pair-code.github.io/lit/tutorials/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this section, we will run LIT online and explore a sentiment analysis classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://pair-code.github.io/lit/tutorials/sentiment/](https://pair-code.github.io/lit/tutorials/sentiment/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Click on **Explore this demo yourself**, and you will enter the intuitive LIT
    interface. The transformer model is a small transformer model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Une image contenant texte  Description générée automatiquement](img/B17948_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Selecting a model'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can change the model by clicking on the model. You can test this type of
    model and similar ones directly on Hugging Face on its hosted API page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://huggingface.co/sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/sshleifer/tiny-distilbert-base-uncased-finetuned-sst-2-english)'
  prefs: []
  type: TYPE_NORMAL
- en: The NLP models might change on LIT’s online version based on subsequent updates.
    The concepts remain the same, just the models change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by selecting the PCA projector and the binary `0` or `1` classification
    label of the sentiment analysis of each example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Selecting the projector and type of label'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then go to the data table and click on a **sentence** and its classification
    **label**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Selecting a sentence'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm is stochastic so the output can vary from one run to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence will also appear in the datapoint editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Datapoint editor'
  prefs: []
  type: TYPE_NORMAL
- en: The datapoint editor allows you to change the context of the sentence. For example,
    you might want to find out what went wrong with a counterfactual classification
    that should have been in one class but ended up in another one. You can change
    the context of the sentence until it appears in the correct class to understand
    how the model works and why it made a mistake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sentence will appear in the PCA projector with its classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: PCA projector in a positive cluster'
  prefs: []
  type: TYPE_NORMAL
- en: You can click the data points in the PCA projector, and the sentences will appear
    in the datapoint editor under the sentence you selected. That way, you can compare
    results.
  prefs: []
  type: TYPE_NORMAL
- en: LIT contains a wide range of interactive functions you can explore and use.
  prefs: []
  type: TYPE_NORMAL
- en: The results obtained in LIT are not always convincing. However, LIT provides
    valuable insights in many cases. Also, it is essential to get involved in these
    emerging tools and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now visualize transformer layers through dictionary learning.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer visualization via dictionary learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer visualization via dictionary learning is based on transformer factors.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A transformer factor is an embedding vector that contains contextualized words.
    A word with no context can have many meanings, creating a polysemy issue. For
    example, the word `separate` can be a verb or an adjective. Furthermore, `separate`
    can mean disconnect, discriminate, scatter, and has many other definitions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Yun* et al., 2021, thus created an embedding vector with contextualized words.
    A word embedding vector can be constructed with sparse linear representations
    of word factors. For example, depending on the context of the sentences in a dataset,
    `separate` can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To ensure that a linear representation remains sparse, we don’t add 0 factors
    that would create huge matrices with 0 values. Thus, we do not include useless
    information such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The whole point is to keep the representation sparse by forcing the coefficients
    of the factors to be greater than 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden state for each word is retrieved for each layer. Since each layer
    progresses in its understanding of the representation of the word in the dataset
    of sentences, the latent dependencies build up. This sparse linear superposition
    of transformer factors becomes a dictionary matrix with a sparse vector of coefficients
    to be inferred that we can sum up as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In which:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_002.png) (phi) is the dictionary matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17948_14_003.png) is the sparse vector of coefficients to be inferred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Yun* et al., 2021, added, ![](img/B17948_14_004.png), Gaussian noise samples
    to force the algorithm to search for deeper representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, to ensure that the representation remains sparse, the equation must be
    written s.t. (such that) ![](img/B17948_14_005.png).
  prefs: []
  type: TYPE_NORMAL
- en: The authors refer to *X* as the set of hidden states of the layers and *x* as
    a sparse linear superposition of transformer factors that belongs to *X*.
  prefs: []
  type: TYPE_NORMAL
- en: 'They beautifully sum up their sparse dictionary learning model as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_006.png)'
  prefs: []
  type: TYPE_IMG
- en: In the dictionary matrix, ![](img/B17948_14_007.png):,c refers to a column of
    the dictionary matrix and contains a transformer factor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_008.png):,c is divided into three levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-level** transformer factors to solve polysemy problems through word-level
    disambiguation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mid-level** transformer factors take us further into sentence-level patterns
    that will bring vital context to the low level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High-level** transformer patterns that will help understand long-range dependencies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method is innovative, exciting, and seems efficient. However, there is no
    visualization functionality at this point. Therefore, *Yun* et al., 2021, created
    the necessary information for LIME, a standard interpretable AI method to visualize
    their findings.
  prefs: []
  type: TYPE_NORMAL
- en: The interactive transformer visualization page is thus based on LIME for its
    outputs. The following section is a brief introduction to LIME.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**LIME** stands for **Local Interpretable Model-Agnostic Explanations**. The
    name of this explainable AI method speaks for itself. It is *model-agnostic*.
    Thus, we can draw immediate consequences about the method of transformer visualization
    via dictionary learning:'
  prefs: []
  type: TYPE_NORMAL
- en: This method does not dig into the matrices, weights, and matrix multiplications
    of transformer layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The method does not explain how a transformer model works, as we did in *Chapter
    2*, *Getting Started with the Architecture of the Transformer Model*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, the method peeks into the mathematical outputs provided by
    the sparse linear superpositions of transformer factors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME does not try to parse all of the information in a dataset. Instead, LIME
    finds out whether a model is *locally reliable* by examining the features around
    a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: LIME does not apply to the model globally. It focuses on the local environment
    of a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: This is particularly efficient when dealing with NLP because LIME explores the
    context of a word, providing invaluable information on the model’s output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In visualization via dictionary learning, an instance *x* can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The interpretable representation of this instance is a binary vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_010.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal is to determine the local presence or absence of a feature or several
    features. In NLP, the features are tokens that can be reconstructed into words.
  prefs: []
  type: TYPE_NORMAL
- en: 'For LIME, *g* represents a transformer model or any other machine learning
    model. *G* represents a set of transformer models containing *g*, among other
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_011.png)'
  prefs: []
  type: TYPE_IMG
- en: LIME’s algorithm can thus be applied to any transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we know that:'
  prefs: []
  type: TYPE_NORMAL
- en: LIME targets a word and searches the local context for other words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME thus provides the local context of a word to explain why that word was
    predicted and not another one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring explainable AI such as LIME is not in the scope of this book on transformers
    for NLP. However, for more on LIME, see the *References* section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now see how LIME fits in the method of transformer visualization via dictionary
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore the visualization interface.
  prefs: []
  type: TYPE_NORMAL
- en: The visualization interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Visit the following site to access the interactive transformer visualization
    page: [https://transformervis.github.io/transformervis/](https://transformervis.github.io/transformervis/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization interface provides intuitive instructions to start analyzing
    a transformer factor of a specific layer in one click, as shown in *Figure 14.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Selecting a transformer factor'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have chosen a factor, you can click on the layer you wish to visualize
    for this factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: Visualize function per layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first visualization shows the activation of the factor layer by layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: Importance of a factor for each layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'Factor `421` focuses on the lexical field of `separate`, as shown in the lower
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.15: The representation of “separate” in the lower layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we visualize higher layers, longer-range representations emerge. Factor
    `421` began with the representation of `separate`. But at higher levels, the transformer
    began to form a deeper understanding of the factor and associated `separate` with
    `distinct`, as shown in *Figure 14.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17948_14_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.16: The higher-layer representations of a transformer factor'
  prefs: []
  type: TYPE_NORMAL
- en: Try several transformer factors to visualize how transformers expand their perception
    and understanding of language, layer by layer.
  prefs: []
  type: TYPE_NORMAL
- en: You will find many good examples and also poor results. Focus on the good examples
    to understand how a transformer makes its way through language learning. Use the
    poor results to understand why it made a mistake. Also, the transformer model
    used for the visualization interface is not the most powerful or well-trained
    one.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, get involved and stay in the loop of this ever-evolving field!
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can explore `Understanding_GPT_2_models_with_Ecco.ipynb`, which
    is in the GitHub repository of this book for this chapter. It shows you how transformers
    generate candidates before choosing a token. It is self-explanatory.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we saw how transformers learn the meaning of words layer by
    layer. A transformer generates candidates before making a choice. As shown in
    the notebook, a transformer model is stochastic and as such chooses among several
    top probabilities. Consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: What word would you choose at the end of the sentence? We all hesitate. So do
    transformers!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the GPT-2 model chooses the word `sky`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B17948_14_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.17: Completing a sequence'
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are other candidates the GPT-2 model may choose in another run, as
    shown in *Figure 14.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B17948_14_18.png)Figure
    14.18: The other candidates for the completion'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that `sky` appears in rank first. However, `morning` appears in rank
    second and could fit as well. If we run the model several times, we may obtain
    different outputs because the model is stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: It seems that the domain of AI and transformers is complete.
  prefs: []
  type: TYPE_NORMAL
- en: However, let’s see why humans still have a lot of work to do before we go.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring models we cannot access
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The visual interfaces explored in this chapter are fascinating. However, there
    is still a lot of work to do!
  prefs: []
  type: TYPE_NORMAL
- en: For example, OpenAI’s GPT-3 model runs online or through an API. Thus, we cannot
    access the weights of some **Software as a Service** (**SaaS**) transformer models.
    This trend will increase and expand in the years to come. Corporations that spend
    millions of dollars on research and computer power will tend to provide pay-as-you-go
    services, not open-source applications.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we had access to the source code or output weights of a GPT-3 model,
    using a visual interface to analyze the 9,216 attention heads (96 layers x 96
    heads) would be quite challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Finding what is wrong will still require some human involvement in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the polysemy issue of the word `coach` in English to French translation
    often represents a problem. In English, a coach can be a person who trains people,
    or a bus. The word `coach` exists in French, but it only applies to a person who
    trains people.
  prefs: []
  type: TYPE_NORMAL
- en: If you go to OpenAI AI GPT-3 playground, [https://openai.com/](https://openai.com/),
    and translate sentences containing the word `coach`, you might obtain mixed results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence 1 is translated correctly by the OpenAI engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`coach` is translated as a bus, which is fine. More context would be required.'
  prefs: []
  type: TYPE_NORMAL
- en: The outputs are stochastic, so the translation might be correct one time and
    false the time after.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Sentence 2 is mistranslated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This time, the GPT-3 engine missed the fact that `coach` meant a person, not
    a bus. The same stochastic runs will provide unstable outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we modify sentence 2 by adding context, we will obtain the proper translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The translation now contains the French word `coach` for the same definition
    of the English word `coach` in this sentence. More context was added.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s solutions, AI in general, and transformer models in particular, are
    continuously progressing. Furthermore, most Industry 4.0 AI-driven micro-decisions
    do not require the level of sophistical of NLP or translation tasks and are effective.
  prefs: []
  type: TYPE_NORMAL
- en: However, human intervention and development at the Cloud AI API level will still
    remain necessary for quite a long time!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer models are trained to resolve word-level polysemy disambiguation
  prefs: []
  type: TYPE_NORMAL
- en: low-level, mid-level, and high-level dependencies. The process is achieved by
    connecting training million- to trillion-parameter models. The task of interpreting
    these giant models seems daunting. However, several tools are emerging.
  prefs: []
  type: TYPE_NORMAL
- en: We first installed `BertViz`. We learned how to interpret the computations of
    the attention heads with an interactive interface. We saw how words interacted
    with other words for each layer.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter continued by defining the scope of probing and non-probing tasks.
    Probing tasks such as NER provide insights into how a transformer model represents
    language. However, non-probing methods analyze how the model makes predictions.
    For example, LIT plugged a PCA project and UMAP representations into the outputs
    of a BERT transformer model. We could then analyze clusters of outputs to see
    how they fit together.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we ran transformer visualization via dictionary learning. A user can
    choose a transformer factor to analyze and visualize the evolution of its representation
    from the lower layers to the higher layers of the transformer. The factor will
    progressively go from polysemy disambiguation to sentence context analysis and
    finally to long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: The tools of this chapter will evolve along with other techniques. However,
    the key takeaway of this chapter is that transformer model activity can be visualized
    and interpreted in a user-friendly manner. In the next chapter, we will discover
    new transformer models. We will also go through risk management methods to choose
    the best implementations for a transformer model project.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BertViz only shows the output of the last layer of the BERT model. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BertViz shows the attention heads of each layer of a BERT model. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BertViz shows how the tokens relate to each other. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LIT shows the inner workings of the attention heads like BertViz. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Probing is a way for an algorithm to predict language representations. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NER is a probing task. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA and UMAP are non-probing tasks. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LIME is model agnostic. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformers deepen the relationships of the tokens layer by layer. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Visual transformer model interpretation adds a new dimension to interpretable
    AI. (True/False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BertViz: *Jesse Vig,2019, A Multiscale Visualization of Attention in the Transformer
    Model,2019,* [https://aclanthology.org/P19-3007.pdf](https://aclanthology.org/P19-3007.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BertViz: [https://github.com/jessevig/BertViz](https://github.com/jessevig/BertViz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LIT, explanation of sentiment analysis representations: [https://pair-code.github.io/lit/tutorials/sentiment/](https://pair-code.github.io/lit/tutorials/sentiment/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LIT: [https://pair-code.github.io/lit/](https://pair-code.github.io/lit/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transformer visualization via dictionary learning*: *Zeyu Yun*, *Yubei Chen*,
    *Bruno A Olshausen*, *Yann LeCun*, 2021, Transformer visualization via dictionary
    learning: contextualized embedding as a linear superposition of transformer factors,
    [https://arxiv.org/abs/2103.15949](https://arxiv.org/abs/2103.15949)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformer visualization via dictionary learning: [https://transformervis.github.io/transformervis/](https://transformervis.github.io/transformervis/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/Transformers](https://www.packt.link/Transformers)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5134042288713321484.png)'
  prefs: []
  type: TYPE_IMG
